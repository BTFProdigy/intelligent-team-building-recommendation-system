Building a Bilingual WordNet-Like Lexicon: 
the New Approach and Algorithms 
Yang Liu, Shiwen Yu, Jiangsheng Yu 
Institute of Computaitional Linguistics, Peking Unviersity 
Beijing, 100871, China 
{liuyang, yusw, yujs} @ pku.edu.cn 
 
 
Abstract 
A bilingual concept MRD is of significance for 
IE, MT, WSD and the like. However, it is 
reasonably difficult to build such a lexicon for 
there exist two ontologies, also, the evolution of 
such a lexicon is quite challenging. In this 
paper, we would like to put forth the new 
approach to building a bilingual WordNet-like 
lexicon and to dwell on some of the pivotal 
algorithms. 
A characteristic of this new approach is to 
emphasize the inheritance and transformation 
of the existent monolingual lexicon.  On the one 
hand, we have extracted all the common 
knowledge in WordNet as the semantic basis 
for further use. On the other hand, we have 
developed a visualized developing tool for the 
lexicographers to interactively operate on to 
express the bilingual semantics. The bilingual 
lexicon has thus gradually come into being in 
this natural process. 
ICL now has benefited a lot by employing 
this new approach to build CCD (Chinese 
Concept Dictionary), a bilingual WordNet-like 
lexicon, in Peking University. 
1 Introduction 
As the processing of content information has 
nowadays become the center of NLP, a 
bilingual concept MRD is of increasingly great 
significance for IE, MT, WSD and the like. And 
it is for sure that the computational linguists 
would find such a lexicon indispensable and 
useful as semantic information when facing 
ambiguities in languages in their applications. 
At the same time, Princeton University?s 
WordNet, after so many years? development, 
has exerted a profound influence on semantic 
lexicons [Vossen, 1998]. 
      When building a Chinese-English bilingual 
concept MRD, we must take the issue of 
compatibility with WordNet into account. In 
other words, for each English concept in 
WordNet, there should exist a corresponding 
Chinese concept in the bilingual lexicon and 
vice versa. Such a bilingual lexicon can offer 
better reusability and openness. 
The Institute of Computational Linguistics 
(ICL), Peking University, with this point of 
view, has launched the Project CCD (Chinese 
Concept Dictionary). 
The expectant CCD might be described as 
follows [Yu et al 2001]: it should carry the 
main relations already defined in WordNet with 
more or less updates to reflect the reality of 
contemporary Chinese, and it should be a 
bilingual concept lexicon with the parallel 
Chinese-English concepts to be simultaneously 
included. 
      Such a bilingual WordNet-like lexicon of 
Chinese-English concepts can largely meet our 
need of applications. 
However, it is by no means easy to build 
such a lexicon. It is quite obvious that there 
synchronously exist two ontologies in the same 
lexicon. One is in the English culture and the 
other is in the Chinese culture. As there might 
be different concepts and relations in each 
language, the mapping of the relevant concepts 
in different languages is inevitable. Also, the 
evolution of such a lexicon with passing of 
time, an issue linked closely to the mapping 
issue, is quite challenging. 
In conclusion, it?s a quite demanding job to 
build such a lexicon, especially for the design of 
the approach and the realization of the 
developing tool. Any fruitful solution should 
give enough consideration to the complexity of 
these issues. 
2 The New Approach to Building a Bilingual 
WordNet-Like Lexicon 
The distinct principles of organization of 
WordNet can be described below: concepts, 
viz. synsets, act as the basic units of lexical 
semantics, and the hyponymy of the concepts 
acts as the basic relation among others. Upon 
this tree structure of hyponymy, there also exist 
some other semantic relations like holonymy, 
antonymy, attribute, entailment, cause, etc., 
which further interweave all the concepts in the 
lexicon into a huge semantic network, say 
99,643 synset nodes all told in WordNet 1.6. 
      What really counts and takes a lot of trouble 
in building WordNet itself is how to set up all 
these synsets and relations properly, and, how 
to maintain the semantic consistencies in case 
of frequent occurrences of modifications during 
the revision [Beckwith et al 1993]. As the 
desirable developing tool based directly on a 
large-scale network has not yet appeared, due to 
the connatural complexity of net structure, this 
problem is all the way a Gordian knot for the 
lexicographers. 
      To build a Chinese WordNet in the same 
route just as Princeton had taken and then to 
construct the mapping between these two 
WordNets may be not a satisfying idea. 
So, it is crucial that we had better find an 
approach to reusing the English common 
knowledge already described in WordNet as the 
semantic basis for Chinese when building the 
bilingual lexicon. And this kind of reusing 
should contain some capabilities of adjustments 
to the bilingual concepts besides word-for-word 
translations. If we can manage it, not only the 
building of the monolingual Chinese lexicon 
benefits but also the mapping between 
Chinese-English [Liu et al 2002]. Actually, the 
practice of mapping has now become a direct 
and dynamic process and the evolution of the 
bilingual lexicon is no longer a problem. A 
comparatively high efficiency may be achieved. 
Such are the essential ideas of the new 
solution.  A characteristic of this approach is to 
emphasize the inheritance and transformation 
of the already existent monolingual lexicon.  
Accordingly, it deals with 2 processes. The 
first process simply gets the semantic basis for 
further use and the lexicographers? work always 
focuses on the second. In fact, the bilingual 
lexicon has just gradually come into being in 
this more natural process. 
2.1 The Inheritance Process of WordNet 
This process is intended to extract the common 
hyponymy information in WordNet as the 
semantic basis for future use. 
      However, to extract the full hyponyms for a 
certain concept is by no means easy. As we 
have examined, the number of hyponyms for a 
synset ranges from 0 to 499 with a maximal 
hyponymy depth of 15 levels in WordNet. This 
shows the structure of the potential hyponymy 
tree is quite unbalanced. Due to this high 
complexity, the ordinary searching algorithm 
can hardly do. If one inputs the word entity as 
entry in WordNet 1.6 and tries to search its full 
hyponyms, he will get nothing but a note of 
failure. Sure enough, if the entry is not entity 
but another word, say entrance, the searching 
will probably do. The cases actually depend on 
the location of the entry word in the potential 
hyponymy tree in WordNet. The higher the 
level of the entry word, the less possibility of 
success the searching will have. 
      By now, we have got a refined searching 
algorithm for getting the full hyponymy 
information in WordNet [Liu et al 2002]. 
By and large, it involves a series of Two 
Way Scanning action and of Gathering/Sieving 
and Encoding action, with each round of the 
series intending to get information of nodes on 
one same level in the hyponymy tree. 
      By this special algorithm, the complexity of 
searching is greatly reduced. We can even get 
all the 45,148 hyponyms for the topmost entry 
word entity, in 100 or so seconds, on an 
ordinary PC. People who are interested in it can 
find more details about the algorithm in [Liu et 
al, 2002]. 
2.2 The Transformation Process of WordNet 
This process is for the lexicographers to 
interactively operate on the hyponymy tree to 
express the bilingual semantics. The bilingual 
lexicon will gradually come into being in this 
process. 
      For this task, we have designed and realized 
a visualized and data-sensitive tree control with 
8 well-defined operations on it, some of the 
pivotal algorithms for which will be discussed 
later. 
      After extracting the hyponymy information 
for each initial semantic unit in WordNet 
respectively, we then organize the information 
into a hyponymy tree by using the above tree 
control. Every tree node, viz. synset, still carries 
all other semantic relations already described in 
WordNet. The lexicographers can now operate 
on the tree interactively. 
The actual practices of the lexicographers 
are as follows: 
      (i) For each tree node in English, if there 
exists a corresponding Chinese concept, the 
lexicographers simply translate the English 
concept into Chinese. 
      (ii) If there does not, cases may be that the 
English concept is either too general or too 
specific for Chinese. 
      (ii1) For the former case, the lexicographers 
can create new hyponyms in Chinese for the 
English concept and link all these new 
hyponyms in Chinese with the English concept. 
(ii2) For the latter case, the lexicographers 
just delete the English concept in a special way, 
which means the English concept has no 
equivalent in Chinese and only links the 
English concept with its hypernym. 
In fact, all the above-mentioned semantic 
manipulations concerning hyponymy relation  
have already been encoded into the 8 visualized 
operations on the hyponymy tree. In addition, in 
the 8 operations, some other semantic relations 
already described in the synsets in WordNet are 
all properly dealt with through systematic and 
reasonable calculations. 
      We can see these adjustments clearly in the 
description of the algorithms. 
      Now, it is of much significance that the 
lexicographers need simply operate on the 
hyponymy tree to express their semantic 
intention and no longer care for lots of details 
about the background database, for the 
foreground operations have already fulfilled all 
the automatic modifications of the database. 
      In this way, the problems of mapping 
between the bilingual concepts and evolution of 
the bilingual lexicon are dynamically resolved. 
Our developing tool for building the 
bilingual WordNet-like lexicon has come out as 
below. 
 
 
 
 
 
 
 
 
 
 
 
The interface view shows the hyponymy 
tree for the entry food, which is one of the 25 
initial semantic units of noun in WordNet with 
the category value of 13. For the currently 
chosen node, the lexicographers can further 
adopt a proper operation on it when needed. 
This new kind of Visualized Auxiliary 
Construction of Lexicon is characteristic of the 
inheritance and transformation of the existent 
monolingual lexicon.  We call it Vacol model 
for short. 
      As we see, the new approach, in fact, is 
independent of any specific languages and 
actually offers a general solution for building a 
bilingual WordNet-like lexicon. 
3 Tree Operations and their Algorithms 
As the lexicographers always work on the tool, 
the visualized, data-sensitive tree control with 
operations on it is the key to the new approach. 
By now, we?ve schemed a set of algorithms 
based on the Treeview control in the Microsoft 
Visual Studio 6.0 and eventually implemented a 
data-sensitive tree control with operations on it. 
3.1 Tree Operations 
The 8 operations that we have semantically well 
defined are listed as follows. When choosing a 
synset node in the hyponymy tree, these are the 
operations from which the lexicographers can 
further adopt one. 
 
      [1] To add a synset as brother node; 
      [2] To add a synset as child node; 
      [3] To delete the synset node (not including 
its descendants if exist); 
[4] To delete the synset node (including all 
its descendants if exist); 
      [5] To cut the subtree; 
      [6] To copy the subtree; 
      [7] To paste the subtree as brother node; 
[8] To paste the subtree as child node. 
 
      These operations are all to edit the tree, with 
respectively No. 1, 2 for addition, No. 3, 4 for 
deletion, and No. 5, 6, 7, 8 for batch movement. 
      In fact, all these operations have been 
carefully decided on to make them concise 
enough, capable enough and semantically 
meaningful enough. 
      It is easy to prove that any facultative tree 
form can be attained by iterative practices of 
these 8 operations. 
3.2 Algorithms for the Tree Operations 
The data structure of a hyponymy tree with n 
nodes can be illustrated by the following table: 
 
Pos1 Ptr11 Ptr12 ? Ptr1m BasicInfo1 
Pos2 Ptr21 Ptr22 ? Ptr2m BasicInfo2 
? ? ? ? ? ? 
Posn Ptrn1 Ptrn2 ? Ptrnm BasicInfon 
 
      There are 3 parts of information in each 
record: the structural information {Posi}, the 
relation information {Ptri1 (viz. hyponymy), 
Ptri2, ? , Ptrim} and all other pieces of basic 
information {BasicInfoi} which are relevant 
only to the concept proper. 
      Among these 3 parts of information, {Posi} 
is used for the tree structure whereas both {Ptri1, 
Ptri2, ? , Ptrim} and {BasicInfoi} for lexical 
semantics. It should be noticed that Posi only 
stands for a special encoding for the tree in the 
foreground and is somewhat different from 
Ptri1, a relational pointer of hyponymy, which 
represents its specific semantics in the 
background database. And it is the relations in 
{Ptri2, ? , Ptrim} that have highly contributed to 
the dense net structure of WordNet. 
      After these analyses, we find that each 
operation should just properly deal with these 3 
parts of information. First, it is crucial that two 
sorts of consistencies should be maintained. 
One is that of the structural information {Posi} 
of the tree and the other is that of the relation 
information {Ptri1, Ptri2, ? , Ptrim} of the 
lexicon. Following that, the cases of the basic 
information {BasicInfoi} are comparatively 
simple for only English-Chinese translations 
are involved. 
      Before we can go on to dwell on the 
algorithms, we still need a little while to touch 
on the structural information {Posi}. When we 
say a position Posi, we actually mean the 
location of a certain node in the tree and it 
serves to organize the tree. For example, a Posi 
by the value ?005001002? is to represent such a 
location of a node in a tree: at the 1st level, its 
ancestor being the 5th; at the 2nd level, its 
ancestor being the 1st; and at the 3rd level, its 
ancestor viz. itself now being the 2nd. In fact, 
such an encoding onto a linear string does fully 
express the structural information in a tree and 
makes all the tree operations algorithms 
feasible by direct and systematic calculations of 
the new position. 
      If we don?t want to badger with much of the 
details, the algorithms for tree operations can be 
described in a general way. Although for each 
line of the pseudocode, there indeed are lots of 
jobs to do for the programmer. 
The algorithms described below are suitable 
for the non-batch-movement operations, viz. 
operations [1, 2, 3, 4]. And the batch-movement 
operations, viz. operations [5, 6, 7, 8], can be 
regarded as their iterative practices. 
 
The lexicographers trigger an action on nodei; 
IF the action is in operations [1, 2, 3, 4] 
    CASE the action 
       Operations [1]: 
           Add a node with its Pos = NewBrother (Posi); 
       Operations [2]: 
           Add a node with its Pos = NewChild (Posi); 
       Operations [3]: 
           Delete the node with Pos = Posi; 
       Operations [4]: 
    Delete all the nodes with their Pos satisfying 
conditions of being descendants of nodei; 
    END CASE 
    Recalculate Pos of the rest nodes in the table 
according to the operation and current Posi; 
    Replace all relevant Ptrj1, Ptrj2 , ? , Ptrjm with new 
ones according to the operation and current nodei; 
    Refresh the tree; 
ELSE IF 
The lexicographers translate current BasicInfoi from 
English to Chinese; 
END IF 
 
      The algorithms have some nice features. 
      Since the structural information {Pos}, 
defined as the primary key of the table, is kept 
in order, the maintenance of tree structure can 
always be completed in a single pass. 
      The maintenance of consistencies of the 
relation information {Ptrj1, Ptrj2, ? , Ptrjm} in 
the lexicon is also limited to a local section of 
the table. 
4 Conclusions 
ICL, Peking University has launched the 
Project CCD since Sept., 2000. Due to the nice 
features of the new approach, we do have 
benefited a lot by employing it to build CCD. 
By now, we have fulfilled more than 32,000 
Chinese-English concept pairs in noun. 
      In the near future, ICL wants to come to a 
total amount of 100,000 or so bilingual 
concepts, which might largely meet our need of 
applications. 
What is more, as the byproducts of the new 
approach and experiences, we have even found 
some errors and faults of semantic expressing 
with WordNet 1.6. 
For example, in the lexicon there are many 
occurrences of a node with multiple-father in 
the identical category (772 times in noun, e.g. 
{radish}) or a node with single-father in the 
other category (2,172 times in noun, e.g. 
{prayer_wheel}). 
In verb, there even exists a node with father 
being oneself (e.g. {reserve, hold, book}). 
      These phenomena are quite abnormal and 
puzzling according to the specification of 
WordNet.  Something may have gone wrong 
with the classification or implementation. 
      There are also many undisciplined locations 
of relational pointers (e.g. ?@? and ?~?, 
respectively 7 and 451 times in noun) in DAT 
files and some other problems. 
Acknowledgements 
This work is a component of researches on 
Chinese Information Extraction funded by 
National Foundation of Natural Science No. 
69973005 and Project 985 in Peking Univ. 
We are especially grateful to Prof. WANG 
Fengxin and Prof. LU Chuan, our linguistics 
advisors, for their unforgettable discussion and 
support. Many thanks go to the fellows who 
have participated in and collaborated on the 
work, among whom we would like to mention 
Mr. ZHANG Huarui, Ms. SONG Chunyan, Dr. 
LI Zuowen, Ms. ZAN Hongying and others. 
Thanks also to the participants to the 1st Global 
WordNet Conference 2002, Mysore, India, for 
their valuable advice and comment. 
References 
Beckwith, R., Miller, G. A. and Tengi, R. (1993) 
Design and Implementation of the WordNet Lexical 
Database and Searching Software. Description of 
WordNet. 
 
Carpuat, M. and Ngai, G. et al (2002) Creating a 
Bilingual Ontology: A Corpus-Based Approach for 
Aligning WordNet and HowNet. GWC2002, India, 
pp 284-292. 
 
Chang, J. S. and You, G. N. et al (2002) Building a 
Bilingual Wordnet and Semantic Concordance from 
Corpus and MRD. WCLS2002, Taipei, China, pp 
209-224. 
 
Cook, G. and Barbara, S. (1995) Principles & 
Practice in Applied Linguistics. Oxford: Oxford 
University Press. 
 
Fellbaum, C. (1993) English Verbs as a Semantic 
Net. Description of WordNet. 
 
Fellbaum, C. (1999) WordNet: an Electronic Lexical 
Database. Cambridge, Mass.: MIT Press. 
 
Kamps, J. (2002) Visualizing WordNet Structure. 
GWC2002, India, pp 182-186. 
 
Keil, F. C. (1979) Smantic and Conceptual 
Development: an Ontological Perspective. 
Cambridge, Mass.: Harvard University Press. 
 
Liu, Y., Yu, J. S., Yu, S. W. (2002)  A Tree-Structure 
Solution for the Development of ChineseNet. 
GWC2002, India, pp 51-56. 
 
Miller, G. A. (1993) Noun in WordNet: a Lexical 
Inheritance System. Description of WordNet. 
 
Miller, G. A. et al (1993) Introduction to WordNet: 
An On-line Lexical Database. Description of 
WordNet. 
 
Pavelek, P., Pala, K. (2002) VisDic ? a New Tool for 
WordNet Editing. GWC2002, India, pp 192-195. 
 
Touretzky, D. S. (1986) The Mathematics of 
Inheritance Systems. Los Altos, Calif.: Morgan 
Kaufmann. 
 
Vossen, P. (1998) EuroWordNet: a Multilingual 
Database with Lexical Semantic Networks. 
Dordrecht: Kluwer. 
 
Wong, S. H. S. and Pala, K. (2002) Chinese 
Characters and Top Ontology in EuroWordNet. 
GWC2002, India, pp 122-133. 
 
Yu, J. S. (2002) Evolution of WordNet-Like Lexicon. 
GWC2002, India, pp 134-142. 
 
Yu, J. S. and Yu, S. W. et al (2001) Introduction to 
CCD. ICCC2001, Singapore, pp 361-366. 
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 89?97,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Maximum Entropy based Rule Selection Model for
Syntax-based Statistical Machine Translation
Qun Liu1 and Zhongjun He1,2 and Yang Liu1 and Shouxun Lin1
1Key Laboratory of Intelligent Information Processing
Institute of Computing Technology, Chinese Academy of Sciences
Beijing, 100190, China
2Graduate University of Chinese Academy of Sciences
Beijing, 100049, China
{liuqun,zjhe,yliu,sxlin}@ict.ac.cn
Abstract
This paper proposes a novel maximum en-
tropy based rule selection (MERS) model
for syntax-based statistical machine transla-
tion (SMT). The MERS model combines lo-
cal contextual information around rules and
information of sub-trees covered by variables
in rules. Therefore, our model allows the de-
coder to perform context-dependent rule se-
lection during decoding. We incorporate the
MERS model into a state-of-the-art linguis-
tically syntax-based SMT model, the tree-
to-string alignment template model. Experi-
ments show that our approach achieves signif-
icant improvements over the baseline system.
1 Introduction
Syntax-based statistical machine translation (SMT)
models (Liu et al, 2006; Galley et al, 2006; Huang
et al, 2006) capture long distance reorderings by us-
ing rules with structural and linguistical information
as translation knowledge. Typically, a translation
rule consists of a source-side and a target-side. How-
ever, the source-side of a rule usually corresponds
to multiple target-sides in multiple rules. Therefore,
during decoding, the decoder should select a correct
target-side for a source-side. We call this rule selec-
tion.
Rule selection is of great importance to syntax-
based SMT systems. Comparing with word selec-
tion in word-based SMT and phrase selection in
phrase-based SMT, rule selection is more generic
and important. This is because that a rule not only
contains terminals (words or phrases), but also con-
NP
DNP
NP
X 1
DEG
NPB
NN
X 2
NN
NP
DNP
NP
X 1
DEG
NPB
NN
X 2
NN
X 1 X 2 levels X 2 standard of X 1
Figure 1: Example of translation rules
tains nonterminals and structural information. Ter-
minals indicate lexical translations, while nontermi-
nals and structural information can capture short or
long distance reorderings. See rules in Figure 1 for
illustration. These two rules share the same syntactic
tree on the source side. However, on the target side,
either the translations for terminals or the phrase re-
orderings for nonterminals are quite different. Dur-
ing decoding, when a rule is selected and applied to a
source text, both lexical translations (for terminals)
and reorderings (for nonterminals) are determined.
Therefore, rule selection affects both lexical transla-
tion and phrase reordering.
However, most of the current syntax-based sys-
tems ignore contextual information when they se-
lecting rules during decoding, especially the infor-
mation of sub-trees covered by nonterminals. For
example, the information of X 1 and X 2 is not
recorded when the rules in Figure 1 extracted from
the training examples in Figure 2. This makes the
decoder hardly distinguish the two rules. Intuitively,
information of sub-trees covered by nonterminals as
well as contextual information of rules are believed
89
NP
DNP
X 1 :NP DEG
NPB
X 2 :NN NN
NP
DNP
X 1 :NP DEG
NPB
X 2 :NN NN
industrial products manufacturing levels overall standard of the match
Figure 2: Training examples for rules in Figure 1
to be helpful for rule selection.
Recent research showed that contextual infor-
mation can help perform word or phrase selec-
tion. Carpuat and Wu (2007b) and Chan et
al. (2007) showed improvents by integrating word-
sense-disambiguation (WSD) system into a phrase-
based (Koehn, 2004) and a hierarchical phrase-
based (Chiang, 2005) SMT system, respectively.
Similar to WSD, Carpuat and Wu (2007a) used con-
textual information to solve the ambiguity prob-
lem for phrases. They integrated a phrase-sense-
disambiguation (PSD) model into a phrase-based
SMT system and achieved improvements.
In this paper, we propose a novel solution for
rule selection for syntax-based SMT. We use the
maximum entropy approach to combine rich con-
textual information around a rule and the informa-
tion of sub-trees covered by nonterminals in a rule.
For each ambiguous source-side of translation rules,
a maximum entropy based rule selection (MERS)
model is built. Thus the MERS models can help the
decoder to perform a context-dependent rule selec-
tion.
Comparing with WSD (or PSD), there are some
advantages of our approach:
? Our approach resolves ambiguity for rules with
multi-level syntactic structure, while WSD re-
solves ambiguity for strings that have no struc-
tures;
? Our approach can help the decoder perform
both lexical selection and phrase reorderings,
while WSD can help the decoder only perform
lexical selection;
? Our method takes WSD as a special case, since
a rule may only consists of terminals.
In our previous work (He et al, 2008), we re-
ported improvements by integrating a MERS model
into a formally syntax-based SMT model, the hier-
archical phrase-based model (Chiang, 2005). In this
paper, we incorporate the MERS model into a state-
of-the-art linguistically syntax-based SMT model,
the tree-to-string alignment template (TAT) model
(Liu et al, 2006). The basic differences are:
? The MERS model here combines rich informa-
tion of source syntactic tree as features since
the translation model is linguistically syntax-
based. He et al (2008) did not use this in-
formation.
? In this paper, we build MERS models for all
ambiguous source-sides, including lexicalized
(source-side which only contains terminals),
partially lexicalized (source-side which con-
tains both terminals and nonterminals), and un-
lexicalized (source-side which only contains
nonterminals). He et al (2008) only built
MERS models for partially lexicalized source-
sides.
In the TAT model, a TAT can be considered as a
translation rule which describes correspondence be-
tween source syntactic tree and target string. TAT
can capture linguistically motivated reorderings at
short or long distance. Experiments show that by
incorporating MERS model, the baseline system
achieves statistically significant improvement.
This paper is organized as follows: Section 2
reviews the TAT model; Section 3 introduces the
MERS model and describes feature definitions; Sec-
tion 4 demonstrates a method to incorporate the
MERS model into the translation model; Section 5
reports and analyzes experimental results; Section 6
gives conclusions.
2 Baseline System
Our baseline system is Lynx (Liu et al, 2006),
which is a linguistically syntax-based SMT system.
For translating a source sentence fJ1 = f1...fj ...fJ ,
Lynx firstly employs a parser to produce a source
syntactic tree T (fJ1 ), and then uses the source
syntactic tree as the input to search translations:
90
e?I1 = argmaxe?I1Pr(e
I
1|f
J
1 )(1)
= argmaxe?I1Pr(T (f
J
1 )|f
J
1 )Pr(e
I
1|T (f
J
1 ))
In doing this, Lynx uses tree-to-string alignment
template to build relationship between source syn-
tactic tree and target string. A TAT is actually a
translation rule: the source-side is a parser tree with
leaves consisting of words and nonterminals, the
target-side is a target string consisting of words and
nonterminals.
TAT can be learned from word-aligned, source-
parsed parallel corpus. Figure 4 shows three types
of TATs extracted from the training example in Fig-
ure 3: lexicalized (the left), partially lexicalized
(the middle), unlexicalized (the right). Lexicalized
TAT contains only terminals, which is similar to
phrase-to-phrase translation in phrase-based model
except that it is constrained by a syntactic tree on the
source-side. Partially lexicalized TAT contains both
terminals and non-terminals, which can be used for
both lexical translation and phrase reordering. Un-
lexicalized TAT contains only nonterminals and can
only be used for phrase reordering.
Lynx builds translation model in a log-linear
framework (Och and Ney, 2002):
P (eI1|T (f
J
1 )) =(2)
exp[
?
m ?mhm(e
I
1, T (f
J
1 ))]?
e? exp[
?
m ?mhm(e
I
1, T (f
J
1 ))]
Following features are used:
? Translation probabilities: P (e?|T? ) and P (T? |e?);
? Lexical weights: Pw(e?|T? ) and Pw(T? |e?);
? TAT penalty: exp(1), which is analogous to
phrase penalty in phrase-based model;
? Language model Plm(eI1);
? Word penalty I .
In Lynx, rule selection mainly depends on trans-
lation probabilities and lexical weights. These four
scores describe how well a source tree links to a tar-
get string, which are estimated on the training cor-
pus according to occurrence times of e? and T? . There
IP
NPB
NN NN NN
VP
VV VPB
VV
The incomes of city and village resident continued to grow
Figure 3: Word-aligned, source-parsed training example.
NN NPB
NN
X 1
NN NN
NPB
NN
X 1
NN
X 2
NN
X 3
city and village incomes of X 1 resident X 3 X 1 X 2
Figure 4: TATs learned from the training example in Fig-
ure 3.
are no features in Lynx that can capture contextual
information during decoding, except for the n-gram
language model which considers the left and right
neighboring n-1 target words. But this information
it very limited.
3 The Maximum Entropy based Rule
Selection Model
3.1 The model
In this paper, we focus on using contextual infor-
mation to help the TAT model perform context-
dependent rule selection. We consider the rule se-
lection task as a multi-class classification task: for
a source syntactic tree T? , each corresponding target
string e? is a label. Thus during decoding, when a
TAT ?T? , e??? is selected, T? is classified into label e??,
actually.
A good way to solve the classification problem is
the maximum entropy approach:
Prs(e?|T? , T (Xk)) =(3)
exp[
?
i ?ihi(e?, C(T? ), T (Xk))]
?
e??
exp[
?
i ?ihi(e??, C(T? ), T (Xk))]
91
where T? and e? are the source tree and target string of
a TAT, respectively. hi is a binary feature functions
and ?i is the feature weight of hi. C(T? ) defines local
contextual information of T? . Xk is a nonterminal in
the source tree T? , where k is an index. T (Xk) is the
source sub-tree covered by Xk.
The advantage of the MERS model is that it uses
rich contextual information to compute posterior
probability for e? given T? . However, the transla-
tion probabilities and lexical weights in Lynx ignore
these information.
Note that for each ambiguous source tree, we
build a MERS model. That means, if there are
N source trees extracted from the training corpus
are ambiguous (the source tree which corresponds
to multiple translations), thus for each ambiguous
source tree Ti (i = 1, ..., N ), a MERS model Mi
(i = 1, ..., N ) is built. Since a source tree may cor-
respond to several hundreds of target translations at
most, the feature space of a MERS model is not pro-
hibitively large. Thus the complexity for training a
MERS model is low.
3.2 Feature Definition
Let ?T? , e?? be a translation rule in the TAT model.
We use f(T? ) to represent the source phrase covered
by T? . To build a MERS model for the source tree T? ,
we explore various features listed below.
1. Lexical Features (LF)
These features are defined on source words.
Specifically, there are two kinds of lexical fea-
tures: external features f?1 and f+1, which
are the source words immediately to the left
and right of f(T? ), respectively; internal fea-
tures fL(T (Xk)) and fR(T (Xk)), which are
the left most and right most boundary words of
the source phrase covered by T (Xk), respec-
tively.
See Figure 5 (a) for illustration. In
this example, f?1=t??ga?o, f+1=zh?`za`o,
fL(T (X1))=go?ngye`, fR(T (X1))=cha?np??n.
2. Parts-of-speech (POS) Features (POSF)
These features are the POS tags of the source
words defined in the lexical features: P?1,
P+1, PL(T (Xk)), PR(T (Xk)) are the POS
tags of f?1, f+1, fL(T (Xk)), fR(T (Xk)), re-
VP
VV
t??ga?o
DNP
X 1 :NP
NN
go?ngye`
NN
cha?np??n
DEG
de
NPB
NN
zh?`za`o
(a) Lexical Features
VP
VV
t??ga?o
DNP
X 1 :NP
NN
go?ngye`
NN
cha?np??n
DEG
de
NPB
NN
zh?`za`o
(b) POS Features
DNP
X 1 :NP
2 words
DEG
de
NP
DNP
X 1 :NP DEG
de
(c) Span Feature (d) Parent Feature
NP
DNP
X 1 :NP DEG
de
NPB
(e) Sibling Feature
Figure 5: Illustration of features of theMERSmodel. The
source tree of the TAT is ? DNP(NP X 1 ) (DEG de)?.
Gray nodes denote information included in the feature.
92
spectively. POS tags can generalize over all
training examples.
Figure 5 (b) shows POS features. P?1=VV,
P+1=NN, PL(T (X1))=NN, PR(T (X1))=NN.
3. Span Features (SPF)
These features are the length of the source
phrase f(T (Xk)) covered by T (Xk). In Liu?s
TATmodel, the knowledge learned from a short
span can be used for a larger span. This is not
reliable. Thus we use span features to allow the
MERS model to learn a preference for short or
large span.
In Figure 5 (c), the span of X 1 is 2.
4. Parent Feature (PF)
The parent node of T? in the parser tree of the
source sentence. The same source sub-tree may
have different parent nodes in different training
examples. Therefore, this feature may provide
information for distinguishing source sub-trees.
Figure 5 (d) shows that the parent is a NP node.
5. Sibling Features (SBF)
The siblings of the root of T? . This feature con-
siders neighboring nodes which share the same
parent node.
In Figure 5 (e), the source tree has one sibling
node NPB.
Those features make use of rich information
around a rule, including the contextual information
of a rule and the information of sub-trees covered
by nonterminals. They are never used in Liu?s TAT
model.
Figure 5 shows features for a partially lexicalized
source tree. Furthermore, we also build MERS mod-
els for lexicalized and unlexicalized source trees.
Note that for lexicalized tree, features do not include
the information of sub-trees since there is no nonter-
minals.
The features can be easily obtained by modify-
ing the TAT extraction algorithm described in (Liu
et al, 2006). When a TAT is extracted from a
word-aligned, source-parsed parallel sentence, we
just record the contextual features and the features of
the sub-trees. Then we use the toolkit implemented
by Zhang (2004) to train MERS models for the am-
biguous source syntactic trees separately. We set the
iteration number to 100 and Gaussian prior to 1.
4 Integrating the MERS Models into the
Translation Model
We integrate the MERS models into the TAT model
during the translation of each source sentence. Thus
the MERS models can help the decoder perform
context-dependent rule selection during decoding.
For integration, we add two new features into the
log-linear translation model:
? Prs(e?|T? , T (Xk)). This feature is computed by
the MERS model according to equation (3),
which gives a probability that the model select-
ing a target-side e? given an ambiguous source-
side T? , considering rich contextual informa-
tion.
? Pap = exp(1). During decoding, if a source
tree has multiple translations, this feature is set
to exp(1), otherwise it is set to exp(0). Since
the MERS models are only built for ambiguous
source trees, the first feature Prs(e?|T? , T (Xk))
for non-ambiguous source tree will be set to
1.0. Therefore, the decoder will prefer to
use non-ambiguous TATs. However, non-
ambiguous TATs usually occur only once in the
training corpus, which are not reliable. Thus
we use this feature to reward ambiguous TATs.
The advantage of our integration is that we need
not change the main decoding algorithm of Lynx.
Furthermore, the weights of the new features can be
trained together with other features of the translation
model.
5 Experiments
5.1 Corpus
We carry out experiments on Chinese-to-English
translation. The training corpus is the FBIS cor-
pus, which contains 239k sentence pairs with 6.9M
Chinese words and 8.9M English words. For the
language model, we use SRI Language Modeling
Toolkit (Stolcke, 2002) with modified Kneser-Ney
smoothing (Chen and Goodman, 1998) to train two
tri-gram language models on the English portion of
93
No. of No. of No. of ambiguous
Type
TATs source trees source trees
% ambiguous
Lexicalized 333,077 16,367 14,380 87.86
Partially Lexicalized 342,767 38,497 28,397 73.76
Unlexicalized 83,024 7,384 5,991 81.13
Total 758,868 62,248 48,768 78.34
Table 1: Statistical information of TATs filtered by test sets of NIST MT 2003 and 2005.
System
Features
P (e?|T? ) P (T? |e?) Pw(e?|T? ) Pw(T? |e?) lm1 lm2 TP WP Prs AP
Lynx 0.210 0.016 0.081 0.051 0.171 0.013 -0.055 0.403 - -
+MERS 0.031 0.008 0.020 0.080 0.152 0.014 0.027 0.270 0.194 0.207
Table 2: Feature weights obtained by minimum error rate training on the development set. The first 8 features are used
by Lynx. TP=TAT penalty, WP=word penalty, AP=ambiguous TAT penalty. Note that in fact, the positive weight for
WP and AP indicate a reward.
the training corpus and the Xinhua portion of the Gi-
gaword corpus, respectively. NIST MT 2002 test set
is used as the development set. NIST MT 2003 and
NIST MT 2005 test sets are used as the test sets.
The translation quality is evaluated by BLEU met-
ric (Papineni et al, 2002), as calculated by mteval-
v11b.pl with case-insensitive matching of n-grams,
where n = 4.
5.2 Training
To train the translation model, we first run GIZA++
(Och and Ney, 2000) to obtain word alignment in
both translation directions. Then the word alignment
is refined by performing ?grow-diag-final? method
(Koehn et al, 2003). We use a Chinese parser de-
veloped by Deyi Xiong (Xiong et al, 2005) to parse
the Chinese sentences of the training corpus.
Our TAT extraction algorithm is similar to Liu et
al. (2006), except that we make some tiny modifica-
tions to extract contextual features for MERS mod-
els. To extract TAT, we set the maximum height of
the source sub-tree to h = 3, the maximum number
of direct descendants of a node of sub-tree to c = 5.
See (Liu et al, 2006) for specific definitions of these
parameters.
Table 1 shows statistical information of TATs
which are filtered by the two test sets. For each type
(lexicalized, partially lexicalized, unlexicalized) of
TATs, a great portion of the source trees are am-
biguous. The number of ambiguous source trees ac-
counts for 78.34% of the total source trees. This in-
dicates that the TAT model faces serious rule selec-
tion problem during decoding.
5.3 Results
We use Lynx as the baseline system. Then the
MERS models are incorporated into Lynx, and
the system is called Lynx+MERS. To run the
decoder, Lynx and Lynx+MERS share the same
settings: tatTable-limit=30, tatTable-threshold=0,
stack-limit=100, stack-threshold=0.00001. The
meanings of the pruning parameters are the same to
Liu et al (2006).
We perform minimum error rate training (Och,
2003) to tune the feature weights for the log-linear
model to maximize the systems?s BLEU score on the
development set. The weights are shown in Table 2.
These weights are then used to run Lynx and
Lynx+MERS on the test sets. Table 3 shows the
results. Lynx obtains BLEU scores of 26.15 on
NIST03 and 26.09 on NIST05. Using all features
described in Section 3.2, Lynx+MERS finally ob-
tains BLEU scores of 27.05 on NIST03 and 27.28
on NIST05. The absolute improvements is 0.90
and 1.19, respectively. Using the sign-test described
by Collins et al (2005), both improvements are
statistically significant at p < 0.01. Moreover,
Lynx+MERS also achieves higher n-gram preci-
sions than Lynx.
94
Test Set System BLEU-4
Individual n-gram precisions
1 2 3 4
NIST03
Lynx 26.15 71.62 35.64 18.64 9.82
+MERS 27.05 72.00 36.72 19.51 10.37
NIST05
Lynx 26.09 70.39 35.12 18.53 10.11
+MERS 27.28 71.16 36.19 19.62 10.95
Table 3: BLEU-4 scores (case-insensitive) on the test sets.
5.4 Analysis
The baseline system only uses four features for
rule selection: the translation probabilities P (e?|T? )
and P (T? |e?); and the lexical weights Pw(e?|T? ) and
Pw(T? |e?). These features are estimated on the train-
ing corpus by the maximum likelihood approach,
which does not allow the decoder to perform a con-
text dependent rule selection. Although Lynx uses
language model as feature, the n-gram language
model only considers the left and right n-1 neigh-
boring target words.
The MERS models combines rich contextual in-
formation as features to help the decoder perform
rule selection. Table 4 shows the effect of different
feature sets. We test two classes of feature sets: the
single feature (the top four rows of Table 4) and the
combination of features (the bottom five rows of Ta-
ble 4). For the single feature set, the POS tags are
the most useful and stable features. Using this fea-
ture, Lynx+MERS achieves improvements on both
the test sets. The reason is that POS tags can be gen-
eralized over all training examples, which can alle-
viate the data sparseness problem.
Although we find that some single features may
hurt the BLEU score, they are useful in combina-
tion of features. This is because one of the strengths
of the maximum entropy model is that it can in-
corporate various features to perform classification.
Therefore, using all features defined in Section 3.2,
we obtain statistically significant improvements (the
last row of Table 4). In order to know how the
MERS models improve translation quality, we in-
spect the 1-best outputs of Lynx and Lynx+MERS.
We find that the first way that theMERSmodels help
the decoder is that they can perform better selection
for words or phrases, similar to the effect of WSD
or PSD. This is because that lexicalized and partially
lexicalized TAT contains terminals. Considering the
Feature Sets NIST03 NIST05
LF 26.12 26.32
POSF 26.36 26.21
PF 26.17 25.90
SBF 26.47 26.08
LF+POSF 26.61 26.59
LF+POSF+SPF 26.70 26.44
LF+POSF+PF 26.81 26.56
LF+POSF+SBF 26.68 26.89
LF+POSF+SPF+PF+SBF 27.05 27.28
Table 4: BLEU-4 scores on different feature sets.
following examples:
? Source:
? Reference: Malta is located in southern Eu-
rope
? Lynx: Malta in southern Europe
? Lynx+MERS: Malta is located in southern Eu-
rope
Here the Chinese word ? ? is incor-
rectly translated into ?in? by the baseline system.
Lynx+MERS produces the correct translation ?is lo-
cated in?. That is because, the MERS model consid-
ers more contextual information for rule selection.
In the MERS model, Prs(in| ) = 0.09, which is
smaller than Prs(is located in| ) = 0.14. There-
fore, the MERS model prefers the translation ?is lo-
cated in?. Note that here the source tree (VV )
is lexicalized, and the role of the MERS model is
actually the same as WSD.
The second way that the MERS models help the
decoder is that they can perform better phrase re-
orderings. Considering the following examples:
95
? Source: [ ]1 [ ]2
...
? Reference: According to its [development
strategy]2 [in the Chinese market]1 ...
? Lynx: Accordance with [the Chinese market]1
[development strategy]2 ...
? Lynx+MERS: According to the [development
strategy]2 [in the Chinese market]1
The syntactic tree of the Chinese phrase ?
? is shown in Figure 6. How-
ever, there are two TATs which can be applied to the
source tree, as shown in Figure 7. The baseline sys-
tem selects the left TAT and produces a monotone
translation of the subtrees ?X 1 :PP? and ?X 2 :NPB?.
However, Lynx+MERS uses the right TAT and per-
forms correct phrase reordering by swapping the two
source phrases. Here the source tree is partially lex-
icalized, and both the contextual information and
the information of sub-trees covered by nontermi-
nals are considered by the MERS model.
6 Conclusion
In this paper, we propose a maximum entropy based
rule selection model for syntax-based SMT. We
use two kinds information as features: the local-
contextual information of a rule, the information of
sub-trees matched by nonterminals in a rule. During
decoding, these features allow the decoder to per-
form a context-dependent rule selection. However,
this information is never used in most of the current
syntax-based SMT models.
The advantage of the MERS model is that it can
help the decoder not only perform lexical selection,
but also phrase reorderings. We demonstrate one
way to incorporate the MERS models into a state-
of-the-art linguistically syntax-based SMT model,
the tree-to-string alignment model. Experiments
show that by incorporating the MERS models, the
baseline system achieves statistically significant im-
provements.
We find that rich contextual information can im-
prove translation quality for a syntax-based SMT
system. In future, we will explore more sophisti-
cated features for the MERS model. Moreover, we
will test the performance of the MERS model on
large scale corpus.
NP
DNP
PP DEG
NPB
in Chinese market of
development strategy
Figure 6: Syntactic tree of the source phrase ?
?.
NP
DNP
PP
X 1
DEG
NPB
X 2
NP
DNP
PP
X 1
DEG
NPB
X 2
X 1 X 2 X 2 X 1
Figure 7: TATs which can be used for the source phrase
? ?.
Acknowledgements
We would like to thank Yajuan Lv for her valuable
suggestions. This work was supported by the Na-
tional Natural Science Foundation of China (NO.
60573188 and 60736014), and the High Technology
Research and Development Program of China (NO.
2006AA010108).
References
Marine Carpuat and Dekai Wu. 2007a. How phrase
sense disambiguation outperforms word sense disam-
biguation for statistical machine translation. In 11th
Conference on Theoretical and Methodological Issues
in Machine Translation, pages 43?52.
Marine Carpuat and Dekai Wu. 2007b. Improving sta-
tistical machine translation using word sense disam-
biguation. In Proceedings of EMNLP-CoNLL 2007,
pages 61?72.
Yee Seng Chan, Hwee Tou Ng, and David Chiang. 2007.
Word sense disambiguation improves statistical ma-
chine translation. In Proceedings of the 45th Annual
96
Meeting of the Association for Computational Linguis-
tics, pages 33?40.
Stanley F. Chen and Joshua Goodman. 1998. An empir-
ical study of smoothing techniques for language mod-
eling. Technical Report TR-10-98, Harvard University
Center for Research in Computing Technology.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
the 43rd Annual Meeting of the Association for Com-
putational Linguistics, pages 263?270.
M. Collins, P. Koehn, and I. Kucerova. 2005. Clause re-
structuring for statistical machine translation. In Proc.
of ACL05, pages 531?540.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proceed-
ings of COLING-ACL 2006, pages 961?968.
Zhongjun He, Qun Liu, and Shouxun Lin. 2008. Im-
proving statistical machine translation using lexical-
ized rule selection. In Proceedings of the 22nd In-
ternational Conference on Computational Linguistics
(Coling 2008), pages 321?328.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proceedings of the 7th Biennial
Conference of the Association for Machine Translation
in the Americas.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings of
HLT-NAACL 2003, pages 127?133.
Philipp Koehn. 2004. Pharaoh: a beam search decoder
for phrase-based statistical machine translation mod-
els. In Proceedings of the Sixth Conference of the
Association for Machine Translation in the Americas,
pages 115?124.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine trans-
lation. In Proceedings of the 44th Annual Meeting of
the Association for Computational Linguistics, pages
609?616.
Franz Josef Och and Hermann Ney. 2000. Improved sta-
tistical alignment models. In Proceedings of the 38th
Annual Meeting of the Association for Computational
Linguistics, pages 440?447.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for statis-
tical machine translation. In Proceedings of the 40th
Annual Meeting of the Association for Computational
Linguistics, pages 295?302.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting of the Association for Computa-
tional Linguistics, pages 160?167.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In Proceedings of the 40th Annual Meet-
ing of the Association for Computational Linguistics,
pages 311?318.
Andreas Stolcke. 2002. Srilm ? an extensible lan-
guage modeling toolkit. In Proceedings of the Inter-
national Conference on Spoken language Processing,
volume 2, pages 901?904.
Deyi Xiong, Shuanglong Li, Qun Liu, Shouxun Lin, and
Yueliang Qian. 2005. Parsing the penn chinese tree-
bank with semantic knowledge. In Proceedings of
IJCNLP 2005, pages 70?81.
Le Zhang. 2004. Maximum entropy model-
ing toolkit for python and c++. available at
http://homepages.inf.ed.ac.uk/s0450736/maxent too-
lkit.html.
97
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1017?1026,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Weighted Alignment Matrices for Statistical Machine Translation
Yang Liu , Tian Xia , Xinyan Xiao and Qun Liu
Key Laboratory of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences
P.O. Box 2704, Beijing 100190, China
{yliu,xiatian,xiaoxinyan,liuqun}@ict.ac.cn
Abstract
Current statistical machine translation sys-
tems usually extract rules from bilingual
corpora annotated with 1-best alignments.
They are prone to learn noisy rules due
to alignment mistakes. We propose a new
structure called weighted alignment matrix
to encode all possible alignments for a par-
allel text compactly. The key idea is to as-
sign a probability to each word pair to in-
dicate how well they are aligned. We de-
sign new algorithms for extracting phrase
pairs from weighted alignment matrices
and estimating their probabilities. Our ex-
periments on multiple language pairs show
that using weighted matrices achieves con-
sistent improvements over using n-best
lists in significant less extraction time.
1 Introduction
Statistical machine translation (SMT) relies heav-
ily on annotated bilingual corpora. Word align-
ment, which indicates the correspondence be-
tween the words in a parallel text, is one of the
most important annotations in SMT. Word-aligned
corpora have been found to be an excellent source
for translation-related knowledge, not only for
phrase-based models (Och and Ney, 2004; Koehn
et al, 2003), but also for syntax-based models
(e.g., (Chiang, 2007; Galley et al, 2006; Shen
et al, 2008; Liu et al, 2006)). Och and Ney
(2003) indicate that the quality of machine transla-
tion output depends directly on the quality of ini-
tial word alignment.
Modern alignment methods can be divided into
two major categories: generative methods and dis-
criminative methods. Generative methods (Brown
et al, 1993; Vogel and Ney, 1996) treat word
alignment as a hidden process and maximize the
likelihood of bilingual training corpus using the
expectation maximization (EM) algorithm. In
contrast, discriminative methods (e.g., (Moore et
al., 2006; Taskar et al, 2005; Liu et al, 2005;
Blunsom and Cohn, 2006)) have the freedom to
define arbitrary feature functions that describe var-
ious characteristics of an alignment. They usu-
ally optimize feature weights on manually-aligned
data. While discriminative methods show supe-
rior alignment accuracy in benchmarks, genera-
tive methods are still widely used to produce word
alignments for large sentence-aligned corpora.
However, neither generative nor discriminative
alignment methods are reliable enough to yield
high quality alignments for SMT, especially for
distantly-related language pairs such as Chinese-
English and Arabic-English. The F-measures for
Chinese-English and Arabic-English are usually
around 80% (Liu et al, 2005) and 70% (Fraser
and Marcu, 2007), respectively. As most current
SMT systems only use 1-best alignments for ex-
tracting rules, alignment errors might impair trans-
lation quality.
Recently, several studies have shown that offer-
ing more alternatives of annotations to SMT sys-
tems will result in significant improvements, such
as replacing 1-best trees with packed forests (Mi
et al, 2008) and replacing 1-best word segmenta-
tions with word lattices (Dyer et al, 2008). Sim-
ilarly, Venugopal et al (2008) use n-best align-
ments instead of 1-best alignments for translation
rule extraction. While they achieve significant im-
provements on the IWSLT data, extracting rules
from n-best alignments might be computationally
expensive.
In this paper, we propose a new structure named
weighted alignment matrix to represent the align-
ment distribution for a sentence pair compactly. In
a weighted matrix, each element that corresponds
to a word pair is assigned a probability to measure
the confidence of aligning the two words. There-
fore, a weighted matrix is capable of using a lin-
1017
the
development
of
China
?s
economy
z
h
o
n
g
g
u
o
d
e
j
i
n
g
j
i
f
a
z
h
a
n
Figure 1: An example of word alignment between
a pair of Chinese and English sentences.
ear space to encode the probabilities of exponen-
tially many alignments. We develop a new algo-
rithm for extracting phrase pairs from weighted
matrices and show how to estimate their relative
frequencies and lexical weights. Experimental re-
sults show that using weighted matrices achieves
consistent improvements in translation quality and
significant reduction in extraction time over using
n-best lists.
2 Background
Figure 1 shows an example of word alignment be-
tween a pair of Chinese and English sentences.
The Chinese and English words are listed horizon-
tally and vertically, respectively. The dark points
indicate the correspondence between the words in
two languages. For example, the first Chinese
word ?zhongguo? is aligned to the fourth English
word ?China?.
Formally, given a source sentence f = fJ
1
=
f
1
, . . . , f
j
, . . . , f
J
and a target sentence e = eI
1
=
e
1
, . . . , e
i
, . . . , e
I
, we define a link l = (j, i) to
exist if f
j
and e
i
are translation (or part of trans-
lation) of one another. Then, an alignment a is a
subset of the Cartesian product of word positions:
a ? {(j, i) : j = 1, . . . , J ; i = 1, . . . , I} (1)
Usually, SMT systems only use the 1-best align-
ments for extracting translation rules. For exam-
ple, given a source phrase ?f and a target phrase
e?, the phrase pair ( ?f , e?) is said to be consistent
(Och and Ney, 2004) with the alignment if and
only if: (1) there must be at least one word in-
side one phrase aligned to a word inside the other
phrase and (2) no words inside one phrase can be
aligned to a word outside the other phrase.
After all phrase pairs are extracted from the
training corpus, their translation probabilities can
be estimated as relative frequencies (Och and Ney,
2004):
?(e?|
?
f) =
count(
?
f, e?)
?
e?
?
count(
?
f , e?
?
)
(2)
where count( ?f , e?) indicates how often the phrase
pair ( ?f, e?) occurs in the training corpus.
Besides relative frequencies, lexical weights
(Koehn et al, 2003) are widely used to estimate
how well the words in ?f translate the words in
e?. To do this, one needs first to estimate a lexi-
cal translation probability distribution w(e|f) by
relative frequency from the same word alignments
in the training corpus:
w(e|f) =
count(f, e)
?
e
?
count(f, e
?
)
(3)
Note that a special source NULL token is added
to each source sentence and aligned to each un-
aligned target word.
As the alignment a? between a phrase pair ( ?f, e?)
is retained during extraction, the lexical weight
can be calculated as
p
w
(e?|
?
f, a?) =
|e?|
?
i=1
1
|{j|(j, i) ? a?}|
?
w(e
i
|f
j
) (4)
If there are multiple alignments a? for a phrase
pair ( ?f , e?), Koehn et al (2003) choose the one
with the highest lexical weight:
p
w
(e?|
?
f) = max
a?
{
p
w
(e?|
?
f, a?)
}
(5)
Simple and effective, relative frequencies and
lexical weights have become the standard features
in modern discriminative SMT systems.
3 Weighted Alignment Matrix
We believe that offering more candidate align-
ments to extracting translation rules might help
improve translation quality. Instead of using n-
best lists (Venugopal et al, 2008), we propose a
new structure called weighted alignment matrix.
We use an example to illustrate our idea. Fig-
ure 2(a) and Figure 2(b) show two alignments of
a Chinese-English sentence pair. We observe that
some links (e.g., (1,4) corresponding to the word
1018
the
development
of
China
?s
economy
z
h
o
n
g
g
u
o
d
e
j
i
n
g
j
i
f
a
z
h
a
n
the
development
of
China
?s
economy
z
h
o
n
g
g
u
o
d
e
j
i
n
g
j
i
f
a
z
h
a
n
the
development
of
China
?s
economy
z
h
o
n
g
g
u
o
d
e
j
i
n
g
j
i
f
a
z
h
a
n
1.0
0.6
0.40.4
1.0
1.0
0.4
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
(a) (b) (c)
Figure 2: (a) One alignment of a sentence pair; (b) another alignment of the same sentence pair; (c)
the resulting weighted alignment matrix that takes the two alignments as samples, of which the initial
probabilities are 0.6 and 0.4, respectively.
pair (?zhongguo?, ?China?)) occur in both align-
ments, some links (e.g., (2,3) corresponding to the
word pair (?de?,?of?)) occur only in one align-
ment, and some links (e.g., (1,1) corresponding
to the word pair (?zhongguo?, ?the?)) do not oc-
cur. Intuitively, we can estimate how well two
words are aligned by calculating its relative fre-
quency, which is the probability sum of align-
ments in which the link occurs divided by the
probability sum of all possible alignments. Sup-
pose that the probabilities of the two alignments in
Figures 2(a) and 2(b) are 0.6 and 0.4, respectively.
We can estimate the relative frequencies for every
word pair and obtain a weighted matrix shown in
Figure 2(c). Therefore, each word pair is associ-
ated with a probability to indicate how well they
are aligned. For example, in Figure 2(c), we say
that the word pair (?zhongguo?, ?China?) is def-
initely aligned, (?zhongguo?, ?the?) is definitely
unaligned, and (?de?, ?of?) has a 60% chance to
get algned.
Formally, a weighted alignment matrix m is a
J ? I matrix, in which each element stores a link
probability p
m
(j, i) to indicate how well f
j
and
e
i
are aligned. Currently, we estimate link proba-
bilities from an n-best list by calculating relative
frequencies:
p
m
(j, i) =
?
a?N
p(a)? ?(a, j, i)
?
a?N
p(a)
(6)
=
?
a?N
p(a)? ?(a, j, i) (7)
where
?(a, j, i) =
{
1 (j, i) ? a
0 otherwise (8)
Note that N is an n-best list, p(a) is the probabil-
ity of an alignment a in the n-best list, ?(a, j, i)
indicates whether a link (j, i) occurs in the align-
ment a or not. We assign 0 to any unseen
alignment. As p(a) is usually normalized (i.e.,
?
a?N
p(a) ? 1), we remove the denominator in
Eq. (6).
Accordingly, the probability that the two words
f
j
and e
i
are not aligned is
p?
m
(j, i) = 1.0? p
m
(j, i) (9)
For example, as shown in Figure 2(c), the prob-
ability for the two words ?de? and ?of? being
aligned is 0.6 and the probability that they are not
aligned is 0.4.
Intuitively, the probability of an alignment a is
the product of link probabilities. If a link (j, i)
occurs in a, we use p
m
(j, i); otherwise we use
p?
m
(j, i). Formally, given a weighted alignment
matrix m, the probability of an alignment a can
be calculated as
p
m
(a) =
J
?
j=1
I
?
i=1
(p
m
(j, i) ? ?(a, j, i) +
p?
m
(j, i) ? (1? ?(a, j, i))) (10)
It proves that the sum of all alignment proba-
bilities is always 1:
?
a?A
p
m
(a) ? 1, where A
1019
1: procedure PHRASEEXTRACT(fJ
1
, e
I
1
, m, l)
2: R ? ?
3: for j
1
? 1 . . . J do
4: j
2
? j
1
5: while j
2
< J ? j
2
? j
1
< l do
6: T ? {i|?j : j
1
? j ? j
2
? p
m
(j, i) > 0}
7: i
l
? MIN(T )
8: i
u
? MAX(T )
9: for n? 1 . . . l do
10: for i
1
? i
l
? n + 1 . . . i
u
do
11: i
2
? i
1
+ n? 1
12: R ? R? {(f j2
j
1
, e
i
2
i
1
)}
13: end for
14: end for
15: j
2
? j
2
+ 1
16: end while
17: end for
18: returnR
19: end procedure
Figure 3: Algorithm for extracting phrase pairs
from a sentence pair ?fJ
1
, e
I
1
? annotated with a
weighted alignment matrix m.
is the set of all possible alignments. Therefore, a
weighted alignment matrix is capable of encoding
the probabilities of 2J?I alignments using only a
J ? I space.
Note that p
m
(a) is not necessarily equal to p(a)
because the encoding of a weighted alignment ma-
trix changes the alignment probability distribu-
tion. For example, while the initial probability of
the alignment in Figure 2(a) (i.e., p(a)) is 0.6, the
probability of the same alignment encoded in the
matrix shown in Figure 2(c) (i.e., p
m
(a)) becomes
0.1296 according to Eq. (10). It should be em-
phasized that a weighted matrix encodes all pos-
sible alignments rather than the input n-best list,
although the link probabilities are estimated from
the n-best list.
4 Phrase Pair Extraction
In this section, we describe how to extract phrase
pairs from the training corpus annotated with
weighted alignment matrices (Section 4.1) and
how to estimate their relative frequencies (Section
4.2) and lexical weights (Section 4.3).
4.1 Extraction Algorithm
Och and Ney (2004) describe a ?phrase-extract?
algorithm for extracting phrase pairs from a sen-
tence pair annotated with a 1-best alignment.
Given a source phrase, they first identify the target
phrase that is consistent with the alignment. Then,
they expand the boundaries of the target phrase if
the boundary words are unaligned.
Unfortunately, this algorithm cannot be directly
used to manipulate a weighted alignment matrix,
which is a compact representation of all pos-
sible alignments. The major difference is that
the ?tight? phrase that has both boundary words
aligned is not necessarily the smallest candidate
in a weighted matrix. For example, in Figure
2(a), the ?tight? target phrase corresponding to
the source phrase ?zhongguo de? is ?of China?.
According to Och?s algorithm, the target phrase
?China? breaks the alignment consistency and
therefore is not valid candidate. However, this is
not true for using the weighted matrix shown in
Figure 2(c). The target phrase ?China? is treated
as a ?potential? candidate 1, although it might be
assigned only a small fractional count (see Table
1).
Therefore, we enumerate all potential phrase
pairs and calculate their fractional counts for
eliminating less promising candidates. Figure 3
shows the algorithm for extracting phrases from
a weighted matrix. The input of the algorithm
is a source sentence fJ
1
, a target sentence eI
1
, a
weighted alignment matrix m, and a phrase length
limit l (line 1). After initializing R that stores col-
lected phrase pairs (line 2), we identify the cor-
responding target phrases for all possible source
phrases (lines 3-5). Given a source phrase f j2
j
1
, we
find the lower and upper bounds of target positions
(i.e., i
l
and i
u
) that have positive link probabili-
ties (lines 6-8). For example, the lower bound is
3 and the upper bound is 5 for the source phrase
?zhongguo de? in Figure 2(c). Finally, we enu-
merate all target phrases that allow for unaligned
boundary words with varying phrase lengths (lines
9-14). Note that we need to ensure that 1 ? i
1
? I
and 1 ? i
2
? I in lines 10-11, which are omitted
for simplicity.
4.2 Calculating Relative Frequencies
To estimate the relative frequency of a phrase pair,
we need to estimate how often it occurs in the
training corpus. Given an n-best list, the fractional
count of a phrase pair is the probability sum of
the alignments with which the phrase pair is con-
sistent. Obviously, it is unrealistic for a weighted
alignment matrix to enumerate all possible align-
ments explicitly to calculate fractional counts. In-
stead, we resort to link probabilities to calculate
1By potential, we mean that the fractional count of a
phrase pair is positive. Section 4.2 describes how to calcu-
late fractional counts.
1020
the
development
of
China
?s
economy
z
h
o
n
g
g
u
o
d
e
j
i
n
g
j
i
f
a
z
h
a
n
1.0
0.6
0.40.4
1.0
1.0
0.4
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
Figure 4: An example of calculating fractional
count. Given the phrase pair (?zhongguo de?, ?of
China?), we divide the matrix into three areas: in-
side (heavy shading), outside (light shading), and
irrelevant (no shading).
counts efficiently. Equivalent to explicit enumera-
tion, we interpret the fractional count of a phrase
pair as the probability that it satisfies the two align-
ment consistency conditions (see Section 2).
Given a phrase pair, we divide the elements of
a weighted alignment matrix into three categories:
(1) inside elements that fall inside the phrase pair,
(2) outside elements that fall outside the phrase
pair while fall in the same row or the same col-
umn, and (3) irrelevant elements that fall outside
the phrase pair while fall in neither the same row
nor the same column. Figure 4 shows an exam-
ple. Given the phrase pair (?zhongguo de?, ?of
China?), we divide the matrix into three areas: in-
side (heavy shading), outside (light shading), and
irrelevant (no shading).
To what extent a phrase pair satisfies the align-
ment consistency is measured by calculating in-
side and outside probabilities. Although there are
the same terms in the parsing literature, they have
different meanings here. The inside probability in-
dicates the chance that there is at least one word
inside one phrase aligned to a word inside the
other phrase. The outside probability indicates the
chance that no words inside one phrase are aligned
to a word outside the other phrase.
Given a phrase pair (f j2
j
1
, e
i
2
i
1
), we denote the in-
side area as in(j
1
, j
2
, i
1
, i
2
) and the outside area
as out(j
1
, j
2
, i
1
, i
2
). Therefore, the inside proba-
bility of a phrase pair is calculated as
?(j
1
, j
2
, i
1
, i
2
) = 1?
?
(j,i)?in(j
1
,j
2
,i
1
,i
2
)
p?
m
(j, i) (11)
target phrase ? ? count
of China 1.0 0.36 0.36
of China ?s 1.0 0.36 0.36
China ?s 1.0 0.24 0.24
China 1.0 0.24 0.24
?s economy 0.4 0 0
Table 1: Some candidate target phrases of the
source phrase ?zhongguo de? in Figure 4, where ?
is inside probability, ? is outside probability, and
count is fractional count.
For example, the inside probability for (?zhong-
guo de?, ?of China?) in Figure 4 is 1.0, which
means that there always exists at least one aligned
word pair inside.
Accordingly, the outside probability of a phrase
pair is calculated as
?(j
1
, j
2
, i
1
, i
2
) =
?
(j,i)?out(j
1
,j
2
,i
1
,i
2
)
p?
m
(j, i) (12)
For example, the outside probability for
(?zhongguo de?, ?of China?) in Figure 4 is 0.36,
which means the probability that there are no
aligned word pairs outside is 0.36.
Finally, we use the product of inside and outside
probabilities as the fractional count of a phrase
pair:
count(f
j
2
j
1
, e
i
2
i
1
) = ?(j
1
, j
2
, i
1
, i
2
)?
?(j
1
, j
2
, i
1
, i
2
) (13)
Table 1 lists some candidate target phrases of
the source phrase ?zhongguo de? in Figure 4. We
also give their inside probabilities, outside proba-
bilities, and fractional counts.
After collecting the fractional counts from the
training corpus, we then use Eq. (2) to calculate
relative frequencies in two translation directions.
Often, our approach extracts a large amount of
phrase pairs from training corpus as we soften
the alignment consistency constraint. To main-
tain a reasonable phrase table size, we discard any
phrase pair that has a fractional count lower than
a threshold t. During extraction, we first obtain
a list of candidate target phrases for each source
phrase, as shown in Table 1. Then, we prune the
list according to the threshold t. For example, we
only retain the top two candidates in Table 1 if
t = 0.3. Note that we perform the pruning locally.
Although it is more reasonable to prune a phrase
table after accumulating all fractional counts from
1021
training corpus, such global pruning strategy usu-
ally leads to very large disk and memory require-
ments.
4.3 Calculating Lexical Weights
Recall that we need to obtain two translation prob-
ability tables w(e|f) and w(f |e) before calculat-
ing lexical weights (see Section 2). Following
Koehn et al (2003), we estimate the two distribu-
tions by relative frequencies from the training cor-
pus annotated with weighted alignment matrices.
In other words, we still use Eq. (3) but the way of
calculating fractional counts is different now.
Given a source word f
j
, a target word e
i
, and
a weighted alignment matrix, the fractional count
count(f
j
, e
i
) is p
m
(j, i). For NULL words, the
fractional counts can be calculated as
count(f
j
, e
0
) =
I
?
i=1
p?
m
(j, i) (14)
count(f
0
, e
i
) =
J
?
j=1
p?
m
(j, i) (15)
For example, in Figure 4, count(de, of) is 0.6,
count(de,NULL) is 0.24, and count(NULL,of) is
0.24.
Then, we adapt Eq. (4) to calculate lexical
weight:
p
w
(e?|
?
f ,m) =
|e?|
?
i=1
(
(
1
{j|p
m
(j, i) > 0}
?
?
?j:p
m
(j,i)>0
p(e
i
|f
j
)? p
m
(j, i)
)
+
p(e
i
|f
0
)?
|
?
f |
?
j=1
p?
m
(j, i)
)
(16)
For example, for the target word ?of? in Figure
4, the sum of aligned and unaligned probabilities
is
1
2
? (p(of|de)? 0.6 + p(of|fazhan)? 0.4) +
p(of|NULL)? 0.24
Note that we take link probabilities into account
and calculate the probability that a target word
translates a source NULL token explicitly.
5 Experiments
5.1 Data Preparation
We evaluated our approach on Chinese-to-English
translation. We used the FBIS corpus (6.9M
+ 8.9M words) as the training data. For lan-
guage model, we used the SRI Language Mod-
eling Toolkit (Stolcke, 2002) to train a 4-gram
model on the Xinhua portion of GIGAWORD cor-
pus. We used the NIST 2002 MT evaluation test
set as our development set, and used the NIST
2005 test set as our test set. We evaluated the trans-
lation quality using case-insensitive BLEU metric
(Papineni et al, 2002).
To obtain weighted alignment matrices, we fol-
lowed Venugopal et al (2008) to produce n-
best lists via GIZA++. We first ran GIZA++
to produce 50-best lists in two translation direc-
tions. Then, we used the refinement technique
?grow-diag-final-and? (Koehn et al, 2003) to all
50 ? 50 bidirectional alignment pairs. Suppose
that p
s2t
and p
t2s
are the probabilities of an align-
ment pair assigned by GIZA++, respectively. We
used p
s2t
? p
t2s
as the probability of the result-
ing symmetric alignment. As different alignment
pairs might produce the same symmetric align-
ments, we followed Venugopal et al (2008) to
remove duplicate alignments and retain only the
alignment with the highest probability. Therefore,
there were 550 candidate alignments on average
for each sentence pair in the training data. We
obtained n-best lists by selecting the top n align-
ments from the 550-best lists. The probability of
each alignment in the n-best list was re-estimated
by re-normalization (Venugopal et al, 2008). Fi-
nally, these n-best alignments served as samples
for constructing weighted alignment matrices.
After extracting phrase pairs from n-best lists
and weighted alignment matrices, we ran Moses
(Koehn et al, 2007) to translate the development
and test sets. We used the simple distance-based
reordering model to remove the dependency of
lexicalization on word alignments for Moses.
5.2 Effect of Pruning Threshold
Our first experiment investigated the effect of
pruning threshold on translation quality (BLEU
scores on the test set) and the phrase table size (fil-
tered for the test set), as shown in Figure 5. To
save time, we extracted phrase pairs just from the
first 10K sentence pairs of the FBIS corpus. We
used 12 different thresholds: 0.0001, 0.001, 0.01,
0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, and 0.9. Obvi-
ously, the lower the threshold is, the more phrase
pairs are extracted. When t = 0.0001, the number
of phrase pairs used on the test set was 460,284
1022
0.195
0.196
0.197
0.198
0.199
0.200
0.201
0.202
0.203
0.204
0.205
0.206
0.207
0.208
 150  200  250  300  350  400  450  500
BL
EU
 sc
or
e
phrase table size (103)
t=10-4
t=10-3
t=10-2
t=0.9...0.1
Figure 5: Effect of pruning threshold on transla-
tion quality and phrase table size.
and the BLEU score was 20.55. Generally, both
the number of phrase pairs and the BLEU score
went down with the increase of t. However, this
trend did not hold within the range [0.1, 0.9]. To
achieve a good tradeoff between translation qual-
ity and phrase table size, we set t = 0.01 for the
following experiments.
5.3 N -best lists Vs. Weighted Matrices
Figure 6 shows the BLEU scores and aver-
age extraction time using n-best alignments and
weighted matrices, respectively. We used the en-
tire training data for phrase extraction. When us-
ing 1-best alignments, Moses achieved a BLEU
score of 0.2826 and the average extraction time
was 4.19 milliseconds per sentence pair (see point
n = 1). The BLEU scores rose with the in-
crease of n for using n-best alignments. How-
ever, the score went down slightly when n = 50.
This suggests that including more noisy align-
ments might be harmful. These improvements
over 1-best alignments are not statistically signif-
icant. This finding failed to echo the promising
results reported by Venogopal et al (2008). We
think that there are two possible reasons. First,
they evaluated their approach on the IWSLT data
while we used the NIST data. It might be easier
to obtain significant improvements on the IWSLT
data in which the sentences are shorter. Sec-
ond, they used the hierarchical phrase-based sys-
tem while we used the phrase-based system, which
might be less sensitive to word alignments because
the alignments inside the phrase pairs hardly have
an effect.
When using weighted alignment matrices, we
0.280
0.281
0.282
0.283
0.284
0.285
0.286
0.287
0.288
0.289
0.290
0.291
0.292
0.293
 0  10  20  30  40  50  60  70  80  90
BL
EU
 sc
or
e
average extracting time (milliseconds/sentence pair)
n=1
n=5
n=10
n=50
n=5
n=10
n=50
n-best
m(n)
Figure 6: Comparison of n-best alignments and
weighted alignment matrices. We use m(n) to de-
note the matrices that take n-best lists as samples.
obtained higher BLEU scores than using n-best
lists with much less extraction time. We achieved
a BLEU score of 0.2901 when using the weighted
matrices estimated from 10-best lists. The abso-
lute improvement of 0.75 over using 1-best align-
ments (from 0.2826 to 0.2901) is statistically sig-
nificant at p < 0.05 by using sign-test (Collins
et al, 2005). Although the improvements over n-
best lists are not always statistically significant,
weighted alignment matrices maintain consistent
superiority in both translation quality and extrac-
tion speed.
5.4 Comparison of Parameter Estimation
In theory, the set of phrase pairs extracted from n-
best alignments is the subset of the set extracted
from the corresponding weighted matrices. In
practice, however, this is not true because we use
the pruning threshold t to maintain a reasonable
table size. Even so, the phrase tables produced by
n-best lists and weighted matrices still share many
phrase pairs.
Table 2 gives some statistics. We use m(10)
to represent the weighted matrices estimated from
10-best lists. ?all? denotes the full phrase table,
?shared? denotes the intersection of two tables,
and ?non-shared? denotes the complement. Note
that the probabilities of ?shared? phrase pairs are
different for the two approaches. We obtained
6.13M and 6.34M phrase pairs for the test set by
using 10-best lists and the corresponding matrices,
respectively. There were 4.58M phrase pairs in-
cluded by both tables. Note that the relative fre-
quencies and lexical weights for the same phrase
1023
shared non-shared all
method phrases BLEU phrases BLEU phrases BLEU
10-best 4.58M 28.35 1.55M 12.32 6.13M 28.47
m(10) 4.58M 28.90 1.76M 13.21 6.34M 29.01
Table 2: Comparison of phrase tables learned from n-best lists and weighted matrices. We use m(10)
to represent the weighted matrices estimated from 10-best lists. ?all? denotes the full phrase table,
?shared? denotes the intersection of two tables, and ?non-shared? denotes the complement. Note that the
probabilities of ?shared? phrase pairs are different for the two approaches.
0.200
0.210
0.220
0.230
0.240
0.250
0.260
0.270
0.280
0.290
 0  50  100  150  200  250
BL
EU
 sc
or
e
training corpus size (103)
1-best
10-best
m(10)
Figure 7: Comparison of n-best alignments and
weighted alignment matrices with varying training
corpus sizes.
pairs might be different in two tables. We found
that using matrices outperformed using n-best lists
even with the same phrase pairs. This suggests that
our methods for parameter estimation make better
use of noisy data. Another interesting finding was
that using the shared phrase pairs achieved almost
the same results with using full phrase tables.
5.5 Effect of Training Corpus Size
To investigate the effect of training corpus size on
our approach, we extracted phrase pairs from n-
best lists and weighted matrices trained on five
training corpora with varying sizes: 10K, 50K,
100K, 150K, and 239K sentence pairs. As shown
in Figure 7, our approach outperformed both 1-
best and n-best lists consistently. More impor-
tantly, the gains seem increase when more training
data are used.
5.6 Results on Other Language Pairs
To further examine the efficacy of the proposed ap-
proach, we scaled our experiments to large data
with multiple language pairs. We used the Eu-
roparl training corpus from the WMT07 shared
S?E F?E G?E
Sentences 1.26M 1.29M 1.26M
Foreign words 33.16M 33.18M 29.58M
English words 31.81M 32.62M 31.93M
Table 3: Statistics of the Europarl training data.
?S? denotes Spanish, ?E? denotes English, ?F? de-
notes French, ?G? denotes German.
1-best 10-best m(10)
S?E 30.90 30.97 31.03
E?S 31.16 31.25 31.34
F?E 30.69 30.76 30.82
E?F 26.42 26.65 26.54
G?E 24.46 24.58 24.66
E?G 18.03 18.30 18.20
Table 4: BLEU scores (case-insensitive) on the
Europarl data. ?S? denotes Spanish, ?E? denotes
English, ?F? denotes French, ?G? denotes Ger-
man.
task. 2 Table 3 shows the statistics of the train-
ing data. There are four languages (Spanish,
French, German, and English) and six transla-
tion directions (Foreign-to-English and English-
to-Foreign). We used the ?dev2006? data in the
?dev? directory as the development set and the
?test2006? data in the ?devtest? directory as the
test set. Both the development and test sets contain
2,000 sentences with single reference translations.
We tokenized and lowercased all the training,
development, and test data. We trained a 4-gram
language model using SRI Language Modeling
Toolkit on the target side of the training corpus for
each task. We ran GIZA++ on the entire train-
ing data to obtain n-best alignments and weighted
matrices. To save time, we just used the first 100K
sentences of each aligned training corpus to ex-
tract phrase pairs.
2http://www.statmt.org/wmt07/shared-task.html
1024
Table 4 lists the case-insensitive BLEU scores
of 1-best, 10-best, and m(10) on the Europarl
data. Using weighted packed matrices continued
to show advantage over using 1-best alignments on
multiple language pairs. However, these improve-
ments were very small and not significant. We at-
tribute this to the fact that GIZA++ usually pro-
duces high quality 1-best alignments for closely-
related European language pairs, especially when
trained on millions of sentences.
6 Related Work
Recent studies has shown that SMT systems
can benefit from making the annotation pipeline
wider: using packed forests instead of 1-best trees
(Mi et al, 2008), word lattices instead of 1-best
segmentations (Dyer et al, 2008), and n-best
alignments instead of 1-best alignments (Venu-
gopal et al, 2008). We propose a compact repre-
sentation of multiple word alignments that enables
SMT systems to make a better use of noisy align-
ments.
Matusov et al (2004) propose ?cost matrices?
for producing symmetric alignments. Kumar et al
(2007) describe how to use ?posterior probabil-
ity matrices? to improve alignment accuracy via
a bridge language. Although not using the term
?weighted matrices? directly, they both assign a
probability to each word pair.
We follow Och and Ney (2004) to develop
a new phrase extraction algorithm for weighted
alignment matrices. The methods for calculating
relative frequencies (Och and Ney, 2004) and lex-
ical weights (Koehn et al, 2003) are also adapted
for the weighted matrix case.
Many researchers (e.g., (Venugopal et al, 2003;
Deng et al, 2008)) observe that softening the
alignment consistency constraint help improve
translation quality. For example, Deng et al
(2008) define a feature named ?within phrase pair
consistency ratio? to measure the degree of consis-
tency. As each link is associated with a probability
in a weighted matrix, we use these probabilities to
evaluate the validity of a phrase pair.
We estimate the link probabilities by calculating
relative frequencies over n-best lists. Niehues and
Vogel (2008) propose a discriminative approach to
modeling the alignment matrix directly. The dif-
ference is that they assign a boolean value instead
of a probability to each word pair.
7 Conclusion and Future Work
We have presented a new structure called weighted
alignment matrix that encodes the alignment dis-
tribution for a sentence pair. Accordingly, we de-
velop new methods for extracting phrase pairs and
estimating their probabilities. Our experiments
show that the proposed approach achieves better
translation quality over using n-best lists in less
extraction time. An interesting finding is that our
approach performs better than the baseline even
they use the same phrase pairs.
Although our approach consistently outper-
forms using 1-best alignments for varying lan-
guage pairs, the improvements are comparatively
small. One possible reason is that taking n-best
lists as samples sometimes might change align-
ment probability distributions inappropriately. A
more principled solution is to directly model the
weighted alignment matrices, either in a genera-
tive or a discriminative way. We believe that better
estimation of alignment distributions will result in
more significant improvements.
Another interesting direction is applying our ap-
proach to extracting translation rules with hierar-
chical structures such as hierarchical phrases (Chi-
ang, 2007) and tree-to-string rules (Galley et al,
2006; Liu et al, 2006). We expect that these
syntax-based systems could benefit more from our
approach.
Acknowledgement
The authors were supported by Microsoft Re-
search Asia Natural Language Processing Theme
Program grant (2009-2010), High-Technology
R&D Program (863) Project No. 2006AA010108,
and National Natural Science Foundation of China
Contract 60736014. Part of this work was done
while Yang Liu was visiting the SMT group led by
Stephan Vogel at CMU. We thank the anonymous
reviewers for their insightful comments. We are
also grateful to Stephan Vogel, Alon Lavie, Fran-
cisco Guzman, Nguyen Bach, Andreas Zollmann,
Vamshi Ambati, and Kevin Gimpel for their help-
ful feedback.
References
Phil Blunsom and Trevor Cohn. 2006. Discrimina-
tive word alignment with conditional random fields.
In Proceedings of COLING/ACL 2006, pages 65?72,
Sydney, Australia, July.
1025
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The
mathematics of statistical machine translation: Pa-
rameter estimation. Computational Linguistics,
19(2):263?311.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Michael Collins, Philipp Koehn, and Ivona Kuc?erova?.
2005. Clause restructuring for statistical machine
translation. In Proceedings of ACL 2005, pages
531?540, Ann Arbor, USA, June.
Yonggang Deng, Jia Xu, and Yuqing Gao. 2008.
Phrase table training for precision and recall: What
makes a good phrase and a good phrase pair?
In Proceedings of ACL/HLT 2008, pages 81?88,
Columbus, Ohio, USA, June.
Christopher Dyer, Smaranda Muresan, and Philip
Resnik. 2008. Generalizing word lattice trans-
lation. In Proceedings of ACL/HLT 2008, pages
1012?1020, Columbus, Ohio, June.
Alexander Fraser and Daniel Marcu. 2007. Measur-
ing word alignment quality for statistical machine
translation. Computational Linguistics, Squibs and
Discussions, 33(3):293?303.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Pro-
ceedings of COLING/ACL 2006, pages 961?968,
Sydney, Australia, July.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings
of HLT/NAACL 2003, pages 127?133, Edmonton,
Canada, May.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In Proceedings of ACL 2007 (poster), pages
77?80, Prague, Czech Republic, June.
Shankar Kumar, Franz J. Och, and Wolfgang
Macherey. 2007. Improving word alignment with
bridge languages. In Proceedings of EMNLP 2007,
pages 42?50, Prague, Czech Republic, June.
Yang Liu, Qun Liu, and Shouxun Lin. 2005. Log-
linear models for word alignment. In Proceedings
of ACL 2005, pages 459?466, Ann Arbor, Michigan,
June.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment template for statistical machine
translation. In Proceedings of COLING/ACL 2006,
pages 609?616, Sydney, Australia, July.
Evgeny Matusov, Richard Zens, and Hermann Ney.
2004. Symmetric word alignments for statistical
machine translation. In Proceedings of COLING
2004, pages 219?225, Geneva, Switzerland, August.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proceedings of ACL/HLT 2008,
pages 192?199, Columbus, Ohio, June.
Robert C. Moore, Wen-tau Yih, and Andreas Bode.
2006. Improved discriminative bilingual word
alignment. In Proceedings of COLING/ACL 2006,
pages 513?520, Sydney, Australia, July.
Jan Niehues and Stephan Vogel. 2008. Discrimina-
tive word alignment via alignment matrix modeling.
In Proceedings of WMT-3, pages 18?25, Columbus,
Ohio, USA, June.
Franz J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29(1):19?51.
Franz J. Och and Hermann Ney. 2004. The alignment
template approach to statistical machine translation.
Computational Linguistics, 30(4):417?449.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings
of ACL 2002, pages 311?318, Philadelphia, Penn-
sylvania, USA, July.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008.
A new string-to-dependency machine translation al-
gorithm with a target dependency language model.
In Proceedings of ACL/HLT 2008, pages 577?585,
Columbus, Ohio, June.
Andreas Stolcke. 2002. Srilm - an extension language
model modeling toolkit. In Proceedings of ICSLP
2002, pages 901?904, Denver, Colorado, Septem-
ber.
Ben Taskar, Simon Lacoste-Julien, and Dan Klein.
2005. A discriminative matching approach to word
alignment. In Proceedings of HLT/EMNLP 2005,
pages 73?80, Vancouver, British Columbia, Canada,
October.
Ashish Venugopal, Stephan Vogel, and Alex Waibel.
2003. Effective phrase translation extraction from
alignment models. In Proceedings of ACL 2003,
pages 319?326, Sapporo, Japan, July.
Ashish Venugopal, Andreas Zollmann, Noah A. Smith,
and Stephan Vogel. 2008. Wider pipelines: n-
best alignments and parses in mt training. In Pro-
ceedings of AMTA 2008, pages 192?201, Waikiki,
Hawaii, October.
Stephan Vogel and Hermann Ney. 1996. Hmm-based
word alignment in statistical translation. In Pro-
ceedings of COLING 1996, pages 836?841, Copen-
hagen, Danmark, August.
1026
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1105?1113,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Lattice-based System Combination for Statistical Machine Translation
Yang Feng, Yang Liu, Haitao Mi, Qun Liu, Yajuan Lu?
Key Laboratory of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences
P.O. Box 2704, Beijing 100190, China
{fengyang, yliu, htmi, liuqun, lvyajuan}@ict.ac.cn
Abstract
Current system combination methods usu-
ally use confusion networks to find consensus
translations among different systems. Requir-
ing one-to-one mappings between the words
in candidate translations, confusion networks
have difficulty in handling more general situa-
tions in which several words are connected to
another several words. Instead, we propose a
lattice-based system combination model that
allows for such phrase alignments and uses
lattices to encode all candidate translations.
Experiments show that our approach achieves
significant improvements over the state-of-
the-art baseline system on Chinese-to-English
translation test sets.
1 Introduction
System combination aims to find consensus transla-
tions among different machine translation systems.
It has been proven that such consensus translations
are usually better than the output of individual sys-
tems (Frederking and Nirenburg, 1994).
In recent several years, the system combination
methods based on confusion networks developed
rapidly (Bangalore et al, 2001; Matusov et al, 2006;
Sim et al, 2007; Rosti et al, 2007a; Rosti et al,
2007b; Rosti et al, 2008; He et al, 2008), which
show state-of-the-art performance in benchmarks. A
confusion network consists of a sequence of sets of
candidate words. Each candidate word is associated
with a score. The optimal consensus translation can
be obtained by selecting one word from each set to
maximizing the overall score.
To construct a confusion network, one first need
to choose one of the hypotheses (i.e., candidate
translations) as the backbone (also called ?skeleton?
in the literature) and then decide the word align-
ments of other hypotheses to the backbone. Hy-
pothesis alignment plays a crucial role in confusion-
network-based system combination because it has a
direct effect on selecting consensus translations.
However, a confusion network is restricted in
such a way that only 1-to-1 mappings are allowed
in hypothesis alignment. This is not the fact even
for word alignments between the same languages. It
is more common that several words are connected
to another several words. For example, ?be capa-
ble of? and ?be able to? have the same meaning.
Although confusion-network-based approaches re-
sort to inserting null words to alleviate this problem,
they face the risk of producing degenerate transla-
tions such as ?be capable to? and ?be able of?.
In this paper, we propose a new system combina-
tion method based on lattices. As a more general
form of confusion network, a lattice is capable of
describing arbitrary mappings in hypothesis align-
ment. In a lattice, each edge is associated with a
sequence of words rather than a single word. There-
fore, we select phrases instead of words in each
candidate set and minimize the chance to produce
unexpected translations such as ?be capable to?.
We compared our approach with the state-of-the-art
confusion-network-based system (He et al, 2008)
and achieved a significant absolute improvement of
1.23 BLEU points on the NIST 2005 Chinese-to-
English test set and 0.93 BLEU point on the NIST
2008 Chinese-to-English test set.
1105
He feels like apples
He prefer apples
He feels like apples
He is fond of apples
(a) unidirectional alignments
He feels like apples
He prefer apples
He feels like apples
He is fond of apples
(b) bidirectional alignments
He feels like ? apples
? prefer of
is fond
(c) confusion network
he feels like apples
? prefer
is fond of
(d) lattice
Figure 1: Comparison of a confusion network and a lat-
tice.
2 Background
2.1 Confusion Network and Lattice
We use an example shown in Figure 1 to illustrate
our idea. Suppose that there are three hypotheses:
He feels like apples
He prefer apples
He is fond of apples
We choose the first sentence as the backbone.
Then, we perform hypothesis alignment to build a
confusion network, as shown in Figure 1(a). Note
that although ?feels like? has the same meaning with
?is fond of?, a confusion network only allows for
one-to-one mappings. In the confusion network
shown in Figure 1(c), several null words ? are in-
serted to ensure that each hypothesis has the same
length. As each edge in the confusion network only
has a single word, it is possible to produce inappro-
priate translations such as ?He is like of apples?.
In contrast, we allow many-to-many mappings
in the hypothesis alignment shown in Figure 2(b).
For example, ?like? is aligned to three words: ?is?,
?fond?, and ?of?. Then, we use a lattice shown in
Figure 1(d) to represent all possible candidate trans-
lations. Note that the phrase ?is fond of? is attached
to an edge. Now, it is unlikely to obtain a translation
like ?He is like of apples?.
A lattice G = ?V,E? is a directed acyclic graph,
formally a weighted finite state automation (FSA),
where V is the set of nodes and E is the set of edges.
The nodes in a lattice are usually labeled according
to an appropriate numbering to reflect how to pro-
duce a translation. Each edge in a lattice is attached
with a sequence of words as well as the associated
probability.
As lattice is a more general form of confusion
network (Dyer et al, 2008), we expect that replac-
ing confusion networks with lattices will further im-
prove system combination.
2.2 IHMM-based Alignment Method
Since the candidate hypotheses are aligned us-
ing Indirect-HMM-based (IHMM-based) alignment
method (He et al, 2008) in both direction, we briefly
review the IHMM-based alignment method first.
Take the direction that the hypothesis is aligned to
the backbone as an example. The conditional prob-
ability that the hypothesis is generated by the back-
bone is given by
p(e
?
1
J
|e
I
1
) =
?
a
J
1
J
?
j=1
[p(a
j
|a
j?1
, I)p(e
?
j
|e
a
j
)]l (1)
Where eI
1
= (e
1
, ..., e
I
) is the backbone, e?J
1
=
(e
?
1
, ..., e
?
J
) is a hypothesis aligned to eI
1
, and aJ
1
=
(a
1
, .., a
J
) is the alignment that specifies the posi-
tion of backbone word that each hypothesis word is
aligned to.
The translation probability p(e?
j
|e
i
) is a linear in-
terpolation of semantic similarity p
sem
(e
?
j
|e
i
) and
surface similarity p
sur
(e
?
j
|e
i
) and ? is the interpo-
lation factor:
p(e
?
j
|e
i
) = ??p
sem
(e
?
j
|e
i
)+(1??)?p
sur
(e
?
j
|e
i
) (2)
The semantic similarity model is derived by using
the source word sequence as a hidden layer, so the
bilingual dictionary is necessary. The semantic sim-
1106
ilarity model is given by
p
sem
(e
?
j
|e
i
) =
K
?
k=0
p(f
k
|e
i
)p(e
?
j
|f
k
, e
i
)
?
K
?
k=0
p(f
k
|e
i
)p(e
?
j
|f
k
)
(3)
The surface similarity model is estimated by calcu-
lating the literal matching rate:
p
sur
(e
?
j
|e
i
) = exp{? ? [s(e
?
j
, e
i
)? 1]} (4)
where s(e?
j
, e
i
) is given by
s(e
?
j
, e
i
) =
M(e
?
j
, e
i
)
max(|e
?
j
|, |e
i
|)
(5)
where M(e?
j
, e
i
) is the length of the longest matched
prefix (LMP) and ? is a smoothing factor that speci-
fies the mapping.
The distortion probability p(a
j
= i|a
j?1
= i
?
, I)
is estimated by only considering the jump distance:
p(i|i
?
, I) =
c(i? i
?
)
?
I
i=1
c(l ? i
?
)
(6)
The distortion parameters c(d) are grouped into 11
buckets, c(? ?4), c(?3), ..., c(0), ..., c(5), c(? 6).
Since the alignments are in the same language, the
distortion model favor monotonic alignments and
penalize non-monotonic alignments. It is given in
a intuitive way
c(d) = (1 + |d? 1|)
?K
, d = ?4, ..., 6 (7)
where K is tuned on held-out data.
Also the probability p
0
of jumping to a null word
state is tuned on held-out data. So the overall distor-
tion model becomes
p(i|i
?
, I) =
{
p
0
if i = null state
(1? p
0
) ? p(i|i
?
, I) otherwise
3 Lattice-based System Combination
Model
Lattice-based system combination involves the fol-
lowing steps:
(1) Collect the hypotheses from the candidate sys-
tems.
(2) Choose the backbone from the hypotheses.
This is performed using a sentence-level Minimum
Bayes Risk (MBR) method. The hypothesis with the
minimum cost of edits against all hypotheses is se-
lected. The backbone is significant for it influences
not only the word order, but also the following align-
ments. The backbone is selected as follows:
E
B
= argmin
E
?
?E
?
E?E
TER(E
?
, E) (8)
(3) Get the alignments of the backbone and hy-
pothesis pairs. First, each pair is aligned in both di-
rections using the IHMM-based alignment method.
In the IHMM alignment model, bilingual dictionar-
ies in both directions are indispensable. Then, we
apply a grow-diag-final algorithm which is widely
used in bilingual phrase extraction (Koehn et al,
2003) to monolingual alignments. The bidirec-
tional alignments are combined to one resorting to
the grow-diag-final algorithm, allowing n-to-n map-
pings.
(4)Normalize the alignment pairs. The word or-
der of the backbone determines the word order of
consensus outputs, so the word order of hypotheses
must be consistent with that of the backbone. All
words of a hypotheses are reordered according to
the alignment to the backbone. For a word aligned
to null, an actual null word may be inserted to the
proper position. The alignment units are extracted
first and then the hypothesis words in each unit are
shifted as a whole.
(5) Construct the lattice in the light of phrase
pairs extracted on the normalized alignment pairs.
The expression ability of the lattice depends on the
phrase pairs.
(6) Decode the lattice using a model similar to the
log-linear model.
The confusion-network-based system combina-
tion model goes in a similar way. The first two steps
are the same as the lattice-based model. The differ-
ence is that the hypothesis pairs are aligned just in
one direction due to the expression limit of the con-
fusion network. As a result, the normalized align-
ments only contain 1-to-1 mappings (Actual null
words are also needed in the case of null alignment).
In the following, we will give more details about the
steps which are different in the two models.
1107
4 Lattice Construction
Unlike a confusion network that operates words
only, a lattice allows for phrase pairs. So phrase
pairs must be extracted before constructing a lat-
tice. A major difficulty in extracting phrase pairs
is that the word order of hypotheses is not consistent
with that of the backbone. As a result, hypothesis
words belonging to a phrase pair may be discon-
tinuous. Before phrase pairs are extracted, the hy-
pothesis words should be normalized to make sure
the words in a phrase pair is continuous. We call a
phrase pair before normalization a alignment unit.
The problem mentioned above is shown in Fig-
ure 2. In Figure 2 (a), although (e?
1
e
?
3
, e
2
) should be
a phrase pair, but /e?
1
0 and /e?
3
0 are discontin-
uous, so the phrase pair can not be extracted. Only
after the words of the hypothesis are reordered ac-
cording to the corresponding words in the backbone
as shown in Figure 2 (b), /e?
1
0 and /e?
3
0 be-
come continuous and the phrase pair (e?
1
e
?
3
, e
2
) can
be extracted. The procedure of reordering is called
alignment normalization
E
h
: e?
1
e
?
2
e
?
3
E
B
:
e
1
e
2
e
3
(a)
E
h
: e?
2
e
?
1
e
?
3
E
B
:
e
1
e
2
e
3
(b)
Figure 2: An example of alignment units
4.1 Alignment Normalization
After the final alignments are generated in the grow-
diag-final algorithm, minimum alignment units are
extracted. The hypothesis words of an alignment
unit are packed as a whole in shift operations.
See the example in Figure 2 (a) first. All mini-
mum alignment units are as follows: (e?
2
, e
1
), (e?
1
e
?
3
,
e
2
) and (?, e
3
). (e?
1
e
?
2
e
?
3
, e
1
e
2
) is an alignment unit,
but not a minimum alignment unit.
Let a?
i
= (e?
?
i
, e?
i
) denote a minimum alignment
unit, and assume that the word string e??
i
covers words
e
?
i
1
,..., e
?
i
m
on the hypothesis side, and the word
string e?
i
covers the consecutive words e
i
1
,..., e
i
n
on
the backbone side. In an alignment unit, the word
string on the hypothesis side can be discontinuous.
The minimum unit a?
i
= (e?
?
i
, e?
i
) must observe the
following rules:
E
B
: e
1
e
2
e
3
E
h
:
e
?
1
e
?
2 (a)
e
1
e
2
e
3
e
?
2
?
e
?
1
E
B
: e
1
e
2
E
h
: e
?
1
e
?
2
e
?
3
e
1
e
2
e
?
1
e
?
3
e
?
1
e
?
2
e
?
3
(b)
E
B
: e
1
e
2
E
h
:
e
?
1
e
?
2
e
?
3
e
1
?
e
2
e
?
1
e
?
2
e
?
3
(c)
Figure 3: Different cases of null insertion
? ? e
?
i
k
? e?
?
i
, e
a
?
i
k
? e?
i
? ? e
i
k
? e?
i
, e
?
a
i
k
= null or e?
a
i
k
? e?
?
i
? ? a?
j
= (e?
?
j
, e?
j
), e?
j
= e
i
1
, ..., e
i
k
or e?
j
=
e
i
k
, ..., e
i
n
, k ? [1, n]
Where a?
i
k
denotes the position of the word in the
backbone that e?
i
k
is aligned to, and a
i
k
denotes the
position of the word in the hypothesis that e
i
k
is
aligned to.
An actual null word may be inserted to a proper
position if a word, either from the hypothesis or from
the backbone, is aligned to null. In this way, the
minimum alignment set is extended to an alignment
unit set, which includes not only minimum align-
ment units but also alignment units which are gener-
ated by adding null words to minimum alignment
units. In general, the following three conditions
should be taken into consideration:
? A backbone word is aligned to null. A null
word is inserted to the hypothesis as shown in
Figure 3 (a).
? A hypothesis word is aligned to null and it is
between the span of a minimum alignment unit.
A new alignment unit is generated by insert-
ing the hypothesis word aligned to null to the
minimum alignment unit. The new hypothesis
string must remain the original word order of
the hypothesis. It is illustrated in Figure 3 (b).
? A hypothesis word is aligned to null and it is
not between the hypothesis span of any mini-
mum alignment unit. In this case, a null word
1108
e1
e
2
?
e
3
e?
?
4
e?
?
5
e?
?
6
(a)
e
1
?
e
2
e
3
e?
?
1
e?
?
2
e?
?
3
(b)
e
1
?
e
2
e
3
e?
?
1
e?
?
2
e?
?
3
e?
?
4
(c)
e
1
?
e
2
?
e
3
e?
?
1
e?
?
2
e?
?
3
e?
?
4
e?
?
5
(d)
e
1
?
e
2
?
e
3
e?
?
1
e?
?
2
e?
?
3
e?
?
4
e?
?
5
e?
?
6
(e)
Figure 4: A toy instance of lattice construction
are inserted to the backbone. This is shown in
Figure 3 (c).
4.2 Lattice Construction Algorithm
The lattice is constructed by adding the normalized
alignment pairs incrementally. One backbone arc in
a lattice can only span one backbone word. In con-
trast, all hypothesis words in an alignment unit must
be packed into one hypothesis arc. First the lattice is
initialized with a normalized alignment pair. Then
given all other alignment pairs one by one, the lat-
tice is modified dynamically by adding the hypothe-
sis words of an alignment pair in a left-to-right fash-
ion.
A toy instance is given in Figure 4 to illustrate the
procedure of lattice construction. Assume the cur-
rent inputs are: an alignment pair as in Figure 4 (a),
and a lattice as in Figure 4 (b). The backbone words
of the alignment pair are compared to the backbone
words of the lattice one by one. The procedure is as
follows:
? e
1
is compared with e
1
. Since they are the
same, the hypothesis arc e??
4
, which comes from
the same node with e
1
in the alignment pair,
is compared with the hypothesis arc e??
1
, which
comes from the same node with e
1
in the lat-
tice. The two hypothesis arcs are not the same,
so e??
4
is added to the lattice as shown in Figure
4(c). Both go to the next backbone words.
? e
2
is compared with ?. The lattice remains the
same. The lattice goes to the next backbone
word e
2
.
? e
2
is compared with e
2
. There is no hypothesis
arc coming from the same node with the bone
arc e
2
in the alignment pair, so the lattice re-
mains the same. Both go to the next backbone
words.
? ? is compared with e
3
. A null backbone arc is
inserted into the lattice between e
2
and e
3
. The
hypothesis arc e??
5
is inserted to the lattice, too.
The modified lattice is shown in Figure 4(d).
The alignment pair goes to the next backbone
word e
3
.
? e
3
is compared with e
3
. For they are the same
and there is no hypothesis arc e??
6
in the lattice,
e?
?
6
is inserted to the lattice as in Figure 4(e).
? Both arrive at the end and it is the turn of the
next alignment pair.
When comparing a backbone word of the given
alignment pair with a backbone word of the lattice,
the following three cases should be handled:
? The current backbone word of the given align-
ment pair is a null word while the current back-
bone word of the lattice is not. A null back-
bone word is inserted to the lattice.
? The current backbone word of the lattice is a
null word while the current word of the given
alignment pair is not. The current null back-
bone word of the lattice is skipped with nothing
to do. The next backbone word of the lattice is
compared with the current backbone word of
the given alignment pair.
1109
Algorithm 1 Lattice construction algorithm.
1: Input: alignment pairs {p
n
}
N
n=1
2: L? p
1
3: Unique(L)
4: for n? 2 .. N do
5: pnode = p
n
? first
6: lnode = L ? first
7: while pnode ? barcnext 6= NULL do
8: if lnode ? barcnext = NULL or pnode ?
bword = null and lnode ? bword 6= null then
9: INSERTBARC(lnode, null)
10: pnode = pnode ? barcnext
11: else
12: if pnode ? bword 6= null and lnode ?
bword = null then
13: lnode = lnode ? barcnext
14: else
15: for each harc of pnode do
16: if NotExist(lnode, pnode ? harc)
then
17: INSERTHARC(lnode, pnode ?
harc)
18: pnode = pnode ? barcnext
19: lnode = lnode ? barcnext
20: Output: lattice L
? The current backbone words of the given align-
ment pair and the lattice are the same. Let
{harc
l
} denotes the set of hypothesis arcs,
which come from the same node with the cur-
rent backbone arc in the lattice, and harc
h
de-
notes one of the corresponding hypothesis arcs
in the given alignment pair. In the {harc
l
},
if there is no arc which is the same with the
harc
h
, a hypothesis arc projecting to harc
h
is
added to the lattice.
The algorithm of constructing a lattice is illus-
trated in Algorithm 1. The backbone words of the
alignment pair and the lattice are processed one by
one in a left-to-right manner. Line 2 initializes the
lattice with the first alignment pair, and Line 3 re-
moves the hypothesis arc which contains the same
words with the backbone arc. barc denotes the back-
bone arc, storing one backbone word only, and harc
denotes the hypothesis arc, storing the hypothesis
words. For there may be many alignment units span
the same backbone word range, there may be more
than one harc coming from one node. Line 8 ? 10
consider the condition 1 and function InsertBarc in
Line 9 inserts a null bone arc to the position right
before the current node. Line 12?13 deal with con-
dition 2 and jump to the next backbone word of the
lattice. Line 15?19 handle condition 3 and function
InsertHarc inserts to the lattice a harc with the same
hypothesis words and the same backbone word span
with the current hypothesis arc.
5 Decoding
In confusion network decoding, a translation is gen-
erated by traveling all the nodes from left to right.
So a translation path contains all the nodes. While
in lattice decoding, a translation path may skip some
nodes as some hypothesis arcs may cross more than
one backbone arc.
Similar to the features in Rosti et al (2007a), the
features adopted by lattice-based model are arc pos-
terior probability, language model probability, the
number of null arcs, the number of hypothesis arcs
possessing more than one non-null word and the
number of all non-null words. The features are com-
bined in a log-linear model with the arc posterior
probabilities being processed specially as follows:
log p(e/f) =
N
arc
?
i=1
log (
N
s
?
s=1
?
s
p
s
(arc))
+ ?L(e) + ?N
nullarc
(e)
+ ?N
longarc
(e) + ?N
word
(e)
(9)
where f denotes the source sentence, e denotes a
translation generated by the lattice-based system,
N
arc
is the number of arcs the path of e covers,
N
s
is the number of candidate systems and ?
s
is the
weight of system s. ? is the language model weight
and L(e) is the LM log-probability. N
nullarcs
(e) is
the number of the arcs which only contain a null
word, and N
longarc
(e) is the number of the arcs
which store more than one non-null word. The
above two numbers are gotten by counting both
backbone arcs and hypothesis arcs. ? and ? are the
corresponding weights of the numbers, respectively.
N
word
(e) is the non-null word number and ? is its
weight.
Each arc has different confidences concerned with
different systems, and the confidence of system s
is denoted by p
s
(arc). p
s
(arc) is increased by
1110
1/(k+1) if the hypothesis ranking k in the system s
contains the arc (Rosti et al, 2007a; He et al, 2008).
Cube pruning algorithm with beam search is em-
ployed to search for the consensus output (Huang
and Chiang, 2005). The nodes in the lattice are
searched in a topological order and each node re-
tains a list of N best candidate partial translations.
6 Experiments
The candidate systems participating in the system
combination are as listed in Table 1: System A is a
BTG-based system using a MaxEnt-based reorder-
ing model; System B is a hierarchical phrase-based
system; System C is the Moses decoder (Koehn et
al., 2007); System D is a syntax-based system. 10-
best hypotheses from each candidate system on the
dev and test sets were collected as the input of the
system combination.
In our experiments, the weights were all tuned on
the NIST MT02 Chinese-to-English test set, includ-
ing 878 sentences, and the test data was the NIST
MT05 Chinese-to-English test set, including 1082
sentences, except the experiments in Table 2. A 5-
gram language model was used which was trained
on the XinHua portion of Gigaword corpus. The re-
sults were all reported in case sensitive BLEU score
and the weights were tuned in Powell?s method to
maximum BLEU score. The IHMM-based align-
ment module was implemented according to He et
al. (2008), He (2007) and Vogel et al (1996). In all
experiments, the parameters for IHMM-based align-
ment module were set to: the smoothing factor for
the surface similarity model, ? = 3; the controlling
factor for the distortion model, K = 2.
6.1 Comparison with
Confusion-network-based model
In order to compare the lattice-based system with
the confusion-network-based system fairly, we used
IHMM-based system combination model on behalf
of the confusion-network-based model described in
He et al (2008). In both lattice-based and IHMM-
based systems, the bilingual dictionaries were ex-
tracted on the FBIS data set which included 289K
sentence pairs. The interpolation factor of the simi-
larity model was set to ? = 0.1.
The results are shown in Table 1. IHMM stands
for the IHMM-based model and Lattice stands for
the lattice-based model. On the dev set, the lattice-
based system was 3.92 BLEU points higher than the
best single system and 0.36 BLEU point higher than
the IHMM-based system. On the test set, the lattice-
based system got an absolute improvement by 3.73
BLEU points over the best single system and 1.23
BLEU points over the IHMM-based system.
System MT02 MT05
BLEU% BLEU%
SystemA 31.93 30.68
SystemB 32.16 32.07
SystemC 32.09 31.64
SystemD 33.37 31.26
IHMM 36.93 34.57
Lattice 37.29 35.80
Table 1: Results on the MT02 and MT05 test sets
The results on another test sets are reported in Ta-
ble 2. The parameters were tuned on the newswire
part of NIST MT06 Chinese-to-English test set, in-
cluding 616 sentences, and the test set was NIST
MT08 Chinese-to-English test set, including 1357
sentences. The BLEU score of the lattice-based sys-
tem is 0.93 BLEU point higher than the IHMM-
based system and 3.0 BLEU points higher than the
best single system.
System MT06 MT08
BLEU% BLEU%
SystemA 32.51 25.63
SystemB 31.43 26.32
SystemC 31.50 23.43
SystemD 32.41 26.28
IHMM 36.05 28.39
Lattice 36.53 29.32
Table 2: Results on the MT06 and MT08 test sets
We take a real example from the output of the
two systems (in Table 3) to show that higher BLEU
scores correspond to better alignments and better
translations. The translation of System C is selected
as the backbone. From Table 3, we can see that
because of 1-to-1 mappings, ?Russia? is aligned to
?Russian? and ??s? to ?null? in the IHMM-based
model, which leads to the error translation ?Russian
1111
Source: ?dIE?h?i??dIEd?i?1??
SystemA: Russia merger of state-owned oil company and the state-run gas company in Russia
SystemB: Russia ?s state-owned oil company is working with Russia ?s state-run gas company mergers
SystemC: Russian state-run oil company is combined with the Russian state-run gas company
SystemD: Russia ?s state-owned oil companies are combined with Russia ?s state-run gas company
IHMM: Russian ?s state-owned oil company working with Russia ?s state-run gas company
Lattice: Russia ?s state-owned oil company is combined with the Russian state-run gas company
Table 3: A real translation example
?s?. Instead, ?Russia ?s? is together aligned to ?Rus-
sian? in the lattice-based model. Also due to 1-to-
1 mappings, null word aligned to ?is? is inserted.
As a result, ?is? is missed in the output of IHMM-
based model. In contrast, in the lattice-based sys-
tem, ?is working with? are aligned to ?is combined
with?, forming a phrase pair.
6.2 Effect of Dictionary Scale
The dictionary is important to the semantic similar-
ity model in IHMM-based alignment method. We
evaluated the effect of the dictionary scale by using
dictionaries extracted on different data sets. The dic-
tionaries were respectively extracted on similar data
sets: 30K sentence pairs, 60K sentence pairs, 289K
sentence pairs (FBIS corpus) and 2500K sentence
pairs. The results are illustrated in Table 4. In or-
der to demonstrate the effect of the dictionary size
clearly, the interpolation factor of similarity model
was all set to ? = 0.1.
From Table 4, we can see that when the cor-
pus size rise from 30k to 60k, the improvements
were not obvious both on the dev set and on the
test set. As the corpus was expanded to 289K, al-
though on the dev set, the result was only 0.2 BLEU
point higher, on the test set, it was 0.63 BLEU point
higher. As the corpus size was up to 2500K, the
BLEU scores both on the dev and test sets declined.
The reason is that, on one hand, there are more noise
on the 2500K sentence pairs; on the other hand, the
289K sentence pairs cover most of the words appear-
ing on the test set. So we can conclude that in or-
der to get better results, the dictionary scale must be
up to some certain scale. If the dictionary is much
smaller, the result will be impacted dramatically.
MT02 MT05
BLEU% BLEU%
30k 36.94 35.14
60k 37.09 35.17
289k 37.29 35.80
2500k 37.14 35.62
Table 4: Effect of dictionary scale
6.3 Effect of Semantic Alignments
For the IHMM-based alignment method, the transla-
tion probability of an English word pair is computed
using a linear interpolation of the semantic similar-
ity and the surface similarity. So the two similarity
models decide the translation probability together
and the proportion is controlled by the interpolation
factor. We evaluated the effect of the two similarity
models by varying the interpolation factor ?.
We used the dictionaries extracted on the FBIS
data set. The result is shown in Table 5. We got the
best result with ? = 0.1. When we excluded the
semantic similarity model (? = 0.0) or excluded the
surface similarity model (? = 1.0), the performance
became worse.
7 Conclusion
The alignment model plays an important role in
system combination. Because of the expression
limitation of confusion networks, only 1-to-1 map-
pings are employed in the confusion-network-based
model. This paper proposes a lattice-based system
combination model. As a general form of confusion
networks, lattices can express n-to-n mappings. So
a lattice-based model processes phrase pairs while
1112
MT02 MT05
BLEU% BLEU%
? = 1.0 36.41 34.92
? = 0.7 37.21 35.65
? = 0.5 36.43 35.02
? = 0.4 37.14 35.55
? = 0.3 36.75 35.66
? = 0.2 36.81 35.55
? = 0.1 37.29 35.80
? = 0.0 36.45 35.14
Table 5: Effect of semantic alignments
a confusion-network-based model processes words
only. As a result, phrase pairs must be extracted be-
fore constructing a lattice.
On NIST MT05 test set, the lattice-based sys-
tem gave better results with an absolute improve-
ment of 1.23 BLEU points over the confusion-
network-based system (He et al, 2008) and 3.73
BLEU points over the best single system. On
NIST MT08 test set, the lattice-based system out-
performed the confusion-network-based system by
0.93 BLEU point and outperformed the best single
system by 3.0 BLEU points.
8 Acknowledgement
The authors were supported by National Natural Sci-
ence Foundation of China Contract 60736014, Na-
tional Natural Science Foundation of China Con-
tract 60873167 and High Technology R&D Program
Project No. 2006AA010108. Thank Wenbin Jiang,
Tian Xia and Shu Cai for their help. We are also
grateful to the anonymous reviewers for their valu-
able comments.
References
Srinivas Bangalore, German Bordel, and Giuseppe Ric-
cardi. 2001. Computing consensus translation from
multiple machine translation systems. In Proc. of
IEEE ASRU, pages 351?354.
Christopher Dyer, Smaranda Muresan, and Philip Resnik.
2008. Generalizing word lattice translation. In Pro-
ceedings of ACL/HLT 2008, pages 1012?1020, Colum-
bus, Ohio, June.
Robert Frederking and Sergei Nirenburg. 1994. Three
heads are better than one. In Proc. of ANLP, pages
95?100.
Xiaodong He, Mei Yang, Jangfeng Gao, Patrick Nguyen,
and Robert Moore. 2008. Indirect-hmm-based hy-
pothesis alignment for computing outputs from ma-
chine translation systems. In Proc. of EMNLP, pages
98?107.
Xiaodong He. 2007. Using word-dependent translation
models in hmm based word alignment for statistical
machine translation. In Proc. of COLING-ACL, pages
961?968.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proceedings of the Ninth International
Workshop on Parsing Technologies (IWPT), pages 53?
64.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proc. of HLT-
NAACL, pages 127?133.
Philipp Koehn, Hieu Hoang, Alexandra Birch Mayne,
Christopher Callison-Burch, Marcello Federico,
Nicola Bertoldi, Brooke Cowan, Wade Shen, Chris-
tine Moran, Richard Zens, Chris Dyer, Ondrej Bojar,
Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open source toolkit for statistical machine
translation. In Proc. of the 45th ACL, Demonstration
Session.
Evgeny Matusov, Nicola Ueffing, and Hermann Ney.
2006. Computing consensus translation from multiple
machine translation systems using enhanced hypothe-
ses alignment. In Proc. of IEEE EACL, pages 33?40.
Antti-Veikko I. Rosti, Spyros Matsoukas, and Richard
Schwartz. 2007a. Improved word-level system com-
bination for machine translation. In Proc. of ACL,
pages 312?319.
Antti-Veikko I. Rosti, Bing Xiang, Spyros Matsoukas,
Richard Schwartz, Necip Fazil Ayan, and Bonnie J.
Dorr. 2007b. Combining outputs from multiple ma-
chine translation systems. In Proc. of NAACL-HLT,
pages 228?235.
Antti-Veikko I. Rosti, Bing Zhang, Spyros Matsoukas,
and Richard Schwartz. 2008. Incremental hypothesis
alignment for building confusion networks with appli-
cation to machine translaiton system combination. In
Proc. of the Third ACL WorkShop on Statistical Ma-
chine Translation, pages 183?186.
Khe Chai Sim, William J. Byrne, Mark J.F. Gales,
Hichem Sahbi, and Phil C. Woodland. 2007. Con-
sensus network decoding for statistical machine trans-
lation system combination. In Proc. of ICASSP, pages
105?108.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. Hmm-based word alignment in statistical trans-
lation. In Proc. of COLING, pages 836?841.
1113
Proceedings of the 43rd Annual Meeting of the ACL, pages 459?466,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Log-linear Models for Word Alignment
Yang Liu , Qun Liu and Shouxun Lin
Institute of Computing Technology
Chinese Academy of Sciences
No. 6 Kexueyuan South Road, Haidian District
P. O. Box 2704, Beijing, 100080, China
{yliu, liuqun, sxlin}@ict.ac.cn
Abstract
We present a framework for word align-
ment based on log-linear models. All
knowledge sources are treated as feature
functions, which depend on the source
langauge sentence, the target language
sentence and possible additional vari-
ables. Log-linear models allow statis-
tical alignment models to be easily ex-
tended by incorporating syntactic infor-
mation. In this paper, we use IBM Model
3 alignment probabilities, POS correspon-
dence, and bilingual dictionary cover-
age as features. Our experiments show
that log-linear models significantly out-
perform IBM translation models.
1 Introduction
Word alignment, which can be defined as an object
for indicating the corresponding words in a parallel
text, was first introduced as an intermediate result of
statistical translation models (Brown et al, 1993). In
statistical machine translation, word alignment plays
a crucial role as word-aligned corpora have been
found to be an excellent source of translation-related
knowledge.
Various methods have been proposed for finding
word alignments between parallel texts. There are
generally two categories of alignment approaches:
statistical approaches and heuristic approaches.
Statistical approaches, which depend on a set of
unknown parameters that are learned from training
data, try to describe the relationship between a bilin-
gual sentence pair (Brown et al, 1993; Vogel and
Ney, 1996). Heuristic approaches obtain word align-
ments by using various similarity functions between
the types of the two languages (Smadja et al, 1996;
Ker and Chang, 1997; Melamed, 2000). The cen-
tral distinction between statistical and heuristic ap-
proaches is that statistical approaches are based on
well-founded probabilistic models while heuristic
ones are not. Studies reveal that statistical alignment
models outperform the simple Dice coefficient (Och
and Ney, 2003).
Finding word alignments between parallel texts,
however, is still far from a trivial work due to the di-
versity of natural languages. For example, the align-
ment of words within idiomatic expressions, free
translations, and missing content or function words
is problematic. When two languages widely differ
in word order, finding word alignments is especially
hard. Therefore, it is necessary to incorporate all
useful linguistic information to alleviate these prob-
lems.
Tiedemann (2003) introduced a word alignment
approach based on combination of association clues.
Clues combination is done by disjunction of single
clues, which are defined as probabilities of associa-
tions. The crucial assumption of clue combination
that clues are independent of each other, however,
is not always true. Och and Ney (2003) proposed
Model 6, a log-linear combination of IBM transla-
tion models and HMM model. Although Model 6
yields better results than naive IBM models, it fails
to include dependencies other than IBM models and
HMM model. Cherry and Lin (2003) developed a
459
statistical model to find word alignments, which al-
low easy integration of context-specific features.
Log-linear models, which are very suitable to in-
corporate additional dependencies, have been suc-
cessfully applied to statistical machine translation
(Och and Ney, 2002). In this paper, we present a
framework for word alignment based on log-linear
models, allowing statistical models to be easily ex-
tended by incorporating additional syntactic depen-
dencies. We use IBM Model 3 alignment proba-
bilities, POS correspondence, and bilingual dictio-
nary coverage as features. Our experiments show
that log-linear models significantly outperform IBM
translation models.
We begin by describing log-linear models for
word alignment. The design of feature functions
is discussed then. Next, we present the training
method and the search algorithm for log-linear mod-
els. We will follow with our experimental results
and conclusion and close with a discussion of possi-
ble future directions.
2 Log-linear Models
Formally, we use following definition for alignment.
Given a source (?English?) sentence e = eI1 = e1,
. . . , ei, . . . , eI and a target language (?French?) sen-
tence f = fJ1 = f1, . . . , fj , . . . , fJ . We define a link
l = (i, j) to exist if ei and fj are translation (or part
of a translation) of one another. We define the null
link l = (i, 0) to exist if ei does not correspond to a
translation for any French word in f . The null link
l = (0, j) is defined similarly. An alignment a is
defined as a subset of the Cartesian product of the
word positions:
a ? {(i, j) : i = 0, . . . , I; j = 0, . . . , J} (1)
We define the alignment problem as finding the
alignment a that maximizes Pr(a | e, f ) given e and
f .
We directly model the probability Pr(a | e, f ).
An especially well-founded framework is maximum
entropy (Berger et al, 1996). In this framework, we
have a set of M feature functions hm(a, e, f), m =
1, . . . , M . For each feature function, there exists
a model parameter ?m, m = 1, . . . , M . The direct
alignment probability is given by:
Pr(a|e, f) = exp[
?M
m=1 ?mhm(a, e, f)]?
a? exp[
?M
m=1 ?mhm(a?, e, f)](2)
This approach has been suggested by (Papineni et
al., 1997) for a natural language understanding task
and successfully applied to statistical machine trans-
lation by (Och and Ney, 2002).
We obtain the following decision rule:
a? = argmax
a
{ M?
m=1
?mhm(a, e, f)
}
(3)
Typically, the source language sentence e and the
target sentence f are the fundamental knowledge
sources for the task of finding word alignments. Lin-
guistic data, which can be used to identify associ-
ations between lexical items are often ignored by
traditional word alignment approaches. Linguistic
tools such as part-of-speech taggers, parsers, named-
entity recognizers have become more and more ro-
bust and available for many languages by now. It
is important to make use of linguistic information
to improve alignment strategies. Treated as feature
functions, syntactic dependencies can be easily in-
corporated into log-linear models.
In order to incorporate a new dependency which
contains extra information other than the bilingual
sentence pair, we modify Eq.2 by adding a new vari-
able v:
Pr(a|e, f ,v) = exp[
?M
m=1 ?mhm(a, e, f ,v)]?
a? exp[
?M
m=1 ?mhm(a?, e, f ,v)](4)
Accordingly, we get a new decision rule:
a? = argmax
a
{ M?
m=1
?mhm(a, e, f ,v)
}
(5)
Note that our log-linear models are different from
Model 6 proposed by Och and Ney (2003), which
defines the alignment problem as finding the align-
ment a that maximizes Pr(f , a | e) given e.
3 Feature Functions
In this paper, we use IBM translation Model 3 as the
base feature of our log-linear models. In addition,
we also make use of syntactic information such as
part-of-speech tags and bilingual dictionaries.
460
3.1 IBM Translation Models
Brown et al (1993) proposed a series of statisti-
cal models of the translation process. IBM trans-
lation models try to model the translation probabil-
ity Pr(fJ1 |eI1), which describes the relationship be-
tween a source language sentence eI1 and a target
language sentence fJ1 . In statistical alignment mod-
els Pr(fJ1 , aJ1 |eI1), a ?hidden? alignment a = aJ1 is
introduced, which describes a mapping from a tar-
get position j to a source position i = aj . The
relationship between the translation model and the
alignment model is given by:
Pr(fJ1 |eI1) =
?
aJ1
Pr(fJ1 , aJ1 |eI1) (6)
Although IBM models are considered more co-
herent than heuristic models, they have two draw-
backs. First, IBM models are restricted in a way
such that each target word fj is assigned to exactly
one source word eaj . A more general way is to
model alignment as an arbitrary relation between
source and target language positions. Second, IBM
models are typically language-independent and may
fail to tackle problems occurred due to specific lan-
guages.
In this paper, we use Model 3 as our base feature
function, which is given by 1:
h(a, e, f) = Pr(fJ1 , aJ1 |eI1)
=
(
m? ?0
?0
)
p0m?2?0p1?0
l?
i=1
?i!n(?i|ei)?
m?
j=1
t(fj |eaj )d(j|aj , l,m) (7)
We distinguish between two translation directions
to use Model 3 as feature functions: treating English
as source language and French as target language or
vice versa.
3.2 POS Tags Transition Model
The first linguistic information we adopt other than
the source language sentence e and the target lan-
guage sentence f is part-of-speech tags. The use
of POS information for improving statistical align-
ment quality of the HMM-based model is described
1If there is a target word which is assigned to more than one
source words, h(a, e, f) = 0.
in (Toutanova et al, 2002). They introduce addi-
tional lexicon probability for POS tags in both lan-
guages.
In IBM models as well as HMM models, when
one needs the model to take new information into
account, one must create an extended model which
can base its parameters on the previous model. In
log-linear models, however, new information can be
easily incorporated.
We use a POS Tags Transition Model as a fea-
ture function. This feature learns POS Tags tran-
sition probabilities from held-out data (via simple
counting) and then applies the learned distributions
to the ranking of various word alignments. We
define eT = eT I1 = eT1, . . . , eTi, . . . , eTI and
fT = fT J1 = fT1, . . . , fTj , . . . , fTJ as POS tag
sequences of the sentence pair e and f . POS Tags
Transition Model is formally described as:
Pr(fT|a, eT) =
?
a
t(fTa(j)|eTa(i)) (8)
where a is an element of a, a(i) is the corresponding
source position of a and a(j) is the target position.
Hence, the feature function is:
h(a, e, f , eT, fT) =
?
a
t(fTa(j)|eTa(i)) (9)
We still distinguish between two translation direc-
tions to use POS tags Transition Model as feature
functions: treating English as source language and
French as target language or vice versa.
3.3 Bilingual Dictionary
A conventional bilingual dictionary can be consid-
ered an additional knowledge source. We could use
a feature that counts how many entries of a conven-
tional lexicon co-occur in a given alignment between
the source sentence and the target sentence. There-
fore, the weight for the provided conventional dic-
tionary can be learned. The intuition is that the con-
ventional dictionary is expected to be more reliable
than the automatically trained lexicon and therefore
should get a larger weight.
We define a bilingual dictionary as a set of entries:
D = {(e, f, conf)}. e is a source language word,
f is a target langauge word, and conf is a positive
real-valued number (usually, conf = 1.0) assigned
461
by lexicographers to evaluate the validity of the en-
try. Therefore, the feature function using a bilingual
dictionary is:
h(a, e, f ,D) =
?
a
occur(ea(i), fa(j), D) (10)
where
occur(e, f,D) =
{
conf if (e, f) occurs in D
0 else
(11)
4 Training
We use the GIS (Generalized Iterative Scaling) al-
gorithm (Darroch and Ratcliff, 1972) to train the
model parameters ?M1 of the log-linear models ac-
cording to Eq. 4. By applying suitable transforma-
tions, the GIS algorithm is able to handle any type of
real-valued features. In practice, We use YASMET
2 written by Franz J. Och for performing training.
The renormalization needed in Eq. 4 requires a
sum over a large number of possible alignments. If
e has length l and f has length m, there are pos-
sible 2lm alignments between e and f (Brown et
al., 1993). It is unrealistic to enumerate all possi-
ble alignments when lm is very large. Hence, we
approximate this sum by sampling the space of all
possible alignments by a large set of highly proba-
ble alignments. The set of considered alignments are
also called n-best list of alignments.
We train model parameters on a development cor-
pus, which consists of hundreds of manually-aligned
bilingual sentence pairs. Using an n-best approx-
imation may result in the problem that the param-
eters trained with the GIS algorithm yield worse
alignments even on the development corpus. This
can happen because with the modified model scaling
factors the n-best list can change significantly and
can include alignments that have not been taken into
account in training. To avoid this problem, we iter-
atively combine n-best lists to train model parame-
ters until the resulting n-best list does not change,
as suggested by Och (2002). However, as this train-
ing procedure is based on maximum likelihood cri-
terion, there is only a loose relation to the final align-
ment quality on unseen bilingual texts. In practice,
2Available at http://www.fjoch.com/YASMET.html
having a series of model parameters when the itera-
tion ends, we select the model parameters that yield
best alignments on the development corpus.
After the bilingual sentences in the develop-
ment corpus are tokenized (or segmented) and POS
tagged, they can be used to train POS tags transition
probabilities by counting relative frequencies:
p(fT |eT ) = NA(fT, eT )N(eT )
Here, NA(fT, eT ) is the frequency that the POS tag
fT is aligned to POS tag eT and N(eT ) is the fre-
quency of eT in the development corpus.
5 Search
We use a greedy search algorithm to search the
alignment with highest probability in the space of all
possible alignments. A state in this space is a partial
alignment. A transition is defined as the addition of
a single link to the current state. Our start state is
the empty alignment, where all words in e and f are
assigned to null. A terminal state is a state in which
no more links can be added to increase the probabil-
ity of the current alignment. Our task is to find the
terminal state with the highest probability.
We can compute gain, which is a heuristic func-
tion, instead of probability for efficiency. A gain is
defined as follows:
gain(a, l) = exp[
?M
m=1 ?mhm(a ? l, e, f)]
exp[?Mm=1 ?mhm(a, e, f)]
(12)
where l = (i, j) is a link added to a.
The greedy search algorithm for general log-
linear models is formally described as follows:
Input: e, f , eT, fT, and D
Output: a
1. Start with a = ?.
2. Do for each l = (i, j) and l /? a:
Compute gain(a, l)
3. Terminate if ?l, gain(a, l) ? 1.
4. Add the link l? with the maximal gain(a, l)
to a.
5. Goto 2.
462
The above search algorithm, however, is not effi-
cient for our log-linear models. It is time-consuming
for each feature to figure out a probability when
adding a new link, especially when the sentences
are very long. For our models, gain(a, l) can be
obtained in a more efficient way 3:
gain(a, l) =
M?
m=1
?mlog
(hm(a ? l, e, f)
hm(a, e, f)
)
(13)
Note that we restrict that h(a, e, f) ? 0 for all fea-
ture functions.
The original terminational condition for greedy
search algorithm is:
gain(a, l) = exp[
?M
m=1 ?mhm(a ? l, e, f)]
exp[?Mm=1 ?mhm(a, e, f)]
? 1.0
That is:
M?
m=1
?m[hm(a ? l, e, f)? hm(a, e, f)] ? 0.0
By introducing gain threshold t, we obtain a new
terminational condition:
M?
m=1
?mlog
(hm(a ? l, e, f)
hm(a, e, f)
)
? t
where
t =
M?
m=1
?m
{
log
(hm(a ? l, e, f)
hm(a, e, f)
)
?[hm(a ? l, e, f)? hm(a, e, f)]
}
Note that we restrict h(a, e, f) ? 0 for all feature
functions. Gain threshold t is a real-valued number,
which can be optimized on the development corpus.
Therefore, we have a new search algorithm:
Input: e, f , eT, fT, D and t
Output: a
1. Start with a = ?.
2. Do for each l = (i, j) and l /? a:
Compute gain(a, l)
3We still call the new heuristic function gain to reduce no-
tational overhead, although the gain in Eq. 13 is not equivalent
to the one in Eq. 12.
3. Terminate if ?l, gain(a, l) ? t.
4. Add the link l? with the maximal gain(a, l)
to a.
5. Goto 2.
The gain threshold t depends on the added link
l. We remove this dependency for simplicity when
using it in search algorithm by treating it as a fixed
real-valued number.
6 Experimental Results
We present in this section results of experiments on
a parallel corpus of Chinese-English texts. Statis-
tics for the corpus are shown in Table 1. We use a
training corpus, which is used to train IBM transla-
tion models, a bilingual dictionary, a development
corpus, and a test corpus.
Chinese English
Train Sentences 108 925
Words 3 784 106 3 862 637
Vocabulary 49 962 55 698
Dict Entries 415 753
Vocabulary 206 616 203 497
Dev Sentences 435
Words 11 462 14 252
Ave. SentLen 26.35 32.76
Test Sentences 500
Words 13 891 15 291
Ave. SentLen 27.78 30.58
Table 1. Statistics of training corpus (Train), bilin-
gual dictionary (Dict), development corpus (Dev),
and test corpus (Test).
The Chinese sentences in both the development
and test corpus are segmented and POS tagged by
ICTCLAS (Zhang et al, 2003). The English sen-
tences are tokenized by a simple tokenizer of ours
and POS tagged by a rule-based tagger written by
Eric Brill (Brill, 1995). We manually aligned 935
sentences, in which we selected 500 sentences as
test corpus. The remaining 435 sentences are used
as development corpus to train POS tags transition
probabilities and to optimize the model parameters
and gain threshold.
Provided with human-annotated word-level align-
ment, we use precision, recall and AER (Och and
463
Size of Training Corpus
1K 5K 9K 39K 109K
Model 3 E ? C 0.4497 0.4081 0.4009 0.3791 0.3745
Model 3 C ? E 0.4688 0.4261 0.4221 0.3856 0.3469
Intersection 0.4588 0.4106 0.4044 0.3823 0.3687
Union 0.4596 0.4210 0.4157 0.3824 0.3703
Refined Method 0.4154 0.3586 0.3499 0.3153 0.3068
Model 3 E ? C 0.4490 0.3987 0.3834 0.3639 0.3533
+ Model 3 C ? E 0.3970 0.3317 0.3217 0.2949 0.2850
+ POS E ? C 0.3828 0.3182 0.3082 0.2838 0.2739
+ POS C ? E 0.3795 0.3160 0.3032 0.2821 0.2726
+ Dict 0.3650 0.3092 0.2982 0.2738 0.2685
Table 2. Comparison of AER for results of using IBM Model 3 (GIZA++) and log-linear models.
Ney, 2003) for scoring the viterbi alignments of each
model against gold-standard annotated alignments:
precision = |A ? P ||A|
recall = |A ? S||S|
AER = 1? |A ? S|+ |A ? P ||A|+ |S|
where A is the set of word pairs aligned by word
alignment systems, S is the set marked in the gold
standard as ?sure? and P is the set marked as ?pos-
sible? (including the ?sure? pairs). In our Chinese-
English corpus, only one type of alignment was
marked, meaning that S = P .
In the following, we present the results of log-
linear models for word alignment. We used GIZA++
package (Och and Ney, 2003) to train IBM transla-
tion models. The training scheme is 15H535, which
means that Model 1 are trained for five iterations,
HMM model for five iterations and finally Model
3 for five iterations. Except for changing the iter-
ations for each model, we use default configuration
of GIZA++. After that, we used three types of meth-
ods for performing a symmetrization of IBM mod-
els: intersection, union, and refined methods (Och
and Ney , 2003).
The base feature of our log-linear models, IBM
Model 3, takes the parameters generated by GIZA++
as parameters for itself. In other words, our log-
linear models share GIZA++ with the same parame-
ters apart from POS transition probability table and
bilingual dictionary.
Table 2 compares the results of our log-linear
models with IBM Model 3. From row 3 to row 7
are results obtained by IBM Model 3. From row 8
to row 12 are results obtained by log-linear models.
As shown in Table 2, our log-linear models
achieve better results than IBM Model 3 in all train-
ing corpus sizes. Considering Model 3 E ? C of
GIZA++ and ours alone, greedy search algorithm
described in Section 5 yields surprisingly better
alignments than hillclimbing algorithm in GIZA++.
Table 3 compares the results of log-linear mod-
els with IBM Model 5. The training scheme is
15H5354555. Our log-linear models still make use
of the parameters generated by GIZA++.
Comparing Table 3 with Table 2, we notice that
our log-linear models yield slightly better align-
ments by employing parameters generated by the
training scheme 15H5354555 rather than 15H535,
which can be attributed to improvement of param-
eters after further Model 4 and Model 5 training.
For log-linear models, POS information and an
additional dictionary are used, which is not the case
for GIZA++/IBM models. However, treated as a
method for performing symmetrization, log-linear
combination alone yields better results than intersec-
tion, union, and refined methods.
Figure 1 shows how gain threshold has an effect
on precision, recall and AER with fixed model scal-
ing factors.
Figure 2 shows the effect of number of features
464
Size of Training Corpus
1K 5K 9K 39K 109K
Model 5 E ? C 0.4384 0.3934 0.3853 0.3573 0.3429
Model 5 C ? E 0.4564 0.4067 0.3900 0.3423 0.3239
Intersection 0.4432 0.3916 0.3798 0.3466 0.3267
Union 0.4499 0.4051 0.3923 0.3516 0.3375
Refined Method 0.4106 0.3446 0.3262 0.2878 0.2748
Model 3 E ? C 0.4372 0.3873 0.3724 0.3456 0.3334
+ Model 3 C ? E 0.3920 0.3269 0.3167 0.2842 0.2727
+ POS E ? C 0.3807 0.3122 0.3039 0.2732 0.2667
+ POS C ? E 0.3731 0.3091 0.3017 0.2722 0.2657
+ Dict 0.3612 0.3046 0.2943 0.2658 0.2625
Table 3. Comparison of AER for results of using IBM Model 5 (GIZA++) and log-linear models.
-12 -10 -8 -6 -4 -2 0 2 4 6 8 10
0.0
0.2
0.4
0.6
0.8
1.0
gain threshold
 Precision
 Recall
 AER
Figure 1. Precision, recall and AER over different
gain thresholds with the same model scaling factors.
and size of training corpus on search efficiency for
log-linear models.
Table 4 shows the resulting normalized model
scaling factors. We see that adding new features also
has an effect on the other model scaling factors.
7 Conclusion
We have presented a framework for word alignment
based on log-linear models between parallel texts. It
allows statistical models easily extended by incor-
porating syntactic information. We take IBM Model
3 as base feature and use syntactic information such
as POS tags and bilingual dictionary. Experimental
1k 5k 9k 39k 109k
200
400
600
800
1000
1200
t
i
m
e
 
c
o
n
s
u
m
e
d
 
f
o
r
 
s
e
a
r
c
h
i
n
g
 
(
s
e
c
o
n
d
)
size of training corpus
 M3EC
 M3EC + M3CE
 M3EC + M3CE + POSEC
 M3EC + M3CE + POSEC + POSCE
 M3EC + M3CE + POSEC + POSCE + Dict
Figure 2. Effect of number of features and size of
training corpus on search efficiency.
MEC +MCE +PEC +PCE +Dict
?1 1.000 0.466 0.291 0.202 0.151
?2 - 0.534 0.312 0.212 0.167
?3 - - 0.397 0.270 0.257
?4 - - - 0.316 0.306
?5 - - - - 0.119
Table 4. Resulting model scaling factors: ?1: Model
3 E ? C (MEC); ?2: Model 3 C ? E (MCE); ?3:
POS E ? C (PEC); ?4: POS C ? E (PCE); ?5: Dict
(normalized such that ?5m=1 ?m = 1).
results show that log-linear models for word align-
ment significantly outperform IBM translation mod-
els. However, the search algorithm we proposed is
465
supervised, relying on a hand-aligned bilingual cor-
pus, while the baseline approach of IBM alignments
is unsupervised.
Currently, we only employ three types of knowl-
edge sources as feature functions. Syntax-based
translation models, such as tree-to-string model (Ya-
mada and Knight, 2001) and tree-to-tree model
(Gildea, 2003), may be very suitable to be added into
log-linear models.
It is promising to optimize the model parameters
directly with respect to AER as suggested in statisti-
cal machine translation (Och, 2003).
Acknowledgement
This work is supported by National High Technol-
ogy Research and Development Program contract
?Generally Technical Research and Basic Database
Establishment of Chinese Platform? (Subject No.
2004AA114010).
References
Adam L. Berger, Stephen A. Della Pietra, and Vincent J.
DellaPietra. 1996. A maximum entropy approach to
natural language processing. Computational Linguis-
tics, 22(1):39-72, March.
Eric Brill. 1995. Transformation-based-error-driven
learning and natural language processing: A case
study in part-of-speech tagging. Computational Lin-
guistics, 21(4), December.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert. L. Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19(2):263-311.
Colin Cherry and Dekang Lin. 2003. A probability
model to improve word alignment. In Proceedings of
the 41st Annual Meeting of the Association for Com-
putational Linguistics (ACL), Sapporo, Japan.
J. N. Darroch and D. Ratcliff. 1972. Generalized itera-
tive scaling for log-linear models. Annals of Mathe-
matical Statistics, 43:1470-1480.
Daniel Gildea. 2003. Loosely tree-based alignment for
machine translation. In Proceedings of the 41st An-
nual Meeting of the Association for Computational
Linguistics (ACL), Sapporo, Japan.
Sue J. Ker and Jason S. Chang. 1997. A class-based ap-
proach to word alignment. Computational Linguistics,
23(2):313-343, June.
I. Dan Melamed 2000. Models of translational equiv-
alence among words. Computational Linguistics,
26(2):221-249, June.
Franz J. Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for statis-
tical machine translation. In Proceedings of the 40th
Annual Meeting of the Association for Computational
Linguistics (ACL), pages 295-302, Philadelphia, PA,
July.
Franz J. Och. 2002. Statistical Machine Translation:
From Single-Word Models to Alignment Templates.
Ph.D. thesis, Computer Science Department, RWTH
Aachen, Germany, October.
Franz J. Och. 2003. Minimum error rate training in sta-
tistical machine translation. In Proceedings of the 41st
Annual Meeting of the Association for Computational
Linguistics (ACL), pages: 160-167, Sapporo, Japan.
Franz J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29(1):19-51, March.
Kishore A. Papineni, Salim Roukos, and Todd Ward.
1997. Feature-based language understanding. In Eu-
ropean Conf. on Speech Communication and Technol-
ogy, pages 1435-1438, Rhodes, Greece, September.
Frank Smadja, Vasileios Hatzivassiloglou, and Kathleen
R. McKeown 1996. Translating collocations for bilin-
gual lexicons: A statistical approach. Computational
Linguistics, 22(1):1-38, March.
Jo?rg Tiedemann. 2003. Combining clues for word align-
ment. In Proceedings of the 10th Conference of Euro-
pean Chapter of the ACL (EACL), Budapest, Hungary,
April.
Kristina Toutanova, H. Tolga Ilhan, and Christopher D.
Manning. 2003. Extensions to HMM-based statistical
word alignment models. In Proceedings of Empirical
Methods in Natural Langauge Processing, Philadel-
phia, PA.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical trans-
lation. In Proceedings of the 16th Int. Conf. on Com-
putational Linguistics, pages 836-841, Copenhagen,
Denmark, August.
Kenji Yamada and Kevin Knight. 2001. A syntax-
based statistical machine translation model. In Pro-
ceedings of the 39th Annual Meeting of the Association
for Computational Linguistics (ACL), pages: 523-530,
Toulouse, France, July.
Huaping Zhang, Hongkui Yu, Deyi Xiong, and Qun Liu.
2003. HHMM-based Chinese lexical analyzer ICT-
CLAS. In Proceedings of the second SigHan Work-
shop affiliated with 41th ACL, pages: 184-187, Sap-
poro, Japan.
466
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 609?616,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Tree-to-String Alignment Template for Statistical Machine Translation
Yang Liu , Qun Liu , and Shouxun Lin
Institute of Computing Technology
Chinese Academy of Sciences
No.6 Kexueyuan South Road, Haidian District
P. O. Box 2704, Beijing, 100080, China
{yliu,liuqun,sxlin}@ict.ac.cn
Abstract
We present a novel translation model
based on tree-to-string alignment template
(TAT) which describes the alignment be-
tween a source parse tree and a target
string. A TAT is capable of generating
both terminals and non-terminals and per-
forming reordering at both low and high
levels. The model is linguistically syntax-
based because TATs are extracted auto-
matically from word-aligned, source side
parsed parallel texts. To translate a source
sentence, we first employ a parser to pro-
duce a source parse tree and then ap-
ply TATs to transform the tree into a tar-
get string. Our experiments show that
the TAT-based model significantly outper-
forms Pharaoh, a state-of-the-art decoder
for phrase-based models.
1 Introduction
Phrase-based translation models (Marcu and
Wong, 2002; Koehn et al, 2003; Och and Ney,
2004), which go beyond the original IBM trans-
lation models (Brown et al, 1993) 1 by model-
ing translations of phrases rather than individual
words, have been suggested to be the state-of-the-
art in statistical machine translation by empirical
evaluations.
In phrase-based models, phrases are usually
strings of adjacent words instead of syntactic con-
stituents, excelling at capturing local reordering
and performing translations that are localized to
1The mathematical notation we use in this paper is taken
from that paper: a source string fJ1 = f1, . . . , fj , . . . , fJ is
to be translated into a target string eI1 = e1, . . . , ei, . . . , eI .
Here, I is the length of the target string, and J is the length
of the source string.
substrings that are common enough to be observed
on training data. However, a key limitation of
phrase-based models is that they fail to model re-
ordering at the phrase level robustly. Typically,
phrase reordering is modeled in terms of offset po-
sitions at the word level (Koehn, 2004; Och and
Ney, 2004), making little or no direct use of syn-
tactic information.
Recent research on statistical machine transla-
tion has lead to the development of syntax-based
models. Wu (1997) proposes Inversion Trans-
duction Grammars, treating translation as a pro-
cess of parallel parsing of the source and tar-
get language via a synchronized grammar. Al-
shawi et al (2000) represent each production in
parallel dependency tree as a finite transducer.
Melamed (2004) formalizes machine translation
problem as synchronous parsing based on multi-
text grammars. Graehl and Knight (2004) describe
training and decoding algorithms for both gen-
eralized tree-to-tree and tree-to-string transduc-
ers. Chiang (2005) presents a hierarchical phrase-
based model that uses hierarchical phrase pairs,
which are formally productions of a synchronous
context-free grammar. Ding and Palmer (2005)
propose a syntax-based translation model based
on a probabilistic synchronous dependency in-
sert grammar, a version of synchronous gram-
mars defined on dependency trees. All these ap-
proaches, though different in formalism, make use
of synchronous grammars or tree-based transduc-
tion rules to model both source and target lan-
guages.
Another class of approaches make use of syn-
tactic information in the target language alone,
treating the translation problem as a parsing prob-
lem. Yamada and Knight (2001) use a parser in
the target language to train probabilities on a set of
609
operations that transform a target parse tree into a
source string.
Paying more attention to source language anal-
ysis, Quirk et al (2005) employ a source language
dependency parser, a target language word seg-
mentation component, and an unsupervised word
alignment component to learn treelet translations
from parallel corpus.
In this paper, we propose a statistical translation
model based on tree-to-string alignment template
which describes the alignment between a source
parse tree and a target string. A TAT is capa-
ble of generating both terminals and non-terminals
and performing reordering at both low and high
levels. The model is linguistically syntax-based
because TATs are extracted automatically from
word-aligned, source side parsed parallel texts.
To translate a source sentence, we first employ a
parser to produce a source parse tree and then ap-
ply TATs to transform the tree into a target string.
One advantage of our model is that TATs can
be automatically acquired to capture linguistically
motivated reordering at both low (word) and high
(phrase, clause) levels. In addition, the training of
TAT-based model is less computationally expen-
sive than tree-to-tree models. Similarly to (Galley
et al, 2004), the tree-to-string alignment templates
discussed in this paper are actually transformation
rules. The major difference is that we model the
syntax of the source language instead of the target
side. As a result, the task of our decoder is to find
the best target string while Galley?s is to seek the
most likely target tree.
2 Tree-to-String Alignment Template
A tree-to-string alignment template z is a triple
?T? , S?, A??, which describes the alignment A? be-
tween a source parse tree T? = T (F J ?1 ) 2 and
a target string S? = EI?1 . A source string F J
?
1 ,
which is the sequence of leaf nodes of T (F J ?1 ),
consists of both terminals (source words) and non-
terminals (phrasal categories). A target string EI?1
is also composed of both terminals (target words)
and non-terminals (placeholders). An alignment
A? is defined as a subset of the Cartesian product
of source and target symbol positions:
A? ? {(j, i) : j = 1, . . . , J ?; i = 1, . . . , I ?} (1)
2We use T (?) to denote a parse tree. To reduce notational
overhead, we use T (z) to represent the parse tree in z. Simi-
larly, S(z) denotes the string in z.
Figure 1 shows three TATs automatically
learned from training data. Note that when
demonstrating a TAT graphically, we represent
non-terminals in the target strings by blanks.
NP
NR
??
NN
??
LCP
NP
NR
??
CC
?
NR
LC
?
NP
DNP
NP DEG
NP
President Bush
between United States and
Figure 1: Examples of tree-to-string alignment
templates obtained in training
In the following, we formally describe how to
introduce tree-to-string alignment templates into
probabilistic dependencies to model Pr(eI1|fJ1 ) 3.
In a first step, we introduce the hidden variable
T (fJ1 ) that denotes a parse tree of the source sen-
tence fJ1 :
Pr(eI1|fJ1 ) =
?
T (fJ1 )
Pr(eI1, T (fJ1 )|fJ1 ) (2)
=
?
T (fJ1 )
Pr(T (fJ1 )|fJ1 )Pr(eI1|T (fJ1 ), fJ1 ) (3)
Next, another hidden variable D is introduced
to detach the source parse tree T (fJ1 ) into a se-
quence of K subtrees T?K1 with a preorder transver-
sal. We assume that each subtree T?k produces
a target string S?k. As a result, the sequence
of subtrees T?K1 produces a sequence of target
strings S?K1 , which can be combined serially to
generate the target sentence eI1. We assume that
Pr(eI1|D,T (fJ1 ), fJ1 ) ? Pr(S?K1 |T?K1 ) because eI1
is actually generated by the derivation of S?K1 .
Note that we omit an explicit dependence on the
detachment D to avoid notational overhead.
Pr(eI1|T (fJ1 ), fJ1 ) =
?
D
Pr(eI1, D|T (fJ1 ), fJ1 ) (4)
=
?
D
Pr(D|T (fJ1 ), fJ1 )Pr(eI1|D,T (fJ1 ), fJ1 ) (5)
=
?
D
Pr(D|T (fJ1 ), fJ1 )Pr(S?K1 |T?K1 ) (6)
=
?
D
Pr(D|T (fJ1 ), fJ1 )
K?
k=1
Pr(S?k|T?k) (7)
3The notational convention will be as follows. We use
the symbol Pr(?) to denote general probability distribution
with no specific assumptions. In contrast, for model-based
probability distributions, we use generic symbol p(?).
610
NP
DNP
NP
NR
??
DEG
?
NP
NN
??
NN
??
NP
DNP
NP DEG
?
NP
NP
NR
??
NP
NN NN
NN
??
NN
??
?? ? ?? ??
parsing
detachment production
of
China
economic development
combination
economic development of China
Figure 2: Graphic illustration for translation pro-
cess
To further decompose Pr(S?|T? ), the tree-to-
string alignment template, denoted by the variable
z, is introduced as a hidden variable.
Pr(S?|T? ) =
?
z
Pr(S?, z|T? ) (8)
=
?
z
Pr(z|T? )Pr(S?|z, T? ) (9)
Therefore, the TAT-based translation model can
be decomposed into four sub-models:
1. parse model: Pr(T (fJ1 )|fJ1 )
2. detachment model: Pr(D|T (fJ1 ), fJ1 )
3. TAT selection model: Pr(z|T? )
4. TAT application model: Pr(S?|z, T? )
Figure 2 shows how TATs work to perform
translation. First, the input source sentence is
parsed. Next, the parse tree is detached into five
subtrees with a preorder transversal. For each sub-
tree, a TAT is selected and applied to produce a
string. Finally, these strings are combined serially
to generate the translation (we use X to denote the
non-terminal):
X1 ? X2 of X3
? X2 of China
? X3 X4 of China
? economic X4 of China
? economic development of China
Following Och and Ney (2002), we base our
model on log-linear framework. Hence, all knowl-
edge sources are described as feature functions
that include the given source string fJ1 , the target
string eI1, and hidden variables. The hidden vari-
able T (fJ1 ) is omitted because we usually make
use of only single best output of a parser. As we
assume that all detachment have the same proba-
bility, the hidden variable D is also omitted. As
a result, the model we actually adopt for exper-
iments is limited because the parse, detachment,
and TAT application sub-models are simplified.
Pr(eI1, zK1 |fJ1 )
= exp[
?M
m=1 ?mhm(eI1, fJ1 , zK1 )]?
e?I1,z?K1 exp[
?M
m=1 ?mhm(e?I1, fJ1 , z?K1 )]
e?I1 = argmax
eI1,zK1
{ M?
m=1
?mhm(eI1, fJ1 , zK1 )
}
For our experiments we use the following seven
feature functions 4 that are analogous to default
feature set of Pharaoh (Koehn, 2004). To simplify
the notation, we omit the dependence on the hid-
den variables of the model.
h1(eI1, fJ1 ) = log
K?
k=1
N(z) ? ?(T (z), T?k)
N(T (z))
h2(eI1, fJ1 ) = log
K?
k=1
N(z) ? ?(T (z), T?k)
N(S(z))
h3(eI1, fJ1 ) = log
K?
k=1
lex(T (z)|S(z)) ? ?(T (z), T?k)
h4(eI1, fJ1 ) = log
K?
k=1
lex(S(z)|T (z)) ? ?(T (z), T?k)
h5(eI1, fJ1 ) = K
h6(eI1, fJ1 ) = log
I?
i=1
p(ei|ei?2, ei?1)
h7(eI1, fJ1 ) = I
4When computing lexical weighting features (Koehn et
al., 2003), we take only terminals into account. If there are
no terminals, we set the feature value to 1. We use lex(?)
to denote lexical weighting. We denote the number of TATs
used for decoding by K and the length of target string by I .
611
Tree String Alignment
( NR?? ) Bush 1:1
( NN?? ) President 1:1
( VV?? ) made 1:1
( NN?? ) speech 1:1
( NP ( NR ) ( NN ) ) X1 | X2 1:2 2:1
( NP ( NR?? ) ( NN ) ) X | Bush 1:2 2:1
( NP ( NR ) ( NN?? ) ) President | X 1:2 2:1
( NP ( NR?? ) ( NN?? ) ) President | Bush 1:2 2:1
( VP ( VV ) ( NN ) ) X1 | a | X2 1:1 2:3
( VP ( VV?? ) ( NN ) ) made | a | X 1:1 2:3
( VP ( VV ) ( NN?? ) ) X | a | speech 1:1 2:3
( VP ( VV?? ) ( NN?? ) ) made | a | speech 1:1 2:3
( IP ( NP ) ( VP ) ) X1 | X2 1:1 2:2
Table 1: Examples of TATs extracted from the TSA in Figure 3 with h = 2 and c = 2
3 Training
To extract tree-to-string alignment templates from
a word-aligned, source side parsed sentence pair
?T (fJ1 ), eI1, A?, we need first identify TSAs (Tree-
String-Alignment) using similar criterion as sug-
gested in (Och and Ney, 2004). A TSA is a triple
?T (f j2j1 ), ei2i1 , A?)? that is in accordance with the
following constraints:
1. ?(i, j) ? A : i1 ? i ? i2 ? j1 ? j ? j2
2. T (f j2j1 ) is a subtree of T (fJ1 )
Given a TSA ?T (f j2j1 ), ei2i1 , A??, a triple
?T (f j4j3 ), ei4i3 , A?? is its sub TSA if and only
if:
1. T (f j4j3 ), ei4i3 , A?? is a TSA
2. T (f j4j3 ) is rooted at the direct descendant of
the root node of T (f j1j2 )
3. i1 ? i3 ? i4 ? i2
4. ?(i, j) ? A? : i3 ? i ? i4 ? j3 ? j ? j4
Basically, we extract TATs from a TSA
?T (f j2j1 ), ei2i1 , A?? using the following two rules:
1. If T (f j2j1 ) contains only one node,
then ?T (f j2j1 ), ei2i1 , A?? is a TAT
2. If the height of T (f j2j1 ) is greater than one,
then build TATs using those extracted from
sub TSAs of ?T (f j2j1 ), ei2i1 , A??.
IP
NP
NR
??
NN
??
VP
VV
??
NN
??
President Bush made a speech
Figure 3: An example of TSA
Usually, we can extract a very large amount of
TATs from training data using the above rules,
making both training and decoding very slow.
Therefore, we impose three restrictions to reduce
the magnitude of extracted TATs:
1. A third constraint is added to the definition of
TSA:
?j?, j?? : j1 ? j? ? j2 and j1 ? j?? ? j2
and (i1, j?) ? A? and (i2, j??) ? A?
This constraint requires that both the first
and last symbols in the target string must be
aligned to some source symbols.
2. The height of T (z) is limited to no greater
than h.
3. The number of direct descendants of a node
of T (z) is limited to no greater than c.
Table 1 shows the TATs extracted from the TSA
in Figure 3 with h = 2 and c = 2.
As we restrict that T (f j2j1 ) must be a subtree of
T (fJ1 ), TATs may be treated as syntactic hierar-
612
chical phrase pairs (Chiang, 2005) with tree struc-
ture on the source side. At the same time, we face
the risk of losing some useful non-syntactic phrase
pairs. For example, the phrase pair
???????? President Bush made
can never be obtained in form of TAT from the
TSA in Figure 3 because there is no subtree for
that source string.
4 Decoding
We approach the decoding problem as a bottom-up
beam search.
To translate a source sentence, we employ a
parser to produce a parse tree. Moving bottom-
up through the source parse tree, we compute a
list of candidate translations for the input subtree
rooted at each node with a postorder transversal.
Candidate translations of subtrees are placed in
stacks. Figure 4 shows the organization of can-
didate translation stacks.
NP
DNP
NP
NR
??
DEG
?
NP
NN
??
NN
??
8
4 7
2 3 5 6
1
...
1
...
2
...
3
...
4
...
5
...
6
...
7
...
8
Figure 4: Candidate translations of subtrees are
placed in stacks according to the root index set by
postorder transversal
A candidate translation contains the following
information:
1. the partial translation
2. the accumulated feature values
3. the accumulated probability
A TAT z is usable to a parse tree T if and only
if T (z) is rooted at the root of T and covers part
of nodes of T . Given a parse tree T , we find all
usable TATs. Given a usable TAT z, if T (z) is
equal to T , then S(z) is a candidate translation of
T . If T (z) covers only a portion of T , we have
to compute a list of candidate translations for T
by replacing the non-terminals of S(z) with can-
didate translations of the corresponding uncovered
subtrees.
NP
DNP
NP DEG
?
NP
8
4 7
2 3
of
...
1
...
2
...
3
...
4
...
5
...
6
...
7
...
8
Figure 5: Candidate translation construction
For example, when computing the candidate
translations for the tree rooted at node 8, the TAT
used in Figure 5 covers only a portion of the parse
tree in Figure 4. There are two uncovered sub-
trees that are rooted at node 2 and node 7 respec-
tively. Hence, we replace the third symbol with
the candidate translations in stack 2 and the first
symbol with the candidate translations in stack 7.
At the same time, the feature values and probabil-
ities are also accumulated for the new candidate
translations.
To speed up the decoder, we limit the search
space by reducing the number of TATs used for
each input node. There are two ways to limit the
TAT table size: by a fixed limit (tatTable-limit) of
how many TATs are retrieved for each input node,
and by a probability threshold (tatTable-threshold)
that specify that the TAT probability has to be
above some value. On the other hand, instead of
keeping the full list of candidates for a given node,
we keep a top-scoring subset of the candidates.
This can also be done by a fixed limit (stack-limit)
or a threshold (stack-threshold). To perform re-
combination, we combine candidate translations
that share the same leading and trailing bigrams
in each stack.
5 Experiments
Our experiments were on Chinese-to-English
translation. The training corpus consists of 31, 149
sentence pairs with 843, 256 Chinese words and
613
System Features BLEU4
d + ?(e|f) 0.0573 ? 0.0033
Pharaoh d + lm + ?(e|f) + wp 0.2019 ? 0.0083
d + lm + ?(f |e) + lex(f |e) + ?(e|f) + lex(e|f) + pp + wp 0.2089 ? 0.0089
h1 0.1639 ? 0.0077
Lynx h1 + h6 + h7 0.2100 ? 0.0089
h1 + h2 + h3 + h4 + h5 + h6 + h7 0.2178 ? 0.0080
Table 2: Comparison of Pharaoh and Lynx with different feature settings on the test corpus
949, 583 English words. For the language model,
we used SRI Language Modeling Toolkit (Stol-
cke, 2002) to train a trigram model with modi-
fied Kneser-Ney smoothing (Chen and Goodman,
1998) on the 31, 149 English sentences. We se-
lected 571 short sentences from the 2002 NIST
MT Evaluation test set as our development cor-
pus, and used the 2005 NIST MT Evaluation test
set as our test corpus. We evaluated the transla-
tion quality using the BLEU metric (Papineni et
al., 2002), as calculated by mteval-v11b.pl with its
default setting except that we used case-sensitive
matching of n-grams.
5.1 Pharaoh
The baseline system we used for comparison was
Pharaoh (Koehn et al, 2003; Koehn, 2004), a
freely available decoder for phrase-based transla-
tion models:
p(e|f) = p?(f |e)?? ? pLM(e)?LM ?
pD(e, f)?D ? ?
length(e)?W(e) (10)
We ran GIZA++ (Och and Ney, 2000) on the
training corpus in both directions using its default
setting, and then applied the refinement rule ?diag-
and? described in (Koehn et al, 2003) to obtain
a single many-to-many word alignment for each
sentence pair. After that, we used some heuristics,
which including rule-based translation of num-
bers, dates, and person names, to further improve
the alignment accuracy.
Given the word-aligned bilingual corpus, we
obtained 1, 231, 959 bilingual phrases (221, 453
used on test corpus) using the training toolkits
publicly released by Philipp Koehn with its default
setting.
To perform minimum error rate training (Och,
2003) to tune the feature weights to maximize the
system?s BLEU score on development set, we used
optimizeV5IBMBLEU.m (Venugopal and Vogel,
2005). We used default pruning settings for
Pharaoh except that we set the distortion limit to
4.
5.2 Lynx
On the same word-aligned training data, it took
us about one month to parse all the 31, 149 Chi-
nese sentences using a Chinese parser written by
Deyi Xiong (Xiong et al, 2005). The parser was
trained on articles 1 ? 270 of Penn Chinese Tree-
bank version 1.0 and achieved 79.4% (F1 mea-
sure) as well as a 4.4% relative decrease in er-
ror rate. Then, we performed TAT extraction de-
scribed in section 3 with h = 3 and c = 5
and obtained 350, 575 TATs (88, 066 used on test
corpus). To run our decoder Lynx on develop-
ment and test corpus, we set tatTable-limit = 20,
tatTable-threshold = 0, stack-limit = 100, and
stack-threshold = 0.00001.
5.3 Results
Table 2 shows the results on test set using Pharaoh
and Lynx with different feature settings. The 95%
confidence intervals were computed using Zhang?s
significance tester (Zhang et al, 2004). We mod-
ified it to conform to NIST?s current definition
of the BLEU brevity penalty. For Pharaoh, eight
features were used: distortion model d, a trigram
language model lm, phrase translation probabili-
ties ?(f |e) and ?(e|f), lexical weightings lex(f |e)
and lex(e|f), phrase penalty pp, and word penalty
wp. For Lynx, seven features described in sec-
tion 2 were used. We find that Lynx outperforms
Pharaoh with all feature settings. With full fea-
tures, Lynx achieves an absolute improvement of
0.006 over Pharaoh (3.1% relative). This differ-
ence is statistically significant (p < 0.01). Note
that Lynx made use of only 88, 066 TATs on test
corpus while 221, 453 bilingual phrases were used
for Pharaoh.
The feature weights obtained by minimum er-
614
FeaturesSystem d lm ?(f |e) lex(f |e) ?(e|f) lex(e|f) pp wp
Pharaoh 0.0476 0.1386 0.0611 0.0459 0.1723 0.0223 0.3122 -0.2000
Lynx - 0.3735 0.0061 0.1081 0.1656 0.0022 0.0824 0.2620
Table 3: Feature weights obtained by minimum error rate training on the development corpus
BLEU4
tat 0.2178 ? 0.0080
tat + bp 0.2240 ? 0.0083
Table 4: Effect of using bilingual phrases for Lynx
ror rate training for both Pharaoh and Lynx are
shown in Table 3. We find that ?(f |e) (i.e. h2) is
not a helpful feature for Lynx. The reason is that
we use only a single non-terminal symbol instead
of assigning phrasal categories to the target string.
In addition, we allow the target string consists of
only non-terminals, making translation decisions
not always based on lexical evidence.
5.4 Using bilingual phrases
It is interesting to use bilingual phrases to
strengthen the TAT-based model. As we men-
tioned before, some useful non-syntactic phrase
pairs can never be obtained in form of TAT be-
cause we restrict that there must be a correspond-
ing parse tree for the source phrase. Moreover,
it takes more time to obtain TATs than bilingual
phrases on the same training data because parsing
is usually very time-consuming.
Given an input subtree T (F j2j1 ), if F
j2
j1 is a string
of terminals, we find all bilingual phrases that the
source phrase is equal to F j2j1 . Then we build a
TAT for each bilingual phrase ?fJ ?1 , eI
?
1 , A??: the
tree of the TAT is T (F j2j1 ), the string is eI
?
1 , and
the alignment is A?. If a TAT built from a bilingual
phrase is the same with a TAT in the TAT table, we
prefer to the greater translation probabilities.
Table 4 shows the effect of using bilingual
phrases for Lynx. Note that these bilingual phrases
are the same with those used for Pharaoh.
5.5 Results on large data
We also conducted an experiment on large data to
further examine our design philosophy. The train-
ing corpus contains 2.6 million sentence pairs. We
used all the data to extract bilingual phrases and
a portion of 800K pairs to obtain TATs. Two tri-
gram language models were used for Lynx. One
was trained on the 2.6 million English sentences
and another was trained on the first 1/3 of the Xin-
hua portion of Gigaword corpus. We also included
rule-based translations of named entities, dates,
and numbers. By making use of these data, Lynx
achieves a BLEU score of 0.2830 on the 2005
NIST Chinese-to-English MT evaluation test set,
which is a very promising result for linguistically
syntax-based models.
6 Conclusion
In this paper, we introduce tree-to-string align-
ment templates, which can be automatically
learned from syntactically-annotated training data.
The TAT-based translation model improves trans-
lation quality significantly compared with a state-
of-the-art phrase-based decoder. Treated as spe-
cial TATs without tree on the source side, bilingual
phrases can be utilized for the TAT-based model to
get further improvement.
It should be emphasized that the restrictions
we impose on TAT extraction limit the expressive
power of TAT. Preliminary experiments reveal that
removing these restrictions does improve transla-
tion quality, but leads to large memory require-
ments. We feel that both parsing and word align-
ment qualities have important effects on the TAT-
based model. We will retrain the Chinese parser
on Penn Chinese Treebank version 5.0 and try to
improve word alignment quality using log-linear
models as suggested in (Liu et al, 2005).
Acknowledgement
This work is supported by National High Tech-
nology Research and Development Program con-
tract ?Generally Technical Research and Ba-
sic Database Establishment of Chinese Plat-
form?(Subject No. 2004AA114010). We are
grateful to Deyi Xiong for providing the parser and
Haitao Mi for making the parser more efficient and
robust. Thanks to Dr. Yajuan Lv for many helpful
comments on an earlier draft of this paper.
615
References
Hiyan Alshawi, Srinivas Bangalore, and Shona Dou-
glas. 2000. Learning dependency translation mod-
els as collections of finite-state head transducers.
Computational Linguistics, 26(1):45-60.
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The
mathematics of statistical machine translation: Pa-
rameter estimation. Computational Linguistics,
19(2):263-311.
Stanley F. Chen and Joshua Goodman. 1998. Am
empirical study of smoothing techniques for lan-
guage modeling. Technical Report TR-10-98, Har-
vard University Center for Research in Computing
Technology.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of 43rd Annual Meeting of the ACL, pages
263-270.
Yuan Ding and Martha Palmer. 2005. Machine trans-
lation using probabilistic synchronous dependency
insert grammars. In Proceedings of 43rd Annual
Meeting of the ACL, pages 541-548.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation rule?
In Proceedings of NAACL-HLT 2004, pages 273-
280.
Jonathan Graehl and Kevin Knight. 2004. Training
tree transducers. In Proceedings of NAACL-HLT
2004, pages 105-112.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings
of HLT-NAACL 2003, pages 127-133.
Philipp Koehn. 2004. Pharaoh: a beam search de-
coder for phrase-based statistical machine trnasla-
tion models. In Proceedings of the Sixth Confer-
ence of the Association for Machine Translation in
the Americas, pages 115-124.
Yang Liu, Qun Liu, and Shouxun Lin. 2005. Log-
linear models for word alignment. In Proceedings
of 43rd Annual Meeting of the ACL, pages 459-466.
Daniel Marcu and William Wong. 2002. A phrase-
based, joint probability model for statistical machine
translation. In Proceedings of the 2002 Conference
on Empirical Methods in Natural Language Pro-
cessing (EMNLP), pages 133-139.
Dan Melamed. 2004. Statistical machine translation
by parsing. In Proceedings of 42nd Annual Meeting
of the ACL, pages 653-660.
Franz J. Och and Hermann Ney. 2000. Improved sta-
tistical alignment models. In Proceedings of 38th
Annual Meeting of the ACL, pages 440-447.
Franz J. Och and Hermann Ney. 2002. Discriminative
training and maximum entropy models for statistical
machine translation. In Proceedings of 40th Annual
Meeting of the ACL, pages 295-302.
Franz J. Och and Hermann Ney. 2004. The alignment
template approach to statistical machine translation.
Computational Linguistics, 30(4):417-449.
Franz J. Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of
41st Annual Meeting of the ACL, pages 160-167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of 40th Annual Meeting of the ACL, pages 311-318.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005.
Dependency treelet translation: Syntactically in-
formed phrasal SMT. In Proceedings of 43rd An-
nual Meeting of the ACL, pages 271-279.
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In Proceedings of Interna-
tional Conference on Spoken Language Processing,
volume 2, pages 901-904.
Ashish Venugopal and Stephan Vogel. 2005. Consid-
erations in maximum mutual information and min-
imum classification error training for statistical ma-
chine translation. In Proceedings of the Tenth Con-
ference of the European Association for Machine
Translation (EAMT-05).
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377-403.
Deyi Xiong, Shuanglong Li, Qun Liu, Shouxun Lin,
and Yueliang Qian. 2005. Parsing the Penn Chinese
treebank with semantic knowledge. In Proceedings
of IJCNLP 2005, pages 70-81.
Kenji Yamada and Kevin Knight. 2001. A syntax-
based statistical translation model. In Proceedings
of 39th Annual Meeting of the ACL, pages 523-530.
Ying Zhang, Stephan Vogel, and Alex Waibel. 2004.
Interpreting BLEU/NIST scores: How much im-
provement do we need to have a better system? In
Proceedings of the Fourth International Conference
on Language Resources and Evaluation (LREC),
pages 2051-2054.
616
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 704?711,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Forest-to-String Statistical Translation Rules
Yang Liu , Yun Huang , Qun Liu and Shouxun Lin
Key Laboratory of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences
P.O. Box 2704, Beijing 100080, China
{yliu,huangyun,liuqun,sxlin}@ict.ac.cn
Abstract
In this paper, we propose forest-to-string
rules to enhance the expressive power of
tree-to-string translation models. A forest-
to-string rule is capable of capturing non-
syntactic phrase pairs by describing the cor-
respondence between multiple parse trees
and one string. To integrate these rules
into tree-to-string translation models, auxil-
iary rules are introduced to provide a gen-
eralization level. Experimental results show
that, on the NIST 2005 Chinese-English test
set, the tree-to-string model augmented with
forest-to-string rules achieves a relative im-
provement of 4.3% in terms of BLEU score
over the original model which allows tree-
to-string rules only.
1 Introduction
The past two years have witnessed the rapid de-
velopment of linguistically syntax-based translation
models (Quirk et al, 2005; Galley et al, 2006;
Marcu et al, 2006; Liu et al, 2006), which induce
tree-to-string translation rules from parallel texts
with linguistic annotations. They demonstrated very
promising results when compared with the state of
the art phrase-based system (Och and Ney, 2004)
in the NIST 2006 machine translation evaluation 1.
While Galley et al (2006) and Marcu et al (2006)
put emphasis on target language analysis, Quirk et
al. (2005) and Liu et al (2006) show benefits from
modeling the syntax of source language.
1See http://www.nist.gov/speech/tests/mt/
One major problem with linguistically syntax-
based models, however, is that tree-to-string rules
fail to syntactify non-syntactic phrase pairs because
they require a syntax tree fragment over the phrase
to be syntactified. Here, we distinguish between syn-
tactic and non-syntactic phrase pairs. By ?syntactic?
we mean that the phrase pair is subsumed by some
syntax tree fragment. The phrase pairs without trees
over them are non-syntactic. Marcu et al (2006)
report that approximately 28% of bilingual phrases
are non-syntactic on their English-Chinese corpus.
We believe that it is important to make available
to syntax-based models all the bilingual phrases that
are typically available to phrase-based models. On
one hand, phrases have been proven to be a simple
and powerful mechanism for machine translation.
They excel at capturing translations of short idioms,
providing local re-ordering decisions, and incorpo-
rating context information straightforwardly. Chi-
ang (2005) shows significant improvement by keep-
ing the strengths of phrases while incorporating syn-
tax into statistical translation. On the other hand,
the performance of linguistically syntax-based mod-
els can be hindered by making use of only syntac-
tic phrase pairs. Studies reveal that linguistically
syntax-based models are sensitive to syntactic anal-
ysis (Quirk and Corston-Oliver, 2006), which is still
not reliable enough to handle real-world texts due to
limited size and domain of training data.
Various solutions are proposed to tackle the prob-
lem. Galley et al (2004) handle non-constituent
phrasal translation by traversing the tree upwards
until reaches a node that subsumes the phrase.
Marcu et al (2006) argue that this choice is inap-
704
propriate because large applicability contexts are re-
quired.
For a non-syntactic phrase pair, Marcu et al
(2006) create a xRS rule headed by a pseudo, non-
syntactic nonterminal symbol that subsumes the
phrase and corresponding multi-headed syntactic
structure; and one sibling xRS rule that explains how
the non-syntactic nonterminal symbol can be com-
bined with other genuine nonterminals so as to ob-
tain genuine parse trees. The name of the pseudo
nonterminal is designed to reflect how the corre-
sponding rule can be fully realized. However, they
neglect alignment consistency when creating sibling
rules. In addition, it is hard for the naming mecha-
nism to deal with more complex phenomena.
Liu et al (2006) treat bilingual phrases as lexi-
calized TATs (Tree-to-string Alignment Template).
A bilingual phrase can be used in decoding if the
source phrase is subsumed by the input parse tree.
Although this solution does help, only syntactic
bilingual phrases are available to the TAT-based
model. Moreover, it is problematic to combine
the translation probabilities of bilingual phrases and
TATs, which are estimated independently.
In this paper, we propose forest-to-string rules
which describe the correspondence between multi-
ple parse trees and a string. They can not only cap-
ture non-syntactic phrase pairs but also have the ca-
pability of generalization. To integrate these rules
into tree-to-string translation models, auxiliary rules
are introduced to provide a generalization level. As
there is no pseudo node or naming mechanism, the
integration of forest-to-string rules is flexible, rely-
ing only on their root nodes. The forest-to-string and
auxiliary rules enable tree-to-string models to derive
in a more general way, while the strengths of con-
ventional tree-to-string rules still remain.
2 Forest-to-String Translation Rules
We define a tree-to-string rule r as a triple ?T? , S?, A??,
which describes the alignment A? between a source
parse tree T? = T (fJ ?
1
) and a target string S? = eI?
1
.
A source string fJ ?
1
, which is the sequence of leaf
nodes of T (fJ ?
1
), consists of both terminals (source
words) and nonterminals (phrasal categories). A tar-
get string eI?
1
is also composed of both terminals
(target words) and nonterminals (placeholders). An
IP
NP
NN
  
VP
SB
 
VP
NP
NN
  
VV
 
PU
 
The gunman was killed by police .
Figure 1: An English sentence aligned with a Chi-
nese parse tree.
alignment A? is defined as a subset of the Cartesian
product of source and target symbol positions:
A? ? {(j, i) : j = 1, . . . , J ?; i = 1, . . . , I ?}
A derivation ? = r
1
? r
2
? . . . ? rn is a left-
most composition of translation rules that explains
how a source parse tree T = T (fJ
1
), a target sen-
tence S = eI
1
, and the word alignment A are syn-
chronously generated. For example, Table 1 demon-
strates a derivation composed of only tree-to-string
rules for the ?T, S,A? tuple in Figure 1 2.
As we mentioned before, tree-to-string rules can
not syntactify phrase pairs that are not subsumed
by any syntax tree fragments. For example, for the
phrase pair ??   ?, ?The gunman was?? in Fig-
ure 1, it is impossible to extract an equivalent tree-
to-string rule that subsumes the same phrase pair
because valid tree-to-string rules can not be multi-
headed.
To address this problem, we propose forest-to-
string rules3 to subsume the non-syntactic phrase
pairs. A forest-to-string rule r 4 is a triple ?F? , S?, A??,
which describes the alignment A? between K source
parse trees F? = T?K
1
and a target string S?. The
source string fJ ?
1
is therefore the sequence of leaf
nodes of F? .
Auxiliary rules are introduced to integrate forest-
to-string rules into tree-to-string translation models.
An auxiliary rule is a special unlexicalized tree-to-
string rule that allows multiple source nonterminals
2We use ?X? to denote a nonterminal in the target string. If
there are more than one nonterminals, they are indexed.
3The term ?forest? refers to an ordered and finite set of trees.
4We still use ?r? to represent a forest-to-string rule to reduce
notational overhead.
705
No. Rule
(1) ( IP ( NP ) ( VP ) ( PU ) ) X
1
X
2
X
3
1:1 2:2 3:3
(2) ( NP ( NN   ) ) The gunman 1:1 1:2
(3) ( VP ( SB  ) ( VP ( NP ( NN ) ) ( VV  ) ) ) was killed by X 1:1 2:4 3:2
(4) ( NN   ) police 1:1
(5) ( PU  ) . 1:1
Table 1: A derivation composed of only tree-to-string rules for Figure 1.
No. Rule
(1) ( IP ( NP ) ( VP ( SB ) ( VP ) ) ( PU ) ) X
1
X
2
1:1 2:1 3:2 4:2
(2) ( NP ( NN   ) ) ( SB  ) The gunman was 1:1 1:2 2:3
(3) ( VP ( NP ) ( VV  ) ) ( PU  ) killed by X . 1:3 2:1 3:4
(4) ( NP ( NN   ) ) police 1:1
Table 2: A derivation composed of tree-to-string, forest-to-string, and auxiliary rules for Figure 1.
to correspond to one target nonterminal, suggesting
that the forest-to-string rules that are rooted at such
source nonterminals can be integrated.
For example, Table 2 shows a derivation com-
posed of tree-to-string, forest-to-string, and auxil-
iary rules for the ?T, S,A? tuple in Figure 1. r
1
is
an auxiliary rule, r
2
and r
3
are forest-to-string rules,
and r
4
is a conventional tree-to-string rule.
Following Marcu et al (2006), we define the
probability of a tuple ?T, S,A? as the sum over all
derivations ?i ? ? that are consistent with the tuple,
c(?) = ?T, S,A?. The probability of each deriva-
tion ?i is given by the product of the probabilities of
all the rules p(rj) in the derivation.
Pr(T, S,A) =
?
?
i
??,c(?)=?T,S,A?
?
r
j
??
i
p(rj) (1)
3 Training
We obtain tree-to-string and forest-to-string rules
from word-aligned, source side parsed bilingual cor-
pus. The extraction algorithm is shown in Figure 2.
Note that T ? denotes either a tree or a forest.
For each span, the ?tree/forest, string, alignment?
triples are identified first. If a triple is consistent with
the alignment, the skeleton of the triple is computed
then. A skeleton s is a rule satisfying the following:
1. s ? R(t), s is induced from t.
2. node(T (s)) ? 2, the tree/forest of s contains
two or more nodes.
3. ?r ? R(t) ? node(T (r)) ? 2, T (s) ? T (r),
the tree/forest of s is the subgraph of that of any
r containing two or more nodes.
1: Input: a source tree T = T (fJ
1
), a target string
S = eI
1
, and word alignment A between them
2: R := ?
3: for u := 0 to J ? 1 do
4: for v := 1 to J ? u do
5: identify the triple set T corresponding to
span (v, v + u)
6: for each triple t = ?T ?, S?, A?? ? T do
7: if ?T ?, S?? is not consistent with A then
8: continue
9: end if
10: if u = 0 ? node(T ?) = 1 then
11: add t to R
12: add ?root(T ?), ?X?, 1:1? to R
13: else
14: compute the skeleton s of the triple t
15: register rules that are built on s using rules
extracted from the sub-triples of t:
R := R? build(s,R)
16: end if
17: end for
18: end for
19: end for
20: Output: rule set R
Figure 2: Rule extraction algorithm.
Given the skeleton and rules extracted from the
sub-triples, the rules for the triple can be acquired.
For example, the algorithm identifies the follow-
ing triple for span (1, 2) in Figure 1:
?( NP ( NN   ) ) ( SB  ),?The gunman was?, 1:1 1:2 2:3?
The skeleton of the triple is:
?( NP ) ( SB ),?X
1
X
2
?, 1:1 2:2?
As the algorithm proceeds bottom-up, five rules
have already been extracted from the sub-triples,
rooted at ?NP? and ?SB? respectively:
?( NP ),?X?, 1:1?
?( NP ( NN ) ),?X?, 1:1?
?( NP ( NN   ) ),?The gunman?, 1:1 1:2?
706
?( SB ),?X?, 1:1?
?( SB  ),?was?, 1:1?
Hence, we can obtain new rules by replacing the
source and target symbols of the skeleton with corre-
sponding rules and also by modifying the alignment
information. For the above triple, the combination
of the five rules produces 2 ? 3 = 6 new rules:
?( NP ) ( SB ),?X
1
X
2
?, 1:1 2:2?
?( NP ) ( SB  ),?X was?, 1:1 2:2?
?( NP ( NN ) ) ( SB ),?X
1
X
2
?, 1:1 2:2?
?( NP ( NN ) ) ( SB  ),?X was?, 1:1 2:2?
?( NP ( NN   ) ) ( SB ),?The gunman X?, 1:1 1:2?
?( NP ( NN   ) ) ( SB  ),?The gunman was?, 1:1 1:2 2:3?
Since we need only to check the alignment con-
sistency, in principle all phrase pairs can be captured
by tree-to-string and forest-to-string rules. To lower
the complexity for both training and decoding, we
impose four restrictions:
1. Both the first and the last symbols in the target
string must be aligned to some source symbols.
2. The height of a tree or forest is no greater than
h.
3. The number of direct descendants of a node is
no greater than c.
4. The number of leaf nodes is no greater than l.
Although possible, it is infeasible to learn aux-
iliary rules from training data. To extract an auxil-
iary rule which integrates at least one forest-to-string
rule, one need traverse the parse tree upwards until
one reaches a node that subsumes the entire forest
without violating the alignment consistency. This
usually results in very complex auxiliary rules, es-
pecially on real-world training data, making both
training and decoding very slow. As a result, we
construct auxiliary rules in decoding instead.
4 Decoding
Given a source parse tree T (fJ
1
), our decoder finds
the target yield of the single best derivation that has
source yield of T (fJ
1
):
S? = argmax
S,A
Pr(T, S,A)
= argmax
S,A
?
?
i
??,c(?)=?T,S,A?
?
r
j
??
i
p(rj)
1: Input: a source parse tree T = T (fJ
1
)
2: for u := 0 to J ? 1 do
3: for v := 1 to J ? u do
4: for each T ? spanning from v to v + u do
5: if T ? is a tree then
6: for each usable tree-to-string rule r do
7: for each derivation ? inferred from r
and derivations in matrix do
8: add ? to matrix[v, v + u, root(T ?)]
9: end for
10: end for
11: search subcell divisions D[v, v + u]
12: for each subcell division d ? D[v, v + u] do
13: if d contains at least one forest cell then
14: construct auxiliary rule r
a
15: for each derivation ? inferred from r
a
and derivations in matrix do
16: add ? to matrix[v, v + u, root(T ?)]
17: end for
18: end if
19: end for
20: else
21: for each usable forest-to-string rule r do
22: for each derivation ? inferred from r
and derivations in matrix do
23: add ? to matrix[v, v + u, ??]
24: end for
25: end for
26: search subcell divisions D[v, v + u]
27: end if
28: end for
29: end for
30: end for
31: find the best derivation ?? in matrix[1, J, root(T )] and
get the best translation ?S = e(??)
32: Output: a target string ?S
Figure 3: Decoding algorithm.
? argmax
S,A,?
?
r
j
??,c(?)=?T,S,A?
p(rj) (2)
Figure 3 demonstrates the decoding algorithm.
It organizes the derivations into an array matrix
whose cells matrix[j
1
, j
2
,X] are sets of derivations.
[j
1
, j
2
,X] represents a tree/forest rooted at X span-
ning from j
1
to j
2
. We use the empty string ?? to
denote the pseudo root of a forest.
Next, we will explain how to infer derivations for
a tree/forest provided a usable rule. If T (r) = T?,
there is only one derivation which contains only the
rule r. This usually happens for leaf nodes. If
T (r) ? T ?, the rule r resorts to derivations from
subcells to infer new derivations. Suppose that the
decoder is to translate the source tree in Figure 1
and finds a usable rule for [1, 5, ?IP?]:
?( IP ( NP ) ( VP ) ( PU ) ),?X
1
X
2
X
3
?, 1:1 2:2 3:3?
707
Subcell Division Auxiliary Rule
[1, 1][2, 2][3, 5] ( IP ( NP ) ( VP ( SB ) ( VP ) ) ( PU ) ) X
1
X
2
X
3
1:1 2:2 3:3 4:3
[1, 2][3, 4][5, 5] ( IP ( NP ) ( VP ( SB ) ( VP ) ) ( PU ) ) X
1
X
2
X
3
1:1 2:1 3:2 4:3
[1, 3][4, 5] ( IP ( NP ) ( VP ( SB ) ( VP ( NP ) ( VV ) ) ) ( PU ) ) X
1
X
2
1:1 2:1 3:1 4:2 5:2
[1, 1][2, 5] ( IP ( NP ) ( VP ) ( PU ) ) X
1
X
2
1:1 2:2 3:2
Table 3: Subcell divisions and corresponding auxiliary rules for the source tree in Figure 1
Since the decoding algorithm proceeds in a
bottom-up fashion, the uncovered portions have al-
ready been translated.
For [1, 1, ?NP?], suppose that we can find a
derivation in matrix:
?( NP ( NN   ) ),?The gunman?, 1:1 1:2?
For [2, 4, ?VP?], we find a derivation in matrix:
?( VP ( SB  ) ( VP ( NP ( NN )) (VV ) ) ),
?was killed by X?, 1:1 2:4 3:2?
?( NN   ),?police?, 1:1?
For [5, 5, ?PU?], we find a derivation in matrix:
?( PU  ),?.?, 1:1?
Henceforth, we get a derivation for [1, 5, ?IP?],
shown in Table 1.
A translation rule r is said to be usable to an input
tree/forest T ? if and only if:
1. T (r) ? T ?, the tree/forest of r is the subgraph
of T ?.
2. root(T (r)) = root(T ?), the root sequence of
T (r) is identical to that of T ?.
For example, the following rules are usable to the
tree ?( NP ( NR   ) ( NN   ) )?:
?( NP ( NR ) ( NN ) ),?X
1
X
2
?, 1:2 2:1?
?( NP ( NR   ) ( NN ) ),?China X?, 1:1 2:2?
?( NP ( NR   ) ( NN  ) ),?China economy?, 1:1 2:2?
Similarly, the forest-to-string rule
?( ( NP ( NR ) ( NN ) ) ( VP ) ),?X
1
X
2
X
3
?, 1:2 2:1 3:3?
is usable to the forest
( NP ( NR ) ( NN   ) ) ( VP (VV )( NN  ) )
As we mentioned before, auxiliary rules are spe-
cial unlexicalized tree-to-string rules that are built in
decoding rather than learnt from real-world data. To
get an auxiliary rule for a cell, we need first identify
its subcell division.
A cell sequence c
1
, c
2
, . . . , cn is referred to as a
subcell division of a cell c if and only if:
1. c
1
.begin = c.begin
1: Input: a cell [j
1
, j
2
], the derivation array matrix,
the subcell division array D
2: if j
1
= j
2
then
3: p? := 0
4: for each derivation ? in matrix[j
1
, j
2
, ?] do
5: p? := max(p(?), p?)
6: end for
7: add {[j
1
, j
2
]} : p? to D[j
1
, j
2
]
8: else
9: if [j
1
, j
2
] is a forest cell then
10: p? := 0
11: for each derivation ? in matrix[j
1
, j
2
, ?] do
12: p? := max(p(?), p?)
13: end for
14: add {[j
1
, j
2
]} : p? to D[j
1
, j
2
]
15: end if
16: for j := j
1
to j
2
? 1 do
17: for each division d
1
? D[j
1
, j] do
18: for each division d
2
? D[j + 1, j
2
] do
19: create a new division: d := d
1
? d
2
20: add d to D[j
1
, j
2
]
21: end for
22: end for
23: end for
24: end if
25: Output: subcell divisions D[j
1
, j
2
]
Figure 4: Subcell division search algorithm.
2. cn.end = c.end
3. cj .end + 1 = cj+1.begin, 1 ? j < n
Given a subcell division, it is easy to construct the
auxiliary rule for a cell. For each subcell, one need
transverse the parse tree upwards until one reaches
nodes that subsume it. All descendants of these
nodes are dropped. The target string consists of only
nonterminals, the number of which is identical to
that of subcells. To limit the search space, we as-
sume that the alignment between the source tree and
the target string is monotone.
Table 3 shows some subcell divisions and corre-
sponding auxiliary rules constructed for the source
tree in Figure 1. For simplicity, we ignore the root
node label.
There are 2n?1 subcell divisions for a cell which
has a length of n. We need only consider the sub-
708
cell divisions which contain at least one forest cell
because tree-to-string rules have already explored
those contain only tree cells.
The actual search algorithm for subcell divisions
is shown in Figure 4. We use matrix[j
1
, j
2
, ?] to de-
note all trees or forests spanning from j
1
to j
2
. The
subcell divisions and their associated probabilities
are stored in an array D. We define an operator ?
between two divisions: their cell sequences are con-
catenated and the probabilities are accumulated.
As sometimes there are no usable rules available,
we introduce default rules to ensure that we can al-
ways get a translation for any input parse tree. A de-
fault rule is a tree-to-string rule 5, built in two ways:
1. If the input tree contains only one node, the
target string of the default rule is equal to the
source string.
2. If the height of the input tree is greater than
one, the tree of the default rule contains only
the root node and its direct descendants of the
input tree, the string contains only nontermi-
nals, and the alignment is monotone.
To speed up the decoder, we limit the search space
by reducing the number of rules used for each cell.
There are two ways to limit the rule table size: by
a fixed limit a of how many rules are retrieved for
each cell, and by a probability threshold ? that spec-
ify that the rule probability has to be above some
value. Also, instead of keeping the full list of deriva-
tions for a cell, we store a top-scoring subset of the
derivations. This can also be done by a fixed limit
b or a threshold ?. The subcell division array D, in
which divisions containing forest cells have priority
over those composed of only tree cells, is pruned by
keeping only a-best divisions.
Following Och and Ney (2002), we base our
model on log-linear framework and adopt the seven
feature functions described in (Liu et al, 2006). It
is very important to balance the preference between
conventional tree-to-string rules and the newly-
introduced forest-to-string and auxiliary rules. As
the probabilities of auxiliary rules are not learnt
from training data, we add a feature that sums up the
5There are no default rules for forests because only tree-to-
string rules are essential to tree-to-string translation models.
node count of auxiliary rules of a derivation to pe-
nalize the use of forest-to-string and auxiliary rules.
5 Experiments
In this section, we report on experiments with
Chinese-to-English translation. The training corpus
consists of 31, 149 sentence pairs with 843, 256 Chi-
nese words and 949, 583 English words. For the
language model, we used SRI Language Modeling
Toolkit (Stolcke, 2002) to train a trigram model with
modified Kneser-Ney smoothing (Chen and Good-
man, 1998) on the 31, 149 English sentences. We
selected 571 short sentences from the 2002 NIST
MT Evaluation test set as our development corpus,
and used the 2005 NIST MT Evaluation test set as
our test corpus. Our evaluation metric is BLEU-4
(Papineni et al, 2002), as calculated by the script
mteval-v11b.pl with its default setting except that
we used case-sensitive matching of n-grams. To
perform minimum error rate training (Och, 2003)
to tune the feature weights to maximize the sys-
tem?s BLEU score on development set, we used the
script optimizeV5IBMBLEU.m (Venugopal and Vo-
gel, 2005).
We ran GIZA++ (Och and Ney, 2000) on the
training corpus in both directions using its default
setting, and then applied the refinement rule ?diag-
and? described in (Koehn et al, 2003) to obtain a
single many-to-many word alignment for each sen-
tence pair. Next, we employed a Chinese parser
written by Deyi Xiong (Xiong et al, 2005) to parse
all the 31, 149 Chinese sentences. The parser was
trained on articles 1-270 of Penn Chinese Treebank
version 1.0 and achieved 79.4% in terms of F1 mea-
sure.
Given the word-aligned, source side parsed bilin-
gual corpus, we obtained bilingual phrases using the
training toolkits publicly released by Philipp Koehn
with its default setting. Then, we applied extrac-
tion algorithm described in Figure 2 to extract both
tree-to-string and forest-to-string rules by restricting
h = 3, c = 5, and l = 7. All the rules, including
bilingual phrases, tree-to-string rules, and forest-to-
string rules, are filtered for the development and test
sets.
According to different levels of lexicalization, we
divide translation rules into three categories:
709
Rule L P U Total
BP 251, 173 0 0 251, 173
TR 56, 983 41, 027 3, 529 101, 539
FR 16, 609 254, 346 25, 051 296, 006
Table 4: Number of rules used in experiments (BP:
bilingual phrase, TR: tree-to-string rule, FR: forest-
to-string rule; L: lexicalized, P: partial lexicalized,
U: unlexicalized).
System Rule Set BLEU4
Pharaoh BP 0.2182 ? 0.0089
BP 0.2059 ? 0.0083
TR 0.2302 ? 0.0089Lynx TR + BP 0.2346 ? 0.0088
TR + FR + AR 0.2402 ? 0.0087
Table 5: Comparison of Pharaoh and Lynx with dif-
ferent rule sets.
1. lexicalized: all symbols in both the source and
target strings are terminals
2. unlexicalized: all symbols in both the source
and target strings are nonterminals
3. partial lexicalized: otherwise
Table 4 shows the statistics of rules used in our ex-
periments. We find that even though forest-to-string
rules are introduced the total number (i.e. 73, 592)
of lexicalized tree-to-string and forest-to-string rules
is still far less than that (i.e. 251, 173) of bilingual
phrases. This difference results from the restriction
we impose in training that both the first and last sym-
bols in the target string must be aligned to some
source symbols. For the forest-to-string rules, par-
tial lexicalized ones are in the majority.
We compared our system Lynx against a freely
available phrase-based decoder Pharaoh (Koehn et
al., 2003). For Pharaoh, we set a = 20, ? = 0,
b = 100, ? = 10?5, and distortion limit dl = 4. For
Lynx, we set a = 20, ? = 0, b = 100, and ? = 0.
Two postprocessing procedures ran to improve the
outputs of both systems: OOVs removal and recapi-
talization.
Table 5 shows results on test set using Pharaoh
and Lynx with different rule sets. Note that Lynx
is capable of using only bilingual phrases plus de-
Forest-to-String Rule Set BLEU4
None 0.2225 ? 0.0085
L 0.2297 ? 0.0081
P 0.2279 ? 0.0083
U 0.2270 ? 0.0087
L + P + U 0.2312 ? 0.0082
Table 6: Effect of lexicalized, partial lexicalized,
and unlexicalized forest-to-string rules.
fault rules to perform monotone search. The 95%
confidence intervals were computed using Zhang?s
significance tester (Zhang et al, 2004). We mod-
ified it to conform to NIST?s current definition of
the BLEU brevity penalty. We find that Lynx out-
performs Pharaoh significantly. The integration of
forest-to-string rules achieves an absolute improve-
ment of 1.0% (4.3% relative) over using tree-to-
string rules only. This difference is statistically sig-
nificant (p < 0.01). It also achieves better result
than treating bilingual phrases as lexicalized tree-to-
string rules. To produce the best result of 0.2402,
Lynx made use of 26, 082 tree-to-string rules, 9, 219
default rules, 5, 432 forest-to-string rules, and 2, 919
auxiliary rules. This suggests that tree-to-string
rules still play a central role, although the integra-
tion of forest-to-string and auxiliary rules is really
beneficial.
Table 6 demonstrates the effect of forest-to-string
rules with different lexicalization levels. We set
a = 3, ? = 0, b = 10, and ? = 0. The second row
?None? shows the result of using only tree-to-string
rules. ?L? denotes using tree-to-string rules and lex-
icalized forest-to-string rules. Similarly, ?L+P+U?
denotes using tree-to-string rules and all forest-to-
string rules. We find that lexicalized forest-to-string
rules are more useful.
6 Conclusion
In this paper, we introduce forest-to-string rules to
capture non-syntactic phrase pairs that are usually
unaccessible to traditional tree-to-string translation
models. With the help of auxiliary rules, forest-to-
string rules can be integrated into tree-to-string mod-
els to offer more general derivations. Experiment re-
sults show that the tree-to-string model augmented
with forest-to-string rules significantly outperforms
710
the original model which allows tree-to-string rules
only.
Our current rule extraction algorithm attaches the
unaligned target words to the nearest ascendants that
subsume them. This constraint hampers the expres-
sive power of our model. We will try a more general
way as suggested in (Galley et al, 2006), making
no a priori assumption about assignment and using
EM training to learn the probability distribution. We
will also conduct experiments on large scale training
data to further examine our design philosophy.
Acknowledgement
This work was supported by National Natural Sci-
ence Foundation of China, Contract No. 60603095
and 60573188.
References
Stanley F. Chen and Joshua Goodman. 1998. An empir-
ical study of smoothing techniques for language mod-
eling. Technical report, Harvard University Center for
Research in Computing Technology.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings
of ACL 2005, pages 263?270, Ann Arbor, Michigan,
June.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In
Proceedings of HLT/NAACL 2004, pages 273?280,
Boston, Massachusetts, USA, May.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proceed-
ings of COLING/ACL 2006, pages 961?968, Sydney,
Australia, July.
Philipp Koehn, Franz Joseph Och, and Daniel Marcu.
2003. Statistical phrase-based translation. InProceed-
ings of HLT/NAACL 2003, pages 127?133, Edmonton,
Canada, May.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine trans-
lation. In Proceedings of COLING/ACL 2006, pages
609?616, Sydney, Australia, July.
Daniel Marcu, Wei Wang, Abdessamad Echihabi, and
Kevin Knight. 2006. Spmt: Statistical machine trans-
lation with syntactified target language phrases. In
Proceedings of EMNLP 2006, pages 44?52, Sydney,
Australia, July.
Franz J. Och and Hermann Ney. 2000. Improved statis-
tical alignment models. In Proceedings of ACL 2000,
pages 440?447.
Franz J. Och and Hermann Ney. 2002. Discriminative
training and maximum entropy models for statistical
machine translation. In Proceedings of ACL 2002,
pages 295?302.
Franz J. Och and Hermann Ney. 2004. The alignment
template approach to statistical machine translation.
Computational Linguistics, 30(4):417?449.
Franz J. Och. 2003. Minimum error rate training in sta-
tistical machine translation. In Proceedings of ACL
2003, pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of ACL
2002, pages 311?318, Philadephia, USA, July.
Chris Quirk and Simon Corston-Oliver. 2006. The im-
pact of parse quality on syntactically-informed statis-
tical machine translation. In Proceedings of EMNLP
2006, pages 62?69, Sydney, Australia, July.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. De-
pendency treelet translation: Syntactically informed
phrasal SMT. In Proceedings of ACL 2005, pages
271?279, Ann Arbor, Michigan, June.
Andreas Stolcke. 2002. Srilm - an extensible lan-
guage modeling toolkit. In Proceedings of Interna-
tional Conference on Spoken Language Processing,
volume 30, pages 901?904.
Ashish Venugopal and Stephan Vogel. 2005. Consid-
erations in maximum mutual information and mini-
mum classification error training for statistical ma-
chine translation. In Proceedings of the Tenth Confer-
ence of the European Association for Machine Trans-
lation, pages 271?279.
Deyi Xiong, Shuanglong Li, Qun Liu, and Shouxun Lin.
2005. Parsing the penn chinese treebank with seman-
tic knowledge. In Proceedings of IJCNLP 2005, pages
70?81.
Ying Zhang, Stephan Vogel, and Alex Waibel. 2004. In-
terpreting bleu/nist scores how much improvement do
we need to have a better system? In Proceedings
of Fourth International Conference on Language Re-
sources and Evaluation, pages 2051?2054.
711
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 558?566,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Improving Tree-to-Tree Translation with Packed Forests
Yang Liu and Yajuan Lu? and Qun Liu
Key Laboratory of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences
P.O. Box 2704, Beijing 100190, China
{yliu,lvyajuan,liuqun}@ict.ac.cn
Abstract
Current tree-to-tree models suffer from
parsing errors as they usually use only 1-
best parses for rule extraction and decod-
ing. We instead propose a forest-based
tree-to-tree model that uses packed forests.
The model is based on a probabilis-
tic synchronous tree substitution gram-
mar (STSG), which can be learned from
aligned forest pairs automatically. The de-
coder finds ways of decomposing trees in
the source forest into elementary trees us-
ing the source projection of STSG while
building target forest in parallel. Compa-
rable to the state-of-the-art phrase-based
system Moses, using packed forests in
tree-to-tree translation results in a signif-
icant absolute improvement of 3.6 BLEU
points over using 1-best trees.
1 Introduction
Approaches to syntax-based statistical machine
translation make use of parallel data with syntactic
annotations, either in the form of phrase structure
trees or dependency trees. They can be roughly
divided into three categories: string-to-tree mod-
els (e.g., (Galley et al, 2006; Marcu et al, 2006;
Shen et al, 2008)), tree-to-string models (e.g.,
(Liu et al, 2006; Huang et al, 2006)), and tree-to-
tree models (e.g., (Eisner, 2003; Ding and Palmer,
2005; Cowan et al, 2006; Zhang et al, 2008)).
By modeling the syntax of both source and tar-
get languages, tree-to-tree approaches have the po-
tential benefit of providing rules linguistically bet-
ter motivated. However, while string-to-tree and
tree-to-string models demonstrate promising re-
sults in empirical evaluations, tree-to-tree models
have still been underachieving.
We believe that tree-to-tree models face two
major challenges. First, tree-to-tree models are
more vulnerable to parsing errors. Obtaining
syntactic annotations in quantity usually entails
running automatic parsers on a parallel corpus.
As the amount and domain of the data used to
train parsers are relatively limited, parsers will
inevitably output ill-formed trees when handling
real-world text. Guided by such noisy syntactic in-
formation, syntax-based models that rely on 1-best
parses are prone to learn noisy translation rules
in training phase and produce degenerate trans-
lations in decoding phase (Quirk and Corston-
Oliver, 2006). This situation aggravates for tree-
to-tree models that use syntax on both sides.
Second, tree-to-tree rules provide poorer rule
coverage. As a tree-to-tree rule requires that there
must be trees on both sides, tree-to-tree mod-
els lose a larger amount of linguistically unmoti-
vated mappings. Studies reveal that the absence of
such non-syntactic mappings will impair transla-
tion quality dramatically (Marcu et al, 2006; Liu
et al, 2007; DeNeefe et al, 2007; Zhang et al,
2008).
Compactly encoding exponentially many
parses, packed forests prove to be an excellent
fit for alleviating the above two problems (Mi et
al., 2008; Mi and Huang, 2008). In this paper,
we propose a forest-based tree-to-tree model. To
learn STSG rules from aligned forest pairs, we in-
troduce a series of notions for identifying minimal
tree-to-tree rules. Our decoder first converts the
source forest to a translation forest and then finds
the best derivation that has the source yield of one
source tree in the forest. Comparable to Moses,
our forest-based tree-to-tree model achieves an
absolute improvement of 3.6 BLEU points over
conventional tree-based model.
558
IP1
NP2 VP3
PP4 VP-B5
NP-B6 NP-B7 NP-B8
NR9 CC10P11 NR12 VV13 AS14 NN15
bushi yu shalong juxing le huitan
Bush held a talk with Sharon
NNP16 VBD17 DT18 NN19 IN20 NNP21
NP22 NP23 NP24
NP25 PP26
NP27
VP28
S 29
Figure 1: An aligned packed forest pair. Each
node is assigned a unique identity for reference.
The solid lines denote hyperedges and the dashed
lines denote word alignments. Shaded nodes are
frontier nodes.
2 Model
Figure 1 shows an aligned forest pair for a Chinese
sentence and an English sentence. The solid lines
denote hyperedges and the dashed lines denote
word alignments between the two forests. Each
node is assigned a unique identity for reference.
Each hyperedge is associated with a probability,
which we omit in Figure 1 for clarity. In a forest,
a node usually has multiple incoming hyperedges.
We use IN(v) to denote the set of incoming hy-
peredges of node v. For example, the source node
?IP1? has following two incoming hyperedges: 1
e1 = ?(NP-B6,VP3), IP1?
e2 = ?(NP2,VP-B5), IP1?
1As there are both source and target forests, it might be
confusing by just using a span to refer to a node. In addition,
some nodes will often have the same labels and spans. There-
fore, it is more convenient to use an identity for referring to a
node. The notation ?IP1? denotes the node that has a label of
?IP? and has an identity of ?1?.
Formally, a packed parse forest is a compact
representation of all the derivations (i.e., parse
trees) for a given sentence under a context-free
grammar. Huang and Chiang (2005) define a for-
est as a tuple ?V,E, v?,R?, where V is a finite set
of nodes, E is a finite set of hyperedges, v? ? V is
a distinguished node that denotes the goal item in
parsing, and R is the set of weights. For a given
sentence w1:l = w1 . . . wl, each node v ? V is in
the form of Xi,j , which denotes the recognition of
non-terminal X spanning the substring from posi-
tions i through j (that is, wi+1 . . . wj). Each hy-
peredge e ? E is a triple e = ?T (e), h(e), f(e)?,
where h(e) ? V is its head, T (e) ? V ? is a vector
of tail nodes, and f(e) is a weight function from
R|T (e)| to R.
Our forest-based tree-to-tree model is based on
a probabilistic STSG (Eisner, 2003). Formally,
an STSG can be defined as a quintuple G =
?Fs,Ft,Ss,St, P ?, where
? Fs andFt are the source and target alhabets,
respectively,
? Ss and St are the source and target start sym-
bols, and
? P is a set of production rules. A rule r is a
triple ?ts, tt,?? that describes the correspon-
dence ? between a source tree ts and a target
tree tt.
To integrate packed forests into tree-to-tree
translation, we model the process of synchronous
generation of a source forest Fs and a target forest
Ft using a probabilistic STSG grammar:
Pr(Fs, Ft) =
?
Ts?Fs
?
Tt?Ft
Pr(Ts, Tt)
=
?
Ts?Fs
?
Tt?Ft
?
d?D
Pr(d)
=
?
Ts?Fs
?
Tt?Ft
?
d?D
?
r?d
p(r) (1)
where Ts is a source tree, Tt is a target tree, D is
the set of all possible derivations that transform Ts
into Tt, d is one such derivation, and r is a tree-to-
tree rule.
Table 1 shows a derivation of the forest pair in
Figure 1. A derivation is a sequence of tree-to-tree
rules. Note that we use x to represent a nontermi-
nal.
559
(1) IP(x1:NP-B, x2:VP)? S(x1:NP, x2:VP)
(2) NP-B(x1:NR)? NP(x1:NNP)
(3) NR(bushi)? NNP(Bush)
(4) VP(x1:PP, VP-B(x2:VV, AS(le), x3:NP-B))? VP(x2:VBD, NP(DT(a), x3:NP), x1:PP)
(5) PP(x1:P, x2:NP-B)? PP(x1:IN, x2:NP)
(6) P(yu)? IN(with)
(7) NP-B(x1:NR)? NP(x1:NP)
(8) NR(shalong) ? NNP(Sharon)
(9) VV(juxing) ? VBD(held)
(10) NP-B(x1:NN)? NP(x1:NN)
(11) NN(huitan) ? NN(talk)
Table 1: A minimal derivation of the forest pair in Figure 1.
id span cspan complement consistent frontier counterparts
1 1-6 1-2, 4-6 1 1 29
2 1-3 1, 5-6 2, 4 0 0
3 2-6 2, 4-6 1 1 1 28
4 2-3 5-6 1-2, 4 1 1 25, 26
5 4-6 2, 4 1, 5-6 1 0
6 1-1 1 2, 4-6 1 1 16, 22
7 3-3 6 1-2, 4-5 1 1 21, 24
8 6-6 4 1-2, 5-6 1 1 19, 23
9 1-1 1 2, 4-6 1 1 16, 22
10 2-2 5 1-2, 4, 6 1 1 20
11 2-2 5 1-2, 4, 6 1 1 20
12 3-3 6 1-2, 4-5 1 1 21, 24
13 4-4 2 1, 4-6 1 1 17
14 5-5 1-2, 4-6 1 0
15 6-6 4 1-2, 5-6 1 1 19, 23
16 1-1 1 2-4, 6 1 1 6, 9
17 2-2 4 1-3, 6 1 1 13
18 3-3 1-4, 6 1 0
19 4-4 6 1-4 1 1 8, 15
20 5-5 2 1, 3-4, 6 1 1 10, 11
21 6-6 3 1-2, 4, 6 1 1 7, 12
22 1-1 1 2-4, 6 1 1 6, 9
23 3-4 6 1-4 1 1 8, 15
24 6-6 3 1-2, 4, 6 1 1 7, 12
25 5-6 2-3 1, 4, 6 1 1 4
26 5-6 2-3 1, 4, 6 1 1 4
27 3-6 2-3, 6 1, 4 0 0
28 2-6 2-4, 6 1 1 1 3
29 1-6 1-4, 6 1 1 1
Table 2: Node attributes of the example forest pair.
3 Rule Extraction
Given an aligned forest pair as shown in Figure
1, how to extract all valid tree-to-tree rules that
explain its synchronous generation process? By
constructing a theory that gives formal seman-
tics to word alignments, Galley et al (2004)
give principled answers to these questions for ex-
tracting tree-to-string rules. Their GHKM proce-
dure draws connections among word alignments,
derivations, and rules. They first identify the
tree nodes that subsume tree-string pairs consis-
tent with word alignments and then extract rules
from these nodes. By this means, GHKM proves
to be able to extract all valid tree-to-string rules
from training instances. Although originally de-
veloped for the tree-to-string case, it is possible to
extend GHKM to extract all valid tree-to-tree rules
from aligned packed forests.
In this section, we introduce our tree-to-tree rule
extraction method adapted from GHKM, which
involves four steps: (1) identifying the correspon-
dence between the nodes in forest pairs, (2) iden-
tifying minimum rules, (3) inferring composed
rules, and (4) estimating rule probabilities.
3.1 Identifying Correspondence Between
Nodes
To learn tree-to-tree rules, we need to find aligned
tree pairs in the forest pairs. To do this, the start-
ing point is to identify the correspondence be-
tween nodes. We propose a number of attributes
for nodes, most of which derive from GHKM, to
facilitate the identification.
Definition 1 Given a node v, its span ?(v) is an
index set of the words it covers.
For example, the span of the source node
?VP-B5? is {4, 5, 6} as it covers three source
words: ?juxing?, ?le?, and ?huitan?. For conve-
nience, we use {4-6} to denotes a contiguous span
{4, 5, 6}.
Definition 2 Given a node v, its corresponding
span ?(v) is the index set of aligned words on an-
other side.
For example, the corresponding span of the
source node ?VP-B5? is {2, 4}, corresponding to
the target words ?held? and ?talk?.
Definition 3 Given a node v, its complement span
?(v) is the union of corresponding spans of nodes
that are neither antecedents nor descendants of v.
For example, the complement span of the source
node ?VP-B5? is {1, 5-6}, corresponding to target
words ?Bush?, ?with?, and ?Sharon?.
Definition 4 A node v is said to be consistent with
alignment if and only if closure(?(v))??(v) = ?.
For example, the closure of the corresponding
span of the source node ?VP-B5? is {2-4} and
its complement span is {1, 5-6}. As the intersec-
tion of the closure and the complement span is an
empty set, the source node ?VP-B5? is consistent
with the alignment.
560
PP4
NP-B7
P11 NR12
PP4
P11 NP-B7
PP4
NP-B7
P11 NR12
PP26
IN20
NP24
NNP21
PP4
P11 NP-B7
PP26
IN 20 NP24
(a) (b) (c) (d)
Figure 2: (a) A frontier tree; (b) a minimal frontier tree; (c) a frontier tree pair; (d) a minimal frontier
tree pair. All trees are taken from the example forest pair in Figure 1. Shaded nodes are frontier nodes.
Each node is assigned an identity for reference.
Definition 5 A node v is said to be a frontier node
if and only if:
1. v is consistent;
2. There exists at least one consistent node v? on
another side satisfying:
? closure(?(v?)) ? ?(v);
? closure(?(v)) ? ?(v?).
v? is said to be a counterpart of v. We use ?(v) to
denote the set of counterparts of v.
A frontier node often has multiple counter-
parts on another side due to the usage of unary
rules in parsers. For example, the source node
?NP-B6? has two counterparts on the target side:
?NNP16? and ?NP22?. Conversely, the target node
?NNP16? also has two counterparts counterparts
on the source side: ?NR9? and ?NP-B6?.
The node attributes of the example forest pair
are listed in Table 2. We use identities to refer to
nodes. ?cspan? denotes corresponding span and
?complement? denotes complement span. In Fig-
ure 1, there are 12 frontier nodes (highlighted by
shading) on the source side and 12 frontier nodes
on the target side. Note that while a consistent
node is equal to a frontier node in GHKM, this is
not the case in our method because we have a tree
on the target side. Frontier nodes play a critical
role in forest-based rule extraction because they
indicate where to cut the forest pairs to obtain tree-
to-tree rules.
3.2 Identifying Minimum Rules
Given the frontier nodes, the next step is to iden-
tify aligned tree pairs, from which tree-to-tree
rules derive. Following Galley et al (2006), we
distinguish between minimal and composed rules.
As a composed rule can be decomposed as a se-
quence of minimal rules, we are particularly inter-
ested in how to extract minimal rules. Also, we in-
troduce a number of notions to help identify mini-
mal rules.
Definition 6 A frontier tree is a subtree in a forest
satisfying:
1. Its root is a frontier node;
2. If the tree contains only one node, it must be
a lexicalized frontier node;
3. If the tree contains more than one nodes,
its leaves are either non-lexicalized frontier
nodes or lexicalized non-frontier nodes.
For example, Figure 2(a) shows a frontier tree
in which all nodes are frontier nodes.
Definition 7 A minimal frontier tree is a frontier
tree such that all nodes other than the root and
leaves are non-frontier nodes.
For example, Figure 2(b) shows a minimal fron-
tier tree.
Definition 8 A frontier tree pair is a triple
?ts, tt,?? satisfying:
1. ts is a source frontier tree;
561
2. tt is a target frontier tree;
3. The root of ts is a counterpart of that of tt;
4. There is a one-to-one correspondence ? be-
tween the frontier leaves of ts and tt.
For example, Figure 2(c) shows a frontier tree
pair.
Definition 9 A frontier tree pair ?ts, tt,?? is said
to be a subgraph of another frontier tree pair
?ts?, tt?,??? if and only if:
1. root(ts) = root(ts?);
2. root(tt) = root(tt?);
3. ts is a subgraph of ts?;
4. tt is a subgraph of tt?.
For example, the frontier tree pair shown in Fig-
ure 2(d) is a subgraph of that in Figure 2(c).
Definition 10 A frontier tree pair is said to be
minimal if and only if it is not a subgraph of any
other frontier tree pair that shares with the same
root.
For example, Figure 2(d) shows a minimal fron-
tier tree pair.
Our goal is to find the minimal frontier tree
pairs, which correspond to minimal tree-to-tree
rules. For example, the tree pair shown in Figure
2(d) denotes a minimal rule as follows:
PP(x1:P,x2:NP-B)? PP(x1:IN, x2:NP)
Figure 3 shows the algorithm for identifying
minimal frontier tree pairs. The input is a source
forest Fs, a target forest Ft, and a source frontier
node v (line 1). We use a set P to store collected
minimal frontier tree pairs (line 2). We first call
the procedure FINDTREES(Fs , v) to identify a set
of frontier trees rooted at v in Fs (line 3). For ex-
ample, for the source frontier node ?PP4? in Figure
1, we obtain two frontier trees:
(PP4(P11)(NP-B7))
(PP4(P11)(NP-B7(NR12)))
Then, we try to find the set of corresponding
target frontier trees (i.e., Tt). For each counter-
part v? of v (line 5), we call the procedure FIND-
TREES(Ft, v?) to identify a set of frontier trees
rooted at v? in Ft (line 6). For example, the source
1: procedure FINDTREEPAIRS(Fs , Ft, v)
2: P = ?
3: Ts ? FINDTREES(Fs , v)
4: Tt ? ?
5: for v? ? ?(v) do
6: Tt ? Tt? FINDTREES(Ft , v?)
7: end for
8: for ?ts, tt? ? Ts ? Tt do
9: if ts ? tt then
10: P ? P ? {?ts, tt,??}
11: end if
12: end for
13: for ?ts, tt,?? ? P do
14: if ??ts?, tt?,??? ? P : ?ts?, tt?,??? ?
?ts, tt,?? then
15: P ? P ? {?ts, tt,??}
16: end if
17: end for
18: end procedure
Figure 3: Algorithm for identifying minimal fron-
tier tree pairs.
frontier node ?PP4? has two counterparts on the
target side: ?NP25? and ?PP26?. There are four
target frontier trees rooted at the two nodes:
(NP25(IN20)(NP24))
(NP25(IN20)(NP24(NNP21)))
(PP26(IN20)(NP24))
(PP26(IN20)(NP24(NNP21)))
Therefore, there are 2 ? 4 = 8 pairs of trees.
We examine each tree pair ?ts, tt? (line 8) to see
whether it is a frontier tree pair (line 9) and then
update P (line 10). In the above example, all the
eight tree pairs are frontier tree pairs.
Finally, we keep only minimal frontier tree pairs
in P (lines 13-15). As a result, we obtain the
following two minimal frontier tree pairs for the
source frontier node ?PP4?:
(PP4(P11)(NP-B7))? (NP25(IN20)(NP24))
(PP4(P11)(NP-B7))? (PP26(IN20)(NP24))
To maintain a reasonable rule table size, we re-
strict that the number of nodes in a tree of an STSG
rule is no greater than n, which we refer to as max-
imal node count.
It seems more efficient to let the procedure
FINDTREES(F, v) to search for minimal frontier
562
trees rather than frontier trees. However, a min-
imal frontier tree pair is not necessarily a pair of
minimal frontier trees. On our Chinese-English
corpus, we find that 38% of minimal frontier tree
pairs are not pairs of minimal frontier trees. As a
result, we have to first collect all frontier tree pairs
and then decide on the minimal ones.
Table 1 shows some minimal rules extracted
from the forest pair shown in Figure 1.
3.3 Inferring Composed Rules
After minimal rules are learned, composed rules
can be obtained by composing two or more min-
imal rules. For example, the composition of the
second rule and the third rule in Table 1 produces
a new rule:
NP-B(NR(shalong))? NP(NNP(Sharon))
While minimal rules derive from minimal fron-
tier tree pairs, composed rules correspond to non-
minimal frontier tree pairs.
3.4 Estimating Rule Probabilities
We follow Mi and Huang (2008) to estimate the
fractional count of a rule extracted from an aligned
forest pair. Intuitively, the relative frequency of a
subtree that occurs in a forest is the sum of all the
trees that traverse the subtree divided by the sum
of all trees in the forest. Instead of enumerating
all trees explicitly and computing the sum of tree
probabilities, we resort to inside and outside prob-
abilities for efficient calculation:
c(r) =
p(ts)? ?(root(ts))?
?
v?leaves(ts) ?(v)
?(v?s)
?
p(tt)? ?(root(tt))?
?
v?leaves(tt) ?(v)
?(v?t)
where c(r) is the fractional count of a rule, ts is the
source tree in r, tt is the target tree in r, root(?) a
function that gets tree root, leaves(?) is a function
that gets tree leaves, and ?(v) and ?(v) are outside
and inside probabilities, respectively.
4 Decoding
Given a source packed forest Fs, our decoder finds
the target yield of the single best derivation d that
has source yield of Ts(d) ? Fs:
e? = e
(
argmax
d s.t. Ts(d)?Fs
p(d)
)
(2)
We extend the model in Eq. 1 to a log-linear
model (Och and Ney, 2002) that uses the follow-
ing eight features: relative frequencies in two di-
rections, lexical weights in two directions, num-
ber of rules used, language model score, number
of target words produced, and the probability of
matched source tree (Mi et al, 2008).
Given a source parse forest and an STSG gram-
mar G, we first apply the conversion algorithm
proposed by Mi et al (2008) to produce a trans-
lation forest. The translation forest has a simi-
lar hypergraph structure. While the nodes are the
same as those of the parse forest, each hyperedge
is associated with an STSG rule. Then, the de-
coder runs on the translation forest. We use the
cube pruning method (Chiang, 2007) to approxi-
mately intersect the translation forest with the lan-
guage model. Traversing the translation forest in
a bottom-up order, the decoder tries to build tar-
get parses at each node. After the first pass, we
use lazy Algorithm 3 (Huang and Chiang, 2005)
to generate k-best translations for minimum error
rate training.
5 Experiments
5.1 Data Preparation
We evaluated our model on Chinese-to-English
translation. The training corpus contains 840K
Chinese words and 950K English words. A tri-
gram language model was trained on the English
sentences of the training corpus. We used the 2002
NIST MT Evaluation test set as our development
set, and used the 2005 NIST MT Evaluation test
set as our test set. We evaluated the translation
quality using the BLEU metric, as calculated by
mteval-v11b.pl with its default setting except that
we used case-insensitive matching of n-grams.
To obtain packed forests, we used the Chinese
parser (Xiong et al, 2005) modified by Haitao
Mi and the English parser (Charniak and Johnson,
2005) modified by Liang Huang to produce en-
tire parse forests. Then, we ran the Python scripts
(Huang, 2008) provided by Liang Huang to out-
put packed forests. To prune the packed forests,
Huang (2008) uses inside and outside probabili-
ties to compute the distance of the best derivation
that traverses a hyperedge away from the glob-
ally best derivation. A hyperedge will be pruned
away if the difference is greater than a threshold
p. Nodes with all incoming hyperedges pruned
are also pruned. The greater the threshold p is,
563
p avg trees # of rules BLEU
0 1 73, 614 0.2021 ? 0.0089
2 238.94 105, 214 0.2165 ? 0.0081
5 5.78 ? 106 347, 526 0.2336 ? 0.0078
8 6.59 ? 107 573, 738 0.2373 ? 0.0082
10 1.05 ? 108 743, 211 0.2385 ? 0.0084
Table 3: Comparison of BLEU scores for tree-
based and forest-based tree-to-tree models.
0.04
0.05
0.06
0.07
0.08
0.09
0.10
 0  1  2  3  4  5  6  7  8  9  10  11
co
ve
ra
ge
maximal node count
p=0
p=2
p=5
p=8
p=10
Figure 4: Coverage of lexicalized STSG rules on
bilingual phrases.
the more parses are encoded in a packed forest.
We obtained word alignments of the training
data by first running GIZA++ (Och and Ney, 2003)
and then applying the refinement rule ?grow-diag-
final-and? (Koehn et al, 2003).
5.2 Forests Vs. 1-best Trees
Table 3 shows the BLEU scores of tree-based and
forest-based tree-to-tree models achieved on the
test set over different pruning thresholds. p is the
threshold for pruning packed forests, ?avg trees?
is the average number of trees encoded in one for-
est on the test set, and ?# of rules? is the number
of STSG rules used on the test set. We restrict that
both source and target trees in a tree-to-tree rule
can contain at most 10 nodes (i.e., the maximal
node count n = 10). The 95% confidence inter-
vals were computed using Zhang ?s significance
tester (Zhang et al, 2004).
We chose five different pruning thresholds in
our experiments: p = 0, 2, 5, 8, 10. The forests
pruned by p = 0 contained only 1-best tree per
sentence. With the increase of p, the average num-
ber of trees encoded in one forest rose dramati-
cally. When p was set to 10, there were over 100M
parses encoded in one forest on average.
p extraction decoding
0 1.26 6.76
2 2.35 8.52
5 6.34 14.87
8 8.51 19.78
10 10.21 25.81
Table 4: Comparison of rule extraction time (sec-
onds/1000 sentence pairs) and decoding time (sec-
ond/sentence)
Moreover, the more trees are encoded in packed
forests, the more rules are made available to
forest-based models. The number of rules when
p = 10 was almost 10 times of p = 0. With the
increase of the number of rules used, the BLEU
score increased accordingly. This suggests that
packed forests enable tree-to-tree model to learn
more useful rules on the training data. However,
when a pack forest encodes over 1M parses per
sentence, the improvements are less significant,
which echoes the results in (Mi et al, 2008).
The forest-based tree-to-tree model outper-
forms the original model that uses 1-best trees
dramatically. The absolute improvement of 3.6
BLEU points (from 0.2021 to 0.2385) is statis-
tically significant at p < 0.01 using the sign-
test as described by Collins et al (2005), with
700(+1), 360(-1), and 15(0). We also ran Moses
(Koehn et al, 2007) with its default setting us-
ing the same data and obtained a BLEU score of
0.2366, slightly lower than our best result (i.e.,
0.2385). But this difference is not statistically sig-
nificant.
5.3 Effect on Rule Coverage
Figure 4 demonstrates the effect of pruning thresh-
old and maximal node count on rule coverage.
We extracted phrase pairs from the training data
to investigate how many phrase pairs can be cap-
tured by lexicalized tree-to-tree rules that con-
tain only terminals. We set the maximal length
of phrase pairs to 10. For tree-based tree-to-tree
model, the coverage was below 8% even the max-
imal node count was set to 10. This suggests that
conventional tree-to-tree models lose over 92%
linguistically unmotivated mappings due to hard
syntactic constraints. The absence of such non-
syntactic mappings prevents tree-based tree-to-
tree models from achieving comparable results to
phrase-based models. With more parses included
564
0.09
0.10
0.11
0.12
0.13
0.14
0.15
0.16
0.17
0.18
0.19
0.20
 0  1  2  3  4  5  6  7  8  9  10  11
BL
EU
maximal node count
Figure 5: Effect of maximal node count on BLEU
scores.
in packed forests, the rule coverage increased ac-
cordingly. When p = 10 and n = 10, the cov-
erage was 9.7%, higher than that of p = 0. As
a result, packed forests enable tree-to-tree models
to capture more useful source-target mappings and
therefore improve translation quality. 2
5.4 Training and Decoding Time
Table 4 gives the rule extraction time (sec-
onds/1000 sentence pairs) and decoding time (sec-
ond/sentence) with varying pruning thresholds.
We found that the extraction time grew faster than
decoding time with the increase of p. One possi-
ble reason is that the number of frontier tree pairs
(see Figure 3) rose dramatically when more parses
were included in packed forests.
5.5 Effect of Maximal Node Count
Figure 5 shows the effect of maximal node count
on BLEU scores. With the increase of maximal
node count, the BLEU score increased dramati-
cally. This implies that allowing tree-to-tree rules
to capture larger contexts will strengthen the ex-
pressive power of tree-to-tree model.
5.6 Results on Larger Data
We also conducted an experiment on larger data
to further examine the effectiveness of our ap-
proach. We concatenated the small corpus we
used above and the FBIS corpus. After remov-
ing the sentences that we failed to obtain forests,
2Note that even we used packed forests, the rule coverage
was still very low. One reason is that we set the maximal
phrase length to 10 words, while an STSG rule with 10 nodes
in each tree usually cannot subsume 10 words.
the new training corpus contained about 260K sen-
tence pairs with 7.39M Chinese words and 9.41M
English words. We set the forest pruning threshold
p = 5. Moses obtained a BLEU score of 0.3043
and our forest-based tree-to-tree system achieved
a BLEU score of 0.3059. The difference is still not
significant statistically.
6 Related Work
In machine translation, the concept of packed for-
est is first used by Huang and Chiang (2007) to
characterize the search space of decoding with lan-
guage models. The first direct use of packed for-
est is proposed by Mi et al (2008). They replace
1-best trees with packed forests both in training
and decoding and show superior translation qual-
ity over the state-of-the-art hierarchical phrase-
based system. We follow the same direction and
apply packed forests to tree-to-tree translation.
Zhang et al (2008) present a tree-to-tree model
that uses STSG. To capture non-syntactic phrases,
they apply tree-sequence rules (Liu et al, 2007)
to tree-to-tree models. Their extraction algorithm
first identifies initial rules and then obtains abstract
rules. While this method works for 1-best tree
pairs, it cannot be applied to packed forest pairs
because it is impractical to enumerate all tree pairs
over a phrase pair.
While Galley (2004) describes extracting tree-
to-string rules from 1-best trees, Mi and Huang et
al. (2008) go further by proposing a method for
extracting tree-to-string rules from aligned forest-
string pairs. We follow their work and focus on
identifying tree-tree pairs in a forest pair, which is
more difficult than the tree-to-string case.
7 Conclusion
We have shown how to improve tree-to-tree trans-
lation with packed forests, which compactly en-
code exponentially many parses. To learn STSG
rules from aligned forest pairs, we first identify
minimal rules and then get composed rules. The
decoder finds the best derivation that have the
source yield of one source tree in the forest. Ex-
periments show that using packed forests in tree-
to-tree translation results in dramatic improve-
ments over using 1-best trees. Our system also
achieves comparable performance with the state-
of-the-art phrase-based system Moses.
565
Acknowledgement
The authors were supported by National Natural
Science Foundation of China, Contracts 60603095
and 60736014, and 863 State Key Project No.
2006AA010108. Part of this work was done
while Yang Liu was visiting the SMT group led
by Stephan Vogel at CMU. We thank the anony-
mous reviewers for their insightful comments.
Many thanks go to Liang Huang, Haitao Mi, and
Hao Xiong for their invaluable help in producing
packed forests. We are also grateful to Andreas
Zollmann, Vamshi Ambati, and Kevin Gimpel for
their helpful feedback.
References
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative
reranking. In Proc. of ACL 2005.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2).
Brooke Cowan, Ivona Kuc?erova?, and Michael Collins.
2006. A discriminative model for tree-to-tree trans-
lation. In Proc. of EMNLP 2006.
Steve DeNeefe, Kevin Knight, Wei Wang, and Daniel
Marcu. 2007. What can syntax-based MT learn
from phrase-based MT? In Proc. of EMNLP 2007.
Yuan Ding and Martha Palmer. 2005. Machine trans-
lation using probabilistic synchronous dependency
insertion grammars. In Proc. of ACL 2005.
Jason Eisner. 2003. Learning non-isomorphic tree
mappings for machine translation. In Proc. of ACL
2003 (Companion Volume).
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation rule?
In Proc. of NAACL/HLT 2004.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proc.
of COLING/ACL 2006.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proc. of IWPT 2005.
Liang Huang and David Chiang. 2007. Forest rescor-
ing: Faster decoding with integrated language mod-
els. In Proc. of ACL 2007.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proc. of AMTA 2006.
Liang Huang. 2008. Forest reranking: Discrimina-
tive parsing with non-local features. In Proc. of
ACL/HLT 2008.
Phillip Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proc. of
NAACL 2003.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proc. of ACL 2007 (demonstration session).
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment template for statistical machine
translation. In Proc. of COLING/ACL 2006.
Yang Liu, Yun Huang, Qun Liu, and Shouxun Lin.
2007. Forest-to-string statistical translation rules. In
Proc. of ACL 2007.
Daniel Marcu, Wei Wang, Abdessamad Echihabi, and
Kevin Knight. 2006. Spmt: Statistical machine
translation with syntactified target language phrases.
In Proc. of EMNLP 2006.
Haitao Mi and Liang Huang. 2008. Forest-based trans-
lation rule extraction. In Proc. of EMNLP 2008.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proc. of ACL/HLT 2008.
Franz J. Och and Hermann Ney. 2002. Discriminative
training and maximum entropy models for statistical
machine translation. In Proc. of ACL 2002.
Franz J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29(1).
Chris Quirk and Simon Corston-Oliver. 2006. The
impact of parsing quality on syntactically-informed
statistical machine translation. In Proc. of EMNLP
2006.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proc. of ACL/HLT 2008.
Deyi Xiong, Shuanglong Li, Qun Liu, and Shouxun
Lin. 2005. Parsing the penn chinese treebank with
semantic knowledge. In Proc. of IJCNLP 2005.
Ying Zhang, Stephan Vogel, and Alex Waibel. 2004.
Interpreting bleu/nist scores how much improve-
ment do we need to have a better system? In Proc.
of LREC 2004.
Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li,
Chew Lim Tan, and Sheng Li. 2008. A tree
sequence alignment-based tree-to-tree translation
model. In Proc. of ACL/HLT 2008.
566
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 576?584,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Joint Decoding with Multiple Translation Models
Yang Liu and Haitao Mi and Yang Feng and Qun Liu
Key Laboratory of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences
P.O. Box 2704, Beijing 100190, China
{yliu,htmi,fengyang,liuqun}@ict.ac.cn
Abstract
Current SMT systems usually decode with
single translation models and cannot ben-
efit from the strengths of other models in
decoding phase. We instead propose joint
decoding, a method that combines multi-
ple translation models in one decoder. Our
joint decoder draws connections among
multiple models by integrating the trans-
lation hypergraphs they produce individu-
ally. Therefore, one model can share trans-
lations and even derivations with other
models. Comparable to the state-of-the-art
system combination technique, joint de-
coding achieves an absolute improvement
of 1.5 BLEU points over individual decod-
ing.
1 Introduction
System combination aims to find consensus trans-
lations among different machine translation sys-
tems. It proves that such consensus translations
are usually better than the output of individual sys-
tems (Frederking and Nirenburg, 1994).
Recent several years have witnessed the rapid
development of system combination methods
based on confusion networks (e.g., (Rosti et al,
2007; He et al, 2008)), which show state-of-the-
art performance in MT benchmarks. A confusion
network consists of a sequence of sets of candidate
words. Each candidate word is associated with a
score. The optimal consensus translation can be
obtained by selecting one word from each set of
candidates to maximizing the overall score. While
it is easy and efficient to manipulate strings, cur-
rent methods usually have no access to most infor-
mation available in decoding phase, which might
be useful for obtaining further improvements.
In this paper, we propose a framework for com-
bining multiple translation models directly in de-
coding phase. 1 Based on max-translation decod-
ing and max-derivation decoding used in conven-
tional individual decoders (Section 2), we go fur-
ther to develop a joint decoder that integrates mul-
tiple models on a firm basis:
? Structuring the search space of each model
as a translation hypergraph (Section 3.1),
our joint decoder packs individual translation
hypergraphs together by merging nodes that
have identical partial translations (Section
3.2). Although such translation-level combi-
nation will not produce new translations, it
does change the way of selecting promising
candidates.
? Two models could even share derivations
with each other if they produce the same
structures on the target side (Section 3.3),
which we refer to as derivation-level com-
bination. This method enlarges the search
space by allowing for mixing different types
of translation rules within one derivation.
? As multiple derivations are used for finding
optimal translations, we extend the minimum
error rate training (MERT) algorithm (Och,
2003) to tune feature weights with respect
to BLEU score for max-translation decoding
(Section 4).
We evaluated our joint decoder that integrated
a hierarchical phrase-based model (Chiang, 2005;
Chiang, 2007) and a tree-to-string model (Liu et
al., 2006) on the NIST 2005 Chinese-English test-
set. Experimental results show that joint decod-
1It might be controversial to use the term ?model?, which
usually has a very precise definition in the field. Some
researchers prefer to saying ?phrase-based approaches? or
?phrase-based systems?. On the other hand, other authors
(e.g., (Och and Ney, 2004; Koehn et al, 2003; Chiang, 2007))
do use the expression ?phrase-based models?. In this paper,
we use the term ?model? to emphasize that we integrate dif-
ferent approaches directly in decoding phase rather than post-
processing system outputs.
576
S ? ?X1,X1?
X ? ?fabiao X1, give a X1?
X ? ?yanjiang, talk?
Figure 1: A derivation composed of SCFG rules
that translates a Chinese sentence ?fabiao yan-
jiang? into an English sentence ?give a talk?.
ing with multiple models achieves an absolute im-
provement of 1.5 BLEU points over individual de-
coding with single models (Section 5).
2 Background
Statistical machine translation is a decision prob-
lem where we need decide on the best of target
sentence matching a source sentence. The process
of searching for the best translation is convention-
ally called decoding, which usually involves se-
quences of decisions that translate a source sen-
tence into a target sentence step by step.
For example, Figure 1 shows a sequence of
SCFG rules (Chiang, 2005; Chiang, 2007) that
translates a Chinese sentence ?fabiao yanjiang?
into an English sentence ?give a talk?. Such se-
quence of decisions is called a derivation. In
phrase-based models, a decision can be translating
a source phrase into a target phrase or reordering
the target phrases. In syntax-based models, deci-
sions usually correspond to transduction rules. Of-
ten, there are many derivations that are distinct yet
produce the same translation.
Blunsom et al (2008) present a latent vari-
able model that describes the relationship between
translation and derivation clearly. Given a source
sentence f , the probability of a target sentence e
being its translation is the sum over all possible
derivations:
Pr(e|f) =
?
d??(e,f)
Pr(d, e|f) (1)
where ?(e, f) is the set of all possible derivations
that translate f into e and d is one such derivation.
They use a log-linear model to define the con-
ditional probability of a derivation d and corre-
sponding translation e conditioned on a source
sentence f :
Pr(d, e|f) = exp
?
m ?mhm(d, e, f)
Z(f) (2)
where hm is a feature function, ?m is the asso-
ciated feature weight, and Z(f) is a constant for
normalization:
Z(f) =
?
e
?
d??(e,f)
exp
?
m
?mhm(d, e, f) (3)
A feature value is usually decomposed as the
product of decision probabilities: 2
h(d, e, f) =
?
d?d
p(d) (4)
where d is a decision in the derivation d.
Although originally proposed for supporting
large sets of non-independent and overlapping fea-
tures, the latent variable model is actually a more
general form of conventional linear model (Och
and Ney, 2002).
Accordingly, decoding for the latent variable
model can be formalized as
e? = argmax
e
{
?
d??(e,f)
exp
?
m
?mhm(d, e, f)
}
(5)
where Z(f) is not needed in decoding because it
is independent of e.
Most SMT systems approximate the summa-
tion over all possible derivations by using 1-best
derivation for efficiency. They search for the 1-
best derivation and take its target yield as the best
translation:
e? ? argmax
e,d
{
?
m
?mhm(d, e, f)
}
(6)
We refer to Eq. (5) as max-translation decoding
and Eq. (6) as max-derivation decoding, which are
first termed by Blunsom et al (2008).
By now, most current SMT systems, adopting
either max-derivation decoding or max-translation
decoding, have only used single models in decod-
ing phase. We refer to them as individual de-
coders. In the following section, we will present
a new method called joint decoding that includes
multiple models in one decoder.
3 Joint Decoding
There are two major challenges for combining
multiple models directly in decoding phase. First,
they rely on different kinds of knowledge sources
2There are also features independent of derivations, such
as language model and word penalty.
577
Sgive
0-1
talk
1-2
give a talk
0-2
give talks
0-2
S
give
0-1
speech
1-2
give a talk
0-2
make a speech
0-2
S
give
0-1
talk
1-2
speech
1-2
give a talk
0-2
give talks
0-2
make a speech
0-2
packing(a) (b)
(c)
Figure 2: (a) A translation hypergraph produced by one model; (b) a translation hypergraph produced by
another model; (c) the packed translation hypergraph based on (a) and (b). Solid and dashed lines denote
the translation rules of the two models, respectively. Shaded nodes occur in both (a) and (b), indicating
that the two models produce the same translations.
and thus need to collect different information dur-
ing decoding. For example, taking a source parse
as input, a tree-to-string decoder (e.g., (Liu et al,
2006)) pattern-matches the source parse with tree-
to-string rules and produces a string on the tar-
get side. On the contrary, a string-to-tree decoder
(e.g., (Galley et al, 2006; Shen et al, 2008)) is a
parser that applies string-to-tree rules to obtain a
target parse for the source string. As a result, the
hypothesis structures of the two models are funda-
mentally different.
Second, translation models differ in decoding
algorithms. Depending on the generating order
of a target sentence, we distinguish between two
major categories: left-to-right and bottom-up. De-
coders that use rules with flat structures (e.g.,
phrase pairs) usually generate target sentences
from left to right while those using rules with hier-
archical structures (e.g., SCFG rules) often run in
a bottom-up style.
In response to the two challenges, we first ar-
gue that the search space of an arbitrary model can
be structured as a translation hypergraph, which
makes each model connectable to others (Section
3.1). Then, we show that a packed translation hy-
pergraph that integrates the hypergraphs of indi-
vidual models can be generated in a bottom-up
topological order, either integrated at the transla-
tion level (Section 3.2) or the derivation level (Sec-
tion 3.3).
3.1 Translation Hypergraph
Despite the diversity of translation models, they all
have to produce partial translations for substrings
of input sentences. Therefore, we represent the
search space of a translation model as a structure
called translation hypergraph.
Figure 2(a) demonstrates a translation hyper-
graph for one model, for example, a hierarchical
phrase-based model. A node in a hypergraph de-
notes a partial translation for a source substring,
except for the starting node ?S?. For example,
given the example source sentence
0 fabiao 1 yanjiang 2
the node ??give talks?, [0, 2]? in Figure 2(a) de-
notes that ?give talks? is one translation of the
source string f21 = ?fabiao yanjiang?.
The hyperedges between nodes denote the deci-
sion steps that produce head nodes from tail nodes.
For example, the incoming hyperedge of the node
??give talks?, [0, 2]? could correspond to an SCFG
rule:
X ? ?X1 yanjiang,X1 talks?
Each hyperedge is associated with a number of
weights, which are the feature values of the corre-
sponding translation rules. A path of hyperedges
constitutes a derivation.
578
Hypergraph Decoding
node translation
hyperedge rule
path derivation
Table 1: Correspondence between translation hy-
pergraph and decoding.
More formally, a hypergraph (Klein and Man-
ning., 2001; Huang and Chiang, 2005) is a tuple
?V,E,R?, where V is a set of nodes, E is a set
of hyperedges, and R is a set of weights. For a
given source sentence f = fn1 = f1 . . . fn, each
node v ? V is in the form of ?t, [i, j]?, which de-
notes the recognition of t as one translation of the
source substring spanning from i through j (that
is, fi+1 . . . fj). Each hyperedge e ? E is a tuple
e = ?tails(e), head(e), w(e)?, where head(e) ?
V is the consequent node in the deductive step,
tails(e) ? V ? is the list of antecedent nodes, and
w(e) is a weight function from R|tails(e)| to R.
As a general representation, a translation hyper-
graph is capable of characterizing the search space
of an arbitrary translation model. Furthermore,
it offers a graphic interpretation of decoding pro-
cess. A node in a hypergraph denotes a translation,
a hyperedge denotes a decision step, and a path
of hyperedges denotes a derivation. A translation
hypergraph is formally a semiring as the weight
of a path is the product of hyperedge weights and
the weight of a node is the sum of path weights.
While max-derivation decoding only retains the
single best path at each node, max-translation de-
coding sums up all incoming paths. Table 1 sum-
marizes the relationship between translation hy-
pergraph and decoding.
3.2 Translation-Level Combination
The conventional interpretation of Eq. (1) is that
the probability of a translation is the sum over all
possible derivations coming from the same model.
Alternatively, we interpret Eq. (1) as that the
derivations could come from different models.3
This forms the theoretical basis of joint decoding.
Although the information inside a derivation
differs widely among translation models, the be-
ginning and end points (i.e., f and e, respectively)
must be identical. For example, a tree-to-string
3The same for all d occurrences in Section 2. For exam-
ple, ?(e, f) might include derivations from various models
now. Note that we still use Z for normalization.
model first parses f to obtain a source tree T (f)
and then transforms T (f) to the target sentence
e. Conversely, a string-to-tree model first parses
f into a target tree T (e) and then takes the surface
string e as the translation. Despite different inside,
their derivations must begin with f and end with e.
This situation remains the same for derivations
between a source substring f ji and its partial trans-
lation t during joint decoding:
Pr(t|f ji ) =
?
d??(t,fji )
Pr(d, t|f ji ) (7)
where d might come from multiple models. In
other words, derivations from multiple models
could be brought together for computing the prob-
ability of one partial translation.
Graphically speaking, joint decoding creates a
packed translation hypergraph that combines in-
dividual hypergraphs by merging nodes that have
identical translations. For example, Figure 2 (a)
and (b) demonstrate two translation hypergraphs
generated by two models respectively and Fig-
ure 2 (c) is the resulting packed hypergraph. The
solid lines denote the hyperedges of the first model
and the dashed lines denote those of the second
model. The shaded nodes are shared by both mod-
els. Therefore, the two models are combined at the
translation level. Intuitively, shared nodes should
be favored in decoding because they offer consen-
sus translations among different models.
Now the question is how to decode with multi-
ple models jointly in just one decoder. We believe
that both left-to-right and bottom-up strategies can
be used for joint decoding. Although phrase-based
decoders usually produce translations from left to
right, they can adopt bottom-up decoding in prin-
ciple. Xiong et al (2006) develop a bottom-up de-
coder for BTG (Wu, 1997) that uses only phrase
pairs. They treat reordering of phrases as a binary
classification problem. On the other hand, it is
possible for syntax-based models to decode from
left to right. Watanabe et al (2006) propose left-
to-right target generation for hierarchical phrase-
based translation. Although left-to-right decod-
ing might enable a more efficient use of language
models and hopefully produce better translations,
we adopt bottom-up decoding in this paper just for
convenience.
Figure 3 demonstrates the search algorithm of
our joint decoder. The input is a source language
sentence fn1 , and a set of translation models M
579
1: procedure JOINTDECODING(fn1 , M )
2: G? ?
3: for l ? 1 . . . n do
4: for all i, j s.t. j ? i = l do
5: for all m ?M do
6: ADD(G, i, j,m)
7: end for
8: PRUNE(G, i, j)
9: end for
10: end for
11: end procedure
Figure 3: Search algorithm for joint decoding.
(line 1). After initializing the translation hyper-
graph G (line 2), the decoder runs in a bottom-
up style, adding nodes for each span [i, j] and for
each model m. For each span [i, j] (lines 3-5),
the procedure ADD(G, i, j,m) add nodes gener-
ated by the model m to the hypergraph G (line 6).
Each model searches for partial translations inde-
pendently: it uses its own knowledge sources and
visits its own antecedent nodes, just running like
a bottom-up individual decoder. After all mod-
els finishes adding nodes for span [i, j], the pro-
cedure PRUNE(G, i, j) merges identical nodes and
removes less promising nodes to control the search
space (line 8). The pruning strategy is similar to
that of individual decoders, except that we require
there must exist at least one node for each model
to ensure further inference.
Although translation-level combination will not
offer new translations as compared to single mod-
els, it changes the way of selecting promising can-
didates in a combined search space and might po-
tentially produce better translations than individ-
ual decoding.
3.3 Derivation-Level Combination
In translation-level combination, different models
interact with each other only at the nodes. The
derivations of one model are unaccessible to other
models. However, if two models produce the same
structures on the target side, it is possible to com-
bine two models within one derivation, which we
refer to as derivation-level combination.
For example, although different on the source
side, both hierarchical phrase-based and tree-to-
string models produce strings of terminals and
nonterminals on the target side. Figure 4 shows
a derivation composed of both hierarchical phrase
IP(x1:VV, x2:NN) ? x1 x2
X ? ?fabiao, give?
X ? ?yanjiang, a talk?
Figure 4: A derivation composed of both SCFG
and tree-to-string rules.
pairs and tree-to-string rules. Hierarchical phrase
pairs are used for translating smaller units and
tree-to-string rules for bigger ones. It is appealing
to combine them in such a way because the hierar-
chical phrase-based model provides excellent rule
coverage while the tree-to-string model offers lin-
guistically motivated non-local reordering. Sim-
ilarly, Blunsom and Osborne (2008) use both hi-
erarchical phrase pairs and tree-to-string rules in
decoding, where source parse trees serve as condi-
tioning context rather than hard constraints.
Depending on the target side output, we dis-
tinguish between string-targeted and tree-targeted
models. String-targeted models include phrase-
based, hierarchical phrase-based, and tree-to-
string models. Tree-targeted models include
string-to-tree and tree-to-tree models. All models
can be combined at the translation level. Models
that share with same target output structure can be
further combined at the derivation level.
The joint decoder usually runs as max-
translation decoding because multiple derivations
from various models are used. However, if all
models involved belong to the same category, a
joint decoder can also adopt the max-derivation
fashion because all nodes and hyperedges are ac-
cessible now (Section 5.2).
Allowing derivations for comprising rules from
different models and integrating their strengths,
derivation-level combination could hopefully pro-
duce new and better translations as compared with
single models.
4 Extended Minimum Error Rate
Training
Minimum error rate training (Och, 2003) is widely
used to optimize feature weights for a linear model
(Och and Ney, 2002). The key idea of MERT is
to tune one feature weight to minimize error rate
each time while keep others fixed. Therefore, each
580
xf(x)
t1
t2
t3
(0, 0) x1 x2
Figure 5: Calculation of critical intersections.
candidate translation can be represented as a line:
f(x) = a? x + b (8)
where a is the feature value of current dimension,
x is the feature weight being tuned, and b is the
dotproduct of other dimensions. The intersection
of two lines is where the candidate translation will
change. Instead of computing all intersections,
Och (2003) only computes critical intersections
where highest-score translations will change. This
method reduces the computational overhead sig-
nificantly.
Unfortunately, minimum error rate training can-
not be directly used to optimize feature weights of
max-translation decoding because Eq. (5) is not a
linear model. However, if we also tune one dimen-
sion each time and keep other dimensions fixed,
we obtain a monotonic curve as follows:
f(x) =
K
?
k=1
eak?x+bk (9)
where K is the number of derivations for a can-
didate translation, ak is the feature value of cur-
rent dimension on the kth derivation and bk is the
dotproduct of other dimensions on the kth deriva-
tion. If we restrict that ak is always non-negative,
the curve shown in Eq. (9) will be a monotoni-
cally increasing function. Therefore, it is possible
to extend the MERT algorithm to handle situations
where multiple derivations are taken into account
for decoding.
The key difference is the calculation of criti-
cal intersections. The major challenge is that two
curves might have multiple intersections while
two lines have at most one intersection. Fortu-
nately, as the curve is monotonically increasing,
we need only to find the leftmost intersection of
a curve with other curves that have greater values
after the intersection as a candidate critical inter-
section.
Figure 5 demonstrates three curves: t1, t2, and
t3. Suppose that the left bound of x is 0, we com-
pute the function values for t1, t2, and t3 at x = 0
and find that t3 has the greatest value. As a result,
we choose x = 0 as the first critical intersection.
Then, we compute the leftmost intersections of t3
with t1 and t2 and choose the intersection closest
to x = 0, that is x1, as our new critical intersec-
tion. Similarly, we start from x1 and find x2 as the
next critical intersection. This iteration continues
until it reaches the right bound. The bold curve de-
notes the translations we will choose over different
ranges. For example, we will always choose t2 for
the range [x1, x2].
To compute the leftmost intersection of two
curves, we divide the range from current critical
intersection to the right bound into many bins (i.e.,
smaller ranges) and search the bins one by one
from left to right. We assume that there is at most
one intersection in each bin. As a result, we can
use the Bisection method for finding the intersec-
tion in each bin. The search process ends immedi-
ately once an intersection is found.
We divide max-translation decoding into three
phases: (1) build the translation hypergraphs, (2)
generate n-best translations, and (3) generate n?-
best derivations. We apply Algorithm 3 of Huang
and Chiang (2005) for n-best list generation. Ex-
tended MERT runs on n-best translations plus n?-
best derivations to optimize the feature weights.
Note that feature weights of various models are
tuned jointly in extended MERT.
5 Experiments
5.1 Data Preparation
Our experiments were on Chinese-to-English
translation. We used the FBIS corpus (6.9M +
8.9M words) as the training corpus. For lan-
guage model, we used the SRI Language Mod-
eling Toolkit (Stolcke, 2002) to train a 4-gram
model on the Xinhua portion of GIGAWORD cor-
pus. We used the NIST 2002 MT Evaluation test
set as our development set, and used the NIST
2005 test set as test set. We evaluated the trans-
lation quality using case-insensitive BLEU metric
(Papineni et al, 2002).
Our joint decoder included two models. The
581
Max-derivation Max-translationModel Combination Time BLEU Time BLEU
hierarchical N/A 40.53 30.11 44.87 29.82
tree-to-string N/A 6.13 27.23 6.69 27.11
translation N/A N/A 55.89 30.79both derivation 48.45 31.63 54.91 31.49
Table 2: Comparison of individual decoding and joint decoding on average decoding time (sec-
onds/sentence) and BLEU score (case-insensitive).
first model was the hierarchical phrase-based
model (Chiang, 2005; Chiang, 2007). We obtained
word alignments of training data by first running
GIZA++ (Och and Ney, 2003) and then applying
the refinement rule ?grow-diag-final-and? (Koehn
et al, 2003). About 2.6M hierarchical phrase pairs
extracted from the training corpus were used on
the test set.
Another model was the tree-to-string model
(Liu et al, 2006; Liu et al, 2007). Based on
the same word-aligned training corpus, we ran a
Chinese parser on the source side to obtain 1-best
parses. For 15,157 sentences we failed to obtain
1-best parses. Therefore, only 93.7% of the train-
ing corpus were used by the tree-to-string model.
About 578K tree-to-string rules extracted from the
training corpus were used on the test set.
5.2 Individual Decoding Vs. Joint Decoding
Table 2 shows the results of comparing individ-
ual decoding and joint decoding on the test set.
With conventional max-derivation decoding, the
hierarchical phrase-based model achieved a BLEU
score of 30.11 on the test set, with an average de-
coding time of 40.53 seconds/sentence. We found
that accounting for all possible derivations in max-
translation decoding resulted in a small negative
effect on BLEU score (from 30.11 to 29.82), even
though the feature weights were tuned with respect
to BLEU score. One possible reason is that we
only used n-best derivations instead of all possi-
ble derivations for minimum error rate training.
Max-derivation decoding with the tree-to-string
model yielded much lower BLEU score (i.e.,
27.23) than the hierarchical phrase-based model.
One reason is that the tree-to-string model fails
to capture a large amount of linguistically unmo-
tivated mappings due to syntactic constraints. An-
other reason is that the tree-to-string model only
used part of the training data because of pars-
ing failure. Similarly, accounting for all possible
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
 0  1  2  3  4  5  6  7  8  9  10  11
pe
rc
en
ta
ge
span width
Figure 6: Node sharing in max-translation de-
coding with varying span widths. We retain at
most 100 nodes for each source substring for each
model.
derivations in max-translation decoding failed to
bring benefits for the tree-to-string model (from
27.23 to 27.11).
When combining the two models at the trans-
lation level, the joint decoder achieved a BLEU
score of 30.79 that outperformed the best result
(i.e., 30.11) of individual decoding significantly
(p < 0.05). This suggests that accounting for
all possible derivations from multiple models will
help discriminate among candidate translations.
Figure 6 demonstrates the percentages of nodes
shared by the two models over various span widths
in packed translation hypergraphs during max-
translation decoding. For one-word source strings,
89.33% nodes in the hypergrpah were shared by
both models. With the increase of span width, the
percentage decreased dramatically due to the di-
versity of the two models. However, there still ex-
ist nodes shared by two models even for source
substrings that contain 33 words.
When combining the two models at the deriva-
tion level using max-derivation decoding, the joint
decoder achieved a BLEU score of 31.63 that out-
performed the best result (i.e., 30.11) of individ-
582
Method Model BLEU
hierarchical 30.11individual decoding
tree-to-string 27.23
system combination both 31.50
joint decoding both 31.63
Table 3: Comparison of individual decoding, sys-
tem combination, and joint decoding.
ual decoding significantly (p < 0.01). This im-
provement resulted from the mixture of hierarchi-
cal phrase pairs and tree-to-string rules. To pro-
duce the result, the joint decoder made use of
8,114 hierarchical phrase pairs learned from train-
ing data, 6,800 glue rules connecting partial trans-
lations monotonically, and 16,554 tree-to-string
rules. While tree-to-string rules offer linguistically
motivated non-local reordering during decoding,
hierarchical phrase pairs ensure good rule cover-
age. Max-translation decoding still failed to sur-
pass max-derivation decoding in this case.
5.3 Comparison with System Combination
We re-implemented a state-of-the-art system com-
bination method (Rosti et al, 2007). As shown
in Table 3, taking the translations of the two indi-
vidual decoders as input, the system combination
method achieved a BLEU score of 31.50, slightly
lower than that of joint decoding. But this differ-
ence is not significant statistically.
5.4 Individual Training Vs. Joint Training
Table 4 shows the effects of individual training and
joint training. By individual, we mean that the two
models are trained independently. We concatenate
and normalize their feature weights for the joint
decoder. By joint, we mean that they are trained
together by the extended MERT algorithm. We
found that joint training outperformed individual
training significantly for both max-derivation de-
coding and max-translation decoding.
6 Related Work
System combination has benefited various NLP
tasks in recent years, such as products-of-experts
(e.g., (Smith and Eisner, 2005)) and ensemble-
based parsing (e.g., (Henderson and Brill, 1999)).
In machine translation, confusion-network based
combination techniques (e.g., (Rosti et al, 2007;
He et al, 2008)) have achieved the state-of-the-
art performance in MT evaluations. From a dif-
Training Max-derivation Max-translation
individual 30.70 29.95
joint 31.63 30.79
Table 4: Comparison of individual training and
joint training.
ferent perspective, we try to combine different ap-
proaches directly in decoding phase by using hy-
pergraphs. While system combination techniques
manipulate only the final translations of each sys-
tem, our method opens the possibility of exploit-
ing much more information.
Blunsom et al (2008) first distinguish between
max-derivation decoding and max-translation de-
coding explicitly. They show that max-translation
decoding outperforms max-derivation decoding
for the latent variable model. While they train the
parameters using a maximum a posteriori estima-
tor, we extend the MERT algorithm (Och, 2003)
to take the evaluation metric into account.
Hypergraphs have been successfully used in
parsing (Klein and Manning., 2001; Huang and
Chiang, 2005; Huang, 2008) and machine trans-
lation (Huang and Chiang, 2007; Mi et al, 2008;
Mi and Huang, 2008). Both Mi et al (2008) and
Blunsom et al (2008) use a translation hyper-
graph to represent search space. The difference is
that their hypergraphs are specifically designed for
the forest-based tree-to-string model and the hier-
archical phrase-based model, respectively, while
ours is more general and can be applied to arbi-
trary models.
7 Conclusion
We have presented a framework for including mul-
tiple translation models in one decoder. Repre-
senting search space as a translation hypergraph,
individual models are accessible to others via shar-
ing nodes and even hyperedges. As our decoder
accounts for multiple derivations, we extend the
MERT algorithm to tune feature weights with re-
spect to BLEU score for max-translation decod-
ing. In the future, we plan to optimize feature
weights for max-translation decoding directly on
the entire packed translation hypergraph rather
than on n-best derivations, following the lattice-
based MERT (Macherey et al, 2008).
583
Acknowledgement
The authors were supported by National Natural
Science Foundation of China, Contracts 60873167
and 60736014, and 863 State Key Project No.
2006AA010108. Part of this work was done while
Yang Liu was visiting the SMT group led by
Stephan Vogel at CMU. We thank the anonymous
reviewers for their insightful comments. We are
also grateful to Yajuan Lu?, Liang Huang, Nguyen
Bach, Andreas Zollmann, Vamshi Ambati, and
Kevin Gimpel for their helpful feedback.
References
Phil Blunsom and Mile Osborne. 2008. Probabilis-
tic inference for machine translation. In Proc. of
EMNLP08.
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
A discriminative latent variable model for statistical
machine translation. In Proc. of ACL08.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Proc.
of ACL05.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2).
Robert Frederking and Sergei Nirenburg. 1994. Three
heads are better than one. In Proc. of ANLP94.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proc.
of ACL06.
Xiaodong He, Mei Yang, Jianfeng Gao, Patrick
Nguyen, and Robert Moore. 2008. Indirect-HMM-
based hypothesis alignment for combining outputs
from machine translation systems. In Proc. of
EMNLP08.
John C. Henderson and Eric Brill. 1999. Exploiting
diversity in natural language processing: Combining
parsers. In Proc. of EMNLP99.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proc. of IWPT05.
Liang Huang and David Chiang. 2007. Forest rescor-
ing: Faster decoding with integrated language mod-
els. In Proc. of ACL07.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proc. of ACL08.
Dan Klein and Christopher D. Manning. 2001. Parsing
and hypergraphs. In Proc. of ACL08.
Phillip Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proc. of
NAACL03.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment template for statistical machine
translation. In Proc. of ACL06.
Yang Liu, Yun Huang, Qun Liu, and Shouxun Lin.
2007. Forest-to-string statistical translation rules. In
Proc. of ACL07.
Wolfgang Macherey, Franz J. Och, Ignacio Thayer, and
Jakob Uszkoreit. 2008. Lattice-based minimum er-
ror rate training for statistical machine translation.
In Proc. of EMNLP08.
Haitao Mi and Liang Huang. 2008. Forest-based trans-
lation rule extraction. In Proc. of EMNLP08.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proc. of ACL08.
Franz J. Och and Hermann Ney. 2002. Discriminative
training and maximum entropy models for statistical
machine translation. In Proc. of ACL02.
Franz J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29(1).
Franz J. Och and Hermann Ney. 2004. The alignment
template approach to statistical machine translation.
Computational Linguistics, 30(4).
Franz J. Och. 2003. Minimum error rate training in
statistical machine translation. In Proc. of ACL03.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proc. of ACL02.
Antti-Veikko Rosti, Spyros Matsoukas, and Richard
Schwartz. 2007. Improved word-level system com-
bination for machine translation. In Proc. of ACL07.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proc. of ACL08.
Noah A. Smith and Jason Eisner. 2005. Contrastive
estimation: Training log-linear models on unlabeled
data. In Proc. of ACL05.
Andreas Stolcke. 2002. Srilm - an extension language
model modeling toolkit. In Proc. of ICSLP02.
Taro Watanabe, Hajime Tsukada, and Hideki Isozaki.
2006. Left-to-right target generation for hierarchical
phrase-based translation. In Proc. of ACL06.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23.
Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Maxi-
mum entropy based phrase reordering model for sta-
tistical machine translation. In Proc. of ACL06.
584
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 137?140,
Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLP
Sub-Sentence Division for Tree-Based Machine Translation 
 
Hao Xiong*, Wenwen Xu+, Haitao Mi*, Yang Liu* and Qun Liu* 
*Key Lab. of Intelligent Information Processing 
+Key Lab. of Computer System and Architecture 
Institute of Computing Technology 
Chinese Academy of Sciences 
P.O. Box 2704, Beijing 100190, China 
{xionghao,xuwenwen,htmi,yliu,liuqun}@ict.ac.cn
 
Abstract 
Tree-based statistical machine translation 
models have made significant progress in re-
cent years, especially when replacing 1-best 
trees with packed forests. However, as the 
parsing accuracy usually goes down dramati-
cally with the increase of sentence length, 
translating long sentences often takes long 
time and only produces degenerate transla-
tions. We propose a new method named sub-
sentence division that reduces the decoding 
time and improves the translation quality for 
tree-based translation. Our approach divides 
long sentences into several sub-sentences by 
exploiting tree structures. Large-scale ex-
periments on the NIST 2008 Chinese-to-
English test set show that our approach 
achieves an absolute improvement of 1.1 
BLEU points over the baseline system in 
50% less time. 
1 Introduction 
Tree-based statistical machine translation 
models in days have witness promising progress 
in recent years, such as tree-to-string models (Liu 
et al, 2006; Huang et al, 2006), tree-to-tree 
models (Quirk et al,2005;Zhang et al, 2008). 
Especially, when incorporated with forest, the 
correspondent forest-based tree-to-string models 
(Mi et al, 2008; Zhang et al, 2009), tree-to-tree 
models (Liu et al, 2009) have achieved a prom-
ising improvements over correspondent tree-
based systems. However, when we translate long 
sentences, we argue that two major issues will be 
raised. On one hand, parsing accuracy will be 
lower as the length of sentence grows. It will in-
evitably hurt the translation quality (Quirk and 
Corston-Oliver, 2006; Mi and Huang, 2008). On 
the other hand, decoding on long sentences will 
be time consuming, especially for forest ap-
proaches. So splitting long sentences into sub- 
 
Figure 1. Main framework of our method 
 
sentences becomes a natural way in MT litera-
ture.  
A simple way is to split long sentences by 
punctuations. However, without concerning 
about the original whole tree structures, this ap-
proach will result in ill-formed sub-trees which 
don?t respect to original structures. In this paper, 
we present a new approach, which pays more 
attention to parse trees on the long sentences. We 
firstly parse the long sentences into trees, and 
then divide them accordingly into sub-sentences, 
which will be translated independently (Section 
3). Finally, we combine sub translations into a 
full translation (Section 4). Large-scale experi-
ments (Section 5) show that the BLEU score 
achieved by our approach is 1.1 higher than di-
rect decoding and 0.3 higher than always split-
ting on commas on the 2008 NIST MT Chinese-
English test set. Moreover, our approach has re-
duced decoding time significantly. 
2 Framework  
Our approach works in following steps. 
(1) Split a long sentence into sub-sentences.  
(2) Translate all the sub-sentences respectively. 
(3) Combine the sub-translations.   
Figure 1 illustrates the main idea of our ap-
proach. The crucial issues of our method are how 
to divide long sentences and how to combine the 
sub-translations.  
3 Sub Sentence Division  
Long sentences could be very complicated in 
grammar and sentence structure, thereby creating 
an obstacle for translation. Consequently, we 
need to break them into shorter and easier 
clauses. To divide sentences by punctuation is 
137
 
 
Figure 2. An undividable parse tree 
 
 
Figure 3. A dividable parse tree 
 
one of the most commonly used methods. How-
ever, simply applying this method might damage 
the accuracy of parsing. As a result, the strategy 
we proposed is to operate division while con-
cerning the structure of parse tree. 
As sentence division should not influence the 
accuracy of parsing, we have to be very cautious 
about sentences whose division might decrease 
the accuracy of parsing. Figure 2(a) shows an 
example of the parse tree of an undividable sen-
tence. 
As can be seen in Figure 2, when we divide 
the sentence by comma, it would break the struc-
ture of ?VP? sub-tree and result in a ill-formed 
sub-tree ?VP? (right sub-tree), which don?t have 
a subject and don?t respect to original tree struc-
tures. 
Consequently, the key issue of sentence divi-
sion is finding the sentences that can be divided 
without loosing parsing accuracy. Figure 2(b) 
shows the parse tree of a sentence that can be 
divided by punctuation, as sub-sentences divided 
by comma are independent. The reference trans-
lation of the sentence in figure 3 is 
 
Less than two hours earlier, a Palestinian took 
on a shooting spree on passengers in the town of 
Kfar Saba in northern Israel. 
Pseudocode 1 Check Sub Sentence Divi-
sion Algorithm 
1: procedure CheckSubSentence(sent) 
2: for each word i in sent 
3:    if(i is a comma) 
4:       left={words in left side of i}; 
          //words between last comma and cur-
rent comma i 
5:       right={words in right side of i}; 
         //words between i and next comma or
 semicolon, period, question mark 
6:       isDividePunct[i]=true; 
7:       for each j in left 
8:          if(( LCA(j, i)!=parent[i]) 
9:             isDividePunct[i]=false; 
10:           break; 
11:     for each j in right 
12:        if(( LCA(j, i)!=parent[i]) 
13:           isDividePunct[i]=false; 
14:           break; 
15: function LCA(i, j) 
16:    return lowest common ancestor(i, j);
 
It demonstrates that this long sentence can be 
divided into two sub-sentences, providing a good 
support to our division. 
In addition to dividable sentences and non-
dividable sentences, there are sentences contain-
ing more than one comma, some of which are 
dividable and some are not. However, this does 
not prove to be a problem, as we process each 
comma independently. In other words, we only 
split the dividable part of this kind of sentences, 
leaving the non-dividable part unchanged.  
To find the sentences that can be divided, we 
present a new method and provide its pseudo 
code. Firstly, we divide a sentence by its commas. 
For each word in the sub-sentence on the left 
side of a comma, we compute its lowest common 
ancestor (LCA) with the comma. And we process 
the words in the sub-sentence on the right side of 
the comma in the same way. Finally, we check if 
all the LCA we have computed are comma?s par-
ent node.  If all the LCA are the comma?s parent 
node, the sub-sentences are independent.  
As shown in figure 3, the LCA (AD ?? , 
PU ?),  is ?IP? ,which is the parent node of 
?PU ??; and the LCA (NR ??? , PU ?) is 
also ?IP?.  Till we have checked all the LCA of 
each word and comma, we finally find that all 
the LCA are ?IP?. As a result, this sentence can 
be divided without loosing parsing accuracy. 
LCA can be computed by using union-set (Tar-
jan, 1971) in lineal time. Concerning the  
138
sub-sentence 1: ???? 
Translation 1: Johndroe said                   A1
Translation 2: Johndroe pointed out       A2
Translation 3: Qiang Zhuo said              A3
comma 1: , 
Translation: punctuation translation (white 
space, that ? ) 
sub-sentence 2: ???????????
??????????????? 
Translation 1: the two presidents also wel-
comed the US-South Korea free trade 
agreement that was signed yesterday       B1
Translation 2: the two presidents also ex-
pressed welcome to the US ? South Korea 
free trade agreement signed yesterday     B2
comma 2: , 
Translation: punctuation translation (white 
space, that ? ) 
sub-sentence 3:???????????
?????? 
Translation 1: and would work to ensure 
that the congresses of both countries ap-
prove this agreement.                               C1
Translation 2: and will make efforts to en-
sure the Congress to approve this agreement 
of the two countries.                                C2
 
Table 1. Sub translation example 
 
implementation complexity, we have reduced the 
problem to range minimum query problem 
(Bender et al, 2005) with a time complexity of  
(1)?  for querying.  
Above all, our approach for sub sentence 
works as follows: 
(1)Split a sentence by semi-colon if there is 
one. 
(2)Parse a sentence if it contains a comma, 
generating k-best parses (Huang Chiang, 2005) 
with k=10.  
 (3)Use the algorithm in pseudocode 1 to 
check the sentence and divide it if there are 
more than 5 parse trees indicates that the sen-
tence is dividable.  
4 Sub Translation Combining  
For sub translation combining, we mainly use the 
best-first expansion idea from cube pruning 
(Huang and Chiang, 2007) to combine sub- 
translations and generate the whole k-best trans-
lations. We first select the best translation from 
sub translation sets, and then use an interpolation 
 
Test Set 02 05 08 
No Sent Division 34.56 31.26 24.53 
Split by Comma 34.59 31.23 25.39 
Our Approach 34.86 31.23 25.69 
 
Table 2. BLEU results (case sensitive) 
 
Test Set 02 05 08 
No Sent Division 28 h 36 h 52 h 
Split by Comma 18h 23h 29h 
Our Approach 18 h 22 h 26 h 
 
Table 3. Decoding time of our experiments 
(h means hours) 
 
language model for rescoring (Huang and Chiang, 
2007).  
For example, we split the following sentence ??
???,??????????????????
????????,?????????????
????? into three sub-sentences and generate 
some translations, and the results are displayed in 
Table 1.  
As seen in Table 1, for each sub-sentence, 
there are one or more versions of translation. For 
convenience, we label the three translation ver-
sions of sub-sentence 1 as A1, A2, and A3, re-
spectively. Similarly, B1, B2, C1, C2 are also 
labels of translation. We push the A1, white 
space, B1, white space, C1 into the cube, and 
then generate the final translation. 
According to cube pruning algorithm, we will 
generate other translations until we get the best 
list we need. Finally, we rescore the k-best list 
using interpolation language model and find the 
best translation which is A1 that B1 white space 
C1. 
5 Experiments  
5.1 Data preparation 
We conduct our experiments on Chinese-English 
translation, and use the Chinese parser of Xiong 
et al (2005) to parse the source sentences. And 
our decoder is based on forest-based tree-to-
string translation model (Mi et al 2008). 
Our training corpus consists of 2.56 million 
sentence pairs. Forest-based rule extractor (Mi 
and Huang 2008) is used with a pruning thresh-
old p=3. And we use SRI Language Modeling 
Toolkit (Stolcke, 2002) to train two 5-gram lan-
guage models with Kneser-Ney smoothing on the 
English side of the training corpus and the Xin-
hua portion of Gigaword corpora respectively. 
139
We use 2006 NIST MT Evaluation test set as 
development set, and 2002, 2005 and 2008 NIST 
MT Evaluation test sets as test sets. We also use 
minimum error-rate training (Och, 2003) to tune 
our feature weights. We evaluate our results with 
case-sensitive BLEU-4 metric (Papineni et al, 
2002). The pruning threshold p for parse forest in 
decoding time is 12. 
5.2 Results 
The final BLEU results are shown in Table 2, our 
approach has achieved a BLEU score that is 1.1 
higher than direct decoding and 0.3 higher than 
always splitting on commas. 
The decoding time results are presented in Ta-
ble 3. The search space of our experiment is ex-
tremely large due to the large pruning threshold 
(p=12), thus resulting in a long decoding time. 
However, our approach has reduced the decoding 
time by 50% over direct decoding, and 10% over 
always splitting on commas. 
6 Conclusion & Future Work  
We have presented a new sub-sentence division 
method and achieved some good results. In the 
future, we will extend our work from decoding to 
training time, where we divide the bilingual sen-
tences accordingly.  
Acknowledgement 
The authors were supported by National Natural 
Science Foundation of China, Contracts 0873167 
and 60736014, and 863 State Key Project 
No.2006AA010108. We thank Liang Huang for 
his insightful suggestions.  
References  
Bender, Farach-Colton, Pemmasani, Skiena, Sumazin, 
Lowest common ancestors in trees and di- 
rected acyclic graphs. J. Algorithms 57(2), 75?
94 (2005) 
Liang Huang and David Chiang. 2005. Better kbest 
Parsing. In Proceedings of IWPT-2005. 
Liang Huang and David Chiang. 2007. Forest res-
coring: Fast decoding with integrated lan-
guage models. In Proceedings of ACL. 
Liang Huang, Kevin Knight, and Aravind Joshi. 2006. 
Statistical syntax-directed translation with ex-
tended domain of locality. In Proceedings of 
AMTA 
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003. 
Statistical phrase-based translation. In Pro-
ceedings of HLT-NAACL 2003, pages 127-133. 
Yang Liu, Qun Liu and Shouxun Lin. 2006. Tree-to-
String alignments template for statistical ma-
chine translation. In Proceedings of ACL. 
Yang Liu, Yajuan Lv and Qun Liu.2009. Improving 
Tree-to-Tree Translation with Packed Forests.To 
appear in Proceedings of ACL/IJCNLP.. 
Daniel Marcu, Wei Wang, AbdessamadEchihabi, and 
Kevin Knight. 2006. Statistical Machine Trans-
lation with syntactifiedtarget language 
phrases. In Proceedings of EMNLP. 
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proceedings of ACL: HLT. 
Haitao Mi and Liang Huang. 2008. Forest-based 
translation rule extraction. In Proceedings of 
EMNLP. 
Franz J. Och. 2003. Minimum error rate training 
in statistical machine translation. In Proceed-
ings of ACL, pages 160?167. 
Kishore Papineni, Salim Roukos, Todd Ward, and 
Wei-Jing Zhu. 2002. Bleu: a method for auto-
matic evaluation of machine translation. In 
Proceedings of ACL, pages 311?318,. 
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. 
Dependency treelet translation: Syntactically 
informed phrasal SMT. In Proceedings of ACL. 
Chris Quirk and Simon Corston-Oliver. 2006. The 
impact of parse quality on syntactically-
informed statistical machine translation. In 
Proceedings of EMNLP. 
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In Proceedings of 
ICSLP, volume 30, pages 901?904. 
Georgianna Tarjan, Depth First Search and Linear 
Graph Algorithms. SIAM J. Comp. 1:2, pp. 146?
160, 1972. 
Deyi Xiong, Shuanglong Li, Qun Liu, and Shouxun 
Lin.2005. Parsing the Penn Chinese Treebank 
with semantic knowledge. In Proceedings of 
IJCNLP. 
Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li, 
Chew Lim Tan, and Sheng Li. 2008. A tree se-
quence alignment-based tree-to-tree transla-
tion model. In Proceedings of ACL. 
Hui Zhang, Min Zhang, Haizhou Li, Aiti Aw and 
Chew Lim Tan. 2009. Forest-based Tree Sequence 
to String Translation Model. To appear in Proceed-
ings of ACL/IJCNLP 
140
Improving Automatic Sentence Boundary Detection
with Confusion Networks
D. Hillard
 
M. Ostendorf
 
University of Washington, EE
 

hillard,mo  @ee.washington.edu
A. Stolcke  Y. Liu  E. Shriberg 
ICSI  and SRI International 

stolcke,ees  @speech.sri.com
yangl@icsi.berkeley.edu
Abstract
We extend existing methods for automatic sen-
tence boundary detection by leveraging multi-
ple recognizer hypotheses in order to provide
robustness to speech recognition errors. For
each hypothesized word sequence, an HMM
is used to estimate the posterior probability of
a sentence boundary at each word boundary.
The hypotheses are combined using confusion
networks to determine the overall most likely
events. Experiments show improved detec-
tion of sentences for conversational telephone
speech, though results are mixed for broadcast
news.
1 Introduction
The output of most current automatic speech recognition
systems is an unstructured sequence of words. Additional
information such as sentence boundaries and speaker la-
bels are useful to improve readability and can provide
structure relevant to subsequent language processing, in-
cluding parsing, topic segmentation and summarization.
In this study, we focus on identifying sentence boundaries
using word-based and prosodic cues, and in particular we
develop a method that leverages additional information
available from multiple recognizer hypotheses.
Multiple hypotheses are helpful because the single best
recognizer output still has many errors even for state-
of-the-art systems. For conversational telephone speech
(CTS) word error rates can be from 20-30%, and for
broadcast news (BN) word error rates are 10-15%. These
errors limit the effectiveness of sentence boundary pre-
diction, because they introduce incorrect words to the
word stream. Sentence boundary detection error rates
on a baseline system increased by 50% relative for CTS
when moving from the reference to the automatic speech
condition, while for BN error rates increased by about
20% relative (Liu et al, 2003). Including additional rec-
ognizer hypotheses allows for alternative word choices to
inform sentence boundary prediction.
To integrate the information from different alterna-
tives, we first predict sentence boundaries in each hypoth-
esized word sequence, using an HMM structure that in-
tegrates prosodic features in a decision tree with hidden
event language modeling. To facilitate merging predic-
tions from multiple hypotheses, we represent each hy-
pothesis as a confusion network, with confidences for
sentence predictions from a baseline system. The final
prediction is based on a combination of predictions from
individual hypotheses, each weighted by the recognizer
posterior for that hypothesis.
Our methods build on related work in sentence bound-
ary detection and confusion networks, as described in
Section 2, and a baseline system and task domain re-
viewed in Section 3. Our approach integrates prediction
on multiple recognizer hypotheses using confusion net-
works, as outlined in Section 4. Experimental results are
detailed in Section 5, and the main conclusions of this
work are summarized in Section 6.
2 Related Work
2.1 Sentence Boundary Detection
Previous work on sentence boundary detection for auto-
matically recognized words has focused on the prosodic
features and words of the single best recognizer output
(Shriberg et al, 2000). That system had an HMM struc-
ture that integrates hidden event language modeling with
prosodic decision tree outputs (Breiman et al, 1984). The
HMM states predicted at each word boundary consisted
of either a sentence or non-sentence boundary classifica-
tion, each of which received a confidence score. Improve-
ments to the hidden event framework have included inter-
polation of multiple language models (Liu et al, 2003).
A related model has been used to investigate punc-
tuation prediction for multiple hypotheses in a speech
recognition system (Kim and Woodland, 2001). That sys-
tem found improvement in punctuation prediction when
rescoring using the classification tree prosodic feature
model, but it also introduced a small increase in word
error rate. More recent work has also implemented a sim-
ilar model, but used prosodic features in a neural net in-
stead of a decision tree (Srivastava and Kubala, 2003).
A maximum entropy model that included pause informa-
tion was used in (Huang and Zweig, 2002). Both finite-
state models and neural nets have been investigated for
prosodic and lexical feature combination in (Christensen
et al, 2001).
2.2 Confusion Networks
Confusion networks are a compacted representation of
word lattices that have strictly ordered word hypothesis
slots (Mangu et al, 2000). The complexity of lattice rep-
resentations is reduced to a simpler form that maintains
all possible paths from the lattice (and more), but trans-
forms the space to a series of slots which each have word
hypotheses (and null arcs) derived from the lattice and as-
sociated posterior probabilities. Confusion networks may
also be constructed from an N-best list, which is the case
for these experiments. Confusion networks are used to
optimize word error rate (WER) by selecting the word
with the highest probability in each particular slot.
3 Tasks & Baseline
This work specifically detects boundaries of sentence-
like units called SUs. An SU roughly corresponds to a
sentence, except that SUs are for the most part defined as
units that include only one independent main clause, and
they may sometimes be incomplete as when a speaker
is interrupted and does not complete their sentence. A
more specific annotation guideline for SUs is available
(Strassel, 2003), which we refer to as the ?V5? standard.
In this work, we focus only on detecting SUs and do not
differentiate among the different types (e.g. statement,
question, etc.) that were used for annotation. We work
with a relatively new corpus and set of evaluation tools,
which are described below.
3.1 Corpora
The system is evaluated for both conversational telephone
speech (CTS) and broadcast news (BN), in both cases us-
ing training, development and test data annotated accord-
ing to the V5 standard. The test data is that used in the
DARPA Rich Transcription (RT) Fall 2003 evaluations;
the development and evaluation test sets together com-
prise the Spring 2003 RT evaluation test sets.
For CTS, there are 40 hours of conversations available
for training from the Switchboard corpus, and 3 hours
(72 conversation sides) each of development and evalua-
tion test data drawn from both the Switchboard and Fisher
corpora. The development and evaluation set each have
roughly 6000 SUs.
The BN data consists of a set of 20 hours of news
shows for training, and 3 hours (6 shows) for testing. The
development and evaluation test data contains 1.5 hours
(3 shows) each for development and evaluation, each with
roughly 1000 SUs. Test data comes from the month of
February in 2001; training data is taken from a previous
time period.
3.2 Baseline System
The automatic speech recognition systems used were up-
dated versions of those used by SRI in the Spring 2003
RT evaluations (NIST, 2003), with a WER of 12.1%
on BN data and 22.9% on CTS data. Both systems
perform multiple recognition and adaptation passes, and
eventually produce up to 2000-best hypotheses per wave-
form segment, which are then rescored with a number of
knowledge sources, such as higher-order language mod-
els, pronunciation scores, and duration models (for CTS).
For best results, the systems combine decoding output
from multiple front ends, each producing a separate N-
best list. All N-best lists for the same waveform segment
are then combined into a single word confusion network
(Mangu et al, 2000) from which the hypothesis with low-
est expected word error is extracted. In our baseline SU
system, the single best word stream thus obtained is then
used as the basis for SU recognition.
Our baseline SU system builds on previous work on
sentence boundary detection using lexical and prosodic
features (Shriberg et al, 2000). The system takes as in-
put alignments from either reference or recognized (1-
best) words, and combines lexical and prosodic infor-
mation using an HMM. Prosodic features include about
100 features reflecting pause, duration, F0, energy, and
speaker change information. The prosody model is a de-
cision tree classifier that generates the posterior probabil-
ity of an SU boundary at each interword boundary given
the prosodic features. Trees are trained from sampled
training data in order to make the model sensitive to fea-
tures of the minority SU class. Recent prosody model im-
provements include the use of bagging techniques in deci-
sion tree training to reduce the variability due to a single
tree (Liu et al, 2003). Language model improvements
include adding information from a POS-based model, a
model using automatically-induced word classes, and a
model trained on separate data.
3.3 Evaluation
Errors are measured by a slot error rate similar to the
WER metric utilized by the speech recognition commu-
nity, i.e. dividing the total number of inserted and deleted
SUs by the total number of reference SUs. (There are
no substitution errors because there is only one sentence
class.) When recognition output is used, the words will
generally not align perfectly with the reference transcrip-
tion and hence the SU boundary predictions will require
some alignment procedure to match to the reference lo-
cation. Here, the alignment is based on the minimum
word error alignment of the reference and hypothesized
word strings, and the minimum SU error alignment if the
WER is equal for multiple alignments. We report num-
bers computed with the su-eval scoring tool from NIST.
SU error rates for the reference words condition of our
baseline system are 49.04% for BN, and 30.13% for CTS,
as reported at the NIST RT03F evaluation (Liu et al,
2003). Results for the automatic speech recognition con-
dition are described in Section 5.
4 Using N-Best Sentence Hypotheses
The large increase in SU detection error rate in mov-
ing from reference to recognizer transcripts motivates an
approach that reduces the mistakes introduced by word
recognition errors. Although the best recognizer output is
optimized to reduce word error rate, alternative hypothe-
ses may together reinforce alternative (more accurate) SU
predictions. The oracle WER for the confusion networks
is much lower than for the single best hypothesis, in the
range of 13-16% WER for the CTS test sets.
4.1 Feature Extraction and SU Detection
Prediction of SUs using multiple hypotheses requires
prosodic feature extraction for each hypothesis, which
in turn requires a forced alignment of each hypothesis.
Thousands of hypotheses are output by the recognizer,
but we prune to a smaller set to reduce the cost of run-
ning forced alignments and prosodic feature extraction.
The recognizer outputs an N-best list of hypotheses and
assigns a posterior probability to each hypothesis, which
is normalized to sum to 1 over all hypotheses. We collect
hypotheses from the N-best list for each acoustic segment
up to 90% of the posterior mass (or to a maximum count
of 1000).
Next, forced alignment and prosodic feature extraction
are run for all segments in this pruned set of hypothe-
ses. Statistics for prosodic feature normalization (such as
speaker and turn F0 mean) are collected from the single
best hypothesis. After obtaining the prosodic features,
the HMM predicts sentence boundaries for each word se-
quence hypothesis independently. For each hypothesis,
an SU prediction is made at all word boundaries, result-
ing in a posterior probability for SU and no SU at each
boundary. The same models are used as in the 1-best pre-
dictions ? no parameters were re-optimized for the N-best
framework. Given independent predictions for the indi-
vidual hypotheses, we then build a system to incorporate
the multiple predictions into a single hypothesis, as de-
scribed next.
4.2 Combining Hypotheses
The prediction results for an individual hypothesis are
represented in a confusion network that consists of a
series of word slots, each followed by a slot with SU and
no SU, as shown in Figure 1 with hypothetical confi-
dences for the between-word events. (This representation
is a somewhat unusual form because the word slots have
only a single hypothesis.) The words in the individual
hypotheses have probability one, and each arc with an
SU or no SU token has a confidence (posterior prob-
ability) assigned from the HMM. The overall network
has a score associated with its N-best hypothesis-level
posterior probability, scaled by a weight corresponding to
the goodness of the system that generated that hypothesis.
president
SU
no_SU at
SU
no_SU war
SU
no_SU
1 1 1
.2
.8
.1
.9
.7
.3
Figure 1: Confusion network for a single hypothesis.
The confusion networks for each hypothesis are
then merged with the SRI Language Modeling Toolkit
(Stolcke, 2002) to create a single confusion network
for an overall hypothesis. This confusion network is
derived from an alignment of the confusion networks
of each individual hypothesis. The resulting network
contains slots with the word hypotheses from the N-best
list and slots with the combined SU/no SU probability,
as shown in Figure 2. The confidences assigned to each
token in the new confusion network are a weighted
linear combination of the probabilities from individual
hypotheses that align to each other, compiled from
the entire hypothesis list, where the weights are the
hypothesis-level scores from the recognizer.
president
SU
no_SU at
SU
no_SU war SU
no_SU
1 .2 .4
.2
.8
.15
.85 .3
or
of
.6
.2
 - .6  - .6
.1
Figure 2: Confusion network for a merged hypothesis.
Finally, the best decision at each point is selected by
choosing the words and boundaries with the highest prob-
ability. Here, the words and SUs are selected indepen-
dently, so that we obtain the same words as would be
selected without inserting the SU tokens and guarantee
no degradation in WER. The key improvement is that the
SU detection is now a result of detection across all recog-
nizer hypotheses, which reduces the effect of word errors
in the top hypothesis.
5 Experiments
Table 1 shows the results in terms of slot error rate on
the four test sets. The middle column indicates the per-
formance on a single hypothesis, with the words derived
from the pruned set of N-best hypotheses. The right col-
umn indicates the performance of the system using mul-
tiple hypotheses merged with confusion networks.
Multiple hypotheses provide a reduction of error for
both test sets of CTS (significant at p   .02 using the Mc-
Nemar test), but give insignificant (and mixed) results for
BN. The small increase in error for the BN evaluation set
WER SU error rate
Single Best Confusion Nets
BN Dev 12.2 55.79% 54.45%
BN Eval 12.0 57.78% 58.42%
CTS Dev 23.6 44.14% 42.72%
CTS Eval 22.2 44.95% 44.01%
Table 1: Word and SU error rates for single best vs. con-
fusion nets.
may be due to the fact that the 1-best parameters were
tuned on different news shows than were represented in
the evaluation data.
We expected a greater gain from the use of confusion
networks in CTS than BN, given the previously shown
impact of WER on 1-best SU detection. Additionally,
incorporating a larger number of N-best hypotheses has
improved results in all experiments so far, so we would
expect this trend to continue for additional increases, but
time constraints limited our ability to run these larger ex-
periments. One possible explanation for the relatively
small performance gains is that we constrained the con-
fusion network topology so that there was no change in
the word recognition results. We imposed this constraint
in our initial investigations to allow us to compare per-
formance using the same words. It it possible that better
performance could be obtained by using confusion net-
work topologies that link words and metadata.
A more specific breakout of error improvement for the
CTS development set is given in Table 2, showing that
both recall and precision benefit from using the N-best
framework. Including multiple hypotheses reduces the
number of SU deletions (improves recall), but the pri-
mary gain is in reducing insertion errors (higher preci-
sion). The same effect holds for the CTS evaluation set.
Single Best Confusion Nets Change
Deletions 1623 1597 -1.6%
Insertions 872 818 -6.2%
Total 2495 2415 -3.2%
Table 2: Errors for CTS development set
6 Conclusion
Detecting sentence structure in automatic speech recog-
nition provides important information for language pro-
cessing or human understanding. Incorporating multiple
hypotheses from word recognition output can improve
overall detection of SUs in comparison to prediction on a
single hypothesis. This is especially true for CTS, which
suffers more from word errors and can therefore benefit
from considering alternative hypotheses.
Future work will involve a tighter integration of SU de-
tection and word recognition by including SU events di-
rectly in the recognition lattice. This will provide oppor-
tunities to investigate the interaction of automatic word
recognition and structural metadata, hopefully resulting
in reduced WER. We also plan to extend these methods
to additional tasks such as disfluency detection.
Acknowledgments
This work is supported in part by DARPA contract no.
MDA972-02-C-0038, and made use of prosodic feature extrac-
tion and modeling tools developed under NSF-STIMULATE
grant IRI-9619921. Any opinions, conclusions or recommen-
dations expressed in this material are those of the authors and
do not necessarily reflect the views of these agencies.
References
L. Breiman et al 1984. Classification And Regression Trees.
Wadsworth International Group, Belmont, CA.
H. Christensen, Y. Gotoh, and S. Renals. 2001. Punctua-
tion annotation using statistical prosody models. In Proc.
ISCA Workshop on Prosody in Speech Recognition and Un-
derstanding.
J. Huang and G. Zweig. 2002. Maximum entropy model for
punctuation annotation from speech. In Proc. Eurospeech.
J.-H. Kim and P. Woodland. 2001. The use of prosody in
a combined system for punctuation generation and speech
recognition. In Proc. Eurospeech, pages 2757?2760.
Y. Liu, E. Shriberg, and A. Stolcke. 2003. Automatic disflu-
ency identification in conversational speech using multiple
knowledge sources. In Proc. Eurospeech, volume 1, pages
957?960.
Y. Liu et al 2003. MDE Research at
ICSI+SRI+UW, NIST RT-03F Workshop.
http://www.nist.gov/speech/tests/rt/rt2003/fall/presentations/.
L. Mangu, E. Brill, and A. Stolcke. 2000. Finding consensus
in speech recognition: word error minimization and other
applications of confusion networks. Computer Speech and
Language, pages 373?400.
NIST. 2003. RT-03S Workshop Agenda and Presentations.
http://www.nist.gov/speech/tests/rt/rt2003/spring/presentations/.
E. Shriberg et al 2000. Prosody-based automatic segmentation
of speech into sentences and topics. Speech Communication,
32(1-2):127?154, September.
A. Srivastava and F. Kubala. 2003. Sentence boundary detec-
tion in arabic speech. In Proc. Eurospeech, pages 949?952.
A. Stolcke. 2002. SRILM?an extensible language modeling
toolkit. In Proc. ICSLP, volume 2, pages 901?904.
S. Strassel, 2003. Simple Metadata Annotation Specification
V5.0. Linguistic Data Consortium.
Proceedings of the 43rd Annual Meeting of the ACL, pages 451?458,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Using Conditional Random Fields For Sentence Boundary Detection In
Speech
Yang Liu
ICSI, Berkeley
yangl@icsi.berkeley.edu
Andreas Stolcke Elizabeth Shriberg
SRI and ICSI
stolcke,ees@speech.sri.com
Mary Harper
Purdue University
harper@ecn.purdue.edu
Abstract
Sentence boundary detection in speech is
important for enriching speech recogni-
tion output, making it easier for humans to
read and downstream modules to process.
In previous work, we have developed hid-
den Markov model (HMM) and maximum
entropy (Maxent) classifiers that integrate
textual and prosodic knowledge sources
for detecting sentence boundaries. In this
paper, we evaluate the use of a condi-
tional random field (CRF) for this task
and relate results with this model to our
prior work. We evaluate across two cor-
pora (conversational telephone speech and
broadcast news speech) on both human
transcriptions and speech recognition out-
put. In general, our CRF model yields a
lower error rate than the HMM and Max-
ent models on the NIST sentence bound-
ary detection task in speech, although it
is interesting to note that the best results
are achieved by three-way voting among
the classifiers. This probably occurs be-
cause each model has different strengths
and weaknesses for modeling the knowl-
edge sources.
1 Introduction
Standard speech recognizers output an unstructured
stream of words, in which the important structural
features such as sentence boundaries are missing.
Sentence segmentation information is crucial and as-
sumed in most of the further processing steps that
one would want to apply to such output: tagging
and parsing, information extraction, summarization,
among others.
1.1 Sentence Segmentation Using HMM
Most prior work on sentence segmentation (Shriberg
et al, 2000; Gotoh and Renals, 2000; Christensen
et al, 2001; Kim and Woodland, 2001; NIST-
RT03F, 2003) have used an HMM approach, in
which the word/tag sequences are modeled by N-
gram language models (LMs) (Stolcke and Shriberg,
1996). Additional features (mostly related to speech
prosody) are modeled as observation likelihoods at-
tached to the N-gram states of the HMM (Shriberg
et al, 2000). Figure 1 shows the graphical model
representation of the variables involved in the HMM
for this task. Note that the words appear in both
the states1 and the observations, such that the
word stream constrains the possible hidden states
to matching words; the ambiguity in the task stems
entirely from the choice of events. This architec-
ture differs from the one typically used for sequence
tagging (e.g., part-of-speech tagging), in which the
?hidden? states represent only the events or tags.
Empirical investigations have shown that omitting
words in the states significantly degrades system
performance for sentence boundary detection (Liu,
2004). The observation probabilities in the HMM,
implemented using a decision tree classifier, capture
the probabilities of generating the prosodic features
1In this sense, the states are only partially ?hidden?.
451
P (F
i
jE
i
;W
i
).
2 An N-gram LM is used to calculate
the transition probabilities:
P (W
i
E
i
jW
1
E
1
: : :W
i 1
E
i 1
) =
P (W
i
jW
1
E
1
: : :W
i 1
E
i 1
)
P (E
i
jW
1
E
1
: : :W
i 1
E
i 1
E
i
)
In the HMM, the forward-backward algorithm is
used to determine the event with the highest poste-
rior probability for each interword boundary:
^
E
i
= argmax
E
i
P (E
i
jW;F ) (1)
The HMM is a generative modeling approach since
it describes a stochastic process with hidden vari-
ables (sentence boundary) that produces the observ-
able data. This HMM approach has two main draw-
backs. First, standard training methods maximize
the joint probability of observed and hidden events,
as opposed to the posterior probability of the correct
hidden variable assignment given the observations,
which would be a criterion more closely related to
classification performance. Second, the N-gram LM
underlying the HMM transition model makes it dif-
ficult to use features that are highly correlated (such
as words and POS labels) without greatly increas-
ing the number of model parameters, which in turn
would make robust estimation difficult. More details
about using textual information in the HMM system
are provided in Section 3.
1.2 Sentence Segmentation Using Maxent
A maximum entropy (Maxent) posterior classifica-
tion method has been evaluated in an attempt to
overcome some of the shortcomings of the HMM
approach (Liu et al, 2004; Huang and Zweig, 2002).
For a boundary position i, the Maxent model takes
the exponential form:
P (E
i
jT
i
; F
i
) =
1
Z

(T
i
; F
i
)
e
P
k

k
g
k
(E
i
;T
i
;F
i
) (2)
where Z

(T
i
; F
i
) is a normalization term and T
i
represents textual information. The indicator func-
tions g
k
(E
i
; T
i
; F
i
) correspond to features defined
over events, words, and prosody. The parameters in
2In the prosody model implementation, we ignore the word
identity in the conditions, only using the timing or word align-
ment information.
Wi Ei
Fi
Oi
Wi+1 Ei+1
Oi+1
Wi Fi+1Wi+1
Figure 1: A graphical model of HMM for the
sentence boundary detection problem. Only one
word+event pair is depicted in each state, but in
a model based on N-grams, the previous N   1
tokens would condition the transition to the next
state. O are observations consisting of words W and
prosodic features F , and E are sentence boundary
events.
Maxent are chosen to maximize the conditional like-
lihood
Q
i
P (E
i
jT
i
; F
i
) over the training data, bet-
ter matching the classification accuracy metric. The
Maxent framework provides a more principled way
to combine the largely correlated textual features, as
confirmed by the results of (Liu et al, 2004); how-
ever, it does not model the state sequence.
A simple combination of the results from the
Maxent and HMM was found to improve upon the
performance of either model alone (Liu et al, 2004)
because of the complementary strengths and weak-
nesses of the two models. An HMM is a generative
model, yet it is able to model the sequence via the
forward-backward algorithm. Maxent is a discrimi-
native model; however, it attempts to make decisions
locally, without using sequential information.
A conditional random field (CRF) model (Laf-
ferty et al, 2001) combines the benefits of the HMM
and Maxent approaches. Hence, in this paper we
will evaluate the performance of the CRF model and
relate the results to those using the HMM and Max-
ent approaches on the sentence boundary detection
task. The rest of the paper is organized as follows.
Section 2 describes the CRF model and discusses
how it differs from the HMM and Maxent models.
Section 3 describes the data and features used in the
models to be compared. Section 4 summarizes the
experimental results for the sentence boundary de-
tection task. Conclusions and future work appear in
Section 5.
452
2 CRF Model Description
A CRF is a random field that is globally conditioned
on an observation sequence O. CRFs have been suc-
cessfully used for a variety of text processing tasks
(Lafferty et al, 2001; Sha and Pereira, 2003; McCal-
lum and Li, 2003), but they have not been widely ap-
plied to a speech-related task with both acoustic and
textual knowledge sources. The top graph in Figure
2 is a general CRF model. The states of the model
correspond to event labels E. The observations O
are composed of the textual features, as well as the
prosodic features. The most likely event sequence ^E
for the given input sequence (observations) O is
^
E = argmax
E
e
P
k

k
G
k
(E;O)
Z

(O)
(3)
where the functions G are potential functions over
the events and the observations, and Z

is the nor-
malization term:
Z

(O) =
X
E
e
P
k

k
G
k
(E;O) (4)
Even though a CRF itself has no restriction on
the potential functions G
k
(E;O), to simplify the
model (considering computational cost and the lim-
ited training set size), we use a first-order CRF in
this investigation, as at the bottom of Figure 2. In
this model, an observation O
i
(consisting of textual
features T
i
and prosodic features F
i
) is associated
with a state E
i
.
The model is trained to maximize the conditional
log-likelihood of a given training set. Similar to the
Maxent model, the conditional likelihood is closely
related to the individual event posteriors used for
classification, enabling this type of model to explic-
itly optimize discrimination of correct from incor-
rect labels. The most likely sequence is found using
the Viterbi algorithm.3
A CRF differs from an HMM with respect to its
training objective function (joint versus conditional
likelihood) and its handling of dependent word fea-
tures. Traditional HMM training does not maxi-
mize the posterior probabilities of the correct la-
bels; whereas, the CRF directly estimates posterior
3The forward-backward algorithm would most likely be bet-
ter here, but it is not implemented in the software we used (Mc-
Callum, 2002).
E 1 E 2 E i E N
O
E i
Oi
E i-1
O i-1
E i+1
O i+1
Figure 2: Graphical representations of a general
CRF and the first-order CRF used for the sentence
boundary detection problem. E represent the state
tags (i.e., sentence boundary or not). O are observa-
tions consisting of words W or derived textual fea-
tures T and prosodic features F .
boundary label probabilities P (EjO). The under-
lying N-gram sequence model of an HMM does
not cope well with multiple representations (fea-
tures) of the word sequence (e.g., words, POS), es-
pecially when the training set is small; however, the
CRF model supports simultaneous correlated fea-
tures, and therefore gives greater freedom for incor-
porating a variety of knowledge sources. A CRF
differs from the Maxent method with respect to its
ability to model sequence information. The primary
advantage of the CRF over the Maxent approach is
that the model is optimized globally over the entire
sequence; whereas, the Maxent model makes a local
decision, as shown in Equation (2), without utilizing
any state dependency information.
We use the Mallet package (McCallum, 2002) to
implement the CRF model. To avoid overfitting, we
employ a Gaussian prior with a zero mean on the
parameters (Chen and Rosenfeld, 1999), similar to
what is used for training Maxent models (Liu et al,
2004).
3 Experimental Setup
3.1 Data and Task Description
The sentence-like units in speech are different from
those in written text. In conversational speech,
these units can be well-formed sentences, phrases,
or even a single word. These units are called SUs
in the DARPA EARS program. SU boundaries, as
453
well as other structural metadata events, were an-
notated by LDC according to an annotation guide-
line (Strassel, 2003). Both the transcription and the
recorded speech were used by the annotators when
labeling the boundaries.
The SU detection task is conducted on two cor-
pora: Broadcast News (BN) and Conversational
Telephone Speech (CTS). BN and CTS differ in
genre and speaking style. The average length of SUs
is longer in BN than in CTS, that is, 12.35 words
(standard deviation 8.42) in BN compared to 7.37
words (standard deviation 8.72) in CTS. This dif-
ference is reflected in the frequency of SU bound-
aries: about 14% of interword boundaries are SUs in
CTS compared to roughly 8% in BN. Training and
test data for the SU detection task are those used in
the NIST Rich Transcription 2003 Fall evaluation.
We use both the development set and the evalua-
tion set as the test set in this paper in order to ob-
tain more meaningful results. For CTS, there are
about 40 hours of conversational data (around 480K
words) from the Switchboard corpus for training
and 6 hours (72 conversations) for testing. The BN
data has about 20 hours of Broadcast News shows
(about 178K words) in the training set and 3 hours
(6 shows) in the test set. Note that the SU-annotated
training data is only a subset of the data used for
the speech recognition task because more effort is
required to annotate the boundaries.
For testing, the system determines the locations
of sentence boundaries given the word sequence W
and the speech. The SU detection task is evaluated
on both the reference human transcriptions (REF)
and speech recognition outputs (STT). Evaluation
across transcription types allows us to obtain the per-
formance for the best-case scenario when the tran-
scriptions are correct; thus factoring out the con-
founding effect of speech recognition errors on the
SU detection task. We use the speech recognition
output obtained from the SRI recognizer (Stolcke et
al., 2003).
System performance is evaluated using the offi-
cial NIST evaluation tools.4 System output is scored
by first finding a minimum edit distance alignment
between the hypothesized word string and the refer-
4See http://www.nist.gov/speech/tests/rt/rt2003/fall/ for
more details about scoring.
ence transcriptions, and then comparing the aligned
event labels. The SU error rate is defined as the total
number of deleted or inserted SU boundary events,
divided by the number of true SU boundaries. In
addition to this NIST SU error metric, we use the
total number of interword boundaries as the denomi-
nator, and thus obtain results for the per-boundary-
based metric.
3.2 Feature Extraction and Modeling
To obtain a good-quality estimation of the condi-
tional probability of the event tag given the obser-
vations P (E
i
jO
i
), the observations should be based
on features that are discriminative of the two events
(SU versus not). As in (Liu et al, 2004), we utilize
both textual and prosodic information.
We extract prosodic features that capture duration,
pitch, and energy patterns associated with the word
boundaries (Shriberg et al, 2000). For all the model-
ing methods, we adopt a modular approach to model
the prosodic features, that is, a decision tree classi-
fier is used to model them. During testing, the de-
cision tree prosody model estimates posterior prob-
abilities of the events given the associated prosodic
features for a word boundary. The posterior prob-
ability estimates are then used in various modeling
approaches in different ways as described later.
Since words and sentence boundaries are mu-
tually constraining, the word identities themselves
(from automatic recognition or human transcrip-
tions) constitute a primary knowledge source for
sentence segmentation. We also make use of vari-
ous automatic taggers that map the word sequence to
other representations. Tagged versions of the word
stream are provided to support various generaliza-
tions of the words and to smooth out possibly un-
dertrained word-based probability estimates. These
tags include part-of-speech tags, syntactic chunk
tags, and automatically induced word classes. In ad-
dition, we use extra text corpora, which were not an-
notated according to the guideline used for the train-
ing and test data (Strassel, 2003). For BN, we use
the training corpus for the LM for speech recogni-
tion. For CTS, we use the Penn Treebank Switch-
board data. There is punctuation information in
both, which we use to approximate SUs as defined
in the annotation guideline (Strassel, 2003).
As explained in Section 1, the prosody model and
454
Table 1: Knowledge sources and their representations in different modeling approaches: HMM, Maxent,
and CRF.
HMM Maxent CRF
generative model conditional approach
Sequence information yes no yes
LDC data set (words or tags) LM N-grams as indicator functions
Probability from prosody model real-valued cumulatively binned
Additional text corpus N-gram LM binned posteriors
Speaker turn change in prosodic features a separate feature,
in addition to being in the prosodic feature set
Compound feature no POS tags and decisions from prosody model
the N-gram LM can be integrated in an HMM. When
various textual information is used, jointly modeling
words and tags may be an effective way to model the
richer feature set; however, a joint model requires
more parameters. Since the training set for the SU
detection task in the EARS program is quite limited,
we use a loosely coupled approach:
 Linearly combine three LMs: the word-based
LM from the LDC training data, the automatic-
class-based LMs, and the word-based LM
trained from the additional corpus.
 These interpolated LMs are then combined
with the prosody model via the HMM. The
posterior probabilities of events at each bound-
ary are obtained from this step, denoted as
P
HMM
(E
i
jW;C;F ).
 Apply the POS-based LM alone to the POS
sequence (obtained by running the POS tag-
ger on the word sequence W ) and generate the
posterior probabilities for each word boundary
P
posLM
(E
i
jPOS), which are then combined
from the posteriors from the previous step,
i.e., P
final
(E
i
jT; F ) = P
HMM
(E
i
jW;C;F )+
P
posLM
(E
i
jP ).
The features used for the CRF are the same as
those used for the Maxent model devised for the SU
detection task (Liu et al, 2004), briefly listed below.
 N-grams of words or various tags (POS tags,
automatically induced classes). Different Ns
and different position information are used (N
varies from one through four).
 The cumulative binned posterior probabilities
from the decision tree prosody model.
 The N-gram LM trained from the extra cor-
pus is used to estimate posterior event proba-
bilities for the LDC-annotated training and test
sets, and these posteriors are then thresholded
to yield binary features.
 Other features: speaker or turn change, and
compound features of POS tags and decisions
from the prosody model.
Table 1 summarizes the features and their repre-
sentations used in the three modeling approaches.
The same knowledge sources are used in these ap-
proaches, but with different representations. The
goal of this paper is to evaluate the ability of these
three modeling approaches to combine prosodic and
textual knowledge sources, not in a rigidly parallel
fashion, but by exploiting the inherent capabilities
of each approach. We attempt to compare the mod-
els in as parallel a fashion as possible; however, it
should be noted that the two discriminative methods
better model the textual sources and the HMM bet-
ter models prosody given its representation in this
study.
4 Experimental Results and Discussion
SU detection results using the CRF, HMM, and
Maxent approaches individually, on the reference
transcriptions or speech recognition output, are
shown in Tables 2 and 3 for CTS and BN data, re-
spectively. We present results when different knowl-
edge sources are used: word N-gram only, word N-
gram and prosodic information, and using all the
455
Table 2: Conversational telephone speech SU detection results reported using the NIST SU error rate (%)
and the boundary-based error rate (% in parentheses) using the HMM, Maxent, and CRF individually and in
combination. Note that the ?all features? condition uses all the knowledge sources described in Section 3.2.
?Vote? is the result of the majority vote over the three modeling approaches, each of which uses all the
features. The baseline error rate when assuming there is no SU boundary at each word boundary is 100%
for the NIST SU error rate and 15.7% for the boundary-based metric.
Conversational Telephone Speech
HMM Maxent CRF
word N-gram 42.02 (6.56) 43.70 (6.82) 37.71 (5.88)
REF word N-gram + prosody 33.72 (5.26) 35.09 (5.47) 30.88 (4.82)
all features 31.51 (4.92) 30.66 (4.78) 29.47 (4.60)
Vote: 29.30 (4.57)
word N-gram 53.25 (8.31) 53.92 (8.41) 50.20 (7.83)
STT word N-gram + prosody 44.93 (7.01) 45.50 (7.10) 43.12 (6.73)
all features 43.05 (6.72) 43.02 (6.71) 42.00 (6.55)
Vote: 41.88 (6.53)
features described in Section 3.2. The word N-
grams are from the LDC training data and the extra
text corpora. ?All the features? means adding textual
information based on tags, and the ?other features? in
the Maxent and CRF models as well. The detection
error rate is reported using the NIST SU error rate,
as well as the per-boundary-based classification er-
ror rate (in parentheses in the table) in order to factor
out the effect of the different SU priors. Also shown
in the tables are the majority vote results over the
three modeling approaches when all the features are
used.
4.1 CTS Results
For CTS, we find from Table 2 that the CRF is supe-
rior to both the HMM and the Maxent model across
all conditions (the differences are significant at p <
0:05). When using only the word N-gram informa-
tion, the gain of the CRF is the greatest, with the dif-
ferences among the models diminishing as more fea-
tures are added. This may be due to the impact of the
sparse data problem on the CRF or simply due to the
fact that differences among modeling approaches are
less when features become stronger, that is, the good
features compensate for the weaknesses in models.
Notice that with fewer knowledge sources (e.g., us-
ing only word N-gram and prosodic information),
the CRF is able to achieve performance similar to or
even better than other methods using all the knowl-
edges sources. This may be useful when feature ex-
traction is computationally expensive.
We observe from Table 2 that there is a large
increase in error rate when evaluating on speech
recognition output. This happens in part because
word information is inaccurate in the recognition
output, thus impacting the effectiveness of the LMs
and lexical features. The prosody model is also af-
fected, since the alignment of incorrect words to the
speech is imperfect, thereby degrading prosodic fea-
ture extraction. However, the prosody model is more
robust to recognition errors than textual knowledge,
because of its lesser dependence on word identity.
The results show that the CRF suffers most from the
recognition errors. By focusing on the results when
only word N-gram information is used, we can see
the effect of word errors on the models. The SU
detection error rate increases more in the STT con-
dition for the CRF model than for the other models,
suggesting that the discriminative CRF model suf-
fers more from the mismatch between the training
(using the reference transcription) and the test con-
dition (features obtained from the errorful words).
We also notice from the CTS results that when
only word N-gram information is used (with or
without combining with prosodic information), the
HMM is superior to the Maxent; only when various
additional textual features are included in the fea-
ture set does Maxent show its strength compared to
456
Table 3: Broadcast news SU detection results reported using the NIST SU error rate (%) and the boundary-
based error rate (% in parentheses) using the HMM, Maxent, and CRF individually and in combination. The
baseline error rate is 100% for the NIST SU error rate and 7.2% for the boundary-based metric.
Broadcast News
HMM Maxent CRF
word N-gram 80.44 (5.83) 81.30 (5.89) 74.99 (5.43)
REF word N-gram + prosody 59.81 (4.33) 59.69 (4.33) 54.92 (3.98)
all features 48.72 (3.53) 48.61 (3.52) 47.92 (3.47)
Vote: 46.28 (3.35)
word N-gram 84.71 (6.14) 86.13 (6.24) 80.50 (5.83)
STT word N-gram + prosody 64.58 (4.68) 63.16 (4.58) 59.52 (4.31)
all features 55.37 (4.01) 56.51 (4.10) 55.37 (4.01)
Vote: 54.29 (3.93)
the HMM, highlighting the benefit of Maxent?s han-
dling of the textual features.
The combined result (using majority vote) of the
three approaches in Table 2 is superior to any model
alone (the improvement is not significant though).
Previously, it was found that the Maxent and HMM
posteriors combine well because the two approaches
have different error patterns (Liu et al, 2004). For
example, Maxent yields fewer insertion errors than
HMM because of its reliance on different knowledge
sources. The toolkit we use for the implementation
of the CRF does not generate a posterior probabil-
ity for a sequence; therefore, we do not combine
the system output via posterior probability interpola-
tion, which is expected to yield better performance.
4.2 BN Results
Table 3 shows the SU detection results for BN. Sim-
ilar to the patterns found for the CTS data, the CRF
consistently outperforms the HMM and Maxent, ex-
cept on the STT condition when all the features are
used. The CRF yields relatively less gain over the
other approaches on BN than on CTS. One possible
reason for this difference is that there is more train-
ing data for the CTS task, and both the CRF and
Maxent approaches require a relatively larger train-
ing set than the HMM. Overall the degradation on
the STT condition for BN is smaller than on CTS.
This can be easily explained by the difference in
word error rates, 22.9% on CTS and 12.1% on BN.
Finally, the vote among the three approaches outper-
forms any model on both the REF and STT condi-
tions, and the gain from voting is larger for BN than
CTS.
Comparing Table 2 and Table 3, we find that the
NIST SU error rate on BN is generally higher than
on CTS. This is partly because the NIST error rate
is measured as the percentage of errors per refer-
ence SU, and the number of SUs in CTS is much
larger than for BN, giving a large denominator and
a relatively lower error rate for the same number of
boundary detection errors. Another reason is that the
training set is smaller for BN than for CTS. Finally,
the two genres differ significantly: CTS has the ad-
vantage of the frequent backchannels and first per-
son pronouns that provide good cues for SU detec-
tion. When the boundary-based classification metric
is used (results in parentheses), the SU error rate is
lower on BN than on CTS; however, it should also
be noted that the baseline error rate (i.e., the priors
of the SUs) is lower on BN than CTS.
5 Conclusion and Future Work
Finding sentence boundaries in speech transcrip-
tions is important for improving readability and aid-
ing downstream language processing modules. In
this paper, prosodic and textual knowledge sources
are integrated for detecting sentence boundaries in
speech. We have shown that a discriminatively
trained CRF model is a competitive approach for
the sentence boundary detection task. The CRF
combines the advantages of being discriminatively
trained and able to model the entire sequence, and
so it outperforms the HMM and Maxent approaches
457
consistently across various testing conditions. The
CRF takes longer to train than the HMM and Max-
ent models, especially when the number of features
becomes large; the HMM requires the least training
time of all approaches. We also find that as more fea-
tures are used, the differences among the modeling
approaches decrease. We have explored different ap-
proaches to modeling various knowledge sources in
an attempt to achieve good performance for sentence
boundary detection. Note that we have not fully op-
timized each modeling approach. For example, for
the HMM, using discriminative training methods is
likely to improve system performance, but possibly
at a cost of reducing the accuracy of the combined
system.
In future work, we will examine the effect of
Viterbi decoding versus forward-backward decoding
for the CRF approach, since the latter better matches
the classification accuracy metric. To improve SU
detection results on the STT condition, we plan to
investigate approaches that model recognition un-
certainty in order to mitigate the effect of word er-
rors. Another future direction is to investigate how
to effectively incorporate prosodic features more di-
rectly in the Maxent or CRF framework, rather than
using a separate prosody model and then binning the
resulting posterior probabilities.
Important ongoing work includes investigating
the impact of SU detection on downstream language
processing modules, such as parsing. For these ap-
plications, generating probabilistic SU decisions is
crucial since that information can be more effec-
tively used by subsequent modules.
6 Acknowledgments
The authors thank the anonymous reviewers for their valu-
able comments, and Andrew McCallum and Aron Culotta at
the University of Massachusetts and Fernando Pereira at the
University of Pennsylvania for their assistance with their CRF
toolkit. This work has been supported by DARPA under
contract MDA972-02-C-0038, NSF-STIMULATE under IRI-
9619921, NSF KDI BCS-9980054, and ARDA under contract
MDA904-03-C-1788. Distribution is unlimited. Any opinions
expressed in this paper are those of the authors and do not reflect
the funding agencies. Part of the work was carried out while the
last author was on leave from Purdue University and at NSF.
References
S. Chen and R. Rosenfeld. 1999. A Gaussian prior for smooth-
ing maximum entropy models. Technical report, Carnegie
Mellon University.
H. Christensen, Y. Gotoh, and S. Renal. 2001. Punctuation an-
notation using statistical prosody models. In ISCA Workshop
on Prosody in Speech Recognition and Understanding.
Y. Gotoh and S. Renals. 2000. Sentence boundary detection in
broadcast speech transcripts. In Proceedings of ISCA Work-
shop: Automatic Speech Recognition: Challenges for the
New Millennium ASR-2000, pages 228?235.
J. Huang and G. Zweig. 2002. Maximum entropy model for
punctuation annotation from speech. In Proceedings of the
International Conference on Spoken Language Processing,
pages 917?920.
J. Kim and P. C. Woodland. 2001. The use of prosody in a com-
bined system for punctuation generation and speech recogni-
tion. In Proceedings of the European Conference on Speech
Communication and Technology, pages 2757?2760.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional
random field: Probabilistic models for segmenting and la-
beling sequence data. In Proceedings of the International
Conference on Machine Learning, pages 282?289.
Y. Liu, A. Stolcke, E. Shriberg, and M. Harper. 2004. Com-
paring and combining generative and posterior probability
models: Some advances in sentence boundary detection in
speech. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing.
Y. Liu. 2004. Structural Event Detection for Rich Transcription
of Speech. Ph.D. thesis, Purdue University.
A. McCallum and W. Li. 2003. Early results for named en-
tity recognition with conditional random fields. In Proceed-
ings of the Conference on Computational Natural Language
Learning.
A. McCallum. 2002. Mallet: A machine learning for language
toolkit. http://mallet.cs.umass.edu.
NIST-RT03F. 2003. RT-03F workshop agenda and
presentations. http://www.nist.gov/speech/tests/rt/rt2003/
fall/presentations/, November.
F. Sha and F. Pereira. 2003. Shallow parsing with conditional
random fields. In Proceedings of Human Language Technol-
ogy Conference / North American Chapter of the Association
for Computational Linguistics annual meeting.
E. Shriberg, A. Stolcke, D. Hakkani-Tur, and G. Tur. 2000.
Prosody-based automatic segmentation of speech into sen-
tences and topics. Speech Communication, pages 127?154.
A. Stolcke and E. Shriberg. 1996. Automatic linguistic seg-
mentation of conversational speech. In Proceedings of the
International Conference on Spoken Language Processing,
pages 1005?1008.
A. Stolcke, H. Franco, R. Gadde, M. Graciarena, K. Pre-
coda, A. Venkataraman, D. Vergyri, W. Wang, and
J. Zheng. 2003. Speech-to-text research at SRI-
ICSI-UW. http://www.nist.gov/speech/tests/rt/rt2003/
spring/presentations/index.htm.
S. Strassel, 2003. Simple Metadata Annotation Specification
V5.0. Linguistic Data Consortium.
458
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 973?981,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Learning to Predict Code-Switching Points
Thamar Solorio and Yang Liu
Human Language Technology Research Institute
The University of Texas at Dallas
Richardson, TX 75080, USA
tsolorio,yangl@hlt.utdallas.edu
Abstract
Predicting possible code-switching points can
help develop more accurate methods for au-
tomatically processing mixed-language text,
such as multilingual language models for
speech recognition systems and syntactic an-
alyzers. We present in this paper exploratory
results on learning to predict potential code-
switching points in Spanish-English. We
trained different learning algorithms using a
transcription of code-switched discourse. To
evaluate the performance of the classifiers, we
used two different criteria: 1) measuring pre-
cision, recall, and F-measure of the predic-
tions against the reference in the transcrip-
tion, and 2) rating the naturalness of artifi-
cially generated code-switched sentences. Av-
erage scores for the code-switched sentences
generated by our machine learning approach
were close to the scores of those generated by
humans.
1 Introduction
Multilingual speakers often switch back and forth
between languages when speaking or writing,
mostly in informal settings. The mixing of lan-
guages involves very elaborated patterns and forms
and we usually use the term Code-Switching (CS)
to encompass all of them (Lipski, 1978). Before the
Internet era, CS was mainly used in its spoken form.
But with so many different informal interaction set-
tings, such as chats, forums, blogs, and web sites
like Myspace and Facebook, CS is being used more
and more in written form. For English and Spanish,
CS has taken a step further. It has become a hall-
mark of the chicano culture as it is evident by the
growing number of chicano writers publishing work
in Spanish-English CS.
We have not completely discovered the process
of human language acquisition, especially dual lan-
guage acquisition. Findings in linguistics, soci-
olinguistics, and psycholinguistics show that the
production of code-switched discourse requires a
very sophisticated knowledge of the languages be-
ing mixed. Some theories suggest bilingual speak-
ers might have a third grammar for processing this
type of discourse. The general agreement regarding
CS is that switches do not take place at random and
instead it is possible to identify rules that bilingual
speakers adhere to.
Understanding the CS process can lead to accu-
rate methods for the automatic processing of bilin-
gual discourse, and corpus-driven studies about CS
can also inform linguistic theories. In this paper we
present exploratory work on learning to predict CS
points using a machine learning approach. Such an
approach can be used to reduce perplexity of lan-
guage models for bilingual discourse. We believe
that CS behavior can be learned by a classifier and
the results presented in this paper support our belief.
One of the difficult aspects of trying to predict
CS points is how to evaluate the performance of
the learner since switching is intrinsically motivated
and there are no forced switches (Sankoff, 1998b).
Therefore, standard classification measures for this
task such as precision, recall, F-measure, or ac-
curacy, are not the best approach for measuring
the effectiveness of a CS predictor. To comple-
973
ment the evaluation of our approach, we designed a
task involving human judgements on the naturalness
of automatically generated code-switched sentences.
Both evaluations yielded encouraging results.
The next section discusses theories explaining
the CS production process. Then in Section 3 we
present our framework for learning to predict CS
points. Section 4 discusses the empirical evaluation
of the classifiers compared to the human reference.
In Section 5 we present results of human evalua-
tions on automatically generated code-switched sen-
tences. Section 6 describes previous work related to
the processing of code-switched text. Finally, we
conclude in Section 7 with a summary of our find-
ings and directions for future work.
2 Bilingual Discourse
The combination of languages can be considered
to be a continuous spectrum where on each end of
the spectrum we have one of the standard languages
and no blending. As one moves closer to the mid-
dle of the spectrum the amount and complexity of
the blending pattern increases. The blending pattern
most widely known, and studied, is code-switching,
which refers to the mixing of words from two lan-
guages, but the words themselves do not suffer any
syntactic or phonological alterations. The CS points
can lie at sentence boundaries, but very often we
will also observe CS inside sentences. According to
(Sankoff, 1998b; Poplack, 1980; Lipski, 1978) when
CS is used inside a sentence, it can only happen at
syntactic boundaries shared by both languages, and
the resulting monolingual fragments will conform to
the grammar of the corresponding language. In this
CS theory the relationship between both languages
is symmetric ?lexical items from one language can
be replaced by the corresponding items in the sec-
ond language and vice versa. Another prevalent lin-
guistic theory argues the contrary: there is an asym-
metric relation where the changes can occur only in
one direction, which reflects the existence of a Ma-
trix Language (ML), the dominant language, and an
Embedded Language (EL), or subordinate language
(Joshi, 1982). The Matrix Language Frame model,
proposed and extended by Scotton-Myers, supports
this asymmetric relation theory. This formalism pre-
scribes that content morphemes can come from the
ML or the EL, whereas late system morphemes,
the elements that indicate grammatical relations, can
only be provided by the ML (Myers-Scotton, 1997).
Until an empirical evaluation is carried out on
large representative samples of discourse involving
a large number of different speakers, and different
language-pairs, the production of CS discourse will
not be explained satisfactorily. The goal of this work
is to move closer to a better understanding of CS by
learning from corpora to predict possible CS points.
3 Learning When To Code-Switch
3.1 The English-Spanish Code-Switched Data
Set
We recorded a conversation among three English-
Spanish bilingual speakers that code-switch regu-
larly when speaking to each other. The conversa-
tion lasts for about 40 minutes (?8k words, 922
sentences). It was manually transcribed and anno-
tated with Part-of-Speech (POS) tags. A total of
239 switches were identified manually. English is
the predominant language used, with a total of 576
monolingual sentences. We refer to this transcrip-
tion as the Spanglish data set. We are currently in the
process of collecting new transcriptions of this con-
versation in order to measure inter annotator agree-
ment.
3.2 Approach
Machine learning algorithms have proven to be sur-
prisingly good at language processing tasks, in-
cluding optical character recognition, text classifica-
tion, named entity extraction, and many more. The
premise of our paper is that machine learning al-
gorithms can also be successful at learning how to
code-switch as well as humans. At the very least
we want to provide encouraging evidence that this
is possible. To the best of our knowledge, there is
no previous work related to the problem of auto-
matically predicting CS points. Our machine learn-
ing framework then is inspired by existing theories
of CS and existing work on part-of-speech tagging
code-switched text (Solorio and Liu, 2008).
In our approach, each word boundary is a poten-
tial point for switching ? an instance of the learning
task. It should be noted that we can only rely on the
history of words preceding potential CS points in or-
974
Feature id Description
1 Word
2 Language id
3 Gold-standard POS tag
4 BIO chunk
5 English Tree Tagger POS
6 English Tree Tagger prob
7 English Tree Tagger lemma
8 Spanish Tree Tagger POS
9 Spanish Tree Tagger prob
10 Spanish Tree Tagger lemma
Table 1: Features explored in learning to predict CS
points.
der to extract meaningful features. Otherwise, if we
look also into the future, we could just do language
identification to extract the CS points. However, our
goal is to provide methods that can be used in real
time applications, where we do not have access to
observations beyond the point of interest. Another
restriction we imposed on the method is related to
the size of the context used. A sentence can be code-
switched in different ways, with all different ver-
sions adhering to the CS ?grammar?. The number
of permissible CS sentences grows almost exponen-
tially with the length of the sentence1. By limiting
the length of the context to at most two words we
are trying to avoid some sort of over fitting by hav-
ing the model making assumptions over the interac-
tion of the two languages that will be too weak, or
speaker-dependent.
Previous studies have identified several socio-
pragmatic functions of code-switching. The most
common include direct quotation, emphasis, clari-
fication, parenthetical comments, tags, and trigger
switches. Other characteristics relevant to CS be-
havior are the topic being discussed, the speakers
involved, the setting where the conversation is tak-
ing place, and the level of familiarity between the
speakers. Having encoded information regarding the
CS function and the aforementioned relevant factors
might help in predicting upcoming CS points. How-
ever, annotating this information in the transcription
can be time consuming and very often this informa-
1Almost exponentially because not all sentences will be con-
sidered grammatical.
tion is not readily available. Therefore, at the ex-
pense of making this task even more difficult, we de-
cided against trying to include this type of informa-
tion and include only lexical and syntactic features,
to evaluate a practical and cost effective method for
this task. Table 1 shows the list of features. All
of these features are associated with word wn, the
word immediately preceding boundary n. Feature 1
is the word form2. Feature 2 is language identifica-
tion. If the production of CS discourse adheres to
the matrix language frame model, then knowledge
of the language can potentially be a good source
of information. Feature 3 is the gold-standard POS
tag. We also include as a feature the position of
the word relative to the phrase constituent using a
Beginning-Inside-Outside (BIO) scheme. For in-
stance, the word at the beginning of the verb phrase
will be labeled as B, the following words inside this
verb phrase will be tagged as I, and words that were
not identified as part of a phrase constituent were
labeled as O. This chunking information was ex-
tracted using the English and Spanish versions of
FreeLing3. We did not measure accuracy on the
chunking information. Features 5 to 9 were gener-
ated by tagging the Spanglish conversation using the
Spanish and the English versions of the Tree Tagger
(Schmid, 1994). Attributes 5 to 7 are extracted from
the English version, which include the POS tag, the
confidence, and the lemma for that word. Similarly,
features 8 to 10 were taken from the Spanish mono-
lingual tree tagger. Features from the monolingual
taggers will have some noisy labels when tagging
fragments of the other language. However, consider-
ing that our feature set is small we want to explore if
adding these features, which include the lemmas and
probability estimates, can contribute to the learning
task.
We also explored using a larger context. In this
case, we extract the same features shown in Table
1 for the two words preceding the word boundary,
resulting in 20 attributes representing each instance.
Evaluation for this task is not straightforward.
Within a sentence, there are several CS points that
will result in a natural sounding code-switched sen-
tence, but none of these CS points are mandatory.
2Strictly speaking these should be called tokens, not words
since punctuation marks are considered as well.
3http://garraf.epsevg.upc.es/freeling/
975
CS has a lot to do with the speaker?s preferences,
the topic being discussed, and the background of the
participants involved. Using the standard approach
for measuring performance of classifiers can be mis-
leading, especially if the reference data set is small
and/or has only a small number of speakers. It is un-
realistic to just consider F-measure, or accuracy, as
truthfully reflecting how well the learners generalize
to the task. Therefore, we evaluated the classifier?s
performance using two different criteria, which are
discussed in the next sections.
4 Evaluation 1: Using the Reference Data
Set
This is the standard evaluation of machine learning
classifiers. We randomly divided the data into sen-
tences and grouped them into 10 subsets to perform
a cross-validation. Tables 2 and 3 show results for
Naive Bayes (NB) and Value Feature Interval (VFI)
(Demiroz and Guvenir, 1997). Using WEKA (Wit-
ten and Frank, 1999), we experimented with differ-
ent subsets of the attributes and two context win-
dows: using only the preceding word and using the
previous two words. The results presented here are
overall averages of 10-fold cross validation. We also
report standard deviations. It should be noted that
the Spanglish data set is highly imbalanced, around
96% of the instances belong to the negative class.
Therefore, our comparisons are based on Precision,
Recall, and F-measure, leaving accuracy aside, since
a weak classifier predicting that all instances belong
to the negative class will reach an accuracy of 96%.
The performance measures shown on Tables 2 and
3 show that NB outperforms VFI in most of the con-
figurations tested. In particular, NB yields the best
results when using a 1 word context with no lexical
forms nor lemmas as attributes (see Table 2 row 3).
This is a fortunate finding ?for most practical prob-
lems there will always be words in the test set that
have not been observed in the training set. For our
small Spanglish data set that will certainly be the
case. In contrast, VFI achieves higher F-measures
when using a context of two words and all the fea-
tures are used.
Analyzing the predictions of the learners we noted
that the NB classifier is heavily biased by the lan-
guage attribute, close to 80% of the positive predic-
tions made by NB are after seeing a word in Span-
ish. This preference seems to support the assump-
tion of the asymmetry between the two languages
and the existence of an ML4. This however is not
the case for VFI, only a little over 50% of the posi-
tive predictions belong to this scenario. Another in-
teresting finding is the learner?s tendency to predict
a code-switch after observing words like ?Yeah?,
?anyway?, ?no?, and ?shower?. The first two seem
to fit the pattern of idiomatic expressions. Accord-
ing to Montes-Alcala? this type of CS includes lin-
guistic routines and fillers that are difficult to trans-
late accurately (Montes-Alcala?, 2007), which might
be the case of ?anyway?, and unconscious changes,
which can explain the case of ?Yeah?. The case
of ?shower? and ?no? are more difficult to explain,
they might be overfitting patterns from the learners.
We also found out that VFI learned to predict that
a CS will take place right after seeing the sequence
of words le dije (I said). This sequence of words is
frequently used when the speaker is about to quote
his/herself, and this quotation is one of the well-
documented CS functions (Montes-Alcala?, 2007).
A greedy search approach for attribute selection
using WEKA showed that out of the 20 attributes
(when using a two word context), the subset with
the highest predictive value included the language
identification for word wn?1 and wn?2, the confi-
dence threshold from the English tagger for word
wn?2, the lemma from the Spanish Tree tagger for
wn?1, and the lexical form of the word wn?1. We
expected the chunk information to be useful and this
does not seem to be the case. Another unexpected
outcome is that higher F-measures are reached by
adding features generated by the monolingual Tree
taggers. Even though these features are noisy, they
still carry useful information.
We only show results from NB and VFI. Initial
experiments with a subset of the data showed that
these algorithms were the most promising for this
task. They both yielded higher F-measures, even
when compared against Support Vector Machines
(SVMs), C4.5, and neural networks. On this ex-
periment all the discriminative classifiers reached
a classification accuracy close to 96%, but an F-
4We remind the reader that in this paper ML stands for Ma-
trix Language.
976
Features Used
English Spanish Naive Bayes
Word Lang POS BIO Tree tagger Tree tagger
C Form id tag chunk POS Prob Lem POS Prob Lem P R F1
1 X X X 0.09(0.01) 0.01(0.00) 0.02(0.00)
1 X X X X 0.23(0.01) 0.32(0.02) 0.27(0.02)
1* X X X X X X X 0.19(0.00) 0.53(0.00) 0.28(0.00)
1 X X X X X X X X X X 0.18(0.00) 0.59(0.00) 0.27(0.00)
2 X X X 0.13(0.00) 0.35(0.00) 0.19(0.00)
2 X X X X 0.16(0.00) 0.46(0.00) 0.23(0.00)
2 X X X X X X X 0.14(0.00) 0.55(0.01) 0.23(0.00)
2 X X X X X X X X X X 0.16(0.00) 0.59(0.01) 0.25(0.00)
Table 2: Prediction results of CS points with NB using different features. Column C indicates the size of the context
used, 1 indicates a 1 word context, and 2 indicates two words preceding the word boundary. Columns P, R, and
F1, show precision, recall, and F-measure, respectively. Numbers in parenthesis show standard deviations. The row
marked with a ?*? shows the configuration used for the generation of CS sentences presented in Section 5.
measure on the positive class of around 0%. NB
and VFI estimate predictions for each class sepa-
rately, which makes them robust to imbalanced data
sets. In addition, generative models are known to
be better for smaller data sets since they reach their
higher asymptotic error much faster than discrimi-
native models (Ng and Jordan, 2002). This might
explain why Naive Bayes outperformed strong clas-
sifiers such as SVMs by a large margin.
The overall prediction performance is not very
high. However, we should remark that for this par-
ticular task expecting a high F-measure is unrealis-
tic. Consider for example, a case where the learners
predict a CS point where the speaker decided not to
switch, this does not imply that particular point is
not a good CS point. And similarly, if the classifier
missed an existing CS point in the reference data set
the resulting sentence might still be grammatical and
natural sounding. This motivated the use of an alter-
native evaluation, which we discuss below.
5 Evaluation 2: Using Human Evaluators
The goal of this evaluation is to explore how humans
perceive our automatically generated CS sentences,
and in particular, how do they compare to the orig-
inal sentences and to the randomly generated ones.
We selected 30 spontaneous and naturally occurring
CS sentences from different sources. Some of them
were selected from the Spanglish Times Magazine5,
some others from blogs found in (Montes-Alcala?,
2007). Other sentences were taken from a paper
discussing CS on e-mails (Montes-Alcala?, 2005).
All of the sentences are true occurrences of writ-
ten CS, from speakers different from the ones in the
Spanglish data set. The sentences were translated
to standard English and Spanish and were manually
aligned. We will use this parallel set of sentences
to predict CS points with our models. Based on the
model predictions we will generate code-switched
sentences by combining monolingual fragments.
It should be noted that the Spanglish data set is
a transcription of spoken CS. In contrast, this new
evaluation set contains only written CS. Recent stud-
ies suggest written CS will adhere to the rules of
spoken CS (Montes-Alcala?, 2005), but there is still
some controversy on this issue. From our perspec-
tive, both samples come from informal conversa-
tional interactions. It is expected that both will have
similar patterns and therefore will provide a good
source for our evaluation.
5.1 Automatically Generated Code-Switching
Sentences
In this subsection we describe how to generate code-
switched sentences randomly and with the learned
models described in the previous sections. For the
5http://www.spanglishtimes.com/
977
Features Used
English Spanish Voting Feature Intervals
Word Lang POS BIO Tree tagger Tree tagger
C Form id tag chunk POS Prob Lem POS Prob Lem P R F1
1 X X X 0.12(0.00) 0.68(0.00) 0.21(0.00)
1 X X X X 0.12(0.00) 0.65(0.01) 0.20(0.00)
1* X X X X X X X 0.12(0.00) 0.72(0.01) 0.21(0.00)
1 X X X X X X X X X X 0.13(0.00) 0.65(0.00) 0.22(0.00)
2 X X X 0.13(0.00) 0.60(0.00) 0.21(0.00)
2 X X X X 0.15(0.00) 0.52(0.01) 0.23(0.00)
2 X X X X X X X 0.13(0.00) 0.68(0.00) 0.22(0.00)
2 X X X X X X X X X X 0.15(0.00) 0.51(0.00) 0.24(0.00)
Table 3: Prediction results of CS points with VFI using different features. The notation on this table is the same as in
Table 2
classifier-based approach, we POS tagged each par-
allel set of sentences, with the monolingual English
and Spanish Tree Taggers, and we extracted the
same set of features described shown in Table 1. We
decided to train the models with a context size of
one word, even though both learners reached higher
F-measures when using a two-word context. This
decision was based on the observation that having a
two-word context will pose restrictions on possible
CS points, since we would not be able to switch un-
less we have inserted into the sentence at least two
tokens from the same language.
We trained the NB and VFI models with the Span-
glish data set (using features 2?6, 8, and 9, see Ta-
ble 1) and generated CS predictions for each paral-
lel file. A code-switched sentence is generated by
adding the first token of the sentence in language 1
(L1), and continue adding more tokens from L1 until
a CS point is found. When a CS prediction is found,
the following tokens are selected from the second
language (L2), and we continue adding tokens from
L2 until the classifier has predicted a change. Differ-
ent versions of the sentences are generated by chang-
ing the definition of L1 and L2.
For the randomly generated CS sentences, switch-
ing decisions are made randomly with a probability
proportional to the positive predictions made by the
classifiers (in this case NB). That is, for the Spanish
sentences switch points are predicted randomly with
a 30% chance of switching while for English switch
points are predicted with a 10% chance.
Generator Average Score
Human 3.64
NB 3.33
Random 2.68
VFI 2.50
Table 4: Average score of 18 judges over the set of 28
code-switched sentences rated.
In total we generated 180 CS sentences: 30 sen-
tences per generator scheme (we have three genera-
tors: NB, VFI, and random), and two versions from
each generator corresponding to the two possible
configurations of L1-L2 (Spanish-English, English-
Spanish). We noticed that in some cases same sen-
tences are generated by different methods and some-
times there are no switches. We narrowed down the
sentences by randomly choosing the combination of
L1-L2 for each generator. This reduced the num-
ber of sentences from having 6 versions, to having
only 3 versions of each sentence. From the resulting
30 sets, we removed 2 sets because one or more of
the generator schemes produced a monolingual sen-
tence. Therefore, we used 28 sets for human evalua-
tions.
5.2 Human Evaluation Results
We had a total of 18 subjects participating in the ex-
periment. All of them identified themselves as be-
ing able to read and write Spanish and English, and
the majority of them said to have used CS at least
978
some times. We showed to the human subjects the
28 sets of sentences. This time we included the orig-
inal version of the sentence. Therefore, each judge
was given 4 versions of each of the 28 code-switched
sentences: the one generated from NB predictions,
the one from VFI, the randomly generated, and the
original one. Then we asked them to rate each sen-
tence with a number from 1 to 5 indicating how nat-
ural and human-like the sentence sounds. A rating
of 5 means that they strongly agree, 4 means they
agree, 3 not sure, 2 disagree, 1 strongly disagree.
The average results are presented in Table 4. The
sentences generated by NB were scored consider-
ably higher than those from VFI and random, and
closer to the human sentences. According to the
paired t-test the difference between the NB score and
the random one is significant (p=0.01). However the
average score for VFI is lower than random. More
experiments are needed to see if by choosing the set-
ting where VFI had the highest F-measure would
make a difference in this respect. Overall the sub-
jects rated the human-generated CS sentences lower
than what we were expecting, although it is clear that
they consider these sentences more natural sound-
ing than the rest. This low rating might be related to
the attitude several evaluators expressed toward CS.
In the evaluation form we asked the judges to ex-
press their opinion on CS and several of them indi-
cated feelings along the lines of ?we shouldn?t code-
switch?.
There are several ways in which two parallel sen-
tences can be combined in CS, and possibly several
will sound natural, but from our results, it is clear
that the NB algorithm was indeed able to generate
a human-like CS behavior that was successfully dif-
ferentiated from randomly-generated sentences.
By looking at the set of automatically generated
code-switched sentences, we realized that the ma-
jority of the sentences are grammatical and natural
sounding. We believe that for a large number of the
sentences it would be hard for a human to distin-
guish the sentences that were automatically gener-
ated from the human-generated ones. One of the
give away clues is when a multi-word expression
is CS, or a tag line. Table 5 shows three examples
from the sentences evaluated. In the table there is
an example in sentence 1c where the noun phrase is
code-switched, the sentence is grammatical accord-
ing to Spanish rules, but it sounds very odd to have
the noun carta followed by the adjective in English,
?astrological?. Other interesting features are present
in example 3 where for the same noun phrase ?pro-
duce section? we have both, the female marking de-
terminer la and the masculine el. The same thing
happens for the noun phrase ?check-out line?. We
would need to have a larger occurrence of these in-
stances in our test set to determine if on average one
form is preferred over the other.
In another experiment, we measured the predic-
tion performance of NB and VFI on the 30 code-
switched sentences used in this part of the evalua-
tion. The best results, an F-measure of 0.418, were
achieved by NB when a context of 1 word was used,
and no words, nor lemmas were included as features.
This is the same setting used for the generation pro-
cess. In contrast, VFI reached an F-measure of 0.351
on this same setting. 30 sentences represent a very
small dataset but the results are very promising since
the speakers are different in the training and testing
dataset. Moreover, these results support the claim
that written and spoken CS obey similar rules.
6 Related Work
There is little prior work on computational linguis-
tic approaches to code-switched discourse. Most
of the previous work includes formalisms to pars-
ing and generating mixed sentences, for example for
Marathi and English (Joshi, 1982), or Hindi and En-
glish (Goyal et al, 2003). Sankoff proposed a pro-
duction model of bilingual discourse that accounts
for the equivalence constraint and the unpredictabil-
ity of code-switching (Sankoff, 1998a). His real-
time production model draws on the alternation of
fragments from two virtual monolingual sentences.
But no statistical assessment has been conducted on
real corpora.
Another related work deals with language iden-
tification on English-Maltese code-switched SMS
messages (Rosner and Farrugia, 2007). What the au-
thors found to work best for language identification
in this noisy domain is a combination of a bigram
Hidden Markov Model, trained on language tran-
sitions, and a trigram character Markov Model for
handling unknown words.
979
1a. Naive Bayes:
By unlocking the information in your astrological chart, puedo ver la respuesta! Ask me!
1b. VFI:
Puedo ver la answer by unlocking the information in your carta astrolo?gica! Ask me !
1c. Random:
By unlocking the information de tu carta astrological, I can see the answer! Ask me !
1d. Human:
By unlocking the information in your astrological chart, puedo ver the answer! Pregu?ntame!
1e. English version:
By unlocking the information in your astrological chart, I can see the answer! Ask me!
2a. Naive Bayes:
Pero siendo this a new year, es tiempo de empezar de nuevo que no?
2b. VFI:
But this being a new year, es tiempo de empezar over isn?t it ?
2c. Random:
But this being a new an?o, it?s tiempo to start over isn?t it?
2d. Human:
Pero this being a new year, it?s a time to start over que no?
2e. English version:
But this being a new year, it?s time to start over isn?t it?
3a. Naive Bayes:
Juan confirmed me that it was very obvious, y no solamente en el produce section, en la check-out line as well.
3b. VFI:
Me confirmo? Juan que it was very obvious, y no solamente en el produce section, tambie?n en la check-out line.
3c. Random:
Juan confirmed que fue very obvious, y not solamente en el a?rea de produce, in the check-out line as well.
3d. Human:
Me confirmo? Juan que fue muy obvio, y no solamente en la produce section, tambie?n en el check-out line.
3e. English version:
Juan confirmed me that it was very obvious, and not only on the produce section, in the check-out line as well.
Table 5: Examples of automatically generated CS sentences.
7 Conclusions
We presented preliminary results on learning to pre-
dict CS points with machine learning. One of the
possible applications of our method involves fine-
tuning the weights in a multilingual language model,
for instance, as part of a speech recognizer for Span-
glish. With this in mind, we restricted the possible
features in the learning scenario allowing only lexi-
cal and syntactic features that could be automatically
generated from the text. Empirical evaluations on
a Spanglish conversation showed that Naive Bayes
and VFI can predict with acceptable F-measures
possible CS points, considering the difficulty of the
task. Prediction of CS points can help improve mul-
tilingual language models.
Evaluation of our approach cannot be done based
only on the gold-standard set since there is no sin-
gle right answer in this task. Therefore, we comple-
mented the evaluation by involving judgements from
bilingual speakers. We generated CS sentences by
taking the predictions from the classifiers to merge
parallel sentences. On average, the sentences gen-
erated from the NB model were rated closer to the
original sentences, and a lot higher than the ones
from a random generator. Most of the sentences
sounded human-like. But because the process is au-
tomatic we did find some awkward constructions,
for example plural vs singular noun-verb agreement,
or multi-word phrases that were code-switched in
the middle. Perhaps a multi-word recognition fea-
ture could improve results.
One of the advantages of technological develop-
ment and economic globalization is that more peo-
ple from different regions of the world with differ-
ent cultures, and therefore, different languages will
980
be in closer contact. As a result, code-switching will
become more popular. It is important to start ad-
dressing this type of bilingual communication from
a computational linguistics point of view. This work
is one of the few attempts to fill the gap.
Some directions for future work include: explor-
ing the extent to which our results can be improved
by including a multi-word expression recognition
system. We also want to investigate the integration
of our approach to multilingual language models and
move beyond CS to address other deeper linguistic
phenomena. Lastly, we would like to explore similar
approaches in other popular language combinations.
Acknowledgements
This research is supported by the National Science
Foundation under grant 0812134. We are grateful to
Ray Mooney, Melissa Sherman and the three anony-
mous reviewers for insightful comments and sug-
gestions. Special thanks to the human judges that
helped with the sentence evaluations.
References
G. Demiroz and H. A. Guvenir. 1997. Classification by
voting feature intervals. In European Conference on
Machine Learning, ECML-97, pages 85?92.
P. Goyal, Manav R. Mital, A. Mukerjee, Achla M. Raina,
D. Sharma, P. Shukla, and K. Vikram. 2003. A
bilingual parser for Hindi, English and code-switching
structures. In Computational Linguistics for South
Asian Languages ?Expanding Synergies with Europe,
EACL-2003 Workshop, Budapest, Hungary.
A. Joshi. 1982. Processing of sentences with intrasenten-
tial code-switching. In Ja?n Horecky?, editor, COLING-
82, pages 145?150, Prague, July.
J. Lipski. 1978. Code-switching and the problem of
bilingual competence. In M. Paradis, editor, Aspects
of bilingualism, pages 250?264. Hornbeam.
C. Montes-Alcala?. 2005. Ma?ndame un e-mail: cam-
bio de co?digos espan?ol-ingle?s online. In Luis Ortiz
and Manel Lacorte, editors, In Contacto y contextos
lingu???sticos: El espan?ol en los Estados Unidos y en
contacto con otras lenguas. Iberoamericana/Vervuert.
C. Montes-Alcala?. 2007. Blogging in two languages:
Code-switching in bilingual blogs. In Jonathan
Holmquist, Augusto Lorenzino, and Lotfi Sayahi, edi-
tors, In Selected Proc. of the Third Workshop on Span-
ish Sociolinguistics, pages 162?170, Somerville, MA.
Cascadilla Proceedings Project.
C. Myers-Scotton. 1997. Duelling Languages: Gram-
matical Structure in Codeswitching. Oxford Univer-
sity Press, 2nd edition.
A. Ng and M. Jordan. 2002. On discriminative vs. gen-
erative classifiers: A comparison of logistic regression
and Naive Bayes. In Advances in Neural Information
Processing Systems (NIPS) 15. MIT Press.
S. Poplack. 1980. Sometimes I?ll start a sentence in
Spanish y termino en espan?ol: toward a typology of
code-switching. Linguistics, 18(7/8):581?618.
M. Rosner and P. J. Farrugia. 2007. A tagging algorithm
for mixed language identification in a noisy domain.
In INTERSPEECH 2007, pages 190?193, Antwerp,
Belguim, August.
D. Sankoff. 1998a. A formal production-based expla-
nation of the facts of code-switching. Bilingualism,
Language and Cognition, (1):39?50.
D. Sankoff. 1998b. The production of code-mixed dis-
course. In 36th ACL, volume I, pages 8?21, Montreal,
Quebec, Canada, August.
H. Schmid. 1994. Probabilistic part-of-speech tagging
using decision trees. In International Conference on
New Methods in Language Processing, September.
T. Solorio and Y. Liu. 2008. Part-of-speech tagging
for English-Spanish code-switched text. In EMNLP-
2008, Honolulu, Hawai, October.
I. H. Witten and E. Frank. 1999. Data Mining, Practi-
cal Machine Learning Tools and Techniques with Java
Implementations. Morgan Kaufmann.
981
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 1051?1060,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Part-of-Speech Tagging for English-Spanish Code-Switched Text
Thamar Solorio and Yang Liu
Human Language Technology Research Institute
The University of Texas at Dallas
Richardson, TX 75080, USA
tsolorio,yangl@hlt.utdallas.edu
Abstract
Code-switching is an interesting linguistic
phenomenon commonly observed in highly
bilingual communities. It consists of mixing
languages in the same conversational event.
This paper presents results on Part-of-Speech
tagging Spanish-English code-switched dis-
course. We explore different approaches to
exploit existing resources for both languages
that range from simple heuristics, to language
identification, to machine learning. The best
results are achieved by training a machine
learning algorithm with features that combine
the output of an English and a Spanish Part-
of-Speech tagger.
1 Introduction
Worldwide the percentage of bilingual speakers is
fairly large, and it keeps increasing at a high rate.
In the U.S., 18% of the total population speaks a
language other than English at home, the major-
ity of which speaks Spanish (U.S. Census Bureau,
2003). A significant percentage of this Spanish-
English bilingual population code-switch between
the two languages in what is often referred as Span-
glish, the mix of Spanish and English. Spanish
and English are not the only occurrence of language
mixtures. Examples of other popular combinations
include Arabic dialects, French and German, Span-
ish and Catalan, Maltese and English, and English
and French. Typically when there are linguistic bor-
ders, or when the country has more than one official
language, we can find instances of code-switching.
Despite the wide use of code-switched discourse
among bilinguals, this linguistic phenomenon has
received little attention in the fields of Natural Lan-
guage Processing and Computational Linguistics.
Part-of-Speech (POS) tagging is a well studied prob-
lem in these fields. For languages such as English,
German, Spanish, and Chinese there are several dif-
ferent POS taggers that reach high accuracies, espe-
cially in news text corpora. However, to our knowl-
edge, there is no previous work on developing a POS
tagger for text with mixes of languages.
In this paper we present results on the problem
of POS tagging English-Spanish code-switched dis-
course by taking advantage of existing taggers for
both languages. This rationale follows the evi-
dence from studies of code-switching on different
language pairs, which have shown code-switching
to be grammatical according to both languages be-
ing switched. We use different heuristics to combine
POS tag information from existing monolingual tag-
gers. We also explore the use of different language
identification methods to select POS tags from the
appropriate monolingual tagger. However, the best
results are achieved by a machine learning approach
using features generated by the monolingual POS
taggers.
The next section presents the facts about code-
switching, including some previous work done
mainly in linguistics. Then in Section 3 we dis-
cuss previous work related to the automated pro-
cessing of code-switched discourse. In Section 4
we describe the English-Spanish code-switched data
set gathered for the experimental evaluation. Sec-
tion 5 presents the heuristics-based approaches for
POS tagging that we explored. In Section 6 we
describe our machine learning approach and show
1051
results on POS tagging code-switched text. An in
depth analysis of results is presented in Section 7,
and we conclude this paper with a summary of the
findings and directions for future work in Section 8.
2 Rules of Code-switching
In the linguistic, sociolinguistic, psychology, and
psycholinguistic literature, bilingualism and the in-
herent phenomena it exhibits have been studied
for nearly a century (Espinosa, 1917; Ervin and
Osgood, 1954; Gumperz, 1964; Gumperz and
Hernandez-Chavez, 1971; Gumperz, 1971; Sankoff,
1968; Lipski, 1978). Despite the numerous previ-
ous studies of linguistic characteristics of bilingual-
ism, there is no clear consensus on the terminol-
ogy related to language alternation patterns in bilin-
gual speakers. The alternation of languages within
a sentence is known as code-mixing, but it has
also been referred as intrasentential code-switching,
and intrasentential alternation (Poplack, 1980; Gros-
jean, 1982; Ardila, 2005). Alternation across sen-
tence boundaries is known as intersentential code-
switching, or just code-switching. In the rest of this
paper we will refer to the mixing of languages as
code-switching. When necessary, we will differen-
tiate the type of code-switching by referring to al-
ternations within sentences as intrasentential code-
switching and alternations across sentence bound-
aries as intersentential code-switching.
Linguistic phenomena in bilingual speakers have
been analyzed on different language pairs, includ-
ing English-French, English-Dutch, Finish-English,
Arabic-French, and Spanish-English, to name a few.
There is a general agreement that code-switched pat-
terns are not generated randomly; according to these
studies, they follow specific grammatical rules. Fur-
thermore, some studies suggest that, if these rules
are violated, the resulting discourse will sound un-
natural (Toribio, 2001b; Toribio, 2001a). The fol-
lowing shows the rules governing code-switching
discourse described in several studies (Poplack,
1980; Poplack, 1981; Sankoff, 1981; Sankoff,
1998a).
? Switches can take place only between full word
boundaries. This is also known as the free mor-
pheme constraint.
? Monolingual constructs within the sentence
will follow the grammatical rules of the mono-
lingual fragment.
? Permissible switch points are those that do not
violate the order of adjacent constituents on
both sides of the switch point of either of the
languages. This is called the equivalence con-
straint.
Although these rules are somewhat controversial,
and most of the studies on this area have been con-
ducted on small samples, we cannot ignore the fact
that patterns bearing the above rules have emerged in
different bilingual communities with different back-
grounds.
3 Automated Processing of Code-Switched
Discourse
A previous work related to the processing of code-
switched text deals with language identification
on English-Maltese code-switched SMS messages
(Rosner and Farrugia, 2007). In addition to deal-
ing with intrasentential code-switching, they have
to deal with text where misspellings and ad hoc
word contractions abound. What Rosner and Far-
rigua have found to work best for language identi-
fication in this noisy domain is a combination of a
bigram Hidden Markov Model, trained on language
transitions, and a trigram character Markov Model
for handling unknown words. In another related
work, Franco and Solorio present preliminary results
on training a language model for Spanish-English
code-switched text (Franco and Solorio, 2007). To
evaluate their language model, they asked a human
subject to judge sentences generated by a PCFG in-
duced from training data and the language model.
However, they only used one human judge.
Regarding the automated POS tagging and pars-
ing of code-mixed utterances there is little prior
work. To the best of our knowledge, there is no
parser, nor POS tagger, currently available for the
syntactic analysis of this type of discourse. There
are theoretical approaches that propose formalisms
to represent the structure of code-switched utter-
ances and describe a framework for parsing and gen-
erating mixed sentences, for example for Marathi
and English (Joshi, 1982), or Hindi and English
(Goyal et al, 2003). Sankoff proposed a production
model of bilingual discourse that accounts for the
1052
equivalence constraint and the unpredictability of
code-switching (Sankoff, 1998a; Sankoff, 1998b).
His real-time production model draws on the alter-
nation of fragments from two virtual monolingual
sentences. It also accounts for other types of code-
switching such as repetition-translation and inser-
tional code-switching. But no statistical assessment
has been conducted on real corpora.
Our goal is to develop a POS tagger for code-
switched utterances, which is the first step of the
syntactic analysis of any language. Among the chal-
lenges we face is the lack of a representative sam-
ple of code-mixed discourse. Most POS taggers are
built using large collections, usually at least a mil-
lion words, such as the Brown corpus (Kucera and
Francis, 1967), the Wall Street Journal corpus (Paul
and Baker, 1992), or the Switchboard corpus (God-
frey et al, 1992). Currently, there is no annotation
of code-switched text of comparable size. But in
contrast to the lack of linguistic resources available
for Spanish-English code-mixed discourse, English
and Spanish have sufficient resources, especially En-
glish. Thus, rather than starting from scratch, we
will draw on existing taggers for both languages,
which will reduce the amount of code-switched data
needed. Some examples of POS taggers that per-
form reasonably well on monolingual text of each
language can be found in (Brants, 2000; Brill, 1992;
Carreras and Padro?, 2002; Charniak, 1993; Ratna-
parkhi, 1996; Schmid, 1994). However, these tools
are designed to work on monolingual text, therefore
if applied as they are to code-switched text, their ac-
curacy will decrease by a large margin. In the fol-
lowing sections we will explore different methods
for combining monolingual taggers.
4 Data Set
Data collections that have instances of Spanish-
English code-switching, Spanglish for short, are not
easily found since code-switching is primarily used
in spoken form. To gather data we recorded a con-
versation among three staff members of a southwest
university in the U.S. The three speakers come from
a highly bilingual background and code-switch reg-
ularly when speaking among themselves, or other
bilingual speakers.
This recording session has around 39 minutes of
Table 1: Excerpts taken from the Spanglish data set.
Spanglish English Translation
(a)Entonces le dio? el
virus y no se lo atendio?
and the virus spread
through his body.
(a)Then he got the
virus and he didn?t re-
ceive treatment and the
virus spread through
his body.
(b)Cuando yo lo vi he
looked pretty bad.
(b)When I saw him he
looked pretty bad.
(c)I think she was
taller than he was.
(c)I think she was taller
than he was.
Y un cara?cter muy
bonito tambie?n ella.
And a very nice char-
acter she as well.
Very easy going. Very easy going.
continuous speech (922 sentences, about 8k words)
and was transcribed and annotated with POS tags by
a human annotator. The annotations were later re-
vised by a different annotator but no inter-annotator
agreement was measured. The POS tag set used in
the annotation is the combination of the tag sets from
the English and the Spanish Tree Taggers (see Sec-
tion 5). The vocabulary of the transcription has a to-
tal of 1,516 different word forms1. In the conversa-
tion a total of 239 switches were identified manually,
out of which 129 are intrasentential code-switches,
and the rest are intersentential. English is the pre-
dominant language used, with a total of 6,020 tokens
and 576 monolingual sentences. In contrast, the
transcription has close to 2k tokens in Spanish. Ta-
ble 1 shows examples of code-switching taken from
the recorded conversation; (a) and (b) are instances
of intrasentential code-switching, and (c) shows in-
tersentential code-switching.
5 Rule-based Methods for Exploiting
Existing Resources
In this section we present several heuristics-based
methods for POS tagging code-switched text. First,
we describe the monolingual taggers used in this
work. Then we present the different approaches ex-
plored and contrast their performance.
1This transcription and the audio file are freely available for
research purposes by contacting the first author.
1053
5.1 Monolingual Taggers
We used the Tree Tagger (Schmid, 1994) for this
work because of the following considerations:
1. It has both English and Spanish versions. The
English tagger uses a slightly modified version of
the Penn Treebank tag set and was trained and eval-
uated on different portions of the Penn Treebank,
reaching a POS tagging accuracy of 96.36%. The
Spanish one uses a different tagset with 75 different
POS tags2 and was trained on the Spanish CRATER
corpus.
2. The transition probabilities are estimated using a
modified version of the ID3 decision tree algorithm
(Quinlan, 1986), which provides more freedom to
learn contextual cues than n-grams.
3. Both taggers include a special tag for foreign
words, PE for Spanish and FW for English. We do
not expect this tag to identify correctly all foreign
words, but when available this information will be
exploited.
4. The Tree tagger generates probability estimates
on the tags that can be used as features.
5. Finally, when the tagger fails to lemmatize a
word it outputs the special token ?unknown?. This
information can be used as a hint of words that do
not belong to that particular language.
5.2 Heuristic-based Systems
For all heuristics the complete Spanglish data set
was given to both taggers as a single text, then the
final tag for each word was selected from the output
of the taggers according to the different heuristics.
Table 2 shows the tagging accuracies of the different
heuristics we explored, which are explained below.
1. Using the monolingual tagger. Here we simply
give the Spanglish text to the Spanish and the
English tree tagger. We expect from both taggers a
performance degradation due to the inclusion of for-
eign words in code-switching, as compared against
their accuracy on monolingual texts. Another
complicating factor to keep in mind is that we are
dealing with spoken language. Hesitations, fillers,
disfluencies, and interruption points, such as Umm,
Mmmhmm, and Uh-huh, are frequently observed in
2The authors were unable to identify the source of the Span-
ish tagset.
Table 2: Accuracy on POS tagging Spanglish text using
simple heuristics for combining the output of the English
and Spanish tagger.
Heuristic Accuracy (%)
1 Spanish Tree Tagger 25.99
English Tree Tagger 54.59
2 Highest prob tag or English 51.51
Highest prob tag or Spanish 49.16
3 Prob + special tags + lemmas 64.27
4 Dictionary-based Language Id 86.03
Character 5-grams Language Id 81.46
Human Language Id 89.72
speech and it is well known that they complicate
the POS tagging task. The tagging accuracy from
using the individual taggers is rather low, 26% for
the Spanish tagger and 54% for the English one.
The large difference between the two taggers can be
attributed to the fact that the majority of the words
in the corpus are in English.
2. Using confidence thresholds. The Tree Tagger
can output probabilities for each tag, showing the
confidence of the tagger on each particular tag. To
use this information we choose for each word the tag
from the tagger with the highest confidence. When
there is a tie we use either the English or the Spanish
tag. Table 2 shows the results for the two cases. The
?Highest prob tag or English? heuristic gives an ac-
curacy of 51%, which is almost as accurate as using
only the English tagger. The ?Highest prob tag or
Spanish? achieves an accuracy of 49%, which is an
improvement over using only the monolingual Span-
ish tagger, but it is still below the accuracy of the En-
glish monolingual tagger. This is also possibly due
to the task being easier for the English tagger.
3. Combining confidence thresholds with knowl-
edge from special tags and lemmas. This heuristic
uses confidence thresholds combined with decisions
based on the special tags, described in Section 5.1,
and the unknown lemmas found. Let POSE(wi)
and POSS(wi) be the POS tags assigned to word
wi by the English and Spanish tagger respectively;
and let ProbE(wi) and ProbS(wi) be the confi-
dence scores of POS tags for word wi computed by
the English and Spanish tree taggers, respectively.
For each word wi in the text, the final POS tag,
1054
POSF (wi), will be assigned as follows:
1. If POSE(wi) = FW , then POSF (wi) ?
POSS(wi)
2. Else if POSS(wi) = PE, then POSF (wi) ?
POSE(wi)
3. Else if POSE(wi) = ?unknown?, then
POSF (wi)? POSS(wi)
4. Else if POSS(wi) = ?unknown?, then
POSF (wi)? POSE(wi)
5. Else if ProbE(wi) > ProbS(wi), then
POSF (wi)? POSE(wi)
6. Else POSF (wi)? POSS(wi)
This heuristic performs better than the other meth-
ods explored so far, yielding an accuracy of 64.27%.
It seems that knowledge of the taggers can be used
to improve results. However, POS tagging accuracy
is still poor.
4. Selecting POS tags based on automated language
identification. We used two different strategies for
automatically identifying the language at the word
level. One is based on dictionary look-up and the
other is character-based language models. For the
first approach, every word in the text is searched in
the English and Spanish dictionaries. If a word is
found in the English dictionary, then we identify that
word as belonging to English and the POS tags from
the English tagger are used for that word and the
following ones, until a word is found in the Span-
ish dictionary. Similarly, for a word not found in
the English dictionary, but found in the Spanish dic-
tionary, we use the Spanish tags until an English
word is found. Note that this simple heuristic will
always label words that belong to both languages as
English, which is also the case for words not found
in either dictionary. This dictionary-based method
has a language identification accuracy of 94% on the
Spanglish corpus.
The character language models were trained on
the Agence France Presse (AFP) portions of the Gi-
gaword for English and Spanish, respectively. For
each of the words in the Spanglish corpus, we first
decide its language by choosing the one with the
lowest perplexity, calculated using character n-gram
language models, then we use the corresponding
POS tag. We experimented with different language
model orders, with n ranging from 2 to 6, and found
that we achieve the highest accuracy, 81.46%, on
POS tagging using a 5-gram language model. This
5-gram method reached a language identification ac-
curacy of 85% for the Spanglish corpus. However,
the language identification method using dictionary
look-up achieved the best POS tagging result so far:
86.03%. The Spanglish conversation is dominated
by every-day language that is easily found in dic-
tionaries, while the text used to train the charac-
ter based n-gram language models includes vocab-
ulary that is not commonly used in conversations.
This can explain why the simple dictionary look-
up approach yielded better results for our corpus.
Performing manual identification of the language
and sending to the appropriate tagger just the corre-
sponding fragments yields a very high POS tagging
accuracy, 89.72%. This shows that it is important
to deal with the language switches for boosting ac-
curacy. However relying on human annotated lan-
guage tags would be expensive and for some tasks
unfeasible.
6 Machine Learning for POS Tagging
Code-Switched Discourse
From Table 2 we can see that, with the exception of
the language identification heuristic, accuracies are
low for the previous experiments. However, we be-
lieve that we can improve results further by using
Machine Learning (ML) algorithms trained specif-
ically for this task. In this section we describe the
ML setting and present a comparison of the differ-
ent algorithms we tested.
6.1 Approach
The key point is that the features selected for de-
scribing the learning instances are the output from
the English and the Spanish taggers. This scheme
is similar to a stacked classifier approach (Wolpert,
1992), where the final classifier takes as input the
predictions made by the different learners on the first
pass and is trained to select the right tag from them,
or a different one if the right answer is not available.
The gold-standard POS tags are used as the
class label, and instances in this learning task are
described by the following attributes:
1055
1. The word (word)
2. English POS tag (Et)
3. English POS tagger lemma (El)
4. English POS tagger confidence (Ep)
5. Spanish POS tag (St)
6. Spanish POS tagger lemma (Sl)
7. Spanish POS tagger confidence (Sp)
Feature 1 is just the lexical word form as it ap-
pears in the transcript. Features 2 to 4 are generated
by the English Tree tagger, while features 5 to 7 are
generated by the Spanish Tree tagger. Thus all fea-
tures are automatically extracted.
6.2 Results
We evaluated experimentally the idea of using ML
with different learning algorithms in WEKA (Wit-
ten and Frank, 1999). We selected some of the most
widely known algorithms, including Support Vector
Machines (SVM) with a polynomial kernel of ex-
ponent one (Scho?lkopf and Smola, 2002), Weka?s
modified version of Quinlan?s C4.5 (J48) (Quinlan,
1986), Additive Logistic Regression with Decision
Stumps (Logit Boost) (Friedman et al, 1998) and
Naive Bayes. The only parameter we modified was
for J48 ?we enabled the option for reducing error
pruning.
Table 3: POS tagging accuracy of Spanglish text with
different Machine Learning algorithms. Oracle shows
the accuracy achieved when always selecting the right
POS tag from the output of both Tree Taggers. Language
Id shows accuracy of identifying the language and then
choosing the output of the corresponding tagger.
ML Algorithms Mean Accuracy (%) Variance
Naive Bayes 88.50 1.9280
SVM 93.48 1.2784
Logit Boost 93.19 1.4437
J48 91.11 2.1527
Oracle 90.31 -
Language Id 85.80 -
Table 3 shows the average accuracy of 10-fold
cross-validation for each classifier together with the
variance. SVM and Logit Boost performed the best
and the difference between the two algorithms is not
significant according to the paired t-test (P-value =
0.1). For comparison, we show the accuracy of the
10 20 30 40 50 60 70 80 90
80
82
84
86
88
90
92
94
96
98
100
Percentage of training data used
A
cc
ur
ac
y
Figure 1: Effect of different amounts of training data on
accuracy
language identification approach together with the
oracle accuracy. The oracle is the accuracy achieved
when always selecting the right POS tag, when it
is available, from the output of both Tree Taggers.
We did not expect the oracle?s accuracy to be an up-
per bound on the accuracy for the ML learning al-
gorithm. Our intuition is that the ML algorithm can
be trained to identify when the taggers have made
a mistake and what the right answer should be. As
the results show, the ML approach can indeed out-
perform the oracle, and the language identification
method.
In Figure 1 we show the effect of the amount
of training data on the accuracy using Logit Boost.
We selected Logit Boost for this and the follow-
ing experiments since its accuracy is comparable to
SVMs but it is computationally less expensive. We
randomly partitioned the transcription into 10 sub-
groups. Then we used one subgroup as the test set
and the rest for training. Starting with one subgroup
in the training set, we incrementally added one sub-
group to the training set and evaluate the tagging
performance of the test set. We repeated this pro-
cess several times, choosing randomly a new test set
each time. The percentages shown are the average
over all the experiments. With only 10% of the sen-
tences for training we are reaching very good accu-
racy already, as high as that from the strategy based
on language identification. The curve flattens after
1056
Table 4: Accuracy of Logit Boost with different subsets
of attributes. ?X? marks attributes included. Et, El, Ep,
and St, Sl, and Sp are the POS tag, lemma and confi-
dence output by the English and the Spanish POS tagger,
respectively.
word Et El Ep St Sl Sp Accuracy
X X X X ? ? ? 88.80
? X X X ? ? ? 86.22
X ? ? ? X X X 78.59
? ? ? ? X X X 65.28
X X X ? X X ? 92.95
X X ? X X ? X 92.53
X X ? ? X ? ? 91.22
X X X ? ? X ? 89.76
X ? X X ? X X 77.08
? ? X ? ? X ? 74.18
X X ? ? ? ? ? 85.76
X ? ? ? ? ? ? 71.17
? ? ? X ? ? X 24.96
X X X X X ? ? 92.55
X X X X ? ? X 88.89
X ? X ? X ? ? 78.74
X X X X ? X ? 89.62
? X ? ? X X ? 90.76
? ? X X ? X X 75.94
X ? X ? X X X 80.24
X ? ? X X X X 79.13
60% of the training data is used. We do not gain
much by adding more training data after this.
Results shown in Table 3 demonstrate that POS
tagging can be learned effectively based on the at-
tributes described in Subsection 6.1, even if we are
not explicitly adding contextual information. To de-
termine the extent to which each attribute is con-
tributing to the learning task, we performed another
set of experiments where we selected different sub-
sets of the attributes. Table 4 shows the results with
Logit Boost. Overall, the attributes taken from the
English POS tagger are more valuable for this learn-
ing task. If we only take the word form and the fea-
tures from the English Tree tagger (first row in Ta-
ble 4) we are reaching an accuracy that outperforms
all heuristics. Still, there is some valuable infor-
mation provided by the Spanish POS tagger output
since the highest accuracy is achieved by including
the Spanish-based attributes in combination with the
English-based ones. Surprisingly, we can manage to
outperform the oracle by using only three attributes:
the lexical word form and the POS tags from the
English and Spanish tagger (see row 7 in table), or
the POS tags from the monolingual taggers together
with the lemma from the Spanish tagger (see row 4
from bottom to top). We also experimented adding
as an attribute the output of the language identifica-
tion method, but found no significant changes in the
accuracy.
7 Discussion
We analyzed the different results gathered through
the experiments and we present here the most rele-
vant insights.
The first discovery, is that a lot of the errors made
by the oracle, and the other methods as well, are due
to the difficulties inherent in dealing with sponta-
neous speech where fillers, interruption points, hes-
itations, and the like abound. About as much as
20% of the errors made by the oracle are due to
these features. Another roughly 20% is due to un-
known tokens in the transcription, such as mum-
bling, slang words such as ?gonna? and ?wanna?,
or other sounds unintelligible for the human tran-
scriber. For the rest of the analysis we decided to
ignore these types of mistakes for all methods and
focus only on the remaining mistakes. In the case
of the oracle we are left with 445 erroneously POS
tagged words. From those, about 50%, or 233 to
be exact, are errors in sentences with code-switches.
We consider this to be a strong indication of the
complexity that intrasentential switches add to the
task of POS tagging. For the taggers, these sentences
are incomplete, or ill-formed, since they have frag-
ments with foreign words and thus, they fail to iden-
tify them. The rest of the oracle mistakes can not be
attributable to a single cause. Some are fragmented
sentences, and some are due to errors inherent of the
tagger, but nothing is particulary salient about them.
The language identification methods share, of
course, the same mistakes made by the oracle, plus
342 more, for a total of 787 (in the case of the
dictionary-based language identification). The chal-
lenge of POS tagging code-switched text is more ev-
ident for this method. Out of the mistakes made by
the language identification method, 540 lie in sen-
tences with code-switching, that is, nearly 70% of
the mistakes. For 307 of these mistakes the right
1057
POS tag was available from one of the taggers.
Some typical examples of these errors are words that
belong to both languages, such as ?a?, ?no?, ?me?
and ?con?.
The ML approach outperformed both the lan-
guage identification method and the oracle. Analyz-
ing the predictions made by SVM we verified that
out of the 445 errors made by the oracle, SVM cor-
rectly tagged 223, the majority of which are words in
sentences with code-switching (142 words). When
compared against the errors from the method based
on language identification, SVM correctly tagged
481 words out of the 787, 374 of which are words
in sentences with code-switches. In summary, the
ML approach is more robust to code-switched sen-
tences. Note that we did find some errors made by
the ML approach that are not shared by the oracle
or the language identification method, a total of 105.
Some of these mistakes are due to inconsistencies
on the human-annotated tags. For instance, in most
cases slang words such as ?gonna? and ?wanna? are
labeled as unknown words, but we found that these
words were labeled as verbs in a few cases. Not sur-
prisingly this caused the ML algorithm to fail, since
these class labels were misleading. The majority of
the mistakes, however, seem to be due to systematic
mistakes by the POS taggers.
One last remark is regarding our decision to find a
method for successfully exploiting the existing tag-
gers for POS tagging Spanglish text. Our origi-
nal motivation came from the lack of linguistic re-
sources to process Spanglish text. However, we did
train from scratch a sequential model for POS tag-
ging Spanglish, namely Conditional Random Fields
(CRFs) (Lafferty et al, 2001). We used MALLET
(McCallum, 2002) for this experiment and the same
training/testing partitions used in the experiment re-
ported in Table 3. The CRF POS tagger was trained
using capitalization information and the previous to-
ken as context. The average accuracy of this CRF
was 81%, which is lower than the language identifi-
cation heuristic. We believe that this low accuracy is
due to the lack of a representative sample of anno-
tated Spanglish. It will be interesting to see if when
more data becomes available the ML algorithms still
yield the best results.
8 Conclusions
Code-switching is a fresh and exciting research area
that has received little attention in the language pro-
cessing community. Research on this topic has many
interesting applications, including automatic speech
recognition, machine translation, and computer as-
sisted language learning. In this paper we present
preliminary work towards developing a POS tagger
for English-Spanish code-switched text that, to the
best of our knowledge, is the first effort towards this
end.
We explored different heuristics for taking advan-
tage of existing linguistic resources for English and
Spanish with unimpressive results. A simple word-
level language identification strategy outperformed
all heuristics tested. But the best results, even bet-
ter than the oracle, were achieved by using machine
learning using the output of monolingual POS tag-
gers as input features.
In the error analysis we showed that most of
the mistakes made by the language identification
method, and the oracle itself, occur in sentences with
intrasentential code-switching, showing the diffi-
culty of the task. In contrast, our machine learning
approach was less sensitive to the complexity of this
alternation pattern.
There is still a lot of work to do in this area. Our
ongoing efforts include gathering a larger corpus,
with different speakers and conversational styles, as
well as written forms of code-switching from blogs
and Internet forums. In addition, we are exploring
the use of context information. The features we are
currently using to represent each word do not take
into account the context surrounding the word. We
want to test if by using contextual features we can
further improve our results.
In this study we focused on code-switching, but
borrowing is another complex language alternation
pattern that we want the POS tagger to handle. We
are working on developing a special method for
identification and morphological analysis of borrow-
ings. This method will help increase the accuracy of
the POS tagger.
Spanish-English is not the only popular combi-
nation of languages. An interesting line of future
work would be to explore if the method presented
here can be adapted to different language combi-
1058
nations. Moreover, multilingual communities will
code-switch among more than two codes and this
poses fascinating research challenges as well.
Acknowledgements
We are grateful to Ray Mooney, Melissa Sher-
man and the four anonymous reviewers for insight-
ful comments and suggestions. Special thanks to
Brenda Medina and Nicolle Whitman for helping
with some experiments. This research is supported
by the National Science Foundation under grant
0812134.
References
Ardila, A. (2005) Spanglish: an anglicized dialect. His-
panic Journal of Behavioral Sciences, 27(1): 60?81.
Brants, T. (2000) TnT - a statistical part-of-speech tagger.
In Sixth Applied Natural Language Processing Confer-
ence ANLP-2000 Seatle, WA.
Brill, E. (1990) A simple rule-based part-of-speech tag-
ger. In 3rd Conference on Applied Natural Language
Processing, pp. 152?155. Trento, Italy.
Carreras, X. and Padro?, L. (2002) A flexible distributed
architecture for natural language analyzers. In Third
International Conference on Language Resources and
Evaluation, LREC-02, pp. 1813?1817. Las Palmas de
Gran Canaria, Spain.
Charniak, E. (1993) Equations for part-of-speech tag-
ging. In 11th National Conference on AI, pp. 784?789.
Ervin, S. and Osgood, C. (1954) Second language learn-
ing and bilingualism. Journal of abnormal and social
phsychology, Supplement 49, pp. 139?146.
Espinosa, A. (1917) Speech mixture in New Mexico: the
influence of English language on New Mexican Span-
ish. In H. Stevens and H. Bolton, (eds.), The Pacific
Ocean in History, pp. 408?428.
Franco, J.C. and Solorio T. (2007) Baby-steps towards
building a Spanglish language model. In 8th Interna-
tional Conference on Computational Linguistics and
Intelligent Text Processing, CICLing-2007, pp. 75?84.
Mexico City, Mexico.
Friedman, J., Hastie, T., and Tibshirani, R. (1998) Addi-
tive logistic regression: a statistical view of boosting.
Technical Report, Stanford University.
Godfrey, J., Holliman, E. and McDaniel, J. (1992)
Switchboard: Telephone speech corpus for research
development. In ICASSP, pp. 517?520, San Francisco,
CA, USA.
Goyal, P., Mital, Manav R., Mukerjee, A., Raina, Achla
M., Sharma, D., Shukla, P. and Vikram, K. (2003) A
bilingual parser for Hindi, English and code-switching
structures. In Computational Linguistics for South
Asian Languages ?Expanding Synergies with Europe,
EACL-2003 Workshop, Budapest, Hungary.
Grosjean, F. (1982) Life with Two Languages: An Intro-
duction to Bilingualism. Harvard University Press.
Gumperz, J. J. and Hernandez-Chavez, E. (1971) Cogni-
tive aspects of bilingual communication. Oxford Uni-
versity Press, London.
Gumperz, J. J. (1964) Linguistic and social interaction in
two communities. In John J. Gumperz (ed.), Language
in social groups, pp. 151?176, Stanford. Stanford Uni-
versity Press.
Gumperz, J. J. (1971) Bilingualism, bidialectism and
classroom interaction. In Language in social groups,
pp. 311?339, Stanford. Stanford University Press.
Joshi, A. K. (1982) Processing of sentences with in-
trasentential code-switching. In Ja?n Horecky? (ed.),
COLING-82, pp. 145?150, Prague.
Kucera, H. and Francis, W. N. (1967) Computational
analysis of present-day American English. Brown Uni-
versity Press.
Lafferty, J. and McCallum, A. and Pereira F. (2001) Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In 18th ICML,
pp. 282?289. MA, USA.
Lipski, J. M. (1978) Code-switching and the problem of
bilingual competence. In M. Paradis (ed.), Aspects of
bilingualism, pp. 250?264, Columbia, SC. Hornbeam.
McCallum, A. (2002) MALLET: A Machine Learning
for Language Toolkit. Retrieved January 7, 2008 from
http://mallet.cs.umass.edu
Paul, D. B. and Baker, J. M. (1992) The design of the
Wall Street Journal-based CSR corpus. In HLT?91:
workshop on speech and Natural Language pp. 357?
362, Morristown, NJ, USA.
Poplack, S., Sankoff, D. and Miller, C. (1988) The social
correlates and linguistic processes of lexical borrowing
and assimilation. Linguistics, 26(1): 47?104.
Poplack, S. (1980) Sometimes I?ll start a sentence in
Spanish y termino en espan?ol: toward a typology of
code-switching. Linguistics, 18(7/8): 581?618.
Poplack, S. (1981) Syntactic structure and social function
of code-switching. In R. Duran (ed.), Latino discourse
and communicative behavior, pp. 169?184, Norwood,
NJ. Ablex.
Quinlan, J. R. (1986) Induction of decision trees. Ma-
chine Learning, 1: 81?106.
Ratnaparkhi, A. (1996) A maximum entropy model
for part-of-speech tagging. In EMNLP, pp. 133?142,
Philadelphia, PA, May.
Rosner, M. and Farrugia, P. (2007) A tagging algorithm
for mixed language identification in a noisy domain.
1059
In INTERSPEECH 2007, pp. 190?193, Antwerp, Bel-
gium.
Sankoff, D. (1968) Social aspects of multilingualism in
New Guinea. Ph.D. thesis, McGill University.
Sankoff, D. (1981) A formal grammar for codeswitching.
Papers in Linguistics: International Journal of Human
communications, 14(1): 3?46.
Sankoff, D. (1998a) A formal production-based explana-
tion of the facts of code-switching. Bilingualism, Lan-
guage and Cognition, 1: 39?50. Cambridge University
Press.
Sankoff, D. (1998b) The production of code-mixed dis-
course. In 36th ACL, volume I, pp. 8?21, Montreal,
Quebec, Canada.
Schmid, H. (1994) Probabilistic part-of-speech tagging
using decision trees. In International Conference on
New Methods in Language Processing, Manchester,
UK.
Scho?lkopf, B. and Smola, A. J. (2002) Learning with Ker-
nels: Support Vector Machines, Regularization, Opti-
mization and Beyond. MIT Press.
Toribio, A. J. (2001a) Accessing Spanish-English code-
switching competence. International Journal of Bilin-
gualism, 5(4):403?436.
Toribio, A. J. (2001b) On the emergence of bilingual
code-switching competence. Bilingual Language and
Cognition, 4(3):203?231.
U.S. Census Bureau. (2003) Language use and En-
glish speaking ability: 2000. Retrieved October 30,
2006 from http://www.census.gov/prod/
2003pubs/c2kbr-29.pdf.
Witten, I. H. and Frank, E. (1999) Data Mining, Practi-
cal Machine Learning Tools and Techniques with Java
Implementations. Morgan Kaufmann.
Wolpert, D. H. (1992) Stacked Generalization. Neural
Networks, 5(2):241?259.
1060
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 81?84,
New York, June 2006. c?2006 Association for Computational Linguistics
Initial Study on Automatic Identification of Speaker Role in Broadcast News
Speech
Yang Liu
University of Texas at Dallas, Richardson, TX
yangl@hlt.utdallas.edu
Abstract
Identifying a speaker?s role (anchor, reporter,
or guest speaker) is important for finding
the structural information in broadcast news
speech. We present an HMM-based approach
and a maximum entropy model for speaker
role labeling using Mandarin broadcast news
speech. The algorithms achieve classification
accuracy of about 80% (compared to the base-
line of around 50%) using the human tran-
scriptions and manually labeled speaker turns.
We found that the maximum entropy model
performs slightly better than the HMM, and
that the combination of them outperforms any
model alone. The impact of the contextual role
information is also examined in this study.
1 Introduction
More effective information access is beneficial to deal
with the increasing amount of broadcast news speech.
Many attempts have been made in the past decade to build
news browser, spoken document retrieval system, and
summarization or question answering system to effec-
tively handle the large volume of news broadcast speech
(e.g., the recent DARPA GALE program). Structural in-
formation, such as story segmentation or speaker cluster-
ing, is critical for all of these applications. In this paper,
we investigate automatic identification of the speakers?
roles in broadcast news speech. A speaker?s role (such
as anchor, reporter or journalist, interviewee, or some
soundbites) can provide useful structural information of
broadcast news. For example, anchors appear through the
entire program and generally introduce news stories. Re-
porters typically report a specific news story, in which
there may be other guest speakers. The transition be-
tween anchors and reporters is usually a good indicator
of story structure. Speaker role information was shown
to be useful for summarizing broadcast news (Maskey
and Hirschberg, 2003). Anchor information has also been
used for video segmentation, such as the systems in the
TRECVID evaluations.1
1See http://www-nlpir.nist.gov/projects/trecvid/ for more in-
formation on video retrieval evaluations.
In this paper, we develop algorithms for speaker role
identification in broadcast news speech. Human tran-
scription and manual speaker turn labels are used in this
initial study. The task is then to classify each speaker?s
turn as anchor, reporter, or other. We use about 170
hours of speech for training and testing. Two approaches
are evaluated, an HMM and a maximum entropy classi-
fier. Our methods achieve about 80% accuracy for the
three-way classification task, compared to around 50%
when every speaker is labeled with the majority class la-
bel, i.e., anchor.2
The rest of the paper is organized as follows. Related
work is introduced in Section 2. We describe our ap-
proaches in Section 3. Experimental setup and results are
presented in Section 4. Summary and future work appear
in Section 5.
2 Related Work
The most related previous work is (Barzilay et al, 2000),
in which Barzilay et al used BoosTexter and the max-
imum entropy model to classify each speaker?s role in
an English broadcast news corpus. Three classes are
used, anchor, journalist, and guest speaker, which are
very similar to the role categories in our study. Lexical
features (key words), context features, duration, and ex-
plicit speaker introduction are used as features. For the
three-way classification task, they reported accuracy of
about 80% compared to the chance of 35%. They have in-
vestigated using both the reference transcripts and speech
recognition output. Our study differs from theirs in that
we use one generative modeling approach (HMM), as
well as the conditional maximum entropy method. We
also evaluate the contextual role information for classifi-
cation. In addition, our experiments are conducted using
a different language, Mandarin broadcast news. There
may be inherent difference across languages and news
sources.
Another task related to our study is anchor segmen-
tation. Huang et al (Huang et al, 1999) used a recog-
nition model for a particular anchor and a background
model to identify anchor segments. They reported very
promising results for the task of determining whether
2Even though this is a baseline (or chance performance), it
is not very meaningful since there is no information provided in
this output.
81
or not a particular anchor is talking. However, this
method is not generalizable to multiple anchors, nor is
it to reporters or other guest speakers. Speaker role
detection is also related to speaker segmentation and
clustering (also called speaker diarization), which was a
benchmark test in the NIST Rich Transcription evalua-
tions in the past few years (for example, NIST RT-04F
http://www.nist.gov/speech/tests/rt/rt2004/fall/). Most of
the speaker diarization systems only use acoustic infor-
mation; however, in recent studies textual sources have
also been utilized to help improve speaker clustering re-
sults, such as (Canseco et al, 2005). The goal of speaker
diarization is to identify speaker change and group the
same speakers together. It is different from our task since
we determine the role of a speaker rather than speaker
identity. In this initial study, instead of using automatic
speaker segmentation and clustering results, we use the
manual speaker segments but without any speaker iden-
tity information.
3 Speaker Role Identification Approaches
3.1 Hidden Markov Model (HMM)
anchor
reporterother
Sentence 1Sentence 2Sentence 3
????
Sentence 1Sentence 2
????
Sentence 1Sentence 2Sentence 3
????
Figure 1: A graphical representation of the HMM ap-
proach for speaker role labeling. This is a simple first
order HMM.
The HMM has been widely used in many tagging prob-
lems. Stolcke et al (Stolcke et al, 2000) used it for dialog
act classification, where each utterance (or dialog act) is
used as the observation. In speaker role detection, the ob-
servation is composed of a much longer word sequence,
i.e., the entire speech from one speaker. Figure 1 shows
the graphical representation of the HMM for speaker role
identification, in which the states are the speaker roles,
and the observation associated with a state consists of the
utterances from a speaker. The most likely role sequence
R? is:
R? = argmax
R
P (R|O) = argmax
R
P (O|R)P (R), (1)
where O is the observation sequence, in which Oi corre-
sponds to one speaker turn. If we assume what a speaker
says is only dependent on his or her role, then:
P (O|R) =
?
i
P (Oi|Ri). (2)
From the labeled training set, we train a language
model (LM), which provides the transition probabilities
in the HMM, i.e., the P (R) term in Equation (1). The vo-
cabulary in this role LM (or role grammar) consists of dif-
ferent role tags. All the sentences belonging to the same
role are put together to train a role specific word-based N-
gram LM. During testing, to obtain the observation prob-
abilities in the HMM, P (Oi|Ri), each role specific LM
is used to calculate the perplexity of those sentences cor-
responding to a test speaker turn.
The graph in Figure 1 is a first-order HMM, in which
the role state is only dependent on the previous state.
In order to capture longer dependency relationship, we
used a 6-gram LM for the role LM. For each role spe-
cific word-based LM, 4-gram is used with Kneser-Ney
smoothing. There is a weighting factor when combin-
ing the state transitions and the observation probabilities
with the best weights tuned on the development set (6 for
the transition probabilities in our experiments). In addi-
tion, in stead of using Viterbi decoding, we used forward-
backward decoding in order to find the most likely role
tag for each segment. Finally we may use only a subset
of the sentences in a speaker?s turn, which are possibly
more discriminative to determine the speaker?s role. The
LM training and testing and HMM decoding are imple-
mented using the SRILM toolkit (Stolcke, 2002).
3.2 Maximum Entropy (Maxent) Classifier
A Maxent model estimates the conditional probability:
P (Ri|O) =
1
Z?(O)
exp(
?
k
?kgk(Ri, O)), (3)
where Z?(O) is the normalization term, functions
gk(Ri, O) are indicator functions weighted by ?, and k is
used to indicate different ?features?. The weights (?) are
obtained to maximize the conditional likelihood of the
training data, or in other words, maximize the entropy
while satisfying all the constraints. Gaussian smoothing
(variance=1) is used to avoid overfitting. In our experi-
ments we used an existing Maxent toolkit (available from
http://homepages.inf.ed.ac.uk/s0450736/maxent toolkit.
html).
The following features are used in the Maxent model:
? bigram and trigram of the words in the first and the
last sentence of the current speaker turn
? bigram and trigram of the words in the last sentence
of the previous turn
82
? bigram and trigram of the words in the first sentence
of the following turn
Our hypothesis is that the first and the last sentence from
a speaker?s turn are more indicative of the speaker?s role
(e.g., self introduction and closing). Similarly the last
sentence from the previous speaker segment and the first
sentence of the following speaker turn also capture the
speaker transition information. Even though sentences
from the other speakers are included as features, the Max-
ent model makes a decision for each test speaker turn in-
dividually without considering the other segments. The
impact of the contextual role tags will be evaluated in our
experiments.
4 Experiments
4.1 Experimental Setup
We used the TDT4 Mandarin broadcast news data in this
study. The data set consists of about 170 hours (336
shows) of news speech from different sources. In the
original transcripts provided by LDC, stories are seg-
mented; however, speaker information (segmentation or
identity) is not provided. Using the reference transcripts
and the audio files, we manually labeled the data with
speaker turns and the role tag for each turn.3 Speaker
segmentation is generally very reliable; however, the role
annotation is ambiguous in some cases. The interanno-
tator agreement will be evaluated in our future work. In
this initial study, we just treat the data as noisy data.
We preprocessed the transcriptions by removing some
bad codes and also did text normalization. We used punc-
tuation (period, question mark, and exclamation) avail-
able from the transcriptions (though not very accurate)
to generate sentences, and a left-to-right longest word
match approach to segment sentences into words. These
words/sentences are then used for feature extraction in
the Maxent model, and LM training and perplexity cal-
culation in the HMM as described in Section 3. Note
that the word segmentation approach we used may not
be the-state-of-art, which might have some effect on our
experiments.
10-fold cross validation is used in our experiments.
The entire data set is split into ten subsets. Each time
one subset is used as the test set, another one is used as
the development set, and the rest are used for training.
The average number of segments (i.e., speaker turns) in
the ten subsets is 1591, among which 50.8% are anchors.
Parameters (e.g., weighting factor) are tuned based on the
average performance over the ten development sets, and
the same weights are applied to all the splits during test-
ing.
3The labeling guideline can be found from
http://www.hlt.utdallas.edu/?yangl/spkr-label/. It was modified
based on the annotation manual used for English at Columbia
University (available from http://www1.cs.columbia.edu/
?smaskey/labeling/Labeling Manual v 2 1.pdf).
4.2 Results
A HMM and Maxent: Table 1 shows the role iden-
tification results using the HMM and the Maxent
model, including the overall classification accuracy
and the precision/recall rate (%) for each role. These
results are the average over the 10 test sets.
HMM Maxent
precision recall precision recall
anchor 78.03 87.33 80.29 87.23
reporter 78.54 66.42 73.34 77.01
other 83.05 68.19 89.52 41.30
Accuracy (%) 77.18 77.42
Table 1: Automatic role labeling results (%) using the
HMM and Maxent classifiers.
From Table 1 we find that the overall classification
performance is similar when using the HMM and
the Maxent model; however, their error patterns are
quite different. For example, the Maxent model is
better than the HMM at identifying ?reporter? role,
but worse at identifying ?other? speakers (see the re-
call rate shown in the table). In the HMM, we only
used the first and the last sentence in a speaker?s
turn, which are more indicative of the speaker?s role.
We observed significant performance degradation,
that is, 74.68% when using all the sentences for
LM training and perplexity calculation, compared
to 77.18% as shown in the table using a subset of
a speaker?s speech. Note that the sentences used in
the HMM and Maxent models are the same; how-
ever, the Maxent does not use any contextual role
tags (which we will examine next), although it does
include some words from the previous and the fol-
lowing speaker segments in its feature set.
B Contextual role information: In order to investi-
gate how important the role sequence is, we con-
ducted two experiments for the Maxent model. In
the first experiment, for each segment, the reference
role tag of the previous and the following segments
and the combination of them are included as features
for model training and testing (a ?cheating? exper-
iment). In the second experiment, a two-step ap-
proach is employed. Following the HMM and Max-
ent experiments (i.e., results as shown in Table 1),
Viterbi decoding is performed using the posterior
probabilities from the Maxent model and the tran-
sition probabilities from the role LM as in the HMM
(with weight 0.3). The average performance over the
ten test sets is shown in Table 2 for these two exper-
iments. For comparison, we also present the decod-
ing results of the HMM with and without using se-
quence information (i.e., the transition probabilities
in the HMM). Additionally, the system combination
83
results of the HMM and Maxent are presented in the
table, with more discussion on this later. We observe
from Table 2 that adding contextual role informa-
tion improves performance. Including the two refer-
ence role tags yields significant gain in the Maxent
model, even though some sentences from the previ-
ous and the following segments are already included
as features. The HMM suffers more than the Max-
ent classifier when role sequence information is not
used during decoding, since that is the only contex-
tual information used in the HMM, unlike the Max-
ent model, which uses features extracted from the
neighboring speaker turns.
Accuracy (%)
0: Maxent (as in Table 1) 77.42
1: Maxent + 2 reference tags 80.90
2: Maxent + sequence decoding 78.59
3: HMM (as in Table 1) 77.18
4: HMM w/o sequence 73.30
Maxent (0) + HMM (3) 79.74
Maxent (2) + HMM (3) 81.97
Table 2: Impact of role sequence information on the
HMM and Maxent classifiers. The combination results
of the HMM and Maxent are also provided.
C System combination: For system combination, we
used two different Maxent results: with and with-
out the Viterbi sequence decoding, corresponding to
experiments (0) and (2) as shown in Table 2 respec-
tively. When combining the HMM and Maxent, i.e.,
the last two rows in Table 2, the posterior probabili-
ties from them are linearly weighted (weight 0.6 for
the Maxent in the upper one, and 0.7 for the Max-
ent in the bottom one). The combination of the two
approaches yields better performance than any sin-
gle model in the two cases. We also investigated
other system combination approaches. For example,
a decision tree or SVM that builds a 3-way super-
classifier using the posterior probabilities from the
HMM and Maxent. However, so far we have not
found any gain from more complicated system com-
bination than a simple linear interpolation. We will
study this in our future work.
5 Summary and Future Work
In this paper we have reported an initial study of speaker
role identification in Mandarin broadcast news speech us-
ing the HMM and Maxent tagging approaches. We find
that the conditional Maxent generally performs slightly
better than the HMM, and that their combination out-
performs each model alone. The HMM and the Max-
ent model show differences in identifying different roles.
The impact of contextual role information is also exam-
ined for the two approaches, and a significant gain is ob-
served when contextual information is modeled. We find
that the beginning and the end sentences in a speaker?s
turn are good cues for role identification. The overall
classification performance in this study is similar to that
reported in (Barzilay et al, 2000); however, the chance
performance is quite different (35% in that study). It is
not clear yet whether it is because of the difference across
the two corpora or languages.
The Maxent model provides a convenient way to in-
corporate various knowledge sources. We will investi-
gate other features to improve the classification results,
such as name information, acoustic or prosodic features,
and speaker clustering results (considering that the same
speaker typically has the same role tag). We plan to
examine the effect of using speech recognition output,
as well as automatic speaker segmentation and cluster-
ing results. Analysis of difference news sources may
also reveal some interesting findings. Since our working
hypothesis is that speaker role information is important
to find structure in broadcast news, we will investigate
whether and how speaker role relates to downstream lan-
guage processing applications, such as summarization or
question answering.
Acknowledgment
The author thanks Julia Hirschberg and Sameer Maskey at
Columbia University and Mari Ostendorf at the University of
Washington for the useful discussions, and Mei-Yuh Hwang
for helping with Mandarin word segmentation and text normal-
ization. This material is based upon work supported by the
Defense Advanced Research Projects Agency (DARPA) under
Contract No. HR0011-06-C-0023. Any opinions, findings and
conclusions or recommendations expressed in this material are
those of the author(s) and do not necessarily reflect the views of
DARPA.
References
R. Barzilay, M. Collins, J. Hirschberg, and S. Whittaker. 2000.
The rules behind roles: Identifying speaker role in radio
broadcasts. In Proc. of AAAI.
L. Canseco, L. Lamel, and J Gauvain. 2005. A comparative
study using manual and automatic transcription for diariza-
tion. In Proc. of ASRU.
Q. Huang, Z. Liu, A. Rosenberg, D. Gibbon, and B. Shahraray.
1999. Automated generation of news content hierarchy by
integrating audio, video, and text information. In Proc. of
ICASSP, pages 3025?3028.
S. Maskey and J. Hirschberg. 2003. Automatic summarization
of broadcast news using structural features. In Eurospeech.
A. Stolcke, K. Ries, N. Coccaro, E. Shriberg, R. Bates,
D. Jurasky, P. Taylor, R. Martin, C.V. Ess-Dykema, and
M. Meteer. 2000. Dialogue act modeling for automatic tag-
ging and recognition of conversational speech. Computa-
tional Linguistics, 26(3):339?373.
A. Stolcke. 2002. SRILM ? An extensible language modeling
toolkit. In Proc. of ICSLP, pages 901?904.
84
Proceedings of NAACL HLT 2007, Companion Volume, pages 101?104,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Look Who is Talking: Soundbite Speaker Name Recognition in  
Broadcast News Speech 
Feifan Liu, Yang Liu 
Department of Computer Science 
The University of Texas at Dallas, Richardson, TX 
{ffliu,yangl}@hlt.utdallas.edu 
 
 
Abstract 
Speaker name recognition plays an important 
role in many spoken language applications, 
such as rich transcription, information extrac-
tion, question answering, and opinion mining. 
In this paper, we developed an SVM-based 
classification framework to determine the 
speaker names for those included speech seg-
ments in broadcast news speech, called sound-
bites. We evaluated a variety of features with 
different feature selection strategies. Experi-
ments on Mandarin broadcast news speech 
show that using our proposed approach, the 
soundbite speaker name recognition (SSNR) 
accuracy is 68.9% on our blind test set, an ab-
solute 10% improvement compared to a base-
line system, which chooses the person name 
closest to the soundbite. 
1 Introduction 
Broadcast news (BN) speech often contains speech or 
interview quotations from specific speakers other than 
reporters and anchors in a show. Identifying speaker 
names for these speech segmentations, called soundbites 
(Maskey and Hirschberg, 2006), is useful for many 
speech processing applications, e.g., question answering, 
opinion mining for a specific person. This has recently 
received increasing attention in programs such as the 
DARPA GALE program, where one query template is 
about a person?s opinion or statement. 
Previous work in this line includes speaker role de-
tection (e.g., Liu, 2006; Maskey and Hirschberg, 2006) 
and speaker diarization (e.g., Canseco et al, 2005). In 
this paper, we formulate the problem of SSNR as a tra-
ditional classification task, and proposed an SVM-based 
identification framework to explore rich linguistic fea-
tures. Experiments on Mandarin BN speech have shown 
that our proposed approach significantly outperforms 
the baseline system, which chooses the closest name as 
the speaker for a soundbite.  
2 Related Work 
To our knowledge, no research has yet been conducted 
on soundbite speaker name identification in Mandarin 
BN domain. However, this work is related to some ex-
tent to speaker role identification, speaker diarization, 
and named entity recognition. 
Speaker role identification attempts to classify speech 
segments based on the speakers? role (anchor, reporter, 
or others). Barzilay et al (2000) used BoosTexter and 
the maximum entropy model for this task in English BN 
corpus, obtaining a classification accuracy of about 80% 
compared to the chance of 35%. Liu (2006) combined a 
generative HMM approach with the conditional maxi-
mum entropy method to detect speaker roles in Manda-
rin BN, reporting a classification accuracy of 81.97% 
against the baseline of around 50%. In Maskey and 
Hirschberg (2006), the task is to recognize soundbites 
(which make up of a large portion of the ?other? role 
category in Liu (2006)). They achieved a recognition 
accuracy of 67.4% in the English BN domain. Different 
from their work, our goal is to identify the person who 
spoke those soundbites, i.e., associate each soundbite 
with a speaker name if any. 
Speaker diarization in BN aims to find speaker 
changes, group the same speakers together, and recog-
nize speaker names. It is an important component for 
rich transcription (e.g., in the DARPA EARS program). 
So far most work in this area has only focused on 
speaker segmentation and clustering, and not included 
name recognition. However, Canseco et al (2005) were 
able to successfully use linguistic information (e.g., 
related to person names) to improve performance of BN 
speaker segmentation and clustering.  
This work is also related to named entity recognition 
(NER), especially person names. There has been a large 
amount of research efforts on NER; however, instead of 
101
recognizing all the names in a document, our task is to 
find the speaker for a particular speech segment.  
3 Framework for Soundbite Speaker 
Name Recognition (SSNR) 
Figure 1 shows our system diagram. SSNR is conducted 
using the speech transcripts, assuming the soundbite 
segments are provided. After running NER in the tran-
scripts, we obtain candidate person names. For a sound-
bite, we use the name hypotheses from the region both 
before and after the soundbite. A ?region? is defined 
based on the turn and topic segmentation information. 
To determine which name among the candidates is the 
corresponding speaker for the soundbite, we recast this 
problem as a binary classification problem for every 
candidate name and the soundbite, which we call an 
instance. A positive tag for an instance means that the 
name is the soundbite speaker. Each instance has an 
associated feature vector, described further in the fol-
lowing section. Note that if a name occurs more than 
once, only one instance is created for it. 
 
Train Set Dev/Test Set
Preprocessing (word 
segmentation, NER)
Instance Creation 
for SSNR
Feature Vector 
Representation
Trained 
Model
Model Training 
and Optimizing
Conflict 
Resolution
Training Testing 
Output
 
Figure 1. System diagram for SSNR. 
 
Any classification approach can be used in this gen-
eral framework for SSNR. We choose to use an SVM 
classifier in our experiments because of its superior per-
formance in many classification tasks. 
3.1 Features  
The features that we have explored can be grouped into 
three categories.  
Positional Features (PF) 
? PF-1: the position of the candidate name relative to 
the soundbite. We hypothesize that names closer to 
a soundbite are more likely to be the soundbite 
speaker. This feature value can be ?last?, ?first?, 
?mid?, or ?unique?. For example, ?last? for a candi-
date before a soundbite means that it is the closest 
name among the hypotheses before the soundbite. 
?Unique? indicates that the candidate is the only 
person name in the region before or after the sound-
bite. Note that if a candidate name occurs more than 
once, the PF-1 feature corresponds to the closest 
name to the soundbite.  
? PF-2: the position of a name in its sentence. Typi-
cally a name appearing earlier in a sentence (e.g., a 
subject) is more likely to be quoted later.  
? PF-3: an indicator feature to show where the name 
has occurred, before, inside, or after the soundbite. 
We added this because it is rare that a name inside a 
soundbite is the speaker of that soundbite.  
? PF-4: an indicator to denote if a candidate is in the 
last sentence just before the soundbite turn, or is in 
the first sentence just after the soundbite turn. 
Frequency Features (Freq) 
We hypothesize that a name with more occurrences 
might be an important subject and thus more likely to be 
the speaker of the soundbite, therefore we include the 
frequency of a candidate name in the feature set.  
Lexical Features (LF) 
In order to capture the cue words around the soundbite 
speaker names in the transcripts, we included unigram 
features. For example, ?pre_word+1=?/said? denotes 
that the candidate name is followed by the word ??
/said?, and that ?pre? means this happens in the region 
before the soundbite.  
3.2 Conflict Resolution 
Another component in the system diagram that is worth 
pointing out is ?conflict resolution?. Since our approach 
treats each candidate name as a separate classification 
task, we need to post-process the cases where there are 
multiple or no positive hypotheses for a soundbite dur-
ing testing. To resolve this situation, we choose the in-
stance with the best confidence value from the classifier.  
4 Experiments 
4.1 Experimental Setup 
We use the TDT4 Mandarin broadcast news data in our 
experiment. The data set consists of about 170 hours 
(336 shows) of news speech from different sources. 
Speaker turns and soundbite segment information were 
annotated manually in the transcripts. Our current study 
102
only uses the soundbites that have a human-labeled 
speaker name in the surrounding transcripts. There are 
1292 such soundbites in our corpus. We put aside 1/10 
of the data as the development set, another 1/10 as the 
test set, and used the rest as our training set. All the 
transcripts were automatically tagged with named enti-
ties using the NYU tagger (Ji and Grishman, 2005). For 
the classifier, we used the libSVM toolkit (Chang and 
Lin, 2001) and the RBF kernel in our experiments.  
A reasonable baseline for SSNR is to choose the 
closest person name before a soundbite as its speaker. 
We will compare our system performance to this base-
line approach.  
We used two performance metrics in our experi-
ments. First is the instance classification accuracy (CA) 
for the candidate names in the framework of the binary 
classification task. Second, we compute name recogni-
tion accuracy (RA) for the soundbites as follows: 
FilesinSoundbitesof
NamesCorrectwithSoundbitesof
RA
#
#=  
4.2 Effects of Different Manually Selected 
Feature Subsets 
We used 10-fold cross validation on the training set to 
evaluate the effect of different features and also for pa-
rameter optimization. Table 1 shows the instance classi-
fication results. ?PF, Freq, LF? are the features 
described in Section 3.1. ?LF-before? means the uni-
gram features before the soundbites. ?All-before? de-
notes using all the features before the soundbites. 
 
Optimized Para. Feature 
Subsets C G 
CA 
(%) 
PF-1 0.125 2 83.48
+PF-2 2048 1.22e-4 85.62
+PF-3 2048 4.88e-4 85.79
+PF-4 2 0.5 86.18
+Freq 2 0.5 86.18
+LF-before 32 7.81e-3 88.44
+LF-after 
i.e., All features 8 0.0313 88.44
All-before 8 0.0313 88.03
Table 1. Instance classification accuracy (CA) using 
different feature sets. C and G are the optimized pa-
rameters in the SVM model. 
 
We notice that the system performance generally 
improves with incrementally expended feature sets, 
yielding an accuracy of 88.44% using all the features.  
Some features seem not helpful to system performance, 
such as ?Freq? and ?LF-after?. Using all the features 
before the soundbites achieves comparable performance 
to using all the features, indicating that the region before 
a soundbite contributes more than that after it. This is 
expected since the reporters typically have already men-
tioned the person?s name before a soundbite. In addition, 
we evaluated some compound features using our current 
feature definition, but adding those did not improve the 
system performance.  
4.3 Automatic Feature Selection 
We also performed automatic feature selection for the 
SVM model based on the F-score criterion (Chen and 
Lin, 2006). There are 6048 features in total in our sys-
tem. Figure 2 shows the classification performance in 
the training set using different number of features via 
automatic feature selection. 
 
88.61
90.79
87.188.12
88.44
88.61
88.7390.14
88.44
86
87
88
89
90
91
92
60
48
30
24
18
39
15
12 75
6
37
8
18
9 94 47
# of features
CA
(%
)
 
Figure 2. Instance classification accuracy (CA) using F-
score based feature selection. 
 
We can see that automatic feature selection further im-
proves the classification performance (2.36% higher 
accuracy than that in Table 1). Table 2 lists some of the 
top features based on their F-scores. Consistent with our 
expectation, we observe that position related features, as 
well as cue words, are good indicators for SSNR.  
 
Feature F-score 
Justbeforeturn (PF-4) 0.3543 
pre_contextpos=last (PF-1) 0.2857 
pre_senpos=unique (PF-2) 0.0631 
pre_word+1=???/morning? (LF) 0.0475 
pre_word+1= ??/said? (LF) 0.0399 
bool_pre=1 (PF-3) 0.0353 
Justafterturn (PF-4) 0.0349 
pre_contextpos=mid (PF-1) 0.0329 
post_contextpos=first (PF-1) 0.0323 
pre_word+1= ???/today? (LF) 0.0288 
pre_word-1=???/reporter? (LF) 0.0251 
pre_word+1=???/express? (LF) 0.0246 
Table 2. Top features ordered by F-score values. 
   
103
4.4 Performance on Development Set 
Up to now our focus has been on feature selection based 
on instance classification accuracy. Since our ultimate 
goal is to identify soundbite speaker names, we chose 
several promising configurations based on the results 
above to apply to the development set and evaluate the 
soundbite name recognition accuracy. Results using the 
two metrics are presented in Table 3. 
Feature Set CA (%) RA (%) 
Baseline 84.0 59.3 
PF 86.7 54.2 
PF+Freq 86.7 60.4 
PF+Freq+LF-before 87.8 63.5 
PF+Freq+LF-before 
+LF-after (ALL) 88.3 67.7 
Top 1512 by f-score 85.6 62.5 
Top 1839 by f-score 85.4 60.4 
Table 3. Results on the dev set using two metrics: in-
stance classification accuracy (CA), and soundbite name 
recognition accuracy (RA). The oracle RA is 79.1%.  
 
Table 3 shows that using all the features (ALL) 
performs the best, yielding an improvement of 4.3% and 
8.4% compared to the baseline in term of the CA and RA 
respectively. However, using the automatically selected 
feature sets (the last two rows in Table 3) only slightly 
outperforms the baseline. This suggests that the F-score 
based feature selection strategy on the training set may 
not generalize well. Interestingly, ?Freq? and ?LF-after? 
features show some useful contribution (the 4th and 6th 
row in Table 3) respectively on the development set, 
different from the results on the training set using 10-
fold cross validation. The results using the two metrics 
also show that they are not always correlated. 
Because of the possible NER errors, we also meas-
ure the oracle RA, defined as the percent of the sound-
bites for which the correct speaker name (based on NER) 
appears in the region surrounding the soundbite. The 
oracle RA on this data set is 79.1%. We also notice that 
8.3% of the soundbites do not have the correct name 
hypothesis due to an NER boundary error, and that 
12.5% is because of missing errors. 
We used the method as described in Section 3.2 to 
resolve conflicts for the results shown in Table 3. In 
addition, we evaluated another approach?we resort to 
the baseline (i.e., chose the name that is closest to the 
soundbite) for those soundbites that have multiple or no 
positive hypothesis. Our experiments on the develop-
ment set showed this approach degrades system per-
formance (e.g., RA of around 61% using all the features).  
4.5 Results on Blind Test Set 
Finally, we applied the all-feature configuration to our 
blind test data and obtained the results as shown in Ta-
ble 4. Using all the features significantly outperforms 
the baseline. The gain is slightly better than that on the 
development set, although the oracle accuracy is also 
higher on the test set. 
 CA (%) RA (oracle: 85.8%) 
Baseline 81.3 58.4 
All feature 85.1 68.9 
Table 4. Results on the test set.    
5 Conclusion 
We proposed an SVM-based approach for soundbite 
speaker name recognition and examined various linguis-
tic features. Experiments in Mandarin BN corpus show 
that our approach yields an identification accuracy of 
68.9%, significantly better than 58.4% from the baseline. 
Our future work will focus on exploring more useful 
features, such as part-of-speech and semantic features. 
In addition, we plan to test this framework using auto-
matic speech recognition output, speaker segmentation, 
and soundbite segment detection.  
6 Acknowledgement 
We thank Sameer Maskey, Julia Hirschberg, and Mari 
Ostendorf for useful discussions, and Heng Ji for shar-
ing the Mandarin named entity tagger. This work is 
supported by DARPA under Contract No. HR0011-06-
C-0023. Any opinions expressed in this material are 
those of the authors and do not necessarily reflect the 
views of DARPA. 
References 
S. Maskey and J. Hirschberg. 2006. Soundbite Detec-
tion in Broadcast News Domain. In Proc. of INTER-
SPEECH2006.  pp: 1543-1546. 
Y. Liu. 2006. Initial Study on Automatic Identification 
of Speaker Role in Broadcast News Speech. In Proc.  
of HLT-NAACL. pp: 81-84. 
R. Barzilay, M. Collins, J. Hirschberg, and S. Whittaker. 
2000. The Rules Behind Roles: Identifying Speaker 
Role in Radio Broadcasts. In Proc. of AAAI. 
L. Canseco, L. Lamel, and J.-L. Gauvain. 2005. A Com-
parative Study Using Manual and Automatic Tran-
scriptions for Diarization. In Proc. of ASRU. 
H. Ji and R. Grishman. 2005. Improving Name Tagging 
by Reference Resolution and Relation Detection. In 
Proc. of ACL. pp: 411-418. 
Y.-W. Chen and C.-J. Lin. 2006. Combining SVMs with 
Various Feature Selection Strategies. Feature Extrac-
tion, Foundations and Applications, Springer.  
C. Chang and C. Lin. 2001. LIBSVM: A Library for 
Support Vector Machines. Software available at 
http://www.csie.ntu.edu.tw/~cjlin/libsvm. 
104
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 46?55,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
A Corpus-Based Approach for the Prediction of Language Impairment in
Monolingual English and Spanish-English Bilingual Children
Keyur Gabani and Melissa Sherman and Thamar Solorio and Yang Liu
Department of Computer Science
The University of Texas at Dallas
keyur,mesh,tsolorio,yangl@hlt.utdallas.edu
Lisa M. Bedore and Elizabeth D. Pen?a
Department of Communication Sciences and Disorders
The University of Texas at Austin
lbedore,lizp@mail.utexas.edu
Abstract
In this paper we explore a learning-based ap-
proach to the problem of predicting language
impairment in children. We analyzed sponta-
neous narratives of children and extracted fea-
tures measuring different aspects of language
including morphology, speech fluency, lan-
guage productivity and vocabulary. Then, we
evaluated a learning-based approach and com-
pared its predictive accuracy against a method
based on language models. Empirical re-
sults on monolingual English-speaking chil-
dren and bilingual Spanish-English speaking
children show the learning-based approach is
a promising direction for automatic language
assessment.
1 Introduction
The question of how best to identify children with
language disorders is a topic of ongoing debate.
One common assessment approach is based on cut-
off scores from standardized, norm-referenced lan-
guage assessment tasks. Children scoring at the
lower end of the distribution, typically more than
1.25 or 1.5 Standard Deviations (SD) below the
mean, are identified as having language impair-
ment (Tomblin et al, 1997). This cutoff-based
approach has several well-documented weaknesses
that may result in both over- and under-identification
of children as language impaired (Plante and Vance,
1994). Recent studies have suggested considerable
overlap between children with language impairment
and their typically developing cohorts on many of
these tasks (e.g., (Pen?a et al, 2006b; Spaulding et
al., 2006)). In addition, scores and cutoffs on stan-
dardized tests depend on the distribution of scores
from the particular samples used in normalizing the
measure. Thus, the validity of the measure for chil-
dren whose demographic and other socioeconomic
characteristics are not well represented in the test?s
normative sample is a serious concern. Finally, most
norm-referenced tests of language ability rely heav-
ily on exposure to mainstream language and expe-
riences, and have been found to be biased against
children from families with low parental education
and socioeconomic status, as well as children from
different ethnic backgrounds (Campbell et al, 1997;
Dollaghan and Campbell, 1998).
This paper aims to develop a reliable and auto-
matic method for identifying the language status of
children. We propose the use of different lexico-
syntactic features, typically used in computational
linguistics, in combination with features inspired
by current assessment practices in the field of lan-
guage disorders to train Machine Learning (ML) al-
gorithms. The two main contributions of this pa-
per are: 1) It is one step towards developing a re-
liable and automatic approach for language status
prediction in English-speaking children; 2) It pro-
vides evidence showing that the same approach can
be adapted to predict language status in Spanish-
English bilingual children.
2 Related Work
2.1 Monolingual English-Speaking Children
Several hypotheses exist that try to explain the gram-
matical deficits of children with Language Impair-
46
ment (LI). Young children normally go through a
stage where they use non-finite forms of verbs in
grammatical contexts where finite forms are re-
quired (Wexler, 1994). This is referred as the op-
tional infinitive stage. The Extended Optional Infini-
tive (EOI) theory (Rice and Wexler, 1996) suggests
that children with LI exhibit the use of a ?young?
grammar for an extended period of time, where
tense, person, and number agreement markers are
omitted.
In contrast to the EOI theory, the surface account
theory (Leonard et al, 1997) assumes that chil-
dren with LI have reduced processing capabilities.
This deficit affects the perception of low stress mor-
phemes, such as -ed, -s, be and do, resulting in an
inconsistent use of these verb morphemes.
Spontaneous narratives are considered as one of
the most ecologically valid ways to measure com-
municative competence (Botting, 2002). They rep-
resent various aspects involved in children?s every-
day communication. Typical measures for sponta-
neous language samples include Mean Length of
Utterance (MLU) in words, Number of Different
Words (NDW), and errors in grammatical morphol-
ogy. Assessment approaches compare children?s
performance on these measures against expected
performance. As mentioned in Section 1, these cut-
off based methods raise questions concerning accu-
racy and bias. Manually analyzing the narratives is
also a very time consuming task. After transcribing
the sample, clinicians need to code for the differ-
ent clinical markers and other morphosyntactic in-
formation. This can take up to several hours for each
child making it infeasible to analyze a large number
of samples.
2.2 Bilingual Spanish-English Speaking
Children
Bilingual children face even more identification
challenges due to their dual language acquisition.
They can be mistakenly labeled as LI due to: 1) the
inadequate use of translations of assessment tools;
2) an over reliance on features specific to English; 3)
a lack of appropriate expectations about how the lan-
guages of a bilingual child should develop (Bedore
and Pen?a, 2008); 4) or the use of standardized
tests where the normal distribution used to compare
language performance is composed of monolingual
children (Restrepo and Gutie?rrez-Clellen, 2001).
Spanish speaking children with LI show differ-
ent clinical markers than English speaking children
with LI. As mentioned above, English speakers have
problems with verb morphology. In contrast, Span-
ish speakers have been found to have problems with
noun morphology, in particular in the use of articles
and clitics (Restrepo and Gutie?rrez-Clellen, 2001;
Jacobson and Schwartz, 2002; Bedore and Leonard,
2005). Bedore and Leonard (2005) also found dif-
ferences in the error patterns of Spanish and related
languages such as Italian. Spanish-speakers tend to
both omit and substitute articles and clitics, while
the dominant errors for Italian-speakers are omis-
sions.
3 Our Approach
We use language models (LMs) in our initial inves-
tigation, and later explore more complex ML algo-
rithms to improve the results. Our ultimate goal is
to discover a highly accurate ML method that can be
used to assist clinicians in the task of LI identifica-
tion in children.
3.1 Language Models for Predicting Language
Impairment
LMs are statistical models used to estimate the prob-
ability of a given sequence of words. They have been
explored previously for clinical purposes. Roark et
al. (2007) proposed cross entropy of LMs trained
on Part-of-Speech (POS) sequences as a measure of
syntactic complexity with the aim of determining
mild cognitive impairment in adults. Solorio and
Liu (2008) evaluated LMs on a small data set in a
preliminary trial on LI prediction.
The intuition behind using LMs is that they can
identify atypical grammatical patterns and help dis-
criminate the population with potential LI from
the Typically Developing (TD) one. We use LMs
trained on POS tags rather than on words. Using
POS tags can address the data sparsity issue in LMs,
and place less emphasis on the vocabulary and more
emphasis on the syntactic patterns.
We trained two separate LMs using POS tags
from the transcripts of TD and LI children, respec-
tively. The language status of a child is predicted
using the following criterion:
47
d(s) =
{ LI if (PPTD(s) > PPLI(s))
TD otherwise
where s represents a transcript from a child, and
PPTD(s) and PPLI(s) are the perplexity values
from the TD and LI LMs, respectively. We used the
SRI Language Modeling Toolkit (Stolcke, 2002) for
training the LMs and calculating perplexities.
3.2 Machine Learning for Predicting Language
Impairment
Although LMs have been used successfully on dif-
ferent human language processing tasks, they are
typically trained and tested on language samples
larger than what is usually collected by clinicians for
the purpose of diagnosing a child with potential LI.
Clinicians make use of additional information be-
yond children?s speech, such as parent and teacher
questionnaires and test scores on different language
assessment tasks. Therefore in addition to using
LMs for children language status prediction, we ex-
plore a machine learning classification approach that
can incorporate more information for better predic-
tion. We aim to identify effective features for this
task and expect this information will help clinicians
in their assessment.
We consider various ML algorithms for the clas-
sification task, including Naive Bayes, Artificial
Neural Networks (ANNs), Support Vector Ma-
chines (SVM), and Boosting with Decision Stumps.
Weka (Witten and Frank, 1999) was used in our ex-
periments due to its known reliability and the avail-
ability of a large number of algorithms. Below we
provide a comprehensive list of features that we ex-
plored for both English and Spanish-English tran-
scripts. We group these features according to the
aspect of language they focus on. Features specific
to Spanish are discussed in Section 5.2.
1. Language productivity
(a) Mean Length of Utterance (MLU) in
words
Due to a general deficit of language abil-
ity, children with LI have been found to
produce language samples with a shorter
MLU in words because they produce
grammatically simpler sentences when
compared to their TD peers.
(b) Total number of words
This measure is widely used when build-
ing language profiles of children for diag-
nostic and treatment purposes.
(c) Degree of support
In spontaneous samples of children?s
speech, it has been pointed out that chil-
dren with potential LI need more encour-
agement from the investigator (Wetherell
et al, 2007) than their TD peers. A sup-
port prompt can be a question like ?What
happened next?? We count the number of
utterances, or turns, of the investigator in-
terviewing the child for this feature.
2. Morphosyntactic skills
(a) Ratio of number of raw verbs to the total
number of verbs
As mentioned previously, children with LI
omit tense markers in verbs more often
than their TD cohorts. For example:
...the boy look into the hole but didn?t
find...
Hence, we include the ratio of the number
of raw verbs to the total number of verbs
as a feature.
(b) Subject-verb agreement
Research has shown that English-speaking
children with LI have difficulties mark-
ing subject-verb agreement (Clahsen and
Hansen, 1997; Schu?tze and Wexler, 1996).
An illustration of subject-verb disagree-
ment is the following:
...and he were looking behind the rocks
As a way of capturing this information
in the machine learning setting, we con-
sider various bigrams of POS tags: noun
and verb, noun and auxiliary verb, pro-
noun and verb, and pronoun and auxiliary
verb. These features are included in a bag-
of-words fashion using individual counts.
Also, we allow a window between these
pairs to capture agreement between sub-
48
ject and verb that may have modifiers in
between.
(c) Number of different POS tags
This feature is the total number of differ-
ent POS tags in each transcript.
3. Vocabulary knowledge
We use the Number of Different Words (NDW)
to represent vocabulary knowledge of a child.
Although such measures can be biased against
children from different backgrounds, we expect
this possible negative effect to decrease as a re-
sult of having a richer pool of features.
4. Speech fluency
Repetitions, revisions, and filled pauses have
been considered indicators of language learn-
ing difficulties (Thordardottir and Weismer,
2002; Wetherell et al, 2007). In this work
we include as features (a) the number of fillers,
such as uh, um, er; and (b) the number of disflu-
encies (abandoned words) found in each tran-
script.
5. Perplexities from LMs
As mentioned in Section 3.1 we trained LMs of
order 1, 2, and 3 on POS tags extracted from
TD and LI children. We use the perplexity val-
ues from these models as features. Addition-
ally, differences in perplexity values from LI
and TD LMs for different orders are used as
features.
6. Standard scores
A standard score, known as a z-score, is the dif-
ference between an observation and the mean
relative to the standard deviation. For this fea-
ture group, we first find separate distributions
for the MLU in words, NDW and total num-
ber of utterances for the TD and LI populations.
Then, for each transcript, we compute the stan-
dard scores based on each of these six distribu-
tions. This represents how well the child is per-
forming relative to the TD and LI populations.
Note that a cross validation setup was used to
obtain the distribution for the TD and LI chil-
dren for training. This is also required for the
LM features above.
4 Experiments with Monolingual Children
4.1 The Monolingual English Data Set
Our target population for this work is children with
an age range of 3 to 6 years old. However, currently
we do not have any monolingual data sets readily
available to test our approach in this age range. In
the field of communication disorders data sharing
is not a common practice due to the sensitive con-
tent of the material in the language samples of chil-
dren, and also due to the large amount of effort and
time it takes researchers to collect, transcribe, and
code the data before they can begin their analysis.
To evaluate our approach we used a dataset from
CHILDES (MacWhinney, 2000) that includes nar-
ratives from English-speaking adolescents with and
without LI with ages ranging between 13 and 16
years old. Even though the age range is outside the
range we are interested in, we believe that this data
set can still be helpful in exploring the feasibility of
our approach as a first step.
This data set contains 99 TD adolescents and 19
adolescents who met the LI profile at one point in
the duration of the study. There are transcripts from
each child for two tasks: a story telling and a spon-
taneous personal narrative. The first task is a picture
prompted story telling task using the wordless pic-
ture book, ?Frog, Where Are You?? (Mayer, 1969).
In this story telling task children first look at the
story book ?to develop a story in memory? and then
are asked to narrate the story. This type of elicitation
task encourages the use of past tense constructions,
providing plenty of opportunities for extracting clin-
ical markers. In the spontaneous personal narrative
task, the child is asked to talk about a person who an-
noys him/her the most and describe the most annoy-
ing features of that person. This kind of spontaneous
personal narrative encourages the participant for the
use of third person singular forms (-s). Detailed in-
formation of this data set can be found in (Wetherell
et al, 2007).
We processed the transcripts using the CLAN
toolkit (MacWhinney, 2000). MOR and POST from
CLAN are used for morphological analysis and POS
tagging of the children?s speech. We decided to use
these analyzers since they are customized for chil-
dren?s speech.
49
Story telling Personal narrative
Method P (%) R (%) F1 (%) P (%) R (%) F1 (%)
Baseline 28.57 10.53 15.38 33.33 15.79 21.43
1-gram LMs 41.03 84.21 55.17 34.21 68.42 45.61
2-gram LMs 75.00 47.37 58.06 55.56 26.32 35.71
3-gram LMs 80.00 21.05 33.33 87.50 36.84 51.85
Table 1: Evaluation of language models on the monolingual English data set.
Story telling Personal narrative
Algorithm P (%) R (%) F1 (%) P (%) R (%) F1 (%)
Naive Bayes 38.71 63.16 48.00 34.78 42.11 38.10
Bayesian Network 58.33 73.68 65.12 28.57 42.11 34.04
SVM 76.47 68.42 72.22 47.06 42.11 44.44
ANNs 62.50 52.63 57.14 50.00 47.37 48.65
Boosting 70.59 63.16 66.67 69.23 47.37 56.25
Table 2: Evaluation of machine learning algorithms on the monolingual English data set.
4.2 Results with Monolingual
English-Speaking Children
The performance measures we use are: precision
(P), recall (R), and F-measure (F1). Here the LI cat-
egory is the positive class and the TD category is the
negative class.
Table 1 shows the results of leave-one-out-cross-
validation (LOOCV) obtained from the LM ap-
proach for the story telling and spontaneous personal
narrative tasks. It also shows results from a base-
line method that predicts language status by using
standard scores on measures that have been asso-
ciated with LI in children (Dollaghan, 2004). The
three measures we used for the baseline are: MLU
in words, NDW, and total number of utterances pro-
duced. To compute this baseline we estimate the
mean and standard deviation of these measures us-
ing LOOCV with the TD population as our norma-
tive sample. The baseline predicts that a child has
LI if the child scores more than 1.25 SD below the
mean on at least two out of the three measures.
Although LMs yield different results for the story
telling and personal narrative tasks, they both pro-
vide consistently better results than the baseline. For
the story telling task the best results, in terms of the
F1 measure, are achieved by a bigram LM (F1 =
58.06%) while for the personal narrative the highest
F1 measure (51.85%) is from the trigram LM. If we
consider precision, both tasks have the same increas-
ing pattern when increasing LM orders. However for
recall that is not the case. In the story telling task,
recall decreases at the expense of higher precision,
but for the personal narrative task, the trigram LM
reaches a better trade-off between precision and re-
call, which yields a high F1 measure. We also evalu-
ated 4-gram LMs, but results did not improve, most
likely because we do not have enough data to train
higher order LMs.
The results for different ML algorithms are shown
in Table 2, obtained by using all features described
in Section 3.2. The feature based approach us-
ing ML algorithms outperformed using only LMs
on both tasks. For the story telling task, SVM
with a linear kernel achieves the best results (F1 =
72.22%), while Boosting with Decision Stumps pro-
vides the best performance (F1 = 56.25%) for the
personal narrative task.
4.3 Feature and Error Analysis
The ML results shown above use the entire feature
set described in Subsection 3.2. The next question
we ask is the effectiveness of different features for
this task. The datasets we are using in our evalua-
tion are very small, especially considering the num-
ber of positive instances. This prevents us from hav-
ing a separate subset of the data for parameter tun-
ing or feature selection. Therefore, we performed
additional experiments to evaluate the usefulness of
individual features. Figure 1 shows the F1 measures
50
 0
 20
 40
 60
 80
 100
1 2 3 4 5 6
F-m
eas
ure
 (%
)
Features
Story TellingPersonal Narrative
Figure 1: Discriminating power of different groups of
features. The numbers on the x-axis correspond to the
feature groups in Section 3.2.
when using different feature groups. The numbers
on the x-axis correspond to the feature groups de-
scribed in Section 3.2. The F1 measure value for
each of the features is the highest value obtained by
running different ML algorithms for classification.
We noticed that for the story telling task, using
perplexity values from LMs (group 5) as a feature
in the ML setting outperforms the LM threshold ap-
proach by a large margin. It seems that having the
perplexity values as well as the perplexity differ-
ences from all the LMs of different orders in the ML
algorithm provides a better estimation of the target
concept.
Only the standard scores (group 6) yield a higher
F1 measure for the personal narrative task than the
story telling one. The majority of the features (5
out of 6 groups) provide higher F1 measures for the
story telling task, which explains the significantly
better results on this task over the personal narrative
in our learning approach. This is consistent with pre-
vious work contrasting narrative genre stating that
the restrictive setting of a story retell is more reveal-
ing of language difficulties than spontaneous narra-
tives, where the subjects have more control on the
content and style (Wetherell et al, 2007).
We also performed some error analysis for some
of the transcripts that were consistently misidenti-
fied by different ML algorithms. In the story telling
task, we find that some LI transcripts are misclassi-
fied as TD because they (1) have fewer fillers, dis-
fluencies, and degree of support; (2) are similar to
the TD transcripts, which is depicted by the perplex-
ity values for these transcripts; or (3) contain higher
MLU in words as compared to their LI peers. Some
of the reasons for classifying transcripts in the TD
category as LI are shorter MLU in words as com-
pared to other TD peers, large number of fillers, and
excessive repetitions of words and phrases unlike the
other TD children. These factors are consistent with
the effective features that we found from Figure 1.
For the personal narrative task, standard scores
(group 6) and language productivity (group 1) have
an important role in classification, as shown in Fig-
ure 1. The TD transcripts that are misidentified have
lower standard scores and MLU in words than those
of their TD peers.
We believe that another source of noise in the
transcripts comes from the POS tags themselves.
For instance, we found that many verbs in present
tense for third person singular are tagged as plural
nouns, which results in a failure to capture subject-
verb agreement.
Lastly, according to the dataset description, chil-
dren in the LI category met the LI criteria at one
stage in their lifetime and some of these children
also had, or were receiving, some educational sup-
port in the school environment at the time of data
collection. This support for children with LI is
meant to improve their performance on language
related tasks, making the automatic classification
problem more complicated. This also raises the
question about the reference label (TD or LI) for
each child in the data set we used. The details about
which children received interventions are not speci-
fied in the dataset description.
5 Experiments with Bilingual Children
In this section we generalize the approach to a
Spanish-English bilingual population. In adapting
the approach to our bilingual population we face two
challenges: first, what shows to be promising for
a monolingual and highly heterogeneous population
may not be as successful in a bilingual setting where
we expect to have a large variability of exposure to
each language; second, there is a large difference
in the mean age of the monolingual setting and that
of our bilingual one. This age difference will result
in different speech patterns. Younger children pro-
51
duce more ill-formed sentences since they are still
in a language acquisition phase. Lastly, the clini-
cal markers in adolescents are geared towards prob-
lems at the pragmatic and discourse levels, while at
younger ages they focus more on syntax and mor-
phology.
For dealing with the first challenge we are extract-
ing language-specific features and hope that by look-
ing at both languages we can reach a good discrim-
ination performance. For the second challenge, our
feature engineering approach has been focused on
younger children from the beginning. We are aiming
to capture the type of morphosyntactic patterns that
can identify LI in young children. In addition, the
samples in the bilingual population are story retells,
and our feature setting showed to be a good match
for this task. Therefore, we expect our approach to
capture relevant classification patterns, even in the
presence of noisy utterances.
5.1 The Bilingual Data Set
The transcripts for the bilingual LI task come from
an on-going longitudinal study of language impair-
ment in Spanish-English speaking children (Pen?a et
al., 2006a). The children in this study were enrolled
in kindergarten with a mean age of about 70 months.
Of the 59 children, 6 were identified as having a
possible LI by an expert in communication disor-
ders, while 53 were identified as TD. Six of the TD
children were excluded due to missing information,
yielding a total of 47 TD children.
Each child told a series of stories based on Mercer
Mayer?s wordless picture books (Mayer, 1969). Two
stories were told in English and two were told in
Spanish, for a total of four transcripts per child. The
books used for English were ?A Boy, A Dog, and
A Frog? and ?Frog, Where Are You?? The books
used for Spanish retelling were ?Frog on His Own?
and ?Frog Goes to Dinner.? The transcripts for each
separate language were combined, yielding one in-
stance per language for each child.
An interesting aspect of the bilingual data is that
the children mix languages in their narratives. This
phenomenon is called code-switching. At the begin-
ning of a retelling session, the interviewer encour-
ages the child to speak the target language if he/she
is not doing so. Once the child begins speaking the
correct language, any code-switching thereafter is
not corrected by the interviewer. Due to this, the En-
glish transcripts contain Spanish utterances and vice
versa. We believe that words in the non-target lan-
guage help contribute to a more accurate language
development profile. Therefore, in our work we de-
cided to keep these code-switched elements. A com-
bined lexicon approach was used to tag the mixed-
language fragments. If a word does not appear in the
target language lexicon, we apply the POS tag from
the non-target language.
5.2 Spanish-Specific Features
Many structural differences exist between Spanish,
a Romance language, and English, a Germanic lan-
guage. Spanish is morphologically richer than En-
glish. It contains a larger number of different verb
conjugations and it uses a two gender system for
nouns, adjectives, determiners, and participles. A
Spanish-speaking child with LI will have difficulties
with different grammatical elements, such as articles
and clitics, than an English-speaking child (Bedore
and Pen?a, 2008). These differences indicate that the
Spanish feature set will need to be tailored towards
the Spanish language.
To account for Spanish-specific patterns we in-
cluded new POS bigrams as features. To capture
the use of correct and incorrect gender and num-
ber marking morphology, we added noun-adjective,
determiner-noun, and number-noun bigrams to the
list of morphosyntactic features.
5.3 Results on Bilingual Children
Results are shown for the baseline and LM threshold
approach for the bilingual data set in Table 3. The
baseline is computed from the same measures as the
monolingual dataset (MLU in words, NDW, and to-
tal utterances).
Compared to Table 1, the values in Table 3
are generally lower than on the monolingual story
telling task. In this inherently difficult task, the bilin-
gual transcripts are more disfluent than the monolin-
gual ones. This could be due to the age of the chil-
dren or their bilingual status. Recent studies on psy-
cholinguistics and language production have shown
that bilingual speakers have both languages active
at speech production time (Kroll et al, 2008) and
it is possible that this may cause interference, espe-
cially in children still in the phase of language acqui-
52
English Spanish
Method P (%) R (%) F1 (%) P (%) R (%) F1 (%)
Baseline 20.00 16.66 18.18 16.66 16.66 16.66
1-gram LMs 40.00 33.33 36.36 17.64 50.00 26.08
2-gram LMs 50.00 33.33 40.00 33.33 16.66 22.22
3-gram LMs 100.00 33.33 50.00 0.00 0.00 -
Table 3: Evaluation of language models on Bilingual Spanish-English data set.
sition. In addition, the LMs in the monolingual task
were trained using more instances per class, possibly
yielding better results.
There are some different patterns between using
the English and Spanish transcripts. In English,
the unigram models provide the least discriminative
value, and the bigram and trigram models improve
discrimination. We also evaluated higher order n-
grams, but did not obtain any further improvement.
We found that the classification accuracy of the LM
approach was influenced by two children with LI
who were consistently marked as LI due to a greater
perplexity value from the TD LM. A further analysis
shows that these children spoke mostly Spanish on
the ?English? tasks yielding larger perplexities from
the TD LM, which was trained from mostly English.
In contrast, the LI LM was created with transcripts
containing more Spanish than the TD one, and thus
test transcripts with a lot of Spanish do not inflate
perplexity values that much.
For Spanish, unigram LMs provide some discrim-
inative usefulness, and then the bigram performance
decreases while the trigram model provides no dis-
criminative value. One reason for this may be that
the Spanish LMs have a larger vocabulary. In the
Spanish LMs, there are 2/3 more POS tags than in
the English LM. This size difference dramatically
increases the possible bigrams and trigrams, there-
fore increasing the number of parameters to esti-
mate. In addition, we are using an ?off the shelf?
POS tagger (provided by CLAN) and this may add
noise in the feature extraction process. Since we do
not have gold standard annotations for these tran-
scripts, we cannot measure the POS tagging accu-
racy. A rough estimate based on manually revis-
ing one transcript in each language showed a POS
tagging accuracy of 90% for English and 84% for
Spanish. Most of the POS tagger errors involve
verbs, nouns and pronouns. Thus while the accu-
 0
 20
 40
 60
 80
 100
1 2 3 4 5 6
F-m
eas
ure
 (%
)
Features
EnglishSpanish
Combined
Figure 2: Discriminating power of different groups of
features for the bilingual population. The numbers on the
x-axis correspond to the feature groups in Section 3.2.
racy might not seem that low, it can still have a ma-
jor impact on our approach since it involves the POS
categories that are more relevant for this task.
Table 4 shows the results from various ML algo-
rithms. In addition to predicting the language status
with the English and Spanish samples separately, we
also combined the English and Spanish transcripts
together for each child, and used all the features
from both languages in order to allow a prediction
based on both samples. The best F1 measure for this
task (60%) is achieved by using the Naive Bayes al-
gorithm with the combined Spanish-English feature
set. This is an improvement over both the separate
English and Spanish trials. The Naive Bayes algo-
rithm provided the best discrimination for the En-
glish (54%) and Combined data sets and Boosting
and SVM provided the best discrimination for the
Spanish set (18%).
5.4 Feature Analysis
Similar to the monolingual dataset, we performed
additional experiments exploring the contribution
of different groups of features. We tested the six
53
English Spanish Combined
Algorithm P (%) R (%) F1 (%) P (%) R (%) F1 (%) P (%) R (%) F1 (%)
ANNs 66.66 33.33 44.44 0.00 0.00 - 100.00 16.66 28.57
SVM 14.28 16.66 15.38 20.00 16.66 18.18 66.66 33.33 44.44
Naive Bayes 60.00 50.00 54.54 0.00 0.00 - 75.00 50.00 60.00
Logistic Regression 25.00 16.66 20.00 - 0.00 - 50.00 33.33 40.00
Boosting 50.00 33.33 40.00 20.00 16.66 18.18 66.66 33.33 44.44
Table 4: Evaluation of machine learning algorithms on the Bilingual Spanish-English data set.
groups of features described in Section 3.2 sepa-
rately. Overall, the combined LM perplexity val-
ues (group 5) provided the best discriminative value
(F1 = 66%). The LM perplexity values performed
the best for English. It even outperformed using all
the features in the ML algorithm, suggesting some
feature selection is needed for this task.
The morpohsyntactic skills (group 2) provided the
best discriminative value for the Spanish language
features, and performed better than the complete
feature set for Spanish. Within group 2, we evalu-
ated different POS bigrams for the Spanish and En-
glish sets and observed that most of the bigram com-
binations by themselves are usually weak predictors
of language status. In the Spanish set, out of all of
the lexical combinations, only the determiner-noun,
noun-verb, and pronoun-verb categories provided
some discriminative value. The determiner-noun
category captured the correct and incorrect gender
marking between the two POS tags. The noun-verb
and pronoun-verb categories covered the correct and
incorrect usage of subject-verb combinations. In-
terestingly enough, the pronoun-verb category per-
formed well by itself, yielding an F1 measure of
54%. There are also some differences in the frequen-
cies of bigram features in the English and Spanish
data sets. For example, there is no noun-auxiliary
POS pattern in Spanish, and the pronoun-auxiliary
bigram appears less frequently in Spanish than in
English because in Spanish the use of personal pro-
nouns is not mandatory since the verb inflection will
disambiguate the subject of the sentence.
The vocabulary knowledge feature (group 3) did
not provide any discriminative value for any of the
language tasks. This may be because bilingual chil-
dren receive less input for each language than a
monolingual child learning one language, or due to
the varied vocabulary acquisition rate in our bilin-
gual population.
6 Conclusions and Future Work
In this paper we present results on the use of LMs
and ML techniques trained on features representing
different aspects of language gathered from spon-
taneous speech samples for the task of assisting
clinicians in determining language status in chil-
dren. First, we evaluate our approach on a monolin-
gual English-speaking population. Next, we show
that this ML approach can be successfully adapted
to a bilingual Spanish-English population. ML al-
gorithms provide greater discriminative power than
only using a threshold approach with LMs.
Our current efforts are devoted to improving pre-
diction accuracy by refining our feature set. We are
working on creating a gold standard corpus of chil-
dren?s transcripts annotated with POS tags. This
data set will help us improve accuracy on our POS-
based features. We are also exploring the use of
socio-demographic features such as the educational
level of parents, the gender of children, and enroll-
ment status on free lunch programs.
Acknowledgments
This work was supported by NSF grant 0812134,
and by grant 5 UL1 RR024982 from NCRR, a com-
ponent of NIH. We also thank the three NAACL re-
viewers for insightful comments on the submitted
version of this paper.
References
Lisa M. Bedore and Laurence B. Leonard. 2005. Verb
inflections and noun phrase morphology in the sponta-
neous speech of Spanish-speaking children with spe-
cific language impairment. Applied Psycholinguistics,
26(2):195?225.
54
Lisa M. Bedore and Elizabeth D. Pen?a. 2008. Assess-
ment of bilingual children for identification of lan-
guage impairment: Current findings and implications
for practice. International Journal of Bilingual Edu-
cation and Bilingualism, 11(1):1?29.
Nicola Botting. 2002. Narrative as a tool for the assess-
ment of linguistic and pragmatic impairments. Child
Language Teaching and Therapy, 18(1):1?21.
Thomas Campbell, Chris Dollaghan, Herbert Needle-
man, and Janine Janosky. 1997. Reducing bias in lan-
guage assessment: Processing-dependent measures.
Journal of Speech, Language, and Hearing Research,
40(3):519?525.
Harald Clahsen and Detlef Hansen. 1997. The grammat-
ical agreement deficit in specific language impairment:
Evidence from therapy experiments. In Myrna Gop-
nik, editor, The Inheritance and Innateness of Gram-
mar, chapter 7. Oxford University Press, New York.
Christine A. Dollaghan and Thomas F. Campbell. 1998.
Nonword repetition and child language impairment.
Journal of Speech, Language, and Hearing Research,
41(5):1136?1146.
Christine A. Dollaghan. 2004. Taxometric analyses of
specific language impairment in 3- and 4-year-old chil-
dren. Journal of Speech, Language, and Hearing Re-
search, 47(2):464?475.
Peggy F. Jacobson and Richard G. Schwartz. 2002.
Morphology in incipient bilingual Spanish-speaking
preschool children with specific language impairment.
Applied Psycholinguistics, 23(1):23?41.
Judith F. Kroll, Chip Gerfen, and Paola E. Dussias. 2008.
Laboratory designs and paradigms: Words, sounds,
sentences. In L. Wei and M. G. Moyer, editors, The
Blackwell Guide to Research Methods in Bilingualism
and Multilingualism, chapter 7. Blackwell Pub.
Laurence B. Leonard, Julia A. Eyer, Lisa M. Bedore,
and Bernard G. Grela. 1997. Three accounts of
the grammatical morpheme difficulties of English-
speaking children with specific language impairment.
Journal of Speech, Language, and Hearing Research,
40(4):741?753.
Brian MacWhinney. 2000. The CHILDES project: Tools
for analyzing talk. Lawrence Erlbaum, Mahwah, NJ.
Mercer Mayer. 1969. Frog, where are you? Dial Press.
Elizabeth D. Pen?a, Lisa M. Bedore, Ronald B. Gillam,
and Thomas Bohman. 2006a. Diagnostic markers
of language impairment in bilingual children. Grant
awarded by the NIDCD, NIH.
Elizabeth D. Pen?a, Tammie J. Spaulding, and Elena
Plante. 2006b. The composition of normative groups
and diagnostic decision making: Shooting ourselves
in the foot. American Journal of Speech-Language
Pathology, 15(3):247?254.
Elena Plante and Rebecca Vance. 1994. Selection
of preschool language tests: A data-based approach.
Language, Speech, and Hearing Services in Schools,
25(1):15?24.
Mar??a Adelaida Restrepo and Vera F. Gutie?rrez-Clellen.
2001. Article use in Spanish-speaking children with
specific language impairment. Journal of Child Lan-
guage, 28(2):433?452.
Mabel L. Rice and Kenneth Wexler. 1996. Toward tense
as a clinical marker of specific language impairment
in English-speaking children. Journal of Speech and
Hearing Research, 39(6):1239?1257.
Brian Roark, Margaret Mitchell, and Kristy Holling-
shead. 2007. Syntactic complexity measures for de-
tecting mild cognitive impairment. In Proceedings of
the Workshop on BioNLP 2007, pages 1?8. ACL.
Carson T. Schu?tze and Kenneth Wexler. 1996. Subject
case licensing and English root infinitives. In Proceed-
ings of the 20th Annual Boston University Conference
on Language Development. Cascadilla Press.
Thamar Solorio and Yang Liu. 2008. Using language
models to identify language impairment in Spanish-
English bilingual children. In Proceedings of the
Workshop on BioNLP 2008, pages 116?117. ACL.
Tammie J. Spaulding, Elena Plante, and Kimberly A.
Farinella. 2006. Eligibility criteria for language im-
pairment: Is the low end of normal always appropri-
ate? Language, Speech, and Hearing Services in
Schools, 37(1):61?72.
Andreas Stolcke. 2002. SRILM ? an extensible lan-
guage modeling toolkit. In Proceedings of the Inter-
national Conference on Spoken Language Processing,
volume 2, pages 901?904.
Elin T. Thordardottir and Susan Ellis Weismer. 2002.
Content mazes and filled pauses on narrative language
samples of children with specific language impair-
ment. Brain and Cognition, 48(2-3):587?592.
J. Bruce Tomblin, Nancy L. Records, Paula Buckwal-
ter, Xuyang Zhang, Elaine Smith, and Marlea O?Brien.
1997. Prevalence of specific language impairment in
kindergarten children. Journal of Speech, Language,
and Hearing Research, 40(6):1245?1260.
Danielle Wetherell, Nicola Botting, and Gina Conti-
Ramsden. 2007. Narrative in adolescent specific
language impairment (SLI): a comparison with peers
across two different narrative genres. International
Journal of Language and Communication Disorders,
42:583?605(23).
Kenneth Wexler. 1994. Optional infinitives. In David
Lightfoot and Norbert Hornstein, editors, Verb Move-
ment. Cambridge University Press.
Ian H. Witten and Eibe Frank. 1999. Data Mining: Prac-
tical Machine Learning Tools and Techniques with
Java Implementations. Morgan Kaufmann.
55
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 620?628,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Unsupervised Approaches for Automatic Keyword Extraction Using
Meeting Transcripts
Feifan Liu, Deana Pennell, Fei Liu and Yang Liu
Computer Science Department
The University of Texas at Dallas
Richardson, TX 75080, USA
{ffliu,deana,feiliu,yangl}@hlt.utdallas.edu
Abstract
This paper explores several unsupervised ap-
proaches to automatic keyword extraction
using meeting transcripts. In the TFIDF
(term frequency, inverse document frequency)
weighting framework, we incorporated part-
of-speech (POS) information, word clustering,
and sentence salience score. We also evalu-
ated a graph-based approach that measures the
importance of a word based on its connection
with other sentences or words. The system
performance is evaluated in different ways, in-
cluding comparison to human annotated key-
words using F-measure and a weighted score
relative to the oracle system performance, as
well as a novel alternative human evaluation.
Our results have shown that the simple un-
supervised TFIDF approach performs reason-
ably well, and the additional information from
POS and sentence score helps keyword ex-
traction. However, the graph method is less
effective for this domain. Experiments were
also performed using speech recognition out-
put and we observed degradation and different
patterns compared to human transcripts.
1 Introduction
Keywords in a document provide important infor-
mation about the content of the document. They
can help users search through information more effi-
ciently or decide whether to read a document. They
can also be used for a variety of language process-
ing tasks such as text categorization and informa-
tion retrieval. However, most documents do not
provide keywords. This is especially true for spo-
ken documents. Current speech recognition system
performance has improved significantly, but there
is no rich structural information such as topics and
keywords in the transcriptions. Therefore, there is
a need to automatically generate keywords for the
large amount of written or spoken documents avail-
able now.
There have been many efforts toward keyword ex-
traction for text domain. In contrast, there is less
work on speech transcripts. In this paper we fo-
cus on one speech genre ? the multiparty meeting
domain. Meeting speech is significantly different
from written text and most other speech data. For
example, there are typically multiple participants
in a meeting, the discussion is not well organized,
and the speech is spontaneous and contains disflu-
encies and ill-formed sentences. It is thus ques-
tionable whether we can adopt approaches that have
been shown before to perform well in written text
for automatic keyword extraction in meeting tran-
scripts. In this paper, we evaluate several differ-
ent keyword extraction algorithms using the tran-
scripts of the ICSI meeting corpus. Starting from
the simple TFIDF baseline, we introduce knowl-
edge sources based on POS filtering, word cluster-
ing, and sentence salience score. In addition, we
also investigate a graph-based algorithm in order to
leverage more global information and reinforcement
from summary sentences. We used different per-
formance measurements: comparing to human an-
notated keywords using individual F-measures and
a weighted score relative to the oracle system per-
formance, and conducting novel human evaluation.
Experiments were conducted using both the human
transcripts and the speech recognition (ASR) out-
620
put. Overall the TFIDF based framework seems to
work well for this domain, and the additional knowl-
edge sources help improve system performance. The
graph-based approach yielded worse results, espe-
cially for the ASR condition, suggesting further in-
vestigation for this task.
2 Related Work
TFIDF weighting has been widely used for keyword
or key phrase extraction. The idea is to identify
words that appear frequently in a document, but do
not occur frequently in the entire document collec-
tion. Much work has shown that TFIDF is very ef-
fective in extracting keywords for scientific journals,
e.g., (Frank et al, 1999; Hulth, 2003; Kerner et al,
2005). However, we may not have a big background
collection that matches the test domain for a reli-
able IDF estimate. (Matsuo and Ishizuka, 2004) pro-
posed a co-occurrence distribution based method us-
ing a clustering strategy for extracting keywords for
a single document without relying on a large corpus,
and reported promising results.
Web information has also been used as an ad-
ditional knowledge source for keyword extraction.
(Turney, 2002) selected a set of keywords first and
then determined whether to add another keyword hy-
pothesis based on its PMI (point-wise mutual infor-
mation) score to the current selected keywords. The
preselected keywords can be generated using basic
extraction algorithms such as TFIDF. It is impor-
tant to ensure the quality of the first selection for the
subsequent addition of keywords. Other researchers
also used PMI scores between each pair of candidate
keywords to select the top k% of words that have
the highest average PMI scores as the final keywords
(Inkpen and Desilets, 2004).
Keyword extraction has also been treated as a
classification task and solved using supervised ma-
chine learning approaches (Frank et al, 1999; Tur-
ney, 2000; Kerner et al, 2005; Turney, 2002; Tur-
ney, 2003). In these approaches, the learning al-
gorithm needs to learn to classify candidate words
in the documents into positive or negative examples
using a set of features. Useful features for this ap-
proach include TFIDF and its variations, position of
a phrase, POS information, and relative length of a
phrase (Turney, 2000). Some of these features may
not work well for meeting transcripts. For exam-
ple, the position of a phrase (measured by the num-
ber of words before its first appearance divided by
the document length) is very useful for news article
text, since keywords often appear early in the doc-
ument (e.g., in the first paragraph). However, for
the less well structured meeting domain (lack of ti-
tle and paragraph), these kinds of features may not
be indicative. A supervised approach to keyword ex-
traction was used in (Liu et al, 2008). Even though
the data set in that study is not very big, it seems that
a supervised learning approach can achieve reason-
able performance for this task.
Another line of research for keyword extrac-
tion has adopted graph-based methods similar to
Google?s PageRank algorithm (Brin and Page,
1998). In particular, (Wan et al, 2007) attempted
to use a reinforcement approach to do keyword ex-
traction and summarization simultaneously, on the
assumption that important sentences usually contain
keywords and keywords are usually seen in impor-
tant sentences. We also find that this assumption also
holds using statistics obtained from the meeting cor-
pus used in this study. Graph-based methods have
not been used in a genre like the meeting domain;
therefore, it remains to be seen whether these ap-
proaches can be applied to meetings.
Not many studies have been performed on speech
transcripts for keyword extraction. The most rel-
evant work to our study is (Plas et al, 2004),
where the task is keyword extraction in the mul-
tiparty meeting corpus. They showed that lever-
aging semantic resources can yield significant per-
formance improvement compared to the approach
based on the relative frequency ratio (similar to
IDF). There is also some work using keywords for
other speech processing tasks, e.g., (Munteanu et
al., 2007; Bulyko et al, 2007; Wu et al, 2007; De-
silets et al, 2002; Rogina, 2002). (Wu et al, 2007)
showed that keyword extraction combined with se-
mantic verification can be used to improve speech
retrieval performance on broadcast news data. In
(Rogina, 2002), keywords were extracted from lec-
ture slides, and then used as queries to retrieve rel-
evant web documents, resulting in an improved lan-
guage model and better speech recognition perfor-
mance of lectures. There are many differences be-
tween written text and speech ? meetings in par-
ticular. Thus our goal in this paper is to investi-
621
gate whether we can successfully apply some exist-
ing techniques, as well as propose new approaches
to extract keywords for the meeting domain. The
aim of this study is to set up some starting points for
research in this area.
3 Data
We used the meetings from the ICSI meeting data
(Janin et al, 2003), which are recordings of naturally
occurring meetings. All the meetings have been
transcribed and annotated with dialog acts (DA)
(Shriberg et al, 2004), topics, and extractive sum-
maries (Murray et al, 2005). The ASR output for
this corpus is obtained from a state-of-the-art SRI
conversational telephone speech system (Zhu et al,
2005), with a word error rate of about 38.2% on
the entire corpus. We align the human transcripts
and ASR output, then map the human annotated DA
boundaries and topic boundaries to the ASR words,
such that we have human annotation of these infor-
mation for the ASR output.
We recruited three Computer Science undergradu-
ate students to annotate keywords for each topic seg-
ment, using 27 selected ICSI meetings.1 Up to five
indicative key words or phrases were annotated for
each topic. In total, we have 208 topics annotated
with keywords. The average length of the topics
(measured using the number of dialog acts) among
all the meetings is 172.5, with a high standard devi-
ation of 236.8. We used six meetings as our devel-
opment set (the same six meetings as the test set in
(Murray et al, 2005)) to optimize our keyword ex-
traction methods, and the remaining 21 meetings for
final testing in Section 5.
One example of the annotated keywords for a
topic segment is:
? Annotator I: analysis, constraints, template
matcher;
? Annotator II: syntactic analysis, parser, pattern
matcher, finite-state transducers;
? Annotator III: lexicon, set processing, chunk
parser.
Note that these meetings are research discussions,
and that the annotators may not be very familiar with
1We selected these 27 meetings because they have been used
in previous work for topic segmentation and summarization
(Galley et al, 2003; Murray et al, 2005).
the topics discussed and often had trouble deciding
the important sentences or keywords. In addition,
limiting the number of keywords that an annotator
can select for a topic also created some difficulty.
Sometimes there are more possible keywords and
the annotators felt it is hard to decide which five are
the most topic indicative. Among the three annota-
tors, we notice that in general the quality of anno-
tator I is the poorest. This is based on the authors?
judgment, and is also confirmed later by an indepen-
dent human evaluation (in Section 6).
For a better understanding of the gold standard
used in this study and the task itself, we thoroughly
analyzed the human annotation consistency. We re-
moved the topics labeled with ?chitchat? by at least
one annotator, and also the digit recording part in
the ICSI data, and used the remaining 140 topic seg-
ments. We calculated the percentage of keywords
agreed upon by different annotators for each topic,
as well as the average for all the meetings. All of the
consistency analysis is performed based on words.
Figure 1 illustrates the annotation consistency over
different meetings and topics. The average consis-
tency rate across topics is 22.76% and 5.97% among
any two and all three annotators respectively. This
suggests that people do not have a high agreement
on keywords for a given document. We also notice
that the two person agreement is up to 40% for sev-
eral meetings and 80% for several individual top-
ics, and the agreement among all three annotators
reaches 20% and 40% for some meetings or topics.
This implies that the consistency depends on topics
(e.g., the difficulty or ambiguity of a topic itself, the
annotators? knowledge of that topic). Further studies
are needed for the possible factors affecting human
agreement. We are currently creating more annota-
tions for this data set for better agreement measure
and also high quality annotation.
4 Methods
Our task is to extract keywords for each of the topic
segments in each meeting transcript. Therefore, by
?document?, we mean a topic segment in the re-
mainder of this paper. Note that our task is different
from keyword spotting, where a keyword is provided
and the task is to spot it in the audio (along with its
transcript).
The core part of keyword extraction is for the sys-
622
0
0.2
0.4
0.6
0.8
1
0 30 60 90 120
3 agree
2 agree
0
0.1
0.2
0.3
0.4
0.5
1 3 5 7 9 11 13 15 17 19 21 23 25 27
3 agree
2 agree
Figure 1: Human annotation consistency across differ-
ent topics (upper graph) and meetings (lower graph). Y-
axis is the percent of the keywords agreed upon by two or
three annotators.
tem to assign an importance score to a word, and
then pick the top ranked words as keywords. We
compare different methods for weight calculation in
this study, broadly divided into the following two
categories: the TFIDF framework and the graph-
based model. Both are unsupervised learning meth-
ods.2 In all of the following approaches, when se-
lecting the final keywords, we filter out any words
appearing on the stopword list. These stopwords are
generated based on the IDF values of the words us-
ing all the meeting data by treating each topic seg-
ment as a document. The top 250 words from this
list (with the lowest IDF values) were used as stop-
words. We generated two different stopword lists for
human transcripts and ASR output respectively. In
addition, in this paper we focus on performing key-
word extraction at the single word level, therefore
no key phrases are generated.
2Note that by unsupervised methods, we mean that no data
annotated with keywords is needed. These methods do require
the use of some data to generate information such as IDF, or
possibly a development set to optimize some parameters or
heuristic rules.
4.1 TFIDF Framework
(A) Basic TFIDF weighting
The term frequency (TF) for a word wi in a doc-
ument is the number of times the word occurs in the
document. The IDF value is:
IDFi = log(N/Ni)
whereNi denotes the number of the documents con-
taining word wi, and N is the total number of the
documents in the collection. We also performed L2
normalization for the IDF values when combining
them with other scores.
(B) Part of Speech (POS) filtering
In addition to using a stopword list to remove
words from consideration, we also leverage POS in-
formation to filter unlikely keywords. Our hypothe-
sis is that verb, noun and adjective words are more
likely to be keywords, so we restrict our selection to
words with these POS tags only. We used the TnT
POS tagger (Brants, 2000) trained from the Switch-
board data to tag the meeting transcripts.
(C) Integrating word clustering
One weakness of the baseline TFIDF is that it
counts the frequency for a particular word, without
considering any words that are similar to it in terms
of semantic meaning. In addition, when the docu-
ment is short, the TF may not be a reliable indicator
of the importance of the word. Our idea is therefore
to account for the frequency of other similar words
when calculating the TF of a word in the document.
For this, we group all the words into clusters in an
unsupervised fashion. If the total term frequency
of all the words in one cluster is high, it is likely
that this cluster contributes more to the current topic
from a thematic point of view. Thus we want to as-
sign higher weights to the words in this cluster.
We used the SRILM toolkit (Stolcke, 2002) for
automatic word clustering over the entire docu-
ment collection. It minimizes the perplexity of the
induced class-based n-gram language model com-
pared to the original word-based model. Using the
clusters, we then adjust the TF weighting by inte-
grating with the cluster term frequency (CTF):
TF CTF (wi) = TF (wi)??(
P
wl?Ci,wl 6=wi freq(wl))
where the last summation component means the to-
tal term frequency of all the other words in this docu-
ment that belong to the same clusterCi as the current
623
word wi. We set parameter ? to be slightly larger
than 1. We did not include stopwords when adding
the term frequencies for the words in a cluster.
(D) Combining with sentence salience score
Intuitively, the words in an important sentence
should be assigned a high weight for keyword ex-
traction. In order to leverage the sentence infor-
mation, we adjust a word?s weight by the salience
scores of the sentences containing that word. The
sentence score is calculated based on its cosine sim-
ilarity to the entire meeting. This score is often used
in extractive summarization to select summary sen-
tences (Radev et al, 2001). The cosine similarity
between two vectors, D1 and D2, is defined as:
sim(D1, D2) =
?
i t1it2i??
i t21i ?
??
i t22i
where ti is the term weight for a word wi, for which
we use the TFIDF value.
4.2 Graph-based Methods
For the graph-based approach, we adopt the itera-
tive reinforcement approach from (Wan et al, 2007)
in the hope of leveraging sentence information for
keyword extraction. This algorithm is based on the
assumption that important sentences/words are con-
nected to other important sentences/words.
Four graphs are created: one graph in which sen-
tences are connected to other sentences (S-S graph),
one in which words are connected to other words
(W-W graph), and two graphs connecting words to
sentences with uni-directional edges (W-S and S-W
graphs). Stopwords are removed before the creation
of the graphs so they will be ineligible to be key-
words.
The final weight for a word node depends on its
connection to other words (W-W graph) and other
sentences (W-S graph); similarly, the weight for
a sentence node is dependent on its connection to
other sentences (S-S graph) and other words (S-W
graph). That is,
u = ?UTu+ ?W? T v
v = ?V T v + ?W Tu
where u and v are the weight vectors for sentence
and word nodes respectively, U, V,W, W? represent
the S-S, W-W, S-W, and W-S connections. ? and ?
specify the contributions from the homogeneous and
the heterogeneous nodes. The initial weight is a uni-
form one for the word and sentence vector. Then
the iterative reinforcement algorithm is used until
the node weight values converge (the difference be-
tween scores at two iterations is below 0.0001 for all
nodes) or 5,000 iterations are reached.
We have explored various ways to assign weights
to the edges in the graphs. Based on the results on
the development set, we use the following setup in
this paper:
? W-W Graph: We used a diagonal matrix for
the graph connection, i.e., there is no connec-
tion among words. The self-loop values are
the TFIDF values of the words. This is also
equivalent to using an identity matrix for the
word-word connection and TFIDF as the initial
weight for each vertex in the graph. We investi-
gated other strategies to assign a weight for the
edge between two word nodes; however, so far
the best result we obtained is using this diago-
nal matrix.
? S-W and W-S Graphs: The weight for an
edge between a sentence and a word is the TF
of the word in the sentence multiplied by the
word?s IDF value. These weights are initially
added only to the S-W graph, as in (Wan et al,
2007); then that graph is normalized and trans-
posed to create the W-S graph.
? S-S Graph: The sentence node uses a vector
space model and is composed of the weights of
those words connected to this sentence in the
S-W graph. We then use cosine similarity be-
tween two sentence vectors.
Similar to the above TFIDF framework, we also
use POS filtering for the graph-based approach. Af-
ter the weights for all the words are determined, we
select the top ranked words with the POS restriction.
5 Experimental Results: Automatic
Evaluation
Using the approaches described above, we com-
puted weights for the words and then picked the top
five words as the keywords for a topic. We chose five
keywords since this is the number of keywords that
624
human annotators used as a guideline, and it also
yielded good performance in the development set.
To evaluate system performance, in this section we
use human annotated keywords as references, and
compare the system output to them. The first metric
we use is F-measure, which has been widely used
for this task and other detection tasks. We compare
the system output with respect to each human anno-
tation, and calculate the maximum and the average
F-scores. Note that our keyword evaluation is word-
based. When human annotators choose key phrases
(containing more than one word), we split them into
words and measure the matching words. Therefore,
when the system only generates five keywords, the
upper bound of the recall rate may not be 100%. In
(Liu et al, 2008), a lenient metric is used which ac-
counts for some inflection of words. Since that is
highly correlated with the results using exact word
match, we report results based on strict matching in
the following experiments.
The second metric we use is similar to Pyramid
(Nenkova and Passonneau, 2004), which has been
used for summarization evaluation. Instead of com-
paring the system output with each individual hu-
man annotation, the method creates a ?pyramid?
using all the human annotated keywords, and then
compares system output to this pyramid. The pyra-
mid consists of all the annotated keywords at dif-
ferent levels. Each keyword has a score based on
how many annotators have selected this one. The
higher the score, the higher up the keyword will be in
the pyramid. Then we calculate an oracle score that
a system can obtain when generating k keywords.
This is done by selecting keywords in the decreas-
ing order in terms of the pyramid levels until we
obtain k keywords. Finally for the system hypoth-
esized k keywords, we compute its score by adding
the scores of the keywords that match those in the
pyramid. The system?s performance is measured us-
ing the relative performance of the system?s pyramid
scores divided by the oracle score.
Table 1 shows the results using human transcripts
for different methods on the 21 test meetings (139
topic segments in total). For comparison, we also
show results using the supervised approach as in
(Liu et al, 2008), which is the average of the 21-
fold cross validation. We only show the maximum
F-measure with respect to individual annotations,
since the average scores show similar trend. In ad-
dition, the weighted relative scores already accounts
for the different annotation and human agreement.
Methods F-measure weighted relative score
TFIDF 0.267 0.368
+ POS 0.275 0.370
+ Clustering 0.277 0.367
+ Sent weight 0.290 0.404
Graph 0.258 0.364
Graph+POS 0.277 0.380
Supervised 0.312 0.401
Table 1: Keyword extraction results using human tran-
scripts compared to human annotations.
We notice that for the TFIDF framework, adding
POS information slightly helps the basic TFIDF
method. In all the meetings, our statistics show that
adding POS filtering removed 2.3% of human anno-
tated keywords from the word candidates; therefore,
this does not have a significant negative impact on
the upper bound recall rate, but helps eliminate un-
likely keyword candidates. Using word clustering
does not yield a performance gain, most likely be-
cause of the clustering technique we used ? it does
clustering simply based on word co-occurrence and
does not capture semantic similarity properly.
Combining the term weight with the sentence
salience score improves performance, supporting the
hypothesis that summary sentences and keywords
can reinforce each other. In fact we performed an
analysis of keywords and summaries using the fol-
lowing two statistics:
(1) k = Psummary(wi)Ptopic(wi)
where Psummary(wi) and Ptopic(wi) represent the
the normalized frequency of a keyword wi in the
summary and the entire topic respectively; and
(2) s = PSsummaryPStopic
where PSsummary represents the percentage of the
sentences containing at least one keyword among all
the sentences in the summary, and similarly PStopic
is measured using the entire topic segment. We
found that the average k and s are around 3.42 and
6.33 respectively. This means that keywords are
625
more likely to occur in the summary compared to the
rest of the topic, and the chance for a summary sen-
tence to contain at least one keyword is much higher
than for the other sentences in the topic.
For the graph-based methods, we notice that
adding POS filtering also improves performance,
similar to the TFIDF framework. However, the
graph method does not perform as well as the TFIDF
approach. Comparing with using TFIDF alone, the
graph method (without using POS) yielded worse re-
sults. In addition to using the TFIDF for the word
nodes, information from the sentences is used in the
graph method since a word is linked to sentences
containing this word. The global information in the
S-S graph (connecting a sentence to other sentences
in the document) is propagated to the word nodes.
Unlike the study in (Wan et al, 2007), this infor-
mation does not yield any gain. We did find that the
graph approach performed better in the development
set, but it seems that it does not generalize to this test
set.
Compared to the supervised results, the TFIDF
approach is worse in terms of the individual maxi-
mum F-measure, but achieves similar performance
when using the weighted relative score. However,
the unsupervised TFIDF approach is much simpler
and does not require any annotated data for train-
ing. Therefore it may be easily applied to a new
domain. Again note that these results used word-
based selection. (Liu et al, 2008) investigated
adding bigram key phrases, which we expect to
be independent of these unigram-based approaches
and adding bigram phrases will yield further per-
formance gain for the unsupervised approach. Fi-
nally, we analyzed if the system?s keyword ex-
traction performance is correlated with human an-
notation disagreement using the unsupervised ap-
proach (TFIDF+POS+Sent weight). The correla-
tion (Spearman?s ? value) between the system?s
F-measure and the three-annotator consistency on
the 27 meetings is 0.5049 (p=0.0072). This indi-
cates that for the meetings with a high disagreement
among human annotators, it is also challenging for
the automatic systems.
Table 2 shows the results using ASR output for
various approaches. The performance measure is
the same as used in Table 1. We find that in gen-
eral, there is a performance degradation compared
to using human transcripts, which is as expected.
We found that only 59.74% of the human annotated
keywords appear in ASR output, that is, the upper
bound of recall is very low. The TFIDF approach
still outperforms the graph method. Unlike on hu-
man transcripts, the addition of information sources
in the TFIDF approach did not yield significant per-
formance gain. A big difference from the human
transcript condition is the use of sentence weight-
ing ? adding it degrades performance in ASR, in
contrast to the improvement in human transcripts.
This is possibly because the weighting of the sen-
tences is poor when there are many recognition er-
rors from content words. In addition, compared to
the supervised results, the TFIDF method has sim-
ilar maximum F-measure, but is slightly worse us-
ing the weighted score. Further research is needed
for the ASR condition to investigate better modeling
approaches.
Methods F-measure weighted relative score
TFIDF 0.191 0.257
+ POS 0.196 0.259
+ Clustering 0.196 0.259
+ Sent weigh 0.178 0.241
Graph 0.173 0.223
Graph+POS 0.183 0.233
Supervised 0.197 0.269
Table 2: Keyword extraction results using ASR output.
6 Experimental Results: Human
Evaluation
Given the disagreement among human annotators,
one question we need to answer is whether F-
measure or even the weighted relative scores com-
pared with human annotations are appropriate met-
rics to evaluate system-generated keywords. For
example, precision measures among the system-
generated keywords how many are correct. How-
ever, this does not measure if the unmatched system-
generated keywords are bad or acceptable. We
therefore performed a small scale human evaluation.
We selected four topic segments from four differ-
ent meetings, and gave output from different sys-
tems to five human subjects. The subjects ranged
in age from 22 to 63, and all but one had only basic
knowledge of computers. We first asked the eval-
626
uators to read the entire topic transcript, and then
presented them with the system-generated keywords
(randomly ordered by different systems). For com-
parison, the keywords annotated by our three hu-
man annotators were also included without reveal-
ing which sets of keywords were generated by a
human and which by a computer. Because there
was such disagreement between annotators regard-
ing what made good keywords, we instead asked our
evaluators to mark any words that were definitely
not keywords. Systems that produced more of these
rejected words (such as ?basically? or ?mmm-hm?)
are assumed to be worse than those containing fewer
rejected words. We then measured the percentage of
rejected keywords for each system/annotator. The
results are shown in Table 3. Not surprisingly, the
human annotations rank at the top. Overall, we find
human evaluation results to be consistent with the
automatic evaluation metrics in terms of the ranking
of different systems.
Systems Rejection rate
Annotator 2 8%
Annotator 3 19%
Annotator 1 25%
TFIDF + POS 28%
TFIDF 30%
Table 3: Human evaluation results: percentage of the re-
jected keywords by human evaluators for different sys-
tems/annotators.
Note this rejection rate is highly related to the re-
call/precision measure in the sense that it measures
how many keywords are acceptable (or rejected)
among the system generated ones. However, instead
of comparing to a fixed set of human annotated key-
words (e.g., five) and using that as a gold standard
to compute recall/precision, in this evaluation, the
human evaluator may have a larger set of accept-
able keywords in their mind. We also measured the
human evaluator agreement regarding the accepted
or bad keywords. We found that the agreement on
a bad keyword among five, four, and three human
evaluator is 10.1%, 14.8%, and 10.1% respectively.
This suggests that humans are more likely to agree
on a bad keyword selection compared to agreement
on the selected keywords, as discussed in Section 3
(even though the data sets in these two analysis are
not the same). Another observation from the human
evaluation is that sometimes a person rejects a key-
word from one system output, but accepts that on
the list from another system. We are not sure yet
whether this is the inconsistency from human evalu-
ators or whether the judgment is based on a word?s
occurrence with other provided keywords and thus
some kind of semantic coherence. Further investi-
gation on human evaluation is still needed.
7 Conclusions and Future Work
In this paper, we evaluated unsupervised keyword
extraction performance for the meeting domain, a
genre that is significantly different from most pre-
vious work. We compared several different ap-
proaches using the transcripts of the ICSI meeting
corpus. Our results on the human transcripts show
that the simple TFIDF based method is very compet-
itive. Adding additional knowledge such as POS and
sentence salience score helps improve performance.
The graph-based approach performs less well in this
task, possibly because of the lack of structure in
this domain. We use different performance measure-
ments, including F-measure with respect to individ-
ual human annotations and a weighted metric rela-
tive to the oracle system performance. We also per-
formed a new human evaluation for this task and our
results show consistency with the automatic mea-
surement. In addition, experiments on the ASR out-
put show performance degradation, but more impor-
tantly, different patterns in terms of the contributions
of information sources compared to using human
transcripts. Overall the unsupervised approaches are
simple but effective; however, system performance
compared to the human performance is still low,
suggesting more work is needed for this domain.
For the future work, we plan to investigate dif-
ferent weighting algorithms for the graph-based ap-
proach. We also need a better way to decide the
number of keywords to generate instead of using a
fixed number. Furthermore, since there are multiple
speakers in the meeting domain, we plan to incor-
porate speaker information in various approaches.
More importantly, we will perform a more rigorous
human evaluation, and also use extrinsic evaluation
to see whether automatically generated keywords fa-
cilitate tasks such as information retrieval or meeting
browsing.
627
Acknowledgments
This work is supported by NSF award IIS-0714132.
Any opinions expressed in this work are those of the
authors and do not necessarily reflect the views of
NSF.
References
T. Brants. 2000. TnT ? a statistical part-of-speech tagger.
In Proceedings of the 6th Applied NLP Conference.
S. Brin and L. Page. 1998. The anatomy of a large-scale
hypertextual web search engine. Computer Networks
and ISDN Systems, 30.
I. Bulyko, M. Ostendorf, M. Siu, T. Ng, A. Stolcke, and
O. Cetin. 2007. Web resources for language modeling
in conversational speech recognition. ACM Transac-
tions on Speech and Language Processing, 5:1?25.
A. Desilets, B.D. Bruijn, and J. Martin. 2002. Extracting
keyphrases from spoken audio documents. In Infor-
mation Retrieval Techniques for Speech Applications,
pages 339?342.
E. Frank, G.W. Paynter, I.H. Witten, C. Gutwin, and C.G.
Nevill-Manning. 1999. Domain-specific keyphrase
extraction. In Proceedings of IJCAI, pages 688?673.
M. Galley, K. McKeown, E. Fosler-Lussier, and H. Jing.
2003. Discourse segmentation of multi-party conver-
sation. In Proceedings of ACL.
A. Hulth. 2003. Improved automatic keyword extraction
given more linguistic knowledge. In Proceedings of
EMNLP, pages 216?223.
D. Inkpen and A. Desilets. 2004. Extracting
semantically-coherent keyphrases from speech. Cana-
dian Acoustics Association, 32:130?131.
A. Janin, D. Baron, J. Edwards, D. Ellis, G . Gelbart,
N. Morgan, B. Peskin, T. Pfau, E. Shriberg, A. Stolcke,
and C. Wooters. 2003. The ICSI meeting corpus. In
Proceedings of ICASSP.
Y.H. Kerner, Z. Gross, and A. Masa. 2005. Automatic
extraction and learning of keyphrases from scientific
articles. In Computational Linguistics and Intelligent
Text Processing, pages 657?669.
F. Liu, F. Liu, and Y. Liu. 2008. Automatic keyword
extraction for the meeting corpus using supervised ap-
proach and bigram expansion. In Proceedings of IEEE
SLT.
Y. Matsuo and M. Ishizuka. 2004. Keyword extraction
from a single document using word co-occurrence sta-
tistical information. International Journal on Artifi-
cial Intelligence, 13(1):157?169.
C. Munteanu, G. Penn, and R. Baecker. 2007. Web-
based language modeling for automatic lecture tran-
scription. In Proceedings of Interspeech.
G. Murray, S. Renals, J. Carletta, and J. Moore. 2005.
Evaluating automatic summaries of meeting record-
ings. In Proceedings of ACL 2005 MTSE Workshop,
pages 33?40.
A. Nenkova and R. Passonneau. 2004. Evaluating con-
tent selection in summarization: the pyramid method.
In Proceedings of HLT/NAACL.
L. Plas, V. Pallotta, M. Rajman, and H. Ghorbel. 2004.
Automatic keyword extraction from spoken text. a
comparison of two lexical resources: the EDR and
WordNet. In Proceedings of the LREC.
D. Radev, S. Blair-Goldensohn, and Z. Zhang. 2001. Ex-
periments in single and multi-document summariza-
tion using MEAD. In Proceedings of The First Docu-
ment Understanding Conference.
I. Rogina. 2002. Lecture and presentation tracking in an
intelligent meeting room. In Proceedings of ICMI.
E. Shriberg, R. Dhillon, S. Bhagat, J. Ang, and H. Carvey.
2004. The ICSI meeting recorder dialog act (MRDA)
corpus. In Proceedings of SIGDial Workshop, pages
97?100.
A. Stolcke. 2002. SRILM ? An extensible language
modeling toolkit. In Proceedings of ICSLP, pages
901?904.
P.D. Turney. 2000. Learning algorithms for keyphrase
extraction. Information Retrieval, 2:303?336.
P.D. Turney. 2002. Mining the web for lexical knowl-
edge to improve keyphrase extraction: Learning from
labeled and unlabeled data. In National Research
Council, Institute for Information Technology, Techni-
cal Report ERB-1096.
P.D. Turney. 2003. Coherent keyphrase extraction via
web mining. In Proceedings of IJCAI, pages 434?439.
X. Wan, J. Yang, and J. Xiao. 2007. Towards an iter-
ative reinforcement approach for simultaneous docu-
ment summarization and keyword extraction. In Pro-
ceedings of ACL, pages 552?559.
C.H. Wu, C.L. Huang, C.S. Hsu, and K.M. Lee. 2007.
Speech retrieval using spoken keyword extraction and
semantic verification. In Proceedings of IEEE Region
10 Conference, pages 1?4.
Q. Zhu, A. Stolcke, B. Chen, and N. Morgan. 2005.
Using MLP features in SRI?s conversational speech
recognition system. In Proceedings of Interspeech.
628
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 161?168,
Sydney, July 2006. c?2006 Association for Computational Linguistics
PCFGs with Syntactic and Prosodic Indicators of Speech Repairs
John Halea Izhak Shafranb Lisa Yungc
Bonnie Dorrd Mary Harperde Anna Krasnyanskayaf Matthew Leaseg
Yang Liuh Brian Roarki Matthew Snoverd Robin Stewartj
a Michigan State University; b,c Johns Hopkins University; d University of Maryland, College Park; e Purdue University
f UCLA; g Brown University; h University of Texas at Dallas; i Oregon Health & Sciences University; j Williams College
Abstract
A grammatical method of combining two
kinds of speech repair cues is presented.
One cue, prosodic disjuncture, is detected
by a decision tree-based ensemble clas-
sifier that uses acoustic cues to identify
where normal prosody seems to be inter-
rupted (Lickley, 1996). The other cue,
syntactic parallelism, codifies the expec-
tation that repairs continue a syntactic
category that was left unfinished in the
reparandum (Levelt, 1983). The two cues
are combined in a Treebank PCFG whose
states are split using a few simple tree
transformations. Parsing performance on
the Switchboard and Fisher corpora sug-
gests that these two cues help to locate
speech repairs in a synergistic way.
1 Introduction
Speech repairs, as in example (1), are one kind
of disfluent element that complicates any sort
of syntax-sensitive processing of conversational
speech.
(1) and [ the first kind of invasion of ] the first
type of privacy seemed invaded to me
The problem is that the bracketed reparan-
dum region (following the terminology of Shriberg
(1994)) is approximately repeated as the speaker
The authors are very grateful for Eugene Charniak?s help
adapting his parser. We also thank the Center for Language
and Speech processing at Johns Hopkins for hosting the sum-
mer workshop where much of this work was done. This
material is based upon work supported by the National Sci-
ence Foundation (NSF) under Grant No. 0121285. Any opin-
ions, findings and conclusions or recommendations expressed
in this material are those of the authors and do not necessarily
reflect the views of the NSF.
?repairs? what he or she has already uttered.
This extra material renders the entire utterance
ungrammatical?the string would not be gener-
ated by a correct grammar of fluent English. In
particular, attractive tools for natural language
understanding systems, such as Treebank gram-
mars for written corpora, naturally lack appropri-
ate rules for analyzing these constructions.
One possible response to this mismatch be-
tween grammatical resources and the brute facts
of disfluent speech is to make one look more
like the other, for the purpose of parsing. In
this separate-processing approach, reparanda are
located through a variety of acoustic, lexical or
string-based techniques, then excised before sub-
mission to a parser (Stolcke and Shriberg, 1996;
Heeman and Allen, 1999; Spilker et al, 2000;
Johnson and Charniak, 2004). The resulting
parse tree then has the reparandum re-attached in
a standardized way (Charniak and Johnson, 2001).
An alternative strategy, adopted in this paper, is
to use the same grammar to model fluent speech,
disfluent speech, and their interleaving.
Such an integrated approach can use syntac-
tic properties of the reparandum itself. For in-
stance, in example (1) the reparandum is an
unfinished noun phrase, the repair a finished
noun phrase. This sort of phrasal correspon-
dence, while not absolute, is strong in conver-
sational speech, and cannot be exploited on the
separate-processing approach. Section 3 applies
metarules (Weischedel and Sondheimer, 1983;
McKelvie, 1998a; Core and Schubert, 1999) in
recognizing these correspondences using standard
context-free grammars.
At the same time as it defies parsing, con-
versational speech offers the possibility of lever-
aging prosodic cues to speech repairs. Sec-
161
Figure 1: The pause between two or s and the glottalization at the end of the first makes it easy for a
listener to identify the repair.
tion 2 describes a classifier that learns to label
prosodic breaks suggesting upcoming disfluency.
These marks can be propagated up into parse
trees and used in a probabilistic context-free gram-
mar (PCFG) whose states are systematically split
to encode the additional information.
Section 4 reports results on Switchboard (God-
frey et al, 1992) and Fisher EARS RT04F data,
suggesting these two features can bring about in-
dependent improvements in speech repair detec-
tion. Section 5 suggests underlying linguistic and
statistical reasons for these improvements. Sec-
tion 6 compares the proposed grammatical method
to other related work, including state of the art
separate-processing approaches. Section 7 con-
cludes by indicating a way that string- and tree-
based approaches to reparandum identification
could be combined.
2 Prosodic disjuncture
Everyday experience as well as acoustic anal-
ysis suggests that the syntactic interruption in
speech repairs is typically accompanied by a
change in prosody (Nakatani and Hirschberg,
1994; Shriberg, 1994). For instance, the spectro-
gram corresponding to example (2), shown in Fig-
ure 1,
(2) the jehovah?s witness or [ or ] mormons or
someone
reveals a noticeable pause between the occurrence
of the two ors, and an unexpected glottalization at
the end of the first one. Both kinds of cues have
been advanced as explanations for human listen-
ers? ability to identify the reparandum even before
the repair occurs.
Retaining only the second explanation, Lickley
(1996) proposes that there is no ?edit signal? per se
but that repair is cued by the absence of smooth
formant transitions and lack of normal juncture
phenomena.
One way to capture this notion in the syntax
is to enhance the input with a special disjunc-
ture symbol. This symbol can then be propa-
gated in the grammar, as illustrated in Figure 2.
This work uses a suffix ?+ to encode the percep-
tion of abnormal prosody after a word, along with
phrasal -BRK tags to decorate the path upwards to
reparandum constituents labeled EDITED. Such
NP
NP EDITED CC NP
NP NNP CC?BRK or NNPS
DT NNP POS witness
the jehovah ?s
or~+ mormons
Figure 2: Propagating BRK, the evidence of dis-
fluent juncture, from acoustics to syntax.
disjuncture symbols are identified in the ToBI la-
beling scheme as break indices (Price et al, 1991;
Silverman et al, 1992).
The availability of a corpus annotated with
ToBI labels makes it possible to design a break
index classifier via supervised training. The cor-
pus is a subset of the Switchboard corpus, con-
sisting of sixty-four telephone conversations man-
ually annotated by an experienced linguist accord-
ing to a simplified ToBI labeling scheme (Osten-
dorf et al, 2001). In ToBI, degree of disjuncture
is indicated by integer values from 0 to 4, where
a value of 0 corresponds to clitic and 4 to a major
phrase break. In addition, a suffix p denotes per-
ceptually disfluent events reflecting, for example,
162
hesitation or planning. In conversational speech
the intermediate levels occur infrequently and the
break indices can be broadly categorized into three
groups, namely, 1, 4 and p as in Wong et al
(2005).
A classifier was developed to predict three
break indices at each word boundary based on
variations in pitch, duration and energy asso-
ciated with word, syllable or sub-syllabic con-
stituents (Shriberg et al, 2005; Sonmez et al,
1998). To compute these features, phone-level
time-alignments were obtained from an automatic
speech recognition system. The duration of these
phonological constituents were derived from the
ASR alignment, while energy and pitch were com-
puted every 10ms with snack, a public-domain
sound toolkit (Sjlander, 2001). The duration, en-
ergy, and pitch were post-processed according to
stylization procedures outlined in Sonmez et al
(1998) and normalized to account for variability
across speakers.
Since the input vector can have missing val-
ues such as the absence of pitch during unvoiced
sound, only decision tree based classifiers were
investigated. Decision trees can handle missing
features gracefully. By choosing different com-
binations of splitting and stopping criteria, an
ensemble of decision trees was built using the
publicly-available IND package (Buntine, 1992).
These individual classifiers were then combined
into ensemble-based classifiers.
Several classifiers were investigated for detect-
ing break indices. On ten-fold cross-validation,
a bagging-based classifier (Breiman, 1996) pre-
dicted prosodic breaks with an accuracy of 83.12%
while chance was 67.66%. This compares favor-
ably with the performance of the supervised classi-
fiers on a similar task in Wong et al (2005). Ran-
dom forests and hidden Markov models provide
marginal improvements at considerable computa-
tional cost (Harper et al, 2005).
For speech repair, the focus is on detecting dis-
fluent breaks. The precision and recall trade-off
on its detection can be adjusted using a thresh-
old on the posterior probability of predicting ?p?,
as shown in Figure 3.
In essence, the large number of acoustic and
prosodic features related to disfluency are encoded
via the ToBI label ?p?, and provided as additional
observations to the PCFG. This is unlike previous
work on incorporating prosodic information (Gre-
00.10.20.30.40.50.6 0
0.1
0.2
0.3
0.4
0.5
0.6
Probability of Miss
Probab
ility of 
False 
Alarm
Figure 3: DET curve for detecting disfluent breaks
from acoustics.
gory et al, 2004; Lease et al, 2005; Kahn et al,
2005) as described further in Section 6.
3 Syntactic parallelism
The other striking property of speech repairs is
their parallel character: subsequent repair regions
?line up? with preceding reparandum regions. This
property can be harnessed to better estimate the
length of the reparandum by considering paral-
lelism from the perspective of syntax. For in-
stance, in Figure 4(a) the unfinished reparandum
noun phrase is repaired by another noun phrase ?
the syntactic categories are parallel.
3.1 Levelt?s WFR and Conjunction
The idea that the reparandum is syntactically par-
allel to the repair can be traced back to Levelt
(1983). Examining a corpus of Dutch picture de-
scriptions, Levelt proposes a bi-conditional well-
formedness rule for repairs (WFR) that relates the
structure of repairs to the structure of conjunc-
tions. The WFR conceptualizes repairs as the con-
junction of an unfinished reparandum string (?)
with a properly finished repair (?). Its original
formulation, repeated here, ignores optional inter-
regna like ?er? or ?I mean.?
Well-formedness rule for repairs (WFR) A re-
pair ???? is well-formed if and only if there
is a string ? such that the string ??? and? ??
is well-formed, where ? is a completion of
the constituent directly dominating the last
element of ?. (and is to be deleted if that
last element is itself a sentence connective)
In other words, the string ? is a prefix of a phrase
whose completion, ??if it were present?would
163
render the whole phrase ?? grammatically con-
joinable with the repair ?. In example (1) ? is the
string ?the first kind of invasion of?, ? is ?the first
type of privacy? and ? is probably the single word
?privacy.?
This kind of conjoinability typically requires
the syntactic categories of the conjuncts to be the
same (Chomsky, 1957, 36). That is, a rule schema
such as (2) where X is a syntactic category, is pre-
ferred over one where X is not constrained to be
the same on either side of the conjunction.
X ? X Conj X (2)
If, as schema (2) suggests, conjunction does fa-
vor like-categories, and, as Levelt suggests, well-
formed repairs are conjoinable with finished ver-
sions of their reparanda, then the syntactic cate-
gories of repairs ought to match the syntactic cat-
egories of (finished versions of) reparanda.
3.2 A WFR for grammars
Levelt?s WFR imposes two requirements on a
grammar
? distinguishing a separate category of ?unfin-
ished? phrases
? identifying a syntactic category for reparanda
Both requirements can be met by adapting Tree-
bank grammars to mirror the analysis of McK-
elvie1 (1998a; 1998b). McKelvie derives phrase
structure rules for speech repairs from fluent rules
by adding a new feature called abort that can
take values true and false. For a given gram-
mar rule of the form
A ? B C
a metarule creates other rules of the form
A [abort = Q] ?
B [abort = false] C [abort = Q]
where Q is a propositional variable. These rules
say, in effect, that the constituent A is aborted just
in case the last daughter C is aborted. Rules that
don?t involve a constant value for Q ensure that the
same value appears on parents and children. The
1McKelvie?s metarule approach declaratively expresses
Hindle?s (1983) Stack Editor and Category Copy Editor rules.
This classic work effectively states the WFR as a program for
the Fidditch deterministic parser.
WFR is then implemented by rule schemas such
as (3)
X ? X [abort = true] (AFF) X (3)
that permit the optional interregnum AFF to con-
join an unfinished X-phrase (the reparandum) with
a finished X-phrase (the repair) that comes after it.
3.3 A WFR for Treebanks
McKelvie?s formulation of Levelt?s WFR can be
applied to Treebanks by systematically recoding
the annotations to indicate which phrases are un-
finished and to distinguish matching from non-
matching repairs.
3.3.1 Unfinished phrases
Some Treebanks already mark unfinished
phrases. For instance, the Penn Treebank pol-
icy (Marcus et al, 1993; Marcus et al, 1994) is
to annotate the lowest node that is unfinished with
an -UNF tag as in Figure 4(a).
It is straightforward to propagate this mark up-
wards in the tree from wherever it is annotated to
the nearest enclosing EDITED node, just as -BRK
is propagated upwards from disjuncture marks on
individual words. This percolation simulates the
action of McKelvie?s [abort = true]. The re-
sulting PCFG is one in which distributions on
phrase structure rules with ?missing? daughters are
segregated from distributions on ?complete? rules.
3.4 Reparanda categories
The other key element of Levelt?s WFR is the
idea of conjunction of elements that are in some
sense the same. In the Penn Treebank annota-
tion scheme, reparanda always receive the label
EDITED. This means that the syntactic category
of the reparandum is hidden from any rule which
could favor matching it with that of the repair.
Adding an additional mark on this EDITED node
(a kind of daughter annotation) rectifies the situ-
ation, as depicted in Figure 4(b), which adds the
notation -childNP to a tree in which the unfin-
ished tags have been propagated upwards. This
allows a Treebank PCFG to represent the general-
ization that speech repairs tend to respect syntactic
category.
4 Results
Three kinds of experiments examined the effec-
tiveness of syntactic and prosodic indicators of
164
SCC EDITED NP
and NP NP
NP PP
DT JJ NN IN NP
the first kind of NP PP?UNF
NN IN
invasion of
DT JJ NN
the first type
(a) The lowest unfinished node is given.
S
CC EDITED?childNP NP
and NP?UNF NP
NP PP?UNF
DT JJ NN IN NP?UNF
the first kind of NP PP?UNF
NN IN
invasion of
DT JJ NN
the first type
(b) -UNF propagated, daughter-annotated Switchboard tree
Figure 4: Input (a) and output (b) of tree transformations.
speech repairs. The first two use the CYK algo-
rithm to find the most likely parse tree on a gram-
mar read-off from example trees annotated as in
Figures 2 and 4. The third experiment measures
the benefit from syntactic indicators alone in Char-
niak?s lexicalized parser (Charniak, 2000). The ta-
bles in subsections 4.1, 4.2, and 4.3 summarize
the accuracy of output parse trees on two mea-
sures. One is the standard Parseval F-measure,
which tracks the precision and recall for all labeled
constituents as compared to a gold-standard parse.
The other measure, EDIT-finding F, restricts con-
sideration to just constituents that are reparanda. It
measures the per-word performance identifying a
word as dominated by EDITED or not. As in pre-
vious studies, reference transcripts were used in all
cases. A check (
?
) indicates an experiment where
prosodic breaks where automatically inferred by
the classifier described in section 2, whereas in the
(?) rows no prosodic information was used.
4.1 CYK on Fisher
Table 1 summarizes the accuracy of a stan-
dard CYK parser on the newly-treebanked
Fisher corpus (LDC2005E15) of phone conver-
sations, collected as part of the DARPA EARS
program. The parser was trained on the entire
Switchboard corpus (ca. 107K utterances) then
tested on the 5368-utterance ?dev2? subset of the
Fisher data. This test set was tagged using MX-
POST (Ratnaparkhi, 1996) which was itself trained
on Switchboard. Finally, as described in section 2
these tags were augmented with a special prosodic
break symbol if the decision tree rated the proba-
bility a ToBI ?p? symbol higher than the threshold
value of 0.75.
A
nn
ot
at
io
n
Br
ea
k
in
de
x
Pa
rs
ev
a
lF
ED
IT
F
none
? 66.54 22.9?
66.08 26.1
daughter annotation ? 66.41 29.4? 65.81 31.6
-UNF propagation ? 67.06 31.5? 66.45 34.8
both ? 69.21 40.2? 67.02 40.6
Table 1: Improvement on Fisher, MXPOSTed tags.
The Fisher results in Table 1 show that syntac-
tic and prosodic indicators provide different kinds
of benefits that combine in an additive way. Pre-
sumably because of state-splitting, improvement
in EDIT-finding comes at the cost of a small decre-
ment in overall parsing performance.
4.2 CYK on Switchboard
Table 2 presents the results of similar experi-
ments on the Switchboard corpus following the
165
train/dev/test partition of Charniak and Johnson
(2001). In these experiments, the parser was given
correct part-of-speech tags as input.
A
nn
ot
at
io
n
Br
ea
k
in
de
x
Pa
rs
ev
a
lF
ED
IT
F
none
? 70.92 18.2?
69.98 22.5
daughter annotation ? 71.13 25.0? 70.06 25.5
-UNF propagation ? 71.71 31.1? 70.36 30.0
both ? 71.16 41.7? 71.05 36.2
Table 2: Improvement on Switchboard, gold tags.
The Switchboard results demonstrate independent
improvement from the syntactic annotations. The
prosodic annotation helps on its own and in com-
bination with the daughter annotation that imple-
ments Levelt?s WFR.
4.3 Lexicalized parser
Finally, Table 3 reports the performance of Char-
niak?s non-reranking, lexicalized parser on the
Switchboard corpus, using the same test/dev/train
partition.
Annotation Parseval F EDIT F
baseline 83.86 57.6
daughter annotation 80.85 67.2
-UNF propagation 81.68 64.7
both 80.16 70.0
flattened EDITED 82.13 64.4
Table 3: Charniak as an improved EDIT-finder.
Since Charniak?s parser does its own tagging,
this experiment did not examine the utility of
prosodic disjuncture marks. However, the com-
bination of daughter annotation and -UNF prop-
agation does lead to a better grammar-based
reparandum-finder than parsers trained on flat-
tened EDITED regions. More broadly, the re-
sults suggest that Levelt?s WFR is synergistic with
the kind of head-to-head lexical dependencies that
Charniak?s parser uses.
5 Discussion
The pattern of improvement in tables 1, 2, and
3 from none or baseline rows where no syntac-
tic parallelism or break index information is used,
to subsequent rows where it is used, suggest why
these techniques work. Unfinished-category an-
notation improves performance by preventing the
grammar of unfinished constituents from being
polluted by the grammar of finished constituents.
Such purification is independent of the fact that
rules with daughters labeled EDITED-childXP
tend to also mention categories labeled XP fur-
ther to the right (or NP and VP, when XP starts
with S). This preference for syntactic parallelism
can be triggered either by externally-suggested
ToBI break indices or grammar rules annotated
with -UNF. The prediction of a disfluent break
could be further improved by POS features and N-
gram language model scores (Spilker et al, 2001;
Liu, 2004).
6 Related Work
There have been relatively few attempts to harness
prosodic cues in parsing. In a spoken language
system for VERBMOBIL task, Batliner and col-
leagues (2001) utilize prosodic cues to dramati-
cally reduce lexical analyses of disfluencies in a
end-to-end real-time system. They tackle speech
repair by a cascade of two stages ? identification of
potential interruption points using prosodic cues
with 90% recall and many false alarms, and the
lexical analyses of their neighborhood. Their ap-
proach, however, does not exploit the synergy be-
tween prosodic and syntactic features in speech re-
pair. In Gregory et al (2004), over 100 real-valued
acoustic and prosodic features were quantized into
a heuristically selected set of discrete symbols,
which were then treated as pseudo-punctuation in
a PCFG, assuming that prosodic cues function like
punctuation. The resulting grammar suffered from
data sparsity and failed to provide any benefits.
Maximum entropy based models have been more
successful in utilizing prosodic cues. For instance,
in Lease et al (2005), interruption point probabil-
ities, predicted by prosodic classifiers, were quan-
tized and introduced as features into a speech re-
pair model along with a variety of TAG and PCFG
features. Towards a clearer picture of the inter-
action with syntax and prosody, this work uses
ToBI to capture prosodic cues. Such a method is
analogous to Kahn et al (2005) but in a genera-
tive framework.
The TAG-based model of Johnson and Charniak
(2004) is a separate-processing approach that rep-
166
resents the state of the art in reparandum-finding.
Johnson and Charniak explicitly model the
crossed dependencies between individual words
in the reparandum and repair regions, intersect-
ing this sequence model with a parser-derived lan-
guage model for fluent speech. This second step
improves on Stolcke and Shriberg (1996) and Hee-
man and Allen (1999) and outperforms the specific
grammar-based reparandum-finders tested in sec-
tion 4. However, because of separate-processing
the TAG channel model?s analyses do not reflect
the syntactic structure of the sentence being ana-
lyzed, and thus that particular TAG-based model
cannot make use of properties that depend on the
phrase structure of the reparandum region. This
includes the syntactic category parallelism dis-
cussed in section 3 but also predicate-argument
structure. If edit hypotheses were augmented to
mention particular tree nodes where the reparan-
dum should be attached, such syntactic paral-
lelism constraints could be exploited in the rerank-
ing framework of Johnson et al (2004).
The approach in section 3 is more closely re-
lated to that of Core and Schubert (1999) who
also use metarules to allow a parser to switch from
speaker to speaker as users interrupt one another.
They describe their metarule facility as a modi-
fication of chart parsing that involves copying of
specific arcs just in case specific conditions arise.
That approach uses a combination of longest-first
heuristics and thresholds rather than a complete
probabilistic model such as a PCFG.
Section 3?s PCFG approach can also be viewed
as a declarative generalization of Roark?s (2004)
EDIT-CHILD function. This function helps an
incremental parser decide upon particular tree-
drawing actions in syntactically-parallel contexts
like speech repairs. Whereas Roark conditions the
expansion of the first constituent of the repair upon
the corresponding first constituent of the reparan-
dum, in the PCFG approach there exists a separate
rule (and thus a separate probability) for each al-
ternative sequence of reparandum constituents.
7 Conclusion
Conventional PCFGs can improve their detection
of speech repairs by incorporating Lickley?s hy-
pothesis about interrupted prosody and by im-
plementing Levelt?s well-formedness rule. These
benefits are additive.
The strengths of these simple tree-based tech-
niques should be combinable with sophisticated
string-based (Johnson and Charniak, 2004; Liu,
2004; Zhang and Weng, 2005) approaches by
applying the methods of Wieling et al (2005)
for constraining parses by externally-suggested
brackets.
References
L. Breiman. 1996. Bagging predictors. Machine
Learning, 24(2):123?140.
W. Buntine. 1992. Tree classication software. In Tech-
nology 2002: The Third National Technology Trans-
fer Conference and Exposition, Baltimore.
E. Charniak and M. Johnson. 2001. Edit detection
and parsing for transcribed speech. In Proceedings
of the 2nd Meeting of the North American Chap-
ter of the Association for Computational Linguistics,
pages 118?126.
E. Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of NAACL-00, pages 132?
139.
N. Chomsky. 1957. Syntactic Structures. Anua Lin-
guarum Series Minor 4, Series Volume 4. Mouton
de Gruyter, The Hague.
M. G. Core and L. K. Schubert. 1999. A syntactic
framework for speech repairs and other disruptions.
In Proceedings of the 37th Annual Meeting of the As-
sociation for Computational Linguistics, pages 413?
420.
J. J. Godfrey, E. C. Holliman, and J. McDaniel. 1992.
SWITCHBOARD: Telephone speech corpus for re-
search and development. In Proceedings of ICASSP,
volume I, pages 517?520, San Francisco.
M. Gregory, M. Johnson, and E. Charniak. 2004.
Sentence-internal prosody does not help parsing the
way punctuation does. In Proceedings of North
American Association for Computational Linguis-
tics.
M. Harper, B. Dorr, J. Hale, B. Roark, I. Shafran,
M. Lease, Y. Liu, M. Snover, and L. Yung. 2005.
Parsing and spoken structural event detection. In
2005 Johns Hopkins Summer Workshop Final Re-
port.
P. A. Heeman and J. F. Allen. 1999. Speech repairs,
intonational phrases and discourse markers: model-
ing speakers? utterances in spoken dialog. Compu-
tational Linguistics, 25(4):527?571.
D. Hindle. 1983. Deterministic parsing of syntactic
non-fluencies. In Proceedings of the ACL.
M. Johnson and E. Charniak. 2004. A TAG-based
noisy channel model of speech repairs. In Proceed-
ings of ACL, pages 33?39.
167
M. Johnson, E. Charniak, and M. Lease. 2004. An im-
proved model for recognizing disfluencies in conver-
sational speech. In Proceedings of Rich Transcrip-
tion Workshop.
J. G. Kahn, M. Lease, E. Charniak, M. Johnson, and
M. Ostendorf. 2005. Effective use of prosody in
parsing conversational speech. In Proceedings of
Human Language Technology Conference and Con-
ference on Empirical Methods in Natural Language
Processing, pages 233?240.
M. Lease, E. Charniak, and M. Johnson. 2005. Pars-
ing and its applications for conversational speech. In
Proceedings of ICASSP.
W. J. M. Levelt. 1983. Monitoring and self-repair in
speech. Cognitive Science, 14:41?104.
R. J. Lickley. 1996. Juncture cues to disfluency. In
Proceedings the International Conference on Speech
and Language Processing.
Y. Liu. 2004. Structural Event Detection for Rich
Transcription of Speech. Ph.D. thesis, Purdue Uni-
versity.
M. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguis-
tics, 19(2):313?330.
M. Marcus, G. Kim, M. A. Marcinkiewicz, R. MacIn-
tyre, A. Bies, M. Ferguson, K. Katz, and B. Schas-
berger. 1994. The Penn Treebank: Annotating
Predicate Argument Structure. In Proceedings of
the 1994 ARPA Human Language Technology Work-
shop.
D. McKelvie. 1998a. SDP ? Spoken Dialog Parser.
ESRC project on Robust Parsing and Part-of-speech
Tagging of Transcribed Speech Corpora, May.
D. McKelvie. 1998b. The syntax of disfluency in spon-
taneous spoken language. ESRC project on Robust
Parsing and Part-of-speech Tagging of Transcribed
Speech Corpora, May.
C. Nakatani and J. Hirschberg. 1994. A corpus-based
study of repair cues in spontaneous speech. Journal
of the Acoustical Society of America, 95(3):1603?
1616, March.
M. Ostendorf, I. Shafran, S. Shattuck-Hufnagel,
L. Carmichael, and W. Byrne. 2001. A prosodically
labelled database of spontaneous speech. In Proc.
ISCA Tutorial and Research Workshop on Prosody
in Speech Recognition and Understanding, pages
119?121.
P. Price, M. Ostendorf, S. Shattuck-Hufnagel, and
C. Fong. 1991. The use of prosody in syntactic
disambiguation. Journal of the Acoustic Society of
America, 90:2956?2970.
A. Ratnaparkhi. 1996. A maximum entropy part-of-
speech tagger. In Proceedings of Empirical Methods
in Natural Language Processing Conference, pages
133?141.
B. Roark. 2004. Robust garden path parsing. Natural
Language Engineering, 10(1):1?24.
E. Shriberg, L. Ferrer, S. Kajarekar, A. Venkataraman,
and A. Stolcke. 2005. Modeling prosodic feature
sequences for speaker recognition. Speech Commu-
nication, 46(3-4):455?472.
E. Shriberg. 1994. Preliminaries to a Theory of Speech
Disfluencies. Ph.D. thesis, UC Berkeley.
H. F. Silverman, M. Beckman, J. Pitrelli, M. Ostendorf,
C. Wightman, P. Price, J. Pierrehumbert, and J. Hir-
shberg. 1992. ToBI: A standard for labeling English
prosody. In Proceedings of ICSLP, volume 2, pages
867?870.
K. Sjlander, 2001. The Snack sound visualization mod-
ule. Royal Institute of Technology in Stockholm.
http://www.speech.kth.se/SNACK.
K. Sonmez, E. Shriberg, L. Heck, and M. Weintraub.
1998. Modeling dynamic prosodic variation for
speaker verification. In Proceedings of ICSLP, vol-
ume 7, pages 3189?3192.
Jo?rg Spilker, Martin Klarner, and Gu?nther Go?rz. 2000.
Processing self-corrections in a speech-to-speech
system. In Wolfgang Wahlster, editor, Verbmobil:
Foundations of speech-to-speech translation, pages
131?140. Springer-Verlag, Berlin.
J. Spilker, A. Batliner, and E. No?th. 2001. How to
repair speech repairs in an end-to-end system. In
R. Lickley and L. Shriberg, editors, Proc. of ISCA
Workshop on Disfluency in Spontaneous Speech,
pages 73?76.
A. Stolcke and E. Shriberg. 1996. Statistical language
modeling for speech disfluencies. In Proceedings
of the IEEE International Conference on Acoustics,
Speech and Signal Processing, pages 405?408, At-
lanta, GA.
R. M. Weischedel and N. K. Sondheimer. 1983.
Meta-rules as a basis for processing ill-formed in-
put. American Journal of Computational Linguis-
tics, 9(3-4):161?177.
M. Wieling, M-J. Nederhof, and G. van Noord. 2005.
Parsing partially bracketed input. Talk presented at
Computational Linguistics in the Netherlands.
D. Wong, M. Ostendorf, and J. G. Kahn. 2005. Us-
ing weakly supervised learning to improve prosody
labeling. Technical Report UWEETR-2005-0003,
University of Washington Electrical Engineering
Dept.
Q. Zhang and F. Weng. 2005. Exploring features for
identifying edited regions in disfluent sentences. In
Proceedings of the Nineth International Workshop
on Parsing Technologies, pages 179?185.
168
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 672?679,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Unsupervised Language Model Adaptation Incorporating  
Named Entity Information 
Feifan Liu and Yang Liu 
Department of Computer Science 
The University of Texas at Dallas, Richardson, TX, USA 
{ffliu,yangl}@hlt.utdallas.edu 
  
Abstract 
Language model (LM) adaptation is im-
portant for both speech and language 
processing. It is often achieved by com-
bining a generic LM with a topic-specific 
model that is more relevant to the target 
document.  Unlike previous work on un-
supervised LM adaptation, this paper in-
vestigates how effectively using named 
entity (NE) information, instead of con-
sidering all the words, helps LM adapta-
tion. We evaluate two latent topic analysis 
approaches in this paper, namely, cluster-
ing and Latent Dirichlet Allocation 
(LDA). In addition, a new dynamically 
adapted weighting scheme for topic mix-
ture models is proposed based on LDA 
topic analysis. Our experimental results 
show that the NE-driven LM adaptation 
framework outperforms the baseline ge-
neric LM. The best result is obtained us-
ing the LDA-based approach by 
expanding the named entities with syntac-
tically filtered words, together with using 
a large number of topics, which yields a 
perplexity reduction of 14.23% compared 
to the baseline generic LM. 
1 Introduction 
Language model (LM) adaptation plays an impor-
tant role in speech recognition and many natural 
language processing tasks, such as machine trans-
lation and information retrieval. Statistical N-gram 
LMs have been widely used; however, they capture 
only local contextual information. In addition, even 
with the increasing amount of LM training data, 
there is often a mismatch problem because of dif-
ferences in domain, topics, or styles. Adaptation of 
LM, therefore, is very important in order to better 
deal with a variety of topics and styles. 
Many studies have been conducted for LM ad-
aptation. One method is supervised LM adaptation, 
where topic information is typically available and a 
topic specific LM is interpolated with the generic 
LM (Kneser and Steinbiss, 1993; Suzuki and Gao, 
2005). In contrast, various unsupervised ap-
proaches perform latent topic analysis for LM ad-
aptation. To identify implicit topics from the 
unlabeled corpus, one simple technique is to group 
the documents into topic clusters by assigning only 
one topic label to a document (Iyer and Ostendorf, 
1996). Recently several other methods in the line 
of latent semantic analysis have been proposed and 
used in LM adaptation, such as latent semantic 
analysis (LSA) (Bellegarda, 2000), probabilistic 
latent semantic analysis (PLSA) (Gildea and Hof-
mann, 1999), and LDA (Blei et al, 2003). Most of 
these existing approaches are based on the ?bag of 
words? model to represent documents, where all 
the words are treated equally and no relation or 
association between words is considered.  
Unlike prior work in LM adaptation, this paper 
investigates how to effectively leverage named 
entity information for latent topic analysis. Named 
entities are very common in domains such as 
newswire or broadcast news, and carry valuable 
information, which we hypothesize is topic indica-
tive and useful for latent topic analysis. We com-
pare different latent topic generation approaches as 
well as model adaptation methods, and propose an 
LDA based dynamic weighting method for the 
topic mixture model. Furthermore, we expand 
672
named entities by incorporating other content 
words, in order to capture more topic information. 
Our experimental results show that the proposed 
method of incorporating named information in LM 
adaptation is effective. In addition, we find that for 
the LDA based adaptation scheme, adding more 
content words and increasing the number of topics 
can further improve the performance significantly. 
The paper is organized as follows. In Section 2 
we review some related work. Section 3 describes 
in detail our unsupervised LM adaptation approach 
using named entities. Experimental results are pre-
sented and discussed in Section 4. Conclusion and 
future work appear in Section 5. 
2 Related Work 
There has been a lot of previous related work on 
LM adaptation. Suzuki and Gao (2005) compared 
different supervised LM adaptation approaches, 
and showed that three discriminative methods sig-
nificantly outperform the maximum a posteriori 
(MAP) method. For unsupervised LM adaptation, 
an earlier attempt is a cache-based model (Kuhn 
and Mori, 1990), developed based on the assump-
tion that words appearing earlier in a document are 
likely to appear again. The cache concept has also 
been used to increase the probability of unseen but 
topically related words, for example, the trigger-
based LM adaptation using the maximum entropy 
approach (Rosenfeld, 1996). 
Latent topic analysis has recently been investi-
gated extensively for language modeling. Iyer and 
Ostendorf (1996) used hard clustering to obtain 
topic clusters for LM adaptation, where a single 
topic is assigned to each document. Bellegarda 
(2000) employed Latent Semantic Analysis (LSA) 
to map documents into implicit topic sub-spaces 
and demonstrated significant reduction in perplex-
ity and word error rate (WER). Its probabilistic 
extension, PLSA, is powerful for characterizing 
topics and documents in a probabilistic space and 
has been used in LM adaptation. For example, 
Gildea and Hofmann (1999) reported noticeable 
perplexity reduction via a dynamic combination of 
many unigram topic models with a generic trigram 
model. Proposed by Blei et al (2003), Latent 
Dirichlet Allocation (LDA) loosens the constraint 
of the document-specific fixed weights by using a 
prior distribution and has quickly become one of 
the most popular probabilistic text modeling tech-
niques. LDA can overcome the drawbacks in the 
PLSA model, and has been shown to outperform 
PLSA in corpus perplexity and text classification 
experiments (Blei et al, 2003). Tam and Schultz 
(2005) successfully applied the LDA model to un-
supervised LM adaptation by interpolating the 
background LM with the dynamic unigram LM 
estimated by the LDA model. Hsu and Glass (2006) 
investigated using hidden Markov model with 
LDA to allow for both topic and style adaptation. 
Mrva and Woodland (2006) achieved WER reduc-
tion on broadcast conversation recognition using 
an LDA based adaptation approach that effectively 
combined the LMs trained from corpora with dif-
ferent styles: broadcast news and broadcast con-
versation data. 
In this paper, we investigate unsupervised LM 
adaptation using clustering and LDA based topic 
analysis. Unlike the clustering based interpolation 
method as in (Iyer and Ostendorf, 1996), we ex-
plore different distance measure methods for topic 
analysis. Different from the LDA based framework 
as in (Tam and Schultz, 2005), we propose a novel 
dynamic weighting scheme for the topic adapted 
LM. More importantly, the focus of our work is to 
investigate the role of named entity information in 
LM adaptation, which to our knowledge has not 
been explored.  
3 Unsupervised LM Adaptation Integrat-
ing Named Entities (NEs) 
3.1 Overview of the NE-driven LM Adapta-
tion Framework 
Figure 1 shows our unsupervised LM adaptation 
framework using NEs. For training, we use the text 
collection to train the generic word-based N-gram 
LM. Then we apply named entity recognition 
(NER) and topic analysis to train multiple topic 
specific N-gram LMs. During testing, NER is per-
formed on each test document, and then a dynami-
cally adaptive LM based on the topic analysis 
result is combined with the general LM. Note that 
in this figure, we evaluate the performance of LM 
adaptation using the perplexity measure. We will 
evaluate this framework for N-best or lattice res-
coring in speech recognition in the future. 
In our experiments, different topic analysis 
methods combined with different topic matching 
and adaptive schemes result in several LM adapta-
673
tion paradigms, which are described below in de-
tails. 
 
Training Text Test Text
NER NER
Latent Topic 
Analysis
Compute 
Perplexity
Generic N-gram 
Training
Topic Model 
Training
Topic Matching
Topic Model 
Adaptation
Model 
Interpolation
 
Figure 1. Framework of NE-driven LM adaptation. 
 
3.2 NE-based Clustering for LM Adaptation 
Clustering is a simple unsupervised topic analysis 
method. We use NEs to construct feature vectors 
for the documents, rather than considering all the 
words as in most previous work. We use the 
CLUTO1 toolkit to perform clustering. It finds a 
predefined number of clusters based on a specific 
criterion, for which we chose the following func-
tion: 
? ?
= ?
=
K
i Suv
k
i
uvsimSSS
1 ,
*
21 ),(maxarg)( L  
where K is the desired number of clusters, Si is the 
set of documents belonging to the ith cluster, v and 
u represent two documents, and sim(v, u) is the 
similarity between them. We use the cosine dis-
tance to measure the similarity between two docu-
ments: 
||||||||
),(
uv
uv
uvsim rr
rr
?
?=                         (1) 
where v
r
 and u
r
 are the feature vectors represent-
ing the two documents respectively, in our experi-
ments composed of NEs. For clustering, the 
elements in every feature vector are scaled based 
on their term frequency and inverse document fre-
                                                          
1 Available at http://glaros.dtc.umn.edu/gkhome/views/cluto 
quency, a concept widely used in information re-
trieval.   
After clustering, we train an N-gram LM, called 
a topic LM, for each cluster using the documents in 
it. 
During testing, we identify the ?topic? for the 
test document, and interpolate the topic specific 
LM with the background LM, that is, if the test 
document belongs to the cluster S*, we can predict 
a word wk in the document given the word?s his-
tory hk using the following equation: 
)|()1(
)|()|(
* kkSTopic
kkGeneralkk
hwp
hwphwp
??+
=
?
?
      (2) 
where ?  is the interpolation weight. 
We investigate two approaches to find the topic 
assignment S* for a given test document. 
(A) cross-entropy measure 
For a test document d=w1,w2,?,wn with a word 
distribution pd(w) and a cluster S with a topic LM 
ps(w), the cross entropy CE(d, S) can be computed 
as: 
?
=
?==
n
i
isidsd wpwpppHSdCE
1
2 ))((log)(),(),(
    From the information theoretic perspective, the 
cluster with the lower cross entropy value is ex-
pected to be more topically correlated to the test 
document. For each test document, we compute the 
cross entropy values according to different clusters, 
and select the cluster S* that satisfies: 
),(minarg
1
*
i
Ki
SdCES
??
=  
(B) cosine similarity  
For each cluster, its centroid can be obtained by: 
?
=
= i
n
k
ik
i
i un
cv
1
1
 
where uik is the vector for the kth document in the ith 
cluster, and ni is the number of documents in the ith 
cluster. The distance between the test document 
and a cluster can then be easily measured by the 
cosine similarity function as in Equation (1). Our 
goal here is to find the cluster S* which the test 
document is closest to, that is, 
||||||||
maxarg
1
*
i
i
Ki cvd
cvd
S ?
?=
??
r
r
 
674
where d
r
is the feature vector for the test document.   
3.3 NE-based LDA for LM Adaptation 
LDA model (Blei et al, 2003) has been introduced 
as a new, semantically consistent generative model, 
which overcomes overfitting and the problem of 
generating new documents in PLSA. It is a three-
level hierarchical Bayesian model. Based on the 
LDA model, a document d is generated as follows. 
? Sample a vector of K topic mixture weights 
?  from a prior Dirichlet distribution with 
parameter ? : 
?
=
?=
K
k
k
kf
1
1);( ????  
? For each word w in d, pick a topic k from the 
multinomial distribution ? . 
? Pick a word w from the multinomial distri-
bution kw,?  given the kth topic. 
For a document d=w1,w2,?wn, the LDA model 
assigns it the following probability: 
? ?? ???
????
? ?=
= =?
????? dfdp n
i
K
k
kkwi
);()(
1 1
 
We use the MATLAB topic Toolbox 1.3 (Grif-
fiths et al, 2004) in the training set to obtain the 
document-topic matrix, DP, and the word-topic 
matrix, WP. Note that here ?words? correspond to 
the elements in the feature vector used to represent 
the document (e.g., NEs). In the DP matrix, an en-
try cik represents the counts of words in a document 
di that are from a topic zk (k=1,2,?,K). In the WP 
matrix, an entry fjk represents the frequency of a 
word wj generated from a topic zk (k=1,2,?,K) 
over the training set.  
For training, we assign a topic zi* to a document 
di such that ik
Kk
i cz ??
=
1
* maxarg . Based on the docu-
ments belonging to the different topics, K topic N-
gram LMs are trained. This ?hard clustering? strat-
egy allows us to train an LM that accounts for all 
the words rather than simply those NEs used in 
LDA analysis, as well as use higher order N-gram 
LMs, unlike the ?unigram? based LDA in previous 
work. 
For a test document d = w1,w2,?,wn that is gen-
erated by multiple topics under the LDA assump-
tion, we formulate a dynamically adapted topic 
model using the mixture of LMs from different 
topics: 
?
=
? ?=
K
i
kkzikkadaptLDA hwphwp i
1
)|()|( ?  
where )|( kkz hwp i  stands for the i
th topic LM, and 
?i is the mixture weight. Different from the idea of 
dynamic topic adaptation in (Tam and Schultz, 
2005), we propose a new weighting scheme to cal-
culate ?i that directly uses the two resulting matri-
ces from LDA analysis during training: 
?
=
=
n
j
jjkk dwpwzp
1
)|()|(?  
??
==
== n
q
q
j
jK
p
jp
jk
jk
wfreq
wfreq
dwp
f
f
wzp
11
)(
)(
)|(,)|(  
where freq(wj) is the frequency of a word wj in the 
document d. Other notations are consistent with the 
previous definitions.  
Then we interpolate this adapted topic model 
with the generic LM, similar to Equation (2): 
)|()1(
)|()|(
kkadaptLDA
kkGeneralkk
hwp
hwphwp
??+
=
?
?
      (3) 
4 Experiments 
4.1 Experimental Setup 
 # of files # of words # of NEs
Training Data 23,985 7,345,644 590,656
Test Data 2,661 831,283 65,867 
Table 1. Statistics of our experimental data. 
 
The data set we used is the LDC Mandarin TDT4 
corpus, consisting of 337 broadcast news shows 
with transcriptions. These files were split into 
small pieces, which we call documents here, ac-
cording to the topic segmentation information 
marked in the LDC?s transcription. In total, there 
are 26,646 such documents in our data set. We 
randomly chose 2661 files as the test data (which 
is balanced for different news sources). The rest 
was used for topic analysis and also generic LM 
training. Punctuation marks were used to deter-
mine sentences in the transcriptions. We used the 
NYU NE tagger (Ji and Grishman, 2005) to recog-
nize four kinds of NEs: Person, Location, Organi-
675
zation, and Geo-political. Table 1 shows the statis-
tics of the data set in our experiments.  
We trained trigram LMs using the SRILM tool-
kit (Stolcke, 2002). A fixed weight (i.e., ?  in 
Equation (2) and (3)) was used for the entire test 
set when interpolating the generic LM with the 
adapted topic LM. Perplexity was used to measure 
the performance of different adapted LMs in our 
experiments.  
4.2 Latent Topic Analysis Results 
 
 Topic # of  Files 
Top 10 Descriptive Items  
(Translated from Chinese) 
1 3526 
U.S., Israel, Washington, Palestine, 
Bush, Clinton, Gore, Voice of Amer-
ica, Mid-East, Republican Party 
2 3067 
Taiwan, Taipei, Mainland, Taipei 
City, Chinese People?s Broadcasting 
Station, Shuibian Chen,  the Execu-
tive Yuan, the Legislative Yuan, De-
mocratic Progressive Party, 
Nationalist Party 
3 4857 
Singapore, Japan, Hong Kong, Indo-
nesia, Asia, Tokyo, Malaysia, Thai-
land, World, China 
4 4495 
World, German, Landon, Russia, 
France, England, Xinhua News 
Agency, Europe, U.S., Italy 
Cluster-
ing 
Based 
5 7586 
China, Beijing, Nation, China Central 
Television Station, Xinhua News 
Agency, Shanghai, World, State 
Council, Zemin Jiang, Beijing City
1 5859 
China, Japan, Hong Kong, Beijing, 
Shanghai, World, Zemin Jiang, Ma-
cao,  China Central Television Sta-
tion, Africa 
2 3794 
U.S., Bush, World,  Gore,  South 
Korea, North Korea, Clinton, George 
Walker Bush, Asia, Thailand 
3 4640 
Singapore, Indonesia, Team, Israel, 
Europe, Germany, England, France, 
Palestine, Wahid 
4 4623 
Taiwan, Russia, Mainland, India, 
Taipei, Shuibian Chen, Philippine, 
Estrada, Communist Party of China, 
RUS. 
LDA 
Based 
5 4729 
Xinhua News Agency, Nation, Bei-
jing, World, Canada, Sydney, Brazil, 
Beijing City, Education Ministry, 
Cuba 
Table 2.  Topic analysis results using clustering 
and LDA (the number of documents and the top 10 
words (NEs) in each cluster). 
 
For latent topic analysis, we investigated two ap-
proaches using named entities, i.e., clustering and 
LDA. 5 latent topics were used in both approaches. 
Table 2 illustrates the resulting topics using the top 
10 words in each topic. We can see that the words 
in the same cluster share some similarity and that 
the words in different clusters seem to be ?topi-
cally? different. Note that errors from automatic 
NE recognition may impact the clustering results. 
For example, ??/team? in the table (in topic 3 in 
LDA results) is an error and is less discriminative 
for topic analysis. 
Table 3 shows the perplexity of the test set us-
ing the background LM (baseline) and each of the 
topic LMs, from clustering and LDA respectively. 
We can see that for the entire test set, a topic LM 
generally performs much worse than the generic 
LM. This is expected, since the size of a topic clus-
ter is much smaller than that of the entire training 
set, and the test set may contain documents from 
different topics. However, we found that when us-
ing an optimal topic model (i.e., the topic LM that 
yields the lowest perplexity among the 5 topic 
LMs), 23.45% of the documents in the test set have 
a lower perplexity value than that obtained from 
the generic LM. This suggests that a topic model 
could benefit LM adaptation and motivates a dy-
namic topic adaptation approach for different test 
documents. 
 
 Perplexity 
Baseline 502.02 
CL-1 1054.36 
CL-2 1399.16 
CL-3 919.237 
CL-4 962.996 
CL-5 981.072 
LDA-1 1224.54 
LDA-2 1375.97 
LDA-3 1330.44 
LDA-4 1328.81 
LDA-5 1287.05 
Table 3. Perplexity results using the baseline LM 
vs. the single topic LMs. 
 
4.3 Clustering vs. LDA Based LM Adaptation 
In this section, we compare three LM adaptation 
paradigms. As we discussed in Section 3, two of 
them are clustering based topic analysis, but using 
different strategies to choose the optimal cluster; 
and the third one is based on LDA analysis that 
676
uses a dynamic weighting scheme for adapted 
topic mixture model.  
Figure 2 shows the perplexity results using dif-
ferent interpolation parameters with the general 
LM.  5 topics were used in both clustering and 
LDA based approaches (as in Section 4.2). ?CL-
CE? means clustering based topic analysis via 
cross entropy criterion, ?CL-Cos? represents clus-
tering based topic analysis via cosine distance cri-
terion, and ?LDA-MIX? denotes LDA based topic 
mixture model, which uses 5 mixture topic LMs. 
 
440
450
460
470
480
490
500
510
520
530
540
0.4 0.5 0.6 0.7 0.8
?
P
er
pl
ex
ity
Baseline CL-CE CL-Cos LDA-MIX
 
Figure 2. Perplexity using different LM adaptation 
approaches and different interpolation weights?  
with the general LM. 
 
We observe that all three adaptation approaches 
outperform the baseline when using a proper inter-
polation weight. ?CL-CE? yields the best perplex-
ity of 469.75 when ?  is 0.5, a reduction of 6.46% 
against the baseline perplexity of 502.02. For clus-
tering based adaptation, between the two strategies 
used to determine the topic for a test document, 
?CL-CE? outperforms ?CL-Cos?. This indicates 
that the cosine distance measure using only names 
is less effective than cross entropy for LM adapta-
tion. In addition, cosine similarity does not match 
perplexity as well as the CE-based distance meas-
ure. Similarly, for the LDA based approach, using 
only NEs may not be sufficient to find appropriate 
weights for the topic model. This also explains the 
bigger interpolation weight for the general LM in 
CL-Cos and LDA-MIX than that in ?CL-CE?.   
For a fair comparison between the clustering 
and LDA based LM adaptation approaches, we 
also evaluated using the topic mixture model for 
the clustering based approach and using only one 
topic in the LDA based method. For clustering 
based adaptation, we constructed topic mixture 
models using the weights obtained from a linear 
normalization of the two distance measures pre-
sented in Section 3.2. In order to use only one topic 
model in LDA based adaptation, we chose the 
topic cluster that has the largest weight in the 
adapted topic mixture model (as in Sec 3.3). Table 
4 shows the perplexity for the three approaches 
(CL-Cos, CL-CE, and LDA) using the mixture 
topic models versus a single topic LM. We observe 
similar trends as in Figure 2 when changing the 
interpolation weight ? with the generic LM; there-
fore, in Table 4 we only present results for one op-
timal interpolation weight. 
 
 Single-Topic Mixture-Topic
CL-Cos (? =0.7) 498.01 497.86 
CL-CE (? =0.5) 469.75 483.09 
LDA (? =0.7) 488.96 489.14 
Table 4. Perplexity results using the adapted topic 
model (single vs. mixture) for clustering and LDA 
based approaches. 
 
We can see from Table 4 that using the mixture 
model in clustering based adaptation does not im-
prove performance. This may be attributed to how 
the interpolation weights are calculated. For ex-
ample, only names are used in cosine distance, 
and the normalized distance may not be appropri-
ate weights. We also notice negligible difference 
when only using one topic in the LDA based 
framework. This might be because of the small 
number of topics currently used. Intuitively, using 
a mixture model should yield better performance, 
since LDA itself is based on the assumption of 
generating words from multiple topics. We will 
investigate the impact of the number of topics on 
LM adaptation in Section 4.5. 
4.4 Effect of Different Feature Configura-
tions on LM Adaptation 
We suspect that using only named entities may not 
provide enough information about the ?topics? of 
the documents, therefore we investigate expanding 
the feature vectors with other words. Since gener-
ally content words are more indicative of the topic 
of a document than function words, we used a POS 
tagger (Hillard et al, 2006) to select words for la-
tent topic analysis. We kept words with three POS 
classes: noun (NN, NR, NT), verb (VV), and modi-
677
fier (JJ), selected from the LDC POS set2. This is 
similar to the removal of stop words widely used in 
information retrieval.  
Figure 3 shows the perplexity results for three 
different feature configurations, namely, all-words 
(w), names (n), and names plus syntactically fil-
tered items (n+), for the CL-CE and LDA based 
approaches. The LDA based LM adaptation para-
digm supports our hypothesis. Using named infor-
mation instead of all the words seems to efficiently 
eliminate redundant information and achieve better 
performance. In addition, expanding named enti-
ties with syntactically filtered items yields further 
improvement. For CL-CE, using named informa-
tion achieves the best result among the three con-
figurations. This might be because that the 
clustering method is less powerful in analyzing the 
principal components as well as dealing with re-
dundant information than the LDA model. 
 
460
465
470
475
480
485
490
495
500
505
0.4 0.5 0.6 0.7 0.8
?
P
er
pl
ex
ity
CL-CE(w) CL-CE(n) CL-CE(n+)
LDA-MIX(w) LDA-MIX(n) LDA-MIX(n+)
 
Figure 3. Comparison of perplexity using different 
feature configurations. 
4.5 Impact of Predefined Topic Number on 
LM Adaptation 
LDA based topic analysis typically uses a large 
number of topics to capture the fine grained topic 
space. In this section, we evaluate the effect of the 
number of topics on LM adaptation. For compari-
son, we evaluate this for both LDA and CL-CE, 
similar to Section 4.3. We use the ?n+? feature 
configuration as in Section 4.4, that is, names plus 
POS filtered items. When using a single-topic 
adapted model in the LDA or CL-CE based ap-
proach, finer-grained topic analysis (i.e., increasing 
the number of topics) leads to worse performance 
mainly because of the smaller clusters for each 
topic; therefore, we only show results here using 
                                                          
2 See http://www.cis.upenn.edu/~chinese/posguide.3rd.ch.pdf 
the mixture topic adapted models. Figure 4 shows 
the perplexity results using different numbers of 
topics. The interpolation weight? with the general 
LM is 0.5 in all the experiments. For the topic mix-
ture LMs, we used a maximum of 9 mixtures (a 
limitation in the current SRILM toolkit) when the 
number of topics is greater than 9.  
We observe that as the number of topics in-
creases, the perplexity reduces significantly for 
LDA. When the number of topics is 50, the 
adapted LM using LDA achieves a perplexity re-
duction of 11.35% compared to using 5 topics, and 
14.23% against the baseline generic LM. Therefore, 
using finer-grained multiple topics in dynamic ad-
aptation improves system performance. When the 
number of topics increases further, e.g., to 100, the 
performance degrades slightly. This might be due 
to the limitation of the number of the topic mix-
tures used. A similar trend is observable for the 
CL-CE approach, but the effect of the topic num-
ber is much greater in LDA than CL-CE.  
  
435.2
477.2
430.6
445.8
485.7
477.3471.8 467.2
483.1 485.1
400
420
440
460
480
500
n=5 n=10 n=20 n=50 n=100
# of Topics
P
er
pl
ex
it
y
LDA CL-CE
 
Figure 4. Perplexity results using different prede-
fined numbers of topics for LDA and CL-CE.  
4.6 Discussion 
As we know, although there is an increasing 
amount of training data available for LM training, 
it is still only for limited domains and styles. Creat-
ing new training data for different domains is time 
consuming and labor intensive, therefore it is very 
important to develop algorithms for LM adaptation. 
We investigate leveraging named entities in the 
LM adaptation task. Though some errors of NER 
may be introduced, our experimental results have 
shown that exploring named information for topic 
analysis is promising for LM adaptation.  
Furthermore, this framework may have other 
advantages. For speech recognition, using NEs for 
topic analysis can be less vulnerable to recognition 
678
errors. For instance, we may add a simple module 
to compute the similarity between two NEs based 
on the word tokens or phonetics, and thus compen-
sate the recognition errors inside NEs. Whereas, 
word-based models, such as the traditional cache 
LMs, may be more sensitive to recognition errors 
that are likely to have a negative impact on the 
prediction of the current word. From this point of 
view, our framework can potentially be more ro-
bust in the speech processing task. In addition, the 
number of NEs in a document is much smaller than 
that of the words, as shown in Table 1; hence, us-
ing NEs can also reduce the computational com-
plexity, in particular in topic analysis for training. 
5 Conclusion and Future Work 
We compared several unsupervised LM adaptation 
methods leveraging named entities, and proposed a 
new dynamic weighting scheme for topic mixture 
model based on LDA topic analysis. Experimental 
results have shown that the NE-driven LM adapta-
tion approach outperforms using all the words, and 
yields perplexity reduction compared to the base-
line generic LM. In addition, we find that for the 
LDA based method, adding other content words, 
combined with an increased number of topics, can 
further improve the performance, achieving up to 
14.23% perplexity reduction compared to the base-
line LM. 
The experiments in this paper combine models 
primarily through simple linear interpolation. Thus 
one direction of our future work is to develop algo-
rithms to automatically learn appropriate interpola-
tion weights. In addition, our work in this paper 
has only showed promising results in perplexity 
reduction. We will investigate using this frame-
work of LM adaptation for N-best or lattice rescor-
ing in speech recognition. 
Acknowledgements 
We thank Mari Ostendorf, Mei-Yuh Hwang, and 
Wen Wang for useful discussions, and Heng Ji for 
sharing the Mandarin named entity tagger. This 
work is supported by DARPA under Contract No. 
HR0011-06-C-0023. Any opinions expressed in 
this material are those of the authors and do not 
necessarily reflect the views of DARPA. 
 
 
References 
 
J. Bellegarda. 2000. Exploiting Latent Semantic Infor-
mation in Statistical Language Modeling. In IEEE 
Transactions on Speech and Audio Processing. 
88(80):1279-1296.   
D. Blei, A. Ng, and M. Jordan. 2003. Latent Dirichlet 
Allocation. Journal of Machine Learning Research. 
3:993-1022. 
D. Gildea and T. Hofmann. 1999. Topic-Based Lan-
guage Models using EM. In Proc. of Eurospeech. 
T. Griffiths, M. Steyvers, D. Blei, and J. Tenenbaum. 
2004. Integrating Topics and Syntax. Adv. in Neural 
Information Processing Systems. 17:537-544. 
D. Hillard, Z. Huang, H. Ji, R. Grishman, D. Hakkani-
Tur, M. Harper, M. Ostendorf, and W. Wang. 2006. 
Impact of Automatic Comma Prediction on 
POS/Name Tagging of Speech. In Proc. of the First 
Workshop on Spoken Language Technology (SLT).  
P. Hsu and J. Glass. 2006. Style & Topic Language 
Model Adaptation using HMM-LDA. In Proc. of 
EMNLP, pp:373-381. 
R. Iyer and M. Ostendorf. 1996. Modeling Long Dis-
tance Dependence in Language: Topic Mixtures vs. 
Dynamic Cache Models. In Proc. of ICSLP. 
H. Ji and R. Grishman. 2005. Improving NameTagging 
by Reference Resolution and Relation Detection. In 
Proc. of ACL. pp: 411-418. 
R. Kneser and V. Steinbiss. 1993. On the Dynamic Ad-
aptation of Stochastic language models. In Proc. of 
ICASSP, Vol 2, pp: 586-589. 
R. Kuhn and R.D. Mori. 1990. A Cache-Based Natural 
Language Model for Speech Recognition. In IEEE 
Transactions on Pattern Analysis and Machine Intel-
ligence, 12: 570-583.  
D. Mrva and P.C. Woodland. 2006. Unsupervised Lan-
guage Model Adaptation for Mandarin Broadcast 
Conversation Transcription. In Proc. of 
INTERSPEECH, pp:2206-2209. 
R. Rosenfeld. 1996. A Maximum Entropy Approach to 
Adaptive Statistical Language Modeling. Computer, 
Speech and Language, 10:187-228. 
A. Stolcke. 2002. SRILM ? An Extensible Language 
Modeling Toolkit. In Proc. of ICSLP. 
H. Suzuki and J. Gao. 2005. A Comparative Study on 
Language Model Adaptation Techniques Using New 
Evaluation Metrics, In Proc. of HLT/EMNLP. 
Y.C. Tam and T. Schultz. 2005. Dynamic Language 
Model Adaptation Using Variational Bayes Inference. 
In Proc. of INTERSPEECH, pp:5-8. 
679
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 201?204,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Correlation between ROUGE and Human Evaluation of Extractive Meeting
Summaries
Feifan Liu, Yang Liu
The University of Texas at Dallas
Richardson, TX 75080, USA
ffliu,yangl@hlt.utdallas.edu
Abstract
Automatic summarization evaluation is critical to
the development of summarization systems. While
ROUGE has been shown to correlate well with hu-
man evaluation for content match in text summa-
rization, there are many characteristics in multiparty
meeting domain, which may pose potential prob-
lems to ROUGE. In this paper, we carefully exam-
ine how well the ROUGE scores correlate with hu-
man evaluation for extractive meeting summariza-
tion. Our experiments show that generally the cor-
relation is rather low, but a significantly better cor-
relation can be obtained by accounting for several
unique meeting characteristics, such as disfluencies
and speaker information, especially when evaluating
system-generated summaries.
1 Introduction
Meeting summarization has drawn an increasing atten-
tion recently; therefore a study on the automatic evalu-
ation metrics for this task is timely. Automatic evalua-
tion helps to advance system development and avoids the
labor-intensive and potentially inconsistent human eval-
uation. ROUGE (Lin, 2004) has been widely used for
summarization evaluation. In the news article domain,
ROUGE scores have been shown to be generally highly
correlated with human evaluation in content match (Lin,
2004). However, there are many differences between
written texts (e.g., news wire) and spoken documents, es-
pecially in the meeting domain, for example, the pres-
ence of disfluencies and multiple speakers, and the lack
of structure in spontaneous utterances. The question of
whether ROUGE is a good metric for meeting summa-
rization is unclear. (Murray et al, 2005) have reported
that ROUGE-1 (unigram match) scores have low correla-
tion with human evaluation in meetings.
In this paper we investigate the correlation between
ROUGE and human evaluation of extractive meeting
summaries and focus on two issues specific to the meet-
ing domain: disfluencies and multiple speakers. Both
human and system generated summaries are used. Our
analysis shows that by integrating meeting characteristics
into ROUGE settings, better correlation can be achieved
between the ROUGE scores and human evaluation based
on Spearman?s rho in the meeting domain.
2 Related work
Automatic summarization evaluation can be broadly clas-
sified into two categories (Jones and Galliers, 1996): in-
trinsic and extrinsic evaluation. Intrinsic evaluation, such
as relative utility based metric proposed in (Radev et al,
2004), assesses a summarization system in itself (for ex-
ample, informativeness, redundancy, and coherence). Ex-
trinsic evaluation (Mani et al, 1998) tests the effective-
ness of a summarization system on other tasks. In this
study, we concentrate on the automatic intrinsic summa-
rization evaluation. It has been extensively studied in
text summarization. Different approaches have been pro-
posed to measure matches using words or more mean-
ingful semantic units, for example, ROUGE (Lin, 2004),
factoid analysis (Teufel and Halteren, 2004), pyramid
method (Nenkova and Passonneau, 2004), and Basic El-
ement (BE) (Hovy et al, 2006).
With the increasing recent research of summarization
moving into speech, especially meeting recordings, is-
sues related to spoken language are yet to be explored
for their impact on the evaluation metrics. Inspired by
automatic speech recognition (ASR) evaluation, (Hori et
al., 2003) proposed the summarization accuracy metric
(SumACCY) based on a word network created by merg-
ing manual summaries. However (Zhu and Penn, 2005)
found a statistically significant difference between the
ASR-inspired metrics and those taken from text summa-
rization (e.g., RU, ROUGE) on a subset of the Switch-
board data. ROUGE has been used in meeting summa-
rization evaluation (Murray et al, 2005; Galley, 2006),
yet the question remained whether ROUGE is a good
metric for the meeting domain. (Murray et al, 2005)
showed low correlation of ROUGE and human evalua-
tion in meeting summarization evaluation; however, they
201
simply used ROUGE as is and did not take into account
the meeting characteristics during evaluation.
In this paper, we ask the question of whether ROUGE
correlates with human evaluation of extractive meeting
summaries and whether we can modify ROUGE to ac-
count for the meeting style for a better correlation with
human evaluation.
3 Experimental Setup
3.1 Data
We used the ICSI meeting data (Janin et al, 2003) that
contains naturally-occurring research meetings. All the
meetings have been transcribed and annotated with dialog
acts (DA) (Shriberg et al, 2004), topics, and extractive
summaries (Murray et al, 2005).
For this study, we used the same 6 test meetings as in
(Murray et al, 2005; Galley, 2006). Each meeting al-
ready has 3 human summaries from 3 common annota-
tors. We recruited another 3 human subjects to generate
3 more human summaries, in order to create more data
points for a reliable analysis. The Kappa statistics for
those 6 different annotators varies from 0.11 to 0.35 for
different meetings. The human summaries have different
length, containing around 6.5% of the selected DAs and
13.5% of the words respectively. We used four different
system summaries for each of the 6 meetings: one based
on the MMR method in MEAD (Carbonell and Gold-
stein, 1998; et al, 2003), the other three are the system
output from (Galley, 2006; Murray et al, 2005; Xie and
Liu, 2008). All the system generated summaries contain
around 5% of the DAs and 16% of the words of the entire
meeting. Thus, in total we have 36 human summaries and
24 system summaries on the 6 test meetings, on which
the correlation between ROUGE and human evaluation
is calculated and investigated.
All the experiments in this paper are based on human
transcriptions, with a central interest on whether some
characteristics of the meeting recordings affect the corre-
lation between ROUGE and human evaluations, without
the effect from speech recognition or automatic sentence
segmentation errors.
3.2 Automatic ROUGE Evaluation
ROUGE (Lin, 2004) measures the n-grammatch between
system generated summaries and human summaries. In
most of this study, we used the same options in ROUGE
as in the DUC summarization evaluation (NIST, 2007),
and modify the input to ROUGE to account for the fol-
lowing two phenomena.
? Disfluencies
Meetings contain spontaneous speech with many
disfluencies, such as filled pauses (uh, um), dis-
course markers (e.g., I mean, you know), repetitions,
corrections, and incomplete sentences. There have
been efforts on the study of the impact of disfluen-
cies on summarization techniques (Liu et al, 2007;
Zhu and Penn, 2006) and human readability (Jones
et al, 2003). However, it is not clear whether dis-
fluencies impact automatic evaluation of extractive
meeting summarization.
Since we use extractive summarization, summary
sentences may contain difluencies. We hand anno-
tated the transcripts for the 6 meetings and marked
the disfluencies such that we can remove them to
obtain cleaned up sentences for those selected sum-
mary sentences. To study the impact of disfluencies,
we run ROUGE using two different inputs: sum-
maries based on the original transcription, and the
summaries with disfluencies removed.
? Speaker information
The existence of multiple speakers in meetings
raises questions about the evaluation method. (Gal-
ley, 2006) considered some location constrains in
meeting summarization evaluation, which utilizes
speaker information to some extent. In this study
we use the data in separate channels for each speaker
and thus have the speaker information available for
each sentence. We associate the speaker ID with
each word, treat them together as a new ?word? in
the input to ROUGE.
3.3 Human Evaluation
Five human subjects (all undergraduate students in Com-
puter Science) participated in human evaluation. In to-
tal, there are 20 different summaries for each of the 6
test meetings: 6 human-generated, 4 system-generated,
and their corresponding ones with disfluencies removed.
We assigned 4 summaries with different configurations to
each human subject: human vs. system generated sum-
maries, with or without disfluencies. Each human evalu-
ated 24 summaries in total, for the 6 test meetings.
For each summary, the human subjects were asked to
rate the following statements using a scale of 1-5 accord-
ing to the extent of their agreement with them.
? S1: The summary reflects the discussion flow in the meet-
ing very well.
? S2: Almost all the important topic points of the meeting
are represented.
? S3: Most of the sentences in the summary are relevant to
the original meeting.
? S4: The information in the summary is not redundant.
? S5: The relationship between the importance of each topic
in the meeting and the amount of summary space given to
that topic seems appropriate.
? S6: The relationship between the role of each speaker and
the amount of summary speech selected for that speaker
seems appropriate.
? S7: Some sentences in the summary convey the same
meaning.
? S8: Some sentences are not necessary (e.g., in terms of
importance) to be included in the summary.
? S9: The summary is helpful to someone who wants to
know what are discussed in the meeting.
202
These statements are an extension of those used in
(Murray et al, 2005) for human evaluation of meeting
summaries. The additional ones we added were designed
to account for the discussion flow in the meetings. Some
of the statements above are used to measure similar as-
pects, but from different perspectives, such as S5 and S6,
S4 and S7. This may reduce some accidental noise in hu-
man evaluation. We grouped these statements into 4 cat-
egories: Informative Structure (IS): S1, S5 and S6; Infor-
mative Coverage (IC): S2 and S9; Informative Relevance
(IRV): S3 and S8; and Informative Redundancy (IRD):
S4 and S7.
4 Results
4.1 Correlation between Human Evaluation and
Original ROUGE Score
Similar to (Murray et al, 2005), we also use Spearman?s
rank coefficient (rho) to investigate the correlation be-
tween ROUGE and human evaluation. We have 36 hu-
man summaries and 24 system summaries for the 6 meet-
ings in our study. For each of the human summaries,
the ROUGE scores are generated using the other 5 hu-
man summaries as references. For system generated sum-
maries, we calculate the ROUGE score using 5 human
references, and then obtain the average from 6 such se-
tups. The correlation results are presented in Table 1.
In addition to the overall average for human evaluation
(H AVG), we calculated the average score for each evalu-
ation category (see Section 3.3). For ROUGE evaluation,
we chose the F-measure for R-1 (unigram) and R-SU4
(skip-bigram with maximum gap length of 4), which is
based on our observation that other scores in ROUGE are
always highly correlated (rho>0.9) to either of them for
this task. We compute the correlation separately for the
human and system summaries in order to avoid the im-
pact due to the inherent difference between the two dif-
ferent summaries.
Correlation on Human Summaries
H AVG H IS H IC H IRV H IRD
R-1 0.09 0.22 0.21 0.03 -0.20
R-SU4 0.18 0.33 0.38 0.04 -0.30
Correlation on System Summaries
R-1 -0.07 -0.02 -0.17 -0.27 -0.02
R-SU4 0.08 0.05 0.01 -0.15 0.14
Table 1: Spearman?s rho between human evaluation (H) and
ROUGE (R) with basic setting.
We can see that R-SU4 obtains a higher correlation
with human evaluation than R-1 on the whole, but still
very low, which is consistent with the previous conclu-
sion from (Murray et al, 2005). Among the four cat-
egories, better correlation is achieved for information
structure (IS) and information coverage (IC) compared
to the other two categories. This is consistent with what
ROUGE is designed for, ?recall oriented understudy gist-
ing evaluation? ? we expect it to model IS and IC well
by ngram and skip-bigram matching but not relevancy
(IRV) and redundancy (IRD) effectively. In addition, we
found low correlation on system generated summaries,
suggesting it is more challenging to evaluate those sum-
maries both by humans and the automatic metrics.
4.2 Impacts of Disfluencies on Correlation
Table 2 shows the correlation results between ROUGE
(R-SU4) and human evaluation on the original and
cleaned up summaries respectively. For human sum-
maries, after removing disfluencies, the correlation be-
tween ROUGE and human evaluation improves on the
whole, but degrades on information structure (IS) and in-
formation coverage (IC) categories. However, for sys-
tem summaries, there is a significant gain of correlation
on those two evaluation categories, even though no im-
provement on the overall average score. Our hypothesis
for this is that removing disfluencies helps remove the
noise in the system generated summaries and make them
more easily to be evaluated by human and machines. In
contrast, the human created summaries have better qual-
ity in terms of the information content and may not suffer
as much from the disfluencies contained in the summary.
Correlation on Human Summaries
H AVG H IS H IC H IRV H IRD
Original 0.18 0.33 0.38 0.04 -0.30
Disfluencies 0.21 0.21 0.31 0.19 -0.16
removed
Correlation on System Summaries
Original 0.08 0.05 0.01 -0.15 0.14)
Disfluencies 0.08 0.22 0.19 -0.02 -0.07
removed
Table 2: Effect of disfluencies on the correlation between R-
SU4 and human evaluation.
4.3 Incorporating Speaker Information
We further incorporated speaker information in ROUGE
setting using the summaries with disfluencies removed.
Table 3 presents the resulting correlation values between
ROUGE SU4 score and human evaluation. For human
summaries, adding speaker information slightly degraded
the correlation, but it is still better compared to using
the original transcripts (results in Table 1). For the sys-
tem summaries, the overall correlation is significantly im-
proved, with some significant improvement in the infor-
mation redundancy (IRD) category. This suggests that
by leveraging speaker information, ROUGE can assign
better credits or penalties to system generated summaries
(same words from different speakers will not be counted
as a match), and thus yield better correlation with human
evaluation; whereas for human summaries, this may not
happen often. For similar sentences from different speak-
ers, human annotators are more likely to agree with each
203
other in their selection compared to automatic summa-
rization.
Correlation on Human Summaries
Speaker Info. H AVG H IS H IC H IRV H IRD
NO 0.21 0.21 0.31 0.19 -0.16
YES 0.20 0.20 0.27 0.12 -0.09
Correlation on System Summaries
NO 0.08 0.22 0.19 -0.02 -0.07
YES 0.14 0.20 0.16 0.02 0.21
Table 3: Effect of speaker information on the correlation be-
tween R-SU4 and human evaluation.
5 Conclusion and Future Work
In this paper, we have made a first attempt to system-
atically investigate the correlation of automatic ROUGE
scores with human evaluation for meeting summariza-
tion. Adaptations on ROUGE setting based on meeting
characteristics are proposed and evaluated using Spear-
man?s rank coefficient. Our experimental results show
that in general the correlation between ROUGE scores
and human evaluation is low, with ROUGE SU4 score
showing better correlation than ROUGE-1 score. There
is significant improvement in correlation when disfluen-
cies are removed and speaker information is leveraged,
especially for evaluating system-generated summaries. In
addition, we observe that the correlation is affected differ-
ently by those factors for human summaries and system-
generated summaries.
In our future work we will examine the correlation be-
tween each statement and ROUGE scores to better rep-
resent human evaluation results instead of using simply
the average over all the statements. Further studies are
also needed using a larger data set. Finally, we plan to in-
vestigate meeting summarization evaluation using speech
recognition output.
Acknowledgments
The authors thank University of Edinburgh for providing the an-
notated ICSI meeting corpus and Michel Galley for sharing his
tool to process the annotated data. We also thank Gabriel Mur-
ray and Michel Galley for letting us use their automatic summa-
rization system output for this study. This work is supported by
NSF grant IIS-0714132. Any opinions expressed in this work
are those of the authors and do not necessarily reflect the views
of NSF.
References
J. Carbonell and J. Goldstein. 1998. The use of mmr, diversity-
based reranking for reordering documents and producing
summaries. In SIGIR, pages 335?336.
M. Galley. 2006. A skip-chain conditional random field
for ranking meeting utterances by importance. In EMNLP,
pages 364?372.
C. Hori, T. Hori, and S. Furui. 2003. Evaluation methods for
automatic speech summarization. In EUROSPEECH, pages
2825?2828.
E. Hovy, C. Lin, L. Zhou, and J. Fukumoto. 2006. Automated
summarization evaluation with basic elements. In LREC.
A. Janin, D. Baron, J. Edwards, D. Ellis, G. Gelbart, N. Norgan,
B. Peskin, T. Pfau, E. Shriberg, A. Stolcke, and C. Wooters.
2003. The icsi meeting corpus. In ICASSP.
K. S. Jones and J. Galliers. 1996. Evaluating natural language
processing systems: An analysis and review. Lecture Notes
in Artificial Intelligence.
D. Jones, F. Wlof, E. Gilbson, E. Williams, E. Fedorenko,
D. Reynolds, and M. Zissman. 2003. Measuring the
readability of automatic speech-to-text transcripts. In EU-
ROSPEECH, pages 1585?1588.
C. Lin. 2004. Rouge: A package for automatic evaluation of
summaries. In Workshop on Text Summarization Branches
Out at ACL, pages 74?81.
Y. Liu, F. Liu, B. Li, and S. Xie. 2007. Do disfluencies af-
fect meeting summarization? a pilot study on the impact of
disfluencies. In MLMI Workshop, Poster Session.
I. Mani, T. Firmin, D. House, M. Chrzanowski, G. Klein,
L. Hirschman, B. Sundheim, and L. Obrst. 1998. The tipster
summac text summarization evaluation: Final report. Tech-
nical report, The MITRE Corporation.
G. Murray, S. Renals, J. Carletta, and J. Moore. 2005. Eval-
uating automatic summaries of meeting recordings. In ACL
2005 MTSE Workshop, pages 33?40.
A. Nenkova and R. Passonneau. 2004. Evaluating con-
tent selection in summarization: the pyramid method. In
HLT/NAACL.
NIST. 2007. Document understanding conference (DUC).
http://duc.nist.gov/.
D. Radev, T. Allison, S. Blair-Goldensohn, J. Blitzer, A. C?elebi,
E. Drabek, W. Lam, D. Liu, H. Qi, H. Saggion, S. Teufel,
M. Topper, and A. Winkel. 2003. The MEAD Multidocu-
ment Summarizer. http://www.summarization.com/mead/.
D. R. Radev, H. Jing, M. Stys, and T. Daniel. 2004. Centroid-
based summarization of multiple documents. Information
Processing and Management, 40:919?938.
E. Shriberg, R. Dhillon, S. Bhagat, J. Ang, and H. Carvey. 2004.
The icsi meeting recorder dialog act (mrda) corpus. In SIG-
DAL Workshop, pages 97?100.
S. Teufel and H. Halteren. 2004. Evaluating information con-
tent by factoid analysis: Human annotation and stability. In
EMNLP.
S. Xie and Y. Liu. 2008. Using corpus and knowledge-based
similarity measure in maximummarginal relevance for meet-
ing summarization. In ICASSP.
X. Zhu and G. Penn. 2005. Evaluation of sentence selection for
speech summarization. In ACL Workshop on Intrinsic and
Extrinsic Evaluation Measures for MT and/or Summariza-
tion.
X. Zhu and G. Penn. 2006. Comparing the roles of tex-
tual, acoustic and spoken-language features on spontaneous-
conversation summarization. In HLT/NAACL.
204
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 540?548,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Semi-supervised Learning for Automatic Prosodic Event Detection
Using Co-training Algorithm
Je Hun Jeon and Yang Liu
Computer Science Department
The University of Texas at Dallas, Richardson, TX, USA
{jhjeon,yangl}@hlt.utdallas.edu
Abstract
Most of previous approaches to automatic
prosodic event detection are based on su-
pervised learning, relying on the avail-
ability of a corpus that is annotated with
the prosodic labels of interest in order to
train the classification models. However,
creating such resources is an expensive
and time-consuming task. In this paper,
we exploit semi-supervised learning with
the co-training algorithm for automatic de-
tection of coarse level representation of
prosodic events such as pitch accents, in-
tonational phrase boundaries, and break
indices. We propose a confidence-based
method to assign labels to unlabeled data
and demonstrate improved results using
this method compared to the widely used
agreement-based method. In addition, we
examine various informative sample selec-
tion methods. In our experiments on the
Boston University radio news corpus, us-
ing only a small amount of the labeled data
as the initial training set, our proposed la-
beling method combined with most confi-
dence sample selection can effectively use
unlabeled data to improve performance
and finally reach performance closer to
that of the supervised method using all the
training data.
1 Introduction
Prosody represents suprasegmental information in
speech since it normally extends over more than
one phoneme segment. Prosodic phenomena man-
ifest themselves in speech in different ways, in-
cluding changes in relative intensity to emphasize
specific words or syllables, variations of the fun-
damental frequency range and contour, and subtle
timing variations, such as syllable lengthening and
insertion of pause. In spoken utterances, speakers
use prosody to convey emphasis, intent, attitude,
and emotion. These are important cues to aid the
listener for interpretation of speech. Prosody also
plays an important role in automatic spoken lan-
guage processing tasks, such as speech act detec-
tion and natural speech synthesis, because it in-
cludes aspect of higher level information that is
not completely revealed by segmental acoustics or
lexical information.
To represent prosodic events for the categorical
annotation schemes, one of the most popular label-
ing schemes is the Tones and Break Indices (ToBI)
framework (Silverman et al, 1992). The most im-
portant prosodic phenomena captured within this
framework include pitch accents (or prominence)
and prosodic phrase boundaries. Within the ToBI
framework, prosodic phrasing refers to the per-
ceived grouping of words in an utterance, and
accent refers to the greater perceived strength or
emphasis of some syllables in a phrase. Cor-
pora annotated with prosody information can be
used for speech analysis and to learn the relation-
ship between prosodic events and lexical, syntac-
tic and semantic structure of the utterance. How-
ever, it is very expensive and time-consuming to
perform prosody labeling manually. Therefore,
automatic labeling of prosodic events is an attrac-
tive alternative that has received attention over the
past decades. In addition, automatically detecting
prosodic events also benefits many other speech
understanding tasks.
Many previous efforts on prosodic event de-
tection were supervised learning approaches that
used acoustic, lexical, and syntactic cues. How-
ever, the major drawback with these methods is
that they require a hand-labeled training corpus
and depend on specific corpus used for training.
Limited research has been conducted using unsu-
pervised and semi-supervised methods. In this pa-
per, we exploit semi-supervised learning with the
540
Figure 1: An example of ToBI annotation on a sentence ?Hennessy will be a hard act to follow.?
co-training algorithm (Blum and Mitchell, 1998)
for automatic prosodic event labeling. Two dif-
ferent views according to acoustic and lexical-
syntactic knowledge sources are used in the co-
training framework. We propose a confidence-
based method to assign labels to unlabeled data
in training iterations and evaluate its performance
combined with different informative sample se-
lection methods. Our experiments on the Boston
Radio News corpus show that the use of unla-
beled data can lead to significant improvement
of prosodic event detection compared to using
the original small training set, and that the semi-
supervised learning result is comparable with su-
pervised learning with similar amount of training
data.
The remainder of this paper is organized as fol-
lows. In the next section, we provide details of
the corpus and the prosodic event detection tasks.
Section 3 reviews previous work briefly. In Sec-
tion 4, we describe the classification method for
prosodic event detection, including the acoustic
and syntactic prosodic models, and the features
used. Section 5 introduces the co-training algo-
rithm we used. Section 6 presents our experiments
and results. The final section gives a brief sum-
mary along with future directions.
2 Corpus and tasks
In this paper, our experiments were carried out
on the Boston University Radio News Corpus
(BU) (Ostendorf et al, 2003) which consists
of broadcast news style read speech and has
ToBI-style prosodic annotations for a part of the
data. The corpus is annotated with orthographic
transcription, automatically generated and hand-
corrected part-of-speech (POS) tags, and auto-
matic phone alignments.
The main prosodic events that we are concerned
to detect automatically in this paper are phrasing
and accent (or prominence). Prosodic phrasing
refers to the perceived grouping of words in an ut-
terance, and prominence refers to the greater per-
ceived strength or emphasis of some syllables in
a phrase. In the ToBI framework, the pitch accent
tones (*) are marked at every accented syllable and
have five types according to pitch contour: H*, L*,
L*+H, L+H*, H+!H*. The phrase boundary tones
are marked at every intermediate phrase boundary
(L-, H-) or intonational phrase boundary (L-L%,
L-H%, H-H%, H-L%) at certain word boundaries.
There are also the break indices at every word
boundary which range in value from 0 through
4, where 4 means intonational phrase boundary, 3
means intermediate phrase boundary, and a value
under 3 means phrase-medial word boundary. Fig-
ure 1 shows a ToBI annotation example for a sen-
tence ?Hennessy will be a hard act to follow.? The
first and second tiers show the orthographic infor-
mation such as words and syllables of the utter-
ance. The third tier shows the accents and phrase
boundary tones. The accent tone is located on each
accented syllable, such as the first syllable of word
?Hennessy.? The boundary tone is marked on ev-
ery final syllable if there is a prosodic boundary.
For example, there are intermediate phrase bound-
aries after words ?Hennessy? and ?act?, and there
is an intonational phrase boundary after word ?fol-
low.? The fourth tier shows the break indices at the
end of every word.
The detailed representation of prosodic events
in the ToBI framework creates a serious sparse
data problem for automatic prosody detection.
This problem can be alleviated by grouping ToBI
labels into coarse categories, such as presence or
absence of pitch accents and phrasal tones. This
also significantly reduces ambiguity of the task. In
this paper, we thus use coarse representation (pres-
ence versus absence) for three prosodic event de-
tection tasks:
541
? Pitch accents: accent mark (*) means pres-
ence.
? Intonational phrase boundaries (IPB): all of
the IPB tones (%) are grouped into one cate-
gory.
? Break indices: value 3 and 4 are grouped to-
gether to represent that there is a break. This
task is equivalent to detecting the presence of
intermediate and intonational phrase bound-
aries.
These three tasks are binary classification prob-
lems. Similar setup has also been used in other
previous work.
3 Previous work
Many previous efforts on prosodic event detec-
tion used supervised learning approaches. In the
work by Wightman and Ostendorf (1994), binary
accent, IPB, and break index were assigned to
syllables based on posterior probabilities com-
puted from acoustic evidence using decision trees,
combined with a bigram model of accent and
boundary patterns. Their method achieved an
accuracy of 84% for accent, 71% for IPB, and
84% for break index detection at the syllable
level. Chen et al (2004) used a Gaussian mix-
ture model for acoustic-prosodic information and
neural network based syntactic-prosodic model
and achieved pitch accent detection accuracy of
84% and IPB detection accuracy of 90% at the
word level. The experiments of Ananthakrish-
nan and Narayanan (2008) with neural network
based acoustic-prosodic model and a factored n-
gram syntactic model reported 87% accuracy on
accent and break index detection at the syllable
level. The work of Sridhar et al (2008) using a
maximum entropy model achieved accent and IPB
detection accuracies of 86% and 93% on the word
level.
Limited research has been done in prosodic
detection using unsupervised or semi-supervised
methods. Ananthakrishnan and Narayanan (2006)
proposed an unsupervised algorithm for prosodic
event detection. This algorithm was based on clus-
tering techniques to make use of acoustic and syn-
tactic cues and achieved accent and IPB detec-
tion accuracies of 77.8% and 88.5%, compared
with the accuracies of 86.5% and 91.6% with su-
pervised methods. Similarly, Levow (2006) tried
clustering based unsupervised approach on ac-
cent detection with only acoustic evidence and
reported accuracy of 78.4% for accent detection
compared with 80.1% using supervised learning.
She also exploited a semi-supervised approach us-
ing Laplacian SVM classification on a small set of
examples. This approach achieved 81.5%, com-
pared to 84% accuracy for accent detection in a
fully supervised fashion.
Since Blum and Mitchell (1998) proposed co-
training, it has received a lot of attention in the re-
search community. This multi-view setting applies
well to learning problems that have a natural way
to divide their features into subsets, each of which
are sufficient to learn the target concept. Theo-
retical and empirical analysis has been performed
for the effectiveness of co-training such as Blum
and Mitchell (1998), Goldman and Zhou (2000),
Nigam and Ghani (2000), and Dasuta et al (2001).
More recently, researchers have begun to explore
ways of combing ideas from sample selection with
that of co-training. Steedman et al (2003) ap-
plied co-training method to statistical parsing and
introduced sample selection heuristics. Clark et
al. (2003) and Wang et al (2007) applied co-
training method in POS tagging using agreement-
based selection strategy. Co-testing (Muslea et
al., 2000), one of active learning approaches, has
a similar spirit. Like co-training, it consists of
two classifiers with redundant views and compares
their outputs for an unlabeled example. If they
disagree, then the example is considered as a con-
tention point, and therefore a good candidate for
human labeling.
In this paper, we apply co-training algorithm
to automatic prosodic event detection and propose
methods to better select samples to improve semi-
supervised learning performance for this task.
4 Prosodic event detection method
We model the prosody detection problem as a clas-
sification task. We separately develop acoustic-
prosodic and syntactic-prosodic models accord-
ing to information sources and then combine the
two models. Our previous supervised learning ap-
proach (Jeon and Liu, 2009) showed that a com-
bined model using Neural Network (NN) classifier
for acoustic-prosodic evidence and Support Vector
Machine (SVM) classifier for syntactic-prosodic
evidence performed better than other classifiers.
We therefore use NN and SVM in this study. Note
542
that our feature extraction is performed at the syl-
lable level. This is straightforward for accent de-
tection since stress is defined associated with syl-
lables. In the case of IPB and break index detec-
tion, we use only the features from the final syl-
lable of a word since those events are associated
with word boundaries.
4.1 The acoustic-prosodic model
The most likely sequence of prosodic events P ? =
{p?1, . . . , p?n} given the sequence of acoustic evi-
dences A = {a1, . . . , an} can be found as follow-
ing:
P ? = arg max
P
p(P |A)
? arg max
P
n
?
i=1
p(pi|ai) (1)
where ai = {a1i , . . . , ati} is the acoustic feature
vector corresponding to a syllable. Note that this
assumes that the prosodic events are independent
and they are only dependent on the acoustic obser-
vations in the corresponding locations.
The primary acoustic cues for prosodic events
are pitch, energy and duration. In order to reduce
the effect by both inter-speaker and intra-speaker
variation, both pitch and energy values were nor-
malized (z-value) with utterance specific means
and variances. The acoustic features used in our
experiments are listed below. Again, all of the fea-
tures are computed for a syllable.
? Pitch range (4 features): maximum pitch,
minimum pitch, mean pitch, and pitch range
(difference between maximum and minimum
pitch).
? Pitch slope (5 features): first pitch slope, last
pitch slope, maximum plus pitch slope, max-
imum minus pitch slope, and the number of
changes in the pitch slope patterns.
? Energy range (4 features): maximum en-
ergy, minimum energy, mean energy, and
energy range (difference between maximum
and minimum energy).
? Duration (3 features): normalized vowel du-
ration, pause duration after the word final syl-
lable, and the ratio of vowel durations be-
tween this syllable and the next syllable.
Among the duration features, the pause dura-
tion and the ratio of vowel durations are only used
to detect IPB and break index, not for accent de-
tection.
4.2 The syntactic-prosodic model
The prosodic events P ? given the sequence of lex-
ical and syntactic evidences S = {s1, . . . , sn} can
be found as following:
P ? = arg max
P
p(P |S)
? arg max
P
n
?
i=1
p(pi|?(si)) (2)
where ?(si) is chosen such that it contains lexi-
cal and syntactic evidence from a fixed window of
syllables surrounding location i.
There is a very strong correlation between the
prosodic events in an utterance and its lexical and
syntactic structure. Previous studies have shown
that for pitch accent detection, the lexical features
such as the canonical stress patterns from the pro-
nunciation dictionary perform better than the syn-
tactic features, while for IPB and break index de-
tection, the syntactic features such as POS work
better than the lexical features. We use different
feature types for each task and the detailed fea-
tures are as follows:
? Accent detection: syllable identity, lexical
stress (exist or not), word boundary informa-
tion (boundary or not), and POS tag. We
also include syllable identity, lexical stress,
and word boundary features from the previ-
ous and next context window.
? IPB and Break index detection: POS tag, the
ratio of syntactic phrases the word initiates,
and the ratio of syntactic phrases the word
terminates. All of these features from the pre-
vious and next context windows are also in-
cluded.
4.3 The combined model
The two models above can be coupled as a classi-
fier for prosodic event detection. If we assume that
the acoustic observations are conditionally inde-
pendent of the syntactic features given the prosody
labels, the task of prosodic detection is to find the
optimal sequence P ? as follows:
P ? = arg max
P
p(P |A,S)
543
? arg max
P
p(P |A)p(P |S)
? arg max
P
n
?
i=1
p(pi|ai)?p(pi|?(si)) (3)
where ? is a parameter that can be used to adjust
the weighting between syntactic and the acoustic
model. In our experiments, the value of ? is esti-
mated based on development data.
5 Co-training strategy for prosodic event
detection
Co-training (Blum and Mitchell, 1998) is a semi-
supervised multi-view algorithm that uses the ini-
tial training set to learn a (weak) classifier in each
view. Then each classifier is applied to all the
unlabeled examples. Those examples that each
classifier makes the most confident predictions are
selected and labeled with the estimated class la-
bels and added to the training set. Based on the
new training set, a new classifier is learned in each
view, and the whole process is repeated for some
iterations. At the end, a final hypothesis is cre-
ated by combining the predictions of the classifiers
learned in each view.
As described in Section 4, we use two classi-
fiers for the prosodic event detection task based
on two different information sources: one is the
acoustic evidence extracted from the speech signal
of an utterance; the other is the lexical and syn-
tactic evidence such as syllables, words, POS tags
and phrasal boundary information. These are two
different views for prosodic event detection and fit
the co-training framework.
The general co-training algorithm we used is
described in Algorithm 1. Given a set L of labeled
data and a set U of unlabeled data, the algorithm
first creates a smaller pool U? containing u unla-
beled data. It then iterates in the following proce-
dure. First, we use L to train two distinct classi-
fiers: the acoustic-prosodic classifier h1, and the
syntactic classifier h2. These two classifiers are
used to examine the unlabeled set U? and assign
?possible? labels. Then we select some samples
to add to L. Finally, the pool U? is recreated from
U at random. This iteration continues until reach-
ing the defined number of iterations or U is empty.
The main issue of co-training is to select train-
ing samples for next iteration so as to minimize
noise and maximize training utility. There are two
issues: (1) the accurate self-labeling method for
unlabeled data and (2) effective heuristics to se-
Algorithm 1 General co-training algorithm.
Given a set L of labeled training data and a set
U of unlabeled data
Randomly select U? from U, |U?|=u
while iteration < k do
Use L to train classifiers h1 and h2
Apply h1 and h2 to assign labels for all ex-
amples in U?
Select n self-labeled samples and add to L
Remove these n samples from U
Recreate U? by choosing u instances ran-
domly from U
end while
lect more informative examples. We investigate
different approaches to address these issues for
the prosodic event detection task. The first is-
sue is how to assign possible labels accurately.
The general method is to let the two classifiers
predict the class for a given sample, and if they
agree, the hypothesized label is used. However,
when this agreement-based approach is used for
prosodic event detection, we notice that there is
not only difference in the labeling accuracy be-
tween positive and negative samples, but also an
imbalance of the self-labeled positive and negative
examples (details in Section 6). Therefore we be-
lieve that using the hard decisions from the two
classifiers along with the agreement-based rule is
not enough to label the unlabeled samples. To ad-
dress this problem, we propose an approximated
confidence measure based on the combined classi-
fier (Equation 3). First, we take a squared root of
the classifier?s posterior probabilities for the two
classes, denoted as score(pos) and score(neg),
respectively. Our proposed confidence is the dis-
tance between these two scores. For example, if
the classifier?s hypothesized label is positive, then:
Positive confidence=score(pos)-score(neg)
Similarly if the classifier?s hypothesis is negative,
we calculate a negative confidence:
Negative confidence=score(neg)-score(pos)
Then we apply different thresholds of confi-
dence level for positive and negative labeling. The
thresholds are chosen based on the accuracy distri-
bution obtained on the labeled development data
and are reestimated at every iteration. Figure 2
shows the accuracy distribution for accent detec-
tion according to different confidence levels in the
first iteration. In Figure 2, if we choose 70% label-
ing accuracy, the positive confidence level is about
544
0 0.2 0.4 0.6 0.8 1
0.2
0.4
0.6
0.8
1
Confidence level
A
cc
ur
ac
y
Figure 2: Approximated confidence level and la-
beling accuracy on accent detection task.
0.1 and the negative confidence level is about 0.8.
In our confidence-based approach, the samples
with a confidence level higher than these thresh-
olds are assigned with the classifier?s hypothesized
labels, and the other samples are disregarded.
The second problem in co-training is how to
select informative samples. Active learning ap-
proaches, such as Muslea et al (2000), can gener-
ally select more informative samples, for example,
samples for which two classifiers disagree (since
one of two classifiers is wrong) and ask for human
labels. Co-training approaches cannot, however,
use this selection method since there is a risk to
label the disagreed samples. Usually co-training
selects samples for which two classifiers have the
same prediction but high difference in their con-
fidence measures. Based on this idea, we applied
three sampling strategies on top of our confidence-
based labeling method:
? Random selection: randomly select samples
from those that the two classifiers have dif-
ferent posterior probabilities.
? Most confident selection: select samples that
have the highest posterior probability based
on one classifier, and at the same time there
is certain posterior probability difference be-
tween the two classifiers.
? Most different selection: select samples that
have the most difference between the two
classifiers? posterior probabilities.
The first strategy is appropriate for base classi-
fiers that lack the capability of estimating the pos-
terior probability of their predictions. The second
is appropriate for base classifiers that have high
classification accuracy and also with high poste-
rior probability. The last one is also appropriate
for accurate classifiers and expected to converge
utter. word syll Speaker
Test Set 102 5,448 8,962 f1a, m1b
Development Set 20 1,356 2,275 f2b, f3b
Labeled set L 5 347 573 m2b, m3b
Unlabeled set U 1,027 77,207 129,305 m4b
Table 1: Training and test sets.
faster since big mistakes of one of the two classi-
fiers can be fixed. These sample selection strate-
gies share some similarity with those in previous
work (Steedman et al, 2003).
6 Experiments and results
Our goal is to determine whether the co-training
algorithm described above could successfully use
the unlabeled data for prosodic event detection. In
our experiment, 268 ToBI labeled utterances and
886 unlabeled utterances in BU corpus were used.
Among labeled data, 102 utterances of all f1a and
m1b speakers are used for testing, 20 utterances
randomly chosen from f2b, f3b, m2b, m3b, and
m4b are used as development set to optimize pa-
rameters such as ? and confidence level thresh-
old, 5 utterances are used as the initial training
set L, and the rest of the data is used as unlabeled
set U, which has 1027 unlabeled utterances (we
removed the human labels for co-training exper-
iments). The detailed training and test setting is
shown in Table 1.
First of all, we compare the learning curves us-
ing our proposed confidence-based method to as-
sign possible labels with the simple agreement-
based random selection method. We expect that if
self-labeling is accurate, adding new samples ran-
domly drawn from these self-labeled data gener-
ally should not make performance worse. For this
experiment, in every iteration, we randomly se-
lect the self-labeled samples that have at least 0.1
difference between two classifiers? posterior prob-
abilities. The number of new samples added to
training is 5% of the size of the previous training
data. Figure 3 shows the learning curves for accent
detection. The number of samples in the x-axis
is the number of syllables. The F-measure score
using the initial training data is 0.69. The dark
solid line in Figure 3 is the learning curve of the
supervised method when varying the size of the
training data. Compared with supervised method,
our proposed relative confidence-based labeling
method shows better performance when there is
545
5,000 10,000 15,000
0.55
0.6
0.65
0.7
0.75
0.8
0.85
# of samples
F?
m
ea
su
re
 
 
Supervised
Agreement based
Confidence based
Figure 3: The learning curve of agreement-based
and our proposed confidence-based random selec-
tion methods for accent detection.
Confidence Agreement
Accent
detection
% of P samples 47% 38%
P sample error 0.17 0.09
N sample error 0.12 0.22
IPB
detection
% of P samples 46% 19%
P sample error 0.12 0.01
N sample error 0.18 0.53
Break
detection
% of P samples 50% 25%
P sample error 0.15 0.03
N sample error 0.17 0.42
Table 2: Percentage of positive samples, and
averaged error rate for positive (P) and nega-
tive (N) samples for the first 20 iterations using
the agreement-based and our confidence labeling
methods.
less data, but after some iteration, the performance
is saturated earlier. However, the agreement-based
method does not yield any performance gain, in-
stead, its performance is much worse after some
iteration. The other two prosodic event detection
tasks also show similar patterns.
To analyze the reason for this performance
degradation using the agreement-based method,
we compare the labels of the newly added samples
in random selection with the reference annotation.
Table 2 shows the percentage of the positive sam-
ples added for the first 20 iterations, and the av-
erage labeling error rate of those samples for the
self-labeled positive and negative classes for two
methods. The agreement-based random selection
added more negative samples that also have higher
error rate than the positive samples. Adding these
samples has a negative impact on the classifier?s
performance. In contrast, our confidence-based
approach balances the number of positive and neg-
ative samples and significantly reduces the error
5,000 10,000 15,000
0.65
0.7
0.75
0.8
# of samples
F?
m
ea
su
re
 
 
Supervised
Random
Most confident
Most different
Figure 4: The learning curve of 3 sample selection
methods for accent detection.
rates for the negative samples as well, thus leading
to performance improvement.
Next we evaluate the efficacy of the three sam-
ple selection methods described in Section 5,
namely, random, most confident, and most dif-
ferent selections. Figure 4 shows the learning
curves for the three selection methods for accent
detection. The same configuration is used as in
the previous experiment, i.e., at least 0.1 posterior
probability difference between the two classifiers,
and adding 5% of new samples in each iteration.
All of these sample selection approaches use the
confidence-based labeling. For comparison, Fig-
ure 4 also shows the learning curve for supervised
learning when varying the training size. We can
see from the figure that compared to random selec-
tion, the most confident selection method shows
similar performance in the first few iterations, but
its performance continues to increase and the sat-
uration point is much later than random selection.
Unlike the other two sample selection methods,
most different selection results in noticeable per-
formance degradation after some iteration. This
difference is caused by the high self-labeling er-
ror rate of selected samples. Both random and
most confident selections perform better than su-
pervised learning at the first few iterations. This is
because the new samples added have different pos-
terior probabilities by the two classifiers, and thus
one of the classifiers benefits from these samples.
Learning curves for the other two tasks (break
index and IPB detection) show similar pattern for
the random and most different selection methods,
but some differences in the most confident selec-
tion results. For the IPB task, the learning curve of
the most confident selection fluctuates somewhat
in the middle of the iterations with similar per-
formance to random selection, however, afterward
the performance is better than random selection.
546
5,000 10,000 15,000 20,000 25,000
0.68
0.7
0.72
0.74
0.76
0.78
0.8
# of samples
F?
m
ea
su
re
 
 
Supervised
5 utterances
10 utterances
20 utterances5 utterances
10 utterances
20 utterances
Figure 5: The learning curves for accent detection
using different amounts of initial labeled training
data.
For the break index detection, the learning curve
of most different selection increases more slowly
than random selection at the beginning, but the sat-
uration point is much later and therefore outper-
forms the random selection at the later iterations.
We also evaluated the effect of the amount of
initial labeled training data. In this experiment,
most confident selection is used, and the other con-
figurations are the same as the previous experi-
ment. The learning curve for accent detection is
shown in Figure 5 using different numbers of utter-
ances in the initial training data. The arrow marks
indicate the start position of each learning curve.
As we can see, the learning curve when using 20
utterances is slightly better than the others, but
there is no significant performance gain according
to the size of initial labeled training data.
Finally we compared our co-training perfor-
mance with supervised learning. For supervised
learning, all labeled utterances except for the test
set are used for training. We used most confi-
dent selection with proposed self-labeling method.
The initial training data in co-training is 3% of
that used for supervised learning. After 74 iter-
ations, the size of samples of co-training is similar
to that in the supervised method. Table 3 presents
the results of three prosodic event detection tasks.
We can see that the performance of co-training for
these three tasks is slightly worse than supervised
learning using all the labeled data, but is signifi-
cantly better than the original performance using
3% of hand labeled data.
Most of the previous work for prosodic event
detection reported their results using classification
accuracy instead of F-measure. Therefore to bet-
ter compare with previous work, we present be-
low the accuracy results in our approach. The co-
training algorithm achieves the accuracy of 85.3%,
Accent IPB Break
Supervised 0.82 0.74 0.77
Co-
training
Initial training (3%) 0.69 0.59 0.62
After 74 iterations 0.80 0.71 0.75
Table 3: The results (F-measure) of prosodic
event detection for supervised and co-training ap-
proaches.
90.1%, and 86.7% respectively for accent, intona-
tional phrase boundary, and break index detection,
compared with 87.6%, 92.3%, and 88.9% in su-
pervised learning. Although the test condition is
different, our result is significantly better than that
of other semi-supervised approaches of previous
work and comparable with supervised approaches.
7 Conclusions
In this paper, we exploit the co-training method
for automatic prosodic event detection. We intro-
duced a confidence-based method to assign possi-
ble labels to unlabeled data and evaluated the per-
formance combined with informative sample se-
lection methods. Our experimental results using
co-training are significantly better than the origi-
nal supervised results using the small amount of
training data, and closer to that using supervised
learning with a large amount of data. This sug-
gests that the use of unlabeled data can lead to sig-
nificant improvement for prosodic event detection.
In our experiment, we used some labeled data
as development set to estimate some parameters.
For the future work, we will perform analysis
of loss function of each classifier in order to es-
timate parameters without labeled development
data. In addition, we plan to compare this to other
semi-supervised learning techniques such as ac-
tive learning. We also plan to use this algorithm
to annotate different types of data, such as sponta-
neous speech, and incorporate prosodic events in
spoken language applications.
Acknowledgments
This work is supported by DARPA under Contract
No. HR0011-06-C-0023. Distribution is unlim-
ited.
References
A. Blum and T. Mitchell. 1998. Combining labeled
and unlabeled data with co-training. Proceedings of
547
the Workshop on Computational Learning Theory,
pp. 92-100.
C. W. Wightman and M. Ostendorf. 1994. Automatic
labeling of prosodic patterns. IEEE Transactions on
Speech and Audio Processing, Vol. 2(4), pp. 69-481.
G. Levow. 2006. Unsupervised and semi-supervised
learning of tone and pitch accent. Proceedings of
HLT-NAACL, pp. 224-231.
I. Muslea, S. Minton and C. Knoblock. 2000. Selec-
tive sampling with redundant views. Proceedings of
the 7th International Conference on Artificial Intel-
ligence, pp. 621-626.
J. Jeon and Y. Liu. 2009. Automatic prosodic event
detection using syllable-base acoustic and syntactic
features. Proceeding of ICASSP, pp. 4565-4568.
K. Chen, M. Hasegawa-Johnson, and A. Cohen. 2004.
An automatic prosody labeling system using ANN-
based syntactic-prosodic model and GMM-based
acoustic prosodic model. Proceedings of ICASSP,
pp. 509-512.
K. Nigam and R. Ghani. 2000 Analyzing the effec-
tiveness and applicability of Co-training Proceed-
ings 9th International Conference on Information
and Knowledge Management, pp. 86-93.
K. Silverman, M. Beckman, J. Pitrelli, M. Ostendorf,
C. Wightman, P. Price, J. Pierrehumbert, and J.
Hirschberg. 1992. ToBI: A standard for labeling
English prosody. Proceedings of ICSLP, pp. 867-
870.
M. Steedman, S. Baker, S. Clark, J. Crim, J. Hocken-
maier, R. Hwa, M. Osborne, P. Ruhlen, A. Sarkar
2003. CLSP WS-02 Final Report: Semi-Supervised
Training for Statistical Parsing.
M. Ostendorf, P. J. Price and S. Shattuck-Hunfnagel.
1995. The Boston University Radio News Corpus.
Linguistic Data Consortium.
S. Ananthakrishnan and S. Narayanan. 2006. Com-
bining acoustic, lexical, and syntactic evidence for
automatic unsupervised prosody labeling. Proceed-
ings of ICSLP, pp. 297-300.
S. Ananthakrishnan and S. Narayanan. 2008. Auto-
matic prosodic event detection using acoustic, lex-
ical and syntactic evidence. IEEE Transactions on
Audio, Speech and Language Processing, Vol. 16(1),
pp. 216-228.
S. Clark, J. Currant, and M. Osborne. 2003. Bootstrap-
ping POS taggers using unlabeled data. Proceedings
of CoNLL, pp. 49-55.
S. Dasupta, M. L. Littman, and D. McAllester. 2001.
PAC generalization bounds for co-training. Ad-
vances in Neural Information Processing Systems,
Vol. 14, pp. 375-382.
S. Goldman and Y. Zhou. 2000. Enhancing supervised
learning with unlabeled data. Proceedings of the
Seventeenth International Conference on Machine
Learning, pp. 327-334.
V. K. Rangarajan Sridhar, S. Bangalore, and S.
Narayanan. 2008. Exploiting acoustic and syntactic
features for automatic prosody labeling in a maxi-
mum entropy framework. IEEE Transactions on Au-
dio, Speech, and Language processing, pp. 797-811.
W. Wang, Z. Huang, and M. Harper. 2007. Semi-
supervised learning for part-of-speech tagging of
Mandarin transcribed speech. Proceeding of
ICASSP, pp. 137-140.
548
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 261?264,
Suntec, Singapore, 4 August 2009.
c?2009 ACL and AFNLP
From Extractive to Abstractive Meeting Summaries: Can It Be Done by
Sentence Compression?
Fei Liu and Yang Liu
Computer Science Department
The University of Texas at Dallas
Richardson, TX 75080, USA
{feiliu, yangl}@hlt.utdallas.edu
Abstract
Most previous studies on meeting summariza-
tion have focused on extractive summariza-
tion. In this paper, we investigate if we can
apply sentence compression to extractive sum-
maries to generate abstractive summaries. We
use different compression algorithms, includ-
ing integer linear programming with an addi-
tional step of filler phrase detection, a noisy-
channel approach using Markovization for-
mulation of grammar rules, as well as hu-
man compressed sentences. Our experiments
on the ICSI meeting corpus show that when
compared to the abstractive summaries, using
sentence compression on the extractive sum-
maries improves their ROUGE scores; how-
ever, the best performance is still quite low,
suggesting the need of language generation for
abstractive summarization.
1 Introduction
Meeting summaries provide an efficient way for people
to browse through the lengthy recordings. Most cur-
rent research on meeting summarization has focused on
extractive summarization, that is, it extracts important
sentences (or dialogue acts) from speech transcripts, ei-
ther manual transcripts or automatic speech recogni-
tion (ASR) output. Various approaches to extractive
summarization have been evaluated recently. Popular
unsupervised approaches are maximum marginal rele-
vance (MMR), latent semantic analysis (LSA) (Mur-
ray et al, 2005a), and integer programming (Gillick et
al., 2009). Supervised methods include hidden Markov
model (HMM), maximum entropy, conditional ran-
dom fields (CRF), and support vector machines (SVM)
(Galley, 2006; Buist et al, 2005; Xie et al, 2008;
Maskey and Hirschberg, 2006). (Hori et al, 2003) used
a word based speech summarization approach that uti-
lized dynamic programming to obtain a set of words to
maximize a summarization score.
Most of these summarization approaches aim for
selecting the most informative sentences, while less
attempt has been made to generate abstractive sum-
maries, or compress the extracted sentences and merge
them into a concise summary. Simply concatenating
extracted sentences may not comprise a good sum-
mary, especially for spoken documents, since speech
transcripts often contain many disfluencies and are re-
dundant. The following example shows two extractive
summary sentences (they are from the same speaker),
and part of the abstractive summary that is related to
these two extractive summary sentences. This is an ex-
ample from the ICSI meeting corpus (see Section 2.1
for more information on the data).
Extractive summary sentences:
Sent1: um we have to refine the tasks more and more which
of course we haven?t done at all so far in order to avoid this
rephrasing
Sent2: and uh my suggestion is of course we we keep the
wizard because i think she did a wonderful job
Corresponding abstractive summary:
the group decided to hire the wizard and continue with the
refinement...
In this paper, our goal is to answer the question if
we can perform sentence compression on an extrac-
tive summary to improve its readability and make it
more like an abstractive summary. Compressing sen-
tences could be a first step toward our ultimate goal
of creating an abstract for spoken documents. Sen-
tence compression has been widely studied in language
processing. (Knight and Marcu, 2002; Cohn and Lap-
ata, 2009) learned rewriting rules that indicate which
words should be dropped in a given context. (Knight
and Marcu, 2002; Turner and Charniak, 2005) applied
the noisy-channel framework to predict the possibil-
ities of translating a sentence to a shorter word se-
quence. (Galley and McKeown, 2007) extended the
noisy-channel approach and proposed a head-driven
Markovization formulation of synchronous context-
free grammar (SCFG) deletion rules. Unlike these ap-
proaches that need a training corpus, (Clarke and La-
pata, 2008) encoded the language model and a variety
of linguistic constraints as linear inequalities, and em-
ployed the integer programming approach to find a sub-
set of words that maximize an objective function.
Our focus in this paper is not on new compression al-
gorithms, but rather on using compression to bridge the
gap of extractive and abstractive summarization. We
use different automatic compression algorithms. The
first one is the integer programming (IP) framework,
where we also introduce a filler phrase (FP) detection
261
module based on the Web resources. The second one
uses the SCFG that considers the grammaticality of the
compressed sentences. Finally, as a comparison, we
also use human compression. All of these compressed
sentences are compared to abstractive summaries. Our
experiments using the ICSI meeting corpus show that
compressing extractive summaries can improve human
readability and the ROUGE scores against the refer-
ence abstractive summaries.
2 Sentence Compression of Extractive
Summaries
2.1 Corpus
We used the ICSI meeting corpus (Janin et al, 2003),
which contains naturally occurring meetings, each
about an hour long. All the meetings have been tran-
scribed and annotated with dialogue acts (DAs), top-
ics, abstractive and extractive summaries (Shriberg et
al., 2004; Murray et al, 2005b). In this study, we use
the extractive and abstractive summaries of 6 meetings
from this corpus. These 6 meetings were chosen be-
cause they have been used previously in other related
studies, such as summarization and keyword extraction
(Murray et al, 2005a). On average, an extractive sum-
mary contains 76 sentences
1
(1252 words), and an ab-
stractive summary contains 5 sentences (111 words).
2.2 Compression Approaches
2.2.1 Human Compression
The data annotation was conducted via Amazon Me-
chanical Turk
2
. Human annotators were asked to gen-
erate condensed version for each of the DAs in the ex-
tractive summaries. The compression guideline is sim-
ilar to (Clarke and Lapata, 2008). The annotators were
asked to only remove words from the original sentence
while preserving most of the important meanings, and
make the compressed sentence as grammatical as pos-
sible. The annotators can leave the sentence uncom-
pressed if they think no words need to be deleted; how-
ever, they were not allowed to delete the entire sen-
tence. Since the meeting transcripts are not as readable
as other text genres, we may need a better compression
guideline for human annotators. Currently we let the
annotators make their own judgment what is an appro-
priate compression for a spoken sentence.
We split each extractive meeting summary sequen-
tially into groups of 10 sentences, and asked 6 to 10
online workers to compress each group. Then from
these results, another human subject selected the best
annotation for each sentence. We also asked this hu-
man judge to select the 4-best compressions. However,
in this study, we only use the 1-best annotation result.
We would like to do more analysis on the 4-best results
in the future.
1
The extractive units are DAs. We use DAs and sentences
interchangeably in this paper when there is no ambiguity.
2
http://www.mturk.com/mturk/welcome
2.2.2 Filler Phrase Detection
We define filler phrases (FPs) as the combination of
two or more words, which could be discourse markers
(e.g., I mean, you know), editing terms, as well as some
terms that are commonly used by human but without
critical meaning, such as, ?for example?, ?of course?,
and ?sort of?. Removing these fillers barely causes any
information loss. We propose to use web information
to automatically generate a list of filler phrases and fil-
ter them out in compression.
For each extracted summary sentence of the 6 meet-
ings, we use it as a query to Google and examine the top
N returned snippets (N is 400 in our experiments). The
snippets may not contain all the words in a sentence
query, but often contain frequently occurring phrases.
For example, ?of course? can be found with high fre-
quency in the snippets. We collect all the phrases that
appear in both the extracted summary sentences and the
snippets with a frequency higher than three. Then we
calculate the inverse sentence frequency (ISF) for these
phrases using the entire ICSI meeting corpus. The ISF
score of a phrase i is:
isf
i
=
N
N
i
where N is the total number of sentences and N
i
is the
number of sentences containing this phrase. Phrases
with low ISF scores mean that they appear in many oc-
casions and are not domain- or topic-indicative. These
are the filler phrases we want to remove to compress
a sentence. The three phrases we found with the low-
est ISF scores are ?you know?, ?i mean? and ?i think?,
consistent with our intuition.
We also noticed that not all the phrases with low
ISF scores can be taken as FPs (?we are? would be a
counter example). We therefore gave the ranked list of
FPs (based on ISF values) to a human subject to select
the proper ones. The human annotator crossed out the
phrases that may not be removable for sentence com-
pression, and also generated simple rules to shorten
some phrases (such as turning ?a little bit? into ?a bit?).
This resulted in 50 final FPs and about a hundred sim-
plification rules. Examples of the final FPs are: ?you
know?, ?and I think?, ?some of?, ?I mean?, ?so far?, ?it
seems like?, ?more or less?, ?of course?, ?sort of?, ?so
forth?, ?I guess?, ?for example?. When using this list
of FPs and rules for sentence compression, we also re-
quire that an FP candidate in the sentence is considered
as a phrase in the returned snippets by the search en-
gine, and its frequency in the snippets is higher than a
pre-defined threshold.
2.2.3 Compression Using Integer Programming
We employ the integer programming (IP) approach in
the same way as (Clarke and Lapata, 2008). Given an
utterance S = w
1
, w
2
, ..., w
n
, the IP approach forms a
compression of this utterance only by dropping words
and preserving the word sequence that maximizes an
objective function, defined as the sum of the signifi-
262
cance scores of the consisting words and n-gram prob-
abilities from a language model:
max ? ?
n
?
i=1
y
i
? Sig(w
i
)
+ (1 ? ?) ?
n?2
?
i=0
n?1
?
j=i+1
n
?
k=j+1
x
ijk
? P (w
k
|w
i
, w
j
)
where y
i
and x
ijk
are two binary variables: y
i
= 1
represents that word w
i
is in the compressed sentence;
x
ijk
= 1 represents that the sequence w
i
, w
j
, w
k
is in the compressed sentence. A trade-off parameter
? is used to balance the contribution from the signif-
icance scores for individual words and the language
model scores. Because of space limitation, we omit-
ted the special sentence beginning and ending symbols
in the formula above. More details can be found in
(Clarke and Lapata, 2008). We only used linear con-
straints defined on the variables, without any linguistic
constraints.
We use the lp solve toolkit.
3
The significance score
for each word is its TF-IDF value (term frequency ?
inverse document frequency). We trained a language
model using SRILM
4
on broadcast news data to gen-
erate the trigram probabilities. We empirically set ? as
0.7, which gives more weight to the word significance
scores. This IP compression method is applied to the
sentences after filler phrases (FPs) are filtered out. We
refer to the output from this approach as ?FP + IP?.
2.2.4 Compression Using Lexicalized Markov
Grammars
The last sentence compression method we use is the
lexicalized Markov grammar-based approach (Galley
and McKeown, 2007) with edit word detection (Char-
niak and Johnson, 2001). Two outputs were generated
using this method with different compression rates (de-
fined as the number of words preserved in the com-
pression divided by the total number of words in the
original sentence).
5
We name them ?Markov (S1)? and
?Markov (S2)? respectively.
3 Experiments
First we perform human evaluation for the compressed
sentences. Again we use the Amazon Mechanical Turk
for the subjective evaluation process. For each extrac-
tive summary sentence, we asked 10 human subjects to
rate the compressed sentences from the three systems,
as well as the human compression. This evaluation was
conducted on three meetings, containing 244 sentences
in total. Participants were asked to read the original
sentence and assign scores to each of the compressed
sentences for its informativeness and grammaticality
respectively using a 1 to 5 scale. An overall score is
calculated as the average of the informativeness and
grammaticality scores. Results are shown in Table 1.
3
http://www.geocities.com/lpsolve
4
http://www.speech.sri.com/projects/srilm/
5
Thanks to Michel Galley to help generate these output.
For a comparison, we also include the ROUGE-1 F-
scores (Lin, 2004) of each system output against the
human compressed sentences.
Approach Info. Gram. Overall R-1 F (%)
Human 4.35 4.38 4.37 -
Markov (S1) 3.64 3.79 3.72 88.76
Markov (S2) 2.89 2.76 2.83 62.99
FP + IP 3.70 3.95 3.82 85.83
Table 1: Human evaluation results. Also shown is the
ROUGE-1 (unigram match) F-score of different sys-
tems compared to human compression.
We can see from the table that as expected, the hu-
man compression yields the best performance on both
informativeness and grammaticality. ?FP + IP? and
?Markov (S1)? approaches also achieve satisfying per-
formance under both evaluation metrics. The relatively
low scores for ?Markov (S2)? output are partly due to
its low compression rate (see Table 2 for the length in-
formation). As an example, we show below the com-
pressed sentences from human and systems for the first
sentence in the example in Sec 1.
Human: we have to refine the tasks in order to avoid
rephrasing
Markov (S1): we have to refine the tasks more and more
which we haven?t done in order to avoid this rephrasing
Markov (S2): we have to refine the tasks which we haven?t
done order to avoid this rephrasing
FP + IP: we have to refine the tasks more and more which
we haven?t done to avoid this rephrasing
Since our goal is to answer the question if we can
use sentence compression to generate abstractive sum-
maries, we compare the compressed summaries, as
well as the original extractive summaries, against the
reference abstractive summaries. The ROUGE-1 re-
sults along with the word compression ratio for each
compression approach are shown in Table 2. We can
see that all of the compression algorithms yield bet-
ter ROUGE score than the original extractive sum-
maries. Take Markov (S2) as an example. The recall
rate dropped only 8% (from the original 66% to 58%)
when only 53% words in the extractive summaries are
preserved. This demonstrates that it is possible for the
current sentence compression systems to greatly con-
dense the extractive summaries while preserving the
desirable information, and thus yield summaries that
are more like abstractive summaries. However, since
the abstractive summaries are much shorter than the ex-
tractive summaries (even after compression), it is not
surprising to see the low precision results as shown in
Table 2. We also observe some different patterns be-
tween the ROUGE scores and the human evaluation
results in Table 1. For example, Markov (S2) has the
highest ROUGE result, but worse human evaluation
score than other methods.
To evaluate the length impact and to further make
263
All Sent. Top Sent.
Approach Word ratio (%) P(%) R(%) F(%) P(%) R(%) F(%)
Original extractive summary 100 7.58 66.06 12.99 29.98 34.29 31.83
Human compression 65.58 10.43 63.00 16.95 34.35 37.39 35.79
Markov (S1) 67.67 10.15 61.98 16.41 34.24 36.88 35.46
Markov (S2) 53.28 11.90 58.14 18.37 32.23 34.96 33.49
FP + IP 76.38 9.11 59.85 14.78 31.82 35.62 33.57
Table 2: Compression ratio of different systems and ROUGE-1 scores compared to human abstractive summaries.
the extractive summaries more like abstractive sum-
maries, we conduct an oracle experiment: we compute
the ROUGE score for each of the extractive summary
sentences (the original sentence or the compressed sen-
tence) against the abstract, and select the sentences
with the highest scores until the number of selected
words is about the same as that in the abstract.
6
The
ROUGE results using these selected top sentences are
shown in the right part of Table 2. There is some dif-
ference using all the sentences vs. the top sentences
regarding the ranking of different compression algo-
rithms (comparing the two blocks in Table 2).
From Table 2, we notice significant performance im-
provement when using the selected sentences to form a
summary. These results indicate that, it may be possi-
ble to convert extractive summaries to abstractive sum-
maries. On the other hand, this is an oracle result since
we compare the extractive summaries to the abstract for
sentence selection. In the real scenario, we will need
other methods to rank sentences. Moreover, the current
ROUGE score is not very high. This suggests that there
is a limit using extractive summarization and sentence
compression to form abstractive summaries, and that
sophisticated language generation is still needed.
4 Conclusion
In this paper, we attempt to bridge the gap between ex-
tractive and abstractive summaries by performing sen-
tence compression. Several compression approaches
are employed, including an integer programming based
framework, where we also introduced a filler phrase de-
tection module, the lexicalized Markov grammar-based
approach, as well as human compression. Results show
that, while sentence compression provides a promising
way of moving from extractive summaries toward ab-
stracts, there is also a potential limit along this direc-
tion. This study uses human annotated extractive sum-
maries. In our future work, we will evaluate using auto-
matic extractive summaries. Furthermore, we will ex-
plore the possibility of merging compressed extractive
sentences to generate more unified summaries.
References
A. Buist, W. Kraaij, and S. Raaijmakers. 2005. Automatic
summarization of meeting data: A feasibility study. In
Proc. of CLIN.
6
Thanks to Shasha Xie for generating these results.
E. Charniak and M. Johnson. 2001. Edit detection and pars-
ing for transcribed speech. In Proc. of NAACL.
J. Clarke and M. Lapata. 2008. Global inference for sentence
compression: An integer linear programming approach.
Journal of Artificial Intelligence Research, 31:399?429.
T. Cohn and M. Lapata. 2009. Sentence compression as tree
transduction. Journal of Artificial Intelligence Research.
M. Galley and K. McKeown. 2007. Lexicalized markov
grammars for sentence compression. In Proc. of
NAACL/HLT.
M. Galley. 2006. A skip-chain conditional random field
for ranking meeting utterances by importance. In Proc.
of EMNLP.
D. Gillick, K. Riedhammer, B. Favre, and D. Hakkani-Tur.
2009. A global optimization framework for meeting sum-
marization. In Proc. of ICASSP.
C. Hori, S. Furui, R. Malkin, H. Yu, and A. Waibel. 2003.
A statistical approach to automatic speech summarization.
Journal on Applied Signal Processing, 2003:128?139.
A. Janin, D. Baron, J. Edwards, D. Ellis, G. Gelbart, N. Mor-
gan, B. Peskin, T. Pfau, E. Shriberg, A. Stolcke, and
C. Wooters. 2003. The ICSI meeting corpus. In Proc.
of ICASSP.
K. Knight and D. Marcu. 2002. Summarization beyond
sentence extraction: A probabilistic approach to sentence
compression. Artificial Intelligence, 139:91?107.
C. Lin. 2004. Rouge: A package for automatic evaluation
of summaries. In Proc. of ACL Workshop on Text Summa-
rization Branches Out.
S. Maskey and J. Hirschberg. 2006. Summarizing speech
without text using hidden markov models. In Proc. of
HLT/NAACL.
G. Murray, S. Renals, and J. Carletta. 2005a. Extractive
summarization of meeting recordings. In Proc. of INTER-
SPEECH.
G. Murray, S. Renals, J. Carletta, and J. Moore. 2005b. Eval-
uating automatic summaries of meeting recordings. In
Proc. of ACL 2005 MTSE Workshop.
E. Shriberg, R. Dhillon, S. Bhagat, J. Ang, and H. Carvey.
2004. The ICSI meeting recorder dialog act (MRDA)
corpus. In Proc. of SIGdial Workshop on Discourse and
Dialogue.
J. Turner and E. Charniak. 2005. Supervised and unsuper-
vised learning for sentence compression. In Proc. of ACL.
S. Xie, Y. Liu, and H. Lin. 2008. Evaluating the effective-
ness of features and sampling in extractive meeting sum-
marization. In Proc. of IEEE Workshop on Spoken Lan-
guage Technology.
264
Proceedings of the Analyzing Conversations in Text and Speech (ACTS) Workshop at HLT-NAACL 2006, pages 8?14,
New York City, New York, June 2006. c?2006 Association for Computational Linguistics
Off-Topic Detection in Conversational Telephone Speech
Robin Stewart and Andrea Danyluk
Department of Computer Science
Williams College
Williamstown, MA 01267
{06rss 2, andrea}@cs.williams.edu
Yang Liu
Department of Computer Science
UT Dallas
Richardson, TX 75080
yangl@hlt.utdallas.edu
Abstract
In a context where information retrieval is
extended to spoken ?documents? includ-
ing conversations, it will be important to
provide users with the ability to seek in-
formational content, rather than socially
motivated small talk that appears in many
conversational sources. In this paper we
present a preliminary study aimed at auto-
matically identifying ?irrelevance? in the
domain of telephone conversations. We
apply a standard machine learning algo-
rithm to build a classifier that detects off-
topic sections with better-than-chance ac-
curacy and that begins to provide insight
into the relative importance of features for
identifying utterances as on topic or not.
1 Introduction
There is a growing need to index, search, summa-
rize and otherwise process the increasing amount of
available broadcast news, broadcast conversations,
meetings, class lectures, and telephone conversa-
tions. While it is clear that users have wide ranging
goals in the context of information retrieval, we as-
sume that some will seek only credible information
about a specific topic and will not be interested in the
socially-motivated utterances which appear through-
out most conversational sources. For these users,
a search for information about weather should not
return conversations containing small talk such as
?Nice weather we?ve been having.?
In this paper we investigate one approach for auto-
matically identifying ?irrelevance? in the domain of
telephone conversations. Our initial data consist of
conversations in which each utterance is labeled as
being on topic or not. We apply inductive classifier
learning algorithms to identify useful features and
build classifiers to automatically label utterances.
We begin in Section 2 by hypothesizing features
that might be useful for the identification of irrel-
evant regions, as indicated by research on the lin-
guistics of conversational speech and, in particular,
small talk. Next we present our data and discuss our
annotation methodology. We follow this with a de-
scription of the complete set of features and machine
learning algorithms investigated. Section 6 presents
our results, including a comparison of the learned
classifiers and an analysis of the relative utility of
various features.
2 Linguistics of Conversational Speech
Cheepen (Cheepen, 1988) posits that speakers have
two primary goals in conversation: interactional
goals in which interpersonal motives such as social
rank and trust are primary; and transactional goals
which focus on communicating useful information
or getting a job done. In a context where conversa-
tions are indexed and searched for information, we
assume in this paper that users will be interested
in the communicated information, rather than the
way in which participants interact. Therefore, we
assume that utterances with primarily transactional
purposes will be most important, while interactional
utterances can be ignored.
Greetings and partings are the most predictable
8
type of interactional speech. They consistently ap-
pear at the beginning and end of conversations and
follow a fairly formulaic pattern of content (Laver,
1981). Thus we hypothesize that: Utterances near
the beginning or end of conversations are less likely
to be relevant.
Cheepen also defines speech-in-action regions
to be segments of conversation that are related to
the present physical world or the activity of chat-
ting, e.g. ?What lovely weather.? or ?It is so nice
to see you.? Since these regions mainly involve
participants identifying their shared social situation,
they are not likely to contain transactional content.
Further, since speech-in-action segments are distin-
guished by their focus on the present, we hypoth-
esize that: Utterances with present tense verbs are
less likely to be relevant.
Finally, small talk that is not intended to demar-
cate social hierarchy tends to be abbreviated, e.g.
?Nice day? (Laver, 1981). From this we hypothe-
size that: Utterances lacking common helper words
such as ?it?, ?there?, and forms of ?to be? are less
likely to be relevant.
3 Related Work
Three areas of related work in natural language pro-
cessing have been particularly informative for our
research.
First, speech act theory states that with each ut-
terance, a conversant is committing an action, such
as questioning, critiquing, or stating a fact. This is
quite similar to the notion of transactional and inter-
actional goals. However, speech acts are generally
focused on the lower level of breaking apart utter-
ances and understanding their purpose, whereas we
are concerned here with a coarser-grained notion of
relevance. Work closer to ours is that of Bates et
al. (Bates et al, 2005), who define meeting acts for
recorded meetings. Of their tags, commentary is
most similar to our notion of relevance.
Second, there has been research on generating
small talk in order to establish rapport between an
automatic system and human user (Bickmore and
Cassell, 2000). Our work complements this by po-
tentially detecting off-topic speech from the human
user as an indication that the system should also re-
spond with interactional language.
Label Utterance
S 2: [LAUGH] Hi.
S 2: How nice to meet you.
S 1: It is nice to meet you too.
M 2: We have a wonderful topic.
M 1: Yeah.
M 1: It?s not too bad. [LAUGH]
T 2: Oh, I ? I am one hundred percent in
favor of, uh, computers in the classroom.
T 2: I think they?re a marvelous tool,
educational tool.
Table 1: A conversation fragment with annotations:
(S)mall Talk, (M)etaconversation, and On-(T)opic.
The two speakers are identified as ?1? and ?2?.
Third, off-topic detection can be viewed as a seg-
mentation of conversation into relevant and irrele-
vant parts. Thus our work has many similarities to
topic segmentation systems, which incorporate cue
words that indicate an abrupt change in topic (e.g.
?so anyway...?), as well as long term variations in
word occurrence statistics (Hearst, 1997; Reynar,
1999; Beeferman et al, 1999, e.g.). Our approach
uses previous and subsequent sentences to approxi-
mate these ideas, but might benefit from a more ex-
plicitly segmentation-based strategy.
4 Data
In our work we use human-transcribed conversa-
tions from the Fisher data (LDC, 2004). In each con-
versation, participants have been given a topic to dis-
cuss for ten minutes. Despite this, participants often
talk about subjects that are not at all related to the as-
signed topic. Therefore, a convenient way to define
irrelevance in conversations in this domain is seg-
ments which do not contribute to understanding the
assigned topic. This very natural definition makes
the domain a good one for initial study; however,
the idea can be readily extended to other domains.
For example, broadcast debates, class lectures, and
meetings usually have specific topics of discussion.
The primary transactional goal of participants in
the telephone conversations is to discuss the as-
signed topic. Since this goal directly involves the
act of discussion itself, it is not surprising that par-
ticipants often talk about the current conversation or
9
the choice of topic. There are enough such segments
that we assign them a special region type: Metacon-
versation. The purely irrelevant segments we call
Small Talk, and the remaining segments are defined
as On-Topic. We define utterances as segments of
speech that are delineated by periods and/or speaker
changes. An annotated excerpt is shown in Table 1.
For the experiments described in this paper, we
selected 20 conversations: 4 from each of the topics
?computers in education?, ?bioterrorism?, ?terror-
ism?, ?pets?, and ?censorship?. These topics were
chosen randomly from the 40 topics in the Fisher
corpus, with the constraint that we wanted to include
topics that could be a part of normal small talk (such
as ?pets?) as well as topics which seem farther re-
moved from small talk (such as ?censorship?).
Our selected data set consists of slightly more
than 5,000 utterances. We had 2-3 human annota-
tors label the utterances in each conversation, choos-
ing from the 3 labels Metaconversation, Small Talk,
and On-Topic. On average, pairs of annotators
agreed with each other on 86% of utterances. The
main source of annotator disagreement was between
Small Talk and On-Topic regions; in most cases this
resulted from differences in opinion of when exactly
the conversation had drifted too far from the topic to
be relevant.
For the 14% of utterances with mismatched la-
bels, we chose the label that would be ?safest? in the
information retrieval context where small talk might
get discarded. If any of the annotators thought a
given utterance was On-Topic, we kept it On-Topic.
If there was a disagreement between Metaconver-
sation and Small Talk, we used Metaconversation.
Thus, a Small Talk label was only placed if all anno-
tators agreed on it.
5 Experimental Setup
5.1 Features
As indicated in Section 1, we apply machine learn-
ing algorithms to utterances extracted from tele-
phone conversations in order to learn classifiers for
Small Talk, Metaconversation, and On-Topic. We
represent utterances as feature vectors, basing our
selection of features on both linguistic insights and
earlier text classification work. As described in Sec-
tion 2, work on the linguistics of conversational
Small Talk Metaconv. On-Topic
hi topic ,
. i ?
?s it you
yeah this that
? dollars the
hello so and
oh is know
?m what a
in was wouldn
my about to
but talk like
name for his
how me they
we okay of
texas do ?t
there phone he
well ah uh
from times um
are really put
here one just
Table 2: The top 20 tokens for distinguishing each
category, as ranked by the feature quality measure
(Lewis and Gale, 1994).
speech (Cheepen, 1988; Laver, 1981) implies that
the following features might be indicative of small
talk: (1) position in the conversation, (2) the use of
present-tense verbs, and (3) a lack of common helper
words such as ?it?, ?there?, and forms of ?to be?.
To model the effect of proximity to the beginning
of the conversation, we attach to each utterance a
feature that describes its approximate position in the
conversation. We do not include a feature for prox-
imity to the end of the conversation because our tran-
scriptions include only the first ten minutes of each
recorded conversation.
In order to include features describing verb tense,
we use Brill?s part-of-speech tagger (Brill, 1992) .
Each part of speech (POS) is taken to be a feature,
whose value is a count of the number of occurrences
in the given utterance.
To account for the words, we use a bag of words
model with counts for each word. We normalize
words from the human transcripts by converting ev-
erything to lower case and tokenizing contractions
10
Features Values
n word tokens for each word, # occurrences
standard POS tags as in Penn Treebank for each tag, # occurrences
line number in conversation 0-4, 5-9, 10-19, 20-49, >49
utterance type statement, question, fragment
utterance length (number of words) 1, 2, ..., 20, >20
number of laughs laugh count
n word tokens in previous 5 utterances for each word, total # occurrences in 5 previous
tags from POS tagger, previous 5 for each tag, total # occurrences in 5 previous
number of words, previous 5 total from 5 previous
number of laughs, previous 5 total from 5 previous
n word tokens, subsequent 5 utterances for each word, total # occ in 5 subsequent
tags from POS tagger, subsequent 5 for each tag, total # occurrences in 5 subsequent
number of words, subsequent 5 total from 5 subsequent
number of laughs, subsequent 5 total from 5 subsequent
Table 3: Summary of features that describe each utterance.
and punctuation. We rank the utility of words ac-
cording to the feature quality measure presented in
(Lewis and Gale, 1994) because it was devised for
the task of classifying similarly short fragments of
text (news headlines), rather than long documents.
We then consider the top n tokens as features, vary-
ing the number in different experiments. Table 2
shows the most useful tokens for distinguishing be-
tween the three categories according to this metric.
Additionally, we include as features the utterance
type (statement, question, or fragment), number of
words in the utterance, and number of laughs in the
utterance.
Because utterances are long enough to classify in-
dividually but too short to classify reliably, we not
only consider features of the current utterance, but
also those of previous and subsequent utterances.
More specifically, summed features are calculated
for the five preceding utterances and for the five sub-
sequent utterances. The number five was chosen em-
pirically.
It is important to note that there is some overlap
in features. For instance, the token ??? can be ex-
tracted as one of the n word tokens by Lewis and
Gale?s feature quality measure; it is also tagged by
the POS tagger; and it is indicative of the utterance
type, which is encoded as a separate feature as well.
However, redundant features do not make up a sig-
nificant percentage of the overall feature set.
Finally, we note that the conversation topic is not
taken to be a feature, as we cannot assume that con-
versations in general will have such labels. The
complete list of features, along with their possible
values, is summarized in Table 3.
5.2 Experiments
We applied several classifier learning algorithms to
our data: Naive Bayes, Support Vector Machines
(SVMs), 1-nearest neighbor, and the C4.5 decision
tree learning algorithm. We used the implementa-
tions in the Weka package of machine learning al-
gorithms (Witten and Frank, 2005), running the al-
gorithms with default settings. In each case, we per-
formed 4-fold cross-validation, training on sets con-
sisting of three of the conversations in each topic
(15 conversations total) and testing on sets of the re-
maining 1 from each topic (5 total). Average train-
ing set size was approximately 3800 utterances, of
which about 700 were Small Talk and 350 Metacon-
versation. The average test set size was 1270.
6 Results
6.1 Performance of a Learned Classifier
We evaluated the results of our experiments ac-
cording to three criteria: accuracy, error cost, and
plausibility of the annotations produced. In all
11
Algorithm % Accuracy Cohen?s Kappa
SVM 76.6 0.44
C4.5 68.8 0.26
k-NN 64.1 0.20
Naive Bayes 58.9 0.27
Table 4: Classification accuracy and Cohen?s Kappa
statistic for each of the machine learning algorithms
we tried, using all features at the 100-words level.
25 50 75 100 125 150 175
74
76
78
80
82
84
number of words used as features
0 200
acc
ura
cy 
(%)
72
86 Inter-annotator agreement
Baseline
Line numbers only
all features
Part of speech tags only
Figure 1: Classification results using SVMs with
varying numbers of words.
cases our best results were obtained with the SVM.
When evaluated on accuracy, the SVM models were
the only ones that exceeded a baseline accuracy of
72.8%, which is the average percentage of On-Topic
utterances in our data set. Table 4 displays the nu-
merical results using each of the machine learning
algorithms.
Figure 1 shows the average accuracy obtained
with an SVM classifier using all features described
in Section 5.1 except part-of-speech features (for
reasons discussed below), and varying the number
of words considered. While the best results were ob-
tained at the 100-words level, all classifiers demon-
strated significant improvement in accuracy over the
baseline. The average standard deviation over the 4
cross-validation runs of the results shown is 6 per-
centage points.
From a practical perspective, accuracy alone is
S M T <? classified as
55% 7% 38% Small Talk
21% 37% 42% Metaconv.
8% 3% 89% On Topic
Table 5: Confusion matrix for 100-word SVM clas-
sifier.
not an appropriate metric for evaluating our re-
sults. If the goal is to eliminate Small Talk regions
from conversations, mislabeling On-Topic regions
as Small Talk potentially results in the elimination
of useful material. Table 5 shows a confusion ma-
trix for an SVM classifier trained on a data set at the
100-word level. We can see that the classifier is con-
servative, identifying 55% of the Small Talk, but in-
correctly labeling On-Topic utterances as Small Talk
only 8% of the time.
Finally, we analyzed (by hand) the test data anno-
tated by the classifiers. We found that, in general,
the SVM classifiers annotated the conversations in a
manner similar to the human annotators, transition-
ing from one label to another relatively infrequently
as illustrated in Table 1. This is in contrast to the
1-nearest neighbor classifiers, which tended to an-
notate in a far more ?jumpy? style.
6.2 Relative Utility of Features
Several of the features we used to describe our
training and test examples were selected due to the
claims of researchers such as Laver and Cheepen.
We were interested in determining the relative con-
tributions of these various linguistically-motivated
features to our learned classifiers. Figure 1 and Table
6 report some of our findings. Using proximity to the
beginning of the conversation (?line numbers?) as a
sole feature, the SVM classifier achieved an accu-
racy of 75.6%. This clearly verifies the hypothesis
that utterances near the beginning of the conversa-
tion have different properties than those that follow.
On the contrary, when we used only POS tags
to train the SVM classifier, it achieved an accuracy
that falls exactly at the baseline. Moreover, remov-
ing POS features from the SVM classifier improved
results (Table 6). This may indicate that detect-
ing off-topic categories will require focusing on the
words rather than the grammar of utterances. On
12
Condition Accuracy Kappa
All features 76.6 0.44
No word features 75.0 0.19
No line numbers 76.9 0.44
No POS features 77.8 0.46
No utterance type, length, 76.9 0.45
or # laughs
No previous/next info 76.3 0.21
Only word features 77.9 0.46
Only line numbers 75.6 0.16
Only POS features 72.8 0.00
Only utterance type, length, 74.1 0.09
and # laughs
Table 6: Percent accuracy and Cohen?s Kappa statis-
tic for the SVM at the 100-words level when features
were left out or put in individually.
the other hand, part of speech information is im-
plicit in the words (for example, an occurrence of
?are? also indicates a present tense verb), so perhaps
labeling POS tags does not add any new informa-
tion. It is also possible that some other detection
approach and/or richer syntactic information (such
as parse trees) would be beneficial.
Finally, the words with the highest feature qual-
ity measure (Table 2) clearly refute most of the third
linguistic prediction. Helper words like ?it?, ?there?,
and ?the? appear roughly evenly in each region type.
Moreover, all of the verbs in the top 20 Small Talk
list are forms of ?to be? (some of them contracted
as in ?I?m?), while no ?to be? words appear in the
list for On-Topic. This is further evidence that dif-
ferentiating off-topic speech depends deeply on the
meaning of the words rather than on some more eas-
ily extracted feature.
7 Future Work
There are many ways in which we plan to expand
upon this preliminary study. We are currently in the
process of annotating more data and including ad-
ditional conversation topics. Other future work in-
cludes:
? applying topic segmentation approaches to our
data and comparing the results to those we have
obtained so far;
? investigating alternate approaches for detecting
Small Talk regions, such as smoothing with a
Hidden Markov Model;
? using semi-supervised and active learning tech-
niques to better utilize the large amount of un-
labeled data;
? running the experiments with automatically
generated (speech recognized) transcriptions,
rather than the human-generated transcriptions
that we have used to date. Our expectation is
that such transcripts will contain more noise
and thus pose new challenges;
? including prosodic information in the feature
set.
Acknowledgements
The authors would like to thank Mary Harper, Brian
Roark, Jeremy Kahn, Rebecca Bates, and Joe Cruz
for providing invaluable advice and data. We would
also like to thank the student volunteers at Williams
who helped annotate the conversation transcripts,
as well as the 2005 Johns Hopkins CLSP summer
workshop, where this research idea was formulated.
References
Rebecca Bates, Patrick Menning, Elizabeth Willingham,
and Chad Kuyper. 2005. Meeting Acts: A Labeling
System for Group Interaction in Meetings. August.
Doug Beeferman, Adam Berger, and John Lafferty.
1999. Statistical Models for Text Segmentation. Ma-
chine Learning.
Timothy Bickmore and Justine Cassell. 2000. How
about this weather?: Social Dialogue with Embodied
Conversational Agents. AAAI Fall Symposium on So-
cially Intelligent Agents.
Eric Brill. 1992. A simple rule-based part of speech tag-
ger. Proc. of the Third Conference on Applied NLP.
Christine Cheepen. 1988. The Predictability of Informal
Conversation. Pinter Publishers, London.
Marti A. Hearst. 1997. TextTiling: Segmenting Text into
Multiparagraph Subtopics Passages. Computational
Linguistics, 23(1):33?64.
John Laver, 1981. Conversational routine, chapter Lin-
guistic routines and politeness in greeting and parting,
pages 289?304. Mouton, The Hague.
13
LDC. 2004. Fisher english training speech part 1, tran-
scripts. http://www.ldc.upenn.edu/Catalog/Catalog
Entry.jsp?catalogId=LDC2004T19.
David D. Lewis and William A. Gale. 1994. A sequential
algorithm for training text classifiers. Proc. of SIGIR.
Jeffrey C. Reynar. 1999. Statistical models for topic seg-
mentation. Proceedings of the 37th Annual Meeting
of the ACL.
Ian H. Witten and Eibe Frank. 2005. Data Mining: Prac-
tical Machine Learning Tools and Techniques, Second
Edition. Morgan Kaufmann, San Francisco.
14
Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 80?83,
Columbus, June 2008. c?2008 Association for Computational Linguistics
What Are Meeting Summaries? An Analysis of Human Extractive
Summaries in Meeting Corpus
Fei Liu, Yang Liu
Erik Jonsson School of Engineering and Computer Science
The University of Texas at Dallas
Richardson, TX, USA
{feiliu,yangl}@hlt.utdallas.edu
Abstract
Significant research efforts have been devoted to
speech summarization, including automatic ap-
proaches and evaluation metrics. However, a fun-
damental problem about what summaries are for the
speech data and whether humans agree with each
other remains unclear. This paper performs an anal-
ysis of human annotated extractive summaries us-
ing the ICSI meeting corpus with an aim to examine
their consistency and the factors impacting human
agreement. In addition to using Kappa statistics and
ROUGE scores, we also proposed a sentence dis-
tance score and divergence distance as a quantitative
measure. This study is expected to help better define
the speech summarization problem.
1 Introduction
With the fast development of recording and storage tech-
niques in recent years, speech summarization has re-
ceived more attention. A variety of approaches have
been investigated for speech summarization, for exam-
ple, maximum entropy, conditional random fields, latent
semantic analysis, support vector machines, maximum
marginal relevance (Maskey and Hirschberg, 2003; Hori
et al, 2003; Buist et al, 2005; Galley, 2006; Murray et
al., 2005; Zhang et al, 2007; Xie and Liu, 2008). These
studies used different domains, such as broadcast news,
lectures, and meetings. In these approaches, different in-
formation sources have been examined from both text and
speech related features (e.g., prosody, speaker activity,
turn-taking, discourse).
How to evaluate speech summaries has also been stud-
ied recently, but so far there is no consensus on eval-
uation yet. Often the goal in evaluation is to develop
an automatic metric to have a high correlation with hu-
man evaluation scores. Different methods have been used
in the above summarization research to compare system
generated summaries with human annotation, such as F-
measure, ROUGE, Pyramid, sumACCY (Lin and Hovy,
2003; Nenkova and Passonneau, 2004; Hori et al, 2003).
Typically multiple reference human summaries are used
in evaluation in order to account for the inconsistency
among human annotations.
While there have been efforts on speech summariza-
tion approaches and evaluation, some fundamental prob-
lems are still unclear. For example, what are speech sum-
maries? Do humans agree with each other on summary
extraction? In this paper, we focus on the meeting do-
main, one of the most challenging speech genre, to an-
alyze human summary annotation. Meetings often have
several participants. Its speech is spontaneous, contains
disfluencies, and lacks structure. These all post new chal-
lenges to the consensus of human extracted summaries.
Our goal in this study is to investigate the variation of
human extractive summaries, and help to better under-
stand the gold standard reference summaries for meet-
ing summarization. This paper aims to answer two key
questions: (1) How much variation is there in human ex-
tractive meeting summaries? (2) What are the factors
that may impact interannotator agreement? We use three
different metrics to evaluate the variation among human
summaries, including Kappa statistic, ROUGE score, and
a new proposed divergence distance score to reflect the
coherence and quality of an annotation.
2 Corpus Description
We use the ICSI meeting corpus (Janin et al, 2003) which
contains 75 naturally-occurred meetings, each about an
hour long. All of them have been transcribed and anno-
tated with dialog acts (DA) (Shriberg et al, 2004), top-
ics, and abstractive and extractive summaries in the AMI
project (Murray et al, 2005).
We selected 27 meetings from this corpus. Three anno-
tators (undergraduate students) were recruited to extract
summary sentences on a topic basis using the topic seg-
ments from the AMI annotation. Each sentence corre-
sponds to one DA annotated in the corpus. The annota-
tors were told to use their own judgment to pick summary
sentences that are informative and can preserve discus-
sion flow. The recommended percentages for the selected
summary sentences and words were set to 8.0% and
16.0% respectively. Human subjects were provided with
both the meeting audio files and an annotation Graphi-
80
cal User Interface, from which they can browse the man-
ual transcripts and see the percentage of the currently se-
lected summary sentences and words.
We refer to the above 27 meetings Data set I in this
paper. In addition, some of our studies are performed
based on the 6 meeting used in (Murray et al, 2005),
for which we have human annotated summaries using 3
different guidelines:
? Data set II: summary annotated on a topic basis. This is
a subset of the 27 annotated meetings above.
? Data set III: annotation is done for the entire meeting
without topic segments.
? Data set IV: the extractive summaries are from the AMI
annotation (Murray et al, 2005).
3 Analysis Results
3.1 Kappa Statistic
Kappa coefficient (Carletta, 1996) is commonly used
as a standard to reflect inter-annotator agreement. Ta-
ble 1 shows the average Kappa results, calculated for
each meeting using the data sets described in Section 2.
Compared to Kappa score on text summarization, which
is reported to be 0.38 by (Mani et al, 2002) on a set
of TREC documents, the inter-annotator agreement on
meeting corpus is lower. This is likely due to the dif-
ference between the meeting style and written text.
Data Set I II III IV
Avg-Kappa 0.261 0.245 0.335 0.290
Table 1: Average Kappa scores on different data sets.
There are several other observations from Table 1.
First, comparing the results for Data Set (II) and (III),
both containing six meetings, the agreement is higher
for Data Set (III). Originally, we expected that by di-
viding the transcript into several topics, human subjects
can focus better on each topic discussed during the meet-
ing. However, the result does not support this hypoth-
esis. Moreover, the Kappa result of Data Set (III) also
outperforms that of Data Set (IV). The latter data set is
from the AMI annotation, where they utilized a different
annotation scheme: the annotators were asked to extract
dialog acts that are highly relevant to the given abstrac-
tive meeting summary. Contrary to our expectation, the
Kappa score in this data set is still lower than that of Data
Set (III), which used a direct sentence extraction scheme
on the whole transcript. This suggests that even using
the abstracts as a guidance, people still have a high varia-
tion in extracting summary sentences. We also calculated
the pairwise Kappa score between annotations in differ-
ent data sets. The inter-group Kappa score is much lower
than those of the intragroup agreement, most likely due
to the different annotation specifications used in the two
different data sets.
3.2 Impacting Factors
We further analyze inter-annotator agreement with re-
spect to two factors: topic length and meeting partic-
ipants. All of the following experiments are based on
Data Set (I) with 27 meetings.
We computed Kappa statistic for each topic instead of
the entire meeting. The distribution of Kappa score with
respect to the topic length (measured using the number of
DAs) is shown in Figure 1. When the topic length is less
than 100, Kappa scores vary greatly, from -0.065 to 1.
Among the entire range of different topic lengths, there
seems no obvious relationship between the Kappa score
and the topic length (a regression from the data points
does not suggest a fit with an interpretable trend).
-0.2
0
0.2
0.4
0.6
0.8
1
1.2
0 200 400 600 800 1000 1200 1400
Topic length
-0.2
0
0.2
0.4
0.6
0.8
1
1.2
0 200 400 600 800 1000 1200 1400
Topic length
K
ap
pa
 s
co
re
Figure 1: Relationship between Kappa score and topic length.
Using the same Kappa score for each topic, we also in-
vestigated its relationship with the number of speakers in
that topic. Here we focused on the topic segments longer
than a threshold (with more than 60 DAs) as there seems
to be a wide range of Kappa results when the topic is
short (in Figure 1). Table 2 shows the average Kappa
score for these long topics, using the number of speak-
ers in the topic as the variable. We notice that when the
speaker number varies from 4 to 7, kappa scores grad-
ually decrease with the increasing of speaker numbers.
This phenomenon is consistent with our intuition. Gener-
ally the more participants are involved in a conversation,
the more discussions can take place. Human annotators
feel more ambiguity in selecting summary sentences for
the discussion part. The pattern does not hold for other
speaker numbers, namely, 2, 3, and 8. This might be due
to a lack of enough data points, and we will further ana-
lyze this in the future research.
# of speakers # of topics Avg Kappa score
2 2 0.204
3 6 0.182
4 26 0.29
5 26 0.249
6 33 0.226
7 19 0.221
8 7 0.3
Table 2: Average Kappa score with respect to the number of
speakers after removing short topics.
3.3 ROUGE Score
ROUGE (Lin and Hovy, 2003) has been adopted as
a standard evaluation metric in various summarization
tasks. It is computed based on the n-gram overlap be-
tween a summary and a set of reference summaries.
Though the Kappa statistics can measure human agree-
ment on sentence selection, it does not account for the
fact that different annotators choose different sentences
81
that are similar in content. ROUGE measures the word
match and thus can compensate this problem of Kappa.
Table 3 shows the ROUGE-2 and ROUGE-SU4 F-
measure results. For each annotator, we computed
ROUGE scores using other annotators? summaries as ref-
erences. For Data Set (I), we present results for each an-
notator, since one of our goals is to evaluate the qual-
ity of different annotator?s summary annotation. The low
ROUGE scores suggest the large variation among human
annotations. We can see from the table that annotator
1 has the lowest ROUGE score and thus lowest agree-
ment with the other two annotators in Data Set (I). The
ROUGE score for Data Set (III) is higher than the others.
This is consistent with the result using Kappa statistic:
the more sentences two summaries have in common, the
more overlapped n-grams they tend to share.
ROUGE-2 ROUGE-SU4
Annotator 1 0.407 0.457
data (I) Annotator 2 0.421 0.471
Annotator 3 0.433 0.483
data (III) 2 annotators 0.532 0.564
data (IV) 3 annotators 0.447 0.484
Table 3: ROUGE F-measure scores for different data sets.
3.4 Sentence Distance and Divergence Scores
From the annotation, we notice that the summary sen-
tences are not uniformly distributed in the transcript, but
rather with a clustering or coherence property. However,
neither Kappa coefficient nor ROUGE score can rep-
resent such clustering tendency of meeting summaries.
This paper attempts to develop an evaluation metric to
measure this property among different human annotators.
For a sentence i selected by one annotator, we define a
distance score di to measure its minimal distance to sum-
mary sentences selected by other annotators (distance be-
tween two sentences is represented using the difference
of their sentence indexes). di is 0 if more than one anno-
tator have extracted the same sentence as summary sen-
tence. Using the annotated summaries for the 27 meet-
ings in Data Set (I), we computed the sentence distance
scores for each annotator. Figure 2 shows the distribution
of the distance score for the 3 annotators. We can see
that the distance score distributions for the three annota-
tors differ. Intuitively, small distance scores mean better
coherence and more consistency with other annotators?
results. We thus propose a mechanism to quantify each
annotator?s summary annotation by using a random vari-
able (RV) to represent an annotator?s sentence distance
scores.
When all the annotators agree with each other, the RV
d will take a value of 0 with probability 1. In general,
when the annotators select sentences close to each other,
the RV d will have small values with high probabilities.
Therefore we create a probability distribution Q for the
ideal situation where the annotators have high agreement,
and use this to quantify the quality of each annotation. Q
is defined as:
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
0 1 2 3 4 5 6 7 8 9 10 >10
Distance Score
Pe
rc
en
ta
ge
Annotator 1 Annotator 2 Annotator 3
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
0 1 2 3 4 5 6 7 8 9 10 >10
Distance Score
Pe
rc
en
ta
ge
Annotator 1 Annotator 2 Annotator 3
Figure 2: Percentage distribution of the summary sentence dis-
tance scores for the 3 annotators in Data Set (I).
Q(i) =
?
?
?
?
?
(dmax ? i + 1) ? q i 6= 0
1 ? ?dmaxi=1 Q(i)
= 1 ? dmax?(dmax+1)2 ? q i = 0
where dmax denotes the maximum distance score based
on the selected summary sentences from all the annota-
tors. We assign linearly decreasing probabilities Q(i) for
different distance values i (i > 0) in order to give more
credit to sentences with small distance scores. The rest
of the probability mass is given to Q(0). The parame-
ter q is small, such that the probability distribution Q can
approximate the ideal situation.
For each annotator, the probability distribution P is de-
fined as:
P (i) =
{
wi?fi
P
i wi?fi
i ? Dp
0 otherwise
where Dp is the set of the possible distance values for this
annotator, fi is the frequency for a distance score i, and
wi is the weight assigned to that distance (wi is i when
i 6= 0; w0 is p). We use parameter p to vary the weighting
scale for the distance scores in order to penalize more for
the large distance values.
Using the distribution P for each annotator and the
ideal distribution Q, we compute their KL-divergence,
called the Divergence Distance score (DD-score):
DD =
?
i
P (i) log P (i)Q(i)
We expect that the smaller the score is, the better the sum-
mary is. In the extreme case, if an annotator?s DD-score
is equal to 0, it means that all of this annotator?s extracted
sentences are selected by other annotators.
Figure 3 shows the DD-score for each annotator cal-
culated using Data Set (I), with varying q parameters.
Our experiments showed that the scale parameter p in the
annotator?s probability distribution only affects the abso-
lute value of the DD-score for the annotators, but does
not change the ranking of each annotator. Therefore we
simply set p = 10 when reporting DD-scores. Figure 3
shows that different weight scale q does not impact the
ranking of the annotators either. We observe in Figure 3,
annotator 1 has the highest DD score to the desirable dis-
tribution. We found this is consistent with the cumulative
distance score obtained from the distance score distribu-
tion, where annotator 1 has the least cumulative frequen-
cies for all the distance values greater than 0. This is
82
also consistent with the ROUGE scores, where annotator
1 has the lowest ROUGE score. These suggest that the
DD-score can be used to quantify the consistency of an
annotator with others.
0
5
10
15
20
25
7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
-log(q)
 D
iv
er
ge
nc
e 
D
is
ta
nc
e
Sc
or
e
Annotator 1
Annotator 2
Annotator 3
0
5
10
15
20
25
7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
-log(q)
 D
iv
er
ge
nc
e
Di
st
an
ce
 S
co
re
Annotator 1
Annotator 2
Annotator 3
Figure 3: Divergence distance score when varying parameter q
in the ideal distribution Q.
We also investigated using the sentence distance scores
to improve the human annotation quality. Our hypothe-
sis is that those selected summary sentences with high
distance scores do not contain crucial information of
the meeting content and thus can be removed from the
reference summary. To verify this, for each annota-
tor, we removed the summary sentences with distance
scores greater than some threshold, and then computed
the ROUGE score for the newly generated summary by
comparing to other two summary annotations that are
kept unchanged. The ROUGE-2 scores when varying the
threshold is shown in Figure 4. No threshold in the X-
axis means that no sentence is taken out from the human
summary. We can see from the figure that the removal
of sentences with high distance scores can result in even
better F-measure scores. This suggests that we can delete
the incoherent human selected sentences while maintain-
ing the content information in the summary.
0.38
0.39
0.4
0.41
0.42
0.43
0.44
0.45
0.46
3 4 5 6 7 8 no threshold
Threshold of Distance Score
F-
sc
or
e
Annotator 1 Annotator 2 Annotator 3
0.38
0.39
0.4
0.41
0.42
0.43
0.44
0.45
0.46
3 4 5 6 7 8 no threshold
Threshold of Distance Score
F-
sc
or
e
Annotator 1 Annotator 2 Annotator 3
Figure 4: ROUGE-2 score after removing summary sentences
with a distance score greater than a threshold.
4 Conclusion
In this paper we conducted an analysis about human an-
notated extractive summaries using a subset of the ICSI
meeting corpus. Different measurements have been used
to examine interannotator agreement, including Kappa
coefficient, which requires exact same sentence selection;
ROUGE, which measures the content similarity using n-
gram match; and our proposed sentence distance scores
and divergence, which evaluate the annotation consis-
tency based on the sentence position. We find that the
topic length does not have an impact on the human agree-
ment using Kappa, but the number of speakers seems to
be correlated with the agreement. The ROUGE score and
the divergence distance scores show some consistency
in terms of evaluating human annotation agreement. In
addition, using the sentence distance score, we demon-
strated that we can remove some poorly chosen sentences
from the summary to improve human annotation agree-
ment and preserve the information in the summary. In
our future work, we will explore other factors, such as
summary length, and the speaker information for the se-
lect summaries. We will also use a bigger data set for a
more reliable conclusion.
Acknowledgments
The authors thank University of Edinburgh for sharing the an-
notation on the ICSI meeting corpus. This research is supported
by NSF award IIS-0714132. The views in this paper are those
of the authors and do not represent the funding agencies.
References
A. H. Buist, W. Kraaij, and S. Raaijmakers. 2005. Automatic
summarization of meeting data: A feasibility study. In Proc.
of the 15th CLIN conference.
J. Carletta. 1996. Assessing agreement on classification tasks:
the kappa statistic. Computational Linguistics, 22(2):249?
254.
M. Galley. 2006. A skip-chain conditional random field
for ranking meeting utterances by importance. In Proc. of
EMNLP, pages 364?372.
C. Hori, T. Hori, and S. Furui. 2003. Evaluation methods for
automatic speech summarization. In Proc. of Eurospeech,
pages 2825?2828.
A. Janin, D. Baron, J. Edwards, D. Ellis, D. Gelbart, N. Morgan,
B. Peskin, T. Pfau, E. Shriberg, A. Stolcke, and C. Wooters.
2003. The ICSI meeting corpus. In Proc. of ICASSP.
C. Y. Lin and E. Hovy. 2003. Automatic evaluation of sum-
maries using n-gram co-occurrence statistics. In Proc. of
HLT?NAACL.
I. Mani, G. Klein, D. House, L. Hirschman, T. Firmin, and
B. Sundheim. 2002. Summac: a text summarization eval-
uation. Natural Language Engineering, 8:43?68.
S. Maskey and J. Hirschberg. 2003. Automatic summariza-
tion of broadcast news using structural features. In Proc. of
EUROSPEECH, pages 1173?1176.
G. Murray, S. Renals, J. Carletta, and J. Moore. 2005. Evalu-
ating automatic summaries of meeting recordings. In Proc.
of the ACL Workshop on Intrinsic and Extrinsic Evaluation
Measures for Machine Translation.
A. Nenkova and R. Passonneau. 2004. Evaluating content se-
lection in summarization: The pyramid method. In Proc. of
HLT-NAACL.
E. Shriberg, R. Dhillon, S. Bhagat, J. Ang, and H. Carvey. 2004.
The ICSI meeting recorder dialog act (MRDA) corpus. In
Proc. of 5th SIGDial Workshop, pages 97?100.
S. Xie and Y. Liu. 2008. Using corpus and knowledge-based
similarity measure in maximum marginal relevance for meet-
ing summarization. In Proc. of ICASSP.
J. Zhang, H. Chan, P. Fung, and L. Cao. 2007. A compara-
tive study on speech summarization of broadcast news and
lecture speech. In Proc. of Interspeech.
83
BioNLP 2008: Current Trends in Biomedical Natural Language Processing, pages 116?117,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Using Language Models to Identify Language Impairment in
Spanish-English Bilingual Children
Thamar Solorio
Department of Computer Science
The University of Texas at Dallas
tsolorio@hlt.utdallas.edu
Yang Liu
Department of Computer Science
The University of Texas at Dallas
yangl@hlt.utdallas.edu
1 Introduction
Children diagnosed with Specific Language Impair-
ment (SLI) experience a delay in acquisition of cer-
tain language skills, with no evidence of hearing im-
pediments, or other cognitive, behavioral, or overt
neurological problems (Leonard, 1991; Paradis et
al., 2005/6). Standardized tests, such as the Test for
Early Grammatical Impairment, have shown to have
great predictive value for assessing English speaking
monolingual children. Diagnosing bilingual chil-
dren with SLI is far more complicated due to the
following factors: lack of standardized tests, lack of
bilingual clinicians, and more importantly, the lack
of a deep understanding of bilingualism and its im-
plications on language disorders. In addition, bilin-
gual children often exhibit code-switching patterns
that will make the assessment task even more chal-
lenging. In this paper, we present preliminary re-
sults from using language models to help discrim-
inating bilingual children with SLI from Typically-
Developing (TD) bilingual children.
2 Our Approach
We believe that statistical inference can assist in
the problem of accurately discriminating language
patterns indicative of SLI. In this work, we use
Language Models (LMs) for this task since they are
a powerful statistical measure of language usage
and have been successfully used to solve a variety
of NLP problems, such as text classification, speech
recognition, hand-writing recognition, augmenta-
tive communication for the disabled, and spelling
error detection (Manning and Schu?tze, 1999).
LMs estimate the probability of a word sequence
W = ?w1, ...wk? as follows (using the chain rule):
p(W ) = ?ki=1 p(wi|w1, . . . , wi?1)
which can be approximated using an N-gram as:
p(W ) ? ?ki=1 p(wi|wi?N+1, wi?N+2, ..., wi?1)
Since in our problem we are interested in differ-
entiating syntactic patterns, we will train the LMs
on Part-of-Speech (POS) patterns instead of words.
Using a 3-gram we have:
p(T ) = ?ki=1 p(ti|ti?2, ti?1)
where T = ?t1, t2, ..., tk? is the sequence of POS
tags assigned to the sequence of words W .
The intuition is that the language patterning of an
SLI child will differ from those of TD children at
two different levels: one is at the syntactic level,
and the second one is at the interaction between
both languages in patterns such as code-switching.
Given that the tagset for each language is differ-
ent, by using the POS tags we will incorporate into
the model the syntactic structure together with the
switch points across languages.
We train two LMs with the POS sequences: MT ,
with data from the TD children and MI , with data
from the SLI bilingual children. Once both LMs are
trained, then we can use them to make predictions
over new speech samples of bilingual children. To
determine whether an unobserved speech sample is
likely to belong to a child suffering from SLI, we
will measure the perplexity of the two LMs over the
POS patterns of this new speech sample. We make
the final decision using a threshold:
d(s) =
{
SLI if (PPT (s) ? PPI(s)) > 0
TD otherwise
116
where PPT (s) is the perplexity of the model MT
over the sample s, and PPI(s) is the perplexity of
the model MI over the same sample s. In other
words, if the perplexity of the LM trained on syn-
tactic patterns of children with SLI is smaller than
that of the LM trained on POS patterns of TD chil-
dren, then we will predict that the sample belongs to
a child with SLI.
In a related work, (Roark et al, 2007) explored
the use of cross entropy of LMs trained on POS tags
as a measure of syntactic complexity. Their results
were inconsistent across language tasks, which may
be due to the meaning attached to cross entropy in
this setting. Unlikely patterns are a deviation from
what is expected; they are not necessarily complex
or syntactically rich.
3 Preliminary Results
We empirically evaluated our approach using tran-
scripts that were made available by a speech pathol-
ogist in our team. The TD samples were comprised
of 5 males and 4 females between 48 and 72 months
old. The children were identified as being bilingual
by their parents, and according to parental report,
these children live in homes where Spanish is spo-
ken an average of 46.3% of the time. Language
samples of SLI bilinguals were collected from chil-
dren being served in the Speech and Hearing Clinic
at UTEP. The samples are from two females aged
53 and 111 months. The clients were diagnosed
with language impairment after diagnostic evalua-
tions which were conducted in Spanish. The tran-
scriptions were POS tagged with the bilingual tagger
developed by (Solorio et al, 2008).
Table 1 shows the preliminary results using cross
validation. With the decision threshold outlined
above, out of the 9 TD children, the models were
able to discriminate 7 as TD; from the 2 SLI chil-
dren both were correctly identified as SLI. Although
the results presented above are not conclusive due to
the very small size corpora at hand, they look very
promising. Stronger conclusions can be drawn once
we collect more data.
4 Final Remarks
This paper presents very promising preliminary re-
sults on the use of LMs for discriminating patterns
Table 1: Perplexity and final output of the LMs for the
discrimination of SLI and TD.
Sample PPT (s) PPI(s) d(s)
TD1 14.73 23.12 TD
TD2 11.37 16.17 TD
TD3 18.35 36.58 TD
TD4 30.23 22.27 SLI
TD5 9.42 15.50 TD
TD6 17.37 36.75 TD
TD7 20.32 33.19 TD
TD8 16.40 24.47 TD
TD9 24.35 23.71 SLI
SLI1 20.21 19.10 SLI
SLI2 19.70 12.43 SLI
average TD 18.06 25.75 TD
average SLI 19.95 15.76 SLI
indicative of SLI in Spanish-English bilingual chil-
dren. As more data becomes available, we expect
to gather stronger evidence supporting our method.
Our current efforts involve collecting more samples,
as well as evaluating the accuracy of LMs on mono-
lingual children with and without SLI.
Acknowledgements
Thanks to Bess Sirmon Fjordbak for her contribution to
the project and the three anonymous reviewers for their
useful comments.
References
L. B. Leonard. 1991. Specific language impairment as
a clinical category. Language, Speech, and Hearing
Services in Schools, 22:66?68.
C. D. Manning and H. Schu?tze. 1999. Foundations of
Statistical Natural Language Processing. The MIT
Press.
J. Paradis, M. Crago, and F. Genesee. 2005/6. Domain-
general versus domain-specific accounts of specific
language impairment: Evidence from bilingual chil-
drens acquisition of object pronouns. Language Ac-
quisition, 13:33?62.
B. Roark, M. Mitchell, and K. Hollingshead. 2007. Syn-
tactic complexity measures for detecting mild cogni-
tive impairment. In BioNLP 2007: Biological, trans-
lational, and clinical language processing, pages 1?8,
Prague, June. ACL.
T. Solorio, Y. Liu, and B. Medina. 2008. Part-of-speech
tagging English-Spanish code-switched text. Submit-
ted to Natural Language Engineering.
117
Word Fragment Identification Using Acoustic-Prosodic Features in
Conversational Speech
Yang Liu
 
 
ICSI, Berkeley, CA 94704 U.S.A

Purdue University, West Lafayette, IN 47907 U.S.A
 	
	 
 




Abstract
Word fragments pose serious problems for
speech recognizers. Accurate identification of
word fragments will not only improve recogni-
tion accuracy, but also be very helpful for dis-
fluency detection algorithm because the occur-
rence of word fragments is a good indicator of
speech disfluencies. Different from the previ-
ous effort of including word fragments in the
acoustic model, in this paper, we investigate the
problem of word fragment identification from
another approach, i.e. building classifiers using
acoustic-prosodic features. Our experiments
show that, by combining a few voice quality
measures and prosodic features extracted from
the forced alignments with the human tran-
scriptions, we obtain a precision rate of 74.3%
and a recall rate of 70.1% on the downsampled
data of spontaneous speech. The overall accu-
racy is 72.9%, which is significantly better than
chance performance of 50%.
1 Introduction
Word fragments1 occur frequently in spontaneous speech,
and are good indicators for speech disfluencies (Heeman
and Allen, 1999; Nakatani and Hirschberg, 1994). When
expressed as a percentage of the disfluencies that con-
tain a word fragment, Levelt found 22% for a pattern de-
scription task in Dutch (Levelt, 1983); Lickley reported
36% for casual conversations in British English (Lickley,
1994); Bear et al found 60% for the ATIS corpus (Bear
et al, 1992). We examined 83 conversations of Switch-
board corpus (Godfrey et al, 1992) and found that about
17% of the disfluencies contain word fragments. How-
ever, accurate identification of word fragments is still an
1A word fragment, also called a partial word, happens when
a speaker cuts off in the middle of a word.
unsolved problem in speech community. In most cases,
they are simply treated as Out-of-Vocabulary words or are
often incorrectly recognized as words in the vocabulary.
This not only affects the neighboring words, causing an
increase in word error rate, but also fails to provide the
important information that a word fragment is detected
thus increasing the probability of a disfluency.
The following is an example of the human transcription
and the speech recognition output2 from the Switchboard
corpus (Godfrey et al, 1992):
Human transcription:
and it?s all just you know i?ve just eating more sort of
eat to my apper- appetite
Recognizer output:
and it?s all just see now i?m just eating more sort of
need to my out bird?s appetite
We can see that in the recognition results, the word
fragment ?apper-? is incorrectly recognized as two words
in the vocabulary. Additionally, due to the failure to iden-
tify the word fragment ?apper-?, it will be extremely dif-
ficult to identify the disfluency in the recognition results.
The study of word fragments has been conducted from
different standpoints. Psychologists and linguists (Levelt,
1989) suggest that speakers rarely interrupt a word when
it is correct on its own, but they often do so when it is not.
Levelt proposed that ?by interrupting a word, a speaker
signals to the addressee that the word is an error. If a word
is completed, the speaker intends the listeners to interpret
it as correctly delivered? (Levelt, 1989). So when a word
is complete, the speakers are committing themselves to
its correctness (at least at that moment).
While linguists and psycholinguists have considered
this problem from the production point of view, we con-
sider this problem from a recognition standpoint, with
2The presence of a word fragment in the example is repre-
sented by a ?-? after the partial word. The recognition output is
from SRI?s recognizer system.
                                                               Edmonton, May-June 2003
                                                 Student Research Workshop , pp. 37-42
                                                         Proceedings of HLT-NAACL 2003
the goal of identifying disfluencies in spontaneous speech
and improving speech recognition.
As noted in (Bear et al, 1992), knowledge about the
location of word fragments would be an invaluable cue
to both detection and correction of disfluencies. Hee-
man and Allen proposed an integrated model for the de-
tection of speech repairs (Heeman and Allen, 1999). In
that model, word fragments are used as an important fea-
ture. Nakatani and Hirschberg proposed a ?speech-first?
model for the detection of speech repairs using acoustic-
prosodic cues, without relying on a word transcription
(Nakatani and Hirschberg, 1994). They found that the
presence of word fragments is an important indicator of
speech repairs, along with the other prosodic-acoustic
features such as silence duration, energy, and pitch. They
analyzed the properties of word fragments, for exam-
ple, the distribution of the fragments in syllable length,
the distribution of initial phonemes in the fragments, and
some acoustic cues (glottalization and coarticulation) in
the fragments. Although the role of word fragments as an
indicator of disfluencies is emphasized, they did not ad-
dress the problem of how to detect the occurrence of word
fragments, but only suggest that a word-based model
for word fragment detection is unlikely. O?Shaughnessy
(O?Shaughnessy, 1993) observed in the corpus of ATIS
that when speaker stopped in the middle of a word and
resumed speaking with no changed or inserted words (i.e.
a repetition), the pause lasted 100-400 ms in 85% of the
examples (with most of the remaining examples having
pause of about 1 second duration). He also found that
three-fourths of the interrupted words do not have a com-
pletion of the vowel in the intended word?s first syllable
(e.g., the speaker stopped after uttering the first conso-
nant).
Although word fragments should play an important
role for the disfluency processing in spontaneous speech,
the identification of word fragments is still an unsolved
problem in the speech community. It is impossible or
possibly confusing to include all the partial words in the
dictionary and therefore treat word fragments as regular
words. If one acoustic model is built for all the word frag-
ments, it may be quite difficult to train a good model to
cover all the word fragments due to the variability of the
possible partial words. Rose and Riccardi modeled word
fragments (using a single word fragment symbol frag) in
their system ?How May I Help You? (Rose and Riccardi,
1999). Their system was improved by explicitly model-
ing all the filled pauses, word fragments and non-speech
events; however, it did not report the effect that modeling
word fragments made.
In this paper, we investigate the problem of word frag-
ment detection using a new approach, i.e. from the prop-
erties of speech analysis. Our goal in this paper is to
investigate whether there are reliable acoustic-prosodic
properties for word fragments that can be used for auto-
matically detecting their presence.
The paper is organized as follows. In Section 2 we in-
troduce the acoustic and prosodic features that we inves-
tigate for word fragment detection. Section 3 describes
the corpus and our experimental results. Conclusions and
future work are found in section 4.
2 Acoustic and Prosodic Features
Our hypothesis is that when the speaker suddenly stops
in the middle of a word, some prosodic cues and voice
quality characteristics exist at the boundary of word frag-
ments; hence, our approach is to extract a variety of
acoustic and prosodic features, and build a classifier us-
ing these features for the automatic identification of word
fragments.
2.1 Prosodic Features
Recently, prosodic information has gained more impor-
tance in speech processing (Shriberg and Stolcke, 2002).
Prosody, the ?rhythm? and ?melody? of speech, is im-
portant for extracting structural information and automat-
ing rich transcriptions. Past research results suggest that
speakers use prosody to impose structure on both spon-
taneous and read speech. Such prosodic indicators in-
clude pause duration, change in pitch range and ampli-
tude, global pitch declination, and speaking rate varia-
tion. Since these features provide information comple-
mentary to the word sequence, they provide a potentially
valuable source of additional information. Furthermore,
prosodic cues by their nature are relatively unaffected by
word identity, and thus may provide a robust knowledge
source when the speech recognition error rate is high.
In the following we describe some of the prosodic fea-
tures we have investigated for the word fragment detec-
tion task. These prosodic features have been employed
previously for the task of detecting structural information
in spontaneous speech such as sentence boundary, dis-
fluencies, and dialog act. Experiments have shown that
prosody model yields a performance improvement when
combined with lexical information over using word level
information alone (Shriberg and Stolcke, 2002).
We used three main types of prosodic features ? du-
ration, pitch and energy. Duration features were ex-
tracted from the alignments obtained from the speech rec-
ognizer. Examples of duration features are word dura-
tion, pause duration, and duration of the last rhyme in the
word. Duration features are normalized in different ways
such as by using the overall phone duration statistics, and
speaker-specific duration statistics.
To obtain F0 features, pitch tracks were extracted from
the speech signal and then post-processed by using a log-
normal tied mixture model and a median filter (Sonmez et
al., 1997), which computes a set of speaker-specific pitch
range parameters. Pitch contours were then stylized, fit
by a piecewise linear model. Examples of pitch fea-
tures computed from the stylized F0 contours are the dis-
tance from the average pitch in the word to the speaker?s
baseline F0 value, the pitch slope of the word before the
boundary, and the difference of the stylized pitch across
word boundary.
For energy features, we first computed the frame-level
energy values of the speech signal, then similarly to the
approach used for F0 features, we post-processed the raw
energy values to get the stylized energy.
In addition to these prosodic features, we also included
features to represent some ancillary information, such as
the gender of the speaker, the position of the current word
in the turn3, and whether there is a turn change. We in-
cluded these non-prosodic features to account for the pos-
sible interactions between them and the other prosodic
features.
2.2 Voice Quality Measures
Human speech sounds are commonly considered to result
from a combination of a sound energy source modulated
by a transfer (filter) function determined by the shape of
the vocal tract. As the vocal cords open and close, puffs
of air flow through glottal opening. The frequency of
these pulses determines the fundamental frequency of the
laryngeal source and contributes to the perceived pitch of
the produced sound.
The voice source is an important factor affecting the
voice quality, and thus much investigation focuses on the
voice source characteristics. The analysis of voice source
has been done by inverse filtering the speech waveform,
analyzing the spectrum, or by directly measuring the air-
flow at the mouth for non-pathological speech. A widely
used model for voice source is the Liljencrants-Fant (LF)
model (Fant et al, 1985; Fant, 1995). Research has
shown that the intensity of the produced acoustic wave
depends more on the derivative of the glottal flow signal
than the amplitude of the flow itself.
An important representation of the glottal flow is given
by the Open Quotient (OQ). OQ is defined as the ratio
of the time in which the vocal folds are open to the total
length of the glottal cycle. From the spectral domain, it
can be formulated empirically as (Fant, 1997):
        		
 
 

 
  

 (1)
where 
  and 
  are the amplitudes of the first and the
second harmonics of the spectrum.
Different phonation types, namely, modal voicing,
creaking voicing and breathy voicing, differ in the
3In discourse analysis, all the contiguous utterances made
by a speaker before the next speaker begins is referred to as a
conversational turn.
amount of time that the vocal folds are open during each
glottal cycle. In modal voicing, the vocal folds are closed
during half of each glottal cycle; In creaky voicing, the
vocal folds are held together loosely resulting in a short
open quotient; In breathy voicing, the vocal folds vibrate
without much contact thus the glottis is open for a rela-
tively long portion of each glottal cycle.
For our word fragment detection task, we investigate
the following voice quality related features.
 Jitter is a measure of perturbation in the pitch period
that has been used by speech pathologists to identify
pathological speech (Rosenberg, 1970); a value of
0.01 represents a jitter of one percent, a lower bound
for abnormal speech.
The value of jitter is obtained from the speech anal-
ysis tool praat (Boersma and Wennik, 1996). The
pitch analysis of a sound is converted to a point pro-
cess, which represents a sequence of time points, in
this case the times associated with the pitch pulses.
The periodic jitter value is defined as the relative
mean absolute third-order difference of the point
process.
 Comparing and Combining Generative and Posterior Probability Models:
Some Advances in Sentence Boundary Detection in Speech
Yang Liu
ICSI and Purdue University
yangl@icsi.berkeley.edu
Andreas Stolcke Elizabeth Shriberg
SRI and ICSI
stolcke,ees@speech.sri.com
Mary Harper
Purdue University
harper@ecn.purdue.edu
Abstract
We compare and contrast two different models for
detecting sentence-like units in continuous speech.
The first approach uses hidden Markov sequence
models based on N-grams and maximum likeli-
hood estimation, and employs model interpolation
to combine different representations of the data.
The second approach models the posterior proba-
bilities of the target classes; it is discriminative and
integrates multiple knowledge sources in the max-
imum entropy (maxent) framework. Both models
combine lexical, syntactic, and prosodic informa-
tion. We develop a technique for integrating pre-
trained probability models into the maxent frame-
work, and show that this approach can improve
on an HMM-based state-of-the-art system for the
sentence-boundary detection task. An even more
substantial improvement is obtained by combining
the posterior probabilities of the two systems.
1 Introduction
Sentence boundary detection is a problem that has
received limited attention in the text-based com-
putational linguistics community (Schmid, 2000;
Palmer and Hearst, 1994; Reynar and Ratnaparkhi,
1997), but which has recently acquired renewed im-
portance through an effort by the DARPA EARS
program (DARPA Information Processing Technol-
ogy Office, 2003) to improve automatic speech tran-
scription technology. Since standard speech recog-
nizers output an unstructured stream of words, im-
proving transcription means not only that word ac-
curacy must be improved, but also that commonly
used structural features such as sentence boundaries
need to be recognized. The task is thus fundamen-
tally based on both acoustic and textual (via auto-
matic word recognition) information. From a com-
putational linguistics point of view, sentence units
are crucial and assumed in most of the further pro-
cessing steps that one would want to apply to such
output: tagging and parsing, information extraction,
and summarization, among others.
Sentence segmentation from speech is a difficult
problem. The best systems benchmarked in a re-
cent government-administered evaluation yield er-
ror rates between 30% and 50%, depending on the
genre of speech processed (measured as the num-
ber of missed and inserted sentence boundaries as
a percentage of true sentence boundaries). Because
of the difficulty of the task, which leaves plenty of
room for improvement, its relevance to real-world
applications, and the range of potential knowledge
sources to be modeled (acoustics and text-based,
lower- and higher-level), this is an interesting chal-
lenge problem for statistical and computational ap-
proaches.
All of the systems participating in the recent
DARPA RT-03F Metadata Extraction evaluation
(National Institute of Standards and Technology,
2003) were based on a hidden Markov model frame-
work, in which word/tag sequences are modeled by
N-gram language models (LMs). Additional fea-
tures (mostly reflecting speech prosody) are mod-
eled as observation likelihoods attached to the N-
gram states of the HMM (Shriberg et al, 2000). The
HMM is a generative modeling approach, since it
describes a stochastic process with hidden variables
(the locations of sentence boundaries) that produces
the observable data. The segmentation is inferred
by comparing the likelihoods of different boundary
hypotheses.
While the HMM approach is computationally ef-
ficient and (as described later) provides a convenient
way for modularizing the knowledge sources, it has
two main drawbacks: First, the standard training
methods for HMMs maximize the joint probability
of observed and hidden events, as opposed to the
posterior probability of the correct hidden variable
assignment given the observations. The latter is a
criterion more closely related to classification error.
Second, the N-gram LM underlying the HMM tran-
sition model makes it difficult to use features that
are highly correlated (such as word and POS labels)
without greatly increasing the number of model pa-
rameters; this in turn would make robust estimation
channelword string
prosody
idea
syntax, semantics,
word selection,
puntuation
impose prosody
signal
prosodic feature
extraction
prosodic
features
textual feature
speech
recognizer
word string
word, POS,
classes
fusion of
knowledge
souces
sentence boundary
hypothesis
Figure 1: Diagram of the sentence segmentation task.
difficult.
In this paper, we describe our effort to overcome
these shortcomings by 1) replacing the generative
model with one that estimates the posterior proba-
bilities directly, and 2) using the maximum entropy
(maxent) framework to estimate conditional distri-
butions, giving us a more principled way to com-
bine a large number of overlapping features. Both
techniques have been used previously for traditional
NLP tasks, but they are not straightforward to ap-
ply in our case because of the diverse nature of the
knowledge sources used in sentence segmentation.
We describe the techniques we developed to work
around these difficulties, and compare classification
accuracy of the old and new approach on different
genres of speech. We also investigate how word
recognition error affects that comparison. Finally,
we show that a simple combination of the two ap-
proaches turns out to be highly effective in improv-
ing the best previous results obtained on a bench-
mark task.
2 The Sentence Segmentation Task
The sentence boundary detection problem is de-
picted in Figure 1 in the source-channel framework.
The speaker intends to say something, chooses the
word string, and imposes prosodic cues (duration,
emphasis, intonation, etc). This signal goes through
the speech production channel to generate an acous-
tic signal. A speech recognizer determines the most
likely word string given this signal. To detect pos-
sible sentence boundaries in the recognized word
string, prosodic features are extracted from the sig-
nal, and combined with textual cues obtained from
the word string. At issue in this paper is the final
box in the diagram: how to model and combine the
available knowledge sources to find the most accu-
rate hypotheses.
Note that this problem differs from the sen-
tence boundary detection problem for written text in
the natural language processing literature (Schmid,
2000; Palmer and Hearst, 1994; Reynar and Rat-
naparkhi, 1997). Here we are dealing with spo-
ken language, therefore there is no punctuation in-
formation, the words are not capitalized, and the
transcripts from the recognition output are errorful.
This lack of textual cues is partly compensated by
prosodic information (timing, pitch, and energy pat-
terns) conveyed by speech. Also note that in spon-
taneous conversational speech ?sentence? is not al-
ways a straightforward notion. For our purposes we
use the definition of a ?sentence-like unit?, or SU,
as defined by the LDC for labeling and evaluation
purposes (Strassel, 2003).
The training data has SU boundaries marked by
annotators, based on both the recorded speech and
its transcription. In testing, a system has to recover
both the words and the locations of sentence bound-
aries, denoted by (W;E) = w
1
e
1
w
2
: : : w
i
e
i
: : : w
n
where W represents the strings of word tokens and
E the inter-word boundary events (sentence bound-
ary or no boundary).
The system output is scored by first finding a min-
imum edit distance alignment between the hypothe-
sized word string and the reference, and then com-
paring the aligned event labels. The SU error rate is
defined as the total number of deleted or inserted SU
boundary events, divided by the number of true SU
boundaries.1 For diagnostic purposes a secondary
evaluation condition allows use of the correct word
transcripts. This condition allows us to study the
segmentation task without the confounding effect of
speech recognition errors, using perfect lexical in-
formation.
3 Features and Knowledge Sources
Words and sentence boundaries are mutually con-
strained via syntactic structure. Therefore, the word
identities themselves (from automatic recognition
or human transcripts) constitute a primary knowl-
edge source for the sentence segmentation task. We
also make use of various automatic taggers that map
the word sequence to other representations. The
TnT tagger (Brants, 2000) is used to obtain part-of-
speech (POS) tags. A TBL chunker trained on Wall
Street Journal corpus (Ngai and Florian, 2001) maps
each word to an associated chunk tag, encoding
chunk type and relative word position (beginning of
an NP, inside a VP, etc.). The tagged versions of
the word stream are provided to allow generaliza-
tions based on syntactic structure and to smooth out
possibly undertrained word-based probability esti-
1This is the same as simple per-event classification accu-
racy, except that the denominator counts only the ?marked?
events, thereby yielding error rates that are much higher than
if one uses all potential boundary locations.
mates. For the same reasons we also generate word
class labels that are automatically induced from bi-
gram word distributions (Brown et al, 1992).
To model the prosodic structure of sentence
boundaries, we extract several hundred features
around each word boundary. These are based on the
acoustic alignments produced by a speech recog-
nizer (or forced alignments of the true words when
given). The features capture duration, pitch, and
energy patterns associated with the word bound-
aries. Informative features include the pause du-
ration at the boundary, the difference in pitch be-
fore and after the boundary, and so on. A cru-
cial aspect of many of these features is that they
are highly correlated (e.g., by being derived from
the same raw measurements via different normaliza-
tions), real-valued (not discrete), and possibly unde-
fined (e.g., unvoiced speech regions have no pitch).
These properties make prosodic features difficult to
model directly in either of the approaches we are ex-
amining in the paper. Hence, we have resorted to a
modular approach: the information from prosodic
features is modeled separately by a decision tree
classifier that outputs posterior probability estimates
P (e
i
jf
i
), where e
i
is the boundary event after w
i
,
and f
i
is the prosodic feature vector associated with
the word boundary. Conveniently, this approach
also permits us to include some non-prosodic fea-
tures that are highly relevant for the task, but not
otherwise represented, such as whether a speaker
(turn) change occurred at the location in question.2
A practical issue that greatly influences model de-
sign is that not all information sources are avail-
able uniformly for all training data. For example,
prosodic modeling assumes acoustic data; whereas,
word-based models can be trained on text-only data,
which is usually available in much larger quantities.
This poses a problem for approaches that model all
relevant information jointly and is another strong
motivation for modular approaches.
4 The Models
4.1 Hidden Markov Model for Segmentation
Our baseline model, and the one that forms the ba-
sis of much of the prior work on acoustic sentence
segmentation (Shriberg et al, 2000; Gotoh and Re-
nals, 2000; Christensen, 2001; Kim and Woodland,
2001), is a hidden Markov model. The states of
the model correspond to words w
i
and following
2Here we are glossing over some details on prosodic mod-
eling that are orthogonal to the discussion in this paper. For
example, instead of simple decision trees we actually use en-
semble bagging to reduce the variance of the classifier (Liu et
al., 2004).
Wi Ei
Fi
Oi
Wi+1 Ei+1
Oi+1
Wi Fi+1Wi+1
Figure 2: The graphical model for the SU detection
problem. Only one word+event is depicted in each state,
but in a model based on N-grams the previous N   1
tokens would condition the transition to the next state.
event labels e
i
. The observations associated with
the states are the words, as well as other (mainly
prosodic) features f
i
. Figure 2 shows a graphi-
cal model representation of the variables involved.
Note that the words appear in both the states and the
observations, such that the word stream constrains
the possible hidden states to matching words; the
ambiguity in the task stems entirely from the choice
of events.
4.1.1 Classification
Standard algorithms are available to extract the most
probable state (and thus event) sequence given a set
of observations. The error metric is based on clas-
sification of individual word boundaries. Therefore,
rather than finding the highest probability sequence
of events, we identify the events with highest poste-
rior individually at each boundary i:
e^
i
= arg max
e
i
P (e
i
jW;F ) (1)
where W and F are the words and features for
the entire test sequence, respectively. The individ-
ual event posteriors are obtained by applying the
forward-backward algorithm for HMMs (Rabiner
and Juang, 1986).
4.1.2 Model Estimation
Training of the HMM is supervised since event-
labeled data is available. There are two sets of pa-
rameters to estimate. The state transition proba-
bilities are estimated using a hidden event N-gram
LM (Stolcke and Shriberg, 1996). The LM is
obtained with standard N-gram estimation meth-
ods from data that contains the word+event tags in
sequence: w
1
; e
1
; w
2
; : : : e
n 1
; w
n
. The resulting
LM can then compute the required HMM transition
probabilities as3
P (w
i
e
i
jw
1
e
1
: : : w
i 1
e
i 1
) =
P (w
i
jw
1
e
1
: : : w
i 1
e
i 1
) 
P (e
i
jw
1
e
1
: : : w
i 1
e
i 1
w
i
)
The N-gram estimator maximizes the joint
word+event sequence likelihood P (W;E) on the
training data (modulo smoothing), and does not
guarantee that the correct event posteriors needed
for classification according to Equation (1) are
maximized.
The second set of HMM parameters are the ob-
servation likelihoods P (f
i
je
i
; w
i
). Instead of train-
ing a likelihood model we make use of the prosodic
classifiers described in Section 3. We have at our
disposal decision trees that estimate P (e
i
jf
i
). If
we further assume that prosodic features are inde-
pendent of words given the event type (a reasonable
simplification if features are chosen appropriately),
observation likelihoods may be obtained by
P (f
i
jw
i
; e
i
) =
P (e
i
jf
i
)
P (e
i
)
P (f
i
) (2)
Since P (f
i
) is constant we can ignore it when car-
rying out the maximization (1).
4.1.3 Knowledge Combination
The HMM structure makes strong independence as-
sumptions: (1) that features depend only on the cur-
rent state (and in practice, as we saw, only on the
event label) and (2) that each word+event label de-
pends only on the last N   1 tokens. In return, we
get a computationally efficient structure that allows
information from the entire sequence W;F to in-
form the posterior probabilities needed for classifi-
cation, via the forward-backward algorithm.
More problematic in practice is the integration
of multiple word-level features, such as POS tags
and chunker output. Theoretically, all tags could
simply be included in the hidden state representa-
tion to allow joint modeling of words, tags, and
events. However, this would drastically increase the
size of the state space, making robust model estima-
tion with standard N-gram techniques difficult. A
method that works well in practice is linear inter-
polation, whereby the conditional probability esti-
mates of various models are simply averaged, thus
reducing variance. In our case, we obtain good re-
sults by interpolating a word-N-gram model with
3To utilize word+event contexts of length greater than one
we have to employ HMMs of order 2 or greater, or equivalently,
make the entire word+event N-gram be the state.
one based on automatically induced word classes
(Brown et al, 1992).
Similarly, we can interpolate LMs trained from
different corpora. This is usually more effective
than pooling the training data because it allows con-
trol over the contributions of the different sources.
For example, we have a small corpus of training data
labeled precisely to the LDC?s SU specifications,
but a much larger (130M word) corpus of standard
broadcast new transcripts with punctuation, from
which an approximate version of SUs could be in-
ferred. The larger corpus should get a larger weight
on account of its size, but a lower weight given the
mismatch of the SU labels. By tuning the interpola-
tion weight of the two LMs empirically (using held-
out data) the right compromise was found.
4.2 Maxent Posterior Probability Model
As observed, HMM training does not maximize the
posterior probabilities of the correct labels. This
mismatch between training and use of the model
as a classifier would not arise if the model directly
estimated the posterior boundary label probabilities
P (e
i
jW;F ). A second problem with HMMs is that
the underlying N-gram sequence model does not
cope well with multiple representations (features) of
the word sequence (words, POS, etc.) short of build-
ing a joint model of all variables. This type of sit-
uation is well-suited to a maximum entropy formu-
lation (Berger et al, 1996), which allows condition-
ing features to apply simultaneously, and therefore
gives greater freedom in choosing representations.
Another desirable characteristic of maxent models
is that they do not split the data recursively to condi-
tion their probability estimates, which makes them
more robust than decision trees when training data
is limited.
4.2.1 Model Formulation and Training
We built a posterior probability model for sentence
boundary classification in the maxent framework.
Such a model takes the familiar exponential form4
P (ejW;F ) =
1
Z

(W;F )
e
P
k

k
g
k
(e;W;F ) (3)
where Z

(W;F ) is the normalization term:
Z

(W;F ) =
X
e
0
e
P
k

k
g
k
(e
0
;W;F ) (4)
The functions g
k
(e;W;F ) are indicator functions
corresponding to (complex) features defined over
4We omit the index i from e here since the ?current? event
is meant in all cases.
events, words, and prosodic features. For example,
one such feature function might be:
g(e;W;F ) =
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 707?715,
Beijing, August 2010
Joint Parsing and Translation
Yang Liu and Qun Liu
Key Laboratory of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences
{yliu,liuqun} @ict.ac.cn
Abstract
Tree-based translation models, which ex-
ploit the linguistic syntax of source lan-
guage, usually separate decoding into two
steps: parsing and translation. Although
this separation makes tree-based decoding
simple and efficient, its translation perfor-
mance is usually limited by the number
of parse trees offered by parser. Alter-
natively, we propose to parse and trans-
late jointly by casting tree-based transla-
tion as parsing. Given a source-language
sentence, our joint decoder produces a
parse tree on the source side and a transla-
tion on the target side simultaneously. By
combining translation and parsing mod-
els in a discriminative framework, our ap-
proach significantly outperforms a forest-
based tree-to-string system by 1.1 ab-
solute BLEU points on the NIST 2005
Chinese-English test set. As a parser,
our joint decoder achieves an F1 score of
80.6% on the Penn Chinese Treebank.
1 Introduction
Recent several years have witnessed the rapid
development of syntax-based translation models
(Chiang, 2007; Galley et al, 2006; Shen et al,
2008; Quirk et al, 2005; Liu et al, 2006; Huang
et al, 2006; Eisner, 2003; Zhang et al, 2008; Chi-
ang, 2010), which incorporate formal or linguis-
tic syntax into translation process. Depending on
whether modeling the linguistic syntax of source
language or not, we divide them into two cate-
gories: string-based and tree-based models. 1
1Mi et al (2008) also distinguish between string-based
and tree-based models but depending on the type of input.
source
target
parse+translate
string tree
string
source
target
string
parse
tree
translate
string
(a)
(b)
Figure 1: Tree-based decoding: (a) separate pars-
ing and translation versus (b) joint parsing and
translation.
String-based models include string-to-string
(Chiang, 2007) and string-to-tree (Galley et al,
2006; Shen et al, 2008). Regardless of the syn-
tactic information on the source side, they treat
decoding as a parsing problem: the decoder parses
a source-language sentence using the source pro-
jection of a synchronous grammar while building
the target sub-translations in parallel.
Tree-based models include tree-to-string (Liu
et al, 2006; Huang et al, 2006) and tree-to-tree
(Quirk et al, 2005; Eisner, 2003; Zhang et al,
2008; Chiang, 2010). These models explicitly
use source parse trees and divide decoding into
two separate steps: parsing and translation. A
parser first parses a source-language sentence into
a parse tree, and then a decoder converts the tree
to a translation on the target side (see Figure 1(a)).
Figure 2 gives a training example for tree-to-
string translation, which consists of a Chinese
tree, an English sentence, and the word align-
ment between them. Romanized Chinese words
are given to facilitate identification. Table 1 shows
707
?? ? ?9 ?1


?!
NR P NR VV AS NN
NPB NPB NPB
PP VPB
VP
IP
bushi yu shalong juxing le huitan
Bush held a meeting with Sharon
Figure 2: A training example that consists of a
Chinese parse, an English sentence, and the word
alignment between them.
a set of tree-to-string rules obtained from Figure
2. The source side of a rule is a tree fragment
and the target side is a string. We use x to denote
non-terminals and the associated subscripts indi-
cate the correspondence between non-terminals
on both sides.
Conventionally, decoding for tree-to-string
translation is cast as a tree parsing problem (Eis-
ner, 2003). The tree parsing algorithm visits each
node in the input source tree in a top-down order
and tries to match each translation rule against the
local sub-tree rooted at the node. For example, the
first rule in Table 1 matches a sub-tree rooted at
IP0,6 in Figure 2. The descendent nodes of IP0,6
(i.e., NPB0,1, PP1,3, and VPB3,6) can be further
matched by other rules in Table 1. The matching
procedure runs recursively until the entire tree is
covered. Finally, the output on the target side can
be taken as a translation.
Compared with its string-based counterparts,
tree-based decoding is simpler and faster: there
is no need for synchronous binarization (Huang
et al, 2009b; Zhang et al, 2006) and tree parsing
generally runs in linear time (Huang et al, 2006).
While separating parsing and translation makes
tree-based decoding simple and efficient, its
search space is limited by the number of parse
trees offered by parser. Studies reveal that tree-
based systems are prone to produce degenerate
translations due to the propagation of parsing mis-
takes (Quirk and Corston-Oliver, 2006). This
problem can be alleviated by offering more alter-
(1) IP(x1:NPB VP(x2:PP x3:VPB))?x1 x3 x2
(2) NPB(NR(bushi))?Bush
(3) PP(P(yu) x1:NPB)?with x1
(4) NPB(NR(shalong))?Sharon
(5) VPB(VV(juxing) AS(le) x1:NPB)?held a x1
(6) NPB(NN(huitan))?meeting
Table 1: Tree-to-string rules extracted from Figure
2.
natives to the pipeline. An elegant solution is to
replace 1-best trees with packed forests that en-
code exponentially many trees (Mi et al, 2008;
Liu et al, 2009). Mi et al (2008) present an
efficient algorithm to match tree-to-string rules
against packed forests that encode millions of
trees. They prove that offering more alternatives
to tree parsing improves translation performance
substantially.
In this paper, we take a further step towards the
direction of offering multiple parses to translation
by proposing joint parsing and translation. As
shown in Figure 1(b), our approach parses and
translates jointly as it finds a parse tree and a
translation of a source-language sentence simul-
taneously. We integrate the tree-to-string model
(Liu et al, 2006; Huang et al, 2006), n-gram lan-
guage model, probabilistic context-free grammar
(PCFG), and Collins? Model 1 (Collins, 2003) in a
discriminative framework (Och, 2003). Allowing
parsing and translation to interact with each other,
our approach obtains an absolute improvement of
1.1 BLEU points over a forest-based tree-to-string
translation system (Mi et al, 2008) on the 2005
NIST Chinese-English test set. As a parser, our
joint decoder achieves an F1 score of 80.6% on
the Penn Chinese Treebank.
2 Joint Parsing and Translation
2.1 Decoding as Parsing
We propose to integrate parsing and translation
into a single step. To achieve joint parsing and
translation, we cast tree-to-string decoding as a
monolingual parsing problem (Melamed, 2004;
Chiang, 2007; Galley et al, 2006): the de-
coder takes a source-language string as input and
parses it using the source-projection of SCFG
while building the corresponding sub-translations
simultaneously.
708
For example, given the Chinese sentence bushi
yu sha long juxing le huitan in Figure 2, the
derivation in Table 1 explains how a Chinese tree,
an English string, and the word alignment be-
tween them are generated synchronously. Unlike
the string-based systems as described in (Chiang,
2007; Galley et al, 2006; Shen et al, 2008), we
exploit the linguistic syntax on the source side
explicitly. Therefore, the source parse trees pro-
duced by our decoder are meaningful from a lin-
guistic point of view.
As tree-to-string rules usually have multiple
non-terminals that make decoding complexity
generally exponential, synchronous binarization
(Huang et al, 2009b; Zhang et al, 2006) is a
key technique for applying the CKY algorithm
to parsing with tree-to-string rules. 2 Huang et
al. (2009b) factor each tree-to-string rule into two
SCFG rules: one from the root nonterminal to
the subtree, and the other from the subtree to the
leaves. In this way, one can uniquely reconstruct
the original tree using a two-step SCFG deriva-
tion.
For example, consider the first rule in Table 1:
IP(x1:NPB VP(x2:PP x3:VPB))?x1 x3 x2
We use a specific non-terminal, say, T, to
uniquely identify the left-hand side subtree and
produce two SCFG rules: 3
IP ? ?T 1 ,T 1 ? (1)
T ? ?NPB 1 PP 2 VPB 3 ,NPB 1 VPB 3 PP 2 ? (2)
where the boxed numbers indicate the correspon-
dence between nonterminals.
Then, the rule (2) can be further binarized into
two rules that have at most two non-terminals:
T ? ?NPB 1 PP-VPB 2 ,NPB 1 PP-VPB 2 ? (3)
PP-VPB ? ?PP 1 VPB 2 ,VPB 2 PP 1 ? (4)
where PP-VPB is an intermediate virtual non-
terminal.
2But CKY is not the only choice. The Earley algorithm
can also be used to parse with tree-to-string rules (Zhao and
Al-Onaizan, 2008). As the Earley algorithm binarizes multi-
nonterminal rules implicitly, there is no need for synchronous
binarization.
3It might look strange that the node VP disappears. This
node is actually stored in the monolithic node T. Please refer
to page 573 of (Huang et al, 2009b) for more details about
how to convert tree-to-string rules to SCFG rules.
We call rules the tree roots of which are vir-
tual non-terminals virtual rules and others natural
rules. For example, the rule (1) is a natural rule
and the rules (3) and (4) are virtual rules. We fol-
low Huang et al (2009b) to keep the probabilities
of a natural rule unchanged and set those of a vir-
tual rule to 1. 4
After binarizing tree-to-string rules into SCFG
rules that have at most two non-terminals, we can
use the CKY algorithm to parse a source sentence
and produce its translation simultaneously as de-
scribed in (Chiang, 2007; Galley et al, 2006).
2.2 Adding Parsing Models
As our decoder produces ?genuine? parse trees
during decoding, we can integrate parsing mod-
els as features together with translation features
such as the tree-to-string model, n-gram language
model, and word penalty into a discriminative
framework (Och, 2003). We expect that pars-
ing and translation could interact with each other:
parsing offers linguistically motivated reordering
to translation and translation helps parsing resolve
ambiguity.
2.2.1 PCFG
We use the probabilistic context-free grammar
(PCFG) as the first parsing feature in our decoder.
Given a PCFG, the probability for a tree is the
product of probabilities for the rules that it con-
tains. That is, if a tree pi is a context-free deriva-
tion that involves K rules of the form ?k ? ?k ,
its probability is given by
P(pi) =
?
k=1...K
Ppcfg(?k ? ?k) (5)
For example, the probability for the tree in Fig-
ure 2 is
P(pi) = Ppcfg(IP ? NPB VP)?
Ppcfg(NPB ? NR)?
Ppcfg(NR ? bushi)?
. . . (6)
4This makes the scores of hypotheses in the same chart
cell hardly comparable because some hypotheses are cov-
ered by a natural non-terminal and others covered by a virtual
non-terminal. To alleviate this problem, we follow Huang et
al. (2009b) to separate natural and virtual hypotheses in dif-
ferent beams.
709
IP
T
NPB PP-VP
PP VPB
IP
NPB VP
PP VPB
Figure 3: Reconstructing original tree from virtual
rules. We first construct the tree on the left by
substituting the trees of the rules (1), (3), and (4)
and then restore the original tree on the right via
the monolithic node T.
There are 13 PCFG rules involved. We omit the
remaining 10 rules.
We formalize the decoding process as a deduc-
tive system to illustrate how to include a PCFG.
Given a natural rule
VP ? ?PP 1 VPB 2 ,VPB 2 PP 1 ? (7)
the following deductive step grows an item in the
chart by the rule
(PP1,3) : (w1, e1) (VPB3,6) : (w2, e2)
(VP1,6) : (w, e2e1)
(8)
where PP1,3 denotes the recognition of the non-
terminal PP spanning from the substring from po-
sition 1 through 3 (i.e., yu shalong in Figure 2), w1
and e1 are the score and translation of the first an-
tecedent item, respectively, and the resulting item
score is calculated as: 5
w = w1 + w2 + logPpcfg(VP ? PP VPB) (9)
As the PCFG probabilities of natural rules are
fixed during decoding, they can be pre-computed
and stored in the rule table. Therefore, including
PCFG for natural rules hardly increases decoding
complexity.
However, calculating the PCFG probabilities
for virtual rules is quite different due to the pres-
ence of virtual non-terminals. For instance, using
the rule (4) in Section 2.1 to generate an item leads
to the following deductive step:
(PP1,3) : (w1, e1) (VPB3,6) : (w2, e2)
(PP-VPB1,6) : (w, e2e1)
(10)
5The logarithmic form of probability is used to avoid ma-
nipulating very small numbers for practical reasons. w1 and
w2 take the PCFG probabilities of the two antecedent items
into consideration.
As PP-VPB is a virtual non-terminal, the sub-
tree it dominates is a virtual tree, for which we
cannot figure out its PCFG probability. There-
fore, we have to postpone the calculation of PCFG
probabilities until reaching a natural non-terminal
such as IP. In other words, only when using the
rule (1) to produce an item, the decoding algo-
rithm can update PCFG probabilities because the
original tree can be restored from the special node
T now. Figure 3 shows how to reconstruct the
original tree from virtual rules. We first construct
the tree on the left by substituting the trees of the
rules (1), (3), and (4) and then restore the origi-
nal tree on the right via T. Now, we can calculate
the PCFG probability of the original tree. 6 In
practice, we pre-compute this PCFG probability
and store it in the rule (1) to reduce computational
overhead.
2.2.2 Lexicalized PCFG
Although widely used in natural language pro-
cessing, PCFGs are often criticized for the lack of
lexicalization, which is very important to capture
the lexical dependencies between words. There-
fore, we use Collins? Model 1 (Collins, 2003), a
simple and effective lexicalized parsing model, as
the second parsing feature in our decoder.
Following Collins (2003), we first lexicalize a
tree by associating a headword h with each non-
terminal. Figure 4 gives the lexicalized tree corre-
sponding to Figure 2. The left-hand side of a rule
in a lexicalized PCFG is P (h) and the right-hand
side has the form:
Ln(ln) . . . L1(l1)H(h)R1(?1) . . . Rm(?m) (11)
where H is the head-child that inherits the
headword h from its parent P , L1 . . . Ln and
R1 . . . Rm are left and right modifiers of H , and
l1 . . . ln and ?1 . . . ?m are the corresponding head-
words. Either n or m may be zero, and n =
m = 0 for unary rules. Collins (2003) extends the
left and right sequences to include a terminating
STOP symbol. Thus, Ln+1 = Rm+1 = STOP.
6Postponing the calculation of PCFG probabilities also
leads to the ?hard-to-compare? problem mentioned in foot-
note 4 due to the presence of virtual non-terminals. We still
maintain multiple beams for natural and virtual hypotheses
(i.e., items) to alleviate this prblem.
710
?? ? ?9 ?1


?!
NR P NR VV AS NN
NPB NPB NPB
PP VPB
VP
IP
bushi yu shalong juxing le huitan
bushi
bushi
yu shalong
shalong
yu
juxing le huitan
huitan
juxing
juxing
juxing
Figure 4: The lexicalized tree corresponding to
Figure 2.
Collins (2003) breaks down the generation of
the right-hand side of a rule into a sequence of
smaller steps. The probability of a rule is decom-
posed as:
Ph(H|P (h)) ??
i=1...n+1
Pl(Li(li)|P (h),H, t,?) ?
?
j=1...m+1
Pr(Rj(?j)|P (h),H, t,?) (12)
where t is the POS tag of of the headword h and ?
is the distance between words that captures head-
modifier relationship.
For example, the probability of the lexicalized
rule IP(juxing) ? NPB(bushi) VP(juxing) can
be computed as 7
Ph(VP|IP, juxing)?
Pl(NPB(bushi)|IP,VP, juxing)?
Pl(STOP|IP,VP, juxing)?
Pr(STOP|IP,VP, juxing) (13)
We still use the deductive system to explain
how to integrate the lexicalized PCFG into the de-
coding process. Now, Eq. (8) can be rewritten as:
(PPyu1,3) : (w1, e1) (VPB
juxing
3,6 ) : (w2, e2)
(VPjuxing1,6 ) : (w, e2e1)
(14)
where yu and juxing are the headwords attached
to PP1,3, VPB3,6, and VP1,6. The resulting item
7For simplicity, we omit POS tag and distance in the pre-
sentation. In practice, we implemented the Collins? Model 1
exactly as described in (Collins, 2003).
score is given by
w = w1 + w2 + logPh(VPB|VP, juxing) +
logPl(PP(yu)|VP,VPB, juxing) +
logPl(STOP|VP,VPB, juxing) +
logPr(STOP|VP,VPB, juxing) (15)
Unfortunately, the lexicalized PCFG probabili-
ties of most natural rules cannot be pre-computed
because the headword of a non-terminal must be
determined on the fly during decoding. Consider
the third rule in Table 1
PP(P(yu) x1:NPB) ? with x1
It is impossible to know what the headword of
NPB is in advance, which depends on the ac-
tual sentence being translated. However, we could
safely say that the headword attached to PP is al-
ways yu because PP should have the same head-
word with its child P.
Similar to the PCFG scenario, calculating lex-
icalized PCFG for virtual rules is different from
natural rules. Consider the rule (4) in Section 2.1,
the corresponding deductive step is
(PPyu1,3) : (w1, e1) (VPB
juxing
3,6 ) : (w2, e2)
(PP-VPB?1,6) : (w, e2e1)
(16)
where ??? denotes that the headword of
PP-VPB1,6 is undefined.
We still need to postpone the calculation of lex-
icalized PCFG probabilities until reaching a nat-
ural non-terminal such as IP. In other words,
only when using the rule (1) to produce an item,
the decoding algorithm can update the lexicalized
PCFG probabilities. After restoring the original
tree from T, we need to visit backwards to fron-
tier nodes of the tree to find headwords and calcu-
late lexicalized PCFG probabilities. More specifi-
cally, updating lexicalized PCFG probabilities for
the rule the rule (1) involves the following steps:
1. Reconstruct the original tree from the rules
(1), (3), and (4) as shown in Figure 3;
2. Attach headwords to all nodes;
3. Calculate the lexicalized PCFG probabilities
according to Eq. (12).
711
Back-off Pl(Li(li)| . . . )
level Ph(H| . . . ) Pr(Rj(?j)| . . . )
1 P , h, t P , H , h, t, ?
2 P , t P , H , t, ?
3 P P , H , ?
Table 2: The conditioning variables for each level
of back-off.
As suggested by Collins (2003), we use back-
off smoothing for sub-model probabilities during
decoding. Table 2 shows the various levels of
back-off for each type of parameter in the lexi-
calized parsing model we use. For example, Ph
estimation p interpolates maximum-likelihood es-
timates p1 = Ph(H|P, h, t), p2 = Ph(H|P, t),
and p3 = Ph(H|P ) as follows:
p1 = ?1p1 + (1? ?1)(?2p2 + (1? ?2)p3) (17)
where ?1, ?2, and ?3 are smoothing parameters.
3 Experiments
In this section, we try to answer two questions:
1. Does tree-based translation by parsing out-
perform the conventional tree parsing algo-
rithm? (Section 3.1)
2. How about the parsing performance of the
joint decoder? (Section 3.2)
3.1 Translation Evaluation
We used a bilingual corpus consisting of 251K
sentences with 7.3M Chinese words and 9.2M En-
glish words to extract tree-to-string rules. The
Chinese sentences in the bilingual corpus were
parsed by an in-house parser (Xiong et al, 2005),
which obtains an F1 score of 84.4% on the Penn
Chinese Treebank. After running GIZA++ (Och
and Ney, 2003) to obtain word alignments, we
used the GHKM algorithm (Galley et al, 2004)
and extracted 11.4M tree-to-string rules from the
source-side parsed, word-aligned bilingual cor-
pus. Note that the bilingual corpus does not con-
tain the bilingual version of Penn Chinese Tree-
bank. In other words, all tree-to-string rules were
learned from noisy parse trees and alignments. We
used the SRILM toolkit (Stolcke, 2002) to train a
4-gram language model on the Xinhua portion of
the GIGAWORD corpus, which contains 238M
English words. We trained PCFG and Collins?
Model 1 on the Penn Chinese Treebank.
We used the 2002 NIST MT Chinese-English
test set as the development set and the 2005 NIST
test set as the test set. Following Huang (2008),
we modified our in-house parser to produce and
prune packed forests on the development and test
sets. There are about 105M parse trees encoded
in a forest of a sentence on average. We also ex-
tracted 1-best trees from the forests.
As the development and test sets have many
long sentences (? 100 words) that make our de-
coder prohibitively slow, we divided long sen-
tences into short sub-sentences simply based on
punctuation marks such as comma and period.
The source trees and target translations of sub-
sentences were concatenated to form the tree and
translation of the original sentence.
We compared our parsing-based decoder with
the tree-to-string translation systems based on the
tree parsing algorithm, which match rules against
either 1-best trees (Liu et al, 2006; Huang et al,
2006) or packed forests (Mi et al, 2008). All the
three systems used the same rule set containing
11.4M tree-to-string rules. Given the 1-best trees
of the test set, there are 1.2M tree-to-string rules
that match fragments of the 1-best trees. For the
forest-based system (Mi et al, 2008), the num-
ber of filtered rules increases to 1.9M after replac-
ing 1-best trees with packed forests, which con-
tain 105M trees on average. As our decoder takes
a string as input, 7.7M tree-to-string rules can be
used to parse and translate the test set. We bi-
narized 99.6% of tree-to-string rules into 16.2M
SCFG rules and discarded non-binarizable rules.
As a result, the search space of our decoder is
much larger than those of the tree parsing coun-
terparts.
Table 3 shows the results. All the three sys-
tems used the conventional translation features
such as relative frequencies, lexical weights, rule
count, n-gram language model, and word count.
Without any parsing models, the tree-based sys-
tem achieves a BLEU score of 29.8. The forest-
based system outperforms the tree-based system
by +1.8 BLEU points. Note that each hyperedge
712
Algorithm Input Parsing model # of rules BLEU (%) Time (s)
tree - 1.2M 29.8 0.56tree parsing forest PCFG 1.9M 31.6 9.49
- 32.0 51.41
PCFG 32.4 55.52parsing string
Lex
7.7M 32.6 89.35
PCFG + Lex 32.7 91.72
Table 3: Comparison of tree parsing and parsing for tree-to-string translation in terms of case-insensitive
BLEU score and average decoding time (second per sentence). The column ?parsing model? indicates
which parsing models were used in decoding. We use ?-? to denote using only translation features.
?Lex? represents the Collins? Model 1. We excluded the extra parsing time for producing 1-best trees
and packed forests.
Forest size Exact match (%) Precision (%)
1 0.55 41.5
390 0.74 47.7
5.8M 0.92 54.1
66M 1.48 62.0
105M 2.22 65.9
Table 4: Comparison of 1-best trees produced by
our decoder and the parse forests produced by the
monolingual Chinese parser. Forest size repre-
sents the average number of trees stored in a for-
est.
in a parse forest is assigned a PCFG probabil-
ity. Therefore, the forest-based system actually in-
cludes PCFG as a feature (Mi et al, 2008). With-
out incorporating any parsing models as features,
our joint decoder achieves a BLEU score of 32.0.
Adding PCFG and Collins? Model 1 (i.e., ?Lex? in
Table 2) increases translation performance. When
both PCFG and Collins? Model 1 are used, our
joint decoder outperforms the tree parsing systems
based on 1-best trees (+2.9) and packed forests
(+1.1) significantly (p < 0.01). This result is also
better than that of using only translation features
significantly (from 32.0 to 32.7, p < 0.05).
Not surprisingly, our decoder is much slower
than pattern matching on 1-best trees and packed
forests (with the same beam size). In particu-
lar, including Collins? Model 1 increases decoding
time significantly because its sub-model probabil-
ities requires back-off smoothing on the fly.
How many 1-best trees produced by our de-
coder are included in the parse forest produced by
a standard parser? We used the Chinese parser
to generate five pruned packed forests with dif-
ferent sizes (average number of trees stored in a
forest). As shown in Table 4, only 2.22% of the
trees produced by our decoder were included in
the biggest forest. One possible reason is that
we used sub-sentence division to reduce decoding
complexity. To further investigate the matching
rate, we also calculated labeled precision, which
indicates how many brackets in the parse match
those in the packed forest. The labeled precision
on the biggest forest is 65.9%, suggesting that the
1-best trees produced by our decoder are signifi-
cantly different from those in the packed forests
produced by a standard parser. 8
3.2 Parsing Evaluation
We followed Petrov and Klein (2007) to divide the
Penn Chinese Treebank (CTB) version 5 as fol-
lows: Articles 1-270 and 400-1151 as the training
set, Articles 301-325 as the development set, and
Articles 271-300 as the test set. We used max-F1
training (Och, 2003) to train the feature weights.
We did not use sub-sentence division as the sen-
tences in the test set have no more than 40 words.
8The packed forest produced by our decoder (?rule?
forest) might be different from the forest produced by a
monolingual parser (?parser? forest). While tree-based and
forest-based decoders search in the intersection of the two
forests (i.e., matched forest), our decoder directly explores
the ?rule? forest, which represents the true search space of
tree-to-string translation. This might be the key difference of
our approach from forest-based translation (Mi et al, 2008).
As sub-sentence division makes direct comparison of the two
forests quite difficult, we leave this to future work.
713
Parsing model F1 (%) Time (s)
- 62.7 23.9
PCFG 65.4 24.7
Lex 79.8 48.8
PCFG + Lex 80.6 50.4
Table 5: Effect of parsing models on parsing per-
formance (? 40 words) and average decoding
time (second per sentence). We use ?-? to denote
only using translation features.
Table 5 shows the results. Translation features
were used for all configurations. Without pars-
ing models, the F1 score is 62.7. Adding Collins?
Model 1 results in much larger gains than adding
PCFG. With all parsing models integrated, our
joint decoder achieves an F1 score of 80.6 on the
test set. Although lower than the F1 score of the
in-house parser that produces the noisy training
data, this result is still very promising because
the tree-to-string rules that construct trees in the
decoding process are learned from noisy training
data.
4 Related Work
Charniak et al (2003) firstly associate lexical-
ized parsing model with syntax-based translation.
They first run a string-to-tree decoder (Yamada
and Knight, 2001) to produce an English parse
forest and then use a lexicalized parsing model to
select the best translation from the forest. As the
parsing model operates on the target side, it actu-
ally serves as a syntax-based language model for
machine translation. Recently, Shen et al (2008)
have shown that dependency language model is
beneficial for capturing long-distance relations
between target words. As our approach adds pars-
ing models to the source side where the source
sentence is fixed during decoding, our decoder
does parse the source sentence like a monolingual
parser instead of a syntax-based language model.
More importantly, we integrate translation models
and parsing models in a discriminative framework
where they can interact with each other directly.
Our work also has connections to joint parsing
(Smith and Smith, 2004; Burkett and Klein, 2008)
and bilingually-constrained monolingual parsing
(Huang et al, 2009a) because we use another
language to resolve ambiguity for one language.
However, while both joint parsing and bilingually-
constrained monolingual parsing rely on the target
sentence, our approach only takes a source sen-
tence as input.
Blunsom and Osborne (2008) incorporate the
source-side parse trees into their probabilistic
SCFG framework and treat every source-parse
PCFG rule as an individual feature. The differ-
ence is that they parse the test set before decoding
so as to exploit the source syntactic information to
guide translation.
More recently, Chiang (2010) has shown
that (?exact?) tree-to-tree translation as pars-
ing achieves comparable performance with Hiero
(Chiang, 2007) using much fewer rules. Xiao et
al. (2010) integrate tokenization and translation
into a single step and improve the performance of
tokenization and translation significantly.
5 Conclusion
We have presented a framework for joint parsing
and translation by casting tree-to-string transla-
tion as a parsing problem. While tree-to-string
rules construct parse trees on the source side
and translations on the target side simultaneously,
parsing models can be integrated to improve both
translation and parsing quality.
This work can be considered as a final step to-
wards the continuum of tree-to-string translation:
from single tree to forest and finally to the inte-
gration of parsing and translation. In the future,
we plan to develop more efficient decoding al-
gorithms, analyze forest matching systematically,
and use more sophisticated parsing models.
Acknowledgement
The authors were supported by National Nat-
ural Science Foundation of China, Contracts
60736014 and 60903138, and 863 State Key
Project No. 2006AA010108. We are grateful to
the anonymous reviewers for their insightful com-
ments. We thank Liang Huang, Hao Zhang, and
Tong Xiao for discussions on synchronous bina-
rization, Haitao Mi and Hao Xiong for provid-
ing and running the baseline systems, and Wenbin
Jiang for helping us train parsing models.
714
References
Blunsom, Phil and Miles Osborne. 2008. Probabilis-
tic inference for machine translation. In Proc. of
EMNLP 2008.
Burkett, David and Dan Klein. 2008. Two languages
are better than one (for syntactic parsing). In Proc.
of EMNLP 2008.
Charniak, Eugene, Kevin Knight, and Kenji Yamada.
2003. Syntax-based language models for statistical
machine translation. In Proc. of MT Summit IX.
Chiang, David. 2007. Hierarchical phrase-based
translation. Computational Linguistics, 33(2):201?
228.
Chiang, David. 2010. Learning to translate with
source and target syntax. In Proc. of ACL 2010.
Collins, Michael. 2003. Head-driven statistical mod-
els for natural language parsing. Computational
Linguistics, 29(4):589?637.
Eisner, Jason. 2003. Learning non-isomorphic tree
mappings for machine translation. In Proc. of ACL
2003.
Galley, Michel, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation rule?
In Proc. of NAACL 2004.
Galley, Michel, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proc.
of ACL 2006.
Huang, Liang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proc. of AMTA 2006.
Huang, Liang, Wenbin Jiang, and Qun Liu. 2009a.
Bilingually-constrained (monolingual) shift-reduce
parsing. In Proc. of EMNLP 2009.
Huang, Liang, Hao Zhang, Daniel Gildea, and Kevin
Knight. 2009b. Binarization of synchronous
context-free grammars. Computational Linguistics,
35(4):559?595.
Huang, Liang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proc. of ACL
2008.
Liu, Yang, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment template for statistical machine
translation. In Proc. of ACL 2006.
Liu, Yang, Yajuan Lu?, and Qun Liu. 2009. Improving
tree-to-tree translation with packed forests. In Proc.
of ACL 2009.
Melamed, I. Dan. 2004. Statistical machine transla-
tion by parsing. In Proc. of ACL 2004.
Mi, Haitao, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proc. of ACL 2008.
Och, Franz J. and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29(1):19?51.
Och, Franz. 2003. Minimum error rate training in sta-
tistical machine translation. In Proc. of ACL 2003.
Petrov, Slav and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proc. of NAACL 2007.
Quirk, Chris and Simon Corston-Oliver. 2006. The
impact of parsing quality on syntactically-informed
statistical machine translation. In Proc. of EMNLP
2006.
Quirk, Chris, Arul Menezes, and Colin Cherry. 2005.
Dependency treelet translation: Syntactically in-
formed phrasal SMT. In Proc. of ACL 2005.
Shen, Libin, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proc. of ACL 2008.
Smith, David and Noah Smith. 2004. Bilingual pars-
ing with factored estimation: using english to parse
korean. In Proc. of EMNLP 2004.
Stolcke, Andreas. 2002. Srilm - an extension language
model modeling toolkit. In Proc. of ICSLP 2002.
Xiao, Xinyan, Yang Liu, Young-Sook Hwang, Qun
Liu, and Shouxun Lin. 2010. Joint tokenization
and translation. In Proc. of COLING 2010.
Xiong, Deyi, Shuanglong Li, Qun Liu, and Shouxun
Lin. 2005. Parsing the penn chinese treebank with
semantic knowledge. In Proc. of IJCNLP 2005.
Yamada, Kenji and Kevin Knight. 2001. A syntax-
based statistical translation model. In Proc. of ACL
2001.
Zhang, Hao, Liang Huang, Daniel Gildea, and Kevin
Knight. 2006. Synchronous binarization for ma-
chine translatio. In Proc. of NAACL 2007.
Zhang, Min, Hongfei Jiang, Aiti Aw, Haizhou Li,
Chew Lim Tan, and Sheng Li. 2008. A tree
sequence alignment-based tree-to-tree translation
model. In Proc. of ACL 2008.
Zhao, Bing and Yaser Al-Onaizan. 2008. General-
izing local and non-local word-reordering patterns
for syntax-based machine translation. In Proc. of
EMNLP 2008.
715
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1092?1100,
Beijing, August 2010
Dependency Forest for Statistical Machine Translation
Zhaopeng Tu ? Yang Liu ? Young-Sook Hwang ? Qun Liu ? Shouxun Lin ?
?Key Lab. of Intelligent Info. Processing ?HILab Convergence Technology Center
Institute of Computing Technology C&I Business
Chinese Academy of Sciences SKTelecom
{tuzhaopeng,yliu,liuqun,sxlin}@ict.ac.cn yshwang@sktelecom.com
Abstract
We propose a structure called dependency
forest for statistical machine translation.
A dependency forest compactly represents
multiple dependency trees. We develop
new algorithms for extracting string-to-
dependency rules and training depen-
dency language models. Our forest-based
string-to-dependency system obtains sig-
nificant improvements ranging from 1.36
to 1.46 BLEU points over the tree-based
baseline on the NIST 2004/2005/2006
Chinese-English test sets.
1 Introduction
Dependency grammars have become increasingly
popular in syntax-based statistical machine trans-
lation (SMT). One important advantage of depen-
dency grammars is that they directly capture the
dependencies between words, which are key to re-
solving most parsing ambiguities. As a result, in-
corporating dependency trees proves to be effec-
tive in improving statistical machine translation
(Quirk et al, 2005; Ding and Palmer, 2005; Shen
et al, 2008).
However, most dependency-based translation
systems suffer from a major drawback: they only
use 1-best dependency trees for rule extraction,
dependency language model training, and decod-
ing, which potentially introduces translation mis-
takes due to the propagation of parsing errors
(Quirk and Corston-Oliver, 2006). While the
treelet system (Quirk et al, 2005) takes a de-
pendency tree as input, the string-to-dependency
system (Shen et al, 2008) decodes on a source-
language string. However, as we will show, the
string-to-dependency system still commits to us-
ing degenerate rules and dependency language
models learned from noisy 1-best trees.
To alleviate this problem, an obvious solu-
tion is to offer more alternatives. Recent studies
have shown that SMT systems can benefit from
widening the annotation pipeline: using packed
forests instead of 1-best trees (Mi and Huang,
2008), word lattices instead of 1-best segmenta-
tions (Dyer et al, 2008), and weighted alignment
matrices instead of 1-best alignments (Liu et al,
2009).
Along the same direction, we propose a struc-
ture called dependency forest, which encodes ex-
ponentially many dependency trees compactly, for
dependency-based translation systems. In this pa-
per, we develop two new algorithms for extracting
string-to-dependency rules and for training depen-
dency language models, respectively. We show
that using the rules and dependency language
models learned from dependency forests leads to
consistent and significant improvements over that
of using 1-best trees on the NIST 2004/2005/2006
Chinese-English test sets.
2 Background
Figure 1 shows a dependency tree of an English
sentence he saw a boy with a telescope. Arrows
point from the child to the parent, which is often
referred to as the head of the child. For example,
in Figure 1, saw is the head of he. A dependency
tree is more compact than its constituent counter-
part because there is no need to build a large su-
perstructure over a sentence.
Shen et al (2008) propose a novel string-to-
dependency translation model that features two
important advantages. First, they define that
a string-to-dependency rule must have a well-
formed dependency structure on the target side,
which makes efficient dynamic programming pos-
sible and manages to retain most useful non-
constituent rules. A well-formed structure can be
either fixed or floating . A fixed structure is a
1092
saw
he boy with
a telescope
a
he saw a boy with a telescope
ta kandao yige dai wangyuanjing de nanhai
Figure 1: A training example for tree-based rule
extraction.
dependency tree with all the children complete.
Floating structures consist of sibling nodes of a
common head, but the head itself is unspecified
or floating. For example, Figure 2(a) and Figure
2(b) are two fixed structures while Figure 2(c) is a
floating one.
Formally, for a given sentence w1:l = w1 . . . wl,
d1 . . . dl represent the parent word IDs for each
word. If wi is a root, we define di = 0.
Definition 1. A dependency structure di..j is fixed
on head h, where h /? [i, j], or fixed for short, if
and only if it meets the following conditions
? dh /? [i, j]
? ?k ? [i, j] and k 6= h, dk ? [i, j]
? ?k /? [i, j], dk = h or dk /? [i, j]
Definition 2. A dependency structure di..j is
floating with children C, for a non-empty set C
? {i, ..., j}, or floating for short, if and only if it
meets the following conditions
? ?h /? [i, j], s.t.?k ? C, dk = h
? ?k ? [i, j] and k /? C, dk ? [i, j]
? ?k /? [i, j], dk /? [i, j]
A dependency structure is well-formed if and
only if it is either fixed or floating.
2.1 Tree-based Rule Extraction
Figure 1 shows a training example consisting of an
English dependency tree, its Chinese translation,
boy
a
(a)
with
telescope
a
(b)
boy with
a telescope
a
(c)
Figure 2: Well-formed dependency structures cor-
responding to Figure 1. (a) and (b) are fixed and
(c) is floating.
and the word alignments between them. To facil-
itate identifying the correspondence between the
English and Chinese words, we also gives the En-
glish sentence. Extracting string-to-dependency
rules from aligned string-dependency pairs is sim-
ilar to extracting SCFG (Chiang, 2007) except that
the target side of a rule is a well-formed struc-
ture. For example, we can first extract a string-to-
dependency rule that is consistent with the word
alignment (Och and Ney, 2004):
with ((a) telescope) ? dai wangyuanjing de
Then a smaller rule
(a) telescope ? wangyuanjing
can be subtracted to obtain a rule with one non-
terminal:
with (X1) ? dai X1 de
where X is a non-terminal and the subscript indi-
cates the correspondence between non-terminals
on the source and target sides.
2.2 Tree-based Dependency Language Model
As dependency relations directly model the se-
mantics structure of a sentence, Shen et al (2008)
introduce dependency language model to better
account for the generation of target sentences.
Compared with the conventional n-gram language
models, dependency language model excels at
capturing non-local dependencies between words
(e.g., saw ... with in Figure 1). Given a depen-
dency tree, its dependency language model prob-
ability is a product of three sub-models defined
between headwords and their dependants. For ex-
ample, the probability of the tree in Figure 1 can
1093
saw0,7
he0,1 boy2,4 with4,7
a2,3 telescope5,7
a5,6
(a)
saw0,7
he0,1 boy2,7
a2,3 with4,7
telescope5,7
a5,6
(b)
saw0,7
he0,1 boy2,4 boy2,7
with4,7
e1 e2
a2,3
e3 e4
telescope5,7
e5
a5,6
e6
(c)
Figure 3: (a) the dependency tree in Figure 1, (b) another dependency tree for the same sentence, and
(c) a dependency forest compactly represents the two trees.
be calculated as:
Prob = PT (saw)
?PL(he|saw-as-head)
?PR(boy|saw-as-head)
?PR(with|boy, saw-as-head)
?PL(a|boy-as-head)
?PR(telescope|with-as-head)
?PL(a|telescope-as-head)
where PT (x) is the probability of word x being
the root of a dependency tree. PL and PR are the
generative probabilities of left and right sides re-
spectively.
As the string-to-tree system relies on 1-best
trees for parameter estimation, the quality of rule
table and dependency language model might be
affected by parsing errors and therefore ultimately
results in translation mistakes.
3 Dependency Forest
We propose to encode multiple dependency trees
in a compact representation called dependency
forest, which offers an elegant solution to the
problem of parsing error propagation.
Figures 3(a) and 3(b) show two dependency
trees for the example English sentence in Figure
1. The prepositional phrase with a telescope could
either depend on saw or boy. Figure 3(c) is a
dependency forest compactly represents the two
trees by sharing common nodes and edges.
Each node in a dependency forest is a word.
To distinguish among nodes, we attach a span to
each node. For example, in Figure 1, the span of
the first a is (2, 3) because it is the third word in
the sentence. As the fourth word boy dominates
the node a2,3, it can be referred to as boy2,4. Note
that the position of boy itself is taken into consid-
eration. Similarly, the word boy in Figure 3(b) can
be represented as boy2,7.
The nodes in a dependency forest are connected
by hyperedges. While an edge in a dependency
tree only points from a dependent to its head, a
hyperedge groups all the dependants that have a
common head. For example, in Figure 3(c), the
hyperedge
e1: ?(he0,1, boy2,4,with4,7), saw0,7?
denotes that he0,1, boy2,4, and with4,7 are depen-
dants (from left to right) of saw0,7.
More formally, a dependency forest is a pair
?V,E?, where V is a set of nodes, and E
is a set of hyperedges. For a given sentence
w1:l = w1 . . . wl, each node v ? V is in the
form of wi,j , which denotes that w dominates
the substring from positions i through j (i.e.,
wi+1 . . . wj). Each hyperedge e ? E is a pair
?tails(e), head(e)?, where head(e) ? V is the
head and tails(e) ? V are its dependants.
A dependency forest has a structure of a hy-
pergraph such as packed forest (Klein and Man-
ning, 2001; Huang and Chiang, 2005). However,
while each hyperedge in a packed forest naturally
treats the corresponding PCFG rule probability as
its weight, it is challenging to make dependency
forest to be a weighted hypergraph because depen-
dency parsers usually only output a score, which
can be either positive or negative, for each edge
in a dependency tree rather than a hyperedge in a
1094
saw0,7
he0,1 boy2,4 boy2,7
with4,7
e1 e2
a2,3
e3 e4
telescope5,7
e5
a5,6
e6
he saw a boy with a telescope
ta kandao yige dai wangyuanjing de nanhai
Figure 4: A training example for forest-based rule
extraction.
dependency forest. For example, in Figure 3(a),
the scores for the edges he ? saw, boy ? saw,
and with ? saw could be 13, 22, and -12, respec-
tively.
To assign a probability to each hyperedge, we
can first obtain a positive number for a hyperedge
using the scores of the corresponding edges:1
c(e) = exp
(?
v?tails(e) s
(
v, head(e)
)
|tails(e)|
)
(1)
where c(e) is the count of a hyperedge e, head(e)
is a head, tails(e) is a set of dependants of the
head, v is one dependant, and s(v, head(e)) is the
score of an edge from v to head(e). For example,
the count of the hyperedge e1 in Figure 3(c) is
c(e1) = exp
(
13 + 22 ? 12
3
)
(2)
Then, the probability of a hyperedge can be ob-
tained by normalizing the count among all hyper-
edges with the same head collected from a training
corpus:
p(e) = c(e)?
e?:head(e?)=head(e) c(e?)
(3)
Therefore, we obtain a weighted dependency
forest in which each hyperedge has a probability.
1It is difficult to assign a probability to each hyperedge.
The current method is arbitrary, and we will improve it in the
future.
Algorithm 1 Forest-based Initial Phrase Extrac-
tion
Input: a source sentence ?, a forest F , an alignment a,
and k
Output: minimal initial phrase setR
1: for each node v ? V in a bottom-up order do
2: for each hyperedge e ? E and head(e) = v do
3: W ? ?
4: fixs? EnumFixed(v,modifiers(e))
5: floatings? EnumFloating(modifiers(e))
6: add structures fixs, floatings to W
7: for each ? ?W do
8: if ? is consistent with a then
9: generate a rule r
10: R.append(r)
11: keep k-best dependency structures for v
4 Forest-based Rule Extraction
In tree-based rule extraction, one just needs to first
enumerate all bilingual phrases that are consis-
tent with word alignment and then check whether
the dependency structures over the target phrases
are well-formed. However, this algorithm fails to
work in the forest scenario because there are usu-
ally exponentially many well-formed structures
over a target phrase.
The GHKM algorithm (Galley et al, 2004),
which is originally developed for extracting tree-
to-string rules from 1-best trees, has been suc-
cessfully extended to packed forests recently (Mi
and Huang, 2008). The algorithm distinguishes
between minimal and composed rules. Although
there are exponentially many composed rules, the
number of minimal rules extracted from each node
is rather limited (e.g., one or zero). Therefore, one
can obtain promising composed rules by combin-
ing minimal rules.
Unfortunately, the GHKM algorithm cannot be
applied to extracting string-to-dependency rules
from dependency forests. This is because the
GHKM algorithm requires a complete subtree to
exist in a rule while neither fixed nor floating de-
pendency structures ensure that all dependants of
a head are included. For example, the floating
structure shown in Figure 2(c) actually contains
two trees.
Alternatively, our algorithm searches for well-
formed structures for each node in a bottom-up
style. Algorithm 1 shows the algorithm for ex-
tracting initial phrases, that is, rules without non-
1095
terminals from dependency forests. The algorithm
maintains k-best well-formed structures for each
node (line 11). The well-formed structures of a
head can be constructed from those of its depen-
dants. For example, in Figure 4, as the fixed struc-
ture rooted at telescope5,7 is
(a) telescope
we can obtain a fixed structure rooted for the node
with4,7 by attaching the fixed structure of its de-
pendant to the node (EnumFixed in line 4). Figure
2(b) shows the resulting fixed structure.
Similarly, the floating structure for the node
saw0,7 can be obtained by concatenating the fixed
structures of its dependants boy2,4 and with4,7
(EnumFloating in line 5). Figure 2(c) shows the
resulting fixed structure. The algorithm is similar
to Wang et al (2007), which binarize each con-
stituent node to create some intermediate nodes
that correspond to the floating structures.
Therefore, we can find k-best fixed and float-
ing structures for a node in a dependency forest
by manipulating the fixed structures of its depen-
dants. Then we can extract string-to-dependency
rules if the dependency structures are consistent
with the word alignment.
How to judge a well-formed structure extracted
from a node is better than others? We follow Mi
and Huang (2008) to assign a fractional count to
each well-formed structure. Given a tree fragment
t, we use the inside-outside algorithm to compute
its posterior probability:
??(t) = ?(root(t)) ?
?
e?t
p(e)
?
?
v?leaves(t)
?(v) (4)
where root(t) is the root of the tree, e is an edge,
leaves(t) is a set of leaves of the tree, ?(?) is out-
side probability, and ?(?) is inside probability.
For example, the subtree rooted at boy2,7 in Fig-
ure 4 has the following posterior probability:
?(boy2,7) ? p(e4) ? p(e5)
?p(e6) ? ?(a2,3) ? ?(a5,6) (5)
Now the fractional count of the subtree t is
c(t) = ??(t)??(TOP ) (6)
where TOP denotes the root node of the forest.
As a well-formed structure might be non-
constituent, we approximate the fractional count
by taking that of the minimal constituent tree frag-
ment that contains the well-formed structure. Fi-
nally, the fractional counts of well-formed struc-
tures can be used to compute the relative frequen-
cies of the rules having them on the target side (Mi
and Huang, 2008):
?(r|lhs(r)) = c(r)?
r?:lhs(r?)=lhs(r) c(r?)
(7)
?(r|rhs(r)) = c(r)?
r?:rhs(r?)=rhs(r) c(r?)
(8)
Often, our approach extracts a large amount of
rules from training corpus as we usually retain ex-
ponentially many well-formed structures over a
target phrase. To maintain a reasonable rule ta-
ble size, we discard any rule that has a fractional
count lower that a threshold t.
5 Forest-based Dependency Language
Model Training
Dependency language model plays an important
role in string-to-dependency system. Shen et
al. (2008) show that string-to-dependency system
achieves 1.48 point improvement in BLEU along
with dependency language model, while no im-
provement without it. However, the string-to-
dependency system still commits to using depen-
dency language model from noisy 1-best trees.
We now turn to dependency forest for it encodes
multiple dependency trees.
To train a dependency language model from a
dependency forest, we need to collect all heads
and their dependants. This can be easily done by
enumerating all hyperedges. Similarly, we use the
inside-outside algorithm to compute the posterior
probability of each hyperedge e,
??(e) = ?(head(e)) ? p(e)
?
?
v?tailes(e)
?(v) (9)
For example, the posterior probability of the hy-
peredge e2 in Figure 4 is calculated as
??(e2) = ?(saw0,7) ? p(e2)
??(he0,1) ? ?(boy2,7) (10)
1096
Rule DepLM NIST 2004 NIST 2005 NIST 2006 time
tree tree 33.97 30.21 30.73 19.6
tree forest 34.42? 31.06? 31.37? 24.1
forest tree 34.60? 31.16? 31.45? 21.7
forest forest 35.33?? 31.57?? 32.19?? 28.5
Table 1: BLEU scores and average decoding time (second/sentence) on the Chinese-English test sets.
The baseline system (row 2) used the rule table and dependency language model learned both from
1-best dependency trees. We use ? *? and ?**? to denote a result is better than baseline significantly at
p < 0.05 and p < 0.01, respectively.
Then, we can obtain the fractional count of a
hyperedge e,
c(e) = ??(e)??(TOP ) (11)
Each n-gram (e.g., ?boy-as-head a?) is assigned
the same fractional count of the hyperedge it be-
longs to.
We also tried training dependency language
model as in (Shen et al, 2008), which means
all hyperedges were on equal footing without re-
garding probabilities. However, the performance
is about 0.8 point lower in BLEU. One possbile
reason is that hyperedges with probabilities could
distinguish high quality structures better.
6 Experiments
6.1 Results on the Chinese-English Task
We used the FBIS corpus (6.9M Chinese words
+ 8.9M English words) as our bilingual train-
ing corpus. We ran GIZA++ (Och and Ney,
2000) to obtain word alignments. We trained a
4-gram language model on the Xinhua portion
of GIGAWORD corpus using the SRI Language
Modeling Toolkit (Stolcke, 2002) with modi-
fied Kneser-Ney smoothing (Kneser and Ney,
1995). We optimized feature weights using the
minimum error rate training algorithm (Och and
Ney, 2002) on the NIST 2002 test set. We evalu-
ated the translation quality using case-insensitive
BLEU metric (Papineni et al, 2002) on the NIST
2004/2005/2006 test sets.
To obtain dependency trees and forests, we
parsed the English sentences of the FBIS corpus
using a shift-reduce dependency parser that en-
ables beam search (Huang et al, 2009). We only
Rules Size New Rules
tree 7.2M -
forest 7.6M 16.86%
Table 2: Statistics of rules. The last column shows
the ratio of rules extracted from non 1-best parses
being used in 1-best derivations.
retained the best well-formed structure for each
node when extracting string-to-tree rules from de-
pendency forests (i.e., k = 1). We trained two
3-gram depLMs (one from trees and another from
forests) on English side of FBIS corpus plus 2M
sentence pairs from other LDC corpus.
After extracting rules and training depLMs, we
ran our replication of string-to-dependency sys-
tem (Shen et al, 2008) to translate the develop-
ment and test sets.
Table 1 shows the BLEU scores on the test
sets. The first column ?Rule? indicates where
the string-to-dependency rules are learned from:
1-best dependency trees or dependency forests.
Similarly, the second column ?DepLM? also dis-
tinguish between the two sources for training de-
pendency language models. The baseline sys-
tem used the rule table and dependency lan-
guage model both learned from 1-best depen-
dency trees. We find that adding the rule table and
dependency language models obtained from de-
pendency forests improves string-to-dependency
translation consistently and significantly, ranging
from +1.3 to +1.4 BLEU points. In addition, us-
ing the rule table and dependency language model
trained from forest only increases decoding time
insignificantly.
How many rules extracted from non 1-best
1097
Rule DepLM BLEU
tree tree 22.31
tree forest 22.73?
forest tree 22.80?
forest forest 23.12??
Table 3: BLEU scores on the Korean-Chinese test
set.
parses are used by the decoder? Table 2 shows the
number of rules filtered on the test set. We observe
that the rule table size hardly increases. One pos-
sible reason is that we only keep the best depen-
dency structure for each node. The last row shows
that 16.86% of the rules used in 1-best deriva-
tions are extracted from non 1-best parses in the
forests, indicating that some useful rules cannot
be extracted from 1-best parses.
6.2 Results on the Korean-Chinese Task
To examine the efficacy of our approach on differ-
ent language pairs, we carried out an experiment
on Korean-Chinese translation. The training cor-
pus contains about 8.2M Korean words and 7.3M
Chinese words. The Chinese sentences were used
to train a 5-gram language model as well as a 3-
gram dependency language model. Both the de-
velopment and test sets consist of 1,006 sentences
with single reference. Table 3 shows the BLEU
scores on the test set. Again, our forest-based ap-
proach achieves significant improvement over the
baseline (p < 0.01).
6.3 Effect of K-best
We investigated the effect of different k-best
structures for each node on translation quality
(BLEU scores on the NIST 2005 set) and the rule
table size (filtered for the tuning and test sets), as
shown in Figure 5. To save time, we extracted
rules just from the first 30K sentence pairs of the
FBIS corpus. We trained a language model and
depLMs on the English sentences. We used 10
different k: 1, 2, 3, 4, 5, 6, 7, 8, 9 and 10. Ob-
viously, the higher the k is, the more rules are
extracted. When k=10, the number of rules used
on the tuning and test sets was 1,299,290 and the
BLEU score was 20.88. Generally, both the num-
ber of rules and the BLEU score went up with
20.4
20.5
20.6
20.7
20.8
20.9
21.0
21.1
21.2
21.3
21.4
21.5
21.6
21.7
21.8
0.95 1.00 1.05 1.10 1.15 1.20 1.25 1.30 1.35
BL
EU
 s
co
re
rule table size(M)
k=1,2,...,10
Figure 5: Effect of k-best on rule table size and
translation quality.
20.4
20.5
20.6
20.7
20.8
20.9
21.0
21.1
21.2
21.3
21.4
21.5
21.6
21.7
21.8
0.98 1.00 1.02 1.04 1.06 1.08 1.10
BL
EU
 s
co
re
rule table size(M)
t=1.0,0.9,...,0.1
Figure 6: Effect of pruning threshold on rule table
size and translation quality.
the increase of k. However, this trend did not
hold within the range [4,10]. We conjecture that
when retaining more dependency structures for
each node, low quality structures would be intro-
duced, resulting in much rules of low quality.
An interesting finding is that the rule table grew
rapidly when k is in range [1,4], while gradually
within the range [4,10]. One possible reason is
that there are limited different dependency struc-
tures in the spans with a maximal length of 10,
which the target side of rules cover.
6.4 Effect of Pruning Threshold
Figure 6 shows the effect of pruning threshold on
translation quality and the rule table size. We
retained 10-best dependency structures for each
node in dependency forests. We used 10 different
1098
pruning thresholds: 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7,
0.8, 0.9 and 1.0. Intuitively, the higher the prun-
ing threshold is, the less rules are extracted. When
t=0.1, the number of rules used on the tuning and
test sets was 1,081,841 and the BLEU score was
20.68.
Lots of rules are pruned when the pruning
threshold increases from 0.0 to 0.3 (around 20%).
After pruning away these rules, we achieved 0.6
point improvement in BLEU. However, when we
filtered more rules, the BLEU score went down.
Figures 5 and 6 show that using two parame-
ters that have to be hand-tuned achieves a small
improvement at the expense of an additional com-
plexity. To simplify the approach, we only keep
the best dependency structure for each node with-
out pruning any rule.
7 Related Works
While Mi and Huang (2008) and we both use
forests for rule extraction, there remain two ma-
jor differences. Firstly, Mi and Huang (2008) use
a packed forest, while we use a dependency forest.
Packed forest is a natural weighted hypergraph
(Klein and Manning, 2001; Huang and Chiang,
2005), for each hyperedge treats the correspond-
ing PCFG rule probability as its weight. However,
it is challenging to make dependency forest to be a
weighted hypergraph because dependency parsers
usually only output a score for each edge in a de-
pendency tree rather than a hyperedge in a depen-
dency forest. Secondly, The GHKM algorithm
(Galley et al, 2004), which is originally devel-
oped for extracting tree-to-string rules from 1-best
trees, has been successfully extended to packed
forests recently (Mi and Huang, 2008). Unfor-
tunately, the GHKM algorithm cannot be applied
to extracting string-to-dependency rules from de-
pendency forests, because the GHKM algorithm
requires a complete subtree to exist in a rule while
neither fixed nor floating dependency structures
ensure that all dependants of a head are included.
8 Conclusion and Future Work
In this paper, we have proposed to use dependency
forests instead of 1-best parses to extract string-to-
dependency tree rules and train dependency lan-
guage models. Our experiments show that our ap-
proach improves translation quality significantly
over a state-of-the-art string-to-dependency sys-
tem on various language pairs and test sets. We
believe that dependency forest can also be used to
improve the dependency treelet system (Quirk et
al., 2005) that takes 1-best trees as input.
Acknowledgement
The authors were supported by SK Telecom C&I
Business, and National Natural Science Founda-
tion of China, Contracts 60736014 and 60903138.
We thank the anonymous reviewers for their in-
sightful comments. We are also grateful to Wen-
bin Jiang for his invaluable help in dependency
forest.
References
Chiang, David. 2007. Hierarchical phrase-based
translation. Computational Linguistics, pages 201?
228.
Ding, Yuan and Martha Palmer. 2005. Machine trans-
lation using probabilistic synchronous dependency
insertion grammars. In Proceedings of ACL.
Dyer, Christopher, Smaranda Muresan, and Philip
Resnik. 2008. Generalizing word lattice transla-
tion. In Proceedings of ACL.
Galley, Michel, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation rule?
In Proceedings of NAACL.
Huang, Liang and David Chiang. 2005. Better k-best
parsing. In Proceedings of IWPT.
Huang, Liang, Wenbin Jiang, and Qun Liu. 2009.
Bilingually-constrained (monolingual) shift-reduce
parsing. In Proceedings of EMNLP.
Klein, Dan and Christopher D. Manning. 2001. Pars-
ing and hypergraphs. In Proceedings of IWPT.
Kneser, R. and H. Ney. 1995. Improved backing-off
for m-gram language modeling. In Proceedings of
Acoustics, Speech, and Signal.
Liu, Yang, Tian Xia, Xinyan Xiao, and Qun Liu. 2009.
Weighted alignment matrices for statistical machine
translation. In Proceedings of EMNLP.
Mi, Haitao and Liang Huang. 2008. Forest-based
translation rule extraction. In Proceedings of
EMNLP.
1099
Och, Franz J. and Hermann Ney. 2000. Improved sta-
tistical alignment models. In Proceedings of ACL.
Och, Franz J. and Hermann Ney. 2002. Discriminative
training and maximum entropy models for statistical
machine translation. In Proceedings of ACL.
Och, Franz J. and Hermann Ney. 2004. The alignment
template approach to statistical machine translation.
Computational Linguistics, 30(4):417?449.
Papineni, Kishore, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings
of ACL.
Quirk, Chris and Simon Corston-Oliver. 2006. The
impact of parsing quality on syntactically-informed
statistical machine translation. In Proceedings of
EMNLP.
Quirk, Chris, Arul Menezes, and Colin Cherry. 2005.
Dependency treelet translation: syntactically in-
formed phrasal smt. In Proceedings of ACL.
Shen, Libin, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proceedings of ACL.
Stolcke, Andreas. 2002. Srilm - an extensible lan-
guage modeling toolkit. In Proceedings of ICSLP.
Wang, Wei, Kevin Knight, and Daniel Marcu. 2007.
Binarizing syntax trees to improve syntax-based
machine translation accuracy. In Proceedings of
EMNLP.
1100
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1200?1208,
Beijing, August 2010
Joint Tokenization and Translation
Xinyan Xiao ? Yang Liu ? Young-Sook Hwang ? Qun Liu ? Shouxun Lin ?
?Key Lab. of Intelligent Info. Processing ?HILab Convergence Technology Center
Institute of Computing Technology C&I Business
Chinese Academy of Sciences SKTelecom
{xiaoxinyan,yliu,liuqun,sxlin}@ict.ac.cn yshwang@sktelecom.com
Abstract
As tokenization is usually ambiguous for
many natural languages such as Chinese
and Korean, tokenization errors might po-
tentially introduce translation mistakes for
translation systems that rely on 1-best to-
kenizations. While using lattices to of-
fer more alternatives to translation sys-
tems have elegantly alleviated this prob-
lem, we take a further step to tokenize
and translate jointly. Taking a sequence
of atomic units that can be combined to
form words in different ways as input, our
joint decoder produces a tokenization on
the source side and a translation on the
target side simultaneously. By integrat-
ing tokenization and translation features
in a discriminative framework, our joint
decoder outperforms the baseline trans-
lation systems using 1-best tokenizations
and lattices significantly on both Chinese-
English and Korean-Chinese tasks. In-
terestingly, as a tokenizer, our joint de-
coder achieves significant improvements
over monolingual Chinese tokenizers.
1 Introduction
Tokenization plays an important role in statistical
machine translation (SMT) because tokenizing a
source-language sentence is always the first step
in SMT systems. Based on the type of input, Mi
and Huang (2008) distinguish between two cat-
egories of SMT systems : string-based systems
(Koehn et al, 2003; Chiang, 2007; Galley et al,
source
target
tokenize+translate
string tokenization
translation
source
target
string
tokenize
tokenization
translate
translation
(a)
(b)
Figure 1: (a) Separate tokenization and translation and (b)
joint tokenization and translation.
2006; Shen et al, 2008) that take a string as input
and tree-based systems (Liu et al, 2006; Mi et al,
2008) that take a tree as input. Note that a tree-
based system still needs to first tokenize the input
sentence and then obtain a parse tree or forest of
the sentence. As shown in Figure 1(a), we refer to
this pipeline as separate tokenization and transla-
tion because they are divided into single steps.
As tokenization for many languages is usually
ambiguous, SMT systems that separate tokeniza-
tion and translation suffer from a major drawback:
tokenization errors potentially introduce transla-
tion mistakes. As some languages such as Chi-
nese have no spaces in their writing systems, how
to segment sentences into appropriate words has
a direct impact on translation performance (Xu et
al., 2005; Chang et al, 2008; Zhang et al, 2008).
In addition, although agglutinative languages such
as Korean incorporate spaces between ?words?,
which consist of multiple morphemes, the gran-
ularity is too coarse and makes the training data
1200
considerably sparse. Studies reveal that seg-
menting ?words? into morphemes effectively im-
proves translating morphologically rich languages
(Oflazer, 2008). More importantly, a tokenization
close to a gold standard does not necessarily leads
to better translation quality (Chang et al, 2008;
Zhang et al, 2008). Therefore, it is necessary
to offer more tokenizations to SMT systems to
alleviate the tokenization error propagation prob-
lem. Recently, many researchers have shown that
replacing 1-best tokenizations with lattices im-
proves translation performance significantly (Xu
et al, 2005; Dyer et al, 2008; Dyer, 2009).
We take a next step towards the direction of
offering more tokenizations to SMT systems by
proposing joint tokenization and translation. As
shown in Figure 1(b), our approach tokenizes
and translates jointly to find a tokenization and
a translation for a source-language string simul-
taneously. We integrate translation and tokeniza-
tion models into a discriminative framework (Och
and Ney, 2002), within which tokenization and
translation models interact with each other. Ex-
periments show that joint tokenization and trans-
lation outperforms its separate counterparts (1-
best tokenizations and lattices) significantly on
the NIST 2004 and 2005 Chinese-English test
sets. Our joint decoder also reports positive results
on Korean-Chinese translation. As a tokenizer,
our joint decoder achieves significantly better to-
kenization accuracy than three monolingual Chi-
nese tokenizers.
2 Separate Tokenization and Translation
Tokenization is to split a string of characters into
meaningful elements, which are often referred to
as words. Typically, machine translation sepa-
rates tokenization from decoding as a preprocess-
ing step. An input string is first preprocessed by a
tokenizer, and then is translated based on the tok-
enized result. Take the SCFG-based model (Chi-
ang, 2007) as an example. Given the character
sequence of Figure 2(a), a tokenizer first splits it
into the word sequence as shown in Figure 2(b),
then the decoder translates the word sequence us-
ing the rules in Table 1.
This approach makes the translation process
simple and efficient. However, it may not be
? ? ? ? ? ? 0? 1 2 3 4 5 6 7
Figure 2: Chinese tokenization: (a) character sequence; (b)
and (c) tokenization instances; (d) lattice created from (b)
and (c). We insert ?-? between characters in a word just for
clarity.
r1 tao-fei-ke ?Taufik
r2 duo fen ? gain a point
r3 x1 you-wang x2 ? x1 will have the chance to x2
Table 1: An SCFG derivation given the tokenization of Fig-
ure 2(b).
optimal for machine translation. Firstly, optimal
granularity is unclear for machine translation. We
might face severe data sparseness problem by us-
ing large granularity, while losing much useful in-
formation with small one. Consider the example
in Figure 2. It is reasonable to split duo fen into
two words as duo and fen, since they have one-
to-one alignments to the target side. Nevertheless,
while you and wang also have one-to-one align-
ments, it is risky to segment them into two words.
Because the decoder is prone to translate wang as
a verb look without the context you. Secondly,
there may be tokenization errors. In Figure2(c),
tao fei ke is recognized as a Chinese person name
with the second name tao and the first name fei-ke,
but the whole string tao fei ke should be a name of
the Indonesian badminton player.
Therefore, it is necessary to offer more tok-
enizations to SMT systems to alleviate the tok-
enization error propagation problem. Recently,
many researchers have shown that replacing 1-
best tokenizations with lattices improves transla-
tion performance significantly. In this approach, a
lattice compactly encodes many tokenizations and
is fixed before decoding.
1201
0 1 2 3 4 5 6 7
1 2
3
Figure 3: A derivation of the joint model for the tokenization
in Figure 2(b) and the translation in Figure 2 by using the
rules in Table 1. N means tokenization while  represents
translation.
3 Joint Tokenization and Translation
3.1 Model
We take a next step towards the direction of of-
fering more tokenizations to SMT systems by
proposing joint tokenization and translation. As
shown in Figure 1(b), the decoder takes an un-
tokenized string as input, and then tokenizes the
source side string while building the correspond-
ing translation of the target side. Since the tradi-
tional rules like those in Table 1 natively include
tokenization information, we can directly apply
them for simultaneous construction of tokeniza-
tion and translation by the source side and target
side of rules respectively. In Figure 3, our joint
model takes the character sequence in Figure 2(a)
as input, and synchronously conducts both trans-
lation and tokenization using the rules in Table 1.
As our model conducts tokenization during de-
coding, we can integrate tokenization models as
features together with translation features under
the discriminative framework. We expect tok-
enization and translation could collaborate with
each other. Tokenization offers translation with
good tokenized results, while translation helps to-
kenization to eliminate ambiguity. Formally, the
probability of a derivation D is represented as
P (D) ?
?
i
?i(D)?i (1)
where ?i are features defined on derivations in-
cluding translation and tokenization, and ?i are
feature weights. We totally use 16 features:
? 8 traditional translation features (Chiang,
2007): 4 rule scores (direct and reverse trans-
lation scores; direct and reverse lexical trans-
lation scores); language model of the target
side; 3 penalties for word count, extracted
rule and glue rule.
? 8 tokenization features: maximum entropy
model, language model and word count of
the source side (Section 3.2). To handle
the Out Of Vocabulary (OOV) problem (Sec-
tion 3.3), we also introduce 5 OOV features:
OOV character count and 4 OOV discount
features.
Since our model is still a string-based model, the
CKY algorithm and cube pruning are still applica-
ble for our model to find the derivation with max
score.
3.2 Adding Tokenization Features
Maximum Entropy model (ME). We first intro-
duce ME model feature for tokenization by cast-
ing it as a labeling problem (Xue and Shen, 2003;
Ng and Low, 2004). We label a character with the
following 4 types:
? b: the begin of a word
? m: the middle of a word
? e: the end of a word
? s: a single-character word
Taking the tokenization you-wang of the string
you wang for example, we first create a label se-
quence b e for the tokenization you-wang and then
calculate the probability of tokenization by
P (you-wang | you wang)
= P (b e | you wang)
= P (b | you, you wang)
? P (e | wang, you wang)
Given a tokenization wL1 with L words for a
character sequence cn1 , we firstly create labels ln1
for every characters and then calculate the proba-
bility by
P (wL1 |cn1 ) = P (ln1 |cn1 ) =
n?
i=1
P (li|ci, cn1 ) (2)
1202
Under the ME framework, the probability of as-
signing the character c with the label l is repre-
sented as:
P (l|c, cn1 ) =
exp[?i ?ihi(l, c, cn1 )]?
l? exp[
?
i ?ihi(l?, c, cn1 )]
(3)
where hi is feature function, ?i is the feature
weight of hi. We use the feature templates the
same as Jiang et al, (2008) to extract features for
ME model. Since we directly construct tokeniza-
tion when decoding, it is straight to calculate the
ME model score of a tokenization according to
formula (2) and (3).
Language Model (LM). We also use the n-
gram language model to calculate the probability
of a tokenization wL1 :
P (wL1 ) =
L?
i=1
P (wi|wi?1i?n+1) (4)
For instance, we compute the probability of the
tokenization shown in Figure 2(b) under a 3-gram
model by
P (tao-fei-ke)
?P (you-wang | tao-fei-ke)
?P (duo | tao-fei-ke, you-wang)
?P (fen | you-wang, duo)
Word Count (WC). This feature counts the
number of words in a tokenization. Language
model is prone to assign higher probabilities to
short sentences in a biased way. This feature can
compensate this bias by encouraging long sen-
tences. Furthermore, using this feature, we can
optimize the granularity of tokenization for trans-
lation. If larger granularity is preferable for trans-
lation, then we can use this feature to punish the
tokenization containing more words.
3.3 Considering All Tokenizations
Obviously, we can construct the potential tok-
enizations and translations by only using the ex-
tracted rules, in line with traditional translation
decoding. However, it may limits the potential to-
kenization space. Consider a string you wang. If
you-wang is not reachable by the extracted rules,
the tokenization you-wang will never be consid-
ered under this way. However, the decoder may
still create a derivation by splitting the string as
small as possible with tokenization you wang and
translating you with a and wang with look, which
may hurt the translation performance. This case
happens frequently for named entity especially.
Overall, it is necessary to assure that the de-
coder can derive all potential tokenizations (Sec-
tion 4.1.3).
To assure that, when a span is not tokenized into
a single word by the extracted rules, we will add
an operation, which is considering the entire span
as an OOV. That is, we tokenize the entire span
into a single word with a translation that is the
copy of source side. We can define the set of all
potential tokenizations ?(cn1 ) for the character se-
quence cn1 in a recursive way by
?(cn1 ) =
n?1?
i
{?(ci1)
?
{w(cni+1)}} (5)
here w(cni+1) means a word contains characters
cni+1 and
?
means the times of two sets. Ac-
cording to this recursive definition, it is easy to
prove that all tokenizations is reachable by using
the glue rule (S ? SX,SX) and the added op-
eration. Here, glue rule is used to concatenate the
translation and tokenization of the two variables S
and X, which acts the role of the operator ? in
equation (5).
Consequently, this introduces a large number
of OOVs. In order to control the generation of
OOVs, we introduce the following OOV features:
OOV Character Count (OCC). This feature
counts the number of characters covered by OOV.
We can control the number of OOV characters by
this feature. It counts 3 when tao-fei-ke is an OOV,
since tao-fei-ke has 3 characters.
OOV Discount (OD). The chances to be OOVs
vary for words with different counts of characters.
We can directly attack this problem by adding
features ODi that reward or punish OOV words
which contains with i characters, or ODi,j for
OOVs contains with i to j characters. 4 OD fea-
tures are used in this paper: 1, 2, 3 and 4+. For
example, OD3 counts 1 when the word tao-fei-ke
is an OOV.
1203
Method Train #Rule Test TFs MT04 MT05 Speed
Separate
ICT 151M ICT ? 34.82 33.06 2.48
SF 148M SF ? 35.29 33.22 2.55
ME 141M ME ? 33.71 30.91 2.34
All 219M Lattice ? 35.79 33.95 3.83? 35.85 33.76 6.79
Joint
ICT 151M
Character
?
36.92 34.69 17.66
SF 148M 37.02 34.56 17.37
ME 141M 36.78 34.17 17.23
All 219M 37.25** 34.88** 17.52
Table 2: Comparison of Separate and Joint methods in terms of BLEU and speed (second per sentence). Columns Train
and Test represents the tokenization methods for training and testing respectively. Column TFs stands for whether the 8
tokenization features is used (?) or not (?). ICT, SF and ME are segmenter names for preprocessing. All means combined
corpus processed by the three segmenters. Lattice represent the system implemented as Dyer et al, (2008). ** means
significantly (Koehn, 2004) better than Lattice (p < 0.01).
4 Experiments
In this section, we try to answer the following
questions:
1. Does the joint method outperform conven-
tional methods that separate tokenization
from decoding. (Section 4.1)
2. How about the tokenization performance of
the joint decoder? (Section 4.2)
4.1 Translation Evaluation
We use the SCFG model (Chiang, 2007) for our
experiments. We firstly work on the Chinese-
English translation task. The bilingual training
data contains 1.5M sentence pairs coming from
LDC data.1 The monolingual data for training
English language model includes Xinhua portion
of the GIGAWORD corpus, which contains 238M
English words. We use the NIST evaluation sets
of 2002 (MT02) as our development data set, and
sets of 2004(MT04) and 2005(MT05) as test sets.
We use the corpus derived from the People?s Daily
(Renmin Ribao) in Feb. to Jun. 1998 containing
6M words for training LM and ME tokenization
models.
Translation Part. We used GIZA++ (Och and
Ney, 2003) to perform word alignment in both di-
rections, and grow-diag-final-and (Koehn et al,
2003) to generate symmetric word alignment. We
extracted the SCFG rules as describing in Chiang
(2007). The language model were trained by the
1including LDC2002E18, LDC2003E07, LDC2003E14,
Hansards portion of LDC2004T07, LDC2004T08 and
LDC2005T06
SRILM toolkit (Stolcke, 2002).2 Case insensitive
NIST BLEU (Papineni et al, 2002) was used to
measure translation performance.
Tokenization Part. We used the toolkit imple-
mented by Zhang (2004) to train the ME model.
Three Chinese word segmenters were used for
comparing: ICTCLAS (ICT) developed by insti-
tute of Computing Technology Chinese Academy
of Sciences (Zhang et al, 2003); SF developed at
Stanford University (Huihsin et al, 2005) and ME
which exploits the ME model described in section
(3.2).
4.1.1 Joint Vs. Separate
We compared our joint tokenization and trans-
lation with the conventional separate methods.
The input of separate tokenization and translation
can either be a single segmentation or a lattice.
The lattice combines the 1-best segmentations of
segmenters. Same as Dyer et al, (2008), we also
extracted rules from a combined bilingual corpus
which contains three copies from different seg-
menters. We refer to this version of rules as All.
Table 2 shows the result.3 Using all rule ta-
ble, our joint method significantly outperforms the
best single system SF by +1.96 and +1.66 points
on MT04 and MT05 respectively, and also out-
performs the lattice-based system by +1.46 and
+0.93 points. However, the 8 tokenization fea-
tures have small impact on the lattice system,
probably because the tokenization space limited
2The calculation of LM probabilities for OOVs is done
by the SRILM without special treatment by ourself.
3The weights are retrained for different test conditions, so
do the experiments in other sections.
1204
ME LM WC OCC OD MT05
? ? ? ? ? 24.97? ? ? ? ? 25.30
? ? ? ? ? 24.70
? ? ? ? ? 24.84
? ? ? ? ? 25.51
? ? ? ? ? 25.34
? ? ? ? ? 25.74? ? ? ? ?
26.37
Table 3: Effect of tokenization features on Chinese-English
translation task. ?
?
? denotes using a tokenization feature
while ??? denotes that it is inactive.
by lattice has been created from good tokeniza-
tion. Not surprisingly, our decoding method is
about 2.6 times slower than lattice method with
tokenization features, since the joint decoder takes
character sequences as input, which is about 1.7
times longer than the corresponding word se-
quences tokenized by segmenters. (Section 4.1.4).
The number of extracted rules with different
segment methods are quite close, while the All
version contains about 45% more rules than the
single systems. With the same rule table, our joint
method improves the performance over separate
method up to +3.03 and +3.26 points (ME). In-
terestingly, comparing with the separate method,
the tokenization of training data has smaller effect
on joint method. The BLEU scores of MT04 and
MT05 fluctuate about 0.5 and 0.7 points when ap-
plying the joint method, while the difference of
separate method is up to 2 and 3 points respec-
tively. It shows that the joint method is more ro-
bust to segmentation performance.
4.1.2 Effect of Tokenization Model
We also investigated the effect of tokenization
features on translation. In order to reduce the time
for tuning weights and decoding, we extracted
rules from the FBIS part of the bilingual corpus,
and trained a 4-gram English language model on
the English side of FBIS.
Table 3 shows the result. Only using the 8 trans-
lation features, our system achieves a BLEU score
of 24.97. By activating all tokenization features,
the joint decoder obtains an absolute improve-
ment by 1.4 BLEU points. When only adding
one single tokenization feature, the LM and WC
fail to show improvement, which may result from
their bias to short or long tokenizations. How-
Method BLEU #Word Grau #OOV
ICT 33.06 30,602 1.65 644
SF 33.22 30,119 1.68 882
ME 30.91 29,717 1.70 1,614
Lattice 33.95 30,315 1.66 494
JointICT 34.69 29,723 1.70 996
JointSF 34.56 29,839 1.69 972
JointME 34.17 29,771 1.70 1,062
JointAll 34.88 29,644 1.70 883
Table 4: Granularity (Grau, counts of character per word)
and counts of OOV words of different methods on MT05.
The subscript of joint means the type of rule table.
ever, these two features have complementary ad-
vantages and collaborate well when using them to-
gether (line 8). The OCC and OD features also
contribute improvements which reflects the fact
that handling the generation of OOV is important
for the joint model.
4.1.3 Considering All Tokenizations?
In order to explain the necessary of considering
all potential tokenizations, we compare the perfor-
mances of whether to tokenize a span as a single
word or not as illustrated in section 3.3. When
only tokenizing by the extracted rules, we obtain
34.37 BLEU on MT05, which is about 0.5 points
lower than considering all tokenizations shown in
Table 2. This indicates that spuriously limitation
of the tokenization space may degenerate transla-
tion performance.
4.1.4 Results Analysis
To better understand why the joint method can
improve the translation quality, this section shows
some details of the results on the MT05 data set.
Table 4 shows the granularity and OOV word
counts of different configurations. The lattice
method reduces the OOV words quite a lot which
is 23% and 70% comparing with ICT and ME. In
contrast, the joint method gain an absolute im-
provement even thought the OOV count do not
decrease. It seems the lattice method prefers to
translate more characters (since smaller granular-
ity and less OOVs), while our method is inclined
to maintain integrity of words (since larger granu-
larity and more OOVs). This also explains the dif-
ficulty of deciding optimal tokenization for trans-
lation before decoding.
There are some named entities or idioms that
1205
Method Type F1 Time
Monolingual
ICT 97.47 0.010
SF 97.48 0.007
ME 95.53 0.008
Joint
ICT 97.68 9.382
SF 97.68 10.454
ME 97.60 10.451
All 97.70 9.248
Table 5: Comparison of segmentation performance in terms
of F1 score and speed (second per sentence). Type column
means the segmenter for monolingual method, while repre-
sents the rule tables used by joint method.
are split into smaller granularity by the seg-
menters. For example:???? which is an English
name ?Stone? or ??-g -u? which means
?teenage?. Although the separate method is possi-
ble to translate them using smaller granularity, the
translation results are in fact wrong. In contrast,
the joint method tokenizes them as entire OOV
words, however, it may result a better translation
for the whole sentence.
We also count the overlap of the segments
used by the JointAll system towards the single
segmentation systems. The tokenization result
of JointAll contains 29, 644 words, and shares
28, 159 , 27, 772 and 27, 407 words with ICT ,
SF and ME respectively. And 46 unique words
appear only in the joint method, where most of
them are named entity.
4.2 Chinese Word Segmentation Evaluation
We also test the tokenization performance of our
model on Chinese word segmentation task. We
randomly selected 3k sentences from the corpus
of People?s Daily in Jan. 1998. 1k sentences
were used for tuning weights, while the other 2k
sentences were for testing. We use MERT (Och,
2003) to tune the weights by minimizing the error
measured by F1 score.
As shown in Table 5, with all features activated,
our joint decoder achieves an F1 score of 97.70
which reduces the tokenization error comparing
with the best single segmenter ICT by 8.7%. Sim-
ilar to the translation performance evaluation, our
joint decoder outperforms the best segmenter with
any version of rule tables.
Feature F1
TFs 97.37
TFs + RS 97.65
TFs + LM 97.67
TFs + RS + LM 97.62
All 97.70
Table 6: Effect of the target side information on Chinese
word segmentation. TFs stands for the 8 tokenization fea-
tures. All represents all the 16 features.
4.2.1 Effect of Target Side Information
We compared the effect of the 4 Rule Scores
(RS), target side Language Model (LM) on tok-
enization. Table 6 shows the effect on Chinese
word segmentation. When only use tokenization
features, our joint decoder achieves an F1 score
of 97.37. Only integrating language model or rule
scores, the joint decoder achieves an absolute im-
provement of 0.3 point in F1 score, which reduces
the error rate by 11.4%. However, when combin-
ing them together, the F1 score deduces slightly,
which may result from the weight tuning. Us-
ing all feature, the performance comes to 97.70.
Overall, our experiment shows that the target side
information can improve the source side tokeniza-
tion under a supervised way, and outperform state-
of-the-art systems.
4.2.2 Best Tokenization = Best Translation?
Previous works (Zhang et al, 2008; Chang et
al., 2008) have shown that preprocessing the in-
put string for decoder by better segmenters do
not always improve the translation quality, we re-
verify this by testing whether the joint decoder
produces good tokenization and good translation
at the same time. To answer the question, we
used the feature weights optimized by maximiz-
ing BLEU for tokenization and used the weights
optimized by maximizing F1 for translation. We
test BLEU on MT05 and F1 score on the test data
used in segmentation evaluation experiments. By
tuning weights regarding to BLEU (the configura-
tion for JointAll in table 2), our decoder achieves
a BLEU score of 34.88 and an F1 score of 92.49.
Similarly, maximizing F1 (the configuration for
the last line in table 6) leads to a much lower
BLEU of 27.43, although the F1 is up to 97.70.
This suggests that better tokenization may not al-
ways lead to better translations and vice versa
1206
Rule #Rule Method Test Time
Morph 46M Separate 21.61 4.12Refined 55M 21.21 4.63
All 74M Joint 21.93* 5.10
Table 7: Comparison of Separate and Joint method in terms
of BLEU score and decoding speed (second per sentence) on
Korean-Chinese translation task.
even by the joint decoding. This also indicates the
hard of artificially defining the best tokenization
for translation.
4.3 Korean-Chinese Translation
We also test our model on a quite different task:
Korean-Chinese. Korean is an agglutinative lan-
guage, which comes from different language fam-
ily comparing with Chinese.
We used a newswire corpus containing 256k
sentence pairs as training data. The development
and test data set contain 1K sentence each with
one single reference. We used the target side of
training set for language model training. The Ko-
rean part of these data were tokenized into mor-
pheme sequence as atomic unit for our experi-
ments.
We compared three methods. First is directly
use morpheme sequence (Morph). The second
one is refined data (Refined), where we use selec-
tive morphological segmentation (Oflazer, 2008)
for combining morpheme together on the training
data. Since the selective method needs alignment
information which is unavailable in the decod-
ing, the test data is still of morpheme sequence.
These two methods still used traditional decoding
method. The third one extracting rules from com-
bined (All) data of methods 1 and 2, and using
joint decoder to exploit the different granularity
of rules.
Table 7 shows the result. Since there is no gold
standard data for tokenization, we do not use ME
and LM tokenization features here. However, our
joint method can still significantly (p < 0.05) im-
prove the performance by about +0.3 points. This
also reflects the importance of optimizing granu-
larity for morphological complex languages.
5 Related Work
Methods have been proposed to optimize tok-
enization for word alignment. For example, word
alignment can be simplified by packing (Ma et al,
2007) several consecutive words together. Word
alignment and tokenization can also be optimized
by maximizing the likelihood of bilingual corpus
(Chung and Gildea, 2009; Xu et al, 2008). In fact,
these work are orthogonal to our joint method,
since they focus on training step while we are con-
cerned of decoding. We believe we can further
the performance by combining these two kinds of
work.
Our work also has connections to multilingual
tokenization (Snyder and Barzilay, 2008). While
they have verified that tokenization can be im-
proved by multilingual learning, our work shows
that we can also improve tokenization by collabo-
rating with translation task in a supervised way.
More recently, Liu and Liu (2010) also shows
the effect of joint method. They integrate parsing
and translation into a single step and improve the
performance of translation significantly.
6 Conclusion
We have presented a novel method for joint tok-
enization and translation which directly combines
the tokenization model into the decoding phase.
Allowing tokenization and translation to collab-
orate with each other, tokenization can be opti-
mized for translation, while translation also makes
contribution to tokenization performance under a
supervised way. We believe that our approach can
be applied to other string-based model such as
phrase-based model (Koehn et al, 2003), string-
to-tree model (Galley et al, 2006) and string-to-
dependency model (Shen et al, 2008).
Acknowledgement
The authors were supported by SK Telecom C&I
Business, and National Natural Science Founda-
tion of China, Contracts 60736014 and 60903138.
We thank the anonymous reviewers for their in-
sightful comments. We are also grateful to Wen-
bin Jiang, Zhiyang Wang and Zongcheng Ji for
their helpful feedback.
1207
References
Chang, Pi-Chuan, Michel Galley, and Christopher D.
Manning. 2008. Optimizing Chinese word segmen-
tation for machine translation performance. In the
Third Workshop on SMT.
Chiang, David. 2007. Hierarchical phrase-based
translation. Computational Linguistics, 33(2):201?
228.
Chung, Tagyoung and Daniel Gildea. 2009. Unsuper-
vised tokenization for machine translation. In Proc.
EMNLP 2009.
Dyer, Christopher, Smaranda Muresan, and Philip
Resnik. 2008. Generalizing word lattice transla-
tion. In Proc. ACL 2008.
Dyer, Chris. 2009. Using a maximum entropy model
to build segmentation lattices for mt. In Proc.
NAACL 2009.
Galley, Michel, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proc.
ACL 2006.
Huihsin, Tseng, Pichuan Chang, Galen Andrew,
Daniel Jurafsky, and Christopher Manning. 2005.
A conditional random field word segmenter. In
Fourth SIGHAN Workshop.
Jiang, Wenbin, Liang Huang, Qun Liu, and Yajuan Lu?.
2008. A cascaded linear model for joint chinese
word segmentation and part-of-speech tagging. In
Proc. ACL 2008.
Koehn, Philipp, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
HLT-NAACL 2003.
Koehn, Philipp. 2004. Statistical significance tests for
machine translation evaluation. In Proc. EMNLP
2004.
Liu, Yang and Qun Liu. 2010. Joint parsing and trans-
lation. In Proc. Coling 2010.
Liu, Yang, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment template for statistical machine
translation. In Proc. ACL 2006.
Ma, Yanjun, Nicolas Stroppa, and Andy Way. 2007.
Bootstrapping word alignment via word packing. In
Proc. ACL 2007.
Mi, Haitao, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proc. of ACL 2008.
Ng, Hwee Tou and Jin Kiat Low. 2004. Chinese part-
of-speech tagging: One-at-a-time or all-at-once?
word-based or character-based? In Proc. EMNLP
2004.
Och, Franz J. and Hermann Ney. 2002. Discriminative
training and maximum entropy models for statistical
machine translation. In Proc. ACL 2002.
Och, Franz Josef and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Och, Franz Josef. 2003. Minimum error rate train-
ing in statistical machine translation. In Proc. ACL
2003.
Oflazer, Kemal. 2008. Statistical machine translation
into a morphologically complex language. In Proc.
CICL 2008.
Papineni, Kishore, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. Bleu: a method for auto-
matic evaluation of machine translation. In Proc.
ACL 2002.
Shen, Libin, Xu Jinxi, and Weischedel Ralph. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proc. ACL 2008.
Snyder, Benjamin and Regina Barzilay. 2008. Un-
supervised multilingual learning for morphological
segmentation. In Proc. ACL 2008.
Stolcke, Andreas. 2002. Srilm ? an extensible lan-
guage modeling toolkit.
Xu, Jia, Evgeny Matusov, Richard Zens, and Her-
mann Ney. 2005. Integrated chinese word segmen-
tation in statistical machine translation. In Proc.
IWSLT2005.
Xu, Jia, Jianfeng Gao, Kristina Toutanova, and Her-
mann Ney. 2008. Bayesian semi-supervised
chinese word segmentation for statistical machine
translation. In Proc. Coling 2008.
Xue, Nianwen and Libin Shen. 2003. Chinese word
segmentation as LMR tagging. In SIGHAN Work-
shop.
Zhang, Hua-Ping, Hong-Kui Yu, De-Yi Xiong, and
Qun Liu. 2003. Hhmm-based chinese lexical an-
alyzer ictclas. In the Second SIGHAN Workshop.
Zhang, Ruiqiang, Keiji Yasuda, and Eiichiro Sumita.
2008. Improved statistical machine translation by
multiple Chinese word segmentation. In the Third
Workshop on SMT.
Zhang, Le. 2004. Maximum entropy modeling toolkit
for python and c++.
1208
Coling 2010: Poster Volume, pages 285?293,
Beijing, August 2010
An Efficient Shift-Reduce Decoding Algorithm for Phrased-Based
Machine Translation
Yang Feng, Haitao Mi, Yang Liu and Qun Liu
Key Laboratory of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences
{fengyang,htmi,yliu,liuqun}@ict.ac.cn
Abstract
In statistical machine translation, decod-
ing without any reordering constraint is
an NP-hard problem. Inversion Transduc-
tion Grammars (ITGs) exploit linguistic
structure and can well balance the needed
flexibility against complexity constraints.
Currently, translation models with ITG
constraints usually employs the cube-time
CYK algorithm. In this paper, we present
a shift-reduce decoding algorithm that can
generate ITG-legal translation from left to
right in linear time. This algorithm runs
in a reduce-eager style and is suited to
phrase-based models. Using the state-of-
the-art decoder Moses as the baseline, ex-
periment results show that the shift-reduce
algorithm can significantly improve both
the accuracy and the speed on different
test sets.
1 Introduction
In statistical machine translation, for the diver-
sity of natural languages, the word order of
source and target language may differ and search-
ing through all possible translations is NP-hard
(Knight, 1999). So some measures have to be
taken to reduce search space: either using a search
algorithm with pruning technique or restricting
possible reorderings.
Currently, beam search is widely used (Till-
mann and Ney, 2003; Koehn, 2004) to reduce
search space. However, the pruning technique
adopted by this algorithm is not risk-free. As a
result, the best partial translation may be ruled out
during pruning. The more aggressive the prun-
ing is, the more likely the best translation escapes.
There should be a tradeoff between the speed and
the accuracy. If some heuristic knowledge is em-
ployed to guide the search, the search algorithm
can discard some implausible hypotheses in ad-
vance and focus on more possible ones.
Inversion Transduction Grammars (ITGs) per-
mit a minimal extra degree of ordering flexibility
and are particularly well suited to modeling or-
dering shifts between languages (Wu, 1996; Wu,
1997). They can well balance the needed flex-
ibility against complexity constraints. Recently,
ITG has been successfully applied to statistical
machine translation (Zens and Ney, 2003; Zens
et al, 2004; Xiong et al, 2006). However, ITG
generally employs the expensive CYK parsing al-
gorithm which runs in cube time. In addition, the
CYK algorithm can not calculate language model
exactly in the process of decoding, as it can not
catch the full history context of the left words in a
hypothesis.
In this paper, we introduce a shift-reduce de-
coding algorithm with ITG constraints which runs
in a left-to-right manner. This algorithm parses
source words in the order of their corresponding
translations on the target side. In the meantime,
it gives all candidate ITG-legal reorderings. The
shift-reduce algorithm is different from the CYK
algorithm, in particular:
? It produces translation in a left-to-right man-
ner. As a result, language model probability
can be calculated more precisely in the light
of full history context.
? It decodes much faster. Applied with distor-
285
target side target side target side
(a) straight (b) inverted (c) discontinuous
Figure 1: Orientation of two blocks.
tion limit, shift-reduce decoding algorithm
can run in linear time, while the CYK runs
in cube time.
? It holds ITG structures generated during de-
coding. That is to say, it can directly give
ITG-legal spans, which leads to faster de-
coding. Furthermore, it can be extended to
syntax-based models.
We evaluated the performance of the shift-
reduce decoding algorithm by adding ITG con-
straints to the state-of-the-art decoder Moses. We
did experiments on three data sets: NIST MT08
data set, NIST MT05 data set and China Work-
shop on Machine Translation 2007 data set. Com-
pared to Moses, the improvements of the accuracy
are 1.59, 0.62, 0.8 BLEU score, respectively, and
the speed improvements are 15%, 24%, 30%, re-
spectively.
2 Decoding with ITG constraints
In this paper, we employ the shift-reduce algo-
rithm to add ITG constraints to phrase-based ma-
chine translation model. It is different from the
traditional shift-reduce algorithm used in natural
language parsing. On one hand, as natural lan-
guage parsing has to cope with a high degree of
ambiguity, it need take ambiguity into considera-
tion. As a result, the traditional one often suffers
shift-reduce divergence. Nonetheless, the shift-
reduce algorithm in this paper does not pay atten-
tion to ambiguity and acts in a reduce-eager man-
ner. On the other hand, the traditional algorithm
can not ensure that all reorderings observe ITG
constraints, so we have to modify the traditional
algorithm to import ITG constraints.
We will introduce the shift-reduce decoding al-
gorithm in the following two steps: First, we
1\1
zairu1
??2
shijian2
N3
diaocha3
]4
ziliaode4
>M5
diannao5
;?6
zaoqie6
The laptop with inquiry data on the event was stolen
(a)
A1
The laptop
diannao5
with
A2
zairu1
inquiry
A3
diaocha3
data
A4
ziliaode4
A5
on the event
shijian2
A6
was stolen
zaoqie6
A7
A8
A9
A10
A11
(b)
Figure 2: A Chinese-to-English sentence pair and
its corresponding ITG tree.
will deduce how to integrate the shift-reduce al-
gorithm and ITG constraints and show its correct-
ness (Section 2.1). Second, we will describe the
shift-reduce decoding algorithm in details (Sec-
tion 2.2).
2.1 Adding ITG constraints
In the process of decoding, a source phrase is re-
garded as a block and a source sentence is seen
as a sequence of blocks. The orientation of two
blocks whose translations are adjacent on the tar-
get side can be straight, inverted or discontinu-
ous, as shown in Figure 1. According to ITG,
two blocks which are straight or inverted can be
merged into a single block. For parsing, differ-
ent mergence order of a sequence of continuous
blocks may yield different derivations. In con-
trast, the phrase-based machine translation does
not compute reordering probabilities hierarchi-
cally, so the mergence order will not impact the
computation of reordering probabilities. As a
result, the shift-reduce decoding algorithm need
not take into consideration the shift-reduce diver-
gence. It merges two continuous blocks as soon
as possible, acting in a reduce-eager style.
Every ITG-legal sentence pair has a corre-
286
S zairu1 shijian2 diaocha3 ziliaode4 diannao5 zaoqie6
The laptop
S zairu1 shijian2 diaocha3 ziliaode4 diannao5 zaoqie6
The laptop with
S zairu1 shijian2 diaocha3 ziliaode4 diannao5 zaoqie6
The laptop with inquiry
(a) (b) (c)
S zairu1 shijian2 diaocha3 ziliaode4 diannao5 zaoqie6
The laptop with inquiry data
S zairu1 shijian2 diaocha3 ziliaode4 diannao5 zaoqie6
The laptop with inquiry data
S zairu1 shijian2 diaocha3 ziliaode4 diannao5 zaoqie6
The laptop with inquiry data on the event
(d) (e) (f)
 
S zairu1 shijian2 diaocha3 ziliaode4 diannao5 zaoqie6
The laptop with inquiry data on the event
S zairu1 shijian2 diaocha3 ziliaode4 diannao5 zaoqie6
The laptop with inquiry data on the event
S zairu1 shijian2 diaocha3 ziliaode4 diannao5 zaoqie6
The laptop with inquiry data on the event
(g) (h) (i)
Figure 3: The partial translation procedure of the sentence in Figure 2.
sponding ITG tree, and source words covered
by every node (eg. A1, ..., A11 in Figure 2(b))
in the ITG tree can be seen as a block. By
watching the tree in Figure 2, we can find that
a block must be adjacent to the block either on
its left or on its right, then they can be merged
into a larger block. For example, A2 matches
the block [zairu1] and A8 matches the block
[shijian2 diaocha3 ziliaode4].1 The two blocks
are adjacent and they are merged into a larger
block [zairu1 shijian2 diaocha3 ziliaode4],
covered by A9. The procedure of translating
zairu1 shijian2 diaocha3 ziliaode4 diannao5
is illustrated in Figure 3.
For a hypothesis during decoding, we assign it
three factors: the current block, the left neigh-
boring uncovered span and the right neighbor-
ing uncovered span. For example, in Figure
3(c), the current block is [diaocha3] and the left
neighboring uncovered span is [shijian2] and the
right neighboring uncovered span is [ziliaode4].
[zaoqie6] is not thought of as the right neighbor-
ing block, for it is not adjacent to [diaocha3]. The
next covered block is [ziliaode4] (as shown in
Figure 3(d)). For [diaocha3] and [ziliaode4] are
adjacent, they are merged. In Figure 3(e), the cur-
rent block is [diaocha3 ziliaode4].
A sentence is translated with ITG constraints iff
1The words within a block are sorted by their order in the
source sentence.
its source side can be covered by an ITG tree. That
is to say, for every hypothesis during decoding, the
next block to cover must be selected from the left
or right neighboring uncovered span.
First, we show that if the next block to cover is
selected in this way, the translation must observe
ITG constraints. For every hypothesis during de-
coding, the immediate left and right words of the
current block face the following three conditions:
(1) The immediately left word is not covered
and the immediately right word is covered, then
the next block to cover must be selected from the
left neighboring uncovered span, eg. for the cur-
rent block [diaocha3 ziliaode4] in Figure 3(e). In
this condition, the ITG tree can be constructed in
the following two ways: either all words in the left
neighboring uncovered span are translated first,
then this span is merged with the current span
(taking three nodes as an example, this case is
shown in Figure 4(a)), or the right part of the left
neighboring uncovered span is merged with the
current block first, then the new block is merged
with the rest part of the left neighboring uncov-
ered span (shown in Figure 4(b)). In a word, only
after all words in the left neighboring uncovered
span are covered, other words can be covered.
(2) The immediately right word is not covered
and the immediately left word is covered. Simi-
larly, only after all words in the right neighboring
uncovered span are covered, other words can be
287
(a) (b)
Figure 4: The two ways that the current block is
merged with its left neighboring uncovered span.
The third node in the first row denotes the current
block, the first and second nodes in the first row
denote left and right parts of the left neighboring
uncovered span, respectively.
covered.
(3) The immediately left and right words are
neither covered. The next block can be selected
from either the left or the right neighboring uncov-
ered span until the immediate left or right word is
covered.
The above operations can be performed recur-
sively until the whole source sentence is merged
into a single block, so the reordering observes ITG
constraints.
Now, we show that translation which is not gen-
erated in the above way must violate ITG con-
straints.
If the next block is selected out of the neighbor-
ing uncovered spans, the current block can be nei-
ther adjacent to the last covered block nor adjacent
to the selected next block, so the current block can
not be merged with any block and the whole sen-
tence can not be covered by an ITG tree. As in
Figure 3(b), if the next block to cover is [zaoqie6],
then [zairu1] is neither adjacent to [diannao5]
nor adjacent to [zaoqie6].
We can conclude that if we select the next block
from the left or right neighboring uncovered span
of the current block, then the translation must ob-
serve ITG constraints.
2.2 Shift-Reduce Decoding Algorithm
In order to generate the translation with ITG con-
straints, the shift-reduce algorithm have to keep
trace of covered blocks, left and right neighboring
uncovered spans. Formally, the shift-reduce de-
coding algorithm uses the following three stacks:
? St: the stack for covered blocks. The blocks
are pushed in the order that they are covered,
not the order that they are in the source sen-
tence.
? Sl : the stack for the left uncovered spans of
the current block. When a block is pushed
into St, its corresponding left neighboring
uncovered span is pushed into Sl.
? Sr :the stack for the right uncovered spans of
the current block. When a block is pushed
into St, its corresponding right neighboring
uncovered span is pushed into Sr.
A translation configuration is a triple c =
?St, Sl, Sr?. Given a source sentence f =
f1, f2, ..., fm, we import a virtual start word and
the whole translation procedure can be seen as
a sequence of transitions from cs to ct, where
cs = ?[0], ?, [1,m]? is the initial configura-
tion, ct = ?[0,m], ?, ?? is the terminal con-
figuration. The configuration for Figure 3 (e) is
?[0][5][1][3, 4], [2], [6]?.
We define three types of transitions from
a configuration to another . Assume the cur-
rent configuration c = ? [ft11, ft12]...[ftk1, ftk2],
[fl11, fl12]...[flu1, flu2], [frv1, frv2]...[fr11, fr12] ?,
then :
? Transitions LShift pop the top element
[flu1, flu2] from Sl and select a block [i, j]
from [flu1, flu2] to translate. In addition,
they push [i, j] into St, and if i 6= flu1, they
push [flu1, i ? 1] into Sl, and if j 6= flu2,
they push [j+1, flu2] into Sr. The precondi-
tion to operate the transition is that Sl is not
null and the top span of Sl is adjacent to the
top block of St. Formally, the precondition
is flu2 + 1 = ftk1.
? Transitions RShift pop the top element
[frv1, frv2] of Sr and select a block [i, j]
from [frv1, frv2] to translate. In addition,
they push [i, j] into St, and if i 6= frv1, they
push [frv1, i?1] into Sl, and if j 6= frv2, they
push [j + 1, frv2] into Sr. The precondition
is that Sr is not null and the top span of Sr is
288
adjacent to the top block of St. Formally, the
precondition is ftk2 + 1 = frv1.
? Transitions Reduce pop the top two blocks
[ftk?11, ftk?12], [ftk1, ftk2] from St and push
the merged span [ftk?11, ftk2] into St. The
precondition is that the top two blocks are ad-
jacent. Formally, the precondition is ftk?12+
1 = ftk1
The transition sequence of the example in Fig-
ure 2 is listed in Figure 5. For the purpose of
efficiency, transitions Reduce are integrated with
transitions LShift and RShift in practical imple-
mentation. Before transitions LShift and RShift
push [i, j] into St, they check whether [i, j] is ad-
jacent to the top block of St. If so, they change
the top block into the merged block directly.
In practical implementation, in order to further
restrict search space, distortion limit is applied be-
sides ITG constraints: a source phrase can be cov-
ered next only when it is ITG-legal and its distor-
tion does not exceed distortion limit. The distor-
tion d is calculated by d = |starti ? endi?1 ? 1|,
where starti is the start position of the current
phrase and endi?1 is the last position of the last
translated phrase.
3 Related Work
Galley and Manning (2008) present a hierarchi-
cal phrase reordering model aimed at improving
non-local reorderings. Via the hierarchical mer-
gence of two blocks, the orientation of long dis-
tance words can be computed. Their shift-reduce
algorithm does not import ITG constraints and ad-
mits the translation violating ITG constraints.
Zens et al (2004) introduce a left-to-
right decoding algorithm with ITG constraints
on the alignment template system (Och et al,
1999). Their algorithm processes candidate
source phrases one by one through the whole
search space and checks if the candidate phrase
complies with ITG constraints. Besides, their al-
gorithm checks validity via cover vector and does
not formalize ITG structure. The shift-reduce de-
coding algorithm holds ITG structure via three
stacks. As a result, it can offer ITG-legal spans
directly and decode faster. Furthermore, with
Transition St Sl Sr
[0] ? [1, 6]
RShift [0][5] [1, 4] [6]
LShift [0][5][1] ? [2, 4][6]
RShift [0][5][1][3] [2] [4][6]
RShift [0][5][1][3][4] [2] [6]
Reduce [0][5][1][3, 4] [2] [6]
LShift [0][5][1][3, 4][2] ? [6]
Reduce [0][5][1][2, 4] ? [6]
Reduce [0][5][1, 4] ? [6]
Reduce [0][1, 5] ? [6]
Reduce [0, 5] ? [6]
RShift [0, 5][6] ? ?
Reduce [0, 6] ? ?
Figure 5: Transition sequence for the example in
Figure 2. The top nine transitions correspond to
Figure 3 (a), ... , Figure 3 (i), respectively.
the help of ITG structure, it can be extended to
syntax-based models easily.
Xiong et al (2006) propose a BTG-based
model, which uses the context to determine the
orientation of two adjacent spans. It employs the
cube-time CYK algorithm.
4 Experiments
We compare the shift-reduce decoder with the
state-of-the-art decoder Moses (Koehn et al,
2007). The shift-reduce decoder was imple-
mented by modifying the normal search algo-
rithm of Moses to our shift-reduce algorithm,
without cube pruning (Huang and Chiang, 2005).
We retained the features of Moses: four trans-
lation features, three lexical reordering features
(straight, inverted and discontinuous), linear dis-
tortion, phrase penalty, word penalty and language
model, without importing any new feature. The
decoding configurations used by all the decoders,
including beam size, phrase table limit and so on,
were the same, so the performance was compared
fairly.
First, we will show the performance of shift-
reduce algorithm on three data sets with large
training data sets (Section 4.1). Then, we will
analyze the performance elaborately in terms of
accuracy, speed and search ability with a smaller
289
training data set (Section 4.2). All experiments
were done on Chinese-to-English translation tasks
and all results are reported with case insensitive
BLEU score. Statistical significance were com-
puted using the sign-test described in Collins et
al. (Collins et al, 2005).
4.1 Performance Evaluation
We did three experiments to compare the perfor-
mance of the shift-reduce decoder, Moses and the
decoder with ITG constraints using cover vector
(denoted as CV). 2 The shift-reduce decoder de-
coded with two sets of parameters: one was tuned
by itself (denoted as SR) and the other was tuned
by Moses (denoted as SR-same), using MERT
(Och, 2003). Two searching algorithms of Moses
are considered: one is the normal search algorithm
without cubing pruning (denoted as Moses), the
other is the search algorithm with cube pruning
(denoted as Moses-cb). For all the decoders, the
distortion limit was set to 6, the nbest size was set
to 100 and the phrase table limit was 50.
In the first experiment, the development set is
part of NIST MT06 data set including 862 sen-
tences, the test set is NIST MT08 data set and
the training data set contains 5 million sentence
pairs. We used a 5-gram language model which
were trained on the Xinhua and AFP portion of
the Gigaword corpus. The results are shown in
Table 1(a).
In the second experiment, the development data
set is NIST MT02 data set and the test set is NIST
MT05 data set. Language model and the training
data set are the same to that of the first experiment.
The result is shown in Table 1(b).
In the third experiment, the development set
is China Workshop on Machine Translation 2008
data set (denoted as CWMT08) and the test set
is China Workshop on Machine Translation 2007
data set (denoted as CWMT07). The training set
contains 2 Million sentence pairs and the language
model are a 6-gram language model trained on
the Reuter corpus and English corpus. Table 1(c)
gives the results.
In the above three experiments, SR decoder
2The decoder CV is implemented by adding the ITG con-
straints to Moses using the algorithm described in (Zens et
al., 2004).
NIST06 NIST08 speed
Moses 30.24 25.08 4.827
Moses-cb 30.27 23.80 1.501
CV 30.35 26.23** 4.335
SR-same ?? 25.09 3.856
SR 30.47 26.67** 4.126
(a)
NIST02 NIST05 speed
Moses 35.68 35.80 7.142
Moses-cb 35.42 35.03 1.811
CV 35.45 36.56** 6.276
SR-same ?? 35.84 5.008
SR 35.99* 36.42** 5.432
(b)
CWMT08 CWMT07 speed
Moses 27.75 25.91 3.061
Moses-cb 27.82 25.16 0.548
CV 27.71 26.58** 2.331
SR-same ?? 25.97 1.988
SR 28.14* 26.71** 2.106
(c)
Table 1: Performance comparison. Moses: Moses
without cube pruning, Moses-cb: Moses with
cube pruning, CV: the decoder using cover vector,
SR-same: the shift-reduce decoder decoding with
parameters tunes by Moses, SR: the shift-reduce
decoder with parameters tuned by itself. The sec-
ond column stands for develop set, the third col-
umn stands for test set and speed column shows
the average time (seconds) of translating one sen-
tence in the test set. **: significance at the .01
level.
improves the accuracy by 1.59, 0.62, 0.8 BLEU
score (p < .01), respectively, and improves the
speed by 15%, 24%, 30%, respectively. we can
see that SR can improve both the accuracy and
the speed while SR-same can increase the speed
significantly with a slight improvement on the ac-
curacy. As both SR and CV decode with ITG
constraints, they match each other on the accu-
290
27.00
27.50
28.00
28.50
29.00
29.50
30.00
 1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17
B
LE
U
average decoding speed (s)
d=-1
d=-1
d=-1
SR
SR-same
Moses
Figure 6: Performance comparison on NIST05.
For a curve, the dots correspond to distortion limit
4, 6, 8, 10, 14 and no distortion from left to right.
d = ?1 stands for no distortion limit.
racy. However, the speed of SR is faster than CV.
Cube pruning can improve decoding speed dra-
matically, but it is not risk-free pruning technol-
ogy, so the BLEU score declines obviously.
4.2 Performance Analysis
We make performance analysis with the same ex-
periment configuration as the second experiment
in Section 4.1, except that the training set in
the analysis experiment is FBIS corpus, includ-
ing 289k sentence pairs. In the following exper-
iments, Moses employs the normal search algo-
rithm without cube pruning.
For the decoders employ the linear distortion
feature, the distortion limit will influence the
translation accuracy. Besides, with different dis-
tortion limit, the proportion of ITG-legal transla-
tion generated by Moses will differ. The smaller
the distortion limit is, the greater the proportion is.
So we first compare the performance with differ-
ent distortion limit.
We compare the shift-reduce decoder with
Moses using different distortion limit. The re-
sults are shown in Figure 6. When distortion limit
is set to 6, every decoder gets a peak value and
SR has an improvement of 0.66 BLEU score over
Moses. From the curves, we can see that the
BLEU score of SR-same with distortion limit 8
28.00
28.50
29.00
29.50
30.00
30.50
31.00
31.50
32.00
32.50
33.00
33.50
34.00
34.50
35.00
35.50
36.00
36.50
37.00
 4  6  8  10  12  14  16
B
LE
U
distortion limit
SR
SR-same
Moses
(a) ITG set
25.00
25.50
26.00
26.50
27.00
27.50
28.00
 4  6  8  10  12  14  16
BL
EU
distortion limit
SR
SR-same
Moses
(b) rest set
Figure 7: Accuracy comparison on the ITG set
and rest set of NIST05. The ITG set includes the
sentences the translations of which generated by
Moses are ITG-legal, and the rest set contains the
rest sentences. distortion limit = 16 denotes no
distortion limit.
is lower than that of Mose with distortion limit
6. This is because the decoding speed of SR-
same with distortion limit 8 is not faster than that
of Moses with distortion limit 6. On the whole,
compared to Moses, SR-same can improve the ac-
curacy slightly with much faster decoding speed,
and SR can obtain improvements on both the ac-
curacy and the speed.
We split the test set into two sets: one contains
the sentences, the translations of which generated
by Moses are ITG-legal (denoted as ITG set) and
the other contains the rest (denoted as rest set).
From Figure 7, we can see that no matter on the
ITG set or on the rest set, SR decoder can gain ob-
vious accuracy improvements with all distortion
291
ITG restd
Moses SR-same total < = > Moses SR-same total < = >
4 28.67 28.68 1050 8 1042 0 25.61 25.82 32 0 0 32
6 31.34 31.42 758 51 705 2 25.78 25.72 324 32 2 290
8 32.59 32.93* 594 72 516 6 25.68 25.65 488 82 3 403
10 34.36 34.99** 456 80 365 11 26.04 26.50* 626 147 3 476
12 33.16 33.61** 454 63 380 11 27.01 27.13 628 165 1 462
14 35.98 36.25* 383 60 316 7 26.35 26.67* 699 203 1 495
-1 34.13 34.96** 351 39 308 4 26.17 26.78** 731 154 0 577
Table 2: Search ability comparison. The ITG set and the rest set of NIST05 were tested, respectively.
On the ITG set, the following six factors are reported from left to right: BLEU score of Moses, BLEU
score of SR-same, the number of sentences in the ITG set, the number of sentences the translation
probabilities of which computed by Moses, compared to that computed by SR, is lower, equal and
greater. The rest set goes similarly. *: significance at the .05 level, **: significance at the .01 level.
limit. While SR-same decoder only gets better re-
sults on the ITG set with all distortion limit. This
may result from the use of the linear distortion
feature. Moses may generate hypotheses the dis-
tortion of which is forbidden in the shift-reduce
decoder. This especially sharpens on the rest set.
So SR-same may suffer from an improper linear
distortion parameter.
The search ability of Moses and the shift-
reduce decoder are evaluated, too. The translation
must be produced with the same set of parameters.
In our experiments, we employed the parameters
tuned by Moses. The test was done on the ITG and
the rest set, respectively. The results are shown in
Table 2. As the distortion limit becomes greater,
the number of the ITG-legal translation generated
by Moses becomes smaller. On the ITG set, trans-
lation probabilities from the shift-reduce decoder
is either greater or equal to that from Moses on
most sentences, and BLEU scores of shift-reduce
decoder is greater than that of Moses with all
distortion limit. Although the search space of
shift-reduce decoder is smaller than that of Moses,
shift-reduce decoder can give the translation that
Moses can not reach. On the rest set, for most sen-
tences, the translation probabilities from Moses is
greater than that from shift-reduce decoder. But
only when distortion limit is 6 and 8, the BLEU
score of Moses is greater than that of the shift-
reduce decoder. We may conclude that greater
score does not certainly lead to greater BLEU
score.
5 Conclusions and Future Work
In this paper, we present a shift-reduce decod-
ing algorithm for phrase-based translation model
that can generate the ITG-legal translation in lin-
ear time. The algorithm need not consider shift-
reduce divergence and performs reduce operation
as soon as possible. We compare the performance
of the shift-reduce decoder with the state-of-the-
art decoder Moses. Experiment results show that
the shift-reduce algorithm can improve both the
accuracy and the speed significantly on different
test sets. We further analyze the performance and
find that on the ITG set, the shift-reduce decoder
is superior over Moses in terms of accuracy, speed
and search ability, while on the rest set, it does
not display advantage, suffering from improper
parameters.
Next, we will extend the shift-reduce algorithm
to syntax-based translation models, to see whether
it works.
6 Acknowledgement
The authors were supported by National Natural
Science Foundation of China Contract 60736014,
National Natural Science Foundation of China
Contract 60873167 and High Technology R&D
Program Project No. 2006AA010108. We are
grateful to the anonymous reviewers for their
valuable comments.
292
References
Collins, Michael, Philipp Koehn, and Ivona Kucerova.
2005. Clause restructuring for statistical machine
translation. In Proc. of ACL, pages 531?540.
Galley, Michel and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. In Proc. of EMNLP, pages 848?856.
Huang, Liang and David Chiang. 2005. Better k-best
parsing. In Proceedings of the Ninth International
Workshop on Parsing Technologies (IWPT), pages
53?64.
Knight, Kevin. 1999. Decoding complexity in word-
replacement translation models. Computational
Linguistics, 25:607?615.
Koehn, Philipp, Hieu Hoang, Alexandra Birch Mayne,
Christopher Callison-Burch, Marcello Federico,
Nicola Bertoldi, Brooke Cowan, Wade Shen, Chris-
tine Moran, Richard Zens, Chris Dyer, Ondrej Bo-
jar, Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open source toolkit for statistical machine
translation. In Proc. of the 45th ACL, Demonstra-
tion Session.
Koehn, Philipp. 2004. Pharaoh: A beam search de-
coder for phrased-based statistical machine transla-
tion. In Proc. of AMTA, pages 115?124.
Och, Frans J., Christoph Tillmann, and Hermann Ney.
1999. Improved alignment models for statistical
machine translation. In Proc. of EMNLP, pages 20?
28.
Och, Frans J. 2003. Minimum error rate training in
statistical machine translation. In Proc. of ACL,
pages 160?167.
Tillmann, Chirstoph and Hermann Ney. 2003.
Word reordering and a dynamic programming beam
search algorithm for statistical machine translation.
Computational Linguistics, 29:97?133.
Wu, Dekai. 1996. A polynomial-time algorithm for
statistical machine translation. In Proc. of ACL,
pages 152?158.
Wu, Dekai. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23:377?403.
Xiong, Deyi, Qun Liu, and Shouxun Lin. 2006. Maxi-
mum entropy based phrase reordering model for sta-
tistical machine translation. In Proc. of ACL, pages
521?528.
Zens, Richard and Hermann Ney. 2003. A compara-
tive study on reordering constraints in statistical ma-
chine translation. In Proc. of ACL, pages 144?151.
Zens, Richard, Hermann Ney, Taro Watanable, and
Eiichiro Sumita. 2004. Reordering constraints
for phrase-based statistical machine translation. In
Proc. of COLING, pages 205?211.
293
Coling 2010: Poster Volume, pages 516?524,
Beijing, August 2010
Effective Constituent Projection across Languages
Wenbin Jiang and Yajuan Lu? and Yang Liu and Qun Liu
Key Laboratory of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences
{jiangwenbin, lvyajuan, yliu, liuqun}@ict.ac.cn
Abstract
We describe an effective constituent pro-
jection strategy, where constituent pro-
jection is performed on the basis of de-
pendency projection. Especially, a novel
measurement is proposed to evaluate the
candidate projected constituents for a tar-
get language sentence, and a PCFG-style
parsing procedure is then used to search
for the most probable projected con-
stituent tree. Experiments show that, the
parser trained on the projected treebank
can significantly boost a state-of-the-art
supervised parser. When integrated into a
tree-based machine translation system, the
projected parser leads to translation per-
formance comparable with using a super-
vised parser trained on thousands of anno-
tated trees.
1 Introduction
In recent years, supervised constituent parsing has
been well studied and achieves the state-of-the-art
for many resource-rich languages (Collins, 1999;
Charniak, 2000; Petrov et al, 2006). Because
of the cost and difficulty in treebank construc-
tion, researchers have also investigated the utiliza-
tion of unannotated text, including the unsuper-
vised parsing which totally uses unannotated data
(Klein and Manning, 2002; Klein and Manning,
2004; Bod, 2006; Seginer, 2007), and the semi-
supervised parsing which uses both annotated and
unannotated data (Sarkar, 2001; Steedman et al,
2003; McClosky et al, 2006).
Because of the higher complexity and lower
performance of unsupervised methods, as well as
the need of reliable priori knowledge in semi-
supervised methods, it seems promising to project
the syntax structures from a resource-rich lan-
guage to a resource-scarce one across a bilingual
corpus. Lots of researches have so far been de-
voted to dependency projection (Hwa et al, 2002;
Hwa et al, 2005; Ganchev et al, 2009; Smith
and Eisner, 2009). While for constituent projec-
tion there is few progress. This is due to the fact
that the constituent syntax describes the language
structure in a more detailed way, and the degree of
isomorphism between constituent structures ap-
pears much lower.
In this paper we propose for constituent pro-
jection a stepwise but totally automatic strategy,
which performs constituent projection on the ba-
sis of dependency projection, and then use a con-
straint EM optimization algorithm to optimized
the initially projected trees. Given a word-aligned
bilingual corpus with source sentences parsed, we
first project the dependency structures of these
constituent trees to the target sentences using a
dynamic programming algorithm, then we gener-
ate a set of candidate constituents for each target
sentence and design a novel evaluation function
to calculate the probability of each candidate con-
stituent, finally, we develop a PCFG-style parsing
procedure to search for the most probable pro-
jected constituent tree in the evaluated candidate
constituent set. In addition, we design a constraint
EM optimization procedure to decrease the noise
in the initially projected constituent treebank.
Experimental results validate the effectiveness
of our approach. On the Chinese-English FBIS
corpus, we project the English parses produced
by the Charniak parser across to the Chinese sen-
516
tences. A berkeley parser trained on this pro-
jected treebank can effectively boost the super-
vised parsers trained on bunches of CTB trees.
Especially, the supervised parser trained on the
smaller CTB 1.0 benefits a significant F-measure
increment of more than 1 point from the projected
parser. When using the projected parser in a tree-
based translation model (Liu et al, 2006), we
achieve translation performance comparable with
using a state-of-the-art supervised parser trained
on thousands of CTB trees. This surprising re-
sult gives us an inspiration that better translation
would be achieved by combining both projected
parsing and supervised parsing into a hybrid pars-
ing schema.
2 Stepwise Constituent Projection
We first introduce the dynamic programming pro-
cedure for dependency projection, then describe
the PCFG-style algorithm for constituent projec-
tion which is conducted on projected dependent
structures, and finally show the constraint EM
procedure for constituent optimization.
2.1 Dependency Projection
For dependency projection we adopt a dynamic
programming algorithm, which searches the most
probable projected target dependency structure
according to the source dependency structure and
the word alignment.
In order to mitigate the effect of word alignment
errors, multiple GIZA++ (Och and Ney, 2000) re-
sults are combined into a compact representation
called alignment matrix. Given a source sentence
with m words, represented as E1:m, and a target
sentence with n words, represented as F1:n, their
word alignment matrix A is an m ? n matrix,
where each element Ai,j denotes the probability
of the source word Ei aligned to the target word
Fj .
Using P (DF |DE , A) to denote the probability
of the projected target dependency structure DF
conditioned on the source dependency structure
DE and the alignment matrix A, the projection al-
gorithm aims to find
D?F = argmax
DF
P (DF |DE , A) (1)
Algorithm 1 Dependency projection.
1: Input: F , and Pe for all word pairs in F
2: for ?i, j? ? ?1, |F |? in topological order do
3: buf ? ?
4: for k? i..j ? 1 do ? all partitions
5: for l ? V[i, k] and r ? V[k + 1, j] do
6: insert DERIV(l, r, Pe) into buf
7: insert DERIV(r, l, Pe) into buf
8: V[i, j]? top K derivations of buf
9: Output: the best derivation of V[1, |F |]
10: function DERIV(p, c, Pe)
11: d? p ? c ? {p ? rooty c ? root} ? new derivation
12: d ? evl ? EVAL(d, Pe) ? evaluation function
13: return d
P (DF |DE , A) can be factorized into each depen-
dency edge xy y in DF
P (DF |DE , A) =
?
xyy?DF
Pe(xy y|DE , A)
Pe can then be obtained by simple accumulation
across all possible situations of correspondence
Pe(xy y|DE , A)
=
?
1?x?,y??|E|
Ax,x? ?Ay,y? ? ?(x?, y?|DE)
where ?(x?, y?|DE) is a 0-1 function that equals
1 only if the dependent relation x? y y? holds in
DE .
The search procedure needed by the argmax op-
eration in equation 1 can be effectively solved
by the Chu-Liu-Edmonds algorithm used in (Mc-
Donald et al, 2005). In this work, however, we
adopt a more general and simple dynamic pro-
gramming algorithm as shown in Algorithm 1,
in order to facilitate the possible expansions. In
practice, the cube-pruning strategy (Huang and
Chiang, 2005) is used to speed up the enumera-
tion of derivations (loops started by line 4 and 5).
2.2 Constituent Projection
The PCFG-style parsing procedure searches for
the most probable projected constituent tree in
a shrunken search space determined by the pro-
jected dependency structure and the target con-
stituent tree. The shrunken search space can be
built as following. First, we generates the candi-
date constituents of the source tree and the can-
didate spans of the target sentence, so as to enu-
merate the candidate constituents of the target sen-
tence. Then we compute the consistent degree for
517
each pair of candidate constituent and span, and
further estimate the probability of each candidate
constituent for the target sentence.
2.2.1 Candidate Constituents and Spans
For the candidate constituents of the source
tree, using only the original constituents imposes
a strong hypothesis of isomorphism on the con-
stituent projection between two languages, since
it requires that each couple of constituent and span
must be strictly matched. While for the candi-
date spans of the target sentences, using all sub-
sequences makes the search procedure suffer from
more perplexity. Therefore, we expand the candi-
date constituent set and restrict the candidate span
set:
? Candidate Constituent: Suppose a produc-
tion in the source constituent tree, denoted as
p ? c1c2..ch..c|p|, and ch is the head child
of the parent p. Each constituent, p or c, is a
triple ?lb, rb, nt?, where nt denotes its non-
terminal, while lb and rb represent its left-
and right bounds of the sub-sequence that the
constituent covers. The candidate constituent
set of this production consists the head of
the production itself, and a set of incomplete
constituents,
{?l, r, p ? nt??|c1 ? lb ? l ? ch ? lb?
ch ? rb ? r ? c|p| ? rb?
(l < ch ? lb ? r > ch ? rb)}
where the symbol ? indicates an incomplete
non-terminal. The candidate constituent set
of the entire source tree is the unification of
the sets extracted from all productions of the
tree.
? Candidate Span: A candidate span of the tar-
get sentence is a tuple ?lb, rb?, where lb and
rb indicate the same as in a constituent. We
define the candidate span set as the spans of
all regular dependent segments in the corre-
sponding projected dependency structure. A
regular dependency segment is a dependent
segment that every modifier of the root is a
complete dependency structure. Suppose a
dependency structure rooted at word p, de-
noted as clL..cl2cl1 x p y cr1cr2..crR, it
has L (L ? 0) modifiers on its left and R
(R ? 0) modifiers on its right, each of them
is a smaller complete dependency structure.
Then the word p itself is a regular depen-
dency segment without any modifier, and
{cli..cl1 x py cr1..crj |0 ? i ? L?
0 ? j ? R?
(i > 0 ? j > 0)}
is a set of regular dependency structures with
at least one modifier. The regular depen-
dency segments of the entire projected de-
pendency structure can simply be accumu-
lated across all dependency nodes.
2.2.2 Span-to-Constituent Correspondence
After determining the candidate constituent set
of the source tree, denoted as ?E , and the can-
didate span set of the target sentence, denoted as
?F , we then calculate the consistent degree for
each pair of candidate constituent and candidate
span.
Given a candidate constituent ? ? ?E and a
candidate span ? ? ?F , their consistent degree
C(?, ?|A) is the probability that they are aligned
to each other according to A.
We display the derivations from bottom to up.
First, we define the alignment probability from a
word i in the span ? to the constituent ? as
P (i 7? ?|A) =
?
??lb?j???rbAi,j?
j Ai,j
Then we define the alignment probability from the
span ? to the constituent ? as
P (? 7? ?|A) =
?
??lb?i???rb
P (i 7? ?|A)
Note that we use i to denote both a word and its in-
dex for simplicity without causing confusion. Fi-
nally, we define C(?,?|A) as
C(?, ?|A) = P (? 7? ?|A)? P (? 7? ?|AT ) (2)
Where P (? 7? ?|AT ) denotes the alignment
probability from the constituent ? to the span ?, it
can be calculated in the same manner.
518
2.2.3 Constituent Projection Algorithm
The purpose of constituent projection is to find
the most probable projected constituent tree for
the target sentence conditioned on the source con-
stituent tree and the word alignment
T?F = argmax
TF??F
P (TF |TE, A) (3)
Here, we use ?F to denote the set of candidate
constituents of the target sentence
?F = ?F ?NT (?E)
= {?F |?(?F ) ? ?F ? nt(?F ) ? NT (?E)}
where ?(?) and nt(?) represent the span and the
non-terminal of a constituent respectively, and
NT (?) represents the set of non-terminals ex-
tracted from a constituent set. Note that TF is a
subset of ?F if we treat a tree as a set of con-
stituents.
The probability of the projected tree TF can be
factorized into the probabilities of the projected
constituents that composes the tree
P (TF |TE , A) =
?
?F?TF
P?(?F |TE , A)
while the probability of the projected source con-
stituent can be defined as a statistics of span-to-
constituent- and constituent-to-constituent consis-
tent degrees
P?(?F |TE , A) =
?
?E??E C(?F , ?E |A)?
?E??E C(?(?F ), ?E |A)
where C(?F , ?E |A) in the numerator denotes the
consistent degree for each pair of constituents,
which can be calculated based on that of span and
constituent described in Formula 2
C(?F , ?E) =
{
0 if ?F ? nt 6= ?E ? nt
C(?(?F ), ?E) else
Algorithm 2 shows the pseudocode for con-
stituent projection. A PCFG-style parsing pro-
cedure searches for the best projected constituent
tree in the constrained space determined by ?F .
Note that the projected trees are binarized, and can
be easily recovered according to the asterisks at
the tails of non-terminals.
Algorithm 2 Constituent projection.
1: Input: ?F , ?F , and P? for all spans in ?F
2: for ?i, j? ? ? in topological order do
3: buf ? ?
4: for p ? ?F s.t. ?(p) = ?i, j? do
5: for k? i..j ? 1 do ? all partitions
6: for l ? V[i, k] and r ? V[k + 1, j] do
7: insert DERIV(l, r, p, P?) into buf
8: V[i, j]? top K derivations of buf
9: Output: the best derivation of V[1, |F |]
10: function DERIV(l, r, p, P?)
11: d? l ? r ? {p} ? new derivation
12: d ? evl ? EVAL(d, P?) ? evaluation function
13: return d
2.3 EM Optimization
Since the constituent projection is conducted on
each sentence pair separately, the projected tree-
bank is apt to suffer from more noise caused by
free translation and word alignment error. It can
be expected that an EM iteration over the whole
projected treebank will lead to trees with higher
consistence.
We adopt the inside-outside algorithm to im-
prove the quality of the initially projected tree-
bank. Different from previous works, all expecta-
tion and maximization operations for a single tree
are performed in a constrained space determined
by the candidate span set of the projected target
dependency structure. That is to say, all the sum-
mation operations, both for calculating ?/? values
and for re-estimating the rule probabilities, only
consider the spans in the candidate span set. This
means that the projected dependency structures
are supposed believable, and the noise is mainly
introduced in the following constituent projection
procedure.
Here we give an overall description of the tree-
bank optimization procedure. First, an initial
PCFG grammar G0F is estimated from the original
projected treebank. Then several iterations of ?/?
calculation and rule probability re-estimation are
performed. For example in the i-the iteration, ?/?
values are calculated based on the current gram-
mar Gi?1F , afterwards the optimized grammar GiF
is obtained based on these ?/? values. The itera-
tive procedure terminates when the likelihood of
whole treebank increases slowly. Finally, with the
optimized grammar, a constrained PCFG parsing
procedure is conducted on each of the initial pro-
519
jected trees, so as to obtain an optimized treebank.
3 Applications of Constituent Projection
The most direct contribution of constituent pro-
jection is pushing an initial step for the statis-
tical constituent parsing of resource-scarce lan-
guages. It also has some meaningful applica-
tions even for the resource-rich languages. For
instances, the projected treebank, due to its large
scale and high coverage, can used to boost an tra-
ditional supervised-trained parser. And, the parser
trained on the projected treebank can adopted to
conduct tree-to-string machine translation, since
it give parsing results with larger isomorphism
with the target language than a supervised-trained
parser dose.
3.1 Boost an Traditional Parser
We first establish a unified framework for the en-
hanced parser where a projected parser is adopted
to guide the parsing procedure of the baseline
parser.
For a given target sentence S, the enhanced
parser selected the best parse T? among the set
of candidates ?(S) according to two evaluation
functions, given by the baseline parser B and the
projected guide parser G, respectively.
T? = argmax
T??(S)
P (T |B)? P (T |G)? (4)
These two evaluation functions can be integrated
deeply into the decoding procedure (Carreras et
al., 2008; Zhang and Clark, 2008; Huang, 2008),
or can be integrated at a shallow level in a rerank-
ing manner (Collins, 2000; Charniak and John-
son, 2005). For simplicity and generability, we
adopt the reranking strategy. In k-best reranking,
?(S) is simply a set of candidate parses, denoted
as {T1, T2, ..., Tk}, and we use the single parse of
the guide parser, TG, to re-evaluate these candi-
dates. Formula 4 can be redefined as
T? (TG) = argmax
T??(S)
w ? f(T, TG) (5)
Here, f(T, TG) and w represent a high dimen-
sional feature representation and a correspond-
ing weight vector, respectively. The first feature
f1(T, TG) = logP (T |B) is the log probability
of the baseline parser, while the remaining fea-
tures are integer-valued guide features, and each
of them represents the guider parser?s predication
result for a particular configuration in candidate
parse T , so as to utilize the projected parser?s
knowledge to guide the parsing procedure of the
traditional parser.
In our work a guide feature is composed of two
parts, the non-terminal of a certain constituent ?
in the candidate parse T ,1 and the non-terminal
at the corresponding span ?(?) in the projected
parse TG. Note that in the projected parse this
span does not necessarily correspond to a con-
stituent. In such situations, we simply use the
non-terminal of the constituent that just be able
to cover this span, and attach a asterisk at the tail
of this non-terminal. Here is an example of the
guide features
f100(T, TG) = V P ? T ? PP? ? TG
It represents that a V P in the candidate parse cor-
responds to a segment of a PP in the projected
parse. The quantity of its weight w100 indicates
how probably a span can be predicated as V P if
the span corresponds to a partial PP in the pro-
jected parse.
We adopt the perceptron algorithm to train
the reranker. To reduce overfitting and pro-
duce a more stable weight vector, we also use
a refinement strategy called averaged parameters
(Collins, 2002).
3.2 Using in Machine Translation
Researchers have achieved promising improve-
ments in tree-based machine translation (Liu et
al., 2006; Huang et al, 2006). Such models use
a parsed tree as input and converts it into a target
tree or string. Given a source language sentence,
first we use a traditional source language parser
to parse the sentence to obtain the syntax tree T ,
and then use the translation decoder to search for
the best derivation d?, where a derivation d is a se-
quence of transformations that converts the source
tree into the target language string
d? = argmax
d?D
P (d|T ) (6)
1Using non-terminals as features brings no improvement
in the reranking experiments, so as to examine the impact of
the projected parser.
520
Here D is the candidate set of d, and it is deter-
mined by the source tree T and the transformation
rules.
Since the tree-based models are based on
the synchronous transformational grammars, they
suffer much from the isomerism between the
source syntax and the target sentence structure.
Considering that the parsed tree produced by a
projected parser may have larger isomorphism
with the target language, it would be a promis-
ing idea to adopt the projected parser to parse the
input sentence for the subsequent translation de-
coding procedure.
4 Experiments
In this section, we first invalidate the effect of con-
stituent projection by evaluating a parser trained
on the projected treebank. Then we investigate
two applications of the projected parser: boosting
an traditional supervised-trained parser, and inte-
gration in a tree-based machine translation sys-
tem. Following the previous works, we depict the
parsing performance by F-score on sentences with
no more than 40 words, and evaluate the transla-
tion quality by the case-sensitive BLEU-4 metric
(Papineni et al, 2002) with 4 references.
4.1 Constituent Projection
We perform constituent projection from English
to Chinese on the FBIS corpus, which contains
239K sentence pairs with about 6.9M/8.9M words
in Chinese/English. The English sentences are
parsed by the Charniak Parser and the dependency
structures are extracted from these parses accord-
ing to the head-finding rules of (Yamada and
Matsumoto, 2003). The word alignment matrixes
are obtained by combining the 10-best results of
GIZA++ according to (Liu et al, 2009).
We first project the dependency structures from
English to Chinese according to section 2.1, and
then project the constituent structures according
to section 2.2. We define an assessment criteria
to evaluate the confidence of the final projected
constituent tree
c = n
?
P (DF |DE , A) ? P (TF |TE , A)
where n is the word count of a Chinese sentence
in our experiments. A series of projected Chi-
Thres c #Resrv Cons-F1 Span-F1
0.5 12.6K 23.9 32.7
0.4 17.8K 23.9 33.4
0.3 27.2K 25.4 35.7
0.2 45.1K 26.6 38.0
0.1 87.0K 27.8 40.4
Table 1: Performances of the projected parsers
on the CTB test set. #Resrv denotes the amount
of reserved trees within threshold c. Cons-F1 is
the traditional F-measure, while Span-F1 is the F-
measure without consideration of non-terminals.
nese treebanks with different scales are obtained
by specifying different c as the filtering threshold.
The state-of-the-art Berkeley Parser is adopted to
train on these treebanks because of its high per-
formance and independence of head word infor-
mation.
Table 1 shows the performances of these pro-
jected parsers on the standard CTB test set, which
is composed of sentences in chapters 271-300.
We find that along with the decrease of the filter-
ing threshold c, more projected trees are reserved
and the performance of the projected parser con-
stantly increases. We also find that the traditional
F-value, Cons-F1, is obviously lower than the one
without considering non-terminals, Span-F1. This
indicates that the constituent projection procedure
introduces more noise because of the higher com-
plexity of constituent correspondence. In all the
rest experiments, however, we simply use the pro-
jected treebank filtered by threshold c = 0.1 and
do not try any smaller thresholds, since it already
takes more than one weak to train the Berkeley
Parser on the 87 thousands trees resulted by this
threshold.
The constrained EM optimization procedure
described in section 2.3 is used to alleviate the
noise in the projected treebank, which may be
caused by free translation, word alignment errors,
and projection on each single sentence pair. Fig-
ure 1 shows the log-likelihood on the projected
treebank after each EM iteration. It is obvious that
the log-likelihood increases very slowly after 10
iterations. We terminate the EM procedure after
40 iterations.
Finally we train the Berkeley Parser on the op-
timized projected treebank, and test its perfor-
521
-65
-64
-63
-62
-61
-60
-59
-58
 0  5  10  15  20  25  30  35  40
Lo
g-
lik
eli
ho
od
EM iteration
Figure 1: Log-likelihood of the 87K-projected
treebank after each EM interation.
Train Set Cons-F1 Span-F1
Original 87K 27.8 40.4
Optimized 87K 22.8 40.2
Table 2: Performance of the parser trained on the
optimized projected treebank, compared with that
of the original projected parser.
Train Set Baseline Bst-Ini Bst-Opt
CTB 1.0 75.6 76.4 76.9
CTB 5.0 85.2 85.5 85.7
Table 3: Performance improvement brought by
the projected parser to the baseline parsers trained
on CTB 1.0 and CTB 5.0, respectively. Bst-
Ini/Bst-Opt: boosted by the parser trained on the
initial/optimized projected treebank.
mance on the standard CTB test set. Table 2
shows the performance of the parser trained on
the optimized projected treebank. Unexpectedly,
we find that the constituent F1-value of the parser
trained on the optimized treebank drops sharply
from the baseline, although the span F1-value re-
mains nearly the same. We assume that the EM
procedure gives the original projected treebank
more consistency between each single tree while
the revised treebank deviates from the CTB anno-
tation standard, but it needs to be validated by the
following experiments.
4.2 Boost an Traditional Parser
The projected parser is used to help the reranking
of the k-best parses produced by another state-of-
the-art parser, which is called the baseline parser
for convenience. In our experiments we choose
the revised Chinese parser (Xiong et al, 2005)
 70
 72
 74
 76
 78
 80
 82
 84
 86
 88
 1000  10000
Pa
rs
ev
al 
F-
sc
or
e 
(%
)
Scale of treebank (log)
CTB 1.0
CTB 5.0
baseline
boosted parser
Figure 2: Boosting performance of the projected
parser on a series of baseline parsers that are
trained on treebanks of different scales.
based on Collins model 2 (Collins, 1999) as the
baseline parser.2
The baseline parser is respectively trained on
CTB 1.0 and CTB 5.0. For both corpora we
follow the traditional corpus splitting: chapters
271-300 for testing, chapters 301-325 for devel-
opment, and else for training. Experimental re-
sults are shown in Table 3. We find that both
projected parsers bring significant improvement to
the baseline parsers. Especially the later, although
performs worse on CTB standard test set, gives a
larger improvement than the former. This to some
degree confirms the previous assumption. How-
ever, more investigation must be conducted in the
future.
We also observe that for the baseline parser
trained on the much larger CTB 5.0, the boost-
ing performance of the projected parser is rela-
tively lower. To further investigate the regularity
that the boosting performance changes according
to the scale of training treebank of the baseline
parser, we train a series of baseline parsers with
different amounts of trees, then use the projected
parser trained on the optimized treebank to en-
hance these baseline parsers. Figure 2 shows the
experimental results. From the curves we can see
that the smaller the training corpus of the baseline
parser, the more significant improvement can be
obtained. This is a good news for the resource-
scarce languages that have no large treebanks.
2The Berkeley Parser fails to give k-best parses for some
sentences when trained on small treebanks, and these sen-
tences have to be deleted in the k-best reranking experiments.
522
4.3 Using in Machine Translation
We investigate the effect of the projected parser
in the tree-based translation model on Chinese-to-
English translation. A series of contrast transla-
tion systems are built, each of which uses a super-
vised Chinese parser (Xiong et al, 2005) trained
on a particular amount of CTB trees.
We use the FBIS Chinese-English bitext as the
training corpus, the 2002 NIST MT Evaluation
test set as our development set, and the 2005 NIST
MT Evaluation test set as our test set. We first ex-
tract the tree-to-string translation rules from the
training corpus by the algorithm of (Liu et al,
2006), and train a 4-gram language model on
the Xinhua portion of GIGAWORD corpus with
Kneser-Ney smoothing using the SRI Language
Modeling Toolkit (Stolcke and Andreas, 2002).
Then we use the standard minimum error-rate
training (Och, 2003) to tune the feature weights
to maximize the system.s BLEU score.
Figure 3 shows the experimental results. We
find that the translation system using the projected
parser achieves the performance comparable with
the one using the supervised parser trained on
CTB 1.0. Considering that the F-score of the pro-
jected parser is only 22.8%, which is far below of
the 75.6% F-score of the supervised parser trained
on CTB 1.0, we can give more confidence to the
assumption that the projected parser is apt to de-
scribe the syntax structure of the counterpart lan-
guage. This surprising result also gives us an in-
spiration that better translation would be achieved
by combining projected parsing and supervised
parsing into hybrid parsing schema.
5 Conclusion
This paper describes an effective strategy for con-
stituent projection, where dependency projection
and constituent projection are consequently con-
ducted to obtain the initial projected treebank,
and an constraint EM procedure is then per-
formed to optimized the projected trees. The
projected parser, trained on the projected tree-
bank, significantly boosts an existed state-of-the-
art supervised-trained parser, especially trained on
a smaller treebank. When using the projected
parser in tree-based translation, we achieve the
0.220
0.230
0.240
0.250
0.260
0.270
 1000  10000
BL
EU
 sc
or
e
Scale of treebank (log)
use projected parser
CTB 1.0
CTB 5.0
use supervised parsers
Figure 3: Performances of the translation systems,
which use the projected parser and a series of su-
pervised parsers trained CTB trees.
translation performance comparable with using a
supervised parser trained on thousands of human-
annotated trees.
As far as we know, this is the first time that
the experimental results are systematically re-
ported about the constituent projection and its ap-
plications. However, many future works need
to do. For example, more energy needs to be
devoted to the treebank optimization, and hy-
brid parsing schema that integrates the strengths
of both supervised-trained parser and projected
parser would be valuable to be investigated for
better translation.
Acknowledgments
The authors were supported by 863 State Key
Project No. 2006AA010108, National Natural
Science Foundation of China Contract 60873167,
Microsoft Research Asia Natural Language Pro-
cessing Theme Program grant (2009-2010), and
National Natural Science Foundation of China
Contract 90920004. We are grateful to the anony-
mous reviewers for their thorough reviewing and
valuable suggestions.
References
Bod, Rens. 2006. An all-subtrees approach to unsu-
pervised parsing. In Proceedings of the COLING-
ACL.
Carreras, Xavier, Michael Collins, and Terry Koo.
2008. Tag, dynamic programming, and the percep-
tron for efficient, feature-rich parsing. In Proceed-
ings of the CoNLL.
523
Charniak, Eugene and Mark Johnson. 2005. Coarse-
to-fine-grained n-best parsing and discriminative
reranking. In Proceedings of the ACL.
Charniak, Eugene. 2000. A maximum-entropy-
inspired parser. In Proceedings of the NAACL.
Collins, Michael. 1999. Head-driven statistical mod-
els for natural language parsing. In Ph.D. Thesis.
Collins, Michael. 2000. Discriminative reranking for
natural language parsing. In Proceedings of the
ICML, pages 175?182.
Collins, Michael. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of the EMNLP, pages 1?8, Philadelphia, USA.
Ganchev, Kuzman, Jennifer Gillenwater, and Ben
Taskar. 2009. Dependency grammar induction via
bitext projection constraints. In Proceedings of the
47th ACL.
Huang, Liang and David Chiang. 2005. Better k-best
parsing. In Proceedings of the IWPT, pages 53?64.
Huang, Liang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proceedings of the AMTA.
Huang, Liang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of
the ACL.
Hwa, Rebecca, Philip Resnik, Amy Weinberg, and
Okan Kolak. 2002. Evaluating translational corre-
spondence using annotation projection. In Proceed-
ings of the ACL.
Hwa, Rebecca, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrap-
ping parsers via syntactic projection across paral-
lel texts. In Natural Language Engineering, vol-
ume 11, pages 311?325.
Klein, Dan and Christopher D. Manning. 2002. A
generative constituent-context model for improved
grammar induction. In Proceedings of the ACL.
Klein, Dan and Christopher D. Manning. 2004. Cor-
pusbased induction of syntactic structure: Models
of dependency and constituency. In Proceedings of
the ACL.
Liu, Yang, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment template for statistical machine
translation. In Proceedings of the ACL.
Liu, Yang, Tian Xia, Xinyan Xiao, and Qun Liu. 2009.
Weighted alignment matrices for statistical machine
translation. In Proceedings of the EMNLP.
McClosky, David, Eugene Charniak, and Mark John-
son. 2006. Reranking and self-training for parser
adaptation. In Proceedings of the ACL.
McDonald, Ryan, Fernando Pereira, Kiril Ribarov, and
Jan Hajic?. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings
of HLT-EMNLP.
Och, Franz J. and Hermann Ney. 2000. Improved
statistical alignment models. In Proceedings of the
ACL.
Och, Franz Joseph. 2003. Minimum error rate train-
ing in statistical machine translation. In Proceed-
ings of the 41th Annual Meeting of the Association
for Computational Linguistics, pages 160?167.
Papineni, Kishore, Salim Roukos, Todd Ward, and
Weijing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings
of the ACL.
Petrov, Slav, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and in-
terpretable tree annotation. In Proceedings of the
ACL.
Sarkar, Anoop. 2001. Applying co-training methods
to statistical parsing. In Proceedings of NAACL.
Seginer, Yoav. 2007. Fast unsupervised incremental
parsing. In Proceedings of the ACL.
Smith, David and Jason Eisner. 2009. Parser adap-
tation and projection with quasi-synchronous gram-
mar features. In Proceedings of EMNLP.
Steedman, Mark, Miles Osborne, Anoop Sarkar,
Stephen Clark, Rebecca Hwa, Julia Hockenmaier,
Paul Ruhlen, Steven Baker, and Jeremiah Crim.
2003. Bootstrapping statistical parsers from small
datasets. In Proceedings of the EACL.
Stolcke and Andreas. 2002. Srilm - an extensible lan-
guage modeling toolkit. In Proceedings of the Inter-
national Conference on Spoken Language Process-
ing, pages 311?318.
Xiong, Deyi, Shuanglong Li, Qun Liu, and Shouxun
Lin. 2005. Parsing the penn chinese treebank with
semantic knowledge. In Proceedings of IJCNLP
2005, pages 70?81.
Yamada, H and Y Matsumoto. 2003. Statistical de-
pendency analysis using support vector machines.
In Proceedings of IWPT.
Zhang, Yue and Stephen Clark. 2008. A tale of
two parsers: investigating and combining graph-
based and transition-based dependency parsing us-
ing beam-search. In Proceedings of EMNLP.
524
Coling 2010: Poster Volume, pages 1185?1193,
Beijing, August 2010
Dependency-Based Bracketing Transduction Grammar
for Statistical Machine Translation
Jinsong Su, Yang Liu, Haitao Mi, Hongmei Zhao, Yajuan Lu?, Qun Liu
Key Laboratory of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences
{sujinsong,yliu,htmi,zhaohongmei,lvyajuan,liuqun}@ict.ac.cn
Abstract
In this paper, we propose a novel
dependency-based bracketing transduc-
tion grammar for statistical machine
translation, which converts a source sen-
tence into a target dependency tree. Dif-
ferent from conventional bracketing trans-
duction grammar models, we encode tar-
get dependency information into our lex-
ical rules directly, and then we employ
two different maximum entropy models
to determine the reordering and combi-
nation of partial dependency structures,
when we merge two neighboring blocks.
By incorporating dependency language
model further, large-scale experiments on
Chinese-English task show that our sys-
tem achieves significant improvements
over the baseline system on various test
sets even with fewer phrases.
1 Introduction
Bracketing transduction grammar (BTG) (Wu,
1995) is an important subclass of synchronous
context free grammar, which employs a special
synchronous rewriting mechanism to parse paral-
lel sentence of both languages.
Due to the prominent advantages such as the
simplicity of grammar and the good coverage of
syntactic diversities in different language pairs,
BTG has attracted increasing attention in statis-
tical machine translation (SMT). In flat reorder-
ing model (Wu, 1996; Zens et al, 2004), which
assigns constant reordering probabilities depend-
ing on the language pairs, BTG constraint proves
to be very effective for reducing the search space
of phrase reordering. To pursue a better method
to predict the order between two neighboring
blocks1, Xiong et al (2006) present an enhanced
BTG with a maximum entropy (ME) based re-
ordering model. Along this line, source-side syn-
tactic knowledge is introduced into the reorder-
ing model to improve BTG-based translation (Se-
tiawan et al, 2007; Zhang et al, 2007; Xiong et
al., 2008; Zhang and Li, 2009). However, these
methods mainly focus on the utilization of source
syntactic knowledge, while ignoring the modeling
of the target-side syntax that directly influences
the translation quality. As a result, how to ob-
tain better translation by exploiting target syntac-
tic knowledge is somehow neglected. Thus, we
argue that it is important to model the target-side
syntax in BTG-based translation.
Recently, modeling syntactic information on
the target side has progressed significantly. De-
pending on the type of output, these models can
be divided into two categories: the constituent-
output systems (Galley et al, 2006; Zhang et
al., 2008; Liu et al, 2009) and dependency-
output systems (Eisner, 2003; Lin, 2004; Ding
and Palmer, 2005; Quirk et al, 2005; Shen et
al., 2008). Compared with the constituent-output
systems, the dependency-output systems provide a
simpler platform to capture the target-side syntac-
tic information, while also having the best inter-
lingual phrasal cohesion properties (Fox, 2002).
Typically, Shen et al (2008) propose a string-to-
dependency model, which integrates the target-
side well-formed dependency structure into trans-
lation rules. With the dependency structure, this
system employs a dependency language model
(LM) to exploit long distance word relations, and
achieves a significant improvement over the hier-
archical phrase-based system (Chiang, 2007). So
1A block is a bilingual phrase without maximum length
limitation.
1185
we think it will be a promising way to integrate the
target-side dependency structure into BTG-based
translation.
In this paper, we propose a novel dependency-
based BTG (DepBTG) for SMT, which represents
translation in the form of dependency tree. Ex-
tended from BTG, our grammars operate on two
neighboring blocks with target dependency struc-
ture. We integrate target syntax into bilingual
phrases and restrict target phrases to the well-
formed structures inspired by (Shen et al, 2008).
Then, we adopt two ME models to predict how to
reorder and combine partial structures into a target
dependency tree, which gives us access to captur-
ing the target-side syntactic information. To the
best of our knowledge, this is the first effort to
combine the translation generation with the mod-
eling of target syntactic structure in BTG-based
translation.
The remainder of this paper is structured as fol-
lows: In Section 2, we give brief introductions to
the bases of our research: BTG and dependency
tree. In Section 3, we introduce DepBTG in detail.
In Section 4, we further illustrate how to create
two ME models to predict the reordering and de-
pendency combination between two neighboring
blocks. Section 5 describes the implementation
of our decoder. Section 6 shows our experiments
on Chinese-English task. Finally, we end with a
summary and future research in Section 7.
2 Background
2.1 BTG
BTG is a special case of synchronous context free
grammar. There are three rules utilized in BTG:
A ? [A1, A2] (1)
A ? ?A1, A2? (2)
A ? x/y (3)
where the reordering rules (1) and (2) are used
to merge two neighboring blocks A1 and A2 in
a straight or inverted order, respectively. The lex-
ical rule (3) is used to translate the source phrase
x into the target phrase y.
 



	




	
 
		

	
 


	


 








Figure 1: The dependency tree for sentence The
UN will provide abundant financial aid to Haiti
next week.
2.2 Dependency Tree
In a given sentence, each word depends on a par-
ent word, except for the root word. The depen-
dency tree for a given sentence reflects the long
distance dependency and grammar relations be-
tween words. Figure 1 shows an example of a de-
pendency tree, where a black arrow points from a
child word to its parent word.
Compared with constituent tree, dependency
tree directly models semantic structure of a sen-
tence in a simpler form. Thus, it provides a desir-
able platform for us to utilize the target-side syn-
tactic knowledge.
3 Dependency-based BTG
3.1 Grammars
In this section, we extend the original BTG into
DepBTG. The rules of DepBTG, which derive
from that of BTG, merge blocks with target de-
pendency structure into a larger one. These rules
take the following forms:
Ad ? [A1d, A2d]CC (4)
Ad ? [A1d, A2d]LA (5)
Ad ? [A1d, A2d]RA (6)
Ad ? ?A1d, A2d?CC (7)
Ad ? ?A1d, A2d?LA (8)
Ad ? ?A1d, A2d?RA (9)
Ad ? x/y (10)
where A1d and A2d represent two neighboring
blocks with target dependency structure. Rules
(4)?(9) are used to determine the reordering and
combination of two dependency structures, when
1186

Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1897?1907, Dublin, Ireland, August 23-29 2014.
A Neural Reordering Model for Phrase-based Translation
Peng Li
?
Yang Liu
?
Maosong Sun
?
Tatsuya Izuha
?
Dakun Zhang
?
?
State Key Laboratory of Intelligent Technology and Systems
Tsinghua National Laboratory for Information Science and Technology
Department of Computer Sci. and Tech., Tsinghua University, Beijing, China
pengli09@gmail.com, {liuyang2011,sms}@tsinghua.edu.cn
?
Toshiba Corporation Corporate Research & Development Center
tatsuya.izuha@toshiba.co.jp
?
Toshiba (China) R&D Center
zhangdakun@toshiba.com.cn
Abstract
While lexicalized reordering models have been widely used in phrase-based translation systems,
they suffer from three drawbacks: context insensitivity, ambiguity, and sparsity. We propose a
neural reordering model that conditions reordering probabilities on the words of both the current
and previous phrase pairs. Including the words of previous phrase pairs significantly improves
context sensitivity and reduces reordering ambiguity. To alleviate the data sparsity problem, we
build one classifier for all phrase pairs, which are represented as continuous space vectors. Ex-
periments on the NIST Chinese-English datasets show that our neural reordering model achieves
significant improvements over state-of-the-art lexicalized reordering models.
1 Introduction
Reordering plays a crucial role in phrase-based translation (Koehn et al., 2003; Och and Ney, 2004).
While local reordering can be directly memorized in phrases, modeling reordering at a phrase level still
remains a major challenge: it can be cast as a travelling salesman problem and proves to be NP-complete
(Knight, 1999; Zaslavskiy et al., 2009).
The past decade has witnessed the rapid development of phrase reordering models (e.g., (Och et al.,
2004; Tillman, 2004; Zens et al., 2004; Xiong et al., 2006; Al-Onaizan and Papineni, 2006; Koehn et
al., 2007; Galley and Manning, 2008; Feng et al., 2010; Green et al., 2010; Bisazza and Federico, 2012;
Cherry, 2013), just to name a few). Among them, lexicalized reordering models (Tillman, 2004; Koehn
et al., 2007; Galley and Manning, 2008) have been widely used in practical phrase-based systems. Un-
like the distance-based reordering model (Koehn et al., 2003) that only penalizes phrase displacements
in terms of the degree of nonmonotonicity, lexicalized reordering models introduce reordering probabil-
ities conditioned on the words of each phrase pair. They often distinguish between three orientations
with respect to the previous phrase pair: monotone, swap, and discontinuous. As lexicalized reordering
models capture the phenomenon that some words are far more likely to be displaced than others, they
outperform unlexicalized reordering models substantially.
Despite their apparent success in statistical machine translation, lexicalized reordering models suffer
from the following three drawbacks:
1. Context insensitivity. Lexicalized reordering models determine the orientations only depending on
the words of current phrase pairs. In fact, a phrase pair usually has different orientations in different
contexts. It is important to include more contexts to improve the expressive power of reordering
models.
2. Ambiguity. Short phrase pairs, which are observed in the training data more frequently, usually have
multiple orientations. We observe that about 92.4% of one-word Chinese-English phrase pairs are
ambiguous. This makes it hard to decide which orientation should be properly used in decoding.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1897
Figure 1: Ambiguity in phrase reordering. The phrase pair ??yingyun?, ?business?? is labeled with
different orientations in different contexts: (a) monotone, (b) swap, and (c) discontinuous. Lexicalized
reordering models use fixed probability distributions (e.g., 17.50% for M, 1.59% for S, and 80.92% for
D) in decoding even though the surrounding contexts keep changing.
3. Sparsity. Lexicalized reordering models maintain a reordering probability distribution for each
phrase pair. As most long phrase pairs that are capable of memorizing local word selection and
reordering only occur once in the training data, maximum likelihood estimation can hardly train the
models accurately.
In this work, we propose a neural reordering model for phrase-based translation. The contribution is
twofold. Firstly, unlike conventional lexicalized reordering models, the neural reordering model condi-
tions reordering probabilities on the words of both the current and previous phrase pairs. Including the
words of previous phrase pairs significantly improves context sensitivity and reduces reordering ambi-
guity. Secondly, to alleviate the data sparsity problem, we build a neural classifier for all phrase pairs,
which are represented as continuous space vectors. Experiments on the NIST Chinese-English datasets
show that our neural reordering model achieves significant improvements over state-of-the-art lexicalized
models.
2 Lexicalized Reordering Models
The lexicalized reordering models (Tillman, 2004; Koehn et al., 2007; Galley and Manning, 2008) have
become the de facto standard in modern phrase-based systems. These models are called lexicalized
because they condition reordering probabilities on the words of each phrase pair. Depending on the
relationship between the current and previous phrase pairs, lexicalized reordering models often define
orientations to classify different reordering patterns.
More formally, we use f = {
?
f
1
, . . . ,
?
f
n
} to denote a sequence of source phrases, e = {e?
1
, . . . , e?
n
}
to denote the phrase sequence on the target side, and a = {a
1
, . . . , a
n
} to denote the alignment be-
tween source and target phrases. A source phrase
?
f
a
i
and a target phrase e?
i
form a phrase pair. Lex-
icalized reordering models aim to estimate the conditional probability of a sequence of orientations
o = {o
1
, . . . , o
n
}:
P (o|f , e,a) =
n
?
i=1
P (o
i
|f , e?
1
, . . . , e?
i
, a
1
, . . . , a
i
) (1)
where each o
i
takes values over a set of predefined orientations. For simplicity, current lexicalized
1898
model
source phrase length
1 2 3 4 5 6 7
P (o
i
|
?
f
a
i
, e?
i
, a
i?1
, a
i
) 92.74 54.01 24.09 14.40 10.78 8.47 6.95
P (o
i
|
?
f
a
i
, e?
i
,
?
f
a
i?1
, e?
a
i?1
, a
i?1
, a
i
) 21.72 5.22 2.63 1.48 0.98 0.67 0.54
Table 1: Percentages of phrase pairs that have multiple orientations. Including previous phrase pairs in
modeling significantly reduces the reordering ambiguity for the M/S/D orientations. For example, while
92.74% of 1-word Chinese-English phrase pairs have multiple orientations observed in the training data,
the ratio dramatically drops to 21.72% if the orientations are conditioned on both the current and previous
phrase pairs.
reordering models use orientations conditioned only on a
i?1
and a
i
:
P (o|f , e,a) ?
n
?
i=1
P (o
i
|
?
f
a
i
, e?
i
, a
i?1
, a
i
) (2)
The most widely used orientations are monotone (M), swap (S), and discontinuous (D):
1
o
i
=
?
?
?
M if a
i
? a
i?1
= 1
S if a
i
? a
i?1
= ?1
D if |a
i
? a
i?1
| =? 1
(3)
As lexicalized reordering models maintain a reordering probability distribution for each phrase pair,
it is hard to accurately learn reordering probabilities for long phrase pairs that are usually observed only
once in the training data. On the contrary, short phrase pairs that occur in the training data for many times
tend to be ambiguous. For example, as shown in Figure 1, a Chinese-English phrase pair ??yingyun?,
?business?? is observed to have different orientations in different contexts.
It is unreasonable to use fixed reordering probability distributions in decoding as the surrounding
contexts keep changing. Previous study shows that considering more contexts into reordering modeling
improves translation performance (Khalilov and Simaan, 2010). Therefore, we need a more powerful
mechanism to include more contexts, resolve the reordering ambiguity, and reduce the data sparsity.
3 A Neural Reordering Model
3.1 The Model
Intuitively, conditioning reordering probabilities on the words of both the current and previous phrase
pairs will significantly reduce both reordering ambiguity and context insensitivity. The new reordering
model is given by
P (o|f , e,a) ?
n
?
i=1
P (o
i
|
?
f
a
i
, e?
i
,
?
f
a
i?1
, e?
i?1
, a
i?1
, a
i
) (4)
where ?
?
f
a
i?1
, e?
i?1
? is the previous phrase pair.
Including the previous phrase pairs improves the context sensitivity. For example, given a phrase pair
??yingyun?, ?business??, its orientation is more likely to be monotone if it is preceded by a noun phrase
pair such as ??xinyongka?, ?credit card??. On the contrary, the probability of the discontinuous orienta-
tion is higher if the previous phrase pairs contain verbs such as ??gaishan?, ?improve??. Therefore, the
new model is capable of capturing the phenomenon that the orientation of a phrase pair depends on its
surrounding contexts.
Another advantage of including previous phrase pairs is the reduction of reordering ambiguity. As
shown in Table 1, 92.74% of 1-word Chinese-English phrase pairs have multiple orientations (i.e., M, S,
1
There are many variants of lexicalized reordering models depending on the model type, orientation, directionality, lan-
guage, and collapsing. See http://www.statmt.org/moses/?n=FactoredTraining.BuildReorderingModel for more details.
1899
and D) observed in the training data. The ratio decreases with the increase of phrase length. In contrast,
the new model is much less ambiguous (e.g., the ratio of ambiguous one-word phrase pairs dramatically
drops to 21.72%) as it is conditioned on both the current and previous phrase pairs.
Unfortunately, including more contexts in modeling also increases the data sparsity. We observe that
about 90% of reordering examples (i.e., the current and previous phrase pairs) are observed only once in
the training data. As a result, it is more difficult to train lexicalized reordering models accurately using
maximum likelihood estimation.
To alleviate the data sparsity problem, we use the following two strategies:
1. Reordering as classification. Instead of maintaining a reordering probability distribution for each
phrase pair, we build a reordering classifier for all phrase pairs (Xiong et al., 2006; Li et al., 2013).
This significantly reduces data sparsity by considering all occurrences of extracted phrase pairs as
training examples. We find that 500, 000 reordering examples suffice to train a robust classifier
(Section 4.5).
2. Continuous space representation. Instead of using a symbolic representation of phrases, we use
a continuous space representation that treats a phrase as a dense real-valued vector (Socher et al.,
2011b; Li et al., 2013). Consider two phrases ?in London? and ?in Centara Grand?. It is usually
easy to predict the orientations of ?in London? because it might be observed in the training data for
many times. This is not the case for ?in Centara Grand? as it might occur only once. However, if
the two phrases happen to have very similar continuous space representations, ?in Centara Grand?
is likely to have a similar reordering probability distribution with ?in London?.
To generate vector space representation for phrases, we follow Socher et al. (2011a) to use recursive
autoencoders. Given two words w
1
and w
2
, suppose their vector space representations are c
1
and c
2
.
The vector space representation p of the two-word phrase {w
1
, w
2
} can be computed using a two-layer
neural network:
p = g
(1)
(W
(1)
[c
1
; c
2
] + b
(1)
) (5)
where [c
1
; c
2
] ? R
2n
is the concatenation of c
1
and c
2
, W
(1)
? R
n?2n
is a weight matrix, b
(1)
is a bias
vector, and g
(1)
is an element-wise activation function.
In order to measure how well p represents c
1
and c
2
, they can be reconstructed using another two-layer
neural network:
[c
?
1
; c
?
2
] = g
(2)
(W
(2)
p + b
(2)
) (6)
where c
?
1
? R
n
and c
?
2
? R
n
are reconstructed vectors of c
1
and c
2
, W
(2)
? R
2n?n
is a weight matrix,
b
(2)
? R
n
is a bias vector, and g
(2)
is an element-wise activation function. The reconstruction error can
be measured by comparing c
1
and c
2
with c
?
1
and c
?
2
. This process runs recursively in a bottom-up style
to obtain the vector space representation of a multi-word phrase (Socher et al., 2011a). Socher et al.
(2011a) find that minimizing the norms of hidden layers leads to the reduction of reconstruction error in
an undesirable way. Therefore, we normalize p such that ||p||
2
= 1.
Treating phrase reordering as a classification problem, we propose a neural reordering classifier that
takes the current and previous phrase pairs as input. The neural network consists of four recursive
autoencoders and a softmax layer. The input of the classifier are the previous phrase pair and the current
phrase pair. Four recursive autoencoders are used to transform the four phrases (i.e.,
?
f
a
i
, e?
i
,
?
f
a
i?1
, e?
i?1
)
into vectors. Then, these vectors are fed to the softmax layer to predict reordering orientations. Note that
the recursive autoencoders for the same language share with the same parameters. Our neural network is
similar to that of Li et al. (2013). The major difference is that Li et al. (2013) need to compute vector
space representation for variable-sized blocks ranging from words to sentences on the fly both in training
and decoding. In contrast, we only need to compute vectors for phrases with up to 7 words in the training
phase, which makes our approach simpler and more scalable to large data.
1900
Formally, given the previous phrase pair ?
?
f
a
i?1
, e?
i?1
?, the current phrase pair ?
?
f
i
, e?
i
? and the orienta-
tion o
i
, the reordering probability is computed as
P (o
i
|
?
f
a
i
, e?
i
,
?
f
a
i?1
, e?
i?1
, a
i?1
, a
i
) = g(W
o
c(
?
f
a
i
, e?
i
,
?
f
a
i?1
, e?
i?1
) + b
o
), (7)
where W
o
is a weight matrix, b
o
is a bias vector, c(
?
f
a
i
, e?
i
,
?
f
a
i?1
, e?
i?1
) is the concatenation of the vectors
of the four phrases.
2
Following Och (2003), we use a linear model in our decoder with conventional features (e.g., trans-
lation probabilities and n-gram language model). The neural reordering model is incorporated into the
discriminative framework as an additional feature.
3.2 Training
Training the neural reordering model involves minimizing the following two kinds of errors:
? Reconstruction error: It measures how well the computed vector space representations represent
the input vectors. It is defined as the average reconstruction error of all the parent nodes in the trees
formed during computing the vector space representation for all the phrases in the training data.
? Classification error: It measures how well the resulting classifier predicts the reordering orienta-
tions. It is defined as the average cross-entropy errors of all the training examples.
In our experiments, the objective function is a linear interpolation of the reconstruction error and the
classification error.
Following Socher et al. (2011b), we use L-BFGS (Liu and Nocedal, 1989) to optimize the parameters.
At the beginning of each iteration, a binary tree for each phrase is constructed using a greedy algorithm
(Socher et al., 2011b).
3
With these trees fixed, the partial derivatives with respect to parameters are
computed via the backpropagation through structures algorithm (Goller and Kuchler, 1996).
When optimizing the parameters of the softmax layer, the training procedure keeps the parameters of
the recursive autoencoders and word embedding matrices fixed. The corresponding error function is the
classification error as described above. We also use L-BFGS to optimize the parameters and the standard
error backpropagation algorithm (Rumelhart et al., 1986) to compute the derivatives.
3.3 Decoding
As the vector space representation of a phrase is calculated based on all the words in the phrase, using
the neural reordering model complicates the conditions for risk-free hypothesis recombination (Koehn et
al., 2003). Therefore, many hypotheses are not likely to be recombined if the neural reordering model
is directly integrated in decoding, making the decoder to only explore in a much smaller search space.
4
Therefore, we use Moses to generate search graphs and then use hypergraph reranking (Huang and
Chiang, 2007; Huang, 2008) to find most probable derivations using the neural reordering model.
4 Experiments
4.1 Data Preparation
We evaluate our reordering model on Chinese-English translation. The training corpus consists of 1.23M
sentence pairs with 32.1M Chinese words and 35.4M English words. A 4-gram language model was
trained on the Xinhua portion of the English GIGAWORD corpus using KenLM (Heafield, 2011), which
contains 398.6M words. We used the NIST 2006 MT Chinese-English dataset as the development set,
and NIST 2002-2005, 2008 MT Chinese-English datasets as the test sets. Case-insensitive BLEU is used
2
In practice, as suggested by Socher et al. (2011b), we feed the four average vectors of the vectors present in each recursive
autoencoders to the softmax layer. Taking ?resident population? as an example, there are three vectors in the binary tree used
by the corresponding recursive autoencoder, denoted as x?
1
, x?
2
and x?
3
. The average vector is computed as x? =
1
3
?
3
i=1
x?
i
.
3
As phrases in phrase-based translation are not necessarily syntactic constituents, we do not use parse trees in this work.
4
Experimental results show that we can only achieve comparable performance with Moses by integrating neural reordering
model directly in decoding.
1901
Model Orientation MT06 MT02 MT03 MT04 MT05 MT08
distance N/A 29.56 31.40 31.27 31.34 29.98 23.87
word
M/S/D 30.19 32.03 31.86 32.09 30.55 24.20
left/right 30.17 31.98 31.52 31.98 30.19 24.30
phrase
M/S/D 30.24 32.35 31.85 32.00 30.78 24.33
left/right 29.57 32.64 31.53 31.90 30.70 24.28
hierarchical
M/S/D 30.46 32.52 31.89 32.09 30.39 24.11
left/right 30.03 32.13 31.59 31.91 30.21 24.41
neural
M/S/D 30.68 32.19 31.94 32.20 30.81 24.71
left/right 31.03** 33.03** 32.48** 32.52** 31.11* 25.20**
Table 2: Comparison of distance-based, lexicalized, and neural reordering models in terms of case-
insensitive BLEU-4 scores. ?distance? denotes the distance-based reordering model (Koehn et al., 2003),
?word? denotes the word-based lexicalized model (Tillman, 2004), ?phrase? denotes the phrase-based
lexicalized model (Koehn et al., 2007), ?hierarchical? denotes the hierarchical phrase-based reordering
model (Galley and Manning, 2008), and ?neural? denotes our model. The ?left? and ?right? orientations
only considers whether the current source phrase is on the left of the previous source phrase or not. We
use ?*? to highlight the result that is significantly better than the best baseline (highlighted in italic)
at p < 0.05 level and ?**? at p < 0.01 level. The neural model does not work well for the M/S/D
orientations due to the non-separability problem (Section 4.3).
as the evaluation metric. As a trade-off between expressive power and computational cost, we set the
dimension of the word embedding vectors to 25.
5
Both g
(1)
and g
(2)
are set to tanh(?). The other
hyperparameters are optimized via random search (Bergstra and Bengio, 2012).
4.2 Comparison of Distance-based, Lexicalized, and Neural Reordering Models
We compare three kinds of reordering models with increasing expressive power:
1. distance-based model: penalizing phrase displacements proportionally to the amount of nonmono-
tonicity (Koehn et al., 2003);
2. lexicalized models: conditioning the reordering probabilities on the current phrase pairs. The ori-
entations can be determined with respect to words (Tillman, 2004), phrases (Koehn et al., 2007), or
hierarchical phrases (Galley and Manning, 2008);
3. neural model: conditioning the reordering probabilities on both the current and previous phrase
pairs.
For lexicalized and neural models, we further distinguish between two kinds of orientation sets:
{monotone, swap, discontinuous} and {left, right}. The left/right orientations only consider whether
the current source phrase is on the left of the previous source phrase or not. Therefore, swap and
discontinuous-left are merged into left while monotone and discontinuous-right into right.
All these reordering models are tested using Moses (Koehn et al., 2007), except that the neural model
needs an additional hypergraph reranking procedure (Section 3.3). Implemented using Java, it takes the
reranker 0.748 second to rerank a hypergraph on average.
Table 2 shows the case-insensitive BLEU-scores of distance-based, lexicalized, and neural reordering
models on the NIST Chinese-English datasets. ?distance? denotes the distance-based reordering model
(Koehn et al., 2003), ?word? denotes the word-based lexicalized model (Tillman, 2004), ?phrase? denotes
the phrase-based lexicalized model (Koehn et al., 2007), ?hierarchical? denotes the hierarchical phrase-
based reordering model (Galley and Manning, 2008), and ?neural? denotes our model.
5
We find that the dimensions of vectors do not have a significant impact on translation performance. For efficiency, we set
the dimension to 25.
1902
Figure 2: The non-separability problem for the neural reordering model. Given an aligned Chinese-
English sentence pair, the unaligned Chinese word ?de? makes a big difference in determining M/S/D
orientations. In (a), ?de? is included in the previous source phrase and thus the orientation is monotone.
In (b), however, it is not included in the previous source phrase and the orientation is discontinuous. In
our neural reordering model, ?liu wan de? and ?liu wan? have very similar vector space representations
yet different orientations (i.e., M and D). In other words, training examples labeled with M, S, D are
prone to be mixed with each other in the vector space. Therefore, it is difficult to find a hyperplane to
separate M, S and D examples in the high-dimensional space.
We find that lexicalized reordering models obtain significant improvements over the distance-based
model, which indicates that conditioning reordering probabilities on the words of the current phrase
pairs does improve the expressive power. Our neural model using left/right orientations significantly
outperforms all variants of lexicalized models. We use ?*? to highlight the result that is significantly
better than the best baseline (highlighted in italic) at p < 0.05 level and ?**? at p < 0.01 level. This
suggests that conditioning reordering probabilities on the words of current and previous phrase pairs is
helpful for resolving reordering ambiguities and reducing context insensitivity.
4.3 The Non-Separability Problem
In Table 2, the neural model using the M/S/D orientations fails to outperform lexicalized models signifi-
cantly. One possible reason is that the neural model suffers from the non-separability problem due to the
M/S/D orientations.
As shown in Figure 2, given an aligned Chinese-English sentence pair, the unaligned Chinese function
word ?de? makes a big difference in determining M/S/D orientations. In (a), ?de? is included in the
previous source phrase and thus the orientation is monotone. In (b), however, ?de? is not included in the
previous source phrase and the orientation is discontinuous. In our neural reordering model, ?liu wan
de? and ?liu wan? have very similar vector space representations yet different orientations (i.e., M and
D). In other words, training examples labeled with M, S, D are prone to be mixed with each other in
the vector space. Therefore, it is difficult to find a hyperplane to separate M, S and D examples in the
high-dimensional space.
Fortunately, we find that using the left/right orientations can alleviate this problem. As the left/right
orientations only consider whether the current source phrase is on the left of the previous source phrase
or not, unaligned source words will not change orientations. For example, both Figure 2(a) and 2(b) are
identified as the right orientation.
As a result, using left/right orientations in the neural reordering model not only has a higher classifi-
cation accuracy (85%) over using the M/S/D orientations (69%), but also achieves higher BLEU scores
on all NIST datasets systematically.
4.4 The Effect of Distortion Limit
Figure 3 shows the performance of the lexicalized model and our neural model with various distortion
limits. The lexicalized model is the word-based model with M/S/D orientations. The neural model uses
left/right orientations. The neural model consistently outperforms the lexicalized model, especially for
large distortion limits. This finding suggests that the neural model is superior to lexicalized models in
predicting long-distance reordering.
1903
2 4 6 822
23
24
25
Distortion Limit
BLEU
 
 
neurallexicalized
Figure 3: BLEU with various distortion limits.
# examples Accuracy BLEU
100,000 83.55 30.92
200,000 84.40 31.03
300,000 84.55 31.01
400,000 84.95 30.93
500,000 85.25 31.27
3,000,000 85.55 31.03
Table 3: Effect of training corpus size.
Vectors MT06 MT02 MT03 MT04 MT05 MT08
ours 31.03 33.03 32.48 32.52 31.11 25.20
word2vec 30.44 32.28 32.00 32.07 30.24 24.54
Table 4: Comparison of neural reordering models trained based on word vectors produced by our model
(ours) and word2vec (Mikolov et al., 2013).
4.5 The Effect of Training Corpus Size
Table 3 shows the classification accuracy and translation performance with various number of randomly
sampled reordering examples for training the neural classifier. The classification accuracy and transla-
tion performance generally rise as the number of reordering example increases.
6
Surprisingly, both the
classification accuracy and translation performance of using 500,000 reordering examples are close to
using 3,000,000 reordering examples, suggesting that a relatively small amount of reordering examples
are enough for training a robust classifier.
4.6 Learned Vector Space Representations
We randomly sampled 200,000 English phrases and found 999 clusters according to the vector space
representations computed by recursive autoencoders using the k-means algorithm (MacQueen, 1967).
The distance between two phrases is calculated by the Euclidean distance between their vector space
representations.
Figure 4 shows 10 of the 999 clusters. An interesting finding is that phrase pairs that are close in the
vector space share with similar reordering patterns rather than semantic similarity. For example, ?by
june 1? and ?within the agencies? have similar distributions on the left/right orientations but are totally
unrelated in terms of meaning. As a result, the vector representations of words trained using unlabeled
data hardly helps in training the neural reordering model. Table 4 shows the results when we replace
the word vectors of our model with those trained using word2vec (Mikolov et al., 2013). The recursive
autoencoders and the classifier are retrained. The performance of the neural reordering model trained in
this way drops significantly, which confirms our analysis.
5 Related Work
Reordering as classification is a common way to alleviate the data sparsity problem. Xiong et al. (2006)
use a maximum entropy model to predict whether to merge two blocks in a straight or an inverted order
in their ITG decoder. Nguyen et al. (2009) build a similar model for hierarchical phrase reordering
models (Galley and Manning, 2008). Green et al. (2010) and Yahyaei and Monz (2010) predict finer-
grained distance bins instead. Another direction is to learn sparse reordering features and create more
flexible distributions (Cherry, 2013). Although these models are effective, feature engineering is a major
challenge. In contrast, our neural reordering model is capable of learning features automatically.
6
The reason why the BLEU scores oscillate slightly on the training set is that classification accuracy is not directly correlated
with BLEU scores. Optimizing the neural reordering model directly with respect to BLEU score may further improve the
performance. We leave this for future work.
1904
but is willing toeconomy is required to
range of services to said his visit is to
is making use of
june 18, 2001late 2011
as detention centergroup all togethertake care of oldby june 1
and complete by end 1998
or other economicand for other
within the agencies
Figure 4: Phrase clusters as calculated by the Euclidean distance in the vector space. English phrases
that have similar reordering probability distributions rather than similar semantic similarity fall into one
cluster.
Along another line, n-gram-based models (Mari`no et al., 2006; Durrani et al., 2011; Durrani et al.,
2013) treat translation as Markov chains over minimal translation units (Mari`no et al., 2006; Durrani et
al., 2013) or operations (Durrani et al., 2011) directly. Although naturally leveraging both the source and
target side contexts, these approaches still face the data sparsity problem.
Our work is closely related to Li et al. (2013). The major difference is that Li et al. (2013) need to
compute vector space representation for variable-sized blocks ranging from words to sentences on the
fly both in training and decoding. In contrast, we only need to compute vectors for phrases with up to 7
words in the training phase, which makes our approach simpler and more scalable to large data.
6 Conclusion
We have shown that surrounding context is effective for resolving reordering ambiguities in phrase-based
models. As the data sparseness problem is the major challenge for using context in reordering models,
we propose to use a single classifier based on recursive autoencoders to predict reordering orientations.
Experimental results show that our neural reordering model outperforms the state-of-the-art lexicalized
reordering models significantly and consistently across all the NIST datasets under various settings.
There are a few future directions we plan to explore. First, as the machine translation system and neu-
ral classifier are trained separately, the neural network training only has an indirect effect on translation
quality. Jointly training the machine translation system and neural classifier is an interesting topic. Sec-
ond, it is interesting to develop more efficient models to leverage larger contexts to resolve reordering
ambiguities. Third, we plan to extend our work to other translation models such as syntax-based and
n-gram based models (Mari`no et al., 2006; Durrani et al., 2011; Durrani et al., 2013). Finally, as we cast
phrase reordering as two-category classification problem (i.e, left vs. right), it is interesting to intersect
structured SVM (Tsochantaridis et al., 2005) with neural networks to develop a large margin training
algorithm for our neural reordering model.
Acknowledgements
This research is supported by the 973 Program (No. 2014CB340501), the National Natural Science
Foundation of China (No. 61331013), the 863 Program (No. 2012AA011102), Toshiba Corporation
Corporate Research & Development Center, and the Singapore National Research Foundation under its
International Research Centre @ Singapore Funding Initiative and administered by the IDM Programme.
1905
References
Yaser Al-Onaizan and Kishore Papineni. 2006. Distortion models for statistical machine translation. In Pro-
ceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the
Association for Computational Linguistics, pages 529?536.
James Bergstra and Yoshua Bengio. 2012. Random search for hyper-parameter optimization. Journal of Machine
Learning Research, 13(1):281?305.
Arianna Bisazza and Marcello Federico. 2012. Modified distortion matrices for phrase-based statistical machine
translation. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Vol-
ume 1: Long Papers), pages 478?487.
Colin Cherry. 2013. Improved reordering for phrase-based translation using sparse features. In Proceedings of
the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human
Language Technologies, pages 22?31.
Nadir Durrani, Helmut Schmid, and Alexander Fraser. 2011. A joint sequence translation model with integrat-
ed reordering. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:
Human Language Technologies, pages 1045?1054.
Nadir Durrani, Alexander Fraser, Helmut Schmid, Hieu Hoang, and Philipp Koehn. 2013. Can Markov models
over minimal translation units help phrase-based SMT? In Proceedings of the 51st Annual Meeting of the
Association for Computational Linguistics (Volume 2: Short Papers), pages 399?405.
Yang Feng, Haitao Mi, Yang Liu, and Qun Liu. 2010. An efficient shift-reduce decoding algorithm for phrased-
based machine translation. In Proceedings of the 23rd International Conference on Computational Linguistics:
Posters, pages 285?293.
Michel Galley and Christopher D. Manning. 2008. A simple and effective hierarchical phrase reordering model.
In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 848?856.
Christoph Goller and Andreas Kuchler. 1996. Learning task-dependent distributed representations by backprop-
agation through structure. In Proceedings of 1996 IEEE International Conference on Neural Networks (Vol-
ume:1), volume 1, pages 347?352.
Spence Green, Michel Galley, and Christopher D. Manning. 2010. Improved models of distortion cost for statis-
tical machine translation. In Proceedings of Human Language Technologies: The 2010 Annual Conference of
the North American Chapter of the Association for Computational Linguistics, pages 867?875.
Kenneth Heafield. 2011. KenLM: faster and smaller language model queries. In Proceedings of the EMNLP 2011
Sixth Workshop on Statistical Machine Translation, pages 187?197.
Liang Huang and David Chiang. 2007. Forest rescoring: Faster decoding with integrated language models. In
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 144?151.
Liang Huang. 2008. Forest reranking: Discriminative parsing with non-local features. In Proceedings of the
46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages
586?594.
Maxim Khalilov and Khalil Simaan. 2010. Source reordering using maxent classifiers and supertags. In Proceed-
ings of The 14th Annual Conference of the European Association for Machine Translation, pages 292?299.
Kevin Knight. 1999. Decoding complexity in word-replacement translation models. Computational Linguistics,
25(4):607?615.
Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings
of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on
Human Language Technology-Volume 1, pages 48?54.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke
Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan
Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the 45th
Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo
and Poster Sessions, pages 177?180.
Peng Li, Yang Liu, and Maosong Sun. 2013. Recursive autoencoders for ITG-based translation. In Proceedings
of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 567?577.
1906
DongC. Liu and Jorge Nocedal. 1989. On the limited memory BFGS method for large scale optimization. Math-
ematical Programming, 45(1-3):503?528.
James MacQueen. 1967. Some methods for classification and analysis of multivariate observations. In Proceed-
ings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability, volume 1, pages 281?297.
Jos?e B. Mari`no, Rafael E. Banchs, Josep M. Crego, Adri`a de Gispert, Patrik Lambert, Jos?e A. R. Fonollosa, and
Marta R. Costa-juss`a. 2006. N-gram-based machine translation. Computational Linguistics, 32(4):527?549.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. 2013. Linguistic regularities in continuous space word rep-
resentations. In Proceedings of the 2013 Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies, pages 746?751.
Vinh Van Nguyen, Akira Shimazu, Minh Le Nguyen, and Thai Phuong Nguyen. 2009. Improving a lexicalized
hierarchical reordering model using maximum entropy. In Proceedings of The twelfth Machine Translation
Summit (MT Summit XII).
Franz Josef Och and Hermann Ney. 2004. The alignment template approach to statistical machine translation.
Computational Linguistics, 30(4):417?449.
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur, Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar Kumar,
Libin Shen, David Smith, Katherine Eng, Viren Jain, Zhen Jin, and Dragomir Radev. 2004. A smorgasbord
of features for statistical machine translation. In Proceedings of the Human Language Technology Conference
of the North American Chapter of the Association for Computational Linguistics: HLT-NAACL 2004: Main
Proceedings, pages 161?168.
Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the 41st
Annual Meeting of the Association for Computational Linguistics, pages 160?167.
David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. 1986. Learning internal representations by
error propagation. Parallel Distributed Processing: Explorations in the Microstructure of Cognition, Volume 1:
Foundations, pages 318?362.
Richard Socher, Eric H. Huang, Jeffrey Pennin, Andrew Y. Ng, and Christopher D. Manning. 2011a. Dynamic
pooling and unfolding recursive autoencoders for paraphrase detection. In Proceedings of Advances in Neural
Information Processing Systems 24, pages 801?809.
Richard Socher, Jeffrey Pennington, Eric H. Huang, Andrew Y. Ng, and Christopher D. Manning. 2011b. Semi-
supervised recursive autoencoders for predicting sentiment distributions. In Proceedings of the 2011 Conference
on Empirical Methods in Natural Language Processing, pages 151?161.
Christoph Tillman. 2004. A unigram orientation model for statistical machine translation. In Proceedings of the
Human Language Technology Conference of the North American Chapter of the Association for Computational
Linguistics: HLT-NAACL 2004: Short Papers, pages 101?104.
Ioannis Tsochantaridis, Thorsten Joachims, Thomas Hofmann, and Yasemin Altun. 2005. Large margin methods
for structured and interdependent output variables. Journal of Machine Learning Research, 6:1453?1484.
Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Maximum entropy based phrase reordering model for statistical
machine translation. In Proceedings of the 21st International Conference on Computational Linguistics and
44th Annual Meeting of the Association for Computational Linguistics, pages 521?528.
Sirvan Yahyaei and Christof Monz. 2010. Dynamic distortion in a discriminative reordering model for statistical
machine translation. In Proceedings of the 7th International Workshop on Spoken Language Translation, pages
353?360.
Mikhail Zaslavskiy, Marc Dymetman, and Nicola Cancedda. 2009. Phrase-based statistical machine translation
as a traveling salesman problem. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 333?341.
Richard Zens, Hermann Ney, Taro Watanabe, and Eiichiro Sumita. 2004. Reordering constraints for phrase-
based statistical machine translation. In Proceedings of the 20th International Conference on Computational
Linguistics, pages 205?211.
1907
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 2031?2041, Dublin, Ireland, August 23-29 2014.
Query Lattice for Translation Retrieval
Meiping Dong
?
, Yong Cheng
?
, Yang Liu
?
, Jia Xu
?
, Maosong Sun
?
,
Tatsuya Izuha

, Jie Hao
#
?
State Key Laboratory of Intelligent Technology and Systems
Tsinghua National Laboratory for Information Science and Technology
Department of Computer Sci. and Tech., Tsinghua University, Beijing, China
hellodmp@163.com, {liuyang2011,sms}@tsinghua.edu.cn
?
Institute for Interdisciplinary Information Sciences
Tsinghua University, Beijing, China
chengyong3001@gmail.com, xu@tsinghua.edu.cn

Toshiba Corporation Corporate Research & Development Center
tatsuya.izuha@toshiba.co.jp
#
Toshiba (China) R&D Center
haojie@toshiba.com.cn
Abstract
Translation retrieval aims to find the most likely translation among a set of target-language strings
for a given source-language string. Previous studies consider the single-best translation as a query
for information retrieval, which may result in translation error propagation. To alleviate this
problem, we propose to use the query lattice, which is a compact representation of exponentially
many queries containing translation alternatives. We verified the effectiveness of query lattice
through experiments, where our method explores a much larger search space (from 1 query to
1.24 ? 10
62
queries), runs much faster (from 0.75 to 0.13 second per sentence), and retrieves
more accurately (from 83.76% to 93.16% in precision) than the standard method based on the
query single-best. In addition, we show that query lattice significantly outperforms the method
of (Munteanu and Marcu, 2005) on the task of parallel sentence mining from comparable corpora.
1 Introduction
Translation retrieval aims to search for the most probable translation candidate from a set of target-
language strings for a given source-language string. Early translation retrieval methods were widely
used in example-based and memory-based translation systems (Sato and Nagao, 1990; Nirenburg et al.,
1993; Baldwin and Tanaka, 2000; Baldwin, 2001). Often, the document set is a list of translation records
that are pairs of source-language and target-language strings. Given an input source string, the retrieval
system returns a translation record of maximum similarity to the input on the source side. Although these
methods prove to be effective in example-based and memory-based translation systems, they heavily rely
on parallel corpora that are limited both in size and domain.
More recently, Liu et al. (2012) have proposed a new translation retrieval architecture that depends
only on monolingual corpora. Given an input source string, their system retrieves translation candidates
from a set of target-language sentences. This can be done by combining machine translation (MT) and
information retrieval (IR): machine translation is used to transform the input source string to a coarse
translation, which serves as a query to retrieve the most probable translation in the monolingual corpus.
Therefore, it is possible for translation retrieval to have access to a huge volume of monolingual corpora
that are readily available on the Web.
However, the MT + IR pipeline suffers from the translation error propagation problem. Liu et al.
(2012) use 1-best translations, which are inevitably erroneous due to the ambiguity and structural di-
vergence of natural languages, as queries to the IR module. As a result, translation mistakes will be
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
Corresponding author: Jia Xu. Tel: +86-10-62781693 Ext 1683. Homepage: iiis.tsinghua.edu.cn/?xu
2031
propagated to the retrieval process. This situation aggravates when high-accuracy MT systems are not
available for resource-scarce languages.
In this work, we propose to use query lattice in translation retrieval to alleviate the translation error
propagation problem. A query lattice is a compact representation of exponentially many queries. We
design a retrieval algorithm that takes the query lattice as input to search for the most probable translation
candidate from a set of target-language sentences. As compared with Liu et al. (2012), our approach
explores a much larger search space (from 1 query to 1.24? 10
62
queries), runs much faster (from 0.75
second per sentence to 0.13), and retrieves more accurately (from 83.76% to 93.16%). We also evaluate
our approach on extracting parallel sentences from comparable corpora. Experiments show that our
translation retrieval system significantly outperforms a state-of-the-art parallel corpus mining system.
2 Related Work
Our work is inspired by three research topics: retrieving translation candidates from parallel corpus,
using lattice to compactly represent exponentially many alternatives, and using lattice as query in infor-
mation retrieval.
1. Translation Retrieval using Parallel Corpus. The idea of retrieving translation candidates from
existing texts originated in example-based and memory-based translation (Sato and Nagao, 1990;
Nirenburg et al., 1993; Baldwin and Tanaka, 2000; Baldwin, 2001). As these early efforts use
a parallel corpus (e.g., translation records that are pairs of source-language and target-language
strings), they focus on calculating the similarity between two source-language strings. In contrast,
we evaluate the translational equivalence of a given source string and a target string in a large
monolingual corpus.
2. Lattice in Machine Translation. Lattices have been widely used in machine translation: consider-
ing Chinese word segmentation alternatives (Xu et al., 2005), speech recognition candidates (Mat-
soukas et al., 2007), SCFG (Dyer et al., 2008) and so on in the decoding process, minimum bayes
risk decoding (Tromble et al., 2008), minimum error rate training (Macherey et al., 2008), system
combination (Feng et al., 2009), just to name a few. In this work, we are interested in how to use a
lattice that encodes exponentially many translation candidates as a single query to retrieve similar
target sentences via an information retrieval system.
3. Query Lattice in Information Retrieval. The use of lattices in information retrieval dates back to
Moore (1958). Most current lattice-based IR systems often treat lattices as conceptional hierarchies
or thesauri in formal concept analysis (Priss, 2000; Cheung and Vogel, 2005). In spoken document
retrieval, however, lattices are used as a compact representation of multiple speech recognition
transcripts to estimate the expected counts of words in each document (Saraclar and Sproat, 2004;
Zhou et al., 2006; Chia et al., 2010). Our work is significantly different from previous work that
uses the bag-of-words model because translation retrieval must take structure and dependencies in
text into account to ensure translational equivalence.
3 Query Lattice for Translation Retrieval
3.1 Translation Retrieval
Let f be a source-language string, E be a set of target-language strings, the problem is how to find the
most probable translation
?
e from E. Note that E is a monolingual corpus rather than a parallel corpus.
Therefore, string matching on the source side (Sato and Nagao, 1990; Nirenburg et al., 1993; Baldwin
and Tanaka, 2000; Baldwin, 2001) does not apply here.
We use P (e|f) to denote the probability that a target-language sentence e is the translation of a source-
language sentence f . As suggested by Liu et al. (2012), it can be decomposed into two sub-models by
2032
introducing a coarse translation q as a hidden variable:
P (e|f) =
?
q?Q(f)
P (q, e|f) (1)
=
?
q?Q(f)
P (q|f)? P (e|q, f) (2)
where P (q|f) is a translation sub-model, P (e|q, f) is a retrieval sub-model, and Q(f) is the set of all
possible translations of the sentence f . Note that q actually serves as a query to the retrieval sub-model.
To take advantage of various translation and retrieval information sources, we use a log-linear model
(Och and Ney, 2002) to define the conditional probability of a query q and a target sentence e conditioned
on a source sentence f parameterized by a real-valued vector ?:
P (q, e|f ;?) =
exp(? ? h(q, e, f))
?
q
?
?Q(f)
?
e
?
?E
exp(? ? h(q
?
, e
?
, f))
(3)
where h(?) is a vector of feature functions and ? is the corresponding feature weight vector.
Accordingly, the decision rule for the latent variable model is given by
?
e = argmax
e?E
{
?
q?Q(f)
exp(? ? h(q, e, f))
}
(4)
As there are exponentially many queries, it is efficient to approximate the summation over all possible
queries by using maximization instead:
?
e ? argmax
e?E
{
max
q?Q(f)
{
? ? h(q, e, f)
}
}
(5)
Unfortunately, the search space is still prohibitively large since we need to enumerate all possible
queries. Liu et al. (2012) split Eq. (5) into two steps. In the first step, a translation module runs to
produce the 1-best translation
?
q of the input string f as a query:
?
q ? argmax
q?Q(f)
{
?
t
? h
t
(q, e, f)
}
(6)
where h
t
(?) is a vector of translation features and ?
t
is the corresponding feature weight vector. In the
second step, a monolingual retrieval module takes the 1-best translation
?
q as a query to search for the
target string
?
e with the highest score:
?
e ? argmax
e?E
{
?
r
? h
r
(
?
q, e, f)
}
(7)
where h
r
(?) is a vector of retrieval features and ?
r
is the corresponding feature weight vector.
Due to the ambiguity of translation, however, state-of-the-art MT systems are still far from producing
high-quality translations, especially for distantly-related languages. As a result, the 1-best translations
are usually erroneous and potentially introduce retrieval mistakes.
A natural solution is to use n-best lists as queries:
?
e ? argmax
e?E
{
max
q?N(f)
{
? ? h(q, e, f)
}
}
(8)
where N(f) ? T(f) is the n-best translations of the input source sentence f .
2033
Figure 1: Two kinds of query lattices: (a) search graph that is generated after phrase-based decoding and
(b) translation option graph that is generated before decoding. Translation option graph is more compact
and encodes more translation candidates.
Although using n-best lists apparently improves the retrieval accuracy over using 1-best lists, there
are two disadvantages. First, the decision rule in Eq. (8) requires to enumerate all the n translations and
retrieve for n times. In other words, the time complexity increases linearly. Second, an n-best list only
accounts for a tiny fraction of the exponential search space of translation. To make things worse, there
are usually very few variations in n-best translations because of spurious ambiguity - a situation where
multiple derivations give similar or even identical translations.
Therefore, we need to find a more elegant way to enable the retrieval module to explore exponentially
many queries without sacrificing efficiency.
3.2 Query Lattice
We propose to use query lattice to compactly represent exponentially many queries. For example, given
a source sentence ?bushi yu shalong juxing huitan?, we can use the search graph produced by a phrase-
based translation system (Koehn et al., 2007) as a lattice to encode exponentially many derivations.
Figure 1(a) shows a search graph for the example source sentence. Each edge is labeled with an
English phrase as well as the corresponding translation feature value vector. Node 0 denotes the starting
node. Node 7 and node 8 are two ending nodes. Each path from the starting node to an ending node
denotes a query. Paths that reach the same node in the lattice correspond to recombined hypotheses
that have equivalent feature histories (e.g., coverage, last generated target words, the end of last covered
source phrase, etc) in phrase-based decoding.
However, there are two problems with using search graph as query lattice. First, it is computationally
expensive to run a phrase-based system to generate search graphs. The time complexity for phrase-based
decoding with beam search is O(n
2
b) (Koehn et al., 2007), where n is the length of source string and b is
the beam width. Moreover, the memory requirement is usually very high due to language models. As a
result, translation is often two orders of magnitude slower than retrieval. Second, a search graph has too
many ?duplicate? edges due to different reordering, which increase the time complexity of retrieval (see
Section 3.3). For example, in Figure 1(a), the English phrase ?Sharon? occurs two times due to different
reordering.
Alternatively, we propose to use translation option graph as query lattice. In a phrase-based trans-
lation system, translation options that are phrase pairs matching a substring in the input source string
are collected before decoding. These translation options form a query lattice with monotonic reorder-
ing. Figure 1(b) shows an example translation option graph, in which nodes are sorted according to the
positions of source words. Each edge is labeled with an English phrase as well as the corresponding
translation feature value vector.
We believe that translation option graph has three advantages over search graph:
1. Improved efficiency in translation. Translation option graph requires no decoding.
2. Improved efficiency in retrieval. Translation option graph has no duplicate edges.
2034
Algorithm 1 Retrieval with lattice as query.
1: procedure LATTICERETRIEVE(L(f),E, k)
2: Q? GETWORDS(L(f)) . Get distinct words in the lattice to form a coarse query
3: E
k
? RETRIEVE(E, Q, k) . Retrieve top-k target sentences using the coarse query
4: for all e ? E
k
do
5: FINDPATH(L(f), e) . Find a path with the highest score
6: end for
7: SORT(E
k
) . Sort retrieved sentences according the scores
8: return E
k
9: end procedure
Algorithm 2 Find a path with the highest score.
1: procedure FINDPATH(L(f), e)
2: for v ? L(f) in topological order do
3: path(v)? ? . Initialize the Viterbi path at node v
4: score(v)? 0 . Initialize the Viterbi score at node v
5: for u ? IN(v) do . Enumerate all antecedents
6: p? path(u) ? {e
u?v
} . Generate a new path
7: s? score(u) + COMPUTESCORE(e
u?v
) . Compute the path score
8: if s > score(v) then
9: path(v)? p . Update the Viterbi path
10: score(v)? s . Update the Viterbi score
11: end if
12: end for
13: end for
14: end procedure
3. Enlarged search space. Translation option graph represents the entire search space of monotonic
decoding while search graph prunes many translation candidates.
In Figure 1, the search graph has 9 nodes, 10 edges, 4 paths, and 3 distinct translations. In contrast,
the translation option graph has 6 nodes, 9 edges, 10 paths, and 10 distinct translations. Therefore,
translation option graph is more compact and encodes more translation candidates.
Although translation option graph ignores language model and lexcialized reordering models, which
prove to be critical information sources in machine translation, we find that it achieves comparable or
even better retrieval accuracy than search graph (Section 4). This confirms the finding of Liu et al. (2012)
that language model and lexicalized reordering models only have modest effects on translation retrieval.
3.3 Retrieval with Query Lattice
Given a target corpus E and a query lattice L(f) ? Q(f), our goal is to find the target sentence
?
e with
the highest score ? ? h(q, e, f):
?
e ? argmax
e?E
{
max
q?L(f)
{
? ? h(q, e, f)
}
}
(9)
Due to the exponentially large search space, we use a coarse-to-fine algorithm to search for the target
sentence with the highest score, as shown in Algorithm 1. We use an example to illustrate the basic idea.
Given an input source sentence ?bushi yu shalong juxing le huitan?, our system first generates a query
lattice like Figure 1(a). It is non-trivial to directly feed the query lattice to a retrieval system. Instead, we
would like to first collect all distinct words in the lattice: {?Bush?, ?and? , ?Sharon?, ?held?, ?a?, ?talk?,
?talks?, ?with?}. This set serves as a coarse single query and the retrieval system returns a list of target
sentences that contain these words:
2035
Chinese English
Training 1.21M 1.21M
Dev in-domain query 5K
document 2.23M
out-of-domain query 5K
document 2.23M
Test in-domain query 5K
document 2.23M
out-of-domain query 5K
document 2.23M
Table 1: The datasets for the retrieval evaluation. The training set is used to train the phrase-based
translation model and language model for Moses (Koehn et al., 2007). The development set is used
to optimize feature weights using the minimum-error-rate algorithm (Och, 2003). A development set
consists of a query set and a document set. The test set is used to evaluate the retrieval accuracy. To
examine the effect of domains on retrieval performance, we used two development and test sets: in-
domain and out-domain.
President Bush gave a talk at a meeting
Bush held a meeting with Sharon
Sharon and Bush attended a meeting held at London
Note that as a retrieval system usually ignores the structural dependencies in text, the retrieved sentences
(scored by retrieval features) are relevant but not necessarily translations of the input. Therefore, we
can match each retrieved sentence against the query lattice to find a path with the highest score using
additional translation features. For example, the Viterbi path for ?Bush held a meeting with Sharon? in
Figure 1(a) is ?Bush held talks with Sharon?. The translation features of matched arcs in the path are
collected to compute the overall score according to Eq. (9). Finally, the algorithm returns a sorted list:
Bush held a meeting with Sharon
President Bush gave a talk at a meeting
Sharon and Bush attended a meeting held at London
More formally, the input of Algorithm 1 are a query lattice L(f), a target corpus E, and a parameter
k (line 1). The function GETWORDS simply collects all the distinct words appearing in the lattice (line
2), which are used for constructing a coarse boolean query Q. Then, the function RETRIEVE runs to
retrieve the top-k target sentences E
k
in the target corpus E only using standard IR features according
to the query Q (line 3). These first two steps eliminate most unlikely candidates and return a coarse set
of target sentence candidates efficiently.
1
Then, a procedure FINDPATH(L(f), e) runs to search for the
translation with the highest score for each candidate (lines 4-6). Finally, the algorithm returns the sorted
list of target sentences (lines 7-9).
Algorithm 2 shows the procedure FINDPATH(L(f), e), which searches for the path with higher score
using a Viterbi-style algorithm. The function COMPUTESCORE scores an edge according to the Eq. (9)
which linearly combines the translation and retrieval features.
Generally, the lattice-based retrieval algorithm has a time complexity of O(k|E|), where |E| is the
number of edges in the lattice.
4 Experiments
In this section, we try to answer two questions:
1. Does using query lattices improve translation retrieval accuracy over using n-best lists?
2. How does translation retrieval benefit other end-to-end NLP tasks such as machine translation?
1
In our experiments, we set the parameter k to 500 as a larger value of k does not give significant improvements but introduce
more noises.
2036
Accordingly, we evaluated our system in two tasks: translation retrieval (Section 4.1) and parallel
corpus mining (Section 4.2).
4.1 Evaluation on Translation Retrieval
4.1.1 Experimental Setup
In this section, we evaluate the accuracy of translation retrieval: given a query set (i.e., source sentences),
our system returns a sorted list of target sentences. The evaluation metrics include precision@n and
recall.
The datasets for the retrieval evaluation are summarized in Table 1. The training set, which is used to
train the phrase-based translation model and language model for the-state-of-the-art phrase-based system
Moses (Koehn et al., 2007), contains 1.21M Chinese-English sentences with 32.0M Chinese words and
35.2M English words. We used the SRILM toolkit (Stolcke, 2002) to train a 4-gram language model on
the English side of the training corpus. The development set, which is used to optimize feature weights
using the minimum-error-rate algorithm (Och, 2003), consists of query set and a document set. We
sampled 5K parallel sentences randomly, in which 5K Chinese sentences are used as queries and half
of their parallelled English sentences(2.5K) mixed with other English sentences(2.3M) as the retrieval
document set. As a result, we can compute precision and recall in a noisy setting. The test set is used
to compute retrieval evaluation metrics. To examine the effect of domains on retrieval performance, we
used two data sets: in-domain and out-domain. The in-domain development and test sets are close to
the training set while the out-domain data sets are not.
We compare three variants of translation retrieval: 1-best list, n-best list, and lattice. For query lattice,
we further distinguish between search graph and translation option graph. They are generated by Moses
with the default setting.
We use both translation and retrieval features in the experiments. The translation features include
phrase translation probabilities, phrase penalty, distance-based and lexicalized reordering models, lan-
guage models, and word penalty. Besides the conventional IR features such as term frequency and
inverse document frequency, we use five additional featured derived from BLEU (Papineni et al., 2002):
the n-gram matching precisions between query and retrieved target sentence (n = 1, 2, 3, 4) and brevity
penalty. These features impose structural constraints on retrieval and ensure translation closeness of re-
trieved target sentences. The minimum-error-rate algorithm supports a variety of loss functions. The loss
function we used in our experiment is 1?P@n. Note that using translation option graph as query lattice
does not include language models and distance-based lexicalized reordering models as features.
4.1.2 Evaluation Results
Table 2 shows the results on the in-domain test set. The ?# candidates? column gives the number of
translation candidates explored by the retrieval module for each source sentence on average. The lattices,
either generated by search graph or by translation options, contain exponentially many candidates. We
find that using lattices dramatically improves the precisions over using 1-best and n-best lists. All the
improvements over 1-best and n-best lists are significant statistically. The 1-best, n-best, and the search
graph lattice share with the same translation time: 5,640 seconds for translating 5,000 queries. Note
that the translation time is zero for the translation option graph because it does not need phrase-based
decoding. For retrieval, the time cost for the n-best list method generally increases linearly. As the search
graph lattice contains many edges, the retrieval time increases by an order of magnitude as compared
with 100-best list. An interesting finding is that using translation options as a lattice contains more
candidates and consumes much less time for retrieval than using search graph as a lattice. One possible
reason is that a search graph generated by Moses usually contains many redundant edges. For example,
Figure 1 is actually a search graph and many phrases occurs multiple times in the lattice (e.g., ?and?
and ?Sharon?). In contrast, a lattice built by translation options hardly has any redundant edges but
still represents exponentially many possible translations. We can also see that the lattice constructed by
search graph considering language model can benefit the precision much, especially when n is little. But
this advantage decreases with n increasing and the time consumed by translation options as lattice is
much less than the search graph as lattice. Besides, the margin between them is not too large so we can
2037
method # candidates
P@n time
n=1 n=5 n=10 n=20 n=100 translation retrieval
1-best 1 87.40 91.40 92.24 92.88 93.64 5,640 82
10-best 10 89.84 93.20 93.96 94.36 95.56 5,640 757
100-best 100 90.76 94.32 95.00 95.76 96.76 5,640 7,421
lattice (graph) 1.20? 10
54
93.60 96.08 96.28 96.52 96.80 5,640 89,795
lattice (options) 4.14? 10
62
93.28 95.84 95.96 96.16 96.84 0 307
Table 2: Results on the in-domain test set. We use the minimum-error-rate training algorithm (Och,
2003) to optimize the feature with the respect to 1?P@n.
method # candidates
P@n time
n=1 n=5 n=10 n=20 n=100 translation retrieval
1-best 1 67.32 76.60 79.40 81.80 83.76 3,660 92
10-best 10 72.68 80.96 83.36 85.84 88.76 3,660 863
100-best 100 78.60 85.76 87.76 89.64 92.16 3,660 8,418
lattice (graph) 1.51? 10
61
84.32 89.40 90.68 91.56 92.44 3,660 67,205
lattice (options) 1.24? 10
65
81.92 88.00 89.80 91.24 93.16 0 645
Table 3: Results on the out-of-domain test set.
0.3 0.4 0.5 0.6 0.7 0.8 0.9 10.3
0.4
0.5
0.6
0.7
0.8
0.9
1
recall
preci
sion
 
 
lattice(search graph)lattice(translation options)100?best?list10?best1?best
Figure 2: In-domain Precision-Recall curves.
0.3 0.4 0.5 0.6 0.7 0.8 0.90.3
0.4
0.5
0.6
0.7
0.8
0.9
1
recall
preci
sion
 
 
lattice(search graph)lattice(translation options)100?best?list10?best1?best
Figure 3: Out-domain Precision-Recall curves.
abandon some little precision for obtain the large time reducing. Therefore, using translation options as
lattices seems to be both effective and efficient.
Table 3 shows the results on the out-of-domain test set. While the precisions for all methods drop, the
margins between lattice-based retrieval and n-best list retrieval increase, suggesting that lattice-based
methods are more robust when dealing with noisy datasets.
Figures 2 and 3 show the Precision-Recall curves on the in-domain and out-of-domain test sets. As
the query set is derived from parallel sentences, recall can be computed in our experiments. The curves
show that using lattices clearly outperforms using 1-best and n-best lists. The margins are larger on the
out-of-domain test set.
4.2 Evaluation on Parallel Corpus Mining
In this section, we evaluate translation retrieval on the parallel corpus mining task: extracting a parallel
corpus from a comparable corpus.
4.2.1 Experimental Setup
The comparable corpus for extracting parallel sentences contains news articles published by Xinhua
News Agency from 1995 to 2010. Table 4 shows the detailed statistics. There are 1.2M Chinese and
1.7M English articles.
We re-implemented the method as described in (Munteanu and Marcu, 2005) as the baseline system.
2038
language articles sentences words vocabulary
Chinese 1.2M 18.5M 441.2M 2.1M
English 1.7M 17.8M 440.2M 3.4M
Table 4: The Xinhua News Comparable Corpus from 1995 to 2010
Munteanu and Marcu (2005) this work
English words Chinese words BLEU English words Chinese Words BLEU
5.00M 4.12M 22.84 5.00M 3.98M 25.44
10.00M 8.20M 25.10 10.00M 8.17M 26.62
15.00M 12.26M 25.41 15.00M 12.49M 26.49
20.00M 16.30M 25.56 20.00M 16.90M 26.87
Table 5: Comparison of BLEU scores using parallel corpora extracted by the baseline and our system.
Given a comparable corpus (see Table 4), both systems extract parallel corpora that are used for training
phrase-base models (Koehn et al., 2007). The baseline system is a re-implementation of the method
described in (Munteanu and Marcu, 2005). Our system uses translation option graph as query lattice.
Our system significantly outperforms the baseline for various sizes.
It assigned a score to each sentence pair using a classifier. Our system used translation option graph as
query lattices due to its simplicity and effectiveness. For each source sentence in the comparable corpus,
our system retrieved the top target sentence together with a score.
To evaluate the quality of extracted parallel corpus, we trained phrase-based models on it and ran
Moses on NIST datasets. The development set is the NIST 2005 test set and the test set is the NIST 2006
test set. The final evaluation metric is case-insensitive BLEU-4.
4.2.2 Evaluation Results
Table 5 shows the comparison of BLEU scores using parallel corpora extracted by the baseline and our
system. We find that our system significantly outperforms the baseline for various parallel corpus sizes.
This finding suggests that using lattice to compactly represent exponentially many alternatives does help
to alleviate the translation error propagation problem and identify parallel sentences of high translational
equivalence.
5 Conclusion
In this work, we propose to use query lattice to address the translation error propagation problem in
translation retrieval. Two kinds of query lattices are used in our experiments: search graph and translation
option graph. We show that translation option graph is more compact and represents a much larger
search space. Our experiments on Chinese-English datasets show that using query lattices significantly
outperforms using n-best lists in the retrieval task. Moreover, we show that translation retrieval is capable
of extracting high-quality parallel corpora from a comparable corpus. In the future, we plan to apply
our approach to retrieving translation candidates directly from the Web, which can be seen as a huge
monolingual corpus.
Acknowledgments
This research is supported by the 973 Program (No. 2014CB340501), the National Natural Science
Foundation of China (No. 61331013 and No. 61033001), the 863 Program (No. 2012AA011102),
Toshiba Corporation Corporate Research & Development Center, and the Singapore National Research
Foundation under its International Research Centre @ Singapore Funding Initiative and administered by
the IDM Programme.
2039
References
T. Baldwin and H. Tanaka. 2000. The effects of word order and segmentation on translation retrieval performance.
In Proceedings of COLING.
Timothy Baldwin. 2001. Low-cost, high-performance translation retrieval: Dumber is better. In Proceedings of
ACL, pages 18?25, Toulouse, France, July. Association for Computational Linguistics.
Karen Cheung and Douglas Vogel. 2005. Complexity reduction in lattice-based information retrieval. Information
Retrieval, pages 285?299.
Tee Kiah Chia, Khe Chai Sim, Haizhou Li, and Hwee Tou Ng. 2010. Statistical lattice-based spoken document
retrieval. ACM Transactions on Information Systems, 28(1).
Christopher Dyer, Smaranda Muresan, and Philip Resnik. 2008. Generalizing word lattice translation. In Pro-
ceedings of ACL-HLT, pages 1012?1020, Columbus, Ohio, June. Association for Computational Linguistics.
Yang Feng, Yang Liu, Haitao Mi, Qun Liu, and Yajuan L?u. 2009. Lattice-based system combination for statis-
tical machine translation. In Proceedings of EMNLP, pages 1105?1113, Singapore, August. Association for
Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke
Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan
Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of ACL - Demo
and Poster Sessions, pages 177?180, Prague, Czech Republic, June. Association for Computational Linguistics.
Chunyang Liu, Qi Liu, Yang Liu, and Maosong Sun. 2012. THUTR: A translation retrieval system. In Proceed-
ings of COLING - Demo and Poster Sessions, pages 321?328, Mumbai, India, December. The COLING 2012
Organizing Committee.
Wolfgang Macherey, Franz Och, Ignacio Thayer, and Jakob Uszkoreit. 2008. Lattice-based minimum error rate
training for statistical machine translation. In Proceedings of EMNLP, pages 725?734, Honolulu, Hawaii,
October. Association for Computational Linguistics.
Spyros Matsoukas, Ivan Bulyko, Bing Xiang, Kham Nguyen, Richard Schwartz, and John Makhoul. 2007. In-
tegrating speech recognition and machine translation. In Proceedings of ICASSP, volume 4, pages IV?1281.
IEEE.
C.N. Moore. 1958. A mathematical theory of the use of language symbols in retrieval. In ICSI 1958.
Dragos Munteanu and Daniel Marcu. 2005. Improving machine translation performance by exploiting non-
parallel corpora. Computational Linguistics, 31(4):477?1504.
S. Nirenburg, C. Domashnev, and D.J. Grannes. 1993. Two approaches to matching in example-based machine
translation. In TMI 1993.
Franz Josef Och and Hermann Ney. 2002. Discriminative training and maximum entropy models for statistical ma-
chine translation. In Proceedings of ACL, pages 295?302, Philadelphia, Pennsylvania, USA, July. Association
for Computational Linguistics.
Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of ACL,
pages 160?167, Sapporo, Japan, July. Association for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evalua-
tion of machine translation. In Proceedings of ACL, pages 311?318, Philadelphia, Pennsylvania, USA, July.
Association for Computational Linguistics.
Uta Priss. 2000. Lattice-based information retrieval. Knwoledge Organization, 27(3):132?142.
Murat Saraclar and Richard Sproat. 2004. Lattice-based search for spoken utterance retrieval. In Daniel Marcu
Susan Dumais and Salim Roukos, editors, HLT-NAACL, pages 129?136, Boston, Massachusetts, USA, May.
Association for Computational Linguistics.
S. Sato and M. Nagao. 1990. Toward memory-based translation. In Proceedings of COLING.
Andreas Stolcke. 2002. Srilm: an extensible language modeling toolkit. In Proceedings of ICSLP.
2040
Roy Tromble, Shankar Kumar, Franz Och, and Wolfgang Macherey. 2008. Lattice Minimum Bayes-Risk decoding
for statistical machine translation. In Proceedings of EMNLP, pages 620?629, Honolulu, Hawaii, October.
Association for Computational Linguistics.
Jia Xu, Evgeny Matusov, Richard Zens, and Hermann Ney. 2005. Integrated chinese word segmentation in
statistical machine translation. In Proceedings of IWSLT 2005, pages 141?147, Pittsburgh, PA, October.
Zheng-Yu Zhou, Peng Yu, Ciprian Chelba, and Frank Seide. 2006. Towards spoken-document retrieval for the
internet: Lattice indexing for large-scale web-search architectures. In Proceedings of HLT-NAACL, pages 415?
422, New York City, USA, June. Association for Computational Linguistics.
2041
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 2107?2116, Dublin, Ireland, August 23-29 2014.
Exploring Fine-grained Entity Type Constraints for Distantly Supervised
Relation Extraction
Yang Liu Kang Liu Liheng Xu Jun Zhao
National Laboratory of Pattern Recognition
Institute of Automation, Chinese Academy of Sciences
Zhongguancun East Road #95, Beijing 100190, China
{yang.liu, kliu, lhxu, jzhao}@nlpr.ia.ac.cn
Abstract
Distantly supervised relation extraction, which can automatically generate training data by align-
ing facts in the existing knowledge bases to text, has gained much attention. Previous work used
conjunction features with coarse entity types consisting of only four types to train their model-
s. Entity types are important indicators for a specific relation, for example, if the types of two
entities are ?PERSON? and ?FILM? respectively, then there is more likely a ?DirectorOf? rela-
tion between the two entities. However, the coarse entity types are not sufficient to capture the
constraints of a relation between entities. In this paper, we propose a novel method to explore
fine-grained entity type constraints, and we study a series of methods to integrate the constraints
with the relation extracting model. Experimental results show that our methods achieve bet-
ter precision/recall curves in sentential extraction with smoother curves in aggregated extraction
which mean more stable models.
1 Introduction
Relation Extraction is the task of extracting semantic relations between a pair of entities from sentences
containing them. It can potentially benefit many applications, such as knowledge base construction,
question answering (Ravichandran and Hovy, 2002), textual entailment (Szpektor et al., 2005), etc. Tra-
ditional supervised approaches for relation extraction (Zhou et al., 2005)(Zhou et al., 2007) need to
manually label training data, which is expensive and limits the ability to scale up. Due to the shortcom-
ing of supervised approaches mentioned above, recently, a more promising approach named distantly
supervised relation extraction (or distant supervision for relation extraction) (Mintz et al., 2009) has be-
come popular. Instead of manual labeling, it automatically generates training data by aligning facts in
existing knowledge bases to text.
However, the paradigm of distant supervision also causes new problems of noisy training data both in
positive training instances and negative training instances. To overcome the false positive problem caused
by the distant supervision assumption, researches in (Riedel et al., 2010)(Hoffmann et al., 2011)(Sur-
deanu et al., 2012) proposed multi-instance models to model noisy positive training data, where they
assumed that at least one sentence in those containing an entity pair is truly positive. Takamatsu et al.
(Takamatsu et al., 2012) claimed that the at-least-one assumption in multi-instance models would fail
when there was only one sentence containing both entities. They proposed a method to learn and filter
noisy pattern features from training instances to overcome the false positive problem. Researchers (Xu
et al., 2013)(Zhang et al., 2013)(Ritter and Etzioni, 2013) tried to address the problem of false negative
training data caused by the incomplete knowledge base. Xu el al. (Xu et al., 2013) used the pseudo-
relevance feedback method trying to find out the false negative instances and add them into positive
training instances. Zhang et al. (Zhang et al., 2013) employed some rules to select negative training in-
stances carefully, hoping not to include the false negative instances. And Ritter et al. (Ritter and Etzioni,
2013) used hidden variables to model the missing data in databases based on a graphical model. The
training data generation process for all the above work is under the framework of (Mintz et al., 2009),
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
2107
one important step of which is to recognize entity mentions from text and assign them entity types which
are used to compose features for training the model. The entity types they used are very coarse only con-
sisting of four categories (PERSON, ORGANIZATION, LOCATION, NONE). We argue that the coarse
entity types are not sufficient to indicate relations.
A specific relation constrains the entity types of its two entities. For instance, the SingerOf relation
limits the entity type of its first entity as PERSON or more fine-grained ARTIST, and the entity type of
its second entity as ART or more fine-grained MUSIC. Therefore, when extracting a relation instance,
the entity types of its two entities are important indicators for a specific relation. Previous work used
conjunction features (Details in Section 3.3) by combining the coarse entity types of entity mentions
with its contextual lexical and syntactic features. However, the conjunction features may fail to dis-
tinguish the relations. For example, the following two sentences contain two relation instances, one is
DirectorOf(Ang Lee, Life of Pi), and the other is AuthorOf(George R.R. Martin, A Song of Ice and Fire).
1. Ang Lee?s Life of Pi surprised many by scoring a leading four Oscars on Sunday night...
2. Westeros is the premiere fansite for George R.R. Martin?s A Song of Ice and Fire.
Only using the above conjunction features, we cannot tell the difference between the two entity pairs,
and are probable to incorrectly classify them as the same relation. By contrast, if we can assign each
entity with fine-grained entity types, for example, Ang Lee as the entity type ARTIST and George R.R.
Martin as AUTHOR, we may succeed in classifying the two entity pairs correctly.
To achieve the goal mentioned above, there are mainly three challenges: (1) how to define the fine-
grained type set; (2) how to assign the types to entity mentions; (3) how to integrate the fine-grained
entity type constraints with the relation extracting model. To address these challenges, in this paper,
we propose a novel approach to explore the fine-grained entity type constraints for distantly supervised
relation extraction. First, we use the types defined in (Ling and Weld, 2012) stemmed from Freebase
1
as the fine-grained entity type set (introduced in Section 3.1). Second, we leverage Web knowledge
to train a fine-grained entity type classifier and predict entity types for each entity mention. Third, we
study several methods to integrate the type constraints with an existing system MULTIR, a multi-instance
multi-label model in (Hoffmann et al., 2011), to train the extractor.
In summary, the contribution of this paper can be concluded as follows.
(a) We explore the effect of fine-grained entity type constraints on distantly supervised relation extrac-
tion. A novel method is proposed to leverage Web knowledge to automatically train a fine-grained
entity type classifier, which is used to predict the fine-grained types of each entity mention.
(b) We study a series of methods for integrating the fine-grained entity type constraints with the extract-
ing model and compare their performance with different parameter settings.
(c) We conduct experiments to demonstrate the effects of the newly exploited fine-grained entity type
constraints. It shows that our method achieves a much better precision/recall curves over the base-
line system in sentential extraction, and improves the performance with a smoother precision/recall
curve in aggregated extraction, which means a more stable model.
2 Distant Supervision for Relation Extraction
We define a relation instance (or a fact), which means a binary relation, as r(e
1
, e
2
). r is the relation, and
e
1
and e
2
mean the two entities in the relation instance, for example, BornIn(Y ao Ming, Shanghai).
Distant supervision supplies a method to automatically generate training data. In this part, we will
introduce the general steps in distant supervision for relation extraction. First, we define the notations
we use. ? denotes sentences comprising the corpus, E denotes entity mentions in the corpus which are
consecutive words with the same named entity tags assigned by an NER system, ? denotes the facts (or
relation instances) in the existing knowledge base. R denotes the relations in ?.
1
http://www.freebase.com/
2108
 Figure 1: Fine-grained entity type set.
Figure 2: Framework of fine-grained entity type classifier.
To generate training data, we align pairs of entity mentions in the same sentence with ?. The aligned
entity mentions E
train
and their sentences ?
train
along with R
train
are used as training data. Features
are extracted from them to train the relation extracting model.
To predict the unknown data for extracting new relation instances, we input pairs of entity mentions
E
predict
and the sentences containing them ?
preidct
into the trained extracting model for extracting new
relation instances.
3 Fine-grained Entity Type Constraints
Entity mentions in sentences are considered consecutive words with the same entity types (Section 2).
The entity types are part of the lexical and syntactic features(Mintz et al., 2009), and the feature setting
is followed by other related work. Their entity types are assigned by an NER system and consist of
four categories (PERSON, ORGANIZATION, LOCATION, NONE). The types of entity mentions in
a relation are important indicators for the very type of relation. However, the coarse (only four types)
entity types may not capture sufficient constraints to distinguish a relation. In this section, we explore
fine-grained entity type constraints and study different methods to integrate them with the extracting
model.
This section first introduces the fine-grained entity type set(Section 3.1), and then describes our method
which leverages Web knowledge to train the fine grained entity type classifier and assign entity mentions
with the fine-grained entity types (Section 3.2). At last, we illustrate methods to integrate fine-grained
entity type constraints with the relation extracting model.
2109
Entity pair [Hank Ratner], [Cablevision]
Sentence
Cablevision?s $600 million offer came in the form of a letter to Peter S.Kalikow,
chairman of the M.T.A., from the Garden?s vice chairman, Hank Ratner.
Conjunction Reverse Left NE1 Middle NE2 Right
Feature examples
False PER ORG
False Hank[NMOD] PER [NMOD]chairman ... offer[SBJ] ORG
True B -1 ORG POS $ ... NN NN, PER .B 1
Table 1: Examples of conjunction features.
3.1 Fine-grained Entity Types
Figure 1 is the type set we use. It was introduced in (Ling and Weld, 2012) and was derived from
Freebase types. The bold types in each small box of Figure 1 are upper-class types for others in that
small box. For example, /actor is a lower-class type of /person which is denoted as /person/actor.
And /person and /person/actor coexist in the type set.
3.2 Fine-grained Entity Type Classifier
In this section, we describe our method that leverages Web knowledge to train a fine-grained entity type
classifier and predict entity types of each entity mention. Its architecture is shown in Figure 2.
3.2.1 Training
The training data are obtained from Wikipedia. Because the defined fine-grained types are tailored based
on Freebase types, we can find the mappings between the two type sets, for example, /person/doctor
maps to two Freebase types /medicine/physician and /medicine/surgeon. And Freebase WEX
2
supplies a mapping between Freebase types to Wikipedia articles. As a result, we can map Wikipedia
articles to defined fine-grained types.
Based on the mappings, we obtain Wikipedia articles for each type as training data and negative
training examples are sampled from articles not contained in the mappings. We preprocess the articles
by: stop words filtering, stemming, and term frequency filtering and use a maxent model to train the
classifier.
3.2.2 Predicting
To predict types of each entity mention, we first use search engines to expand entity mentions. Specif-
ically, each entity mention is used as a query sent to the search engine
3
. Titles and descriptions of top
k returned snippets are selected (We keep the top 20 in the experiments). The obtained text are pre-
processed with the same method as training examples. Then we use the trained fine-grained entity type
classifier to predict the types of each entity mention.
After predicting, we obtain a ranked list of types for each entity mention, which are ranked by the
predicting scores.
3.3 Integrating Fine-grained Entity Type Constraints into the Extracting Model
This section introduces our methods to integrate the fine-grained entity type constraints with the ex-
tracting model. First of all, we briefly review the features used in previous models which derived from
(Mintz et al., 2009) and (Riedel et al., 2010). Their features mainly comprise two types: lexical features
(POS tags, words and entity types) and syntactic features (dependency parsing tags, words and entity
types). Each feature is a conjunction with several parts: entity types of two entity mentions, the left
context window of the first entity mention, the right context window of the second entity mention and
the part between them (the window contains none or one or two words ). Table 1 shows an example of
the conjunction features.
2
http://wiki.freebase.com/wiki/WEX
3
We use Bing search API. http://datamarket.azure.com/dataset/bing/search
2110
To integrate the exploited fine-grained entity type constraints with the extracting model, we proposed
three methods (substitution, augment and selection) to make the type constraints take effects.
3.3.1 Substitution Method
In this method, we substitute coarse entity types of the features with the entity mentions? fine-grained
types, and use the new features to train the model. Instead of substituting directly, an entity mention
is first represented by its fine-grained types and the upper-class of the fine-grained type, for example,
/person/politician derives two types /person and /person/politician itself. The reason is that the
extracting model can benefit from the related types like the upper-class types. And then we use the
obtained entity types to substitute the old coarse entity types as new features greedily, which mean-
s that all the possible combinations of types between the entity pair are considered. For example,
?Barack Obama? has the fine-grained type /person/politician and his birth place ?Hawaii? has
the type /location/island, then there are 4 combinations between the two entities, they are (/person,
/location), (/person, /location/island), (/person/politician, /location) and (/person/politician,
/location).
3.3.2 Augment Method
In this method, we generate new features by substituting the coarse entity types with predicted fine-
grained types, and expand the old features with new features. Different from the substitution method, we
do not add the upper-class types, for that we think the coarse types in old features have the same effect.
In this method, we use the fine-grained constraints as a complementary.
3.3.3 Selection Method
The selection method is similar to the augment method. The difference is that we do not expand all
old features with new features. We select some of them to expand. The reason is that some of the
conjunction features are of high-precision themselves, it can clearly indicate the relations with its left,
middle and right parts, even without the entity types (informative ones). If we expand these features,
it may cause more noisy features. So we expect to only expand the ones that lack of the indicating
abilities (non-informative ones). In this paper, we employ a simple method to distinguish between the
informative ones and non-informative ones by the length of the features, which means that the longer is
more informative than the shorter. In our experiments, the length threshold is set as 20.
In the predicting phase (Section 3.2), we obtain a ranked type list for each entity mention. The top list
types are considered in our methods. Experiments in Section 4.3 are conducted on top k {k ? 1, 2, 3}
type/types in the obtained ranked list. And they are combined with a greedy method similar to that in the
substitution method explained above.
4 Experiments
4.1 Settings
We use the same data sets as (Riedel et al., 2010) and (Hoffmann et al., 2011), where NYTimes sentences
in the years 2005-2006 are used as training corpus ?
train
for distant supervision and sentences in 2007
are used as testing corpus ?
predict
. The data was first tagged with an NER system (Finkel et al., 2005)
and consecutive words with the same tag are extracted as entity mentions. And then, entity mentions
E
train
in training corpus are aligned to facts ? in Freebase as training examples to train the models.
We integrate our fine-grained entity type constraint with MULTIR, an existing multi-instance multi-
label extracting model in (Hoffmann et al., 2011). Following their setttings, we conduct experiments on
aggregated extraction and sentential extraction to show the effect of fine-grained entity type constraints.
? Aggregated extraction: Aggregated extraction is corpus-level extraction. When given an entity
pair, it predicts its relation types based on the whole corpus. After extraction, the precision and
recall are computed by comparing the results with facts in Freebase. The evaluation underestimates
the accuracy because there may be correct facts in the extracted results but not existing in Freebase,
these facts are labeled as incorrect by mistake here. Because aggregated extraction is an automatic
evaluation, it is used to tune parameters like held-out evaluation in (Mintz et al., 2009).
2111
(a) PR curves of the substitution method (b) PR curves of the augment method
(c) PR curves of the selection method (d) Comparison with other methods
Figure 3: Precision-recall (PR) curves of the aggregated extraction.
? Sentential extraction: Sentential extraction predicts an entity pair only based on a specified sen-
tence containing the pair of entities. We use manually labeled data in (Hoffmann et al., 2011) as
benchmark. The data consist of 1,000 sentences and are sampled from the results their system out-
puts and sentences aligned with facts in Freebase. As they stated in their paper, these results provide
a good approximation to the true precision but can overestimate the actual recall.
4.2 Experimental Results
In aggregated extraction, we first evaluate the three type-constraint integration methods (substitution,
augment and selection) with the top k {k ? 1, 2, 3} type/types (Section 3.3). And then, we compare the
best parameter setting methods with previous work. In sentential extraction, we compare methods tuned
in aggregated extraction with MULTIR.
4.2.1 Aggregated Extraction
Figure 3 shows the precision-recall (PR) curves of the aggregated extraction. In it, Sub topk {k ?
1, 2, 3} means using the substitution method (Section 3.3) with top k fine-grained entities types re-
turned by the type classifier in Section 3.2. Correspondingly, Aug topk is for the augment method
and Select topk is for the selection method.
Figure 3(a) shows that Sub top3 outperforms the other two settings of k in the substitution method,
it seems that more fine-grained types produce better curves. In Figure 3(b), Aug top1 and Aug top2
achieve similar performances. However, when adding one more type with k = 3, we obtain a lower
curve, which contradicts the trend showed in the curves of the substitution method (Figure 3(a)). Fig-
ure 3(c) shows the PR curves of three selection methods, Select top1 has a better performance at the
beginning. Then Select top2 exceeds it a bit consistently.
In Figure 3(d), we demonstrate the comparison of best tuned methods above with previous work.
They are Sub top3, Aug top1 and Select top2. From Figure 3(d), it shows that, among the three of
our methods, Aug top1 achieves better precisions along the PR curves, and Select top2 reaches the best
2112
Figure 4: Comparison with MULTIR
recall at the highest recall point. Comparing to other methods, the PR curve of Aug top1 reaches a higher
recall with 29.3% at the highest recall point than MULTIR (24.5%). Select top2 achieves 29.3% at the
highest recall point, best among all methods. And by integrating the fine-grained entity type constraints,
they improve the PR curve of MULTIR with a more smoother curve without most of the depressions seen
in MULTIR. As stated in (Hoffmann et al., 2011), the smoother curve indicated a more stable model.
4.2.2 Sentential Extraction
Figure 4 shows the precision-recall (PR) curves of the sentential extraction. In the evaluation, we com-
pare the three best integration methods tuned in aggregated extraction with original MULTIR. Among our
three method, Aug top1 outperforms in precision and achieves a better curve in general among the three
methods, however, Select top2 gains a better recall at the end. Sub top3 has the worst recall. In gen-
eral, our methods have much better precisions than MULTIR. Aug top1 and Select top2 achieve better
curves than MULTIR. Since the evaluation of sentential extraction is a good approximation of precision,
it implies that the proposed methods are effective.
4.2.3 Analysis
On one hand, among the three proposed integration methods, generally, the augment method and selec-
tion method get better performance. The reason is that substitution method uses predicted fine-grained
entity types to replace the old coarse features in the conjunction features completely, and the conjunction
features are sensitive to entity types for different entity types indicate different conjunction features, as
a result, if we can not promise a good accuracy in the type classification which is hard to achieve in
classifying hundreds of fine-grained types, the performance will be badly influenced. Different from the
substitution method, augment method and selection method keep the old features with coarse features,
they use the features with fine-grained entity type constraints as extra information to help the extraction
and achieve better results.
On the other hand, comparing to other methods, by integration the exploited fine-grained entity type
constraints, our methods achieve improvements in both aggregated and sentential extraction. It proves
that the fine-grained entity type constraints we exploit are effective, and our proposed integration meth-
ods succeed in integrating the constraints into the extracting model. Our augment method outperforms
MULTIR in precision along the PR curves in sentential extraction and improve it performance with a
more smoother PR curve in aggregated extraction, which indicates a more stable model. Moreover, the
method gets a better recall. And our selection method consistently outperforms MULTIR in sentential
2113
k=1 k=2 k=3
Recall@k 0.596 0.740 0.806
Table 2: Evaluation of the fine-grained type classifier.
extraction. In aggregated extraction, it also achieves a smoother curve and an impressive promotion at
the highest recall point. Since the evaluation of aggregated extraction only considers the facts existing
in Freebase which may incorrectly label the right extracting results and underestimate the true precision,
and based on its better performance of precision in sentential extraction, we consider it is a more promis-
ing method. This paper only employs very naive method to select the non-informative features by its
length (Section 3.3.3), a more effective selecting method may lead further improvements.
4.3 Performance of Entity Type Classifier
We evaluate the performance of the fine-grained entity type classifier (Section 3.2). In section 3.2, we
sample the training examples from a collection of Wikipeida articles mapped with the fine-grained types.
To generate test entity mentions, we first remove the sampled training articles from the collection, and
then sample the articles from it, where the titles of sampled articles are used as the test entity mentions
(we sample 12,000 test entity mentions) and their mapped fine-grained types are used as benchmark.
After that, the predicting method in Section 3.2.2 is used to expand mentions and predict the types of
each test entity mention. After predicting, we obtain a ranked list of types for each test entity mention.
To evaluate, we define a notation of Hit@k, which equals 1 if the true type of an entity mention is
hit in the top k predicted types, otherwise equals 0. And then we evaluate it by the Recall@k defined
bellow.
Recall@k =
?
12000
i=1
Hit@k
i
12000
(1)
In equation (1), i means the ith test entity mention. Table 2 shows the results for the top 3 predicted
types.
5 Related Work
Distant supervision (also known as weak supervision or self supervision) is used to a broad class of meth-
ods in information extraction which aims to automatically generate labeled data by aligning with data
in knowledge bases. It is introduced by Craven and Kumlien (Craven et al., 1999) who used the Yeast
Protein Database to generate labeled data and trained a naive-Bayes extractor. Bellare and McCallum
(Bellare and McCallum, 2007) used BibTex records as the source of distant supervision. The KYLIN
system in (Wu and Weld, 2007) used article titles and infoboxes of Wikipedia to label sentences and
trained a CRF extractor aiming to generate infoboxes automatically. The Open IE systems TEXTRUN-
NER (Yates et al., 2007) and WOE (Wu and Weld, 2010) trained their extractors with the automatic
labeled data from Penn Treebank and Wikipedia infoboxes respectively.
Mintz (Mintz et al., 2009) first introduced their work that performed distant supervision for relation
extraction. It used Freebase as the knowledge base to align sentences in Wikipedia as training data and
trained a logistic regression classifier to extract relations between entities.Distant supervision supplied a
method to generate training data automatically, however it also bring the problem of noisy labeling. After
their work, a variety of methods focused to solve this problem. Riedel (Riedel et al., 2010) proposed a
multi-instance model to model the false positive noise in training data with the assumption that at least
one of the labeled sentences truly expressed their relation. After their work, Hoffmann (Hoffmann et
al., 2011) and Surdeanu (Surdeanu et al., 2012) tried to not only model the noisy training data, but also
overcame the problem of multi-label where two entities may exist more than one relation, they proposed
graphic models as kinds of multi-instance multi-label learning methods and made improvements over
previous work. The at-least-one assumption would fail when encountering entity pairs with only one
aligned sentence. Takamatsu (Takamatsu et al., 2012) employed an alternative approach without the
mentioned assumptions. Their work predicted negative patterns using a generative model and remove
labeled data containing negative patterns to reducing noise in labeled data.
2114
Besides the problem of false positive training examples caused by distant supervision. There were a
bunch of researches trying to solve the problem of false negative training examples caused by incomplete
knowledge bases. Zhang (Zhang et al., 2013) made heuristic rules to filter the false negative training
examples. And Xu (Xu et al., 2013) tried to overcom this problem by pseudo-relevance feedback. Min
(Min et al., 2013) improved MIML in (Surdeanu et al., 2012) by adding a new layer in their 3-layer
graphic model to model the incomplete knowledge base. Ritter (Ritter and Etzioni, 2013) employed
similar intuition with (Xu et al., 2013) that they thought rear entities missing in the database would
be often mentioned in the text. They proposed a latent-variable approach to model it and showed its
improvement over aggregate and sentential extraction.
6 Conclusion
In this paper, we propose a novel approach to explore the fine-grained entity type constraints for distantly
supervised relation extraction. We leverage Web knowledge to automatically train a fine-grained entity
type classifier and predict entity types of each entity mention. And we study a series of methods to inte-
grate the type constraints with a relation extraction model. At last, thorough experiments are conducted.
The experimental results imply our methods are effective with better precision/recall curves in senten-
tial extraction and smoother precision/recall curves in aggregated extraction, which indicate more stable
models.
In the future we hope to explore more details of integration methods that integrates fine-grained entity
type constraints with relation extraction models, especially the selection integration method. We consider
that a more effective method to distinguish between the informative and non-informative features will
lead more improvements.
Acknowledgements
This work was sponsored by the National Basic Research Program of China (No. 2014CB340503) and
the National Natural Science Foundation of China (No. 61202329). This work was supported in part by
Noahs Ark Lab of Huawei Tech. Co. Ltd.
References
Kedar Bellare and Andrew McCallum. 2007. Learning extractors from unlabeled text using relevant databases. In
Sixth International Workshop on Information Integration on the Web.
Mark Craven, Johan Kumlien, et al. 1999. Constructing biological knowledge bases by extracting information
from text sources. In Proceedings of the Seventh International Conference on Intelligent Systems for Molecular
Biology, pages 77?86. Heidelberg, Germany.
Jenny Rose Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating non-local information into
information extraction systems by gibbs sampling. In Proceedings of the 43rd Annual Meeting on Association
for Computational Linguistics, pages 363?370. Association for Computational Linguistics.
Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke Zettlemoyer, and Daniel S Weld. 2011. Knowledge-based
weak supervision for information extraction of overlapping relations. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics: Human Language Technologies, volume 1, pages 541?
550.
Xiao Ling and DS Weld. 2012. Fine-Grained Entity Recognition. In AAAI.
Bonan Min, Ralph Grishman, Li Wan, Chang Wang, and David Gondek. 2013. Distant supervision for relation
extraction with an incomplete knowledge base. In Proceedings of NAACL-HLT, pages 777?782.
Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky. 2009. Distant supervision for relation extraction without
labeled data. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th
International Joint Conference on Natural Language Processing of the AFNLP: Volume 2-Volume 2, pages
1003?1011. Association for Computational Linguistics.
2115
Deepak Ravichandran and Eduard Hovy. 2002. Learning surface text patterns for a question answering system. In
Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, pages 41?47. Associa-
tion for Computational Linguistics.
Sebastian Riedel, Limin Yao, and Andrew McCallum. 2010. Modeling relations and their mentions without
labeled text. In Machine Learning and Knowledge Discovery in Databases, pages 148?163. Springer.
Alan Ritter and Oren Etzioni. 2013. Modeling Missing Data in Distant Supervision for Information Extraction.
Transactions of the Association for Computational Linguistics, 1:367?378.
Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati, and Christopher D Manning. 2012. Multi-instance multi-
label learning for relation extraction. In Proceedings of the 2012 Joint Conference on Empirical Methods in
Natural Language Processing and Computational Natural Language Learning, pages 455?465. Association for
Computational Linguistics.
Idan Szpektor, Hristo Tanev, Ido Dagan, Bonaventura Coppola, et al. 2005. Scaling Web-based aquisition of
entailment relations. Ph.D. thesis, Tel Aviv University.
Shingo Takamatsu, Issei Sato, and Hiroshi Nakagawa. 2012. Reducing wrong labels in distant supervision for
relation extraction. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics:
Long Papers-Volume 1, pages 721?729. Association for Computational Linguistics.
Fei Wu and Daniel S Weld. 2007. Autonomously semantifying wikipedia. In Proceedings of the sixteenth ACM
conference on Conference on information and knowledge management, pages 41?50. ACM.
Fei Wu and Daniel S Weld. 2010. Open information extraction using wikipedia. In Proceedings of the 48th An-
nual Meeting of the Association for Computational Linguistics, pages 118?127. Association for Computational
Linguistics.
W Xu, RH Le Zhao, and R Grishman. 2013. Filling Knowledge Base Gaps for Distant Supervision of Relation
Extraction. Proceedings of Association for Computational Linguistics.
Alexander Yates, Michael Cafarella, Michele Banko, Oren Etzioni, Matthew Broadhead, and Stephen Soderland.
2007. Textrunner: open information extraction on the web. In Proceedings of Human Language Technolo-
gies: The Annual Conference of the North American Chapter of the Association for Computational Linguistics:
Demonstrations, pages 25?26. Association for Computational Linguistics.
Xingxing Zhang, jianwen Zhang, Junyu Zeng, Jun Yan, Zheng Chen, and Zuifang Sui. 2013. Towards Accurate
Distant Supervision for Relational Facts Extraction. In Proceedings of Association for Computational Linguis-
tics.
GuoDong Zhou, Jian Su, Jie Zhang, and Min Zhang. 2005. Exploring various knowledge in relation extraction.
In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 427?434.
Association for Computational Linguistics.
GuoDong Zhou, Min Zhang, Dong Hong Ji, and Qiaoming Zhu. 2007. Tree kernel-based relation extraction with
context-sensitive structured parse tree information. In Proceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Computational Natural Language Learning.
2116
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 880?888,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Fast Generation of Translation Forest
for Large-Scale SMT Discriminative Training
Xinyan Xiao, Yang Liu, Qun Liu, and Shouxun Lin
Key Laboratory of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences
{xiaoxinyan,yliu,liuqun,sxlin}@ict.ac.cn
Abstract
Although discriminative training guarantees to
improve statistical machine translation by in-
corporating a large amount of overlapping fea-
tures, it is hard to scale up to large data due to
decoding complexity. We propose a new al-
gorithm to generate translation forest of train-
ing data in linear time with the help of word
alignment. Our algorithm also alleviates the
oracle selection problem by ensuring that a
forest always contains derivations that exactly
yield the reference translation. With millions
of features trained on 519K sentences in 0.03
second per sentence, our system achieves sig-
nificant improvement by 0.84 BLEU over the
baseline system on the NIST Chinese-English
test sets.
1 Introduction
Discriminative model (Och and Ney, 2002) can
easily incorporate non-independent and overlapping
features, and has been dominating the research field
of statistical machine translation (SMT) in the last
decade. Recent work have shown that SMT benefits
a lot from exploiting large amount of features (Liang
et al, 2006; Tillmann and Zhang, 2006; Watanabe
et al, 2007; Blunsom et al, 2008; Chiang et al,
2009). However, the training of the large number
of features was always restricted in fairly small data
sets. Some systems limit the number of training ex-
amples, while others use short sentences to maintain
efficiency.
Overfitting problem often comes when training
many features on a small data (Watanabe et al,
2007; Chiang et al, 2009). Obviously, using much
more data can alleviate such problem. Furthermore,
large data also enables us to globally train millions
of sparse lexical features which offer accurate clues
for SMT. Despite these advantages, to the best of
our knowledge, no previous discriminative training
paradigms scale up to use a large amount of training
data. The main obstacle comes from the complexity
of packed forests or n-best lists generation which
requires to search through all possible translations
of each training example, which is computationally
prohibitive in practice for SMT.
To make normalization efficient, contrastive esti-
mation (Smith and Eisner, 2005; Poon et al, 2009)
introduce neighborhood for unsupervised log-linear
model, and has presented positive results in various
tasks. Motivated by these work, we use a translation
forest (Section 3) which contains both ?reference?
derivations that potentially yield the reference trans-
lation and also neighboring ?non-reference? deriva-
tions that fail to produce the reference translation.1
However, the complexity of generating this transla-
tion forest is up to O(n6), because we still need bi-
parsing to create the reference derivations.
Consequently, we propose a method to fast gener-
ate a subset of the forest. The key idea (Section 4)
is to initialize a reference derivation tree with maxi-
mum score by the help of word alignment, and then
traverse the tree to generate the subset forest in lin-
ear time. Besides the efficiency improvement, such
a forest allows us to train the model without resort-
1Exactly, there are no reference derivations, since derivation
is a latent variable in SMT. We call them reference derivation
just for convenience.
880
0,4
0,1
2,4
3,4
1 30 4
21
3
5
4
6
2
hyper-
edge rule
e1 r1 X ? ?X1 bei X2, X1 was X2?
e2 r2 X ? ?qiangshou bei X1,
the gunman was X1?
e3 r3 X ? ?jingfang X1, X1 by the police?
e4 r4 X ? ?jingfang X1, police X1 ?
e5 r5 X ? ?qiangshou, the gunman?
e6 r6 X ? ?jibi, shot dead?
Figure 1: A translation forest which is the running example throughout this paper. The reference translation is ?the
gunman was killed by the police?. (1) Solid hyperedges denote a ?reference? derivation tree t1 which exactly yields
the reference translation. (2) Replacing e3 in t1 with e4 results a competing non-reference derivation t2, which fails to
swap the order ofX3,4. (3) Removing e1 and e5 in t1 and adding e2 leads to another reference derivation t3. Generally,
this is done by deleting a node X0,1.
ing to constructing the oracle reference (Liang et al,
2006; Watanabe et al, 2007; Chiang et al, 2009),
which is non-trivial for SMT and needs to be deter-
mined experimentally. Given such forests, we glob-
ally learn a log-linear model using stochastic gradi-
ent descend (Section 5). Overall, both the generation
of forests and the training algorithm are scalable, en-
abling us to train millions of features on large-scale
data.
To show the effect of our framework, we globally
train millions of word level context features moti-
vated by word sense disambiguation (Chan et al,
2007) together with the features used in traditional
SMT system (Section 6). Training on 519K sentence
pairs in 0.03 seconds per sentence, we achieve sig-
nificantly improvement over the traditional pipeline
by 0.84 BLEU.
2 Synchronous Context Free Grammar
We work on synchronous context free grammar
(SCFG) (Chiang, 2007) based translation. The el-
ementary structures in an SCFG are rewrite rules of
the form:
X ? ??, ??
where ? and ? are strings of terminals and nonter-
minals. We call ? and ? as the source side and the
target side of rule respectively. Here a rule means a
phrase translation (Koehn et al, 2003) or a transla-
tion pair that contains nonterminals.
We call a sequence of translation steps as a
derivation. In context of SCFG, a derivation is a se-
quence of SCFG rules {ri}. Translation forest (Mi
et al, 2008; Li and Eisner, 2009) is a compact repre-
sentation of all the derivations for a given sentence
under an SCFG (see Figure 1). A tree t in the forest
corresponds to a derivation. In our paper, tree means
the same as derivation.
More formally, a forest is a pair ?V,E?, where V
is the set of nodes, E is the set of hyperedge. For
a given source sentence f = fn1 , Each node v ? V
is in the form Xi,j , which denotes the recognition
of nonterminal X spanning the substring from the i
through j (that is fi+1...fj). Each hyperedge e ? E
connects a set of antecedent to a single consequent
node and corresponds to an SCFG rule r(e).
3 Our Translation Forest
We use a translation forest that contains both ?ref-
erence? derivations that potentially yield the refer-
ence translation and also some neighboring ?non-
reference? derivations that fail to produce the ref-
erence translation. Therefore, our forest only repre-
sents some of the derivations for a sentence given an
SCFG rule table. The motivation of using such a for-
est is efficiency. However, since this space contains
both ?good? and ?bad? translations, it still provides
evidences for discriminative training.
First see the example in Figure 1. The derivation
tree t1 represented by solid hyperedges is a reference
derivation. We can construct a non-reference deriva-
tion by making small change to t1. By replacing the
e3 of t1 with e4, we obtain a non-reference deriva-
881
tion tree t2. Considering the rules in each derivation,
the difference between t1 and t2 lies in r3 and r4. Al-
though r3 has a same source side with r4, it produces
a different translation. While r3 provides a swap-
ping translation, r4 generates a monotone transla-
tion. Thus, the derivation t2 fails to move the sub-
ject ?police? to the behind of verb ?shot dead?, re-
sulting a wrong translation ?the gunman was police
shot dead?. Given such derivations, we hope that
the discriminative model is capable to explain why
should use a reordering rule in this context.
Generally, our forest contains all the reference
derivationsRT for a sentence given a rule table, and
some neighboring non-reference derivations NT ,
which can be defined fromRT .
More formally, we call two hyperedges e1 and e2
are competing hyperedges, if their corresponding
rules r(e1) = ??1, ?1? and r(e2) = ??2, ?2? :
?1 = ?2 ? ?1 ?= ?2 (1)
This means they give different translations for a
same source side. We use C(e) to represent the set
of competing hyperedges of e.
Two derivations t1 = ?V 1, E1? and t2 =
?V 2, E2? are competing derivations if there exists
e1 ? E1 and e2 ? E2: 2
V 1 = V 2 ? E1 ? e1 = E2 ? e2
? e2 ? C(e1) (2)
In other words, derivations t1 and t2 only differ in
e1 and e2, and these two hyperedges are competing
hyperedges. We useC(t) to represent the set of com-
peting derivations of tree t, and C(t,e) to represent
the set of competing derivations of t if the competi-
tion occurs in hyperedge e in t.
Given a rule table, the set of reference derivations
RT for a sentence is determined. Then, the set of
non-reference derivations NT can be defined from
RT :
?t?RT C(t) (3)
Overall, our forest is the compact representation of
RT and NT .
2The definition of derivation tree is similar to forest, except
that the tree contains exactly one tree while forest contains ex-
ponentially trees. In tree, the hyperedge degrades to edge.
Algorithm 1 Forest Generation
1: procedure GENERATE(t)
2: list? t
3: for v ? t in post order do
4: e? incoming edge of v
5: append C(t, e) to list;
6: for u ? child(v) from left to right do
7: tn? OPERATE(t, u)
8: if tn ?= t then
9: append tn to list
10: for e? ? tn ? e? /? t do
11: append C(tn,e?) to list
12: if SCORE(t) < SCORE(tn) then
13: t? tn
14: return t,list
4 Fast Generation
It is still slow to calculate the entire forest defined
in Section 3, therefore we use a greedy decoding for
fast generating a subset of the forest. Starting form
a reference derivation, we try to slightly change the
derivation into a new reference derivation. During
this process, we collect the competing derivations
of reference derivations. We describe the details of
local operators for changing a derivation in section
4.1, and then introduce the creation of initial refer-
ence derivation with max score in Section 4.2.
For example, given derivation t1, we delete the
node X0,1 and the related hyperedge e1 and e5. Fix-
ing the other nodes and edges, we try to add a new
edge e2 to create a new reference translation. In this
case, if rule r2 really exists in our rule table, we get
a new reference derivation t3. After constructing t3,
we first collect the new tree and C(t3, e2). Then, we
will move to t3, if the score of t3 is higher than t2.
Notably, if r2 does not exist in the rule table, we fail
to create a new reference derivation. In such case,
we keep the origin derivation unchanged.
Algorithm 1 shows the process of generation.3
The input is a reference derivation t, and the out-
put is a new derivation and the generated derivations.
3For simplicity, we list all the trees, and do not compress
them into a forest in practice. It is straight to extent the algo-
rithm to get a compact forest for those generated derivations.
Actually, instead of storing the derivations, we call the generate
function twice to calculate gradient of log-linear model.
882
0,4
0,1 2,4
0,4
2,4
0,4
2,40,2
Figure 2: Lexicalize and generalize operators over t1 (part) in Figure 1. Although here only shows the nodes, we also
need to change relative edges actually. (1) Applying lexicalize operator on the non-terminal node X0,1 in (a) results a
new derivation shown in (b). (2) When visiting bei in (b), the generalize operator changes the derivation into (c).
The list used for storing forest is initialized with the
input tree (line 2). We visit the nodes in t in post-
order (line 3). For each node v, we first append the
competing derivations C(t,e) to list, where e is in-
coming edge of v (lines 4-5). Then, we apply oper-
ators on the child nodes of v from left to right (lines
6-13). The operators returns a reference derivation
tn (line 7). If it is new (line 8), we collect both the tn
(line 9), and also the competing derivationsC(tn, e?)
of the new derivation on those edges e? which only
occur in the new derivation (lines 10-11). Finally, if
the new derivation has a larger score, we will replace
the origin derivation with new one (lines 12-13).
Although there is a two-level loop for visiting
nodes (line 3 and 6), each node is visited only one
time in the inner loops. Thus, the complexity is
linear with the number of nodes #node. Consid-
ering that the number of source word (also leaf node
here) is less than the total number of nodes and is
more than ?(#node+1)/2?, the time complexity of
the process is also linear with the number of source
word.
4.1 Lexicalize and Generalize
The function OPERATE in Algorithm 1 uses two op-
erators to change a node: lexicalize and generalize.
Figure 2 shows the effects of the two operators. The
lexicalize operator works on nonterminal nodes. It
moves away a nonterminal node and attaches the
children of current node to its parent. In Figure 2(b),
the node X0,1 is deleted, requiring a more lexical-
ized rule to be applied to the parent node X0,4 (one
more terminal in the source side). We constrain the
lexicalize operator to apply on pre-terminal nodes
whose children are all terminal nodes. In contrast,
the generalize operator works on terminal nodes and
inserts a nonterminal node between current node and
its parent node. This operator generalizes over the
continuous terminal sibling nodes left to the current
node (including the current node). Generalizing the
node bei in Figure 2(b) results Figure 2(c). A new
node X0,2 is inserted as the parent of node qiang-
shou and node bei.
Notably, there are two steps when apply an oper-
ator. Suppose we want to lexicalize the node X0,1
in t1 of Figure 1, we first delete the node X0,1 and
related edge e1 and e5, then we try to add the new
edge e2. Since rule table is fixed, the second step
is a process of decoding. Therefore, sometimes we
may fail to create a new reference derivation (like
r2 may not exist in the rule table). In such case, we
keep the origin derivation unchanged.
The changes made by the two operators are local.
Considering the change of rules, the lexicalize oper-
ator deletes two rules and adds one new rule, while
the generalize operator deletes one rule and adds two
new rules. Such local changes provide us with a way
to incrementally calculate the scores of new deriva-
tions. We use this method motivated by Gibbs Sam-
pler (Blunsom et al, 2009) which has been used for
efficiently learning rules. The different lies in that
we use the operator for decoding where the rule ta-
ble is fixing.
4.2 Initialize a Reference Derivation
The generation starts from an initial reference
derivation with max score. This requires bi-parsing
(Dyer, 2010) over the source sentence f and the ref-
erence translation e. In practice, we may face three
problems.
First is efficiency problem. Exhaustive search
over the space under SCFG requires O(|f |3|e|3).
883
To parse quickly, we only visit the tight consistent
(Zhang et al, 2008) bi-spans with the help of word
alignment a. Only visiting tight consistent spans
greatly speeds up bi-parsing. Besides efficiency,
adoption of this constraint receives support from the
fact that heuristic SCFG rule extraction only extracts
tight consistent initial phrases (Chiang, 2007).
Second is degenerate problem. If we only use
the features as traditional SCFG systems, the bi-
parsing may end with a derivation consists of some
giant rules or rules with rare source/target sides,
which is called degenerate solution (DeNero et al,
2006). That is because the translation rules with rare
source/target sides always receive a very high trans-
lation probability. We add a prior score log(#rule)
for each rule, where #rule is the number of occur-
rence of a rule, to reward frequent reusable rules and
derivations with more rules.
Finally, we may fail to create reference deriva-
tions due to the limitation in rule extraction. We
create minimum trees for (f , e,a) using shift-reduce
(Zhang et al, 2008). Some minimum rules in the
trees may be illegal according to the definition of
Chiang (2007). We also add these rules to the rule
table, so as to make sure every sentence is reachable
given the rule table. A source sentence is reachable
given a rule table if reference derivations exists. We
refer these rules as added rules. However, this may
introduce rules with more than two variables and in-
crease the complexity of bi-parsing. To tackle this
problem, we initialize the chart with minimum par-
allel tree from the Zhang et al (2008) algorithm,
ensuring that the bi-parsing has at least one path to
create a reference derivation. Then we only need to
consider the traditional rules during bi-parsing.
5 Training
We use the forest to train a log-linear model with a
latent variable as describe in Blunsom et al(2008).
The probability p(e|f) is the sum over all possible
derivations:
p(e|f) =
?
t??(e,f)
p(t, e|f) (4)
where ?(e, f) is the set of all possible derivations
that translate f into e and t is one such derivation.4
4Although the derivation is typically represent as d, we de-
notes it by t since our paper use tree to represent derivation.
Algorithm 2 Training
1: procedure TRAIN(S)
2: Training Data S = {fn, en,an}Nn=1
3: Derivations T = {}Nn=1
4: for n = 1 to N do
5: tn ? INITIAL(fn, en,an)
6: i? 0
7: for m = 0 to M do
8: for n = 0 to N do
9: ? ? LEARNRATE(i)
10: (?L(wi, tn), tn)?GENERATE(tn)
11: wi ? wi + ? ??L(wi, tn)
12: i? i + 1
13: return
?MN
i=1 wi
MN
This model defines the conditional probability of
a derivation t and the corresponding translation e
given a source sentence f as:
p(t, e|f) = exp
?
i ?ihi(t, e, f)
Z(f) (5)
where the partition function is
Z(f) =
?
e
?
t??(e,f)
exp
?
i
?ihi(t, e, f) (6)
The partition function is approximated by our for-
est, which is labeled as Z?(f), and the derivations
that produce reference translation is approximated
by reference derivations in Z?(f).
We estimate the parameters in log-linear model
using maximum a posteriori (MAP) estimator. It
maximizes the likelihood of the bilingual corpus
S = {fn, en}Nn=1, penalized using a gaussian prior
(L2 norm) with the probability density function
p0(?i) ? exp(??2i /2?2). We set ?2 to 1.0 in our
experiments. This results in the following gradient:
?L
??i
= Ep(t|e,f)[hi]? Ep(e|f)[hi]?
?i
?2 (7)
We use an online learning algorithm to train the
parameters. We implement stochastic gradient de-
scent (SGD) recommended by Bottou.5 The dy-
namic learning rate we use is N(i+i0) , where N is the
5http://leon.bottou.org/projects/sgd
884
number of training example, i is the training itera-
tion, and i0 is a constant number used to get a initial
learning rate, which is determined by calibration.
Algorithm 2 shows the entire process. We first
create an initial reference derivation for every train-
ing examples using bi-parsing (lines 4-5), and then
online learn the parameters using SGD (lines 6-12).
We use the GENERATE function to calculate the gra-
dient. In practice, instead of storing all the deriva-
tions in a list, we traverse the tree twice. The first
time is calculating the partition function, and the
second time calculates the gradient normalized by
partition function. During training, we also change
the derivations (line 10). When training is finished
after M epochs, the algorithm returns an averaged
weight vector (Collins, 2002) to avoid overfitting
(line 13). We use a development set to select total
epoch m, which is set as M = 5 in our experiments.
6 Experiments
Our method is able to train a large number of fea-
tures on large data. We use a set of word context
features motivated by word sense disambiguation
(Chan et al, 2007) to test scalability. A word level
context feature is a triple (f, e, f+1), which counts
the number of time that f is aligned to e and f+1 oc-
curs to the right of f . Triple (f, e, f?1) is similar ex-
cept that f?1 locates to the left of f . We retain word
alignment information in the extracted rules to ex-
ploit such features. To demonstrate the importance
of scaling up the size of training data and the effect
of our method, we compare three types of training
configurations which differ in the size of features
and data.
MERT. We use MERT (Och, 2003) to training 8
features on a small data. The 8 features is the same
as Chiang (2007) including 4 rule scores (direct and
reverse translation scores; direct and reverse lexi-
cal translation scores); 1 target side language model
score; 3 penalties for word counts, extracted rules
and glue rule. Actually, traditional pipeline often
uses such configuration.
Perceptron. We also learn thousands of context
word features together with the 8 traditional features
on a small data using perceptron. Following (Chiang
et al, 2009), we only use 100 most frequent words
for word context feature. This setting use CKY de-
TRAIN RTRAIN DEV TEST
#Sent. 519,359 186,810 878 3,789
#Word 8.6M 1.3M 23K 105K
Avg. Len. 16.5 7.3 26.4 28.0
Lon. Len. 99 95 77 116
Table 1: Corpus statistics of Chinese side, where Sent.,
Avg., Lon., and Len. are short for sentence, longest,
average, and length respectively. RTRAIN denotes the
reachable (given rule table without added rules) subset of
TRAIN data.
coder to generate n-best lists for training. The com-
plexity of CKY decoding limits the training data into
a small size. We fix the 8 traditional feature weights
as MERT to get a comparable results as MERT.
Our Method. Finally, we use our method to train
millions of features on large data. The use of large
data promises us to use full vocabulary of training
data for the context word features, which results mil-
lions of fully lexicalized context features. During
decoding, when a context feature does not exit, we
simply ignore it. The weights of 8 traditional fea-
tures are fixed the same as MERT also. We fix these
weights because the translation feature weights fluc-
tuate intensely during online learning. The main rea-
son may come from the degeneration solution men-
tioned in Section 4.2, where rare rules with very high
translation probability are selected as the reference
derivations. Another reason could be the fact that
translation features are dense intensify the fluctua-
tion. We leave learning without fixing the 8 feature
weights to future work.
6.1 Data
We focus on the Chinese-to-English translation task
in this paper. The bilingual corpus we use con-
tains 519, 359 sentence pairs, with an average length
of 16.5 in source side and 20.3 in target side,
where 186, 810 sentence pairs (36%) are reach-
able (without added rules in Section 4.2). The
monolingual data includes the Xinhua portion of
the GIGAWORD corpus, which contains 238M En-
glish words. We use the NIST evaluation sets of
2002 (MT02) as our development set, and sets of
MT03/MT04/MT05 as test sets. Table 2 shows the
statistics of all bilingual corpus.
We use GIZA++ (Och and Ney, 2003) to perform
885
System #DATA #FEAT MT03 MT04 MT05 ALL
MERT 878 8 33.03 35.12 32.32 33.85
Perceptron 878 2.4K 32.89 34.88 32.55 33.76
Our Method 187K 2.0M 33.64 35.48 32.91* 34.41*519K 13.9M 34.19* 35.72* 33.09* 34.69*
Improvement over MERT +1.16 +0.60 +0.77 +0.84
Table 2: Effect of our method comparing with MERT and perceptron in terms of BLEU. We also compare our fast
generation method with different data (only reachable or full data). #Data is the size of data for training the feature
weights. * means significantly (Koehn, 2004) better than MERT (p < 0.01).
word alignment in both directions, and grow-diag-
final-and (Koehn et al, 2003) to generate symmet-
ric word alignment. We extract SCFG rules as de-
scribed in Chiang (2007) and also added rules (Sec-
tion 4.2). Our algorithm runs on the entire training
data, which requires to load all the rules into the
memory. To fit within memory, we cut off those
composed rules which only happen once in the train-
ing data. Here a composed rule is a rule that can be
produced by any other extracted rules. A 4-grams
language model is trained by the SRILM toolkit
(Stolcke, 2002). Case-insensitive NIST BLEU4 (Pa-
pineni et al, 2002) is used to measure translation
performance.
The training data comes from a subset of the
LDC data including LDC2002E18, LDC2003E07,
LDC2003E14, Hansards portion of LDC2004T07,
LDC2004T08 and LDC2005T06. Since the rule ta-
ble of the entire data is too large to be loaded to
the memory (even drop one-count rules), we remove
many sentence pairs to create a much smaller data
yet having a comparable performance with the entire
data. The intuition lies in that if most of the source
words of a sentence need to be translated by the
added rules, then the word alignment may be highly
crossed and the sentence may be useless. We cre-
ate minimum rules from a sentence pair, and count
the number of source words in those minimum rules
that are added rules. For example, suppose the result
minimum rules of a sentence contain r3 which is an
added rule, then we count 1 time for the sentence. If
the number of such source word is more than 10%
of the total number, we will drop the sentence pair.
We compare the performances of MERT setting
on three bilingual data: the entire data that contains
42.3M Chinese and 48.2M English words; 519K
data that contains 8.6M Chinese and 10.6M En-
glish words; FBIS (LDC2003E14) parts that con-
tains 6.9M Chinese and 9.1M English words. They
produce 33.11/32.32/30.47 BLEU tested on MT05
respectively. The performance of 519K data is com-
parable with that of entire data, and much higher
than that of FBIS data.
6.2 Result
Table 3 shows the performance of the three different
training configurations. The training of MERT and
perceptron run on MT02. For our method, we com-
pare two different training sets: one is trained on
all 519K sentence pairs, the other only uses 186K
reachable sentences.
Although the perceptron system exploits 2.4K
features, it fails to produce stable improvements
over MERT. The reason may come from overfitting,
since the training data for perceptron contains only
878 sentences. However, when use our method to
learn the word context feature on the 519K data,
we significantly improve the performance by 0.84
points on the entire test sets (ALL). The improve-
ments range from 0.60 to 1.16 points on MT03-
05. Because we use the full vocabulary, the num-
ber of features increased into 13.9 millions, which is
impractical to be trained on the small development
set. These results confirm the necessity of exploiting
more features and learning the parameters on large
data. Meanwhile, such results also demonstrate that
we can benefits from the forest generated by our fast
method instead of traditional CKY algorithm.
Not surprisingly, the improvements are smaller
when only use 186K reachable sentences. Some-
times we even fail to gain significant improvement.
This verifies our motivation to guarantee all sentence
886
 0
 30
 60
 90
 120
 150
 180
 0  10  20  30  40  50  60  70  80  90
Tr
ain
ing
 Ti
me
(M
illis
eco
nds
)
Sentence Length
Figure 3: Plot of training times (including forest genera-
tion and SGD training) versus sentence length. We ran-
domly select 1000 sentence from the 519K data for plot-
ting.
are reachable, so as to use all training data.
6.3 Speed
How about the speed of our framework? Our method
learns in 32 mlliseconds/sentence. Figure 3 shows
training times (including forest generation and SGD
training) versus sentence length. The plot confirms
that our training algorithm scales linearly. If we
use n-best lists which generated by CKY decoder
as MERT, it takes about 3105 milliseconds/sentence
for producing 100-best lists. Our method accelerates
the speed about 97 times (even though we search
twice to calculate the gradient). This shows the effi-
ciency of our method.
The procedure of training includes two steps. (1)
Bi-parsing to initialize a reference derivation with
max score. (2) Training procedure which generates
a set of derivations to calculate the gradient and up-
date parameters. Step (1) only runs once. The av-
erage time of processing a sentence for each step
is about 9.5 milliseconds and 30.2 milliseconds re-
spectively.
For simplicity we do not compress the generated
derivations into forests, therefore the size of result-
ing derivations is fairly small, which is about 265.8
for each sentence on average, where 6.1 of them are
reference derivations. Furthermore, we use lexical-
ize operator more often than generalize operator (the
ration between them is 1.5 to 1). Lexicalize operator
is used more frequently mainly dues to that the ref-
erence derivations are initialized with reusable (thus
small) rules.
7 Related Work
Minimum error rate training (Och, 2003) is perhaps
the most popular discriminative training for SMT.
However, it fails to scale to large number of features.
Researchers have propose many learning algorithms
to train many features: perceptron (Shen et al, 2004;
Liang et al, 2006), minimum risk (Smith and Eisner,
2006; Li et al, 2009), MIRA (Watanabe et al, 2007;
Chiang et al, 2009), gradient descent (Blunsom et
al., 2008; Blunsom and Osborne, 2008). The com-
plexity of n-best lists or packed forests generation
hamper these algorithms to scale to a large amount
of data.
For efficiency, we only use neighboring deriva-
tions for training. Such motivation is same as con-
trastive estimation (Smith and Eisner, 2005; Poon et
al., 2009). The difference lies in that the previous
work actually care about their latent variables (pos
tags, segmentation, dependency trees, etc), while
we are only interested in their marginal distribution.
Furthermore, we focus on how to fast generate trans-
lation forest for training.
The local operators lexicalize/generalize are use
for greedy decoding. The idea is related to ?peg-
ging? algorithm (Brown et al, 1993) and greedy de-
coding (Germann et al, 2001). Such types of local
operators are also used in Gibbs sampler for syn-
chronous grammar induction (Blunsom et al, 2009;
Cohn and Blunsom, 2009).
8 Conclusion and Future Work
We have presented a fast generation algorithm for
translation forest which contains both reference
derivations and neighboring non-reference deriva-
tions for large-scale SMT discriminative training.
We have achieved significantly improvement of 0.84
BLEU by incorporate 13.9M feature trained on 519K
data in 0.03 second per sentence.
In this paper, we define the forest based on com-
peting derivations which only differ in one rule.
There may be better classes of forest that can pro-
duce a better performance. It?s interesting to modify
the definition of forest, and use more local operators
to increase the size of forest. Furthermore, since the
generation of forests is quite general, it?s straight to
887
apply our forest on other learning algorithms. Fi-
nally, we hope to exploit more features such as re-
ordering features and syntactic features so as to fur-
ther improve the performance.
Acknowledgement
We would like to thank Yifan He, Xianhua Li, Daqi
Zheng, and the anonymous reviewers for their in-
sightful comments. The authors were supported by
National Natural Science Foundation of China Con-
tracts 60736014, 60873167, and 60903138.
References
Phil Blunsom and Miles Osborne. 2008. Probabilistic
inference for machine translation. In Proc. of EMNLP
2008.
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
A discriminative latent variable model for statistical
machine translation. In Proc. of ACL-08.
Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles Os-
borne. 2009. A gibbs sampler for phrasal synchronous
grammar induction. In Proc. of ACL 2009.
Peter F. Brown, Vincent J.Della Pietra, Stephen A. Della
Pietra, and Robert. L. Mercer. 1993. The mathemat-
ics of statistical machine translation. Computational
Linguistics, 19:263?311.
Yee Seng Chan, Hwee Tou Ng, and David Chiang. 2007.
Word sense disambiguation improves statistical ma-
chine translation. In Proc. of ACL 2007, pages 33?40.
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 new features for statistical machine translation.
In Proc. of NAACL 2009.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Trevor Cohn and Phil Blunsom. 2009. A Bayesian model
of syntax-directed tree to string grammar induction. In
Proc. of EMNLP 2009.
Michael Collins. 2002. Discriminative training methods
for hidden markov models: Theory and experiments
with perceptron algorithms. In Proc. of EMNLP 2002.
John DeNero, Dan Gillick, James Zhang, and Dan Klein.
2006. Why generative phrase models underperform
surface heuristics. In Proc. of the HLT-NAACL 2006
Workshop on SMT.
Chris Dyer. 2010. Two monolingual parses are better
than one (synchronous parse). In Proc. of NAACL
2010.
Ulrich Germann, Michael Jahr, Kevin Knight, Daniel
Marcu, and Kenji Yamada. 2001. Fast decoding and
optimal decoding for machine translation. In Proc. of
ACL 2001.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
of HLT-NAACL 2003.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proc. of EMNLP
2004.
Zhifei Li and Jason Eisner. 2009. First- and second-order
expectation semirings with applications to minimum-
risk training on translation forests. In Proc. of EMNLP
2009.
Zhifei Li, Jason Eisner, and Sanjeev Khudanpur. 2009.
Variational decoding for statistical machine transla-
tion. In Proc. of ACL 2009.
Percy Liang, Alexandre Bouchard-Co?te?, Dan Klein, and
Ben Taskar. 2006. An end-to-end discriminative ap-
proach to machine translation. In Proc. of ACL 2006.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proc. of ACL 2008.
Franz J. Och and Hermann Ney. 2002. Discriminative
training and maximum entropy models for statistical
machine translation. In Proc. of ACL 2002.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proc. of ACL 2003.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalua-
tion of machine translation. In Proc. of ACL 2002.
Hoifung Poon, Colin Cherry, and Kristina Toutanova.
2009. Unsupervised morphological segmentation with
log-linear models. In Proc. of NAACL 2009.
Libin Shen, Anoop Sarkar, and Franz Josef Och. 2004.
Discriminative reranking for machine translation. In
Proc. of NAACL 2004.
Noah A. Smith and Jason Eisner. 2005. Contrastive esti-
mation: Training log-linear models on unlabeled data.
In Proc. of ACL 2005.
David A. Smith and Jason Eisner. 2006. Minimum risk
annealing for training log-linear models. In Proc. of
COLING/ACL 2006.
Andreas Stolcke. 2002. Srilm ? an extensible language
modeling toolkit. In Proc. of ICSLP 2002.
Christoph Tillmann and Tong Zhang. 2006. A discrim-
inative global training algorithm for statistical mt. In
Proc. of ACL 2006.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki
Isozaki. 2007. Online large-margin training for statis-
tical machine translation. In Proc. of EMNLP-CoNLL
2007.
Hao Zhang, Daniel Gildea, and David Chiang. 2008. Ex-
tracting synchronous grammar rules from word-level
alignments in linear time. In Proc. of Coling 2008.
888
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 501?511, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Joint Chinese Word Segmentation, POS Tagging and Parsing
Xian Qian Yang Liu
Computer Science Department
The University of Texas at Dallas
qx,yangl@hlt.utdallas.edu
Abstract
In this paper, we propose a novel decoding al-
gorithm for discriminative joint Chinese word
segmentation, part-of-speech (POS) tagging,
and parsing. Previous work often used a
pipeline method ? Chinese word segmentation
followed by POS tagging and parsing, which
suffers from error propagation and is unable
to leverage information in later modules for
earlier components. In our approach, we train
the three individual models separately during
training, and incorporate them together in a u-
nified framework during decoding. We extend
the CYK parsing algorithm so that it can deal
with word segmentation and POS tagging fea-
tures. As far as we know, this is the first work
on joint Chinese word segmentation, POS tag-
ging and parsing. Our experimental result-
s on Chinese Tree Bank 5 corpus show that
our approach outperforms the state-of-the-art
pipeline system.
1 Introduction
For Asian languages such as Japanese and Chi-
nese that do not contain explicitly marked word
boundaries, word segmentation is an important first
step for many subsequent language processing tasks,
such as POS tagging, parsing, semantic role label-
ing, and various applications. Previous studies for
POS tagging and syntax parsing on these languages
sometimes assume that gold standard word segmen-
tation information is provided, which is not the re-
al scenario. In a fully automatic system, a pipeline
approach is often adopted, where raw sentences are
first segmented into word sequences, then POS tag-
ging and parsing are performed. This kind of ap-
proach suffers from error propagation. For exam-
ple, word segmentation errors will result in tagging
and parsing errors. Additionally, early modules can-
not use information from subsequent modules. In-
tuitively a joint model that performs the three tasks
together should help the system make the best deci-
sions.
In this paper, we propose a unified model for joint
Chinese word segmentation, POS tagging, and pars-
ing. Three sub-models are independently trained
using the state-of-the-art methods. We do not use
the joint inference algorithm for training because of
the high complexity caused by the large amount of
parameters. We use linear chain Conditional Ran-
dom Fields (CRFs) (Lafferty et al2001) to train the
word segmentation model and POS tagging model,
and averaged perceptron (Collins, 2002) to learn the
parsing model. During decoding, parameters of each
sub-model are scaled to represent its importance in
the joint model. Our decoding algorithm is an exten-
sion of CYK parsing. Initially, weights of all possi-
ble words together with their POS tags are calcu-
lated. When searching the parse tree, the word and
POS tagging features are dynamically generated and
the transition information of POS tagging is consid-
ered in the span merge operation.
Experiments are conducted on Chinese Tree Bank
(CTB) 5 dataset, which is widely used for Chinese
word segmentation, POS tagging and parsing. We
compare our proposed joint model with the pipeline
system, both built using the state-of-the-art sub-
models. We also propose an evaluation metric to
501
calculate the bracket scores for parsing in the face of
word segmentation errors. Our experimental results
show that the joint model significantly outperform-
s the pipeline method based on the state-of-the-art
sub-models.
2 Related Work
There is very limited previous work on joint Chinese
word segmentation, POS tagging, and parsing. Pre-
vious joint models mainly focus on word segmenta-
tion and POS tagging task, such as the virtual nodes
method (Qian et al2010), cascaded linear model
(Jiang et al2008a), perceptron (Zhang and Clark,
2008), sub-word based stacked learning (Sun, 2011),
reranking (Jiang et al2008b). These joint models
showed about 0.2 ? 1% F-score improvement over
the pipeline method. Recently, joint tagging and de-
pendency parsing has been studied as well (Li et al
2011; Lee et al2011).
Previous research has showed that word segmen-
tation has a great impact on parsing accuracy in
the pipeline method (Harper and Huang, 2009). In
(Jiang et al2009), additional data was used to im-
prove Chinese word segmentation, which resulted
in significant improvement on the parsing task us-
ing the pipeline framework. Joint segmentation and
parsing was also investigated for Arabic (Green and
Manning, 2010). A study that is closely related to
ours is (Goldberg and Tsarfaty, 2008), where a s-
ingle generative model was proposed for joint mor-
phological segmentation and syntactic parsing for
Hebrew. Different from that work, we use a discrim-
inative model, which benefits from large amounts of
features and is easier to deal with unknown words.
Another main difference is that, besides segmenta-
tion and parsing, we also incorporate the POS tag-
ging model into the CYK parsing framework.
3 Methods
For a given Chinese sentence, our task is to gener-
ate the word sequence, its POS tag sequence, and
the parse tree (constituent parsing). A joint model
is expected to make more optimal decisions than a
pipeline approach; however, such a model will be
too complex and it is difficult to estimate model pa-
rameters. Therefore we do not perform joint infer-
ence for training. Instead, we develop three individ-
ual models independently during training and per-
form joint decoding using them. In this section, we
first describe the three sub-models and then the joint
decoding algorithm.
3.1 Word Segmentation Model
Methods for Chinese word segmentation can be
broadly categorized into character based and word
based models. Previous studies showed that
character-based models are more effective to detect
out-of-vocabulary words while word-based model-
s are more accurate to predict in-vocabulary words
(Zhang et al2006). Here, we use order-0 semi-
Markov model (Sarawagi and Cohen, 2004) to take
advantages of both approaches.
More specifically, given a sentence x =
c1, c2, . . . , cl (where ci is the ith Chinese character,
l is the sentence length), the character-based mod-
el assigns each character with a word boundary tag.
Here we use the BCDIES tag set, which achieved
the best official performance (Zhao and Kit, 2008):
B, C, D, E denote the first, second, third, and last
character of a multi-character word respectively, I
denotes the other characters, and S denotes the s-
ingle character word. We use the same character-
based feature templates as in the best official system,
shown in Table 1 (1.1-1.3), including character un-
igram and bigram features, and transition features.
Linear chain CRFs are used for training.
Feature templates in the word-based model are
shown in Table 1 (1.4-1.6), including word features,
sub-word features, and character bigrams within
words. The word feature is activated if a predicted
word w is in the vocabulary (i.e., appears in train-
ing data). Subword(w) is the longest in-vocabulary
word within w. To use word features, we adopt a K-
best reranking approach. The top K candidate seg-
mentation results for each training sample are gen-
erated using the character-based model, and the gold
segmentation is added if it is not in the candidate set.
We use the Maximum Entropy (ME) model to learn
the weights of word features such that the probabil-
ity of the gold candidate is maximal.
A problem arises when combining the two mod-
els and using it in joint segmentation and parsing,
since the linear chain used in the character-based
model is incompatible with CYK parsing model and
the word-based model due to the transition informa-
502
Character Level Feature Templates
(1.1) ci?2yi, ci?1yi, ciyi, ci+1yi, ci+2yi
(1.2) ci?1ciyi, cici+1yi, ci?1ci+1yi
(1.3) yi?1yi
Word Level Feature Templates
(1.4) word w
(1.5) subword(w)
(1.6) character bigrams within w
Table 1: Feature templates for word segmentation. ci is
the ith character in the sentence, yi is its label, w is a
predicted word.
tion. Thus, we slightly modify the linear chain CRF-
s by fixing the weights of transition features during
training and testing. That is, weights of impossible
transition features (e.g., B?B) are set to ??, and
weights of the other transition features (e.g., E?B)
are set to 0. In this way, the transition feature could
be neglected in testing for two reasons. First, all ille-
gal label assignments are prohibited in prediction, s-
ince their weights are ??; second, because weights
of legal transition features are 0, they do not affec-
t the prediction at all. In the following, transition
features are excluded.
Now we can use order-0 semi Markov model as
the hybrid model. We define the score of a word as
the sum of the weights of all the features within the
word. Formally, the score of a multi-character word
w = ci, . . . , cj is defined as:
scoreseg(x, i, j) = ?CRF ? fCRF (x, yi = B) + . . .
+?CRF ? fCRF (x, yj = E) + ?ME ? fME(x, i, j)
? ?segfseg(x, i, j) (1)
where fCRF and fME are the feature vectors in the
character and word based models respectively, and
?CRF , ?ME are their corresponding weight vectors.
For simplicity, we denote ?seg = ?CRF?ME , fseg =
fCRF?ME , where ?CRF?ME means the concatena-
tion of ?CRF and ?ME . Scores for single character
words are defined similarly. These word scores will
be used in the joint segmentation and parsing task
Section 3.4.
3.2 POS Tagging Model
Though syntax parsing model can directly predict
the POS tag itself, we choose not to use this, but use
an independent POS tagger for two reasons. First,
there is a large amount of data with labeled POS tags
but no syntax annotations, such as the People?s Daily
corpus and SIGHAN bakeoff corpora (Jin and Chen,
2008). Such data can only be used to train POS tag-
gers, but not for training the parsing model. Often
using a larger training set will result in a better POS
tagger. Second, the state-of-the-art POS tagging sys-
tems are often trained by sequence labeling models,
not parsing models.
(2.1) wi?2ti, wi?1ti, witi, wi+1ti, wi+2ti
(2.2) wi?2wi?1ti, wi?1witi, wiwi+1ti,
wi+1wi+2ti wi?1wi+1ti
(2.3) c1(wi)ti, c2(wi)ti, c3(wi)ti, c?2(wi)ti
c?1(wi)ti
(2.4) c1(wi)c2(wi)ti, c?2(wi)c?1(wi)ti
(2.5) l(wi)ti
(2.5) ti?1ti
Table 2: Feature templates for POS tagging. wi is the
ith word in the sentence, ti is its POS tag. For a word w,
cj(w) is its jth character, c?j(w) is the last jth character,
and l(w) is its length.
The POS tagging problem is to assign a POS tag
t ? T to each word in a sentence. We also use lin-
ear chain CRFs for POS tagging. Feature templates
shown in Table 2 are the same as those in (Qian
et al2010), which have been shown effective on
CTB corpus. Three feature sets are considered: (i)
word level features, including surrounding word uni-
grams, bigrams, and word length; (ii) character level
features, such as the first and last characters in the
words; (iii) transition features.
3.3 Parsing Model
We choose discriminative models for parsing since it
is easy to handle unknown words by simply adding
character level features. Online structured learn-
ing algorithms were demonstrated to be effective for
training, such as stochastic optimization (Finkel et
al., 2008). In this study, we use averaged perceptron
algorithm for parameter estimation since it is easier
to implement and has competitive performance.
A Context Free Grammar (CFG) consists of (i) a
set of terminals; (ii) a set of nonterminals {Nk}; (i-
ii) a designated start symbol ROOT; and (iv) a set of
rules, {r = N i ? ?j}, where ?j is a sequence of
terminals and nonterminals. In the parsing task, ter-
503
!" "# $% &'(
Shanghai customs Chongming office
NR NN NR NN
NP
!"
"#
$% &'(
Shanghai
customs
Chongming office
NR
NN
NR NN
NP
NR_NN
NN_NR
Figure 1: Parse tree binarization
minals are the words, and nonterminals are the POS
tags and phrase types. In this paper, nonterminal is
named state for short. A parse tree T of sentence
x can be factorized into several one-level subtrees,
each corresponding to a rule r.
In practice, binarization of rules is necessary to
obtain cubic parsing time. That is, the right hand
side of each rule should contain no more than 2 s-
tates. We used right branching binarization, as il-
lustrated in Figure 1. We did not use parent anno-
tation, since we found it degraded the performance
in our experiments (shown in Section 4). We used
the same preprocessing step as (Harper and Huang,
2009), collapsing all the allowed nonterminal-yield
unary chains to single unary rules. Therefore, all s-
pans in the binarized trees contain no more than one
unary rules. To facilitate decoding, we unify the for-
m of spans so that each span contains exactly one u-
nary rule. This is done by adding identity unary rules
(N ? N ) to spans that have no unary rule. These
identity unary rules will be removed in evaluation.
Hence, there are two states of a span: the top state
N and the bottom state N that correspond to the left
and right hand of the unary rule runary = N ? N
respectively, as shown in Figure 2.
Table 3 lists the feature templates we use for pars-
ing. There are 4 feature sets: (i) bottom state fea-
tures fbottom(i, j,x, N i,j), which depend on the bot-
!" #$
Lastyear realized
VV
VP
IP
CP
!" #$
Last year realized
NP VV
VP
CP
NT VV
top state
bottom state
NP
NT
Figure 2: Unary rule normalization. Nonterminal-yield
unary chains are collapsed to single unary rules. Identity
unary rules are added to spans that have no unary rule.
tom states; (ii) top state features ftop(i, j,x, N i,j);
(iii) unary rule features funary(i, j,x, runaryi,j ), which
extract the transition information from bottom s-
tates to top states; (iv) binary rule features
fbinary(i, j, k,x, rbinaryi,j,k = N i,j ? N i,k?1 +Nk,r),
where N i,k?1, Nk,r are the top states of the left and
right children.
The score function for a sentence xwith parse tree
T is defined as:
score(x, T ) =
?
N i,j?T
?bottom ? fbottom(i, j,x, N i,j)
+
?
N i,j?T
?top ? ftop(i, j,x, N i,j)
+
?
runaryi,j ?T
?unary ? funary(i, j,x, runaryi,j )
+
?
rbinaryi,j,k ?T
?binary ? fbinary(i, j,x, rbinaryi,j,k )
where ?bottom, ?top, ?unary, ?binary are the weight
vectors of the four feature sets.
Given the training corpus {(xi, T?i)}, the learning
task is to estimate the weight vectors so that for each
sentence xi, the gold standard tree T?i achieves the
maximal score among all the possible trees. The per-
ceptron algorithm is guaranteed to find the solution
if it exists.
3.4 Joint Decoding
The three models described above are separately
trained to make parameter estimation feasible as
well as optimize each individual component. In test-
504
(3.1) Binary rule templates
N ? N l +Nr
Xl Xm?1Xr lenllenr Xl Xm Xr lenl lenr
Xl Xm?1 Xr wordm?1(ROOT) Xl + Xm Xr wordm(ROOT)
(3.2) Unary rule templates
N ? N
(3.3) Bottom state templates
Xllen Xrlen
Xl?2Xl?1 Xr+1len Xl?1 Xr+1 Xr+2len
wllwlrXllen wllwlrXrlen XlXrwlllen XlXrwlrlen
wordlwordrXlXrlen wordlwordrXlXr
Xl?1Xl(LEAF) Xl+1Xl(LEAF) Xlwordl(LEAF) Xlwll(LEAF)
Xl+aXr+blen wordl+awordr+b ?1 ? a, b ? 1
(3.3) Top state templates
Xl?1Xl(LEAF) Xl+1Xl(LEAF) Xlwordl(LEAF) Xlwll(LEAF)
Xl+aXr+blen wordl+awordr+b ?1 ? a, b ? 1
Table 3: Feature templates for parsing, where X can be word, first and last character of word, first and last character
bigram of word, POS tag. Xl+a/Xr?a denotes the first/last ath X in the span, while Xl?a/Xr+a denotes the ath X
left/right to span. Xm is the first X of right child, and Xm?1 is the last X of the left child. len, lenl, lenr denote the
length of the span, left child and right child respectively. wl is the length of word. ROOT/LEAF means the template
can only generate the features for the root/initial span.
ing, we perform joint decoding to combine informa-
tion from the three models. Parameters of word seg-
mentation (?seg), POS tagging (?pos), and parsing
models (?parse = ?bottom?top? unary?bianry) are s-
caled by three positive hyper-parameters ?, ?, and
? respectively, which control their contribution in
the joint model. If ? >> ? >> ?, then the join-
t model is equivalent to a pipeline model, in which
there is no feedback from downstream models to up-
stream ones. For well tuned hyper-parameters, we
expect that segmentation and POS tagging results
can be improved by parsing information. The hyper-
parameters are tuned on development data. In the
following sections, for simplicity we drop ?, ?, ?,
and just use ?seg, ?pos, ?parse to represent the scaled
parameters.
The basic idea of our decoding algorithm is to ex-
tend the CYK parsing algorithm so that it can deal
with transition features in POS tagging and segmen-
tation scores in word segmentation.
3.4.1 Algorithm
The joint decoding algorithm is shown in Algo-
rithm 1. Given a sentence x = c1, . . . , cl, Line 0
calculates the scores of all possible words in the sen-
tence using Eq(1). There are l(l + 1)/2 word candi-
dates in total.
Surrounding words are important features for
POS tagging and parsing; however, they are un-
available because segmentation is incomplete before
parsing. Therefore, we adopt pseudo surrounding
features by simply fixing the context words as the s-
ingle most likely ones. Given a word candidate wi,j
from ci to cj , its previous word s? is the rightmost
one in the best word sequence of c1, . . . , ci?1, which
can be obtained by dynamic programming. Recur-
sively, the second word left to wi,j is the previous
word of s?. The next word of wi,j is defined similar-
ly. In Line 1, we use bidirectional Viterbi decoding
to obtain all the surrounding words. In the forward
direction, the algorithm starts from the first charac-
ter boundary to the last, and finds the best previous
word for the ith character boundary bi. In the back-
ward direction, the algorithm starts from right to left,
and finds the best next word of each bi.
In Line 2, for each word candidate, we can calcu-
late the score of each POS tag using state features in
the POS tagging model, since the context words are
available now. The score function of word wi,j with
POS tag t is:
scoreseg?pos(x, i, j, t) =
scoreseg(x, i, j) + ?pos ? fpos(x, wi,j , t) (2)
In Line 3, POS tags of surrounding words can
be obtained similarly using bidirectional decoding.
505
Algorithm 1 Joint Word Segmentation, POS tagging, and Parsing Algorithm
Input: Sentence x = c1, . . . , cl, beam size B, scaled word segmentation model, POS tagging model and
parsing model.
Output: Word sequence, POS tag sequence, and parse tree
0: ?0 ? i ? j ? l ? 1, calculate scoreseg(x, i, j) using Equation (1)
1: For each character boundary bi, 0 ? i ? l, get the best previous and next words of bi using bidirectional
Viterbi decoding
2: ?0 ? i ? j ? l ? 1, t ? T , calculate scoreseg?pos(x, i, j, t) using Equation (2)
3: ?bi, 0 ? i ? l, t ? T , get the best POS tags of words left/right to bi using bidirectional viterbi
decoding.
4: For each word candidate wi,j , 0 ? i ? j ? l ? 1
5: For each bottom state N , POS tag t ? T  step 1 (Line 5-7): get bottom states
6: scorebottom(x, i, j, wi,j , t, N) = scoreseg?pos(x, i, j, t) + ?bottom ? fbottom(x, i, j, wi,j , t, N)
7: Keep B best scorebottom.
8: For each top state N  step 2 (Line 8-9): get top states
9: scoretop(x, i, j, wi,j , t, N) = maxN {scorebottom(x, i, j, wi,j , t, N) + ?top ? ftop(x, i, j, wi,j , t, N)
+?unary ? funary(x, i, j, wi,j , t, N ? N)
}
10: for i = 0, . . . , l ? 1 do
11: for width = 1, . . . , l ? 1 do
12: j = i + width
13: for k = i + 1, . . . , j do
14: scorebottom(x, i, j,w, t, N) = maxl,r
{
scoretop(x, i, k ? 1,wl, tl, N l) + scoretop(x, k, j,wr, tr, Nr)
+?binary ? fbinary(x, i, j, k,w, t, N ? Nr + Nr) + ?pos ? fpos(tlastl ? tfirstr )
+?bottomfbottom(x, i, j,w, t, N)}
15: Keep B best scorebottom  step 1 (Line 14-15): get bottom states
16: For each top state N  step 2 (Line 16-17): get top states
17: scoretop(x, i, j,w, t, N) = maxN {scorebottom(x, i, j,w, t, N)
+?unary ? funary(x, i, j,w, t, N ? N)
}
18: end for
19: end for
20: end for
Line 0 1 2 3 6 9 14 15 Total Bound(w.r.t. l)
Complexity l2 l2 |T |l2 |T |2l2 |T |Ml2 BMl2 l3MB2 BMl2 l3MB2
Table 4: Complexity Analysis of Algorithm 1.
That is, for wi,j with POS tag t, we use Viterbi algo-
rithm to search the optimal POS tags of its left and
right words.
In Lines 4-9, each word was initialized as a basic
span. A span structure in the joint model is a 6-tuple:
S(i, j,w, t, N,N), where i, j are the boundary in-
dices,w, t are the word sequence and POS sequence
within the span respectively, and N,N are the bot-
tom and top states. There are two types of surround-
ing n-grams: one is inside the span, for example, the
first word of a span, which can be obtained from w;
the other is outside the span, for example, the pre-
vious word of a span, which is obtained from the
pseudo context information. The score of a basic s-
pan depends on its corresponding word and POS pair
score, and the weights of the active state and unary
features.
To avoid enumerating the combination of the bot-
tom and top states, initialization for each span is di-
vided into 2 steps. In the first step, the score of ev-
ery bottom state is calculated using bottom state fea-
tures, and only the B best states are maintained (see
Line 6-7). In the second step, top state features and
unary rule features are used to get the score of each
top state (Line 9), and only the top B states are pre-
served.
506
Similarly, there are two steps in the merge opera-
tion: S(i, j,w, t, N,N) = Sl(i, k,wl, tl, Nl, Nl)+
Sr(k + 1, j,wr, tr, Nr, Nr). The score of the bot-
tom state N is calculated using binary features
fbinary(x, i, j, k,w, t, N ? N r+N r), bottom state
features fbottom(x, i, j,w, t, N), and POS tag transi-
tion features that depend on the boundary POS tags
of Sl and Sr. See Line 14 of Algorithm 1, where
tlastl and t
first
r are the POS tags of the last word in
the left child span and the first word in the right child
span respectively.
3.4.2 Complexity analysis
Given a sentence of length l, the complexity for
each line of Algorithm 1 is listed in Table 4, where
|T | is the size of POS tag set, M is the number of
states, and B is the beam size.
4 Experiments
4.1 Data
For comparison with other systems, we use the CT-
B5 corpus, which has been studied for Chinese word
segmentation, POS tagging and parsing. We use the
standard train/develop/test split of the data. Details
are shown in Table 5.
CTB files # sent. # words
Training 1-270 18089 493,939
400-1151
Develop 301-325 350 6,821
Test 271-300 348 8,008
Table 5: Training, development, and test data of CTB 5.
4.2 Evaluation Metric
We evaluate system performance on the individual
tasks, as well as the joint tasks.1 For word segmen-
tation, three metrics are used for evaluation: pre-
cision (P), recall (R), and F-score (F) defined by
2PR/(P+R). Precision is the percentage of correct
words in the system output. Recall is the percent-
age of words in gold standard annotations that are
correctly predicted. For parsing, we use the stan-
dard parseval evaluation metrics: bracketing preci-
sion, recall and F-score.
1Note that the joint task refers to automatic segmentation
and tagging/parsing. It can be achieved using a pipeline system
or our joint decoding method.
For joint word segmentation and POS tagging, a
word is correctly predicted if both the boundaries
and the POS tag are correctly identified. For joint
segmentation, POS tagging, and parsing task, when
calculating the bracket scores using existing parseval
tools, we need to consider possible word segmenta-
tion errors. To do this, we add the word boundary
information in states ? a bracket is correct only if
its boundaries, label and word segmentation are all
correct. One example is shown in Figure 3. Notice
that identity unary rules are removed during evalua-
tion. The basic spans are characters, not words, be-
cause the number of words in reference and predic-
tion may be different. POS tags are removed since
they do not affect the bracket scores. If the segmen-
tation is perfect, then the bracket scores of the mod-
ified tree are exactly the same as the original tree.
This is similar to evaluating parsing performance on
speech transcripts with automatic sentence segmen-
tation (Roark et al2006).
! " # $ %
NP(0,2,5)
!" #$%
Shanghai office
NP
NR NN - - - - -
Shanghai office
Figure 3: Boundary information is added to states to cal-
culate the bracket scores in the face of word segmentation
errors. Left: the original parse tree, Right: the converted
parse tree. The numbers in the brackets are the indices of
the character boundaries based on word segmentation.
4.3 Parameter Estimation
We train three submodels using the gold features,
that is, POS tagger is trained using the perfect seg-
mentation, and parser is trained using perfect seg-
mentation and POS tags. Some studies reported
that better performance may be achieved by train-
ing subsequent models using representative output
of the preceding models (Che et al2009). Hence
for comparison we trained another parser using auto-
matically generated POS tags obtained from 10-fold
cross validation, but did not find significant differ-
ence between these two parsers when testing on the
perfectly segmented development dataset. Therefore
507
we use the parser trained with perfect POS tags for
the joint task.
Three hyper-parameters, ?, ?, and ?, are tuned
on development data using a heuristic search. Pa-
rameters that achieved the best joint parsing result
are selected. In the search, we fixed ? = 1 and
varied ?, ?. First, we set ? = 1, and enumerate
? = 14 ,
1
2 , 1, 2, . . . , and choose the best ?
?. Then,
we set ? = ?? and vary ? = 14 ,
1
2 , 1, 2, . . . , and
select the best ??.
Table 6 lists the parameters we used for training
the submodels, as well as the hyper-parameters for
joint decoding.
Model Parameter Value
Character based Gaussian prior 0.01
word segmentor # Feature 3,875,802
Word based Gaussian prior 0.01
word segmentor # Feature 312,533
POS tagger Gaussian prior 0.1
# Feature 48,608,802
Parser Iteration Number 10
# Feature 49,369,843
Hyper-parameter ? 4
Hyper-parameter ? 0.5
Joint Hyper-parameter ? 1
Beam Size B 20
Table 6: Parameters used in our system.
4.4 Experimental Results
In this section we first show that our sub-models are
better than or comparable to state-of-the-art systems,
and then the joint model is superior to the pipeline
approach.
4.4.1 Evaluating Sub-models
Table 7 shows word segmentation results using
our word segmentation submodel, in comparison to
a few state-of-the-art systems. For our segmentor,
we show results for two variants: one removes tran-
sition features as described in Section 3.1, the other
uses CRFs to learn the weights of transition features.
We can see that our system is competitive with al-
l the others except Sun?s that used additional idiom
resources. Our two word segmentors have similar
performance. Since the one without transition fea-
tures can be naturally integrated into the joint sys-
tem, we use it in the following joint tasks.
System P R F
(Jiang et al2008b) - - 97.74
(Jiang et al2008a) - - 97.85
(Kruengkrai et al2009) 97.46 98.29 97.87
(Zhang and Clark, 2010) - - 97.78
(Zhang and Clark, 2011) - - 97.78
(Sun, 2011) - - 98.17
Ours (w/o transition features) 97.45 98.24 97.85
Ours (with transition features) 97.44 98.23 97.84
Table 7: Word segmentation results.
For the POS tagging only task that takes gold s-
tandard word segmentation as input, we have two
systems. One uses the linear chain CRFs as de-
scribed in Section 3.2, the other is obtained using the
parser described in Section 3.3 ? the parser gener-
ates POS tag hypotheses when POS tag features are
not used. The POS tagging accuracy is 95.53% and
95.10% using these two methods respectively. The
better performance from the former system may be
because the local label dependency is more helpful
for POS tagging than the long distance dependencies
that might be noisy. This result also confirms our
choice of using an independent POS tagger for the
sub-model, rather than relying on a parser for POS
tagging. However, since there are no reported results
for this setup, we demonstrate the competence of our
POS tagger using the joint word segmentation and
POS tagging task. Table 8 shows the performance of
a few systems along with ours, all using the pipeline
approach where automatic segmentation is followed
by POS tagging. We can see that our POS tagger is
comparable to the others.
System P R F
(Jiang et al2008b) - - 93.37
(Jiang et al2008a) - - 93.41
(Kruengkrai et al2009) 93.28 94.07 93.67
(Zhang and Clark, 2010) - - 93.67
(Zhang and Clark, 2011) - - 93.67
(Sun, 2011) - - 94.02
Ours (pipeline) 93.10 93.96 93.53
Table 8: Results for the joint word segmentation and POS
tagging task.
For parsing, Table 9 presents the parsing result
on gold standard segmented sentence. Notice that
the result of (Harper and Huang, 2009; Zhang and
508
Clark, 2011) are not directly comparable to ours, as
they used a different data split. The best published
system result on CTB5 is Petrov and Klein?s, which
used PCFG with latent Variables. Our system per-
forms better mainly because it benefits from a large
amount of features.
System LP LR F
(Petrov and Klein, 2007) 84.8 81.9 83.3
(Jiang et al2009) - - 82.35
(Harper and Huang, 2009)* 83.22 82.84 83.03
(Zhang and Clark, 2011)* 78.6 78.0 78.3
Ours 84.57 83.68 84.13
Ours (w/ parent annotation) 83.35 82.73 83.04
Ours (no POS tag feature) 83.49 82.97 83.23
Table 9: Parsing results using gold standard word seg-
mentation.
For our parser, besides the model described in
Section 3.3, we tried two variations: one does not
use the automatic POS tag features, the other one is
learned on the parent annotated training data. The
results in Table 9 show that there is a performance
degradation when using parent annotation. This may
be due to the introduction of a large number of s-
tates, resulting in sparse features. We also notice
that with the help of the POS tag information, even
automatically generated, the parser gained 0.9% im-
provement in F-score. This demonstrates the advan-
tage of using a better independent POS tagger and
incorporating it in parsing.
Finally Table 10 shows the results for the three
tasks using our joint decoding method in compari-
son to the pipeline method. We can see that the joint
model outperforms the pipeline one. This is mainly
because of a better parsing module as well as join-
t decoding. In the table we also include results of
(Jiang et al2009), which is the only reported join-
t parsing result we found using the same data split
on CTB5. They achieved 80.28% parsing F-score
using automatic word segmentation. Their adapted
system Jiang09+ leveraged additional corpus to im-
prove Chinese word segmentation, resulting in an F-
score of 81.07%. Our system has better performance
than these.
System Task P R F
Jiang09 Parse - - 80.28
Jiang09+ Parse - - 81.07
Ours Seg. 97.45 98.24 97.85
Pipeline POS 93.10 93.96 93.53
Parse 81.87 81.65 81.76
Ours Seg. 97.56 98.36 97.96
Joint POS 93.43 94.20 93.81
Parse 83.03 82.66 82.85
Table 10: Results for the joint segmentation, tagging, and
parsing task using pipeline and joint models.
4.5 Error Analysis
We compared the results from the pipeline and our
joint decoding systems in order to understand the
impact of the joint model on word segmentation and
POS tagging. We notice that the joint model tend to
generate more words than the pipeline model. For
example, ?????? is one word in the pipeline
model, but correctly segmented as two words ??
?/??? in the joint model. This tendency of seg-
mentation also makes it fail to recognize some long
words, especially OOV words. For example, ??
??? is segmented as ???/??. In the data set,
we find that, the joint model corrected 10 missing
boundaries over the pipeline method, and introduced
3 false positive segmentation errors.
For the analysis of POS tags, we only examined
the words that are correctly segmented by both the
pipeline and the joint models. Table 11 shows the
increase and decrease of error patterns of the joint
model over the pipeline POS tagger. An error pat-
tern ?X ? Y? means that the word whose true tag is
?X? is assigned a tag ?Y?. All the patterns are ranked
in descending order of the reduction/increase of the
error number. We can see that the joint model has a
clear advantage in the disambiguation of {VV, NN}
and {DEG, DEC}, which results in the overall im-
proved performance. In contrast, the joint method
performs worse on ambiguous POS pairs such as
{NN,NR}. This observation is similar to those re-
ported by (Li et al2011; Hatori et al2011).
5 Conclusion
In this paper, we proposed a new algorithm for joint
Chinese word segmentation, POS tagging, and pars-
ing. Our algorithm is an extension of the CYK
509
error pattern # ? error pattern # ?
NN? VV 47 19 NN? NR 15 12
VV? NN 42 13 NR? NN 7 5
DEG? DEC 23 10 JJ? P 1 4
NN? JJ 29 8 NN? DT 2 4
DEC? DEG 11 4 P? VV 3 2
JJ? NN 12 4 AD? NN 1 2
Table 11: POS tagging error patterns. # means the error
number of the corresponding pattern made by the pipeline
tagging model. ? and ? mean the error number reduced
or increased by the joint model.
parsing method. The sub-models are independently
trained for the three tasks to reduce model complex-
ity and optimize individual sub-models. Our exper-
iments demonstrate the advantage of the joint mod-
els. In the future work, we will compare this joint
model to the pipeline approach that uses multiple
candidates or soft decisions in the early modules.
We will also investigate methods for joint learning
as well as ways to speed up the joint decoding algo-
rithm.
Acknowledgments The authors thank Zhongqiang
Huang for his help with experiments. This work
is partly supported by DARPA under Contrac-
t No. HR0011-12-C-0016. Any opinions expressed
in this material are those of the authors and do not
necessarily reflect the views of DARPA.
References
Wanxiang Che, Zhenghua Li, Yongqiang Li, Yuhang
Guo, Bing Qin, and Ting Liu. 2009. Multilingual
dependency-based syntactic and semantic parsing. In
Proceedings of CoNLL 09, pages 49?54.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings of
EMNLP 2002, pages 1?8.
Jenny Rose Finkel, Alex Kleeman, and Christopher D.
Manning. 2008. Efficient, feature-based, condition-
al random field parsing. In Proceedings of ACL-08:
HLT, pages 959?967.
Yoav Goldberg and Reut Tsarfaty. 2008. A single gener-
ative model for joint morphological segmentation and
syntactic parsing. In Proceedings of ACL 2008: HLT,
pages 371?379.
Spence Green and Christopher D. Manning. 2010. Better
arbic parsing: Baselines, evaluations, and analysis. In
Proceedings of Coling 2010, pages 394?402.
Mary Harper and Zhongqiang Huang. 2009. Chinese
statistical parsing. In Gale Book.
Jun Hatori, Takuya Matsuzaki, Yusuke Miyao, and
Jun?ichi Tsujii. 2011. Incremental joint pos tagging
and dependency parsing in chinese. In Proceedings of
IJCNLP 2011, pages 1216?1224.
Wenbin Jiang, Liang Huang, Qun Liu, and Yajuan Lu?.
2008a. A cascaded linear model for joint chinese word
segmentation and part-of-speech tagging. In Proceed-
ings of ACL 2008: HLT, pages 897?904.
Wenbin Jiang, Haitao Mi, and Qun Liu. 2008b. Word lat-
tice reranking for chinese word segmentation and part-
of-speech tagging. In Proceedings of Coling 2008,
pages 385?392.
Wenbin Jiang, Liang Huang, and Qun Liu. 2009. Au-
tomatic adaptation of annotation standards: Chinese
word segmentation and pos tagging ? a case study. In
Proceedings of ACL-IJCNLP 2009, pages 522?530.
Guangjin Jin and Xiao Chen. 2008. The fourth interna-
tional chinese language processing bakeoff: Chinese
word segmentation, named entity recognition and chi-
nese pos tagging. In Proceedings of Sixth SIGHAN
Workshop on Chinese Language Processing.
Canasai Kruengkrai, Kiyotaka Uchimoto, Jun?ichi Kaza-
ma, Yiou Wang, Kentaro Torisawa, and Hitoshi Isa-
hara. 2009. An error-driven word-character hybrid
model for joint chinese word segmentation and pos
tagging. In Proceedings of ACL 2009, pages 513?521.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic mod-
els for segmenting and labeling sequence data. In Pro-
ceedings of ICML 2001, pages 282?289.
John Lee, Jason Naradowsky, and David A. Smith. 2011.
A discriminative model for joint morphological disam-
biguation and dependency parsing. In Proceedings A-
CL 2011: HLT, pages 885?894.
Zhenghua Li, Min Zhang, Wanxiang Che, Ting Liu, Wen-
liang Chen, and Haizhou Li. 2011. Joint models for
chinese pos tagging and dependency parsing. In Pro-
ceedings of EMNLP 2011, pages 1180?1191.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of NAACL
2007, pages 404?411.
Xian Qian, Qi Zhang, Yaqian Zhou, Xuanjing Huang, and
Lide Wu. 2010. Joint training and decoding using
virtual nodes for cascaded segmentation and tagging
tasks. In Proceedings of EMNLP 2010, pages 187?
195.
Brian Roark, Mary Harper, Eugene Charniak, Bonnie
Dorr, Mark Johnson, Jeremy G. Kahn, Yang Liu, Mari
Ostendorf, John Hale, Anna Krasnyanskaya, Matthew
Lease, Izhak Shafran, Matthew Snover, Robin Stew-
art, Lisa Yung, and Lisa Yung. 2006. Sparseval: E-
510
valuation metrics for parsing speech. In Proceedings
Language Resources and Evaluation (LREC).
Sunita Sarawagi and William W. Cohen. 2004. Semi-
markov conditional random fields for information ex-
traction. In Proceedings of NIPS 2004.
Weiwei Sun. 2011. A stacked sub-word model for join-
t chinese word segmentation and part-of-speech tag-
ging. In Proceedings of ACL 2011, pages 1385?1394.
Yue Zhang and Stephen Clark. 2008. Joint word seg-
mentation and POS tagging using a single perceptron.
In Proceedings of ACL 2008: HLT, pages 888?896.
Yue Zhang and Stephen Clark. 2010. A fast decoder for
joint word segmentation and POS-tagging using a sin-
gle discriminative model. In Proceedings of EMNLP
2010, pages 843?852.
Yue Zhang and Stephen Clark. 2011. Syntactic process-
ing using the generalized perceptron and beam search.
Comput. Linguist., 37(1):105?151.
Ruiqiang Zhang, Genichiro Kikui, and Eiichiro Sumi-
ta. 2006. Subword-based tagging for confidence-
dependent chinese word segmentation. In Proceedings
of the COLING/ACL 2006, pages 961?968.
Hai Zhao and Chunyu Kit. 2008. Unsupervised segmen-
tation helps supervised learning of character tagging
forword segmentation and named entity recognition.
In Proceedings of Sixth SIGHANWorkshop on Chinese
Language Processing, pages 106?111.
511
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1191?1200, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Left-to-Right Tree-to-String Decoding with Prediction
Yang Feng? Yang Liu? Qun Liu? Trevor Cohn?
? Department of Computer Science
The University of Sheffield, Sheffield, UK
{y.feng, t.cohn}@sheffield.ac.uk
? State Key Laboratory on Intelligent Technology and Systems
Tsinghua National Laboratory for Information Science and Technology
Department of Computer Sci. and Tech., Tsinghua University, Beijing, China
liuyang2011@tsinghua.edu.cn
?Key Laboratory of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences, Beijing, China
liuqun@ict.ac.cn
Abstract
Decoding algorithms for syntax based ma-
chine translation suffer from high compu-
tational complexity, a consequence of in-
tersecting a language model with a con-
text free grammar. Left-to-right decoding,
which generates the target string in order,
can improve decoding efficiency by simpli-
fying the language model evaluation. This
paper presents a novel left to right decod-
ing algorithm for tree-to-string translation, us-
ing a bottom-up parsing strategy and dynamic
future cost estimation for each partial trans-
lation. Our method outperforms previously
published tree-to-string decoders, including a
competing left-to-right method.
1 Introduction
In recent years there has been rapid progress in the
development of tree-to-string models for statistical
machine translation. These models use the syntac-
tic parse tree of the source language to inform its
translation, which allows the models to capture con-
sistent syntactic transformations between the source
and target languages, e.g., from subject-verb-object
to subject-object-verb word orderings. Decoding al-
gorithms for grammar-based translation seek to find
the best string in the intersection between a weighted
context free grammar (the translation mode, given a
source string/tree) and a weighted finite state accep-
tor (an n-gram language model). This intersection
is problematic, as it results in an intractably large
grammar, and makes exact search impossible.
Most researchers have resorted to approximate
search, typically beam search (Chiang, 2007). The
decoder parses the source sentence, recording the
target translations for each span.1 As the partial
translation hypothesis grows, its component ngrams
are scored and the hypothesis score is updated. This
decoding method though is inefficient as it requires
recording the language model context (n? 1 words)
on the left and right edges of each chart cell. These
contexts allow for boundary ngrams to be evaluated
when the cell is used in another grammar produc-
tion. In contrast, if the target string is generated
in left-to-right order, then only one language model
context is required, and the problem of language
model evaluation is vastly simplified.
In this paper, we develop a novel method of left-
to-right decoding for tree-to-string translation using
a shift-reduce parsing strategy. A central issue in
any decoding algorithm is the technique used for
pruning the search space. Our left-to-right decod-
ing algorithm groups hypotheses, which cover the
same number of source words, into a bin. Pruning
requires the evaluation of different hypotheses in the
same bin, and elimating the least promising options.
As each hypotheses may cover different sets of tree
1The process is analogous for tree-to-string models, except
that only rules and spans matching those in the source trees are
considered. Typically nodes are visited according to a post-
order traversal.
1191
nodes, it is necessary to consider the cost of uncov-
ered nodes, i.e., the future cost. We show that a good
future cost estimate is essential for accurate and effi-
cient search, leading to high quality translation out-
put.
Other researchers have also considered the left-
to-right decoding algorithm for tree-to-string mod-
els. Huang and Mi (2010) developed an Earley-
style parsing algorithm (Earley, 1970). In their ap-
proach, hypotheses covering the same number of
tree nodes were binned together. Their method uses
a top-down depth-first search, with a mechanism for
early elimation of some rules which lead to dead-
ends in the search. Huang and Mi (2010)?s method
was shown to outperform the traditional post-order-
traversal decoding algorithm, considering fewer hy-
potheses and thus decoding much faster at the same
level of performance. However their algorithm used
a very rough estimate of future cost, resulting in
more search errors than our approach.
Our experiments show that compared with the
Earley-style left-to-right decoding (Huang and Mi,
2010) and the traditional post-order-traversal de-
coding (Liu et al 2006) algorithms, our algorithm
achieves a significant improvement on search capac-
ity and better translation performance at the same
level of speed.
2 Background
A typical tree-to-string system (Liu et al 2006;
Huang et al 2006) searches through a 1-best source
parse tree for the best derivation. It transduces the
source tree into a target-language string using a Syn-
chronous Tree Substitution Grammar (STSG). The
grammar rules are extracted from bilingual word
alignments using the GHKM algorithm (Galley et
al., 2004).
We will briefly review the traditional decoding al-
gorithm (Liu et al 2006) and the Earley-style top-
down decoding algorithm (Huang and Mi, 2010) for
the tree-to-string model.
2.1 Traditional Decoding
The traditional decoding algorithm processes source
tree nodes one by one according to a post-order
traversal. For each node, it applies matched STSG
rules by substituting each non-terminal with its cor-
in theory beam search
traditional O(nc|?V |4(g?1)) O(ncb2)
top-down O(c(cr)d|V |g?1) O(ncb)
bottom-up O((cr)d|V |g?1) O(nub)
Table 1: Time complexity of different algorithms. tra-
ditional : Liu et al(2006), top-down : Huang and Mi
(2010). n is the source sentence length, b is the beam
width, c is the number of rules used for each node, V
is the target word vocabulary, g is the order of the lan-
guage model, d is the depth of the source parse tree, u is
the number of viable prefixes for each node and r is the
maximum arity of each rule.
responding translation. For the derivation in Figure
1 (b), the traditional algorithm applies r2 at node
NN2
r2 : NN2 (jieguo) ? the result,
to obtain ?the result? as the translation of NN2. Next
it applies r4 at node NP,
r4 : NP ( NN1 (toupiao), x1 : NN2 )
? x1 of the vote
and replaces NN2 with its translation ?the result?,
then it gets the translation of NP as ?the result of the
vote?.
This algorithm needs to contain boundary words
at both left and right extremities of the target string
for the purpose of LM evaluation, which leads to a
high time complexity. The time complexity in the-
ory and with beam search (Huang and Mi, 2010) is
shown in Table 1.
2.2 Earley-style Top-down Decoding
The Earley-style decoding algorithm performs a top-
down depth-first parsing and generates the target
translation left to right. It applies Context-Free
Grammar (CFG) rules and employs three actions:
predict, scan and complete (Section 3.1 describes
how to convert STSG rules into CFG rules). We can
simulate its translation process using a stack with a
dot  indicating which symbol to process next. For
the derivation in Figure 1(b) and CFG rules in Fig-
ure 1(c), Figure 2 illustrates the whole translation
process.
The time complexity is shown in Table 1 .
1192
3 Bottom-Up Left-to-Right Decoding
We propose a novel method of left-to-right decoding
for tree-to-string translation using a bottom-up pars-
ing strategy. We use viable prefixes (Aho and John-
son, 1974) to indicate all possible target strings the
translations of each node should starts with. There-
fore, given a tree node to expand, our algorithm
can drop immediately to target terminals no matter
whether there is a gap or not. We say that there is a
gap between two symbols in a derivation when there
are many rules separating them, e.g. IP r6? ... r4?
NN2. For the derivation in Figure 1(b), our algo-
rithm starts from the root node IP and applies r2
first although there is a gap between IP and NN2.
Then it applies r4, r5 and r6 in sequence to generate
the translation ?the result of the vote was released
at night?. Our algorithm takes the gap as a black-
box and does not need to fix which partial deriva-
tion should be used for the gap at the moment. So it
can get target strings as soon as possible and thereby
perform more accurate pruning. A valid derivation
is generated only when the source tree is completely
matched by rules.
Our bottom-up decoding algorithm involves the
following steps:
1. Match STSG rules against the source tree.
2. Convert STSG rules to CFG rules.
3. Collect the viable prefix set for each node in a
post-order transversal.
4. Search bottom-up for the best derivation.
3.1 From STSG to CFG
After rule matching, each tree node has its applica-
ble STSG rule set. Given a matched STSG rule, our
decoding algorithm only needs to consider the tree
node the rule can be applied to and the target side,
so we follow Huang and Mi (2010) to convert STSG
rules to CFG rules. For example, an STSG rule
NP ( NN1 (toupiao), x1 : NN2 ) ? x1 of the vote
can be converted to a CFG rule
NP ? NN2 of the vote
The target non-terminals are replaced with corre-
sponding source non-terminals. Figure 1 (c) shows
all converted CFG rules for the toy example. Note
IP
NP
NN1
to?up?`ao
NN2
j??eguo?
VP
NT
wa?nsha`ng
VV
go?ngbu`
(a) Source parse tree
r6: IP
NP VP
? ?
r4: NP
NN1
to?up?`ao
NN2
r5: VP
NT
wa?nsha`ng
VV
go?ngbu`
?
r2: NN2
j??eguo?
the result of the vote was released at night
(b) A derivation
r1: NN1 ? the vote
r2: NN2 ? the result
r3: NP ? NN2 of NN1
r4: NP ? NN2 of the vote
r5: VP ? was released at night
r6: IP ? NP VP
r7: IP ? NN2 of the vote VP
r8: IP ? VP NP
(c) Target-side CFG rule set
Figure 1: A toy example.
that different STSG rules might be converted to the
same CFG rule despite having different source tree
structures.
3.2 Viable Prefix
During decoding, how do we decide which rules
should be used next given a partial derivation, es-
pecially when there is a gap? A key observation is
that some rules should be excluded. For example,
any derivation for Figure 1(a) will never begin with
r1 as there is no translation starting with ?the vote?.
In order to know which rules can be excluded for
each node, we can recursively calculate the start-
ing terminal strings for each node. For example,
1193
NN1: {the vote} NN2: {the result}
NT: ? VV: ?
NP: {the result}
VP: {was released at night}
IP: {the result, was released at night}
Table 2: The Viable prefix sets for Figure 1 (c)
according to r1, the starting terminal string of the
translation for NN1 is ?the vote?. According to r2,
the starting terminal string for NN2 is ?the result?.
According to r3, the starting terminal string of NP
must include that of NN2. Table 2 lists the starting
terminal strings of all nodes in Figure 1(a). As the
translations of node IP should begin with either ?the
result? or ?was released at night?, the first rule must
be either r2 or r5. Therefore, r1 will never be used
as the first rule in any derivation.
We refer to starting terminal strings of a node as
a viable prefixes, a term borrowed from LR pars-
ing (Aho and Johnson, 1974). Viable prefixes are
used to decide which rule should be used to ensure
efficient left-to-right target generation. Formally, as-
sume that VN denotes the set of non-terminals (i.e.,
source tree node labels), VT denotes the set of ter-
minals (i.e., target words), v1, v2 ? VN , w ? VT ,
pi ? {VT ? VN}?, we say that w is a viable prefix of
v1 if and only if:
? v1 ? w, or
? v1 ? wv2pi, or
? v1 ? v2pi, and w is a viable prefix of v2.
Note that we bundle all successive terminals in one
symbol.
3.3 Shift-Reduce Parsing
We use a shift-reduce algorithm to search for the
best deviation. The algorithm maintains a stack of
dotted rules (Earley, 1970). Given the source tree in
Figure 1(a), the stack is initialized with a dotted rule
for the root node IP:
[ IP].
Then, the algorithm selects one viable prefix of IP
and appends it to the stack with the dot at the begin-
ning (predict):
[ IP] [ the result]2.
Then, a scan action is performed to produce a partial
translation ?the result?:
[ IP] [the result ].
Next, the algorithm searches for the CFG rules start-
ing with ?the result? and gets r2. Then, it pops the
rightmost dotted rule and append the left-hand side
(LHS) of r2 to the stack (complete):
[ IP] [NN2 ].
Next, the algorithm chooses r4 whose right-hand
side ?NN2 of the vote? matches the rightmost dot-
ted rule in the stack3 and grows the rightmost dotted
rule:
[ IP] [NN2  of the vote].
Figure 3 shows the whole process of derivation
generation.
Formally, we define four actions on the rightmost
rule in the stack:
? Predict. If the symbol after the dot in the right-
most dotted rule is a non-terminal v, this action
chooses a viable prefix w of v and generates a
new dotted rule for w with the dot at the begin-
ning. For example:
[ IP] predict?? [ IP] [ the result]
? Scan. If the symbol after the dot in the right-
most dotted rule is a terminal string w, this ac-
tion advances the dot to update the current par-
tial translation. For example:
[ IP] [ the result] scan?? [ IP] [the result ]
? Complete. If the rightmost dotted rule ends
with a dot and it happens to be the right-hand
side of a rule, then this action removes the
right-most dotted rule. Besides, if the symbol
after the dot in the new rightmost rule corre-
sponds to the same tree node as the LHS non-
terminal of the rule, this action advance the dot.
For example,
[ IP] [NP  VP] [was released at night ]
complete?? [ IP] [NP VP ]
2There are another option: ?was released at night?
3Here there is an alternative: r3 or r7
1194
step action rule used stack hypothesis
0 [ IP]
1 p r6 [ IP] [ NP VP]
2 p r4 [ IP] [ NP VP] [ NN2 of the vote]
3 p r2 [ IP] [ NP VP] [ NN2 of the vote] [ the result]
4 s [ IP] [ NP VP] [ NN2 of the vote] [the result ] the result
5 c [ IP] [ NP VP] [NN2  of the vote] the result
6 s [ IP] [ NP VP] [NN2 of the vote ] the result of the vote
7 c [ IP] [NP  VP] the result of the vote
8 p r5 [ IP] [NP  VP] [ was released at night] the result of the vote
9 s [ IP] [NP  VP] [was released at night ] the ... vote was ... night
10 c [ IP] [NP VP ] the ... vote was ... night
11 c [IP ] the ... vote was ... night
Figure 2: Simulation of top-down translation process for the derivation in Figure 1(b). Actions: p, predict; s, scan; c,
complete. ?the ... vote? and ?was ... released? are the abbreviated form of ?the result of the vote? and ?was released at
night?, respectively.
step action rule used stack number hypothesis
0 [ IP] 0
1 p [ IP] [ the result] 0
2 s [ IP] [the result ] 1 the result
3 c r2 [ IP] [NN2 ] 1 the result
4 g r4 or r7 [ IP] [NN2  of the vote] 1 the result
5 s [ IP] [NN2 of the vote ] 2 the result of the vote
6 c r4 [ IP] [NP ] 2 the result of the vote
7 g r6 [ IP] [NP  VP] 2 the result of the vote
8 p [ IP] [NP  VP] [ was released at night] 2 the result of the vote
9 s [ IP] [NP  VP] [was released at night ] 4 the ... vote was ... night
10 c r5 [ IP] [NP VP ] 4 the ... vote was ... night
11 c r6 [IP ] 4 the ... vote was ... night
Figure 3: Simulation of bottom-up translation process for the derivation in Figure 1(b). Actions: p, predict; s, scan; c,
complete; g, grow. The column of number gives the number of source words the hypothesis covers.
If the string cannot rewrite on the frontier non-
terminal, then we add the LHS to the stack with
the dot after it. For example:
[ IP] [the result ] complete?? [ IP] [NN2 ]
? Grow. If the right-most dotted rule ends with
a dot and it happens to be the starting part of
a CFG rule, this action appends one symbol of
the remainder of that rule to the stack 4. For
example:
4We bundle the successive terminals in one rule into a sym-
bol
[ IP] [NN2 ]
grow?? [ IP] [NN2  of the vote]
From the above definition, we can find that there
may be an ambiguity about whether to use a com-
plete action or a grow action. Similarly, predict ac-
tions must select a viable prefix form the set for a
node. For example in step 5, although we select
to perform complete with r4 in the example, r7 is
applicable, too. In our implementation, if both r4
and r7 are applicable, we apply them both to gener-
ate two seperate hypotheses. To limit the exponen-
tial explosion of hypotheses (Knight, 1999), we use
beam search over bins of similar partial hypotheses
(Koehn, 2004).
1195
IP
NP
NN2 of NN1
of the vote
VP
was released at night
r7
r4 r5
r6
r3
Figure 4: The translation forest composed of applicable
CFG rules for the partial derivation of step 3 in Figure 3.
3.4 Future Cost
Partial derivations covering different tree nodes may
be grouped in the same bin for beam pruning5. In
order to performmore accurate pruning, we take into
consideration future cost, the cost of the uncovered
part. The merit of a derivation is the covered cost
(the cost of the covered part) plus the future cost.
We borrow ideas from the Inside-Outside algorithm
(Charniak and Johnson, 2005; Huang, 2008; Mi et
al., 2008) to compute the merit. In our algorithm,
the merit of a derivation is just the Viterbi inside cost
? of the root node calculated with the derivations
continuing from the current derivation.
Given a partial derivation, we calculate its future
cost by searching through the translation forest de-
fined by all applicable CFG rules. Figure 4 shows
the translation forest for the derivation of step 3. We
calculate the future cost for each node as follows:
given a node v, we define its cost function f(v) as
f(v) =
?
?
?
?
?
1 v is completed
lm(v) v is a terminal string
maxr?Rv f(r)
?
pi?rhs(r) f(pi) otherwise
where VN is the non-terminal set, VT is the terminal
set, v, pi ? VN ? VT+, Rv is the set of currently ap-
plicable rules for v, rhs(r) is the right-hand symbol
set of r, lm is the local language model probability,
f(r) is calculated using a linear model whose fea-
tures are bidirectional translation probabilities and
lexical probabilities of r. For the translation forest
in Figure 4, if we calculate the future cost of NP with
5Section 3.7 will describe the binning scheme
r4, then
f(NP ) = f(r4) ? f(NN2) ? lm(of the vote)
= f(r4) ? 1 ? lm(of the vote)
Note that we calculate lm(of the vote) locally and do
not take ?the result? derived from NN2 as the con-
text. The lm probability of ?the result? has been in-
cluded in the covered cost.
As a partial derivation grows, some CFG rules
will conflict with the derivation (i.e. inapplicable)
and the translation forest will change accordingly.
For example, when we reach step 5 from step 3 (see
Figure 4 for its translation forest), r3 is inapplica-
ble and thereby should be ruled out. Then the nodes
on the path from the last covered node (it is ?of the
vote? in step 5) to the root node should update their
future cost, as they may employ r3 to produce the
future cost. In step 5, NP and IP should be updated.
In this sense, we say that the future cost is dynamic.
3.5 Comparison with Top-Down Decoding
In order to generate the translation ?the result? based
on the derivation in Figure 1(b), Huang and Mi?s
top-down algorithm needs to specify which rules to
apply starting from the root node until it yields ?the
result?. In this derivation, rule r6 is applied to IP, r4
to NP, r2 to NN2. That is to say, it needs to repre-
sent the partial derivation from IP to NN2 explicitly.
This can be a problem when combined with beam
pruning. If the beam size is small, it may discard the
intermediate hypotheses and thus never consider the
string. In our example with a beam of 1, we must
select a rule for IP among r6, r7 and r8 although we
do not get any information for NP and VP.
Instead, our bottom-up algorithm allows top-
down and bottom-up information to be used together
with the help of viable prefixes. This allows us to
encode more candidate derivations than the purely
top-down method. In the above example, our al-
gorithm does not specify the derivation for the gap
from IP and ?the result?. In fact, all derivations
composed of currently applicable rules are allowed.
When needed, our algorithm derives the derivation
dynamically using applicable rules. So when our
algorithm performs pruning at the root node, it has
got much more information and consequently intro-
duces fewer pruning errors.
1196
3.6 Time Complexity
Assume the depth of the source tree is d, the max-
imum number of matched rules for each node is c,
the maximum arity of each rule is r, the language
model order is g and the target-language vocabulary
is V, then the time complexity of our algorithm is
O((cr)d|V |g?1). Analysis is as follows:
Our algorithm expands partial paths with termi-
nal strings to generate new hypotheses, so the time
complexity depends on the number of partial paths
used. We split a path which is from the root node to a
leaf node with a node on it (called the end node) and
get the segment from the root node to the end node
as a partial path, so the length of the partial path is
not definite with a maximum of d. If the length is
d?(d? ? d), then the number of partial paths is (cr)d? .
Besides, we use the rightest g ? 1 words to signa-
ture each partial path, so we can get (cr)d? |V |g?1
states. For each state, the number of viable prefixes
produced by predict operation is cd?d? , so the total
time complexity is f = O((cr)d? |V |g?1cd?d?) =
O(cdrd? |V |g?1) = O((cr)d|V |g?1).
3.7 Beam Search
Tomake decoding tractable, we employ beam search
(Koehn, 2004) and choose ?binning? as follows: hy-
potheses covering the same number of source words
are grouped in a bin. When expanding a hypothe-
sis in a beam (bin), we take series of actions until
new terminals are appended to the hypothesis, then
add the new hypothesis to the corresponding beam.
Figure 3 shows the number of source words each hy-
pothesis covers.
Among the actions, only the scan action changes
the number of source words each hypothesis cov-
ers. Although the complete action does not change
source word number, it changes the covered cost of
hypotheses. So in our implementation, we take scan
and complete as ?closure? actions. That is to say,
once there are some complete actions after a scan ac-
tion, we finish all the compete actions until the next
action is grow. The predict and grow actions decide
which rules can be used to expand hypotheses next,
so we update the applicable rule set during these two
actions.
Given a source sentence with n words, we main-
tain n beams, and let each beam hold b hypotheses
at most. Besides, we prune viable prefixes of each
node up to u, so each hypothesis can expand to u
new hypotheses at most, so the time complexity of
beam search is O(nub).
4 Related Work
Watanabe et al(2006) present a novel Earley-
style top-down decoding algorithm for hierarchical
phrase-based model (Chiang, 2005). Their frame-
work extracts Greibach Normal Form rules only,
which always has at least one terminal on the left
of each rule, and discards other rules.
Dyer and Resnik (2010) describe a translation
model that combines the merits of syntax-based
models and phrase-based models. Their decoder
works in two passes: for first pass, the decoder col-
lects a context-free forest and performs tree-based
source reordering without a LM. For the second
pass, the decoder adds a LM and performs bottom-
up CKY decoding.
Feng et al(2010) proposed a shift-reduce algo-
rithm to add BTG constraints to phrase-based mod-
els. This algorithm constructs a BTG tree in a
reduce-eager manner while the algorithm in this pa-
per searches for a best derivation which must be de-
rived from the source tree.
Galley and Manning (2008) use the shift-reduce
algorithm to conduct hierarchical phrase reordering
so as to capture long-distance reordering. This al-
gorithm shows good performance on phrase-based
models, but can not be applied to syntax-based mod-
els directly.
5 Experiments
In the experiments, we use two baseline systems:
our in-house tree-to-string decoder implemented ac-
cording to Liu et al(2006) (denoted as traditional)
and the Earley-style top-down decoder implemented
according to Huang and Mi (2010) (denoted as top-
down), respectively. We compare our bottom-up
left-to-right decoder (denoted as bottom-up) with
the baseline in terms of performance, translation
quality and decoding speed with different beam
sizes, and search capacity. Lastly, we show the in-
fluence of future cost. All systems are implemented
in C++.
1197
5.1 Data Setup
We used the FBIS corpus consisting of about 250K
Chinese-English sentence pairs as the training set.
We aligned the sentence pairs using the GIZA++
toolkit (Och and Ney, 2003) and extracted tree-to-
string rules according to the GHKM algorithm (Gal-
ley et al 2004). We used the SRILM toolkit (Stol-
cke, 2002) to train a 4-gram language model on the
Xinhua portion of the GIGAWORD corpus.
We used the 2002 NIST MT Chinese-English test
set (571 sentences) as the development set and the
2005 NIST MT Chinese-English test set (1082 sen-
tences) as the test set. We evaluated translation qual-
ity using BLEU-metric (Papineni et al 2002) with
case-insensitive n-gram matching up to n = 4. We
used the standard minimum error rate training (Och,
2003) to tune feature weights to maximize BLEU
score on the development set.
5.2 Performance Comparison
Our bottom-up left-to-right decoder employs the
same features as the traditional decoder: rule proba-
bility, lexical probability, language model probabil-
ity, rule count and word count. In order to compare
them fairly, we used the same beam size which is 20
and employed cube pruning technique (Huang and
Chiang, 2005).
We show the results in Table 3. From the re-
sults, we can see that the bottom-up decoder out-
performs top-down decoder and traditional decoder
by 1.1 and 0.8 BLEU points respectively and the
improvements are statistically significant using the
sign-test of Collins et al(2005) (p < 0.01). The
improvement may result from dynamically search-
ing for a whole derivation which leads to more ac-
curate estimation of a partial derivation. The addi-
tional time consumption of the bottom-up decoder
against the top-down decoder comes from dynamic
future cost computation.
Next we compare decoding speed versus transla-
tion quality using various beam sizes. The results
are shown in Figure 5. We can see that our bottom-
up decoder can produce better BLEU score at the
same decoding speed. At small beams (decoding
time around 0.5 second), the improvement of trans-
lation quality is much bigger.
System BLEU(%) Time (s)
Traditional 29.8 0.84
Top-down 29.5 0.41
Bottom-up 30.6 0.81
Table 3: Performance comparison.
29.4
29.6
29.8
30.0
30.2
30.4
30.6
30.8
 0.2  0.4  0.6  0.8  1  1.2  1.4  1.6  1.8
BL
EU
 S
co
re
Avg Decoding Time (secs per sentence)
bottom-up
top-down
traditional
Figure 5: BLEU score against decoding time with various
beam size.
5.3 Search Capacity Comparison
We also compare the search capacity of the bottom-
up decoder and the traditional decoder. We do this
in the following way: we let both decoders use the
same weights tuned on the traditional decoder, then
we compare their translation scores of the same test
sentence.
From the results in Table 4, we can see that for
many test sentences, the bottom-up decoder finds
target translations with higher score, which have
been ruled out by the traditional decoder. This may
result from more accurate pruning method. Yet for
some sentences, the traditional decoder can attain
higher translation score. The reason may be that the
traditional decoder can hold more than two nonter-
minals when cube pruning, while the bottom-up de-
coder always performs dual-arity pruning.
Next, we check whether higher translation scores
bring higher BLEU scores. We compute the BLEU
score of both decoders on the test sentence set on
which bottom-up decoder gets higher translation
scores than the traditional decoder does. We record
the results in Figure 6. The result shows that higher
score indeed bring higher BLEU score, but the im-
provement of BLEU score is not large. This is be-
cause the features we use don?t reflect the real statis-
1198
28.0
29.0
30.0
31.0
32.0
33.0
34.0
35.0
 10  20  30  40
BL
EU
 S
co
re
Beam Size
bottom-up
traditional
Figure 6: BLEU score with various beam sizes on the sub
test set consisting of sentences on which the bottom-up
decoder gets higher translation score than the traditional
decoder does.
b > = <
10 728 67% 347 32% 7 1%
20 657 61% 412 38% 13 1%
30 615 57% 446 41% 21 2%
40 526 49% 523 48% 33 3%
50 315 29% 705 65% 62 6%
Table 4: Search capacity comparison. The first column is
beam size, the following three columns denote the num-
ber of test sentences, on which the translation scores of
the bottom-up decoder are greater, equal to, lower than
that of the traditional decoder.
System BLEU(%) Time (s)
with 30.6 0.81
without 28.8 0.39
Table 5: Influence of future cost. The results of the
bottom-up decoder with and without future cost are given
in the second and three rows, respectively.
tical distribution of hypotheses well. In addition, the
weights are tuned on the traditional decoder, not on
the bottom-up decoder. The bottom-up decoder can
perform better with weights tuned by itself.
5.4 Influence of Future Cost
Next, we will show the impact of future cost via ex-
periments. We give the results of the bottom-up de-
coder with and without future cost in Table 5. From
the result, we can conclude that future cost plays a
significant role in decoding. If the bottom-up de-
coder does not employ future cost, its performance
will be influenced dramatically. Furthermore, cal-
culating dynamic future cost is time consuming. If
the bottom-up decoder does not use future cost, it
decodes faster than the top-down decoder. This is
because the top-down decoder has |T | beams, while
the bottom-up decoder has n beams, where T is the
source parse tree and n is the length of the source
sentence.
6 Conclusions
In this paper, we describe a bottom-up left-to-right
decoding algorithm for tree-to-string model. With
the help of viable prefixes, the algorithm generates
a translation by constructing a target-side CFG tree
according to a post-order traversal. In addition, it
takes into consideration a dynamic future cost to es-
timate hypotheses.
On the 2005 NIST Chinese-English MT transla-
tion test set, our decoder outperforms the top-down
decoder and the traditional decoder by 1.1 and 0.8
BLEU points respectively and shows more powerful
search ability. Experiments also prove that future
cost is important for more accurate pruning.
7 Acknowledgements
We would like to thank Haitao Mi and Douwe
Gelling for their feedback, and anonymous review-
ers for their valuable comments and suggestions.
This work was supported in part by EPSRC grant
EP/I034750/1 and in part by High Technology R&D
Program Project No. 2011AA01A207.
References
A. V. Aho and S. C. Johnson. 1974. Lr parsing. Com-
puting Surveys, 6:99?124.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proc. of ACL, pages 173?180.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proc. of ACL,
pages 263?270.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33:201?228.
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause restructuring for statistical machine
translation. In Proc. of ACL, pages 531?540.
1199
Chris Dyer and Philip Resnik. 2010. Context-free re-
ordering, finite-state translation. In Proc. of NAACL,
pages 858?866, June.
Jay Earley. 1970. An efficient context-free parsing algo-
rithm. Communications of the ACM, 13:94?102.
Yang Feng, Haitao Mi, Yang Liu, and Qun Liu. 2010. An
efficient shift-reduce decoding algorithm for phrased-
based machine translation. In Proc. of Coling, pages
285?293.
Michel Galley and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. In Proc. of EMNLP, pages 848?856.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In Proc of
NAACL, pages 273?280.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proc. of IWPT, pages 53?64.
Liang Huang and Haitao Mi. 2010. Efficient incremen-
tal decoding for tree-to-string translation. In Proc. of
EMNLP, pages 273?283.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proceedings of AMTA.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proc. of ACL,
pages 586?594.
Kevin Knight. 1999. Decoding complexity in word-
replacement translation models. Computational Lin-
guistics, 25:607?615.
Philipp Koehn. 2004. Pharaoh: A beam search decoder
for phrased-based statistical machine translation. In
Proc. of AMTA, pages 115?124.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine trans-
lation. In Proceedings of COLING-ACL, pages 609?
616, July.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proc. of ACL, pages 192?199.
Frans J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29:19?51.
Frans J. Och. 2003. Minimum error rate training in sta-
tistical machine translation. In Proc. of ACL, pages
160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalu-
ation of machine translation. In Proceedings of ACL,
pages 311?318.
Andreas Stolcke. 2002. Srilm-an extensible language
modeling toolkit. In Proc. of ICSLP.
Taro Watanabe, Hajime Tsukada, and Hideki Isozaki.
2006. Left-to-right target generation for hierarchical
phrase-based translation. In Proc. of COLING, pages
777?784.
1200
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 490?500,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Document Summarization via Guided Sentence Compression
Chen Li1, Fei Liu2, Fuliang Weng2, Yang Liu1
1 Computer Science Department, The University of Texas at Dallas
Richardson, Texas 75080, USA
2 Research and Technology Center, Robert Bosch LLC
Palo Alto, California 94304, USA
{chenli,yangl@hlt.utdallas.edu}
{fei.liu, fuliang.weng@us.bosch.com}
Abstract
Joint compression and summarization has
been used recently to generate high quality
summaries. However, such word-based joint
optimization is computationally expensive. In
this paper we adopt the ?sentence compression
+ sentence selection? pipeline approach for
compressive summarization, but propose to
perform summary guided compression, rather
than generic sentence-based compression. To
create an annotated corpus, the human anno-
tators were asked to compress sentences while
explicitly given the important summary words
in the sentences. Using this corpus, we train
a supervised sentence compression model us-
ing a set of word-, syntax-, and document-
level features. During summarization, we use
multiple compressed sentences in the inte-
ger linear programming framework to select
salient summary sentences. Our results on the
TAC 2008 and 2011 summarization data sets
show that by incorporating the guided sen-
tence compression model, our summarization
system can yield significant performance gain
as compared to the state-of-the-art.
1 Introduction
Automatic summarization can be broadly divided
into two categories: extractive and abstractive sum-
marization. Extractive summarization focuses on
selecting the salient sentences from the document
collection and concatenating them to form a sum-
mary; while abstractive summarization is generally
considered more difficult, involving sophisticated
techniques for meaning representation, content plan-
ning, surface realization, etc., and the ?true abstrac-
tive summarization remains a researcher?s dream?
(Radev et al, 2002).
There has been a surge of interest in recent
years on generating compressed document sum-
maries as a viable step towards abstractive sum-
marization. These compressive summaries often
contain more information than sentence-based ex-
tractive summaries since they can remove insignif-
icant sentence constituents and make space for more
salient information that is otherwise dropped due to
the summary length constraint. Two general strate-
gies have been used for compressive summarization.
One is a pipeline approach, where sentence-based
extractive summarization is followed or proceeded
by sentence compression (Knight and Marcu, 2000;
Lin, 2003; Zajic et al, 2007; Wang et al, 2013).
Another line of work uses joint compression and
summarization. They have been shown to achieve
promising performance (Daume?, 2006; Martins and
Smith, 2009; Berg-Kirkpatrick et al, 2011; Chali
and Hasan, 2012; Almeida and Martins, 2013; Qian
and Liu, 2013). One popular approach for such joint
compression and summarization is via integer lin-
ear programming (ILP). However, since words are
the units in the optimization framework, solving this
ILP problem can be expensive.
In this study, we use the pipeline compression
and summarization method because of its compu-
tational efficiency. Prior work using such pipeline
methods simply uses generic sentence-based com-
pression for each sentence in the documents, no mat-
ter whether compression is done before or after sum-
mary sentence extraction. We propose to use sum-
490
mary guided compression combined with ILP-based
sentence selection for summarization in this paper.
We create a compression corpus for this purpose.
Using human summaries for a set of documents, we
identify salient words in the sentences. During anno-
tation, the human annotators are given these salient
words and asked to generate compressed sentences.
We expect such ?guided? sentence compression is
beneficial for the pipeline compression and summa-
rization task. In addition, previous research on joint
modeling for compression and summarization sug-
gested that the labeled extraction and compression
data sets would be helpful for learning a better joint
model (Daume?, 2006; Martins and Smith, 2009).
We hope that our work on this guided compression
will also be of benefit to the future joint modeling
studies.
Using our created compression data, we train
a supervised compression model using a variety
of word-, sentence-, and document-level features.
During summarization, we generate multiple com-
pression candidates for each sentence, and use the
ILP framework to select compressed summary sen-
tences. In addition, we also propose to apply a pre-
selection step to select some important sentences,
which can both speed up the summarization system
and improve performance. We evaluate our pro-
posed summarization approach on the TAC 2008
and 2011 data sets using the standard ROUGE met-
ric (Lin, 2004). Our results show that by incorporat-
ing a guided sentence compression model, our sum-
marization system can yield significant performance
gain as compared to the state-of-the-art reported re-
sults.
2 Related Work
Summarization research has seen great development
over the last fifty years (Nenkova and McKeown,
2011). Compared to the abstractive counterpart, ex-
tractive summarization has received considerable at-
tention due to its clear problem formulation ? to ex-
tract a set of salient and non-redundant sentences
from the given document set. Both unsupervised and
supervised approaches have been explored for sen-
tence selection. The supervised approaches include
the Bayesian classifier (Kupiec et al, 1995), max-
imum entropy (Osborne, 2002), skip-chain condi-
tional random fields (CRF) (Galley, 2006), discrim-
inative reranking (Aker et al, 2010), among others.
The extractive summary sentence selection prob-
lem can also be formulated in an optimization
framework. Previous approaches include the inte-
ger linear programming (ILP) and submodular func-
tions, which are used to solve the optimization prob-
lem. In particular, Gillick et al (2009) proposed
a concept-based ILP approach for summarization.
Li et al (2013) improved it by using supervised
stragety to estimate concept weight in ILP frame-
work. In (Lin and Bilmes, 2010), the authors model
the sentence selection problem as maximizing a sub-
modular function under a budget constraint. A
greedy algorithm is proposed to efficiently approxi-
mate the solution to this NP-hard problem.
Compressive summarization receives increasing
attention in recent years, since it offers a viable
step towards abstractive summarization. The com-
pressed summaries can be generated through a joint
model of the sentence selection and compression
processes, or through a pipeline approach that in-
tegrates a generic sentence compression model with
a summary sentence pre-selection or post-selection
step.
Many studies explore the joint sentence compres-
sion and selection setting. Martins and Smith (2009)
jointly perform sentence extraction and compression
by solving an ILP problem; Berg-Kirkpatrick et al
(2011) propose an approach to score the candidate
summaries according to a combined linear model
of extractive sentence selection and compression.
They train the model using a margin-based objec-
tive whose loss captures the final summary qual-
ity. Woodsend and Lapata (2012) present a method
where the summary?s informativeness, succinctness,
and grammaticality are learned separately from data
but optimized jointly using an ILP setup; Yoshikawa
et al (2012) incorporate semantic role information
in the ILP model; Chali and Hasan (2012) investi-
gate three strategies in compressive summarization:
compression before extraction, after extraction, or
joint compression and extraction in one global op-
timization framework. These joint models offer a
promise for high quality summaries, but they often
have high computational cost. Qian and Liu (2013)
propose a graph-cut based method that improves the
speed of joint compression and summarization.
491
The pipeline approach, where sentence-based ex-
tractive summarization is followed or proceeded by
sentence compression, is also popular. Knight and
Marcu (2000) utilize the noisy channel and deci-
sion tree method to perform sentence compression;
Lin (2003) shows that pure syntactic-based com-
pression may not improve the system performance;
Zajic et al (2007) compare two sentence compres-
sion approaches for multi-document summarization,
including a ?parse-and-trim? and a noisy-channel ap-
proach; Galanis and Androutsopoulos (2010) use
the maximum entropy model to generate the candi-
date compressions by removing the branches from
the source sentences; Liu and Liu (2013) couple
the sentence compression and extraction approaches
for summarizing the spoken documents; Wang et al
(2013) design a series of learning-based compres-
sion models built on parse trees, and integrate them
in query-focused multi-document summarization.
Prior studies often rely heavily on the generic sen-
tence compression approaches (McDonald, 2006;
Nomoto, 2007; Clarke and Lapata, 2008; Thadani
and McKeown, 2013) for compressing the sentences
in the documents, yet a generic compression system
may not be the best fit for the summarization pur-
pose.
In this paper, we adopt the pipeline-based com-
pressive summarization framework, but propose a
novel guided compression method that is catered to
the summarization task. We expect this approach
to take advantage of the efficient pipeline process-
ing while producing satisfying results as the joint
models. We train a supervised guided compression
model to produce n-best compressions for each sen-
tence, and use an ILP formulation to select the best
set of summary sentences. In addition, we pro-
pose to apply a sentence pre-selection step to fur-
ther accelerate the processing and enhance the per-
formance.
3 Guided Compression Corpus
The goal of guided sentence compression is to create
compressed sentences that are grammatically cor-
rect and contain the important information that we
would like to preserve in the final summary. Fol-
lowing the compression literature (Clarke and Lap-
ata, 2008), the compression task is defined as a word
Original Sentence:
The gas leak was contained Monday afternoon , nearly 18
hours after it was reported , Statoil spokesman Oeivind
Reinertsen said .
Compression A:
The gas leak was contained
Compression B:
The gas leak was contained Monday afternoon
Compression C:
The gas leak was contained nearly 18 hours after it was
reported
Table 1: Example sentence and three compressions.
deletion problem, that is, the human annotators (and
also automatic compression systems) are allowed to
only remove words from the original sentence to
form a compression. The key difference between
our proposed guided compression with generic sen-
tence compression is that, we provide guidance to
the human compression process by specifying a set
of ?important words? that we wish to keep for each
sentence. We expect this kind of summary oriented
compression would benefit the ultimate summariza-
tion task. Take the sentence shown in Table 1 as an
example. For generic sentence compression, there
may be multiple ?good? human compressions for this
sentence, such as those listed in the table. Without
guidance, a human annotator (or automatic system)
is likely to use option A or B; however, if ?18 hours?
appears in the summary, then we want to provide this
guidance in the compression process, hence option
C may be the best compression choice. This guided
compression therefore avoids removing the salient
words that are important to the final summary.
To generate the guided compression corpus, we
use the TAC 2010 data set1 that was used for
the multi-document summarization task. There are
46 topics. Each has 10 news documents, and
also four human-created abstractive reference sum-
maries. Since annotating all the sentences in this
data set is time consuming and some sentences are
not very important for the summarization task, we
choose a set of sentences that are highly related to
the human abstracts for annotation. We compare
each sentence with the four human abstracts using
the ROUGE-2 metric (Lin, 2004), and the sentences
1http://www.nist.gov/tac/2010/
492
Original Sentence:
He said Vietnam veterans are presumed to have been ex-
posed to Agent Orange and veterans with any of the 10 dis-
eases is presumed to have contracted it from the exposure ,
without individual proof .
Guided Compression:
Vietnam veterans are presumed to have been exposed to
Agent Orange.
Original Sentence:
The province has limited the number of trees to be chopped
down in the forest area in northwest Yunnan and has stopped
building sugar factories in the Xishuangbanna region to
preserve the only tropical rain forest in the country located
there .
Guided Compression:
province has stopped building sugar factories in the
Xishuangbanna region to preserve tropical rain forest.
Table 2: Example original sentences and their guided
compressions. The ?guiding words? are italicized and
marked in red.
with the highest scores are selected.
In annotation, human annotators are provided
with important ?guiding words? (highlighted in the
annotation interface) that we want to preserve in the
sentences. We calculate the word overlap between a
sentence and each of those sentences in the human
abstracts, and use a set of heuristic rules to deter-
mine the ?guiding words? in a sentence: the longest
consecutive word overlaps (greater than 2 words) in
each sentence pair are first selected; the rest overlaps
that contain 2 or more words (excluding the stop-
words) are also selected. We suggest the human an-
notators to use their best judgment to keep the guid-
ing words as many as possible while compressing
the sentence.
We use the Amazon Mechanical Turk (AMT) for
data annotation2. In total, we select 1,150 sentences
from the TAC news documents. They are grouped
into about 230 human intelligence tasks (HITs) with
5 sentences in each HIT. A sentence was compressed
by 3 human annotatorsand we select the shortest
candidate as the goldstandard compression for each
sentence. In Table 2, we show two example sen-
tences, their guiding words (bold), and the human
compressions. The first example shows that giving
up some guiding words is acceptable, since more
2http://www.mturk.com
unnecessary words will be included in order to ac-
commodate all the guiding words; the second ex-
ample shows that the guided compression can lead
to more aggressive word deletions since the con-
stituents that are not important to the summary will
be deleted even though they contain salient informa-
tion by themselves.
For our compression corpus, which contains
1,150 sentences and their guided compressions, the
average compression rate, as measured by the per-
centage of dropped words, is about 50%. This com-
pression ratio is higher compared to other generic
sentence compression corpora, in which the word
deletion rate ranges from 24% to 34% depending
on different text genres and annotation guidelines
(Clarke and Lapata, 2008; Liu and Liu, 2009). This
suggests that the annotators can remove words more
aggressively when they are provided with a limited
set of guiding words.
4 Summarization System
Our summarization system consists of three key
components: we train a supervised guided compres-
sion model using our created compression data, with
a variety of features.then we use this model to gener-
ate n-best compressions for each sentence; we feed
the multiple compressed sentences to the ILP frame-
work to select the best summary sentences. In ad-
dition, we propose a sentence pre-selection step that
can both speed up the summarization system and im-
prove the performance.
4.1 Guided Sentence Compression
Sentence compression has been explored in previous
studies using both supervised and unsupervised ap-
proaches, including the noisy-channel and decision
tree model (Knight and Marcu, 2000; Turner and
Charniak, 2005), discriminative learning (McDon-
ald, 2006), integer linear programming (Clarke and
Lapata, 2008; Thadani and McKeown, 2013), con-
ditional random fields (CRF) (Nomoto, 2007; Liu
and Liu, 2013), etc. In this paper, we employ the
CRF-based compression approach due to its proved
performance and its flexibility to integrate differ-
ent levels of discriminative features. Under this
framework, sentence compression is formulated as
a sequence labeling problem, where each word is
493
labeled as either ?0? (retained) or ?1? (removed).
We develop different levels of features to capture
word-specific characteristics, sentence related infor-
mation, and document level importance. Most of the
features are extracted based only on the sentence to
be compressed. However, we introduce a few doc-
ument level features. These are designed to cap-
ture the word and sentence significance within the
given document collection and are thus expected to
be more summary related.
Word and sentence features:
? Word n-grams: identity of the current word
and two words before and after, as well as all
the bigrams and trigrams that can be formed by
the adjacent words and the current word.
? POS n-grams: same as the word n-grams, but
use the part-of-speech tags instead.
? Named entity tags: binary features represent-
ing whether the current word is a person, loca-
tion, or temporal expression. We use the Stan-
ford CoreNLP tools3 for named entity tagging.
? Stopwords: whether the current word is a stop-
word or not.
? Conjunction features: (1) conjunction of the
current word with its relative position in the
sentence; (2) conjunction of the NER tag with
its relative position.
? Syntactic features: We obtain the syntactic
parsing tree using the Berkeley Parser (Petrov
and Klein, 2007), then obtain the following fea-
tures: (1) the last sentence constituent tag in
the path from the root to the word; (2) depth:
length of the path starting from the root node
to the word; (3) normalized depth: depth di-
vided by the longest path in the parsing tree;
(4) whether the word is under an SBAR node;
(5) depth and normalized depth of the SBAR
node if the word is under an SBAR node;
? Dependency features: We employ the
Penn2Malt toolkit 4 to convert the parse re-
sult from the Berkeley parser to the depen-
dency parsing tree, and use these dependency
3http://nlp.stanford.edu/software/corenlp.shtml
4http://stp.lingfil.uu.se/?nivre/research/Penn2Malt.html
features: (1) dependency relations such as
?AMOD? (adjective modifier), ?NMOD? (noun
modifier), etc. (2) whether the word has a child,
left child, or right child in the dependency tree.
Document-level features:
? Sentence salience score: We use a simple re-
gression model to estimate a salience score for
each sentence (more details in Section 4.3),
which represents the importance of the sen-
tence in the document. This score is discretized
into four binary features according to the aver-
age sentence salience.
? Unigram document frequency: this is the
current word?s document frequency based on
the 10 documents associated with each topic.
? Bigram document frequency: document fre-
quency for the two bigrams, the current word
and its previous or next word.
Some of the above features were employed in re-
lated sentence compression studies (Nomoto, 2007;
Liu and Liu, 2013). In addition to these features, we
explored other related features, including the abso-
lute position of the current word, whether the word
appears in the corresponding topic title and descrip-
tions, conjunction of the syntactic tag with the tree
depth, etc.; however, these features did not lead to
improved performance. We train the CRF model
with the Pocket CRF toolkit5 using the guided com-
pression corpus collected in Section 3. During sum-
marization, we apply the model to a given sentence
to generate its n-best guided compressions and use
them in the following summarization step.
4.2 Summary Sentence Selection
The sentence selection process is similar to the stan-
dard sentence-based extractive summarization, ex-
cept that the input to the selection module is a list
of compressed sentences in our work. Many extrac-
tive summarization approaches can be applied for
this purpose. In this work, we choose the integer
linear programming (ILP) method, specifically, the
concept-based ILP framework introduced in (Gillick
5http://sourceforge.net/projects/pocket-crf-1/
494
et al, 2009), mainly because it yields best perfor-
mance in the TAC evaluation tasks. This ILP ap-
proach aims to extract sentences that can cover as
many important concepts as possible, while ensuring
the summary length is within a given constraint. We
follow the study in (Gillick et al, 2009) to use word
bi-grams as concepts, and assign a weight to each
bi-gram using its document frequency in the given
document collection for a test topic. Two differences
are between our ILP setup and that in (Gillick et al,
2009). First, since we use multiple compressions
for one sentence, we need to introduce an additional
constraint: for each sentence, only one of the n-best
compressions may be included in the summary. Sec-
ond, we optimize a joint score of the concept cover-
age and the sentence salience. The formal ILP for-
mulation is shown below:
max
?
i
wici +
?
j
vj
?
k
sjk (1)
s.t.
?
k
sjk ? 1?j (2)
sjkOcci jk ? ci (3)
?
jk
sjkOcci jk ? ci (4)
?
jk
ljksjk ? L (5)
ci ? {0, 1} ?i (6)
sjk ? {0, 1} ?j, k (7)
where ci and sjk are binary variables indicating the
presence of a concept and a sentence respectively;
sjk denotes the kth candidate compression of the
jth sentence; wi represents the weight of the con-
cept; vj is the sentence salience score of the jth
sentence, predicted using a regression model (Sec-
tion 4.3), and all of its compressed candidates share
this value. (1) is the new objective function we use
that combines the coverage of the concepts and the
sentence salience scores. (2) represents our addi-
tional constraint, which requires that for each sen-
tence j, only one candidate compression will be cho-
sen. Occi jk represents the occurrence of concept i
in the sentence sjk. Inequalities (3) and (4) associate
the sentences and the concepts. Constraint (5) con-
trols the summary length, as measured by the total
number of words in the summary. We use an open
source ILP solver6.
4.3 Sentence Pre-selection
The above ILP method can offer an exact solution
to the defined objective function. However, ILP is
computationally expensive when the formulation in-
volves large quantities of variables, i.e, when we
have many sentences and a large number of candi-
date compressions for each sentence. We therefore
propose to apply a sentence pre-selection step be-
fore the compression. This kind of selection step
has been used in previous ILP-based summarization
systems (Berg-Kirkpatrick et al, 2011; Gillick et al,
2009). In this work, we propose to use a simple su-
pervised support vector regression (SVR) model (Ng
et al, 2012) to predict a salience score for each sen-
tence and select the top ranked sentences for further
processing (compression and summarization).
To train the SVR model, the target value for each
sentence is the ROUGE-2 score between the sen-
tence and the four human abstracts (this same value
is used for sentence selection in corpus annotation
(Section 3)). We employ three commonly used fea-
tures: (1) sentence position in the document; (2) sen-
tence length as indicated by a binary feature: it takes
the value of 0 if the number of words in the sentence
is greater than 50 or less than 10, otherwise the fea-
ture value is 1; (3) interpolated n-gram document
frequency as introduced in (Ng et al, 2012), which
is a weighted linear combination of the document
frequency of the unigrams and bigrams contained in
the sentence:
f(s) =
?
?
wu?S
DF (wu) + (1? ?)
?
wb?S
DF (wb)
|S|
where wu and wb represent the unigrams and bi-
grams contained in the sentence S; ? is a balancing
factor; |S| denotes the number of words in the sen-
tence.
The SVR model was trained using the SVMlight
toolkit7. Using this model, we can predict a salience
score (Vj in Eq 1) for each sentence and only select
the top n sentences and supply them to the compres-
sion and summarization steps. In practice, using a
fixed n may not be a good choice since the number
6http://www.gnu.org/software/glpk/
7http://svmlight.joachims.org/
495
of sentences varies greatly for different topics. We
therefore set n heuristically based on the total num-
ber of sentencesm for each topic: n=15 ifm > 150;
n=10 if m < 100; n=0.1 ?m otherwise.
5 Experimental Results
5.1 Experimental Setup
For our experiments, we use the standard TAC data
sets8, which have been used in the NIST competi-
tions and in other summarization studies. In par-
ticular, we used the TAC 2010 data set for creating
the guided compression corpus and training the SVR
pre-selection model, the TAC 2009 data set as devel-
opment set for parameter tuning, and the TAC 2008
and 2011 data sets as the test set for reporting the
final summarization results.
We compare our pipeline summarization sys-
tem against three recent studies, which have re-
ported some of the highest published results on this
task. Berg-Kirkpatrick et al (2011) introduce a
joint model for sentence extraction and compres-
sion. The model is trained using a margin-based ob-
jective whose loss captures the end summary qual-
ity; Woodsend and Lapata (2012) learn individ-
ual summary aspects from data, e.g., informative-
ness, succinctness, grammaticality, stylistic writ-
ing conventions, and jointly optimize the outcome
in an integer linear programming framework. Ng
et al (2012) exploit category-specific information
for multi-document summarization. In addition to
the three previous studies, we also report the best
achieved results in the TAC competitions.
5.2 Summarization Results
In Table 3 and Table 4, we present the results of our
system and the aforementioned summarization stud-
ies. We use the ROUGE evaluation metrics (Lin,
2004), with R-2 measuring the bigram overlap be-
tween the system and reference summaries and R-
SU4 measuring the skip-bigram with the maximum
gap length of 4. ?Our System? uses the pipeline
setting including the three components described in
Section 4. We use the SVR-based approach to pre-
select a set of sentences from the document set; these
sentences are further fed to the guided compression
module that produces n-best compressions for each
8http://www.nist.gov/tac/data/index.html
System R-2 R-SU4 CompR
TAC?08 Best System 11.03 13.96 n/a
(Berg-Kirkpatrick et al, 2011) 11.70 14.38 n/a
(Woodsend et al, 2012) 11.37 14.47 n/a
Our System 12.35? 15.27? 43.06%
Our System w/o Pre-selection 12.02 14.98 55.69%
Our System w/ Generic Comp 10.88 13.79 30.90%
Table 3: Results on the TAC 2008 data set. ?Our Sys-
tem? uses the SVR-based sentence pre-selection + guided
compression + ILP-based summary sentence selection.
?Our System w/ Generic Comp? uses the pre-selection +
generic compression + ILP summary sentence selection
setting. ?CompR? represents the compression ratio, i.e.,
percentage of dropped words. ? represents our system
outperforms the best previous result at the 95% signifi-
cance level.
System R-2 R-SU4 CompR
TAC?11 Best System 13.44 16.51 n/a
(Ng et al, 2012) 13.93 16.83 n/a
Our System 14.40 16.89 39.90%
Our System w/o Pre-selection 13.74 16.5 53.81%
Our System w/ Generic Comp 13.08 16.23 30.10%
Table 4: Results on the TAC 2011 data set. The systems
use the same settings as for the TAC 2008 data set.
sentence; the ILP-based framework is then used to
select the summary sentences from these compres-
sions.
We can see from the table that in general, our sys-
tem achieves considerably better results compared to
the state-of-the-art on both the TAC 2008 and 2011
data sets. On the TAC 2008 data set, our system out-
performs the best reported result at the 95% signifi-
cance level; on the TAC 2011 data set, our system
also yields considerable performance gain though
not exceed the 95% significance level. In the fol-
lowing, we show more detailed analysis to study the
effect of different system parameters.
With or without sentence pre-selection. First
we evaluate the impact of sentence pre-selection
step. In Table 3 and Table 4, we include the
results when this step is not used (?Our System
w/o Pre-selection?). That is, all of the sentences
in the documents (excluding those containing less
than 5 words) are compressed and used in the ILP-
496
based summary sentence selection module. We can
see that although sentence pre-selection removes
some sentences from consideration in the later sum-
marization step, it actually significantly improves
system performance. In the TAC 2008 data set,
each topic contains averagely 210 sentences; while
the pre-selection step chooses 13 sentences among
them. These numbers are 185 and 12 for the TAC
2011 data set. Table 5 shows the average running
time of each topic in TAC 2011 data for the two sys-
tems, with or without the pre-selection step. Here
we fix the number of compressions to 100 in both
cases for fair comparison. We can see the selec-
tion step greatly accelerates the system processing.
When applying the pre-selection step, fewer sen-
tences are used in the compression and summariza-
tion, this means we are able to use more compres-
sion candidates for each sentence (considering the
complexity of ILP module). Using the TAC 2009
as development set, we tuned the number of can-
didate compressions generated for each sentence.
Without pre-selection, we used the 100-best candi-
dates generated from the compression model; with
pre-selection, we are able to increase the number
to 200-best candidate compressions and still main-
tain reasonable computational cost. These are the
numbers used in the results in Table 3 and 4. Us-
ing more compressions helps improve summariza-
tion performance. We also notice that the compres-
sion ratios are quite different when using sentence
pre-selection vs. not. This suggests that in the im-
portant sentences (those are kept after pre-selection),
there is more summary related information and thus
the compression model keeps more words in them
(lower compression ratio).
System
Compressed Number of Running
Sentences Compressions Time (sec)
w/o Pre-selection 185 100 3.9
w/ Pre-selection 12 100 0.85
Table 5: Average running time of our system, w/ or w/o
the sentence pre-selection step. Experiments conducted
on the TAC 2011 data set. Running time refers only to
the execution time of the ILP module for each topic.
Number of compression candidates. This pa-
rameter (denoted as n) also impacts system perfor-
mance. Figure 1 shows the R-2 scores of the two
systems (with and without the sentence pre-selection
step) when using different number of compressions
for each sentence. In general, we find that the R-2
scores do not change much when n is large enough.
For example, the ?with pre-selection? system can
achieve relatively stable R-2 scores on the TAC 2008
data set (ranging from 12.2 to 12.4) when m is
greater than 140; similarly, the R-2 scores on the
TAC 2011 data is over 14.2 when m is greater than
100. Without the pre-selection step, the scores are
less stable in regard to the changing of the m value,
since the large amount of sentences plus a high vol-
ume of the compression candidates may incur huge
computational cost to the ILP solver. This is also the
reason that in Figure 1, for the system without pre-
selection, we only vary n from 1 to 100. In general,
we also notice that given more compression candi-
dates, the R-2 score is still improving, as indicated
by Figure 1. The improved performance of ?with
pre-selection? over ?without pre-selection? is partly
because fewer sentences are used and thus we are
able to increase the number of compression candi-
dates for these sentences in the ILP sentence extrac-
tion module.
Quality of sentence compression training data.
In order to illustrate the contribution of our
summary-guided sentence compression component,
we train a generic sentence compression model
and use this in our compression and summariza-
tion pipeline. The generic compression model was
trained using the Edinburgh sentence compression
corpus (Clarke and Lapata, 2008), which contains
1370 sentences collected from news articles. This
data set has been widely used in other summariza-
tion studies (Martins and Smith, 2009). Each sen-
tence has 3 compressions and we choose the short-
est compression as the reference. The average com-
pression rate of this corpus is about 28%, lower than
that in our summary guided compression data. Note
that in generic sentence compression, we only use
those word and sentence features described in Sec-
tion 4.1, not the document-level features since they
are not available for the Edinburgh data set. Results
of our system using the generic compression model
(with sentence pre-selection) are shown in the last
row of Table 3 and Table 4. We can see that the sys-
tem with this generic compression model performs
497
 11
 11.5
 12
 12.5
 13
 13.5
 14
 14.5
 15
 0  20  40  60  80  100
RO
UG
E-
2
# Compression Candidates
TAC 2011
TAC 2008
 11
 11.5
 12
 12.5
 13
 13.5
 14
 14.5
 15
 0  50  100  150  200  250
RO
UG
E-
2
# Compression Candidates
TAC 2011
TAC 2008
 11
 11.5
 12
 12.5
 13
 13.5
 14
 14.5
 15
 0  20  40  60  80  100
RO
UG
E-
2
# Compression Candidates
TAC 2011
TAC 2008
 11
 11.5
 12
 12.5
 13
 13.5
 14
 14.5
 15
 0  50  100  150  200  250
RO
UG
E-
2
# Compression Candidates
TAC 2011
TAC 2008
Figure 1: R-2 scores of the two systems (without and
with the sentence pre-selection step) when using differ-
ent number of compressions for each sentence.
worse than ours, and is also inferior to the TAC best
performing system on both data sets, which signi-
fies the importance of our proposed summary guided
sentence compression approach. We can also see
there is a difference in the compression ratio in the
system generated compressions when using differ-
ent compression corpora to train the compression
models. The resulting compression ratio patterns are
consistent with those in the training data, that is, us-
ing our guided compression corpus our system com-
pressed sentences more aggressively.
Learning curve of guided compression. Since
we use a supervised compression model, we further
consider the relationship between the summarization
performance and the number of sentence pairs used
for training the guided compression model. In to-
tal, there are 1150 training sentence pairs in our cor-
pus. We incrementally add 100 sentence pairs each
time and plot the learning curve in Figure 2. In
the compression step, we generate only the 1-best
compression candidate in order to remove the im-
pact caused by the downstream summary sentence
selection module. As seen from Figure 2, increasing
the compression training data generally improves
summarization performance, although there are also
fluctuations. When adding more training sentence
pairs, the system performance is likely to further in-
crease.
 10.5
 11
 11.5
 12
 12.5
 200  400  600  800 1000 1200
RO
UG
E-
2
# Sentence Pairs in the Training Set
TAC 2011
TAC 2008
Figure 2: ROUGE-2 scores when using different number
of sentences to train the guided compression model.
6 Conclusion and Future Work
In this paper, we propose a pipeline summariza-
tion approach that combines a novel guided com-
pression model with ILP-based summary sentence
selection. We create a guided compression cor-
pus, where the human annotators were explicitly in-
formed about the important summary words during
the compression annotation. We then train a super-
vised compression model to capture the guided com-
pression process using a set of word-, sentence-, and
document-level features. We conduct experiments
on the TAC 2008 and 2011 summarization data sets
and show that by incorporating the guided sentence
compression model, our summarization system can
yield significant performance gain as compared to
the state-of-the-art. In future, we would like to
further explore the reinforcement relationship be-
tween keywords and summaries (Wan et al, 2007),
improve the readability of the sentences generated
from the guided compression system, and report re-
sults using multiple evaluation metrics (Nenkova et
al., 2007; Louis and Nenkova, 2012) as well as per-
forming human evaluations.
498
Acknowledgments
Part of this work was done during the first au-
thor?s internship in Bosch Research and Technol-
ogy Center. The work is also partially supported
by NSF award IIS-0845484 and DARPA Contract
No. FA8750-13-2-0041. Any opinions, findings,
and conclusions or recommendations expressed are
those of the author and do not necessarily reflect the
views of the funding agencies.
References
Ahmet Aker, Trevor Cohn, and Robert Gaizauskas. 2010.
Multi-document summarization using a* search and
discriminative training. In Proceedings of EMNLP.
Miguel B. Almeida and Andre F. T. Martins. 2013. Fast
and robust compressive summarization with dual de-
composition and multi-task learning. In Proceedings
of ACL.
Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein.
2011. Jointly learning to extract and compress. In
Proceedings of ACL.
Yllias Chali and Sadid A. Hasan. 2012. On the effective-
ness of using sentence compression models for query-
focused multi-document summarization. In Proceed-
ings of COLING.
James Clarke and Mirella Lapata. 2008. Global infer-
ence for sentence compression an integer linear pro-
gramming approach. Journal of Artificial Intelligence
Research.
Hal Daume?. 2006. Practical structured learning tech-
niques for natural language processing. Ph.D. thesis,
University of Southern California.
Dimitrios Galanis and Ion Androutsopoulos. 2010. An
extractive supervised two-stage method for sentence
compression. In Human Language Technologies: The
2010 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics.
Michel Galley. 2006. A skip-chain conditional random
field for ranking meeting utterances by importance. In
Proceedings of EMNLP.
Dan Gillick, Benoit Favre, Dilek Hakkani-Tur, Berndt
Bohnet, Yang Liu, and Shasha Xie. 2009. The icsi/utd
summarization system at tac 2009. In Proceedings of
TAC.
Kevin Knight and Daniel Marcu. 2000. Statistics-based
summarization - step one: Sentence compression.
Julian Kupiec, Jan Pedersen, and Francine Chen. 1995.
A trainable document summarizer. In Proceedings of
SIGIR.
Chen Li, Xian Qian, and Yang Liu. 2013. Using super-
vised bigram-based ilp for extractive summarization.
In Proceedings of ACL.
Hui Lin and Jeff Bilmes. 2010. Multi-document sum-
marization via budgeted maximization of submodular
functions. In Proceedings of NAACL.
Chin-Yew Lin. 2003. Improving summarization perfor-
mance by sentence compression - A pilot study. In
Proceeding of the Sixth International Workshop on In-
formation Retrieval with Asian Language.
Chin-Yew Lin. 2004. Rouge: a package for automatic
evaluation of summaries. In Proceedings of ACL.
Fei Liu and Yang Liu. 2009. From extractive to abstrac-
tive meeting summaries: Can it be done by sentence
compression? In Proceedings of ACL-IJCNLP.
Fei Liu and Yang Liu. 2013. Towards abstractive speech
summarization: Exploring unsupervised and super-
vised approaches for spoken utterance compression.
IEEE Transactions on Audio, Speech, and Language
Processing.
Annie Louis and Ani Nenkova. 2012. Automati-
cally assessing machine summary content with a gold-
standard. Computational Linguistics.
Andre F. T. Martins and Noah A. Smith. 2009. Summa-
rization with a joint model for sentence extraction and
compression. In Proceedings of the ACL Workshop
on Integer Linear Programming for Natural Language
Processing.
Ryan McDonald. 2006. Discriminative sentence com-
pression with soft syntactic evidence. In Proceedings
of EACL.
Ani Nenkova and Kathleen McKeown. 2011. Automatic
summarization. Foundations and Trends in Informa-
tion Retrieval.
Ani Nenkova, Rebecca Passonneau, and Kathleen McK-
eown. 2007. The pyramid method: Incorporating hu-
man content selection variation in summarization eval-
uation. ACM Transactions on Speech and Language
Processing.
Jun-Ping Ng, Praveen Bysani, Ziheng Lin, Min-Yen Kan,
and Chew-Lim Tan. 2012. Exploiting category-
specific information for multi-document summariza-
tion. In Proceedings of COLING.
Tadashi Nomoto. 2007. Discriminative sentence com-
pression with conditional random fields. Information
Processing and Management.
Miles Osborne. 2002. Using maximum entropy for sen-
tence extraction. In Proceedings of the ACL-02 Work-
shop on Automatic Summarization.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of HLT-
NAACL.
499
Xian Qian and Yang Liu. 2013. Fast joint compression
and summarization via graph cuts. In Proceedings of
EMNLP.
Dragomir R. Radev, Eduard Hovy, and Kathleen McKe-
own. 2002. Introduction to the special issue on sum-
marization. In Computational Linguistics.
Kapil Thadani and Kathleen McKeown. 2013. Sentence
compression with joint structural inference. In Pro-
ceedings of CoNLL.
Jenine Turner and Eugene Charniak. 2005. Supervised
and unsupervised learning for sentence compression.
In Proceedings of ACL.
Xiaojun Wan, Jianwu Yang, and Jianguo Xiao. 2007. To-
wards an iterative reinforcement approach for simulta-
neous document summarization and keyword extrac-
tion. In Proceedings of ACL.
Lu Wang, Hema Raghavan, Vittorio Castelli, Radu Flo-
rian, and Claire Cardie. 2013. A sentence com-
pression based framework to query-focused multi-
document summarization. In Proceedings of ACL.
Kristian Woodsend and Mirella Lapata. 2012. Multiple
aspect summarization using integer linear program-
ming. In Proceedings of EMNLP-CoNLL.
Katsumasa Yoshikawa, Tsutomu Hirao, Ryu Iida, and
Manabu Okumura. 2012. Sentence compression with
semantic role constraints. In Proceedings of ACL.
David Zajic, Bonnie J. Dorr, Jimmy Lin, and Richard
Schwartz. 2007. Multi-candidate reduction: Sentence
compression as a tool for document summarization
tasks. In Information Processing and Management.
500
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 567?577,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Recursive Autoencoders for ITG-based Translation
Peng Li, Yang Liu and Maosong Sun
State Key Laboratory of Intelligent Technology and Systems
Tsinghua National Laboratory for Information Science and Technology
Department of Computer Science and Technology, Tsinghua University, Beijing 100084, China
pengli09@gmail.com, {liuyang2011,sms}@tsinghua.edu.cn
Abstract
While inversion transduction grammar (ITG)
is well suited for modeling ordering shifts
between languages, how to make applying
the two reordering rules (i.e., straight and
inverted) dependent on actual blocks being
merged remains a challenge. Unlike previous
work that only uses boundary words, we pro-
pose to use recursive autoencoders to make
full use of the entire merging blocks alter-
natively. The recursive autoencoders are ca-
pable of generating vector space representa-
tions for variable-sized phrases, which enable
predicting orders to exploit syntactic and se-
mantic information from a neural language
modeling?s perspective. Experiments on the
NIST 2008 dataset show that our system sig-
nificantly improves over the MaxEnt classifier
by 1.07 BLEU points.
1 Introduction
Phrase-based models (Koehn et al, 2003; Och and
Ney, 2004) have been widely used in practical ma-
chine translation (MT) systems due to their effec-
tiveness, simplicity, and applicability. First, as se-
quences of consecutive words, phrases are capable
of memorizing local word selection and reorder-
ing, making them an effective mechanism for trans-
lating idioms or translations with word insertions
or omissions. Moreover, n-gram language models
can be seamlessly integrated into phrase-based de-
coders since partial translations grow left to right
in decoding. Finally, phrase-based systems can be
applicable to most domains and languages, espe-
cially for resource-scarce languages without high-
accuracy parsers.
However, as phrase-based decoding casts transla-
tion as a string concatenation problem and permits
arbitrary permutations, it proves to be NP-complete
(Knight, 1999). Therefore, phrase reordering mod-
eling has attracted intensive attention in the past
decade (e.g., Och et al, 2004; Tillman, 2004; Zens
et al, 2004; Al-Onaizan and Papineni, 2006; Xiong
et al, 2006; Koehn et al, 2007; Galley and Man-
ning, 2008; Feng et al, 2010; Green et al, 2010;
Bisazza and Federico, 2012; Cherry, 2013).
Among them, reordering models based on inver-
sion transduction grammar (ITG) (Wu, 1997) are
one of the important ongoing research directions.
As a formalism for bilingual modeling of sentence
pairs, ITG is particularly well suited to predicting
ordering shifts between languages. As a result, a
number of authors have incorporated ITG into left-
to-right decoding to constrain the reordering space
and reported significant improvements (e.g., Zens et
al., 2004; Feng et al, 2010). Along another line,
Xiong et al (2006) propose a maximum entropy
(MaxEnt) reordering model based on ITG. They use
the CKY algorithm to recursively merge two blocks
(i.e., a pair of source and target strings) into larger
blocks, either in a straight or an inverted order. Un-
like lexicalized reordering models (Tillman, 2004;
Koehn et al, 2007; Galley and Manning, 2008) that
are defined on individual bilingual phrases, the Max-
Ent ITG reordering model is a two-category classi-
fier (i.e., straight or inverted) for two arbitrary bilin-
gual phrases of which the source phrases are adja-
cent. This potentially alleviates the data sparseness
567
problem since there are usually a large number of
reordering training examples available (Xiong et al,
2006). As a result, the MaxEnt ITG model and its
extensions (Xiong et al, 2008; Xiong et al, 2010)
have achieved competing performance as compared
with state-of-the-art phrase-based systems.
Despite these successful efforts, the ITG reorder-
ing classifiers still face a major challenge: how to
extract features from training examples (i.e., a pair
of bilingual strings). It is hard to decide which words
are representative for predicting reordering, either
manually or automatically, especially for long sen-
tences. As a result, Xiong et al (2006) only use
boundary words (i.e., the first and the last words in
a string) to predict the ordering. What if we look
inside? Is it possible to avoid manual feature engi-
neering and learn semantic representations from the
data?
Fortunately, the rapid development of intersect-
ing deep learning with natural language processing
(Bengio et al, 2003; Collobert and Weston, 2008;
Collobert et al, 2011; Glorot et al, 2011; Bordes et
al., 2011; Socher et al, 2011a; Socher et al, 2011b;
Socher et al, 2011c; Socher et al, 2012; Bordes et
al., 2012; Huang et al, 2012; Socher et al, 2013;
Hermann and Blunsom, 2013) brings hope for alle-
viating this problem. In these efforts, natural lan-
guage words are represented as real-valued vectors,
which can be naturally fed to neural networks as in-
put. More importantly, it is possible to learn vec-
tor space representations for multi-word phrases us-
ing recursive autoencoders (Socher et al, 2011c),
which opens the door to leveraging semantic repre-
sentations of phrases in reordering models from a
neural language modeling point of view.
In this work, we propose an ITG reordering clas-
sifier based on recursive autoencoders. The neu-
ral network consists of four autoencoders (i.e., the
first source phrase, the first target phrase, the sec-
ond source phrase, and the second target phrase)
and a softmax layer. The recursive autoencoders,
which are trained on reordering examples extracted
from word-aligned bilingual corpus, are capable
of producing vector space representations for arbi-
trary multi-word strings in decoding. Therefore,
our model takes the whole phrases rather than only
boundary words into consideration when predict-
ing phrase permutations. Experiments on the NIST
2008 dataset show that our system significantly im-
proves over the MaxEnt classifier by 1.07 in terms
of case-insensitive BLEU score.
2 Recursive Autoencoders for ITG-based
Translation
2.1 Inversion Transduction Grammar
Inversion transduction grammar (ITG) (Wu, 1997)
is a formalism for synchronous parsing of bilingual
sentence pairs. Xiong et al (2006) apply bracketing
transduction grammar (BTG), which is a simplified
version of ITG, to phrase-based translation using the
following production rules:
X ? [X1, X2] (1)
X ? ?X1, X2? (2)
X ? f/e (3)
where X is a block that consists of a pair of source
and target strings, f is a source phrase, and e is a tar-
get phrase. X1 and X2 are two neighboring blocks
of which the two source phrases are adjacent. While
rule (1) merges two target phrases in a straight or-
der, rule (2) merges in an inverted order. Besides
these two reordering rules, rule (3) is a lexical rule
that translates a source phrase f into a target phrase
e. This is exactly a bilingual phrase used in conven-
tional phrase-based systems.
An ITG derivation, which consists of a sequence
of production rules, explains how a sentence pair is
generated simultaneously. Figure 1 shows an ITG
derivation for a Chinese sentence and its English
translation. We distinguish between two types of
blocks:
1. atomic blocks: blocks generated by applying
lexical rules,
2. composed blocks: blocks generated by apply-
ing reordering rules.
In Figure 1, the sentence pair is segmented into
five atomic blocks:
X0,3,0,3 : wo you yi ge? I have a
X3,5,5,6 : cong mei you? never
X5,8,6,8 : jian guo de? seen before
X8,10,3,5 : nv xing peng you? female friend
X10,11,8,9 : .? .
568
(1) X0,11,0,9 ? [X0,10,0,8, X10,11,8,9]
(2) X0,10,0,8 ? [X0,3,0,3, X3,10,3,8]
(3) X0,3,0,3 ? wo you yi ge / I have a
(4) X3,10,3,8 ? ?X3,8,5,8, X8,10,3,5?
(5) X3,8,5,8 ? [X3,5,5,6, X5,8,6,8]
(6) X3,5,5,6 ? cong mei you / never
(7) X5,8,6,8 ? juan guo de / seen before
(8) X8,10,3,5 ? nv xing peng you/ female friend
(9) X10,11,8,9 ? . / .
Figure 1: An ITG derivation for a Chinese sentence and its translation. We useXi,j,k,l = ?f
j
i , e
l
k? to represent a block.
Our neural ITG reordering model first assigns vector space representations to single words and then produces vectors
for phrases using recursive autoencoders, which form atomic blocks. The atomic blocks are recursively merged into
composed blocks, the vector space representations of which are produced by recursive autoencoders simultaneously.
The neural classifier makes decisions at each node using the vectors of all its descendants.
569
where X3,5,5,6 indicates that the block consists of a
source phrase spanning from position 3 to position 5
(i.e., ?cong mei you?) and a target phrase spanning
from position 5 to position 6 (i.e., ?never?). More
formally, a block Xi,j,k,l = ?f
j
i , e
l
k? is a pair of a
source phrase f ji = fi+1 . . . fj and a target phrase
elk = ek+1 . . . el. Obviously, these atomic blocks
are generated by lexical rules.
Two blocks of which the source phrases are adja-
cent can be merged into a larger one in two ways:
concatenating the target phrases in a straight order
using rule (1) or in an inverted order using rule (2).
For example, atomic blocks X3,5,5,6 and X5,8,6,8 are
merged into a composed block X3,8,5,8 in a straight
order, which is further merged with an atomic block
X8,10,3,5 into another composed block X3,10,3,8 in
an inverted order. This process recursively proceeds
until the entire sentence pair is generated.
The major challenge of applying ITG to machine
translation is to decide when to merge two blocks
in a straight order and when in an inverted order.
Therefore, the ITG reordering model can be seen as
a two-category classifier P (o|X1, X2), where o ?
{straight, inverted}.
A naive way is to assign fixed probabilities to two
reordering rules, which is referred to as flat model
by Xiong et al (2006):
P (o|X1, X2) =
{
p o = straight
1? p o = inverted
(4)
The drawback of the flat model is ignoring the
actual blocks being merged. Intuitively, different
blocks should have different preferences between
the two orders.
To alleviate this problem, Xiong et al (2006) pro-
pose a maximum entropy (MaxEnt) classifier:
P (o|X1, X2) =
exp(? ? h(o,X1, X2))
?
o? exp(? ? h(o
?, X1, X2))
(5)
where h(?) is a vector of features defined on the
blocks and the order, ? is a vector of feature weights.
While MaxEnt is a flexible and powerful frame-
work for including arbitrary features, feature engi-
neering becomes a major challenge for the MaxEnt
classifier. Xiong et al (2006) find that boundary
words (i.e., the first and the last words in a string)
are informative for predicting reordering. Actually,
Figure 2: A recursive autoencoder for multi-word strings.
The example is adapted from (Socher et al, 2011c). Blue
and grey nodes are original and reconstructed ones, re-
spectively.
it is hard to decide which internal words in a long
composed blocks are representative and informa-
tive. Therefore, they only use boundary words as
the main features.
However, it seems not enough to just consider
boundary words and ignore all internal words when
making order predictions, especially for long sen-
tences.1 Indeed, Xiong et al (2008) find that the
MaxEnt classifier with boundary words as features
is prone to make wrong predictions for long com-
posed blocks. As a result, they have to impose a hard
constraint to always prefer merging long composed
blocks in a monotonic way.
Therefore, it is important to consider more than
boundary words to make more accurate reordering
predictions. We need a new mechanism to achieve
this goal.
2.2 Recursive Autoencoders
2.2.1 Vector Space Representations for Words
In neural networks, a natural language word is
represented as a real-valued vector (Bengio et al,
2003; Collobert and Weston, 2008). For example,
we can use [0.1 0.8 0.4]T to represent ?female? and
1Strictly speaking, the ITG reordering model is not a phrase
reordering model since phrase pairs are only the atomic blocks.
Instead, it is defined to work on arbitrarily long strings because
composed blocks become larger and larger until the entire sen-
tence pair is generated.
570
Figure 3: A neural ITG reordering model. The binary classifier makes decisions based on the vector space representa-
tions of the source and target sides of merging blocks.
[0.7 0.1 0.5]T to represent ?friend?. Such vector
space representations enable natural language words
to be fed to neural networks as input.
Formally, we denote each word as a vector x ?
Rn. These word vectors are then stacked into a word
embedding matrix L ? Rn?|V |, where |V | is the vo-
cabulary size. Given a sentence that is an ordered list
ofmwords, each word has an associated vocabulary
index k into the word embedding matrix L that we
use to retrieve the word?s vector space representa-
tion. This look-up operation can be seen as a simple
projection layer:
xi = Lbk ? Rn (6)
where bk is a binary vector which is zero in all posi-
tions except for the kth index.
In Figure 1, we assume n = 3 for simplicity and
can retrieve vectors for Chinese and English words
from two embedding matrices, respectively.
2.2.2 Vector Space Representations for
Multi-Word Strings
To apply neural networks to ITG-based transla-
tion, it is important to generate vector space repre-
sentations for atomic and composed blocks.
For example, since the vector of ?female? is
[0.1 0.8 0.4]T and the vector of ?friend? is
[0.7 0.1 0.5]T , what is the vector of the phrase ?fe-
male friend?? If we denote ?female friend? as p
(i.e., parent), ?female? as c1 (i.e., the first child),
and ?friend? as c2 (i.e., the second child), this can
be done by applying a function f (1):
p = f (1)(W (1)[c1; c2] + b
(1)) (7)
where [c1; c2] ? R2n?1 is the concatenation of c1
and c2, W (1) ? Rn?2n is a parameter matrix, b(1) ?
Rn?1 is a bias term, and f (1) is an element-wise ac-
tivation function such as tanh(?), which is used in
our experiments.
Note that the resulting vector for the parent is also
an n-dimensional vector, e.g, [0.6 0.9 0.2]T . The
same neural network can be recursively applied to
two strings until the vector of the entire sentence is
generated. As ITG derivation builds a binary parse
tree, the neural network can be naturally integrated
into CKY parsing.
To assess how well the learned vector p represents
its children, we can reconstruct the children in a
reconstruction layer:
[c?1; c
?
2] = f
(2)(W (2)p+ b(2)) (8)
where c?1 and c
?
2 are the reconstructed children,W
(2)
is a parameter matrix for reconstruction, b(2) is a bias
term for reconstruction, and f (2) is an element-wise
activation function, which is also set as tanh(?) in
our experiments. Similarly, the same reconstruction
neural network can be applied to each node in an
ITG parse.
These neural networks are called recursive au-
toencoders (Socher et al, 2011c). Figure 2 illus-
trates an application of a recursive autoencoder to a
571
binary tree. The blue and grey nodes are the original
and reconstructed nodes, respectively. The autoen-
coder is re-used at each node of the tree. The bi-
nary tree is composed of a set of triplets in the form
of (p ? c1 c2), where p is a parent vector and c1
and c2 are children vectors of p. Each child can be
either an input word vector or a multi-word vector.
Therefore, the tree in Figure 2 can be represented as
three triplets: (y1 ? x1 x2), (y2 ? y1 x3), and
(y3 ? y2 x4).
In Figure 1, we use recursive autoencoders to gen-
erate vector space representations for Chinese and
English phrases, which form the atomic blocks for
further block merging.
2.2.3 A Neural ITG Reordering Model
Once the vectors for blocks are generated, it is
straightforward to introduce a neural ITG reorder-
ing model. As shown in Figure 3, the neural net-
work consists of an input layer and a softmax layer.
The input layer is composed of the vectors of the
first source phrase, the first target phrase, the second
source phrase, and the second target phrase. Note
that all phrases in the same language use the the
same recursive autoencoder. The softmax layer out-
puts the probabilities of the two merging orders:
P (o|X1, X2) =
exp(g(o,X1, X2))
?
o? exp(g(o
?, X1, X2))
(9)
g(o,X1, X2) = f(W oc(X1, X2) + bo) (10)
where o ? {straight, inverted}, W o ? R1?4n
is a parameter matrix, bo ? R is a bias term, and
c(X1, X2) ? R4n?1 is the concatenation of the vec-
tors of the four phrases.
3 Training
There are three sets of parameters in our recursive
autoencoders:
1. ?L: word embedding matrix L for both source
and target languages (Section 2.2.1);
2. ?rec: recursive autoencoder parameter matrices
W (1), W (2) and bias terms b(1), b(2) for both
source and target languages (Section 2.2.2);
3. ?reo: neural ITG reordering model parameter
matrix W o and bias term bo (Section 2.2.3).
All these parameters are learned automatically from
the training data. For clarity, we will use ? to denote
all these parameters in the rest of the paper.
For training word embedding matrix, there are
two settings commonly used. In the first setting,
the word embedding matrix is initialized randomly.
This works well in a supervised scenario, in which
a neural network updates the matrix in order to op-
timize some task-specific objectives (Collobert et
al., 2011; Socher et al, 2011c). In the second set-
ting, the word embedding matrix is pre-trained us-
ing an unsupervised neural language model (Bengio
et al, 2003; Collobert and Weston, 2008) with huge
amount of unlabeled data. In this work, we prefer to
the first setting because the word embedding matri-
ces can be trained to minimize errors with respect to
reordering modeling.
There are two kinds of errors involved
1. reconstruction error: how well the learned
vector space representations represent the cor-
responding strings?
2. reordering error: how well the classifier pre-
dicts the merging order?
As described in Section 2.2.2, the input vector
c1 and c2 of a recursive autoencoder can be recon-
structed using Eq. 8 as c?1 and c
?
2. We use Euclidean
distance between the input and the reconstructed
vectors to measure the reconstruction error:
Erec([c1; c2]; ?) =
1
2
?
?[c1; c2]? [c
?
1; c
?
2]
?
?2 . (11)
Given a sentence, there are exponentially many
ways to obtain its vector space representation. Note
that each way corresponds to a binary tree like Fig-
ure 2. To find a binary tree with minimal reconstruc-
tion error, we follow Socher et al (2011c) to use a
greedy algorithm. Taking Figure 2 as an example,
the greedy algorithm begins with computing the re-
construction error Erec(?) for each pair of consecu-
tive vectors, i.e., Erec([x1;x2]; ?), Erec([x2;x3]; ?)
and Erec([x3;x4]; ?). Suppose Erec([x1;x2]; ?) is
the smallest, the algorithm will replace x1 and x2
with their vector representation y1 produced by the
recursive autoencoder. Then, the algorithm evalu-
ates Erec([y1;x3]; ?) and Erec([x3;x4]; ?) and re-
peats the above replacing steps until only one vector
572
remains. Socher et al (2011c) find that the greedy
algorithm runs fast without significant loss in perfor-
mance as compared with CKY-style algorithms.
Given a training example set S = {ti =
(oi, X1i , X
2
i )}, the average reconstruction error on
the source side on the training set is defined as
Erec,s(S; ?) =
1
Ns
?
i
?
p?T ?R(ti,s)
Erec([p.c1, p.c2]; ?)
(12)
where T ?R(ti, s) denotes all the intermediate nodes
on the source side in binary trees, Ns is the num-
ber of these intermediate nodes, and p.ck is the kth
child vector of p. The average reconstruction error
on the target side, denoted by Erec,t(S; ?), can be
computed in a similar way.
Therefore, the reconstruction error is defined as
Erec(S; ?) = Erec,s(S; ?) + Erec,t(S; ?). (13)
Given a training example ti = (oi, X1i , X
2
i ), we
assume the probability distribution dti for its label
is [1, 0] when oi = straight, and [0, 1] when oi =
inverted. Then the cross-entropy error is
Ec(ti; ?) = ?
?
o
dti(o) log
(
P?(o|X
1, X2)
)
(14)
where o ? {straight, inverted}. As a result, the
reordering error is defined as
Ereo(S; ?) =
1
|S|
?
i
Ec(ti; ?). (15)
Therefore, the joint training objective function is
J = ?Erec(S; ?)+(1??)Ereo(S; ?)+R(?) (16)
where ? is a parameter used to balance the prefer-
ence between reconstruction error and reordering er-
ror, R(?) is the regularizer and defined as 2
R(?) =
?L
2
??L?
2 +
?rec
2
??rec?
2 +
?reo
2
??reo?
2 .
(17)
As Socher et al (2011c) stated, a naive way for
lowering the reconstruction error is to make the
magnitude of the hidden layer very small, which is
2The bias terms b(1), b(2) and bo are not regularized. We do
not exclude them from the equation explicitly just for clarity.
not desirable. In order to prevent such behavior, we
normalize all the output vectors of the hidden layers
to have length 1 in the same way as (Socher et al,
2011c). Namely we set p = p||p|| after computing p
as in Eq. 7, and c?1 =
c?1
||c?1||
, c?2 =
c?2
||c?2||
in Eq. 8.
Following Socher et al (2011c), we use L-BFGS
to estimate the parameters with respect to the joint
training objective. Given a set of parameters, we
construct binary trees for all the phrases using the
greedy algorithm. The derivatives for these fixed
binary trees can be computed via backpropagation
through structures (Goller and Kuchler, 1996).
4 Experiments
4.1 Data Preparation
We evaluated our system on Chinese-English trans-
lation. The training corpus contains 1.23M sen-
tence pairs with 32.1M Chinese words and 35.4M
English words. We used SRILM (Stolcke, 2002)
to train a 4-gram language model on the Xinhua
portion of the GIGAWORD corpus, which con-
tains 398.6M words. We used the NIST 2006 MT
Chinese-English dataset as the development set and
NIST 2008 dataset as the test set. The evaluation
metric is case-insensitive BLEU. Because of the ex-
pensive computational cost for training our neural
ITG reordering model, only the reordering exam-
ples extracted from about 1/5 of the entire parallel
training corpus were used to train our neural ITG re-
ordering model.
For the neural ITG reordering model, we set the
dimension of the word embedding vectors to 25 em-
pirically, which is a trade-off between computational
cost and expressive power. We use the early stop-
ping principle to determine when to stop L-BFGS.
The hyper-parameters ?, ?L, ?rec and ?reo are op-
timized by random search (Bergstra and Bengio,
2012). As preliminary experiments show that classi-
fication accuracy has a high correlation with BLEU
score, we optimize these hyper-parameters with re-
spect to classification accuracy instead of BLEU
to reduce computational cost. We randomly select
400,000 reordering examples as training set, 500 as
development set, and another 500 as test set. The
numbers of straight and inverted reordering exam-
ples in the development/test set are set to be equal
to avoid biases. We draw ? uniformly from 0.05
573
System NIST 2006 (tune) NIST 2008
maxent 30.40 23.75
neural 31.61* 24.82*
Table 1: BLEU scores on the NIST 2006 and 2008
datasets. *: significantly better (p < 0.01). ?maxent?
denotes the baseline maximum entropy system and ?neu-
ral? denotes our recursive autoencoder system.
length > = <
[1, 10] 43 121 57
[11, 20] 181 67 164
[21, 30] 170 11 152
[31, 40] 105 3 90
[41, 50] 69 1 53
[51, 119] 40 0 30
Table 2: Number of sentences that our system has a
higher (>), equal (=) or lower (<) sentence-level BLEU-
4 score on the NIST 2008 dataset.
to 0.3, and ?L, ?rec, ?reo exponentially from 10?8
to 10?2. We use the following hyper-parameters in
our experiments: ? = 0.11764, ?L = 7.59 ? 10?5,
?rec = 1.30? 10?5 and ?reo = 3.80? 10?4. 3
The baseline system is a re-implementation of
(Xiong et al, 2006). Our system is different from the
baseline by replacing the MaxEnt reordering model
with a neural model. Both the systems have the same
pruning settings: the threshold pruning parameter is
set to 0.5 and the histogram pruning parameter to
40. For minimum-error-rate training, both systems
generate 200-best lists.
4.2 MT Evaluation
Table 1 shows the case-insensitive BLEU-4 scores
of the baseline system and our system on the devel-
opment and test sets. Our system outperforms the
baseline system by 1.21 BLEU points on the de-
velopment set and 1.07 on the test set. Both the
differences are statistically significant at p = 0.01
level (Riezler and Maxwell, 2005).
Table 2 shows the number of sentences that our
system has a higher (>), equal (=) or lower (<)
BLEU score on the NIST 2008 dataset. We find that
our system is superior to the baseline system for long
3The choice of ? is very important for achieving high BLEU
scores. We tried a number of intervals and found that the clas-
sification accuracy is most stable in the interval [0.100,0.125].
5 10 15 20 25 30 351
90
95
100
88 Length
Accur
acy (%
)
 
 
neuralmaxent
Figure 4: Comparison of reordering classification accu-
racies between the MaxEnt and neural classifiers over
varying phrase lengths. ?Length? denotes the sum of the
lengths of two source phrases in a reordering example.
Our classifier (neural) outperforms the MaxEnt classi-
fier (maxent) consistently, especially for predicting long-
distance reordering.
# of examples NIST 2006 (tune) NIST 2008
100,000 30.88 23.78
200,000 30.75 23.89
400,000 30.80 24.35
800,000 31.01 24.45
6,004,441 31.61 24.82
Table 3: The effect of reordering training data size on
BLEU scores. The BLEU scores rise with the increase of
training data size. Due to the computational cost, we only
used 1/5 of the entire bilingual corpus to train our neural
reordering model.
sentences.
Figure 4 compares classification accuracies of the
neural and MaxEnt classifiers. ?Length? denotes the
sum of the lengths of two source phrases in a re-
ordering example. For each length, we randomly se-
lect 200 unseen reordering examples to calculate the
classification accuracy. Our classifier outperforms
the baseline consistently, especially for long com-
posed blocks.
Xiong et al (2008) find that the performance of
the baseline system can be improved by forbidding
inverted reordering if the phrase length exceeds a
pre-defined distortion limit. This heuristic increases
the BLEU score of the baseline system significantly
to 24.46 but is still significantly worse (p < 0.05)
than our system without the heuristic. We find that
imposing this heuristic fails to improve our system
574
cluster 1 cluster 2 cluster 3 cluster 4 cluster 5
1.18 works for alternative duties these people who of the three
accessibility verify on one-day conference the reasons why on the fundamental
wheelchair tunnels from armed groups the story of how over the entire
candies transparency in chinese language works the system which through its own
cough opinion at eating habits the trend towards with the best
Table 4: Words and phrases that are close in the Euclidean space. The words and phrases in the same cluster have
similar behaviors from a reordering point of view rather than relatedness, suggesting that the vector representations
produced by the recursive autoencoders are helpful for capturing reordering regularities.
significantly. One possible reason is that there is
limited room for improvement as our system makes
fewer wrong predictions for long composed blocks.
The above results suggest that our system does go
beyond using boundary words and make a better use
of the merging blocks by using vector space repre-
sentations.
Table 3 shows the effect of training dataset size
on BLEU scores. We find that BLEU scores on both
the development and test sets rise with the increase
of the training dataset size. As the training process is
very time-consuming, only the reordering examples
extracted from 1/5 of the entire parallel training cor-
pus are used in our experiments to train our model.
Obviously, with more efficient training algorithms,
making full use of all the reordering examples ex-
tracted from the entire corpus will result in better
results. We leave this for future work.
4.3 Qualitative Analysis on Vector
Representations
Table 4 shows a number of words and phrases that
are close (measured by Euclidean distance) in the
n-dimensional space. We randomly select about
370K target side phrases used in our experiments
and cluster them into 983 clusters using k-means al-
gorithm (MacQueen, 1967). The distance between
two phrases are measured by the Euclidean distance
between their vector representations. As shown in
Table 4, cluster 1 mainly consists of nouns, clus-
ter 2 mainly contains verb/noun+preposition struc-
tures, cluster 3 contains compound phrases, cluster
4 consists of phrases which should be followed by
a clause, and cluster 5 mainly contains the begin-
ning parts of prepositional phrases that tend to be
followed by a noun phrase or word. We find that
the words and phrases in the same cluster have sim-
ilar behaviors from a reordering point of view rather
than relatedness. This indicates that the vector rep-
resentations produced by the recursive autoencoders
are helpful for capturing reordering regularities.
5 Conclusion
We have presented an ITG reordering classifier
based on recursive autoencoders. As recursive au-
toencoders are capable of producing vector space
representations for arbitrary multi-word strings in
decoding, our neural ITG system achieves an ab-
solute improvement of 1.07 BLEU points over the
baseline on the NIST 2008 Chinese-English dataset.
There are a number of interesting directions we
would like to pursue in the near future. First, re-
placing the MaxEnt classifier with a neural one re-
defines the conditions for risk-free hypothesis re-
combination. We find that the number of hypothe-
ses that can be recombined reduces in our system.
Therefore, we plan to use forest reranking (Huang,
2008) to alleviate this problem. Second, it is in-
teresting to follow Socher et al (2013) to combine
linguistically-motivated labels with recursive neural
networks. Another problem with our system is that
the decoding speed is much slower than the baseline
system because of the computational overhead intro-
duced by RAEs. It is necessary to investigate more
efficient decoding algorithms. Finally, it is possible
to apply our method to other phrase-based and even
syntax-based systems.
Acknowledgments
This research is supported by the 863 Program un-
der the grant No. 2012AA011102, by the Boeing
Tsinghua Joint Research Project on Language Pro-
cessing (Agreement TBRC-008-SDB-2011 Phase 3
575
(2013)), by the Singapore National Research Foun-
dation under its International Research Centre @
Singapore Funding Initiative and administered by
the IDM Programme Office, and by a Research Fund
No. 20123000007 from Tsinghua MOE-Microsoft
Joint Laboratory. Many thanks go to Chunyang Liu
and Chong Kuang for their great help for setting up
the computing platform. We also thank Min-Yen
Kan, Meng Zhang and Yu Zhao for their insightful
discussions.
References
Yaser Al-Onaizan and Kishore Papineni. 2006. Distor-
tion models for statistical machine translation. In Pro-
ceedings of the 21st International Conference on Com-
putational Linguistics and 44th Annual Meeting of
the Association for Computational Linguistics, pages
529?536, Sydney, Australia, July.
Yoshua Bengio, Re?jean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Research,
3:1137?1155, March.
James Bergstra and Yoshua Bengio. 2012. Random
search for hyper-parameter optimization. The Jour-
nal of Machine Learning Research, 13(1):281?305,
February.
Arianna Bisazza and Marcello Federico. 2012. Modi-
fied distortion matrices for phrase-based statistical ma-
chine translation. In Proceedings of the 50th Annual
Meeting of the Association for Computational Linguis-
tics (Volume 1: Long Papers), pages 478?487, Jeju Is-
land, Korea, July.
Antoine Bordes, Jason Weston, Ronan Collobert, and
Yoshua Bengio. 2011. Learning structured embed-
dings of knowledge bases. In Proceedings of the
Twenty-Fifth AAAI Conference on Artificial Intelli-
gence, pages 301?306.
Antoine Bordes, Xavier Glorot, Jason Weston, and
Yoshua Bengio. 2012. Joint learning of words and
meaning representations for open-text semantic pars-
ing. In International Conference on Artificial Intelli-
gence and Statistics, pages 127?135.
Colin Cherry. 2013. Improved reordering for phrase-
based translation using sparse features. In Proceed-
ings of the 2013 Conference of the North American
Chapter of the Association for Computational Linguis-
tics: Human Language Technologies, pages 22?31,
Atlanta, Georgia, June.
Ronan Collobert and Jason Weston. 2008. A unified ar-
chitecture for natural language processing: Deep neu-
ral networks with multitask learning. In Proceed-
ings of the 25th International Conference on Machine
Learning, pages 160?167.
R. Collobert, J. Weston, L. Bottou, M. Karlen,
K. Kavukcuoglu, and P. Kuksa. 2011. Natural lan-
guage processing (almost) from scratch. Journal of
Machine Learning Research, 12:2493?2537.
Yang Feng, Haitao Mi, Yang Liu, and Qun Liu. 2010. An
efficient shift-reduce decoding algorithm for phrased-
based machine translation. In Proceedings of the 23rd
International Conference on Computational Linguis-
tics: Posters, pages 285?293, Beijing, China, August.
Michel Galley and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Processing,
pages 848?856, Honolulu, Hawaii, October.
Xavier Glorot, Antoine Bordes, and Yoshua Bengio.
2011. Domain adaptation for large-scale sentiment
classification: A deep learning approach. In Proceed-
ings of the 28th International Conference on Machine
Learning (ICML-11), pages 513?520.
Christoph Goller and Andreas Kuchler. 1996. Learning
task-dependent distributed representations by back-
propagation through structure. In Proceedings of 1996
IEEE International Conference on Neural Networks
(Volume:1), volume 1, pages 347?352.
Spence Green, Michel Galley, and Christopher D. Man-
ning. 2010. Improved models of distortion cost for
statistical machine translation. In Proceedings of Hu-
man Language Technologies: The 2010 Annual Con-
ference of the North American Chapter of the Associ-
ation for Computational Linguistics, pages 867?875,
Los Angeles, California, June.
Karl Moritz Hermann and Phil Blunsom. 2013. The role
of syntax in vector space models of compositional se-
mantics. In Proceedings of the 51st Annual Meeting
of the Association for Computational Linguistics (Vol-
ume 1: Long Papers), pages 894?904, Sofia, Bulgaria,
August.
Eric Huang, Richard Socher, Christopher Manning, and
Andrew Ng. 2012. Improving word representations
via global context and multiple word prototypes. In
Proceedings of the 50th Annual Meeting of the Associ-
ation for Computational Linguistics (Volume 1: Long
Papers), pages 873?882, Jeju Island, Korea, July.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of the
46th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies,
pages 586?594, Columbus, Ohio, June.
Kevin Knight. 1999. Decoding complexity in word-
replacement translation models. Computational Lin-
guistics, 25(4):607?615, December.
576
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of the 2003 Conference of the North American
Chapter of the Association for Computational Linguis-
tics on Human Language Technology-Volume 1, pages
48?54.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the Association for
Computational Linguistics Companion Volume Pro-
ceedings of the Demo and Poster Sessions, pages 177?
180, Prague, Czech Republic, June.
James MacQueen. 1967. Some methods for classifica-
tion and analysis of multivariate observations. In Pro-
ceedings of the Fifth Berkeley Symposium on Mathe-
matical Statistics and Probability, volume 1.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics, 30(4):417?449.
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,
Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar
Kumar, Libin Shen, David Smith, Katherine Eng,
Viren Jain, Zhen Jin, and Dragomir Radev. 2004. A
smorgasbord of features for statistical machine trans-
lation. In Proceedings of the Human Language Tech-
nology Conference of the North American Chapter of
the Association for Computational Linguistics: HLT-
NAACL 2004: Main Proceedings, pages 161?168,
Boston, Massachusetts, USA, May.
Stefan Riezler and John T. Maxwell. 2005. On some
pitfalls in automatic evaluation and significance test-
ing for MT. In Proceedings of the ACL Workshop on
Intrinsic and Extrinsic Evaluation Measures for Ma-
chine Translation and/or Summarization, pages 57?
64, Ann Arbor, Michigan, June.
Richard Socher, Eric H. Huang, Jeffrey Pennin, An-
drew Y. Ng, and Christopher D. Manning. 2011a. Dy-
namic pooling and unfolding recursive autoencoders
for paraphrase detection. In Proceedings of Advances
in Neural Information Processing Systems 24, pages
801?809.
Richard Socher, Cliff C. Lin, Andrew Y. Ng, and Christo-
pher D. Manning. 2011b. Parsing natural scenes and
natural language with recursive neural networks. In
Proceedings of the 26th International Conference on
Machine Learning (ICML), pages 129?136.
Richard Socher, Jeffrey Pennington, Eric H. Huang, An-
drew Y. Ng, and Christopher D. Manning. 2011c.
Semi-supervised recursive autoencoders for predicting
sentiment distributions. In Proceedings of the 2011
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 151?161, Edinburgh, Scot-
land, UK., July.
Richard Socher, Brody Huval, Christopher D. Manning,
and Andrew Y. Ng. 2012. Semantic compositional-
ity through recursive matrix-vector spaces. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, pages 1201?
1211, Jeju Island, Korea, July.
Richard Socher, John Bauer, Christopher D. Manning,
and Ng Andrew Y. 2013. Parsing with compositional
vector grammars. In Proceedings of the 51st Annual
Meeting of the Association for Computational Linguis-
tics (Volume 1: Long Papers), pages 455?465, Sofia,
Bulgaria, August.
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In Proceedings of Interna-
tional Conference on Spoken Language Processing,
vol. 2, pages 901?904, September.
Christoph Tillman. 2004. A unigram orientation model
for statistical machine translation. In Proceedings of
the Human Language Technology Conference of the
North American Chapter of the Association for Com-
putational Linguistics: HLT-NAACL 2004: Short Pa-
pers, pages 101?104, Boston, Massachusetts, USA,
May.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403.
Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Maxi-
mum entropy based phrase reordering model for sta-
tistical machine translation. In Proceedings of the 21st
International Conference on Computational Linguis-
tics and 44th Annual Meeting of the Association for
Computational Linguistics, pages 521?528, Sydney,
Australia, July.
Deyi Xiong, Min Zhang, Aiti Aw, Haitao Mi, Qun Liu,
and Shouxun Lin. 2008. Refinements in BTG-based
statistical machine translation. In Proceedings of the
Third International Joint Conference on Natural Lan-
guage Processing: Volume-I, pages 505?512.
Deyi Xiong, Min Zhang, Aiti Aw, and Haizhou Li. 2010.
Linguistically annotated reordering: Evaluation and
analysis. Computational Linguistics, 36(3):535?568,
September.
Richard Zens, Hermann Ney, Taro Watanabe, and Ei-
ichiro Sumita. 2004. Reordering constraints for
phrase-based statistical machine translation. In Pro-
ceedings of the 20th International Conference on
Computational Linguistics, pages 205?211, Geneva,
Switzerland, Aug 23?Aug 27.
577
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1492?1502,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Fast Joint Compression and Summarization via Graph Cuts
Xian Qian and Yang Liu
The University of Texas at Dallas
800 W. Campbell Rd., Richardson, TX, USA
{qx,yangl}@hlt.utdallas.edu
Abstract
Extractive summarization typically uses sen-
tences as summarization units. In contrast,
joint compression and summarization can use
smaller units such as words and phrases, re-
sulting in summaries containing more infor-
mation. The goal of compressive summariza-
tion is to find a subset of words that max-
imize the total score of concepts and cut-
ting dependency arcs under the grammar con-
straints and summary length constraint. We
propose an efficient decoding algorithm for
fast compressive summarization using graph
cuts. Our approach first relaxes the length con-
straint using Lagrangian relaxation. Then we
propose to bound the relaxed objective func-
tion by the supermodular binary quadratic pro-
gramming problem, which can be solved ef-
ficiently using graph max-flow/min-cut. S-
ince finding the tightest lower bound suffers
from local optimality, we use convex relax-
ation for initialization. Experimental results
on TAC2008 dataset demonstrate our method
achieves competitive ROUGE score and has
good readability, while is much faster than the
integer linear programming (ILP) method.
1 Introduction
Automatic multi-document summarization helps
readers get the most important information from
large amounts of texts. Summarization techniques
can be roughly divided into two categories: extrac-
tive and abstractive. Extractive summarization casts
the summarization task as a sentence selection prob-
lem: identifying important summary sentences from
one or multiple documents. Many methods have
been developed in the past decades, including super-
vised approaches that use classifiers to predict sum-
mary sentences, graph based approaches to rank the
sentences, and recent global optimization methods
such as integer linear programming (Gillick et al,
2008) (ILP) and submodular maximization methods
(Lin and Bilmes, 2011). Though extractive summa-
rization is popular because of its simplicity and high
readability, it has limitations in that it selects each
sentence as a whole, and thus may miss informative
partial sentences.
To improve the informativeness, joint com-
pression and summarization was proposed (Berg-
Kirkpatrick et al, 2011), which uses words as sum-
marization units, unlike extractive summarization
where each sentence is a basic undecomposable u-
nit. To achieve better readability, manually defined
grammar constraints or automatically learned mod-
els based on syntax trees are added during the sum-
marization process. Up to now, the state of the art
compressive systems are based on integer linear pro-
gramming (ILP). Because ILP suffers from expo-
nential complexity, word-based compression sum-
marization is an order of magnitude slower than
sentence-based extraction.
One common way to solve an ILP problem is to
use its LP relaxation and round the results. How-
ever Berg-Kirkpatrick et al (2011) found that LP
relaxation gave poor results, finding unacceptably
suboptimal solutions. For speedup, they proposed a
two stage method where they performed some sen-
tence selection in the first step to reduce the number
of candidates. Despite their empirical success, such
1492
a pruning approach has its inherent problem in that
it may eliminate correct sentences in the first step.
Recently, Almeida and Martins (2013) proposed a
fast joint decoding algorithm based on dual decom-
position. For fast convergence, they added quadratic
penalty terms to alleviate the learning rate problem.
In this paper, we propose an efficient decoding al-
gorithm for fast ILP based compressive summariza-
tion using graph cuts. Our assumption is that all con-
cepts are word n-grams and non-negatively scored.
The rationale for the non-negativity assumption is s-
traightforward: the score of a concept reflects its in-
formativeness, hence should be non-negative. Given
a set of documents, each word is associated with a
binary variable, indicating whether the word is se-
lected in the summary. Our idea is to approximate
the ILP as a binary quadratic programming problem
where coefficients of all quadratic terms are non-
negative. It is well known that such binary quadrat-
ic function is supermodular, and its maximum can
be solved efficiently using graph max-flow/min-cut.
Hence the key is to find the coefficients of the super-
modular binary quadratic function (SBQF) so that
its maximum is close to the optimal ILP objective
function. Our solution consists of 3 steps. First,
we show that the subtree deletion model and gram-
mar constraints can be eliminated by adding SBQF-
s to the objective function. Second, we relax the
summary length constraint using Lagrangian relax-
ation. Third, we propose a family of SBQFs that
are lower bounds of the ILP objective function. S-
ince finding the tightest lower bound suffers from
local optimality, we choose to use convex relaxation
for initialization. To demonstrate our technique, we
conduct experiments on Text Analysis Conference
(TAC) datasets using the same train/test splits as pre-
vious work (Berg-Kirkpatrick et al, 2011). We com-
pare our approach with the state-of-the-art ILP based
approach in terms of summary quality (ROUGE s-
cores and sentence quality) and speed. Experimen-
tal results show that our proposed method achieves
competitive performance with ILP, while about 100
times faster.
2 Compressive Summarization
2.1 Extractive Summarization
As our method is an approximation of ILP based
method, we first briefly review the ILP based extrac-
tive summarization and compressive summarization.
Gillick and Favre (2009) introduced the concept-
based ILP for summarization. A concept is a basic
semantic unit. They used word bigrams as such lan-
guage concepts. Their system achieved the highest
ROUGE score on the TAC 2009 evaluation. This
approach selects sentences so that the total score
of language concepts appearing in the summary is
maximized. The association between the language
concepts and sentences serves as the constraints, in
addition to the summary length constraint.
Formally, given a set of sentences S = {sn}Nn=1,
extractive summarization can be represented by a bi-
nary vector y, where yn indicates whether sentence
sn is selected. Let C = {c1, . . . cJ} denote the set
of concepts in S, e.g., word bigrams (Gillick and
Favre, 2009). Each concept cj is associated with a
given score wj and a binary variable vj indicating
if cj is selected in the summary. Let njk denote the
index of the sentence containing the kth occurrence
of concept cj , and ln denote the length of sentence
sn. The ILP based extractive summarization system
can be formulated as below:
max
y,v
J
?
j=1
wjvj
s.t. vj =
?
k
ynjk 1 ? j ? J (1)
N
?
i=1
ynln ? L
v,y are binary
The first constraint is imposed by the relation be-
tween concept selection and sentence selection: s-
electing a sentence leads to the selection of all the
concepts it contains, and selecting a concept only
happens when it is present in at least one of the se-
lected sentences. The second constraint is the sum-
mary length constraint.
As solving an ILP problem is generally NP-hard,
pre-pruning of candidate concepts and sentences is
necessary for efficient summarization. For exam-
1493
ple, the ICSI system (Gillick et al, 2008) removed
the sentences that are too short or have non-overlap
with the queries, and concepts with document fre-
quency less than 3, resulting in 95.8 sentences and
about 80 concepts per topic on the TAC2009 dataset.
Therefore the actual scale of ILP is rather small after
pruning (e.g., 176 variables and 372 constraints per
topic). Empirical studies showed that such small s-
cale ILP can be solved within a few seconds (Gillick
and Favre, 2009).
2.2 Compressive Summarization
The quality of sentence-based extractive summariza-
tion is limited by the informativeness of the orig-
inal sentences and the summary length constraint.
To remove the unimportant part from a long sen-
tence, sentence compression is proposed to generate
more informative summaries (Liu and Liu, 2009; Li
et al, 2013a). Recent studies show that joint sen-
tence compression and extraction, namely compres-
sive summarization, outperforms pipeline systems
that run extractive summarization on the compressed
sentences or compress selected summary sentences
(Martins and Smith, 2009; Berg-Kirkpatrick et al,
2011; Chali and Hasan, 2012). In Berg-Kirkpatrick
et al (2011), compressive summarization inte-
grates the concept model for extractive summariza-
tion (Gillick and Favre, 2009) and subtree deletion
model for sentence compression. The score of a
compressive summary consists of two parts, scores
of selected concepts, and scores of the broken arcs
in the dependency parse trees. The selected word-
s must satisfy the length constraint and grammar
constraints that include subtree constraint and some
manually defined hard constraints.
Formally, let x = x1 . . . xI denote the word se-
quence of documents, where s1 = x1, . . . xl1 corre-
sponds to the first sentence, s2 = xl1+1, . . . , xl1+l2
corresponds to the second sentence, and so on. A
compressive summary can be represented by a bi-
nary vector z, where zi indicates whether word xi
is selected in the summary. Let ahm denote the arc
xh ? xm in the dependency parse tree of the cor-
responding sentence containing words xh and xm,
and A = {ahm} denote the set of dependency arcs.
The subtree constraint ensures that word xm is se-
lected only if its head xh is selected. In order to
guarantee the readability, grammar constraints are
added to prohibit the breaks of some specific arc-
s. For example, Clarke and Lapata (2008) nev-
er deleted an arc whose dependency label is SUB,
OBJ, PMOD, SBAR or VC. In this paper, we use
B ? A to denote the set of these arcs that must
not be broken in summarization. We use ojk to de-
note the indices of words corresponding to the kth
occurrence of cj . For example, suppose the jth
concept European Union appears twice in the doc-
ument: x22x23 = x50x51 =European Union, then
oj1 = {22, 23}, oj2 = {50, 51}.
The compressive summarization model can be
formulated as an integer programming problem
max
z,v
J
?
j=1
wj ? vj +
?
ahm?A
wahmzh(1? zm)
s.t. vj =
?
k
?
i?ojk
zi ?j
?
i
zi ? L
zh ? zm ?ahm ? A (2)
zh = zm ?ahm ? B
z,v are binary
According to the subtree deletion model, the score
of arc ahm is included if zh = 1 and zm = 0, which
can be formulated as wahm ? zh(1 ? zm). The first
constraint is similar to that in extractive summariza-
tion, that is, a concept is selected if and only if any
of its occurrence is selected. The third and fourth
constraints are the subtree constraints and manual-
ly defined grammar constraints respectively. In the
rest of the paper, without loss of generality, we re-
move the fourth constraint by directly substituting
one variable for the other.
Finding the optimal summary is generally NP-
hard. Unlike extractive summarization where the s-
cale of the problem (the number of sentences and
concepts) is small, the number of variables in com-
pressive summarization is linear in the number of
words, which is usually thousands on the TAC
datasets. Hence solving such a problem using ILP
based decoding algorithms is not efficient especially
when the document set is large.
1494
3 Fast Decoding via Graph Cuts
In this section, we introduce our fast decoding al-
gorithm. We assume that all the concepts are word
n-grams, and their scores are non-negative. The non-
negativity assumption can reduce the computational
complexity, but is also reasonable: the score of a
concept denotes its informativeness, hence should
be non-negative. For example, Li et al (2013b)
proposed to use the estimated normalized frequen-
cies of concepts as scores, which are essentially non-
negative. The basic idea of our method is to approx-
imate the above optimization problem (2) by the su-
permodular binary quadratic programming (SBQP)
problem:
max
z
?
i
?izi +
?
ij
?ijzizj
s.t. z is binary (3)
where ?ij ? 0. It is known that such a binary
quadratic function is supermodular, and its maxi-
mum can be solved efficiently using graph max-
flow/min-cut (Billionnet and Minoux, 1985; Kol-
mogorov and Zabih, 2004). Now the problem is to
find the optimal ?, ? for a good approximation.
3.1 Formulate Grammar Constraints and
Subtree Deletion Model by SBQF
We show that the subtree deletion model can be for-
mulated equivalently using SBQF. First, we can e-
liminate the constraint zh ? zm by adding a penalty
term to the objective function. That is,
max f(z)
s.t. zh ? zm
z is binary
is equivalent to
max f(z)??(1? zh)zm
s.t. z is binary
We can see that the penalty term??(1?zh)zm ex-
cludes zh = 0, zm = 1 from the feasible set, and
for zh ? zm, both problems have the same objective
function value. Hence the two problems are equiva-
lent. Notice that the coefficient of quadratic term in
??(1 ? zh)zm is positive, hence the penalty term
is supermodular.
Now we eliminate the third constraint in problem
(2) using the penalized objective function described
above. Note that the fourth constraint has been elim-
inated by variable substitution, we have
max
z,v
J
?
j=1
wj ? vj +
?
ahm?A
wahmzh(1? zm)
??
?
ahm?A
(1? zh)zm
s.t. vj =
?
k
?
i?ojk
zi ?j (4)
?
i
zi ? L
z,v are binary
We can see that for each arc ahm, there must be a
positive quadratic term +?zhzm in the objective
function, which guarantees the supermodularity of
the objective function, no matter what wahm is.
3.2 Eliminate Length Constraint Using
Lagrangian Relaxation
Problem (4) is NP-hard, because for any feasible v,
it is a SBQP with a length constraint. Since size con-
strained minimum cut problem is generally NP-hard
(Nagano et al, 2011), Problem (4) can not be cast
as a SBQP as long as P ?= NP. One popular way to
deal with the size constrained optimization problem
is Lagrangian relaxation. We introduce Lagrangian
multiplier ? to the length constraint in Problem (4),
and get
min
?
max
z,v
J
?
j=1
wj ? vj +
?
ahm?A
wahmzh(1? zm)
??
?
ahm?A
(1? zh)zm
+?(L?
?
i
zi)
s.t. vj =
?
k
?
i?ojk
zi ?j (5)
? ? 0
z,v are binary
We solve the relaxed problem iteratively. In each
iteration, we fix ? and solve the inner maximiza-
tion problem (details described below). The score of
1495
each word is penalized by ? ? with larger ?, few-
er words are selected. Hence the summary length
can be adjusted by ?. The optimal ? can be found
using binary search. We maintain an upper bound
?max , and a lower bound ?min, which is initially 0.
In each iteration, we choose ? = 12(?max + ?min)
and search the optimal z. If the duality gap vanish-
es, i.e., ?(L ?
?
i zi) = 0 and
?
i zi ? L, then we
get the global solution of Problem (4). Otherwise,
if
?
i zi > L, then the current ? is too small, so we
set ?min = ?; otherwise, ? > 0 and
?
i zi < L,
we set ?max = ?. The search process terminates if
?max ? ?min is less than a predefined threshold.
3.3 Eliminate v Using Supermodular
Relaxation
Now we consider the inner maximization Problem
(5). It is still not a SBQP, since the objective func-
tion is not a linear function of zizj . We propose to
approximate the objective function using SBQP. Our
solution consists of two steps. First we relax the first
constraint of Problem (5) by bounding the objec-
tive function with a family of supermodular pseudo
boolean functions (Boros and Hammer, 2002). Sec-
ond we reformulate these pseudo boolean functions
equivalently as quadratic functions.
Similar to the bounding strategy in (Qian and Liu,
2013), we relax the logical disjunction by lineariza-
tion. Using the fact that for any binary vector a, we
have
?
ai = max
p??
?
i
piai
where ? denotes the probability simplex
? = {p|
?
k
pk = 1, pk ? 0}
We have
vj =
?
k
?
i?ojk
zi
= max
pj??
?
k
pjk
?
i?ojk
zi
Plug the equation above into the objective function
of Problem (5), we get the following optimization
problem
max
z,p
J
?
j=1
?
?
?
k
pjkwj
?
i?ojk
zi
?
?
+
?
ahm?A
wahmzh(1? zm)
??
?
ahm?A
(1? zh)zm
+?(L?
?
i
zi)
s.t. z is binary (6)
pj ? ? ?j
LetQ(p, z) denote the objective function of Prob-
lem (6). Given p, we can see that Q is a supermod-
ular pseudo boolean function because coefficients
of all non-linear terms are non-negative. Using the
fact that for any binary vector a = [a1, . . . ar]T ,
ai ? {0, 1}, 1 ? i ? r,
r
?
i=1
ai = max
b?{0,1}
( r
?
i=1
ai ? r + 1
)
b
(Freedman and Drineas, 2005), we get the following
equivalent optimization problem of Problem (6)
max
z,p,q
J
?
j=1
?
k
pjkwjqjk
?
?
?
i?ojk
zi ? |ojk|+ 1
?
?
+
?
ahm?A
wahmzh(1? zm)
??
?
ahm?A
(1? zh)zm
+?(L?
?
i
zi)
s.t. z,q are binary (7)
pj ? ? ?j
where |ojk| is the size of ojk.
Let R(z,p,q) denote the objective function of
Problem (7), to search the optimal point, we al-
ternatively update p and z,q. First we initialize
p = p(0). In each iteration, we first fix p. It is ob-
vious that Problem (7) is a SBQP, hence the optimal
z,q can be solved efficiently using max-flow/min-
cut. Then we fix z,q, and update p using projected
1496
subgradient. That is
pnewj = P?
(
pj +
?R
?pj
?
)
(8)
where ? > 0 is the step size in line search, and func-
tion P?(q) denotes the projection of q onto the fea-
sible set ?
P?(q) = min
p??
?p? q?2
which can be solved efficiently by sorting (Duchi et
al., 2008).
3.4 Initialize p Using Convex Relaxation
Since R is non-concave, searching its maximum us-
ing subgradient method suffers from local optimali-
ty. Though one can use techniques such as branch-
and-bound for exact inference (Qian and Liu, 2013;
Gormley and Eisner, 2013), here for fast decoding,
we use convex relaxation to choose a good seed
p(0). Recall that pjk denotes the percentage of the
kth occurrence contributing to cj . The larger pjk is,
the more likely the kth occurrence is selected. To
estimate such likelihood, we replace the binary con-
straint in extractive summarization (Problem (1)) by
0 ? y,v ? 1, since solving a relaxed LP is much
faster than ILP. Suppose y? is the optimal solution
for such a relaxed LP problem, we initialize p by
pjk =
y?njk
?
k y?njk
(9)
If for all k, y?njk = 0, then we initialize pjk using
uniform distribution
pjk =
1
|oj |
where |oj | is the frequency of cj .
3.5 Summary
For clarity, we summarize our decoding algorithm in
Algorithm 1. Initial ?max can be arbitrarily large. In
our experiments, we set ?max =
?
j wj , which em-
pirically guarantees the summary length
?
i zi ? L
when ? = ?max. The choice of the step size for
updating p is similar to the projected subgradien-
t method in dual decomposition (Koo et al, 2010).
Algorithm 1 Compressive Summarization via
Graph Cuts
Require: Scores of concepts {wj} and arcs {wahm},
max summary length L.
Ensure: Compressive summarization z?, where zi indi-
cates whether the ith word is selected.
Solve the relaxed LP of Problem (1) (replace the binary
constraint by 0 ? y,v ? 1) to get y.
Initialize p(0) using Eq (9).
Initialize sufficient large ?max, and ?min = 0
while ?max ? ?min > ? do
Set ? = 12 (?min + ?max)
Set p = p(0).
repeat
Fix p, solve Problem (7) to get z using max-
flow/min-cut.
Update p using Eq (8).
until convergence
if
?
i zi > L then
?min = ?
else if
?
i zi < L then
?max = ?
else
break
end if
end while
4 Features and Hard Constraints
We choose discriminative models to learn the scores
of concepts and arcs. For concept cj , its score is
wj = ?Tconceptfconcept(cj)
where fconcept(cj) is the feature vector of cj , and
?concept is the corresponding weight vector of fea-
ture fconcept(cj). Similarly, score wahm is defined
as
wahm = ?
T
arcfarc(ahm)
Though our algorithm can handle general word n-
gram concepts, we restrict the concepts to word bi-
grams, which have been widely used recently in the
sentence-based ILP extractive summarization sys-
tems. For a concept cj , we define the following
features, some of which have been used in previous
work (Brandow et al, 1995; Aker and Gaizauskas,
2009; Edmundson, 1969; Radev, 2001; Li et al,
2013b). All of these features are non-negative.
? Term frequency: the frequency of cj in the giv-
en topic.
1497
? Stop word ratio: ratio of stop words in cj . The
value can be {0, 0.5, 1}.
? Similarity with topic title: the number of com-
mon words in these two strings, divided by the
length of the longer string.
? Document ratio: percentage of documents con-
taining cj .
? Sentence ratio: percentage of sentences con-
taining cj .
? Sentence-title similarity: word uni-
gram/bigrams cosine similarity between
the sentence containing cj and the topic title.
For concepts appearing in multiple sentences,
we choose the maximal similarity.
? Sentence-query similarity: word uni-
gram/bigram cosine similarity between
the sentence containing cj and the topic query
(concatenation of topic title and description).
For concepts appearing in multiple sentences,
we choose the maximal similarity.
? Sentence position: position of the sentence
containing cj in the document. For concepts
appearing in multiple sentences, we choose the
minimum.
? Sentence length: length of the sentence con-
taining cj . For concepts appearing in multiple
sentences, we choose the maximum.
? Paragraph starter: binary feature indicating
whether cj appears in the first sentence of a
paragraph.
For subtree deletion model, we define the follow-
ing features for arc ahm.
? POS tags of head word xh and child word xm
and their concatenations.
? Dependency label of arc ahm and its parent arc.
? Word xm if xm is a conjunction word or prepo-
sition word. Word xh if xm is a conjunction
word or preposition word.
? Binary feature indicating whether the modifier
xm is a temporal word such as Friday.
We also define some hard constraints for subtree
deletion to improve the readability of the generated
compressed sentences.
? C0 Arc ahm can be cut only if one of the two
conditions holds: (1) there is a comma, colon,
or semicolon between the head and the modifi-
er; (2) the modifier word is a preposition (POS
tag is IN) or a wh-word, such as what, who,
whose (corresponds to POS tag IN, WDT, WP,
WP$, WRB).
? C1 Arcs with dependency labels SUB, OBJ,
PRD, SBAR or VC can not be cut.
? C2 Arcs in set phrases like so far, more than,
according to can not be cut.
? C3 All arcs in coordinate structures can not be
cut, such as cats and dogs.
Note that compared with previous work, our com-
pression is more conservative. Constraint C0 al-
lows only a small portion of arcs to be cut. This
is based on our observation of the sentence com-
pression corpus: removing preposition phrases (PP)
or sub-clauses can greatly reduce the length of sen-
tence, while hurting the readability little. Cutting
other arcs like NMOD usually removes only one or
two words, and possibly affects the sentence?s read-
ability.
5 Experimental Results
5.1 Experimental Setup
Due to the lack of training data for compressive
summarization, we learn the subtree deletion mod-
el and the concept model separately. Specifically,
the sentence compression dataset (Clarke and La-
pata, 2008) (referred as CL08) is used for subtree
deletion model training (?arc). A sentence pair in
the corpus is kept for training the subtree deletion
model if the compressed sentence can be derived by
deleting subtrees from the parse tree of the origi-
nal sentence. There are 3, 178 out of 5, 739 such
pairs. The concept model (?concept) is learned from
the TAC2009 dataset. We create the oracle extrac-
tive summaries with the maximal bigram recall as
the reference summary. TAC2010 data is used as
1498
Corpus Sent. Words Topics
Train TAC2009 4, 216 117, 304 44
CL08 3, 178 52, 624 N/A
Develop TAC2010 2, 688 72, 609 46
Test TAC2008 4, 518 123, 946 48
Table 1: Corpus statistics. Training data consist of
two parts, TAC2009 for learning the concept mod-
el, CL08 (Clarke and Lapata, 2008) for learning the
subtree deletion model.
development set for various parameter tuning. Table
1 has the descriptions of all the data used.
We choose averaged perceptron for fast training.
The number of iterations is tuned on the develop-
ment data. Remind that our algorithm is based on the
assumption that scores of concepts are non-negative,
?j, wj ? 0. We assume that feature vector fconcept
is non-negative (e.g., term frequency, n-gram fea-
tures), then ?concept ? 0 is required to guarantee the
non-negativity of wj . Therefore, we project ?concept
onto the non-negative space after each iteration. S-
ince training is offline, we use ILP based exact in-
ference for accurate learning. 1
To control the contributions of the concept mod-
el and the subtree deletion model, we introduce a
parameter ?, and modify the original maximization
problem (Problem 2) to:
max
z,v
J
?
j=1
wj ? vj + ??
?
ahm?A
wahmzh(1? zm)
We tune ? on TAC2010 dataset. For max-flow/min-
cut, in our experiments, we implemented the im-
proved shortest augmented path (SAP) method (Ed-
monds and Karp, 1972).
For performance measure of the summaries, we
use both ROUGE and linguistic quality. ROUGE
has been widely used for summarization perfor-
mance and can measure the informativeness of the
summaries (content match between system and ref-
erence summaries). Since joint compression and
summarization tends to pick isolated words to max-
imize the information coverage in the system gener-
ated summaries, it may have poor readability. There-
fore we conduct human evaluation for the linguis-
1we choose the GLPK as our ILP solver, which is used in
(Berg-Kirkpatrick et al, 2011)
tic quality for various systems. The linguistic qual-
ity consists of two parts. One evaluates the gram-
mar quality within a sentence. Annotators marked
if a compressed sentence is grammatically correc-
t. Typical grammar errors include lack of verb or
subordinate clause. The other evaluates the coher-
ence between sentences, including the order of sen-
tences and irrelevant sentences. For compressive
summaries, we removed the sentences with gram-
mar errors when evaluating coherence. The overall
linguistic quality score is the combined score of the
percentage of grammatically correct sentences and
the correct ordering of the summary sentences. The
score is scaled and ranges from 1 (bad) to 10 (good).
5.2 Results on the Development Set
We conducted a series of experiments on the de-
velopment dataset to investigate the effect of the
non-negative score assumption, SBQP approxima-
tion, and initialization. First, we build a stan-
dard ILP based compressive summarizer without the
non-negative score assumption. We varied ? over
{2?4, 2?3, . . . 24} and selected the optimal ? = 2?2
according to both ROUGE-2 score and linguistic
quality. This interpolation weight is used in all the
other experiments.
To study the impact of the non-negative score as-
sumption, we build another summarizer by replac-
ing the concept model with the one trained under the
non-negative constraint. We also compared three d-
ifferent initialization strategies for p. The first one
is uniform initialization, i.e., pjk = 1|oj | . The second
one is a greedy approach, where extractive summa-
rization is obtained by greedy search (i.e., add the
top ranked sentence iteratively), then we use the cor-
responding y and Eq (9) to initialize p. The last one
is our convex relaxation method described in Sec-
tion 3.4.
Table 2 shows the comparison results. For com-
parison, we also include the sentence-based ILP ex-
tractive summarization results. We can see that the
impact of initial p is substantial. Using convex re-
laxation helps our method to survive from local opti-
mality. The non-negativity assumption has very lit-
tle effect on the standard compressive summariza-
tion (comparing the first two rows). This empir-
ical result demonstrates the appropriateness of the
assumption we use in our proposed method.
1499
System R-2 LQ
ILP (? = 2?2) 11.22 6.3
ILP (Non Neg.) 11.18 6.4
Graph Cut (uniform) 9.54 5.9
Graph Cut (greedy) 10.13 6.2
Graph Cut (LP) 11.06 6.1
Sent Extractive 10.11 7.3
Table 2: Experimental results on developmen-
t dataset. R-2 and LQ are short for ROUGE-2 score
multiplied by 100, and linguistic quality respective-
ly.
5.3 Results on Test Dataset
Table 3 shows the summarization results for various
systems on the TAC2008 data set. We show both the
summarization performance and the speed2 of the
system. The presented systems include our graph-
cut based method, the ILP based compression and
summarization, and the sentence-based extractive
summarization. ILP 2-step refers to the 2-step fast
decoding strategy proposed by (Berg-Kirkpatrick et
al., 2011).
We also list the performance of some state-of-the-
art systems, including the two ICSI systems (Gillick
et al, 2008), the compressive summarization sys-
tem of Berg-Kirkpatrick et al (2011) (GBK?11),
the multi-aspect ILP system of Woodsend and Lapa-
ta (2012)(WL?12) and the dual decomposition based
system (Almeida andMartins, 2013) (AM?13). Note
that for these referred systems since the linguistic
quality results are not comparable due to different
judgment methods. For our graph-cut based method,
to study the tradeoff between the readability of the
summary and the ROUGE scores, we present two
versions for this method: one uses all the constraints
(C0-C3), the other does not use C0.
We can see that our proposed method balanced
speed and quality. Compared with ILP, we achieved
competitive ROUGE scores, but with about 100x
speedup. Our method is also faster than the 2-step
ILP system. We also tried another state-of-the-art
LP solver, Gurobi version 5.53, it achieves 0.17 sec-
onds per topic, much faster than GLPK, but stil-
2For fair comparison, we only recode the running time for
decoding. Other time costs like feature extraction, IO opera-
tions are excluded.
3www.gurobi.com
System R-2 R-SU4 LQ sec.
Graph Cut 11.74 14.54 6.5 0.056
Graph Cut w/o C0 12.05 14.71 5.4 0.053
ILP 11.86 14.62 6.6 5.5
ILP (Non Neg.) 11.82 14.60 6.6 5.2
ILP (2-step) 11.72 14.49 6.5 1.1
Sent Extractive 11.06 13.93 7.1 0.13
ICSI-1 11.0 13.4 - 0.38?
ICSI-2 11.1 14.3 - -
BGK?11 11.70 14.38 6.5? -
WL?12 11.37 14.47 - -
AM?13 12.30+ 15.18+ 4.2? 0.41?
Table 3: Experimental results on TAC2008 dataset.
Columns 2-5 are scores of ROUGE-2, ROUGE-
SU4, linguistic quality, and speed (seconds per top-
ic). ROUGE-2 and ROUGE-SU4 scores are multi-
plied by 100. All the experiments are conducted on
the platform Intel Core i5-2500 CPU 3.30GHz. ?
numbers are not directly comparable due to differ-
ent annotations or platforms. + extra resources are
used.
l slower than ours. Regarding the grammar con-
straints used in our system, from the two result-
s for our graph-cut based method, we can see that
adding constraint C0 significantly decreases the R-2
score but improves the language quality. This shows
that word-based joint compression and summariza-
tion can improve ROUGE score; however, we need
to keep in mind about linguistic quality and find a
tradeoff between the ROUGE score and the linguis-
tic quality. Almeida and Martins (2013) trained their
model on extra corpora using multi-task learning,
and achieved better results than ours. The results
of our system and theirs showed that Lagrangian re-
laxation based method combined with combinatorial
optimization algorithms such as dynamic program-
ming or minimum cut can exploit the inner structure
of problems and achieve significant speedup over
ILP.
Four example summaries produced by our system
are shown below. Words in gray are not selected in
the summary.
1500
India?s space agency is ready to send a man to space within sev-
en years if the government gives the nod, while preparations have
lready begun for the launch of an unmanned lunar mission, a top
official said. India will launch more missions to the moon if its
maiden unmanned spacecraft Chandrayaan-1, slated to be launched
by 2008, is successful a top space fficial said Tuesday. The Unit-
ed States, the European Space Agency, China, Japan and India are
all planning lunar missions during the ext decade.India is ?a step
ahead? of China in satellite technology and can surpass Beijing
in space research by tapping the talent of its huge pool of young
scientists, India?s space research chief said Monday. The space
agencies of India and France signed an agreement on Friday to co-
operate in launching a satellite in four years that will help make
climate predictions more accurate. The Indian Space Research Or-
ganization (ISRO) has short-listed experiments from five nation-
s including the United States, Britain and Germany, for a slot on
India?s unmanned moon mission Chandrayaan-1 to be undertaken
by 2006-2007, the Press Trust of India (PTI) reported Monday. A
three-member Afghan delegation is in Bangalore seeking help to
set up a high-tech telemedicine facility in 10 Afghan cities linked
via Indian satellites, Indo-Asian News Service reported Saturday.
A woman was killed in Mississippi when a tree crashed on her car,
becoming the 11th fatality blamed on the powerful Hurricane Kat-
rina that slammed the US Gulf coast after pounding Florida, local
TV reportedMonday. The bill for the Hurricane Katrina disaster ef-
fort has so far reached 2.87 billion dollars, federal officials said on
Tuesday. The official death toll from Hurricane Katrina has risen
to 118 people in and around the swamped city of New Orleans,
officials said Thursday. The Foreign Ministry on Friday reported
the first confirmed death of a Guatemalan due to Hurricane Kat-
rina in the United States. The Ugandan government has pledged
200,000 US dollars toward relief and rebuilding efforts in the after-
math of Hurricane Katrina, local press reported on Friday. Swiss
Reinsurance Co., the world?s second largest reinsurance company
on Monday doubled to 40 billion US dollars its initial estimate of
the global insured losses caused by Hurricane Katrina in the United
States.
The A380 ?superjumbo?, which will be presented to the world in
a lavish ceremony in southern France on Tuesday, will be prof-
itable from 2008, its maker Airbus told the French financial news-
paper La Tribune. The A380 will take over from the Boeing 747
as the biggest jet in the skies. An association of residents living n-
ear Paris?s Charles-de-Gaulle airport on Wednesday denounced the
noise pollution generated by the giant Airbus A380, after the new
airliner?s maiden flight. One problem that Airbus is encountering
with its new A380 is that the craft pushes the envelope on the max-
imum size of a commercial airplane. With a whisper more than a
roar, the largest passenger airliner ever built, the Airbus 380, took
off on its maiden flight Wednesday.
?When she came in, she was in good spirits,? a prison staffer told
the New York Daily News. Martha Stewart, the American celebrity
homemaker who had her own cooking and home improvement TV
show, reported to a federal prison in Alderson, West Virginia, on
Friday to serve a five-month sentence for lying about a stock sale.
Home fashion guru Martha Stewart said on Friday that she has ad-
justed to prison life and is keeping busy behind bars since reporting
a week ago to a federal penal camp in West Virginia, where she
is serving a five-month sentence for lying about a stock sale. The
lawyer said he did not know what she is writing, but Stewart has
suggested since her conviction that she might write a book about
her recent experience with the legal system. Walter Dellinger, the
lawyer leading the appeal, said on NBC?s ?Today? that Stewart is
exploring ?innovative ways to do microwave cooking? The lawyer
said he did not know with her fellow inmates. As Martha Stewart
arrives at the red-brick federal prison in Alderson, W. Va., on Fri-
day to begin a five-month sentence, the company she founded is
focused both on life without her and on life once she returns.
In most cases, the removed phrases do not hurt the
readability of the summaries. The errors are mainly
caused by the break of sub-clauses or main claus-
es that are separated by commas, for example, the
fourth sentence in the last summary, The lawyer said
he did not know what she is writing. The compressed
sentence is grammatically correct, but semantically
incomplete. Other errors are due to the lack of verb,
subject, or object, or incorrect removal of PP, such
as the last sentence of the last summary.
6 Conclusion
In this paper, we propose a fast decoding algorith-
m for compressive summarization using graph cuts.
Our idea is to approximate the original ILP prob-
lem using supermodular binary quadratic program-
ming (SBQP) problem. Under the assumption that
scores of concepts are non-negative, we eliminate
subtree constraints and grammar constraints, and
relax the length constraint and non-supermodular
part of the problem step by step. Our experimen-
tal results showed that the graph cut based method
achieved competitive performance compared to ILP,
while about 100 times faster.
There are several possibilities for further research
involving our graph cut algorithms. One idea is to
apply it to the language model based compression
method (Clarke and Lapata, 2008). The other is
to adapt it to social media text summarization task,
where text is much more noisy. As graph cut is a
general method, applying it to other binary struc-
tured learning tasks is also an interesting direction.
Acknowledgments
We?d like to thank three anonymous reviewers for
their valuable comments. This work is partly sup-
ported by NSF award IIS-0845484 and DARPA un-
der Contract No. FA8750-13-2-0041. Any opinions
expressed in this material are those of the authors
and do not necessarily reflect the views of the fund-
ing agencies.
References
A. Aker and R. Gaizauskas. 2009. Summary generation
for toponym-referenced images using object type lan-
guage models. In Proceedings of RANLP.
1501
Miguel Almeida and Andre Martins. 2013. Fast and ro-
bust compressive summarization with dual decompo-
sition and multi-task learning. In Proceedings of ACL,
pages 196?206, August.
Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein.
2011. Jointly learning to extract and compress. In
Proceedings of ACL-HLT, pages 481?490, June.
A. Billionnet and M. Minoux. 1985. Maximizing a su-
permodular pseudoboolean function: A polynomial al-
gorithm for supermodular cubic functions. Discrete
Applied Mathematics, 12(1):1 ? 11.
Endre Boros and Peter L. Hammer. 2002. Pseudo-
boolean optimization. Discrete Applied Mathematics,
123(1C3):155 ? 225.
Ronald Brandow, Karl Mitze, and Lisa F. Rau. 1995.
Automatic condensation of electronic publications by
sentence selection. Information Processing & Man-
agement, 31(5):675 ? 685.
Yllias Chali and Sadid A. Hasan. 2012. On the effective-
ness of using sentence compression models for query-
focused multi-document summarization. In COLING,
pages 457?474.
James Clarke and Mirella Lapata. 2008. Global in-
ference for sentence compression: An integer linear
programming approach. J. Artif. Intell. Res. (JAIR),
31:399?429.
John Duchi, Shai Shalev-Shwartz, Yoram Singer, and
Tushar Chandra. 2008. Efficient projections onto the
L1-ball for learning in high dimensions. In Proceed-
ings of ICML, pages 272?279.
Jack Edmonds and Richard M. Karp. 1972. Theoret-
ical improvements in algorithmic efficiency for net-
work flow problems. J. ACM, 19(2):248?264, April.
H. P. Edmundson. 1969. New methods in automatic ex-
tracting. J. ACM, 16(2):264?285, April.
Daniel Freedman and Petros Drineas. 2005. Energy min-
imization via graph cuts: Settling what is possible. In
CVPR (2), pages 939?946.
Dan Gillick and Benoit Favre. 2009. A scalable global
model for summarization. In Proceedings of the Work-
shop on Integer Linear Programming for Natural Lan-
guage Processing, pages 10?18, June.
Dan Gillick, Benoit Favre, and Dilek Hakkani-Tur. 2008.
The ICSI summarization system at tac 2008. In Pro-
ceedings of the Text Understanding Conference.
Matthew R. Gormley and Jason Eisner. 2013. Noncon-
vex global optimization for latent-variable models. In
Proceedings of ACL, pages 444?454, August.
Vladimir Kolmogorov and Ramin Zabih. 2004. What en-
ergy functions can be minimized via graph cuts. IEEE
Transactions on Pattern Analysis and Machine Intelli-
gence, 26:65?81.
Terry Koo, Alexander M. Rush, Michael Collins, Tommi
Jaakkola, and David Sontag. 2010. Dual decomposi-
tion for parsing with non-projective head automata. In
Proceedings of EMNLP 2010, pages 1288?1298, Oc-
tober.
Chen Li, Fei Liu, Fuliang Weng, and Yang Liu. 2013a.
Document summarization via guided sentence com-
pression. In Proceedings of EMNLP (to appear), Oc-
tober.
Chen Li, Xian Qian, and Yang Liu. 2013b. Using super-
vised bigram-based ILP for extractive summarization.
In Proceedings of ACL, pages 1004?1013, August.
Hui Lin and Jeff Bilmes. 2011. A class of submodular
functions for document summarization. In Proceed-
ings of ACL, pages 510?520, June.
Fei Liu and Yang Liu. 2009. From extractive to abstrac-
tive meeting summaries: Can it be done by sentence
compression? In Proceedings of ACL-IJCNLP 2009,
pages 261?264, August.
Andre? F. T. Martins and Noah A. Smith. 2009. Summa-
rization with a joint model for sentence extraction and
compression. In Proceedings of the Workshop on In-
teger Linear Programming for Natural Langauge Pro-
cessing, ILP ?09, pages 1?9.
Kiyohito Nagano, Yoshinobu Kawahara, and Kazuyuk-
i Aihara. 2011. Size-constrained submodular min-
imization through minimum norm base. In ICML,
pages 977?984.
Xian Qian and Yang Liu. 2013. Branch and bound algo-
rithm for dependency parsing with non-local features.
TACL, 1:105?151.
Dragomir R. Radev. 2001. Experiments in single and
multidocument summarization using mead. In In First
Document Understanding Conference.
Kristian Woodsend and Mirella Lapata. 2012. Mul-
tiple aspect summarization using integer linear pro-
gramming. In Proceedings of EMNLP-CoNLL, pages
233?243, July.
1502
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 691?701,
October 25-29, 2014, Doha, Qatar. c?2014 Association for Computational Linguistics
Improving Multi-documents Summarization by Sentence Compression
based on Expanded Constituent Parse Trees
Chen Li1, Yang Liu1, Fei Liu2, Lin Zhao3, Fuliang Weng3
1 Computer Science Department, The University of Texas at Dallas
Richardson, TX 75080, USA
2 School of Computer Science, Carnegie Mellon University
Pittsburgh, PA 15213, USA
3 Research and Technology Center, Robert Bosch LLC
Palo Alto, California 94304, USA
{chenli,yangl@hlt.utdallas.edu}
{feiliu@cs.cmu.edu}
{lin.zhao,fuliang.weng@us.bosch.com}
Abstract
In this paper, we focus on the problem
of using sentence compression techniques
to improve multi-document summariza-
tion. We propose an innovative sentence
compression method by considering every
node in the constituent parse tree and de-
ciding its status ? remove or retain. In-
teger liner programming with discrimina-
tive training is used to solve the problem.
Under this model, we incorporate various
constraints to improve the linguistic qual-
ity of the compressed sentences. Then we
utilize a pipeline summarization frame-
work where sentences are first compressed
by our proposed compression model to ob-
tain top-n candidates and then a sentence
selection module is used to generate the
final summary. Compared with state-of-
the-art algorithms, our model has simi-
lar ROUGE-2 scores but better linguistic
quality on TAC data.
1 Introduction
Automatic summarization can be broadly divided
into two categories: extractive and abstractive
summarization. Extractive summarization focuses
on selecting salient sentences from the document
collection and concatenating them to form a sum-
mary; while abstractive summarization is gener-
ally considered more difficult, involving sophisti-
cated techniques for meaning representation, con-
tent planning, surface realization, etc.
There has been a surge of interest in recent years
on generating compressed document summaries as
a viable step towards abstractive summarization.
These compressive summaries often contain more
information than sentence-based extractive sum-
maries since they can remove insignificant sen-
tence constituents and make space for more salient
information that is otherwise dropped due to the
summary length constraint. Two general strate-
gies have been used for compressive summariza-
tion. One is a pipeline approach, where sentence-
based extractive summarization is followed or pro-
ceeded by sentence compression (Lin, 2003; Zajic
et al., 2007; Vanderwende et al., 2007; Wang et al.,
2013). Another line of work uses joint compres-
sion and summarization. Such methods have been
shown to achieve promising performance (Daume?,
2006; Chali and Hasan, 2012; Almeida and Mar-
tins, 2013; Qian and Liu, 2013), but they are typi-
cally computationally expensive.
In this study, we propose an innovative sen-
tence compression model based on expanded con-
stituent parse trees. Our model uses integer lin-
ear programming (ILP) to search the entire space
of compression, and is discriminatively trained.
It is built based on the discriminative sentence
compression model from (McDonald, 2006) and
(Clarke and Lapata, 2008), but our method uses
an expanded constituent parse tree rather than only
the leaf nodes in previous work. Therefore we
can extract rich features for every node in the con-
stituent parser tree. This is an advantage of tree-
based compression technique (Knight and Marcu,
2000; Galley and McKeown, 2007; Wang et al.,
2013). Similar to (Li et al., 2013a), we use a
pipeline summarization framework where multi-
ple compression candidates are generated for each
pre-selected important sentence, and then an ILP-
691
based summarization model is used to select the
final compressed sentences. We evaluate our pro-
posed method on the TAC 2008 and 2011 data
sets using the standard ROUGE metric (Lin, 2004)
and human evaluation of the linguistic quality.
Our results show that using our proposed sentence
compression model in the summarization system
can yield significant performance gain in linguis-
tic quality, without losing much performance on
the ROUGE metric.
2 Related Work
Summarization research has seen great develop-
ment over the last fifty years (Nenkova and McKe-
own, 2011). Compared to the abstractive counter-
part, extractive summarization has received con-
siderable attention due to its clear problem for-
mulation: to extract a set of salient and non-
redundant sentences from the given document
set. Both unsupervised and supervised approaches
have been explored for sentence selection. Su-
pervised approaches include the Bayesian classi-
fier (Kupiec et al., 1995), maximum entropy (Os-
borne, 2002), skip-chain CRF (Galley, 2006), dis-
criminative reranking (Aker et al., 2010), among
others. The extractive summary sentence selec-
tion problem can also be formulated in an opti-
mization framework. Previous methods include
using integer linear programming (ILP) and sub-
modular functions to solve the optimization prob-
lem (Gillick et al., 2009; Li et al., 2013b; Lin and
Bilmes, 2010).
Compressive summarization receives increas-
ing attention in recent years, since it offers a vi-
able step towards abstractive summarization. The
compressed summaries can be generated through a
joint model of the sentence selection and compres-
sion processes, or through a pipeline approach that
integrates a sentence compression model with a
summary sentence pre-selection or post-selection
step.
Many studies have explored the joint sentence
compression and selection setting. Martins and
Smith (2009) jointly performed sentence extrac-
tion and compression by solving an ILP prob-
lem. Berg-Kirkpatrick et al. (2011) proposed an
approach to score the candidate summaries ac-
cording to a combined linear model of extrac-
tive sentence selection and compression. They
trained the model using a margin-based objec-
tive whose loss captures the final summary qual-
ity. Woodsend and Lapata (2012) presented an-
other method where the summary?s informative-
ness, succinctness, and grammaticality are learned
separately from data but optimized jointly using an
ILP setup. Yoshikawa et al. (2012) incorporated
semantic role information in the ILP model.
Our work is closely related with the pipeline
approach, where sentence-based extractive sum-
marization is followed or proceeded by sentence
compression. There have been many studies on
sentence compression, independent of the summa-
rization task. McDonald (2006) firstly introduced
a discriminative sentence compression model to
directly optimize the quality of the compressed
sentences produced. Clarke and Lapata (2008)
improved the above discriminative model by us-
ing ILP in decoding, making it convenient to
add constraints to preserve grammatical structure.
Nomoto (2007) treated the compression task as
a sequence labeling problem and used CRF for
it. Thadani and McKeown (2013) presented an
approach for discriminative sentence compression
that jointly produces sequential and syntactic rep-
resentations for output text. Filippova and Altun
(2013) presented a method to automatically build
a sentence compression corpus with hundreds of
thousands of instances on which deletion-based
compression algorithms can be trained.
In addition to the work on sentence compres-
sion as a stand-alone task, prior studies have also
investigated compression for the summarization
task. Knight and Marcu (2000) utilized the noisy
channel and decision tree method to perform sen-
tence compression in the summarization task. Lin
(2003) showed that pure syntactic-based compres-
sion may not significantly improve the summariza-
tion performance. Zajic et al. (2007) compared
two sentence compression approaches for multi-
document summarization, including a ?parse-and-
trim? and a noisy-channel approach. Galanis and
Androutsopoulos (2010) used the maximum en-
tropy model to generate the candidate compres-
sions by removing branches from the source sen-
tences. Woodsend and Lapata (2010) presented a
joint content selection and compression model for
single-document summarization. They operated
over a phrase-based representation of the source
document which they obtained by merging infor-
mation from PCFG parse trees and dependency
graphs. Liu and Liu (2013) adopted the CRF-
based sentence compression approach for summa-
692
rizing spoken documents. Unlike the word-based
operation, some of these models e.g (Knight and
Marcu, 2000; Siddharthan et al., 2004; Turner
and Charniak, 2005; Galanis and Androutsopou-
los, 2010; Wang et al., 2013), are tree-based ap-
proaches that operate on the parse trees and thus
the compression decision can be made for a con-
stituent, instead of a single word.
3 Sentence Compression Method
Sentence compression is a task of producing a
summary for a single sentence. The compressed
sentence should be shorter, contain important con-
tent from the original sentence, and be grammat-
ical. In some sense, sentence compression can
be described as a ?scaled down version of the
text summarization problem? (Knight and Marcu,
2002). Here similar to much previous work on
sentence compression, we just focus on how to re-
move/select words in the original sentence without
using operation like rewriting sentence.
3.1 Discriminative Compression Model by
ILP
McDonald (2006) presented a discriminative com-
pression model, and Clarke and Lapata (2008) im-
proved it by using ILP for decoding. Since our
proposed method is based upon this model, in
the following we briefly describe it first. Details
can be found in (Clarke and Lapata, 2008). In
this model, the following score function is used
to evaluate each compression candidate:
s(x, y) =
|y|
?
j=2
s(x, L(y
j?1
), L(y
j
)) (1)
where x = x
1
x
2
, ..., x
n
represents an original sen-
tence and y = y
1
y
2
, ..., y
m
denotes a compressed
sentence. Because the sentence compression prob-
lem is defined as a word deletion task, y
j
must oc-
cur in x. Function L(y
i
) ? [1...n] maps word y
i
in
the compression to the word index in the original
sentence x. Note that L(y
i
) < L(y
i+1
) is required,
that is, each word in x can only occur at most
once in compression y. In this model, a first or-
der Markov assumption is used for the score func-
tion. Decoding this model is to find the combina-
tion of bigrams that maximizes the score function
in Eq (1). Clarke and Lapata (2008) introduced the
following variables and used ILP to solve it:
?
i
=
{
1 if x
i
is in the compression
0 otherwise
?i ? [1..n]
?
i
=
{
1 if x
i
starts the compression
0 otherwise
?i ? [1..n]
?
i
=
{
1 if x
i
ends the compression
0 otherwise
?i ? [1..n]
?
ij
=
{
1 if x
i
, x
j
are in the compression
0 otherwise
?i ? [1..n ? 1]?j ? [i + 1..n]
Using these variables, the objective function can
be defined as:
max z =
n
?
i=1
?
i
? s(x, 0, i)
+
n?1
?
i=1
n
?
j=i+1
?
ij
? s(x, i, j)
+
n
?
i=1
?
i
? s(x, i, n + 1) (2)
The following four basic constraints are used to
make the compressed result reasonable:
n
?
i=1
?
i
= 1 (3)
?
j
? ?
j
?
j
?
i=1
?
ij
= 0 ?j ? [1..n] (4)
?
i
?
n
?
j=i+1
?
ij
? ?
i
= 0 ?i ? [1..n] (5)
n
?
i=1
?
i
= 1 (6)
Formula (3) and (6) denote that exactly one
word can begin or end a sentence. Formula (4)
means if a word is in the compressed sentence, it
must either start the compression or follow another
word; formula (5) represents if a word is in the
693
compressed sentence, it must either end the sen-
tence or be followed by another word.
Furthermore, discriminative models are used for
the score function:
s(x, y) =
|y|
?
j=2
w ? f(x, L(y
j?1
), L(y
j
)) (7)
High dimensional features are used and their cor-
responding weights are trained discriminatively.
Above is the basic supervised ILP formula-
tion for sentence compression. Linguistically and
semantically motivated constraints can be added
in the ILP model to ensure the correct grammar
structure in the compressed sentence. For exam-
ple, Clarke and Lapata (2008) forced the introduc-
ing term of prepositional phrases and subordinate
clauses to be included in the compression if any
word from within that syntactic constituent is also
included, and vice versa.
3.2 Compression Model based on Expanded
Constituent Parse Tree
In the above ILP model, variables are defined for
each word in the sentence, and the task is to pre-
dict each word?s status. In this paper, we propose
to adopt the above ILP framework, but operate di-
rectly on the nodes in the constituent parse tree,
rather than just the words (leaf nodes in the tree).
This way we can remove or retain a chunk of the
sentence rather than isolated words, which we ex-
pect can improve the readability and grammar cor-
rectness of the compressed sentences.
The top part of Fig1 is a standard constituent
parse tree. For some levels of the tree, the nodes
at that same level can not represent a sentence. We
extend the parse tree by duplicating non-POS con-
stituents so that leaf nodes (words and their corre-
sponding POS tags) are aligned at the bottom level
as shown in bottom of as Fig1. In the example tree,
the solid lines represent relationship of nodes from
the original parse tree, the long dot lines denote the
extension of the duplication nodes from the up-
per level to the lower level, and the nodes at the
same level are connected (arrowed lines) to repre-
sent that is a sequence. Based on this expanded
constituent parse tree, we can consider every level
as a ?sentence? and the tokens are POS tags and
parse tree labels. We apply the above compression
model in Section 3.1 on every level to decide every
node?s status in the final compressed sentence. In
order to make the compressed parsed tree reason-
able, we model the relationship of nodes between
PRP/
I 
VBP/ 
am 
DT/  
a 
NN/ 
worker 
IN/ 
from 
NNP/
USA
NP IN 
PRP 
PRP 
PRP 
DT/ 
the
 
NNP/
USA
 
IN/ 
from 
NN/ 
worker 
PP 
NP 
PRP/ 
I 
DT/ 
the
S 
VP NP 
VBP NP 
NP PP 
DT NN 
VBP 
VBP 
S 
VP NP 
VBP/ 
am 
NP 
NP 
DT/ 
a 
Figure 1: A regular constituent parse tree and its
Expanded constituent tree.
adjacent levels as following: if the parent node is
labeled as removed, all of its children will be re-
moved; one node will retain if at least one of its
children is kept.
Therefore, the objective function in the new ILP
formulation is:
max z =
height
?
l=1
(
n
l
?
i=1
?
l
i
? s(x, 0, l
i
)
+
n
l
?1
?
i=1
n
l
?
j=i+1
?
l
ij
? s(x, l
i
, l
j
)
+
n
l
?
i=1
?
l
i
? s(x, l
i
, n
l
+ 1) ) (8)
where height is the depth for a parse tree (starting
from level 1 for the tree), and n
l
means the length
of level l (for example, n
5
= 6 in the example
in Fig1). Then every level will have a set of pa-
rameters ?l
i
, ?
l
i
, ?
l
i
, and ?l
ij
, and the corresponding
constraints as shown in Formula (3) to (6). The re-
lationship between nodes from adjacent levels can
be expressed as:
?
l
i
? ?
(l+1)
j
(9)
?
l
i
?
?
?
(l+1)
j
(10)
in which node j at level (l+1) is the child of node
694
i at level l. In addition, 1 ? l ? height ? 1,
1 ? i ? n
l
and 1 ? j ? n
l+1
.
3.3 Linguistically Motivated Constraints
In our proposed model, we can jointly decide the
status of every node in the constituent parse tree
at the same time. One advantage is that we can
add constraints based on internal nodes or rela-
tionship in the parse tree, rather than only using
the relationship based on words. In addition to
the constraints proposed in (Clarke and Lapata,
2008), we introduce more linguistically motivated
constraints to keep the compressed sentence more
grammatically correct. The following describes
the constraints we used based on the constituent
parse tree.
? If a node?s label is ?SBAR?, its parent?s label
is ?NP? and its first child?s label is ?WHNP? or
?WHPP? or ?IN?, then if we can find a noun
in the left siblings of ?SBAR?, this subordi-
nate clause could be an attributive clause or
appositive clause. Therefore the found noun
node should be included in the compression
if the ?SBAR? is also included, because the
node ?SBAR? decorates the noun. For exam-
ple, the top part of Fig 2 is part of expanded
constituent parse tree of sentence ?Those who
knew David were all dead.? The nodes in el-
lipse should share the same status.
? If a node?s label is ?SBAR?, its parent?s label
is ?VP? and its first child?s label is ?WHNP?,
then if we can find a verb in the left siblings
of ?SBAR?, this subordinate clause could be
an objective clause. Therefore, the found
verb node should be included in the compres-
sion if the ?SBAR? node is also included, be-
cause the node ?SBAR? is the object of that
verb. An example is shown in the bottom part
of Fig 2. The nodes in ellipse should share the
same status.
? If a node?s label is ?SBAR?, its parent?s
label is ?VP? and its first child?s label is
?WHADVP?, then if the first leaf for this node
is a wh-word (e.g., ?where, when, why?) or
?how?, this clause may be an objective clause
(when the word is ?why, how, where?) or at-
tributive clause (when the word is ?where?) or
adverbial clause (when the word is ?when?).
Therefore, similar to above, if a verb or noun
is found in the left siblings of ?SBAR?, the
VBD/ 
knew 
NNP/ 
David 
NP 
DT 
DT 
DT 
VP 
WP 
WP/ 
who 
DT/ 
Those 
VBD 
S 
NP 
PRP/ 
he 
PRP/ 
    I 
VBP/ 
believe  
PRP VBP 
 WP/ 
what 
WHNP S PRP 
 
VBP 
VBD/ 
said 
SBAR 
VP NP 
NP VP WP PRP 
 
VBP 





















NP 
SBAR 
WHNP 
WHNP 
 
S 
Figure 2: Expanded constituent parse tree for ex-
amples.
found verb or noun node should be included
in the compression if the ?SBAR? node is also
included.
? If a node?s label is ?SBAR? and its parent?s la-
bel is ?ADJP?, then if we can find a ?JJ?, ?JJR?,
or ?JJS? in the left siblings of ?SBAR?, the
?SBAR? node should be included in the com-
pression if the found ?JJ?, ?JJR? or ?JJS? node
is also included because the node ?SBAR? is
decorated by the adjective.
? The node with a label of ?PRN? can be re-
moved without other constraints.
We also include some other constraints based on
the Stanford dependency parse tree. Table 1 lists
the dependency relations we considered.
? For type I relations, the parent and child node
with those relationships should have the same
value in the compressed result (both are kept
or removed).
? For type II relations, if the child node in
those relations is retained in the compressed
sentence, the parent node should be also re-
tained.
695
Dependency Relation Example
prt: phrase verb particle They shut down the station. prt(shut,down)
prep: prepositional modifier He lives in a small village. prep(lives,in)
I pobj: object of a preposition I sat on the chair. pobj(on,chair)
nsubj: nominal subject The boy is cute. nsubj(cute,boy)
cop: copula Bill is big. cop(big,is)
partmod: participial modifier Truffles picked during the spring are tasty. partmod(truffles,picked)
II nn: noun compound modifier Oil price futures. nn(futures,oil)
acomp: adjectival complement She looks very beautiful. acomp(looks,beautiful)
pcomp: prepositional complement He felt sad after learning that tragedy. pcomp(after,learning)
III ccomp: clausal complement I am certain that he did it. ccomp(certain,did)
tmod: temporal modifier Last night I swam in the pool. tmod(swam,night)
Table 1: Some dependency relations used for extra constraints. All the examples are from (Marneffe and
Manning, 2002)
? For type III relations, if the parent node in
these relations is retained, the child node
should be kept as well.
3.4 Features
So far we have defined the decoding process
and related constraints used in decoding. These
all rely on the score function s(x, y) = w ?
f(x, L(y
j?1
), L(y
j
)) for every level in the con-
stituent parse tree. We included all the features in-
troduced in (Clarke and Lapata, 2008) (those fea-
tures are designed for leaves). Table 2 lists the
additional features we used in our system.
General Features for Every Node
1. individual node label and concatenation of a pair of
nodes
2. distance of two nodes at the same level
3. is the node at beginning or end at that level?
4. do the two nodes have the same parent?
5. if two nodes do not have the same parent, then is the left
node the rightmost child of its parent? is the right node the
leftmost child of its parent?
6. combination of parent label if the node pair are not
under the same parent
7. number of node?s children: 1/0/>1
8. depth of nodes in the parse tree
Extra Features for Leaf nodes
1. word itself and concatenation of two words
2. POS and concatenation of two words? POS
3. whether the word is a stopword
4. node?s named entity tag
5. dependency relationship between two leaves
Table 2: Features used in our system besides those
used in (Clarke and Lapata, 2008).
3.5 Learning
To learn the feature weights during training, we
perform ILP decoding on every sentence in the
training set, to find the best hypothesis for each
node in the expanded constituent parse tree. If
the hypothesis is incorrect, we update the feature
weights using the structured perceptron learning
strategy (Collins, 2002). The reference label for
every node in the expanded constituent parse tree
is obtained automatically from the bottom to the
top of the tree. Since every leaf node (word) is
human annotated (removed or retain), we annotate
the internal nodes as removed if all of its children
are removed. Otherwise, the node is annotated as
retained.
During perceptron training, a fixed learning rate
is used and parameters are averaged to prevent
overfitting. In our experiment, we observe sta-
ble convergence using the held-out development
corpus, with best performance usually obtained
around 10-20 epochs.
4 Summarization System
Similar to (Li et al., 2013a), our summarization
system is , which consists of three key compo-
nents: an initial sentence pre-selection module
to select some important sentence candidates; the
above compression model to generate n-best com-
pressions for each sentence; and then an ILP sum-
marization method to select the best summary sen-
tences from the multiple compressed sentences.
The sentence pre-selection model is a simple su-
pervised support vector regression (SVR) model
that predicts a salience score for each sentence and
selects the top ranked sentences for further pro-
cessing (compression and summarization). The
target value for each sentence during training is
the ROUGE-2 score between the sentence and the
human written abstracts. We use three common
features: (1) sentence position in the document;
(2) sentence length; and (3) interpolated n-gram
document frequency as introduced in (Ng et al.,
2012).
The final sentence selection process follows the
696
ILP method introduced in (Gillick et al., 2009).
Word bi-grams are used as concepts, and their doc-
ument frequency is used as weights. Since we use
multiple compressions for one sentence, an addi-
tional constraint is used: for each sentence, only
one of its n-best compressions may be included in
the summary.
For the compression module, using the ILP
method described above only finds the best com-
pression result for a given sentence. To generate
n-best compression candidates, we use an iterative
approach ? we add one more constraints to prevent
it from generating the same answer every time af-
ter getting one solution.
5 Experimental Results
5.1 Experimental Setup
Summarization Data For summarization experi-
ments, we use the standard TAC data sets1, which
have been used in the NIST competitions. In par-
ticular, we used the TAC 2010 data set as train-
ing data for the SVR sentence pre-selection model,
TAC 2009 data set as development set for parame-
ter tuning, and the TAC 2008 and 2011 data as the
test set for reporting the final summarization re-
sults. The training data for the sentence compres-
sion module in the summarization system is sum-
mary guided compression corpus annotated by (Li
et al., 2013a) using TAC2010 data. In the com-
pression module, for each word we also used its
document level feature.2
Compression Data We also evaluate our com-
pression model using the data set from (Clarke
and Lapata, 2008). It includes 82 newswire arti-
cles with manually produced compression for each
sentence. We use the same partitions as (Martins
and Smith, 2009), i.e., 1,188 sentences for training
and 441 for testing.
Data Processing We use Stanford CoreNLP
toolkit3 to tokenize the sentences, extract name en-
tity tags, and generate the dependency parse tree.
Berkeley Parser (Petrov et al., 2006) is adopted
to obtain the constituent parse tree for every sen-
tence and POS tag for every token. We use Pocket
1http://www.nist.gov/tac/data/index.html
2Document level features for a word include information
such as the word?s document frequency in a topic. These
features cannot be extracted from a single sentence, as in the
standard sentence compression task, and are related to the
document summarization task.
3http://nlp.stanford.edu/software/corenlp.shtml
CRF4 to implement the CRF sentence compres-
sion model. SVMlight5 is used for the summary
sentence pre-selection model. Gurobi ILP solver6
does all ILP decoding.
5.2 Summarization Results
We compare our summarization system against
four recent studies, which have reported some of
the highest published results on this task. Berg-
Kirkpatrick et al. (2011) introduced a joint model
for sentence extraction and compression. Wood-
send and Lapata (2012) learned individual sum-
mary aspects from data, e.g., informativeness, suc-
cinctness, grammaticalness, stylistic writing con-
ventions, and jointly optimized the outcome in
an ILP framework. Ng et al. (2012) exploited
category-specific information for multi-document
summarization. Almeida and Martins (2013) pro-
posed compressive summarization method by dual
decomposition and multi-task learning. Our sum-
marization framework is the same as (Li et al.,
2013a), except they used a CRF-based compres-
sion model. In addition to the four previous stud-
ies, we also report the best achieved results in the
TAC competitions.
Table 3 shows the summarization results of our
method and others. The top part contains the re-
sults for TAC 2008 data and bottom part is for
TAC 2011 data. We use the ROUGE evaluation
metrics (Lin, 2004), with R-2 measuring the bi-
gram overlap between the system and reference
summaries and R-SU4 measuring the skip-bigram
with the maximum gap length of 4. In addition,
we evaluate the linguistic quality (LQ) of the sum-
maries for our system and (Li et al., 2013a).7 The
linguistic quality consists of two parts. One eval-
uates the grammar quality within a sentence. For
this, annotators marked if a compressed sentence
is grammatically correct. Typical grammar errors
include lack of verb or subordinate clause. The
other evaluates the coherence between sentences,
including the order of sentences and irrelevant sen-
tences. We invited 3 English native speakers to do
this evaluation. They gave every compressed sen-
tence a grammar score and a coherence score for
4http://sourceforge.net/projects/pocket-crf-1/
5http://svmlight.joachims.org/
6http://www.gurobi.com
7We chose to evaluate the linguistic quality for this system
because of two reasons: one is that we have an implementa-
tion of that method; the other more important one is that it
has the highest reported ROUGE results among the compared
methods.
697
System R-2 R-SU4 Gram Cohere
TAC?08 Best System 11.03 13.96 n/a n/a
(Berg-Kirk et al., 2011) 11.70 14.38 n/a n/a
(Woodsend et al., 2012) 11.37 14.47 n/a n/a
(Almeida et al.,2013) 12.30 15.18 n/a n/a
(Li et al., 2013a) 12.35 15.27 3.81 3.41
Our System 12.23 15.47 4.29 4.11
TAC?11 Best System 13.44 16.51 n/a n/a
(Ng et al., 2012) 13.93 16.83 n/a n/a
(Li et al., 2013a) 14.40 16.89 3.67 3.32
Our System 14.04 16.67 4.18 4.07
Table 3: Summarization results on the TAC 2008
and 2011 data sets.
each topic. The score is scaled and ranges from 1
(bad) to 5 (good). Therefore, in table 3, the gram-
mar score is the average score for each sentence
and coherence score is the average for each topic.
We measure annotators? agreement in the follow-
ing way: we consider the scores from each anno-
tator as a distribution and we find that these three
distributions are not statistically significantly dif-
ferent each other (p > 0.05 based on paired t-test).
We can see from the table that in general, our
system achieves better ROUGE results than most
previous work except (Li et al., 2013a) on both
TAC 2008 and TAC 2011 data. However, our
system?s linguistic quality is better than (Li et
al., 2013a). The CRF-based compression model
used in (Li et al., 2013a) can not well model the
grammar. Particularly, our results (ROUGE-2) are
statistically significantly (p < 0.05) higher than
TAC08 Best system, but are not statistically signif-
icant compared with (Li et al., 2013a) (p > 0.05).
The pattern is similar in TAC 2011 data. Our result
(R-2) is statistically significantly (p < 0.05) better
than TAC11 Best system, but not statistically (p >
0.05) significantly different from (Li et al., 2013a).
However, for the grammar and coherence score,
our results are statistically significantly (p < 0.05)
than (Li et al., 2013a). All the above statistics are
based on paired t-test.
5.3 Compression Results
The results above show that our summarization
system is competitive. In this section we focus
on the evaluation of our proposed compression
method. We compare our compression system
against four other models. HedgeTrimmer in Dorr
et al. (2003) applied a variety of linguistically-
motivated heuristics to guide the sentences com-
System C Rate (%) Uni-F1 Rel-F1
HedgeTrimmer 57.64 0.64 0.50
McDonald (2006) 70.95 0.77 0.55
Martins (2009) 71.35 0.77 0.56
Wang (2013) 68.06 0.79 0.59
Our System 71.19 0.77 0.58
Table 4: Sentence compression results. The hu-
man compression rate of the test set is 69%.
pression; McDonald (2006) used the output of two
parsers as features in a discriminative model that
decomposes over pairs of consecutive words; Mar-
tins and Smith (2009) built the compression model
in the dependency parse and utilized the relation-
ship between the head and modifier to preserve the
grammar relationship; Wang et al. (2013) devel-
oped a novel beam search decoder using the tree-
based compression model on the constituent parse
tree, which could find the most probable compres-
sion efficiently.
Table 4 shows the compression results of vari-
ous systems, along with the compression ratio (C
Rate) of the system output. We adopt the com-
pression metrics as used in (Martins and Smith,
2009) that measures the macro F-measure for the
retained unigrams (Uni-F1), and the one used
in (Clarke and Lapata, 2008) that calculates the
F1 score of the grammatical relations labeled by
(Briscoe and Carroll, 2002) (Rel-F1). We can see
that our proposed compression method performs
well, similar to the state-of-the-art systems.
To evaluate the power of using the expanded
parse tree in our model, we conducted another ex-
periment where we only consider the bottom level
of the constituent parse tree. In some sense, this
could be considered as the system in (Clarke and
Lapata, 2008). Furthermore, we use two differ-
ent setups: one uses the lexical features (about the
words) and the other does not. Table 5 shows the
results using the data in (Clarke and Lapata, 2008).
For a comparison, we also include the results us-
ing the CRF-based compression model (the one
used in (Nomoto, 2007; Li et al., 2013a)). We
report results using both the automatically calcu-
lated compression metrics and the linguistic qual-
ity score. Three English native speaker annotators
were asked to judge two aspects of the compressed
sentence compared with the gold result: one is the
content that looks at whether the important words
are kept and the other is the grammar score which
evaluates the sentence?s readability. Each of these
698
two scores ranges from 1(bad) to 5(good).
Table 5 shows that when using lexical features,
our system has statistically significantly (p < 0.05)
higher Grammar value and content importance
value than the CRF and the leaves only system.
When no lexical features are used, default system
can achieve statistically significantly (p < 0.01)
higher results than the CRF and the leaves only
system.
We can see that using the expanded parse tree
performs better than using the leaves only, espe-
cially when lexical features are not used. In ad-
dition, we observe that our proposed compression
method is more generalizable than the CRF-based
model. When our system does not use lexical
features in the leaves, it achieves better perfor-
mance than the CRF-based model. This is impor-
tant since such a model is more robust and may be
used in multiple domains, whereas a model rely-
ing on lexical information may suffer more from
domain mismatch. From the table we can see our
proposed tree based compression method consis-
tently has better linguistic quality. On the other
hand, the CRF compression model is the most
computationally efficient one among these three
compression methods. It is about 200 times faster
than our model using the expanded parse tree. Ta-
ble 6 shows some examples using different meth-
ods.
System C Rate(%) Uni-F1 Rel-F1 Gram Imp
Using lexical features
CRF 79.98 0.80 0.51 3.9 4.0
ILP(I) 80.54 0.79 0.57 4.0 4.2
ILP(II) 79.90 0.80 0.57 4.2 4.4
No lexical features
CRF 77.75 0.78 0.51 3.35 3.5
ILP(I) 77.77 0.78 0.56 3.7 3.9
ILP(II) 77.78 0.80 0.58 4.1 4.2
Table 5: Sentence compression results: effect of
lexical features and expanded parse tree. ILP(I)
represents the system using only bottom nodes in
constituent parse tree. ILP(II) is our system. Imp
means the content importance value.
6 Conclusion
In this paper, we propose a discriminative ILP sen-
tence compression model based on the expanded
constituent parse tree, which aims to improve the
linguistic quality of the compressed sentences in
the summarization task. Linguistically motivated
constraints are incorporated to improve the sen-
tence quality. We conduct experiments on the TAC
Using lexical features
Source:
Apart from drugs, detectives believe money is laun-
dered from a variety of black market deals involving
arms and high technology.
Human compress:
detectives believe money is laundered from a variety of
black market deals.
CRF result :
Apart from drugs detectives believe money is laundered
from a black market deals involving arms and technol-
ogy.
ILP(I) Result:
detectives believe money is laundered from a variety of
black deals involving arms.
ILP(II) Result:
detectives believe money is laundered from black mar-
ket deals.
No lexical features
Source:
Mrs Allan?s son disappeared in May 1989, after a party
during his back packing trip across North America.
Human compress:
Mrs Allan?s son disappeared in 1989, after a party dur-
ing his trip across North America.
CRF result :
Mrs Allan?s son disappeared May 1989, after during his
packing trip across North America.
ILP(I) Result:
Mrs Allan?s son disappeared in May, 1989, after a party
during his packing trip across North America .
ILP(II) Result:
Mrs Allan?s son disappeared in May 1989, after a party
during his trip.
Table 6: Examples of original sentences and their
compressed sentences from different systems.
2008 and 2011 summarization data sets and show
that by incorporating this sentence compression
model, our summarization system can yield signif-
icant performance gain in linguistic quality with-
out losing much ROUGE results. The analysis
of the compression module also demonstrates its
competitiveness, in particular the better linguistic
quality and less reliance on lexical cues.
Acknowledgments
We thank the anonymous reviewers for their de-
tailed and insightful comments on earlier drafts
of this paper. The work is also partially sup-
ported by NSF award IIS-0845484 and DARPA
Contract No. FA8750-13-2-0041. Any opinions,
findings, and conclusions or recommendations ex-
pressed are those of the authors and do not neces-
sarily reflect the views of the funding agencies.
699
References
Ahmet Aker, Trevor Cohn, and Robert Gaizauskas.
2010. Multi-document summarization using a*
search and discriminative training. In Proceedings
of EMNLP.
Miguel B. Almeida and Andre F. T. Martins. 2013.
Fast and robust compressive summarization with
dual decomposition and multi-task learning. In Pro-
ceedings of ACL.
Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein.
2011. Jointly learning to extract and compress. In
Proceedings of ACL.
Ted Briscoe and John Carroll. 2002. Robust accurate
statistical annotation of general text. In Proceedings
of LREC.
Yllias Chali and Sadid A. Hasan. 2012. On the effec-
tiveness of using sentence compression models for
query-focused multi-document summarization. In
Proceedings of COLING.
James Clarke and Mirella Lapata. 2008. Global in-
ference for sentence compression an integer linear
programming approach. Journal of Artificial Intelli-
gence Research.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of EMNLP.
Hal Daume?. 2006. Practical structured learning tech-
niques for natural language processing. Ph.D. the-
sis, University of Southern California.
Bonnie Dorr, David Zajic, and Richard Schwartz.
2003. Hedge trimmer: A parse-and-trim approach
to headline generation. In Proceedings of NAACL.
Katja Filippova and Yasemin Altun. 2013. Overcom-
ing the lack of parallel data in sentence compression.
In Proceedings of EMNLP.
Dimitrios Galanis and Ion Androutsopoulos. 2010. An
extractive supervised two-stage method for sentence
compression. In Proceedings of NAACL.
Michel Galley and Kathleen McKeown. 2007. Lexi-
calized markov grammars for sentence compression.
In Processings of NAACL.
Michel Galley. 2006. A skip-chain conditional random
field for ranking meeting utterances by importance.
In Proceedings of EMNLP.
Dan Gillick, Benoit Favre, Dilek Hakkani-Tur, Berndt
Bohnet, Yang Liu, and Shasha Xie. 2009. The
icsi/utd summarization system at tac 2009. In Pro-
ceedings of TAC.
Kevin Knight and Daniel Marcu. 2000. Statistics-
based summarization - step one: Sentence compres-
sion. In Proceedings of AAAI.
Kevin Knight and Daniel Marcu. 2002. Summariza-
tion beyond sentence extraction: A probabilistic ap-
proach to sentence compression. Artificial Intelli-
gence, 139(1):91?107.
Julian Kupiec, Jan Pedersen, and Francine Chen. 1995.
A trainable document summarizer. In Proceedings
of SIGIR.
Chen Li, Fei Liu, Fuliang Weng, and Yang Liu. 2013a.
Document summarization via guided sentence com-
pression. In Proceedings of the EMNLP.
Chen Li, Xian Qian, and Yang Liu. 2013b. Using su-
pervised bigram-based ilp for extractive summariza-
tion. In Proceedings of ACL.
Hui Lin and Jeff Bilmes. 2010. Multi-document sum-
marization via budgeted maximization of submodu-
lar functions. In Proceedings of NAACL.
Chin-Yew Lin. 2003. Improving summarization per-
formance by sentence compression - A pilot study.
In Proceeding of the Sixth International Workshop
on Information Retrieval with Asian Language.
Chin-Yew Lin. 2004. Rouge: a package for automatic
evaluation of summaries. In Proceedings of ACL.
Fei Liu and Yang Liu. 2013. Towards abstractive
speech summarization: Exploring unsupervised and
supervised approaches for spoken utterance com-
pression. IEEE Transactions on Audio, Speech, and
Language Processing.
Marie-Catherine de Marneffe and Christopher D Man-
ning. 2002. Stanford typed dependencies manual.
Andre F. T. Martins and Noah A. Smith. 2009. Sum-
marization with a joint model for sentence extrac-
tion and compression. In Proceedings of the ACL
Workshop on Integer Linear Programming for Natu-
ral Language Processing.
Ryan McDonald. 2006. Discriminative sentence com-
pression with soft syntactic evidence. In Proceed-
ings of EACL.
Ani Nenkova and Kathleen McKeown. 2011. Auto-
matic summarization. Foundations and Trends in
Information Retrieval.
Jun-Ping Ng, Praveen Bysani, Ziheng Lin, Min-Yen
Kan, and Chew-Lim Tan. 2012. Exploiting
category-specific information for multi-document
summarization. In Proceedings of COLING.
Tadashi Nomoto. 2007. Discriminative sentence com-
pression with conditional random fields. Informa-
tion Processing and Management.
Miles Osborne. 2002. Using maximum entropy for
sentence extraction. In Proceedings of the ACL-02
Workshop on Automatic Summarization.
700
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and
interpretable tree annotation. In Proceedings of
COLING-ACL.
Xian Qian and Yang Liu. 2013. Fast joint compression
and summarization via graph cuts. In Proceedings
of EMNLP.
Advaith Siddharthan, Ani Nenkova, and Kathleen
McKeown. 2004. Syntactic simplification for im-
proving content selection in multi-document sum-
marization. In Proceedings of Coling.
Kapil Thadani and Kathleen McKeown. 2013. Sen-
tence compression with joint structural inference. In
Proceedings of CoNLL.
Jenine Turner and Eugene Charniak. 2005. Super-
vised and unsupervised learning for sentence com-
pression. In Proceedings of ACL.
Lucy Vanderwende, Hisami Suzuki, Chris Brockett,
and Ani Nenkova. 2007. Beyond sumbasic: Task-
focused summarization with sentence simplification
and lexical expansion. Information Processing &
Management.
Lu Wang, Hema Raghavan, Vittorio Castelli, Radu Flo-
rian, and Claire Cardie. 2013. A sentence com-
pression based framework to query-focused multi-
document summarization. In Proceedings of ACL.
Kristian Woodsend and Mirella Lapata. 2010. Auto-
matic generation of story highlights. In Proceedings
of ACL.
Kristian Woodsend and Mirella Lapata. 2012. Mul-
tiple aspect summarization using integer linear pro-
gramming. In Proceedings of EMNLP-CoNLL.
Katsumasa Yoshikawa, Tsutomu Hirao, Ryu Iida, and
Manabu Okumura. 2012. Sentence compression
with semantic role constraints. In Proceedings of
ACL.
David Zajic, Bonnie J. Dorr, Jimmy Lin, and Richard
Schwartz. 2007. Multi-candidate reduction: Sen-
tence compression as a tool for document summa-
rization tasks. In Information Processing and Man-
agement.
701
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 367?376,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
User Participation Prediction in Online Forums
Zhonghua Qu and Yang Liu
The University of Texas at Dallas
{qzh,yangl@hlt.utdallas.edu}
Abstract
Online community is an important source
for latest news and information. Accurate
prediction of a user?s interest can help pro-
vide better user experience. In this paper,
we develop a recommendation system for
online forums. There are a lot of differ-
ences between online forums and formal me-
dia. For example, content generated by users
in online forums contains more noise com-
pared to formal documents. Content topics
in the same forum are more focused than
sources like news websites. Some of these
differences present challenges to traditional
word-based user profiling and recommenda-
tion systems, but some also provide oppor-
tunities for better recommendation perfor-
mance. In our recommendation system, we
propose to (a) use latent topics to interpo-
late with content-based recommendation; (b)
model latent user groups to utilize informa-
tion from other users. We have collected
three types of forum data sets. Our experi-
mental results demonstrate that our proposed
hybrid approach works well in all three types
of forums.
1 Introduction
Internet is an important source of information. It
has become a habit of many people to go to the in-
ternet for latest news and updates. However, not all
articles are equally interesting for different users.
In order to intelligently predict interesting articles
for individual users, personalized news recommen-
dation systems have been developed. There are in
general two types of approaches upon which rec-
ommendation systems are built. Content based rec-
ommendation systems use the textual information
of news articles and user generated content to rank
items. Collaborative filtering, on the other hand,
uses co-occurrence information from a collection
of users for recommendation.
During the past few years, online community
has become a large part of internet. More often,
latest information and knowledge appear at on-
line community earlier than other formal media.
This makes it a favorable place for people seeking
timely update and latest information. Online com-
munity sites appear in many forms, for example,
online forums, blogs, and social networking web-
sites. Here we focus our study on online forums. It
is very helpful to build an automatic system to sug-
gest latest information a user would be interested
in. However, unlike formal news media, user gen-
erated content in forums is usually less organized
and not well formed. This presents a great chal-
lenge to many existing news article recommenda-
tion systems. In addition, what makes online fo-
rums different from other media is that users of
online communities are not only the information
consumers but also active providers as participants.
Therefore in this study we develop a recommen-
dation system to account for these characteristics
of forums. We propose several improvements over
previous work:
? Latent topic interpolation: This is to address
the issue with the word-based content repre-
sentation. In this paper we used Latent Dirich-
let Allocation (LDA), a generative multino-
mial mixture model, for topic inference inside
threads. We build a system based on words
367
and latent topics, and linearly interpolate their
results.
? User modeling: We model users? participa-
tion inside threads as latent user groups. Each
latent group is a multinomial distribution on
users. Then LDA is used to infer the group
mixture inside each thread, based on which
the probability of a user?s participation can be
derived.
? Hybrid system: Since content and user-
based methods rely on different information
sources, we combine the results from them for
further improvement.
We have evaluated our proposed method using
three data sets collected from three representative
forums. Our experimental results show that in all
forums, by using latent topics information, system
can achieve better accuracy in predicting threads
for recommendation. In addition, by modeling la-
tent user groups in thread participation, further im-
provement is achieved in the hybrid system. Our
analysis also showed that each forum has its nature,
resulting in different optimal parameters in the dif-
ferent forums.
2 Related Work
Recommendation systems can help make informa-
tion retrieving process more intelligent. Generally,
recommendation methods are categorized into two
types (Adomavicius and Tuzhilin, 2005), content-
based filtering and collaborative filtering.
Systems using content-based filtering use the
content information of recommendation items a
user is interested in to recommend new items to
the user. For example, in a news recommendation
system, in order to recommend appropriate news
articles to a user, it finds the most prominent fea-
tures (e.g., key words, tags, category) in the docu-
ment that a user likes, then suggests similar articles
based on this ?personal profile?. In Fabs system
(Balabanovic and Shoham, 1997), Skyskill & We-
bert system (Pazzani et al 1997), documents are
represented using a set of most important words
according to a weighting measure. The most popu-
lar measure of word ?importance? is TF-IDF (term
frequency, inverse document frequency) (Salton
and Buckley, 1988), which gives weights to words
according to its ?informativeness?. Then, base on
this ?personal profile? a ranking machine is applied
to give a ranked recommendation list. In Fabs sys-
tem, Rocchio? algorithm (Rocchio, 1971) is used
to learn the average TF-IDF vector of highly rated
documents. Skyskill & Webert?s system uses Naive
Bayes classifiers to give the probability of docu-
ments being liked. Winnow?s algorithm (Little-
stone, 1988), which is similar to perception algo-
rithm, has been shown to perform well when there
are many features. An adaptive framework is intro-
duced in (Li et al 2010) using forum comments
for news recommendation. In (Wu et al 2010),
a topic-specific topic flow model is introduced to
rank the likelihood of user participating in a thread
in online forums.
Collaborative-filtering based systems, unlike
content-based systems, predict the recommending
items using co-occurrence information between
users. For example, in a news recommendation
system, in order to recommend an article to user
c, the system tries to find users with similar taste
as c. Items favored by similar users would be rec-
ommended. Grundy (Rich, 1979) is known to be
one of the first collaborative-filtering based sys-
tems. Collaborative filtering systems can be ei-
ther model based or memory based (Breese et al
1998). Memory-based algorithms, such as (Del-
gado and Ishii, 1999; Nakamura and Abe, 1998;
Shardanand and Maes, 1995), use a utility function
to measure the similarity between users. Then rec-
ommendation of an item is made according to the
sum of the utility values of active users that partic-
ipate in it. Model-based algorithms, on the other
hand, try to formulate the probability function of
one item being liked statistically using active user
information. (Ungar et al 1998) clustered sim-
ilar users into groups for recommendation. Dif-
ferent clustering methods have been experimented,
including K-means and Gibbs Sampling. Other
probabilistic models have also been used to model
collaborative relationships, including a Bayesian
model (Chien and George, 1999), linear regres-
sion model (Sarwar et al 2001), Gaussian mix-
ture models (Hofmann, 2003; Hofmann, 2004). In
(Blei et al 2001) a collaborative filtering appli-
cation is discussed using LDA. However in this
model, re-estimation of parameters for the whole
system is needed when a new item comes in. In
368
this paper, we formulate users? participation differ-
ently using the LDA mixture model.
Some previous work has also evaluated using
a hybrid model with both content and collabora-
tive features and showed outstanding performance.
For example, in (Basu et al 1998), hybrid features
are used to make recommendation using inductive
learning.
3 Forum Data
We have collected data from three forums in this
study.1 Ubuntu community forum is a technical
support forum; World of Warcraft (WoW) forum is
about gaming; Fitness forum is about how to live
a healthy life. These three forums are quite rep-
resentative of online forums on the internet. Us-
ing three different types of forums for task eval-
uation helps to demonstrate the robustness of our
proposed method. In addition, it can show how the
same method could have substantial performance
difference on forums of different nature. Users?
behaviors in these three forums are very differ-
ent. Casual forums like ?Wow gaming? have much
more posts in each thread. However its posts are
the shortest in length. This is because discussions
inside these types of forums are more like casual
conversation, and there is not much requirement
on the user?s background, and thus there is more
user participation. In contrast, technical forums
like ?Ubuntu? have fewer average posts in each
thread, and have the longest post length. This is
because a Question and Answer (QA) forum tends
to be very goal oriented. If a user finds the thread
is unrelated, then there will be no motivation for
participation.
Inside forums, different boards are created to
categorize the topics allowed for discussion. From
the data we find that users tend to participate in a
few selected boards of their choices. To create a
data set for user interest prediction in this study,
we pick the most popular boards in each forum.
Even within the same board, users tend to partici-
pate in different threads base on their interest. We
use a user?s participation information as an indica-
tion whether a thread is interesting to a user or not.
Hence, our task is to predict the user participation
in forum threads. Note this approach could intro-
1Please contact the authors to obtain the data.
duce some bias toward negative instances in terms
of user interests. A users? absence from a thread
does not necessarily mean the user is not interested
in that thread; it may be a result of the user being
offline by that time or the thread is too behind in
pages. As a matter of fact, we found most users
read only the threads on the first page during their
time of visit of a forum. This makes participation
prediction an even harder task than interest predic-
tion.
In online forums, threads are ordered by the time
stamp of their last participating post. Provided with
the time stamp for each post, we can calculate the
order of a thread on its board during a user?s par-
ticipation. Figure 1 shows the distribution of post
location during users? participation. We found that
most of the users read only the posts on the first
page. In order to minimize the false negative in-
stances from the data set, we did thread location
filtering. That is, we want to filter out messages
that actually interest the user but do not have the
user?s participation because they are not on the first
page. For any user, only those threads appearing in
the first 10 entries on a page during a user?s visit
are included in the data set.
Figure 1: Thread position during users? participation.
In the pre-processing step of the experiment, first
we use online status filtering discussed above to
remove threads that a user does not see while of-
fline. The statistics of the boards we have used in
each forum are shown in Table 1. The statistics
are consistent with the full forum statistics. For
example, users in technical forums tend to post
less than casual forums. We define active users as
those who have participated in 10 or more threads.
Column ?Part. @300? shows the average number
369
of threads the top 300 users have participated in.
?Filt. Threads@300? shows the average number of
threads after using online filtering with a window
of 10. Thread participation in ?Ubuntu? forum is
very sparse for each user, having only 10.01% par-
ticipating threads for each user after filtering. ?Fit-
ness? and ?Wow Forum? have denser participation,
at 18.97% and 13.86% respectively.
4 Interesting Thread Prediction
In the task of interesting thread prediction, the sys-
tem generates a ranked list of threads a user is
likely to be interested in based on users? past his-
tory of thread participation. Here, instead of pre-
dicting the true interestedness, we predict the par-
ticipation of the user, which is a sufficient condi-
tion for interestedness. This approach is also used
by (Wu et al 2010) for their task evaluation. In
this section, we describe our proposed approaches
for thread participation prediction.
4.1 Content-based Filtering
In the content-based filtering approach, only con-
tent of a thread is used as features for prediction.
Recommendation through content-based filtering
has its deep root in information retrieval. Here we
use a Naive Bayes classifier for ranking the threads
using information based on the words and the la-
tent topic analysis.
4.1.1 Naive Bayes Classification
In (Pazzani et al 1997) Naive Bayesian classi-
fier showed outstanding performance in web page
recommendation compared to several other clas-
sifiers. A Naive Bayes classifier is a generative
model in which words inside a document are as-
sumed to be conditionally independent. That is,
given the class of a document, words are generated
independently. The posterior probability of a test
instance in Naive Bayes classifier takes the follow-
ing form:
P (Ci|f1..k) =
1
Z
P (Ci)
?
j
P (fj |Ci) (1)
where Z is the class label independent normaliza-
tion term, f1..k is the bag-of-word feature vector
for the document. Naive Bayes classifier is known
for not having a well calibrated posterior probabil-
ity (Bennett, 2000). (Pavlov et al 2004) showed
that normalization by document length yielded
good empirical results in approximating a well cal-
ibrated posterior probability for Naive Bayes clas-
sifier. The normalized Naive Bayes classifier they
used is as follows:
P (Ci|f1..k) =
1
Z
P (Ci)
?
j
P (fj |Ci)
1
|f | (2)
In this equation, the probability of generat-
ing each word is normalized by the length of
the feature vector |f |. The posterior probabil-
ity P (interested|f1..k) from (normalized) Naive
Bayes classifier is used for recommendation item
ranking.
4.1.2 Latent Topics based Interpolation
Because of noisy forum writing and limited
training data, the above bag-of-word model used in
naive Bayes classifier may suffer from data sparsity
issues. We thus propose to use latent topic model-
ing to alleviate this problem. Latent Dirichlet Allo-
cation (LDA) is a generative model based on latent
topics. The major difference between LDA and
previous methods such as probabilistic Latent Se-
mantic Analysis (pLSA) is that LDA can efficiently
infer topic composition of new documents, regard-
less of the training data size (Blei et al 2001). This
makes it ideal for efficiently reducing the dimen-
sion of incoming documents.
In an online forum, words contained in threads
tend to be very noisy. Irregular words, such as
abbreviation, misspelling and synonyms, are very
common in an online environment. From our ex-
periments, we observe that LDA seems to be quite
robust to these phenomena and able to capture
word relationship semantically. To illustrate the
words inside latent topics in the LDA model in-
ferred from online forums, we show in Table 2 the
top words in 3 out of 20 latent topics inferred from
?Ubuntu? forum according to its multinomial dis-
tribution. We can see that variations of the same
words are grouped into the same topic.
Since each post could be very short and LDA is
generally known not to work well with short docu-
ments, we concatenated the content of posts inside
each thread to form documents. In order to build
a valid evaluation configuration, only posts before
the first time the testing user participated are used
for model fitting and inference.
370
Forum Name Threads Posts Active Users Part. @300 Filt. Threads @300
Ubuntu 185,747 940,230 1,700 464.72 4641.25
Fitness 27,250 529,201 2,808 613.15 3231.04
Wow Gaming 34,187 1,639,720 19,173 313.77 2264.46
Table 1: Data statistics after filtering.
Topic 1 Topic 2 Topic 3
lol?d wine email
lol. Wine mail
imo. game Thunderbird
,? fixme evolution
-, stub send
lulz. not emails
lmao. WINE gmail
rofl. play postfix
Table 2: Example of LDA topics that capture words
with different variations.
After model fitting for LDA, the topic distri-
butions on new threads can be inferred using the
model. Compared to the original bag-of-word fea-
ture vector, the topic distribution vector is not only
more robust against noise, but also closer to hu-
man interpretation of words. For example in topic
3 in Table 2, people who care about ?Thunder-
bird?, an email client, are also very likely to show
interest in ?postfix?, which is a Linux email ser-
vice. These closely related words, however, might
not be captured using the bag-of-word model since
that would require the exact words to appear in the
training set.
In order to take advantage of the topic level in-
formation while not losing the ?fine-grained? word
level feature, we use the topic distribution as ad-
ditional features in combination with the bag-of-
word features. To tune the contribution of topic
level features in classifiers like Naive Bayes clas-
sifiers, we normalize the topic level feature to a
length of Lt = ?|f | and bag-of-word feature to
Lw = (1??)|f |. ? is a tuning parameter from 0 to
1 that determines the proportion of the topic infor-
mation used in the features. |f | is from the original
bag-of-word feature vector. The final feature vec-
tor for each thread can be represented as:
F = Lww1, ..., Lwwk ? Lt?1, ..., Lt?T (3)
where ?1, ..., ?t is the multinomial distribution of
topics for the thread.
4.2 Collaborative Filtering
Collaborative filtering techniques make prediction
using information from similar users. It has ad-
vantages over content-based filtering in that it can
correctly predict items that are vastly different in
content but similar in concepts indicated by users?
participation.
In some previous work, clustering methods were
used to partition users into several groups, Then,
predictions were made using information from
users in the same group. However, in the case
of thread recommendation, we found that users?
interest does not form clean clusters. Figure 2
shows the mutual information between users after
doing an average-link clustering on their pairwise
mutual information. In a clean clustering, intra-
cluster mutual information should be high, while
inter-cluster mutual information is very low. If so,
we would expect that the figure shows clear rect-
angles along the diagonal. Unfortunately, from this
figure it appears that users far away in the hierarchy
tree still have a lot of common thread participation.
Here, we propose to model user similarity based on
latent user groups.
4.2.1 Latent User Groups
In this paper, we model users? participation in-
side threads as an LDA generative model. We
model each user group as a multinomial distribu-
tion. Users inside each group are assumed to have
common interests in certain topic(s). A thread in an
online forum typically contains several such top-
ics. We could model a user?s participation in a
thread as a mixture of several different user groups.
Since one thread typically attracts a subset of user
groups, it is reasonable to add a Dirichlet prior on
the user group mixture.
The generative process is the same as the LDA
used above for topic modeling, except now users
371
Figure 2: Mutual information between users in Average
Link Hierarchical clustering.
are ?words? and user groups are ?topics?. Using
LDA to model user participation can be viewed
as soft-clustering of users in a sense that one user
could appear in multiple groups at the same time.
The generative process for participating users is as
follows.
1. Choose ? ? Dir(?)
2. For each of N participating users, un:
(a) Choose a group zn ?Multinomial(?)
(b) Choose a user un ? p(un|zn)
One thing worth noting is that in LDA model a
document is assumed to consist of many words. In
the case of modeling user participation, a thread
typically has far fewer users than words inside a
document. This could potentially cause problem
during variable estimation and inference. How-
ever, we show that this approach actually works
well in practice (experimental results in Section 5).
4.2.2 Using Latent User Groups for
Prediction
For an incoming new thread, first the latent
group distribution is inferred using collapsed Gibbs
Sampling (Griffiths and Steyvers, 2004). The pos-
terior probability of a user ui participating in thread
j given the user group distribution is as follows.
P (ui|?j , ?) =
?
k?T
P (ui|?k)P (k|?j) (4)
In the equation, ?k is the multinomial distribution
of users in group k, T is the number of latent user
groups, and ?j is the group composition in thread
j after inference using the training data. In gen-
eral, the probability of user ui appearing in thread
j is proportional to the membership probabilities
of this user in the groups that compose the partici-
pating users.
4.3 Hybrid System
Up to this point we have two separate systems that
can generate ranked recommendation lists based on
different factors of threads. In order to generate the
final ranked list, we give each item a score accord-
ing to the ranked lists from the two systems. Then
the two scores are linearly interpolated using a tun-
ing parameter ? as shown in Equation 5. The final
ranked list is generated accordingly.
Ci =(1? ?)Scorecontent
+ ?Scorecollaborative
(5)
We propose several different rescoring methods
to generate the scores in the above formula for the
two individual systems.
? Posterior: The posterior probabilities of each
item from the two systems are used directly as
the score.
Scoredir = p(clike|itemi) (6)
This way the confidence of ?how likely? an
item is interesting is preserved. However,
the downside is that the two different sys-
tems have different calibration on its posterior
probability, which could be problematic when
directly adding them together.
? Linear rescore: To counter the problem asso-
ciated with posterior probability calibration,
we use linear rescoring based on the ranked
list:
Scorelin = 1?
posi
N
(7)
In the formula, posi is the position of item i
in the ranked list, and N is the total number
of items being ranked. The resulting score is
between 0 and 1, 1 being the first item on the
list and 0 being the last.
? Sigmoid rescore: In a ranked list, usually
items on the top and bottom of the list have
372
higher confidence than those in the middle.
That is to say more ?emphasis? should be put
on both ends of the list. Hence we use a sig-
moid function on the Scorelinear to capture
this.
Scoresig =
1
1 + e?l(Scorelin?0.5)
(8)
A sigmoid function is relatively flat on both
ends while being steep in the middle. In the
equation, l is a tuning parameter that decides
how ?flat? the score of both ends of the list is
going to be. Determining the best value for l
is not a trivial problem. Here we empirically
assign l = 10.
5 Experiment and Evaluation
In this section, we evaluate our approach empiri-
cally on the three forum data sets described in Sec-
tion 3. We pick the top 300 most active users from
each forum for the evaluation. Among the 300
users, 100 of them are randomly selected as the de-
velopment set for parameter tuning, while the rest
is test set. All the data sets are filtered using an on-
line filter as previously described, with a window
size of 10 threads.
Threads are tokenized into words and filtered us-
ing a simple English stop word list. All words
are then ordered by their occurrences multiplied by
their inverse document frequencies (IDF).
idfw = log
|D|
|{d : w ? d}|
(9)
The top 4,000 words from this list are then used to
form the vocabulary.
We used standard mean average precision
(MAP) as the evaluation metric. This standard in-
formation retrieval evaluation metric measures the
quality of the returned rank lists from a system.
Entries higher in the rank are more accurate than
lower ones. For an interesting thread recommenda-
tion system, it is preferable to provide a short and
high-quality list of recommendation; therefore, in-
stead of reporting full-range MAP, we report MAP
on top 10 relevant threads (MAP@10). The reason
why we picked 10 as the number of relevant doc-
ument for MAP evaluation is that users might not
have time to read too many posts, even if they are
relevant.
During evaluation, a 3-fold cross-validation is
performed for each user in the test set. In each fold,
MAP@10 score is calculated from the ranked list
generated by the system. Then the average from all
the folds and all the users is computed as the final
result.
To make a proper evaluation configuration, for
each user, only posts up to the first participation of
the testing user are used for the test set.
5.1 Content-based Results
Here we evaluate the performance of interest
thread prediction using only features from text.
First we use the ranking model with latent topic
information only on the development set to deter-
mine an optimal number of topics. Empirically,
we use hyper parameter ? = 0.1 and ? = 1/K
(K is the number of topics). We use the perfor-
mance of content-based recommendation directly
to determine the optimal topic number K. We var-
ied the latent topic number K from 10 to 100, and
found that the best performance was achieved us-
ing 30 topics in all three forums. Hence we use
K = 30 for content based recommendation unless
otherwise specified.
Next, we show how topic information can help
content-based recommendation achieve better re-
sults. We tune the parameter ? described in Sec-
tion 4.1.2 and show corresponding performances.
We compare the performance using Naive Bayes
classifier, before and after normalization. The
MAP@10 results on the test set are shown in Fig-
ure 3 for three forums. When ? = 0, no latent topic
information is used, and when ? = 1, latent topics
are used without any word features.
When using Naive Bayes classifier without nor-
malization, we find relatively larger performance
gain from adding topic information for the ? val-
ues of close to 0. This phenomenon is probably
because of the poor posterior probabilities of the
Naive Bayes classifier, which are close to either 1
or 0.
For normalized Naive Bayes classifier, interpo-
lating with latent topics based ranking yields per-
formance improvement compared to word-based
results consistently for the three forums. In
?Wow Gaming? corpus, the optimal performance
is achieved with a relatively high ? value (at around
0.5), and it is even higher for the ?Fitness? forum.
373
This means that the system relies more on the la-
tent topics information. This is because in these fo-
rums, casual conversation contains more irregular
words, causing more severe data sparsity problem
than others.
Between the two naive Bayes classifiers, we
can see that using normalized probabilities out-
performs the original one in ?Wow Gaming? and
?Ubuntu? forums. This observation is consistent
with previous work (e.g., (Pavlov et al 2004)).
However, we found that in ?Fitness Forum?, the
performance degrades with normalization. Further
work is still needed to understand why this is the
case.
5.2 Latent User Group Classification
In this section, collaborative filtering using latent
user groups is evaluated. First, participating users
from the training set are used to estimate an LDA
model. Then, users participating in a thread are
used to infer the topic distribution of the thread.
Candidate threads are then sorted by the proba-
bility of a target user?s participation according to
Equation 4. Note that all the users in the forum are
used to estimate the latent user groups, but only the
top 300 active users are used in evaluation. Here,
we vary the number of latent user groups G from
5 to 100. Hyper parameters were set empirically:
? = 1/G, ? = 0.1.
Figure 4 shows the MAP@10 results using dif-
ferent numbers of latent groups for the three fo-
rums. We compare the performance using latent
groups with a baseline using SVM ranking. In
the baseline system, users? participation in a thread
is used as a binary feature. LibSVM with radius
based function (RBF) kernel is used to estimate the
probability of a user?s participation.
From the results, we find that ranking using la-
tent groups information outperforms the baseline
in almost all non-trivial cases. In the case of
?Ubuntu? forum, the performance gain is less com-
pared to other forums. We believe this is because
in this technical support forum, the average user
participation in threads is much less, thus making
it hard to infer a reliable group distribution in a
thread. In addition, the optimal number of user
groups differs greatly between ?Fitness? forum and
?Wow Gaming? forum. We conjecture the reason
behind this is that in the ?Fitness? forum, users
XVHU

Z
R
U
G
Figure 5: Position of items with different #users and
#words in a ranked list. (red=0 being higher on the
ranked list and green being lower)
may be interested in a larger variety of topics and
thus the user distribution in different topics is not
very obvious. In contrast, people in the gaming
forum are more specific to the topics they are inter-
ested in.
It is known that LDA tends to perform poorly
when there are too few words/users. To have a
general idea of how much user participation is
?enough? for decent prediction, we show a graph
(Figure 5) depicting the relationships among the
number of users, the number of words, and the po-
sition of the positive instances in the ranked lists.
In this graph, every dot is a positive thread instance
in ?Wow Gaming? forum. Red color shows that
the positive thread is indeed getting higher ranks
than others. We observe that threads with around
16 participants can already achieve a decent perfor-
mance.
5.3 Hybrid System Performance
In this section, we evaluate the performance of the
hybrid system output. Parameters used in each fo-
rum data set are the optimal parameters found in
the previous sections. Here we show the effect of
the tuning parameter ? (described in Section 4.3).
Also, we compare three different scoring schemes
used to generate the final ranked list. Performance
of the hybrid system is shown in Table 3.
We can see that the combination of the two sys-
tems always outperforms any one model alone.
374
 0.36
 0.39
 0.42
 0.45
 0.48
 0.51
 0.54
 0  0.2  0.4  0.6  0.8  1
MA
P 
10
Gamma
Ubuntu Forum
Naive Bayes
Normalized NB
 0.2
 0.22
 0.24
 0.26
 0.28
 0.3
 0  0.2  0.4  0.6  0.8  1
MA
P 
10
Gamma
Wow Gaming
Naive Bayes
Normalized NB
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0  0.2  0.4  0.6  0.8  1
MA
P 
10
Gamma
Fitness Forum
Naive Bayes
Normalized NB
Figure 3: Content-based filtering results: MAP@10 vs. ? (contribution of topic-based features).
 0.14
 0.16
 0.18
 0.2
 0.22
 1  10  100
MA
P 
10
Number of Groups
Ubuntu Forum
Latent Group
SVM
 0.15
 0.2
 0.25
 0.3
 0.35
 1  10  100
MA
P 
10
Number of Groups
Wow Gaming
Latent Group
SVM
 0.2
 0.3
 0.4
 0.5
 0.6
 1  10  100
MA
P 
10
Number of Groups
Fitness Forum
Latent Group
SVM
Figure 4: Collaborative filtering results: MAP@10 vs. user group number.
Forum
Contribution Factor ?
0.0 1.0 Optimal
Ubuntu 0.523 0.198 0.534 (? = 0.9)
Wow 0.278 0.283 0.304 (? = 0.1)
Fitness 0.545 0.457 0.551 (? = 0.85)
Table 3: Performance of the hybrid system with differ-
ent ? values.
This is intuitive since the two models use differ-
ent information sources. A MAP@10 score of 0.5
means that around half of the suggested results do
have user participation. We think this is a good re-
sult considering that this is not a trivial task.
We also notice that based on the nature of differ-
ent forums, the optimal ? value could be substan-
tially different. For example, in ?Wow gaming?
forum where people participate in more threads, a
higher ? value is observed which favors collabo-
rative filtering score. In contrast, in ?Ubuntu? fo-
rum, where people participate in far fewer threads,
the content-based system is more reliable in thread
prediction, hence a lower ? is used. This observa-
tion also shows that the hybrid system is more ro-
bust against differences among forums compared
with single model systems.
6 Conclusion
In this paper, we proposed a new system that can
intelligently recommend threads from online com-
munity according to a user?s interest. The system
uses both content-based filtering and collaborative-
filtering techniques. In content-based filtering, we
solve the problem of data sparsity in online con-
tent by smoothing using latent topic information.
In collaborative filtering, we model users? partici-
pation in threads with latent groups under an LDA
framework. The two systems compliment each
other and their combination achieves better per-
formance than individual ones. Our experiments
across different forums demonstrate the robustness
of our methods and the difference among forums.
In the future work, we plan to explore how social
information could help further refine a user?s inter-
est.
References
Gediminas Adomavicius and Alexander Tuzhilin.
2005. Toward the next generation of recommender
systems: A survey of the state-of-the-art and possi-
ble extensions. IEEE TRANSACTIONS ON KNOWL-
EDGE AND DATA ENGINEERING, 17(6):734?749.
Marko Balabanovic and Yoav Shoham. 1997.
375
Fab: Content-based, collaborative recommendation.
Communications of the ACM, 40:66?72.
Chumki Basu, Haym Hirsh, and William Cohen. 1998.
Recommendation as classification: Using social and
content-based information in recommendation. In In
Proceedings of the Fifteenth National Conference on
Artificial Intelligence, pages 714?720. AAAI Press.
Paul N. Bennett. 2000. Assessing the calibration of
naive bayes? posterior estimates.
David Blei, Andrew Y. Ng, and Michael I. Jordan.
2001. Latent dirichlet alcation. Journal of Ma-
chine Learning Research, 3:2003.
John S. Breese, David Heckerman, and Carl Kadie.
1998. Empirical analysis of predictive algorithms for
collaborative filtering. pages 43?52. Morgan Kauf-
mann.
Y H Chien and E I George, 1999. A bayesian model for
collaborative filtering. Number 1.
Joaquin Delgado and Naohiro Ishii. 1999. Memory-
based weighted-majority prediction for recom-
mender systems.
Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
Academy of Sciences of the United States of Amer-
ica, 101(Suppl 1):5228?5235, April.
Thomas Hofmann. 2003. Collaborative filtering via
gaussian probabilistic latent semantic analysis. In
Proceedings of the 26th annual international ACM
SIGIR conference on Research and development in
informaion retrieval, SIGIR ?03, pages 259?266,
New York, NY, USA. ACM.
Thomas Hofmann. 2004. Latent semantic models
for collaborative filtering. ACM Trans. Inf. Syst.,
22(1):89?115.
Qing Li, Jia Wang, Yuanzhu Peter Chen, and Zhangxi
Lin. 2010. User comments for news recom-
mendation in forum-based social media. Inf. Sci.,
180:4929?4939, December.
Nick Littlestone. 1988. Learning quickly when irrele-
vant attributes abound: A new linear-threshold algo-
rithm. In Machine Learning, pages 285?318.
Atsuyoshi Nakamura and Naoki Abe. 1998. Collab-
orative filtering using weighted majority prediction
algorithms. In Proceedings of the Fifteenth Interna-
tional Conference on Machine Learning, ICML ?98,
pages 395?403, San Francisco, CA, USA. Morgan
Kaufmann Publishers Inc.
Dmitry Pavlov, Ramnath Balasubramanyan, Byron
Dom, Shyam Kapur, and Jignashu Parikh. 2004.
Document preprocessing for naive bayes classifica-
tion and clustering with mixture of multinomials. In
Proceedings of the tenth ACM SIGKDD international
conference on Knowledge discovery and data min-
ing, KDD ?04, pages 829?834, New York, NY, USA.
ACM.
Michael Pazzani, Daniel Billsus, S. Michalski, and
Janusz Wnek. 1997. Learning and revising user pro-
files: The identification of interesting web sites. In
Machine Learning, pages 313?331.
Elaine Rich. 1979. User modeling via stereotypes.
Cognitive Science, 3(4):329?354.
J. Rocchio, 1971. Relevance Feedback in Information
Retrieval.
Gerard Salton and Christopher Buckley. 1988. Term-
weighting approaches in automatic text retrieval.
In INFORMATION PROCESSING AND MANAGE-
MENT, pages 513?523.
Badrul Sarwar, George Karypis, Joseph Konstan, and
John Reidl. 2001. Item-based collaborative fil-
tering recommendation algorithms. In WWW ?01:
Proceedings of the 10th international conference on
World Wide Web, pages 285?295, New York, NY,
USA. ACM.
Upendra Shardanand and Pattie Maes. 1995. So-
cial information filtering: Algorithms for automating
?word of mouth?. In CHI, pages 210?217.
Lyle Ungar, Dean Foster, Ellen Andre, Star Wars,
Fred Star Wars, Dean Star Wars, and Jason Hiver
Whispers. 1998. Clustering methods for collabo-
rative filtering. AAAI Press.
Hao Wu, Jiajun Bu, Chun Chen, Can Wang, Guang Qiu,
Lijun Zhang, and Jianfeng Shen. 2010. Modeling
dynamic multi-topic discussions in online forums. In
AAAI.
376
Discriminative Word Alignment by
Linear Modeling
Yang Liu?
Institute of Computing Technology
Chinese Academy of Sciences
Qun Liu?
Institute of Computing Technology
Chinese Academy of Sciences
Shouxun Lin?
Institute of Computing Technology
Chinese Academy of Sciences
Word alignment plays an important role in many NLP tasks as it indicates the correspondence
between words in a parallel text. Although widely used to align large bilingual corpora, gen-
erative models are hard to extend to incorporate arbitrary useful linguistic information. This
article presents a discriminative framework for word alignment based on a linear model. Within
this framework, all knowledge sources are treated as feature functions, which depend on a source
language sentence, a target language sentence, and the alignment between them. We describe a
number of features that could produce symmetric alignments. Our model is easy to extend and
can be optimized with respect to evaluation metrics directly. The model achieves state-of-the-art
alignment quality on three word alignment shared tasks for five language pairs with varying
divergence and richness of resources. We further show that our approach improves translation
performance for various statistical machine translation systems.
1. Introduction
Word alignment, which can be defined as an object for indicating the corresponding
words in a parallel text, was first introduced as an intermediate result of statistical
machine translation (Brown et al 1993).
Consider the following Chinese sentence and its English translation:
Published under Creative Commons Attribution-NonCommercial 3.0 Unported license
     
Zhongguo jianzhuye duiwaikaifang chengxian xin geju
The opening of China?s construction industry to the outside presents a new structure
? Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese
Academy of Sciences, No. 6 Kexueyuan South Road, Haidian District, P.O. Box 2704, Beijing 100190,
China. E-mail: {yliu, liuqun, sxlin}@ict.ac.cn.
Submission received: 13 September 2007; revised submission received: 7 January 2010; accepted for
publication: 21 February 2010.
? 2010 Association for Computational Linguistics
Computational Linguistics Volume 36, Number 3
The Chinese word Zhongguo is aligned to the English word China because they are
translations of one another. Similarly, the Chinese word xin is aligned to the English
word new. These connections are not necessarily one-to-one. For example, one Chinese
word jianzhuye corresponds to two English words construction industry. In addition, the
English words (e.g., opening ... to the outside) connected to a Chinese word (e.g., dui-
waikaifang) could be discontinuous. Figure 1 shows an alignment for this sentence pair.
The Chinese and English words are listed horizontally and vertically, respectively. They
are numbered to facilitate identification. The dark points indicate the correspondence
between the words in two languages. The goal of word alignment is to identify such
correspondences in a parallel text.
Word alignment plays an important role in many NLP tasks. In statistical machine
translation, word-aligned corpora serve as an excellent source for translation-related
knowledge. The estimation of translation model parameters usually relies heavily on
word-aligned corpora, not only for phrase-based and hierarchical phrase-based models
(Koehn, Och, and Marcu 2003; Och and Ney 2004; Chiang 2005, 2007), but also for
syntax-based models (Quirk, Menezes, and Cherry 2005; Galley et al 2006; Liu, Liu,
and Lin 2006; Marcu et al 2006). Besides machine translation, many applications for
word-aligned corpora have been suggested, including machine-assisted translation,
Figure 1
Example of a word alignment between a Chinese?English sentence pair. The Chinese and
English words are listed horizontally and vertically, respectively. They are numbered to facilitate
identification. The dark points indicate the correspondences between the words in the two
languages.
304
Liu, Liu, and Lin Discriminative Word Alignment by Linear Modeling
translation assessment and critiquing tools, text generation, bilingual lexigraphy, and
word sense disambiguation.
Various methods have been proposed for finding word alignments between parallel
texts. Among them, generative alignment models (Brown et al 1993; Vogel and Ney
1996) have been widely used to produce word alignments for large bilingual corpora.
Describing the relationship of a bilingual sentence pair, a generative model treats word
alignment as a hidden process and maximizes the likelihood of a training corpus using
the expectation maximization (EM) algorithm. After the maximization process is com-
plete, the unknown model parameters are determined and the word alignments are set
to the maximum posterior predictions of the model.
However, one drawback of generative models is that they are hard to extend. Gen-
erative models usually impose strong independence assumptions between sub-models,
making it very difficult to incorporate arbitrary features explicitly. For example, when
considering whether to align two words, generative models cannot include information
about lexical and syntactic features such as part of speech and orthographic similarity in
an easy way. Such features would allow for more effective use of sparse data and result
in a model that is more robust in the presence of unseen words. Extending a generative
model requires that the interdependence of information sources be modeled explicitly,
which often makes the resulting system quite complex.
In this article, we introduce a discriminative framework for word alignment based
on the linear modeling approach. Within this framework, we treat all knowledge
sources as feature functions that depend on a source sentence, a target sentence,
and the alignment between them. Each feature function is associated with a feature
weight. The linear combination of features gives an overall score to each candidate
alignment. The best alignment is the one with the highest overall score. A linear
model not only allows for easy integration of new features, but also admits optimiz-
ing feature weights directly with respect to evaluation metrics. Experimental results
show that our approach improves both alignment quality and translation performance
significantly.
This article is organized as follows. Section 2 gives a formal description of our
model. We show how to train feature weights by taking evaluation metrics into account
and how to find the most probable alignment in an exponential search space efficiently.
Section 3 describes a number of features used in our experiments, focusing on the
features that produce symmetric alignments. In Section 4, we evaluate our model in
both alignment and translation tasks. Section 5 reviews previous work related to our
approach and the article closes with a conclusion in Section 6.
2. Approach
2.1 The Model
Given a source language sentence f = f1, . . . , fj, . . . , fJ and a target language sentence e =
e1, . . . , ei, . . . , eI, we define a link l = ( j, i) to exist if fj and ei are translations (or part of a
translation) of one another. Then, an alignment is defined as a subset of the Cartesian
product of the word positions:
a ? {( j, i) : j = 1, . . . , J; i = 1, . . . , I} (1)
305
Computational Linguistics Volume 36, Number 3
We propose a linear alignment model:
score(f, e, a) =
M
?
m=1
?mhm(f, e, a) (2)
where hm(f, e, a) is a feature function and ?m is its associated feature weight. The linear
combination of features gives an overall score score(f, e, a) to each candidate alignment
a for a given sentence pair ?f, e?.
2.2 Training
To achieve good alignment quality, it is essential to find a good set of feature weights
?M1 . Before discussing how to train ?
M
1 , we first describe two evaluation metrics that
measure alignment quality, because we will optimize ?M1 with respect to them directly.
2.2.1 Evaluation Metrics. The first metric is alignment error rate (AER), proposed by
Och and Ney (2003). AER has been used as official evaluation criterion in most word
alignment shared tasks. Och and Ney define two kinds of links in hand-aligned align-
ments: sure links for alignments that are unambiguous and possible links for ambiguous
alignments. Sure links usually connect content words such as Zhongguo and China.
In contrast, possible links often align words within idiomatic expressions and free
translations.
An AER score is given by
AER(S, P, A) = 1 ? |A ? S| + |A ? P||A| + |S| (3)
where S is a set of sure links in a reference alignment that is hand-aligned by human
experts, P is a set of possible links in the reference alignment, and A is a candidate
alignment. Note that S is a subset of P: S ? P. The lower the AER score is, the better the
alignment quality is.
Although widely used, AER has been criticized for correlating poorly with transla-
tion quality (Ayan and Dorr 2006a; Fraser and Marcu 2007b). In other words, lower AER
scores do not necessarily lead to better translation quality.1 Fraser and Marcu (2007b)
argue that reference alignments should consist of only sure links. They propose a new
measure called the balanced F-measure:
precision(S, A) =
|A ? S|
|S| (4)
recall(S, A) =
|A ? S|
|A| (5)
F-measure(S, ?, A) = 1
?
precision(S,A) +
1??
recall(S,A)
(6)
1 It has not yet been uniformly accepted that better word alignments yield better translations. Ayan and
Dorr (2006a) present a detailed discussion of the impact of word alignment on statistical machine
translation.
306
Liu, Liu, and Lin Discriminative Word Alignment by Linear Modeling
where ? is a parameter that sets the trade-off between precision and recall. Higher
F-measure means better alignment quality. Obviously, ? less than 0.5 weights recall
higher, whereas ? greater than 0.5 weights precision higher.
We use both AER and F-measure in our experiments. AER is used in experiments
evaluating alignment quality (Section 4.1) and F-measure is used in experiments evalu-
ating translation performance (Section 4.2).
2.2.2 Minimum Error Rate Training. Suppose we have three candidate alignments: a1, a2,
and a3. Their AER scores are 0.21, 0.20, and 0.22, respectively. Therefore, a2 is the best
candidate alignment, a1 is the second best, and a3 is the third best. We use three features
to score each candidate. Table 1 lists the feature values for each candidate.
If the set of feature weights is {1.0, 1.0, 1.0}, the model scores (see Equation (2)) of
the three candidates are ?71, ?74, and ?76, respectively. Whereas reference alignment
considers a2 as the best candidate, a1 has the maximal model score. This is undesirable
because the model fails to agree with the reference. If we change the feature weights
to {1.0,?2.0, 2.0}, the model scores become ?73, ?71, and ?83, respectively. Now, the
model chooses a2 as the best candidate correctly.
If a set of feature weights manages to make model predictions agree with refer-
ence alignments in training examples, we would expect the model to achieve good
alignment quality on unseen data as well. To do this, we adopt the minimum er-
ror rate training (MERT) algorithm proposed by Och (2003) to find feature weights
that minimize AER or maximize F-measure on a representative hand-aligned training
corpus.
Given a reference alignment r and a candidate alignment a, we use a loss func-
tion E(r, a) to measure alignment performance. Note that E(r, a) can be either AER or
1 ? F-measure. Given a bilingual corpus ?fS1, eS1? with a reference alignment rs and a set
of K different candidate alignments Cs = {as,1 . . . as,K} for each sentence pair ?fs, es?, our
goal is to find a set of feature weights ??M1 that minimizes the overall loss on the training
corpus:
??M1 = argmin
?M1
{ S
?
s=1
E(rs, a?(fs, es; ?
M
1 ))
}
(7)
= argmin
?M1
{ S
?
s=1
K
?
k=1
E(rs, as,k)?(a?(fs, es; ?
M
1 ), as,k)
}
(8)
Table 1
Example feature values and alignment error rates.
feature values
candidate h1 h2 h3 AER
a1 ?85 4 10 0.21
a2 ?89 3 12 0.20
a3 ?93 6 11 0.22
307
Computational Linguistics Volume 36, Number 3
where a?(fs, es; ?M1 ) is the best candidate alignment produced by the linear model:
a?(fs, es; ?
M
1 ) = argmax
a
{ M
?
m=1
?mhm(fs, es, a)
}
(9)
The basic idea of MERT is to optimize only one parameter (i.e., feature weight)
each time and keep all other parameters fixed. This process runs iteratively over M
parameters until the overall loss on the training corpus does not decrease.
Formally, suppose we tune a parameter and keep the other M ? 1 parameters fixed;
each candidate alignment corresponds to a line in the plane with ? as the independent
variable:
? ? ?(f, e, a) + ?(f, e, a) (10)
where ? denotes the parameter being tuned (i.e., ?m) and ?(f, e, a) and ?(f, e, a) are
constants with respect to ?:
?(f, e, a) = hm(f, e, a) (11)
?(f, e, a) =
M
?
m?=1,m? =m
?m?hm? (f, e, a) (12)
The set of candidates in Cs defines a set of lines. For example, given the candidate
alignments in Table 1, suppose we only tune ?2 and keep ?1 and ?3 fixed with an initial
set of parameters {1.0, 1.0, 1.0}. According to Equation (10), a1 corresponds to a line
4?? 75, a2 corresponds to a line 3?? 77, and a3 corresponds to a line 6?? 82.
The decision rule in Equation (9) states that a? is the line with the highest model
score for a given ?. The selection of ? for each sentence pair ultimately determines the
loss at ?. How do we find values of ? that could generate different loss values?
As the loss can only change if we move to a ? where the highest line is different
than before, Och (2003) suggests only evaluating the loss at values in between the
intersections that line the top surface of the cluster of lines. Figure 2 demonstrates eight
Figure 2
Candidate alignments in dimension ? and the critical intersections. Each candidate alignment is
represented as a line. ?1,?2, and ?3 are critical intersections where the best candidate a?
(highlighted in bold) will change: a? is a1 in (??, ?1], a2 in (?1, ?2], a7 in (?2, ?3], and a5 in
(?3, +?).
308
Liu, Liu, and Lin Discriminative Word Alignment by Linear Modeling
candidate alignments. The sequence of the topmost line segments highlighted in bold
constitutes an upper envelope, which indicates the best candidate alignments the model
predicts with various values of ?. Instead of computing all possible K2 intersections
between the lines in Cs, we just need to find the critical intersections where the topmost
line changes. In Figure 2, ?1, ?2, and ?3 are critical intersections. In the interval (??, ?1],
a1 has the highest score. Similarly, the best candidates are a2 for (?1, ?2], a7 for (?2, ?3],
and a5 for (?3, +?), respectively. The optimal ?? can be found by collecting all critical
intersections on the training corpus and choosing one ? that results in the minimal loss
value. Please refer to Och (2003) for more details.
2.3 Search
Given a source language sentence f and a target language sentence e, we try to find the
best candidate alignment with the highest model score:
a? = argmax
a
{
score(f, e, a)
}
(13)
= argmax
a
{ M
?
m=1
?mhm(f, e, a)
}
(14)
To do this, we begin with an empty alignment and keep adding new links until
the model score of the current alignment does not increase. Figure 3 illustrates this
search process. Given a source language sentence f1f2 and a target language sentence
e1e2, the initial alignment a1 is empty (i.e., all words are unaligned). Then, we obtain a
new alignment a2 by adding a link (1, 1) to a1. Similarly, the addition of (1, 2) to a1 leads
to a3. a2 and a3 can be further extended to produce more alignments.
Graphically speaking, the search space of a sentence pair can be organized as a
directed acyclic graph. Each node in the graph is a candidate alignment and each edge
corresponds to a link. We say that alignments that have the same number of links
constitute a level. There are 2J?I possible nodes and J ? I + 1 levels in a graph. In
Figure 3, a2, a3, a4, and a5 belong to the same level because they all contain one link.
The maximum level width is given by
( J?I
 J?I2 
)
. In Figure 3, the maximal level width is
(
4
2
)
= 6. Our goal is to find the node with the highest model score in a search graph.
As the search space of word alignment is exponential (although enumerable), it is
computationally prohibitive to explore all the graph. Instead, we can search efficiently
in a greedy way. In Figure 3, starting from a1, we add single links to a1 and obtain four
new alignments: a2, a3, a4, and a5. We retain the best new alignment that has a higher
score than a1, say a3, and discard the others. Then, we add single links to a3 and obtain
three new alignments: a7, a9, and a11. After choosing a9 as the current best alignment, the
next candidates are a12 and a14. Suppose the model scores of both a12 and a14 are lower
than that of a9. We terminate the search process and choose a9 as the best candidate
alignment.
During this search process, we expect that the addition of a single link l to the
current best alignment a will result in a new alignment a ? {l} with a higher score:
score(f, e, a ? {l}) > score(f, e, a) (15)
309
Computational Linguistics Volume 36, Number 3
Figure 3
Search space of a sentence pair: f1 f2 and e1e2. Each node in the directed graph is a candidate
alignment and each edge denotes a transition between two nodes by adding a link.
that is
M
?
m=1
?m
(
hm(f, e, a ? {l}) ? hm(f, e, a)
)
> 0 (16)
As a result, we can remove most of the computational overhead by calculating only
the difference of scores instead of the scores themselves. The difference of alignment
scores with the addition of a link, which we refer to as a link gain, is defined as
G(f, e, a, l) =
M
?
m=1
?mgm(f, e, a, l) (17)
where gm(f, e, a, l) is a feature gain, which is the incremental feature value after adding
a link l to the current alignment a:
gm(f, e, a, l) = hm(f, e, a ? {l}) ? hm(f, e, a) (18)
In our experiments, we use a beam search algorithm that is more general than the
above greedy algorithm. In the greedy algorithm, we retain at most one candidate in
each level of the space graph while traversing top-down. In the beam search algorithm,
we retain at most b candidates at each level.
Algorithm 1 shows the beam search algorithm. The input is a source language
sentence f and a target language sentence e (line 1). The algorithm maintains a list of
310
Liu, Liu, and Lin Discriminative Word Alignment by Linear Modeling
Algorithm 1 A beam search algorithm for word alignment
1: procedure ALIGN(f, e)
2: open ? ?  a list of active alignments
3: N ? ?  n-best list
4: a ? ?  begin with an empty alignment
5: ADD(open, a, ?, b)  initialize the list
6: while open = ? do
7: closed ? ?  a list of promising alignments
8: for all a ? open do
9: for all l ? J ? I ? a do  enumerate all possible new links
10: a? ? a ? {l}  produce a new alignment
11: g ? GAIN(f, e, a, l)  compute the link gain
12: if g > 0 then  ensure that the score will increase
13: ADD(closed, a?, ?, b)  update promising alignments
14: end if
15: ADD(N , a?, 0, n)  update n-best list
16: end for
17: end for
18: open ? closed  update active alignments
19: end while
20: return N  return n-best list
21: end procedure
active alignments open (line 2) and an n-best list N (line 3). The aligning process begins
with an empty alignment a (line 4) and the procedure ADD(open, a, ?, b) adds a to open.
The procedure prunes the search space by discarding any alignment that has a score
worse than:
1. ? multiplied with the best score in the list, or
2. the score of b-th best alignment in the list.
For each iteration (line 6), we use a list closed to store promising alignments that
have higher scores than the current alignment. For every possible link l (line 9), we
produce a new alignment a? (line 10) and calculate the link gain G by calling the
procedure GAIN(f, e, a, l). If a? has a higher score (line 12), it is added to closed (line 13).
We also update N to keep the top n alignments explored during the search (line 15). The
n-best list will be used in training feature weights by MERT. This process iterates
until there are no promising alignments. The theoretical running time of this algorithm
is O(bJ2I2).
3. Feature Functions
The primary art in discriminative modeling is to define useful features that capture var-
ious characteristics of word alignments. Intuitively, we can include generative models
such as the IBM Models 1?5 (Brown et al 1993) as features in a discriminative model.
A straightforward way is to use a generative model itself as a feature directly (Liu, Liu,
311
Computational Linguistics Volume 36, Number 3
and Lin 2005). Another way is to treat each sub-model of a generative model as a feature
(Fraser and Marcu 2006). In either case, a generative model can be regarded as a special
case of a discriminative model where all feature weights are one. A detailed discussion
of the treatment of the IBM models as features can be found in Appendix B.
One major drawback of the IBM models is asymmetry. They are restricted such that
each source word is assigned to exactly one target word. This is not the case for many
language pairs. For example, in our running example, one Chinese word jianzhuye cor-
responds to two English words construction industry. As a result, our linear model will
produce only one-to-one alignments if the IBM models in two translation directions (i.e.,
source-to-target and target-to-source) are both used. Although some authors would use
the one-to-one assumption to simplify the modeling problem (Melamed 2000; Taskar,
Lacoste-Julien, and Klein 2005), many translation phenomena cannot be handled and
the recall cannot reach 100% in principle.
A more general way is to model alignment as an arbitrary relation between source
and target language word positions. As our linear model is capable of including many
overlapping features regardless of their interdependencies, it is easy to add features
that characterize symmetric alignments. In the following subsections, we will introduce
a number of symmetric features used in our experiments.
3.1 Translation Probability Product
To determine the correspondence of words in two languages, word-to-word translation
probabilities are always the most important knowledge source. To model a symmetric
alignment, a straightforward way is to compute the product of the translation probabil-
ities of each link in two directions.
For example, suppose that there is an alignment {(1, 2)} for a source language
sentence f1 f2 and a target language sentence e1e2; the translation probability prod-
uct is
t(e2| f1) ? t( f1|e2)
where t(e| f ) is the probability that f is translated to e and t( f |e) is the probability that e
is translated to f , respectively.
Unfortunately, the underlying model is biased: The more links added, the smaller
the product will be. For example, if we add a link (2, 2) to the current alignment and
obtain a new alignment {(1, 2), (2, 2)}, the resulting product will decrease after being
multiplied with t(e2| f2) ? t( f2|e2):
t(e2| f1) ? t( f1|e2) ? t(e2| f2) ? t( f2|e2)
The problem results from the absence of empty cepts. Following Brown et al (1993),
a cept in an alignment is either a single source word or it is empty. They assign cepts
to positions in the source sentence and reserve position zero for the empty cept. All
unaligned target words are assumed to be ?aligned? to the empty cept. For example,
in the current example alignment {(1, 2)}, the unaligned target word e1 is said to be
?aligned? to the empty cept f0. As our model is symmetric, we use f0 to denote the
empty cept on the source side and use e0 to denote the empty cept on the target side,
respectively.
312
Liu, Liu, and Lin Discriminative Word Alignment by Linear Modeling
If we take empty cepts into account, the product for {(1, 2)} can be rewritten as
t(e2| f1) ? t( f1|e2) ? t(e1| f0) ? t( f2|e0)
Similarly, the product for {(1, 2), (2, 2)} now becomes
t(e2| f1) ? t( f1|e2) ? t(e2| f2) ? t( f2|e2) ? t(e1| f0)
Note that after adding the link (2, 2), the new product still has more factors than the old
product. However, the new product is not necessarily always smaller than the old one.
In this case, the new product divided by the old product is
t(e2| f2) ? t( f2|e2)
t( f2|e0)
Whether a new product increases or not depends on actual translation probabilities.2
Depending on whether they are aligned or not, we divide the words in a sentence
pair into two categories: aligned and unaligned. For each aligned word, we use trans-
lation probabilities conditioned on its counterpart in two directions (i.e., t(ei| fj) and
t( fj|ei)). For each unaligned word, we use translation probabilities conditioned on empty
cepts on the other side in two directions (i.e., t(ei| f0) and t( fj|e0)).
Formally, the feature function for translation probability product is given by3
htpp(f, e, a) =
?
( j,i)?a
(
log
(
t(ei| fj)
)
+ log
(
t( fj|ei)
))
+
J
?
j=1
log
(
?(?j, 0) ? t( fj|e0) + 1 ? ?(?j, 0)
)
+
I
?
i=1
log
(
?(?i, 0) ? t(ei| f0) + 1 ? ?(?i, 0)
)
(19)
where ?(x, y) is the Kronecker function, which is 1 if x = y and 0 otherwise. We define
the fertility of a source word fj as the number of aligned target words:
?j =
?
( j?,i)?a
?( j?, j) (20)
2 Even though we take empty cepts into account, the bias problem still exists because the product will
decrease by adding new links if there are no unaligned words. For example, the product will go down if
we further add a link (1, 1) to {(1, 2), (2, 2)} as all source words are aligned. This might not be a bad bias
because reference alignments usually do not have all words aligned and contain too many links.
Although translation probability product is degenerate as a generative model, the bias problem can be
alleviated when this feature is combined with other features such as link count (see Section 3.8).
3 We use the logarithmic form of translation probability product to avoid manipulating very small
numbers (e.g., 4.3 ? e?100) just for practical reasons.
313
Computational Linguistics Volume 36, Number 3
Table 2
Calculating feature values of translation probability product for a source sentence f1 f2 and a
target sentence e1e2.
alignment feature value
{} log
(
t(e1| f0) ? t(e2| f0) ? t( f1|e0) ? t( f2|e0)
)
{(1, 2)} log
(
t(e1| f0) ? t(e2| f1) ? t( f1|e2) ? t( f2|e0)
)
{(1, 2), (2, 2)} log
(
t(e1| f0) ? t(e2| f1) ? t(e2| f2) ? t( f1|e2) ? t( f2|e2)
)
Similarly, the fertility of a target word ei is the number of aligned source words:
?i =
?
( j,i? )?a
?(i?, i) (21)
For example, as only one English word China is aligned to the first Chinese word
Zhongguo in Figure 1, the fertility of Zhongguo is ?1 = 1. Similarly, the fertility of the
third Chinese word duiwaikaifang is ?3 = 4 because there are four aligned English
words. The fertility of the first English word The is ?1 = 0. Obviously, the words with
zero fertilities (e.g., The, ?s, and a in Figure 1) are unaligned.
In Equation (19), the first term calculates the product of aligned words, the second
term deals with unaligned source words, and the third term deals with unaligned target
words. Table 2 shows the feature values for some word alignments.
For efficiency, we need to calculate the difference of feature values instead of the
values themselves, which we call feature gain (see Equation (18)). The feature gain for
translation probability product is4
gtpp(f, e, a, j, i) = log
(
t(ei| fj)
)
+ log
(
t( fj|ei)
)
?
log
(
?(?j, 0) ? t( fj|e0) + 1 ? ?(?j, 0)
)
?
log
(
?(?i, 0) ? t(ei| f0) + 1 ? ?(?i, 0)
)
(22)
where ?j and ?i are the fertilities before adding the link ( j, i).
Although this feature is symmetric, we obtain the translation probabilities t( f |e) and
t(e| f ) by training the IBM models using GIZA++ (Och and Ney 2003).
3.2 Exact Match
Motivated by the fact that proper names (e.g., IBM) or specialized terms (e.g., DNA) are
often the same in both languages, Taskar, Lacoste-Julien, and Klein (2005) use a feature
that sums up the number of words linked to identical words. We adopt this exact match
feature in our model:
hem(f, e, a) =
?
( j,i)?a
?( fj, ei) (23)
4 For clarity, we use gtpp(f, e, a, j, i) instead of gtpp(f, e, a, l) because j and i appear in the equation.
314
Liu, Liu, and Lin Discriminative Word Alignment by Linear Modeling
gem(f, e, a, j, i) = ?( fj, ei) (24)
3.3 Cross Count
Due to the diversity of natural languages, word orders between two languages are usu-
ally different. For example, subject-verb-object (SVO) languages such as Chinese and
English often put an object after a verb while subject-object-verb (SOV) languages such
as Japanese and Turkish often put an object before a verb. Even between SVO languages
such as Chinese and English, word orders could be quite different too. In Figure 1,
while Zhongguo is the first Chinese word, its counterpart China is the fourth English
word. Meanwhile, the third Chinese word duiwaikaifang after Zhongguo is aligned to the
second English word opening before China. We say that there is a cross between the two
links (1, 4) and (3, 2) because (1 ? 3) ? (4 ? 2) < 0. In Figure 1, there is only one cross.
As a result, we could use the number of crosses in alignments to capture the divergence
of word orders between two languages.
Formally, the cross count feature function is given by
hcc(f, e, a) =
?
( j,i)?a
?
( j?,i? )?a
( j? j?) ? (i ? i?) < 0 (25)
gcc(f, e, a, j, i) =
?
( j?,i? )?a
( j? j?) ? (i ? i?) < 0 (26)
where expr is an indicator function that takes a boolean expression expr as the
argument:
expr =
{
1 if expr is true
0 otherwise
(27)
3.4 Neighbor Count
Moore (2005) finds that word alignments between closely related languages tend to be
approximately monotonic. Even for distantly related languages, the number of crossing
links is far less than chance since phrases tend to be translated as contiguous chunks.
In Figure 1, the dark points are positioned approximately in parallel with the diagonal
line, indicating that the alignment is approximately monotonic.
To capture such monotonicity, we follow Lacoste-Julien et al (2006) to encourage
strictly monotonic alignments by adding a bonus for any pair of links ( j, i) and ( j?, i?)
such that
j ? j? = 1 ? i ? i? = 1
In Figure 1, there is one such link pair: (3, 10) and (4, 11). We call these links
neighbors. Similarly, (5, 13) and (6, 14) are also neighbors.
Formally, the neighbor count feature function is given by
hnc(f, e, a) =
?
( j,i)?a
?
( j?,i? )?a
 j ? j? = 1 ? i ? i? = 1 (28)
315
Computational Linguistics Volume 36, Number 3
gnc(f, e, a, j, i) =
?
( j?,i? )?a
 j ? j? = 1 ? i ? i? = 1 (29)
3.5 Fertility Probability Product
Casual inspection of some word alignments quickly establishes that some Chinese
words such as Zhongguo and chengxian are often aligned to one English word whereas
other Chinese words such as duiwaikaifang tend to be translated into multiple English
words. Brown et al (1993) call the number of target words to which a source word f is
connected the fertility of f . Recall that we have given the formal definition of fertility in
the symmetric scenario in Equation (20) and Equation (21).
Besides word association (Sections 3.1 and 3.2) and word distortion (Sections 3.3
and 3.4), fertility also proves to be very important in modeling alignment because
sophisticated generative models such as the IBM Models 3?5 parameterize fertilities
directly. As our goal is to produce symmetric alignments, we calculate the product of
fertility probabilities in two directions.
Given an alignment {(1,2)} for a source sentence f1 f2 and a target sentence e1e2, the
fertility probability product is
n(1| f0) ? n(1| f1) ? n(0| f2) ? n(1|e0) ? n(0|e1) ? n(1|e2)
where n(?j| fj) is the probability that fj has a fertility of ?j and n(?i|ei) is the probability
that ei has a fertility of ?i, respectively.5 For example, n(1| f0) denotes the probability
that one target word is ?aligned? to the source empty cept f0 and n(1|e2) denotes the
probability that one source word is aligned to e2.
If we add a link (2, 2) to the current alignment and obtain a new alignment
{(1, 2), (2, 2)}, the resulting product will be
n(1| f0) ? n(1| f1) ? n(1| f2) ? n(0|e0) ? n(0|e1) ? n(2|e2)
The new product divided by the old product is
n(1| f2) ? n(0|e0) ? n(2|e2)
n(0| f2) ? n(1|e0) ? n(1|e2)
Formally, the feature function for fertility probability product is given by
hfpp(f, e, a) =
J
?
j=0
log(n(?j| fj)) +
I
?
i=0
log(n(?i|ei)) (30)
5 Brown et al (1993) treat the empty cept in a different way. They assume that at most half of the source
words in an alignment are not aligned (i.e., ?0 ? J/2) and define a binomial distribution relying on an
auxiliary parameter p0. Here, we use n(?0|e0) instead of the original form n0(?0|
?I
i=1 ?i ) just for
simplicity. See Appendix B for more details.
316
Liu, Liu, and Lin Discriminative Word Alignment by Linear Modeling
The corresponding feature gain is
gfpp(f, e, a, j, i) = log(n(?0 ? ?(?i, 0)| f0)) ? log(n(?0| f0)) +
log(n(?j + 1| fj) ? log(n(?j| fj)) +
log(n(?0 ? ?(?j, 0)|e0)) ? log(n(?0|e0)) +
log(n(?i + 1|ei)) ? log(n(?i|ei)) (31)
where ?j and ?i are the fertilities before adding the link ( j, i).
Table 3 gives the feature values for some word alignments. In practice, we also
obtain all fertility probabilities n(?j| fj) and n(?i|ei) by using the output of GIZA++
directly.
3.6 Linked Word Count
We observe that there should not be too many unaligned words in good alignments.
For example, there are only three unaligned words on the target side in Figure 1: The,
?s, and a. Unaligned words are usually function words that have little lexical meaning
but instead serve to express grammatical relationships with other words or specify the
attitude or mood of the speaker. To control the number of unaligned words, we follow
Moore, Yih, and Bode (2006) to introduce a linked word count feature that simply counts
the number of aligned words:
hlwc(f, e, a) =
J
?
j=1
?j > 0 +
I
?
i=1
?i > 0 (32)
glwc(f, e, a, j, i) = ?(?j, 0) + ?(?i, 0) (33)
In Equation (33), ?j and ?i are the fertilities before adding l.
3.7 Sibling Distance
In word alignments, there are usually several words connected to the same word on the
other side. For example, in Figure 1, two English words construction and industry are
aligned to one Chinese word jianzhuye. We call the words aligned to the same word on
the other side siblings. In Figure 1, opening, to, the, and outside are also siblings because
they are aligned to duiwaikaifang. A word (e.g., jianzhuye) often tends to produce a series
of words in another language that belong together, whereas others (e.g., duiwaikaifang)
Table 3
Calculating feature values of fertility probability product for a source sentence f1 f2 and a target
sentence e1e2.
alignment feature value
{} log(n(2| f0) ? n(0| f1) ? n(0| f2) ? n(2|e0) ? n(0|e1) ? n(0|e2))
{(1, 2)} log(n(1| f0) ? n(1| f1) ? n(0| f2) ? n(1|e0) ? n(0|e1) ? n(1|e2))
{(1, 2), (2, 2)} log(n(1| f0) ? n(1| f1) ? n(1| f2) ? n(0|e0) ? n(0|e1) ? n(2|e2))
317
Computational Linguistics Volume 36, Number 3
tend to produce a series of words that should be separate. To model this tendency, we
introduce a feature that sums up the distances between siblings.
Formally, we use ?j,k to denote the position of the k-th target word aligned to a
source word fj and use ?i,k to denote the position of the k-th source word aligned to a
target word ei. For example, jianzhuye is the second source word (i.e., f2) in Figure 1.
As the first target word aligned to f2 is construction (i.e., e6), therefore we say that
?2,1 = 6. Similarly, ?2,2 = 7 because industry (i.e., e7) is the second target word aligned
to jianzhuye. Obviously, ?j,k+1 is always greater than ?j,k by definition.
As construction and industry are siblings, we define the distance between them
as ?2,2 ? ?2,1 ? 1 = 0. Note that we give no penalty to siblings that belong closely
together. In Figure 1, there are four siblings opening, to, the, and outside aligned to the
source word duiwaikaifang. The sum of distances between them is calculated as
?3,2 ? ?3,1 ? 1 + ?3,3 ? ?3,2 ? 1 + ?3,4 ? ?3,3 ? 1
= ?3,4 ? ?3,1 ? 3
= 10 ? 2 ? 3
= 5
Therefore, the distance sum of fj can be efficiently calculated as
?( j, ?j) =
{
?j,?j ? ?j,1 ? ?j + 1 if ?j > 1
0 otherwise
(34)
Accordingly, the distance sum of ei is
?(i, ?i) =
{
?i,?i ? ?i,1 ? ?i + 1 if ?i > 1
0 otherwise
(35)
Formally, the feature function for sibling distance is given by
hsd(f, e, a) =
J
?
j=1
?( j, ?j) +
I
?
i=1
?(i, ?i) (36)
The corresponding feature gain is
gsd(f, e, a, j, i) = ?( j, ?j + 1) ? ?( j, ?j) +
?(i, ?i + 1) ??(i, ?i) (37)
where ?j and ?i are the fertilities before adding the link ( j, i).
3.8 Link Count
Given a source sentence with J words and a target sentence with I words, there are
J ? I possible links. However, the actual number of links in a reference alignment is
usually far less. For example, there are only 10 links in Figure 1 although the maximum
is 6 ? 14 = 84. The number of links has an important effect on alignment quality because
318
Liu, Liu, and Lin Discriminative Word Alignment by Linear Modeling
more links result in higher recall while fewer links result in higher precision. A good
trade-off between recall and precision usually results from a reasonable number of links.
Using the number of links as a feature could also alleviate the bias problem posed by
the translation probability product feature (see Section 3.1). A negative weight of the
link count feature often leads to fewer links while a positive weight favors more links.
Formally, the feature function for link count is
hlc(f, e, a) = |a| (38)
glc(f, e, a, l) = 1 (39)
where |a| is the cardinality of a (i.e., the number of links in a).
3.9 Link Type Count
Due to the different fertilities of words, there are different types of links. For instance,
one-to-one links indicate that one source word (e.g., Zhongguo) is translated into ex-
actly one target word (e.g., China) while many-to-many links exist for phrase-to-phrase
translation. The distribution of link types differs for different language pairs. For ex-
ample, one-to-one links occur more frequently in closely related language pairs (e.g.,
French?English) and one-to-many links are more common in distantly related language
pairs (e.g., Chinese?English). To capture the distribution of link types independent of
languages, we use features to count different types of links.
Following Moore (2005), we divide links in an alignment into four categories:
1. one-to-one links, in which neither the source nor the target word
participates in other links;
2. one-to-many links, in which only the source word participates in other
links;
3. many-to-one links, in which only the target word participates in other
links;
4. many-to-many links, in which both the source and target words
participate in other links.
In Figure 1, (1, 4), (4, 11), (5, 13), and (6, 14) are one-to-one links and the others are
one-to-many links.
As a result, we introduce four features:
ho2o(f, e, a) =
?
( j,i)?a
?j = 1 ? ?i = 1 (40)
ho2m(f, e, a) =
?
( j,i)?a
?j > 1 ? ?i = 1 (41)
hm2o(f, e, a) =
?
( j,i)?a
?j = 1 ? ?i > 1 (42)
hm2m(f, e, a) =
?
( j,i)?a
?j > 1 ? ?i > 1 (43)
319
Computational Linguistics Volume 36, Number 3
Their feature gains cannot be calculated in a straightforward way because the
addition of a link might change the link types of its siblings on both the source and
target sides. For example, if we align the Chinese word chengxian and the English word
industry, the newly added link (4, 7) is a many-to-many link. Its source sibling (2, 7),
which was a one-to-many link, now becomes a many-to-many link. Meanwhile, its
target sibling (4, 11), which was a one-to-one link, now becomes a one-to-many link.
Algorithm 2 shows how to calculate the four feature gains. After initialization
(line 2), we first decide the type of l (lines 3?11). Then, we consider the siblings of l
on the target side (lines 12?24) and those on the source side (lines 25?38), respectively.
Note that the feature gains of siblings will not change if ?i = 1 or ?j = 1.
3.10 Bilingual Dictionary
A conventional bilingual dictionary can be considered an additional knowledge source.
The intuition is that a dictionary is expected to be more reliable than an automatically
trained lexicon. For example, if Zhongguo and China appear in an entry of a dictionary,
they should be more likely to be aligned. Thus, we use a single indicator feature to
encourage linking word pairs that occur in a dictionary D:
hbd(f, e, a, D) =
?
( j,i)?a
( fj, ei) ? D (44)
gbd(f, e, a, D, j, i) = ( fj, ei) ? D (45)
3.11 Link Co-Occurrence Count
The system combination technique that integrates predictions from multiple systems
proves to be effective in machine translation (Rosti, Matsoukas, and Schwartz 2007;
He et al 2008). In word alignment, a link should be aligned if it appears in most
system predictions. Taskar, Lacoste-Julien, and Klein (2005) include the IBM Model 4
predictions as features and obtain substantial improvements.
To enable system combination, we design a feature to favor links voted by most
systems. Given an alignment a? produced by another system, we use the number of
links of the intersection of a and a? as a feature:
hlcc(f, e, a, a
?) = |a ? a?| (46)
glcc(f, e, a, a
?, j, i) = l ? a ? a? (47)
4. Experiments
In this section, we try to answer two questions:
1. Does the proposed approach achieve higher alignment quality than
generative alignment models?
2. Do statistical machine translation systems produce better translations if
we replace generative alignment models with the proposed approach?
In Section 4.1, we evaluate our approach on three word alignment shared tasks for
five language pairs with varying divergence and richness of resources. Experimental
320
Liu, Liu, and Lin Discriminative Word Alignment by Linear Modeling
Algorithm 2 Calculating gains for the link type count features
1: procedure GAINLINKTYPECOUNT(f, e, a, j, i)
2: {go2o, go2m, gm2o, gm2m} ? {0, 0, 0, 0}  initialize the feature gains
3: if ?j = 0 ? ?i = 0 then  consider ( j, i) first
4: go2o ? go2o + 1
5: else if ?j > 0 ? ?i = 0 then
6: go2m ? go2m + 1
7: else if ?j = 0 ? ?i > 0 then
8: gm2o ? gm2o + 1
9: else
10: gm2m ? gm2m + 1
11: end if
12: if ?j = 1 then  consider the siblings of ( j, i) on the target side
13: for i? = 1 . . . I do
14: if ( j, i?) ? a ? i? = i then  ( j, i?) is a sibling of ( j, i) on the target side
15: if ?i? = 1 then  ( j, i?) was a one-to-one link
16: go2o ? go2o ? 1
17: go2m ? go2m + 1  ( j, i?) now becomes a one-to-many link
18: else  ( j, i?) was a many-to-one link
19: gm2o ? gm2o ? 1
20: gm2m ? gm2m + 1  ( j, i?) now becomes a many-to-many link
21: end if
22: end if
23: end for
24: end if
25: if ?i = 1 then  consider the siblings of ( j, i) on the source side
26: for j? = 1 . . . J do
27: if ( j?, i) ? a ? j? = j then  ( j?, i) is a sibling of ( j, i) on the source side
28: if ?j? = 1 then  ( j?, i) was a one-to-one link
29: go2o ? go2o ? 1
30: gm2o ? gm2o + 1  ( j?, i) now becomes a many-to-one link
31: else  ( j?, i) was a one-to-many link
32: go2m ? go2m ? 1
33: gm2m ? gm2m + 1  ( j?, i) now becomes a many-to-many link
34: end if
35: end if
36: end for
37: end if
38: return {go2o, go2m, gm2o, gm2m}  return the four feature gains
39: end procedure
results show that our system outperforms systems participating in the three shared
tasks significantly and achieves comparable results with other state-of-the-art discrimi-
native alignment models.
In Section 4.2, we investigate the effect of our model on translation quality. By
training feature weights with respect to F-measure instead of AER, our model results
in superior translation quality over generative methods for phrase-based, hierarchical
phrase-based, and tree-to-string SMT systems.
321
Computational Linguistics Volume 36, Number 3
4.1 Evaluation of Alignment Quality
In this section, we present results of experiments on three word alignment shared tasks:
1. HLT/NAACL 2003 shared task (Mihalcea and Pedersen 2003). As part of
the HLT/NAACL 2003 workshop on ?Building and Using Parallel Texts:
Data Driven Machine Translation and Beyond,? this shared task includes
two language pairs: English?French and Romanian?English. Participants
can use both limited and unlimited resources.
2. ACL 2005 shared task (Martin, Mihalcea, and Pedersen 2005). As part of
the ACL 2005 workshop on ?Building and Using Parallel Texts: Data
Driven Machine Translation and Beyond,? this shared task includes three
language pairs to cover different language and data characteristics:
English?Inuktitut, Romanian?English, and English?Hindi. Participants
can use both limited and unlimited resources.
3. HTRDP 2005 shared task. As part of the 2005 HTRDP (National High
Technology Research and Development Program of China, also called
?863? Program) Evaluation on Chinese Information Processing and
Intelligent Human-Machine Interface Technology, this shared task
included only one language pair: Chinese?English. Participants can use
unlimited resources.
Among these, we choose two tasks, English?French and Chinese?English, to report
detailed experimental results. Results for the other tasks can also be found in Table 11.
Corpus statistics for the English?French and Chinese?English tasks are shown in
Tables 4 and 5. The English?French data from the HLT/NAACL 2003 shared task consist
of a training corpus of 1,130,104 sentence pairs, a development corpus of 37 sentence
pairs, and a test corpus of 447 sentence pairs. The development and test sets are manu-
ally aligned and marked with both sure and possible labels. Although the Canadian
Hansard bilingual corpus is widely used in the community, direct comparisons are
difficult due to the differences in splitting of training data, development data, and test
data. To make our results more comparable to previous work, we followed Lacoste-
Table 4
Corpus characteristics of the English?French task.
English French
Training corpus Sentences 1,130,104
Words 20.01M 23.61M
Vocabulary 68, 019 86, 591
Development corpus Sentences 37
Words 661 721
Vocabulary 322 344
Test corpus Sentences 447
Words 7, 020 7, 761
Vocabulary 1, 732 1, 943
322
Liu, Liu, and Lin Discriminative Word Alignment by Linear Modeling
Table 5
Corpus characteristics of the Chinese?English task.
Chinese English
Training corpus Sentences 837,594
Words 10.32M 10.71M
Vocabulary 93, 532 134, 143
Development corpus Sentences 502
Words 9, 338 9, 364
Vocabulary 2, 608 2, 587
Test corpus Sentences 505
Words 9, 088 10, 224
Vocabulary 2, 319 2, 651
Julien et al (2006) splitting the original test set into two parts: the first 200 sentences
as the development set and the remaining 247 sentences as the test set. To compare
with systems participating in the 2003 NAACL shared task, we also used the small
development set of 37 sentences to optimize feature weights, and ran our system on the
original test set of 447 sentences. The results are shown in Table 11.
The Chinese?English data from the HTRDP 2005 shared task contains a develop-
ment corpus of 502 sentence pairs and a test corpus of 505 sentence pairs. We use
a training corpus of 837, 594 sentence pairs available from Chinese Linguistic Data
Consortium and a bilingual dictionary containing 415, 753 entries.
4.1.1 Comparison of the Search Algorithm with GIZA++. We develop a word alignment
system named Vigne based on the linear modeling approach. As we mentioned before,
our model can include the IBM models as features (see Appendix B). To investigate the
effectiveness of our search algorithm, we compare Vigne with GIZA++ by using the
same models.
Table 6 shows the alignment error rate percentages for various IBM models in
GIZA++ and Vigne. To make the results comparable, we ensured that Vigne shared
Table 6
Comparison of AER scores for various IBM models in GIZA++ and Vigne. These models are
trained only on development and test sets. The pruning setting for Vigne is ? = 0 and b = 1. All
differences are not statistically significant.
English?French Chinese?English
Model Training Scheme Direction GIZA++ Vigne GIZA++ Vigne
Model 1 15 S ? T 50.6 50.6 58.0 58.0
T ? S 46.2 46.2 56.1 56.1
Model 2 1525 S ? T 47.8 47.8 59.3 59.3
T ? S 43.6 43.6 57.4 57.4
Model 3 15H533 S ? T 31.6 31.4 45.0 44.5
T ? S 27.9 27.9 47.4 46.5
Model 4 15H53343 S ? T 34.5 34.2 44.9 44.6
T ? S 30.8 30.6 46.7 46.4
323
Computational Linguistics Volume 36, Number 3
the same parameters with GIZA++.6 Table 6 also gives the training schemes used for
GIZA++. For example, the training scheme for Model 4 is 15H53343. This notation
indicates that five iterations of Model 1, five iterations of HMM, three iterations of
Model 3, and three iterations of Model 4 are performed. As the two systems use the same
model parameters, the amount of training data will have no effect on the comparison.
Therefore, we trained the IBM Models only on the development and test sets. As a re-
sult, the AER scores in Table 6 look quite high.
In GIZA++, there exist simple polynomial algorithms to find the Viterbi alignments
for Models 1 and 2. We observe that the greedy search algorithm (? = 0 and b = 1)
used by Vigne can also find the optimal alignments. Note that the two systems achieve
identical AER scores because there are no search errors.
For Models 3 and 4, maximization over all alignments cannot be efficiently carried
out as the corresponding search problem is NP-complete. To alleviate the problem,
GIZA++ resorts to a greedy search algorithm. The basic idea is to compute a Viterbi
alignment of a simple model such as Model 2 or HMM. This alignment (an intermediate
node in the search space) is then iteratively improved with respect to the alignment
probability of the refined model by moving or swapping links. In contrast, our search
algorithm starts from an empty alignment and has only one operation: adding a link.
In addition, we treat the fertility probability of an empty cept in a different way (see
Equation B.7). Interestingly, Vigne achieves slightly better results than GIZA++ for both
models. All differences are not statistically significant.
4.1.2 Comparison to Generative Models Using Asymmetric Features. Table 7 compares the
AER scores achieved by GIZA++, Cross-EM (Liang, Taskar, and Klein 2006), and Vigne.
On both tasks, we lowercased all English words in the training, development, and
test sets as a preprocessing step. For GIZA++, we used the default training scheme
of 15H53545. We used the three symmetrization heuristics proposed by Och and Ney
(2003): intersection, union, and refined method. For Cross-EM, we also used the default
configuration and jointly trained Model 1 and HMM for five iterations. For Vigne, we
used a greedy search strategy by setting ? = 0 and b = 1. Note that both GIZA++ and
Cross-EM are unsupervised alignment methods.
On the English?French task, the refined combination of Model 4 alignments pro-
duced by GIZA++ in both translation directions yields an AER of 5.9%. Cross-EM
outperforms GIZA++ significantly by achieving 5.1%. For Vigne, we use Model 4 as
the primary feature. The linear combination of Model 4 in both directions achieves
a lower AER than either one separately. The link count feature controls the number
of links in the resulting alignments and leads to an absolute improvement of 0.1%.
With the addition of cross count and neighbor count features, the AER score drops to
5.4%. We attribute this to the fact that the two features are capable of capturing the
locality and monotonicity properties of natural languages, especially for closely related
language pairs such as English?French. After adding the linked word count feature, our
model achieves an AER of 5.2%. Finally, Vigne achieves an AER of 4.0% by combining
predictions from refined Model 4 and jointly trained HMM.
On the Chinese?English task, one-to-many and many-to-one relationships occur
more frequently in the reference alignments than the English?French task. As Cross-EM
6 In GIZA++ training, the final parameters are estimated on the final alignments, which are computed
using the parameters obtained in the previous iteration. As a result, Vigne made use of the parameters
generated by the iteration before the final iteration. In other experiments, Vigne used the final parameters.
324
Liu, Liu, and Lin Discriminative Word Alignment by Linear Modeling
Table 7
Comparison of GIZA++, Cross-EM, and Vigne on both tasks. Note that Vigne yields only
one-to-one alignments if both ?Model 4 s2t? and ?Model 4 t2s? features are used. The pruning
setting for Vigne is ? = 0 and b = 1. While the final results of our system are better than the best
baseline generative models significantly at p < 0.01, adding a single feature will not always
produce a significant improvement, especially for English?French.
System Setting English?French Chinese?English
Model 4 s2t 7.7 20.9
Model 4 t2s 9.2 30.3
GIZA++ Intersection 6.8 21.8
Union 9.6 28.1
Refined method 5.9 18.4
Cross-EM HMM, joint 5.1 18.9
Model 4 s2t 7.8 20.5
+Model 4 t2s 5.6 18.3
+link count 5.5 17.7
+cross count 5.4 17.6
+neighbor count 5.2 17.4Vigne
+exact match 5.3 -
+linked word count 5.2 17.3
+bilingual dictionary - 17.1
+link co-occurrence count (GIZA++) 5.1 16.3
+link co-occurrence count (Cross-EM) 4.0 15.7
is prone to produce one-to-one alignments by encouraging agreement, symmetrizing
Model 4 by refined method yields better results than Cross-EM. We observe that the ad-
vantages of adding features such as link count, cross count, neighbor count, and linked
word count to our linear model continue to hold, resulting in a much lower AER than
both GIZA++ and Cross-EM. The addition of the bilingual dictionary is beneficial and
yields an AER of 17.1%. Further improvements were obtained by including predictions
from GIZA++ and Cross-EM.
As the IBM models do not allow a source word to be aligned with more than one
target word, the activation of the IBM models in both directions always yields one-
to-one alignments and thus has a loss in recall. To alleviate this problem, we use a
heuristic postprocessing step to produce many-to-one or one-to-many alignments. First,
we collect links that have higher translation probabilities than corresponding null links
in both directions. Then, these candidate links are sorted according to their translation
probabilities. Finally, they are added to the alignments under structural constraints
similar to those of Och and Ney (2003).
On the English?French task, this symmetrization method achieves relatively small
but very consistent improvements ranging from 0.1% to 0.2%. On the Chinese?English
task, the improvements are more significant, ranging from 0.1% to 0.8%. This differ-
ence also results from the fact that the reference alignments of the Chinese?English
task contain more one-to-many and many-to-one relationships than the English?French
task. After symmetrization, the final AER scores for the two tasks are 3.8% and 15.1%,
respectively.
4.1.3 Resulting Feature Weights. Table 8 shows the resulting feature weights of minimum
error rate training. We observe that adding new features has an effect on the weights
325
Computational Linguistics Volume 36, Number 3
Table 8
Resulting feature weights of minimum error rate training on the Chinese?English task (M4ST:
Model 4 s2t; M4TS: Model 4 t2s; LC: link count; CC: cross count; NC: neighbor count; LWC:
linked word count; BD: bilingual dictionary; LCCG: link co-occurrence count (GIZA++); LCCC:
link co-occurrence count (Cross-EM)).
M4ST M4TS LC CC NC LWC BD LCCG LCCC
M4ST 1.00 - - - - - - - -
+M4TS 0.63 0.37 - - - - - - -
+LC 0.18 0.07 ?0.75 - - - - - -
+CC 0.19 0.07 ?0.56 ?0.18 - - - - -
+NC 0.12 0.06 ?0.55 ?0.08 0.17 - - - -
+LWC 0.14 0.08 ?0.22 ?0.08 0.25 ?0.26 - - -
+BD 0.07 0.02 ?0.35 ?0.05 0.16 0.01 0.34 - -
+LCCG 0.03 0.04 ?0.13 ?0.05 0.20 ?0.16 0.28 0.11 -
+LCCC 0.02 0.02 0.14 ?0.03 0.10 ?0.26 0.30 0.04 0.09
of other features. The weights of the cross count feature are consistently negative,
suggesting that crossing links are always discouraged for Chinese?English. Also, the
positive weights of the neighbor count feature indicate that monotonic alignments
are encouraged. When the bilingual dictionary was included, the weights of Model 4
features in both directions dramatically decreased.
4.1.4 Results of the Symmetric Alignment Model. As we mentioned before, the linear model
can model many-to-many alignments directly without any postprocessing symmetriza-
tion heuristics.
Table 9 demonstrates the results of the symmetric alignment model on both tasks.
As the activation of translation and fertility probability products allows for arbitrary
relationships, the addition of the link count feature excludes most loosely related links
Table 9
AER scores achieved by the symmetric alignment model on both tasks. The pruning setting for
Vigne is ? = 0 and b = 1. Although the final model obviously outperforms the initial model
significantly at p < 0.01, adding a single feature will not always result in a significant
improvement, especially for English?French.
Features English?French Chinese?English
translation probability product 17.3 23.6
+fertility probability product 14.6 22.6
+link count 14.5 21.6
+cross count 5.8 18.5
+neighbor count 5.2 17.2
+exact match 5.1 -
+linked word count 5.2 17.0
+link types 5.0 16.9
+sibling distance 4.9 16.2
+bilingual dictionary - 15.9
+link co-occurrence count (GIZA++) 4.5 15.1
+link co-occurrence count (Cross-EM) 3.7 14.5
326
Liu, Liu, and Lin Discriminative Word Alignment by Linear Modeling
and results in more significant improvements than for asymmetric IBM models. One
interesting finding is that the cross count feature is very useful, leading to dramatic
absolute reduction of 8.7% on the English?French task and 3.1% on the Chinese?English
task, respectively. We find that the advantages of adding neighbor count and linked
word count still hold. By further including predictions from GIZA++ and Cross-EM,
our linear model achieves the best result: 3.7% on the English?French task and 14.5%
on the Chinese?English task.
We find that the symmetric linear model outperforms the asymmetric one, espe-
cially on the Chinese?English task. This suggests that although the asymmetric model
can produce symmetric alignments via symmetrization heuristics, the ?genuine? sym-
metric model produces many-to-many alignment in a more natural way.
4.1.5 Effect of Beam Search. Table 10 shows the effect of varying beam widths. The aligning
speed (words per second) decreases almost linearly with the increase of beam width b.
For simple alignment models such as using only the translation probability product
feature, enlarging the beam size fails to bring improvements due to modeling errors.
When more features are added, the model becomes more expressive. Therefore, our
system benefits from larger beam size consistently, although some benefits are not
significant statistically. When we set b = 10, the final AER scores for the English?French
and Chinese?English tasks are 3.6% and 14.3%, respectively.
4.1.6 Effect of Training Corpus Size. One disadvantage of our approach is that we need
a hand-aligned training corpus for training feature weights. However, compared with
building a treebank, manual alignment is far less expensive because one annotator only
needs to answer yes?no questions: Should this pair of words be aligned or not? If well
trained, even a non-linguist who is familiar with both source and target languages could
Table 10
Comparison of aligning speed (words per second) and AER score with varying beam widths for
the Chinese?English task. We fix ? = 0.01. Bold numbers refer to the results that are better than
the baseline but not significantly so. We use ?+? to denote the results that outperform the best
baseline (b = 1) and are statistically significant at p < 0.05. Similarly, we use ?++? to denote
significantly better than baseline at p < 0.01.
b=1 b=5 b=10
Features w/sec AER w/sec AER w/sec AER
translation probability product 3, 941 23.6 843 23.6 426 23.7
+fertility probability product 1, 418 22.6 300 22.7 150 22.9
+link count 1, 557 21.6 330 21.7 166 21.9
+cross count 1, 696 18.5 359 18.6 180 18.6
+neighbor count 1, 648 17.2 355 16.8+ 178 16.7+
+linked word count 1, 627 17.0 351 16.4+ 176 16.5+
+sibling distance 1, 531 16.9 326 16.5+ 165 16.4+
+link types 899 16.2 187 15.6+ 96 15.5++
+bilingual dictionary 890 15.9 187 15.6 94 15.5+
+link co-occurrence count (GIZA++) 877 15.1 182 15.0 92 14.9
+link co-occurrence count (Cross-EM) 867 14.5 183 14.4 92 14.3
327
Computational Linguistics Volume 36, Number 3
produce high-quality alignments. We estimate that aligning a sentence pair usually
takes only two minutes on average.
An interesting question is: How many training examples are needed to train a
good discriminative model? Figure 4 shows the learning curves with different numbers
of features on the Chinese?English task. We choose four feature groups with varying
numbers of features: 3, 6, 10, and 14. There are eight fractions of the training corpus: 10,
20, 50, 100, 200, 300, 400, and 502. Generally, the more features a model uses, the more
training examples are needed to train feature weights. Surprisingly, even when we use
14 features, 50 sentences seem to be good enough for minimum error rate training. This
finding suggests that our approach could work well even with a quite small training
corpus.
4.1.7 Summary of Results. Table 11 summarizes the results on all three shared tasks.
Vigne used the same configuration for all tasks. We used the symmetric linear model
and activated all features. The pruning setting is ? = 0.01 and b = 10. Our system
outperforms the systems participating in all the three shared tasks significantly.
Note that for the English?French task we used the small development set of 37
sentences to optimize feature weights, and ran our system on the original test set of
447 sentences. For the Romanian?English language pair, we follow Fraser and Marcu
(2006) in reducing the vocabulary by stemming Romanian and English words down to
their first four characters. For the other language pairs, English?Inuktitut and English?
Hindi, the symmetric linear model maintains its superiority over the asymmetric linear
model and yields better results than the other participants.
Figure 4
Effect of training corpus size on the Chinese?English task. We choose four feature groups with
varying numbers of features: 3, 6, 10, and 14. There are eight training corpora with varying
numbers of sentence pairs: 10, 20, 50, 100, 200, 300, 400, and 502.
328
Liu, Liu, and Lin Discriminative Word Alignment by Linear Modeling
Table 11
Comparison with the systems participating in the three shared tasks. ?non-null? denotes that the
reference alignments have no null links, ?null? denotes that the reference alignments have null
links, ?limited? denotes only limited resources can be used, and ?unlimited? denotes that there
are no restrictions on resources used.
Shared Task Task Participants Vigne
Romanian?English, non-null, limited 28.9?52.7 23.5
Romanian?English, null, limited 37.4?59.8 26.9HLT-NAACL 2003
English?French, non-null, limited 8.5?29.4 4.0
English?French, null, limited 18.5?51.7 4.6
English?Inuktitut, limited 9.5?71.3 8.9
ACL 2005 Romanian?English, limited 26.6?44.5 24.7
English?Hindi, limited 51.4 44.8
HTRDP 2005 Chinese?English, unlimited 23.5?49.2 14.3
4.1.8 Comparison to Other Work. In the word alignment literature, the Canadian Hansard
bilingual corpus is the most widely used data set. Table 12 lists alignment error rates
achieved by previous work and our system. Note that direct comparisons are problem-
atic due to the different configurations of training data, development data, and test data.
Our result matches the state-of-the-art performance on the Hansard data (Lacoste-Julien
et al 2006; Moore, Yih, and Bode 2006).
4.2 Evaluation of Translation Quality
In this section, we report on experiments with Chinese-to-English translation. To inves-
tigate the effect of our discriminative model on translation performance, we used three
translation systems:
1. Moses (Koehn and Hoang 2007), a state-of-the-art phrase-based SMT
system;
2. Hiero (Chiang 2007), a state-of-the-art hierarchical phrase-based system;
3. Lynx (Liu, Liu, and Lin 2006), a linguistically syntax-based system that
makes use of tree-to-string rules.
Table 12
Comparison of some word alignment systems on the Canadian Hansard data.
System Training Test AER
Och and Ney (2003) 1.5M 500 5.2
Moore (2005) 500K 223 7.5
Taskar, Lacoste-Julien, and Klein (2005) 1.1M 347 5.4
Liang, Taskar, and Klein (2006) 1.1M 347 4.9
Lacoste-Julien et al (2006) 1.1M 247 3.8
Blunsom and Cohn (2006) 1.1M 347 5.2
Moore, Yih, and Bode (2006) 1.1M 223 3.7
This work 1.1M 247 3.6
329
Computational Linguistics Volume 36, Number 3
For all three systems we trained the translation models on the FBIS corpus
(7.2M+9.2M words). For the language model, we used the SRI Language Modeling
Toolkit (Stolcke 2002) to train a trigram model with modified Kneser-Ney smoothing
on the Xinhua portion of the Gigaword corpus. We used the 2002 NIST MT evaluation
test set as the development set for training feature weights of translation systems, the
2005 test set as the devtest set for choosing optimal values of ? for different translation
systems, and the 2008 test set as the final test set. Our evaluation metric is case-sensitive
BLEU-4, as defined by NIST, that is, using the shortest (as opposed to closest) reference
length for brevity penalty.
We annotated the first 200 sentences of the FBIS corpus using the Blinker guidelines
(Melamed 1998). All links are sure ones. These hand-aligned sentences served as the
training corpus for Vigne. To train the feature weights in our discriminative model
using minimum-error-rate training (Och 2003), we adopt balanced F-measure (Fraser
and Marcu 2007b) as the optimization criterion.
The pipeline begins by running GIZA++ and Cross-EM on the FBIS corpus. We
used seven generative alignment methods based on IBM Model 4 and HMM as baseline
systems: (1) C?E, (2) E?C, (3) intersection, (4) union, (5) refined method (Och and
Ney 2003), (6) grow-diag-final (Koehn, Och, and Marcu 2003), and (7) Cross-EM (Liang,
Taskar, and Klein 2006). Instead of exploring the entire search space, our linear model
only searches within the union of baseline predictions, which enables our system to
align large bilingual corpus at a very fast speed of 3, 000 words per second. In other
words, our system is able to annotate the FBIS corpus in about 1.5 hours. Then, we train
the feature weights of the linear model on the training corpus with respect to F-measure
under different settings of ?. After that, our system runs on the FBIS corpus to produce
word alignments using the optimized weights. Finally, the three SMT systems train their
models on the word-aligned FBIS corpus.
Can our approach achieve higher F-measure scores than generative methods with
different values of ? (the weighting factor in F-measure)? Table 13 shows the results of
all the systems on the development set. To estimate the loss from restricting the search
Table 13
Maximization of F-measure with different settings of ? (the weighting factor in the balanced
F-measure). We use IBM Model 4 and HMM as baseline systems. Our system restricts the search
space by exploring only the union of baseline predictions. We compute the ?oracle? alignments
by intersecting the union with reference alignments. We use ?+? to denote the result that
outperforms the best baseline result with statistical significance at p < 0.05. Similarly, we use
?++? to denote significantly better than baseline at p < 0.01.
? = 0.1 ? = 0.3 ? = 0.5 ? = 0.7 ? = 0.9
IBM Model 4 C?E 82.6 81.3 80.1 79.0 77.8
IBM Model 4 E?C 68.2 70.5 73.0 75.7 78.5
IBM Model 4 intersection 63.6 68.9 75.2 82.8 92.1
IBM Model 4 union 86.6 82.0 77.9 74.2 70.8
IBM Model 4 refined method 75.4 78.5 81.8 85.4 89.4
IBM Model 4 grow-diag-final 82.4 82.1 81.7 81.4 81.1
Cross-EM HMM 70.4 73.7 77.3 81.2 85.5
oracle 91.9 93.6 95.3 97.1 99.0
Vigne 87.8++ 85.8++ 86.4++ 88.6++ 93.3++
330
Liu, Liu, and Lin Discriminative Word Alignment by Linear Modeling
space, we compute oracle alignments by intersecting the union of baseline predictions
with reference alignments. The F-measures achieved by oracle alignments range from
91.9 to 99.0, indicating that the union of baseline predictions is good enough to approx-
imate the true search space. We observe that C?E, union, and grow-diag-final weight
recall higher because F-measure decreases when ? increases. On the other hand, E?C,
intersection, refined method, and Cross-EM weight precision higher. In particular, ?
has a weak effect on grow-diag-final as its F-measure always keeps above 0.8 when ?
is varied. For each ?, we trained a set of feature weights to maximize the F-measure on
the development set. We find that our discriminative model outperforms the baseline
systems significantly at all values of ?.
Table 14 shows the BLEU scores of the three systems on the devtest set. For Moses
and Hiero, we used the default setting. For Lynx, we used the phrase pairs learned by
Moses to improve rule coverage (Liu, Liu, and Lin 2006). The best generative alignment
method is grow-diag-final, which is widely used in SMT. For all the three SMT systems,
our system outperforms the baseline systems statistically significantly. For Moses, the
best value of ? is 0.5. For Hiero and Lynx, the best ? is 0.3, suggesting that recall-
oriented alignments yield better translation performance.
Table 15 gives the BLEU scores of the three systems on the final test set. We used the
parameters optimized on the dev and devtest sets. More specifically, Moses used grow-
diag-final and ? = 0.5, Hiero used grow-diag-final and ? = 0.3, and Lynx used union
and ? = 0.3. We find that our discriminative alignment model improves the three
systems significantly.
5. Related Work
The first generative alignment models were the IBM Models 1?5 proposed by Brown
et al (1993). Vogel and Ney (1996) propose a first-order Hidden Markov model (HMM)
for word alignment. They show that it is beneficial to make the alignment probabilities
dependent on differences in position rather than on the absolute positions. Och and
Table 14
BLEU scores on the devtest set. We use ?+? to denote the result that outperforms the best
baseline result (highlighted in bold) statistically significantly at p < 0.05. Similarly, we use ?++?
to denote significantly better than baseline at p < 0.01.
Moses Hiero Lynx
IBM Model 4 C?E 24.7 25.7 24.8
IBM Model 4 E?C 20.6 23.5 21.6
IBM Model 4 intersection 20.1 23.2 21.2
IBM Model 4 union 24.3 24.1 25.1
IBM Model 4 refined method 24.2 24.0 24.2
IBM Model 4 grow-diag-final 25.0 25.8 24.3
Cross-EM HMM 23.6 24.9 24.8
? = 0.1 tuned 23.9 25.3 26.0++
? = 0.3 tuned 24.9 26.8++ 26.1++
Vigne ? = 0.5 tuned 25.7+ 26.6++ 24.3
? = 0.7 tuned 23.7 25.4 24.7
? = 0.9 tuned 21.9 24.7 23.9
331
Computational Linguistics Volume 36, Number 3
Table 15
BLEU scores on the final test set. We use the parameters optimized on the dev and devtest sets.
We use ?+? to denote the result that outperforms the best baseline result (indicated in bold)
statistically significantly at p < 0.05. Similarly, we use ?++? to denote significantly better than
baseline at p < 0.01.
Moses Hiero Lynx
generative 20.1 20.7 19.9
discriminative 20.8+ 21.6+ 21.0++
Ney (2003) re-implement the IBM models and the HMM model and compare them with
heuristic approaches systematically. The resulting toolkit GIZA++ developed by Franz
J. Och is the most popular alignment system nowadays. Liang, Taskar, and Klein (2006)
present an unsupervised way to produce symmetric alignments by training two simple
asymmetric models (e.g., IBM Model 1 and the HMM model) jointly to maximize a
combination of data likelihood and agreement between the models. Fraser and Marcu
(2007a) introduce a new generative model called LEAF that directly models many-
to-many non-consecutive word alignments. Their model can be trained using both
unsupervised and semi-supervised training methods.
Recent years have witnessed the rapid development of discriminative alignment
methods. As a first attempt, Och and Ney (2003) proposed the Model 6, which is a
log-linear combination of the IBM models and the HMM model. Cherry and Lin (2003)
develop a statistical model to find word alignments, which allows for easy integration
of context-specific features. Liu, Liu, and Lin (2005) apply the log-linear model used
in SMT (Och and Ney 2002) to word alignment and report significant improvements
over the IBM models. Moore (2005) presents a discriminative framework for word
alignment and uses averaged perceptron for parameter optimization. Taskar, Lacoste-
Julien, and Klein (2005) treat the alignment prediction task as a maximum weight
bipartite matching problem and use the large-margin method to train feature weights.
Neural networks and transformation-based learning have also been introduced to word
alignment (Ayan, Dorr, and Monz 2005a, 2005b). Blunsom and Cohn (2006) propose a
new discriminative model based on conditional random fields (CRF). Fraser and Marcu
(2006) use sub-models of IBM Model 4 as features and train feature weights using a
semi-supervised algorithm. Ayan and Dorr (2006b) use a maximum entropy model to
combine word alignments. Cherry and Lin (2006) show that introducing soft syntactic
constraints through discriminative training can improve alignment quality. Lacoste-
Julien et al (2006) extend the bipartite matching model of Taskar, Lacoste-Julien, and
Klein (2005) by including fertility and first-order interactions. Recently, max-product
belief propagation has been successfully applied to discriminative word alignment
(Niehues and Vogel 2008; Cromiere`s and Kurohashi 2009). Haghighi et al (2009) investi-
gate supervised word alignment methods that exploit inversion transduction grammar
(ITG) constraints.
Our work can be seen as an application of the linear model (Och 2003) in word
alignment. While aiming at producing symmetric word alignments in a discriminative
way, our approach uses asymmetric generative models (Brown et al 1993) as the major
information sources. Our linear model is similar to that of Moore, Yih, and Bode (2006).
They train two linear models called stage 1 and stage 2. The feature values are extracted
from word-aligned sentence pairs. After the stage 1 model aligns the entire training
corpus automatically, the stage 2 model uses features based not only on the parallel
332
Liu, Liu, and Lin Discriminative Word Alignment by Linear Modeling
sentences themselves but also on statistics of the alignments produced by the stage 1
model. They use average perceptron and support vector machine (SVM) to train feature
weights and use a beam search algorithm to find the most probable alignments. Table 12
shows that the two methods achieve comparable results on the Hansard data, confirm-
ing Moore, Yih, and Bode?s (2006) claim that model structure and feature selection are
more important than discriminative training method.
6. Conclusions and Future Work
We have presented a discriminative framework for word alignment based on the linear
modeling approach. This framework is easy to extend by including features that char-
acterize the aligning process. In addition, our approach supports symmetric alignment
modeling that allows for an arbitrary relationship between source and target language
positions. As the linear model offers excellent flexibility in using a large variety of
features and in combining information from various sources, it is able to produce good
predictions on language pairs that are either closely related (e.g., English?French) or dis-
tantly related (e.g., English?Inuktitut), either with rich resources (e.g., Chinese?English)
or with scarce resources (e.g., English?Hindi). We further show that our approach can
benefit different types of SMT systems: phrase-based, hierarchical phrase-based, and
syntax-based.
The real benefit of our model does not stem from the use of the linear model, but
rather from the discriminative training that optimizes feature weights with respect to
evaluation metrics on the gold-standard word alignments. One disadvantage of our
approach is the need for annotated training data. Although we have shown that a
very small number of training examples would be enough for parameter estimation
(Section 4.1.6), it is difficult to select such a representative training corpus to ensure that
the model will work well on unseen data, especially when the bilingual corpus to be
aligned consists of parallel texts from different domains.
Another problem is that it is hard to find an evaluation metric for word alignment
that correlates well with translation quality because the relationship between alignment
and translation is still not quite clear. Without a good loss function, discriminative
models cannot outperform generative models in large-scale applications. Therefore, it is
important to investigate how to select training examples and how to choose optimiza-
tion criterion.
The design of feature functions is most important for a discriminative alignment
model. Often, we need to try various feature groups manually on the development set
to determine the optimal feature group. Furthermore, a feature group optimized for one
language pair may not have the same effect on another one. In the future, we plan to
investigate an algorithm for automatic feature selection.
Appendix A: Table of Notation
f source sentence
fS1 sequence of source sentences: f1, . . . , fs, . . . , fS
f source word
J length of f
j position in f, j = 1, 2, . . . , J
fj the j-th word in f
f0 empty cept on the source side
333
Computational Linguistics Volume 36, Number 3
e target sentence
eS1 sequence of target sentences: e1, . . . , es, . . . , eS
e target word
I length of e
i position in e, i = 1, 2, . . . , I
ei the i-th word in e
e0 empty cept on the target side
a alignment
l a link ( j, i) in a
?j number of positions of e connected to position j of f
?i number of positions of f connected to position i of e
?j,k position of the k-th target word aligned to fj
?i,k position of the k-th source word aligned to ei
?( j, ?j) sum of sibling distances for fj
?(i, ?i) sum of sibling distances for ei
score(f, e, a) a score that indicates how well a is the alignment between f and e
a? the best candidate alignment
? feature weight
? the feature weight being optimized
h(f, e, a) feature function
G(f, e, a, l) link gain after adding l to a
g(f, e, a, l) feature gain after adding l to a
?(f, e, a) value of the feature being optimized
?(f, e, a) dot-product of fixed features
t(e| f ) the probability that f is translated to e
t( f |e) the probability that e is translated to f
n(?| f ) the probability that f has a fertility of ?
n(?|e) the probability that e has a fertility of ?
r reference alignment
Cs set of candidate alignments for the s-th training example
as,k the k-th candidate alignment for the s-th training example
E(r, a) loss function that measures alignment quality
? the precision/recall weighting factor in balanced F-measure
? pruning threshold in the beam search algorithm
b beam size in the beam search algorithm
?(x, y) the Kronecker function, which is 1 if x = y and 0 otherwise
expr an indicator function taking a boolean expression expr as the argument
Appendix B: Using the IBM Models as Feature Functions
In this article, we use IBM Models 1?4 as feature functions by taking the logarithm of the
models themselves rather than the sub-models just for simplicity. It is easy to separate
each sub-model as a feature as suggested by Fraser and Marcu (2006). We distinguish
334
Liu, Liu, and Lin Discriminative Word Alignment by Linear Modeling
between two translation directions (i.e., source-to-target and target-to-source) to use the
IBM models as feature functions. All model parameters are estimated by GIZA++ (Och
and Ney 2003).
The feature function for the IBM Model 1 is
hm1 (f, e, a) = log
(
( J|I)
(I + 1)J
J
?
j=1
t( fj|eaj )
)
(B.1)
where ( J|I) predicts the length of the source sentence conditioned on that of the target
sentence, (I + 1)?J defines a uniform distribution of the alignment between source and
target words, and t( fj|ei) is a translation sub-model. Note that aj = i, which means that
fj is connected to ei.
The corresponding feature gain is
gm1 (f, e, a, l) = log
(
t( fj|ei)) ? log(t( fj|e0)
)
(B.2)
where fj and ei are linked by l and e0 is the empty cept to which all unaligned source
words are ?aligned.?
Based on a similar generative story to Model 1, Model 2 replaces the uniform
alignment probability distribution with an alignment sub-model a(i| j, I, J). This sub-
model assumes that the position of ei depends on the position of its translation fj and
sentence lengths I and J.
The feature function for Model 2 is
hm2 (f, e, a) = log
(
( J|I)
J
?
j=1
t( fj|eaj )a(aj| j, I, J)
)
(B.3)
The corresponding feature gain is
gm2 (f, e, a, l) = log(t( fj|ei)) ? log(t( fj|e0)) +
log(a(i| j, I, J)) ? log(a(0| j, I, J)) (B.4)
where fj and ei are linked by l and 0 is the index of the empty cept e0.
Model 3 is a fertility-based model that parameterizes fertility of words. Unlike
Model 2, Model 3 uses a fertility sub-model n(?i|ei) and a distortion sub-model d( j|i, I, J).
Formally, the feature function of Model 3 is given by
hm3 (a, f, e) = log
(
n0
(
?0|
I
?
i=1
?i
)
I
?
i=1
n(?i|ei)?i!
J
?
j=1
t( fj|eaj )
?
?
j:aj =0
d( j|i, I, J)
)
(B.5)
Brown et al (1993) treat n0(?0|
?I
i=1 ?i), the fertility probability of e0, in a differ-
ent way. They assume that at most half of the source words in an alignment are not
335
Computational Linguistics Volume 36, Number 3
aligned (i.e., ?0 ? J2 ) and define a binomial distribution relying on an auxiliary parame-
ter p0:
n0
(
?0|
I
?
i=1
?i
)
=
{
(J??0
?0
)
pJ?2?00 (1 ? p0)?0 if ?0 ?
J
2
0 otherwise
(B.6)
Note that we follow Brown et al (1993) in replacing
?I
i=1 ?i with J ? ?0 for simplicity.
The original form should be (
?I
i=1?i
?0
)
p
?I
i=1?i??0
0 (1 ? p0 )?0 .
However, this assumption results in a problem for our search algorithm that begins
with an empty alignment (see Algorithm 1), for which ?0 is J and the feature value
hm3 (f, e, a) is negative infinity. To alleviate this problem, we modify Equation B.6 slightly
by adding a smoothing parameter pn ? (0, 1):
n0
(
?0|
I
?
i=1
?i
)
=
{
(J??0
?0
)
pJ?2?00 (1 ? p0)?0pn if ?0 ?
J
2
1?pn
 J2 
otherwise
(B.7)
Therefore, the feature gain for Model 3 is
gm3 (f, e, a, l) = log(gn0 ( J, ?0)) +
log(n(?i + 1|ei)) ? log(n(?i|ei)) +
log(?i + 1) +
log(t( fj|ei)) ? log(t( fj|e0)) +
log(d( j|i, I, J)) (B.8)
where fj and ei are linked by l, ?i is the fertility before adding l, and gn0 ( J, ?0) is the gain
for n0(?0|
?I
i=1 ?i):
gn0 ( J, ?0) =
?
?
?
?
?
?
?
?0?( J??0+1)
( J?2?0+1)?( J?2?0+2) if ?0 ?
J
2
1?pn
(J??0+1?0?1 )?p
J?2?0+2
0 ?(1?p0 )?0?1?pn?
J
2 
J
2 < ?0 ?
J
2 + 1
1 otherwise
(B.9)
Model 4 defines a new distortion sub-model D(a) that relies on word classes A and
B to capture movement of phrases. The feature function for Model 4 is
hm4 (a, f, e) = log
(
n0(?0|
I
?
i=1
?i)
I
?
i=1
n(?i|ei)
J
?
j=1
t( fj|eaj )
1
?0!
D(a)
)
(B.10)
where
D(a) =
I
?
i=1
?i
?
k=1
pik(?ik) (B.11)
336
Liu, Liu, and Lin Discriminative Word Alignment by Linear Modeling
pik( j) =
{
d1( j ? c?i |A(e?i ),B(?i1)) if k = 1
d>1( j? ?i,k?1|B(?ik)) otherwise
(B.12)
Brown et al (1993) propose two distortion models for Model 4: d1(?) for the first
word of a tablet ? and d>1(?) for the other words of the tablet. In Equation B.12, ?i is
the first position to the left of i for which ?i > 0, c?i is the ceiling of the average position
of the words in ??, ?ik denotes the k-th French word aligned to ek, ?i,k?1 denotes the
position of the k ? 1-th French word aligned to ei, and A(?) and B(?) are word classes
for the source and target languages, respectively. Please refer to Brown et al (1993) for
more details.
The corresponding feature gain is
gm4 (f, e, a, l) = log(gn0 ( J, ?0)) +
log(n(?i + 1|ei)) ? log(n(?i|ei)) +
log(t( fj|ei)) ? log(t( fj|e0)) +
log(?0) +
log(D(a ? {l})) ? log(D(a)) (B.13)
where fj and ei are linked by l and ?i is the fertility before adding l.
In Model 4, the addition of a single link might change the distortion probabilities
pik( j) of other links. As a result, we have to compute the overall distortion probabilities
D(a) every time.
Acknowledgments
This work was supported by National
Natural Science Foundation of China,
Contract No. 60603095 and 60573188.
Thanks to the three anonymous reviewers
for their insightful and constructive
comments and suggestions. We are
grateful to Rada Mihalcea for giving us
the Romanian?English training data and
David Chiang for allowing us to use Hiero.
Stephan Vogel, Vamshi Ambati, and
Kelly Widmaier offered valuable feedback
on an earlier version of this article.
References
Ayan, Necip Fazil and Bonnie J. Dorr. 2006a.
Going beyond AER: An extensive analysis
of word alignments and their impact on
MT. In Proceedings of COLING-ACL 2006,
pages 9?16, Sydney.
Ayan, Necip Fazil and Bonnie J. Dorr. 2006b.
A maximum entropy approach to
combining word alignments. In Proceedings
of HLT-NAACL 2006, pages 96?103, New
York, NY.
Ayan, Necip Fazil, Bonnie J. Dorr, and
Christof Monz. 2005a. Alignment link
projection using transformation-based
learning. In Proceedings of HLT-EMNLP
2005, pages 185?192, Vancouver.
Ayan, Necip Fazil, Bonnie J. Dorr, and
Christof Monz. 2005b. Neuralign:
Combining word alignments using neural
networks. In Proceedings of HLT-EMNLP
2005, pages 65?72, Vancouver.
Blunsom, Phil and Trevor Cohn. 2006.
Discriminative word alignment with
conditional random fields. In Proceedings
of COLING-ACL 2006, pages 65?72,
Sydney.
Brown, Peter F., Stephen A. Della Pietra,
Vincent J. Della Pietra, and Robert L.
Mercer. 1993. The mathematics of
statistical machine translation: Parameter
estimation. Computational Linguistics,
19(2):263?311.
Cherry, Colin and Dekang Lin. 2003. A
probability model to improve word
alignment. In Proceedings of ACL 2003,
pages 88?95, Sapporo.
Cherry, Colin and Dekang Lin. 2006. Soft
syntactic constraints for word alignment
through discriminative training. In
Proceedings of COLING-ACL 2006 (poster),
pages 105?112, Sydney.
Chiang, David. 2005. A hierarchical
phrase-based model for statistical machine
337
Computational Linguistics Volume 36, Number 3
translation. In Proceedings of ACL 2005,
pages 263?270, Ann Arbor, MI.
Chiang, David. 2007. Hierarchical
phrase-based translation. Computational
Linguistics, 33(2):201?228.
Cromiere`s, Fabien and Sadao Kurohashi.
2009. An alignment algorithm using belief
propagation and a structure-based
distortion model. In Proceedings of EACL
2009, pages 166?174, Athens.
Fraser, Alexander and Daniel Marcu. 2006.
Semi-supervised training for statistical
word alignment. In Proceedings of
COLING-ACL 2006, pages 769?776,
Sydney.
Fraser, Alexander and Daniel Marcu. 2007a.
Getting the structure right for word
alignment: LEAF. In Proceedings of
EMNLP-CoNLL 2007, pages 51?60, Prague.
Fraser, Alexander and Daniel Marcu. 2007b.
Measuring word alignment quality for
statistical machine translation.
Computational Linguistics, 33(3):293?303.
Galley, Michel, Jonathan Graehl, Kevin
Knight, Daniel Marcu, Steve DeNeefe, Wei
Wang, and Ignacio Thayer. 2006. Scalable
inference and training of context-rich
syntactic translation models. In Proceedings
of COLING-ACL 2006, pages 961?968,
Sydney.
Haghighi, Aria, John Blitzer, John DeNero,
and Dan Klein. 2009. Better word
alignments with supervised ITG models.
In Proceedings of ACL-IJCNLP 2009,
pages 923?931, Suntec.
He, Xiaodong, Mei Yang, Jianfeng Gao,
Patrick Nguyen, and Robert Moore. 2008.
Indirect-HMM-based hypothesis
alignment for combining outputs from
machine translation systems. In
Proceedings of EMNLP 2008, pages 98?107,
Honolulu, HI.
Koehn, Philipp and Hieu Hoang. 2007.
Factored translation models. In Proceedings
of EMNLP-CoNLL 2007, pages 868?876,
Prague.
Koehn, Philipp, Franz J. Och, and Daniel
Marcu. 2003. Statistical phrase-based
translation. In Proceedings of HLT-NAACL
2003, pages 127?133, Edmonton.
Lacoste-Julien, Simon, Ben Taskar, Dan Klein,
and Michael I. Jordan. 2006. Word
alignment via quadratic assignment.
In Proceedings of HLT-NAACL 2007,
pages 112?119, New York, NY.
Liang, Percy, Ben Taskar, and Dan Klein.
2006. Alignment by agreement. In
Proceedings of HLT-NAACL 2006,
pages 104?111, New York, NY.
Liu, Yang, Qun Liu, and Shouxun Lin. 2005.
Log-linear models for word alignment. In
Proceedings of ACL 2005, pages 459?466,
Ann Arbor, MI.
Liu, Yang, Qun Liu, and Shouxun Lin. 2006.
Tree-to-string alignment template for
statistical machine translation. In
Proceedings of COLING-ACL 2006,
pages 609?616, Sydney.
Marcu, Daniel, Wei Wang, Abdessamad
Echihabi, and Kevin Knight. 2006. SPMT:
Statistical machine translation with
syntactified target language phrases. In
Proceedings of EMNLP 2006, pages 44?52,
Sydney.
Martin, Joel, Rada Mihalcea, and Ted
Pedersen. 2005. Word alignment for
languages with scarce resources. In
Proceedings of the ACL 2005 Workshop on
Building and Using Parallel Texts,
pages 65?74, Ann Arbor, MI.
Melamed, I. Dan. 1998. Annotation style
guide for the blinker project. Technical
report No. 98-06, University of
Pennsylvania, Philadelphia.
Melamed, I. Dan. 2000. Models for
translational equivalence among words.
Computational Linguistics, 26(2):221?249.
Mihalcea, Rada and Ted Pedersen. 2003.
An evaluation exercise for word
alignment. In Proceedings of HLT-NAACL
2003 Workshop on Building and Using
Parallel Texts, pages 1?10, Edmonton.
Moore, Robert C. 2005. A discriminative
framework for bilingual word alignment.
In Proceedings of HLT-EMNLP 2005,
pages 81?88, Vancouver.
Moore, Robert C., Wen-tau Yih, and Andreas
Bode. 2006. Improved discriminative
bilingual word alignment. In Proceedings
of COLING-ACL 2006, pages 513?520,
Sydney.
Niehues, Jan and Stephan Vogel. 2008.
Discriminative word alignment via
alignment matrix modeling. In
Proceedings of the Third Workshop on
Statistical Machine Translation, pages 18?25,
Columbus, OH.
Och, Franz J. 2003. Minimum error rate
training in statistical machine translation.
In Proceedings of ACL 2003, pages 160?167,
Sapporo.
Och, Franz J. and Hermann Ney. 2002.
Discriminative training and maximum
entropy models for statistical machine
translation. In Proceedings of ACL 2002,
pages 295?302, Philadephia, PA.
Och, Franz J. and Hermann Ney. 2003. A
systematic comparison of various
338
Liu, Liu, and Lin Discriminative Word Alignment by Linear Modeling
statistical alignment models.
Computational Linguistics, 29(1):19?51.
Och, Franz J. and Hermann Ney. 2004.
The alignment template approach
to statistical machine translation.
Computational Linguistics,
30(4):417?449.
Quirk, Chris, Arul Menezes, and Colin
Cherry. 2005. Dependency treelet
translation: Syntactically informed
phrasal SMT. In Proceedings of ACL 2005,
pages 271?279, Ann Arbor, MI.
Rosti, Antti-Veikko, Spyros Matsoukas, and
Richard Schwartz. 2007. Improved
word-level system combination for
machine translation. In Proceedings of ACL
2007, pages 312?319, Prague.
Stolcke, Andreas. 2002. SRILM?an
extensible language modeling toolkit. In
Proceedings of ICSLP 2002, pages 901?904,
Denver, CO.
Taskar, Ben, Simon Lacoste-Julien, and Dan
Klein. 2005. A discriminative matching
approach to word alignment. In
Proceedings of HLT-EMNLP 2005,
pages 73?80, Vancouver.
Vogel, Stephan and Hermann Ney. 1996.
HMM-based word alignment in statistical
translation. In Proceedings of COLING 1996,
pages 836?841, Copenhagen.
339

Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 46?54,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Using Confusion Networks for Speech Summarization
Shasha Xie and Yang Liu
Department of Computer Science
The University of Texas at Dallas
{shasha,yangl}@hlt.utdallas.edu
Abstract
For extractive meeting summarization, previ-
ous studies have shown performance degrada-
tion when using speech recognition transcripts
because of the relatively high speech recogni-
tion errors on meeting recordings. In this pa-
per we investigated using confusion networks
to improve the summarization performance
on the ASR condition under an unsupervised
framework by considering more word candi-
dates and their confidence scores. Our ex-
perimental results showed improved summa-
rization performance using our proposed ap-
proach, with more contribution from leverag-
ing the confidence scores. We also observed
that using these rich speech recognition re-
sults can extract similar or even better sum-
mary segments than using human transcripts.
1 Introduction
Speech summarization has received increasing in-
terest recently. It is a very useful technique that
can help users to browse a large amount of speech
recordings. The problem we study in this paper is
extractive meeting summarization, which selects the
most representative segments from the meeting tran-
scripts to form a summary. Compared to text sum-
marization, speech summarization is more challeng-
ing because of not only its more spontaneous style,
but also word errors in automatic speech recogni-
tion (ASR) output. Intuitively the incorrect words
have a negative impact on downstream summariza-
tion performance. Previous research has evaluated
summarization using either the human transcripts or
ASR output with word errors. Most of the prior
work showed that performance using ASR output is
consistently lower (to different extent) comparing to
that using human transcripts no matter whether su-
pervised or unsupervised approaches were used.
To address the problem caused by imperfect
recognition transcripts, in this paper we investigate
using rich speech recognition results for summariza-
tion. N-best hypotheses, word lattices, and confu-
sion networks have been widely used as an inter-
face between ASR and subsequent spoken language
processing tasks, such as machine translation, spo-
ken document retrieval (Chelba et al, 2007; Chia
et al, 2008), and shown outperforming using 1-
best hypotheses. However, studies using these rich
speech recognition results for speech summariza-
tion are very limited. In this paper, we demonstrate
the feasibility of using confusion networks under an
unsupervised MMR (maximum marginal relevance)
framework to improve summarization performance.
Our experimental results show better performance
over using 1-best hypotheses with more improve-
ment observed from using confidence measure of the
words. Moreover, we find that the selected summary
segments are similar to or even better than those gen-
erated using human transcripts.
2 Related Work
Many techniques have been proposed for the meet-
ing summarization task, including both unsuper-
vised and supervised approaches. Since we use un-
supervised methods in this study, we will not de-
scribe previous work using supervised approaches
because of the space limit. Unsupervised meth-
46
ods are simple and robust to different corpora, and
do not need any human labeled data for training.
MMR was introduced in (Carbonell and Goldstein,
1998) for text summarization, and was used widely
in meeting summarization (Murray et al, 2005a; Xie
and Liu, 2008). Latent semantic analysis (LSA) ap-
proaches have also been used (Murray et al, 2005a),
which can better measure document similarity at the
semantic level rather than relying on literal word
matching. In (Gillick et al, 2009), the authors intro-
duced a concept-based global optimization frame-
work using integer linear programming (ILP), where
concepts were used as the minimum units, and the
important sentences were extracted to cover as many
concepts as possible. They showed better perfor-
mance than MMR. In a follow-up study, (Xie et al,
2009) incorporated sentence information in this ILP
framework. Graph-based methods, such as LexRank
(Erkan and Radev, 2004), have been originally used
for extractive text summarization, where the docu-
ment is modeled as a graph and sentences as nodes,
and sentences are ranked according to its similarity
with other nodes. (Garg et al, 2009) proposed Clus-
terRank, a modified graph-based method in order
to take into account the conversational speech style
in meetings. Recently (Lin et al, 2009) suggested
to formulate the summarization task as optimizing
submodular functions defined on the document?s se-
mantic graph, and showed better performance com-
paring to other graph-based approaches.
Rich speech recognition results, such as N-best
hypotheses and confusion networks, were first used
in multi-pass ASR systems to improve speech recog-
nition performance (Stolcke et al, 1997; Mangu et
al., 2000). They have been widely used in many sub-
sequent spoken language processing tasks, such as
machine translation, spoken document understand-
ing and retrieval. Confusion network decoding was
applied to combine the outputs of multiple machine
translation systems (Sim et al, 2007; Matusov et
al., 2006). In the task of spoken document retrieval,
(Chia et al, 2008) proposed to compute the expected
word counts from document and query lattices, and
estimate the statistical models from these counts,
and reported better retrieval accuracy than using
only 1-best transcripts. (Hakkani-Tur et al, 2006)
investigated using confusion networks for name en-
tity detection and extraction and user intent classifi-
cation. They also obtained better performance than
using ASR 1-best output.
There is very limited previous work using more
than 1-best ASR output for speech summarization.
Several studies used acoustic confidence scores in
the 1-best ASR hypothesis in the summarization sys-
tems (Valenza et al, 1999; Zechner and Waibel,
2000; Hori and Furui, 2003). (Liu et al, 2010) eval-
uated using n-best hypotheses for meeting summa-
rization, and showed improved performance with the
gain coming mainly from the first few candidates. In
(Lin and Chen, 2009), confusion networks and po-
sition specific posterior lattices were considered in
a generative summarization framework for Chinese
broadcast news summarization, and they showed
promising results by using more ASR hypotheses.
We investigate using confusion networks for meet-
ing summarization in this study. This work differs
from (Lin and Chen, 2009) in terms of the language
and genre used in the summarization task, as well
as the summarization approaches. We also perform
more analysis on the impact of confidence scores,
different pruning methods, and different ways to
present system summaries.
3 Summarization Approach
In this section, we first describe the baseline sum-
marization framework, and then how we apply it to
confusion networks.
3.1 Maximum Marginal Relevance (MMR)
MMR is a widely used unsupervised approach in
text and speech summarization, and has been shown
perform well. We chose this method as the basic
framework for summarization because of its sim-
plicity and efficiency. We expect this is a good
starting point for the study of feasibility of us-
ing confusion networks for summarization. For
each sentence segment Si in one document D, its
score (MMR(i)) is calculated using Equation 1
according to its similarity to the entire document
(Sim1(Si, D)) and the similarity to the already ex-
tracted summary (Sim2(Si, Summ)).
MMR(i) =
?? Sim1(Si, D)? (1? ?)? Sim2(Si, Summ)
(1)
47
where parameter ? is used to balance the two factors
to ensure the selected summary sentences are rel-
evant to the entire document (thus important), and
compact enough (by removing redundancy with the
currently selected summary sentences). Cosine sim-
ilarity can be used to compute the similarity of two
text segments. If each segment is represented as a
vector, cosine similarity between two vectors (V1,
V2) is measured using the following equation:
sim(V1, V2) =
?
i t1it2i??
i t
2
1i ?
??
i t
2
2i
(2)
where ti is the term weight for a word wi, for which
we can use the TFIDF (term frequency, inverse doc-
ument frequency) value, as widely used in the field
of information retrieval.
3.2 Using Confusion Networks for
Summarization
Confusion networks (CNs) have been used in many
natural language processing tasks. Figure 1 shows
a CN example for a sentence segment. It is a di-
rected word graph from the starting node to the end
node. Each edge represents a word with its associ-
ated posterior probability. There are several word
candidates for each position. ?-? in the CN repre-
sents a NULL hypothesis. Each path in the graph is
a sentence hypothesis. For the example in Figure 1,
?I HAVE IT VERY FINE? is the best hypothesis
consisting of words with the highest probabilities for
each position. Compared to N-best lists, confusion
networks are a more compact and powerful repre-
sentation for word candidates. We expect the rich in-
formation contained in the confusion networks (i.e.,
more word candidates and associated posterior prob-
abilities) can help to determine words? importance
for summarization.
Figure 1: An example of confusion networks.
The core problems when using confusion net-
works under the MMR summarization framework
are the definitions for Si, D, and Summ, as shown
in Equation 1. The extractive summary unit (for
each Si) we use is the segment provided by the rec-
ognizer. This is often different from syntactic or se-
mantic meaningful unit (e.g., a sentence), but is a
more realistic setup. Most of the previous studies
for speech summarization used human labeled sen-
tences as extraction units (for human transcripts, or
map them to ASR output), which is not the real sce-
nario when performing speech summarization on the
ASR condition. In the future, we will use automatic
sentence segmentation results, which we expect are
better units than pause-based segmentation used in
ASR. We still use a vector space model to represent
each summarization unit Si. The entire document
(D) and the current selected summary (Summ) are
formed by simply concatenating the corresponding
segments Si together. In the following, we describe
different ways to represent the segments and how to
present the final summary.
A. Segmentation representation
First, we construct the vector for each segment
simply using all the word candidates in the CNs,
without considering any confidence measure or pos-
terior probability information. The same TFIDF
computation is used as before, i.e., counting the
number of times a word appears (TF) and how many
documents it appears (used to calculate IDF).
Second, we leverage the confidence scores to
build the vector. For the term frequency of word wi,
we calculate it by summing up its posterior proba-
bilities p(wik) at each position k, that is,
TF (wi) =
?
k
p(wik) (3)
Similarly, the IDF values can also be computed us-
ing the confidence scores. The traditional method
for calculating a word?s IDF uses the ratio of the
total number of documents (N ) and the number of
documents containing this word. Using the confi-
dence scores, we calculate the IDF values as follows,
IDF (wi) = log(
N
?
D (maxk p(wik))
) (4)
If a word wi appears in the document, we find its
maximum posterior probability among all the posi-
tions it occurs in the CNs, which is used to signal
wi?s soft appearance in this document. We add these
soft counts for all the documents as the denomina-
tor in Equation 4. Different from the traditional IDF
48
calculation method, where the number of documents
containing a word is an integer number, here the de-
nominator can be any real number.
B. Confusion network pruning
The above vectors are constructed using the entire
confusion networks. We may also use the pruned
ones, in which the words with low posterior prob-
abilities are removed beforehand. This can avoid
the impact of noisy words, and increase the system
speed as well. We investigate three different pruning
methods, listed below.
? absolute pruning: In this method, we delete
words if their posterior probabilities are lower
than a predefined threshold, i.e., p(wi) < ?.
? max diff pruning: First for each position k,
we find the maximum probability among all
the word candidates: Pmaxk = maxj p(wjk).
Then we remove a word wi in this position if
the absolute difference of its probability with
the maximum score is larger than a predefined
threshold, i.e., Pmaxk ? p(wik) > ?.
? max ratio pruning: This is similar to the above
one, but instead of absolute difference, we use
the ratio of their probabilities, i.e., p(wik)Pmaxk < ?.
Again, for the last two pruning methods, the com-
parison is done for each position in the CNs.
C. Summary rendering
With a proper way of representing the text seg-
ments, we then extract the summary segments using
the MMR method described in Section 3.1. Once the
summary segments are selected using the confusion
network input, another problem we need to address
is how to present the final summary. When using
the human transcripts or the 1-best ASR hypothesis
for summarization, we can simply concatenate the
corresponding transcripts of the selected sentence
segments as the final summary for the users. How-
ever, when using the confusion networks as the rep-
resentation of each sentence segment, we only know
which segments are selected by the summarization
system. To provide the final summary to the users,
there are two choices. We can either use the best hy-
pothesis from CNs of those selected segments as a
text summary; or return the speech segments to the
users to allow them to play it back. We will evaluate
both methods in this paper. For the latter, in order to
use similar word based performance measures, we
will use the corresponding reference transcripts in
order to focus on evaluation of the correctness of the
selected summary segments.
4 Experiments
4.1 Corpus and Evaluation Measurement
We use the ICSI meeting corpus, which contains 75
recordings from natural meetings (most are research
discussions) (Janin et al, 2003). Each meeting is
about an hour long and has multiple speakers. These
meetings have been transcribed, and annotated with
extractive summaries (Murray et al, 2005b). The
ASR output is obtained from a state-of-the-art SRI
speech recognition system, including the confusion
network for each sentence segment (Stolcke et al,
2006). The word error rate (WER) is about 38.2%
on the entire corpus.
The same 6 meetings as in (Murray et al, 2005a;
Xie and Liu, 2008; Gillick et al, 2009; Lin et al,
2009) are used as the test set in this study. Fur-
thermore, 6 other meetings were randomly selected
from the remaining 69 meetings in the corpus to
form a development set. Each meeting in the de-
velopment set has only one human-annotated sum-
mary; whereas for the test meetings, we use three
summaries from different annotators as references
for performance evaluation. The lengths of the ref-
erence summaries are not fixed and vary across an-
notators and meetings. The average word compres-
sion ratio for the test set is 14.3%, and the mean de-
viation is 2.9%. We generated summaries with the
word compression ratio ranging from 13% to 18%,
and only provide the best results in this paper.
To evaluate summarization performance, we use
ROUGE (Lin, 2004), which has been widely used
in previous studies of speech summarization (Zhang
et al, 2007; Murray et al, 2005a; Zhu and Penn,
2006). ROUGE compares the system generated
summary with reference summaries (there can be
more than one reference summary), and measures
different matches, such as N-gram, longest com-
mon sequence, and skip bigrams. In this paper,
we present our results using both ROUGE-1 and
49
ROUGE-2 F-scores.
4.2 Characteristics of CNs
First we perform some analysis of the confusion net-
works using the development set data. We define
two measurements:
? Word coverage. This is to verify that CNs con-
tain more correct words than the 1-best hy-
potheses. It is defined as the percentage of
the words in human transcripts (measured us-
ing word types) that appear in the CNs. We
use word types in this measurement since we
are using a vector space model and the multi-
ple occurrence of a word only affects its term
weights, not the dimension of the vector. Note
that for this analysis, we do not perform align-
ment that is needed in word error rate measure
? we do not care whether a word appears in the
exact location; as long as a word appears in the
segment, its effect on the vector space model is
the same (since it is a bag-of-words model).
? Average node density. This is the average num-
ber of candidate words for each position in the
confusion networks.
Figure 2 shows the analysis results for these two
metrics, which are the average values on the devel-
opment set. In this analysis we used absolute prun-
ing method, and the results are presented for dif-
ferent pruning thresholds. For a comparison, we
also include the results using the 1-best hypotheses
(shown as the dotted line in the figure), which has an
average node density of 1, and the word coverage of
71.55%. When the pruning threshold is 0, the results
correspond to the original CNs without pruning.
We can see that the confusion networks include
much more correct words than 1-best hypotheses
(word coverage is 89.3% vs. 71.55%). When in-
creasing the pruning thresholds, the word coverage
decreases following roughly a linear pattern. When
the pruning threshold is 0.45, the word coverage of
the pruned CNs is 71.15%, lower than 1-best hy-
potheses. For node density, the non-pruned CNs
have an average density of 11.04. With a very small
pruning threshold of 0.01, the density decreases
rapidly to 2.11. The density falls less than 2 when
the threshold is 0.02, which means that for some
0123
4567
891011
12
0 0.01 0.02 0.03 0.04 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5Pruning Threshold
Node Density
7075
8085
90
Word Coverage 
(%)node density word coverage
Figure 2: Average node density and word coverage of the
confusion networks on the development set.
nodes there is only one word candidate preserved
after pruning (i.e., only one word has a posterior
probability higher than 0.02). When the threshold
increases to 0.4, the density is less than 1 (0.99),
showing that on average there is less than one candi-
date left for each position. This is consistent with the
word coverage results ? when the pruning thresh-
old is larger than 0.45, the confusion networks have
less word coverage than 1-best hypotheses because
even the top word hypotheses are deleted. There-
fore, for our following experiments we only use the
thresholds ? ? 0.45 for absolute pruning.
Note that the results in the figure are based on
absolute pruning. We also performed analysis us-
ing the other two pruning methods described in Sec-
tion 3.2. For those methods, because the decision
is made by comparing each word?s posterior proba-
bility with the maximum score for that position, we
can guarantee that at least the best word candidate is
included in the pruned CNs. We varied the pruning
threshold from 0 to 0.95 for these pruning methods,
and observed similar patterns as in absolute prun-
ing for the word coverage and node density analysis.
As expected, the fewer word candidates are pruned,
the better word coverage and higher node density the
pruned CNs have.
4.3 Summarization Results
4.3.1 Results on dev set using 1-best hypothesis
and human transcripts
We generate the baseline summarization result
using the best hypotheses from the confusion net-
50
works. The summary sentences are extracted using
the MMR method introduced in Section 3.1. The
term weighting is the traditional TFIDF value. The
ROUGE-1 and ROUGE-2 scores for the baseline are
listed in Table 1.
Because in this paper our task is to evaluate the
summarization performance using ASR output, we
generate an oracle result, where the summary ex-
traction and IDF calculation are based on the human
transcripts for each ASR segment. These results are
also presented in Table 1. Comparing the results for
the two testing conditions, ASR output and human
transcripts, we can see the performance degradation
due to recognition errors. The difference between
them seems to be large enough to warrant investiga-
tion of using rich ASR output for improved summa-
rization performance.
ROUGE-1 ROUGE-2
Baseline: best hyp 65.60 26.83
Human transcript 69.98 33.21
Table 1: ROUGE results (%) using 1-best hypotheses and
human transcripts on the development set.
4.3.2 Results on the dev set using CNs
A. Effect of segmentation representation
We evaluate the effect on summarization using
different vector representations based on confusion
networks. Table 2 shows the results on the develop-
ment set using various input under the MMR frame-
work. We also include the results using 1-best and
human transcripts in the table as a comparison. The
third row in the table uses the 1-best hypothesis, but
the term weight for each word is calculated by con-
sidering its posterior probability in the CNs (denoted
by ?wp?). We calculate the TF and IDF values us-
ing Equation 3 and 4 introduced in Section 3.2. The
other representations in the table are for the non-
pruned and pruned CNs based on different pruning
methods, and with or without using the posteriors to
calculate term weights.
In general, we find that using confusion networks
improves the summarization performance compar-
ing with the baseline. Since CNs contain more can-
didate words and posterior probabilities, a natural
segment representation ROUGE-1 ROUGE-2
Best hyp 65.60 26.83
Best hyp (wp) 66.83 29.84
Non-pruned CNs 66.58 28.22
Non-pruned CNs (wp) 66.47 29.27
Pruned CNs
Absolute 67.44 29.02
Absolute (wp) 66.98 29.99
Max diff 67.29 28.97
Max diff (wp) 67.10 29.76
Max ratio 67.43 28.97
Max ratio (wp) 67.06 29.90
Human transcript 69.98 33.21
Table 2: ROUGE results (%) on the development set us-
ing different vector representations based on confusion
networks: non-pruned and pruned, using posterior prob-
abilities (?wp?) and without using them.
question to ask is, which factor contributes more to
the improved performance? We can compare the re-
sults in Table 2 across different conditions that use
the same candidate words, one with standard TFIDF,
and the other with posteriors for TFIDF, or that use
different candidate words and the same setup for
TFIDF calculation. Our results show that there is
more improvement using our proposed method for
TFIDF calculation based on posterior probabilities,
especially ROUGE-2 scores. Even when just us-
ing 1-best hypotheses, if we consider posteriors, we
can obtain very competitive results. There is also
a difference in the effect of using posterior proba-
bilities. When using the top hypotheses representa-
tion, posteriors help both ROUGE-1 and ROUGE-2
scores; when using confusion networks, non-pruned
or pruned, using posterior probabilities improves
ROUGE-2 results, but not ROUGE-1.
Our results show that adding more candidates in
the vector representation does not necessarily help
summarization. Using the pruned CNs yields bet-
ter performance than the non-pruned ones. There is
not much difference among different pruning meth-
ods. Overall, the best results are achieved by using
pruned CNs: best ROUGE-1 result without using
posterior probabilities, and best ROUGE-2 scores
when using posteriors.
B. Presenting summaries using human tran-
scripts
51
segment representation ROUGE-1 ROUGE-2
Best hyp 68.26 32.25
Best hyp (wp) 69.16 33.99
Non-pruned CNs 69.28 33.49
Non-pruned CNs (wp) 67.84 32.95
Pruned CNs
Absolute 69.66 34.06
Absolute (wp) 69.37 34.25
Max diff 69.88 34.17
Max diff (wp) 69.38 33.94
Max ratio 69.76 34.06
Max ratio (wp) 69.44 34.39
Human transcript 69.98 33.21
Table 3: ROUGE results (%) on the development set
using different segment representations, with the sum-
maries constructed using the corresponding human tran-
scripts for the selected segments.
In the above experiments, we construct the final
summary using the best hypotheses from the con-
fusion networks once the summary sentence seg-
ments are determined. Although we notice obvious
improvement comparing with the baseline results,
the ROUGE scores are still much lower than using
the human transcripts. One reason for this is the
speech recognition errors. Even if we select the cor-
rect utterance segment as in the reference summary
segments, the system performance is still penalized
when calculating the ROUGE scores. In order to
avoid the impact of word errors and focus on evalu-
ating whether we have selected the correct segments,
next we use the corresponding human transcripts of
the selected segments to obtain performance mea-
sures. The results from this experiment are shown in
Table 3 for different segment representations.
We can see that the summaries formed using hu-
man transcripts are much better comparing with the
results presented in Table 2. These two setups use
the same utterance segments. The only difference
lies in the construction of the final summary for
performance measurement, using the top hypothe-
ses or the corresponding human transcripts for the
selected segments. We also notice that the differ-
ence between using 1-best hypothesis and human
transcripts is greatly reduced using this new sum-
mary formulation. This suggests that the incorrect
word hypotheses do not have a very negative im-
pact in terms of selecting summary segments; how-
ever, word errors still account for a significant part
of the performance degradation on ASR condition
when using word-based metrics for evaluation. Us-
ing the best hypotheses with their posterior proba-
bilities we can obtain similar ROUGE-1 score and
a little higher ROUGE-2 score comparing to the re-
sults using human transcripts. The performance can
be further improved using the pruned CNs.
Note that when using the non-pruned CNs and
posterior probabilities for term weighting, the
ROUGE scores are worse than most of other condi-
tions. We performed some analysis and found that
one reason for this is the selection of some poor
segments. Most of the word candidates in the non-
pruned CNs have very low confidence scores, result-
ing in high IDF values using our proposed methods.
Since some top hypotheses are NULL words in the
poorly selected summary segments, it did not affect
the results when using the best hypothesis for eval-
uation, but when using human transcripts, it leads to
lower precision and worse overall F-scores. This is
not a problem for the pruned CNs since words with
low probabilities have been pruned beforehand, and
thus do not impact segment selection. We will inves-
tigate better methods for term weighting to address
this issue in our future work.
These experimental results prove that using the
confusion networks and confidence scores can help
select the correct sentence segments. Even though
the 1-best WER is quite high, if we can con-
sider more word candidates and/or their confidence
scores, this will not impact the process of select-
ing summary segments. We can achieve similar
performance as using human transcripts, and some-
times even slightly better performance. This sug-
gests using more word candidates and their confi-
dence scores results in better term weighting and
representation in the vector space model. Some
previous work showed that using word confidence
scores can help minimize the WER of the extracted
summaries, which then lead to better summarization
performance. However, we think the main reason
for the improvement in our study is from selecting
better utterances, as shown in Table 3. In our ex-
periments, because different setups select different
segments as the summary, we can not directly com-
pare the WER of extracted summaries, and analyze
whether lower WER is also helpful for better sum-
52
output summary
best hypotheses human transcripts
R-1 R-2 R-1 R-2
Best hyp 65.73 26.79 68.60 32.03
Best hyp (wp) 65.92 27.27 68.91 32.69
Pruned CNs 66.47 27.73 69.53 34.05
Human transcript N/A N/A 69.08 33.33
Table 4: ROUGE results (%) on the test set.
marization performance. In our future work, we will
perform more analysis along this direction.
4.3.3 Experimental results on test set
The summarization results on the test set are pre-
sented in Table 4. We show four different evalua-
tion conditions: baseline using the top hypotheses,
best hypotheses with posterior probabilities, pruned
CNs, and using human transcripts. For each condi-
tion, the final summary is evaluated using the best
hypotheses or the corresponding human transcripts
of the selected segments. The summarization system
setups (the pruning method and threshold, ? value in
MMR function, and word compression ratio) used
for the test set are decided based on the results on
the development set.
For the results on the test set, we observe sim-
ilar trends as on the development set. Using the
confidence scores and confusion networks can im-
prove the summarization performance comparing
with the baseline. The performance improvements
from ?Best hyp? to ?Best hyp (wp)? and from ?Best
hyp (wp)? to ?Pruned CNs? using both ROUGE-1
and ROUGE-2 measures are statistically significant
according to the paired t-test (p < 0.05). When the
final summary is presented using the human tran-
scripts of the selected segments, we observe slightly
better results using pruned CNs than using human
transcripts as input for summarization, although the
difference is not statistically significant. This shows
that using confusion networks can compensate for
the impact from recognition errors and still allow us
to select correct summary segments.
5 Conclusion and Future Work
Previous research has shown performance degrada-
tion when using ASR output for meeting summa-
rization because of word errors. To address this
problem, in this paper we proposed to use confu-
sion networks for speech summarization. Under the
MMR framework, we introduced a vector represen-
tation for the segments by using more word can-
didates in CNs and their associated posterior prob-
abilities. We evaluated the effectiveness of using
different confusion networks, the non-pruned ones,
and the ones pruned using three different methods,
i.e., absolute, max diff and max ratio pruning. Our
experimental results on the ICSI meeting corpus
showed that even when we only use the top hypothe-
ses from the CNs, considering the word posterior
probabilities can improve the summarization perfor-
mance on both ROUGE-1 and ROUGE-2 scores.
By using the pruned CNs we can obtain further im-
provement. We found that more gain in ROUGE-
2 results was yielded by our proposed soft term
weighting method based on posterior probabilities.
Our experiments also demonstrated that it is pos-
sible to use confusion networks to achieve similar
or even better performance than using human tran-
scripts if the goal is to select the right segments. This
is important since one possible rendering of summa-
rization results is to return the audio segments to the
users, which does not suffer from recognition errors.
In our experiments, we observed less improve-
ment from considering more word candidates than
using the confidence scores. One possible reason is
that the confusion networks we used are too confi-
dent. For example, on average 90.45% of the can-
didate words have a posterior probability lower than
0.01. Therefore, even though the correct words were
included in the confusion networks, their contribu-
tion may not be significant enough because of low
term weights. In addition, low probabilities also
cause problems to our proposed soft IDF computa-
tion. In our future work, we will investigate prob-
ability normalization methods and other techniques
for term weighting to cope with these problems.
6 Acknowledgment
This research is supported by NSF award IIS-
0845484. Any opinions expressed in this work are
those of the authors and do not necessarily reflect
the views of NSF. The authors thank Shih-Hsiang
Lin and Fei Liu for useful discussions.
53
References
Jaime Carbonell and Jade Goldstein. 1998. The use of
MMR, diversity-based reranking for reordering docu-
ments and producing summaries. In Proceedings of
SIGIR.
Ciprian Chelba, Jorge Silva, and Alex Acero. 2007.
Soft indexing of speech content for search in spoken
documents. In Computer Speech and Language, vol-
ume 21, pages 458?478.
Tee Kiah Chia, Khe Chai Sim, Haizhou Li, and Hwee Tou
Ng. 2008. A lattice-based approach to query-by-
example spoken document retrieval. In Proceedings
of SIGIR.
Gunes Erkan and Dragomir R. Radev. 2004. LexRank:
graph-based lexical centrality as salience in text sum-
marization. Artificial Intelligence Research, 22:457?
479.
Nikhil Garg, Benoit Favre, Korbinian Reidhammer, and
Dilek Hakkani-Tur. 2009. ClusterRank: a graph based
method for meeting summarization. In Proceedings of
Interspeech.
Dan Gillick, Korbinian Riedhammer, Benoit Favre, and
Dilek Hakkani-Tur. 2009. A global optimization
framework for meeting summarization. In Proceed-
ings of ICASSP.
Dilek Hakkani-Tur, Frederic Behet, Giuseppe Riccardi,
and Gokhan Tur. 2006. Beyond ASR 1-best: using
word confusion networks in spoken language under-
standing. Computer Speech and Language, 20(4):495
? 514.
Chiori Hori and Sadaoki Furui. 2003. A new approach to
automatic speech summarization. IEEE Transactions
on Multimedia, 5(3):368?378.
Adam Janin, Don Baron, Jane Edwards, Dan Ellis,
David Gelbart, Nelson Morgan, Barbara Peskin, Thilo
Pfau, Elizabeth Shriberg, Andreas Stolcke, and Chuck
Wooters. 2003. The ICSI meeting corpus. In Pro-
ceedings of ICASSP.
Shih-Hsiang Lin and Berlin Chen. 2009. Improved
speech summarization with multiple-hypothesis repre-
sentations and Kullback-Leibler divergence measures.
In Proceedings of Interspeech.
Hui Lin, Jeff Bilmes, and Shasha Xie. 2009. Graph-
based submodular selection for extractive summariza-
tion. In Proceedings of ASRU.
Chin-Yew Lin. 2004. ROUGE: A package for auto-
matic evaluation of summaries. In the Workshop on
Text Summarization Branches Out.
Yang Liu, Shasha Xie, and Fei Liu. 2010. Using n-best
recognition output for extractive summarization and
keyword extraction in meeting speech. In Proceedings
of ICASSP.
Lidia Mangu, Eric Brill, and Andreas Stolcke. 2000.
Finding consensus in speech recognition: word error
minimization and other applications of confusion net-
works. Computer Speech and Language, 14:373?400.
Evgeny Matusov, Nicola Ueffing, and Hermann Ney.
2006. Computing consensus translation from multiple
machine translation systems using enhanced hypothe-
ses alignment. In Proceedings of EACL.
Gabriel Murray, Steve Renals, and Jean Carletta. 2005a.
Extractive summarization of meeting recordings. In
Proceedings of Interspeech.
Gabriel Murray, Steve Renals, Jean Carletta, and Johanna
Moore. 2005b. Evaluating automatic summaries of
meeting recordings. In Proceedings of the ACL Work-
shop on Intrinsic and Extrinsic Evaluation Measures
for Machine Translation.
Khe Chai Sim, William Byrne, Mark Gales, Hichem
Sahbi, and Phil Woodland. 2007. Consensus net-
work decoding for statistical machine translation sys-
tem combination. In Proceedings of ICASSP.
Andreas Stolcke, Yochai Konig, and Mitchel Weintraub.
1997. Explicit word error minimization in N-best list
rescoring. In Proceedings of Eurospeech.
Andreas Stolcke, Barry Chen, Horacio Franco,
Venkata Ra mana Rao Gadde, Martin Graciarena,
Mei-Yuh Hwang, Katrin Kirchhoff, Arindam Mandal,
Nelson Morgan, Xin Lei, Tim Ng, and et al 2006.
Recent innovations in speech-to-text transcription at
SRI-ICSI-UW. IEEE Transactions on Audio, Speech,
and Language Processing, 14(5):1729?1744.
Robin Valenza, Tony Robinson, Marianne Hickey, and
Roger Tucker. 1999. Summarization of spoken audio
through information extraction. In Proceedings of the
ESCA Workshop on Accessing Information in Spoken
Audio, pages 111?116.
Shasha Xie and Yang Liu. 2008. Using corpus
and knowledge-based similarity measure in maximum
marginal relevance for meeting summarization. In
Proceedings of ICASSP.
Shasha Xie, Benoit Favre, Dilek Hakkani-Tur, and Yang
Liu. 2009. Leveraging sentence weights in concept-
based optimization framework for extractive meeting
summarization. In Proceedings of Interspeech.
Klaus Zechner and Alex Waibel. 2000. Minimizing word
error rate in textual summaries of spoken language. In
Proceedings of NAACL.
Jian Zhang, Ho Yin Chan, Pascale Fung, and Lu Cao.
2007. A comparative study on speech summarization
of broadcast news and lecture speech. In Proceedings
of Interspeech.
Xiaodan Zhu and Gerald Penn. 2006. Summarization of
spontaneous conversations. In Proceedings of Inter-
speech.
54
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 309?312,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Improving Blog Polarity Classification via Topic Analysis and Adaptive
Methods
Feifan Liu
University of Wisconsin, Milwaukee
liuf@uwm.edu
Dong Wang, Bin Li, Yang Liu
The University of Texas at Dallas
dongwang,yangl@hlt.utdallas.edu
Abstract
In this paper we examine different linguistic features
for sentimental polarity classification, and perform a
comparative study on this task between blog and re-
view data. We found that results on blog are much
worse than reviews and investigated two methods
to improve the performance on blogs. First we ex-
plored information retrieval based topic analysis to
extract relevant sentences to the given topics for po-
larity classification. Second, we adopted an adaptive
method where we train classifiers from review data
and incorporate their hypothesis as features. Both
methods yielded performance gain for polarity clas-
sification on blog data.
1 Introduction
Sentimental analysis is a task of text categorization that
focuses on recognizing and classifying opinionated text
towards a given subject. Different levels of sentimental
analysis has been performed in prior work, from binary
classes to more fine grained categories. Pang et al (2002)
defined this task as a binary classification task and ap-
plied it to movie reviews. More sentiment classes, such
as document objectivity and subjectivity as well as dif-
ferent rating scales on the subjectivity, have also been
taken into consideration (Pang and Lee, 2005; Boiy et
al., 2007). In terms of granularity, this task has been
investigated from building word level sentiment lexicon
(Turney, 2002; Moilanen and Pulman, 2008) to detecting
phrase-level (Wilson et al, 2005; Agarwal et al, 2009)
and sentence-level (Riloff and Wiebe, 2003; Hu and Liu,
2004) sentiment orientation. However, most previous
work has mainly focused on reviews (Pang et al, 2002;
Hu and Liu, 2004), news resources (Wilson et al, 2005),
and multi-domain adaptation (Blitzer et al, 2007; Man-
sour et al, 2008). Sentiment analysis on blogs (Chesley
et al, 2005; Kim et al, 2009) is still at its early stage.
In this paper we investigate binary polarity classifica-
tion (positive vs. negative). We evaluate the genre effect
between blogs and review data and show the difference of
feature effectiveness. We demonstrate improved polarity
classification performance in blogs using two methods:
(a) integrating topic relevance analysis to perform topic
specific polarity classification; (b) adopting an adaptive
method by incorporating multiple classifiers? hypotheses
from different review domains as features. Our manual
analysis also points out some challenges and directions
for further study in blog domain.
2 Features for Polarity Classification
For the binary polarity classification task, we use a super-
vised learning framework to determine whether a docu-
ment is positive or negative. We used a subjective lex-
icon, containing 2304 positive words and 4145 negative
words respectively, based on (Wilson et al, 2005). The
features we explored are listed below.
(i) Lexical features (LF)
We use the bag of words for the lexical features as they
have been shown very useful in previous work.
(ii) Polarized lexical features (PL)
We tagged each sentiment word in our data set with its
polarity tag based on the sentiment lexicon (?POS? for
positive, and ?NEG? for negative), along with its part-
of-speech tag. For example, in the sentence ?It is good,
and I like it?, ?good? is tagged as ?POS/ADJ?, ?like? is
tagged as ?POS/VRB?. Then we encode the number of
the polarized tags in a document as features.
(iii) Polarized bigram features (PB)
Contextual information around the polarized words
can be useful for sentimental analysis. A word may
flip the polarity of its neighboring sentiment words even
though this word itself is not necessarily a negative word.
For example, in ?Given her sudden celebrity with those
on the left...? (a sentence in a political blog), ?sudden?
preceding ?celebrity? implies the author?s negative atti-
tude towards ?her?. We combine the sentiment word?s
polarized tag and its following and preceding word or
its part-of-speech to comprise different bigram features
to represent this kind of contextual information. For ex-
309
ample, in ?I recommend this.?, ?recommend? is a posi-
tive verb, denoted as ?POS/VRB?, and the bigram fea-
tures including this tag and its previous word ?I? are
?I POS/VRB? and ?pron POS/VRB?.
(iv) Transition word features (T)
Transition words, such as ?although?, ?even though?,
serve as function words that may change the literal opin-
ion polarity in the current sentence. This information has
not been widely explored for sentiment analysis. In this
study, we compiled a transition word list containing 31
words. We use the co-occurring feature between a transi-
tion word and its nearby content words (noun, verb, ad-
jective and adverb) or polarized tags of sentiment words
within the same sentence, but not spanning over other
transition words. For example, in ?Although it is good?,
we use features like ?although is?,?although good? and
?although POS/ADJ?, where ?POS/ADJ? is the PL fea-
ture for word ?good?.
3 Feature Effectiveness on Blogs and
Reviews
The blog data we used is from the TREC Blog Track eval-
uation in 2006 and 2007. The annotation was conducted
for the 100 topics used in the evaluation (blogs are rele-
vant to a given topic and also opinionated). We use 6,896
positive and 5,300 negative blogs. For the review data,
we combined multiple review data sets from (Pang et al,
2002; Blitzer et al, 2007) together. It contains reviews
from movies and four product domains (kitchen, elec-
tronics, books, and dvd), each of which has 1000 neg-
ative and 1000 positive samples. For the data without
sentence information (e.g., blog data, some review data),
we generated sentences using the maximum entropy sen-
tence boundary detection tool1. We used TnT tagger to
obtain the part-of-speech tags for these data sets.
For classification, we use the maximum entropy clas-
sifier2 with a Gaussian prior of 1 and 100 iterations in
model training. For all the experiments below, we use
a 10-fold cross validation setup and report the average
classification accuracy. Table 1 shows classification re-
sults using various feature sets on blogs and review data.
We keep the lexical feature (LF) as a base feature, and
investigate the effectiveness of adding more different fea-
tures. We used Wilcox signed test for statistical signifi-
cance test. Symbols ??? and ??? in the table indicate the
significant level of 0.05 and 0.1 respectively, compared to
the baseline performance using LF feature setup.
For the review domain, most of the feature sets can sig-
nificantly improve the classification performance over the
baseline of using ?LF? features. ?PB? features yielded
more significant improvement than ?PL? or ?T? feature
categories. Combining ?PL? and ?T? features resulted in
some slight further improvement, achieving the best ac-
1http://stp.ling.uu.se/?gustav/java/classes/MXTERMINATOR.html
2http://homepages.inf.ed.ac.uk/s0450736/maxent toolkit.html
Feature Set Blogs Reviews
LF 72.07 81.67
LF+PL 70.93 81.93
LF+PB 72.44 83.62?
LF+T 72.17 81.76
LF+PL+PB 70.81 83.61?
LF+PL+T 72.74 82.13?
LF+PB+T 72.29 83.73?
LF+PL+PB+T 71.85 83.94?
Table 1: Polarity classification results (accuracy in %) using
different features for blogs and reviews.
curacy of 83.94%. We notice that incorporating our pro-
posed transition feature (T) always achieves some gain on
different feature settings, suggesting that those transition
features are useful for sentimental analysis on reviews.
From Table 1, we can see that overall the performance
on blogs is worse than on the review data. We hypoth-
esize this may be due to the large variation in terms of
contents and styles in blogs. Regarding the feature ef-
fectiveness, we also observe some differences between
blogs and reviews. Adding the polarized bigram feature
and transition feature (PB and T) individually can yield
some improvement; however, adding both of them did
not result in any further improvement ? performance de-
grades compared to LF+PB. Interestingly, although ?PL?
feature alone does not seem to help, by adding ?PL? and
?T? together, the performance achieved the best accuracy
of 72.74%. We also found that adding all the features
together hurts the performance, suggesting that different
features interact with each other and some do not com-
bine well (e.g., PB and T features). In addition, all the
improvements here are not statistically significant.
Note that for the blog data, we randomly split them for
the cross validation experiments regardless of the queries.
In order to better understand whether the poor results on
blog data is due to the effect of different queries, we per-
formed another experiment where for each query, we ran-
domly divided the corresponding blogs into training and
test splits. Only 66 queries were kept for this experi-
ments ? we did not include those queries that have fewer
than 10 relevant blogs. The results for the query balanced
split on blogs are shown in Figure 1. We also include re-
sults for the five individual review data sets in order to see
the topic effect. We present results using four represen-
tative feature sets chosen according to Table 1. For the
review data, we notice some difference across different
data sets, suggesting their inherent difference in terms of
task difficulty. We observe slight performance increase
for some feature sets using the query balanced setup for
blog data, but overall it is still much worse than the review
data. This shows that the query unbalanced training/test
split does not explain the performance gap between blogs
and reviews. This is consistent with (Zhang et al, 2007)
that found that a query-independent classifier performs
even better than query-dependent one. We expect that the
310
query unbalanced setup is more realistic, therefore, in the
following experiments, we continue with this setup.
7173
7577
7981
8385
87
Accur
acy(%
)
blog_data
blog_data_query_balancedreview_books
review_dvd
review_kitchen
review_movie
review_elec
Figure 1: Polarity classification results on query balanced blog
data and five individual review data sets.
4 Improving Blog Polarity Classification
To improve the performance of polarity classification on
blogs, we propose two methods: (a) extract only topic-
relevant segments from blogs for sentiment analysis; (b)
apply adaptive methods to leverage review data.
4.1 Using topic-relevant blog context
Generally a review is written towards one product or one
kind of service, but a blog may cover several topics with
possibly different opinions towards each topic. The blog
data we used is annotated based on some specific topics
in the TREC Blog Track evaluation. Take topic 870 in
the data as an example, ?Find opinions on alleged use
of steroids by baseball player Barry?. There is one blog
that talks about 5 different baseball players in issues of
using steroids. Since the reference opinion tag of a blog
is determined by polarity towards the given query topic, it
might be confusing for the classifier if we use the whole
blog to derive features. Recently topic analysis has been
used for polarity classification (Zhang et al, 2007; Titov
and McDonald, 2008; Wiegand and Klakow, 2009). We
take a different approach in this study.
In order to obtain a topic-relevant context, we retrieved
the top 10 relevant sentences corresponding to the given
topic using the Lemur toolkit3. Then we used these sen-
tences and their immediate previous and following sen-
tences for feature extraction in the same way as what
we did on the whole blog. In addition to using all the
words in the relevant context, we also investigated using
only content words since those are more topic indicative
than function words. We extracted content words (nouns,
verbs, adjectives and adverbs) from each blog in their
original order and apply the same feature extraction pro-
cess as for using all the words.
3http://www.lemurproject.org/lemur/
Table 2 shows the blog polarity classification results
using the whole blog vs. relevant context composed of
all the words or only content words. For the significance
test, the comparison was done for using relevant context
with all the words vs. using the whole blog; and us-
ing content words only vs. using all the words in rele-
vant context. Each comparison was with respect to the
same feature setup. We observe improved polarity classi-
fication performance when using sentence retrieval based
topic analysis to extract relevant context. Using all the
words in the topic relevant context, all the improvements
compared to using the original entire blog are statistically
significant at the level of 0.01. We also notice that un-
like on the entire blog document, the ?PL? features con-
tribute positively when combined with ?LF?. All the fea-
ture settings with ?PL? perform very well. The best ac-
curacy of 75.32% is achieved using feature combination
of ?LF+PL? or ?LF+PL+T?. This suggests that polarized
lexical features suffered from the off-topic content when
using the entire blog and are more useful within contexts
of certain topics.
When using content words only, we observe consistent
gain across all the feature sets. Three feature settings,
?LF+PB?,?LF+T? and ?LF+PL+PB+T?, achieve statisti-
cally significant further improvement (compared to using
all the words of relevant contexts). The best accuracy
(75.6%) is achieved by using the ?LF+PB? features.
Feature Set Whole Relevant Context
Blog All Words Content Words
LF 72.07 74.92? 75.14
LF+PL 70.93 75.32? 75.34
LF+PB 72.44 75.03? 75.6?
LF+T 72.17 75.01? 75.35?
LF+PL+PB 70.81 75.27? 75.35
LF+PL+T 72.74 75.32? 75.41
LF+PB+T 72.29 75.17? 75.42
LF+PL+PB+T 71.85 75.21? 75.45?
Table 2: Blog polarity classification results (accuracy in %) us-
ing topic relevant context composed of all the words or only
content words.
4.2 Adaptive methods using review data
Domain adaptation has been studied in some previous
work (e.g., (Blitzer et al, 2007; Mansour et al, 2008)).
In this paper, we evaluate two adaptive approaches in or-
der to leverage review data to improve blog polarity clas-
sification. In the first approach, in each of the 10-fold
cross-validation training, we pool the blog training data
(90% of the entire blog data) together with all the review
data from 5 different domains. In the second method, we
augment features with hypotheses obtained from classi-
fiers trained using other domain data. Specifically, we
first trained 5 classifiers from 5 review domain data sets
respectively, and encoded the hypotheses from different
classifiers as features for blog training (together with the
original features of the blog data). Results of these two
approaches are shown in Table 3. We use the topic rele-
311
vant context with content words only in this experiment,
and present results for different feature combinations (ex-
cept the baseline ?LF? setting). The significance test is
conducted in comparison to the results using only blog
data for training, for the same feature setting.
We find that the first approach does not yield any gain,
even though the added data is about the same size as
the blog data. It indicates that due to the large differ-
ence between the two genres, simply combining blogs
and reviews in training is not effective. However, we
can see that using augmented features in training signifi-
cantly improved the performance across different feature
sets. The best result is achieved using ?LF+T? features,
76.84% compared with the best accuracy of 75.6% when
using the blog data only (?LF+PB? features).
Feature Set Only Blog Pool Data Augment Features
LF+PL 75.34 75.05 76.12?
LF+PB 75.6 74.35 76.28?
LF+T 75.35 74.47 76.84?
LF+PL+PB 75.35 74.94 76.7?
LF+PL+T 75.41 74.85 76.32?
LF+PB+T 75.42 74.46 76.3?
LF+PL+PB+T 75.45 74.96 76.53?
Table 3: Results (accuracy in %) of blog polarity classification
using two methods leveraging review data.
4.3 Error analysis
Notice that after achieving some improvements the per-
formance on blogs is still much worse than on review
data. Thus we performed a manual error analysis for a
better understanding of the difficulties of sentiment anal-
ysis on blog data, and identified the following challenges.
(a) Idiomatic expressions. Compared to reviews, blog-
gers seem to use more idioms. For example, ?Of course
he has me over the barrel...? expresses negative opinion,
however, there are no superficially indicative features.
(b) Ironic writing style. Some bloggers prefer ironic
style especially when speaking against something or
somebody, whereas opinions are often expressed using
plain writing style in reviews. Simply using the surface
word level features is not able to model these properly.
(c) Background knowledge. In some political blogs,
the polarized expressions are implicit. Correctly recog-
nizing them requires background knowledge and deeper
language analysis techniques.
5 Conclusions and Future Work
In this paper, we have evaluated various features and the
domain effect on sentimental polarity classification. Our
experiments on blog and review data demonstrated dif-
ferent feature effectiveness and the overall poorer perfor-
mance on blogs than reviews. We found that the polarized
features and the transition word features we introduced
are useful for polarity classification. We also show that
by extracting topic-relevant context and considering only
content words, the system can achieve significantly better
performance on blogs. Furthermore, an adaptive method
using augmented features can effectively leverage data
from other domains, and yield improvement compared
to using in-domain training or training on combined data
from different domains. For our future work, we plan
to investigate other adaption methods, and try to address
some of the problems identified in our error analysis.
6 Acknowledgment
The authors thank the three anonymous reviewers for
their suggestions.
References
Apoorv Agarwal, Fadi Biadsy, and Kathleen McKeown. 2009.
Contextual phrase-level polarity analysis using lexical affect
scoring and syntactic n-grams. In Proc. of EACL.
John Blitzer, Mark Dredze, and Fernando Pereira. 2007. Bi-
ographies, bollywood, boom-boxes and blenders: Domain
adaptation for sentiment classification. In Proc. of ACL.
Erik Boiy, Pieter Hens, Koen Deschacht, and Marie-Francine
Moens. 2007. Automatic sentiment analysis in on-line text.
In Proc. of ELPUB.
Paula Chesley, Bruce Vincent, Li Xu, and Rohini K. Srihari.
2005. Using verbs and adjectives to automatically classify
blog sentiment. In Proc. of AAAI.
Minqing Hu and Bing Liu. 2004. Mining and summarizing
customer reviews. In Proc. of ACM SIGKDD.
Jungi Kim, Jin-Ji Li, and Jong-Hyeok Lee. 2009. Discovering
the discriminative views: Measuring term weights for senti-
ment analysis. In Proc. of ACL-IJCNLP.
Yishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh.
2008. Domain adaptation with multiple sources. In Proc.
of NIPS.
Karo Moilanen and Stephen Pulman. 2008. The good, the bad,
and the unknown: Morphosyllabic sentiment tagging of un-
seen words. In Proc. of ACL.
Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting class
relationships for sentiment categorization with respect to rat-
ing scales. In Proc. of ACL.
Bo Pang, Lilian Lee, and Shrivakumar Vaithyanathan. 2002.
Thumbs up? sentiment classification using machine learning
techniques. In Proc. of EMNLP.
Ellen Riloff and Janyce Wiebe. 2003. Learning extraction pat-
terns for subjective expressions. In Proc. of EMNLP.
Ivan Titov and Ryan McDonald. 2008. Modeling online re-
views with multi-grain topic models. In Proc. of WWW.
Peter D. Turney. 2002. Thumbs up or thumbs down? semantic
orientation applied to unsupervised classification of reviews.
In Proc. of ACL.
Michael Wiegand and Dietrich Klakow. 2009. Topic-Related
polarity classification of blog sentences. In Proc. of the 14th
Portuguese Conference on Artificial Intelligence: Progress
in Artificial Intelligence, pages 658?669.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005.
Recognizing contextual polarity in phrase-level sentiment
analysis. In Proc. of HLT-EMNLP.
Wei Zhang, Clement Yu, and Weiyi Meng. 2007. Opinion re-
trieval from blogs. In Proc. of CIKM, pages 831?840.
312
Proceedings of NAACL-HLT 2013, pages 820?825,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Disfluency Detection Using Multi-step Stacked Learning
Xian Qian and Yang Liu
Computer Science Department
The University of Texas at Dallas
{qx,yangl}@hlt.utdallas.edu
Abstract
In this paper, we propose a multi-step stacked
learning model for disfluency detection. Our
method incorporates refined n-gram features
step by step from different word sequences.
First, we detect filler words. Second, edited
words are detected using n-gram features ex-
tracted from both the original text and filler fil-
tered text. In the third step, additional n-gram
features are extracted from edit removed texts
together with our newly induced in-between
features to improve edited word detection. We
useMax-MarginMarkov Networks (M3Ns) as
the classifier with the weighted hamming loss
to balance precision and recall. Experiments
on the Switchboard corpus show that the re-
fined n-gram features from multiple steps and
M3Ns with weighted hamming loss can signif-
icantly improve the performance. Our method
for disfluency detection achieves the best re-
ported F-score 0.841 without the use of addi-
tional resources.1
1 Introduction
Detecting disfluencies in spontaneous speech can
be used to clean up speech transcripts, which help-
s improve readability of the transcripts and make it
easy for downstream language processing modules.
There are two types of disfluencies: filler words in-
cluding filled pauses (e.g., ?uh?, ?um?) and discourse
markers (e.g., ?I mean?, ?you know?), and edited
words that are repeated, discarded, or corrected by
1Our source code is available at
http://code.google.com/p/disfluency-detection/downloads/list
the following words. An example is shown below
that includes edited words and filler words.
I want a flight to Boston
? ?? ?
edited
uh I mean
? ?? ?
filler
to Denver
Automatic filler word detection is much more ac-
curate than edit detection as they are often fixed
phrases (e.g., ?uh?, ?you know?, ?I mean?), hence
our work focuses on edited word detection.
Many models have been evaluated for this task.
Liu et al (2006) used Conditional Random Fields
(CRFs) for sentence boundary and edited word de-
tection. They showed that CRFs significantly out-
performed Maximum Entropy models and HMM-
s. Johnson and Charniak (2004) proposed a TAG-
based noisy channel model which showed great im-
provement over boosting based classifier (Charniak
and Johnson, 2001). Zwarts and Johnson (2011)
extended this model using minimal expected F-loss
oriented n-best reranking. They obtained the best re-
ported F-score of 83.8% on the Switchboard corpus.
Georgila (2009) presented a post-processing method
during testing based on Integer Linear Programming
(ILP) to incorporate local and global constraints.
From the view of features, in addition to tex-
tual information, prosodic features extracted from
speech have been incorporated to detect edited
words in some previous work (Kahn et al, 2005;
Zhang et al, 2006; Liu et al, 2006). Zwarts and
Johnson (2011) trained an extra language model on
additional corpora, and used output log probabili-
ties of language models as features in the reranking
stage. They reported that the language model gained
about absolute 3% F-score for edited word detection
on the Switchboard development dataset.
820
In this paper, we propose a multi-step stacked
learning approach for disfluency detection. In our
method, we first perform filler word detection, then
edited word detection. In every step, we generate
new refined n-gram features based on the processed
text (remove the detected filler or edited words from
the previous step), and use these in the next step.
We also include a new type of features, called in-
between features, and incorporate them into the last
step. For edited word detection, we use Max-Margin
Markov Networks (M3Ns) with weighted hamming
loss as the classifier, as it can well balance the pre-
cision and recall to achieve high performance. On
the commonly used Switchboard corpus, we demon-
strate that our proposed method outperforms other
state-of-the-art systems for edit disfluency detection.
2 Balancing Precision and Recall Using
Weighted M3Ns
We use a sequence labeling model for edit detection.
Each word is assigned one of the five labels: BE (be-
ginning of the multi-word edited region), IE (in the
edited region), EE (end of the edited region), SE (s-
ingle word edited region), O (other). For example,
the previous sentence is represented as:
I/O want/O a/O flight/O to/BE Boston/EE uh/O
I/O mean/O to/O Denver/O
We use the F-score as the evaluation metrics
(Zwarts and Johnson, 2011; Johnson and Charniak,
2004), which is defined as the harmonic mean of the
precision and recall of the edited words:
P = #correctly predicted edited words
#predicted edited words
R = #correctly predicted edited words
#gold standard edited words
F = 2? P ?R
P + R
There are many methods to train the sequence mod-
el, such as CRFs (Lafferty et al, 2001), averaged
structured perceptrons (Collins, 2002), structured
SVM (Altun et al, 2003), online passive aggressive
learning (Crammer et al, 2006). Previous work has
shown that minimizing F-loss is more effective than
minimizing log-loss (Zwarts and Johnson, 2011),
because edited words are much fewer than normal
words.
In this paper, we use Max-margin Markov Net-
works (Taskar et al, 2004) because our preliminary
results showed that they outperform other classifier-
s, and using weighted hamming loss is simple in this
approach (whereas for perceptron or CRFs, the mod-
ification of the objective function is not straightfor-
ward).
The learning task for M3Ns can be represented as
follows:
min
?
1
2
C?
?
x,y
?x,y?f(x, y)?22 +
?
x,y
?x,yL(x, y)
s.t.
?
y
?x,y = 1 ?x
?x,y ? 0, ?x, y
The above shows the dual form for trainingM3Ns,
where x is the observation of a training sample,
y ? Y is a label. ? is the parameter needed
to be optimized, C > 0 is the regularization pa-
rameter. ?f(x, y) is the residual feature vector:
f(x, y?) ? f(x, y), where y? is the true label of x.
L(x, y) is the loss function. Taskar et al (2004) used
un-weighted hamming loss, which is the number
of incorrect components: L(x, y) =
?
t ?(yt, y?t),
where ?(a, b) is the binary indicator function (it is 0
if a = b). In our work, we use the weighted ham-
ming loss:
L(x, y) =
?
t
v(yt, y?t)?(yt, y?t)
where v(yt, y?t) is the weighted loss for the error
when y?t is mislabeled as yt. Such a weighted loss
function allows us to balance the model?s precision
and recall rates. For example, if we assign a large
value to v(O, ?E) (?E denotes SE, BE, IE, EE), then
the classifier is more sensitive to false negative er-
rors (edited word misclassified as non-edited word),
thus we can improve the recall rate. In our work,
we tune the weight matrix v using the development
dataset.
3 Multi-step Stacked Learning for Edit
Disfluency Detection
Rather than just using the above M3Ns with some
features, in this paper we propose to use stacked
learning to incorporate gradually refined n-gram fea-
tures. Stacked learning is a meta-learning approach
(Cohen and de Carvalho, 2005). Its idea is to use two
821
(or more) levels of predictors, where the outputs of
the low level predictors are incorporated as features
into the next level predictors. It has the advantage
of incorporating non-local features as well as non-
linear classifiers. In our task, we do not just use the
classifier?s output (a word is an edited word or not)
as a feature, rather we use such output to remove the
disfluencies and extract new n-gram features for the
subsequent stacked classifiers. We use 10 fold cross
validation to train the low level predictors. The fol-
lowing describes the three steps in our approach.
3.1 Step 1: Filler Word Detection
In the first step, we automatically detect filler word-
s. Since filler words often occur immediately after
edited words (before the corrected words), we ex-
pect that removing them will make rough copy de-
tection easy. For example, in the previous example
shown in Section 1, if ?uh I mean? is removed, then
the reparandum ?to Boston? and repair ?to Denver?
will be adjacent and we can use word/POS based n-
gram features to detect that disfluency. Otherwise,
the classifier needs to skip possible filler words to
find the rough copy of the reparandum.
For filler word detection, similar to edited word
detection, we define 5 labels: BP , IP , EP , SP , O.
We use un-weighted hamming loss to learn M3Ns
for this task. Since for filler word detection, our per-
formance metric is not F-measure, but just the over-
all accuracy in order to generate cleaned text for sub-
sequent n-gram features, we did not use the weight-
ed hamming hoss for this. The features we used are
listed in Table 1. All n-grams are extracted from the
original text.
3.2 Step 2: Edited Word Detection
In the second step, edited words are detected using
M3Ns with the weighted-hamming loss. The fea-
tures we used are listed in Table 2. All n-grams in
the first step are also used here. Besides that, word
n-grams, POS n-grams and logic n-grams extracted
from filler word removed text are included. Feature
templates I(w0, w?i) is to generate features detecting
rough copies separated by filler words.
3.3 Step 3: Refined Edited Word Detection
In this step, we use n-gram features extracted from
the text after removing edit disfluencies based on
unigrams w0, w?1, w1, w?2, w2
p0, p?1, p1, p?2, p2, w0p0
bigrams w?1w0, w0w1, p?1p0, p0p1
trigrams p?2p?1p0, p?1p0p1, p0p1p2
logic unigrams I(wi, w0), I(pi, p0), ?4 ? i ? 4
logic bigrams I(wi?1wi, w?1, w0)
I(pi?1pi, p?1p0)
I(wiwi+1, w0w1)
I(pipi+1, p0p1), ?4 ? i ? 4
transitions y?1y0
Table 1: Feature templates for filler word detection.
w0, p0 denote the current word and POS tag respective-
ly. w?i denotes the ith word to the left, wi denotes the
ith word to the right. The logic function I(a, b) indicates
whether a and b are identical (eigher unigrams or bigram-
s).
All templates in Table 1
unigrams w?1, w?2, w?3, w?4
bigrams p0p?1, p0p?2, p0p?3, p0p?4
w0p?1, w0p?2, w0p?3, w0p?4
w0p1, w0p2, w0p3, w0p4
logic unigrams I(w0, w?i), 1 ? i ? 4
transitions p0y?1y0
Table 2: Feature templates for edit detection (step 2).
w?i, p?i denote the ith word/POS tag to the right in the filler
words removed text. If current word w0 is removed in
step 1, we use its original n-gram features rather than the
refined n-gram features.
the previous step. According to our analysis of the
errors produced by step 2, we observed that many
errors occurred at the boundaries of the disfluen-
cies, and the word bigrams after removing the edited
words are unnatural. The following is an example:
? Ref: The new type is prettier than what
their/SE they used to look like.
? Sys: The new type is prettier than what/BE
their/EE they used to look like.
Using the system?s prediction, we would have bi-
gram than they, which is odd. Usually, the pronoun
following than is accusative case. We expect adding
n-gram features derived from the cleaned-up sen-
tences would allow the new classifier to fix such hy-
pothesis. This kind of n-gram features is similar to
the language models used in (Zwarts and Johnson,
822
2011). They have the benefit of measuring the flu-
ency of the cleaned text.
Another common error we noticed is caused by
the ambiguities of coordinates, because the coordi-
nates have similar patterns as rough copies. For ex-
ample,
? Coordinates: they ca n?t decide which are the
good aspects and which are the bad aspects
? Rough Copies: it/BE ?s/IE a/IE pleasure/IE
to/EE it s good to get outside
To distinguish the rough copies and the coordinate
examples shown above, we analyze the training data
statistically. We extract all the pieces lying between
identical word bigrams AB . . . AB. The observation
is that coordinates are often longer than edited se-
quences. Hence we introduce the in-between fea-
tures for each word. If a word lies between identical
word bigrams, then its in-between feature is the log
length of the subsequence lying between the two bi-
grams; otherwise, it is zero (we use log length to
avoid sparsity). We also used other patterns such as
A . . . A and ABC . . . ABC, but they are too noisy or
infrequent and do not yield much performance gain.
Table 3 lists the feature templates used in this last
step.
All templates in Table 1, Table 2
word n-grams w??1 , w0w??1
in-between LAB , w0bAB , bAB
Table 3: Feature templates for refined edit detection (step
3). w??i denotes the ith word tag to the right in the edit-
ed word removed text. LAB denotes the log length of
the sub-sequence in the pattern AB. . . AB, bAB indicates
whether the current word lies between two identical bi-
grams.
4 Experiments
4.1 Experimental Setup
We use the Switchboard corpus in our experimen-
t, with the same train/develop/test split as the pre-
vious work (Johnson and Charniak, 2004). We al-
so remove the partial words and punctuation from
the training and test data for the reason to simulate
the situation when speech recognizers are used and
such kind of information is not available (Johnson
and Charniak, 2004).
We tuned the weight matrix for hamming loss on
the development dataset using simple grid search.
The diagonal elements are fixed at 0; for false pos-
itive errors, O ? ?E (non-edited word mis-labeled
as edited word), their weights are fixed at 1; for false
negative errors, ?E ? O, we tried the weight from
1 to 3, and increased the weight 0.5 each time. The
optimal weight matrix is shown in Table 4. Note
that we use five labels in the sequence labeling task;
however, for edited word detection evaluation, it is
only a binary task, that is, all of the words labeled
with ?E will be mapped to the class of edited words.
P
P
P
P
P
P
P
truth
predict BE IE EE SE O
BE 0 1 1 1 2
IE 1 0 1 1 2
EE 1 1 0 1 2
SE 1 1 1 0 2
O 1 1 1 1 0
Table 4: Weighted hamming loss for M3Ns.
4.2 Results
We compare several sequence labeling models:
CRFs, structured averaged perceptron (AP), M3Ns
with un-weighted/weighted loss, and online passive-
aggressive (PA) learning. For each model, we tuned
the parameters on the development data: Gaussian
prior for CRFs is 1.0, iteration number for AP is 10,
iteration number and regularization penalty for PA
are 10 and 1. For M3Ns, we use Structured Sequen-
tial Minimal Optimization (Taskar, 2004) for model
training. Regularization penalty is C = 0.1 and iter-
ation number is 30.
Table 5 shows the results using different models
and features. The baseline models use only the n-
grams features extracted from the original text. We
can see that M3Ns with the weighted hamming loss
achieve the best performance, outperforming all the
other models. Regarding the features, the gradually
added n-gram features have consistent improvemen-
t for all models. Using the weighted hamming loss
in M3Ns, we observe a gain of 2.2% after deleting
filler words, and 1.8% after deleting edited words. In
our analysis, we also noticed that the in-between fea-
823
CRF AP PA M3N w. M3N
Baseline 78.8 79.0 78.9 79.4 80.1
Step 2 81.0 81.1 81.1 81.5 82.3
Step 3 82.9 83.0 82.8 83.3 84.1
Table 5: Effect of training strategy and recovered features
for stacked learning. F scores are reported. AP = Aver-
aged Perceptron, PA = online Passive Aggresive, M3N =
un-weighted M3Ns, w. M3N = weighted M3Ns.
tures yield about 1% improvement in F-score for all
models (the gain of step 3 over step 2 is because of
the in-between features and the new n-gram features
extracted from the text after removing previously
detected edited words). We performed McNemar?s
test to evaluate the significance of the difference a-
mong various methods, and found that when using
the same features, weighted M3Ns significantly out-
performs all the other models (p value < 0.001).
There are no significant differences among CRFs,
AP and PA. Using recovered n-gram features and in-
between features significantly improves all sequence
labeling models (p value < 0.001).
We also list the state-of-the-art systems evaluat-
ed on the same dataset, as shown in Table 6. We
achieved the best F-score. The most competitive
system is (Zwarts and Johnson, 2011), which uses
extra resources to train language models.
System F score
(Johnson and Charniak, 2004) 79.7
(Kahn et al, 2005) 78.2
(Zhang et al, 2006)? 81.2
(Georgila, 2009)? 80.1
(Zwarts and Johnson, 2011)+ 83.8
This paper 84.1
Table 6: Comparison with other systems. ? they used
the re-segmented Switchboard corpus, which is not ex-
actly the same as ours. ? they reported the F-score of
BE tag (beginning of the edited sequences). + they used
language model learned from 3 additional corpora.
5 Conclusion
In this paper, we proposed multi-step stacked learn-
ing to extract n-gram features step by step. The first
level removes the filler words providing new ngram-
s for the second level to remove edited words. The
third level uses the n-grams from the original tex-
t and the cleaned text generated by the previous t-
wo steps for accurate edit detection. To minimize
the F-loss approximately, we modified the hamming
loss in M3Ns. Experimental results show that our
method is effective, and achieved the best reported
performance on the Switchboard corpus without the
use of any additional resources.
Acknowledgments
We thank three anonymous reviewers for their valu-
able comments. This work is partly supported by
DARPA under Contract No. HR0011-12-C-0016
and FA8750-13-2-0041. Any opinions expressed in
this material are those of the authors and do not nec-
essarily reflect the views of DARPA.
References
Yasemin Altun, Ioannis Tsochantaridis, and Thomas
Hofmann. 2003. Hidden markov support vector ma-
chines. In Proc. of ICML.
Eugene Charniak and Mark Johnson. 2001. Edit detec-
tion and parsing for transcribed speech. In Proc. of
NAACL.
William W. Cohen and Vitor Rocha de Carvalho. 2005.
Stacked sequential learning. In Proc. of IJCAI.
Michael Collins. 2002. Discriminative training methods
for hidden markov models: Theory and experiments
with perceptron algorithms. In Proc. of EMNLP.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, Yoram Singer, and Yoram Singer. 2006. On-
line passive-aggressive algorithms. Journal of Ma-
chine Learning Research.
Kallirroi Georgila. 2009. Using integer linear program-
ming for detecting speech disfluencies. In Proc. of
NAACL.
Mark Johnson and Eugene Charniak. 2004. A TAG-
based noisy-channel model of speech repairs. In Proc.
of ACL.
Jeremy G. Kahn, Matthew Lease, Eugene Charniak,
Mark Johnson, and Mari Ostendorf. 2005. Effective
use of prosody in parsing conversational speech. In
Proc. of HLT-EMNLP.
John D. Lafferty, AndrewMcCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Probabilis-
tic models for segmenting and labeling sequence data.
In Proc. of ICML.
Yang Liu, E. Shriberg, A. Stolcke, D. Hillard, M. Osten-
dorf, and M. Harper. 2006. Enriching speech recog-
nition with automatic detection of sentence bound-
824
aries and disfluencies. IEEE Transactions on Audio,
Speech, and Language Processing, 14(5).
Ben Taskar, Carlos Guestrin, and Daphne Koller. 2004.
Max-margin markov networks. In Proc. of NIPS.
Ben Taskar. 2004. Learning Structured Prediction Mod-
els: A Large Margin Approach. Ph.D. thesis, Stanford
University.
Qi Zhang, Fuliang Weng, and Zhe Feng. 2006. A pro-
gressive feature selection algorithm for ultra large fea-
ture spaces. In Proc. of ACL.
Simon Zwarts and Mark Johnson. 2011. The impact of
language models and loss functions on repair disfluen-
cy detection. In Proc. of ACL-HLT.
825
Proceedings of the ACL 2010 Conference Short Papers, pages 12?16,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Learning Lexicalized Reordering Models from Reordering Graphs
Jinsong Su, Yang Liu, Yajuan Lu?, Haitao Mi, Qun Liu
Key Laboratory of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences
P.O. Box 2704, Beijing 100190, China
{sujinsong,yliu,lvyajuan,htmi,liuqun}@ict.ac.cn
Abstract
Lexicalized reordering models play a crucial
role in phrase-based translation systems. They
are usually learned from the word-aligned
bilingual corpus by examining the reordering
relations of adjacent phrases. Instead of just
checking whether there is one phrase adjacent
to a given phrase, we argue that it is important
to take the number of adjacent phrases into
account for better estimations of reordering
models. We propose to use a structure named
reordering graph, which represents all phrase
segmentations of a sentence pair, to learn lex-
icalized reordering models efficiently. Exper-
imental results on the NIST Chinese-English
test sets show that our approach significantly
outperforms the baseline method.
1 Introduction
Phrase-based translation systems (Koehn et al,
2003; Och and Ney, 2004) prove to be the state-
of-the-art as they have delivered translation perfor-
mance in recent machine translation evaluations.
While excelling at memorizing local translation and
reordering, phrase-based systems have difficulties in
modeling permutations among phrases. As a result,
it is important to develop effective reordering mod-
els to capture such non-local reordering.
The early phrase-based paradigm (Koehn et al,
2003) applies a simple distance-based distortion
penalty to model the phrase movements. More re-
cently, many researchers have presented lexicalized
reordering models that take advantage of lexical
information to predict reordering (Tillmann, 2004;
Xiong et al, 2006; Zens and Ney, 2006; Koehn et
Figure 1: Occurrence of a swap with different numbers
of adjacent bilingual phrases: only one phrase in (a) and
three phrases in (b). Black squares denote word align-
ments and gray rectangles denote bilingual phrases. [s,t]
indicates the target-side span of bilingual phrase bp and
[u,v] represents the source-side span of bilingual phrase
bp.
al., 2007; Galley and Manning, 2008). These mod-
els are learned from a word-aligned corpus to pre-
dict three orientations of a phrase pair with respect
to the previous bilingual phrase: monotone (M ),
swap (S), and discontinuous (D). Take the bilingual
phrase bp in Figure 1(a) for example. The word-
based reordering model (Koehn et al, 2007) ana-
lyzes the word alignments at positions (s?1, u?1)
and (s ? 1, v + 1). The orientation of bp is set
to D because the position (s ? 1, v + 1) contains
no word alignment. The phrase-based reordering
model (Tillmann, 2004) determines the presence
of the adjacent bilingual phrase located in position
(s? 1, v+1) and then treats the orientation of bp as
S. Given no constraint on maximum phrase length,
the hierarchical phrase reordering model (Galley and
Manning, 2008) also analyzes the adjacent bilingual
phrases for bp and identifies its orientation as S.
However, given a bilingual phrase, the above-
mentioned models just consider the presence of an
adjacent bilingual phrase rather than the number of
adjacent bilingual phrases. See the examples in Fig-
12
Figure 2: (a) A parallel Chinese-English sentence pair and (b) its corresponding reordering graph. In (b), we denote
each bilingual phrase with a rectangle, where the upper and bottom numbers in the brackets represent the source
and target spans of this bilingual phrase respectively. M = monotone (solid lines), S = swap (dotted line), and D =
discontinuous (segmented lines). The bilingual phrases marked in the gray constitute a reordering example.
ure 1 for illustration. In Figure 1(a), bp is in a swap
order with only one bilingual phrase. In Figure 1(b),
bp swaps with three bilingual phrases. Lexicalized
reordering models do not distinguish different num-
bers of adjacent phrase pairs, and just give bp the
same count in the swap orientation.
In this paper, we propose a novel method to better
estimate the reordering probabilities with the con-
sideration of varying numbers of adjacent bilingual
phrases. Our method uses reordering graphs to rep-
resent all phrase segmentations of parallel sentence
pairs, and then gets the fractional counts of bilin-
gual phrases for orientations from reordering graphs
in an inside-outside fashion. Experimental results
indicate that our method achieves significant im-
provements over the traditional lexicalized reorder-
ing model (Koehn et al, 2007).
This paper is organized as follows: in Section 2,
we first give a brief introduction to the traditional
lexicalized reordering model. Then we introduce
our method to estimate the reordering probabilities
from reordering graphs. The experimental results
are reported in Section 3. Finally, we end with a
conclusion and future work in Section 4.
2 Estimation of Reordering Probabilities
Based on Reordering Graph
In this section, we first describe the traditional lexi-
calized reordering model, and then illustrate how to
construct reordering graphs to estimate the reorder-
ing probabilities.
2.1 Lexicalized Reordering Model
Given a phrase pair bp = (ei, fai), where ai de-
fines that the source phrase fai is aligned to the
target phrase ei, the traditional lexicalized reorder-
ing model computes the reordering count of bp in
the orientation o based on the word alignments of
boundary words. Specifically, the model collects
bilingual phrases and distinguishes their orientations
with respect to the previous bilingual phrase into
three categories:
o =
?
??
??
M ai ? ai?1 = 1
S ai ? ai?1 = ?1
D |ai ? ai?1| 6= 1
(1)
Using the relative-frequency approach, the re-
ordering probability regarding bp is
p(o|bp) = Count(o, bp)?
o? Count(o?, bp)
(2)
2.2 Reordering Graph
For a parallel sentence pair, its reordering graph in-
dicates all possible translation derivations consisting
of the extracted bilingual phrases. To construct a
reordering graph, we first extract bilingual phrases
using the way of (Och, 2003). Then, the adjacent
13
bilingual phrases are linked according to the target-
side order. Some bilingual phrases, which have
no adjacent bilingual phrases because of maximum
length limitation, are linked to the nearest bilingual
phrases in the target-side order.
Shown in Figure 2(b), the reordering graph for
the parallel sentence pair (Figure 2(a)) can be rep-
resented as an undirected graph, where each rect-
angle corresponds to a phrase pair, each link is the
orientation relationship between adjacent bilingual
phrases, and two distinguished rectangles bs and be
indicate the beginning and ending of the parallel sen-
tence pair, respectively. With the reordering graph,
we can obtain all reordering examples containing
the given bilingual phrase. For example, the bilin-
gual phrase ?zhengshi huitan, formal meetings? (see
Figure 2(a)), corresponding to the rectangle labeled
with the source span [6,7] and the target span [4,5],
is in a monotone order with one previous phrase
and in a discontinuous order with two subsequent
phrases (see Figure 2(b)).
2.3 Estimation of Reordering Probabilities
We estimate the reordering probabilities from re-
ordering graphs. Given a parallel sentence pair,
there are many translation derivations correspond-
ing to different paths in its reordering graph. As-
suming all derivations have a uniform probability,
the fractional counts of bilingual phrases for orien-
tations can be calculated by utilizing an algorithm in
the inside-outside fashion.
Given a phrase pair bp in the reordering graph,
we denote the number of paths from bs to bp with
?(bp). It can be computed in an iterative way
?(bp) = ?bp? ?(bp?), where bp? is one of the pre-
vious bilingual phrases of bp and ?(bs)=1. In a sim-
ilar way, the number of paths from be to bp, notated
as ?(bp), is simply ?(bp) = ?bp?? ?(bp??), where
bp?? is one of the subsequent bilingual phrases of bp
and ?(be)=1. Here, we show the ? and ? values of
all bilingual phrases of Figure 2 in Table 1. Espe-
cially, for the reordering example consisting of the
bilingual phrases bp1=?jiang juxing, will hold? and
bp2=?zhengshi huitan, formal meetings?, marked in
the gray color in Figure 2, the ? and ? values can be
calculated: ?(bp1) = 1, ?(bp2) = 1+1 = 2, ?(bs) =
8+1 = 9.
Inspired by the parsing literature on pruning
src span trg span ? ?
[0, 0] [0, 0] 1 9
[1, 1] [1, 1] 1 8
[1, 7] [1, 7] 1 1
[4, 4] [2, 2] 1 1
[4, 5] [2, 3] 1 3
[4, 6] [2, 4] 1 1
[4, 7] [2, 5] 1 2
[2, 7] [2, 7] 1 1
[5, 5] [3, 3] 1 1
[6, 6] [4, 4] 2 1
[6, 7] [4, 5] 1 2
[7, 7] [5, 5] 3 1
[2, 2] [6, 6] 5 1
[2, 3] [6, 7] 2 1
[3, 3] [7, 7] 5 1
[8, 8] [8, 8] 9 1
Table 1: The ? and ? values of the bilingual phrases
shown in Figure 2.
(Charniak and Johnson, 2005; Huang, 2008), the
fractional count of (o, bp?, bp) is
Count(o, bp?, bp) = ?(bp
?) ? ?(bp)
?(bs) (3)
where the numerator indicates the number of paths
containing the reordering example (o, bp?, bp) and
the denominator is the total number of paths in the
reordering graph. Continuing with the reordering
example described above, we obtain its fractional
count using the formula (3): Count(M, bp1, bp2) =
(1? 2)/9 = 2/9.
Then, the fractional count of bp in the orientation
o is calculated as described below:
Count(o, bp) =
?
bp?
Count(o, bp?, bp) (4)
For example, we compute the fractional count of
bp2 in the monotone orientation by the formula (4):
Count(M, bp2) = 2/9.
As described in the lexicalized reordering model
(Section 2.1), we apply the formula (2) to calculate
the final reordering probabilities.
3 Experiments
We conduct experiments to investigate the effec-
tiveness of our method on the msd-fe reorder-
ing model and the msd-bidirectional-fe reordering
model. These two models are widely applied in
14
phrase-based system (Koehn et al, 2007). The msd-
fe reordering model has three features, which rep-
resent the probabilities of bilingual phrases in three
orientations: monotone, swap, or discontinuous. If a
msd-bidirectional-fe model is used, then the number
of features doubles: one for each direction.
3.1 Experiment Setup
Two different sizes of training corpora are used in
our experiments: one is a small-scale corpus that
comes from FBIS corpus consisting of 239K bilin-
gual sentence pairs, the other is a large-scale corpus
that includes 1.55M bilingual sentence pairs from
LDC. The 2002 NIST MT evaluation test data is
used as the development set and the 2003, 2004,
2005 NIST MT test data are the test sets. We
choose the MOSES1 (Koehn et al, 2007) as the ex-
perimental decoder. GIZA++ (Och and Ney, 2003)
and the heuristics ?grow-diag-final-and? are used to
generate a word-aligned corpus, where we extract
bilingual phrases with maximum length 7. We use
SRILM Toolkits (Stolcke, 2002) to train a 4-gram
language model on the Xinhua portion of Gigaword
corpus.
In exception to the reordering probabilities, we
use the same features in the comparative experi-
ments. During decoding, we set ttable-limit = 20,
stack = 100, and perform minimum-error-rate train-
ing (Och, 2003) to tune various feature weights. The
translation quality is evaluated by case-insensitive
BLEU-4 metric (Papineni et al, 2002). Finally, we
conduct paired bootstrap sampling (Koehn, 2004) to
test the significance in BLEU scores differences.
3.2 Experimental Results
Table 2 shows the results of experiments with the
small training corpus. For the msd-fe model, the
BLEU scores by our method are 30.51 32.78 and
29.50, achieving absolute improvements of 0.89,
0.66 and 0.62 on the three test sets, respectively. For
the msd-bidirectional-fe model, our method obtains
BLEU scores of 30.49 32.73 and 29.24, with abso-
lute improvements of 1.11, 0.73 and 0.60 over the
baseline method.
1The phrase-based lexical reordering model (Tillmann,
2004) is also closely related to our model. However, due to
the limit of time and space, we only use Moses-style reordering
model (Koehn et al, 2007) as our baseline.
model method MT-03 MT-04 MT-05
baseline 29.62 32.12 28.88m-f RG 30.51?? 32.78?? 29.50?
baseline 29.38 32.00 28.64m-b-f RG 30.49?? 32.73?? 29.24?
Table 2: Experimental results with the small-scale cor-
pus. m-f: msd-fe reordering model. m-b-f: msd-
bidirectional-fe reordering model. RG: probabilities esti-
mation based on Reordering Graph. * or **: significantly
better than baseline (p < 0 .05 or p < 0 .01 ).
model method MT-03 MT-04 MT-05
baseline 31.58 32.39 31.49m-f RG 32.44?? 33.24?? 31.64
baseline 32.43 33.07 31.69m-b-f RG 33.29?? 34.49?? 32.79??
Table 3: Experimental results with the large-scale cor-
pus.
Table 3 shows the results of experiments with
the large training corpus. In the experiments of
the msd-fe model, in exception to the MT-05 test
set, our method is superior to the baseline method.
The BLEU scores by our method are 32.44, 33.24
and 31.64, which obtain 0.86, 0.85 and 0.15 gains
on three test set, respectively. For the msd-
bidirectional-fe model, the BLEU scores produced
by our approach are 33.29, 34.49 and 32.79 on the
three test sets, with 0.86, 1.42 and 1.1 points higher
than the baseline method, respectively.
4 Conclusion and Future Work
In this paper, we propose a method to improve the
reordering model by considering the effect of the
number of adjacent bilingual phrases on the reorder-
ing probabilities estimation. Experimental results on
NIST Chinese-to-English tasks demonstrate the ef-
fectiveness of our method.
Our method is also general to other lexicalized
reordering models. We plan to apply our method
to the complex lexicalized reordering models, for
example, the hierarchical reordering model (Galley
and Manning, 2008) and the MEBTG reordering
model (Xiong et al, 2006). In addition, how to fur-
ther improve the reordering model by distinguishing
the derivations with different probabilities will be-
come another study emphasis in further research.
15
Acknowledgement
The authors were supported by National Natural Sci-
ence Foundation of China, Contracts 60873167 and
60903138. We thank the anonymous reviewers for
their insightful comments. We are also grateful to
Hongmei Zhao and Shu Cai for their helpful feed-
back.
References
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proc. of ACL 2005, pages 173?180.
Michel Galley and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. In Proc. of EMNLP 2008, pages 848?856.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proc. of ACL 2008,
pages 586?594.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
of HLT-NAACL 2003, pages 127?133.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proc. of
ACL 2007, Demonstration Session, pages 177?180.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proc. of EMNLP
2004, pages 388?395.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Franz Joseph Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics, pages 417?449.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proc. of ACL 2003,
pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proc. of ACL 2002,
pages 311?318.
Andreas Stolcke. 2002. Srilm - an extensible language
modeling toolkit. In Proc. of ICSLP 2002, pages 901?
904.
Christoph Tillmann. 2004. A unigram orientation model
for statistical machine translation. In Proc. of HLT-
ACL 2004, Short Papers, pages 101?104.
Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Maxi-
mum entropy based phrase reordering model for statis-
tical machine translation. In Proc. of ACL 2006, pages
521?528.
Richard Zens and Hermann Ney. 2006. Discriminvative
reordering models for statistical machine translation.
In Proc. of Workshop on Statistical Machine Transla-
tion 2006, pages 521?528.
16
Tutorial Abstracts of ACL 2010, page 2,
Uppsala, Sweden, 11 July 2010. c?2010 Association for Computational Linguistics
Tree-based and Forest-based Translation
Yang Liu
Institute of Computing Technology
Chinese Academy of Sciences
yliu@ict.ac.cn
Liang Huang
Information Sciences Institute
University of Southern California
lhuang@isi.edu
1 Introduction
The past several years have witnessed rapid ad-
vances in syntax-based machine translation, which
exploits natural language syntax to guide transla-
tion. Depending on the type of input, most of these
efforts can be divided into two broad categories:
(a) string-based systems whose input is a string,
which is simultaneously parsed and translated by a
synchronous grammar (Wu, 1997; Chiang, 2005;
Galley et al, 2006), and (b) tree-based systems
whose input is already a parse tree to be directly
converted into a target tree or string (Lin, 2004;
Ding and Palmer, 2005; Quirk et al, 2005; Liu et
al., 2006; Huang et al, 2006).
Compared with their string-based counterparts,
tree-based systems offer many attractive features:
they are much faster in decoding (linear time vs.
cubic time), do not require sophisticated bina-
rization (Zhang et al, 2006), and can use sepa-
rate grammars for parsing and translation (e.g. a
context-free grammar for the former and a tree
substitution grammar for the latter).
However, despite these advantages, most tree-
based systems suffer from a major drawback: they
only use 1-best parse trees to direct translation,
which potentially introduces translation mistakes
due to parsing errors (Quirk and Corston-Oliver,
2006). This situation becomes worse for resource-
poor source languages without enough Treebank
data to train a high-accuracy parser.
This problem can be alleviated elegantly by us-
ing packed forests (Huang, 2008), which encodes
exponentially many parse trees in a polynomial
space. Forest-based systems (Mi et al, 2008; Mi
and Huang, 2008) thus take a packed forest instead
of a parse tree as an input. In addition, packed
forests could also be used for translation rule ex-
traction, which helps alleviate the propagation of
parsing errors into rule set. Forest-based transla-
tion can be regarded as a compromise between the
string-based and tree-based methods, while com-
bining the advantages of both: decoding is still
fast, yet does not commit to a single parse. Sur-
prisingly, translating a forest of millions of trees
is even faster than translating 30 individual trees,
and offers significantly better translation quality.
This approach has since become a popular topic.
2 Content Overview
This tutorial surveys tree-based and forest-based
translation methods. For each approach, we will
discuss the two fundamental tasks: decoding,
which performs the actual translation, and rule ex-
traction, which learns translation rules from real-
world data automatically. Finally, we will in-
troduce some more recent developments to tree-
based and forest-based translation, such as tree
sequence based models, tree-to-tree models, joint
parsing and translation, and faster decoding algo-
rithms. We will conclude our talk by pointing out
some directions for future work.
3 Tutorial Overview
1. Tree-based Translation
? Motivations and Overview
? Tree-to-String Model and Decoding
? Tree-to-String Rule Extraction
? Language Model-Integrated Decoding:
Cube Pruning
2. Forest-based Translation
? Packed Forest
? Forest-based Decoding
? Forest-based Rule Extraction
3. Extensions
? Tree-Sequence-to-String Models
? Tree-to-Tree Models
? Joint Parsing and Translation
? Faster Decoding Methods
4. Conclusion and Open Problems
2
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 331?339,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
A Pilot Study of Opinion Summarization in Conversations
Dong Wang Yang Liu
The University of Texas at Dallas
dongwang,yangl@hlt.utdallas.edu
Abstract
This paper presents a pilot study of opinion
summarization on conversations. We create
a corpus containing extractive and abstrac-
tive summaries of speaker?s opinion towards
a given topic using 88 telephone conversa-
tions. We adopt two methods to perform ex-
tractive summarization. The first one is a
sentence-ranking method that linearly com-
bines scores measured from different aspects
including topic relevance, subjectivity, and
sentence importance. The second one is a
graph-based method, which incorporates topic
and sentiment information, as well as addi-
tional information about sentence-to-sentence
relations extracted based on dialogue struc-
ture. Our evaluation results show that both
methods significantly outperform the baseline
approach that extracts the longest utterances.
In particular, we find that incorporating di-
alogue structure in the graph-based method
contributes to the improved system perfor-
mance.
1 Introduction
Both sentiment analysis (opinion recognition) and
summarization have been well studied in recent
years in the natural language processing (NLP) com-
munity. Most of the previous work on sentiment
analysis has been conducted on reviews. Summa-
rization has been applied to different genres, such
as news articles, scientific articles, and speech do-
mains including broadcast news, meetings, conver-
sations and lectures. However, opinion summariza-
tion has not been explored much. This can be use-
ful for many domains, especially for processing the
increasing amount of conversation recordings (tele-
phone conversations, customer service, round-table
discussions or interviews in broadcast programs)
where we often need to find a person?s opinion or
attitude, for example, ?how does the speaker think
about capital punishment and why??. This kind of
questions can be treated as a topic-oriented opin-
ion summarization task. Opinion summarization
was run as a pilot task in Text Analysis Conference
(TAC) in 2008. The task was to produce summaries
of opinions on specified targets from a set of blog
documents. In this study, we investigate this prob-
lem using spontaneous conversations. The problem
is defined as, given a conversation and a topic, a
summarization system needs to generate a summary
of the speaker?s opinion towards the topic.
This task is built upon opinion recognition and
topic or query based summarization. However, this
problem is challenging in that: (a) Summarization in
spontaneous speech is more difficult than well struc-
tured text (Mckeown et al, 2005), because speech
is always less organized and has recognition errors
when using speech recognition output; (b) Senti-
ment analysis in dialogues is also much harder be-
cause of the genre difference compared to other do-
mains like product reviews or news resources, as re-
ported in (Raaijmakers et al, 2008); (c) In conversa-
tional speech, information density is low and there
are often off topic discussions, therefore presenting
a need to identify utterances that are relevant to the
topic.
In this paper we perform an exploratory study
on opinion summarization in conversations. We
compare two unsupervised methods that have been
331
widely used in extractive summarization: sentence-
ranking and graph-based methods. Our system at-
tempts to incorporate more information about topic
relevancy and sentiment scores. Furthermore, in
the graph-based method, we propose to better in-
corporate the dialogue structure information in the
graph in order to select salient summary utterances.
We have created a corpus of reasonable size in this
study. Our experimental results show that both
methods achieve better results compared to the base-
line.
The rest of this paper is organized as follows. Sec-
tion 2 briefly discusses related work. Section 3 de-
scribes the corpus and annotation scheme we used.
We explain our opinion-oriented conversation sum-
marization system in Section 4 and present experi-
mental results and analysis in Section 5. Section 6
concludes the paper.
2 Related Work
Research in document summarization has been well
established over the past decades. Many tasks have
been defined such as single-document summariza-
tion, multi-document summarization, and query-
based summarization. Previous studies have used
various domains, including news articles, scientific
articles, web documents, reviews. Recently there
is an increasing research interest in speech sum-
marization, such as conversational telephone speech
(Zhu and Penn, 2006; Zechner, 2002), broadcast
news (Maskey and Hirschberg, 2005; Lin et al,
2009), lectures (Zhang et al, 2007; Furui et al,
2004), meetings (Murray et al, 2005; Xie and Liu,
2010), voice mails (Koumpis and Renals, 2005).
In general speech domains seem to be more diffi-
cult than well written text for summarization. In
previous work, unsupervised methods like Maximal
Marginal Relevance (MMR), Latent Semantic Anal-
ysis (LSA), and supervised methods that cast the ex-
traction problem as a binary classification task have
been adopted. Prior research has also explored using
speech specific information, including prosodic fea-
tures, dialog structure, and speech recognition con-
fidence.
In order to provide a summary over opinions, we
need to find out which utterances in the conversa-
tion contain opinion. Most previous work in senti-
ment analysis has focused on reviews (Pang and Lee,
2004; Popescu and Etzioni, 2005; Ng et al, 2006)
and news resources (Wiebe and Riloff, 2005). Many
kinds of features are explored, such as lexical fea-
tures (unigram, bigram and trigram), part-of-speech
tags, dependency relations. Most of prior work used
classification methods such as naive Bayes or SVMs
to perform the polarity classification or opinion de-
tection. Only a handful studies have used conver-
sational speech for opinion recognition (Murray and
Carenini, 2009; Raaijmakers et al, 2008), in which
some domain-specific features are utilized such as
structural features and prosodic features.
Our work is also related to question answering
(QA), especially opinion question answering. (Stoy-
anov et al, 2005) applies a subjectivity filter based
on traditional QA systems to generate opinionated
answers. (Balahur et al, 2010) answers some spe-
cific opinion questions like ?Why do people criti-
cize Richard Branson?? by retrieving candidate sen-
tences using traditional QA methods and selecting
the ones with the same polarity as the question. Our
work is different in that we are not going to an-
swer specific opinion questions, instead, we provide
a summary on the speaker?s opinion towards a given
topic.
There exists some work on opinion summariza-
tion. For example, (Hu and Liu, 2004; Nishikawa et
al., 2010) have explored opinion summarization in
review domain, and (Paul et al, 2010) summarizes
contrastive viewpoints in opinionated text. How-
ever, opinion summarization in spontaneous conver-
sation is seldom studied.
3 Corpus Creation
Though there are many annotated data sets for the
research of speech summarization and sentiment
analysis, there is no corpus available for opinion
summarization on spontaneous speech. Thus for this
study, we create a new pilot data set using a sub-
set of the Switchboard corpus (Godfrey and Holli-
man, 1997).1 These are conversational telephone
speech between two strangers that were assigned a
topic to talk about for around 5 minutes. They were
told to find the opinions of the other person. There
are 70 topics in total. From the Switchboard cor-
1Please contact the authors to obtain the data.
332
pus, we selected 88 conversations from 6 topics for
this study. Table 1 lists the number of conversations
in each topic, their average length (measured in the
unit of dialogue acts (DA)) and standard deviation
of length.
topic #Conv. avg len stdev
space flight and exploration 6
165.5 71.40
capital punishment 24
gun control 15
universal health insurance 9
drug testing 12
universal public service 22
Table 1: Corpus statistics: topic description, number of
conversations in each topic, average length (number of
dialog acts), and standard deviation.
We recruited 3 annotators that are all undergrad-
uate computer science students. From the 88 con-
versations, we selected 18 (3 from each topic) and
let al three annotators label them in order to study
inter-annotator agreement. The rest of the conversa-
tions has only one annotation.
The annotators have access to both conversation
transcripts and audio files. For each conversation,
the annotator writes an abstractive summary of up
to 100 words for each speaker about his/her opin-
ion or attitude on the given topic. They were told to
use the words in the original transcripts if possible.
Then the annotator selects up to 15 DAs (no mini-
mum limit) in the transcripts for each speaker, from
which their abstractive summary is derived. The se-
lected DAs are used as the human generated extrac-
tive summary. In addition, the annotator is asked
to select an overall opinion towards the topic for
each speaker among five categories: strongly sup-
port, somewhat support, neutral, somewhat against,
strongly against. Therefore for each conversation,
we have an abstractive summary, an extractive sum-
mary, and an overall opinion for each speaker. The
following shows an example of such annotation for
speaker B in a dialogue about ?capital punishment?:
[Extractive Summary]
I think I?ve seen some statistics that say that, uh, it?s
more expensive to kill somebody than to keep them in
prison for life.
committing them mostly is, you know, either crimes of
passion or at the moment
or they think they?re not going to get caught
but you also have to think whether it?s worthwhile on
the individual basis, for example, someone like, uh, jeffrey
dahlmer,
by putting him in prison for life, there is still a possi-
bility that he will get out again.
I don?t think he could ever redeem himself,
but if you look at who gets accused and who are the
ones who actually get executed, it?s very racially related
? and ethnically related
[Abstractive Summary]
B is against capital punishment except under certain
circumstances. B finds that crimes deserving of capital
punishment are ?crimes of the moment? and as a result
feels that capital punishment is not an effective deterrent.
however, B also recognizes that on an individual basis
some criminals can never ?redeem? themselves.
[Overall Opinion]
Somewhat against
Table 2 shows the compression ratio of the extrac-
tive summaries and abstractive summaries as well as
their standard deviation. Because in conversations,
utterance length varies a lot, we use words as units
when calculating the compression ratio.
avg ratio stdev
extractive summaries 0.26 0.13
abstractive summaries 0.13 0.06
Table 2: Compression ratio and standard deviation of ex-
tractive and abstractive summaries.
We measured the inter-annotator agreement
among the three annotators for the 18 conversations
(each has two speakers, thus 36 ?documents? in to-
tal). Results are shown in Table 3. For the ex-
tractive or abstractive summaries, we use ROUGE
scores (Lin, 2004), a metric used to evaluate auto-
matic summarization performance, to measure the
pairwise agreement of summaries from different an-
notators. ROUGE F-scores are shown in the table
for different matches, unigram (R-1), bigram (R-2),
and longest subsequence (R-L). For the overall opin-
ion category, since it is a multiclass label (not binary
decision), we use Krippendorff?s ? coefficient to
measure human agreement, and the difference func-
tion for interval data: ?2ck = (c? k)
2 (where c, k are
the interval values, on a scale of 1 to 5 corresponding
to the five categories for the overall opinion).
We notice that the inter-annotator agreement for
extractive summaries is comparable to other speech
333
extractive summaries
R-1 0.61
R-2 0.52
R-L 0.61
abstractive summaries
R-1 0.32
R-2 0.13
R-L 0.25
overall opinion ? = 0.79
Table 3: Inter-annotator agreement for extractive and ab-
stractive summaries, and overall opinion.
summary annotation (Liu and Liu, 2008). The
agreement on abstractive summaries is much lower
than extractive summaries, which is as expected.
Even for the same opinion or sentence, annotators
use different words in the abstractive summaries.
The agreement for the overall opinion annotation
is similar to other opinion/emotion studies (Wil-
son, 2008b), but slightly lower than the level rec-
ommended by Krippendorff for reliable data (? =
0.8) (Hayes and Krippendorff, 2007), which shows
it is even difficult for humans to determine what
opinion a person holds (support or against some-
thing). Often human annotators have different inter-
pretations about the same sentence, and a speaker?s
opinion/attitude is sometimes ambiguous. Therefore
this also demonstrates that it is more appropriate to
provide a summary rather than a simple opinion cat-
egory to answer questions about a person?s opinion
towards something.
4 Opinion Summarization Methods
Automatic summarization can be divided into ex-
tractive summarization and abstractive summariza-
tion. Extractive summarization selects sentences
from the original documents to form a summary;
whereas abstractive summarization requires genera-
tion of new sentences that represent the most salient
content in the original documents like humans do.
Often extractive summarization is used as the first
step to generate abstractive summary.
As a pilot study for the problem of opinion sum-
marization in conversations, we treat this problem
as an extractive summarization task. This section
describes two approaches we have explored in gen-
erating extractive summaries. The first one is a
sentence-ranking method, in which we measure the
salience of each sentence according to a linear com-
bination of scores from several dimensions. The sec-
ond one is a graph-based method, which incorpo-
rates the dialogue structure in ranking. We choose to
investigate these two methods since they have been
widely used in text and speech summarization, and
perform competitively. In addition, they do not re-
quire a large labeled data set for modeling training,
as needed in some classification or feature based
summarization approaches.
4.1 Sentence Ranking
In this method, we use Equation 1 to assign a score
to each DA s, and select the most highly ranked ones
until the length constriction is satisfied.
score(s) = ?simsim(s,D) + ?relREL(s, topic)
+?sentsentiment(s) + ?lenlength(s)
?
i
?i = 1 (1)
? sim(s,D) is the cosine similarity between DA
s and all the utterances in the dialogue from
the same speaker, D. It measures the rele-
vancy of s to the entire dialogue from the tar-
get speaker. This score is used to represent the
salience of the DA. It has been shown to be an
important indicator in summarization for var-
ious domains. For cosine similarity measure,
we use TF*IDF (term frequency, inverse docu-
ment frequency) term weighting. The IDF val-
ues are obtained using the entire Switchboard
corpus, treating each conversation as a docu-
ment.
? REL(s, topic) measures the topic relevance of
DA s. It is the sum of the topic relevance of all
the words in the DA. We only consider the con-
tent words for this measure. They are identified
using TreeTagger toolkit.2 To measure the rel-
evance of a word to a topic, we use Pairwise
Mutual Information (PMI):
PMI(w, topic) = log2
p(w&topic)
p(w)p(topic)
(2)
2http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/De
cisionTreeTagger.html
334
where all the statistics are collected from the
Switchboard corpus: p(w&topic) denotes the
probability that word w appears in a dialogue
of topic t, and p(w) is the probability of w ap-
pearing in a dialogue of any topic. Since our
goal is to rank DAs in the same dialog, and
the topic is the same for all the DAs, we drop
p(topic) when calculating PMI scores. Be-
cause the value of PMI(w, topic) is negative,
we transform it into a positive one (denoted
by PMI+(w, topic)) by adding the absolute
value of the minimum value. The final rele-
vance score of each sentence is normalized to
[0, 1] using linear normalization:
RELorig(s, topic) =
?
w?s
PMI+(w, topic)
REL(s, topic) =
RELorig(s, topic)?Min
Max?Min
? sentiment(s) indicates the probability that ut-
terance s contains opinion. To obtain this,
we trained a maximum entropy classifier with
a bag-of-words model using a combination
of data sets from several domains, including
movie data (Pang and Lee, 2004), news articles
from MPQA corpus (Wilson and Wiebe, 2003),
and meeting transcripts from AMI corpus (Wil-
son, 2008a). Each sentence (or DA) in these
corpora is annotated as ?subjective? or ?objec-
tive?. We use each utterance?s probability of
being ?subjective? predicted by the classifier as
its sentiment score.
? length(s) is the length of the utterance. This
score can effectively penalize the short sen-
tences which typically do not contain much
important content, especially the backchannels
that appear frequently in dialogues. We also
perform linear normalization such that the final
value lies in [0, 1].
4.2 Graph-based Summarization
Graph-based methods have been widely used in doc-
ument summarization. In this approach, a document
is modeled as an adjacency matrix, where each node
represents a sentence, and the weight of the edge be-
tween each pair of sentences is their similarity (co-
sine similarity is typically used). An iterative pro-
cess is used until the scores for the nodes converge.
Previous studies (Erkan and Radev, 2004) showed
that this method can effectively extract important
sentences from documents. The basic framework we
use in this study is similar to the query-based graph
summarization system in (Zhao et al, 2009). We
also consider sentiment and topic relevance infor-
mation, and propose to incorporate information ob-
tained from dialog structure in this framework. The
score for a DA s is based on its content similarity
with all other DAs in the dialogue, the connection
with other DAs based on the dialogue structure, the
topic relevance, and its subjectivity, that is:
score(s) = ?sim
?
v?C
sim(s, v)
?
z?C sim(z, v)
score(v)
+?rel
REL(s, topic)
?
z?C REL(z, topic)
+?sent
sentiment(s)
?
z?C sentiment(z)
+?adj
?
v?C
ADJ(s, v)
?
z?C ADJ(z, v)
score(v)
?
i
?i = 1 (3)
where C is the set of all DAs in the dialogue;
REL(s, topic) and sentiment(s) are the same
as those in the above sentence ranking method;
sim(s, v) is the cosine similarity between two DAs
s and v. In addition to the standard connection be-
tween two DAs with an edge weight sim(s, v), we
introduce new connections ADJ(s, v) to model di-
alog structure. It is a directed edge from s to v, de-
fined as follows:
? If s and v are from the same speaker and within
the same turn, there is an edge from s to v and
an edge from v to s with weight 1/dis(s, v)
(ADJ(s, v) = ADJ(v, s) = 1/dis(s, v)),
where dis(s, v) is the distance between s and
v, measured based on their DA indices. This
way the DAs in the same turn can reinforce
each other. For example, if we consider that
335
one DA is important, then the other DAs in the
same turn are also important.
? If s and v are from the same speaker, and
separated only by one DA from another
speaker with length less than 3 words (usu-
ally backchannel), there is an edge from s to
v as well as an edge from v to s with weight 1
(ADJ(s, v) = ADJ(v, s) = 1).
? If s and v form a question-answer pair from two
speakers, then there is an edge from question s
to answer v with weight 1 (ADJ(s, v) = 1).
We use a simple rule-based method to deter-
mine question-answer pairs ? sentence s has
question marks or contains ?wh-word? (i.e.,
?what, how, why?), and sentence v is the im-
mediately following one. The motivation for
adding this connection is, if the score of a ques-
tion sentence is high, then the answer?s score is
also boosted.
? If s and v form an agreement or disagreement
pair, then there is an edge from v to s with
weight 1 (ADJ(v, s) = 1). This is also de-
termined by simple rules: sentence v contains
the word ?agree? or ?disagree?, s is the previ-
ous sentence, and from a different speaker. The
reason for adding this is similar to the above
question-answer pairs.
? If there are multiple edges generated from the
above steps between two nodes, then we use the
highest weight.
Since we are using a directed graph for the sen-
tence connections to model dialog structure, the re-
sulting adjacency matrix is asymmetric. This is dif-
ferent from the widely used graph methods for sum-
marization. Also note that in the first sentence rank-
ing method or the basic graph methods, summariza-
tion is conducted for each speaker separately. Ut-
terances from one speaker have no influence on the
summary decision for the other speaker. Here in our
proposed graph-based method, we introduce con-
nections between the two speakers, so that the adja-
cency pairs between them can be utilized to extract
salient utterances.
5 Experiments
5.1 Experimental Setup
The 18 conversations annotated by all 3 annotators
are used as test set, and the rest of 70 conversa-
tions are used as development set to tune the param-
eters (determining the best combination weights). In
preprocessing we applied word stemming. We per-
form extractive summarization using different word
compression ratios (ranging from 10% to 25%). We
use human annotated dialogue acts (DA) as the ex-
traction units. The system-generated summaries are
compared to human annotated extractive and ab-
stractive summaries. We use ROUGE as the eval-
uation metrics for summarization performance.
We compare our methods to two systems. The
first one is a baseline system, where we select the
longest utterances for each speaker. This has been
shown to be a relatively strong baseline for speech
summarization (Gillick et al, 2009). The second
one is human performance. We treat each annota-
tor?s extractive summary as a system summary, and
compare to the other two annotators? extractive and
abstractive summaries. This can be considered as
the upper bound of our system performance.
5.2 Results
From the development set, we used the grid search
method to obtain the best combination weights for
the two summarization methods. In the sentence-
ranking method, the best parameters found on the
development set are ?sim = 0, ?rel = 0.3, ?sent =
0.3, ?len = 0.4. It is surprising to see that the sim-
ilarity score is not useful for this task. The possible
reason is, in Switchboard conversations, what peo-
ple talk about is diverse and in many cases only topic
words (except stopwords) appear more than once. In
addition, REL score is already able to catch the topic
relevancy of the sentence. Thus, the similarity score
is redundant here.
In the graph-based method, the best parameters
are ?sim = 0, ?adj = 0.3, ?rel = 0.4, ?sent = 0.3.
The similarity between each pair of utterances is
also not useful, which can be explained with similar
reasons as in the sentence-ranking method. This is
different from graph-based summarization systems
for text domains. A similar finding has also been
shown in (Garg et al, 2009), where similarity be-
336
384348535863
0.1
0.15
0.2
0.25
com
pres
sion
 ratio
ROUGE-1(%)
max-
lengt
h
sente
nce-
rank
ing
grap
h
hum
an
(a) compare to reference extractive summary
1719212325272931
0.1
0.15
0.2
0.25
com
pres
sion
 ratio
ROUGE-1(%)
max-
lengt
h
sente
nce-
rank
ing
grap
h
hum
an
(b) compare to reference abstractive summary
Figure 1: ROUGE-1 F-scores compared to extractive
and abstractive reference summaries for different sys-
tems: max-length, sentence-ranking method, graph-
based method, and human performance.
tween utterances does not perform well in conversa-
tion summarization.
Figure 1 shows the ROUGE-1 F-scores compar-
ing to human extractive and abstractive summaries
for different compression ratios. Similar patterns are
observed for other ROUGE scores such as ROUGE-
2 or ROUGE-L, therefore they are not shown here.
Both methods improve significantly over the base-
line approach. There is relatively less improvement
using a higher compression ratio, compared to a
lower one. This is reasonable because when the
compression ratio is low, the most salient utterances
are not necessarily the longest ones, thus using more
information sources helps better identify important
sentences; but when the compression ratio is higher,
longer utterances are more likely to be selected since
they contain more content.
There is no significant difference between the two
methods. When compared to extractive reference
summaries, sentence-ranking is slightly better ex-
cept for the compression ratio of 0.1. When com-
pared to abstractive reference summaries, the graph-
based method is slightly better. The two systems
share the same topic relevance score (REL) and
sentiment score, but the sentence-ranking method
prefers longer DAs and the graph-based method
prefers DAs that are emphasized by the ADJ ma-
trix, such as the DA in the middle of a cluster of
utterances from the same speaker, the answer to a
question, etc.
5.3 Analysis
To analyze the effect of dialogue structure we in-
troduce in the graph-based summarization method,
we compare two configurations: ?adj = 0 (only us-
ing REL score and sentiment score in ranking) and
?adj = 0.3. We generate summaries using these two
setups and compare with human selected sentences.
Table 4 shows the number of false positive instances
(selected by system but not by human) and false neg-
ative ones (selected by human but not by system).
We use all three annotators? annotation as reference,
and consider an utterance as positive if one annotator
selects it. This results in a large number of reference
summary DAs (because of low human agreement),
and thus the number of false negatives in the system
output is very high. As expected, a smaller compres-
sion ratio (fewer selected DAs in the system output)
yields a higher false negative rate and a lower false
positive rate. From the results, we can see that gen-
erally adding adjacency matrix information is able
to reduce both types of errors except when the com-
pression ratio is 0.15.
The following shows an example, where the third
DA is selected by the system with ?adj = 0.3, but
not by ?adj = 0. This is partly because the weight
of the second DA is enhanced by the the question-
337
?adj = 0 ?adj = 0.3
ratio FP FN FP FN
0.1 37 588 33 581
0.15 60 542 61 546
0.2 100 516 90 511
0.25 137 489 131 482
Table 4: The number of false positive (FP) and false neg-
ative (FN) instances using the graph-based method with
?adj = 0 and ?adj = 0.3 for different compression ratios.
answer pair (the first and the second DA), and thus
subsequently boosting the score of the third DA.
A: Well what do you think?
B: Well, I don?t know, I?m thinking about from one to
ten what my no would be.
B: It would probably be somewhere closer to, uh, less
control because I don?t see, -
We also examined the system output and human
annotation and found some reasons for the system
errors:
(a) Topic relevance measure. We use the statis-
tics from the Switchboard corpus to measure the rel-
evance of each word to a given topic (PMI score),
therefore only when people use the same word in
different conversations of the topic, the PMI score of
this word and the topic is high. However, since the
size of the corpus is small, some topics only con-
tain a few conversations, and some words only ap-
pear in one conversation even though they are topic-
relevant. Therefore the current PMI measure cannot
properly measure a word?s and a sentence?s topic
relevance. This problem leads to many false neg-
ative errors (relevant sentences are not captured by
our system).
(b) Extraction units. We used DA segments as
units for extractive summarization, which can be
problematic. In conversational speech, sometimes
a DA segment is not a complete sentence because
of overlaps and interruptions. We notice that anno-
tators tend to select consecutive DAs that constitute
a complete sentence, however, since each individual
DA is not quite meaningful by itself, they are often
not selected by the system. The following segment
is extracted from a dialogue about ?universal health
insurance?. The two DAs from speaker B are not
selected by our system but selected by human anno-
tators, causing false negative errors.
B: and it just can devastate ?
A: and your constantly, -
B: ? your budget, you know.
6 Conclusion and Future Work
This paper investigates two unsupervised methods
in opinion summarization on spontaneous conver-
sations by incorporating topic score and sentiment
score in existing summarization techniques. In the
sentence-ranking method, we linearly combine sev-
eral scores in different aspects to select sentences
with the highest scores. In the graph-based method,
we use an adjacency matrix to model the dialogue
structure and utilize it to find salient utterances in
conversations. Our experiments show that both
methods are able to improve the baseline approach,
and we find that the cosine similarity between utter-
ances or between an utterance and the whole docu-
ment is not as useful as in other document summa-
rization tasks.
In future work, we will address some issues iden-
tified from our error analysis. First, we will in-
vestigate ways to represent a sentence?s topic rel-
evance. Second, we will evaluate using other ex-
traction units, such as applying preprocessing to re-
move disfluencies and concatenate incomplete sen-
tence segments together. In addition, it would be
interesting to test our system on speech recognition
output and automatically generated DA boundaries
to see how robust it is.
7 Acknowledgments
The authors thank Julia Hirschberg and Ani
Nenkova for useful discussions. This research is
supported by NSF awards CNS-1059226 and IIS-
0939966.
References
Alexandra Balahur, Ester Boldrini, Andre?s Montoyo, and
Patricio Mart??nez-Barco. 2010. Going beyond tra-
ditional QA systems: challenges and keys in opinion
question answering. In Proceedings of COLING.
Gu?nes Erkan and Dragomir R. Radev. 2004. LexRank:
graph-based lexical centrality as salience in text sum-
marization. Journal of Artificial Intelligence Re-
search.
338
Sadaoki Furui, Tomonori Kikuchi, Yousuke Shinnaka,
and Chior i Hori. 2004. Speech-to-text and speech-to-
speech summarization of spontaneous speech. IEEE
Transactions on Audio, Speech & Language Process-
ing, 12(4):401?408.
Nikhil Garg, Benoit Favre, Korbinian Reidhammer, and
Dilek Hakkani Tu?r. 2009. ClusterRank: a graph
based method for meeting summarization. In Proceed-
ings of Interspeech.
Dan Gillick, Korbinian Riedhammer, Benoit Favre, and
Dilek Hakkani-Tur. 2009. A global optimization
framework for meeting summarization. In Proceed-
ings of ICASSP.
John J. Godfrey and Edward Holliman. 1997.
Switchboard-1 Release 2. In Linguistic Data Consor-
tium, Philadelphia.
Andrew Hayes and Klaus Krippendorff. 2007. Answer-
ing the call for a standard reliability measure for cod-
ing data. Journal of Communication Methods and
Measures, 1:77?89.
Minqing Hu and Bing Liu. 2004. Mining and sum-
marizing customer reviews. In Proceedings of ACM
SIGKDD.
Konstantinos Koumpis and Steve Renals. 2005. Auto-
matic summarization of voicemail messages using lex-
ical and prosodic features. ACM - Transactions on
Speech and Language Processing.
Shih Hsiang Lin, Berlin Chen, and Hsin min Wang.
2009. A comparative study of probabilistic ranking
models for chinese spoken document summarization.
ACM Transactions on Asian Language Information
Processing, 8(1).
Chin-Yew Lin. 2004. ROUGE: a package for auto-
matic evaluation of summaries. In Proceedings of ACL
workshop on Text Summarization Branches Out.
Fei Liu and Yang Liu. 2008. What are meeting sum-
maries? An analysis of human extractive summaries
in meeting corpus. In Proceedings of SIGDial.
Sameer Maskey and Julia Hirschberg. 2005. Com-
paring lexical, acoustic/prosodic, structural and dis-
course features for speech summarization. In Pro-
ceedings of Interspeech.
Kathleen Mckeown, Julia Hirschberg, Michel Galley, and
Sameer Maskey. 2005. From text to speech summa-
rization. In Proceedings of ICASSP.
Gabriel Murray and Giuseppe Carenini. 2009. Detecting
subjectivity in multiparty speech. In Proceedings of
Interspeech.
Gabriel Murray, Steve Renals, and Jean Carletta. 2005.
Extractive summarization of meeting recordings. In
Proceedings of EUROSPEECH.
Vincent Ng, Sajib Dasgupta, and S.M.Niaz Arifin. 2006.
Examining the role of linguistic knowledge sources in
the automatic identification and classification of re-
views. In Proceedings of the COLING/ACL.
Hitoshi Nishikawa, Takaaki Hasegawa, Yoshihiro Mat-
suo, and Genichiro Kikui. 2010. Opinion summariza-
tion with integer linear programming formulation for
sentence extraction and ordering. In Proceedings of
COLING.
Bo Pang and Lilian Lee. 2004. A sentiment educa-
tion: sentiment analysis using subjectivity summariza-
tion based on minimum cuts. In Proceedings of ACL.
Michael Paul, ChengXiang Zhai, and Roxana Girju.
2010. Summarizing contrastive viewpoints in opinion-
ated text. In Proceedings of EMNLP.
Ana-Maria Popescu and Oren Etzioni. 2005. Extracting
product features and opinions from reviews. In Pro-
ceedings of HLT-EMNLP.
Stephan Raaijmakers, Khiet Truong, and Theresa Wilson.
2008. Multimodal subjectivity analysis of multiparty
conversation. In Proceedings of EMNLP.
Veselin Stoyanov, Claire Cardie, and Janyce Wiebe.
2005. Multi-perspective question answering using the
OpQA corpus. In Proceedings of EMNLP/HLT.
Janyce Wiebe and Ellen Riloff. 2005. Creating sub-
jective and objective sentence classifiers from unan-
notated texts. In Proceedings of CICLing.
Theresa Wilson and Janyce Wiebe. 2003. Annotating
opinions in the world press. In Proceedings of SIG-
Dial.
Theresa Wilson. 2008a. Annotating subjective content in
meetings. In Proceedings of LREC.
Theresa Wilson. 2008b. Fine-grained subjectivity and
sentiment analysis: recognizing the intensity, polarity,
and attitudes of private states. Ph.D. thesis, University
of Pittsburgh.
Shasha Xie and Yang Liu. 2010. Improving super-
vised learning for meeting summarization using sam-
pling and regression. Computer Speech and Lan-
guage, 24:495?514.
Klaus Zechner. 2002. Automatic summarization of
open-domain multiparty dialogues in dive rse genres.
Computational Linguistics, 28:447?485.
Justin Jian Zhang, Ho Yin Chan, and Pascale Fung. 2007.
Improving lecture speech summarization using rhetor-
ical information. In Proceedings of Biannual IEEE
Workshop on ASRU.
Lin Zhao, Lide Wu, and Xuanjing Huang. 2009. Using
query expansion in graph-based approach for query-
focused multi-document summarization. Journal of
Information Processing and Management.
Xiaodan Zhu and Gerald Penn. 2006. Summarization of
spontaneous conversations. In Proceedings of Inter-
speech.
339
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 732?741,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
N-Best Rescoring Based on Pitch-accent Patterns
Je Hun Jeon1 Wen Wang2 Yang Liu1
1Department of Computer Science, The University of Texas at Dallas, USA
2Speech Technology and Research Laboratory, SRI International, USA
{jhjeon,yangl}@hlt.utdallas.edu, wwang@speech.sri.com
Abstract
In this paper, we adopt an n-best rescoring
scheme using pitch-accent patterns to improve
automatic speech recognition (ASR) perfor-
mance. The pitch-accent model is decoupled
from the main ASR system, thus allowing us
to develop it independently. N-best hypothe-
ses from recognizers are rescored by addi-
tional scores that measure the correlation of
the pitch-accent patterns between the acoustic
signal and lexical cues. To test the robustness
of our algorithm, we use two different data
sets and recognition setups: the first one is En-
glish radio news data that has pitch accent la-
bels, but the recognizer is trained from a small
amount of data and has high error rate; the sec-
ond one is English broadcast news data using
a state-of-the-art SRI recognizer. Our experi-
mental results demonstrate that our approach
is able to reduce word error rate relatively by
about 3%. This gain is consistent across the
two different tests, showing promising future
directions of incorporating prosodic informa-
tion to improve speech recognition.
1 Introduction
Prosody refers to the suprasegmental features of nat-
ural speech, such as rhythm and intonation, since
it normally extends over more than one phoneme
segment. Speakers use prosody to convey paralin-
guistic information such as emphasis, intention, atti-
tude, and emotion. Humans listening to speech with
natural prosody are able to understand the content
with low cognitive load and high accuracy. How-
ever, most modern ASR systems only use an acous-
tic model and a language model. Acoustic informa-
tion in ASR is represented by spectral features that
are usually extracted over a window length of a few
tens of milliseconds. They miss useful information
contained in the prosody of the speech that may help
recognition.
Recently a lot of research has been done in au-
tomatic annotation of prosodic events (Wightman
and Ostendorf, 1994; Sridhar et al, 2008; Anan-
thakrishnan and Narayanan, 2008; Jeon and Liu,
2009). They used acoustic and lexical-syntactic
cues to annotate prosodic events with a variety of
machine learning approaches and achieved good
performance. There are also many studies us-
ing prosodic information for various spoken lan-
guage understanding tasks. However, research using
prosodic knowledge for speech recognition is still
quite limited. In this study, we investigate leverag-
ing prosodic information for recognition in an n-best
rescoring framework.
Previous studies showed that prosodic events,
such as pitch-accent, are closely related with acous-
tic prosodic cues and lexical structure of utterance.
The pitch-accent pattern given acoustic signal is
strongly correlated with lexical items, such as syl-
lable identity and canonical stress pattern. There-
fore as a first study, we focus on pitch-accent in this
paper. We develop two separate pitch-accent de-
tection models, using acoustic (observation model)
and lexical information (expectation model) respec-
tively, and propose a scoring method for the cor-
relation of pitch-accent patterns between the two
models for recognition hypotheses. The n-best list
is rescored using the pitch-accent matching scores
732
combined with the other scores from the ASR sys-
tem (acoustic and language model scores). We show
that our method yields a word error rate (WER) re-
duction of about 3.64% and 2.07% relatively on two
baseline ASR systems, one being a state-of-the-art
recognizer for the broadcast news domain. The fact
that it holds across different baseline systems sug-
gests the possibility that prosody can be used to help
improve speech recognition performance.
The remainder of this paper is organized as fol-
lows. In the next section, we review previous work
briefly. Section 3 explains the models and features
for pitch-accent detection. We provide details of our
n-best rescoring approach in Section 4. Section 5
describes our corpus and baseline ASR setup. Sec-
tion 6 presents our experiments and results. The last
section gives a brief summary along with future di-
rections.
2 Previous Work
Prosody is of interest to speech researchers be-
cause it plays an important role in comprehension
of spoken language by human listeners. The use
of prosody in speech understanding applications has
been quite extensive. A variety of applications
have been explored, such as sentence and topic seg-
mentation (Shriberg et al, 2000; Rosenberg and
Hirschberg, 2006), word error detection (Litman et
al., 2000), dialog act detection (Sridhar et al, 2009),
speaker recognition (Shriberg et al, 2005), and emo-
tion recognition (Benus et al, 2007), just to name a
few.
Incorporating prosodic knowledge is expected
to improve the performance of speech recogni-
tion. However, how to effectively integrate prosody
within the traditional ASR framework is a difficult
problem, since prosodic features are not well de-
fined and they come from a longer region, which is
different from spectral features used in current ASR
systems. Various research has been conducted try-
ing to incorporate prosodic information in ASR. One
way is to directly integrate prosodic features into
the ASR framework (Vergyri et al, 2003; Ostendorf
et al, 2003; Chen and Hasegawa-Johnson, 2006).
Such efforts include prosody dependent acoustic and
pronunciation model (allophones were distinguished
according to different prosodic phenomenon), lan-
guage model (words were augmented by prosody
events), and duration modeling (different prosodic
events were modeled separately and combined with
conventional HMM). This kind of integration has
advantages in that spectral and prosodic features are
more tightly coupled and jointly modeled. Alterna-
tively, prosody was modeled independently from the
acoustic and language models of ASR and used to
rescore recognition hypotheses in the second pass.
This approach makes it possible to independently
model and optimize the prosodic knowledge and to
combine with ASR hypotheses without any modi-
fication of the conventional ASR modules. In or-
der to improve the rescoring performance, various
prosodic knowledge was studied. (Ananthakrishnan
and Narayanan, 2007) used acoustic pitch-accent
pattern and its sequential information given lexi-
cal cues to rescore n-best hypotheses. (Kalinli and
Narayanan, 2009) used acoustic prosodic cues such
as pitch and duration along with other knowledge
to choose a proper word among several candidates
in confusion networks. Prosodic boundaries based
on acoustic cues were used in (Szaszak and Vicsi,
2007).
We take a similar approach in this study as the
second approach above in that we develop prosodic
models separately and use them in a rescoring
framework. Our proposed method differs from pre-
vious work in the way that the prosody model is used
to help ASR. In our approach, we explicitly model
the symbolic prosodic events based on acoustic and
lexical information. We then capture the correla-
tion of pitch-accent patterns between the two differ-
ent cues, and use that to improve recognition perfor-
mance in an n-best rescoring paradigm.
3 Prosodic Model
Among all the prosodic events, we use only pitch-
accent pattern in this study, because previous stud-
ies have shown that acoustic pitch-accent is strongly
correlated with lexical items, such as canonical
stress pattern and syllable identity that can be eas-
ily acquired from the output of conventional ASR
and pronunciation dictionary. We treat pitch-accent
detection as a binary classification task, that is, a
classifier is used to determine whether the base unit
is prominent or not. Since pitch-accent is usually
733
carried by syllables, we use syllables as our units,
and the syllable definition of each word is based
on CMU pronunciation dictionary which has lexi-
cal stress and syllable boundary marks (Bartlett et
al., 2009). We separately develop acoustic-prosodic
and lexical-prosodic models and use the correlation
between the two models for each syllable to rescore
the n-best hypotheses of baseline ASR systems.
3.1 Acoustic-prosodic Features
Similar to most previous work, the prosodic features
we use include pitch, energy, and duration. We also
add delta features of pitch and energy. Duration in-
formation for syllables is derived from the speech
waveform and phone-level forced alignment of the
transcriptions. In order to reduce the effect by both
inter-speaker and intra-speaker variation, both pitch
and energy values are normalized (z-value) with ut-
terance specific means and variances. For pitch, en-
ergy, and their delta values, we apply several cate-
gories of 12 functions to generate derived features.
? Statistics (7): minimum, maximum, range,
mean, standard deviation, skewness and kurto-
sis value. These are used widely in prosodic
event detection and emotion detection.
? Contour (5): This is approximated by taking
5 leading terms in the Legendre polynomial
expansion. The approximation of the contour
using the Legendre polynomial expansion has
been successfully applied in quantitative pho-
netics (Grabe et al, 2003) and in engineering
applications (Dehak et al, 2007). Each term
models a particular aspect of the contour, such
as the slope, and information about the curva-
ture.
We use 6 duration features, that is, raw, normal-
ized, and relative durations (ms) of the syllable and
vowel. Normalization (z-value) is performed based
on statistics for each syllable and vowel. The rela-
tive value is the difference between the normalized
current duration and the following one.
In the above description, we assumed that the
event of a syllable is only dependent on its observa-
tions, and did not consider contextual effect. To al-
leviate this restriction, we expand the features by in-
corporating information about the neighboring sylla-
bles. Based on the study in (Jeon and Liu, 2010) that
evaluated using left and right contexts, we choose to
use one previous and one following context in the
features. The total number of features used in this
study is 162.
3.2 Lexical-prosodic Features
There is a very strong correlation between pitch-
accent in an utterance and its lexical information.
Previous studies have shown that the lexical fea-
tures perform well for pitch-accent prediction. The
detailed features for training the lexical-prosodic
model are as follows.
? Syllable identity: We kept syllables that appear
more than 5 times in the training corpus. The
other syllables that occur less are collapsed into
one syllable representation.
? Vowel phone identity: We used vowel phone
identity as a feature.
? Lexical stress: This is a binary feature to rep-
resent if the syllable corresponds to a lexical
stress based on the pronunciation dictionary.
? Boundary information: This is a binary feature
to indicate if there is a word boundary before
the syllable.
For lexical features, based on the study in (Jeon
and Liu, 2010), we added two previous and two fol-
lowing contexts in the final features.
3.3 Prosodic Model Training
We choose to use a support vector machine (SVM)
classifier1 for the prosodic model based on previous
work on prosody labeling study in (Jeon and Liu,
2010). We use RBF kernel for the acoustic model,
and 3-order polynomial kernel for the lexical model.
In our experiments, we investigate two kinds
of training methods for prosodic modeling. The
first one is a supervised method where models are
trained using all the labeled data. The second is
a semi-supervised method using co-training algo-
rithm (Blum and Mitchell, 1998), described in Algo-
rithm 1. Given a set L of labeled data and a set U of
unlabeled data with two views, it then iterates in the
1LIBSVM ? A Library for Support Vector Machines, loca-
tion: http://www.csie.ntu.edu.tw/?cjlin/libsvm/
734
Algorithm 1 Co-training algorithm.
Given:
- L: labeled examples; U: unlabeled examples
- there are two views V1 and V2 on an example x
Initialize:
- L1=L, samples used to train classifiers h1
- L2=L, samples used to train classifiers h2
Loop for k iterations
- create a small pool U? choosing from U
- use V1(L1) to train classifier h1
and V2(L2) to train classifier h2
- let h1 label/select examples Dh1 from U?
- let h2 label/select examples Dh2 from U?
- add self-labeled examples Dh1 to L2
and Dh2 to L1
- remove Dh1 and Dh2 from U
following procedure. The algorithm first creates a
smaller pool U? containing unlabeled data from U. It
uses Li (i = 1, 2) to train two distinct classifiers: the
acoustic classifier h1, and the lexical classifier h2.
We use function Vi (i = 1, 2) to represent that only
a single view is used for training h1 or h2. These two
classifiers are used to make predictions for the unla-
beled setU?, and only when they agree on the predic-
tion for a sample, their predicted class is used as the
label for this sample. Then among these self-labeled
samples, the most confident ones by one classifier
are added to the data set Li for training the other
classifier. This iteration continues until reaching the
defined number of iterations. In our experiment, the
size of the pool U? is 5 times of the size of training
data Li, and the size of the added self-labeled ex-
ample set, Dhi , is 5% of Li. For the newly selected
Dhi , the distribution of the positive and negative ex-
amples is the same as that of the training data Li.
This co-training method is expected to cope with
two problems in prosodic model training. The first
problem is the different decision patterns between
the two classifiers: the acoustic model has relatively
higher precision, while the lexical model has rela-
tively higher recall. The goal of the co-training al-
gorithm is to learn from the difference of each clas-
sifier, thus it can improve the performance as well
as reduce the mismatch of two classifiers. The sec-
ond problem is the mismatch of data used for model
training and testing, which often results in system
performance degradation. Using co-training, we can
use the unlabeled data from the domain that matches
the test data, adapting the model towards test do-
main.
4 N-Best Rescoring Scheme
In order to leverage prosodic information for bet-
ter speech recognition performance, we augment the
standard ASR equation to include prosodic informa-
tion as following:
W? = argmax
W
p(W |As, Ap)
= argmax
W
p(As, Ap|W )p(W ) (1)
where As and Ap represent acoustic-spectral fea-
tures and acoustic-prosodic features. We can further
assume that spectral and prosodic features are con-
ditionally independent given a word sequence W ,
therefore, Equation 1 can be rewritten as following:
W? ? argmax
W
p(As|W )p(W )p(Ap|W ) (2)
The first two terms stand for the acoustic and lan-
guage models in the original ASR system, and the
last term means the prosody model we introduce. In-
stead of using the prosodic model in the first pass de-
coding, we use it to rescore n-best candidates from
a speech recognizer. This allows us to train the
prosody models independently and better optimize
the models.
For p(Ap|W ), the prosody score for a word se-
quence W , in this work we propose a method to es-
timate it, also represented as scoreW?prosody(W ).
The idea of scoring the prosody patterns is that there
is some expectation of pitch-accent patterns given
the lexical sequence (W ), and the acoustic pitch-
accent should match with this expectation. For in-
stance, in the case of a prominent syllable, both
acoustic and lexical evidence show pitch-accent, and
vice versa. In order to maximize the agreement be-
tween the two sources, we measure how good the
acoustic pitch-accent in speech signal matches the
given lexical cues. For each syllable Si in the n-best
list, we use acoustic-prosodic cues (ai) to estimate
the posterior probability that the syllable is promi-
nent (P), p(P |ai). Similarly, we use lexical cues (li)
735
to determine the syllable?s pitch-accent probability
p(P |li). Then the prosody score for a syllable Si is
estimated by the match of the pitch-accent patterns
between acoustic and lexical information using the
difference of the posteriors from the two models:
scoreS?prosody(Si) ? 1? | p(P |ai) ? p(P |li) | (3)
Furthermore, we take into account the effect due
to varying durations for different syllables. We no-
tice that syllables without pitch-accent have much
shorter duration than the prominent ones, and the
prosody scores for the short syllables tend to be
high. This means that if a syllable is split into two
consecutive non-prominent syllables, the agreement
score may be higher than a long prominent syllable.
Therefore, we introduce a weighting factor based on
syllable duration (dur(i)). For a candidate word se-
quence (W) consisting of n syllables, its prosodic
score is the sum of the prosodic scores for all the
syllables in it weighted by their duration (measured
using milliseconds), that is:
scoreW?prosody(W ) ?
n
?
i=1
log(scoreS?prosody(Si)) ? dur(i) (4)
We then combine this prosody score with the
original acoustic and language model likelihood
(P (As|W ) and P (W ) in Equation 2). In practice,
we need to weight them differently, therefore, the
combined score for a hypothesis W is:
Score(W ) = ? ? scoreW?prosody(W )
+ scoreASR(W ) (5)
where scoreASR(W ) is generated by ASR systems
(composed of acoustic and language model scores)
and ? is optimized using held out data.
5 Data and Baseline Systems
Our experiments are carried out using two different
data sets and two different recognition systems as
well in order to test the robustness of our proposed
method.
The first data set is the Boston University Radio
News Corpus (BU) (Ostendorf et al, 1995), which
consists of broadcast news style read speech. The
BU corpus has about 3 hours of read speech from
7 speakers (3 female, 4 male). Part of the data has
been labeled with ToBI-style prosodic annotations.
In fact, the reason that we use this corpus, instead of
other corpora typically used for ASR experiments,
is because of its prosodic labels. We divided the
entire data corpus into a training set and a test set.
There was no speaker overlap between training and
test sets. The training set has 2 female speakers (f2
and f3) and 3 male ones (m2, m3, m4). The test set is
from the other two speakers (f1 and m1). We use 200
utterances for the recognition experiments. Each ut-
terance in BU corpus consists of more than one sen-
tences, so we segmented each utterance based on
pause, resulting in a total number of 713 segments
for testing. We divided the test set roughly equally
into two sets, and used one for parameter tuning and
the other for rescoring test. The recognizer used for
this data set was based on Sphinx-32. The context-
dependent triphone acoustic models with 32 Gaus-
sian mixtures were trained using the training par-
tition of the BU corpus described above, together
with the broadcast new data. A standard back-off tri-
gram language model with Kneser-Ney smoothing
was trained using the combined text from the train-
ing partition of the BU, Wall Street Journal data, and
part of Gigaword corpus. The vocabulary size was
about 10K words and the out-of-vocabulary (OOV)
rate on the test set was 2.1%.
The second data set is from broadcast news (BN)
speech used in the GALE program. The recognition
test set contains 1,001 utterances. The n-best hy-
potheses for this data set are generated by a state-of-
the-art SRI speech recognizer, developed for broad-
cast news speech (Stolcke et al, 2006; Zheng et
al., 2007). This system yields much better perfor-
mance than the first one. We also divided the test
set roughly equally into two sets for parameter tun-
ing and testing. From the data used for training the
speech recognizer, we randomly selected 5.7 hours
of speech (4,234 utterances) for the co-training al-
gorithm for the prosodic models.
For prosodic models, we used a simple binary
representation of pitch-accent in the form of pres-
ence versus absence. The reference labels are de-
2CMU Sphinx - Speech Recognition Toolkit, location:
http://www.speech.cs.cmu.edu/sphinx/tutorial.html
736
rived from the ToBI annotation in the BU corpus,
and the ratio of pitch-accented syllables is about
34%. Acoustic-prosodic and lexical-prosodic mod-
els were separately developed using the features de-
scribed in Section 3. Feature extraction was per-
formed at the syllable level from force-aligned data.
For the supervised approach, we used those utter-
ances in the training data partition with ToBI labels
in the BU corpus (245 utterances, 14,767 syllables).
For co-training, the labeled data from BU corpus is
used as initial training, and the other unlabeled data
from BU and BN are used as unlabeled data.
6 Experimental Results
6.1 Pitch-accent Detection
First we evaluate the performance of our acoustic-
prosodic and lexical-prosodic models for pitch-
accent detection. For rescoring, not only the ac-
curacies of the two individual prosodic models are
important, but also the pitch-accent agreement score
between the two models (as shown in Equation 3)
is critical, therefore, we present results using these
two metrics. Table 1 shows the accuracy of each
model for pitch-accent detection, and also the av-
erage prosody score of the two models (i.e., Equa-
tion 3) for positive and negative classes (using ref-
erence labels). These results are based on the BU
labeled data in the test set. To compare our pitch ac-
cent detection performance with previous work, we
include the result of (Jeon and Liu, 2009) as a ref-
erence. Compared to previous work, the acoustic
model achieved similar performance, while the per-
formance of lexical model is a bit lower. The lower
performance of lexical model is mainly because we
do not use part-of-speech (POS) information in the
features, since we want to only use the word output
from the ASR system (without additional POS tag-
ging).
As shown in Table 1, when using the co-training
algorithm, as described in Section 3.3, the over-
all accuracies improve slightly and therefore the
prosody score is also increased. We expect this im-
proved model will be more beneficial for rescoring.
6.2 N-Best Rescoring
For the rescoring experiment, we use 100-best hy-
potheses from the two different ASR systems, as de-
Accuracy(%) Prosody score
Acoustic Lexical Pos Neg
Supervised 83.97 84.48 0.747 0.852
Co-training 84.54 84.99 0.771 0.867
Reference 83.53 87.92 - -
Table 1: Pitch accent detection results: performance of
individual acoustic and lexical models, and the agreement
between the twomodels (i.e., prosody score for a syllable,
Equation 3) for positive and negative classes. Also shown
is the reference result for pitch accent detection from Jeon
and Liu (2009).
scribed in Section 5. We apply the acoustic and lex-
ical prosodic models to each hypothesis to obtain its
prosody score, and combine it with ASR scores to
find the top hypothesis. The weights were optimized
using one test set and applied to the other. We report
the average result of the two testings.
Table 2 shows the rescoring results using the first
recognition system on BU data, which was trained
with a relatively small amount of data. The 1-
best baseline uses the first hypothesis that has the
best ASR score. The oracle result is from the best
hypothesis that gives the lowest WER by compar-
ing all the candidates to the reference transcript.
We used two prosodic models as described in Sec-
tion 3.3. The first one is the base prosodic model us-
ing supervised training (S-model). The second is the
prosodic model with the co-training algorithm (C-
model). For these rescoring experiments, we tuned
? (in Equation 5) when combining the ASR acous-
tic and language model scores with the additional
prosody score. The value in parenthesis in Table 2
means the relative WER reduction when compared
to the baseline result. We show the WER results for
both the development and the test set.
As shown in Table 2, we observe performance
improvement using our rescoring method. Using
the base S-model yields reasonable improvement,
and C-model further reduces WER. Even though the
prosodic event detection performance of these two
prosodic models is similar, the improved prosody
score between the acoustic and lexical prosodic
models using co-training helps rescoring. After
rescoring using prosodic knowledge, the WER is re-
duced by 0.82% (3.64% relative). Furthermore, we
notice that the difference between development and
737
WER (%)
1-best baseline 22.64
S-model
Dev 21.93 (3.11%)
Test 22.10 (2.39%)
C-model
Dev 21.76 (3.88%)
Test 21.81 (3.64%)
Oracle 15.58
Table 2: WER of the baseline system and after rescoring
using prosodic models. Results are based on the first ASR
system.
test data is smaller when using the C-model than S-
model, which means that the prosodic model with
co-training is more stable. In fact, we found that
the optimal value of ? is 94 and 57 for the two
folds using S-model, and is 99 and 110 for the C-
model. These verify again that the prosodic scores
contribute more in the combination with ASR likeli-
hood scores when using the C-model, and are more
robust across different tuning sets. Ananthakrish-
nan and Narayanan (2007) also used acoustic/lexical
prosodic models to estimate a prosody score and re-
ported 0.3% recognition error reduction on BU data
when rescoring 100-best list (their baseline WER is
22.8%). Although there is some difference in experi-
mental setup (data, classifier, features) between ours
and theirs, our S-model showed comparable perfor-
mance gain and the result of C-model is significantly
better than theirs.
Next we test our n-best rescoring approach using a
state-of-the-art SRI speech recognizer on BN data to
verify if our approach can generalize to better ASR
n-best lists. This is often the concern that improve-
ments observed on a poor ASR system do not hold
for better ASR systems. The rescoring results are
shown in Table 3. We can see that the baseline per-
formance of this recognizer is much better than that
of the first ASR system (even though the recogni-
tion task is also harder). Our rescoring approach
still yields performance gain even using this state-
of-the-art system. The WER is reduced by 0.29%
(2.07% relative). This error reduction is lower than
that in the first ASR system. There are several pos-
sible reasons. First, the baseline ASR performance
is higher, making further improvement hard; sec-
ond, and more importantly, the prosody models do
not match well to the test domain. We trained the
prosody model using the BU data. Even though co-
training is used to leverage unlabeled BN data to re-
duce data mismatch, it is still not as good as using
labeled in-domain data for model training.
WER (%)
1-best baseline 13.77
S-model
Dev 13.53 (1.78%)
Test 13.55 (1.63%)
C-model
Dev 13.48 (2.16%)
Test 13.49 (2.07%)
Oracle 9.23
Table 3: WER of the baseline system and after rescoring
using prosodic models. Results are based on the second
ASR system.
6.3 Analysis and Discussion
We also analyze what kinds of errors are reduced
using our rescoring approach. Most of the error re-
duction came from substitution and insertion errors.
Deletion error rate did not change much or some-
times even increased. For a better understanding of
the improvement using the prosody model, we ana-
lyzed the pattern of corrections (the new hypothesis
after rescoring is correct while the original 1-best is
wrong) and errors. Table 4 shows some positive and
negative examples from rescoring results using the
first ASR system. In this table, each word is asso-
ciated with some binary expressions inside a paren-
thesis, which stand for pitch-accent markers. Two
bits are used for each syllable: the first one is for
the acoustic-prosodic model and the second one is
for the lexical-prosodic model. For both bits, 1 rep-
resents pitch-accent, and 0 indicates none. These
hard decisions are obtained by setting a threshold of
0.5 for the posterior probabilities from the acoustic
or lexical models. For example, when the acoustic
classifier predicts a syllable as pitch-accented and
the lexical one as not accented, ?10? marker is as-
signed to the syllable. The number of such pairs of
pitch-accent markers is the same as the number of
syllables in a word. The bold words indicate correct
words and italic means errors. As shown in the pos-
itive example of Table 4, we find that our prosodic
model is effective at identifying an erroneous word
when it is split into two words, resulting in dif-
ferent pitch-accent patterns. Language models are
738
Positive example
1-best : most of the massachusetts
(11 ) (10) (00) (11 00 01 00)
rescored : most other massachusetts
(11 ) (11 00) (11 00 01 00)
Negative example
1-best : robbery and on a theft
(11 00 00) (00) (10) (00) (11)
rescored : robbery and lot of theft
(11 00 00) (00) (11) (00) (11)
Table 4: Examples of rescoring results. Binary expressions inside the parenthesis below a word represent pitch-accent
markers for the syllables in the word.
not good at correcting this kind of errors since both
word sequences are plausible. Our model also intro-
duces some errors, as shown in the negative exam-
ple, which is mainly due to the inaccurate prosody
model.
We conducted more prosody rescoring experi-
ments in order to understand the model behavior.
These analyses are based on the n-best list from the
first ASR system for the entire test set. In the first
experiment, among the 100 hypotheses in n-best list,
we gave a prosody score of 0 to the 100th hypothe-
sis, and used automatically obtained prosodic scores
for the other hypotheses. A zero prosody score
means the perfect agreement given acoustic and lex-
ical cues. The original scores from the recognizer
were combined with the prosodic scores for rescor-
ing. This was to verify that the range of the weight-
ing factor ? estimated on the development data (us-
ing the original, not the modified prosody scores for
all candidates) was reasonable to choose proper hy-
pothesis among all the candidates. We noticed that
27% of the times the last hypothesis on the list was
selected as the best hypothesis. This hypothesis has
the highest prosodic scores, but lowest ASR score.
This result showed that if the prosodic models were
accurate enough, the correct candidate could be cho-
sen using our rescoring framework.
In the second experiment, we put the reference
text together with the other candidates. We use the
same ASR scores for all candidates, and generated
prosodic scores using our prosody model. This was
to test that our model could pick up correct candi-
date using only the prosodic score. We found that
for 26% of the utterances, the reference transcript
was chosen as the best one. This was significantly
better than random selection (i.e., 1/100), suggest-
ing the benefit of the prosody model; however, this
percentage is not very high, implying the limitation
of prosodic information for ASR or the current im-
perfect prosodic models.
In the third experiment, we replaced the 100th
candidate with the reference transcript and kept its
ASR score. When using our prosody rescoring ap-
proach, we obtained a relative error rate reduction
of 6.27%. This demonstrates again that our rescor-
ing method works well ? if the correct hypothesis is
on the list, even though with a low ASR score, us-
ing prosodic information can help identify the cor-
rect candidate.
Overall the performance improvement we ob-
tained from rescoring by incorporating prosodic in-
formation is very promising. Our evaluation using
two different ASR systems shows that the improve-
ment holds even when we use a state-of-the-art rec-
ognizer and the training data for the prosody model
does not come from the same corpus. We believe
the consistent improvements we observed for differ-
ent conditions show that this is a direction worthy of
further investigation.
7 Conclusion
In this paper, we attempt to integrate prosodic infor-
mation for ASR using an n-best rescoring scheme.
This approach decouples the prosodic model from
the main ASR system, thus the prosodic model can
be built independently. The prosodic scores that we
use for n-best rescoring are based on the matching
of pitch-accent patterns by acoustic and lexical fea-
tures. Our rescoring method achieved a WER reduc-
tion of 3.64% and 2.07% relatively using two differ-
ent ASR systems. The fact that the gain holds across
different baseline systems (including a state-of-the-
739
art speech recognizer) suggests the possibility that
prosody can be used to improve speech recognition
performance.
As suggested by our experiments, better prosodic
models can result in more WER reduction. The per-
formance of our prosodic model was improved with
co-training, but there are still problems, such as the
imbalance of the two classifiers? prediction, as well
as for the two events. In order to address these prob-
lems, we plan to improve the labeling and selec-
tion method in the co-training algorithm, and also
explore other training algorithms to reduce domain
mismatch. Furthermore, we are also interested in
evaluating our approach on the spontaneous speech
domain, which is quite different from the data we
used in this study.
In this study, we used n-best rather than lattice
rescoring. Since the prosodic features we use in-
clude cross-word contextual information, it is not
straightforward to apply it directly to lattices. In
our future work, we will develop models with only
within-word context, and thus allowing us to explore
lattice rescoring, which we expect will yield more
performance gain.
References
Sankaranarayanan Ananthakrishnan and Shrikanth
Narayanan. 2007. Improved speech recognition using
acoustic and lexical correlated of pitch accent in a
n-best rescoring framework. Proc. of ICASSP, pages
65?68.
Sankaranarayanan Ananthakrishnan and Shrikanth
Narayanan. 2008. Automatic prosodic event detec-
tion using acoustic, lexical and syntactic evidence.
IEEE Transactions on Audio, Speech, and Language
Processing, 16(1):216?228.
Susan Bartlett, Grzegorz Kondrak, and Colin Cherry.
2009. On the syllabification of phonemes. Proc. of
NAACL-HLT, pages 308?316.
Stefan Benus, Agust??n Gravano, and Julia Hirschberg.
2007. Prosody, emotions, and whatever. Proc. of In-
terspeech, pages 2629?2632.
Avrim Blum and Tom Mitchell. 1998. Combining la-
beled and unlabeled data with co-training. Proc. of the
Workshop on Computational Learning Theory, pages
92?100.
Ken Chen and Mark Hasegawa-Johnson. 2006. Prosody
dependent speech recognition on radio news corpus
of American English. IEEE Transactions on Audio,
Speech, and Language Processing, 14(1):232? 245.
Najim Dehak, Pierre Dumouchel, and Patrick Kenny.
2007. Modeling prosodic features with joint fac-
tor analysis for speaker verification. IEEE Transac-
tions on Audio, Speech, and Language Processing,
15(7):2095?2103.
Esther Grabe, Greg Kochanski, and John Coleman. 2003.
Quantitative modelling of intonational variation. Proc.
of SASRTLM, pages 45?57.
Je Hun Jeon and Yang Liu. 2009. Automatic prosodic
events detection suing syllable-based acoustic and syn-
tactic features. Proc. of ICASSP, pages 4565?4568.
Je Hun Jeon and Yang Liu. 2010. Syllable-level promi-
nence detection with acoustic evidence. Proc. of Inter-
speech, pages 1772?1775.
Ozlem Kalinli and Shrikanth Narayanan. 2009. Contin-
uous speech recognition using attention shift decoding
with soft decision. Proc. of Interspeech, pages 1927?
1930.
Diane J. Litman, Julia B. Hirschberg, and Marc Swerts.
2000. Predicting automatic speech recognition perfor-
mance using prosodic cues. Proc. of NAACL, pages
218?225.
Mari Ostendorf, Patti Price, and Stefanie Shattuck-
Hufnagel. 1995. The Boston University radio news
corpus. Linguistic Data Consortium.
Mari Ostendorf, Izhak Shafran, and Rebecca Bates.
2003. Prosody models for conversational speech
recognition. Proc. of the 2nd Plenary Meeting and
Symposium on Prosody and Speech Processing, pages
147?154.
Andrew Rosenberg and Julia Hirschberg. 2006. Story
segmentation of broadcast news in English, Mandarin
and Arabic. Proc. of HLT-NAACL, pages 125?128.
Elizabeth Shriberg, Andreas Stolcke, Dilek Hakkani-Tu?r,
and Go?khan Tu?r. 2000. Prosody-based automatic seg-
mentation of speech into sentences and topics. Speech
Communication, 32(1-2):127?154.
Elizabeth Shriberg, Luciana Ferrer, Sachin S. Kajarekar,
Anand Venkataraman, and Andreas Stolcke. 2005.
Modeling prosodic feature sequences for speaker
recognition. Speech Communication, 46(3-4):455?
472.
Vivek Kumar Rangarajan Sridhar, Srinivas Bangalore,
and Shrikanth S. Narayanan. 2008. Exploiting acous-
tic and syntactic features for automatic prosody label-
ing in a maximum entropy framework. IEEE Trans-
actions on Audio, Speech, and Language Processing,
16(4):797?811.
Vivek Kumar Rangarajan Sridhar, Srinivas Bangalore,
and Shrikanth Narayanan. 2009. Combining lexi-
cal, syntactic and prosodic cues for improved online
740
dialog act tagging. Computer Speech and Language,
23(4):407?422.
Andreas Stolcke, Barry Chen, Horacio Franco, Venkata
Ramana Rao Gadde, Martin Graciarena, Mei-Yuh
Hwang, Katrin Kirchhoff, Arindam Mandal, Nelson
Morgan, Xin Lin, Tim Ng, Mari Ostendorf, Kemal
So?nmez, Anand Venkataraman, Dimitra Vergyri, Wen
Wang, Jing Zheng, and Qifeng Zhu. 2006. Recent in-
novations in speech-to-text transcription at SRI-ICSI-
UW. IEEE Transactions on Audio, Speech and Lan-
guage Processing, 14(5):1729?1744. Special Issue on
Progress in Rich Transcription.
Gyorgy Szaszak and Klara Vicsi. 2007. Speech recogni-
tion supported by prosodic information for fixed stress
languages. Proc. of TSD Conference, pages 262?269.
Dimitra Vergyri, Andreas Stolcke, Venkata R. R. Gadde,
Luciana Ferrer, and Elizabeth Shriberg. 2003.
Prosodic knowledge sources for automatic speech
recognition. Proc. of ICASSP, pages 208?211.
Colin W. Wightman and Mari Ostendorf. 1994. Auto-
matic labeling of prosodic patterns. IEEE Transaction
on Speech and Auido Processing, 2(4):469?481.
Jing Zheng, Ozgur Cetin, Mei-Yuh Hwang, Xin Lei, An-
dreas Stolcke, and Nelson Morgan. 2007. Combin-
ing discriminative feature, transform, and model train-
ing for large vocabulary speech recognition. Proc. of
ICASSP, pages 633?636.
741
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1278?1287,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Adjoining Tree-to-String Translation
Yang Liu, Qun Liu, and Yajuan Lu?
Key Laboratory of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences
P.O. Box 2704, Beijing 100190, China
{yliu,liuqun,lvyajuan}@ict.ac.cn
Abstract
We introduce synchronous tree adjoining
grammars (TAG) into tree-to-string transla-
tion, which converts a source tree to a target
string. Without reconstructing TAG deriva-
tions explicitly, our rule extraction algo-
rithm directly learns tree-to-string rules from
aligned Treebank-style trees. As tree-to-string
translation casts decoding as a tree parsing
problem rather than parsing, the decoder still
runs fast when adjoining is included. Less
than 2 times slower, the adjoining tree-to-
string system improves translation quality by
+0.7 BLEU over the baseline system only al-
lowing for tree substitution on NIST Chinese-
English test sets.
1 Introduction
Syntax-based translation models, which exploit hi-
erarchical structures of natural languages to guide
machine translation, have become increasingly pop-
ular in recent years. So far, most of them have
been based on synchronous context-free grammars
(CFG) (Chiang, 2007), tree substitution grammars
(TSG) (Eisner, 2003; Galley et al, 2006; Liu et
al., 2006; Huang et al, 2006; Zhang et al, 2008),
and inversion transduction grammars (ITG) (Wu,
1997; Xiong et al, 2006). Although these for-
malisms present simple and precise mechanisms for
describing the basic recursive structure of sentences,
they are not powerful enough to model some impor-
tant features of natural language syntax. For ex-
ample, Chiang (2006) points out that the transla-
tion of languages that can stack an unbounded num-
ber of clauses in an ?inside-out? way (Wu, 1997)
provably goes beyond the expressive power of syn-
chronous CFG and TSG. Therefore, it is necessary
to find ways to take advantage of more powerful syn-
chronous grammars to improve machine translation.
Synchronous tree adjoining grammars (TAG)
(Shieber and Schabes, 1990) are a good candidate.
As a formal tree rewriting system, TAG (Joshi et al,
1975; Joshi, 1985) provides a larger domain of lo-
cality than CFG to state linguistic dependencies that
are far apart since the formalism treats trees as basic
building blocks. As a mildly context-sensitive gram-
mar, TAG is conjectured to be powerful enough to
model natural languages. Synchronous TAG gener-
alizes TAG by allowing the construction of a pair
of trees using the TAG operations of substitution
and adjoining on tree pairs. The idea of using syn-
chronous TAG in machine translation has been pur-
sued by several researchers (Abeille et al, 1990;
Prigent, 1994; Dras, 1999), but only recently in
its probabilistic form (Nesson et al, 2006; De-
Neefe and Knight, 2009). Shieber (2007) argues that
probabilistic synchronous TAG possesses appealing
properties such as expressivity and trainability for
building a machine translation system.
However, one major challenge for applying syn-
chronous TAG to machine translation is computa-
tional complexity. While TAG requires O(n6) time
for monolingual parsing, synchronous TAG requires
O(n12) for bilingual parsing. One solution is to use
tree insertion grammars (TIG) introduced by Sch-
abes and Waters (1995). As a restricted form of
TAG, TIG still allows for adjoining of unbounded
trees but only requires O(n3) time for monolingual
parsing. Nesson et al (2006) firstly demonstrate
1278
o?
zo?ngto?ng
NN
NP
President
X,?1
{I
me?iguo?
NR
NP
US
X,?2
NP? NP?
NP
X? X?
X
,?1
NP
NP? NP
NN
o?
zo?ngto?ng
X
X? X
President
,?2
NP
NP
NR
{I
me?iguo?
NP
NN
o?
zo?ngto?ng
X
X
US
X
President
,?3
Figure 1: Initial and auxiliary tree pairs. The source side (Chinese) is a Treebank-style linguistic tree. The target side
(English) is a purely structural tree using a single non-terminal (X). By convention, substitution and foot nodes are
marked with a down arrow (?) and an asterisk (?), respectively. The dashed lines link substitution sites (e.g., NP? and
X? in ?1) and adjoining sites (e.g., NP and X in ?2) in tree pairs. Substituting the initial tree pair ?1 at the NP?-X?
node pair in the auxiliary tree pair ?1 yields a derived tree pair ?2, which can be adjoined at NN-X in ?2 to generate
?3.
the use of synchronous TIG for machine translation
and report promising results. DeNeefe and Knight
(2009) prove that adjoining can improve translation
quality significantly over a state-of-the-art string-
to-tree system (Galley et al, 2006) that uses syn-
chronous TSG with tractable computational com-
plexity.
In this paper, we introduce synchronous TAG into
tree-to-string translation (Liu et al, 2006; Huang et
al., 2006), which is the simplest and fastest among
syntax-based approaches (Section 2). We propose
a new rule extraction algorithm based on GHKM
(Galley et al, 2004) that directly induces a syn-
chronous TAG from an aligned and parsed bilingual
corpus without converting Treebank-style trees to
TAG derivations explicitly (Section 3). As tree-to-
string translation takes a source parse tree as input,
the decoding can be cast as a tree parsing problem
(Eisner, 2003): reconstructing TAG derivations from
a derived tree using tree-to-string rules that allow for
both substitution and adjoining. We describe how to
convert TAG derivations to translation forest (Sec-
tion 4). We evaluated the new tree-to-string system
on NIST Chinese-English tests and obtained con-
sistent improvements (+0.7 BLEU) over the STSG-
based baseline system without significant loss in ef-
ficiency (1.6 times slower) (Section 5).
2 Model
A synchronous TAG consists of a set of linked ele-
mentary tree pairs: initial and auxiliary. An initial
tree is a tree of which the interior nodes are all la-
beled with non-terminal symbols, and the nodes on
the frontier are either words or non-terminal sym-
bols marked with a down arrow (?). An auxiliary
tree is defined as an initial tree, except that exactly
one of its frontier nodes must be marked as foot
node (?). The foot node must be labeled with a non-
terminal symbol that is the same as the label of the
root node.
Synchronous TAG defines two operations to build
derived tree pairs from elementary tree pairs: substi-
tution and adjoining. Nodes in initial and auxiliary
tree pairs are linked to indicate the correspondence
between substitution and adjoining sites. Figure 1
shows three initial tree pairs (i.e., ?1, ?2, and ?3)
and two auxiliary tree pairs (i.e., ?1 and ?2). The
dashed lines link substitution nodes (e.g., NP? and
X? in ?1) and adjoining sites (e.g., NP and X in ?2)
in tree pairs. Substituting the initial tree pair ?1 at
1279
{I
me?iguo?
o?
zo?ngto?ng
n?
a`oba?ma?
?
du`?
l?
qia?ngj??
??
sh`?jia`n
??
yu?y??
gI
qia?nze?
0 1 2 3 4 5 6 7 8
NR NN NR P NN NN VV NN
NP NP NP NP NP
NP PP VP
NP VP
IP
US President Obama has condemned the shooting incident
Figure 2: A training example. Tree-to-string rules can be extracted from shaded nodes.
node minimal initial rule minimal auxiliary rule
NR0,1 [1] ( NR me?iguo? ) ? US
NP0,1 [2] ( NP ( x1:NR? ) ) ? x1
NN1,2 [3] ( NN zo?ngto?ng ) ? President
NP1,2 [4] ( NP ( x1:NN? ) ) ? x1
[5] ( NP ( x1:NP? ) ( x2:NP? ) ) ? x1 x2
[6] ( NP0:1 ( x1:NR? ) ) ? x1 [7] ( NP ( x1:NP? ) ( x2:NP? ) ) ? x1 x2
NP0,2 [8] ( NP0:2 ( x1:NP? ) ( x2:NP? ) ) ? x1 x2
[9] ( NP0:1 ( x1:NN? ) ) ? x1 [10] ( NP ( x1:NP? ) ( x2:NP? ) ) ? x1 x2
[11] ( NP0:2 ( x1:NP? ) ( x2:NP? ) ) ? x1 x2
NR2,3 [12] ( NR a`oba?ma? ) ? Obama
NP2,3 [13] ( NP ( x1:NR? ) ) ? x1
[14] ( NP ( x1:NP? ) ( x2:NP? ) ) ? x1 x2
[15] ( NP0:2 ( x1:NP? ) ( x2:NP? ) ) ? x1 x2 [16] ( NP ( x1:NP? ) ( x2:NP? ) ) ? x1 x2
NP0,3 [17] ( NP0:1 ( x1:NR? ) ) ? x1 [18] ( NP ( x1:NP? ) ( x2:NP? ) ) ? x1 x2
[19] ( NP0:1 ( x1:NN? ) ) ? x1
[20] ( NP0:1 ( x1:NR? ) ) ? x1
NN4,5 [21] ( NN qia?ngj?? ) ? shooting
NN5,6 [22] ( NN sh?`jia`n ) ? incident
NP4,6 [23] ( NP ( x1:NN? ) ( x2:NN? ) ) ? x1 x2
PP3,6 [24] ( PP ( du?` ) ( x1:NP? ) ) ? x1
NN7,8 [25] ( NN qia?nze? ) ? condemned
NP7,8 [26] ( NP ( x1:NN? ) ) ? x1
VP6,8 [27] ( VP ( VV yu?y?? ) ( x1:NP? ) ) ? x1
[28] ( VP ( x1:PP? ) ( x2:VP? ) ) ? x2 the x1VP3,8 [29] ( VP0:1 ( VV yu?y?? ) ( x1:NP? ) ) ? x1 [30] ( VP ( x1:PP? ) ( x2:VP? ) ) ? x2 the x1
IP0,8 [31] ( IP ( x1:NP? ) ( x2:VP? ) ) ? x1 has x2
Table 1: Minimal initial and auxiliary rules extracted from Figure 2. Note that an adjoining site has a span as subscript.
For example, NP0:1 in rule 6 indicates that the node is an adjoining site linked to a target node dominating the target
string spanning from position 0 to position 1 (i.e., x1). The target tree is hidden because tree-to-string translation only
considers the target surface string.
1280
the NP?-X? node pair in the auxiliary tree pair ?1
yields a derived tree pair ?2, which can be adjoined
at NN-X in ?2 to generate ?3.
For simplicity, we represent ?2 as a tree-to-string
rule:
( NP0:1 ( NR me?iguo? ) ) ? US
where NP0:1 indicates that the node is an adjoin-
ing site linked to a target node dominating the tar-
get string spanning from position 0 to position 1
(i.e., ?US?). The target tree is hidden because tree-
to-string translation only considers the target surface
string. Similarly, ?1 can be written as
( NP ( x1:NP? ) ( x2:NP? ) ) ? x1 x2
where x denotes a non-terminal and the subscripts
indicate the correspondence between source and tar-
get non-terminals.
The parameters of a probabilistic synchronous
TAG are
?
?
Pi(?) = 1 (1)
?
?
Ps(?|?) = 1 (2)
?
?
Pa(?|?) + Pa(NONE|?) = 1 (3)
where ? ranges over initial tree pairs, ? over aux-
iliary tree pairs, and ? over node pairs. Pi(?) is
the probability of beginning a derivation with ?;
Ps(?|?) is the probability of substituting ? at ?;
Pa(?|?) is the probability of adjoining ? at ?; fi-
nally, Pa(NONE|?) is the probability of nothing ad-
joining at ?.
For tree-to-string translation, these parameters
can be treated as feature functions of a discrimi-
native framework (Och, 2003) combined with other
conventional features such as relative frequency, lex-
ical weight, rule count, language model, and word
count (Liu et al, 2006).
3 Rule Extraction
Inducing a synchronous TAG from training data
often begins with converting Treebank-style parse
trees to TAG derivations (Xia, 1999; Chen and
Vijay-Shanker, 2000; Chiang, 2003). DeNeefe and
Knight (2009) propose an algorithm to extract syn-
chronous TIG rules from an aligned and parsed
bilingual corpus. They first classify tree nodes
into heads, arguments, and adjuncts using heuristics
(Collins, 2003), then transform a Treebank-style tree
into a TIG derivation, and finally extract minimally-
sized rules from the derivation tree and the string on
the other side, constrained by the alignments. Proba-
bilistic models can be estimated by collecting counts
over the derivation trees.
However, one challenge is that there are many
TAG derivations that can yield the same derived tree,
even with respect to a single grammar. It is difficult
to choose appropriate single derivations that enable
the resulting grammar to translate unseen data well.
DeNeefe and Knight (2009) indicate that the way to
reconstruct TIG derivations has a direct effect on fi-
nal translation quality. They suggest that one possi-
ble solution is to use derivation forest rather than a
single derivation tree for rule extraction.
Alternatively, we extend the GHKM algorithm
(Galley et al, 2004) to directly extract tree-to-string
rules that allow for both substitution and adjoining
from aligned and parsed data. There is no need for
transforming a parse tree into a TAG derivation ex-
plicitly before rule extraction and all derivations can
be easily reconstructed using extracted rules. 1 Our
rule extraction algorithm involves two steps: (1) ex-
tracting minimal rules and (2) composition.
3.1 Extracting Minimal Rules
Figure 2 shows a training example, which consists of
a Chinese parse tree, an English string, and the word
alignment between them. By convention, shaded
nodes are called frontier nodes from which tree-to-
string rules can be extracted. Note that the source
phrase dominated by a frontier node and its corre-
sponding target phrase are consistent with the word
alignment: all words in the source phrase are aligned
to all words in the corresponding target phrase and
vice versa.
We distinguish between three categories of tree-
1Note that our algorithm does not take heads, complements,
and adjuncts into consideration and extracts all possible rules
with respect to word alignment. Our hope is that this treatment
would make our system more robust in the presence of noisy
data. It is possible to use the linguistic preferences as features.
We leave this for future work.
1281
to-string rules:
1. substitution rules, in which the source tree is
an initial tree without adjoining sites.
2. adjoining rules, in which the source tree is an
initial tree with at least one adjoining site.
3. auxiliary rules, in which the source tree is an
auxiliary tree.
For example, in Figure 1, ?1 is a substitution rule,
?2 is an adjoining rule, and ?1 is an auxiliary rule.
Minimal substitution rules are the same with those
in STSG (Galley et al, 2004; Liu et al, 2006) and
therefore can be extracted directly using GHKM. By
minimal, we mean that the interior nodes are not
frontier and cannot be decomposed. For example,
in Table 2, rule 1 (for short r1) is a minimal substi-
tution rule extracted from NR0,1.
Minimal adjoining rules are defined as minimal
substitution rules, except that each root node must
be an adjoining site. In Table 2, r2 is a minimal
substitution rule extracted from NP0,1. As NP0,1 is
a descendant of NP0,2 with the same label, NP0,1
is a possible adjoining site. Therefore, r6 can be
derived from r2 and licensed as a minimal adjoining
rule extracted from NP0,2. Similarly, four minimal
adjoining rules are extracted from NP0,3 because it
has four frontier descendants labeled with NP.
Minimal auxiliary rules are derived from minimal
substitution and adjoining rules. For example, in Ta-
ble 2, r7 and r10 are derived from the minimal sub-
stitution rule r5 while r8 and r11 are derived from
r15. Note that a minimal auxiliary rule can have ad-
joining sites (e.g., r8).
Table 1 lists 17 minimal substitution rules, 7 min-
imal adjoining rules, and 7 minimal auxiliary rules
extracted from Figure 2.
3.2 Composition
We can obtain composed rules that capture rich con-
texts by substituting and adjoining minimal initial
and auxiliary rules. For example, the composition
of r12, r17, r25, r26, r29, and r31 yields an initial
rule with two adjoining sites:
( IP ( NP0:1 ( NR a`oba?ma? ) ) ( VP2:3 ( VV yu?y?? )
( NP ( NN qia?nze? ) ) ) ) ? Obama has condemned
Note that the source phrase ?a`oba?ma? . . . yu?y?? qia?nze??
is discontinuous. Our model allows both the source
and target phrases of an initial rule with adjoining
sites to be discontinuous, which goes beyond the ex-
pressive power of synchronous CFG and TSG.
Similarly, the composition of two auxiliary rules
r8 and r16 yields a new auxiliary rule:
( NP ( NP ( x1:NP? ) ( x2:NP? ) ) ( x3:NP? ) ) ? x1x2x3
We first compose initial rules and then com-
pose auxiliary rules, both in a bottom-up way. To
maintain a reasonable grammar size, we follow Liu
(2006) to restrict that the tree height of a rule is no
greater than 3 and the source surface string is no
longer than 7.
To learn the probability models Pi(?), Ps(?|?),
Pa(?|?), and Pa(NONE|?), we collect and normal-
ize counts over these extracted rules following De-
Neefe and Knight (2009).
4 Decoding
Given a synchronous TAG and a derived source tree
pi, a tree-to-string decoder finds the English yield
of the best derivation of which the Chinese yield
matches pi:
e? = e
(
arg max
D s.t. f(D)=pi
P (D)
)
(4)
This is called tree parsing (Eisner, 2003) as the de-
coder finds ways of decomposing pi into elementary
trees.
Tree-to-string decoding with STSG is usually
treated as forest rescoring (Huang and Chiang,
2007) that involves two steps. The decoder first con-
verts the input tree into a translation forest using a
translation rule set by pattern matching. Huang et
al. (2006) show that this step is a depth-first search
with memorization in O(n) time. Then, the decoder
searches for the best derivation in the translation for-
est intersected with n-gram language models and
outputs the target string. 2
Decoding with STAG, however, poses one major
challenge to forest rescoring. As translation forest
only supports substitution, it is difficult to construct
a translation forest for STAG derivations because of
2Mi et al (2008) give a detailed description of the two-step
decoding process. Huang and Mi (2010) systematically analyze
the decoding complexity of tree-to-string translation.
1282
?1
IP0,8
NP2,3 VP3,8?
NR2,3?
?2
NR2,3
n?
a`oba?ma?
?1
NP0,3
NP1,2 NP2,3?
NN1,2?
?2
NP0,3
NP0,2? NP
2,3
?
?3
NP0,2
NP0,1 NP1,2?
NR0,1?
?3
NN2,3
o?
zo?ngto?ng
elementary tree translation rule
?1 r1 ( IP ( NP0:1 ( x1:NR? ) ) ( x2:VP? ) ) ? x1 x2
?2 r2 ( NR a`oba?ma? ) ? Obama
?1 r3 ( NP ( NP0:1 ( x1:NN? ) ) ( x2:NP? ) ) ? x1 x2
?2 r4 ( NP ( x1:NP? ) ( x2:NP? ) ) ? x1 x2
?3 r5 ( NP ( NP ( x1:NR? ) ) ( x2:NP? ) ) ? x1 x2
?3 r6 ( NN zo?ngto?ng ) ? President
Figure 3: Matched trees and corresponding rules. Each node in a matched tree is annotated with a span as superscript
to facilitate identification. For example, IP0,8 in ?1 indicates that IP0,8 in Figure 2 is matched. Note that its left child
NP2,3 is not its direct descendant in Figure 2, suggesting that adjoining is required at this site.
?1
?2(1.1) ?1(1) ?2(1)
?3(1) ?3(1.1)
IP0,8
NP0,2 VP3,8
NR0,1 NN1,2 NR2,3
e1 e2
e3 e4
hyperedge translation rule
e1 r1 + r4 ( IP ( NP ( x1:NP? ) ( NP ( x2:NR? ) ) ) ( x3:VP? ) ? x1 x2 x3
e2 r1 + r3 + r5 ( IP ( NP ( NP ( x1:NP? ) ( x2:NP? ) ) ( NP ( x3:NR? ) ) ) ( x4:VP? ) ) ? x1 x2 x3 x4
e3 r6 ( NN zo?ngto?ng ) ? President
e4 r2 ( NR a`oba?ma? ) ? Obama
Figure 4: Converting a derivation forest to a translation forest. In a derivation forest, a node in a derivation forest is a
matched elementary tree. A hyperedge corresponds to operations on related trees: substitution (dashed) or adjoining
(solid). We use Gorn addresses as tree addresses. ?2(1.1) denotes that ?2 is substituted in the tree ?1 at the node NR2,3?
of address 1.1 (i.e., the first child of the first child of the root node). As translation forest only supports substitution, we
combine trees with adjoining sites to form an equivalent tree without adjoining sites. Rules are composed accordingly
(e.g., r1 + r4).
1283
adjoining. Therefore, we divide forest rescoring for
STAG into three steps:
1. matching, matching STAG rules against the in-
put tree to obtain a TAG derivation forest;
2. conversion, converting the TAG derivation for-
est into a translation forest;
3. intersection, intersecting the translation forest
with an n-gram language model.
Given a tree-to-string rule, rule matching is to find
a subtree of the input tree that is identical to the
source side of the rule. While matching STSG rules
against a derived tree is straightforward, it is some-
what non-trivial for STAG rules that move beyond
nodes of a local tree. We follow Liu et al (2006) to
enumerate all elementary subtrees and match STAG
rules against these subtrees. This can be done by first
enumerating all minimal initial and auxiliary trees
and then combining them to obtain composed trees,
assuming that every node in the input tree is fron-
tier (see Section 3). We impose the same restrictions
on the tree height and length as in rule extraction.
Figure 3 shows some matched trees and correspond-
ing rules. Each node in a matched tree is annotated
with a span as superscript to facilitate identification.
For example, IP0,8 in ?1 means that IP0,8 in Figure
2 is matched. Note that its left child NP2,3 is not
its direct descendant in Figure 2, suggesting that ad-
joining is required at this site.
A TAG derivation tree specifies uniquely how
a derived tree is constructed using elementary trees
(Joshi, 1985). A node in a derivation tree is an ele-
mentary tree and an edge corresponds to operations
on related elementary trees: substitution or adjoin-
ing. We introduce TAG derivation forest, a com-
pact representation of multiple TAG derivation trees,
to encodes all matched TAG derivation trees of the
input derived tree.
Figure 4 shows part of a TAG derivation forest.
The six matched elementary trees are nodes in the
derivation forest. Dashed and solid lines represent
substitution and adjoining, respectively. We use
Gorn addresses as tree addresses: 0 is the address
of the root node, p is the address of the pth child of
the root node, and p ? q is the address of the qth child
of the node at the address p. The derivation forest
should be interpreted as follows: ?2 is substituted in
the tree ?1 at the node NR2,3? of address 1.1 (i.e., the
first child of the first child of the root node) and ?1 is
adjoined in the tree ?1 at the node NP2,3 of address
1.
To take advantage of existing decoding tech-
niques, it is necessary to convert a derivation forest
to a translation forest. A hyperedge in a transla-
tion forest corresponds to a translation rule. Mi et
al. (2008) describe how to convert a derived tree
to a translation forest using tree-to-string rules only
allowing for substitution. Unfortunately, it is not
straightforward to convert a derivation forest includ-
ing adjoining to a translation forest. To alleviate this
problem, we combine initial rules with adjoining
sites and associated auxiliary rules to form equiv-
alent initial rules without adjoining sites on the fly
during decoding.
Consider ?1 in Figure 3. It has an adjoining site
NP2,3. Adjoining ?2 in ?1 at the node NP2,3 pro-
duces an equivalent initial tree with only substitution
sites:
( IP0,8 ( NP0,3 ( NP0,2? ) ( NP2,3 ( NR2,3? ) ) ) ( VP3,8? ) )
The corresponding composed rule r1 + r4 has no
adjoining sites and can be added to translation forest.
We define that the elementary trees needed to be
composed (e.g., ?1 and ?2) form a composition tree
in a derivation forest. A node in a composition tree is
a matched elementary tree and an edge corresponds
to adjoining operations. The root node must be an
initial tree with at least one adjoining site. The de-
scendants of the root node must all be auxiliary trees.
For example, ( ?1 ( ?2 ) ) and ( ?1 ( ?1 ( ?3 ) ) ) are
two composition trees in Figure 4. The number of
children of a node in a composition tree depends on
the number of adjoining sites in the node. We use
composition forest to encode all possible composi-
tion trees.
Often, a node in a composition tree may have mul-
tiple matched rules. As a large amount of composi-
tion trees and composed rules can be identified and
constructed on the fly during forest conversion, we
used cube pruning (Chiang, 2007; Huang and Chi-
ang, 2007) to achieve a balance between translation
quality and decoding efficiency.
1284
category description number
VP verb phrase 12.40
NP noun phrase 7.69
IP simple clause 7.26
QP quantifier phrase 0.14
CP clause headed by C 0.10
PP preposition phrase 0.09
CLP classifier phrase 0.02
ADJP adjective phrase 0.02
LCP phrase formed by ?XP+LC? 0.02
DNP phrase formed by ?XP+DEG? 0.01
Table 2: Top-10 phrase categories of foot nodes and their
average occurrences in training corpus.
5 Evaluation
We evaluated our adjoining tree-to-string translation
system on Chinese-English translation. The bilin-
gual corpus consists of 1.5M sentences with 42.1M
Chinese words and 48.3M English words. The Chi-
nese sentences in the bilingual corpus were parsed
by an in-house parser. To maintain a reasonable
grammar size, we follow Liu et al (2006) to re-
strict that the height of a rule tree is no greater than
3 and the surface string?s length is no greater than 7.
After running GIZA++ (Och and Ney, 2003) to ob-
tain word alignment, our rule extraction algorithm
extracted 23.0M initial rules without adjoining sites,
6.6M initial rules with adjoining sites, and 5.3M
auxiliary rules. We used the SRILM toolkit (Stol-
cke, 2002) to train a 4-gram language model on the
Xinhua portion of the GIGAWORD corpus, which
contains 238M English words. We used the 2002
NIST MT Chinese-English test set as the develop-
ment set and the 2003-2005 NIST test sets as the
test sets. We evaluated translation quality using the
BLEU metric, as calculated by mteval-v11b.pl with
case-insensitive matching of n-grams.
Table 2 shows top-10 phrase categories of foot
nodes and their average occurrences in training cor-
pus. We find that VP (verb phrase) is most likely
to be the label of a foot node in an auxiliary rule.
On average, there are 12.4 nodes labeled with VP
are identical to one of its ancestors per tree. NP and
IP are also found to be foot node labels frequently.
Figure 4 shows the average occurrences of foot node
labels VP, NP, and IP over various distances. A dis-
tance is the difference of levels between a foot node
0.0
0.5
1.0
1.5
2.0
2.5
3.0
3.5
4.0
4.5
 0  1  2  3  4  5  6  7  8  9  10  11
av
er
ag
e 
oc
cu
rre
nc
e
distance
VP
IP
NP
Figure 5: Average occurrences of foot node labels VP,
NP, and IP over various distances.
system grammar MT03 MT04 MT05
Moses - 33.10 33.96 32.17
hierarchical SCFG 33.40 34.65 32.88
STSG 33.13 34.55 31.94
tree-to-string STAG 33.64 35.28 32.71
Table 3: BLEU scores on NIST Chinese-English test sets.
Scores marked in bold are significantly better that those
of STSG at pl.01 level.
and the root node. For example, in Figure 2, the dis-
tance between NP0,1 and NP0,3 is 2 and the distance
between VP6,8 and VP3,8 is 1. As most foot nodes
are usually very close to the root nodes, we restrict
that a foot node must be the direct descendant of the
root node in our experiments.
Table 3 shows the BLEU scores on the NIST
Chinese-English test sets. Our baseline system is the
tree-to-string system using STSG (Liu et al, 2006;
Huang et al, 2006). The STAG system outper-
forms the STSG system significantly on the MT04
and MT05 test sets at pl.01 level. Table 3 also
gives the results of Moses (Koehn et al, 2007) and
an in-house hierarchical phrase-based system (Chi-
ang, 2007). Our STAG system achieves compara-
ble performance with the hierarchical system. The
absolute improvement of +0.7 BLEU over STSG is
close to the finding of DeNeefe and Knight (2009)
on string-to-tree translation. We feel that one major
obstacle for achieving further improvement is that
composed rules generated on the fly during decod-
ing (e.g., r1 + r3 + r5 in Figure 4) usually have too
many non-terminals, making cube pruning in the in-
1285
STSG STAG
matching 0.086 0.109
conversion 0.000 0.562
intersection 0.946 1.064
other 0.012 0.028
total 1.044 1.763
Table 4: Comparison of average decoding time.
tersection phase suffering from severe search errors
(only a tiny fraction of the search space can be ex-
plored). To produce the 1-best translations on the
MT05 test set that contains 1,082 sentences, while
the STSG system used 40,169 initial rules without
adjoining sites, the STAG system used 28,046 initial
rules without adjoining sites, 1,057 initial rules with
adjoining sites, and 1,527 auxiliary rules.
Table 4 shows the average decoding time on the
MT05 test set. While rule matching for STSG needs
0.086 second per sentence, the matching time for
STAG only increases to 0.109 second. For STAG,
the conversion of derivation forests to translation
forests takes 0.562 second when we restrict that at
most 200 rules can be generated on the fly for each
node. As we use cube pruning, although the trans-
lation forest of STAG is bigger than that of STSG,
the intersection time barely increases. In total, the
STAG system runs in 1.763 seconds per sentence,
only 1.6 times slower than the baseline system.
6 Conclusion
We have presented a new tree-to-string translation
system based on synchronous TAG. With translation
rules learned from Treebank-style trees, the adjoin-
ing tree-to-string system outperforms the baseline
system using STSG without significant loss in effi-
ciency. We plan to introduce left-to-right target gen-
eration (Huang and Mi, 2010) into the STAG tree-
to-string system. Our work can also be extended to
forest-based rule extraction and decoding (Mi et al,
2008; Mi and Huang, 2008). It is also interesting to
introduce STAG into tree-to-tree translation (Zhang
et al, 2008; Liu et al, 2009; Chiang, 2010).
Acknowledgements
The authors were supported by National Natural
Science Foundation of China Contracts 60736014,
60873167, and 60903138. We thank the anonymous
reviewers for their insightful comments.
References
Anne Abeille, Yves Schabes, and Aravind Joshi. 1990.
Using lexicalized tags for machine translation. In
Proc. of COLING 1990.
John Chen and K. Vijay-Shanker. 2000. Automated ex-
traction of tags from the penn treebank. In Proc. of
IWPT 2000.
David Chiang. 2003. Statistical parsing with an au-
tomatically extracted tree adjoining grammar. Data-
Oriented Parsing.
David Chiang. 2006. An introduction to synchronous
grammars. ACL Tutorial.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
David Chiang. 2010. Learning to translate with source
and target syntax. In Proc. of ACL 2010.
Michael Collins. 2003. Head-driven statistical models
for natural language parsing. Computational Linguis-
tics, 29(4).
Steve DeNeefe and Kevin Knight. 2009. Synchronous
tree adjoining machine translation. In Proc. of
EMNLP 2009.
Mark Dras. 1999. A meta-level grammar: Redefining
synchronous tag for translation and paraphrase. In
Proc. of ACL 1999.
Jason Eisner. 2003. Learning non-isomorphic tree map-
pings for machine translation. In Proc. of ACL 2003.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In Proc.
of NAACL 2004.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proc. of
ACL 2006.
Liang Huang and David Chiang. 2007. Forest rescoring:
Faster decoding with integrated language models. In
Proc. of ACL 2007.
Liang Huang and Haitao Mi. 2010. Efficient incremen-
tal decoding for tree-to-string translation. In Proc. of
EMNLP 2010.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proc. of AMTA 2006.
Aravind Joshi, L. Levy, and M. Takahashi. 1975. Tree
adjunct grammars. Journal of Computer and System
Sciences, 10(1).
Aravind Joshi. 1985. How much contextsensitiv-
ity is necessary for characterizing structural descrip-
tions)tree adjoining grammars. Natural Language
1286
Processing)Theoretical, Computational, and Psy-
chological Perspectives.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Pro-
ceedings of ACL 2007 (poster), pages 77?80, Prague,
Czech Republic, June.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine trans-
lation. In Proc. of ACL 2006.
Yang Liu, Yajuan Lu?, and Qun Liu. 2009. Improving
tree-to-tree translation with packed forests. In Proc. of
ACL 2009.
Haitao Mi and Liang Huang. 2008. Forest-based transla-
tion rule extraction. In Proceedings of EMNLP 2008.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proceedings of ACL/HLT 2008,
pages 192?199, Columbus, Ohio, USA, June.
Rebecca Nesson, Stuart Shieber, and Alexander Rush.
2006. Induction of probabilistic synchronous tree-
insertion grammars for machine translation. In Proc.
of AMTA 2006.
Franz J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29(1):19?51.
Franz Och. 2003. Minimum error rate training in statis-
tical machine translation. In Proc. of ACL 2003.
Gilles Prigent. 1994. Synchronous tags and machine
translation. In Proc. of TAG+3.
Yves Schabes and Richard Waters. 1995. A cubic-time,
parsable formalism that lexicalizes context-free gram-
mar without changing the trees produced. Computa-
tional Linguistics, 21(4).
Stuart M. Shieber and Yves Schabes. 1990. Synchronous
tree-adjoining grammars. In Proc. of COLING 1990.
Stuart M. Shieber. 2007. Probabilistic synchronous tree-
adjoining grammars for machine translation: The ar-
gument from bilingual dictionaries. In Proc. of SSST
2007.
Andreas Stolcke. 2002. Srilm - an extensible language
modeling toolkit. In Proceedings of ICSLP 2002,
pages 901?904.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?404.
Fei Xia. 1999. Extracting tree adjoining grammars from
bracketed corpora. In Proc. of the Fifth Natural Lan-
guage Processing Pacific Rim Symposium.
Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Maxi-
mum entropy based phrase reordering model for sta-
tistical machine translation. In Proc. of ACL 2006.
Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li,
Chew Lim Tan, and Sheng Li. 2008. A tree se-
quence alignment-based tree-to-tree translation model.
In Proc. of ACL 2008.
1287
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 71?76,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Insertion, Deletion, or Substitution? Normalizing Text Messages without
Pre-categorization nor Supervision
Fei Liu1 Fuliang Weng2 Bingqing Wang3 Yang Liu1
1Computer Science Department, The University of Texas at Dallas
2Research and Technology Center, Robert Bosch LLC
3School of Computer Science, Fudan University
{feiliu, yangl}@hlt.utdallas.edu1
fuliang.weng@us.bosch.com2, wbq@fudan.edu.cn3
Abstract
Most text message normalization approaches
are based on supervised learning and rely on
human labeled training data. In addition, the
nonstandard words are often categorized into
different types and specific models are de-
signed to tackle each type. In this paper,
we propose a unified letter transformation ap-
proach that requires neither pre-categorization
nor human supervision. Our approach mod-
els the generation process from the dictionary
words to nonstandard tokens under a sequence
labeling framework, where each letter in the
dictionary word can be retained, removed, or
substituted by other letters/digits. To avoid
the expensive and time consuming hand label-
ing process, we automatically collected a large
set of noisy training pairs using a novel web-
based approach and performed character-level
alignment for model training. Experiments on
both Twitter and SMS messages show that our
system significantly outperformed the state-
of-the-art deletion-based abbreviation system
and the jazzy spell checker (absolute accuracy
gain of 21.69% and 18.16% over jazzy spell
checker on the two test sets respectively).
1 Introduction
Recent years have witnessed the explosive growth
of text message usage, including the mobile phone
text messages (SMS), chat logs, emails, and sta-
tus updates from the social network websites such
as Twitter and Facebook. These text message col-
lections serve as valuable information sources, yet
the nonstandard contents within them often degrade
2gether (6326) togetha (919) tgthr (250) togeda (20)
2getha (1266) togather (207) t0gether (57) toqethaa (10)
2gthr (178) togehter (94) togeter (49) 2getter (10)
2qetha (46) togethor (29) tagether (18) 2gtr (6)
Table 1: Nonstandard tokens originated from ?together?
and their frequencies in the Edinburgh Twitter corpus.
the existing language processing systems, calling
the need of text normalization before applying the
traditional information extraction, retrieval, senti-
ment analysis (Celikyilmaz et al, 2010), or sum-
marization techniques. Text message normalization
is also of crucial importance for building text-to-
speech (TTS) systems, which need to determine pro-
nunciation for nonstandard words.
Text message normalization aims to replace the
non-standard tokens that carry significant mean-
ings with the context-appropriate standard English
words. This is a very challenging task due to the
vast amount and wide variety of existing nonstan-
dard tokens. We found more than 4 million dis-
tinct out-of-vocabulary tokens in the English tweets
of the Edinburgh Twitter corpus (see Section 2.2).
Table 1 shows examples of nonstandard tokens orig-
inated from the word ?together?. We can see that
some variants can be generated by dropping let-
ters from the original word (?tgthr?) or substitut-
ing letters with digit (?2gether?); however, many
variants are generated by combining the letter in-
sertion, deletion, and substitution operations (?to-
qethaa?, ?2gthr?). This shows that it is difficult to
divide the nonstandard tokens into exclusive cate-
gories.
Among the literature of text normalization
71
(for text messages or other domains), Sproat et
al. (2001), Cook and Stevenson (2009) employed the
noisy channel model to find the most probable word
sequence given the observed noisy message. Their
approaches first classified the nonstandard tokens
into various categories (e.g., abbreviation, stylistic
variation, prefix-clipping), then calculated the pos-
terior probability of the nonstandard tokens based
on each category. Choudhury et al (2007) de-
veloped a hidden Markov model using hand anno-
tated training data. Yang et al (2009), Pennell and
Liu (2010) focused on modeling word abbreviations
formed by dropping characters from the original
word. Toutanova and Moore (2002) addressed the
phonetic substitution problem by extending the ini-
tial letter-to-phone model. Aw et al (2006), Kobus
et al (2008) viewed the text message normalization
as a statistical machine translation process from the
texting language to standard English. Beaufort et
al. (2010) experimented with the weighted finite-
state machines for normalizing French SMS mes-
sages. Most of the above approaches rely heavily
on the hand annotated data and involve categorizing
the nonstandard tokens in the first place, which gives
rise to three problems: (1) the labeled data is very
expensive and time consuming to obtain; (2) it is
hard to establish a standard taxonomy for categoriz-
ing the tokens found in text messages; (3) the lack of
optimized way to integrate various category-specific
models often compromises the system performance,
as confirmed by (Cook and Stevenson, 2009).
In this paper, we propose a general letter trans-
formation approach that normalizes nonstandard to-
kens without categorizing them. A large set of noisy
training word pairs were automatically collected via
a novel web-based approach and aligned at the char-
acter level for model training. The system was tested
on both Twitter and SMS messages. Results show
that our system significantly outperformed the jazzy
spell checker and the state-of-the-art deletion-based
abbreviation system, and also demonstrated good
cross-domain portability.
2 Letter Transformation Approach
2.1 General Framework
Given a noisy text message T , our goal is to nor-
malize it into a standard English word sequence S.
b - - - - d a y f - o t o z
h u b b i e
(1) birthday --> bday
(2) photos --> fotoz
(4) hubby --> hubbie
b i r t h d a y
p h o t o s
h u b b y
s o m e 1 - -
(6) someone --> some1
s o m e o n e
n u t h i n -
(3) nothing --> nuthin
n o t h i n g
4 - - e v a -
(5) forever --> 4eva
f o r e v e r
Figure 1: Examples of nonstandard tokens generated by
performing letter transformation on the dictionary words.
Under the noisy channel model, this is equivalent to
finding the sequence S? that maximizes p(S|T ):
S? = argmaxS p(S|T ) = argmaxS(
?
i
p(Ti|Si))p(S)
where we assume that each non-standard token Ti
is dependent on only one English word Si, that is,
we are not considering acronyms (e.g., ?bbl? for
?be back later?) in this study. p(S) can be cal-
culated using a language model (LM). We formu-
late the process of generating a nonstandard token
Ti from dictionary word Si using a letter transfor-
mation model, and use the model confidence as the
probability p(Ti|Si). Figure 1 shows several exam-
ple (word, token) pairs1. To form a nonstandard to-
ken, each letter in the dictionary word can be labeled
with: (a) one of the 0-9 digits; (b) one of the 26 char-
acters including itself; (c) the null character ?-?; (d)
a letter combination. This transformation process
from dictionary words to nonstandard tokens will be
learned automatically through a sequence labeling
framework that integrates character-, phonetic-, and
syllable-level information.
In general, the letter transformation approach will
handle the nonstandard tokens listed in Table 2 yet
without explicitly categorizing them. Note for the
tokens with letter repetition, we first generate a set
of variants by varying the repetitive letters (e.g. Ci =
{?pleas?, ?pleeas?, ?pleaas?, ?pleeaas?, ?pleeeaas?}
for Ti = {?pleeeaas?}), then select the maximum
posterior probability among all the variants:
p(Ti|Si) = max
T?i?Ci
p(T?i|Si)
1The ideal transform for example (5) would be ?for? to ?4?.
But in this study we are treating each letter in the English word
separately and not considering the phrase-level transformation.
72
(1) abbreviation tgthr, weeknd, shudnt
(2) phonetic sub w/- or w/o digit 4got, sumbody, kulture
(3) graphemic sub w/- or w/o digit t0gether, h3r3, 5top, doinq
(4) typographic error thimg, macam
(5) stylistic variation betta, hubbie, cutie
(6) letter repetition pleeeaas, togtherrr
(7) any combination of (1) to (6) luvvvin, 2moro, m0rnin
Table 2: Nonstandard tokens that can be processed by the
unified letter transformation approach.
2.2 Web based Data Collection w/o Supervision
We propose to automatically collect training data
(annotate nonstandard words with the corresponding
English forms) using a web-based approach, there-
fore avoiding the expensive human annotation. We
use the Edinburgh Twitter corpus (Petrovic et al,
2010) for data collection, which contains 97 mil-
lion Twitter messages. The English tweets were
extracted using the TextCat language identification
toolkit (Cavnar and Trenkle, 1994), and tokenized
into a sequence of clean tokens consisting of letters,
digits, and apostrophe.
For the out-of-vocabulary (OOV) tokens consist-
ing of letters and apostrophe, we form n Google
queries for each of them in the form of either
?w1 w2 w3? OOV or OOV ?w1 w2 w3?, where w1
to w3 are consecutive context words extracted from
the tweets that contain this OOV. n is set to 6 in this
study. The first 32 returned snippets for each query
are parsed and the words in boldface that are differ-
ent from both the OOV and the context words are
collected as candidate normalized words. Among
them, we further select the words that have longer
common character sequence with the OOV than with
the context words, and pair each of them with the
OOV to form the training pairs. For the OOV tokens
consisting of both letters and digits, we use simple
rules to recover possible original words. These rules
include: 1 ? ?one?, ?won?, ?i?; 2 ? ?to?, ?two?,
?too?; 3 ? ?e?; 4 ? ?for?, ?fore?, ?four?; 5 ? ?s?;
6 ? ?b?; 8 ? ?ate?, ?ait?, ?eat?, ?eate?, ?ight?,
?aight?. The OOV tokens and any resulting words
from the above process are included in the noisy
training pairs. In addition, we add 932 word pairs
of chat slangs and their normalized word forms col-
lected from InternetSlang.com that are not covered
by the above training set.
These noisy training pairs were further expanded
and purged. We apply the transitive rule on these
initially collected training pairs. For example, if the
two pairs ?(cause, cauz)? and ?(cauz, coz)? are in the
data set, we will add ?(cause, coz)? as another train-
ing pair. We remove the data pairs whose word can-
didate is not in the CMU dictionary. We also remove
the pairs whose word candidate and OOV are simply
inflections of each other, e.g., ?(headed, heading)?,
using a set of rules. In total, this procedure generated
62,907 training word pairs including 20,880 unique
candidate words and 46,356 unique OOVs.2
2.3 Automatic Letter-level Alignment
Given a training pair (Si, Ti) consisting of a word Si
and its nonstandard variant Ti, we propose a proce-
dure to align each letter in Si with zero, one, or more
letters/digits in Ti. First we align the letters of the
longest common sequence between the dictionary
word and the variant (which gives letter-to-letter cor-
respondence in those common subsequences). Then
for the letter chunks in between each of the obtained
alignments, we process them based on the following
three cases:
(a) (many-to-0): a chunk in the dictionary word
needs to be aligned to zero letters in the variant.
In this case, we map each letter in the chunk to
?-? (e.g., ?birthday? to ?bday?), obtaining letter-
level alignments.
(b) (0-to-many): zero letters in the dictionary word
need to be aligned to a letter/digit chunk in the
variant. In this case, if the first letter in the
chunk can be combined with the previous letter
to form a digraph (such as ?wh? when aligning
?sandwich? to ?sandwhich?), we combine these
two letters. The remaining letters, or the entire
chunk when the first letter does not form a di-
graph with the previous letter, are put together
with the following aligned letter in the variant.
(c) (many-to-many): non-zero letters in the dictio-
nary word need to be aligned to a chunk in the
variant. Similar to (b), the first letter in the vari-
ant chunk is merged with the previous alignment
if they form a digraph. Then we map the chunk
in the dictionary word to the chunk in the vari-
ant as one alignment, e.g., ?someone? aligned to
?some1?.
2Please contact the first author for the collected word pairs.
73
The (b) and (c) cases above generate chunk-level
(with more than one letter) alignments. To elimi-
nate possible noisy training pairs, such as (?you?,
?haveu?), we keep all data pairs containing digits,
but remove the data pairs with chunks involving
three letters or more in either the dictionary word or
the variant. For the chunk alignments in the remain-
ing pairs, we sequentially align the letters (e.g., ?ph?
aligned to ?f-?). Note that for those 1-to-2 align-
ments, we align the single letter in the dictionary
word to a two-letter combination in the variant. We
limit to the top 5 most frequent letter combinations,
which are ?ck?, ?ey?, ?ie?, ?ou?, ?wh?, and the pairs
involving other combinations are removed.
After applying the letter alignment to the col-
lected noisy training word pairs, we obtained
298,160 letter-level alignments. Some example
alignments and corresponding word pairs are:
e ? ? ? (have, hav) q ? k (iraq, irak)
e ? a (another, anotha) q ? g (iraq, irag)
e? 3 (online, 0nlin3) w?wh (watch, whatch)
2.4 Sequence Labeling Model for P (Ti|Si)
For a letter sequence Si, we use the conditional ran-
dom fields (CRF) model to perform sequence tag-
ging to generate its variant Ti. To train the model,
we first align the collected dictionary word and its
variant at the letter level, then construct a feature
vector for each letter in the dictionary word, using
its mapped character as the reference label. This la-
beled data set is used to train a CRF model with L-
BFGS (Lafferty et al, 2001; Kudo, 2005). We use
the following features:
? Character-level features
Character n-grams: c?1, c0, c1, (c?2 c?1),
(c?1 c0), (c0 c1), (c1 c2), (c?3 c?2 c?1),
(c?2 c?1 c0), (c?1 c0 c1), (c0 c1 c2), (c1 c2 c3).
The relative position of character in the word.
? Phonetic-level features
Phoneme n-grams: p?1, p0, p1, (p?1 p0),
(p0 p1). We use the many-to-many letter-
phoneme alignment algorithm (Jiampojamarn
et al, 2007) to map each letter to multiple
phonemes (1-to-2 alignment). We use three bi-
nary features to indicate whether the current,
previous, or next character is a vowel.
? Syllable-level features
Relative position of the current syllable in the
word; two binary features indicating whether
the character is at the beginning or the end of
the current syllable. The English hyphenation
dictionary (Hindson, 2006) is used to mark all
the syllable information.
The trained CRF model can be applied to any En-
glish word to generate its variants with probabilities.
3 Experiments
We evaluate the system performance on both Twitter
and SMS message test sets. The SMS data was used
in previous work (Choudhury et al, 2007; Cook and
Stevenson, 2009). It consists of 303 distinct non-
standard tokens and their corresponding dictionary
words. We developed our own Twitter message test
set consisting of 6,150 tweets manually annotated
via the Amazon Mechanical Turk. 3 to 6 turkers
were required to convert the nonstandard tokens in
the tweets to the standard English words. We extract
the nonstandard tokens whose most frequently nor-
malized word consists of letters/digits/apostrophe,
and is different from the token itself. This results
in 3,802 distinct nonstandard tokens that we use as
the test set. 147 (3.87%) of them have more than
one corresponding standard English words. Similar
to prior work, we use isolated nonstandard tokens
without any context, that is, the LM probabilities
P (S) are based on unigrams.
We compare our system against three approaches.
The first one is a comprehensive list of chat slangs,
abbreviations, and acronyms collected by Internet-
Slang.com; it contains normalized word forms for
6,105 commonly used slangs. The second is the
word-abbreviation lookup table generated by the su-
pervised deletion-based abbreviation approach pro-
posed in (Pennell and Liu, 2010). It contains
477,941 (word, abbreviation) pairs automatically
generated for 54,594 CMU dictionary words. The
third is the jazzy spell checker based on the Aspell
algorithm (Idzelis, 2005). It integrates the phonetic
matching algorithm (DoubleMetaphone) and Leven-
shtein distance that enables the interchanging of two
adjacent letters, and changing/deleting/adding of let-
ters. The system performance is measured using the
n-best accuracy (n=1,3). For each nonstandard to-
ken, the system is considered correct if any of the
corresponding standard words is among the n-best
output from the system.
74
System Accuracy
Twitter (3802 pairs) SMS (303 pairs)
1-best 3-best 1-best 3-best
InternetSlang 7.94 8.07 4.95 4.95
(Pennell et al 2010) 20.02 27.09 21.12 28.05
Jazzy Spell Checker 47.19 56.92 43.89 55.45
LetterTran (Trim) 57.44 64.89 58.09 70.63
LetterTran (All) 59.15 67.02 58.09 70.96
LetterTran (All) + Jazzy 68.88 78.27 62.05 75.91
(Choudhury et al 2007) n/a n/a 59.9 n/a
(Cook et al 2009) n/a n/a 59.4 n/a
Table 3: N-best performance on Twitter and SMS data
sets using different systems.
Results of system accuracies are shown in Ta-
ble 3. For the system ?LetterTran (All)?, we first
generate a lookup table by applying the trained CRF
model to the CMU dictionary to generate up to
30 variants for each dictionary word.3 To make
the comparison more meaningful, we also trim our
lookup table to the same size as the deletion ta-
ble, namely ?LetterTran (Trim)?. The trimming was
performed by selecting the most frequent dictionary
words and their generated variants until the length
limit is reached. Word frequency information was
obtained from the entire Edinburgh corpus. For both
the deletion and letter transformation lookup tables,
we generate a ranked list of candidate words for each
nonstandard token, by sorting the combined score
p(Ti|Si)?C(Si), where p(Ti|Si) is the model con-
fidence and C(Si) is the unigram count generated
from the Edinburgh corpus (we used counts instead
of unigram probability P (Si)). Since the string sim-
ilarity and letter switching algorithms implemented
in jazzy can compensate the letter transformation
model, we also investigate combining it with our ap-
proach, ?LetterTran(All) + Jazzy?. In this configura-
tion, we combine the candidate words from both sys-
tems and rerank them according to the unigram fre-
quency; since the ?LetterTran? itself is very effective
in ranking candidate words, we only use the jazzy
output for tokens where ?LetterTran? is not very
confident about its best candidate ((p(Ti|Si)?C(Si)
is less than a threshold ? = 100).
We notice the accuracy using the InternetSlang
list is very poor, indicating text message normal-
ization is a very challenging task that can hardly
3We heuristically choose this large number since the learned
letter/digit insertion, substitution, and deletion patterns tend to
generate many variants for each dictionary word.
be tackled by using a hand-crafted list. The dele-
tion table has modest performance given the fact
that it covers only deletion-based abbreviations and
letter repetitions (see Section 2.1). The ?Letter-
Tran? approach significantly outperforms all base-
lines even after trimming. This is because it han-
dles different ways of forming nonstandard tokens
in an unified framework. Taking the Twitter test
set for an example, the lookup table generated by
?LetterTran? covered 69.94% of the total test to-
kens, and among them, 96% were correctly normal-
ized in the 3-best output, resulting in 67.02% over-
all accuracy. The test tokens that were not covered
by the ?LetterTrans? model include those generated
by accidentally switching and inserting letters (e.g.,
?absolotuely? for ?absolutely?) and slangs (?addy?
or ?address?). Adding the output from jazzy com-
pensates these problems and boosts the 1-best ac-
curacy, achieving 21.69% and 18.16% absolute per-
formance gain respectively on the Twitter and SMS
test sets, as compared to using jazzy only. We also
observe that the ?LetterTran? model can be easily
ported to the SMS domain. When combined with
the jazzy module, it achieved 62.05% 1-best accu-
racy, outperforming the domain-specific supervised
system in (Choudhury et al, 2007) (59.9%) and
the pre-categorized approach by (Cook and Steven-
son, 2009) (59.4%). Regarding different feature cat-
egories, we found the character-level features are
strong indicators, and using phonetic- and syllabic-
level features also slightly benefits the performance.
4 Conclusion
In this paper, we proposed a generic letter trans-
formation approach for text message normaliza-
tion without pre-categorizing the nonstandard to-
kens into insertion, deletion, substitution, etc. We
also avoided the expensive and time consuming hand
labeling process by automatically collecting a large
set of noisy training pairs. Results in the Twitter
and SMS domains show that our system can signif-
icantly outperform the state-of-the-art systems and
have good domain portability. In the future, we
would like to compare our method with a statistical
machine translation approach performed at the let-
ter level, evaluate the system using sentences by in-
corporating context word information, and consider
many-to-one letter transformation in the model.
75
5 Acknowledgments
The authors thank Deana Pennell for sharing the
look-up table generated using the deletion-based ab-
breviation approach. Thank Sittichai Jiampojamarn
for providing the many-to-many letter-phoneme
alignment data sets and toolkit. Part of this work
was done while Fei Liu was working as a research
intern in Bosch Research and Technology Center.
References
AiTi Aw, Min Zhang, Juan Xiao, and Jian Su. 2006. A
phrase-based statistical model for sms text normaliza-
tion. In Proceedings of the COLING/ACL, pages 33?
40.
Richard Beaufort, Sophie Roekhaut, Louise-Ame?lie
Cougnon, and Ce?drick Fairon. 2010. A hybrid
rule/model-based finite-state framework for normaliz-
ing sms messages. In Proceedings of the ACL, pages
770?779.
William B. Cavnar and John M. Trenkle. 1994. N-gram-
based text categorization. In Proceedings of Third An-
nual Symposium on Document Analysis and Informa-
tion Retrieval, pages 161?175.
Asli Celikyilmaz, Dilek Hakkani-Tur, and Junlan Feng.
2010. Probabilistic model-based sentiment analysis of
twitter messages. In Proceedings of the IEEE Work-
shop on Spoken Language Technology, pages 79?84.
Monojit Choudhury, Rahul Saraf, Vijit Jain, Animesh
Mukherjee, Sudeshna Sarkar, and Anupam Basu.
2007. Investigation and modeling of the structure of
texting language. International Journal on Document
Analysis and Recognition, 10(3):157?174.
Paul Cook and Suzanne Stevenson. 2009. An unsuper-
vised model for text messages normalization. In Pro-
ceedings of the NAACL HLT Workshop on Computa-
tional Approaches to Linguistic Creativity, pages 71?
78.
Matthew Hindson. 2006. En-
glish language hyphenation dictionary.
http://www.hindson.com.au/wordpress/2006/11/11/english-
language-hyphenation-dictionary/.
Mindaugas Idzelis. 2005. Jazzy: The java open source
spell checker. http://jazzy.sourceforge.net/.
Sittichai Jiampojamarn, Grzegorz Kondrak, and Tarek
Sherif. 2007. Applying many-to-many alignments
and hidden markov models to letter-to-phoneme con-
version. In Proceedings of the HLT/NAACL, pages
372?379.
Catherine Kobus, Franc?ois Yvon, and Ge?raldine
Damnati. 2008. Normalizing sms: Are two metaphors
better than one? In Proceedings of the COLING, pages
441?448.
Taku Kudo. 2005. CRF++: Yet another CRF took kit.
http://crfpp.sourceforge.net/.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic mod-
els for segmenting and labeling sequence data. In Pro-
ceedings of the ICML, pages 282?289.
Deana L. Pennell and Yang Liu. 2010. Normalization
of text messages for text-to-speech. In Proceedings of
the ICASSP, pages 4842?4845.
Sasa Petrovic, Miles Osborne, and Victor Lavrenko.
2010. The edinburgh twitter corpus. In Proceedings
of the NAACL HLT Workshop on Computational Lin-
guistics in a World of Social Media, pages 25?26.
Richard Sproat, Alan W. Black, Stanley Chen, Shankar
Kumar, Mari Ostendorf, and Christopher Richards.
2001. Normalization of non-standard words. Com-
puter Speech and Language, 15(3):287?333.
Kristina Toutanova and Robert C. Moore. 2002. Pronun-
ciation modeling for improved spelling correction. In
Proceedings of the ACL, pages 144?151.
Dong Yang, Yi cheng Pan, and Sadaoki Furui. 2009.
Automatic chinese abbreviation generation using con-
ditional random field. In Proceedings of the NAACL
HLT, pages 273?276.
76
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 519?523,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Interactive Group Suggesting for Twitter
Zhonghua Qu, Yang Liu
The University of Texas at Dallas
{qzh,yangl}@hlt.utdallas.edu
Abstract
The number of users on Twitter has drasti-
cally increased in the past years. However,
Twitter does not have an effective user group-
ing mechanism. Therefore tweets from other
users can quickly overrun and become in-
convenient to read. In this paper, we pro-
pose methods to help users group the peo-
ple they follow using their provided seeding
users. Two sources of information are used to
build sub-systems: textural information cap-
tured by the tweets sent by users, and social
connections among users. We also propose
a measure of fitness to determine which sub-
system best represents the seed users and use
it for target user ranking. Our experiments
show that our proposed framework works well
and that adaptively choosing the appropriate
sub-system for group suggestion results in in-
creased accuracy.
1 Introduction
Twitter is a well-known social network service that
allows users to post short 140 character status update
which is called ?Tweet?. A twitter user can ?follow?
other users to get their latest updates. Twitter cur-
rently has 19 million active users. These users fol-
lows 80 other users on average. Default Twitter ser-
vice displays ?Tweets? in the order of their times-
tamps. It works well when the number of tweets
the user receives is not very large. However, the
flat timeline becomes tedious to read even for av-
erage users with less than 80 friends. As Twitter
service grows more popular in the past few years,
users? ?following? list starts to consist of Twitter ac-
counts for different purposes. Take an average user
?Bob? for example. Some people he follows are his
?Colleagues?, some are ?Technology Related Peo-
ple?, and others could be ?TV show comedians?.
When Bob wants to read the latest news from his
?Colleagues?, because of lacking effective ways to
group users, he has to scroll through all ?Tweets?
from other users. There have been suggestions from
many Twitter users that a grouping feature could be
very useful. Yet, the only way to create groups is
to create ?lists? of users in Twitter manually by se-
lecting each individual user. This process is tedious
and could be sometimes formidable when a user is
following many people.
In this paper, we propose an interactive group cre-
ating system for Twitter. A user creates a group by
first providing a small number of seeding users, then
the system ranks the friend list according to how
likely a user belongs to the group indicated by the
seeds. We know in the real world, users like to group
their ?follows? in many ways. For example, some
may create groups containing all the ?computer sci-
entists?, others might create groups containing their
real-life friends. A system using ?social informa-
tion? to find friend groups may work well in the lat-
ter case, but might not effectively suggest correct
group members in the former case. On the other
hand, a system using ?textual information? may be
effective in the first case, but is probably weak in
finding friends in the second case. Therefore in
this paper, we propose to use multiple information
sources for group member suggestions, and use a
cross-validation approach to find the best-fit sub-
519
system for the final suggestion. Our results show
that automatic group suggestion is feasible and that
selecting approximate sub-system yields additional
gain than using individual systems.
2 Related Work
There is no previous research on interactive sug-
gestion of friend groups on Twitter to our knowl-
edge; however, some prior work is related and can
help our task. (Roth et al, 2010) uses implicit so-
cial graphs to help suggest email addresses a person
is likely to send to based on the addresses already
entered. Also, using the social network informa-
tion, hidden community detection algorithms such
as (Palla et al, 2005) can help suggest friend groups.
Besides the social information, what a user tweets is
also a good indicator to group users. To character-
ize users? tweeting style, (Ramage et al, 2010) used
semi-supervised topic modeling to map each user?s
tweets into four characteristic dimensions.
3 Interactive Group Creation
Creating groups manually is a tedious process.
However, creating groups in an entirely un-
supervised fashion could result in unwanted results.
In our system, a user first indicates a small number
of users that belong to a group, called ?seeds?, then
the system suggests other users that might belong to
this group. The general structure of the system is
shown in Figure 1.
[ Social Sub-System
??
Textual Sub-System
Sub-System 
Selector
Seed Users
Target Users Ranks
Figure 1: Overview of the system architecture
As mentioned earlier, we use different informa-
tion sources to determine user/group similarity, in-
cluding textual information and social connections.
A module is designed for each information source to
rank users based on their similarity to the provided
seeds. In our approach, the system first tries to detect
what sub-system can best fit the seed group. Then,
the corresponding system is used to generate the fi-
nal ranked list of users according to the likelihood of
belonging to the group.
After the rank list is given, the user can adjust the
size of the group to best fit his/her needs. In addition,
a user can correct the system by specifically indicat-
ing someone as a ?negative seed?, which should not
be on the top of the list. In this paper, we only con-
sider creating one group at a time with only ?positive
seed? and do not consider the relationships between
different groups.
Since determining the best fitting sub-system or
the group type from the seeds needs the use of the
two sub-systems, we describe them first. Each sub-
system takes a group of seed users and unlabeled
target users as the input, and provides a ranked list
of the target users belonging to the group indicated
by the seeds.
3.1 Tweet Based Sub-system
In this sub-system, user groups are modeled using
the textual information contained in their tweets. We
collected all the tweets from a user and grouped
them together.
To represent the tweets information, we could use
a bag-of-word model for each user. However, since
Twitter messages are known to be short and noisy,
it is very likely that traditional natural language pro-
cessing methods will perform poorly. Topic mod-
eling approaches, such as Latent Dirichlet Alloca-
tion (LDA) (Blei et al, 2003), model document as a
mixture of multinomial distribution of words, called
topics. They can reduce the dimension and group
words with similar semantics, and are often more
robust in face of data sparsity or noisy data. Be-
cause tweet messages are very short and hard to infer
topics directly from them, we merge all the tweets
from a user to form a larger document. Then LDA
is applied to the collection of documents from all
the users to derive the topics. Each user?s tweets
can then be represented using a bag-of-topics model,
where the ith component is the proportion of the ith
520
topic appearing in the user?s tweet.
Given a group of seed users, we want to find target
users that are similar to the seeds in terms of their
tweet content. To take multiple seed instances into
consideration, we use two schemes to calculate the
similarity between one target user and a seed group.
? centroid: we calculate the centroid of seeds,
then use the similarity between the centroid and
the target user as the final similarity value.
? average: we calculate the similarity between
the target and each individual seed user, then
take the average as the final similarity value.
In this paper, we explore using two different sim-
ilarity functions between two vectors (ui and vi),
cosine similarity and inverse Euclidean distance,
shown below respectively.
dcosine(u, v) =
1
| u || v |
n?
i=1
ui ? vi (1)
deuclidean(u, v) =
1
??n
i=1(ui ? vi)2
(2)
After calculating similarity for all the target users,
this tweet-based sub-system gives the ranking ac-
cordingly.
3.2 Friend Based Sub-system
As an initial study, we use a simple method to model
friend relationship in user groups. In the future, we
will replace it with other better performing meth-
ods. In this sub-system, we model people using
their social information. In Twitter, social informa-
tion consists of ?following? relation and ?mentions?.
Unlike other social networks like ?Facebook? or
?Myspace?, a ?following? relation in Twitter is di-
rected. In Twitter, a ?mention? happens when some-
one refers to another Twitter user in their tweets.
Usually it happens in replies and retweets. Because
this sub-system models the real-life friend groups,
we only consider bi-directional following relation
between people. That is, we only consider an edge
between users when both of them follow each other.
There are many hidden community detection algo-
rithms that have been proposed for network graphs
(Newman, 2004; Palla et al, 2005). Our task is how-
ever different in that we know the seed of the target
group and the output needs to be a ranking. Here, we
use the count of bi-directional friends and mentions
between a target user and the seed group as the score
for ranking. The intuition is that the social graph be-
tween real life friends tends to be very dense, and
people who belong to the clique should have more
edges to the seeds than others.
3.3 Group Type Detection
The first component in our system is to determine
which sub-system to use to suggest user groups. We
propose to evaluate the fitness of each sub-system
base on the seeds provided using a cross-validation
approach. The assumption is that if a sub-system
(information source used to form the group) is a
good match, then it will rank the users in the seed
group higher than others not in the seed.
The procedure of calculating the fitness score of
each sub-system is shown in Algorithm 1. In the in-
put, S is the seed users (with more than one user),
U is the target users to be ranked, and subrank is
a ranking sub-system (two systems described above,
each taking seed users and target users as input, and
producing the ranking of the target users). This pro-
cedure loops through the seed users. Each time, it
takes one seed user Si out and puts it together with
other target users. Then it calls the sub-system to
rank the new list and finds out the resulting rank for
Si. The final fitness score is the sum of all the ranks
for the seed instances. The system with the highest
score is then selected and used to rank the original
target users.
Algorithm 1 Fitness of a sub-system for a seed
group
proc fitness(S,U, subrank) ?
ranks := ?
for i := 1 to size(S) do
U ? := Si ? U
S? := S \ Si
r := subrank(U ?, S?);
t := rankOf(Si, r);
ranks := ranks ? t; od
fitness := sum(ranks);
print(fitness);
end
4 Data
Our data set is collected from Twitter website using
its Web API. Because twitter does not provide direct
functions to group friends, we use lists created by
521
twitter users as the reference friend group in testing
and evaluation. We exclude users that have less than
20 or more than 150 friends; that do not have a qual-
ified list (more than 20 and less than 200 list mem-
bers); and that do not use English in their tweets.
After applying these filtering criteria, we found 87
lists from 12 users. For these qualified users, their
1, 383 friends information is retrieved, again using
Twitter API. For the friends that are retrieved, their
180, 296 tweets and 584, 339 friend-of-friend infor-
mation are also retrieved. Among all the retrieved
tweets, there are 65, 329 mentions in total.
5 Experiment
In our experiment, we evaluate the performance of
each sub-system and then use group type detection
algorithm to adaptively combine the systems. We
use the Twitter lists we collected as the reference
user groups for evaluation. For each user group, we
randomly take out 6 users from the list and use as
seed candidate. The target user consists of the rest of
the list members and other ?friends? that the list cre-
ator has. From the ranked list for the target users, we
calculate the mean average precision (MAP) score
with the rank position of the list members. For each
group, we run the experiment 10 times using ran-
domly selected seeds. Then the average MAP on all
runs on all groups is reported. In order to evaluate
the effect of the seed size on the final performance,
we vary the number of seeds from 2 to 6 using the 6
taken-out list members.
In the tweet based sub-system, we optimize its hy-
per parameter automatically based on the data. After
trying different numbers of topics in LDA, we found
optimal performance with 50 topics (? = 0.5 and
? = 0.04).
System Seed Size2 3 5 6
Tweet Sub
CosCent 28.45 29.34 29.54 31.18
CosAvg 28.37 29.51 30.01 31.45
EucCent 27.32 28.12 28.97 29.75
EucAvg 27.54 28.74 29.12 29.97
Social Sub 26.45 27.78 28.12 30.21
Adaptive 30.17 32.43 33.01 34.74
BOW baseline 23.45 24.31 24.73 24.93
Random Baseline 17.32
Table 1: Ranking Result (Mean Average Precision) using
Different Systems.
Table 1 shows the performance of each sub-
system as well as the adaptive system. We include
the baseline results generated using random ranking.
As a stronger baseline (BOW baseline), we used co-
sine similarity between users? tweets as the similar-
ity measure. In this baseline, we used a vocabulary
of 5000 words that have the highest TF-IDF values.
Each user?s tweet content is represented using a bag-
of-words vector using this vocabulary. The ranking
of this baseline is calculated using the average simi-
larity with the seeds.
In the tweet-based sub-system, ?Cos? and ?Euc?
mean cosine similarity and inverse Euclidean dis-
tance respectively as the similarity measure. ?Cent?
and ?Avg? mean using centroid vector and average
similarity respectively to measure the similarities
between a target user and the seed group. From the
results, we can see that in general using a larger seed
group improves performance since more informa-
tion can be obtained from the group. The ?CosAvg?
scheme (which uses cosine similarity with average
similarity measure) achieves the best result. Using
cosine similarity measure gives better performance
than inverse Euclidean distance. This is not surpris-
ing since cosine similarity has been widely adopted
as an appropriate similarity measure in the vector
space model for text processing. The bag-of-word
baseline is much better than the random baseline;
however, using LDA topic modeling to collapse the
dimension of features achieves even better results.
This confirms that topic modeling is very useful in
representing noisy data, such as tweets.
In the adaptive system, we also used ?CosAvg?
scheme in the tweet based sub-system. After the au-
tomatic sub-system selection, we observe increased
performance. This indicates that users form lists
based on different factors and thus always using
one single system is not the best solution. It also
demonstrates that our proposed fitness measure us-
ing cross-validation works well, and that the two in-
formation sources used to build sub-systems can ap-
propriately capture the group characteristics.
6 Conclusion
In this paper, we have proposed an interactive group
creation system for Twitter users to organize their
?followings?. The system takes friend seeds pro-
vided by users and generates a ranked list according
522
to the likelihood of a test user being in the group.
We introduced two sub-systems, based on tweet text
and social information respectively. We also pro-
posed a group type detection procedure that is able
to use the most appropriate system for group user
ranking. Our experiments show that by using differ-
ent systems adaptively, better performance can be
achieved compared to using any single system, sug-
gesting this framework works well. In the future, we
plan to add more sophisticated sub-systems in this
framework, and also explore combining ranking out-
puts from different sub-systems. Furthermore, we
will incorporate negative seeds into the process of
interactive suggestion.
References
David M. Blei, Andrew Y. Ng, Michael I. Jordan, and
John Lafferty. 2003. Latent dirichlet alocation. Jour-
nal of Machine Learning Research, 3:2003.
Mark Newman. 2004. Analysis of weighted networks.
Physical Review E, 70(5), November.
Gergely Palla, Imre Derenyi, Illes Farkas, and Tamas Vic-
sek. 2005. Uncovering the overlapping community
structure of complex networks in nature and society.
Nature, 435(7043):814?818, June.
Daniel Ramage, Susan Dumais, and Dan Liebling. 2010.
Characterizing microblogs with topic models. In
ICWSM.
Maayan Roth, Assaf Ben-David, David Deutscher, Guy
Flysher, Ilan Horn, Ari Leichtberg, Naty Leiser, Yossi
Matias, and Ron Merom. 2010. Suggesting friends
using the implicit social graph. In SIGKDD, KDD ?10,
pages 233?242. ACM.
523
11
Automatic Summarization
Ani Nenkova University of Pennsylvania
Sameer Maskey IBM Research
Yang Liu University of Texas at Dallas
2
Why summarize?
23
Text summarization
News articles
Scientific Articles
Emails
Books
Websites
Social Media 
Streams
4
Speech summarization
MeetingPhone Conversation
Classroom
Radio NewsBroadcast News
Talk Shows
Lecture
Chat
35
How to 
summarize
Text & Speech?
-Algorithms
-Issues
-Challenges
-Systems
Tutorial
6
Motivation & Definition
Topic Models
Graph Based Methods
Supervised Techniques
Optimization Methods
Speech Summarization
Evaluation
Manual (Pyramid), Automatic (Rouge, F-Measure)
Fully Automatic
Frequency, Lexical chains, TF*IDF,
Topic Words, Topic Models [LSA, EM, Bayesian]
Features, Discriminative Training
Sampling, Data, Co-training
Iterative, Greedy, Dynamic Programming
ILP, Sub-Modular Selection
Segmentation, ASR
Acoustic Information, Disfluency 
47
Motivation: where does summarization 
help?
 Single document summarization 
 Simulate the work of intelligence analyst
 Judge if a document is relevant to a topic of interest
?Summaries as short as 17% of the full text length speed up 
decision making twice, with no significant degradation in 
accuracy.?
?Query-focused summaries enable users to find more relevant 
documents more accurately, with less need to consult the full text 
of the document.?
[Mani et al, 2002]
8
Motivation: multi-document summarization 
helps in compiling and presenting
 Reduce search time, especially when the goal of the 
user is to find as much information as possible about a 
given topic
 Writing better reports, finding more relevant information, 
quicker
 Cluster similar articles and provide a multi-document 
summary of the similarities
 Single document summary of the information unique to 
an article
[Roussinov and Chen, 2001; Mana-Lopez et al, 2004; McKeown et al, 2005 ]
59
Benefits from speech summarization
 Voicemail
 Shorter time spent on listening (call centers)
 Meetings
 Easier to find main points
 Broadcast News
 Summary of story from mulitiple channels
 Lectures
 Useful for reviewing of course materials
[He et al, 2000; Tucker and Whittaker, 2008; Murray et al, 2009]
10
Assessing summary quality: overview
 Responsiveness
 Assessor directly rate each summary on a scale
 In official evaluations but rarely reported in papers
 Pyramid
 Assessors create model summaries
 Assessors identifies semantic overlap between summary 
and models
 ROUGE
 Assessors create model summaries
 ROUGE automatically computes word overlap
611
Tasks in summarization
Content (sentence) selection
 Extractive summarization
Information ordering
 In what order to present the selected sentences, especially 
in multi-document summarization
Automatic editing, information fusion and compression
 Abstractive summaries
12
Extractive (multi-document) summarization
Input text2Input text1 Input text3
Summary
1. Selection
2. Ordering
3. Fusion
Compute Informativeness
713
Computing informativeness
 Topic models (unsupervised)
 Figure out what the topic of the input
 Frequency, Lexical chains, TF*IDF
 LSA, content models (EM, Bayesian) 
 Select informative sentences based on the topic
 Graph models (unsupervised)
 Sentence centrality
 Supervised approaches
 Ask people which sentences should be in a summary
 Use any imaginable feature to learn to predict human 
choices
14
Motivation & Definition
Topic Models
Graph Based Methods
Supervised Techniques
Global Optimization Methods
Speech Summarization
Evaluation
Frequency, Lexical chains, TF*IDF, 
Topic Words,Topic Models [LSA, EM, Bayesian]
Manual (Pyramid), Automatic (Rouge, F-Measure)
Fully Automatic
Features, Discriminative Training
Sampling, Data, Co-training
Iterative, Greedy, Dynamic Programming
ILP, Sub-Modular Selection
Segmentation, ASR
Acoustic Information, Disfluency 
815
Frequency as document topic proxy
10 incarnations of an intuition
 Simple intuition, look only at the document(s)
 Words that repeatedly appear in the document are likely to 
be related to the topic of the document
 Sentences that repeatedly appear in different input 
documents represent themes in the input
 But what appears in other documents is also helpful 
in determining the topic
 Background corpus probabilities/weights for word 
16
What is an article about?
 Word probability/frequency
 Proposed by Luhn in 1958 [Luhn 1958]
 Frequent content words would be indicative of the 
topic of the article
 In multi-document summarization, words or 
facts repeated in the input are more likely to 
appear in human summaries [Nenkova et al, 2006]
917
Word probability/weights 
Libya
bombing
trail
Gadafhi
suspects
Libya refuses 
to surrender 
two Pan Am 
bombing 
suspects 
Pan Am
INPUT
SUMMARY
WORD PROBABILITY TABLE
Word Probability
pan 0.0798
am 0.0825
libya 0.0096
suspects 0.0341
gadafhi 0.0911
trail 0.0002
?.
usa 0.0007
HOW?
UK and 
USA
18
HOW: Main steps in sentence selection 
according to word probabilities
Step 1 Estimate word weights (probabilities)
Step 2 Estimate sentence weights
Step 3 Choose best sentence
Step 4 Update word weights
Step 5 Go to 2 if desired length not reached
)()( SentwCFSentWeight i ?=
10
19
More specific choices [Vanderwende et al, 2007; Yih et al, 
2007; Haghighi and Vanderwende, 2009]
 Select highest scoring sentence
 Update word probabilities for the selected sentence 
to reduce redundancy
 Repeat until desired summary length
?
?
=
Sw
wp
S
SScore )(||
1)(
pnew (w) = pold (w).pold (w)
20
Is this a reasonable approach: yes, people 
seem to be doing something similar
 Simple test
 Compute word probability table from the input
 Get a batch of summaries written by H(umans) and S(ystems)
 Compute the likelihood of the summaries given the word 
probability table 
 Results
 Human summaries have higher likelihood
HSSSSSSSSSSHSSSHSSHHSHHHHH
HIGH LIKELIHOODLOW
11
21
Obvious shortcomings of the pure 
frequency approaches
 Does not take account of related words
 suspects -- trail
 Gadhafi ? Libya
 Does not take into account evidence from 
other documents
 Function words: prepositions, articles, etc.
 Domain words: ?cell? in cell biology articles
 Does not take into account many other 
aspects
22
Two easy fixes
 Lexical chains [Barzilay and Elhadad, 1999, Silber and McCoy, 
2002, Gurevych and Nahnsen, 2005]
 Exploits existing lexical resources (WordNet)
 TF*IDF weights [most summarizers]
 Incorporates evidence from a background corpus
12
23
Lexical chains and WordNet relations
 Lexical chains
 Word sense disambiguation is performed 
 Then topically related words represent a topic
 Synonyms, hyponyms, hypernyms
 Importance is determined by frequency of the words in a 
topic rather than a single word
 One sentence per topic is selected 
 Concepts based on WordNet [Schiffman et al, 2002, Ye et al, 
2007]
 No word sense disambiguation is performed
 {war, campaign, warfare, effort, cause, operation}
 {concern, carrier, worry, fear, scare}
24
TF*IDF weights for words
Combining evidence for document topics from the 
input and from a background corpus
 Term Frequency (TF)
 Times a word occurs in the input 
 Inverse Document Frequency (IDF)
 Number of documents (df) from a background 
corpus of N documents that contain the word
)/log(* dfNtfIDFTF ?=
13
25
Motivation & Definition
Topic Models
Graph Based Methods
Supervised Techniques
Global Optimization Methods
Speech Summarization
Evaluation
Frequency, TF*IDF, Topic Words
Topic Models [LSA, EM, Bayesian]
Manual (Pyramid), Automatic (Rouge, F-Measure)
Fully Automatic
Features, Discriminative Training
Sampling, Data, Co-training
Iterative, Greedy, Dynamic Programming
ILP, Sub-Modular Selection
Segmentation, ASR
Acoustic Information, Disfluency 
26
Topic words (topic signatures)
 Which words in the input are most descriptive?
 Instead of assigning probabilities or weights to all words, 
divide words into two classes: descriptive or not
 For iterative sentence selection approach, the binary 
distinction is key to the advantage over frequency and 
TF*IDF
 Systems based on topic words have proven to be the most 
successful in official summarization evaluations 
14
27
Example input and associated topic words
 Input for summarization: articles relevant to the 
following user need
Title: Human Toll of Tropical 
Storms Narrative: What has been the human toll in death or injury 
of tropical storms in recent years? Where and when have each of 
the storms caused human casualties? What are the approximate 
total number of casualties attributed to each of the storms?
ahmed, allison, andrew, bahamas, bangladesh, bn, caribbean, carolina, caused, cent, 
coast, coastal, croix, cyclone, damage, destroyed, devastated, disaster, dollars, drowned, 
flood, flooded, flooding, floods, florida, gulf, ham, hit, homeless, homes, hugo, hurricane, 
insurance, insurers, island, islands, lloyd, losses, louisiana, manila, miles, nicaragua, 
north, port, pounds, rain, rains, rebuild, rebuilding, relief, remnants, residents, roared, salt, 
st, storm, storms, supplies, tourists, trees, tropical, typhoon, virgin, volunteers, weather, 
west, winds, yesterday.
Topic Words
28
Formalizing the problem of identifying topic 
words 
 Given
 t: a word that appears in the input
 T: cluster of articles on a given topic (input)
 NT: articles not on topic T (background corpus)
 Decide if t is a topic word or not
 Words that have (almost) the same probability in T 
and NT are not topic words
15
29
Computing probabilities
 View a text as a sequence of Bernoulli trails
 A word is either our term of interest t or not
 The likelihood of observing term t which occurs with 
probability p in a text consisting of N words is given by 
 Estimate the probability of t in three ways
 Input + background corpus combines
 Input only
 Background only
t
30
Testing which hypothesis is more 
likely: log-likelihood ratio test
has a known statistical distribution: chi-square 
At a given significance level, we can decide if a word is 
descriptive of the input or not.
This feature is used in the best performing systems for 
multi-document summarization of news [Lin and Hovy, 
2000; Conroy et al, 2006]
Likelihood of the data given H1
Likelihood of the data given H2
? =
-2 log ?
16
31
Motivation & Definition
Topic Models
Graph Based Methods
Supervised Techniques
Global Optimization Methods
Speech Summarization
Evaluation
Frequency, TF*IDF, Topic Words
Topic Models [LSA, EM, Bayesian]
Manual (Pyramid), Automatic (Rouge, F-Measure)
Fully Automatic
Features, Discriminative Training
Sampling, Data, Co-training
Iterative, Greedy, Dynamic Programming
ILP, Sub-Modular Selection
Segmentation, ASR
Acoustic Information, Disfluency 
32
The background corpus takes more 
central stage
 Learn topics from the background corpus
 topic ~ themes often discusses in the background
 topic representation ~ word probability tables
 Usually one time training step
 To summarize an input 
 Select sentences from the input that correspond 
to the most prominent topics
17
33
Latent semantic analysis (LSA) [Gong and Liu, 
2001, Hachey et al, 2006, Steinberger et al, 2007]
 Discover topics from the background corpus with n unique 
words and d documents
 Represent the background corpus as nxd matrix A
 Rows correspond to words
 Aij=number of times word I appears in document j
 Use standard change of coordinate system and dimensionality 
reduction techniques
 In the new space each row corresponds to the most important 
topics in the corpus
 Select the best sentence to cover each topic
TUPVA =
34
Notes on LSA and other approaches
 The original article that introduced LSA for 
single document summarization of news did 
not find significant difference with TF*IDF
 For multi-document summarization of news 
LSA approaches have not outperformed topic 
words or extensions of frequency approaches
 Other topic/content models have been much 
more influential
18
35
Domain dependent content models
 Get sample documents from the domain
 background corpus
 Cluster sentences from these documents 
 Implicit topics
 Obtain a word probability table for each topic
 Counts only from the cluster representing the 
topic
 Select sentences from the input with highest 
probability for main topics 
36
Text structure can be learnt
 Human-written examples from a domain
Location, time
relief efforts
magnitude
damage
19
37
Topic = cluster of similar sentences from 
the background corpus
 Sentences cluster from earthquake articles
 Topic ?earthquake location?
 The Athens seismological institute said the temblor?s epicenter 
was located 380 kilometers (238 miles) south of the capital.
 Seismologists in Pakistan?s Northwest Frontier Province said the 
temblor?s epicenter was about 250 kilometers (155 miles) north of 
the provincial capital Peshawar.
 The temblor was centered 60 kilometers (35 miles) north- west of 
the provincial capital of Kunming, about 2,200 kilometers (1,300
miles) southwest of Beijing, a bureau seismologist said.
38
Content model [Barzilay and Lee, 2004, Pascale et al, 2003] 
 Hidden Markov Model (HMM)-based 
 States - clusters of related sentences ?topics?
 Transition prob. - sentence precedence in corpus
 Emission prob. - bigram language model
location, 
magnitude casualties
relief efforts
)|()|(),|,( 11111 +++++ ?=><>< iieiitiiii hsphhphshsp
Earthquake reportsTransition from previous 
topic
Generating 
sentence in 
current topic
20
39
Learning the content model
 Many articles from the same domain
 Cluster sentences: each cluster represents a topic from 
the domain
 Word probability tables for each topic
 Transitions between clusters can be computed from 
sentence adjacencies in the original articles  
 Probabilities of going from one topic to another
 Iterate between clustering and transition probability 
estimation to obtain domain model
40
To select a summary
 Find main topics in the domain
 using a small collection of summary-input pairs
 Find the most likely topic for each sentence in 
the input 
 Select the best sentence per main topic
21
41
Historical note
 Some early approaches to multi-document 
summarization relied on clustering the 
sentences in the input alone [McKeown et al, 1999, 
Siddharthan et al, 2004]
 Clusters of similar sentences represent a theme in 
the input
 Clusters with more sentences are more important
 Select one sentence per important cluster
42
Example cluster
Choose one sentence to represent the cluster
1. PAL was devastated by a pilots' strike in June and by the 
region's currency crisis.
2. In June, PAL was embroiled in a crippling three-week 
pilots' strike.
3. Tan wants to retain the 200 pilots because they stood by 
him when the majority of PAL's pilots staged a 
devastating strike in June.
22
43
Bayesian content models
 Takes a batch of inputs for summarization
 Many word probability tables
 One for general English
 One for each of the inputs to be summarized
 One for each document in any input
To select a summary S with L words from 
document collection D given as input
The goal is to select the summary, not a 
sentence. Greedy selection vs. global will 
be discussed in detail later
S* = minS:words(S)?LKL(PD||PS)
44
KL divergence
 Distance between two probability distributions: P, Q
 P, Q: Input and summary word distributions  
KL (P || Q) = pP (w) log2 pP (w)pQ (w)w?
23
45
Intriguing side note
 In the full Bayesian topic models, word 
probabilities for all words is more important 
than binary distinctions of topic and non-topic 
word
 Haghighi and Vanderwende report that a 
system that chooses the summary with 
highest expected number of topic words 
performs as SumBasic
46
Review
 Frequency based informativeness has been 
used in building summarizers
 Topic words probably more useful
 Topic models
 Latent Semantic Analysis
 Domain dependent content model
 Bayesian content model
24
47
Motivation & Definition
Topic Models
Graph Based Methods
Supervised Techniques
Optimization Methods
Speech Summarization
Evaluation
Frequency, TF*IDF, Topic Words
Topic Models [LSA, EM, Bayesian]
Manual (Pyramid), Automatic (Rouge, F-Measure)
Fully Automatic
Features, Discriminative Training
Sampling, Data, Co-training
Iterative, Greedy, Dynamic Programming
ILP, Sub-Modular Selection
Segmentation, ASR
Acoustic Information, Disfluency 
48
Using graph representations [Erkan and Radev, 
2004; Mihalcea and Tarau, 2004; Leskovec et al, 2005 ]
 Nodes
 Sentences
 Discourse entities
 Edges
 Between similar sentences
 Between syntactically related entities
 Computing sentence similarity
 Distance between their TF*IDF weighted vector 
representations
25
49
50
Sentence :
Iraqi vice president?
Sentence :
Ivanov contended?
Sim(d1s1, d3s2)
26
51
Advantages of the graph model
 Combines word frequency and sentence 
clustering
 Gives a formal model for computing 
importance: random walks
 Normalize weights of edges to sum to 1
 They now represent probabilities of transitioning 
from one node to another
52
Random walks for summarization
 Represent the input text as graph
 Start traversing from node to node 
 following the transition probabilities 
 occasionally hopping to a new node
 What is the probability that you are in any 
particular node after doing this process for a 
certain time? 
 Standard solution (stationary distribution)
 This probability is the weight of the sentence
27
53
Motivation & Definition
Topic Models
Graph Based Methods
Supervised Techniques
Global Optimization Methods
Speech Summarization
Evaluation
Frequency, TF*IDF, Topic Words
Topic Models [LSA, EM, Bayesian]
Manual (Pyramid), Automatic (Rouge, F-Measure)
Fully Automatic
Features, Discriminative Training
Sampling, Data, Co-training
Iterative, Greedy, Dynamic Programming
ILP, Sub-Modular Selection
Segmentation, ASR
Acoustic Information, Disfluency 
54
Supervised methods 
 For extractive summarization, the task can be 
represented as binary classification
 A sentence is in the summary or not
 Use statistical classifiers to determine the score of a 
sentence: how likely it?s included in the summary
 Feature representation for each sentence
 Classification models trained from annotated data
 Select the sentences with highest scores (greedy for 
now, see other selection methods later)
28
55
Features
 Sentence length
 long sentences tend to be more important
 Sentence weight
 cosine similarity with documents
 sum of term weights for all words in a sentence
 calculate term weight after applying LSA
56
Features
 Sentence position
 beginning is often more important
 some sections are more important (e.g., in 
conclusion section)
 Cue words/phrases 
 frequent n-grams
 cue phrases (e.g., in summary, as a conclusion)
 named entities
29
57
Features
 Contextual features
 features from context sentences
 difference of a sentence and its neighboring ones 
 Speech related features (more later):
 acoustic/prosodic features
 speaker information (who said the sentence, is the 
speaker dominant?)
 speech recognition confidence measure 
58
Classifiers
 Can classify each sentence individually, or 
use sequence modeling
 Maximum entropy [Osborne, 2002]
 Condition random fields (CRF) [Galley, 2006]
 Classic Bayesian Method [Kupiec et al, 1995]
 HMM [Conroy and O'Leary, 2001; Maskey, 2006 ]
 Bayesian networks 
 SVMs [Xie and Liu, 2010]
 Regression [Murray et al, 2005]
 Others
30
59
So that is it with supervised methods? 
 It seems it is a straightforward classification 
problem
 What are the issues with this method?
 How to get good quality labeled training data
 How to improve learning
 Some recent research has explored a few 
directions
 Discriminative training, regression, sampling, co-
training, active learning
60
Motivation & Definition
Topic Models
Graph Based Methods
Supervised Techniques
Global Optimization Methods
Speech Summarization
Evaluation
Frequency, TF*IDF, Topic Words
Topic Models [LSA, EM, Bayesian]
Manual (Pyramid), Automatic (Rouge, F-Measure)
Fully Automatic
Features, Discriminative Training
Sampling, Data, Co-training
Iterative, Greedy, Dynamic Programming
ILP, Sub-Modular Selection
Segmentation, ASR
Acoustic Information, Disfluency 
31
61
Improving supervised methods: different 
training approaches
 What are the problems with standard training 
methods?
 Classifiers learn to determine a sentence?s label 
(in summary or not)   
 Sentence-level accuracy is different from 
summarization evaluation criterion (e.g., 
summary-level ROUGE scores)
 Training criterion is not optimal
 Sentences? labels used in training may be too 
strict (binary classes)
62
Improving supervised methods: MERT 
discriminative training
 Discriminative training based on MERT [Aker et 
al., 2010]
 In training, generate multiple summary candidates 
(using A* search algorithm)
 Adjust model parameters (feature weights) 
iteratively to optimize ROUGE scores
Note: MERT has been used for machine translation discriminative training
32
63
Improving supervised methods: ranking 
approaches 
 Ranking approaches [Lin et al 2010]
 Pair-wise training
 Not classify each sentence individually
 Input to learner is a pair of sentences
 Use Rank SVM to learn the order of two sentences
 Direct optimization
 Learns how to correctly order/rank summary candidates 
(a set of sentences)
 Use AdaRank [Xu and Li 2007] to combine weak rankers
64
Improving supervised methods: regression 
model
 Use regression model [Xie and Liu, 2010]
 In training, a sentence?s label is not +1 and -1
 Each one is labeled with numerical values to 
represent their importance
 Keep +1 for summary sentence
 For non-summary sentences (-1), use their similarity to 
the summary as labels
 Train a regression model to better discriminate 
sentence candidates
33
65
Improving supervised methods: sampling
 Problems -- in binary classification setup for 
summarization, the two classes are 
imbalanced
 Summary sentences are minority class. 
 Imbalanced data can hurt classifier training
 How can we address this?
 Sampling to make distribution more balanced to 
train classifiers
 Has been studied a lot in machine learning
66
Improving supervised methods: sampling
 Upsampling: increase minority samples
 Replicate existing minority samples
 Generate synthetic examples (e.g., by some kind 
of interpolation)
 Downsampling: reduce majority samples
 Often randomly select from existing majority 
samples
34
67
Improving supervised methods: sampling
 Sampling for summarization [Xie and Liu, 2010]
 Different from traditional upsampling and downsampling
 Upsampling
 select non-summary sentences that are like summary 
sentences based on cosine similarity or ROUGE scores
 change their label to positive 
 Downsampling: 
 select those that are different from summary sentences
 These also address some human annotation disagreement
 The instances whose labels are changed are often the ones 
that humans have problems with
68
Motivation & Definition
Topic Models
Graph Based Methods
Supervised Techniques
Global Optimization Methods
Speech Summarization
Evaluation
Frequency, TF*IDF, Topic Words
Topic Models [LSA, EM, Bayesian]
Manual (Pyramid), Automatic (Rouge, F-Measure)
Fully Automatic
Features, Discriminative Training
Sampling, Data, Co-raining
Iterative, Greedy, Dynamic Programming
ILP, Sub-Modular Selection
Segmentation, ASR
Acoustic Information, Disfluency 
35
69
Supervised methods: data issues
 Need labeled data for model training
 How do we get good quality training data? 
 Can ask human annotators to select extractive 
summary sentences
 However, human agreement is generally low
 What if data is not labeled at all? or it only 
has abstractive summary?
70
 Distributions of content units and words are similar
 Few units are expressed by everyone; many units 
are expressed by only one person
Do humans agree on summary sentence 
selection? Human agreement on word/sentence/fact selection
36
71
Supervised methods: semi-supervised 
learning
 Question ? can we use unlabeled data to 
help supervised methods? 
 A lot of research has been done on semi-
supervised learning for various tasks
 Co-training and active learning have been 
used in summarization
72
Co-training
 Use co-training to leverage unlabeled data
 Feature sets represent different views
 They are conditionally independent given the 
class label
 Each is sufficient for learning
 Select instances based on one view, to help the 
other classifier
37
73
Co-training in summarization
 In text summarization [Wong et al, 2008]
 Two classifiers (SVM, na?ve Bayes) are used on 
the same feature set
 In speech summarization [Xie et al, 2010]
 Two different views: acoustic and lexical features
 They use both sentence and document as 
selection units
74
Active learning in summarization
 Select samples for humans to label
 Typically hard samples, machines are not 
confident, informative ones
 Active learning in lecture summarization [Zhang 
et al 2009]
 Criterion: similarity scores between the extracted 
summary sentences and the sentences in the 
lecture slides are high
38
75
Supervised methods: using labeled 
abstractive summaries
 Question -- what if I only have abstractive 
summaries, but not extractive summaries? 
 No labeled sentences to use for classifier 
training in extractive summarization 
 Can use reference abstract summary to 
automatically create labels for sentences
 Use similarity of a sentence to the human written 
abstract (or ROUGE scores, other metrics)
76
Comment on supervised performance
 Easier to incorporate more information
 At the cost of requiring a large set of human 
annotated training data
 Human agreement is low, therefore labeled 
training data is noisy
 Need matched training/test conditions
 may not easily generalize to different domains
 Effective features vary for different domains
 e.g., position is important for news articles
39
77
Comments on supervised performance
 Seems supervised methods are more 
successful in speech summarization than in 
text
 Speech summarization is almost never multi-
document
 There are fewer indications about the topic of the 
input in speech domains
 Text analysis techniques used in speech 
summarization are relatively simpler 
78
Motivation & Definition
Topic Models
Graph Based Methods
Supervised Techniques
Optimization Methods
Speech Summarization
Evaluation
Frequency, TF*IDF, Topic Words
Topic Models [LSA, EM, Bayesian]
Manual (Pyramid), Automatic (Rouge, F-Measure)
Fully Automatic
Features, Discriminative Training
Sampling, Data, Co-training
Iterative, Greedy, Dynamic Programming
ILP, Sub-Modular Selection
Segmentation, ASR
Acoustic Information, Disfluency 
40
79
Parameters to optimize
 In summarization methods we try to find 
1. Most significant sentences
2. Remove redundant ones
3. Keep the summary under given length
 Can we combine all 3 steps in one?
 Optimize all 3 parameters at once
80
Summarization as an optimization problem
 Knapsack Optimization Problem 
Select boxes such that amount of money is 
maximized while keeping total weight under X Kg
 Summarization Problem 
Select sentences such that summary relevance is 
maximized while keeping total length under X words
 Many other similar optimization problems  
 General Idea: Maximize a function given a set of 
constraints
41
81
Optimization methods for summarization
 Different flavors of solutions
 Greedy Algorithm
 Choose highest valued boxes
 Choose the most relevant sentence 
 Dynamic Programming algorithm
 Save intermediate computations
 Look at both relevance and length
 Integer Linear Programming
 Exact Inference
 Scaling Issues
We will now discuss these 3 types of optimization solutions
82
Motivation & Definition
Topic Models
Graph Based Methods
Supervised Techniques
Optimization Methods
Speech Summarization
Evaluation
Frequency, TF*IDF, Topic Words
Topic Models [LSA, EM, Bayesian]
Manual (Pyramid), Automatic (Rouge, F-Measure)
Fully Automatic
Features, Discriminative Training
Sampling, Data, Co-training
Greedy, Dynamic Programming
ILP, Sub-Modular Selection
Segmentation, ASR
Acoustic Information, Disfluency 
42
83
Greedy optimization algorithms
 Greedy solution is an approximate algorithm which 
may not be optimal
 Choose the most relevant + least redundant 
sentence if the total length does not exceed the 
summary length
 Maximal Marginal Relevance is one such greedy algorithm 
proposed by [Carbonell et al, 1998]
84
Maximal Marginal Relevance (MMR) 
[Carbonell et al, 1998]
 Summary: relevant and non-redundant information
 Many summaries are built based on sentences ranked by 
relevance
 E.g. Extract most relevant 30% of sentences
Relevance Redundancyvs.
 Summary should maximize relevant information as 
well as reduce redundancy
43
85
Marginal relevance
 ?Marginal Relevance? or ?Relevant Novelty?
 Measure relevance and novelty separately
 Linearly combine these two measures
 High Marginal relevance if
 Sentence is relevant to story (significant information)
 Contains minimal similarity to previously selected sentences 
(new novel information)
 Maximize Marginal Relevance to get summary that 
has significant non-redundant information
86
Relevance with query or centroid
 We can compute relevance of text snippet 
with respect to query or centroid
 Centroid as defined in [Radev, 2004]
 based on the content words of  a document 
 TF*IDF vector of all documents in corpus
 Select words above a threshold : remaining vector 
is a centroid vector
44
87
Maximal Marginal Relevance (MMR) 
[Carbonell et al, 1998]
 Q ? document centroid/user query
 D ? document collection
 R ? ranked listed
 S ? subset of documents in R already selected
 Sim ? similarity metric 
 Lambda =1 produces most significant ranked list
 Lambda = 0 produces most diverse ranked list
MMR? Argmax(Di?R?S)[?(Sim1(Di, Q))?(1??)max(Dj?S)Sim2(Di, Dj)]
88
MMR based Summarization [Zechner, 2000]
Iteratively select next sentence
Next Sentence = 
Frequency Vector 
of all content words
centroid
45
89
MMR based summarization
 Why this iterative sentence selection process 
works?
 1st Term: Find relevant sentences similar to 
centroid of the document
 2nd Term: Find redundancy ? sentences that are 
similar to already selected sentences are not 
selected
90
 MMR is an iterative sentence selection 
process
 decision made for each sentence
 Is this selected sentence globally optimal?
Sentence selection in MMR
Sentence with same level of relevance but shorter may not be 
selected if a longer relevant sentence is already selected
46
91
Motivation & Definition
Topic Models
Graph Based Methods
Supervised Techniques
Optimization Methods
Speech Summarization
Evaluation
Frequency, TF*IDF, Topic Words
Topic Models [LSA, EM, Bayesian]
Manual (Pyramid), Automatic (Rouge, F-Measure)
Fully Automatic
Features, Discriminative Training
Sampling, Data, Co-training
Iterative, Greedy, Dynamic Programming
ILP, Sub-Modular Selection
Segmentation, ASR
Acoustic Information, Disfluency 
92
Global inference
D=t1, t2, , tn?1, tn
 Modify our greedy algorithm 
 add constraints for sentence length as well
 Let us define document D with tn textual 
units
47
93
Global inference
 Let us define
Relevance of ti to be in the 
summary
Redundancy between ti and tj
Length of til(i)
Red(i,j)
Rel(i)
94
Inference problem [McDonald, 2007]
 Let us define inference problem as 
Summary Score
Pairwise RedundancyMaximum Length
48
95
Greedy solution [McDonald, 2007]
Sort by Relevance
Select Sentence
 Sorted list may have longer sentences at the top
 Solve it using dynamic programming
 Create table and fill it based on length and redundancy 
requirements
No consideration of
sentence length
96
Dynamic programming solution [McDonald, 2007]
High scoring summary
of length k and i-1
text unitsHigh scoring 
summary of
length k-l(i) +
ti
Higher ?
49
97
 Better than the previously shown greedy 
algorithm
 Maximizes the space utilization by not 
inserting longer sentences
 These are still approximate algorithms: 
performance loss?
Dynamic programming algorithm [McDonald, 2007]
98
Inference algorithms comparison
[McDonald, 2007]
System 50 100 200
Baseline 26.6/5.3 33.0/6.8 39.4/9.6
Greedy 26.8/5.1 33.5/6.9 40.1/9.5
Dynamic Program 27.9/5.9 34.8/7.3 41.2/10.0
Summarization results: Rouge-1/Rouge-2
Sentence Length
50
99
Motivation & Definition
Topic Models
Graph Based Methods
Supervised Techniques
Optimization Methods
Speech Summarization
Evaluation
Frequency, TF*IDF, Topic Words
Topic Models [LSA, EM, Bayesian]
Manual (Pyramid), Automatic (Rouge, F-Measure)
Fully Automatic
Features, Discriminative Training
Sampling, Data, Co-training
Iterative, Greedy, Dynamic Programming
ILP, Sub-Modular Selection
Segmentation, ASR
Acoustic Information, Disfluency 
100
Integer Linear Programming (ILP) [Gillick 
and Favre, 2009; Gillick et al, 2009; McDonald, 2007]
 Greedy algorithm is an approximate solution
 Use exact solution algorithm with ILP (scaling issues 
though)
 ILP is constrained optimization problem
 Cost and constraints are linear in a set of integer variables
 Many solvers on the web
 Define the constraints based on relevance and 
redundancy for summarization
 Sentence based ILP
 N-gram based ILP
51
101
Sentence-level ILP formulation [McDonald, 
2007]
1 if ti in summary
Constraints
Optimization Function
102
N-gram ILP formulation [Gillick and Favre, 2009; 
Gillick et al, 2009]
 Sentence-ILP constraint on redundancy is 
based on sentence pairs
 Improve by modeling n-gram-level 
redundancy
 Redundancy implicitly defined
Ci indicates presence
of n-gram i in summary 
and its weight is wi
?
i wici
52
103
N-gram ILP formulation [Gillick and Favre, 2009]
Constraints
Optimization Function n-gram level ILP has different  optimization 
function than one shown before
104
Sentence vs. n-gram ILP
System ROUGE-2 Pyramid
Baseline 0.058 0.186
Sentence ILP
[McDonald, 2007] 0.072 0.295
N-gram ILP
[Gillick and Favre, 2009] 0.110 0.345
53
105
Other optimization based summarization 
algorithms
 Submodular selection [Lin et al, 2009]
 Submodular set functions for optimization
 Modified greedy algorithm [Filatova, 2004]
 Event based features
 Stack decoding algorithm [Yih et al, 2007]
 Multiple stacks, each stack represents hypothesis of different 
length
 A* Search [Aker et al, 2010]
 Use scoring and heuristic functions
106
Submodular selection for summarization 
[Lin et al, 2009]
 Summarization Setup
 V ? set of all sentences in document
 S ? set of extraction sentences
 f(.) scores the quality of the summary
 Submodularity been used in solving many 
optimization problems in near polynomial time
 For summarization: 
Select subset S (sentences) representative of V 
given the constraint |S| =< K (budget)
54
107
Submodular selection [Lin et al, 2009]
 If V are nodes in a Graph G=(V,E) representing 
sentences
 And E represents edges (i,j) such that w(i,j) 
represents similarity between sentences i and j
 Introduce submodular set functions which measures 
?representative? S of entire set V
 [Lin et al, 2009] presented 4 submodular set functions
108
Submodular selection for summarization 
[Lin et al, 2009]
Comparison of results using different methods
55
109
Review: optimization methods
 Global optimization methods have shown to be 
superior than 2-step selection process and reduce 
redundancy
 3 parameters are optimized together
 Relevance
 Redundancy
 Length
 Various Algorithms for Global Inference
 Greedy
 Dynamic Programming 
 Integer Linear Programming
 Submodular Selection
110
Motivation & Definition
Topic Models
Graph Based Methods
Supervised Techniques
Optimization Methods
Speech Summarization
Evaluation
Frequency, TF*IDF, Topic Words
Topic Models [LSA, EM, Bayesian]
Manual (Pyramid), Automatic (Rouge, F-Measure)
Fully Automatic
Features, Discriminative Training
Sampling, Data, Co-training
Iterative, Greedy, Dynamic Programming
ILP, Sub-Modular Selection
Segmentation, ASR
Acoustic Information, Disfluency 
56
111
Speech summarization
 Increasing amount of data available in 
speech form
 meetings, lectures, broadcast, youtube, voicemail
 Browsing is not as easy as for text domains
 users need to listen to the entire audio
 Summarization can help effective information 
access
 Summary output can be in the format of text 
or speech
112
Domains 
 Broadcast news
 Lectures/presentations
 Multiparty meetings
 Telephone conversations
 Voicemails
57
113
Example
Meeting transcripts and summary sentences (in red)
so it?s possible that we could do something like a 
summary node of some sort that
me003
but there is some technology you could try to applyme010
yeahme010
now I don?t know that any of these actually apply in 
this case
me010
uh so if you co- you could ima- and i-me010
mmmme003
there?re ways to uh sort of back off on the purity of 
your bayes-net-edness
me010
andme010
uh i- i slipped a paper to bhaskara and about noisy-
or?s and noisy-maxes
me010
which is there are technical ways of doing itme010
uh let me just mention something that i don?t want 
to pursue today
me010
there there are a variety of ways of doing itme010
Broadcast news transcripts and summary (in red)
try to use electrical appliances before p.m. and after p.m. and 
turn off computers, copiers and lights when they're not being 
used
set your thermostat at 68 degrees when you're home, 55 
degrees when  you're away
energy officials are offering tips to conserve electricity, they say, 
to delay holiday lighting until after at night
the area shares power across many states
meanwhile, a cold snap in the pacific northwest is putting an 
added strain on power supplies
coupled with another unit, it can provide enough power for about
2 million people
it had been shut down for maintenance
a unit at diablo canyon nuclear plant is expected to resume 
production today
california's strained power grid is getting a boost today which 
might help increasingly taxed power supplies
114
Speech vs. text summarization: similarities
 When high quality transcripts are available
 Not much different from text summarization
 Many similar approaches have been used
 Some also incorporate acoustic information
 For genres like broadcast news, style is also 
similar to text domains
58
115
Speech vs. text summarization: differences
 Challenges in speech summarization
 Speech recognition errors can be very high
 Sentences are not as well formed as in most text 
domains: disfluencies, ungrammatical
 There are not clearly defined sentences
 Information density is also low (off-topic 
discussions, chit chat, etc.)
 Multiple participants
116
Motivation & Definition
Topic Models
Graph Based Methods
Supervised Techniques
Optimization Methods
Speech Summarization
Evaluation
Frequency, TF*IDF, Topic Words
Topic Models [LSA, EM, Bayesian]
Manual (Pyramid), Automatic (Rouge, F-Measure)
Fully Automatic
Features, Discriminative Training
Sampling, Data, Co-training
Iterative, Greedy, Dynamic Programming
ILP, Sub-Modular Selection
Segmentation, ASR
Acoustic Information, Disfluency 
59
117
What should be extraction units in speech 
summarization?
 Text domain
 Typically use sentences (based on punctuation 
marks)
 Speech domain
 Sentence information is not available
 Sentences are not as clearly defined
Utterance from previous example:
there there are a variety of ways of doing it uh let me just mention something 
that i don?t want to pursue today which is there are technical ways of doing it
118
Automatic sentence segmentation (side note) 
 For a word boundary, determine whether it?s a sentence 
boundary
 Different approaches: 
 Generative: HMM
 Discriminative: SVM, boosting, maxent, CRF
 Information used: word n-gram, part-of-speech, parsing 
information, acoustic info (pause, pitch, energy)
60
119
What is the effect of different 
units/segmentation on summarization?
 Research has used different units in speech 
summarization
 Human annotated sentences or dialog acts
 Automatic sentence segmentation
 Pause-based segments
 Adjacency pairs
 Intonational phrases 
 Words
120
What is the effect of different 
units/segmentation on summarization?
 Findings from previous studies
 Using intonational phrases (IP) is better than 
automatic sentence segmentation, pause-based 
segmentation [Maskey, 2008 ]
 IPs are generally smaller than sentences, also 
linguistically meaningful
 Using sentences is better than words, between 
filler segments [Furui et al, 2004]
 Using human annotated dialog acts is better than 
automatically generated ones [Liu and Xie, 2008]
61
121
Motivation & Definition
Topic Models
Graph Based Methods
Supervised Techniques
Optimization Methods
Speech Summarization
Evaluation
Frequency, TF*IDF, Topic Words
Topic Models [LSA, EM, Bayesian]
Manual (Pyramid), Automatic (Rouge, F-Measure)
Fully Automatic
Features, Discriminative Training
Sampling, Data, Co-training
Iterative, Greedy, Dynamic Programming
ILP, Sub-Modular Selection
Segmentation, ASR
Acoustic Information, Disfluency 
122
Using acoustic information in 
summarization
 Acoustic/prosodic features: 
 F0 (max, min, mean, median, range)
 Energy (max, min, mean, median, range)
 Sentence duration
 Speaking rate (# of words or letters)
 Need proper normalization
 Widely used in supervised methods, in 
combination with textual features
62
123
Using acoustic information in 
summarization
 Are acoustic features useful when combining 
it with lexical information?
 Results vary depending on the tasks and 
domains 
 Often lexical features are ranked higher
 But acoustic features also contribute to overall 
system performance
 Some studies showed little impact when adding 
speech information to textual features [Penn and Zhu, 
2008]
124
Using acoustic information in 
summarization
 Can we use acoustic information only for speech 
summarization?
 Transcripts may not be available
 Another way to investigate contribution of acoustic 
information
 Studies showed using just acoustic information can 
achieve similar performance to using lexical 
information [Maskey and Hirschberg, 2005; Xie et al, 2009; Zhu et al, 
2009]
 Caveat: in some experiments, lexical information is used 
(e.g., define the summarization units)
63
125
Speech recognition errors
 ASR is not perfect, often high word error rate
 10-20% for read speech
 40% or even higher for conversational speech
 Recognition errors generally have negative 
impact on summarization performance
 Important topic indicative words are incorrectly 
recognized
 Can affect term weighting and sentence scores
126
Speech recognition errors
 Some studies evaluated effect of recognition 
errors on summarization by varying word 
error rate [Christensen et al, 2003; Penn and Zhu, 2008; Lin et al, 
2009]
 Degradation is not much when word error 
rate is not too low (similar to spoken 
document retrieval)
 Reason: better recognition accuracy in summary 
sentences than overall  
64
127
What can we do about ASR errors? 
 Deliver summary using original speech 
 Can avoid showing recognition errors in the 
delivered text summary
 But still need to correctly identify summary 
sentences/segments
 Use recognition confidence measure and 
multiple candidates to help better summarize
128
Address problems due to ASR errors
 Re-define summarization task: select 
sentences that are most informative, at the 
same time have high recognition accuracy
 Important words tend to have high recognition 
accuracy
 Use ASR confidence measure or n-gram 
language model scores in summarization
 Unsupervised methods [Zechner, 2002; Kikuchi et al, 2003; 
Maskey, 2008]
 Use as a feature in supervised methods
65
129
Address problems due to ASR errors
 Use multiple recognition candidates
 n-best lists [Liu et al, 2010]
 Lattices [Lin et al, 2010]
 Confusion network [Xie and Liu, 2010]
 Use in MMR framework
 Summarization segment/unit contains all the word 
candidates (or pruned ones based on probabilities)
 Term weights (TF, IDF) use candidate?s posteriors
 Improved performance over using 1-best recognition 
output
130
Motivation & Definition
Topic Models
Graph Based Methods
Supervised Techniques
Optimization Methods
Speech Summarization
Evaluation
Frequency, TF*IDF, Topic Words
Topic Models [LSA, EM, Bayesian]
Manual (Pyramid), Automatic (Rouge, F-Measure)
Fully Automatic
Features, Discriminative Training
Sampling, Data, Co-training
Iterative, Greedy, Dynamic Programming
ILP, Sub-Modular Selection
Segmentation, ASR
Acoustic Information, Disfluency
66
131
Disfluencies and summarization
 Disfluencies (filler words, repetitions, revisions, 
restart, etc) are frequent in conversational speech
 Example from meeting transcript:
so so does i- just remind me of what what you were going to do with the 
what what what what's
y- you just described what you've been doing
 Existence may hurt summarization systems, also 
affect human readability of the summaries
132
Disfluencies and summarization
 Natural thought: remove disfluenices 
 Word-based selection can avoid disfluent 
words 
 Using n-gram scores tends to select fluent 
parts [Hori and Furui, 2001]
 Remove disfluencies first, then perform 
summarization 
 Does it work? not consistent results 
 Small improvement [Maskey, 2008; Zechner, 2002]
 No improvement [Liu et al, 2007]
67
133
Disfluencies and summarization
 In supervised classification, information related to 
disfluencies can be used as features for 
summarization 
 Small improvement on Switchboard data [Zhu and Penn, 2006]
 Going beyond disfluency removal, can perform 
sentence compression in conversational speech to 
remove un-necessary words [Liu and Liu, 2010]
 Help improve sentence readability
 Output is more like abstractive summaries
 Compression helps summarization
134
Review on speech summarization
 Speech summarization has been performed 
for different domains
 A lot of text-based approaches have been 
adopted
 Some speech specific issues have been 
investigated
 Segmentation 
 ASR errors
 Disfluencies
 Use acoustic information
68
135
Motivation & Definition
Topic Models
Graph Based Methods
Supervised Techniques
Global Optimization Methods
Speech Summarization
Evaluation
Frequency, TF*IDF, Topic Words
Topic Models [LSA, EM, Bayesian]
Manual (Pyramid), Automatic (Rouge, F-Measure)
Fully Automatic
Features, Discriminative Training
Sampling, Data, Co-training
Iterative, Greedy, Dynamic Programming
ILP, Sub-Modular Selection
Segmentation, ASR
Acoustic Information, Disfluency 
136
Manual evaluations
 Task-based evaluations 
 too expensive
 Bad decisions possible, hard to fix
 Assessors rate summaries on a scale
 Responsiveness
 Assessors compare with gold-standards
 Pyramid
69
137
Automatic and fully automatic 
evaluation
 Automatically compare with gold-standard
 Precision/recall (sentence level)
 ROUGE (word level)
 No human gold-standard is used
 Automatically compare input and summary
138
Precision and recall for extractive 
summaries
 Ask a person to select the most important 
sentences
Recall: system-human choice 
overlap/sentences chosen by human
Precision: system-human choice 
overlap/sentences chosen by system
70
139
Problems?
 Different people choose different sentences
 The same summary can obtain a recall score 
that is between 25% and 50% different 
depending on which of two available human 
extracts is used for evaluation
 Recall more important/informative than 
precision?
140
More problems?
 Granularity
We need help. Fires have spread in the nearby 
forest and threaten several villages in this remote 
area.
 Semantic equivalence
 Especially in multi-document summarization
 Two sentences convey almost the same 
information: only one will be chosen in the human 
summary
71
141
Pyramid
Responsiveness
ROUGE
Fully automatic
Model 
summaries
Manual comparison/ 
ratings
Evaluation methods for content
142
Pyramid method [Nenkova and Passonneau, 2004; Nenkova et al, 
2007]
 Based on Semantic Content Units (SCU)
 Emerge from the analysis of several texts
 Link different surface realizations with the 
same meaning
72
143
SCU example
S1 Pinochet arrested in London on Oct 16 at a 
Spanish judge?s request for atrocities against 
Spaniards in Chile.
S2 Former Chilean dictator Augusto Pinochet has 
been arrested in London at the request of the 
Spanish government.
S3 Britain caused international controversy and 
Chilean turmoil by arresting former Chilean 
dictator Pinochet in London.
144
SCU: label, weight, contributors 
Label London was where Pinochet was 
arrested
Weight=3
S1 Pinochet arrested in London on Oct 16 at a Spanish 
judge?s request for atrocities against Spaniards in Chile.
S2 Former Chilean dictator Augusto Pinochet has been
arrested in London at the request of the Spanish
government.
S3 Britain caused international controversy and Chilean 
turmoil by arresting former Chilean dictator Pinochet in 
London.
73
145
Ideally informative summary
 Does not include an SCU from a lower tier 
unless all SCUs from higher tiers are 
included as well
146
Ideally informative summary
 Does not include an SCU from a lower tier 
unless all SCUs from higher tiers are 
included as well
74
147
Ideally informative summary
 Does not include an SCU from a lower tier 
unless all SCUs from higher tiers are 
included as well
148
Ideally informative summary
 Does not include an SCU from a lower tier 
unless all SCUs from higher tiers are 
included as well
75
149
Ideally informative summary
 Does not include an SCU from a lower tier 
unless all SCUs from higher tiers are 
included as well
150
Ideally informative summary
 Does not include an SCU from a lower tier 
unless all SCUs from higher tiers are 
included as well
76
151
Different equally good summaries
 Pinochet arrested
 Arrest in London
 Pinochet is a former 
Chilean dictator
 Accused of atrocities 
against Spaniards
152
Different equally good summaries
 Pinochet arrested
 Arrest in London
 On Spanish warrant
 Chile protests
77
153
Diagnostic ? why is a summary bad?
 Good  Less relevant 
summary
154
Importance of content 
 Can observe distribution in human 
summaries
 Assign relative importance
 Empirical rather than subjective
 The more people agree, the more important
78
155
Pyramid score for evaluation
 New summary with n content units
 Estimates the percentage of information that is 
maximally important
IdealWight
ightObservedWe
Ideal
Weight
n
i
i
n
i
i
=
?
?
=
=
1
1
156
ROUGE [Lin, 2004]
 De facto standard for evaluation in text 
summarization
 High correlation with manual evaluations in that 
domain
 More problematic for some other domains, 
particularly speech
 Not highly correlated with manual evaluations
 May fail to distinguish human and machine 
summaries
79
157
ROUGE details
 In fact a suite of evaluation metrics
 Unigram
 Bigram
 Skip bigram
 Longest common subsequence
 Many settings concerning
 Stopwords
 Stemming
 Dealing with multiple models
158
How to evaluate without human 
involvement? [Louis and Nenkova, 2009]
 A good summary should be similar to the 
input
 Multiple ways to measure similarity
 Cosine similarity
 KL divergence
 JS divergence
 Not all work!
80
159
 Distance between two distributions as 
average KL divergence from their mean 
distribution
JS divergence between input and 
summary
)]||()||([)||( 21 ASummKLAInpKLSummInpJS +=
SummaryandInputofondistributimeanSummInpA ,
2
+
=
160
Summary likelihood given the input
 Probability that summary is generated according to 
term distribution in the input
Higher likelihood ~ better summary
 Unigram Model
 Multinomial Model
ii
n
rInp
n
Inp
n
Inp
wwordofsummaryincountn
vocabularysummaryr
wpwpwp r
=
?
)()()( 21 21 K
sizesummarynN
wpwpwp
i
i
n
rInp
n
Inp
n
Inpnn
N r
r
==?
)()()( 21
1 21!!
! KK
81
161
 Fraction of summary = input?s topic words
 % of input?s topic words also appearing in summary 
 Capture variety
 Cosine similarity: input?s topic words and all summary 
words
 Fewer dimensions, more specific vectors
Topic words identified by log-likelihood 
test
162
How good are these metrics? 
48 inputs, 57 systems
JSD -0.880 -0.736
0.795 0.627
-0.763 -0.694
0.712 0.647
0.712 0.602
-0.688 -0.585
-0.188 -0.101
0.222 0.235
% input?s topic in summary
KL div summ-input
Cosine similarity
% of summary = topic words
KL div input-summ
Unigram summ prob.
Multinomial summ prob.
-0.699 0.629Topic word similarity
Pyramid Responsiveness
Spearman correlation on macro level for the query focused task.
82
163
 JSD correlations with pyramid scores even better than 
R1-recall
 R2-recall is consistently better
 Can extend features using higher order n-grams 
How good are these metrics?
0.870.90R2-recall
0.800.85R1-recall
-0.73-0.88JSD
Resp.Pyramid
164
Motivation & Definition
Topic Models
Graph Based Methods
Supervised Techniques
Global Optimization Methods
Speech Summarization
Evaluation
Frequency, TF*IDF, Topic Words
Topic Models [LSA, EM, Bayesian]
Manual (Pyramid), Automatic (Rouge, F-Measure)
Fully Automatic
Features, Discriminative Training
Sampling, Data, Co-training
Iterative, Greedy, Dynamic Programming
ILP, Sub-Modular Selection
Segmentation, ASR
Acoustic Information, Disfluency 
83
165
Current summarization research  
 Summarization for various new genres
 Scientific articles
 Biography
 Social media (blog, twitter)
 Other text and speech data 
 New task definition 
 Update summarization 
 Opinion summarization
 New summarization approaches 
 Incorporate more information (deep linguistic knowledge, information 
from the web)
 Adopt more complex machine learning techniques
 Evaluation issues
 Better automatic metrics
 Extrinsic evaluations
And more?
166
 Check out summarization papers at ACL this 
year
 Workshop at ACL-HLT 2011:
 Automatic summarization for different genres, 
media, and languages [June 23, 2011]
 http://www.summarization2011.org/
84
167
References
 Ahmet Aker, Trevor Cohn, Robert Gaizauska. 2010. Multi-document summarization using A* search and 
discriminative training. Proc. of EMNLP.
 R. Barzilay and M. Elhadad. 2009. Text summarizations with lexical chains. In: I. Mani and M. Maybury (eds.): 
Advances in Automatic Text Summarization.
 Jaime Carbonell and Jade Goldstein. 1998. The Use of MMR, Diversity-Based reranking for Reordering 
Documents and Producing Summaries. Proceedings of the 21st Annual International ACM SIGIR Conference on 
Research and Development in Information Retrieval.
 H. Christensen, Y. Gotoh, B. Killuru, and S. Renals. 2003. Are Extractive Text Summarization Techniques 
Portable to Broadcast News? Proc. of ASRU.
 John Conroy and Dianne O'Leary. 2001. Text Summarization via Hidden Markov Models. Proc. of SIGIR.
 J. M. Conroy, J. D. Schlesinger, and D. P. OLeary. 2006. Topic-Focused Multi-Document Summarization Using an 
Approximate Oracle Score. Proc. COLING/ACL 2006. pp. 152-159.
 Thomas Cormen, Charles E. Leiserson, and Ronald L. Rivest.1990. Introduction to algorithms. MIT Press.
 G. Erkan and D. R. Radev.2004. LexRank: Graph-based Centrality as Salience in Text Summarization. Journal of 
Artificial Intelligence Research (JAIR).
 Pascale Fung, Grace Ngai, and Percy Cheung. 2003. Combining optimal clustering and hidden Markov models for 
extractive summarization. Proceedings of ACL Workshop on Multilingual Summarization.Sadoki Furui, T. Kikuchi, 
Y. Shinnaka, and C. Hori. 2004. Speech-to-text and Speech-to-speech Summarization of Spontaneous Speech. 
IEEE Transactions on Audio, Speech, and Language Processing. 12(4), pages 401-408.
 Michel Galley. 2006. A Skip-Chain Conditional Random Field for Ranking Meeting Utterances by Importance. 
Proc. of EMNLP.
 Dan Gillick, Benoit Favre. 2009. A scalable global model for summarization. Proceedings of the Workshop on 
Integer Linear Programming for Natural Language Processing.
 Dan Gillick, Korbinian Riedhammer, Benoit Favre, Dilek Hakkani-Tur. 2009. A global optimization framework for 
meeting summarization. Proceedings of ICASSP.
 Jade Goldstein, Vibhu Mittal, Jaime Carbonell, and Mark Kantrowitz. 2000. Multi-document summarization by 
sentence extraction. Proceedings of the 2000 NAACL-ANLP Workshop on Automatic Summarization.
168
References
 Y. Gong and X. Liu. 2001. Generic text summarization using relevance measure and latent semantic analysis. 
Proc. ACM SIGIR.
 I. Gurevych and T. Nahnsen. 2005. Adapting Lexical Chaining to Summarize Conversational Dialogues. Proc. 
RANLP.
 B. Hachey, G. Murray, and D. Reitter.2006. Dimensionality reduction aids term co-occurrence based multi-
document summarization. In: SumQA 06: Proceedings of the Workshop on Task-Focused Summarization and 
Question Answering. 
 Aria Haghighi and Lucy Vanderwende. 2009. Exploring content models for multi-document summarization. Proc. of 
NAACL-HLT.
 L. He, E. Sanocki, A. Gupta, and J. Grudin. 2000. Comparing presentation summaries: Slides vs. reading vs.
listening. Proc. of SIGCHI on Human factors in computing systems.
 C. Hori and Sadaoki Furui. 2001. Advances in Automatic Speech Summarization. Proc. of Eurospeech.
 T. Kikuchi, S. Furui, and C. Hori. 2003. Automatic Speech Summarization based on Sentence Extractive and 
Compaction. Proc. of ICSLP.  
 Julian Kupiec, Jan Pedersen, and Francine Chen. 1995. A Trainable Document Summarizer. Proc. of SIGIR.
 J. Leskovec, N. Milic-frayling, and M. Grobelnik. 2005. Impact of Linguistic Analysis on the Semantic Graph 
Coverage and Learning of Document Extracts. Proc. AAAI.
 Chin-Yew Lin. 2004. ROUGE: A Package for Automatic Evaluation of Summaries, Workshop on Text 
Summarization Branches Out. 
 C.Y. Lin and E. Hovy. 2000. The automated acquisition of topic signatures for text summarization. Proc. COLING.
 Hui Lin and Jeff Bilmes. 2010. Multi-document summarization via budgeted maximization of submodular functions. 
Proc. of NAACL.
 Hui Lin and Jeff Bilmes and Shasha Xie. 2009. Graph-based Submodular Selection for Extractive Summarization. 
Proceedings of ASRU.
 Shih-Hsiang Lin and Berlin Chen. 2009. Improved Speech Summarization with Multiple-hypothesis 
Representations and Kullback-Leibler Divergence Measures. Proc. of Interspeech.
 Shih-Hsiang Lin, Berlin Chen, and H. Min Wang. 2009. A Comparative Study of Probabilistic Ranking Models for 
Chinese Spoken Document Summarization. ACM Transactions on Asian Language Information Processing.
85
169
References
 Shih Hsiang Lin, Yu Mei Chang, Jia Wen Liu, Berlin Chen. 2010 Leveraging Evaluation Metric-related Training 
Criteria for Speech Summarization. Proc. of ICASSP.
 Fei Liu and Yang Liu. 2009. From Extractive to Abstractive Meeting Summaries: Can it be done by sentence 
compression? Proc. of ACL.
 Fei Liu and Yang Liu. 2010. Using Spoken Utterance Compression for Meeting Summarization: A pilot study. Proc. 
of IEEE SLT.
 Yang Liu and Shasha Xie. 2008. Impact of Automatic Sentence Segmentation on Meeting Summarization. Proc. of 
ICASSP.
 Yang Liu, Feifan Liu, Bin Li, and Shasha Xie. 2007. Do Disfluencies Affect Meeting Summarization: A pilot study 
on the impact of disfluencies. Poster at MLMI.
 Yang Liu, Shasha Xie, and Fei Liu. 2010. Using n-best Recognition Output for Extractive Summarization and 
Keyword Extraction in Meeting Speech. Proc. of ICASSP.
 Annie Louis and Ani Nenkova. 2009. Automatically evaluating content selection in summarization without 
human models. Proceedings of EMNLP
 H.P. Luhn. 1958. The Automatic Creation of Literature Abstracts. IBM Journal of Research and Development 2(2).
 Inderjeet Mani, Gary Klein, David House, Lynette Hirschman, Therese Firmin, and Beth Sundheim. 2002. 
SUMMAC: a text summarization evaluation. Natual Language Engineering. 8,1 (March 2002), 43-68.
 Manuel J. Mana-Lopez, Manuel De Buenaga, and Jose M. Gomez-Hidalgo. 2004. Multidocument summarization: 
An added value to clustering in interactive retrieval. ACM Trans. Inf. Systems.
 Sameer Maskey. 2008. Automatic Broadcast News Summarization. Ph.D thesis. Columbia University.
 Sameer Maskey and Julia Hirschberg. 2005. Comparing lexical, acoustic/prosodic, discourse and structural 
features for speech summarization. Proceedings of Interspeech.
 Sameer Maskey and Julia Hirschberg. 2006. Summarizing Speech Without Text Using Hidden Markov Models. 
Proc. of HLT-NAACL.
 Ryan McDonald. 2007. A Study of Global Inference Algorithms in Multi-document Summarization. Lecture Notes in 
Computer Science. Advances in Information Retrieval. 
 Kathleen McKeown, Rebecca J. Passonneau, David K. Elson, Ani Nenkova, and Julia Hirschberg. 2005. Do 
summaries help?. Proc. of SIGIR.
 K. McKeown, J. L. Klavans, V. Hatzivassiloglou, R. Barzilay, and E. Eskin.1999. Towards multidocument
summarization by reformulation: progress and prospects. Proc. AAAI 1999.
170
References
 R. Mihalcea and P. Tarau .2004. Textrank: Bringing order into texts. Proc. of EMNLP 2004. 
 G. Murray, S. Renals, J. Carletta, J. Moore. 2005. Evaluating Automatic Summaries of Meeting Recordings. Proc. 
of ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation.
 G. Murray, T. Kleinbauer, P. Poller, T. Becker, S. Renals, and J. Kilgour. 2009. Extrinsic Summarization 
Evaluation: A Decision Audit Task. ACM Transactions on Speech and Language Processing.
 A. Nenkova and R. Passonneau. 2004. Evaluating Content Selection in Summarization: The Pyramid Method. 
Proc. HLT-NAACL.
 A. Nenkova, L. Vanderwende, and K. McKeown. 2006. A compositional context sensitive multi-document 
summarizer: exploring the factors that influence summarization. Proc. ACM SIGIR.
 A. Nenkova, R. Passonneau, and K. McKeown. 2007. The Pyramid Method: Incorporating human content 
selection variation in summarization evaluation. ACM Trans. Speech Lang. Processing.
 Miles Osborne. 2002. Using maximum entropy for sentence extraction. Proc. of ACL Workshop on Automatic 
Summarization.
 Gerald Penn and Xiaodan Zhu. 2008. A critical Reassessement of Evaluation Baselines for Speech 
Summarization. Proc. of ACL-HLT.
 Dmitri G. Roussinov and Hsinchun Chen. 2001. Information navigation on the web by clustering and summarizing 
query results. Inf. Process. Manage. 37, 6 (October 2001), 789-816. 
 B. Schiffman, A. Nenkova, and K. McKeown. 2002. Experiments in Multidocument Summarization. Proc. HLT.
 A. Siddharthan, A. Nenkova, and K. Mckeown.2004. Syntactic Simplification for Improving Content Selection in 
Multi-Document Summarization. Proc. COLING.
 H. Grogory Silber and Kathleen F. McCoy. 2002. Efficiently computed lexical chains as an intermediate 
representation for automatic text summarization. Computational. Linguist. 28, 4 (December 2002), 487-496.
 J. Steinberger, M. Poesio, M. A. Kabadjov, and K. Jeek. 2007. Two uses of anaphora resolution in summarization. 
Inf. Process. Manage. 43(6).
 S. Tucker and S. Whittaker. 2008. Temporal compression of speech: an evaluation. IEEE Transactions on Audio, 
Speech and Language Processing, pages 790-796.
 L. Vanderwende, H. Suzuki, C. Brockett, and A. Nenkova. 2007. Beyond SumBasic: Task-focused summarization 
with sentence simplification and lexical expansion. Information Processing and Management 43.
86
171
References
 Kam-Fai Wong, Mingli Wu, and Wenjie Li. 2008. Extractive Summarization using Supervised and Semi-supervised 
learning. Proc. of ACL.
 Shasha Xie and Yang Liu. 2010. Improving Supervised Learning for Meeting Summarization using Sampling and 
Regression. Computer Speech and Language. V24, pages 495-514.
 Shasha Xie and Yang Liu. 2010. Using Confusion Networks for Speech Summarization. Proc. of NAACL.
 Shasha Xie, Dilek Hakkani-Tur, Benoit Favre, and Yang Liu. 2009. Integrating Prosodic Features in Extractive 
Meeting Summarization. Proc. of ASRU.
 Shasha Xie, Hui Lin, and Yang Liu. 2010. Semi-supervised Extractive Speech Summarization via Co-training 
Algorithm. Proc. of Interspeech.
 S. Ye, T.-S. Chua, M.-Y. Kan, and L. Qiu. 2007. Document concept lattice for text understanding and 
summarization. Information Processing and Management 43(6).
 W. Yih, J. Goodman, L. Vanderwende, and H. Suzuki. 2007. Multi-Document Summarization by Maximizing 
Informative Content-Words. Proc. IJCAI 2007.
 Klaus Zechner. 2002. Automatic Summarization of Open-domain Multiparty Dialogues in Diverse Genres. 
Computational Linguistics. V28, pages 447-485.
 Klaus Zechner and Alex Waibel. 2000. Minimizing word error rate in textual summaries of spoken language. 
Proceedings of the 1st North American chapter of the Association for Computational Linguistics conference.
 Justin Zhang and Pascale Fung. 2009. Extractive Speech Summarization by Active Learning. Proc. of ASRU.
 Xiaodan Zhu and Gerald Penn. 2006. Comparing the Roles of Textual, Acoustic and Spoken-language Features 
on Spontaneous Conversation Summarization. Proc. of HLT-NAACL.
 Xiaodan Zhu, Gerald Penn, and F. Rudzicz. 2009. Summarizing Multiple Spoken Documents: Finding Evidence 
from Untranscribed Audio. Proc. of ACL.
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 554?562,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Sentence Dependency Tagging in Online Question Answering Forums
Zhonghua Qu and Yang Liu
The University of Texas at Dallas
{qzh,yangl@hlt.utdallas.edu}
Abstract
Online forums are becoming a popular re-
source in the state of the art question answer-
ing (QA) systems. Because of its nature as an
online community, it contains more updated
knowledge than other places. However, go-
ing through tedious and redundant posts to
look for answers could be very time consum-
ing. Most prior work focused on extracting
only question answering sentences from user
conversations. In this paper, we introduce the
task of sentence dependency tagging. Finding
dependency structure can not only help find
answer quickly but also allow users to trace
back how the answer is concluded through
user conversations. We use linear-chain con-
ditional random fields (CRF) for sentence type
tagging, and a 2D CRF to label the depen-
dency relation between sentences. Our ex-
perimental results show that our proposed ap-
proach performs well for sentence dependency
tagging. This dependency information can
benefit other tasks such as thread ranking and
answer summarization in online forums.
1 Introduction
Automatic Question Answering (QA) systems rely
heavily on good sources of data that contain ques-
tions and answers. Question answering forums, such
as technical support forums, are places where users
find answers through conversations. Because of
their nature as online communities, question answer-
ing forums provide more updated answers to new
problems. For example, when the latest release of
Linux has a bug, we can expect to find solutions
in forums first. However, unlike other structured
knowledge bases, often it is not straightforward to
extract information such as questions and answers in
online forums because such information spreads in
the conversations among multiple users in a thread.
A lot of previous work has focused on extract-
ing the question and answer sentences from forum
threads. However, there is much richer information
in forum conversations, and simply knowing a sen-
tence is a question or answer is not enough. For
example, in technical support forums, often it takes
several iterations of asking and clarifications to de-
scribe the question. The same happens to answers.
Usually several candidate answers are provided, and
not all answers are useful. In this case users? feed-
back is needed to judge the correctness of answers.
Figure 1 shows an example thread in a technical
support forum. Each sentence is labeled with its type
(a detailed description of sentence types is provided
Table 1). We can see from the example that ques-
tions and answers are not expressed in a single sen-
tence or a single post. Only identifying question and
answering sentences from the thread is not enough
for automatic question answering. For this example,
in order to get the complete question, we would need
to know that sentence S3 is a question that inquires
for more details about the problem asked earlier, in-
stead of stating its own question. Also, sentence S5
should not be included in the correct answer since
it is not a working solution, which is indicated by a
negative feedback in sentence S6. The correct solu-
tion should be sentence S7, because of a user?s posi-
tive confirmation S9. We define that there is a depen-
dency between a pair of sentences if one sentence
554
A: [S1:M-GRET] Hi everyone. [S2:P-STAT] I
have recently purchased USB flash and I am having
trouble renaming it, please help me.
B: [S3:A-INQU] What is the size and brand of this
flash?
A: [S4:Q-CLRF] It is a 4GB SanDisk flash.
B: [S5:A-SOLU] Install gparted, select flash drive
and rename.
A: [S6:M-NEGA] I got to the Right click on
partition and the label option was there but grayed
out.
B: [S7:A-SOLU] Sorry again, I meant to right click
the partition and select Unmount and then select
Change name while in gparted.
A: [S8:C-GRAT] Thank you so much. [S9:M-
POST] I now have an ?Epic USB? You Rock!
Figure 1: Example of a Question Answering Thread in
Ubuntu Support Forum
exists as a result of another sentence. For example,
question context sentences exist because of the ques-
tion itself; an answering sentence exists because of
a question; or a feedback sentence exists because of
an answer. The sentence dependency structure of
this example dialog is shown in Figure 2.
S1: M-GRET
S2: P-STAT
S3: A-INQU S4:Q-CLRF
S5:A-SOLU S6:M-NEGA S7:A-SOLU
S8:C-GRAT
S9:M-POST
Figure 2: Dependency Structure of the Above Example
This example shows that in order to extract in-
formation from QA forums accurately, we need to
understand the sentence dependency structure of a
QA thread. Towards this goal, in this paper, we de-
fine two tasks: labeling the types for sentences, and
finding the dependency relations between sentences.
For the first task of sentence type labeling, we de-
fine a rich set of categories representing the purpose
of the sentences. We use linear-chain conditional
random fields (CRF) to take advantage of many
long-distance and non-local features. The second
task is to identify relations between sentences. Most
previous work only focused on finding the answer-
question relationship between sentences. However,
other relations can also be useful for information ex-
traction from online threads, such as user?s feed-
backs on the answers, problem detail inquiry and
question clarifications. In this study, we use two
approaches for labeling of dependency relation be-
tween sentences. First each sentence is considered
as a source, and we run a linear-chain CRF to la-
bel whether each of the other sentences is its tar-
get. Because multiple runs of separate linear-chain
CRFs ignore the dependency between source sen-
tences, the second approach we propose is to use a
2D CRF that models all pair relationships jointly.
The data we used was collected from Ubuntu
forum general help section. Our experimental re-
sults show that our proposed sentence type tagging
method works very well, even for the minority cate-
gories, and that using 2D CRF further improves per-
formance over linear-chain CRFs for identifying de-
pendency relation between sentences.
The paper is organized as follows. In the follow-
ing section, we discuss related work on finding ques-
tions and answers in online environment as well as
some dialog act tagging techniques. In Section 3, we
introduce the use of CRFs for sentence type and de-
pendency tagging. Section 4 describes data collec-
tion, annotation, and some analysis. In Section 5, we
show that our approach achieves promising results
in thread sentence dependency tagging. Finally we
conclude the paper and suggest some possible future
extensions.
2 Related Work
There is a lot of useful knowledge in the user gener-
ated content such as forums. This knowledge source
could substantially help automatic question answer-
ing systems. There has been some previous work
focusing on the extraction of question and corre-
sponding answer pairs in online forums. In (Ding
et al, 2008), a two-pass approach was used to find
relevant solutions for a given question, and a skip-
chain CRF was adopted to model long range de-
555
pendency between sentences. A graph propagation
method was used in (Cong et al, 2008) to rank
relevant answers to questions. An approach using
email structure to detect and summarize question an-
swer pairs was introduced in (Shrestha and Mck-
eown, 2004). These studies focused primarily on
finding questions and answers in an online envi-
ronment. In this paper, in order to provide a bet-
ter foundation for question answer detection in on-
line forums, we investigate tagging sentences with a
much richer set of categories, as well as identifying
their dependency relationships. The sentence types
we use are similar to dialog acts (DA), but defined
specifically for question answering forums. Work of
(Clark and Popescu-Belis, 2004) defined a reusable
multi-level tagset that can be mapped from conversa-
tional speech corpora such as the ICSI meeting data.
However, it is hard to reuse any available corpus or
DA tagset because our task is different, and also on-
line forum has a different style from speech data.
Automatic DA tagging has been studied a lot previ-
ously. For example, in (Stolcke et al, 2000), Hidden
Markov Models (HMMs) were used for DA tagging;
in (Ji and Bilmes, 2005), different types of graphical
models were explored.
Our study is different in several aspects: we are
using forum domains, unlike most work of DA tag-
ging on conversational speech; we use CRFs for sen-
tence type tagging; and more importantly, we also
propose to use different CRFs for sentence relation
detection. Unlike the pair-wise sentence analysis
proposed in (Boyer et al, 2009) in which HMM
was used to model the dialog structure, our model is
more flexible and does not require related sentences
to be adjacent.
3 Thread Structure Tagging
As described earlier, we decompose the structure
analysis of QA threads into two tasks, first deter-
mine the sentence type, and then identify related
sentences. This section provides details for each
task.
3.1 Sentence Type Tagging
In human conversations, especially speech conver-
sations, DAs have been used to represent the pur-
pose or intention of a sentence. Different sets of
DAs have been adopted in various studies, ranging
from very coarse categories to fine grained ones. In
this study, we define 13 fine grained sentence types
(corresponding to 4 coarse categories) tailored to our
domain of QA forum threads. Table 1 shows the cat-
egories and their description. Some tags such as P-
STAT and A-SOLU are more important in that users
try to state a problem and provide solutions accord-
ingly. These are the typical ones used in previous
work on question answering. Our set includes other
useful tags. For example, C-NEGA and C-POSI can
evaluate how good an answer is. Even though C-
GRAT does not provide any direct feedback on the
solutions, existence of such a tag often strongly im-
plies a positive feedback to an answer. These sen-
tence types can be grouped into 4 coarse categories,
as shown in Table 1.
Types Category Description
Problems
P-STAT question of problem
P-CONT problem context
P-CLRF problem clarification
Answers
A-SOLU solution sentence
A-EXPL explanation on solutions
A-INQU inquire problem details
Confirm.
C-GRAT gratitude
C-NEGA negative feedback
C-POSI positive feedback
Misc.
M-QCOM question comment
M-ACOM comment on the answer
M-GRET greeting and politeness
M-OFF off-topic sentences
Table 1: Sentence Types for QA Threads
To automatically label sentences in a thread with
their types, we adopt a sequence labeling approach,
specifically linear-chain conditional random fields
(CRFs), which have shown good performance in
many other tasks (Lafferty, 2001). Intuitively there
is a strong dependency between adjacent sentences.
For example, in our data set, 45% sentences follow-
ing a greeting sentence (M-GRET) are question re-
lated sentences; 53% sentences following a question
inquiry sentence (Q-INQ) are solution related sen-
tences. The following describes our modeling ap-
proaches and features used for sentence type tag-
ging.
556
3.1.1 Linear-chain Conditional Random Field
Linear-chain CRFs is a type of undirected graphi-
cal models. Distribution of a set of variables in undi-
rected graphical models can be written as
p(x, y) =
1
Z
?
A
?A(xA, yA) (1)
Z is the normalization constant to guarantee valid
probability distributions. CRFs is a special case
of undirected graphical model in which ? are log-
linear functions:
?A(xA, yA) = exp
{
?
k
?AkfAk(xA, yA)
}
(2)
?A is a real value parameter vector for feature
function set fA. In the sequence labeling task, fea-
ture functions across the sequence are often tied to-
gether. In other words, feature functions at different
locations of the sequence share the same parameter
vector ?.
Figure 3: Graphical Structure of Linear-chain CRFs.
Linear-chain CRF is a special case of the general
CRFs. In linear-chain CRF, cliques only involve two
adjacent variables in the sequence. Figure 3 shows
the graphical structure of a linear-chain CRF. In our
case of sentence tagging, cliques only contain two
adjacent sentences. Given the observation x, the
probability of label sequence y is as follows:
p(y|x) =
1
Z
|y|?
i=1
?e(x, y, i)
|y|?
j=0
?v(x, y, j) (3)
?e(x, y, i) = exp
{
?
k
?ekfek(yi?1, yi, x, i)
}
(4)
?v(x, y, j) = exp
{
?
k
?vkfvk(yj , x, j)
}
(5)
where feature templates fek and fvk correspond to
edge features and node features respectively.
Feature Description
Cosine similarity with previous sentence.
Quote segment within two adjacent sentences?
Code segment within two adjacent sentences?
Does this sentence belong to author?s post?
Is it the first sentence in a post?
Post author participated thread before?
Does the sentence contain any negative words?
Does the sentence contain any URL?
Does the sentence contain any positive words?
Does the sentence contain any question mark?
Length of the sentence.
Presence of verb.
Presence of adjective.
Sentence perplexity based on a background LM.
Bag of word features.
Table 2: Features Used in Sentence Type Tagging.
3.1.2 Sentence Type Tagging Features
We used various types of feature functions in sen-
tence type tagging. Table 2 shows the complete list
of features we used. Edge features are closely re-
lated to the transition between sentences. Here we
use the cosine similarity between sentences, where
each sentence is represented as a vector of words,
with term weight calculated using TD-IDF (term fre-
quency times inverse document frequency). High
similarity between adjacent sentences suggests sim-
ilar or related types. For node features, we explore
different sources of information about the sentence.
For example, the presence of a question mark indi-
cates that a sentence may be a question or inquiry.
Similarly, we include other cues, such as positive
or negative words, verb and adjective words. Since
technical forums tend to contain many system out-
puts, we include the perplexity of the sentence as a
feature which is calculated based on a background
language model (LM) learned from common En-
glish documents. We also use bag-of-word features
as in many other text categorization tasks.
Furthermore, we add features to represent post
level information to account for the structure of QA
threads, for example, whether or not a sentence be-
longs to the author?s post, or if a sentence is the be-
ginning sentence of a post.
557
3.2 Sentence Dependency Tagging
Knowing only the sentence types without their de-
pendency relations is not enough for question an-
swering tasks. For example, correct labeling of an
answer without knowing which question it actually
refers to is problematic; not knowing which answer
a positive or negative feedback refers to will not be
helpful at all. In this section we describe how sen-
tence dependency information is determined. Note
that sentence dependency relations might not be a
one-to-one relation. A many-to-many relation is also
possible. Take question answer relation as an ex-
ample. There could be potentially many answers
spreading in many sentences, all depending on the
same question. Also, it is very likely that a question
is expressed in multiple sentences too.
Dependency relationship could happen between
many different types of sentences, for example, an-
swer(s) to question(s), problem clarification to ques-
tion inquiry, feedback to solutions, etc. Instead of
developing models for each dependency type, we
treat them uniformly as dependency relations be-
tween sentences. Hence, for every two sentences,
it becomes a binary classification problem, i.e.,
whether or not there exists a dependency relation
between them. For a pair of sentences, we call the
depending sentence the source sentence, and the de-
pended sentence the target sentence. As described
earlier, one source sentence can potentially depend
on many different target sentences, and one target
sentence can also correspond to multiple sources.
The sentence dependency task is formally defined
as, given a set of sentences St of a thread, find the
dependency relation {(s, t)|s ? St, t ? St}, where s
is the source sentence and t is the target sentence that
s depends on.
We propose two methods to find the dependency
relationship. In the first approach, for each source
sentence, we run a labeling procedure to find the de-
pendent sentences. From the data, we found given a
source sentence, there is strong dependency between
adjacent target sentences. If one sentence is a tar-
get sentence of the source, often the next sentence
is a target sentence too. In order to take advantage
of such adjacent sentence dependency, we use the
linear-chain CRFs for the sequence labeling. Fea-
tures used in sentence dependency labeling are listed
in Table 3. Note that a lot of the node features used
here are relative to the source sentence since the task
here is to determine if the two sentences are related.
For a thread of N sentences, we need to perform N
runs of CRF labeling, one for each sentence (as the
source sentence) in order to label the target sentence
corresponding to this source sentence.
Feature Description
* Cosine similarity with previous sentence.
* Is adjacent sentence of the same type?
* Pair of types of the adjacent target sentences.
Pair of types of the source and target sentence.
Is target in the same post as the source?
Do target and source belong to the same author?
Cosine similarity between target and source sentence.
Does target sentence happen before source?
Post distance between source and target sentence.
* indicates an edge feature
Table 3: Features Used in Sentence Dependency Labeling
The linear-chain CRFs can represent the depen-
dency between adjacent target sentences quite well.
However they cannot model the dependency be-
tween adjacent source sentences, because labeling
is done for each source sentence individually. To
model the dependency between both the source sen-
tences and the target sentences, we propose to use
2D CRFs for sentence relation labeling. 2D CRFs
are used in many applications considering two di-
mension dependencies such as object recognitions
(Quattoni et al, 2004) and web information extrac-
tion (Zhu et al, 2005). The graphical structure of
a 2D CRF is shown in Figure 4. Unlike one di-
mensional sequence labeling, a node in 2D environ-
ment is dependent on both x-axis neighbors and y-
axis neighbors. In the sentence relation task, the
source and target pair is a 2D relation in which its
label depends on labels of both its adjacent source
and its adjacent target sentence. As shown in Fig-
ure 4, looking from x-axis is the sequence of target
sentences with a fixed source sentence, and from y-
axis is the sequence of source sentences with a fixed
target sentence. This model allows us to model all
the sentence relationships jointly. 2D CRFs contain
3 templates of features: node template, x-axis edge
template, and y-axis edge template. We use the same
edge features and node features as in linear-chain
CRFs for node features and y-axis edge features in
558
2D CRFs. For the x-axis edge features, we use the
same feature functions as for y-axis, except that now
they represent the relation between adjacent source
sentences.
y i y i + 1 . . .. . .
X
y 0 0 . . .
. . .
. . . . . . . . .
y 10
y 0 1 y 11 X
So u r c e
T
arg
et
Figure 4: Graphical Structure of 2D CRF for Sentence
Relation Labeling.
In a thread containing N sentences, we would
have a 2D CRF containing N2 nodes in a N ? N
grid. Exact inference in such a graph is intractable.
In this paper we use loopy belief propagation algo-
rithm for the inference. Loopy belief propagation is
a message passing algorithm for graph inference. It
calculates the marginal distribution for each node in
the graph. The result is exact in some graph struc-
tures (e.g., linear-chain CRFs), and often converges
to a good approximation for general graphs.
4 Data
We used data from ubuntu community forum gen-
eral help section for the experiments and evalua-
tion. This is a technical support section that provides
answers to the latest problems in Ubuntu Linux.
Among all the threads that we have crawled, we se-
lected 200 threads for this initial study. They con-
tain between 2? 10 posts and at least 2 participants.
Sentences inside each thread are segmented using
Apache OpenNLP tools. In total, there are 706 posts
and 3,483 sentences. On average, each thread con-
tains 3.53 posts, and each post contains around 4.93
sentences. Two annotators were recruited to anno-
tate the sentence type and the dependency relation
between sentences. Annotators are both computer
science department undergraduate students. They
are provided with detailed explanation of the anno-
tation standard. The distribution of sentence types
in the annotated data is shown in Table 4, along with
inter-annotator Kappa statistics calculated using 20
common threads annotated by both annotators. We
can see that the majority of the sentences are about
problem descriptions and solutions. In general, the
agreement between the two annotators is quite good.
General Type Category Percentage Kappa
Problems
P-STAT 12.37 0.88
P-CONT 37.30 0.77
P-CLRF 1.01 0.98
Answers
A-SOLU 9.94 0.89
A-EXPL 11.60 0.89
A-INQU 1.38 0.99
Confirmation
C-GRAT 5.06 0.98
C-NEGA 1.98 0.96
C-POSI 1.84 0.96
Miscellaneous
M-QCOM 1.98 0.93
M-ACOM 1.47 0.96
M-GRET 1.01 0.96
M-OFF 7.92 0.96
Table 4: Distribution and Inter-annotator Agreement of
Sentence Types in Data
There are in total 1, 751 dependency relations
identified by the annotators among those tagged sen-
tences. Note that we are only dealing with intra-
thread sentence dependency, that is, no dependency
among sentences in different threads is labeled.
Considering all the possible sentence pairs in each
thread, the labeled dependency relations represent a
small percentage. The most common dependency
is problem description to problem question. This
shows that users tend to provide many details of
the problem. This is especially true in technical fo-
rums. Seeing questions without their context would
be confusing and hard to solve. The relation of an-
swering solutions and question dependency is also
very common, as expected. The third common re-
lation is the feedback dependency. Even though the
number of feedback sentences is small in the data
set, it plays a vital role to determine the quality of
answers. The main reason for the small number is
that, unlike problem descriptions, much fewer sen-
tences are needed to give feedbacks.
5 Experiment
In the experiment, we randomly split annotated
threads into three disjoint sets, and run a three-fold
cross validation. Within each fold, first sentence
types are labeled using linear-chain CRFs, then the
559
resulting sentence type tagging is used in the sec-
ond pass to determine dependency relations. For
part-of-speech (POS) tagging of the sentences, we
used Stanford POS Tagger (Toutanova and Man-
ning, 2000). All the graphical inference and estima-
tions are done using MALLET package (McCallum,
2002).
In this paper, we evaluate the results using stan-
dard precision and recall. In the sentence type tag-
ging task, we calculate precision, recall, and F1
score for each individual tag. For the dependency
tagging task, a pair identified by the system is cor-
rect only if the exact pair appears in the reference an-
notation. Precision and recall scores are calculated
accordingly.
5.1 Sentence Type Tagging Results
The results of sentence type tagging using linear-
chain CRFs are shown in Table 5. For a comparison,
we include results using a basic first-order HMM
model. Because HMM is a generative model, we
use only bag of word features in the generative pro-
cess. The observation probability is the probabil-
ity of the sentence generated by a unigram language
model, trained for different sentence types. Since
for some applications, fine grained categories may
not be needed, for example, in the case of finding
questions and answers in a thread, we also include
in the table the tagging results when only the gen-
eral categories are used in both training and testing.
We can see from the table that using CRFs
achieves significantly better performance than
HMMs for most categories, except greeting and off-
topic types. This is mainly because of the advantage
of CRFs, allowing the incorporation of rich discrimi-
native features. For the two major types of problems
and answers, in general, our system shows very good
performance. Even for minority types like feed-
backs, it also performs reasonably well. When using
coarse types, the performance on average is better
compared to the finer grained categories, mainly be-
cause of the fewer classes in the classification task.
Using the fine grained categories, we found that the
system is able to tell the difference between ?prob-
lem statement? (P-STAT) and ?problem context? (P-
CONT). Note that in our task, a problem statement is
not necessarily a question sentence. Instead it could
be any sentence that expresses the need for a solu-
Linear-chain CRF First-order HMM
13 Fine Grained Types
Tag Prec. / Rec. F1 Prec. / Rec. F1
M-GRET 0.45 / 0.58 0.51 0.73 / 0.57 0.64
P-STAT 0.79 / 0.72 0.75 0.35 / 0.34 0.35
P-CONT 0.80 / 0.74 0.77 0.58 / 0.18 0.27
A-INQU 0.37 / 0.48 0.42 0.11 / 0.25 0.15
A-SOLU 0.78 / 0.64 0.71 0.27 / 0.29 0.28
A-EXPL 0.4 / 0.76 0.53 0.24 / 0.19 0.21
M-POST 0.5 / 0.41 0.45 0.04 / 0.1 0.05
C-GRAT 0.43 / 0.53 0.48 0.01 / 0.25 0.02
M-NEGA 0.67 / 0.5 0.57 0.09 / 0.31 0.14
M-OFF 0.11 / 0.23 0.15 0.20 / 0.23 0.21
P-CLRF 0.15 / 0.33 0.21 0.10 / 0.12 0.11
M-ACOM 0.27 / 0.38 0.32 0.09 / 0.1 0.09
M-QCOM 0.34 / 0.32 0.33 0.08 / 0.23 0.11
4 General Types
Tag Prec. / Rec. F1 Prec. / Rec. F1
Problem 0.85 / 0.76 0.80 0.73 / 0.27 0.39
Answers 0.65 / 0.72 0.68 0.45 / 0.36 0.40
Confirm. 0.80 / 0.74 0.77 0.06 / 0.26 0.10
Misc. 0.43 / 0.61 0.51 0.04 / 0.36 0.08
Table 5: Sentence Type Tagging Performance Using
CRFs and HMM.
tion.
We also performed some analysis of the features
using the feature weights in the trained CRF mod-
els. We find that some post level information is rela-
tively important. For example, the feature represent-
ing whether the sentence is before a ?code? segment
has a high weight for problem description classifica-
tion. This is because in linux support forum, people
usually put some machine output after their problem
description. We also notice that the weights for verb
words are usually high. This is intuitive since the
?verb? of a sentence can often determine its purpose.
5.2 Sentence Dependency Tagging Results
Table 6 shows the results using linear-chain CRFs
(L-CRF) and 2D CRFs for sentence dependency tag-
ging. We use different settings in our experiments.
For the categories of sentence types, we evaluate us-
ing both the fine grained (13 types) and the coarse
categories (4 types). Furthermore, we examine two
ways to obtain the sentence types. First, we use the
output from automatic sentence type tagging. In the
second one, we use the sentence type information
from the human annotated data in order to avoid the
error propagation from automatic sentence type la-
560
beling. This gives an oracle upper bound for the
second pass performance.
Using Oracle Sentence Type
Setup Precision Recall F1
13 types
L-CRF 0.973 0.453 0.618
2D-CRF 0.985 0.532 0.691
4 general
L-CRF 0.941 0.124 0.218
2D-CRF 0.956 0.145 0.252
Using System Sentence Type
Setup Precision Recall F1
13 types
L-CRF 0.943 0.362 0.523
2D-CRF 0.973 0.394 0.561
4 general
L-CRF 0.939 0.101 0.182
2D-CRF 0.942 0.127 0.223
Table 6: Sentence Dependency Tagging Performance
From the results we can see that 2D CRFs out-
perform linear-chain CRFs for all the conditions.
This shows that by modeling the 2D dependency in
source and target sentences, system performance is
improved. For the sentence types, when using auto-
matic sentence type tagging systems, there is a per-
formance drop. The performance gap between us-
ing the reference and automatic sentence types sug-
gests that there is still room for improvement from
better sentence type tagging. Regarding the cate-
gories used for the sentence types, we observe that
they have an impact on dependence tagging perfor-
mance. When using general categories, the perfor-
mance is far behind that using the fine grained types.
This is because some important information is lost
when grouping categories. For example, a depen-
dency relation can be: ?A-EXPL? (explanation for
solutions) depends on ?A-SOLU? (solutions); how-
ever, when using coarse categories, both are mapped
to ?Solution?, and having one ?Solution? depending
on another ?Solution? is not very intuitive and hard
to model properly. This shows that detailed cate-
gory information is very important for dependency
tagging even though the tagging accuracy from the
first pass is far from perfect.
Currently our system does not put constraints on
the sentence types for which dependencies exist. In
the system output we find that sometimes there are
obvious dependency errors, such as a positive feed-
back depending on a negative feedback. We may
improve our models by taking into account different
sentence types and dependency relations.
6 Conclusion
In this paper, we investigated sentence dependency
tagging of question and answer (QA) threads in on-
line forums. We define the thread tagging task as a
two-step process. In the first step, sentence types
are labeled. We defined 13 sentence types in or-
der to capture rich information of sentences to bene-
fit question answering systems. Linear chain CRF
is used for sentence type tagging. In the second
step, we label actual dependency between sentences.
First, we propose to use a linear-chain CRF to label
possible target sentences for each source sentence.
Then we improve the model to consider the depen-
dency between sentences along two dimensions us-
ing a 2D CRF. Our experiments show promising
performance in both tasks. This provides a good
pre-processing step towards automatic question an-
swering. In the future, we plan to explore using
constrained CRF for more accurate dependency tag-
ging. We will also use the result from this work in
other tasks such as answer quality ranking and an-
swer summarization.
7 Acknowledgment
This work is supported by DARPA under Contract
No. HR0011-12-C-0016 and NSF No. 0845484.
Any opinions expressed in this material are those of
the authors and do not necessarily reflect the views
of DARPA or NSF.
References
Kristy Elizabeth Boyer, Robert Phillips, Eun Young Ha,
Michael D. Wallis, Mladen A. Vouk, and James C.
Lester. 2009. Modeling dialogue structure with ad-
jacency pair analysis and hidden markov models. In
Proc. NAACL-Short, pages 49?52.
Alexander Clark and Andrei Popescu-Belis. 2004.
Multi-level dialogue act tags. In Proc. SIGDIAL,
pages 163?170.
Gao Cong, Long Wang, Chinyew Lin, Youngin Song, and
Yueheng Sun. 2008. Finding question-answer pairs
from online forums. In Proc. SIGIR, pages 467?474.
Shilin Ding, Gao Cong, Chinyew Lin, and Xiaoyan Zhu.
2008. Using conditional random fields to extract con-
texts and answers of questions from online forums. In
Proc. ACL-HLT.
Gang Ji and J Bilmes. 2005. Dialog Act Tagging Using
Graphical Models. In Proc. ICASSP.
561
John Lafferty. 2001. Conditional random fields: Proba-
bilistic models for segmenting and labeling sequence
data. In Proc. ICML, pages 282?289.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
Ariadna Quattoni, Michael Collins, and Trevor Darrell.
2004. Conditional random fields for object recogni-
tion. In Proc. NIPS, pages 1097?1104.
Lokesh Shrestha and Kathleen Mckeown. 2004. Detec-
tion of question-answer pairs in email conversations.
In Proc. Coling, pages 889?895.
Andreas Stolcke, Klaus Ries, Noah Coccaro, Eliza-
beth Shriberg, Rebecca Bates, Daniel Jurafsky, Paul
Taylor, Rachel Martin, Carol Van Ess-Dykema, and
Marie Meteer. 2000. Dialogue act modeling for
automatic tagging and recognition of conversational
speech. Computational Linguistics, 26:339?373.
Kristina Toutanova and Christopher D. Manning. 2000.
Enriching the knowledge sources used in a maximum
entropy part-of-speech tagger. In Proc. EMNLP/VLC,
pages 63?70.
Jun Zhu, Zaiqing Nie, Ji R. Wen, Bo Zhang, and Wei Y.
Ma. 2005. 2D Conditional Random Fields for Web
information extraction. In Proc. ICML, pages 1044?
1051, New York, NY, USA.
562
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 166?170,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
A Two-step Approach to Sentence Compression of Spoken Utterances
Dong Wang, Xian Qian, Yang Liu
The University of Texas at Dallas
dongwang,qx,yangl@hlt.utdallas.edu
Abstract
This paper presents a two-step approach to
compress spontaneous spoken utterances. In
the first step, we use a sequence labeling
method to determine if a word in the utterance
can be removed, and generate n-best com-
pressed sentences. In the second step, we
use a discriminative training approach to cap-
ture sentence level global information from
the candidates and rerank them. For evalua-
tion, we compare our system output with mul-
tiple human references. Our results show that
the new features we introduced in the first
compression step improve performance upon
the previous work on the same data set, and
reranking is able to yield additional gain, espe-
cially when training is performed to take into
account multiple references.
1 Introduction
Sentence compression aims to preserve the most im-
portant information in the original sentence with
fewer words. It can be used for abstractive summa-
rization where extracted important sentences often
need to be compressed and merged. For summariza-
tion of spontaneous speech, sentence compression
is especially important, since unlike fluent and well-
structured written text, spontaneous speech contains
a lot of disfluencies and much redundancy. The fol-
lowing shows an example of a pair of source and
compressed spoken sentences1 from human annota-
tion (removed words shown in bold):
[original sentence]
1For speech domains, ?sentences? are not clearly defined.
We use sentences and utterances interchangeably when there is
no ambiguity.
and then um in terms of the source the things uh the
only things that we had on there I believe were whether...
[compressed sentence]
and then in terms of the source the only things that we
had on there were whether...
In this study we investigate sentence compres-
sion of spoken utterances in order to remove re-
dundant or unnecessary words while trying to pre-
serve the information in the original sentence. Sen-
tence compression has been studied from formal
text domain to speech domain. In text domain,
(Knight and Marcu, 2000) applies noisy-channel
model and decision tree approaches on this prob-
lem. (Galley and Mckeown, 2007) proposes to use a
synchronous context-free grammars (SCFG) based
method to compress the sentence. (Cohn and La-
pata, 2008) expands the operation set by including
insertion, substitution and reordering, and incorpo-
rates grammar rules. In speech domain, (Clarke and
Lapata, 2008) investigates sentence compression in
broadcast news using an integer linear programming
approach. There is only a few existing work in spon-
taneous speech domains. (Liu and Liu, 2010) mod-
eled it as a sequence labeling problem using con-
ditional random fields model. (Liu and Liu, 2009)
compared the effect of different compression meth-
ods on a meeting summarization task, but did not
evaluate sentence compression itself.
We propose to use a two-step approach in this pa-
per for sentence compression of spontaneous speech
utterances. The contributions of our work are:
? Our proposed two-step approach allows us to
incorporate features from local and global lev-
els. In the first step, we adopt a similar se-
quence labeling method as used in (Liu and
Liu, 2010), but expanded the feature set, which
166
results in better performance. In the second
step, we use discriminative reranking to in-
corporate global information about the com-
pressed sentence candidates, which cannot be
accomplished by word level labeling.
? We evaluate our methods using different met-
rics including word-level accuracy and F1-
measure by comparing to one reference com-
pression, and BLEU scores comparing with
multiple references. We also demonstrate that
training in the reranking module can be tailed
to the evaluation metrics to optimize system
performance.
2 Corpus
We use the same corpus as (Liu and Liu, 2010)
where they annotated 2,860 summary sentences in
26 meetings from the ICSI meeting corpus (Murray
et al, 2005). In their annotation procedure, filled
pauses such as ?uh/um? and incomplete words are
removed before annotation. In the first step, 8 anno-
tators were asked to select words to be removed to
compress the sentences. In the second step, 6 an-
notators (different from the first step) were asked
to pick the best one from the 8 compressions from
the previous step. Therefore for each sentence, we
have 8 human compressions, as well a best one se-
lected by the majority of the 6 annotators in the sec-
ond step. The compression ratio of the best human
reference is 63.64%.
In the first step of our sentence compression ap-
proach (described below), for model training we
need the reference labels for each word, which rep-
resents whether it is preserved or deleted in the com-
pressed sentence. In (Liu and Liu, 2010), they used
the labels from the annotators directly. In this work,
we use a different way. For each sentence, we still
use the best compression as the gold standard, but
we realign the pair of the source sentence and the
compressed sentence, instead of using the labels
provided by annotators. This is because when there
are repeated words, annotators sometimes randomly
pick removed ones. However, we want to keep the
patterns consistent for model training ? we always
label the last appearance of the repeated words as
?preserved?, and the earlier ones as ?deleted?. An-
other difference in our processing of the corpus from
the previous work is that when aligning the original
and the compressed sentence, we keep filled pauses
and incomplete words since they tend to appear to-
gether with disfluencies and thus provide useful in-
formation for compression.
3 Sentence Compression Approach
Our compression approach has two steps: in the
first step, we use Conditional Random Fields (CRFs)
to model this problem as a sequence labeling task,
where the label indicates whether the word should be
removed or not. We select n-best candidates (n = 25
in our work) from this step. In the second step we
use discriminative training based on a maximum En-
tropy model to rerank the candidate compressions,
in order to select the best one based on the quality
of the whole candidate sentence, which cannot be
performed in the first step.
3.1 Generate N-best Candidates
In the first step, we cast sentence compression as
a sequence labeling problem. Considering that in
many cases phrases instead of single words are
deleted, we adopt the ?BIO? labeling scheme, simi-
lar to the name entity recognition task: ?B? indicates
the first word of the removed fragment, ?I? repre-
sents inside the removed fragment (except the first
word), and ?O? means outside the removed frag-
ment, i.e., words remaining in the compressed sen-
tence. Each sentence with n words can be viewed as
a word sequence X1, X2, ..., Xn, and our task is to
find the best label sequence Y1, Y2, ..., Yn where Yi
is one of the three labels. Similar to (Liu and Liu,
2010), for sequence labeling we use linear-chain
first-order CRFs. These models define the condi-
tional probability of each labeling sequence given
the word sequence as:
p(Y |X) ?
exp
Pn
k=1(
P
j ?jfj(yk, yk?1, X) +
P
i ?igi(xk, yk, X))
where fj are transition feature functions (here first-
order Markov independence assumption is used); gi
are observation feature functions; ?j and ?i are their
corresponding weights. To train the model for this
step, we use the best reference compression to obtain
the reference labels (as described in Section 2).
In the CRF compression model, each word is rep-
resented by a feature vector. We incorporate most
of the features used in (Liu and Liu, 2010), includ-
ing unigram, position, length of utterance, part-of-
speech tag as well as syntactic parse tree tags. We
did not use the discourse parsing tree based features
because we found they are not useful in our exper-
iments. In this work, we further expand the feature
set in order to represent the characteristics of disflu-
encies in spontaneous speech as well as model the
adjacent output labels. The additional features we
167
introduced are:
? the distance to the next same word and the next
same POS tag.
? a binary feature to indicate if there is a filled
pause or incomplete word in the following 4-
word window. We add this feature since filled
pauses or incomplete words often appear after
disfluent words.
? the combination of word/POS tag and its posi-
tion in the sentence.
? language model probabilities: the bigram prob-
ability of the current word given the previous
one, and followed by the next word, and their
product. These probabilities are obtained from
the Google Web 1T 5-gram.
? transition features: a combination of the current
output label and the previous one, together with
some observation features such as the unigram
and bigrams of word or POS tag.
3.2 Discriminative Reranking
Although CRFs is able to model the dependency
of adjacent labels, it does not measure the quality
of the whole sentence. In this work, we propose
to use discriminative training to rerank the candi-
dates generated in the first step. Reranking has been
used in many tasks to find better global solutions,
such as machine translation (Wang et al, 2007),
parsing (Charniak and Johnson, 2005), and disflu-
ency detection (Zwarts and Johnson, 2011). We use
a maximum Entropy reranker to learn distributions
over a set of candidates such that the probability of
the best compression is maximized. The conditional
probability of output y given observation x in the
maximum entropy model is defined as:
p(y|x) = 1Z(x)exp
[?k
i=1 ?if(x, y)
]
where f(x, y) are feature functions and ?i are their
weighting parameters; Z(x) is the normalization
factor.
In this reranking model, every compression can-
didate is represented by the following features:
? All the bigrams and trigrams of words and POS
tags in the candidate sentence.
? Bigrams and trigrams of words and POS tags in
the original sentence in combination with their
binary labels in the candidate sentence (delete
the word or not). For example, if the origi-
nal sentence is ?so I should go?, and the can-
didate compression sentence is ?I should go?,
then ?so I 10?, ?so I should 100? are included
in the features (1 means the word is deleted).
? The log likelihood of the candidate sentence
based on the language model.
? The absolute difference of the compression ra-
tio of the candidate sentence with that of the
first ranked candidate. This is because we try
to avoid a very large or small compression ra-
tio, and the first candidate is generally a good
candidate with reasonable length.
? The probability of the label sequence of the
candidate sentence given by the first step CRFs.
? The rank of the candidate sentence in 25 best
list.
For discriminative training using the n-best can-
didates, we need to identify the best candidate from
the n-best list, which can be either the reference
compression (if it exists on the list), or the most
similar candidate to the reference. Since we have
8 human compressions and also want to evaluate
system performance using all of them (see exper-
iments later), we try to use multiple references in
this reranking step. In order to use the same train-
ing objective (maximize the score for the single best
among all the instances), for the 25-best list, if m
reference compressions exist, we split the list into
m groups, each of which is a new sample containing
one reference as positive and several negative can-
didates. If no reference compression appears in 25-
best list, we just keep the entire list and label the in-
stance that is most similar to the best reference com-
pression as positive.
4 Experiments
We perform a cross-validation evaluation where one
meeting is used for testing and the rest of them are
used as the training set. When evaluating the system
performance, we do not consider filled pauses and
incomplete words since they can be easily identi-
fied and removed. We use two different performance
metrics in this study.
? Word-level accuracy and F1 score based on the
minor class (removed words). This was used
in (Liu and Liu, 2010). These measures are ob-
tained by comparing with the best compression.
In evaluation we map the result using ?BIO? la-
bels from the first-step compression to binary
labels that indicate a word is removed or not.
168
? BLEU score. BLEU is a widely used metric
in evaluating machine translation systems that
often use multiple references. Since there is a
great variation in human compression results,
and we have 8 reference compressions, we ex-
plore using BLEU for our sentence compres-
sion task. BLEU is calculated based on the pre-
cision of n-grams. In our experiments we use
up to 4-grams.
Table 1 shows the averaged scores of the cross
validation evaluation using the above metrics for
several methods. Also shown in the table is the com-
pression ratio of the system output. For ?reference?,
we randomly choose one compression from 8 ref-
erences, and use the rest of them as references in
calculating the BLEU score. This represents human
performance. The row ?basic features? shows the
result of using all features in (Liu and Liu, 2010)
except discourse parsing tree based features, and us-
ing binary labels (removed or not). The next row
uses this same basic feature set and ?BIO? labels.
Row ?expanded features? shows the result of our ex-
panded feature set using ?BIO? label set from the
first step of compression. The last two rows show
the results after reranking, trained using one best ref-
erence or 8 reference compressions, respectively.
accuracy F1 BLEU ratio (%)
reference 81.96 69.73 95.36 76.78
basic features (Liu
and Liu, 2010)
76.44 62.11 91.08 73.49
basic features, BIO 77.10 63.34 91.41 73.22
expanded features 79.28 67.37 92.70 72.17
reranking
train w/ 1 ref 79.01 67.74 91.90 70.60
reranking
train w/ 8 refs 78.78 63.76 94.21 77.15
Table 1: Compression results using different systems.
Our result using the basic feature set is similar to
that in (Liu and Liu, 2010) (their accuracy is 76.27%
when compression ratio is 0.7), though the experi-
mental setups are different: they used 6 meetings as
the test set while we performed cross validation. Us-
ing the ?BIO? label set instead of binary labels has
marginal improvement for the three scores. From
the table, we can see that our expanded feature set is
able to significantly improve the result, suggesting
the effectiveness of the new introduced features.
Regarding the two training settings in reranking,
we find that there is no gain from reranking when
using only one best compression, however, train-
ing with multiple references improves BLEU scores.
This indicates the discriminative training used in
maximum entropy reranking is consistent with the
performance metrics. Another reason for the per-
formance gain for this condition is that there is less
data imbalance in model training (since we split the
n-best list, each containing fewer negative exam-
ples). We also notice that the compression ratio af-
ter reranking is more similar to the reference. As
suggested in (Napoles et al, 2011), it is not appro-
priate to compare compression systems with differ-
ent compression ratios, especially when considering
grammars and meanings. Therefore for the com-
pression system without reranking, we generated re-
sults with the same compression ratio (77.15%), and
found that using reranking still outperforms this re-
sult, 1.19% higher in BLEU score.
For an analysis, we check how often our sys-
tem output contains reference compressions based
on the 8 references. We found that 50.8% of sys-
tem generated compressions appear in the 8 refer-
ences when using CRF output with a compression
ration of 77.15%; and after reranking this number
increases to 54.8%. This is still far from the oracle
result ? for 84.7% of sentences, the 25-best list con-
tains one or more reference sentences, that is, there
is still much room for improvement in the reranking
process. The results above also show that the token
level measures by comparing to one best reference
do not always correlate well with BLEU scores ob-
tained by comparing with multiple references, which
shows the need of considering multiple metrics.
5 Conclusion
This paper presents a 2-step approach for sentence
compression: we first generate an n-best list for each
source sentence using a sequence labeling method,
then rerank the n-best candidates to select the best
one based on the quality of the whole candidate sen-
tence using discriminative training. We evaluate the
system performance using different metrics. Our re-
sults show that our expanded feature set improves
the performance across multiple metrics, and rerank-
ing is able to improve the BLEU score. In future
work, we will incorporate more syntactic informa-
tion in the model to better evaluate sentence quality.
We also plan to perform a human evaluation for the
compressed sentences, and use sentence compres-
sion in summarization.
169
6 Acknowledgment
This work is partly supported by DARPA un-
der Contract No. HR0011-12-C-0016 and NSF
No. 0845484. Any opinions expressed in this ma-
terial are those of the authors and do not necessarily
reflect the views of DARPA or NSF.
References
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proceedings of the 43rd Annual Meeting
on Association for Computational Linguistics, pages
173?180, Stroudsburg, PA, USA. Proceedings of ACL.
James Clarke and Mirella Lapata. 2008. Global infer-
ence for sentence compression an integer linear pro-
gramming approach. Journal of Artificial Intelligence
Research, 31:399?429, March.
Trevor Cohn and Mirella Lapata. 2008. Sentence com-
pression beyond word deletion. In Proceedings of
COLING.
Michel Galley and Kathleen R. Mckeown. 2007. Lex-
icalized Markov grammars for sentence compression.
In Proceedings of HLT-NAACL.
Kevin Knight and Daniel Marcu. 2000. Statistics-based
summarization-step one: Sentence compression. In
Proceedings of AAAI.
Fei Liu and Yang Liu. 2009. From extractive to abstrac-
tive meeting summaries: can it be done by sentence
compression? In Proceedings of the ACL-IJCNLP.
Fei Liu and Yang Liu. 2010. Using spoken utterance
compression for meeting summarization: a pilot study.
In Proceedings of SLT.
Gabriel Murray, Steve Renals, and Jean Carletta. 2005.
Extractive summarization of meeting recordings. In
Proceedings of EUROSPEECH.
Courtney Napoles, Benjamin Van Durme, and Chris
Callison-Burch. 2011. Evaluating Sentence Com-
pression: Pitfalls and Suggested Remedies. In Pro-
ceedings of the Workshop on Monolingual Text-To-Text
Generation, pages 91?97, Portland, Oregon, June. As-
sociation for Computational Linguistics.
Wen Wang, A. Stolcke, and Jing Zheng. 2007. Rerank-
ing machine translation hypotheses with structured
and web-based language models. In Proceedings of
IEEE Workshop on Speech Recognition and Under-
standing, pages 159?164, Kyoto.
Simon Zwarts and Mark Johnson. 2011. The impact of
language models and loss functions on repair disflu-
ency detection. In Proceedings of ACL.
170
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1?10,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
A Shift-Reduce Parsing Algorithm for Phrase-based
String-to-Dependency Translation
Yang Liu
State Key Laboratory of Intelligent Technology and Systems
Tsinghua National Laboratory for Information Science and Technology
Department of Computer Science and Technology
Tsinghua University, Beijing 100084, China
liuyang2011@tsinghua.edu.cn
Abstract
We introduce a shift-reduce parsing
algorithm for phrase-based string-to-
dependency translation. As the algorithm
generates dependency trees for partial
translations left-to-right in decoding, it
allows for efficient integration of both
n-gram and dependency language mod-
els. To resolve conflicts in shift-reduce
parsing, we propose a maximum entropy
model trained on the derivation graph of
training data. As our approach combines
the merits of phrase-based and string-to-
dependency models, it achieves significant
improvements over the two baselines on
the NIST Chinese-English datasets.
1 Introduction
Modern statistical machine translation approaches
can be roughly divided into two broad categories:
phrase-based and syntax-based. Phrase-based ap-
proaches treat phrase, which is usually a sequence
of consecutive words, as the basic unit of trans-
lation (Koehn et al, 2003; Och and Ney, 2004).
As phrases are capable of memorizing local con-
text, phrase-based approaches excel at handling
local word selection and reordering. In addition,
it is straightforward to integrate n-gram language
models into phrase-based decoders in which trans-
lation always grows left-to-right. As a result,
phrase-based decoders only need to maintain the
boundary words on one end to calculate language
model probabilities. However, as phrase-based de-
coding usually casts translation as a string con-
catenation problem and permits arbitrary permuta-
tion, it proves to be NP-complete (Knight, 1999).
Syntax-based approaches, on the other hand,
model the hierarchical structure of natural lan-
guages (Wu, 1997; Yamada and Knight, 2001;
Chiang, 2005; Quirk et al, 2005; Galley et al,
2006; Liu et al, 2006; Huang et al, 2006;
Shen et al, 2008; Mi and Huang, 2008; Zhang
et al, 2008). As syntactic information can be
exploited to provide linguistically-motivated re-
ordering rules, predicting non-local permutation
is computationally tractable in syntax-based ap-
proaches. Unfortunately, as syntax-based de-
coders often generate target-language words in a
bottom-up way using the CKY algorithm, inte-
grating n-gram language models becomes more
expensive because they have to maintain target
boundary words at both ends of a partial trans-
lation (Chiang, 2007; Huang and Chiang, 2007).
Moreover, syntax-based approaches often suffer
from the rule coverage problem since syntac-
tic constraints rule out a large portion of non-
syntactic phrase pairs, which might help decoders
generalize well to unseen data (Marcu et al,
2006). Furthermore, the introduction of non-
terminals makes the grammar size significantly
bigger than phrase tables and leads to higher mem-
ory requirement (Chiang, 2007).
As a result, incremental decoding with hierar-
chical structures has attracted increasing attention
in recent years. While some authors try to inte-
grate syntax into phrase-based decoding (Galley
and Manning, 2008; Galley and Manning, 2009;
Feng et al, 2010), others develop incremental al-
gorithms for syntax-based models (Watanabe et
al., 2006; Huang and Mi, 2010; Dyer and Resnik,
2010; Feng et al, 2012). Despite these success-
ful efforts, challenges still remain for both direc-
tions. While parsing algorithms can be used to
parse partial translations in phrase-based decod-
ing, the search space is significantly enlarged since
there are exponentially many parse trees for expo-
nentially many translations. On the other hand, al-
though target words can be generated left-to-right
by altering the way of tree transversal in syntax-
based models, it is still difficult to reach full rule
coverage as compared with phrase table.
1
zongtong jiang yu siyue lai lundun fangwen
The President will visit London in April
source phrase target phrase dependency category
r1 fangwen visit {} fixed
r2 yu siyue in April {1? 2} fixed
r3 zongtong jiang The President will {2? 1} floating left
r4 yu siyue lai lundun London in April {2? 3} floating right
r5 zongtong jiang President will {} ill-formed
Figure 1: A training example consisting of a (romanized) Chinese sentence, an English dependency
tree, and the word alignment between them. Each translation rule is composed of a source phrase, a
target phrase with a set of dependency arcs. Following Shen et al (2008), we distinguish between fixed,
floating, and ill-formed structures.
In this paper, we propose a shift-reduce parsing
algorithm for phrase-based string-to-dependency
translation. The basic unit of translation in our
model is string-to-dependency phrase pair, which
consists of a phrase on the source side and a depen-
dency structure on the target side. The algorithm
generates well-formed dependency structures for
partial translations left-to-right using string-to-
dependency phrase pairs. Therefore, our approach
is capable of combining the advantages of both
phrase-based and syntax-based approaches:
1. compact rule table: our rule table is a subset
of the original string-to-dependency gram-
mar (Shen et al, 2008; Shen et al, 2010) by
excluding rules with non-terminals.
2. full rule coverage: all phrase pairs, both
syntactic and non-syntactic, can be used in
our algorithm. This is the same with Moses
(Koehn et al, 2007).
3. efficient integration of n-gram language
model: as translation grows left-to-right in
our algorithm, integrating n-gram language
models is straightforward.
4. exploiting syntactic information: as the
shift-reduce parsing algorithm generates tar-
get language dependency trees in decoding,
dependency language models (Shen et al,
2008; Shen et al, 2010) can be used to en-
courage linguistically-motivated reordering.
5. resolving local parsing ambiguity: as de-
pendency trees for phrases are memorized
in rules, our approach avoids resolving local
parsing ambiguity and explores in a smaller
search space than parsing word-by-word on
the fly in decoding (Galley and Manning,
2009).
We evaluate our method on the NIST Chinese-
English translation datasets. Experiments show
that our approach significantly outperforms both
phrase-based (Koehn et al, 2007) and string-to-
dependency approaches (Shen et al, 2008) in
terms of BLEU and TER.
2 Shift-Reduce Parsing for Phrase-based
String-to-Dependency Translation
Figure 1 shows a training example consisting of
a (romanized) Chinese sentence, an English de-
pendency tree, and the word alignment between
them. Following Shen et al (2008), string-to-
dependency rules without non-terminals can be
extracted from the training example. As shown
in Figure 1, each rule is composed of a source
phrase and a target dependency structure. Shen et
al. (2008) divide dependency structures into two
broad categories:
1. well-formed
(a) fixed: the head is known or fixed;
2
0 ? ? ? ? ? ? ?
1 S r3 [The President will] ? ? ? ? ? ? ?
2 S r1 [The President will] [visit] ? ? ? ? ? ? ?
3 Rl [The President will visit] ? ? ? ? ? ? ?
4 S r4 [The President will visit] [London in April] ? ? ? ? ? ? ?
5 Rr [The President will visit London in April] ? ? ? ? ? ? ?
step action rule stack coverage
Figure 2: Shift-reduce parsing with string-to-dependency phrase pairs. For each state, the algorithm
maintains a stack to store items (i.e., well-formed dependency structures). At each step, it chooses one
action to extend a state: shift (S), reduce left (Rl), or reduce right (Rr). The decoding process terminates
when all source words are covered and there is a complete dependency tree in the stack.
(b) floating: sibling nodes of a common
head, but the head itself is unspecified
or floating. Each of the siblings must be
a complete constituent.
2. ill-formed: neither fixed nor floating.
We further distinguish between left and right
floating structures according to the position of
head. For example, as ?The President will? is the
left dependant of its head ?visit?, it is a left floating
structure.
To integrate the advantages of phrase-based
and string-to-dependency models, we propose a
shift-reduce algorithm for phrase-based string-to-
dependency translation.
Figure 2 shows an example. We describe a state
(i.e., parser configuration) as a tuple ?S, C? where
S is a stack that stores items and C is a cover-
age vector that indicates which source words have
been translated. Each item s ? S is a well-formed
dependency structure. The algorithm starts with
an empty state. At each step, it chooses one of the
three actions (Huang et al, 2009) to extend a state:
1. shift (S): move a target dependency structure
onto the stack;
2. reduce left (Rl): combine the two items on
the stack, st and st?1 (t ? 2), with the root of
st as the head and replace them with a com-
bined item;
3. reduce right (Rr): combine the two items on
the stack, st and st?1 (t ? 2), with the root
of st?1 as the head and replace them with a
combined item.
The decoding process terminates when all source
words are covered and there is a complete depen-
dency tree in the stack.
Note that unlike monolingual shift-reduce
parsers (Nivre, 2004; Zhang and Clark, 2008;
Huang et al, 2009), our algorithm does not main-
tain a queue for remaining words of the input be-
cause the future dependency structure to be shifted
is unknown in advance in the translation scenario.
Instead, we use a coverage vector on the source
side to determine when to terminate the algorithm.
For an input sentence of J words, the number of
actions is 2K ? 1, where K is the number of rules
used in decoding. 1 There are always K shifts and
1Empirically, we find that the average number of stacks
for J words is about 1.5 ? J on the Chinese-English data.
3
[The President] [will] [visit]
[The President] [will] [visit] [London]
[The President] [will] [visit London]
[The President] [will visit London]
[The President] [will visit]
[The President will visit]
[The President will visit] [London]
[The President will visit London]
S
Rr
Rl
Rl
Rl
Rl
S
Rr
Figure 3: Ambiguity in shift-reduce parsing.
st?1 st legal action(s)
yes S
h yes S
l yes S
r no
h h yes S, Rl, Rr
h l yes S
h r yes Rr
l h yes Rl
l l yes S
l r no
r h no
r l no
r r no
Table 1: Conflicts in shift-reduce parsing. st and
st?1 are the top two items in the stack of a state.
We use ?h? to denote fixed structure, ?l? to de-
note left floating structure, and ?r? to denote right
floating structure. It is clear that only ?h+h? is am-
biguous.
K ? 1 reductions.
It is easy to verify that the reduce left and re-
duce right actions are equivalent to the left adjoin-
ing and right adjoining operations defined by Shen
et al (2008). They suffice to operate on well-
formed structures and produce projective depen-
dency parse trees.
Therefore, with dependency structures present
in the stacks, it is possible to use dependency lan-
guage models to encourage linguistically plausible
phrase reordering.
3 A Maximum Entropy Based
Shift-Reduce Parsing Model
Shift-reduce parsing is efficient but suffers from
parsing errors caused by syntactic ambiguity. Fig-
ure 3 shows two (partial) derivations for a depen-
dency tree. Consider the item on the top, the algo-
rithm can either apply a shift action to move a new
item or apply a reduce left action to obtain a big-
ger structure. This is often referred to as conflict
in the shift-reduce dependency parsing literature
(Huang et al, 2009). In this work, the shift-reduce
parser faces four types of conflicts:
1. shift vs. shift;
2. shift vs. reduce left;
3. shift vs. reduce right;
4. reduce left vs. reduce right.
Fortunately, if we distinguish between left and
right floating structures, it is possible to rule out
most conflicts. Table 1 shows the relationship
between conflicts, dependency structures and ac-
tions. We use st and st?1 to denote the top two
4
[The President will visit London][in April]
DT NNP MD VB NNP IN IN
type feature templates
Unigram c Wh(st) Wh(st?1)
Wlc(st) Wrc(st?1) Th(st)
Th(st?1) Tlc(st) Trc(st?1)
Bigram Wh(st) ?Wh(st?1) Th(St) ? Th(st?1) Wh(st) ? Th(st)
Wh(st?1) ? Th(st?1) Wh(st) ?Wrc(st?1) Wh(st?1) ?Wlc(st)
Trigram c ?Wh(st) ?W (st?1) c ? Th(st) ? Th(st?1) Wh(st) ?Wh(st?1) ? Tlc(st)
Wh(st) ?Wh(st?1) ? Trc(st?1) Th(st) ? Th(st?1) ? Tlc(st) Th(st) ? Th(st?1) ? Trc(st?1)
Figure 4: Feature templates for maximum entropy based shift-reduce parsing model. c is a boolean
value that indicate whether all source words are covered (shift is prohibited if true), Wh(?) and Th(?)
are functions that get the root word and tag of an item, Wlc(?) and Tlc(?) returns the word and tag of
the left most child of the root, Wrc(?) amd Trc(?) returns the word and tag of the right most child of the
root. Symbol ? denotes feature conjunction. In this example, c = true, Wh(st) = in, Th(st) = IN,
Wh(st?1) = visit, Wlc(st?1) = London.
items in the stack. ?h? stands for fixed struc-
ture, ?l? for left floating structure, and ?r? for right
floating structure. If the stack is empty, the only
applicable action is shift. If there is only one item
in the stack and the item is either fixed or left float-
ing, the only applicable action is shift. Note that it
is illegal to shift a right floating structure onto an
empty stack because it will never be reduced. If
the stack contains at least two items, only ?h+h?
is ambiguous and the others are either unambigu-
ous or illegal. Therefore, we only need to focus on
how to resolve conflicts for the ?h+h? case (i.e.,
the top two items in a stack are both fixed struc-
tures).
We propose a maximum entropy model to re-
solve the conflicts for ?h+h?: 2
P?(a|c, st, st?1) =
exp(? ? h(a, c, st, st?1))?
a exp(? ? h(a, c, st, st?1))
where a ? {S,Rl, Rr} is an action, c is a boolean
value that indicates whether all source words are
covered (shift is prohibited if true), st and st?1
are the top two items on the stack, h(a, c, st, st?1)
is a vector of binary features and ? is a vector of
feature weights.
Figure 4 shows the feature templates used in our
experiments. Wh(?) and Th(?) are functions that
get the root word and tag of an item, Wlc(?) and
Tlc(?) returns the word and tag of the left most
child of the root, Wrc(?) and Trc(?) returns the
2The shift-shift conflicts always exist because there are
usually multiple rules that can be shifted. This can be re-
volved using standard features in phrase-based models.
word and tag of the right most child of the root.
In this example, c = true, Wh(st) = in, Th(st) =
IN, Wh(st?1) = visit, Wlc(st?1) = London.
To train the model, we need an ?oracle? or gold-
standard action sequence for each training exam-
ple. Unfortunately, such oracle turns out to be
non-unique even for monolingual shift-reduce de-
pendency parsing (Huang et al, 2009). The situ-
ation for phrase-based shift-reduce parsing aggra-
vates because there are usually multiple ways of
segmenting sentence into phrases.
To alleviate this problem, we introduce a struc-
ture called derivation graph to compactly repre-
sent all derivations of a training example. Figure 3
shows a (partial) derivation graph, in which a node
corresponds to a state and an edge corresponds to
an action. The graph begins with an empty state
and ends with the given training example.
More formally, a derivation graph is a directed
acyclic graph G = ?V,E? where V is a set of
nodes and E is a set of edges. Each node v cor-
responds to a state in the shift-reduce parsing pro-
cess. There are two distinguished nodes: v0, the
staring empty state, and v|V |, the ending com-
pleted state. Each edge e = (a, i, j) transits node
vi to node vj via an action a ? {S,Rl, Rr}.
To build the derivation graph, our algorithm
starts with an empty state and iteratively extends
an unprocessed state until reaches the completed
state. During the process, states that violate the
training example are discarded. Even so, there are
still exponentially many states for a training exam-
ple, especially for long sentences. Fortunately, we
5
Algorithm 1 Beam-search shift-reduce parsing.
1: procedure PARSE(f )
2: V ? ?
3: ADD(v0, V[0])
4: k ? 0
5: while V[k] 6= ? do
6: for all v ? V[k] do
7: for all a ? {S,Rl, Rr} do
8: EXTEND(f , v, a, V)
9: end for
10: end for
11: k ? k + 1
12: end while
13: end procedure
only need to focus on ?h+h? states. In addition,
we follow Huang et al (2009) to use the heuristic
of ?shortest stack? to always prefer Rl to S.
4 Decoding
Our decoder is based on a linear model (Och,
2003) with the following features:
1. relative frequencies in two directions;
2. lexical weights in two directions;
3. phrase penalty;
4. distance-based reordering model;
5. lexicaized reordering model;
6. n-gram language model model;
7. word penalty;
8. ill-formed structure penalty;
9. dependency language model;
10. maximum entropy parsing model.
In practice, we extend deterministic shift-
reduce parsing with beam search (Zhang and
Clark, 2008; Huang et al, 2009). As shown in Al-
gorithm 1, the algorithm maintains a list of stacks
V and each stack groups states with the same num-
ber of accumulated actions (line 2). The stack list
V initializes with an empty state v0 (line 3). Then,
the states in the stack are iteratively extended un-
til there are no incomplete states (lines 4-12). The
search space is constrained by discarding any state
that has a score worse than:
1. ? multiplied with the best score in the stack,
or
2. the score of b-th best state in the stack.
As the stack of a state keeps changing during the
decoding process, the context information needed
to calculate dependency language model and max-
imum entropy model probabilities (e.g., root word,
leftmost child, etc.) changes dynamically as well.
As a result, the chance of risk-free hypothesis re-
combination (Koehn et al, 2003) significantly de-
creases because complicated contextual informa-
tion is much less likely to be identical.
Therefore, we use hypergraph reranking
(Huang and Chiang, 2007; Huang, 2008), which
proves to be effective for integrating non-local
features into dynamic programming, to alleviate
this problem. The decoding process is divided
into two passes. In the first pass, only standard
features (i.e., features 1-7 in the list in the
beginning of this section) are used to produce
a hypergraph. 3 In the second pass, we use the
hypergraph reranking algorithm (Huang, 2008) to
find promising translations using additional de-
pendency features (i.e., features 8-10 in the list).
As hypergraph is capable of storing exponentially
many derivations compactly, the negative effect of
propagating mistakes made in the first pass to the
second pass can be minimized.
To improve rule coverage, we follow Shen et
al. (2008) to use ill-formed structures in decoding.
If an ill-formed structure has a single root, it can
treated as a (pseudo) fixed structure; otherwise it is
transformed to one (pseudo) left floating structure
and one (pseudo) right floating structure. We use
a feature to count how many ill-formed structures
are used in decoding.
5 Experiments
We evaluated our phrase-based string-to-
dependency translation system on Chinese-
English translation. The training data consists
of 2.9M pairs of sentences with 76.0M Chinese
words and 82.2M English words. We used the
Stanford parser (Klein and Manning, 2003) to
get dependency trees for English sentences. We
used the SRILM toolkit (Stolcke, 2002) to train a
3Note that the first pass does not work like a phrase-based
decoder because it yields dependency trees on the target side.
A uniform model (i.e., each action has a fixed probability of
1/3) is used to resolve ?h+h? conflicts.
6
MT02 (tune) MT03 MT04 MT05system BLEU TER BLEU TER BLEU TER BLEU TER
phrase 34.88 57.00 33.82 57.19 35.48 56.48 32.52 57.62
dependency 35.23 56.12 34.20 56.36 36.01 55.55 33.06 56.94
this work 35.71?? 55.87?? 34.81??+ 55.94??+ 36.37?? 55.02??+ 33.53?? 56.58??
Table 2: Comparison with Moses (Koehn et al, 2007) and a re-implementation of the bottom-up string-
to-dependency decoder (Shen et al, 2008) in terms of uncased BLEU and TER. We use randomiza-
tion test (Riezler and Maxwell, 2005) to calculate statistical significance. *: significantly better than
Moses (p < 0.05), **: significantly better than Moses (p < 0.01), +: significantly better than string-to-
dependency (p < 0.05), ++: significantly better than string-to-dependency (p < 0.01).
features BLEU TER
standard 34.79 56.93
+ depLM 35.29? 56.17??
+ maxent 35.40?? 56.09??
+ depLM & maxent 35.71?? 55.87??
Table 3: Contribution of maximum entropy shift-
reduce parsing model. ?standard? denotes us-
ing standard features of phrase-based system.
Adding dependency language model (?depLM?)
and the maximum entropy shift-reduce parsing
model (?maxent?) significantly improves BLEU
and TER on the development set, both separately
and jointly.
4-gram language model on the Xinhua portion of
the GIGAWORD coprus, which contians 238M
English words. A 3-gram dependency language
model was trained on the English dependency
trees. We used the 2002 NIST MT Chinese-
English dataset as the development set and the
2003-2005 NIST datasets as the testsets. We
evaluated translation quality using uncased BLEU
(Papineni et al, 2002) and TER (Snover et al,
2006). The features were optimized with respect
to BLEU using the minimum error rate training
algorithm (Och, 2003).
We chose the following two systems that are
closest to our work as baselines:
1. The Moses phrase-based decoder (Koehn et
al., 2007).
2. A re-implementation of bottom-up string-to-
dependency decoder (Shen et al, 2008).
All the three systems share with the same target-
side parsed, word-aligned training data. The his-
togram pruning parameter b is set to 100 and
rules coverage BLEU TER
well-formed 44.87 34.42 57.35
all 100.00 35.71?? 55.87??
Table 4: Comparison of well-formed and ill-
formed structures. Using all rules significantly
outperforms using only well-formed structures.
BLEU and TER scores are calculated on the de-
velopment set.
phrase table limit is set to 20 for all the three sys-
tems. Moses shares the same feature set with our
system except for the dependency features. For the
bottom-up string-to-dependency system, we in-
cluded both well-formed and ill-formed structures
in chart parsing. To control the grammar size, we
only extracted ?tight? initial phrase pairs (i.e., the
boundary words of a phrase must be aligned) as
suggested by (Chiang, 2007). For our system, we
used the Le Zhang?s maximum entropy modeling
toolkit to train the shift-reduce parsing model after
extracting 32.6M events from the training data. 4
We set the iteration limit to 100. The accuracy on
the training data is 90.18%.
Table 2 gives the performance of Moses, the
bottom-up string-to-dependency system, and our
system in terms of uncased BLEU and TER
scores. From the same training data, Moses
extracted 103M bilingual phrases, the bottom-
up string-to-dependency system extracted 587M
string-to-dependency rules, and our system ex-
tracted 124M phrase-based dependency rules. We
find that our approach outperforms both baselines
systematically on all testsets. We use randomiza-
tion test (Riezler and Maxwell, 2005) to calculate
statistical significance. As our system can take full
advantage of lexicalized reordering and depen-
4http://homepages.inf.ed.ac.uk/lzhang10/maxent.html
7
30.50
31.00
31.50
32.00
32.50
33.00
33.50
34.00
34.50
 0  2  4  6  8  10  12
B
LE
U
distortion limit
this work
Moses
Figure 5: Performance of Moses and our system
with various distortion limits.
dency language models without loss in rule cov-
erage, it achieves significantly better results than
Moses on all test sets. The gains in TER are much
larger than BLEU because dependency language
models do not model n-grams directly. Compared
with the bottom-up string-to-dependency system,
our system outperforms consistently but not sig-
nificantly in all cases. The average decoding time
for Moses is 3.67 seconds per sentence, bottom-
up string-to-dependency is 13.89 seconds, and our
system is 4.56 seconds.
Table 3 shows the effect of hypergraph rerank-
ing. In the first pass, our decoder uses standard
phrase-based features to build a hypergraph. The
BLEU score is slightly lower than Moses with the
same configuration. One possible reason is that
our decoder organizes stacks with respect to ac-
tions, whereas Moses groups partial translations
with the same number of covered source words in
stacks. In the second pass, our decoder reranks
the hypergraph with additional dependency fea-
tures. We find that adding dependency language
and maximum entropy shift-reduce models consis-
tently brings significant improvements, both sepa-
rately and jointly.
We analyzed translation rules extracted from the
training data. Among them, well-formed struc-
tures account for 43.58% (fixed 33.21%, float-
ing left 9.01%, and floating right 1.36%) and ill-
formed structures 56.42%. As shown in Table
4, using all rules clearly outperforms using only
well-formed structures.
Figure 5 shows the performance of Moses and
our system with various distortion limits on the
development set. Our system consistently outper-
forms Moses in all cases, suggesting that adding
dependency helps improve phrase reordering.
6 Related Work
The work of Galley and Manning (2009) is clos-
est in spirit to ours. They introduce maximum
spanning tree (MST) parsing (McDonald et al,
2005) into phrase-based translation. The system
is phrase-based except that an MST parser runs to
parse partial translations at the same time. One
challenge is that MST parsing itself is not incre-
mental, making it expensive to identify loops dur-
ing hypothesis expansion. On the contrary, shift-
reduce parsing is naturally incremental and can
be seamlessly integrated into left-to-right phrase-
based decoding. More importantly, in our work
dependency trees are memorized for phrases rather
than being generated word by word on the fly in
decoding. This treatment might not only reduce
decoding complexity but also potentially revolve
local parsing ambiguity.
Our decoding algorithm is similar to Gimpel
and Smith (2011)?s lattice parsing algorithm as we
divide decoding into two steps: hypergraph gener-
ation and hypergraph rescoring. The major differ-
ence is that our hypergraph is not a phrasal lat-
tice because each phrase pair is associated with
a dependency structure on the target side. In
other words, our second pass is to find the Viterbi
derivation with addition features rather than pars-
ing the phrasal lattice. In addition, their algorithm
produces phrasal dependency parse trees while the
leaves of our dependency trees are words, making
dependency language models can be directly used.
Shift-reduce parsing has been successfully used
in phrase-based decoding but limited to adding
structural constraints. Galley and Manning (2008)
propose a shift-reduce algorithm to integrate a hi-
erarchical reordering model into phrase-based sys-
tems. Feng et al (2010) use shift-reduce parsing
to impose ITG (Wu, 1997) constraints on phrase
permutation. Our work differs from theirs by go-
ing further to incorporate linguistic syntax into
phrase-based decoding.
Along another line, a number of authors have
developed incremental algorithms for syntax-
based models (Watanabe et al, 2006; Huang and
Mi, 2010; Dyer and Resnik, 2010; Feng et al,
2012). Watanabe et al (2006) introduce an Early-
style top-down parser based on binary-branching
Greibach Normal Form. Huang et al (2010), Dyer
8
and Resnik (2010), and Feng et al (2012) use dot-
ted rules to change the tree transversal to gener-
ate target words left-to-right, either top-down or
bottom-up.
7 Conclusion
We have presented a shift-reduce parsing al-
gorithm for phrase-based string-to-dependency
translation. The algorithm generates depen-
dency structures incrementally using string-to-
dependency phrase pairs. Therefore, our ap-
proach is capable of combining the advantages of
both phrase-based and string-to-dependency mod-
els, it outperforms the two baselines on Chinese-
to-English translation.
In the future, we plan to include more con-
textual information (e.g., the uncovered source
phrases) in the maximum entropy model to re-
solve conflicts. Another direction is to adapt
the dynamic programming algorithm proposed by
Huang and Sagae (2010) to improve our string-to-
dependency decoder. It is also interesting to com-
pare with applying word-based shift-reduce pars-
ing to phrase-based decoding similar to (Galley
and Manning, 2009).
Acknowledgments
This research is supported by the 863 Program
under the grant No 2012AA011102 and No.
2011AA01A207, by the Singapore National Re-
search Foundation under its International Re-
search Centre @ Singapore Funding Initiative and
administered by the IDM Programme Office, and
by a Research Fund No. 20123000007 from Ts-
inghua MOE-Microsoft Joint Laboratory.
References
David Chiang. 2005. A hiearchical phrase-based
model for statistical machine translation. In Proc.
of ACL 2005.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Chris Dyer and Philip Resnik. 2010. Context-free re-
ordering, finite-state translation. In Proc. of NAACL
2010.
Yang Feng, Haitao Mi, Yang Liu, and Qun Liu.
2010. An efficient shift-reduce decoding algorithm
for phrased-based machine translation. In Proc. of
COLING 2010.
Yang Feng, Yang Liu, Qun Liu, and Trevor Cohn.
2012. Left-to-right tree-to-string decoding with pre-
diction. In Proc. of EMNLP 2012.
Michel Galley and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. In Proc. of EMNLP 2008.
Michel Galley and Christopher D. Manning. 2009.
Quadratic-time dependency parsing for machine
translation. In Proc. of ACL 2009.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proc.
of ACL 2006.
Kevin Gimpel and Noah A. Smith. 2011. Quasi-
synchronous phrase dependency grammars for ma-
chine translation. In Proc. of EMNLP 2011.
Liang Huang and David Chiang. 2007. Forest rescor-
ing: Faster decoding with integrated language mod-
els. In Proc. of ACL 2007.
Liang Huang and Haitao Mi. 2010. Efficient incre-
mental decoding for tree-to-string translation. In
Proc. of EMNLP 2010.
Liang Huang and Kenji Sagae. 2010. Dynamic pro-
gramming for linear-time incremental parsing. In
Proc. of ACL 2010.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proc. of AMTA 2006.
Liang Huang, Wenbin Jiang, and Qun Liu. 2009.
Bilingually-constrained (monolingual) shift-reduce
parsing. In Proc. of EMNLP 2009.
Liang Huang. 2008. Forest reranking: Discrimina-
tive parsing with non-local features. In Proc. of ACL
2008.
Dan Klein and Christopher Manning. 2003. Accurate
unlexicalized parsing. In Proc. of ACL 2003.
Kevin Knight. 1999. Decoding complexity in word-
replacement translation models. Computational
Linguistics.
Philipp Koehn, Franz Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proc. of
NAACL 2003.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proc. of ACL 2007.
9
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment template for statistical machine
translation. In Proc. of ACL 2006.
Daniel Marcu, Wei Wang, Abdessamad Echihabi, and
Kevin Knight. 2006. Spmt: Statistical machine
translation with syntactified target language phrases.
In Proc. of EMNLP 2006.
R. McDonald, F. Pereira, K. Ribarov, and J. Hajic.
2005. Non-projective dependency parsing using
spanning tree algorithms. In Proc. of EMNLP 2005.
HaitaoMi and Liang Huang. 2008. Forest-based trans-
lation. In Proc. of ACL 2008.
Joakim Nivre. 2004. Incrementality in deterministic
dependency parsing. In Proc. of ACL 2004 Work-
shop Incremental Parsing: Bringning Engineering
and Cognition Together.
Franz Och and Hermann Ney. 2004. The alignment
template approach to statistical machine translation.
Computational Linguistics, 30(4).
Franz Och. 2003. Minimum error rate training in sta-
tistical machine translation. In Proc. of ACL 2003.
Kishore Papineni, Salim Roukos, ToddWard, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proc. of ACL 2002.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005.
Dependency treelet translation: Syntactically in-
formed phrasal smt. In Proc. of ACL 2005.
S. Riezler and J. Maxwell. 2005. On some pitfalls
in automatic evaluation and significance testing for
mt. In Proc. of ACL 2005 Workshop on Intrinsic and
Extrinsic Evaluation Measures for Machine Trans-
lation and/or Summarization.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proc. of ACL 2008.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2010.
String-to-dependency statistical machine transla-
tion. Computational Linguistics, 36(4).
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proc. of AMTA 2006.
Andreas Stolcke. 2002. Srilm - an extensible language
modeling toolkit. In Proc. of ICSLP 2002.
Taro Watanabe, Hajime Tsukuda, and Hideki Isozaki.
2006. Left-to-right target generation for hierarchical
phrase-based translation. In Proc. of ACL 2006.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics.
Kenji Yamada and Kevin Knight. 2001. A syntax-
based statistical translation model. In Proc. of ACL
2001.
Yue Zhang and Stephen Clark. 2008. A tale of
two parsers: investigating and combining graph-
based and transition-based dependency parsing us-
ing beam search. In Proc. of EMNLP 2008.
Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li,
Chew Lim Tan, and Sheng Li. 2008. A tree
sequence alignment-based tree-to-tree translation
model. In Proc. of ACL 2008.
10
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 852?861,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Statistical Machine Translation Improves Question Retrieval in
Community Question Answering via Matrix Factorization
Guangyou Zhou, Fang Liu, Yang Liu, Shizhu He, and Jun Zhao
National Laboratory of Pattern Recognition
Institute of Automation, Chinese Academy of Sciences
95 Zhongguancun East Road, Beijing 100190, China
{gyzhou,fliu,liuyang09,shizhu.he,jzhao}@nlpr.ia.ac.cn
Abstract
Community question answering (CQA)
has become an increasingly popular re-
search topic. In this paper, we focus on the
problem of question retrieval. Question
retrieval in CQA can automatically find
the most relevant and recent questions that
have been solved by other users. However,
the word ambiguity and word mismatch
problems bring about new challenges for
question retrieval in CQA. State-of-the-art
approaches address these issues by implic-
itly expanding the queried questions with
additional words or phrases using mono-
lingual translation models. While use-
ful, the effectiveness of these models is
highly dependent on the availability of
quality parallel monolingual corpora (e.g.,
question-answer pairs) in the absence of
which they are troubled by noise issue.
In this work, we propose an alternative
way to address the word ambiguity and
word mismatch problems by taking advan-
tage of potentially rich semantic informa-
tion drawn from other languages. Our pro-
posed method employs statistical machine
translation to improve question retrieval
and enriches the question representation
with the translated words from other lan-
guages via matrix factorization. Experi-
ments conducted on a real CQA data show
that our proposed approach is promising.
1 Introduction
With the development of Web 2.0, community
question answering (CQA) services like Yahoo!
Answers,1 Baidu Zhidao2 and WkiAnswers3 have
attracted great attention from both academia and
industry (Jeon et al, 2005; Xue et al, 2008;
Adamic et al, 2008; Wang et al, 2009; Cao et al,
2010). In CQA, anyone can ask and answer ques-
tions on any topic, and people seeking information
are connected to those who know the answers. As
answers are usually explicitly provided by human,
they can be helpful in answering real world ques-
tions.
In this paper, we focus on the task of question
retrieval. Question retrieval in CQA can automati-
cally find the most relevant and recent questions
(historical questions) that have been solved by
other users, and then the best answers of these his-
torical questions will be used to answer the users?
queried questions. However, question retrieval is
challenging partly due to the word ambiguity and
word mismatch between the queried questions
and the historical questions in the archives. Word
ambiguity often causes the retrieval models to re-
trieve many historical questions that do not match
the users? intent. This problem is also amplified
by the high diversity of questions and users. For
example, depending on different users, the word
?interest? may refer to ?curiosity?, or ?a charge
for borrowing money?.
Another challenge is word mismatch between
the queried questions and the historical questions.
The queried questions may contain words that are
different from, but related to, the words in the rele-
vant historical questions. For example, if a queried
question contains the word ?company? but a rele-
vant historical question instead contains the word
?firm?, then there is a mismatch and the historical
1http://answers.yahoo.com/
2http://zhidao.baidu.com/
3http://wiki.answers.com/
852
English Chinese
word ambiguity
How do I get a loan ?(w?)??(r?h?)?(c?ng)
from a bank? ??(y?nh?ng)??(d?iku?n)?
How to reach the ??(r?h?)??(qi?nw?ng)
bank of the river? ??(h??n)?
word mismatch
company ??(g?ngs?)
firm ??(g?ngs?)
rheum ??(g?nm?o)
catarrh ??(g?nm?o)
Table 1: Google translate: some illustrative examples.
question may not be easily distinguished from an
irrelevant one.
Researchers have proposed the use of word-
based translation models (Berger et al, 2000;
Jeon et al, 2005; Xue et al, 2008; Lee et al,
2008; Bernhard and Gurevych, 2009) to solve
the word mismatch problem. As a principle ap-
proach to capture semantic word relations, word-
based translation models are built by using the
IBM model 1 (Brown et al, 1993) and have
been shown to outperform traditional models (e.g.,
VSM, BM25, LM) for question retrieval. Be-
sides, Riezler et al (2007) and Zhou et al (2011)
proposed the phrase-based translation models for
question and answer retrieval. The basic idea is
to capture the contextual information in model-
ing the translation of phrases as a whole, thus
the word ambiguity problem is somewhat allevi-
ated. However, all these existing studies in the
literature are basically monolingual approaches
which are restricted to the use of original language
of questions. While useful, the effectiveness of
these models is highly dependent on the availabil-
ity of quality parallel monolingual corpora (e.g.,
question-answer pairs) in the absence of which
they are troubled by noise issue. In this work,
we propose an alternative way to address the word
ambiguity and word mismatch problems by taking
advantage of potentially rich semantic information
drawn from other languages. Through other lan-
guages, various ways of adding semantic informa-
tion to a question could be available, thereby lead-
ing to potentially more improvements than using
the original language only.
Taking a step toward using other languages, we
propose the use of translated representation by al-
ternatively enriching the original questions with
the words from other languages. The idea of im-
proving question retrieval with statistical machine
translation is based on the following two observa-
tions: (1) Contextual information is exploited dur-
ing the translation from one language to another.
For example in Table 1, English words ?interest?
and ?bank? that have multiple meanings under
different contexts are correctly addressed by us-
ing the state-of-the-art translation tool ??Google
Translate.4 Thus, word ambiguity based on con-
textual information is naturally involved when
questions are translated. (2) Multiple words that
have similar meanings in one language may be
translated into an unique word or a few words in a
foreign language. For example in Table 1, English
words such as ?company? and ?firm? are trans-
lated into ??? (g?ngs?)?, ?rheum? and ?catarrh?
are translated into ???(g?nm?o)? in Chinese.
Thus, word mismatch problem can be somewhat
alleviated by using other languages.
Although Zhou et al (2012) exploited bilin-
gual translation for question retrieval and obtained
the better performance than traditional monolin-
gual translation models. However, there are two
problems with this enrichment: (1) enriching
the original questions with the translated words
from other languages increases the dimensionality
and makes the question representation even more
sparse; (2) statistical machine translation may in-
troduce noise, which can harm the performance of
question retrieval. To solve these two problems,
we propose to leverage statistical machine transla-
tion to improve question retrieval via matrix fac-
torization.
The remainder of this paper is organized as fol-
lows. Section 2 describes the proposed method
by leveraging statistical machine translation to im-
prove question retrieval via matrix factorization.
Section 3 presents the experimental results. In sec-
tion 4, we conclude with ideas for future research.
4http://translate.google.com/translate t
853
2 Our Approach
2.1 Problem Statement
This paper aims to leverage statistical machine
translation to enrich the question representation.
In order to address the word ambiguity and word
mismatch problems, we expand a question by
adding its translation counterparts. Statistical ma-
chine translation (e.g., Google Translate) can uti-
lize contextual information during the question
translation, so it can solve the word ambiguity and
word mismatch problems to some extent.
Let L = {l1, l2, . . . , lP } denote the language
set, where P is the number of languages con-
sidered in the paper, l1 denotes the original lan-
guage (e.g., English) while l2 to lP are the for-
eign languages. Let D1 = {d(1)1 , d(1)2 , . . . , d(1)N }
be the set of historical question collection in origi-
nal language, where N is the number of historical
questions in D1 with vocabulary size M1. Now
we first translate each original historical question
from language l1 into other languages lp (p ?
[2, P ]) by Google Translate. Thus, we can ob-
tain D2, . . . , DP in different languages, and Mp is
the vocabulary size of Dp. A question d(p)i in Dp
is simply represented as a Mp dimensional vector
d(p)i , in which each entry is calculated by tf-idf.
The N historical questions in Dp are then repre-
sented in a Mp ? N term-question matrix Dp =
{d(p)1 ,d
(p)
2 , . . . ,d
(p)
N }, in which each row corre-
sponds to a term and each column corresponds to
a question.
Intuitively, we can enrich the original ques-
tion representation by adding the translated words
from language l2 to lP , the original vocabu-
lary size is increased from M1 to ?Pp=1 Mp.
Thus, the term-question matrix becomes D =
{D1,D2, . . . ,DP } and D ? R(
?P
p=1 Mp)?N .
However, there are two problems with this enrich-
ment: (1) enriching the original questions with the
translated words from other languages makes the
question representation even more sparse; (2) sta-
tistical machine translation may introduce noise.5
To solve these two problems, we propose to
leverage statistical machine translation to improve
question retrieval via matrix factorization. Figure
1 presents the framework of our proposed method,
where qi represents a queried question, and qi is a
vector representation of qi.
5Statistical machine translation quality is far from satis-
factory in real applications.
??
??
??
??
 
HistoricalQuestionCollectionRepresentation
 
QueryRepresentation
Figure 1: Framework of our proposed approach
for question retrieval.
2.2 Model Formulation
To tackle the data sparseness of question represen-
tation with the translated words, we hope to find
two or more lower dimensional matrices whose
product provides a good approximate to the orig-
inal one via matrix factorization. Previous stud-
ies have shown that there is psychological and
physiological evidence for parts-based representa-
tion in the human brain (Wachsmuth et al, 1994).
The non-negative matrix factorization (NMF) is
proposed to learn the parts of objects like text
documents (Lee and Seung, 2001). NMF aims
to find two non-negative matrices whose product
provides a good approximation to the original ma-
trix and has been shown to be superior to SVD in
document clustering (Xu et al, 2003; Tang et al,
2012).
In this paper, NMF is used to induce the reduced
representation Vp of Dp, Dp is independent on
{D1,D2, . . . ,Dp?1,Dp+1, . . . ,DP }. When ig-
noring the coupling between Vp, it can be solved
by minimizing the objective function as follows:
O1(Up,Vp) = minUp?0,Vp?0 ?Dp ?UpVp?
2
F (1)
where ? ? ?F denotes Frobenius norm of a matrix.
Matrices Up ? RMp?K and Vp ? RK?N are the
reduced representation for terms and questions in
the K dimensional space, respectively.
To reduce the noise introduced by statistical ma-
chine translation, we assume that Vp from lan-
guage Dp (p ? [2, P ]) should be close to V1
854
from the original language D1. Based on this as-
sumption, we minimize the distance between Vp
(p ? [2, P ]) and V1 as follows:
O2(Vp) = minVp?0
P?
p=2
?Vp ?V1?2F (2)
Combining equations (1) and (2), we get the fol-
lowing objective function:
O(U1, . . . ,UP ;V1, . . . ,VP ) (3)
=
P?
p=1
?Dp ?UpVp?2F +
P?
p=2
?p?Vp ?V1?2F
where parameter ?p (p ? [2, P ]) is used to adjust
the relative importance of these two components.
If we set a small value for ?p, the objective func-
tion behaves like the traditional NMF and the im-
portance of data sparseness is emphasized; while a
big value of ?p indicatesVp should be very closed
to V1, and equation (3) aims to remove the noise
introduced by statistical machine translation.
By solving the optimization problem in equa-
tion (4), we can get the reduced representation of
terms and questions.
minO(U1, . . . ,UP ;V1, . . . ,VP ) (4)
subject to : Up ? 0,Vp ? 0, p ? [1, P ]
2.3 Optimization
The objective function O defined in equation (4)
performs data sparseness and noise removing si-
multaneously. There are 2P coupling components
in O, and O is not convex in both U and V to-
gether. Therefore it is unrealistic to expect an al-
gorithm to find the global minima. In the follow-
ing, we introduce an iterative algorithm which can
achieve local minima. In our optimization frame-
work, we optimize the objective function in equa-
tion (4) by alternatively minimizing each compo-
nent when the remaining 2P ? 1 components are
fixed. This procedure is summarized in Algorithm
1.
2.3.1 Update of MatrixUp
Holding V1, . . . ,VP and U1, . . . ,Up?1,Up+1,
. . . ,UP fixed, the update of Up amounts to the
following optimization problem:
min
Up?0
?Dp ?UpVp?2F (5)
Algorithm 1 Optimization framework
Input: Dp ? Rmp?N , p ? [1, P ]
1: for p = 1 : P do
2: V(0)p ? RK?N ? random matrix
3: for t = 1 : T do  T is iteration times
4: U(t)p ? UpdateU(Dp,V(t?1)p )
5: V(t)p ? UpdateV(Dp,U(t)p )
6: end for
7: returnU(T )p , V(T )p
8: end for
Algorithm 2 Update Up
Input: Dp ? RMp?N , Vp ? RK?N
1: for i = 1 : Mp do
2: u?(p)?i = (VpVTp )?1Vpd?(p)i
3: end for
4: returnUp
Let d?(p)i = (d(p)i1 , . . . , d(p)iK )T and u?(p)i =
(u(p)i1 , . . . , u
(p)
iK )T be the column vectors whose en-
tries are those of the ith row of Dp and Up re-
spectively. Thus, the optimization of equation (5)
can be decomposed into Mp optimization prob-
lems that can be solved independently, with each
corresponding to one row of Up:
min
u?(p)i ?0
?d?(p)i ?VTp u?
(p)
i ?22 (6)
for i = 1, . . . ,Mp.
Equation (6) is a standard least squares prob-
lems in statistics and the solution is:
u?(p)?i = (VpVTp )?1Vpd?
(p)
i (7)
Algorithm 2 shows the procedure.
2.3.2 Update of MatrixVp
Holding U1, . . . ,UP and V1, . . . ,Vp?1,Vp+1,
. . . ,VP fixed, the update of Vp amounts to the
optimization problem divided into two categories.
if p ? [2, P ], the objective function can be writ-
ten as:
min
Vp?0
?Dp ?UpVp?2F + ?p?Vp ?V1?2F (8)
if p = 1, the objective function can be written
as:
min
Vp?0
?Dp ?UpVp?2F + ?p?Vp?2F (9)
855
Let d(p)j be the jth column vector of Dp, and
v(p)j be the jth column vector of Vp, respectively.
Thus, equation (8) can be rewritten as:
min
{v(p)j ?0}
N?
j=1
?d(p)j ?Upv
(p)
j ?22+
N?
j=1
?p?v(p)j ?v
(1)
j ?22
(10)
which can be decomposed into N optimization
problems that can be solved independently, with
each corresponding to one column of Vp:
min
v(p)j ?0
?d(p)j ?Upv
(p)
j ?22+?p?v
(p)
j ?v
(1)
j ?22 (11)
for j = 1, . . . , N .
Equation (12) is a least square problem with L2
norm regularization. Now we rewrite the objective
function in equation (12) as
L(v(p)j ) = ?d
(p)
j ?Upv
(p)
j ?22 + ?p?v
p
j ? v
(1)
j ?22
(12)
where L(v(1)j ) is convex, and hence has a unique
solution. Taking derivatives, we obtain:
?L(v(p)j )
?v(p)j
= ?2UTp (d(p)j ?Upv
(p)
j )+2?p(v
(p)
j ?v
(1)
j )
(13)
Forcing the partial derivative to be zero leads to
v(p)?j = (UTpUp + ?pI)?1(UTp d
(p)
j + ?pv
(1)
j )
(14)
where p ? [2, P ] denotes the foreign language rep-
resentation.
Similarly, the solution of equation (9) is:
v(p)?j = (UTpUp + ?pI)?1UTp d
(p)
j (15)
where p = 1 denotes the original language repre-
sentation.
Algorithm 3 shows the procedure.
2.4 Time Complexity Analysis
In this subsection, we discuss the time complex-
ity of our proposed method. The optimization
u?(p)i using Algorithm 2 should calculate VpVTp
and Vpd?(p)i , which takes O(NK2 + NK) op-
erations. Therefore, the optimization Up takes
O(NK2 + MpNK) operations. Similarly, the
time complexity of optimization Vi using Algo-
rithm 3 is O(MpK2 + MpNK).
Another time complexity is the iteration times
T used in Algorithm 1 and the total number of
Algorithm 3 Update Vp
Input: Dp ? RMp?N , Up ? RMp?K
1: ?? (UTpUp + ?pI)?1
2: ?? UTpDp
3: if p = 1 then
4: for j = 1 : N do
5: v(p)j ? ??j , ?j is the jth column of ?
6: end for
7: end if
8: returnV1
9: if p ? [2, P ] then
10: for j = 1 : N do
11: v(p)j ? ?(?j + ?pv(1)j )
12: end for
13: end if
14: returnVp
languages P , the overall time complexity of our
proposed method is:
P?
p=1
T ?O(NK2 + MpK2 + 2MpNK) (16)
For each language Dp, the size of vocabulary
Mp is almost constant as the number of questions
increases. Besides, K ? min(Mp, N), theoreti-
cally, the computational time is almost linear with
the number of questions N and the number of lan-
guages P considered in the paper. Thus, the pro-
posed method can be easily adapted to the large-
scale information retrieval task.
2.5 Relevance Ranking
The advantage of incorporating statistical machine
translation in relevance ranking is to reduce ?word
ambiguity? and ?word mismatch? problems. To
do so, given a queried question q and a historical
question d from Yahoo! Answers, we first trans-
late q and d into other foreign languages (e.g., Chi-
nese, French etc.) and get the corresponding trans-
lated representation qi and di (i ? [2, P ]), where
P is the number of languages considered in the pa-
per. For queried question q = q1, we represent it
in the reduced space:
vq1 = argminv?0 ?q1 ?U1v?
2
2 + ?1?v?22 (17)
where vector q1 is the tf-idf representation of
queried question q1 in the term space. Similarly,
for historical question d = d1 (and its tf-idf repre-
sentation d1 in the term space) we represent it in
the reduced space as vd1 .
856
The relevance score between the queried ques-
tion q1 and the historical question d1 in the re-
duced space is, then, calculated as the cosine sim-
ilarity between vq1 and vd1 :
s(q1, d1) =
< vq1 ,vd1 >
?vq1?2 ? ?vd1?2
(18)
For translated representation qi (i ? [2, P ]), we
also represent it in the reduced space:
vqi = argminv?0 ?qi?Uiv?
2
2+?i?v?vq1?22 (19)
where vector qi is the tf-idf representation of qi
in the term space. Similarly, for translated rep-
resentation di (and its tf-idf representation di in
the term space) we also represent it in the reduced
space as vdi . The relevance score s(qi, di) be-
tween qi and di in the reduced space can be cal-
culated as the cosine similarity between vqi and
vdi .
Finally, we consider learning a relevance func-
tion of the following general, linear form:
Score(q, d) = ?T ??(q, d) (20)
where feature vector ?(q, d) =
(sV SM (q, d), s(q1, d1), s(q2, d2), . . . , s(qP , dP )),
and ? is the corresponding weight vector, we
optimize this parameter for our evaluation metrics
directly using the Powell Search algorithm (Paul
et al, 1992) via cross-validation. sV SM (q, d) is
the relevance score in the term space and can be
calculated using Vector Space Model (VSM).
3 Experiments
3.1 Data Set and Evaluation Metrics
We collect the data set from Yahoo! Answers and
use the getByCategory function provided in Ya-
hoo! Answers API6 to obtain CQA threads from
the Yahoo! site. More specifically, we utilize
the resolved questions and the resulting question
repository that we use for question retrieval con-
tains 2,288,607 questions. Each resolved ques-
tion consists of four parts: ?question title?, ?ques-
tion description?, ?question answers? and ?ques-
tion category?. For question retrieval, we only use
the ?question title? part. It is assumed that ques-
tion title already provides enough semantic infor-
mation for understanding the users? information
needs (Duan et al, 2008). There are 26 categories
6http://developer.yahoo.com/answers
Category #Size Category # Size
Arts & Humanities 86,744 Home & Garden 35,029
Business & Finance 105,453 Beauty & Style 37,350
Cars & Transportation 145,515 Pet 54,158
Education & Reference 80,782 Travel 305,283
Entertainment & Music 152,769 Health 132,716
Family & Relationships 34,743 Sports 214,317
Politics & Government 59,787 Social Science 46,415
Pregnancy & Parenting 43,103 Ding out 46,933
Science & Mathematics 89,856 Food & Drink 45,055
Computers & Internet 90,546 News & Events 20,300
Games & Recreation 53,458 Environment 21,276
Consumer Electronics 90,553 Local Businesses 51,551
Society & Culture 94,470 Yahoo! Products 150,445
Table 2: Number of questions in each first-level
category.
at the first level and 1,262 categories at the leaf
level. Each question belongs to a unique leaf cat-
egory. Table 2 shows the distribution across first-
level categories of the questions in the archives.
We use the same test set in previous work (Cao
et al, 2009; Cao et al, 2010). This set contains
252 queried questions and can be freely down-
loaded for research communities.7
The original language of the above data set is
English (l1) and then they are translated into four
other languages (Chinese (l2), French (l3), Ger-
man (l4), Italian (l5)), thus the number of language
considered is P = 5) by using the state-of-the-art
translation tool ??Google Translate.
Evaluation Metrics: We evaluate the perfor-
mance of question retrieval using the following
metrics: Mean Average Precision (MAP) and
Precision@N (P@N). MAP rewards methods that
return relevant questions early and also rewards
correct ranking of the results. P@N reports the
fraction of the top-N questions retrieved that are
relevant. We perform a significant test, i.e., a t-
test with a default significant level of 0.05.
We tune the parameters on a small development
set of 50 questions. This development set is also
extracted from Yahoo! Answers, and it is not in-
cluded in the test set. For parameter K, we do an
experiment on the development set to determine
the optimal values among 50, 100, 150, ? ? ? , 300 in
terms of MAP. Finally, we set K = 100 in the ex-
periments empirically as this setting yields the best
performance. For parameter ?1, we set ?1 = 1
empirically, while for parameter ?i (i ? [2, P ]),
we set ?i = 0.25 empirically and ensure that?
i ?i = 1.
7http://homepages.inf.ed.ac.uk/gcong/qa/
857
# Methods MAP P@10
1 VSM 0.242 0.226
2 LM 0.385 0.242
3 Jeon et al (2005) 0.405 0.247
4 Xue et al (2008) 0.436 0.261
5 Zhou et al (2011) 0.452 0.268
6 Singh (2012) 0.450 0.267
7 Zhou et al (2012) 0.483 0.275
8 SMT + MF (P = 2, l1, l2) 0.527 0.284
9 SMT + MF (P = 5) 0.564 0.291
Table 3: Comparison with different methods for
question retrieval.
3.2 Question Retrieval Results
Table 3 presents the main retrieval performance.
Row 1 and row 2 are two baseline systems, which
model the relevance score using VSM (Cao et al,
2010) and language model (LM) (Zhai and Laf-
ferty, 2001; Cao et al, 2010) in the term space.
Row 3 and row 6 are monolingual translation mod-
els to address the word mismatch problem and
obtain the state-of-the-art performance in previ-
ous work. Row 3 is the word-based translation
model (Jeon et al, 2005), and row 4 is the word-
based translation language model, which linearly
combines the word-based translation model and
language model into a unified framework (Xue et
al., 2008). Row 5 is the phrase-based translation
model, which translates a sequence of words as
whole (Zhou et al, 2011). Row 6 is the entity-
based translation model, which extends the word-
based translation model and explores strategies to
learn the translation probabilities between words
and the concepts using the CQA archives and a
popular entity catalog (Singh, 2012). Row 7 is
the bilingual translation model, which translates
the English questions from Yahoo! Answers into
Chinese questions using Google Translate and ex-
pands the English words with the translated Chi-
nese words (Zhou et al, 2012). For these previ-
ous work, we use the same parameter settings in
the original papers. Row 8 and row 9 are our pro-
posed method, which leverages statistical machine
translation to improve question retrieval via ma-
trix factorization. In row 8, we only consider two
languages (English and Chinese) and translate En-
glish questions into Chinese using Google Trans-
late in order to compare with Zhou et al (2012).
In row 9, we translate English questions into other
four languages. There are some clear trends in the
result of Table 3:
(1) Monolingual translation models signifi-
cantly outperform the VSM and LM (row 1 and
row 2 vs. row 3, row 4, row 5 and row 6).
(2) Taking advantage of potentially rich seman-
tic information drawn from other languages via
statistical machine translation, question retrieval
performance can be significantly improved (row 3,
row 4, row 5 and row 6 vs. row 7, row 8 and row 9,
all these comparisons are statistically significant at
p < 0.05).
(3) Our proposed method (leveraging statisti-
cal machine translation via matrix factorization,
SMT + MF) significantly outperforms the bilin-
gual translation model of Zhou et al (2012) (row
7 vs. row 8, the comparison is statistically signifi-
cant at p < 0.05). The reason is that matrix factor-
ization used in the paper can effectively solve the
data sparseness and noise introduced by the ma-
chine translator simultaneously.
(4) When considering more languages, ques-
tion retrieval performance can be further improved
(row 8 vs. row 9).
Note that Wang et al (2009) also addressed the
word mismatch problem for question retrieval by
using syntactic tree matching. We do not compare
with Wang et al (2009) in Table 3 because pre-
vious work (Ming et al, 2010) demonstrated that
word-based translation language model (Xue et
al., 2008) obtained the superior performance than
the syntactic tree matching (Wang et al, 2009).
Besides, some other studies attempt to improve
question retrieval with category information (Cao
et al, 2009; Cao et al, 2010), label ranking (Li et
al., 2011) or world knowledge (Zhou et al, 2012).
However, their methods are orthogonal to ours,
and we suspect that combining the category infor-
mation or label ranking into our proposed method
might get even better performance. We leave it for
future research.
3.3 Impact of the Matrix Factorization
Our proposed method (SMT +MF) can effectively
solve the data sparseness and noise via matrix fac-
torization. To further investigate the impact of
the matrix factorization, one intuitive way is to
expand the original questions with the translated
words from other four languages, without consid-
ering the data sparseness and noise introduced by
machine translator. We compare our SMT + MF
with this intuitive enriching method (SMT + IEM).
Besides, we also employ our proposed matrix fac-
torization to the original question representation
(VSM + MF). Table 4 shows the comparison.
858
# Methods MAP P@10
1 VSM 0.242 0.226
2 VSM + MF 0.411 0.253
3 SMT + IEM (P = 5) 0.495 0.280
4 SMT + MF (P = 5) 0.564 0.291
Table 4: The impact of matrix factorization.
(1) Our proposed matrix factorization can sig-
nificantly improve the performance of question re-
trieval (row 1 vs. row2; row3 vs. row4, the
improvements are statistically significant at p <
0.05). The results indicate that our proposed ma-
trix factorization can effectively address the issues
of data spareness and noise introduced by statisti-
cal machine translation.
(2) Compared to the relative improvements of
row 3 and row 4, the relative improvements of row
1 and row 2 is much larger. The reason may be
that although matrix factorization can be used to
reduce dimension, it may impair the meaningful
terms.
(3) Compared to VSM, the performance of
SMT + IEM is significantly improved (row 1
vs. row 3), which supports the motivation that
the word ambiguity and word mismatch problems
could be partially addressed by Google Translate.
3.4 Impact of the Translation Language
One of the success of this paper is to take ad-
vantage of potentially rich semantic information
drawn from other languages to solve the word am-
biguity and word mismatch problems. So we con-
struct a dummy translator (DT) that translates an
English word to itself. Thus, through this trans-
lation, we do not add any semantic information
into the original questions. The comparison is pre-
sented in Table 5. Row 1 (DT + MF) represents
integrating two copies of English questions with
our proposed matrix factorization. From Table 5,
we have several different findings:
(1) Taking advantage of potentially rich seman-
tic information drawn from other languages can
significantly improve the performance of question
retrieval (row 1 vs. row 2, row 3, row 4 and row 5,
the improvements relative to DT + MF are statisti-
cally significant at p < 0.05).
(2) Different languages contribute unevenly for
question retrieval (e.g., row 2 vs. row 3). The
reason may be that the improvements of leverag-
ing different other languages depend on the qual-
ity of machine translation. For example, row 3
# Methods MAP
1 DT + MF (l1, l1) 0.352
2 SMT + MF (P = 2, l1, l2) 0.527
3 SMT + MF (P = 2, l1, l3) 0.553
4 SMT + MF (P = 2, l1, l4) 0.536
5 SMT + MF (P = 2, l1, l5) 0.545
6 SMT + MF (P = 3, l1, l2, l3) 0.559
7 SMT + MF (P = 4, l1, l2, l3, l4) 0.563
8 SMT + MF (P = 5, l1, l2, l3, l4, l5) 0.564
Table 5: The impact of translation language.
Method Translation MAP
SMT + MF (P = 2, l1, l2) Dict 0.468GTrans 0.527
Table 6: Impact of the contextual information.
is better than row 2 because the translation qual-
ity of English-French is much better than English-
Chinese.
(3) Using much more languages does not seem
to produce significantly better performance (row 6
and row 7 vs. row 8). The reason may be that in-
consistency between different languages may exist
due to statistical machine translation.
3.5 Impact of the Contextual Information
In this paper, we translate the English questions
into other four languages using Google Translate
(GTrans), which takes into account contextual in-
formation during translation. If we translate a
question word by word, it discards the contextual
information. We would expect that such a transla-
tion would not be able to solve the word ambiguity
problem.
To investigate the impact of contextual infor-
mation for question retrieval, we only consider
two languages and translate English questions
into Chinese using an English to Chinese lexicon
(Dict) in StarDict8. Table 6 shows the experi-
mental results, we can see that the performance is
degraded when the contextual information is not
considered for the translation of questions. The
reason is that GTrans is context-dependent and
thus produces different translated Chinese words
depending on the context of an English word.
Therefore, the word ambiguity problem can be
solved during the English-Chinese translation.
4 Conclusions and Future Work
In this paper, we propose to employ statistical ma-
chine translation to improve question retrieval and
8StarDict is an open source dictionary software, available
at http://stardict.sourceforge.net/.
859
enrich the question representation with the trans-
lated words from other languages via matrix fac-
torization. Experiments conducted on a real CQA
data show some promising findings: (1) the pro-
posed method significantly outperforms the pre-
vious work for question retrieval; (2) the pro-
posed matrix factorization can significantly im-
prove the performance of question retrieval, no
matter whether considering the translation lan-
guages or not; (3) considering more languages can
further improve the performance but it does not
seem to produce significantly better performance;
(4) different languages contribute unevenly for
question retrieval; (5) our proposed method can
be easily adapted to the large-scale information re-
trieval task.
As future work, we plan to incorporate the ques-
tion structure (e.g., question topic and question fo-
cus (Duan et al, 2008)) into the question represen-
tation for question retrieval. We also want to fur-
ther investigate the use of the proposed method for
other kinds of data set, such as categorized ques-
tions from forum sites and FAQ sites.
Acknowledgments
This work was supported by the National Natural
Science Foundation of China (No. 61070106, No.
61272332 and No. 61202329), the National High
Technology Development 863 Program of China
(No. 2012AA011102), the National Basic Re-
search Program of China (No. 2012CB316300),
We thank the anonymous reviewers for their in-
sightful comments. We also thank Dr. Gao Cong
for providing the data set and Dr. Li Cai for some
discussion.
References
L. Adamic, J. Zhang, E. Bakshy, and M. Ackerman.
2008. Knowledge sharing and yahoo answers: ev-
eryone knows and something. In Proceedings of
WWW.
A. Berger, R. Caruana, D. Cohn, D. Freitag, and V.Mit-
tal. 2000. Bridging the lexical chasm: statistical ap-
proach to answer-finding. In Proceedings of SIGIR,
pages 192-199.
D. Bernhard and I. Gurevych. 2009. Combining
lexical semantic resources with question & answer
archives for translation-based answer finding. In
Proceedings of ACL, pages 728-736.
P. F. Brown, V. J. D. Pietra, S. A. D. Pietra, and R. L.
Mercer. 1993. The mathematics of statistical ma-
chine translation: parameter estimation. Computa-
tional Linguistics, 19(2):263-311.
X. Cao, G. Cong, B. Cui, C. Jensen, and C. Zhang.
2009. The use of categorization information in lan-
guage models for question retrieval. In Proceedings
of CIKM, pages 265-274.
X. Cao, G. Cong, B. Cui, and C. Jensen. 2010. A
generalized framework of exploring category infor-
mation for question retrieval in community question
answer archives. In Proceedings of WWW, pages
201-210.
H. Duan, Y. Cao, C. Y. Lin, and Y. Yu. 2008. Searching
questions by identifying questions topics and ques-
tion focus. In Proceedings of ACL, pages 156-164.
C. L. Lawson and R. J. Hanson. 1974. Solving least
squares problems. Prentice-Hall.
J. -T. Lee, S. -B. Kim, Y. -I. Song, and H. -C. Rim.
2008. Bridging lexical gaps between queries and
questions on large online Q&A collections with
compact translation models. In Proceedings of
EMNLP, pages 410-418.
W. Wang, B. Li, and I. King. 2011. Improving ques-
tion retrieval in community question answering with
label ranking. In Proceedings of IJCNN, pages 349-
356.
D. D. Lee and H. S. Seung. 2001. Algorithms for
non-negative matrix factorization. In Proceedings
of NIPS.
Z. Ming, K. Wang, and T. -S. Chua. 2010. Prototype
hierarchy based clustering for the categorization and
navigation of web collections. In Proceedings of SI-
GIR, pages 2-9.
J. Jeon, W. Croft, and J. Lee. 2005. Finding similar
questions in large question and answer archives. In
Proceedings of CIKM, pages 84-90.
C. Paige and M. Saunders. 1982. LSQR: an algo-
rithm for sparse linear equations and sparse least
squares. ACM Transaction on Mathematical Soft-
ware, 8(1):43-71.
W. H. Press, S. A. Teukolsky, W. T. Vetterling, and B.
P. Flannery. 1992. Numerical Recipes In C. Cam-
bridge Univ. Press.
S. Riezler, A. Vasserman, I. Tsochantaridis, V. Mittal,
and Y. Liu. 2007. Statistical machine translation for
query expansion in answer retrieval. In Proceedings
of ACL, pages 464-471.
A. Singh. 2012. Entity based q&a retrieval. In Pro-
ceedings of EMNLP-CoNLL, pages 1266-1277.
J. Tang, X. Wang, H. Gao, X. Hu, and H. Liu. 2012.
Enriching short text representation in microblog for
clustering. Front. Comput., 6(1):88-101.
860
E. Wachsmuth, M. W. Oram, and D. I. Perrett. 1994.
Recognition of objects and their component parts:
responses of single units in the temporal cortex of
teh macaque. Cerebral Cortex, 4:509-522.
K. Wang, Z. Ming, and T-S. Chua. 2009. A syntac-
tic tree matching approach to find similar questions
in community-based qa services. In Proceedings of
SIGIR, pages 187-194.
B. Wang, X. Wang, C. Sun, B. Liu, and L. Sun. 2010.
Modeling semantic relevance for question-answer
pairs in web social communities. In Proceedings of
ACL, pages 1230-1238.
W. Xu, X. Liu, and Y. Gong. 2003. Document cluster-
ing based on non-negative matrix factorization. In
Proceedings of SIGIR, pages 267-273.
X. Xue, J. Jeon, and W. B. Croft. 2008. Retrieval mod-
els for question and answer archives. In Proceedings
of SIGIR, pages 475-482.
C. Zhai and J. Lafferty. 2001. A study of smooth meth-
ods for language models applied to ad hoc informa-
tion retrieval. In Proceedings of SIGIR, pages 334-
342.
G. Zhou, L. Cai, J. Zhao, and K. Liu. 2011. Phrase-
based translation model for question retrieval in
community question answer archives. In Proceed-
ings of ACL, pages 653-662.
G. Zhou, K. Liu, and J. Zhao. 2012. Exploiting bilin-
gual translation for question retrieval in community-
based question answering. In Proceedings of COL-
ING, pages 3153-3170.
G. Zhou, Y. Liu, F. Liu, D. Zeng, and J. Zhao. 2013.
Improving Question Retrieval in Community Ques-
tion Answering Using World Knowledge. In Pro-
ceedings of IJCAI.
861
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1004?1013,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Using Supervised Bigram-based ILP for Extractive Summarization
Chen Li, Xian Qian, and Yang Liu
The University of Texas at Dallas
Computer Science Department
chenli,qx,yangl@hlt.utdallas.edu
Abstract
In this paper, we propose a bigram based
supervised method for extractive docu-
ment summarization in the integer linear
programming (ILP) framework. For each
bigram, a regression model is used to es-
timate its frequency in the reference sum-
mary. The regression model uses a vari-
ety of indicative features and is trained dis-
criminatively to minimize the distance be-
tween the estimated and the ground truth
bigram frequency in the reference sum-
mary. During testing, the sentence selec-
tion problem is formulated as an ILP prob-
lem to maximize the bigram gains. We
demonstrate that our system consistently
outperforms the previous ILP method on
different TAC data sets, and performs
competitively compared to the best results
in the TAC evaluations. We also con-
ducted various analysis to show the im-
pact of bigram selection, weight estima-
tion, and ILP setup.
1 Introduction
Extractive summarization is a sentence selection
problem: identifying important summary sen-
tences from one or multiple documents. Many
methods have been developed for this problem, in-
cluding supervised approaches that use classifiers
to predict summary sentences, graph based ap-
proaches to rank the sentences, and recent global
optimization methods such as integer linear pro-
gramming (ILP) and submodular methods. These
global optimization methods have been shown to
be quite powerful for extractive summarization,
because they try to select important sentences and
remove redundancy at the same time under the
length constraint.
Gillick and Favre (Gillick and Favre, 2009) in-
troduced the concept-based ILP for summariza-
tion. Their system achieved the best result in the
TAC 09 summarization task based on the ROUGE
evaluation metric. In this approach the goal is
to maximize the sum of the weights of the lan-
guage concepts that appear in the summary. They
used bigrams as such language concepts. The as-
sociation between the language concepts and sen-
tences serves as the constraints. This ILP method
is formally represented as below (see (Gillick and
Favre, 2009) for more details):
max ?i wici (1)
s.t. sjOccij ? ci (2)?
j sjOccij ? ci (3)?
j ljsj ? L (4)
ci ? {0, 1} ?i (5)
sj ? {0, 1} ?j (6)
ci and sj are binary variables (shown in (5) and
(6)) that indicate the presence of a concept and
a sentence respectively. wi is a concept?s weight
and Occij means the occurrence of concept i in
sentence j. Inequalities (2)(3) associate the sen-
tences and concepts. They ensure that selecting a
sentence leads to the selection of all the concepts
it contains, and selecting a concept only happens
when it is present in at least one of the selected
sentences.
There are two important components in this
concept-based ILP: one is how to select the con-
cepts (ci); the second is how to set up their weights
(wi). Gillick and Favre (Gillick and Favre, 2009)
used bigrams as concepts, which are selected from
a subset of the sentences, and their document fre-
quency as the weight in the objective function.
In this paper, we propose to find a candidate
summary such that the language concepts (e.g., bi-
grams) in this candidate summary and the refer-
ence summary can have the same frequency. We
expect this restriction is more consistent with the
1004
ROUGE evaluation metric used for summarization
(Lin, 2004). In addition, in the previous concept-
based ILP method, the constraints are with respect
to the appearance of language concepts, hence it
cannot distinguish the importance of different lan-
guage concepts in the reference summary. Our
method can decide not only which language con-
cepts to use in ILP, but also the frequency of these
language concepts in the candidate summary. To
estimate the bigram frequency in the summary,
we propose to use a supervised regression model
that is discriminatively trained using a variety of
features. Our experiments on several TAC sum-
marization data sets demonstrate this proposed
method outperforms the previous ILP system and
often the best performing TAC system.
2 Proposed Method
2.1 Bigram Gain Maximization by ILP
We choose bigrams as the language concepts in
our proposed method since they have been suc-
cessfully used in previous work. In addition, we
expect that the bigram oriented ILP is consistent
with the ROUGE-2 measure widely used for sum-
marization evaluation.
We start the description of our approach for the
scenario where a human abstractive summary is
provided, and the task is to select sentences to
form an extractive summary. Then Our goal is
to make the bigram frequency in this system sum-
mary as close as possible to that in the reference.
For each bigram b, we define its gain:
Gain(b, sum) = min{nb,ref , nb,sum} (7)
where nb,ref is the frequency of b in the reference
summary, and nb,sum is the frequency of b in the
automatic summary. The gain of a bigram is no
more than its frequency in the reference summary,
hence adding redundant bigrams will not increase
the gain.
The total gain of an extractive summary is de-
fined as the sum of every bigram gain in the sum-
mary:
Gain(sum) =
?
b
Gain(b, sum)
=
?
b
min{nb,ref ,
?
s
z(s) ? nb,s} (8)
where s is a sentence in the document, nb,s is
the frequency of b in sentence s, z(s) is a binary
variable, indicating whether s is selected in the
summary. The goal is to find z that maximizes
Gain(sum) (formula (8)) under the length con-
straint L.
This problem can be casted as an ILP problem.
First, using the fact that
min{a, x} = 0.5(?|x ? a| + x + a), x, a ? 0
we have
?
b
min{nb,ref ,
?
s
z(s) ? nb,s} =
?
b
0.5 ? (?|nb,ref ?
?
s
z(s) ? nb,s|+
nb,ref +
?
s
z(s) ? nb,s)
Now the problem is equivalent to:
max
z
?
b
(?|nb,ref ?
?
s
z(s) ? nb,s| +
nb,ref +
?
s
z(s) ? nb,s)
s.t.
?
s
z(s) ? |S| ? L; z(s) ? {0, 1}
This is equivalent to the ILP:
max
?
b
(
?
s
z(s) ? nb,s ?Cb) (9)
s.t.
?
s
z(s) ? |S| ? L (10)
z(s) ? {0, 1} (11)
?Cb ? nb,ref ?
?
s
z(s) ? nb,s ? Cb
(12)
where Cb is an auxiliary variable we introduce that
is equal to |nb,ref ?
?
s z(s) ? nb,s|, and nb,ref is
a constant that can be dropped from the objective
function.
2.2 Regression Model for Bigram Frequency
Estimation
In the previous section, we assume that nb,ref is
at hand (reference abstractive summary is given)
and propose a bigram-based optimization frame-
work for extractive summarization. However, for
the summarization task, the bigram frequency is
unknown, and thus our first goal is to estimate such
frequency. We propose to use a regression model
for this.
Since a bigram?s frequency depends on the sum-
mary length (L), we use a normalized frequency
1005
in our method. Let nb,ref = Nb,ref ? L, where
Nb,ref = n(b,ref)?
b n(b,ref)
is the normalized frequency
in the summary. Now the problem is to automati-
cally estimate Nb,ref .
Since the normalized frequency Nb,ref is a real
number, we choose to use a logistic regression
model to predict it:
Nb,ref =
exp{w?f(b)}?
j exp{w?f(bj)}
(13)
where f(bj) is the feature vector of bigram bj and
w? is the corresponding feature weight. Since even
for identical bigrams bi = bj , their feature vectors
may be different (f(bi) 6= f(bj)) due to their dif-
ferent contexts, we sum up frequencies for identi-
cal bigrams {bi|bi = b}:
Nb,ref =
?
i,bi=b
Nbi,ref
=
?
i,bi=b exp{w?f(bi)}?
j exp{w?f(bj)}
(14)
To train this regression model using the given
reference abstractive summaries, rather than trying
to minimize the squared error as typically done,
we propose a new objective function. Since the
normalized frequency satisfies the probability con-
straint
?
b Nb,ref = 1, we propose to use KL di-
vergence to measure the distance between the es-
timated frequencies and the ground truth values.
The objective function for training is thus to mini-
mize the KL distance:
min
?
b
N?b,ref log
N?b,ref
Nb,ref
(15)
where N?b,ref is the true normalized frequency of
bigram b in reference summaries.
Finally, we replace Nb,ref in Formula (15) with
Eq (14) and get the objective function below:
max
?
b
N?b,ref log
?
i,bi=b exp{w?f(bi)}?
j exp{w?f(bj)}
(16)
This shares the same form as the contrastive es-
timation proposed by (Smith and Eisner, 2005).
We use gradient decent method for parameter esti-
mation, initial w is set with zero.
2.3 Features
Each bigram is represented using a set of features
in the above regression model. We use two types
of features: word level and sentence level features.
Some of these features have been used in previous
work (Aker and Gaizauskas, 2009; Brandow et al,
1995; Edmundson, 1969; Radev, 2001):
? Word Level:
? 1. Term frequency1: The frequency of
this bigram in the given topic.
? 2. Term frequency2: The frequency of
this bigram in the selected sentences1 .
? 3. Stop word ratio: Ratio of stop words
in this bigram. The value can be {0, 0.5,
1}.
? 4. Similarity with topic title: The
number of common tokens in these two
strings, divided by the length of the
longer string.
? 5. Similarity with description of the
topic: Similarity of the bigram with
topic description (see next data section
about the given topics in the summariza-
tion task).
? Sentence Level: (information of sentence
containing the bigram)
? 6. Sentence ratio: Number of sentences
that include this bigram, divided by the
total number of the selected sentences.
? 7. Sentence similarity: Sentence sim-
ilarity with topic?s query, which is the
concatenation of topic title and descrip-
tion.
? 8. Sentence position: Sentence posi-
tion in the document.
? 9. Sentence length: The number of
words in the sentence.
? 10. Paragraph starter: Binary feature
indicating whether this sentence is the
beginning of a paragraph.
3 Experiments
3.1 Data
We evaluate our method using several recent TAC
data sets, from 2008 to 2011. The TAC summa-
rization task is to generate at most 100 words sum-
maries from 10 documents for a given topic query
(with a title and more detailed description). For
model training, we also included two years? DUC
data (2006 and 2007). When evaluating on one
TAC data set, we use the other years of the TAC
data plus the two DUC data sets as the training
data.
1See next section about the sentence selection step
1006
3.2 Summarization System
We use the same system pipeline described in
(Gillick et al, 2008; McDonald, 2007). The key
modules in the ICSI ILP system (Gillick et al,
2008) are briefly described below.
? Step 1: Clean documents, split text into sen-
tences.
? Step 2: Extract bigrams from all the sen-
tences, then select those bigrams with doc-
ument frequency equal to more than 3. We
call this subset as initial bigram set in the fol-
lowing.
? Step 3: Select relevant sentences that contain
at least one bigram from the initial bigram
set.
? Step 4: Feed the ILP with sentences and the
bigram set to get the result.
? Step 5: Order sentences identified by ILP as
the final result of summary.
The difference between the ICSI and our system
is in the 4th step. In our method, we first extract all
the bigrams from the selected sentences and then
estimate each bigram?s Nb,ref using the regression
model. Then we use the top-n bigrams with their
Nb,ref and all the selected sentences in our pro-
posed ILP module for summary sentence selec-
tion. When training our bigram regression model,
we use each of the 4 reference summaries sepa-
rately, i.e., the bigram frequency is obtained from
one reference summary. The same pre-selection of
sentences described above is also applied in train-
ing, that is, the bigram instances used in training
are from these selected sentences and the reference
summary.
4 Experiment and Analysis
4.1 Experimental Results
Table 1 shows the ROUGE-2 results of our pro-
posed system, the ICSI system, and also the best
performing system in the NIST TAC evaluation.
We can see that our proposed system consistently
outperforms ICSI ILP system (the gain is statis-
tically significant based on ROUGE?s 95% confi-
dence internal results). Compared to the best re-
ported TAC result, our method has better perfor-
mance on three data sets, except 2011 data. Note
that the best performing system for the 2009 data
is the ICSI ILP system, with an additional com-
pression step. Our ILP method is purely extrac-
tive. Even without using compression, our ap-
proach performs better than the full ICSI system.
The best performing system for the 2011 data also
has some compression module. We expect that af-
ter applying sentence compression and merging,
we will have even better performance, however,
our focus in this paper is on the bigram-based ex-
tractive summarization.
ICSI Proposed TAC Rank1
ILP System System
2008 0.1023 0.1076 0.1038
2009 0.1160 0.1246 0.1216
2010 0.1003 0.1067 0.0957
2011 0.1271 0.1327 0.1344
Table 1: ROUGE-2 summarization results.
There are several differences between the ICSI
system and our proposed method. First is the
bigrams (concepts) used. We use the top 100
bigrams from our bigram estimation module;
whereas the ICSI system just used the initial bi-
gram set described in Section 3.2. Second, the
weights for those bigrams differ. We used the es-
timated value from the regression model; the ICSI
system just uses the bigram?s document frequency
in the original text as weight. Finally, two systems
use different ILP setups. To analyze which fac-
tors (or all of them) explain the performance dif-
ference, we conducted various controlled experi-
ments for these three factors (bigrams, weights,
ILP). All of the following experiments use the
TAC 2009 data as the test set.
4.2 Effect of Bigram Weights
In this experiment, we vary the weighting methods
for the two systems: our proposed method and the
ICSI system. We use three weighting setups: the
estimated bigram frequency value in our method,
document frequency, or term frequency from the
original text. Table 2 and 3 show the results using
the top 100 bigrams from our system and the ini-
tial bigram set from the ICSI system respectively.
We also evaluate using the two different ILP con-
figurations in these experiments.
First of all, we can see that for both ILP sys-
tems, our estimated bigram weights outperform
the other frequency-based weights. For the ICSI
ILP system, using bigram document frequency
achieves better performance than term frequency
(which verified why document frequency is used
in their system). In contrast, for our ILP method,
1007
# Weight ILP ROUGE-2
1 Estimated value Proposed 0.12462 ICSI 0.1178
3 Document freq Proposed 0.11094 ICSI 0.1132
5 Term freq Proposed 0.11166 ICSI 0.1080
Table 2: Results using different weighting meth-
ods on the top 100 bigrams generated from our
proposed system.
# Weight ILP ROUGE-2
1 Estimated value Proposed 0.11572 ICSI 0.1161
3 Document freq Proposed 0.11014 ICSI 0.1160
5 Term freq Proposed 0.11096 ICSI 0.1072
Table 3: Results using different weighting meth-
ods based on the initial bigram sets. The average
number of bigrams is around 80 for each topic.
the bigram?s term frequency is slightly more use-
ful than its document frequency. This indicates
that our estimated value is more related to bi-
gram?s term frequency in the original text. When
the weight is document frequency, the ICSI?s re-
sult is better than our proposed ILP; whereas when
using term frequency as the weights, our ILP has
better results, again suggesting term frequency fits
our ILP system better. When the weight is esti-
mated value, the results depend on the bigram set
used. The ICSI?s ILP performs slightly better than
ours when it is equipped with the initial bigram,
but our proposed ILP has much better results us-
ing our selected top100 bigrams. This shows that
the size and quality of the bigrams has an impact
on the ILP modules.
4.3 The Effect of Bigram Set?s size
In our proposed system, we use 100 top bigrams.
There are about 80 bigrams used in the ICSI ILP
system. A natural question to ask is the impact
of the number of bigrams and their quality on the
summarization system. Table 4 shows some statis-
tics of the bigrams. We can see that about one
third of bigrams in the reference summary are in
the original text (127.3 out of 321.93), verifying
that people do use different words/bigram when
writing abstractive summaries. We mentioned that
we only use the top-N (n is 100 in previous ex-
periments) bigrams in our summarization system.
On one hand, this is to save computational cost for
the ILP module. On the other hand, we see from
the table that only 127 of these more than 2K bi-
grams are in the reference summary and are thus
expected to help the summary responsiveness. In-
cluding all the bigrams would lead to huge noise.
# bigrams in ref summary 321.93
# bigrams in text and ref summary 127.3
# bigrams used in our regression model 2140.7
(i.e., in selected sentences)
Table 4: Bigram statistics. The numbers are the
average ones for each topic.
Fig 1 shows the bigram coverage (number of bi-
grams used in the system that are also in reference
summaries) when we vary N selected bigrams. As
expected, we can see that as n increases, there
are more reference summary bigrams included in
the system. There are 25 summary bigrams in the
top-50 bigrams and about 38 in top-100 bigrams.
Compared with the ICSI system that has around 80
bigrams in the initial bigram set and 29 in the ref-
erence summary, our estimation module has better
coverage.
0
10
20
30
40
50
60
70
80
90
100
110
120
130
50 500 950 1400 1850 2300 2750 3200
Number of Selected Bigram
N
um
be
r
of
B
ig
ra
m
bo
th
in
Se
le
ct
ed
an
d
R
ef
er
en
ce
Figure 1: Coverage of bigrams (number of bi-
grams in reference summary) when varying the
number of bigrams used in the ILP systems.
Increasing the number of bigrams used in the
system will lead to better coverage, however, the
incorrect bigrams also increase and have a nega-
tive impact on the system performance. To exam-
ine the best tradeoff, we conduct the experiments
by choosing the different top-N bigram set for the
two ILP systems, as shown in Fig 2. For both the
ILP systems, we used the estimated weight value
for the bigrams.
1008
We can see that the ICSI ILP system performs
better when the input bigrams have less noise
(those bigrams that are not in summary). However,
our proposed method is slightly more robust to this
kind of noise, possibly because of the weights we
use in our system ? the noisy bigrams have lower
weights and thus less impact on the final system
performance. Overall the two systems have sim-
ilar trends: performance increases at the begin-
ning when using more bigrams, and after certain
points starts degrading with too many bigrams.
The optimal number of bigrams differs for the two
systems, with a larger number of bigrams in our
method. We also notice that the ICSI ILP system
achieved a ROUGE-2 of 0.1218 when using top
60 bigrams, which is better than using the initial
bigram set in their method (0.1160).
0.109
0.111
0.113
0.115
0.117
0.119
0.121
0.123
0.125
40 50 60 70 80 90 100 110 120 130
Number of selected bigram
R
ou
ge
-2
Proposed ILP
ICSI
Figure 2: Summarization performance when vary-
ing the number of bigrams for two systems.
4.4 Oracle Experiments
Based on the above analysis, we can see the impact
of the bigram set and their weights. The following
experiments are designed to demonstrate the best
system performance we can achieve if we have ac-
cess to good quality bigrams and weights. Here we
use the information from the reference summary.
The first is an oracle experiment, where we use
all the bigrams from the reference summaries that
are also in the original text. In the ICSI ILP
system, the weights are the document frequency
from the multiple reference summaries. In our ILP
module, we use the term frequency of the bigram.
The oracle results are shown in Table 5. We can
see these are significantly better than the automatic
systems.
From Table 5, we notice that ICSI?s ILP per-
forms marginally better than our proposed ILP. We
hypothesize that one reason may be that many bi-
grams in the summary reference only appear once.
Table 6 shows the frequency of the bigrams in the
summary. Indeed 85% of bigram only appear once
ILP System ROUGE-2
Our ILP 0.2124
ICSI ILP 0.2128
Table 5: Oracle experiment: using bigrams and
their frequencies in the reference summary as
weights.
and no bigrams appear more than 9 times. For the
majority of the bigrams, our method and the ICSI
ILP are the same. For the others, our system has
slight disadvantage when using the reference term
frequency. We expect the high term frequency
may need to be properly smoothed/normalized.
Freq 1 2 3 4 5 6 7 8 9
Ave# 277 32 7.5 3.2 1.1 0.3 0.1 0.1 0.04
Table 6: Average number of bigrams for each term
frequency in one topic?s reference summary.
We also treat the oracle results as the gold stan-
dard for extractive summarization and compared
how the two automatic summarization systems
differ at the sentence level. This is different from
the results in Table 1, which are the ROUGE re-
sults comparing to human written abstractive sum-
maries at the n-gram level. We found that among
the 188 sentences in this gold standard, our system
hits 31 and ICSI only has 23. This again shows
that our system has better performance, not just
at the word level based on ROUGE measures, but
also at the sentence level. There are on average
3 different sentences per topic between these two
results.
In the second experiment, after we obtain the
estimated Nb,ref for every bigram in the selected
sentences from our regression model, we only
keep those bigrams that are in the reference sum-
mary, and use the estimated weights for both ILP
modules. Table 7 shows the results. We can
consider these as the upper bound the system
can achieve if we use the automatically estimated
weights for the correct bigrams. In this experi-
ment ICSI ILP?s performance still performs better
than ours. This might be attributed to the fact there
is less noise (all the bigrams are the correct ones)
and thus the ICSI ILP system performs well. We
can see that these results are worse than the pre-
vious oracle experiments, but are better than using
the automatically generated bigrams, again show-
ing the bigram and weight estimation is critical for
1009
summarization.
# Weight ILP ROUGE-2
1 Estimated value Proposed 0.18882 ICSI 0.1942
Table 7: Summarization results when using the es-
timated weights and only keeping the bigrams that
are in the reference summary.
4.5 Effect of Training Set
Since our method uses supervised learning, we
conduct the experiment to show the impact of
training size. In TAC?s data, each topic has two
sets of documents. For set A, the task is a standard
summarization, and there are 4 reference sum-
maries, each 100 words long; for set B, it is an up-
date summarization task ? the summary includes
information not mentioned in the summary from
set A. There are also 4 reference summaries, with
400 words in total. Table 8 shows the results on
2009 data when using the data from different years
and different sets for training. We notice that when
the training data only contains set A, the perfor-
mance is always better than using set B or the com-
bined set A and B. This is not surprising because
of the different task definition. Therefore, for the
rest of the study on data size impact, we only use
data set A from the TAC data and the DUC data as
the training set. In total there are about 233 topics
from the two years? DUC data (06, 07) and three
years? TAC data (08, 10, 11). We incrementally
add 20 topics every time (from DUC06 to TAC11)
and plot the learning curve, as shown in Fig 3. As
expected, more training data results in better per-
formance.
Training Set # Topics ROUGE-2
08 Corpus (A) 48 0.1192
08 Corpus( B) 48 0.1178
08 Corpus (A+B) 96 0.1188
10 Corpus (A) 46 0.1174
10 Corpus (B) 46 0.1167
10 Corpus (A+B) 92 0.1170
11 Corpus (A) 44 0.1157
11 Corpus (B) 44 0.1130
11 Corpus (A+B) 88 0.1140
Table 8: Summarization performance when using
different training corpora.
0.112
0.113
0.114
0.115
0.116
0.117
0.118
0.119
0.12
0.121
0.122
0.123
0.124
0.125
20 40 60 80 100 120 140 160 180 200 220 240
Number of trainning topics
R
ou
ge
-2
Figure 3: Learning curve
4.6 Summary of Analysis
The previous experiments have shown the impact
of the three factors: the quality of the bigrams
themselves, the weights used for these bigrams,
and the ILP module. We found that the bigrams
and their weights are critical for both the ILP se-
tups. However, there is negligible difference be-
tween the two ILP methods.
An important part of our system is the super-
vised method for bigram and weight estimation.
We have already seen for the previous ILP method,
when using our bigrams together with the weights,
better performance can be achieved. Therefore we
ask the question whether this is simply because
we use supervised learning, or whether our pro-
posed regression model is the key. To answer this,
we trained a simple supervised binary classifier
for bigram prediction (positive means that a bi-
gram appears in the summary) using the same set
of features as used in our bigram weight estima-
tion module, and then used their document fre-
quency in the ICSI ILP system. The result for
this method is 0.1128 on the TAC 2009 data. This
is much lower than our result. We originally ex-
pected that using the supervised method may out-
perform the unsupervised bigram selection which
only uses term frequency information. Further ex-
periments are needed to investigate this. From this
we can see that it is not just the supervised meth-
ods or using annotated data that yields the over-
all improved system performance, but rather our
proposed regression setup for bigrams is the main
reason.
5 Related Work
We briefly describe some prior work on summa-
rization in this section. Unsupervised methods
have been widely used. In particular, recently sev-
eral optimization approaches have demonstrated
1010
competitive performance for extractive summa-
rization task. Maximum marginal relevance
(MMR) (Carbonell and Goldstein, 1998) uses a
greedy algorithm to find summary sentences. (Mc-
Donald, 2007) improved the MMR algorithm to
dynamic programming. They used a modified ob-
jective function in order to consider whether the
selected sentence is globally optimal. Sentence-
level ILP was also first introduced in (McDon-
ald, 2007), but (Gillick and Favre, 2009) revised
it to concept-based ILP. (Woodsend and Lapata,
2012) utilized ILP to jointly optimize different as-
pects including content selection, surface realiza-
tion, and rewrite rules in summarization. (Gala-
nis et al, 2012) uses ILP to jointly maximize the
importance of the sentences and their diversity
in the summary. (Berg-Kirkpatrick et al, 2011)
applied a similar idea to conduct the sentence
compression and extraction for multiple document
summarization. (Jin et al, 2010) made a com-
parative study on sentence/concept selection and
pairwise and list ranking algorithms, and con-
cluded ILP performed better than MMR and the
diversity penalty strategy in sentence/concept se-
lection. Other global optimization methods in-
clude submodularity (Lin and Bilmes, 2010) and
graph-based approaches (Erkan and Radev, 2004;
Leskovec et al, 2005; Mihalcea and Tarau, 2004).
Various unsupervised probabilistic topic models
have also been investigated for summarization and
shown promising. For example, (Celikyilmaz and
Hakkani-Tu?r, 2011) used it to model the hidden
abstract concepts across documents as well as the
correlation between these concepts to generate
topically coherent and non-redundant summaries.
(Darling and Song, 2011) applied it to separate
the semantically important words from the low-
content function words.
In contrast to these unsupervised approaches,
there are also various efforts on supervised learn-
ing for summarization where a model is trained to
predict whether a sentence is in the summary or
not. Different features and classifiers have been
explored for this task, such as Bayesian method
(Kupiec et al, 1995), maximum entropy (Osborne,
2002), CRF (Galley, 2006), and recently reinforce-
ment learning (Ryang and Abekawa, 2012). (Aker
et al, 2010) used discriminative reranking on mul-
tiple candidates generated by A* search. Recently,
research has also been performed to address some
issues in the supervised setup, such as the class
data imbalance problem (Xie and Liu, 2010).
In this paper, we propose to incorporate the
supervised method into the concept-based ILP
framework. Unlike previous work using sentence-
based supervised learning, we use a regression
model to estimate the bigrams and their weights,
and use these to guide sentence selection. Com-
pared to the direct sentence-based classification or
regression methods mentioned above, our method
has an advantage. When abstractive summaries
are given, one needs to use that information to au-
tomatically generate reference labels (a sentence
is in the summary or not) for extractive summa-
rization. Most researchers have used the similarity
between a sentence in the document and the ab-
stractive summary for labeling. This is not a per-
fect process. In our method, we do not need to
generate this extra label for model training since
ours is based on bigrams ? it is straightforward to
obtain the reference frequency for bigrams by sim-
ply looking at the reference summary. We expect
our approach also paves an easy way for future au-
tomatic abstractive summarization. One previous
study that is most related to ours is (Conroy et al,
2011), which utilized a Naive Bayes classifier to
predict the probability of a bigram, and applied
ILP for the final sentence selection. They used
more features than ours, whereas we use a discrim-
inatively trained regression model and a modified
ILP framework. Our proposed method performs
better than their reported results in TAC 2011 data.
Another study closely related to ours is (Davis et
al., 2012), which leveraged Latent Semantic Anal-
ysis (LSA) to produce term weights and selected
summary sentences by computing an approximate
solution to the Budgeted Maximal Coverage prob-
lem.
6 Conclusion and Future Work
In this paper, we leverage the ILP method as a core
component in our summarization system. Dif-
ferent from the previous ILP summarization ap-
proach, we propose a supervised learning method
(a discriminatively trained regression model) to
determine the importance of the bigrams fed to
the ILP module. In addition, we revise the ILP to
maximize the bigram gain (which is expected to
be highly correlated with ROUGE-2 scores) rather
than the concept/bigram coverage. Our proposed
method yielded better results than the previous
state-of-the-art ILP system on different TAC data
1011
sets. From a series of experiments, we found that
there is little difference between the two ILP mod-
ules, and that the improved system performance is
attributed to the fact that our proposed supervised
bigram estimation module can successfully gather
the important bigram and assign them appropriate
weights. There are several directions that warrant
further research. We plan to consider the context
of bigrams to better predict whether a bigram is in
the reference summary. We will also investigate
the relationship between concepts and sentences,
which may help move towards abstractive summa-
rization.
Acknowledgments
This work is partly supported by DARPA under
Contract No. HR0011-12-C-0016 and FA8750-
13-2-0041, and NSF IIS-0845484. Any opinions
expressed in this material are those of the authors
and do not necessarily reflect the views of DARPA
or NSF.
References
Ahmet Aker and Robert Gaizauskas. 2009. Summary
generation for toponym-referenced images using ob-
ject type language models. In Proceedings of the
International Conference RANLP.
Ahmet Aker, Trevor Cohn, and Robert Gaizauskas.
2010. Multi-document summarization using a*
search and discriminative training. In Proceedings
of the EMNLP.
Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein.
2011. Jointly learning to extract and compress. In
Proceedings of the ACL.
Ronald Brandow, Karl Mitze, and Lisa F. Rau. 1995.
Automatic condensation of electronic publications
by sentence selection. Inf. Process. Manage.
Jaime Carbonell and Jade Goldstein. 1998. The use of
mmr, diversity-based reranking for reordering docu-
ments and producing summaries. In Proceedings of
the SIGIR.
Asli Celikyilmaz and Dilek Hakkani-Tu?r. 2011. Dis-
covery of topically coherent sentences for extractive
summarization. In Proceedings of the ACL.
John M. Conroy, Judith D. Schlesinger, Jeff Kubina,
Peter A. Rankel, and Dianne P. O?Leary. 2011.
Classy 2011 at tac: Guided and multi-lingual sum-
maries and evaluation metrics. In Proceedings of the
TAC.
William M. Darling and Fei Song. 2011. Probabilistic
document modeling for syntax removal in text sum-
marization. In Proceedings of the ACL.
Sashka T. Davis, John M. Conroy, and Judith D.
Schlesinger. 2012. Occams - an optimal combinato-
rial covering algorithm for multi-document summa-
rization. In Proceedings of the ICDM.
H. P. Edmundson. 1969. New methods in automatic
extracting. J. ACM.
Gu?nes Erkan and Dragomir R. Radev. 2004. Lexrank:
graph-based lexical centrality as salience in text
summarization. J. Artif. Int. Res.
Dimitrios Galanis, Gerasimos Lampouras, and Ion An-
droutsopoulos. 2012. Extractive multi-document
summarization with integer linear programming and
support vector regression. In Proceedings of the
COLING.
Michel Galley. 2006. A skip-chain conditional random
field for ranking meeting utterances by importance.
In Proceedings of the EMNLP.
Dan Gillick and Benoit Favre. 2009. A scalable global
model for summarization. In Proceedings of the
Workshop on Integer Linear Programming for Natu-
ral Langauge Processing on NAACL.
Dan Gillick, Benoit Favre, and Dilek Hakkani-Tu?r.
2008. In The ICSI Summarization System at TAC
2008.
Feng Jin, Minlie Huang, and Xiaoyan Zhu. 2010. A
comparative study on ranking and selection strate-
gies for multi-document summarization. In Pro-
ceedings of the COLING.
Julian Kupiec, Jan Pedersen, and Francine Chen. 1995.
A trainable document summarizer. In Proceedings
of the SIGIR.
Jure Leskovec, Natasa Milic-Frayling, and Marko Gro-
belnik. 2005. Impact of linguistic analysis on the
semantic graph coverage and learning of document
extracts. In Proceedings of the AAAI.
Hui Lin and Jeff Bilmes. 2010. Multi-document sum-
marization via budgeted maximization of submodu-
lar functions. In Proceedings of the NAACL.
Chin-Yew Lin. 2004. Rouge: a package for auto-
matic evaluation of summaries. In Proceedings of
the ACL.
Ryan McDonald. 2007. A study of global inference al-
gorithms in multi-document summarization. In Pro-
ceedings of the European conference on IR research.
Rada Mihalcea and Paul Tarau. 2004. Textrank:
Bringing order into text. In Proceedings of the
EMNLP.
Miles Osborne. 2002. Using maximum entropy for
sentence extraction. In Proceedings of the ACL-02
Workshop on Automatic Summarization.
1012
Dragomir R. Radev. 2001. Experiments in single and
multidocument summarization using mead. In In
First Document Understanding Conference.
Seonggi Ryang and Takeshi Abekawa. 2012. Frame-
work of automatic text summarization using rein-
forcement learning. In Proceedings of the EMNLP.
Noah A. Smith and Jason Eisner. 2005. Contrastive
estimation: training log-linear models on unlabeled
data. In Proceedings of the ACL.
Kristian Woodsend and Mirella Lapata. 2012. Mul-
tiple aspect summarization using integer linear pro-
gramming. In Proceedings of the EMNLP.
Shasha Xie and Yang Liu. 2010. Improving supervised
learning for meeting summarization using sampling
and regression. Comput. Speech Lang.
1013
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 327?332,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Polynomial Time Joint Structural Inference for Sentence Compression
Xian Qian and Yang Liu
The University of Texas at Dallas
800 W. Campbell Rd., Richardson, TX, USA
{qx,yangl}@hlt.utdallas.edu
Abstract
We propose two polynomial time infer-
ence algorithms to compress sentences un-
der bigram and dependency-factored ob-
jectives. The first algorithm is exact and
requires O(n
6
) running time. It extend-
s Eisner?s cubic time parsing algorithm
by using virtual dependency arcs to link
deleted words. Two signatures are added
to each span, indicating the number of
deleted words and the rightmost kept word
within the span. The second algorithm is
a fast approximation of the first one. It re-
laxes the compression ratio constraint us-
ing Lagrangian relaxation, and thereby re-
quires O(n
4
) running time. Experimental
results on the popular sentence compres-
sion corpus demonstrate the effectiveness
and efficiency of our proposed approach.
1 Introduction
Sentence compression aims to shorten a sentence
by removing uninformative words to reduce read-
ing time. It has been widely used in compres-
sive summarization (Liu and Liu, 2009; Li et al,
2013; Martins and Smith, 2009; Chali and Hasan,
2012; Qian and Liu, 2013). To make the com-
pressed sentence readable, some techniques con-
sider the n-gram language models of the com-
pressed sentence (Clarke and Lapata, 2008; Mc-
Donald, 2006). Recent studies used a subtree dele-
tion model for compression (Berg-Kirkpatrick et
al., 2011; Morita et al, 2013; Qian and Liu, 2013),
which deletes a word only if its modifier in the
parse tree is deleted. Despite its empirical suc-
cess, such a model fails to generate compressions
that are not subject to the subtree constraint (see
Figure 1). In fact, we parsed the Edinburgh sen-
tence compression corpus using the MSTparser
1
,
1
http://sourceforge.net/projects/mstparser/
Warrensays the economy continues the steady improvementROOT
Warren says steadythe economy continues the improvementROOT
Figure 1: The compressed sentence is not a sub-
tree of the original sentence. Words in gray are
removed.
and found that 2561 of 5379 sentences (47.6%) do
not satisfy the subtree deletion model.
Methods beyond the subtree model are also ex-
plored. Trevor et al proposed synchronous tree
substitution grammar (Cohn and Lapata, 2009),
which allows local distortion of the tree topolo-
gy and can thus naturally capture structural mis-
matches. (Genest and Lapalme, 2012; Thadani
and McKeown, 2013) proposed the joint compres-
sion model, which simultaneously considers the n-
grammodel and dependency parse tree of the com-
pressed sentence. However, the time complexity
greatly increases since the parse tree dynamical-
ly depends on the compression. They used Integer
Linear Programming (ILP) for inference which re-
quires exponential running time in the worst case.
In this paper, we propose a new exact decod-
ing algorithm for the joint model using dynam-
ic programming. Our method extends Eisner?s
cubic time parsing algorithm by adding signa-
tures to each span, which indicate the number of
deleted words and the rightmost kept word with-
in the span, resulting in O(n
6
) time complexity
andO(n
4
) space complexity. We further propose a
faster approximate algorithm based on Lagrangian
relaxation, which has TO(n
4
) running time and
O(n
3
) space complexity (T is the iteration num-
ber in the subgradient decent algorithm). Experi-
ments on the popular Edinburgh dataset show that
327
x x ... x x ... x ...0 (root) 2 i i+1 jx x1 n
w0idep
w i2dep
w ijdep
w i i+1dep
w2ibgr w i i+1bgr w i+1 jbgr
Figure 2: Graph illustration for the objective func-
tion. In this example, words x
2
, x
i
, x
i+1
, x
j
are
kept, others are deleted. The value of the ob-
jective function is w
tok
2
+ w
tok
i
+ w
tok
i+1
+ w
tok
j
+
w
dep
0i
+w
dep
i2
+w
dep
ii+1
+w
dep
ij
+w
bgr
2i
+w
bgr
ii+1
+w
bgr
i+1j
.
the proposed approach is 10 times faster than a
high-performance commercial ILP solver.
2 Task Definition
We define the sentence compression task as: given
a sentence composed of n words, x = x
1
, . . . , x
n
,
and a length L ? n, we need to remove (n ? L)
words from x, so that the sum of the weights of
the dependency tree and word bigrams of the re-
maining part is maximized. Formally, we solve
the following optimization problem:
max
z,y
?
i
w
tok
i
z
i
+
?
i,j
w
dep
ij
z
i
z
j
y
ij
(1)
+
?
i<j
w
bgr
ij
z
i
z
j
?
i<k<j
(1? z
k
)
s.t. z is binary ,
?
i
z
i
= L
y is a projective parse tree over the
subgraph: {x
i
|z
i
= 1}
where z is a binary vector, z
i
indicates x
i
is kep-
t or not. y is a square matrix denoting the pro-
jective dependency parse tree over the remaining
words, y
ij
indicates if x
i
is the head of x
j
(note
that each word has exactly one head). w
tok
i
is the
informativeness of x
i
, w
bgr
ij
is the score of bigram
x
i
x
j
in an n-gram model, w
dep
is the score of de-
pendency arc x
i
? x
j
in an arc-factored depen-
dency parsing model. Hence, the first part of the
objective function is the total score of the kep-
t words, the second and third parts are the scores
of the parse tree and bigrams of the compressed
sentence, z
i
z
j
?
i<k<j
(1? z
k
) = 1 indicates both
x
i
and x
j
are kept, and are adjacent after compres-
sion. A graph illustration of the objective function
is shown in Figure 2.
Warren says steadythe economy continues the improvementROOT
Figure 3: Connect deleted words using virtual arc-
s.
3 Proposed Method
3.1 Eisner?s Cubic Time Parsing Algorithm
Throughout the paper, we assume that all the parse
trees are projective. Our method is a generaliza-
tion of Eisner?s dynamic programming algorithm
(Eisner, 1996), where two types of structures are
used in each iteration, incomplete spans and com-
plete spans. A span is a subtree over a number of
consecutive words, with the leftmost or the right-
most word as its root. An incomplete span denoted
as I
i
j
is a subtree inside a single arc x
i
? x
j
, with
root x
i
. A complete span is denoted as C
i
j
, where
x
i
is the root of the subtree, and x
j
is the furthest
descendant of x
i
.
Eisner?s algorithm searches the optimal tree in
a bottom up order. In each step, it merges two
adjacent spans into a larger one. There are two
rules for merging spans: one merges two complete
spans into an incomplete span, the other merges an
incomplete span and a complete span into a large
complete span.
3.2 Exact O(n
6
) Time Algorithm
First we consider an easy case, where the bigram
scores w
bgr
ij
in the objective function are ignored.
The scores of unigrams w
tok
i
can be transfered
to the dependency arcs, so that we can remove al-
l linear terms w
tok
i
z
i
from the objective function.
That is:
?
i
w
tok
i
z
i
+
?
i,j
w
dep
ij
z
i
z
j
y
ij
=
?
i,j
(w
dep
ij
+ w
tok
j
)z
i
z
j
y
ij
This can be easily verifed. If z
j
= 0, then in both
equations, all terms having z
j
are zero; If z
j
= 1,
i.e., x
j
is kept, since it has exactly one head word
x
k
in the compressed sentence, the sum of the
terms having z
j
is w
tok
j
+ w
dep
kj
for both equations.
Therefore, we only need to consider the scores
of arcs. For any compressed sentence, we could
augment its dependency tree by adding a virtual
328
i i+1i i+1
+ =
i jr+1 j
+ =
i r
i ji+1 j
+ =
i i+1
... ...
i jr j
+ =
i r
Case 1
Case 2
Case 3
Case 4
Figure 4: Merging rules for dependency-factored
sentence compression. Incomplete spans and
complete spans are represented by trapezoids and
triangles respectively.
arc i? 1 ? i for each deleted word x
i
. If the first
word x
1
is deleted, we connect it to the root of the
parse tree x
0
, as shown in Figure 3. In this way,
we derive a full parse tree of the original sentence.
This is a one-to-one mapping. We can reversely
get the the compressed parse tree by removing all
virtual arcs from the full parse tree. We restrict
the score of all the virtual arcs to be zero, so that
scores of the two parse trees are equivalent.
Now the problem is to search the optimal full
parse tree with n? L virtual arcs.
We modify Eisner?s algorithm by adding a sig-
nature to each span indicating the number of vir-
tual arcs within the span. Let I
i
j
(k) and C
i
j
(k)
denote the incomplete and complete spans with k
virtual arcs respectively. When merging two span-
s, there are 4 cases, as shown in Figure 4.
? Case 1 Link two complete spans by a virtual
arc : I
i
i+1
(1) = C
i
i
(0) + C
i+1
i+1
(0).
The two complete spans must be single word-
s, as the length of the virtual arc is 1.
? Case 2 Link two complete spans by a non-
virtual arc: I
i
j
(k) = C
i
r
(k
?
)+C
j
r+1
(k
??
), k
?
+
k
??
= k.
? Case 3 Merge an incomplete span and a com-
plete span. The incomplete span is covered
by a virtual arc: I
i
j
(j ? i) = I
i
i+1
(1) +
C
i+1
j
(j ? i ? 1). The number of the virtu-
al arcs within C
i+1
j
must be j ? i ? 1, since
the descendants of the modifier of a virtual
arc x
j
must be removed.
? Case 4 Merge an incomplete span and a com-
plete span. The incomplete span is covered
by a non-virtual arc: C
i
j
(k) = I
i
r
(k
?
) +
C
r
j
(k
??
), k
?
+ k
??
= k.
The score of the new span is the sum of the two
spans. For case 2, the weight of the dependency
arc i ? j, w
dep
ij
is also added to the final score.
The root node is allowed to have two modifiers:
one is the modifier in the compressed sentence, the
other is the first word if it is removed.
For each combination, the algorithm enumer-
ates the number of virtual arcs in the left and right
spans, and the split position (e.g., k
?
, k
??
, r in case
2), thus it takes O(n
3
) running time. The overall
time complexity is O(n
5
) and the space complex-
ity is O(n
3
).
Next, we consider the bigram scores. The fol-
lowing proposition is obvious.
Proposition 1. For any right-headed span I
i
j
or
C
i
j
, i > j, words x
i
, x
j
must be kept.
Proof. Suppose x
j
is removed, there must be a vir-
tual arc j? 1 ? j which is a conflict with the fact
that x
j
is the leftmost word. As x
j
is a descendant
of x
i
, x
i
must be kept.
When merging two spans, a new bigram is cre-
ated, which connects the rightmost kept words in
the left span and the leftmost kept word in the right
span. According to the proposition above, if the
right span is right-headed, its leftmost word is kep-
t. If the right span is left-headed, there are two
cases: its leftmost word is kept, or no word in the
span is kept. In any case, we only need to consider
the leftmost word in the right span.
Let I
i
j
(k, p) and C
i
j
(k, p) denote the single and
complete span with k virtual arcs and the right-
most kept word x
p
. According to the proposition
above, we have, for any right-headed span p = i.
We slightly modify the two merging rules
above, and obtain:
? Case 2? Link two complete spans by a
non-virtual arc: I
i
j
(k, j) = C
i
r
(k
?
, p) +
C
j
r+1
(k
??
, j), k
?
+ k
??
= k. The score of the
new span is the sum of the two spans plus
w
dep
ij
+ w
bgr
p,r+1
.
329
? Case 4? Merge an incomplete span and a
complete span. The incomplete span is cov-
ered by a non-virtual arc. For left-headed
spans, the rule is C
i
j
(k, q) = I
i
r
(k
?
, p) +
C
r
j
(k
??
, q), k
?
+ k
??
= k, and the score of
the new span is the sum of the two span-
s plus w
bgr
pr
; for right-headed spans, the rule
is C
i
j
(k, i) = I
i
r
(k
?
, i) + C
r
j
(k
??
, r), and the
score of the new span is the sum of the two
spans.
The modified algorithm requires O(n
6
) running
time and O(n
4
) space complexity.
3.3 Approximate O(n
4
) Time Algorithm
In this section, we propose an approximate algo-
rithm where the length constraint
?
i
z
i
= L is re-
laxed by Lagrangian Relaxation. The relaxed ver-
sion of Problem (1) is
min
?
max
z,y
?
i
w
tok
i
z
i
+
?
i,j
w
dep
ij
z
i
z
j
y
ij
(2)
+
?
i<j
w
bgr
ij
z
i
z
j
?
i<k<j
(1? z
k
)
+?(
?
i
z
i
? L)
s.t. z is binary
y is a projective parse tree over the
subgraph: {x
i
|z
i
= 1}
Fixing ?, the optimal z,y can be found using a
simpler version of the algorithm above. We drop
the signature of the virtual arc number from each
span, and thus obtain an O(n
4
) time algorithm. S-
pace complexity is O(n
3
). Fixing z,y, the dual
variable is updated by
? = ? + ?(L?
?
i
z
i
)
where ? > 0 is the learning rate. In this paper, our
choice of ? is the same as (Rush et al, 2010).
4 Experiments
4.1 Data and Settings
We evaluate our method on the data set from
(Clarke and Lapata, 2008). It includes 82
newswire articles with manually produced com-
pression for each sentence. We use the same par-
titions as (Martins and Smith, 2009), i.e., 1,188
sentences for training and 441 for testing.
Our model is discriminative ? the scores of
the unigrams, bigrams and dependency arcs are
the linear functions of features, that is, w
tok
i
=
v
T
f(x
i
), where f is the feature vector of x
i
, and v
is the weight vector of features. The learning task
is to estimate the feature weight vector based on
the manually compressed sentences.
We run a second order dependency parser
trained on the English Penn Treebank corpus to
generate the parse trees of the compressed sen-
tences. Then we augment these parse trees by
adding virtual arcs and get the full parse trees
of their corresponding original sentences. In this
way, the annoation is transformed into a set of
sentences with their augmented parse trees. The
learning task is similar to training a parser. We run
a CRF based POS tagger to generate POS related
features.
We adopt the compression evaluation metric as
used in (Martins and Smith, 2009) that measures
the macro F-measure for the retained unigrams
(F
ugr
), and the one used in (Clarke and Lapata,
2008) that calculates the F1 score of the grammat-
ical relations labeled by RASP (Briscoe and Car-
roll, 2002).
We compare our method with other 4 state-of-
the-art systems. The first is linear chain CRFs,
where the compression task is casted as a bina-
ry sequence labeling problem. It usually achieves
high unigram F1 score but low grammatical rela-
tion F1 score since it only considers the local inter-
dependence between adjacent words. The second
is the subtree deletion model (Berg-Kirkpatrick et
al., 2011) which is solved by integer linear pro-
gramming (ILP)
2
. The third one is the bigram
model proposed by McDonald (McDonald, 2006)
which adopts dynamic programming for efficient
inference. The last one jointly infers tree struc-
tures alongside bigrams using ILP (Thadani and
McKeown, 2013). For fair comparison, system-
s were restricted to produce compressions that
matched their average gold compression rate if
possible.
4.2 Features
Three types of features are used to learn our mod-
el: unigram features, bigram features and depen-
dency features, as shown in Table 1. We also use
the in-between features proposed by (McDonald et
2
We use Gurobi as the ILP solver in the paper.
http://www.gurobi.com/
330
Features for unigram x
i
w
i?2
, w
i?1
, w
i
, w
i+1
, w
i+2
t
i?2
, t
i?1
, t
i
, t
i+1
, t
i+2
w
i
t
i
w
i?1
w
i
, w
i
w
i+1
t
i?2
t
i?1
, t
i?1
t
i
, t
i
t
i+1
, t
i+1
t
i+2
t
i?2
t
i?1
t
i
, t
i?1
t
i
t
i+1
, t
i
t
i+1
t
i+2
whether w
i
is a stopword
Features for selected bigram x
i
x
j
distance between the two words: j ? i
w
i
w
j
, w
i?1
w
j
, w
i+1
w
j
, w
i
w
j?1
, w
i
w
j+1
t
i
t
j
, t
i?1
t
j
, t
i+1
t
j
, t
i
t
j?1
, t
i
t
j+1
Concatenation of the templates above
{t
i
t
k
t
j
|i < k < j}
Dependency Features for arc x
h
? x
m
distance between the head and modifier h?m
dependency type
direction of the dependency arc (left/right)
w
h
w
m
, w
h?1
w
m
, w
h+1
w
m
, w
h
w
m?1
, w
h
w
m+1
t
h
t
m
, t
h?1
t
m
, t
h+1
t
m
, t
h
t
m?1
, t
h
t
m+1
t
h?1
t
h
t
m?1
t
m
, t
h
t
h+1
t
m?1
t
m
t
h?1
t
h
t
m
t
m+1
, t
h
t
h+1
t
m
t
m+1
Concatenation of the templates above
{t
h
t
k
t
m
|x
k
lies between x
h
and x
m
}
Table 1: Feature templates. w
i
denotes the word
form of token x
i
and t
i
denotes the POS tag of x
i
.
al., 2005), which were shown to be very effective
for dependency parsing.
4.3 Results
We show the comparison results in Table 2. As
expected, the joint models (ours and TM13) con-
sistently outperform the subtree deletion model, s-
ince the joint models do not suffer from the sub-
tree restriction. They also outperform McDon-
ald?s, demonstrating the effectiveness of consid-
ering the grammar structure for compression. It
is not surprising that CRFs achieve high unigram
F scores but low syntactic F scores as they do not
System C Rate F
uni
RASP Sec.
Ours(Approx) 0.68 0.802 0.598 0.056
Ours(Exact) 0.68 0.805 0.599 0.610
Subtree 0.68 0.761 0.575 0.022
TM13 0.68 0.804 0.599 0.592
McDonald06 0.71 0.776 0.561 0.010
CRFs 0.73 0.790 0.501 0.002
Table 2: Comparison results under various quality
metrics, including unigram F1 score (F
uni
), syn-
tactic F1 score (RASP), and compression speed
(seconds per sentence). C Rate is the compression
ratio of the system generated output. For fair com-
parison, systems were restricted to produce com-
pressions that matched their average gold com-
pression rate if possible.
consider the fluency of the compressed sentence.
Compared with TM13?s system, our model with
exact decoding is not significantly faster due to the
high order of the time complexity. On the oth-
er hand, our approximate approach is much more
efficient, about 10 times faster than TM13? sys-
tem, and achieves competitive accuracy with the
exact approach. Note that it is worth pointing
out that the exact approach can output compressed
sentences of all lengths, whereas the approximate
method can only output one sentence at a specific
compression rate.
5 Conclusion
In this paper, we proposed two polynomial time
decoding algorithms using joint inference for sen-
tence compression. The first one is an exac-
t dynamic programming algorithm, and requires
O(n
6
) running time. This one does not show
significant advantage in speed over ILP. The sec-
ond one is an approximation of the first algorith-
m. It adopts Lagrangian relaxation to eliminate the
compression ratio constraint, yielding lower time
complexity TO(n
4
). In practice it achieves nearly
the same accuracy as the exact one, but is much
faster.
3
The main assumption of our method is that the
dependency parse tree is projective, which is not
true for some other languages. In that case, our
method is invalid, but (Thadani and McKeown,
2013) still works. In the future, we will study the
non-projective cases based on the recent parsing
techniques for 1-endpoint-crossing trees (Pitler et
al., 2013).
Acknowledgments
We thank three anonymous reviewers for their
valuable comments. This work is partly support-
ed by NSF award IIS-0845484 and DARPA under
Contract No. FA8750-13-2-0041. Any opinion-
s expressed in this material are those of the au-
thors and do not necessarily reflect the views of
the funding agencies.
References
Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein.
2011. Jointly learning to extract and compress. In
Proceedings of ACL-HLT, pages 481?490, June.
3
Our code is available at http://code.google.com/p/sent-
compress/
331
T. Briscoe and J. Carroll. 2002. Robust accurate statis-
tical annotation of general text.
Yllias Chali and Sadid A. Hasan. 2012. On the effec-
tiveness of using sentence compression models for
query-focused multi-document summarization. In
Proceedings of COLING, pages 457?474.
James Clarke and Mirella Lapata. 2008. Global in-
ference for sentence compression: An integer linear
programming approach. J. Artif. Intell. Res. (JAIR),
31:399?429.
Trevor Cohn and Mirella Lapata. 2009. Sentence
compression as tree transduction. J. Artif. Int. Res.,
34(1):637?674, April.
Jason M. Eisner. 1996. Three new probabilistic mod-
els for dependency parsing: an exploration. In Pro-
ceedings of COLING.
Pierre-Etienne Genest and Guy Lapalme. 2012. Fully
abstractive approach to guided summarization. In
Proceedings of the ACL, pages 354?358.
Chen Li, Fei Liu, Fuliang Weng, and Yang Liu. 2013.
Document summarization via guided sentence com-
pression. In Proceedings of EMNLP, October.
Fei Liu and Yang Liu. 2009. From extractive to ab-
stractive meeting summaries: Can it be done by
sentence compression? In Proceedings of ACL-
IJCNLP 2009, pages 261?264, August.
Andr?e F. T. Martins and Noah A. Smith. 2009. Sum-
marization with a joint model for sentence extraction
and compression. In Proceedings of the Workshop
on Integer Linear Programming for Natural Lan-
gauge Processing, pages 1?9.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
pendency parsers. In Proceedings of ACL.
Ryan McDonald. 2006. Discriminative Sentence
Compression with Soft Syntactic Constraints. In
Proceedings of EACL, April.
Hajime Morita, Ryohei Sasano, Hiroya Takamura, and
Manabu Okumura. 2013. Subtree extractive sum-
marization via submodular maximization. In Pro-
ceedings of ACL, pages 1023?1032, August.
Emily Pitler, Sampath Kannan, and Mitchell Marcus.
2013. Finding optimal 1-endpoint-crossing trees. In
Transactions of the Association for Computational
Linguistics, 2013 Volume 1.
Xian Qian and Yang Liu. 2013. Fast joint compression
and summarization via graph cuts. In Proceedings
of EMNLP, pages 1492?1502, October.
Alexander M Rush, David Sontag, Michael Collins,
and Tommi Jaakkola. 2010. On dual decomposi-
tion and linear programming relaxations for natural
language processing. In Proceedings of EMNLP.
Kapil Thadani and Kathleen McKeown. 2013. Sen-
tence compression with joint structural inference. In
Proceedings of the CoNLL, August.
332
Proceedings of the ACL 2014 Student Research Workshop, pages 86?93,
Baltimore, Maryland USA, June 22-27 2014. c?2014 Association for Computational Linguistics
Improving Text Normalization via Unsupervised Model and
Discriminative Reranking
Chen Li and Yang Liu
The University of Texas at Dallas
Computer Science Department
chenli,yangl@hlt.utdallas.edu
Abstract
Various models have been developed for
normalizing informal text. In this paper,
we propose two methods to improve nor-
malization performance. First is an unsu-
pervised approach that automatically iden-
tifies pairs of a non-standard token and
proper word from a large unlabeled cor-
pus. We use semantic similarity based on
continuous word vector representation, to-
gether with other surface similarity mea-
surement. Second we propose a reranking
strategy to combine the results from differ-
ent systems. This allows us to incorporate
information that is hard to model in indi-
vidual systems as well as consider multi-
ple systems to generate a final rank for a
test case. Both word- and sentence-level
optimization schemes are explored in this
study. We evaluate our approach on data
sets used in prior studies, and demonstrate
that our proposed methods perform better
than the state-of-the-art systems.
1 Introduction
There has been a lot of research efforts recently
on analysis of social media text (e.g., from Twit-
ter and Facebook) (Ritter et al., 2011; Owoputi et
al., 2013; Liu et al., 2012b). One challenge in
processing social media text is how to deal with
the frequently occurring non-standard words, such
as bday (meaning birthday), snd (meaning sound)
and gl (meaning girl) . Normalizing informal text
(changing non-standard words to standard ones)
will ease subsequent language processing mod-
ules.
Text normalization has been an important topic
for the text-to-speech field. See (Sproat et al.,
2001) for a good report of this problem. Recently,
much research on normalization has been done
for social text domain, which has many abbrevi-
ations or non-standard tokens. A simple approach
for normalization would be applying traditional
spell checking model, which is usually based on
edit distance (Damerau, 1964; Levenshtein, 1966).
However, this model can not well handle the non-
standard words in social media text due to the large
variation in generating them.
Another line of work in normalization adopts
a noisy channel model. For a non-standard to-
ken A, this method finds the most possible stan-
dard word ?S based on the Bayes rule: ?S =
argmaxP (S|A) = argmaxP (A|S) ? P (S).
Different methods have been used to compute
P (A|S). Pennell and Liu (2010) used a CRF se-
quence modeling approach for deletion-based ab-
breviations. Liu et al. (2011) further extended this
work by considering more types of non-standard
words without explicit pre-categorization for non-
standard tokens.
In addition, the noisy channel model has also
been utilized on the sentence level. Choudhury et
al. (2007) used a hidden Markov model to sim-
ulate SMS message generation, considering the
non-standard tokens in the input sentence as emis-
sion states in HMM and labeling results as pos-
sible candidates. Cook and Stevenson (2009) ex-
tended work by adding several more subsystems
in this error model according to the most common
non-standard token?s formation process.
Machine translation (MT) is another commonly
chosen method for text normalization. It is also
used on both the token and the sentence level. Aw
et al. (2006) treated SMS as another language, and
used MT methods to translate this ?foreign lan-
guage? to regular English. Contractor et al. (2010)
used an MT model as well but the focus of their
work is to utilize an unsupervised method to clean
noisy text. Pennell and Liu (2011) firstly intro-
duced an MT method at the token level which
translates an unnormalized token to a possible cor-
86
rect word.
Recently, a new line of work surges relying on
the analysis of huge amount of twitter data, of-
ten in an unsupervised fashion. By using con-
text information from a large corpus, Han et al.
(2012) generated possible variant and normaliza-
tion pairs, and constructed a dictionary of lexical
variants of known words, which are further ranked
by string similarity. This dictionary can facilitate
lexical normalization via simple string substitu-
tion. Hassan and Menezes (2013) proposed an ap-
proach based on the random walk algorithm on a
contextual similarity bipartite graph, constructed
from n-gram sequences on a large unlabeled text
corpus. Yang and Eisenstein (2013) presented a
unified unsupervised statistical model for text nor-
malization.
2 Previous Normalization Methods Used
in Reranking
In this work we adopt several normalization meth-
ods developed in previous studies. The following
briefly describes these previous approaches. Next
section will introduce our proposed methods using
unsupervised learning and discriminative rerank-
ing for system combination.
2.1 Character-block level MT
Pennell and Liu (2011) proposed to use a
character-level MT model for text normalization.
The idea is similar to traditional translation,
except that the translation unit is characters,
not words. Formally, for a non-standard word
A = a
1
a
2
...a
n
, the MT method finds the
most likely standard word S = s
1
s
2
...s
m
(a
i
and s
i
are the characters in the words): S =
argmaxP (S|A) = argmaxP (A|S)P (S) =
argmaxP (a
1
a
2
...a
n
|s
1
s
2
...s
m
)P (s
1
s
2
...s
m
)
where P (a
1
a
2
...a
n
|s
1
s
2
...s
m
) is from a character-
level translation model, and P (s
1
s
2
...s
m
) is from
a character-level language model. (Li and Liu,
2012a) modified this approach to perform the
translation at the character-block level in order
to generate better alignment between characters
(analogous to the word vs. phrase based alignment
in traditional MT). This system generates one
ranked list of word candidates.
2.2 Character-level Two-step MT
Li and Liu (2012b) extended the character-level
MT model by incorporating the pronunciation in-
formation. They first translate non-standard words
to possible pronunciations, which are then trans-
lated to standard words in the second step. This
method has been shown to yield high coverage
(high accuracy in its n-best hypotheses). There are
two candidate lists generated by this two-step MT
method. The first one is based on the pronuncia-
tion list produced in the first step (some phonetic
sequences directly correspond to standard words).
The second list is generated from the second trans-
lation step.
2.3 Character-Block level Sequence Labeling
Pennell and Liu (2010) used sequence labeling
model (CRF) for normalizing deletion-based ab-
breviation at the character-level. The model labels
every character in a standard word as ?Y? or ?N?
to represent whether it appears or not in a possible
abbreviation token. The features used for the clas-
sification task represent the character?s position,
pronunciation and context information. Using the
sequence labeling model, a standard word can
generate many possible non-standard words. A re-
verse look-up table is used to store the correspond-
ing possible standard words for the non-standard
words for reverse lookup during testing. Liu et al.
(2011) extended the above model to handle other
types of non-standard words. (Li and Liu, 2012a)
used character-blocks (same ones as that in the
character-block MT method above) as the units in
this sequence labeling framework. There is one
list of word candidates from this method.
2.4 Spell Checker
The forth normalization subsystem is the Jazzy
Spell Checker1, which is based on edit distance
and integrates a phonetic matching algorithm as
well. This provides one list of hypotheses.
3 Proposed Method
All the above models except the Spell Checker are
supervised methods that need labeled data con-
sisting of pairs of non-standard words and proper
words. In this paper we propose an unsupervised
method to create the lookup table of the non-
standard words and their corresponding proper
words offline. We further propose to use differ-
ent discriminative reranking approaches to com-
bine multiple individual systems.
1http://jazzy.sourceforge.net
87
3.1 Unsupervised Corpus-based Similarity
for Normalization
Previous work has shown that unlabeled text can
be used to induce unsupervised word clusters
that can improve performance of many supervised
NLP tasks (Koo et al., 2008; Turian et al., 2010;
Ta?ckstro?m et al., 2012). We investigate using a
large unlabeled Twitter corpus to automatically
identify pairs of non-standard words and their cor-
responding standard words.
We use the Edinburgh Twitter corpus (Petro-
vic et al., 2010), and a dictionary obtained
from http://ciba.iciba.com/ to identify all the in-
vocabulary and out-of-vocabulary (OOV) words in
the corpus. The task is then to automatically find
the corresponding OOV words (if any) for each
dictionary word, and the likelihood of each pair.
The key question is how to compute this likelihood
or similarity.
We propose to use an unsupervised method
based on the large corpus to induce dense real-
valued low-dimension word embedding and then
use the inner product as a measure of semantic
similarity. We use the continuous bag-of-words
model that is similar to the feedforward neural
network language model to compute vector rep-
resentations of words. This model was first in-
troduced by (Mikolov et al., 2013). We use the
tool word2vec2 to implement this model. Two
constraints are used in order to eliminate unlikely
word pairs: (I) OOV words need to begin with the
same letter as the dictionary standard word; (II)
OOV words can only consist of English letter and
digits.
In addition to considering the above semantic
similarity, for the normalization task, we use other
information including the surface character level
similarity based on longest common sequence be-
tween the two tokens, and the frequency of the to-
ken. The final score between a dictionary word w
and an OOV word t is:
sim(w, t) =
longest common string(w, t)
length(t)
? log(TermFreq(t))
? inner product(vec(w), vec(t))
?
longest common seq(w, t)
length(t)
(1)
The first and second term share the same property
of visual prime value used in (Liu et al., 2012a).
2https://code.google.com/p/word2vec/
The third term is the vector-based semantic simi-
larity of the two words, calculated by our proposed
model. The last term is the length of longest com-
mon sequence between the two words divided by
the length of the OOV word.
Using this method, we can identify all the pos-
sible OOV words for each dictionary word based
on an unlabeled large corpus. Each pair has a
similarity score. Then a reverse lookup table is
created to store the corresponding possible stan-
dard words for each non-standard word, which is
used during testing. This framework is similar to
the sequence labeling method described in Sec-
tion 2.3 in the sense of creating the mapping ta-
ble between the OOV and dictionary words. How-
ever, the difference is that this is an unsupervised
method whereas the sequence labeling uses super-
vised learning to generate possible candidates.
3.2 Reranking for System Combination
3.2.1 Word Level Reranking
Each of the above systems has its own strength and
weakness. The MT model and the sequence la-
beling models have better precision, the two-step
MT model has a broader coverage of candidates,
and the spell checker has a high confidence for
simple non-standard words. Therefore combining
these systems is expected to yield better overall
results. We propose to use a supervised maximum
entropy reranking model to combine our proposed
unsupervised method with those described in Sec-
tion 2 (4 systems that have 5 candidate lists). The
features we used in the normalization reranking
model are shown in Table 1. This maxent rerank-
ing method has shown success in many previous
work such as (Charniak and Johnson, 2005; Ji et
al., 2006).
Features:
1.Boolean value to indicate whether a candidate is on the
list of each system. There are 6 lists and thus 6 such fea-
tures.
2.A concatenation of the 6 boolean features above.
3.The position of this candidate in each candidate list. If
this candidate is not on a list, the value of this feature is -1
for that list.
4.The unigram language model probability of the candi-
date.
5.Boolean value to indicate whether the first character of
the candidate and non-standard word is the same.
6.Boolean value to indicate whether the last character of
the candidate and non-standard word is the same.
Table 1: Features for Reranking.
The first three features are related to the indi-
88
vidual systems, and the last three features com-
pare the candidate with the non-standard word. It
is computationally expensive to include informa-
tion represented in the last three features in the in-
dividual systems since they need to consider more
candidates in the normalization step; whereas in
reranking, only a small set of word candidates
are evaluated, thus it is more feasible to use such
global features in the reranking model. We also
tried some other lexical features such as the length
difference of the non-standard word and the can-
didate, whether non-standard word contains num-
bers, etc. But they did not obtain performance
gain. Another advantage of the reranker is that we
can use information about multiple systems, such
as the first three features.
3.2.2 Sentence Level Reranking and
Decoding
In the above reranking method, we only use infor-
mation about the individual words. When contex-
tual words are available (in sentences or Tweets),
we can use that information. If a sentence con-
taining OOV words is given during testing, we
can perform standard sentence level Viterbi decod-
ing to combine information from the normaliza-
tion candidates and language model scores.
Furthermore, if sentences are available during
training (not just isolated word pairs as used in all
the previous supervised individual systems and the
Maxent reranking above), we can also use contex-
tual information for training the reranker. This can
be achieved in two different ways. First, we add
the Language Model score from context words as
features in the reranker. In this work, in addition to
the features in Table 1, we add a trigram probabil-
ity to represent the context information. For every
candidate of a non-standard word, we use trigram
probability from the language model. The trigram
consists of this candidate, and the previous and the
following token of the non-standard word. If the
previous/following word is also a non-standard to-
ken, then we calculate the trigram using all of their
candidates and then take the average. After adding
the additional LM probability feature, the same
Maxent reranking method as above is used, which
optimizes the word level accuracy.
The second method is to change the training ob-
jective and perform the optimization at the sen-
tence level. The feature set can be the same as the
word level reranker, or with the additional contex-
tual LM score features. To train the model (feature
weights), we perform sentence level Viterbi de-
coding on the training set to find the best hypoth-
esis for each non-standard word. If the hypothe-
sis is incorrect, we update the feature weight us-
ing structured perceptron strategy (Collins, 2002).
We will explore these different feature and train-
ing configurations for reranking in the following
experiments.
4 Experiments
4.1 Experimental Setup
The following data sets are used in our experi-
ments. We use Data 1 and Data 2 as test data, and
Data 3 as training data for all the supervised mod-
els.
? Data 1: 558 pairs of non-standard tokens and
standard words collected from 549 tweets in
2010 by (Han and Baldwin, 2011).
? Data 2: 3,962 pairs of non-standard tokens
and standard words collected from 6,160
tweets between 2009 and 2010 by (Liu et al.,
2011).
? Data 3: 2,333 unique pairs of non-standard
tokens and standard words, collected from
2,577 Twitter messages (selected from the
Edinburgh Twitter corpus) used in (Pennell
and Liu, 2011). We made some changes on
this data, removing the pairs that have more
than one proper words, and sentences that
only contain such pairs.3
? Data 4: About 10 million twitter messages
selected from the the Edinburgh Twitter cor-
pus mentioned above, consisting of 3 million
unique tokens. This data is used by the un-
supervised method to create the mapping ta-
ble, and also for building the word-based lan-
guage model needed in sentence level nor-
malization.
The dictionary we used is obtained from
http://ciba.iciba.com/, which includes 75,262 En-
glish word entries and their corresponding pho-
netic symbols (IPA symbols). This is used in var-
ious modules in the normalization systems. The
number of the final standard words used to create
the look-up table is 10,105 because we only use
the words that have the same number of character-
block segments and phones. These 10,105 words
3http://www.hlt.utdallas.edu/?chenli/normalization
89
cover 90.77% and 93.74% standard words in Data
set 1 and Data set 2 respectively. For the non-
standard words created in the CRF model, they
cover 80.47% and 86.47% non-standard words in
Data set1 and Data set 2. This coverage using the
non-standard words identified by the new unsuper-
vised model is 91.99% and 92.32% for the two
data sets, higher than that by the CRF model.
During experiments, we use CRF++ toolkit 4
for our sequence labeling model, SRILM toolkit
(Stolcke, 2002) to build all the language models,
Giza++ (Och and Ney, 2003) for automatic word
alignment, and Moses (Koehn et al., 2007) for
translation decoding in three MT systems.
4.2 Isolated Word Normalization
Experiments
Table 2 shows the isolated word normalization re-
sults on the two test data sets for various systems.
The performance metrics include the accuracy for
the top-1 candidate and other top-N candidates.
Coverage means how many test cases correct an-
swers can be obtained in the final list regardless
of its positions. The top part presents the results
on Data Set 1 and the bottom shows the results on
Data Set 2. We can see that our proposed unsu-
pervised corpus similarity model achieves better
top-1 accuracy than the other individual systems
described in Section 2. Its top-n coverage is not
always the best ? the 2-step MT method has advan-
tages in its coverage. The results in the table also
show that reranking improves system performance
over any of the used individual systems, which is
expected. After reranking, on Data set 1, our sys-
tem yields better performance than previously re-
ported ones. On Data set 2, it has better top-1 ac-
curacy than (Liu et al., 2012a), but slightly worse
top-N coverage. However, the method in (Liu et
al., 2012a) has higher computational cost because
of the calculation of the prime visual values for
each non-standard word on the fly during testing.
In addition, they also used more training data than
ours.
4.3 Sentence Level Normalization Results
We have already seen that after reranking we ob-
tain better word-level normalization performance,
for both top-1 and other top-N candidates. One
follow-up question is whether this improved per-
formance carries over to sentence level normaliza-
4http://crfpp.googlecode.com/
System Accuracy %Top1 Top3 Top10 Top20 Cover
Data 1
MT 61.81 73.53 78.50 79.57 80.00
MT21 39.61 52.93 63.59 65.36 65.72
MT22 53.64 68.56 77.44 80.46 88.10
SL 53.29 61.99 69.09 71.92 75.85
SC 50.27 56.31 56.84 57.02 57.02
UCS 61.81 69.98 74.60 76.55 82.17
Rerank 77.14 86.96 93.04 94.82 95.90
Sys1 75.69 n/a n/a n/a n/a
Sys2 73 81.9 86.7 89.2 94.2
Data 2
MT 55.02 63.3 66.99 67.77 68.00
MT21 35.64 47.65 54.67 56.01 56.4
MT22 49.02 62.49 70.99 74.86 80.07
SL 46.52 55.05 61.21 62.97 66.21
SC 51.16 55.48 55.88 55.88 55.88
UCS 57.29 65.75 70.55 72.64 80.84
Rerank 74.44 84.57 90.25 92.37 93.5
Sys1 69.81 82.51 92.24 93.79 95.71
Sys2 62.6 75.1 84 87.5 90.7
Sys3 73.04 n/a n/a n/a n/a
Table 2: MT: Character-block Level MT;
MT21&MT22: First&Second step in Character-
level Two-step MT; SL: Sequence Labeling sys-
tem; SC: Spell Checker; UCS: Unsupervised Cor-
pus Similarity Model; Sys1 is from (Liu et al.,
2012a); Sys2 is from (Li and Liu, 2012a); Sys3
is from (Yang and Eisenstein, 2013).
tion when context information is used via the in-
corporation of a language model. Since detecting
which tokens need normalization in the first place
is a hard task itself in social media text and is an
open question currently, similar to some previous
work, we assume that we already know the non-
standard words that need to be normalized for a
given sentence. Then the sentence-level normal-
ization task is just to find which candidate from
the n-best lists for each of those already ?detected?
non-standard words is the best one. We use the
tweets in the Data set 1 described above because
Data set 2 only has token pairs but not sentences.
Table 3 shows the sentence level normaliza-
tion results using different reranking configura-
tions with respect to the features used in the
reranker and the training process. Regarding fea-
tures, reranker 1 and 3 use the features described
90
in Section 3.2.1, i.e., features based on the words
only, without the additional trigram LM probabil-
ity feature; reranker 2 and 4 use the additional LM
probability feature. About training, reranker 1 and
2 use the Maxent reranking that is trained and op-
timized for the word level; reranker 3 and 4 use
structure perceptron training at the sentence level.
Note that all of the systems perform Viterbi decod-
ing during testing to determine the final top one
candidate for each non-standard word in the sen-
tence. The scores from the reranked normalization
output and the LM probabilities are combined in
decoding. From the results, we can see that adding
contextual information (LM probabilities) as fea-
tures in the reranker is useful. When this feature
is not used, using sentence-level training objec-
tive benefits (reranker 3 outperforms 1); however,
when this feature is used, performing sentence-
level training via structure perceptron is not useful
(reranker 2 outperforms 4), partly because the con-
textual information is incorporated in the features
already and using it in sentence-level decoding for
training is redundant and does not bring additional
gain. Finally compared to the previously report
results, our system performs the best.
System Acc % System Acc %
Reranker1 84.30 Reranker2 86.91
Reranker3 85.03 Reranker4 85.37
Sys1 84.13 Sys2 82.23
Table 3: Sentence level normalization results on
Data Set 1 using different reranking setups. Sys1
is from (Liu et al., 2012a); Sys2 is from (Yang and
Eisenstein, 2013). Acc % is the top one accuracy.
4.4 Impact of Unsupervised Corpus
Similarity Model
Our last question is regarding unsupervised model
importance in the reranking system and contribu-
tions of its different similarity measure compo-
nents. We conduct the following two experiments:
First, we removed the new model and just use the
other remaining models in reranking (five candi-
date lists). Second, we kept this new model but
changed the corpus similarity measure (removed
the third item in Eq(1) that represents the seman-
tic similarity). This way we can evaluate the im-
pact of the semantic similarity measure based on
the continuous word vector representation.
Table 4 shows the word level and sentence re-
sults on Data set 1 and 2 using these different
setups. Because of space limit, we only present
the top one accuracy. The other top-n results
have similar patterns. Sentence level normaliza-
tion uses the Reranker 2 described above. We can
see that there is a degradation in both of the new
setups, suggesting that the unsupervised method
itself is beneficial, and in particular the word vec-
tor based semantic similarity component is crucial
to the system performance.
System Word Level Sent LevelData1 Data2 Data1
system-A 73.75 70.33 84.51
system-B 74.77 70.83 86.22
system-C 77.14 74.44 86.91
Table 4: Word level and Sentence level normaliza-
tion results (top-1 accuracy in %) after reranking
on Data Set 1 and 2. System-A is without using
the unsupervised model, system-B is without its
semantic similarity measure, and system-C is our
proposed system.
5 Conclusions
In this paper, we proposed a novel normalization
system by using unsupervised methods in a large
corpus to identify non-standard words and their
corresponding proper words. We further combine
it with several previously developed normalization
systems by a reranking strategy. In addition, we
explored different sentence level reranking meth-
ods to evaluate the impact of context information.
Our experiments show that the reranking system
not only significantly improves the word level nor-
malization accuracy, but also helps the sentence
level decoding. In the future work, we plan to ex-
plore more useful features and also leverage pair-
wise and link reranking strategy.
Acknowledgments
We thank the NSF for travel and conference sup-
port for this paper. The work is also partially sup-
ported by DARPA Contract No. FA8750-13-2-
0041. Any opinions, findings, and conclusions or
recommendations expressed are those of the au-
thor and do not necessarily reflect the views of the
funding agencies.
91
References
Aiti Aw, Min Zhang, Juan Xiao, Jian Su, and Jian Su.
2006. A phrase-based statistical model for sms text
normalization. In Processing of COLING/ACL.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative
reranking. In Proceedings of the 43rd ACL.
Monojit Choudhury, Rahul Saraf, Vijit Jain, Animesh
Mukherjee, Sudeshna Sarkar, and Anupam Basu.
2007. Investigation and modeling of the structure
of texting language. IJDAR.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of EMNLP.
Danish Contractor, Tanveer A. Faruquie, L. Venkata
Subramaniam, and L. Venkata Subramaniam. 2010.
Unsupervised cleansing of noisy text. In Proceed-
ings of COLING.
Paul Cook and Suzanne Stevenson. 2009. An unsu-
pervised model for text message normalization. In
Proceedings of NAACL.
Fred J Damerau. 1964. A technique for computer de-
tection and correction of spelling errors. Communi-
cations of the ACM, 7(3):171?176.
Bo Han and Timothy Baldwin. 2011. Lexical normali-
sation of short text messages: Makn sens a #twitter.
In Proceeding of 49th ACL.
Bo Han, Paul Cook, and Timothy Baldwin. 2012. Au-
tomatically constructing a normalisation dictionary
for microblogs. In Proceedings of the 2012 EMNLP.
Hany Hassan and Arul Menezes. 2013. Social text
normalization using contextual graph random walks.
In Proceedings of ACL.
Heng Ji, Cynthia Rudin, and Ralph Grishman. 2006.
Re-ranking algorithms for name tagging. In Pro-
ceedings of the Workshop on Computationally Hard
Problems and Joint Inference in Speech and Lan-
guage Processing.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, Evan Herbst, and Evan Herbst. 2007.
Moses: Open source toolkit for statistical machine
translation. In Proceedings of ACL.
Terry Koo, Xavier Carreras, and Michael Collins.
2008. Simple semi-supervised dependency parsing.
In Proceedings of ACL.
Vladimir I Levenshtein. 1966. Binary codes capable
of correcting deletions, insertions and reversals. In
Soviet physics doklady, volume 10, page 707.
Chen Li and Yang Liu. 2012a. Improving text nor-
malization using character-blocks based models and
system combination. In Proceedings of COLING
2012.
Chen Li and Yang Liu. 2012b. Normalization of text
messages using character- and phone-based machine
translation approaches. In Proceedings of 13th In-
terspeech.
Fei Liu, Fuliang Weng, Bingqing Wang, and Yang Liu.
2011. Insertion, deletion, or substitution?: normal-
izing text messages without pre-categorization nor
supervision. In Proceedings of the 49th ACL: short
papers.
Fei Liu, Fuliang Weng, and Xiao Jiang. 2012a. A
broad-coverage normalization system for social me-
dia language. In Proceedings of the 50th ACL.
Xiaohua Liu, Ming Zhou, Xiangyang Zhou,
Zhongyang Fu, and Furu Wei. 2012b. Joint
inference of named entity recognition and normal-
ization for tweets. In Proceedings of ACL.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word represen-
tations in vector space. Proceedings of Workshop at
ICLR.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Olutobi Owoputi, Brendan O?Connor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A.
Smith. 2013. Improved part-of-speech tagging for
online conversational text with word clusters. In
Proceedings of NAACL.
Deana Pennell and Yang Liu. 2010. Normalization of
text messages for text-to-speech. In ICASSP.
Deana Pennell and Yang Liu. 2011. A character-level
machine translation approach for normalization of
sms abbreviations. In Proceedings of 5th IJCNLP.
Sasa Petrovic, Miles Osborne, and Victor Lavrenko.
2010. The edinburgh twitter corpus. In Proceedings
of NAACL.
Alan Ritter, Sam Clark, Oren Etzioni, et al. 2011.
Named entity recognition in tweets: an experimental
study. In Proceedings of EMNLP.
Richard Sproat, Alan W. Black, Stanley F. Chen,
Shankar Kumar, Mari Ostendorf, and Christopher
Richards. 2001. Normalization of non-standard
words. Computer Speech & Language, 15(3):287?
333.
Andreas Stolcke. 2002. SRILM-an extensible lan-
guage modeling toolkit. In Proceedings of Interna-
tional Conference on Spoken Language Processing.
92
Oscar Ta?ckstro?m, Ryan McDonald, and Jakob Uszko-
reit. 2012. Cross-lingual word clusters for direct
transfer of linguistic structure. In Proceedings of
NAACL.
Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio.
2010. Word representations: A simple and general
method for semi-supervised learning. In Proceed-
ings of ACL.
Yi Yang and Jacob Eisenstein. 2013. A log-linear
model for unsupervised text normalization. In Pro-
ceedings of EMNLP.
93
Transactions of the Association for Computational Linguistics, 1 (2013) 37?48. Action Editor: Ryan McDonald.
Submitted 11/2012; Revised 2/2013; Published 3/2013. c?2013 Association for Computational Linguistics.
Branch and Bound Algorithm for Dependency Parsing
with Non-local Features
Xian Qian and Yang Liu
Computer Science Department
The University of Texas at Dallas
{qx,yangl}@hlt.utdallas.edu
Abstract
Graph based dependency parsing is inefficient
when handling non-local features due to high
computational complexity of inference. In
this paper, we proposed an exact and effi-
cient decoding algorithm based on the Branch
and Bound (B&B) framework where non-
local features are bounded by a linear combi-
nation of local features. Dynamic program-
ming is used to search the upper bound. Ex-
periments are conducted on English PTB and
Chinese CTB datasets. We achieved competi-
tive Unlabeled Attachment Score (UAS) when
no additional resources are available: 93.17%
for English and 87.25% for Chinese. Parsing
speed is 177 words per second for English and
97 words per second for Chinese. Our algo-
rithm is general and can be adapted to non-
projective dependency parsing or other graph-
ical models.
1 Introduction
For graph based projective dependency parsing, dy-
namic programming (DP) is popular for decoding
due to its efficiency when handling local features.
It performs cubic time parsing for arc-factored mod-
els (Eisner, 1996; McDonald et al, 2005a) and bi-
quadratic time for higher order models with richer
sibling and grandchild features (Carreras, 2007; Koo
and Collins, 2010). However, for models with gen-
eral non-local features, DP is inefficient.
There have been numerous studies on global in-
ference algorithms for general higher order parsing.
One popular approach is reranking (Collins, 2000;
Charniak and Johnson, 2005; Hall, 2007). It typi-
cally has two steps: the low level classifier gener-
ates the top k hypotheses using local features, then
the high level classifier reranks these candidates us-
ing global features. Since the reranking quality is
bounded by the oracle performance of candidates,
some work has combined candidate generation and
reranking steps using cube pruning (Huang, 2008;
Zhang and McDonald, 2012) to achieve higher or-
acle performance. They parse a sentence in bottom
up order and keep the top k derivations for each s-
pan using k best parsing (Huang and Chiang, 2005).
After merging the two spans, non-local features are
used to rerank top k combinations. This approach
is very efficient and flexible to handle various non-
local features. The disadvantage is that it tends to
compute non-local features as early as possible so
that the decoder can utilize that information at inter-
nal spans, hence it may miss long historical features
such as long dependency chains.
Smith and Eisner modeled dependency parsing
using Markov Random Fields (MRFs) with glob-
al constraints and applied loopy belief propaga-
tion (LBP) for approximate learning and inference
(Smith and Eisner, 2008). Similar work was done
for Combinatorial Categorial Grammar (CCG) pars-
ing (Auli and Lopez, 2011). They used posterior
marginal beliefs for inference to satisfy the tree con-
straint: for each factor, only legal messages (satisfy-
ing global constraints) are considered in the partition
function.
A similar line of research investigated the use
of integer linear programming (ILP) based parsing
(Riedel and Clarke, 2006; Martins et al, 2009). This
37
method is very expressive. It can handle arbitrary
non-local features determined or bounded by linear
inequalities of local features. For local models, LP is
less efficient than DP. The reason is that, DP works
on a small number of dimensions in each recursion,
while for LP, the popular revised simplex method
needs to solve a m dimensional linear system in
each iteration (Nocedal and Wright, 2006), where
m is the number of constraints, which is quadratic
in sentence length for projective dependency pars-
ing (Martins et al, 2009).
Dual Decomposition (DD) (Rush et al, 2010;
Koo et al, 2010) is a special case of Lagrangian re-
laxation. It relies on standard decoding algorithms
as oracle solvers for sub-problems, together with a
simple method for forcing agreement between the
different oracles. This method does not need to con-
sider the tree constraint explicitly, as it resorts to dy-
namic programming which guarantees its satisfac-
tion. It works well if the sub-problems can be well
defined, especially for joint learning tasks. Howev-
er, for the task of dependency parsing, using various
non-local features may result in many overlapped
sub-problems, hence it may take a long time to reach
a consensus (Martins et al, 2011).
In this paper, we propose a novel Branch and
Bound (B&B) algorithm for efficient parsing with
various non-local features. B&B (Land and Doig,
1960) is generally used for combinatorial optimiza-
tion problems such as ILP. The difference between
our method and ILP is that the sub-problem in ILP
is a relaxed LP, which requires a numerical solution,
while ours bounds the non-local features by a lin-
ear combination of local features and uses DP for
decoding as well as calculating the upper bound of
the objective function. An exact solution is achieved
if the bound is tight. Though in the worst case,
time complexity is exponential in sentence length,
it is practically efficient especially when adopting a
pruning strategy.
Experiments are conducted on English PennTree
Bank and Chinese Tree Bank 5 (CTB5) with stan-
dard train/develop/test split. We achieved 93.17%
Unlabeled Attachment Score (UAS) for English at a
speed of 177 words per second and 87.25% for Chi-
nese at a speed of 97 words per second.
2 Graph Based Parsing
2.1 Problem Definition
Given a sentence x = x1, x2, . . . , xn where xi is
the ith word of the sentence, dependency parsing as-
signs exactly one head word to each word, so that
dependencies from head words to modifiers form a
tree. The root of the tree is a special symbol de-
noted by x0 which has exactly one modifier. In this
paper, we focus on unlabeled projective dependency
parsing but our algorithm can be adapted for labeled
or non-projective dependency parsing (McDonald et
al., 2005b).
The inference problem is to search the optimal
parse tree y?
y? = argmaxy?Y(x)?(x, y)
where Y(x) is the set of all candidate parse trees of
sentence x. ?(x, y) is a given score function which
is usually decomposed into small parts
?(x, y) =
?
c?y
?c(x) (1)
where c is a subset of edges, and is called a factor.
For example, in the all grandchild model (Koo and
Collins, 2010), the score function can be represented
as
?(x, y) =
?
ehm?y
?ehm(x) +
?
egh,ehm?y
?egh,ehm(x)
where the first term is the sum of scores of all edges
xh ? xm, and the second term is the sum of the
scores of all edge chains xg ? xh ? xm.
In discriminative models, the score of a parse tree
y is the weighted sum of the fired feature functions,
which can be represented by the sum of the factors
?(x, y) = wT f(x, y) =
?
c?y
wT f(x, c) =
?
c?y
?c(x)
where f(x, c) is the feature vector that depends on
c. For example, we could define a feature for grand-
child c = {egh, ehm}
f(x, c) =
?
??
??
1 if xg = would ? xh = be
?xm = happy ? c is selected
0 otherwise
38
2.2 Dynamic Programming for Local Models
In first order models, all factors c in Eq(1) contain a
single edge. The optimal parse tree can be derived
by DP with running time O(n3) (Eisner, 1996). The
algorithm has two types of structures: complete s-
pan, which consists of a headword and its descen-
dants on one side, and incomplete span, which con-
sists of a dependency and the region between the
head and modifier. It starts at single word spans, and
merges the spans in bottom up order.
For second order models, the score function
?(x, y) adds the scores of siblings (adjacent edges
with a common head) and grandchildren
?(x, y) =
?
ehm?y
?ehm(x)
+
?
egh,ehm?y
?ehm,egh(x)
+
?
ehm,ehs?y
?ehm,ehs(x)
There are two versions of second order models,
used respectively by Carreras (2007) and Koo et al
(2010). The difference is that Carreras? only con-
siders the outermost grandchildren, while Koo and
Collin?s allows all grandchild features. Both models
permit O(n4) running time.
Third-order models score edge triples such as
three adjacent sibling modifiers, or grand-siblings
that score a word, its modifier and its adjacent grand-
children, and the inference complexity is O(n4)
(Koo and Collins, 2010).
In this paper, for all the factors/features that can
be handled by DP, we call them the local fac-
tors/features.
3 The Proposed Method
3.1 Basic Idea
For general high order models with non-local fea-
tures, we propose to use Branch and Bound (B&B)
algorithm to search the optimal parse tree. A B&B
algorithm has two steps: branching and bounding.
The branching step recursively splits the search s-
pace Y(x) into two disjoint subspaces Y(x) =
Y1
?Y2 by fixing assignment of one edge. For each
subspace Yi, the bounding step calculates the upper
bound of the optimal parse tree score in the sub-
space: UBYi ? maxy?Yi ?(x, y). If this bound is
no more than any obtained parse tree score UBYi ?
?(x, y?), then all parse trees in subspace Yi are no
more optimal than y?, and Yi could be pruned safely.
The efficiency of B&B depends on the branching
strategy and upper bound computation. For exam-
ple, Sun et al (2012) used B&B for MRFs, where
they proposed two branching strategies and a novel
data structure for efficient upper bound computation.
Klenner and Ailloud (2009) proposed a variation of
Balas algorithm (Balas, 1965) for coreference reso-
lution, where candidate branching variables are sort-
ed by their weights.
Our bounding strategy is to find an upper bound
for the score of each non-local factor c containing
multiple edges. The bound is the sum of new scores
of edges in the factor plus a constant
?c(x) ?
?
e?c
?e(x) + ?c
Based on the new scores {?e(x)} and constants
{?c}, we define the new score of parse tree y
?(x, y) =
?
c?y
(?
e?c
?e(x) + ?c
)
Then we have
?(x, y) ? ?(x, y), ?y ? Y(x)
The advantage of such a bound is that, it is the
sum of new edge scores. Hence, its optimum tree
maxy?Y(x) ?(x, y) can be found by DP, which is
the upper bound of maxy?Y(x) ?(x, y), as for any
y ? Y(x), ?(x, y) ? ?(x, y).
3.2 The Upper Bound Function
In this section, we derive the upper bound function
?(x, y) described above. To simplify notation, we
drop x throughout the rest of the paper. Let zc be
a binary variable indicating whether factor c is se-
lected in the parse tree. We reformulate the score
function in Eq(1) as
?(y) ? ?(z) =
?
c
?czc (2)
39
Correspondingly, the tree constraint is replaced by
z ? Z . Then the parsing task is
z? = argmaxz?Z?czc (3)
Notice that, for any zc, we have
zc = mine?c ze
which means that factor c appears in parse tree if and
only if all its edges {e|e ? c} are selected in the tree.
Here ze is short for z{e} for simplicity.
Our bounding method is based on the following
fact: for a set {a1, a2, . . . ar} (aj denotes the jth el-
ement) , its minimum
min{aj} = min
p??
?
j
pjaj (4)
where ? is probability simplex
? = {p|pj ? 0,
?
j
pj = 1}
We discuss the bound for ?czc in two cases: ?c ?
0 and ?c < 0.
If ?c ? 0, we have
?czc = ?cmine?c ze
= ?c minpc??
?
e?c
pecze
= min
pc??
?
e?c
?cpecze
The second equation comes from Eq(4). For sim-
plicity, let
gc(pc, z) =
?
e?c
?cpecze
with domain domgc = {pc ? ?; ze ? {0, 1}, ?e ?
c}. Then we have
?czc = minpc gc(pc, z) (5)
If ?c < 0, we have two upper bounds. One is
commonly used in ILP when all the variables are bi-
nary
a? = min
j
{aj}rj=1
?
a? ? aj
a? ?
?
j
aj ? (r ? 1)
According to the last inequality, we have the upper
bound for negative scored factors
?czc ? ?c
(?
e?c
ze ? (rc ? 1)
)
(6)
where rc is the number of edges in c. For simplicity,
we use the notation
?c(z) = ?c
(?
e?c
ze ? (rc ? 1)
)
The other upper bound when ?c < 0 is simple
?czc ? 0 (7)
Notice that, for any parse tree, one of the upper
bounds must be tight. Eq(6) is tight if c appears
in the parse tree: zc = 1, otherwise Eq(7) is tight.
Therefore
?czc = min {?c(z), 0}
Let
hc(pc, z) = p1c?c(z) + p2c ? 0
with domhc = {pc ? ?; ze ? {0, 1}, ?e ? c}.
According to Eq(4), we have
?czc = minpc hc(pc, z) (8)
Let
?(p, z) =
?
c,?c?0
gc(pc, z) +
?
c,?c<0
hc(pc, z)
Minimize ? with respect to p, we have
min
p
?(p, z)
= min
p
?
? ?
c,?c?0
gc(pc, z) +
?
c,?c<0
hc(pc, z)
?
?
=
?
c,?c?0
min
pc
gc(pc, z) +
?
c,?c<0
min
pc
hc(pc, z)
=
?
c,?c?0
?czc +
?
c,?c<0
?czc
= ?(z)
The second equation holds since, for any two fac-
tors, c and c?, gc (or hc) and gc? (or hc?) are separable.
The third equation comes from Eq(5) and Eq(8).
Based on this, we have the following proposition:
40
Proposition 1. For any p, pc ? ?, and z ? Z ,
?(p, z) ? ?(z).
Therefore, ?(p, z) is an upper bound function of
?(z). Furthermore, fixing p, ?(p, z) is a linear func-
tion of ze , see Eq(5) and Eq(8), variables zc for large
factors are eliminated. Hence z? = argmaxz?(p, z)
can be solved efficiently by DP.
Because
?(p, z?) ? ?(p, z?) ? ?(z?) ? ?(z?)
after obtaining z? , we get the upper bound and lower
bound of ?(z?): ?(p, z?) and ?(z?).
The upper bound is expected to be as tight as pos-
sible. Using min-max inequality, we get
max
z?Z
?(z) = max
z?Z
min
p
?(p, z)
? min
p
max
z?Z
?(p, z)
which provides the tightest upper bound of ?(z?).
Since ? is not differentiable w.r.t p, projected
sub-gradient (Calamai and More?, 1987; Rush et al,
2010) is used to search the saddle point. More
specifically, in each iteration, we first fix p and
search z using DP, then we fix z and update p by
pnew = P?
(
p+ ???p ?
)
where ? > 0 is the step size in line search, function
P?(q) denotes the projection of q onto the proba-
bility simplex ?. In this paper, we use Euclidean
projection, that is
P?(q) = minp?? ?p? q?2
which can be solved efficiently by sorting (Duchi et
al., 2008).
3.3 Branch and Bound Based Parsing
As discussed in Section 3.1, the B&B recursive pro-
cedure yields a binary tree structure called Branch
and Bound tree. Each node of the B&B tree has
some fixed ze, specifying some must-select edges
and must-remove edges. The root of the B&B tree
has no constraints, so it can produce all possible
parse trees including z?. Each node has two chil-
dren. One adds a constraint ze = 1 for a free edge
z =e1 0 1
0 1 0 1z =e2
??=9=4
?<LB
??=8=5 ??=7=4
??=7=4 ??=7=5 ??=4=3 ??=6=2
minp maxz?Z
ze1=0
ze2=1
?(p, z)
6
Figure 1: A part of B&B tree. ?, ? are short for
?(z?) and ?(p?, z?) respectively. For each node,
some edges of the parse tree are fixed. All parse
trees that satisfy the fixed edges compose the subset
of S ? Z . A min-max problem is solved to get the
upper bound and lower bound of the optimal parse
tree over S. Once the upper bound ? is less than
LB, the node is removed safely.
e and the other fixes ze = 0. We can explore the
search space {z|ze ? {0, 1}} by traversing the B&B
tree in breadth first order.
Let S ? Z be subspace of parse trees satisfying
the constraint, i.e., in the branch of the node. For
each node in B&B tree, we solve
p?, z? = argmin
p
max
z?S
?(p, z)
to get the upper bound and lower bound of the best
parse tree in S. A global lower bound LB is main-
tained which is the maximum of all obtained lower
bounds. If the upper bound of the current node is
lower than the global lower bound, the node can be
pruned from the B&B tree safely. An example is
shown in Figure 1.
When the upper bound is not tight: ? > LB, we
need to choose a good branching variable to gener-
ate the child nodes. Let G(z?) = ?(p?, z?) ? ?(z?)
denote the gap between the upper bound and lower
bound. This gap is actually the accumulated gaps of
all factors c. Let Gc be the gap of c
Gc =
{
gc(p?c, z?)? ?cz?c if ?c ? 0
hc(p?c, z?)? ?cz?c if ?c < 0
41
We choose the branching variable heuristically:
for each edge e, we define its gap as the sum of the
gaps of factors that contain it
Ge =
?
c,e?c
Gc
The edge with the maximum gap is selected as the
branching variable.
Suppose there are N nodes on a level of B&B
tree, and correspondingly, we get N branching vari-
ables, among which, we choose the one with the
highest lower bound as it likely reaches the optimal
value faster.
3.4 Lower Bound Initialization
A large lower bound is critical for efficient pruning.
In this section, we discuss an alternative way to ini-
tialize the lower bound LB. We apply the similar
trick to get the lower bound function of ?(z).
Similar to Eq(8), for ?c ? 0, we have
?czc = max{?c
(?
e?c
ze ? (rc ? 1)
)
, 0}
= max{?c(z), 0}
Using the fact that
max{aj} = max
p??
?
j
pjaj
we have
?czc = maxpc??
p1c?c(z) + p2c ? 0
= max
pc
hc(pc, z)
For ?c < 0, we have
?czc = maxe?c {?cze}
= max
pc??
?
e?c
pec?cze
= max
pc
gc(pc, z)
Put the two cases together, we get the lower bound
function
?(p, z) =
?
c,?c?0
hc(pc, z) +
?
c,?c<0
gc(pc, z)
Algorithm 1 Branch and Bound based parsing
Require: {?c}
Ensure: Optimal parse tree z?
Solve p?, z? = argmaxp,z?(p, z)
Initialize S = {Z}, LB = ?(p?, z?)
while S ?= ? do
Set S ? = ?{nodes that survive from pruning}
foreach S ? S
Solve minp maxz ?(p, z) to get LBS , UBS
LB = max{LB,LBS?S}, update z?
foreach S ? S, add S to S ?, if UBS > LB
Select a branching variable ze.
Clear S = ?
foreach S ? S ?
Add S1 = {z|z ? S, ze = 1} to S
Add S2 = {z|z ? S, ze = 0} to S.
end while
For any p, pc ? ?, z ? Z
?(p, z) ? ?(z)
?(p, z) is not concave, however, we could alterna-
tively optimize z and p to get a good approximation,
which provides a lower bound for ?(z?).
3.5 Summary
We summarize our B&B algorithm in Algorithm 1.
It is worth pointing out that so far in the above
description, we have used the assumption that the
backbone DP uses first order models, however, the
backbone DP can be the second or third order ver-
sion. The difference is that, for higher order DP,
higher order factors such as adjacent siblings, grand-
children are directly handled as local factors.
In the worst case, all the edges are selected for
branching, and the complexity grows exponentially
in sentence length. However, in practice, it is quite
efficient, as we will show in the next section.
4 Experiments
4.1 Experimental Settings
The datasets we used are the English Penn Tree
Bank (PTB) and Chinese Tree Bank 5.0 (CTB5). We
use the standard train/develop/test split as described
in Table 1.
We extracted dependencies using Joakim Nivre?s
Penn2Malt tool with standard head rules: Yamada
and Matsumoto?s (Yamada and Matsumoto, 2003)
42
Train Develop Test
PTB sec. 2-21 sec. 22 sec. 23
CTB5 sec. 001-815 sec. 886-931 sec. 816-885
1001-1136 1148-1151 1137-1147
Table 1: Data split in our experiment
for English, and Zhang and Clark?s (Zhang and
Clark, 2008) for Chinese. Unlabeled attachment s-
core (UAS) is used to evaluate parsing quality1. The
B&B parser is implemented with C++. All the ex-
periments are conducted on the platform Intel Core
i5-2500 CPU 3.30GHz.
4.2 Baseline: DP Based Second Order Parser
We use the dynamic programming based second or-
der parser (Carreras, 2007) as the baseline. Aver-
aged structured perceptron (Collins, 2002) is used
for parameter estimation. We determine the number
of iterations on the validation set, which is 6 for both
corpora.
For English, we train the POS tagger using linear
chain perceptron on training set, and predict POS
tags for the development and test data. The parser is
trained using the automatic POS tags generated by
10 fold cross validation. For Chinese, we use the
gold standard POS tags.
We use five types of features: unigram features,
bigram features, in-between features, adjacent sib-
ling features and outermost grand-child features.
The first three types of features are firstly introduced
by McDonald et al (2005a) and the last two type-
s of features are used by Carreras (2007). All the
features are the concatenation of surrounding words,
lower cased words (English only), word length (Chi-
nese only), prefixes and suffixes of words (Chinese
only), POS tags, coarse POS tags which are derived
from POS tags using a simple mapping table, dis-
tance between head and modifier, direction of edges.
For English, we used 674 feature templates to gener-
ate large amounts of features, and finally got 86.7M
non-zero weighted features after training. The base-
line parser got 92.81% UAS on the testing set. For
Chinese, we used 858 feature templates, and finally
got 71.5M non-zero weighted features after train-
1For English, we follow Koo and Collins (2010) and ignore
any word whose gold-standard POS tag is one of { ? ? : , .}. For
Chinese, we ignore any word whose POS tag is PU.
ing. The baseline parser got 86.89% UAS on the
testing set.
4.3 B&B Based Parser with Non-local Features
We use the baseline parser as the backbone of our
B&B parser. We tried different types of non-local
features as listed below:
? All grand-child features. Notice that this fea-
ture can be handled by Koo?s second order
model (Koo and Collins, 2010) directly.
? All great grand-child features.
? All sibling features: all the pairs of edges with
common head. An example is shown in Fig-
ure 2.
? All tri-sibling features: all the 3-tuples of edges
with common head.
? Comb features: for any word with more than 3
consecutive modifiers, the set of all the edges
from the word to the modifiers form a comb.2
? Hand crafted features: We perform cross val-
idation on the training data using the baseline
parser, and designed features that may correc-
t the most common errors. We designed 13
hand-craft features for English in total. One ex-
ample is shown in Figure 3. For Chinese, we
did not add any hand-craft features, as the er-
rors in the cross validation result vary a lot, and
we did not find general patterns to fix them.
4.4 Implementation Details
To speed up the solution of the min-max subprob-
lem, for each node in the B&B tree, we initialize p
with the optimal solution of its parent node, since
the child node fixes only one additional edge, its op-
timal point is likely to be closed to its parent?s. For
the root node of B&B tree, we initialize pec = 1rc for
factors with non-negative weights and p1c = 0 for
2In fact, our algorithm can deal with non-consecutive mod-
ifiers; however, in such cases, factor detection (detect regular
expressions like x1. ? x2. ? . . . ) requires the longest com-
mon subsequence algorithm (LCS), which is time-consuming
if many comb features are generated. Similar problems arise
for sub-tree features, which may contain many non-consecutive
words.
43
c 0 c 1 c 2 c 3h
c 0 c 1h c 0 c 2h c 0 c 3h
c 1 c 2h c 2 c 3h c 1 c 3h
secondorder higher order
Figure 2: An example of all sibling features. Top:
a sub-tree; Bottom: extracted sibling features. Ex-
isting higher order DP systems can not handle the
siblings on both sides of head.
regulation occurs through inaction , rather than through ...
Figure 3: An example of hand-craft feature: for the
word sequence A . . . rather than A, where A is a
preposition, the first A is the head of than, than is
the head of rather and the second A.
negative weighted factors. Step size ? is initialized
with maxc,?c ?=0{ 1|?c|}, as the vector p is bounded ina unit box. ? is updated using the same strategy as
Rush et al (2010). Two stopping criteria are used.
One is 0 ? ?old ??new ? ?, where ? > 0 is a given
precision3. The other checks if the bound is tight:
UB = LB. Because all features are boolean (note
that they can be integer), their weights are integer
during each perceptron update, hence the scores of
parse trees are discrete. The minimal gap between
different scores is 1N?T after averaging, where N isthe number of training samples, and T is the itera-
tion number for perceptron training. Therefore the
upper bound can be tightened as UB = ?NT??NT .
During testing, we use the pre-pruning method as
used in Martins et al (2009) for both datasets to bal-
ance parsing quality and speed. This method uses a
simple classifier to select the top k candidate head-
s for each word and exclude the other heads from
search space. In our experiment, we set k = 10.
3we use ? = 10?8 in our implementation
System PTB CTB
Our baseline 92.81 86.89
B&B +all grand-child 92.97 87.02
+all great grand-child 92.78 86.77
+all sibling 93.00 87.05
+all tri-sibling 92.79 86.81
+comb 92.86 86.91
+hand craft 92.89 N/A
+all grand-child + all sibling + com-
b + hand craft
93.17 87.25
3rd order re-impl. 93.03 87.07
TurboParser (reported) 92.62 N/A
TurboParser (our run) 92.82 86.05
Koo and Collins (2010) 93.04 N/A
Zhang and McDonald (2012) 93.06 86.87
Zhang and Nivre (2011) 92.90 86.00
System integration
Bohnet and Kuhn (2012) 93.39 87.5
Systems using additional resources
Suzuki et al (2009) 93.79 N/A
Koo et al (2008) 93.5 N/A
Chen et al (2012) 92.76 N/A
Table 2: Comparison between our system and the-
state-of-art systems.
4.5 Main Result
Experimental results are listed in Table 2. For com-
parison, we also include results of representative
state-of-the-art systems. For the third order pars-
er, we re-implemented Model 1 (Koo and Collins,
2010), and removed the longest sentence in the CTB
dataset, which contains 240 words, due to theO(n4)
space complexity 4. For ILP based parsing, we used
TurboParser5, a speed-optimized parser toolkit. We
trained full models (which use all grandchild fea-
tures, all sibling features and head bigram features
(Martins et al, 2011)) for both datasets using its de-
fault settings. We also list the performance in its
documentation on English corpus.
The observation is that, the all-sibling features are
most helpful for our parser, as some good sibling
features can not be encoded in DP based parser. For
example, a matched pair of parentheses are always
siblings, but their head may lie between them. An-
4In fact, Koo?s algorithm requires only O(n3) space. Our
implementation is O(n4) because we store the feature vectors
for fast training.
5http://www.ark.cs.cmu.edu/TurboParser/
44
other observation is that all great grandchild features
and all tri-sibling features slightly hurt the perfor-
mance and we excluded them from the final system.
When no additional resource is available, our
parser achieved competitive performance: 93.17%
Unlabeled Attachment Score (UAS) for English at
a speed of 177 words per second and 87.25% for
Chinese at a speed of 97 words per second. High-
er UAS is reported by joint tagging and parsing
(Bohnet and Nivre, 2012) or system integration
(Bohnet and Kuhn, 2012) which benefits from both
transition based parsing and graph based parsing.
Previous work shows that combination of the two
parsing techniques can learn to overcome the short-
comings of each non-integrated system (Nivre and
McDonald, 2008; Zhang and Clark, 2008). Sys-
tem combination will be an interesting topic for our
future research. The highest reported performance
on English corpus is 93.79%, obtained by semi-
supervised learning with a large amount of unla-
beled data (Suzuki et al, 2009).
4.6 Tradeoff Between Accuracy and Speed
In this section, we study the trade off between ac-
curacy and speed using different pre-pruning setups.
In Table 3, we show the parsing accuracy and in-
ference time in testing stage with different numbers
of candidate heads k in pruning step. We can see
that, on English dataset, when k ? 10, our pars-
er could gain 2 ? 3 times speedup without losing
much parsing accuracy. There is a further increase
of the speed with smaller k, at the cost of some ac-
curacy. Compared with TurboParser, our parser is
less efficient but more accurate. Zhang and McDon-
ald (2012) is a state-of-the-art system which adopts
cube pruning for efficient parsing. Notice that, they
did not use pruning which seems to increase parsing
speed with little hit in accuracy. Moreover, they did
labeled parsing, which also makes their speed not
directly comparable.
For each node of B&B tree, our parsing algorithm
uses projected sub-gradient method to find the sad-
dle point, which requires a number of calls to a DP,
hence the efficiency of Algorithm 1 is mainly deter-
mined by the number of DP calls. Figure 4 and Fig-
ure 5 show the averaged parsing time and number of
calls to DP relative to the sentence length with differ-
ent pruning settings. Parsing time grows smoothly
PTB CTB
System UAS w/s UAS w/s
Ours (no prune) 93.18 52 87.28 73
Ours (k = 20) 93.17 105 87.28 76
Ours (k = 10) 93.17 177 87.25 97
Ours (k = 5) 93.10 264 86.94 108
Ours (k = 3) 92.68 493 85.76 128
TurboParser(full) 92.82 402 86.05 192
TurboParser(standard) 92.68 638 85.80 283
TurboParser(basic) 90.97 4670 82.28 2736
Zhang and McDon-
ald (2012)?
93.06 220 86.87 N/A
Table 3: Trade off between parsing accuracy (UAS)
and speed (words per second) with different pre-
pruning settings. k denotes the number of candi-
date heads of each word preserved for B&B parsing.
?Their speed is not directly comparable as they per-
forms labeled parsing without pruning.
when sentence length ? 40. There is some fluctua-
tion for the long sentences. This is because there are
very few sentences for a specific long length (usual-
ly 1 or 2 sentences), and the statistics are not stable
or meaningful for the small samples.
Without pruning, there are in total 132, 161 calls
to parse 2, 416 English sentences, that is, each sen-
tence requires 54.7 calls on average. For Chinese,
there are 84, 645 calls for 1, 910 sentences, i.e., 44.3
calls for each sentence on average.
5 Discussion
5.1 Polynomial Non-local Factors
Our bounding strategy can handle a family of non-
local factors that can be expressed as a polynomial
function of local factors. To see this, suppose
zc =
?
i
?i
?
e?Ei
ze
For each i, we introduce new variable zEi =
mine?Ei ze. Because ze is binary, zEi =
?
e?Ei ze.In this way, we replace zc by several zEi that can be
handled by our bounding strategy.
We give two examples of these polynomial non-
local factors. First is the OR of local factors: zc =
max{ze, z?e}, which can be expressed by zc = ze +
z?e?zez?e. The second is the factor of valency feature
45
0 10 20 30 40 50 600
5
10
parsi
ng tim
e (sec
.)
sentence length
k=3k=5k=10k=20no prune
(a) PTB corpus
0 20 40 60 80 100 120 1400
20
40
60
80
parsi
ng tim
e (sec
.)
sentence length
k=3k=5k=10k=20no prune
(b) CTB corpus
Figure 4 Averaged parsing time (seconds) relative to sentence length with different pruning settings, k
denotes the number of candidate heads of each word in pruning step.
0 10 20 30 40 50 600
100
200
Calls
 to DP
sentence length
k=3k=5k=10k=20no prune
(a) PTB corpus
0 20 40 60 80 100 120 1400
500
1000
Calls
 to DP
sentence length
k=3k=5k=10k=20no prune
(b) CTB corpus
Figure 5 Averaged number of Calls to DP relative to sentence length with different pruning settings, k
denotes the number of candidate heads of each word in pruning step.
(Martins et al, 2009). Let binary variable vik indi-
cate whether word i has k modifiers. Given {ze} for
the edges with head i, then {vik|k = 1, . . . , n ? 1}
can be solved by
?
k
kjvik =
(?
e
ze
)j
0 ? j ? n? 1
The left side of the equation is the linear function of
vik. The right side of the equation is a polynomial
function of ze. Hence, vik could be expressed as a
polynomial function of ze.
5.2 k Best Parsing
Though our B&B algorithm is able to capture a va-
riety of non-local features, it is still difficult to han-
dle many kinds of features, such as the depth of the
parse tree. Hence, a reranking approach may be use-
ful in order to incorporate such information, where
k parse trees can be generated first and then a second
pass model is used to rerank these candidates based
on more global or non-local features. In addition,
k-best parsing may be needed in many applications
to use parse information and especially utilize infor-
mation from multiple candidates to optimize task-
specific performance. We have not conducted any
experiment for k best parsing, hence we only dis-
cuss the algorithm.
According to proposition 1, we have
Proposition 2. Given p and subset S ? Z , let zk
denote the kth best solution of maxz?S ?(p, z). If a
parse tree z? ? S satisfies ?(z?) ? ?(p, zk), then z?
is one of the k best parse trees in subset S.
Proof. Since zk is the kth best solution of ?(p, z),
for zj , j > k, we have ?(p, zk) ? ?(p, zj) ?
?(zj). Since the size of the set {zj |j > k} is
|S| ? k, hence there are at least |S| ? k parse trees
whose scores ?(zj) are less than ?(p, zk). Because
?(z?) ? ?(p, zk), hence z? is at least the kth best
parse tree in subset S.
Therefore, we can search the k best parse trees
in this way: for each sub-problem, we use DP to
derive the k best parse trees. For each parse tree
z, if ?(z) ? ?(p, zk), then z is selected into the k
best set. Algorithm terminates until the kth bound is
tight.
46
6 Conclusion
In this paper we proposed a new parsing algorithm
based on a Branch and Bound framework. The mo-
tivation is to use dynamic programming to search
for the bound. Experimental results on PTB and
CTB5 datasets show that our method is competitive
in terms of both performance and efficiency. Our
method can be adapted to non-projective dependen-
cy parsing, as well as the k best MST algorithm
(Hall, 2007) to find the k best candidates.
Acknowledgments
We?d like to thank Hao Zhang, Andre Martins and
Zhenghua Li for their helpful discussions. We al-
so thank Ryan McDonald and three anonymous re-
viewers for their valuable comments. This work
is partly supported by DARPA under Contract No.
HR0011-12-C-0016 and FA8750-13-2-0041. Any
opinions expressed in this material are those of the
authors and do not necessarily reflect the views of
DARPA.
References
Michael Auli and Adam Lopez. 2011. A comparison of
loopy belief propagation and dual decomposition for
integrated CCG supertagging and parsing. In Proc. of
ACL-HLT.
Egon Balas. 1965. An additive algorithm for solving
linear programs with zero-one variables. Operations
Research, 39(4).
Bernd Bohnet and Jonas Kuhn. 2012. The best of
bothworlds ? a graph-based completion model for
transition-based parsers. In Proc. of EACL.
Bernd Bohnet and Joakim Nivre. 2012. A transition-
based system for joint part-of-speech tagging and la-
beled non-projective dependency parsing. In Proc. of
EMNLP-CoNLL.
Paul Calamai and Jorge More?. 1987. Projected gradien-
t methods for linearly constrained problems. Mathe-
matical Programming, 39(1).
Xavier Carreras. 2007. Experiments with a higher-order
projective dependency parser. In Proc. of EMNLP-
CoNLL.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proc. of ACL.
Wenliang Chen, Min Zhang, and Haizhou Li. 2012. U-
tilizing dependency language models for graph-based
dependency parsing models. In Proc. of ACL.
Michael Collins. 2000. Discriminative reranking for nat-
ural language parsing. In Proc. of ICML.
Michael Collins. 2002. Discriminative training methods
for hidden markov models: Theory and experiments
with perceptron algorithms. In Proc. of EMNLP.
John Duchi, Shai Shalev-Shwartz, Yoram Singer, and
Tushar Chandra. 2008. Efficient projections onto the
l1-ball for learning in high dimensions. In Proc. of
ICML.
Jason M. Eisner. 1996. Three new probabilistic models
for dependency parsing: an exploration. In Proc. of
COLING.
Keith Hall. 2007. K-best spanning tree parsing. In Proc.
of ACL.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proc. of IWPT.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proc. of ACL-HLT.
Manfred Klenner and E?tienne Ailloud. 2009. Opti-
mization in coreference resolution is not needed: A
nearly-optimal algorithm with intensional constraints.
In Proc. of EACL.
Terry Koo and Michael Collins. 2010. Efficient third-
order dependency parsers. In Proc. of ACL.
Terry Koo, Xavier Carreras, and Michael Collins. 2008.
Simple semi-supervised dependency parsing. In Proc.
of ACL-HLT.
Terry Koo, Alexander M. Rush, Michael Collins, Tommi
Jaakkola, and David Sontag. 2010. Dual decomposi-
tion for parsing with non-projective head automata. In
Proc. of EMNLP.
Ailsa H. Land and Alison G. Doig. 1960. An automat-
ic method of solving discrete programming problems.
Econometrica, 28(3):497?520.
Andre Martins, Noah Smith, and Eric Xing. 2009. Con-
cise integer linear programming formulations for de-
pendency parsing. In Proc. of ACL.
Andre Martins, Noah Smith, Mario Figueiredo, and Pe-
dro Aguiar. 2011. Dual decomposition with many
overlapping components. In Proc. of EMNLP.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005a. Online large-margin training of dependency
parsers. In Proc. of ACL.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic. 2005b. Non-projective dependency pars-
ing using spanning tree algorithms. In Proc. of HLT-
EMNLP.
Joakim Nivre and Ryan McDonald. 2008. Integrating
graph-based and transition-based dependency parsers.
In Proc. of ACL-HLT.
Jorge Nocedal and Stephen J. Wright. 2006. Numerical
Optimization. Springer, 2nd edition.
47
Sebastian Riedel and James Clarke. 2006. Incremental
integer linear programming for non-projective depen-
dency parsing. In Proc. of EMNLP.
Alexander M Rush, David Sontag, Michael Collins, and
Tommi Jaakkola. 2010. On dual decomposition and
linear programming relaxations for natural language
processing. In Proc. of EMNLP.
David Smith and Jason Eisner. 2008. Dependency pars-
ing by belief propagation. In Proc. of EMNLP.
Min Sun, Murali Telaprolu, Honglak Lee, and Silvio
Savarese. 2012. Efficient and exact MAP-MRF in-
ference using branch and bound. In Proc. of AISTATS.
Jun Suzuki, Hideki Isozaki, Xavier Carreras, andMichael
Collins. 2009. An empirical study of semi-supervised
structured conditional models for dependency parsing.
In Proc. of EMNLP.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical
dependency analysis with support vector machines. In
Proc. of IWPT.
Yue Zhang and Stephen Clark. 2008. A tale of t-
wo parsers: Investigating and combining graph-based
and transition-based dependency parsing. In Proc. of
EMNLP.
Hao Zhang and Ryan McDonald. 2012. Generalized
higher-order dependency parsing with cube pruning.
In Proc. of EMNLP.
Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
Proc. of ACL-HLT.
48
Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk, pages 148?151,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Non-Expert Evaluation of Summarization Systems is Risky
Dan Gillick
University of California, Berkeley
Computer Science Division
dgillick@cs.berkeley.edu
Yang Liu
University of Texas, Dallas
Department of Computer Science
yangl@hlt.utdallas.edu
Abstract
We provide evidence that intrinsic evalua-
tion of summaries using Amazon?s Mechan-
ical Turk is quite difficult. Experiments mir-
roring evaluation at the Text Analysis Con-
ference?s summarization track show that non-
expert judges are not able to recover system
rankings derived from experts.
1 Introduction
Automatic summarization is a particularly difficult
task to evaluate. What makes a good summary?
What information is relevant? Is it possible to sepa-
rate information content from linguistic quality?
Besides subjectivity issues, evaluation is time-
consuming. Ideally, a judge would read the original
set of documents before deciding how well the im-
portant aspects are conveyed by a summary. A typ-
ical 10-document problem could reasonably involve
25 minutes of reading or skimming and 5 more min-
utes for assessing a 100-word summary. Since sum-
mary output can be quite variable, at least 30 top-
ics should be evaluated to get a robust estimate of
performance. Assuming a single judge evaluates all
summaries for a topic (more redundancy would be
better), we get a rough time estimate: 17.5 hours to
evaluate two systems.
Thus it is of great interest to find ways of speeding
up evaluation while minimizing subjectivity. Ama-
zon?s Mechanical Turk (MTurk) system has been
used for a variety of labeling and annotation tasks
(Snow et al, 2008), but such crowd-sourcing has not
been tested for summarization.
We describe an experiment to test whether MTurk
is able to reproduce system-level rankings that
match expert opinion. Unlike the results of other
crowd-sourcing annotations for natural language
tasks, we find that non-expert judges are unable to
provide expert-like scores and tend to disagree sig-
nificantly with each other.
This paper is organized as follows: Section 2 in-
troduces the particular summarization task and data
we use in our experiments; Section 3 describes the
design of our Human Intelligence Task (HIT). Sec-
tion 4 shows experimental results and gives some
analysis. Section 5 reviews our main findings and
provides suggestions for researchers wishing to con-
duct their own crowd-sourcing evaluations.
2 TAC Summarization Task
Topic: Peter Jennings
Description: Describe Peter Jennings? lung cancer and its
effects.
Reference: Peter Jennings?s announcement April 5, 2005,
that he had lung cancer left his colleagues at ABC News sad-
dened and dismayed. He had been ?World News Tonight?
anchorman since 1983. By the end of the week, ABC had re-
ceived 3,400 e-mails offering him prayers and good wishes.
A former heavy smoker, Jennings had not been well for some
time and was unable to travel abroad to cover foreign events.
However, his diagnosis came as a surprise to him. ABC an-
nounced that Jennings would continue to anchor the news
during chemotherapy treatment, but he was unable to do so.
Table 1: An example topic and reference summary from
the TAC 2009 summarization task.
Our data comes from the submissions to the Text
Analysis Conference (TAC) summarization track in
2009 (Dang, 2009). The main task involved 44
query-focused topics, each requiring a system to
produce a 100-word summary of 10 related news
documents. Experts provided four reference sum-
maries for each topic. Table 1 shows an example.
148
Score Difference
0 1 2 3 mean
OQ 119 92 15 0 0.54
LQ 117 82 20 7 0.63
Table 2: Identical summaries often were given different
scores by the same expert human judge at TAC 2009.
Counts of absolute score differences are shown for Over-
all Quality (OQ) and Linguistic Quality (LQ).
2.1 Agreement and consistency
In the official TAC evaluation, each summary was
judged by one of eight experts for ?Overall Quality?
and ?Linguistic Quality? on a 1 (?very poor?) to 10
(?very good?) scale. Unfortunately, the lack of re-
dundant judgments means we cannot estimate inter-
annotator agreement. However, we note that out of
all 4576 submitted summaries, there are 226 pairs
that are identical, which allows us to estimate anno-
tator consistency. Table 2 shows that an expert an-
notator will give the same summary the same score
just over half the time.
2.2 Evaluation without source documents
One way to dramatically speed up evaluation is to
use the experts? reference summaries as a gold stan-
dard, leaving the source documents out entirely.
This is the idea behind automatic evaluation with
ROUGE (Lin, 2004), which measures ngram over-
lap with the references, and assisted evaluation with
Pyramid (Nenkova and Passonneau, 2004), which
measures overlap of facts or ?Semantic Content
Units? with the references. The same idea has also
been employed in various manual evaluations, for
example by Haghighi and Vanderwende (2009), to
directly compare the summaries of two different sys-
tems. The potential bias introduced by such abbre-
viated evaluation has not been explored.
3 HIT design
The overall structure of the HIT we designed for
summary evaluation is as follows: The worker is
asked to read the topic and description, and then
two reference summaries (there is no mention of the
source documents). The candidate summary appears
next, followed by instructions to provide scores be-
tween 1 (very poor) and 10 (very good) in each cat-
egory1. Mouse-over on the category names provides
1Besides Overall Quality and Linguistic Quality, we include
Information Content, to encourage judges to distinguish be-
extra details, copied with slight modifications from
Dang (2007).
Our initial HIT design asked workers to perform
a head-to-head comparison of two candidate sum-
maries, but we found this unsatisfactory for a num-
ber of reasons. First, many of the resulting scores
did not obey the transitive property: given sum-
maries x, y, and z, a single worker showed a pref-
erence for y > x and z > y, but also x > z.
Second, while this kind of head-to-head evalua-
tion may be useful for system development, we are
specifically interested here in comparing non-expert
MTurk evaluation with expert TAC evaluation.
We went through a few rounds of revisions to the
language in the HIT after observing worker feed-
back. Specifically, we found it was important to em-
phasize that a good summary not only responds to
the topic and description, but also conveys the infor-
mation in the references.
3.1 Quality control
Only workers with at least a 96% HIT approval rat-
ing2 were allowed access to this task. We moni-
tored results manually and blocked workers (reject-
ing their work) if they completed a HIT in under 25
seconds. Such suspect work typically showed uni-
form scores (usually all 10s). Nearly 30% of HITs
were rejected for this reason.
To encourage careful work, we included this note
in our HITs: ?High annotator consistency is impor-
tant. If the scores you provide deviate from the av-
erage scores of other annotators on the same HIT,
your work will be rejected. We will award bonuses
for particularly good work.? We gave a few small
bonuses ($0.50) to workers who left thoughtful com-
ments.
3.2 Compensation
We experimented with a few different compensation
levels and observed a somewhat counter-intuitive re-
sult. Higher compensation ($.10 per HIT) yielded
lower quality work than lower compensation ($.07
per HIT), judging by the number of HITs we re-
jected. It seems that lower compensation attracts
workers who are less interested in making money,
and thus willing to spend more time and effort.
There is a trade-off, though, as there are fewer work-
ers willing to do the task for less money.
tween content and readability.
2MTurk approval ratings calculated as the fraction of HITs
approved by requesters.
149
Sys TAC MTurk
OQ LQ OQ LQ C
A 5.16 5.64 7.03 7.27 7.27
B 4.84 5.27 6.78 6.97 6.78
C 4.50 4.93 6.51 6.85 6.49
D 4.20 4.09 6.15 6.59 6.50
E 3.91 4.70 6.19 6.54 6.58
F 3.64 6.70 7.06 7.78 6.56
G 3.57 3.43 5.82 6.33 6.28
H 3.20 5.23 5.75 6.06 5.62
Table 3: Comparison of Overall Quality (OQ) and Lin-
guistic Quality (LQ) scores between the TAC and MTurk
evaluations. Content (C) is evaluated by MTurk workers
as well. Note that system F is the lead baseline.
4 Experiments and Analysis
To assess how well MTurk workers are able to em-
ulate the work of expert judges employed by TAC,
we chose a subset of systems and analyze the results
of the two evaluations. The systems were chosen to
represent the entire range of average Overall Qual-
ity scores. System F is a simple lead baseline, which
generates a summary by selecting the first sentences
up to 100 words of the most recent document. The
rest of the systems were submitted by various track
participants. The MTurk evaluation included two-
times redundancy. That is, each summary was eval-
uated by two different people. The cost for the full
evaluation, including 44 topics, 8 systems, and 2x
redundancy, at $.07 per HIT, plus 10% commission
for Amazon, was $55.
Table 3 shows average scores for the two evalu-
ations. The data suggest that the MTurk judges are
better at evaluating Linguistic Quality than Content
or Overall Quality. In particular, the MTurk judges
appear to have difficulty distinguishing Linguistic
Quality from Content. We will defend these claims
with more analysis, below.
4.1 Worker variability
The first important question to address involves the
consistency of the workers. We cannot compare
agreement between TAC and MTurk evaluations, but
the MTurk agreement statistics suggest considerable
variability. In Overall Quality, the mean score differ-
ence between two workers for the same HIT is 2.4
(the standard deviation is 2.0). The mean is 2.2 for
Linguistic Quality (the standard deviation is 1.5).
In addition, the TAC judges show more similarity
with each other?as if they are roughly in agreement
about what makes a good summary. We compute
each judge?s average score and look at the standard
deviation of these averages for the two groups. The
TAC standard deviation is 1.0 (ranging from 3.0 to
6.1), whereas the MTurk standard deviation is 2.3
(ranging from 1.0 to 9.5). Note that the average
number of HITs performed by each MTurk worker
was just over 5.
Finally, we can use regression analysis to show
what fraction of the total score variance is captured
by judges, topics, and systems. We fit linear models
in R using binary indicators for each judge, topic,
and system. Redundant evaluations in the MTurk
set are removed for unbiased comparison with the
TAC set. Table 4 shows that the differences between
the TAC and MTurk evaluations are quite striking:
Taking the TAC data alone, the topics are the major
source of variance, whereas the judges are the major
source of variance in the MTurk data. The systems
account for only a small fraction of the variance in
the MTurk evaluation, which makes system ranking
more difficult.
Eval Judges Topics Systems
TAC 0.28 0.40 0.13
MTurk 0.44 0.13 0.05
Table 4: Linear regression is used to model Overall Qual-
ity scores as a function of judges, topics, and systems, re-
spectively, for each data set. The R2 values, which give
the fraction of variance explained by each of the six mod-
els, are shown.
4.2 Ranking comparisons
The TAC evaluation, while lacking redundant judg-
ments, was a balanced experiment. That is, each
judge scored every system for a single topic. The
same is not true for the MTurk evaluation, and as
a result, the average per-system scores shown in
Table 3 may be biased. As a result, and because
we need to test multiple system-level differences si-
multaneously, a simple t-test is not quite sufficient.
We use Tukey?s Honestly Significant Differences
(HSD), explained in detail by Yandell (1997), to as-
sess statistical significance.
Tukey?s HSD test computes significance intervals
based on the range of the sample means rather than
individual differences, and includes an adjustment to
correct for imbalanced experimental designs. The R
implementation takes as input a linear model, so we
150
Eval Ranking
TAC (OQ) A B C DA EB FC GC HD
MTurk (OQ) F A B C EF GF DB HB
TAC (LQ) F AF BF HF CF EA DB GE
MTurk (LQ) F A BF CF DF EF HC GC
MTurk (C) A B E F D C GA HD
Table 5: Systems are shown in rank order from highest
(left) to lowest (right) for each scoring metric: Over-
all Quality (OQ), Linguistic Quality (LQ), and Content
(C). The superscripts indicate the rightmost system that
is significantly different (at 95% confidence) according
to Tukey?s HSD test.
model scores using binary indicators for (J)udges,
(T)opics, and (S)ystems (see equation 1), and mea-
sure significance in the differences between system
coefficients (?k).
score = ?+
?
i
?iJi +
?
j
?jTj +
?
k
?kSk (1)
Table 5 shows system rankings for the two evalu-
ations. The most obvious discrepancy between the
TAC and MTurk rankings is system F, the base-
line. Both TAC and MTurk judges gave F the high-
est scores for Linguistic Quality, a reasonable result
given its construction, whereas the other summaries
tend to pull sentences out of context. But the MTurk
judges also gave F the highest scores in Overall
Quality, suggesting that readability is more impor-
tant to amateur judges than experts, or at least easier
to identify. Content appears the most difficult cate-
gory for the MTurk judges, as few significant score
differences emerge. Even with more redundancy, it
seems unlikely that MTurk judges could produce a
ranking resembling the TAC Overall Quality rank-
ing using this evaluation framework.
5 Discussion
Through parallel evaluations by experts at TAC and
non-experts on MTurk, we have shown two main
results. First, as expected, MTurk workers pro-
duce considerably noisier work than experts. That
is, more redundancy is required to achieve statisti-
cal significance on par with expert judgments. This
finding matches prior work with MTurk. Second,
MTurk workers are unlikely to produce a score rank-
ing that matches expert rankings for Overall Quality.
This seems to be the result of some confusion in sep-
arating content from readability.
What does this mean for future evaluations? If
we want to assess overall summary quality?that is,
balancing content and linguistic quality like expert
judges do?we will need to redesign the task for
non-experts. Perhaps MTurk workers will be bet-
ter able to understand Nenkova?s Pyramid evaluation
(2004), which is designed to isolate content. Extrin-
sic evaluation, where judges use the summary to an-
swer questions derived from the source documents
or the references, as done by Callison-Burch for
evaluation of Machine Translation systems (2009),
is another possibility.
Finally, our results suggest that anyone conduct-
ing an evaluation of summarization systems using
non-experts should calibrate their results by asking
their judges to score summaries that have already
been evaluated by experts.
Acknowledgments
Thanks to Benoit Favre for discussing the evaluation
format and to the anonymous reviewers for helpful,
detailed feedback.
References
C. Callison-Burch. 2009. Fast, cheap, and creative:
Evaluating translation quality using Amazons Me-
chanical Turk. Proceedings of EMNLP.
H.T. Dang. 2007. Overview of DUC 2007. In Proceed-
ings of the Document Understanding Conference.
H.T. Dang. 2009. Overview of the TAC 2009 opinion
question answering and summarization tasks. In Pro-
ceedings of Text Analysis Conference (TAC 2009).
A. Haghighi and L. Vanderwende. 2009. Exploring con-
tent models for multi-document summarization. In
Proceedings of HLT-NAACL.
C.Y. Lin. 2004. Rouge: A package for automatic evalu-
ation of summaries. In Proceedings of the Workshop:
Text Summarization Branches Out.
A. Nenkova and R. Passonneau. 2004. Evaluating con-
tent selection in summarization: The pyramid method.
In Proceedings of HLT-NAACL.
R. Snow, B. O?Connor, D. Jurafsky, and A.Y. Ng. 2008.
Cheap and fast?but is it good?: Evaluating non-
expert annotations for natural language tasks. In Pro-
ceedings of EMNLP.
B.S. Yandell. 1997. Practical data analysis for designed
experiments. Chapman & Hall/CRC.
151
Proceedings of the Workshop on Language in Social Media (LSM 2011), pages 66?75,
Portland, Oregon, 23 June 2011. c?2011 Association for Computational Linguistics
Why is ?SXSW? trending? Exploring Multiple Text Sources for
Twitter Topic Summarization
Fei Liu1 Yang Liu1 Fuliang Weng2
1Computer Science Department, The University of Texas at Dallas
2Research and Technology Center, Robert Bosch LLC
{feiliu, yangl}@hlt.utdallas.edu1
fuliang.weng@us.bosch.com2
Abstract
User-contributed content is creating a surge on
the Internet. A list of ?buzzing topics? can
effectively monitor the surge and lead people
to their topics of interest. Yet a topic phrase
alone, such as ?SXSW?, can rarely present
the information clearly. In this paper, we
propose to explore a variety of text sources
for summarizing the Twitter topics, includ-
ing the tweets, normalized tweets via a ded-
icated tweet normalization system, web con-
tents linked from the tweets, as well as inte-
gration of different text sources. We employ
the concept-based optimization framework for
topic summarization, and conduct both au-
tomatic and human evaluation regarding the
summary quality. Performance differences are
observed for different input sources and types
of topics. We also provide a comprehensive
analysis regarding the task challenges.
1 Introduction
User contributed content has become a major source
of information in the Web 2.0 era. People follow
their topics of interest, share their experience or
opinions on a variety of interactive platforms, in-
cluding forums, blogs, microblogs, social network-
ing sites, etc. To keep track of the trends online
and suggest topics of interest to the general public,
many leading websites provide a ?buzzing? service
by publishing the current most popular topics on
their entrance page and update them regularly, such
as the ?popular now? column on Bing.com, ?trend-
ing topics? on Twitter.com, ?trending now? on Ya-
hoo.com, Google Trends, and so forth. Often pop-
ular topics are in the form of a list of keywords or
phrases1. Take Twitter.com as an example. Clicking
on a trending topic phrase will return a set of relevant
Twitter posts (tweets) or web pages. Nonetheless,
whether this is a convenient way for users to navi-
gate through the popular topic information is still ar-
guable. For example, when ?SXSW? was listed as a
trending topic, it seems difficult to understand at the
first glance. A condensed topic summary would be
extremely helpful for the users before diving into the
massive search results to figure out what this topic
phrase is about and why it is trending. In this paper,
our goal is to generate a short text summary for any
given topic phrase. Note that the proposed approach
is not limited to trending topics, but can be applied
to arbitrary Twitter topics.
There are a lot of differences between tweets and
traditional written text that has been widely used
for automatic summarization. In Table 1, we show
example tweets for the topic ?SXSW?. The tweets
were extracted by searching the Twitter site using
the topic phrase as a query. We also provide an ex-
cerpt of the linked web content to help understand
the topic. The tweets present some unique charac-
teristics:
? All tweets are limited to 140 characters. Some
tweets are news headlines from the official me-
dia, others are generated by users with vari-
ous degrees of familiarity with the social me-
dia. The resulting tweets can be very different
regarding the text quality and word usage.
1They are referred to as topic phrases hereafter, with no dis-
tinction between keywords and key phrases.
66
Twitter Topic: ?SXSW?
Twts
I wish I could go to SXSW... I will, one day!
http://sxsw.com/
RT @user123: SXSW Film
Round-Up: Documentaries http://bit.ly/fg033b
@user456 yo.whats good,i met u at sxsw, talkin
bout that feature.I was gonna see about sending
u a few beats.u lookin for only original?
The South by Southwest (SXSW) Conferences
Web & Festivals offer the unique convergence of
Cont original music, independent films, and
emerging technologies...(http://sxsw.com/)
Table 1: Example tweets and an excerpt of the linked web
content for Twitter topic ?SXSW?.
? Tweets lack structure information, contain var-
ious ill-formed sentences and grammatical er-
rors. There are lots of noisy nonstandard to-
kens, such as abbreviations (?feelin? for ?feel-
ing?), substitutions (?Pr1mr0se? for ?Prim-
rose?), emoticons, etc.
? Twitter invented its own markup language.
?@user? is used to reply to a specific user or
call for attentions. The hashtag ?#topic? aims
to assign a topic label to the tweet, and is fre-
quently employed by the twitter users.
? Tweets frequently contain embedded URLs
that direct users to other online content, such
as news web pages, blogs, organization home-
pages (Wu et al, 2011). According to Twitter?s
news release in September 2010 (Rao, 2010),
25% of tweets contain an URL. These linked
web pages provide a much richer source of in-
formation than is possible in the 140-character
tweet.
These Twitter-specific characteristics may pose
challenges to the automatic summarization systems
for identifying the essential information. In this pa-
per, we focus on two such characteristics that are
not studied in previous literature, the web content
link and the non-standard tokens in tweets. Specif-
ically, we ask two questions: (1) Is the web content
linked from the tweets useful for summarization?
Can we integrate different text sources, including
the tweets and linked web pages, to generate more
informative Twitter topic summaries? (2) what is
the effect of nonstandard tokens on summarization
performance? Will the summaries be improved if
the noisy tweets were pre-normalized into standard
English sentences? We investigate these two ques-
tions under a concept-based summarization frame-
work using integer linear programming (ILP). We
utilize text input that has various quality and is orig-
inated from multiple sources, and thoroughly ana-
lyze the resulting summaries using both automatic
and human evaluation metrics.
2 Related Work
There is not much previous work on summarizing
the Twitter topics. Most previous summarization lit-
erature focused on the written text domain, as driven
by the annual evaluation tracks of the DUC (Doc-
ument Understanding Conference) and TAC (Text
Analysis Conference). To some extent, Twitter topic
summarization is related to spoken document sum-
marization, since both tasks deal with the conver-
sational text that is contributed by multiple par-
ticipants and contains lots of ill-formed sentences,
colloquial expressions, nonstandard word tokens or
high word error rate, etc. To summarize the spo-
ken text, (Zechner, 2002) aimed to address prob-
lems related to disfluencies, extraction units, cross-
speaker coherence, etc. (Maskey and Hirschberg,
2005; Murray et al, 2006; Galley, 2006; Xie et
al., 2008; Liu and Liu, 2010a) incorporated lexical,
structural, speaker, and discourse cues to generate
textual summaries for broadcast news and meeting
conversations.
For microblog summarization, (Sharifi et al,
2010a) proposed a phrase reinforcement (PR) algo-
rithm to summarize the Twitter topic in one sen-
tence. The algorithm builds a word graph using the
topic phrase as the root node; each word node is
weighted in proportion to its distance to the root and
the corresponding phrase frequency. The summary
sentence is selected as one of the highest weighted
paths in the graph. (Sharifi et al, 2010b; Inouye,
2010) introduced a hybrid TF-IDF approach to ex-
tract one- or multiple-sentence summary for each
topic. Sentences were ranked according to the av-
erage TF-IDF score of the consisting words; top
weighted sentences were iteratively extracted, but
excluding those that have high cosine similarity with
the existing summary sentences. They showed the
Hybrid TF-IDF approach performs constantly bet-
67
ter than the PR algorithm and other traditional sum-
marization systems. Our approach of summarizing
the Twitter topics is different from the above stud-
ies in that, we focus on exploring richer informa-
tion sources (such as the online web content) and in-
vestigating effect of non-standard tokens. There are
also studies working on visualizing Twitter topics
by identifying a set of topic phrases and presenting
the related tweets to users (O?Connor et al, 2010;
Marcus et al, 2011). Our proposed approach can be
beneficial to these systems by providing informative
topic summaries generated from rich text sources.
3 Data Collection
We collected 5,537 topic phrases and the reference
topic descriptions by crawling the Twitter.com and
WhatTheTrend.com simultaneously during the pe-
riod of Aug 22th, 2010 to Oct 30th, 2010 (about 70
days). The Twitter API was queried every 5 min-
utes for the current top ten trending topics. For each
of these topics, a search query was submitted to the
Twitter Search API to retrieve only English tweets
related to this topic. If any tweet contains embedded
URLs linked to the other web pages, the contents
of these web pages were retrieved. For each topic,
we limit the maximum number of retrieved tweets to
5,000 and webpages to 100. An example is shown in
Table 1 for a topic phrase, some related tweets, and
an excerpt of the linked webpage. WhatTheTrend
API provides short topic descriptions contributed
and constantly updated by the Twitter users. There
is also a manually assigned category tag for each
topic phrase. We found the top categories among
the collected topics are ?Entertainment (29.26%)?,
?Sports (25.58%)?, and ?Meme (15.69%, pointless
babble)?. We divided the collected topics into two
groups: the general topics (e.g., ?Chilean miners?,
?MTV VMA?) and the hashtag topics that start with
the ?#? (e.g., ?#top10rappers?, ?#octoberwish?).
To generate reference summaries for the Twit-
ter topics, two human annotators were asked to
pick the topic descriptions/sentences (collected from
WhatTheTrend.com) that are appropriate and valu-
able to be included in the summary. This is per-
formed on a selected set of 1,511 topics with both
trending duration and number of tweets greater than
our predefined thresholds. For each of the topic sen-
tences, we ask the annotators to label its category:
(1) the sentence is a general description of the topic;
(2) the sentence is trying to explain why the topic is
trending; (3) it is hard to tell the difference. Over-
all, the two annotators have good agreement (Kappa
= 0.67) regarding whether or not to include a sen-
tence in the summary. Among the selected summary
sentences, 22.58% of them were assigned with con-
flicting purpose tags such as (1) or (2). To form
a reference summary, we concatenate all the topic
sentences selected by both annotators. Since some
reference descriptions are simply repetition of oth-
ers with very minor changes, we reduce the dupli-
cates by iteratively removing the oldest sentences if
all the consisting words are covered by the remain-
ing sentence collection, until no sentence can be re-
moved. On average, the reference summary for gen-
eral and hashtag topics contains 44 and 40 words
respectively.
4 Summarization System
For each of the topic phrases, our goal is to gener-
ate a short textual summary that can best convey the
main ideas of the topic contents. We explore and
compare multiple text sources as summarization in-
put, including the user-contributed tweets, web con-
tents linked from the tweets, as well as combination
of the two sources. The concept-based optimization
approach (Gillick et al, 2009; Xie et al, 2009; Mur-
ray et al, 2010) was employed for selecting informa-
tive summary sentences and minimizing the redun-
dancy. Note that our focus of this paper is not devel-
oping new summarization systems, but rather utiliz-
ing and integrating different text sources for gener-
ating more informative Twitter topic summaries.
4.1 Concept-based Optimization Framework
Concept-based summarization approach first ex-
tracts a set of important concepts for each topic, then
selects a collection of sentences that can cover as
many important concepts as possible, while within
the specified length limit. This idea is realized us-
ing the integer linear programming-based (ILP) op-
timization framework, with objective function set to
maximize the sum of the weighted concepts:
max
?
i
wici
68
where ci is a binary variable indicating whether the
concept i is covered by the summary; wi is the
weight assigned to ci.
We enforce two sets of length constraints to the
summary: sentence- or word-based. Sentence con-
straint requires the total number of selected sum-
mary sentences to not to exceed a length limit L1;
while word constraint requires the total words of
selected sentences not to exceed length limit L2.
These two constraints are:
?
j
sj < L1 or
?
j
ljsj < L2
where sj is a binary variable indicating whether sen-
tence j was selected in the summary; lj represents
the number of words in sj .
Further, we connect concept i with sentence j us-
ing two sets of constraints. For all the sentences that
contain concept i, if any sentence was selected in
the summary, the concept i should be covered by the
summary; reversely, if concept i was covered by the
summary, at least one of the sentences containing
concept i should be selected.
?i ci ?
?
j
oijsj
?i, j ci ? oijsj
where the binary variable oij is used to indicate
whether concept i exists in sentence j.
The concepts are selected by extracting n-grams
(n=1, 2, 3) from the input documents corresponding
to each topic. Similar to (Xie et al, 2009), we re-
move (1) n-grams that appear only once in the docu-
ments; (2) n-grams that have a consisting word with
inverse document frequency (IDF) value lower than
a threshold; (3) n-grams that are enclosed by higher
order n-grams with the same frequency. These fil-
ters are designed to exclude insignificant n-grams
from the concept set. The IDF scores were calcu-
lated from a large background corpus corresponding
to the input text source, using individual sentences
or tweets as pseudo-documents; words with low IDF
scores (such as stopwords) tend to appear in many
sentences and therefore should be removed from the
concept set. We assign a weight wi to an n-gram
concept as follows:
wi = tf(ngrami)? n?max
j
idf(wij)
where tf(ngrami) is the term frequency of ngrami
in the input document of the topic; n denotes the
order of ngrami; wij are the consisting words of
ngrami; idf(wij) represents IDF value of word
wij . This approach aims to extract n-grams that ap-
pear frequently in each topic, but do not appear fre-
quently in a large background corpus. The weights
are also biased towards longer n-grams since they
carry more information.
4.2 Summarization Input
In this section, we explore different text sources
as input to the summarization system. Different
from previous studies that take input from a sin-
gle text source, we propose to utilize both the
user-contributed tweets and the linked web con-
tents for Twitter topic summarization, since these
two sources provide very different text quality and
may contain complementary information regarding
the topic. These text sources also pose great chal-
lenges to the summarization system: the tweets are
short and extremely noisy; while the online contents
linked from the tweets may have vastly different lay-
outs and contain a variety of information.
4.2.1 Original Tweets
As shown in Table 1, the initially collected tweets
are very noisy. They are passed through a set of pre-
processors to remove non-ascii characters, HTML
special characters, URLs, emoticons, punctuation
marks, retweet tags (RT @user), etc. We also re-
move the reply (@) and hashtag (#) tokens that do
not carry important syntactic roles (such as in the
subject or object position) by using a set of regular
expressions. These preprocessed tweets are sorted
by date and taken as the first input source to the sum-
marization system (denoted by ?OrigTweets?).
4.2.2 Normalized Tweets
The original tweets contain various nonstandard
word tokens. In Table 2, we list the possible to-
ken categories and corresponding examples. We hy-
pothesize that normalizing these nonstandard tokens
into standard English words and using the normal-
ized tweets as input can help boost the summariza-
tion performance.
We developed a twitter message normalization
system based on the noisy-channel framework and
a proposed letter transformation model (Liu et al,
69
Category Example
(1) abbreviation tgthr, weeknd, shudnt
(2) phonetic sub w/- or w/o digit 4got, sumbody, kulture
(3) graphemic sub w/- or w/o digit t0gether, h3r3, 5top, doinq
(4) typographic error thimg, macam
(5) stylistic variation betta, hubbie, cutie
(6) letter repetition pleeeaas, togtherrr
(7) any combination of (1) to (6) luvvvin, 2moro, m0rnin
Table 2: Nonstandard token categories and examples.
2011). Given a noisy tweet T , our goal is to nor-
malize it into a standard English word sequence S.
Under the noisy channel model, this is equivalent to
finding the sequence S? that maximizes p(S|T ):
S? = argmaxS p(S|T ) = argmaxS(
?
i
p(Ti|Si))p(S)
where we assume that each non-standard token Ti
is dependent on only one English word Si, that is,
we are not considering acronyms (e.g., ?bbl? for ?be
back later?) in this study. p(S) can be calculated
using a language model (LM). We formulate the
process of generating a nonstandard token Ti from
dictionary word Si using a letter transformation
model, and use the model confidence as the prob-
ability p(Ti|Si). This transformation process will be
learned automatically through a sequence labeling
framework. To form a nonstandard token, each let-
ter in the dictionary word can be labeled with: (a)
one of the 0-9 digits; (b) one of the 26 characters
including itself; (c) the null character ?-?; (d) a let-
ter combination. We integrate character-, phonetic-,
and syllable-level features in the model that can ef-
fectively characterize the formation process of non-
standard tokens. In general, the letter transforma-
tion approach will handle the nonstandard tokens
listed in Table 2 yet without explicitly categorizing
them. The proposed system also achieved robust
performance using the automatically collected train-
ing word pairs. On a test set of 3,802 distinct non-
standard tokens collected from Twitter, our system
achieved 68.88% 1-best normalization word accu-
racy and 78.27% 3-best accuracy.
We identify the nonstandard tokens that need to
be normalized using the following criteria: (1) it is
not in the CMU dictionary2; (2) it does not contain
capitalized letter; (3) it appears infrequently in the
2http://www.speech.cs.cmu.edu/cgi-bin/cmudict
topic (less than a threshold); (4) it is not a popular
chat acronyms (such as ?lol?, ?omg?); (5) it contains
letters/digits/apostrophe, but should not be numbers
only. These criteria are designed to avoid normaliz-
ing the named entities, frequently appearing out-of-
vocabulary terms (such as ?itunes?), chat acronyms,
usernames, and hashtags. The selected nonstandard
tokens in the original tweets will be replaced by the
system generated 1-best candidate word. Note that
we do not discriminate the context when replacing
each nonstandard token. This will be addressed in
the future work. We use these normalized tweets as
a second source of summarization input and name
them ?NormTweets?.
4.2.3 Linked Web Contents
For each Twitter topic, we collect a set of web
pages linked by the topic tweets and use them as
another source of summarization input. For each
topic, we select up to n (n = 10) URLs that appear
most frequently in the topic tweets and infrequently
across different Twitter topics. This scheme is sim-
ilar to the TF-IDF measure. This way we can se-
lect the salient URLs for each topic while avoiding
the spam URLs. The contents of these URLs were
collected and only distinct web pages were retained.
We use an HTML parser3 to extract the textual con-
tents, and perform sentence segmentation (Reynar
and Ratnaparkhi, 1997) on the parsed web pages.
All the pages corresponding to the same topic were
sorted by the date they were first cited in the tweets.
These web pages were taken as another input text
source for the summarization system, denoted as
?Web?.
4.2.4 Combining Tweets and Web Contents
We expect that taking advantage of both tweets
and linked web contents would benefit the topic
summarization system. Consolidating the distinct
text sources may help boost the weight of key con-
cepts and eliminate the spam information. As a pre-
liminary study, we investigate concatenating either
the original tweets or the normalized tweets with
the linked web pages as input to the concept-based
summarization system. This results in two inputs
?Web + OrigTweets? and ?Web + NormTweets?. We
will explore other ways of combining the two text
3http://jericho.htmlparser.net/docs/index.html
70
sources in future work.
5 Experiments
5.1 Experimental Setup
Among the collected topics, we select 500 general
topics (such as ?Chilean miners?) and 50 hashtag
topics (such as ?#octoberwish?, ?#wheniwasakid?)
for experimentation. On average, a general topic
contains 1673 tweets and 3.43 extracted linked web
pages; while a hashtag topic contains 3316 tweets
but does not have meaningful linked web pages.
The concept-based optimization system was con-
figured to extract a collection of sentences/tweets
for each topic, using either the sentence- or word-
constraint (denoted as ?#Sent? and ?#Word?). We
opt to set individual length constraint for each topic
rather than using a uniform length limit for all the
topics, since the topics can be very different in
length and duration. We use the number of sen-
tences/words in the reference summary as the sen-
tence/word constraint for each topic. Note that in
practice this reference summary length information
may not be available. We use the length constraints
obtained from the reference summary in this ex-
ploratory study, since our focus is to first evaluate if
twitter trending summarization is feasible, and what
are the effects of different information sources and
non-standard tokens. For a comparison to our ap-
proach, we implement the Hybrid TF-IDF approach
in (Sharifi et al, 2010b; Inouye, 2010) as a baseline
using ?OrigTweets? as input. For the baseline, the
summary length is altered according to the sentence-
or word-constraint. The last summary tweet is cut in
the middle if it exceeds the word limit.
The ROUGE-1 F-scores (Lin, 2004) are used to
measure the n-gram (n=1) overlap between the sys-
tem summaries and reference summaries. Since the
ROUGE scores may not correlate well with the hu-
man judgments (Liu and Liu, 2010b), we also per-
formed human evaluation by asking annotators to
score both the system and reference summaries re-
garding the linguistic quality and content respon-
siveness, in the hope this will benefit future research
in this direction.
5.2 Automatic Evaluation
We present the results (ROUGE-1 F-measure) for
the general topics in Table 3. ROUGE-2 and
General Topics R-1 F(%) RefSum
Input Source Render #Sent #Word Cov(%)
OrigTweets
Orig 29.53 30.21 94.81
Norm 29.41 30.21 94.81
NormTweets Norm 29.69 30.35 94.60
Web 24.32 25.07 63.74
Web + OrigTweets 29.58 30.44 95.37
Web + NormTweets 29.66 30.54 95.16
OrigTweets
(Sharifi et al, 2010b) 24.37 25.68 94.81
Table 3: ROUGE-1 F-measure and reference summary
coverage scores for general topics.
ROUGE-4 scores show similar trends and thus are
not presented. Five different text sources were ex-
ploited as the system inputs, as described in Sec-
tion 4.2. To measure the quality of the input for
summarization, we also include reference summary
coverage score in the table, defined as the percent-
age of words in the reference summary that are cov-
ered by the input text source. When using tweets
as input, we also investigate whether we should ap-
ply tweet normalization before or after the summa-
rization process, that is ?pre-normalization? (using
?NormTweets? as input), or ?post-normalization?
(using ?OrigTweets? as input, and rendering the nor-
malized summary tweets).
Compared to the Hybrid TF-IDF approach (Shar-
ifi et al, 2010b; Inouye, 2010), our system per-
forms significantly better (p < 0.05) according
to the paired t-test; however, we also notice the
ROUGE scores are lower compared to summariza-
tion in other text domains. This indicates that Twit-
ter topic summarization is very challenging. Com-
paring the two constraints used in the concept-based
optimization framework, we found that the word
constraint performs constantly better for the gen-
eral topics. This is natural since the word constraint
tightly bounds the length of the system output, while
the sentence constraint is relatively loose. For the
different sources, we notice using linked web pages
alone yields worse summarization performance, as
well as lower reference summary coverage; how-
ever, when combined with the tweets, there is a
slight increase in the coverage scores, and some-
times improved summarization results. This sug-
gests that the linked web pages can contain extra
71
useful information for generating summaries. Re-
garding normalization, results show that the ?pre-
normalization? (using normalized tweets as input)
can generally improve the summary tweet selec-
tion. For general topics, the best performance was
achieved by combining the normalized tweets and
linked web pages as input source and using the
word-level constraint.
Hashtag Topics R-1 F(%) RefSum
Input Source Render #Sent #Word Cov(%)
OrigTweets
Orig 9.08 7.19 93.93
Norm 9.09 7.16 93.93
NormTweets Norm 9.35 7.14 93.71
OrigTweets
(Sharifi et al, 2010b) 7.03 7.72 93.93
Table 4: ROUGE-1 F-measure and reference summary
coverage scores for hashtag topics.
Results for hashtag topics were shown in Table
4 using tweets as input (there are no linked web-
pages for these topics). We notice the reference cov-
erage scores are satisfying, yet the system output
barely matches the reference summaries (very low
ROUGE-1 scores). Looking at the reference and
system generated summaries for the hashtag top-
ics, we found the system output is more specific
(e.g., ?#octoberwish everything goes well.?), while
the reference summaries are often very general (e.g.,
?people tweeting about their wishes for October.?).
The human annotators also noted that most hashtag
topics (such as ?#octoberwish?, ?#wheniwasakid?)
are self-explainable and may require special atten-
tion to redefine an appropriate summary. Using
sentence constraints yields better performance than
word-based one, with larger performance difference
than that for the general topics. We found the
word-constraint summaries tend to include tweets
that are very short and noisy. Our system with
sentence-based length constraint also significantly
outperforms the Hybrid TF-IDF approach (Sharifi
et al, 2010b; Inouye, 2010). For hashtag topics,
the best performance was achieved using the ?pre-
normalization? with sentence constraint.
For an analysis, we generate oracle system per-
formance by using the reference summaries to ex-
tract a set of unweighted concepts to use in the ILP
optimization framework for sentences/tweets selec-
tion. This results in 61.76% ROUGE-1 F-score for
the general topics and 40.34% for the hashtag topics,
indicating abundant space for future improvement.
We also notice that though there is some perfor-
mance gain using normalized tweets and linked web
contents, the improvement is not statistically signifi-
cant as compared to using the original tweets. Upon
closer examination, we found the normalization sys-
tem replaced 1.08% and 1.8% of the total word to-
kens for the general and hashtag topics respectively;
these tokens spread in 13.12% and 16.85% of the
total tweets. The relatively small percentage of the
normalized tokens partly explains the marginal per-
formance gain when using the normalized tweets as
input. Similarly for linked web content, though it
contains some sentences that can provide more de-
tails of the topic, but they can also take more space
in the summary as compared to the short and con-
densed tweets. Therefore using the combined tweets
and linked webpages does not significantly outper-
form using just the tweets.
5.3 Human Evaluation
General Hashtag
Tweet Web Ref Tweet Ref
Gram. 3.13 3.42 4.52 3.04 4.24
NRedun. 3.93 4.64 4.30 4.82 3.62
Clarity 4.07 3.91 4.77 4.06 4.60
Focus 3.64 3.03 4.75 3.22 4.72
Content 2.82 2.55 n/a 2.60 n/a
ExtraInfo n/a 2.63 n/a n/a n/a
Table 5: Linguistic quality, content coverage, and useful-
ness scores judged by human assessors.
We ask two human annotators to manually evalu-
ate the system and reference summaries regarding
the readability and content coverage. Readability
includes grammaticality, non-redundancy, referen-
tial clarity, and focus; content coverage was eval-
uated for system summaries against the reference
summary. The annotators were also asked to rate
the ?Web? summaries regarding whether they pro-
vided extra useful topic information on top of the
?Tweet? summary. 50 general topics and 25 hash-
tag topics were randomly selected for assessment.
The ?Tweet? and ?Web? summaries were generated
using the original tweets and linked web pages with
word constraint for general topics, and sentence con-
straint for hashtag topics. Each of the assessors was
72
General Topic: ?3PAR?
RefSum
Dell Inc. and Hewlett-Packard Co. are both bidding for storage device maker 3Par Inc.
3Par jumped 21 percent after Hewlett- Packard Co. offered $30 a share for the company.
TweetSum
Dell ups 3Par offer yet again, to $27 per share
Dell Raises 3par Offer to Match HP Bid
Dell Matches HP?s Offer for 3Par, Boosting Bid to $1.8 Billion
WebSum
Dell Matches HP?s $27 Offer, Is Accepted by 3PAR.
3PAR has accepted an increased acquisition offer from Dell of US$27 per share, matching
Hewlett-Packard?s earlier raised bid.
Hashtag Topic: ?#wheniwasakid?
RefSum
when i was a kid.... people are sharing there best (good or bad) memories from childhood.
People reminise the wonderful times about being a kid.
TweetSum
#whenIwasakid getting wasted meant eating all the ice cream and candy you could until you puked!
#whenIWasAKid Apple & Blackberry were fruits not phones.
Table 6: Example system and reference summaries for both general and hashtag topics.
asked to judge all the summaries and assign a score
for each criterion on a 1 to 5 Likert scale (5 being
the best quality). The average scores of the two as-
sessors were presented in Table 5.
For general topics, the ?Web? summaries outper-
form the ?Tweet? summaries on both grammatical-
ity and non-redundancy, confirming the advantage
of using the high-quality linked web pages. The
referential clarity and focus scores of the ?Web?
summaries are not very high, since the summary
sentences were extracted simultaneously from sev-
eral web pages, and the system subjects to simi-
lar challenges as in multi-document summarization.
The content coverage scores of both system sum-
maries seem to correlate well with the ROUGE-1
F-measure, with a higher score for ?Tweet? sum-
maries. The assessors also rated that 48% of the
?Web? summaries contain ?Somewhat Useful? ex-
tra topic information, and 21% are ?Very Useful?.
Note that this could be just because of the inherent
difference of the two summaries, regardless of the
input source, but in general we believe the linked
web pages (such as the news documents) can pro-
vide more detailed and coherent stories as compared
to the 140-character tweets. For hashtag topics, the
?Tweet? summaries yield worse grammaticality and
focus scores, but have very high non-redundancy
score. On the contrary, the reference summaries
often contain redundant information. The content
match score between the system and reference sum-
maries (2.6) does not seem to reflect the ROUGE
scores. We hypothesize that even though the speci-
ficity of the two summaries is different, the asses-
sors may still think the system summaries match the
reference ones to some extent. A larger scale human
evaluation is needed to study the correlation between
human and automatic evaluation.
5.4 Discussions
We show an example of reference and system gen-
erated summaries for a general and a hashtag topic
in Table 6, and summarize some challenges for this
summarization task below:
? Gold standard summaries are difficult and
time-consuming to obtain. The reference de-
scriptions from WhatTheTrend.com were cre-
ated by Twitter users, which vary a lot in
word usage and would be unavoidably biased
to the information available in Twitter. The
user-contributed descriptions may also contain
spam descriptions, repetitions, nonstandard to-
kens, etc. It would be better to have a con-
cise non-redundant sentence collection for de-
veloping future summarization systems. In
particular, hashtag topics need special atten-
tion. They account for 40% of the total trend-
ing topics in 2010 according to the statistics
in WhatTheTrend.com4. Yet there still lacks
standard definition regarding a good hashtag
summary. From the example topic ?#wheni-
wasakid? in Table 6, we can see they are very
different in nature from general topics, thus fu-
ture efforts are needed to define an appropriate
summary.
4http://yearinreview.whatthetrend.com/
73
? Evaluation issues. Word based evaluation
measures will rarely consider semantic relat-
edness between concepts, or name entity vari-
ations, such as ?Hewlett-Packard? vs. ?HP?,
?Dell ups 3Par offer? vs. ?Dell Raises 3par
Offer?, etc. When comparing the system
summaries with short human-written reference
summaries, the word overlap varies a lot for
different human summarizers.
? Dynamically changing topics/events. Some
general topics are related to events that are con-
stantly changing. Take the ?3PAR? topic in
Table 6 as an example, where two companies
take turns to raise the bid for 3Par Inc. A good
topic summary should be able to develop a se-
ries of sub-events and show the topic evolving
process.
6 Conclusion
In this paper, we proposed to explore a variety of text
sources for summarizing the Twitter topics. We em-
ployed the concept-based optimization framework
with multiple input text sources to generate the sum-
maries. We conducted both automatic and human
evaluation regarding the summary quality. Better
performance is observed when using the normalized
tweets as input, indicating special treatment should
be performed before feeding the noisy tweets to the
summarization system. We also found the linked
web contents can provide extra useful topic infor-
mation. In future work, we will compare our sys-
tem with other dedicated microblog summarization
systems, as well as address some of the challenges
identified in this study.
Acknowledgments
This work is partly supported by NSF award IIS-
0845484. Any opinions expressed in this work are
those of the authors and do not necessarily reflect the
views of NSF.
References
Michel Galley. 2006. A skip-chain conditional random
field for ranking meeting utterances by importance. In
Proc. of EMNLP.
Dan Gillick, Korbinian Riedhammer, Benoit Favre, and
Dilek Hakkani-Tu?r. 2009. A global optimization
framework for meeting summarization. In Proc. of
ICASSP.
David Inouye. 2010. Multiple post microblog summa-
rization. REU Research Final Report.
Chin-Yew Lin. 2004. ROUGE: A package for automatic
evaluation of summaries. In Workshop on Text Sum-
marization Branches Out.
Fei Liu and Yang Liu. 2010a. Exploring speaker char-
acteristics for meeting summarization. In Proc. of IN-
TERSPEECH.
Feifan Liu and Yang Liu. 2010b. Exploring correlation
between ROUGE and human evaluation on meeting
summaries. IEEE Transactions on Audio, Speech, and
Language Processing, 18(1):187?196.
Fei Liu, Fuliang Weng, Bingqing Wang, and Yang Liu.
2011. Insertion, deletion, or substitution? Normaliz-
ing text messages without pre-categorization nor su-
pervision. In Proc. of ACL-HLT.
Adam Marcus, Michael S. Bernstein, Osama Badar,
David R. Karger, Samuel Madden, and Robert C.
Miller. 2011. TwitInfo: Aggregating and visualizing
microblogs for event exploration. In Proc. of CHI.
Sameer Maskey and Julia Hirschberg. 2005. Compar-
ing lexical, acoustic/prosodic, structural and discourse
features for speech summarization. In Proc. of Eu-
rospeech.
Gabriel Murray, Steve Renals, Jean Carletta, and Johanna
Moore. 2006. Incorporating speaker and discourse
features into speech summarization. In Proc. of HLT-
NAACL.
Gabriel Murray, Giuseppe Carenini, and Raymond Ng.
2010. Interpretation and transformation for abstract-
ing conversations. In Proc. of NAACL.
Brendan O?Connor, Michel Krieger, and David Ahn.
2010. Tweetmotif: Exploratory search and topic sum-
marization for twitter. In Proc. of the International
AAAI Conference on Weblogs and Social Media.
Leena Rao. 2010. Twitter seeing 90 mil-
lion tweets per day, 25 percent contain links.
http://techcrunch.com/2010/09/14/twitter-seeing-90-
million-tweets-per-day/.
Jeffrey C. Reynar and Adwait Ratnaparkhi. 1997. A
maximum entropy approach to identifying sentence
boundaries. In Proc. of the Fifth Conference on Ap-
plied Natural Language Processing.
Beaux Sharifi, Mark-Anthony Hutton, and Jugal Kalita.
2010a. Summarizing microblogs automatically. In
Proc. of HLT/NAACL.
Beaux Sharifi, Mark-Anthony Hutton, and Jugal K.
Kalita. 2010b. Experiments in microblog summariza-
tion. In Proc. of IEEE Second International Confer-
ence on Social Computing.
74
Shaomei Wu, Jake M. Hofman, Winter A. Mason, and
Duncan J. Watts. 2011. Who says what to whom on
twitter. In Proc. of WWW.
Shasha Xie, Yang Liu, and Hui Lin. 2008. Evaluating
the effectiveness of features and sampling in extractive
meeting summarization. In Proc. of IEEE Workshop
on Spoken Language Technology.
Shasha Xie, Benoit Favre, Dilek Hakkani-Tu?r, and Yang
Liu. 2009. Leveraging sentence weights in a concept-
based optimization framework for extractive meeting
summarization. In Proc. of INTERSPEECH.
Klaus Zechner. 2002. Automatic summarization of
open-domain multiparty dialogues in diverse genres.
Computational Linguistics, 28(4):447?485.
75
Proceedings of the Sixth Workshop on Innovative Use of NLP for Building Educational Applications, pages 87?95,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
Measuring Language Development in Early Childhood Education: A Case
Study of Grammar Checking in Child Language Transcripts
Khairun-nisa Hassanali
Computer Science Department
The University of Texas at Dallas
Richardson, TX, USA
nisa@hlt.utdallas.edu
Yang Liu
Computer Science Department
The University of Texas at Dallas
Richardson, TX, USA
yangl@hlt.utdallas.edu
Abstract
Language sample analysis is an important
technique used in measuring language devel-
opment. At present, measures of grammati-
cal complexity such as the Index of Productive
Syntax (Scarborough, 1990) are used to mea-
sure language development in early childhood.
Although these measures depict the overall
competence in the usage of language, they do
not provide for an analysis of the grammati-
cal mistakes made by the child. In this paper,
we explore the use of existing Natural Lan-
guage Processing (NLP) techniques to provide
an insight into the processing of child lan-
guage transcripts and challenges in automatic
grammar checking. We explore the automatic
detection of 6 types of verb related grammat-
ical errors. We compare rule based systems
to statistical systems and investigate the use
of different features. We found the statistical
systems performed better than the rule based
systems for most of the error categories.
1 Introduction
Automatic grammar checking and correction has
been used extensively in several applications. One
such application is in word processors where the
user is notified of a potential ungrammatical sen-
tence. This feature makes it easier for the users to
detect and correct ungrammatical sentences. Au-
tomatic grammar checking can also be beneficial
in language learning where students are given sug-
gestions on potential grammatical errors (Lee and
Seneff, 2006). Another application of grammar
checking is in improving a parser?s performance for
ungrammatical sentences. Since most parsers are
trained on written data consisting mostly of gram-
matical sentences, the parsers face issues when pars-
ing ungrammatical sentences. Automatic detection
and correction of these ungrammatical sentences
would improve the parser?s performance by detect-
ing the ungrammatical sentences and performing
a second parse on the corrected sentences (Caines
and Buttery, 2010). From an education perspective,
measuring language skills has been extensively ex-
plored. There are systems in place that automatically
detect and correct errors for second language learn-
ers (Eeg-Olofsson and Knuttson, 2003; Leacock et
al., 2010).
One method used in measuring language devel-
opment is the analysis of transcripts of child lan-
guage speech. Child language transcripts are sam-
ples of a child?s utterances during a specified pe-
riod of time. Educators and speech language pathol-
ogists use these samples to measure language de-
velopment. In particular, speech language pathol-
ogists score these transcripts for grammatical mea-
sures of complexity amidst other measures. Since
manual analysis of transcripts is time consuming,
many of these grammatical complexity measures re-
quire the speech language pathologists to look for
just a few examples. The Index of Productive Syn-
tax (IPSyn) (Scarborough, 1990) is one such mea-
sure of morphological and syntactic structure devel-
oped for measuring language samples of preschool
children. The advantage of measures such as IPSyn
is that they give a single score that can be used to
holistically measure language development. How-
ever, they focus on grammatical constructs that the
87
child uses correctly and do not take into account
the number and type of grammatical errors that are
made by the child.
Educators wishing to measure language develop-
ment and competence in a child will benefit from
having access to the grammatical errors made by a
child. Analysis of these grammatical errors will en-
able educators and speech language pathologists to
identify shortcomings in the child?s language and
recommend intervention techniques customized to
the child. Since manual identification of grammat-
ical errors is both cumbersome and time consum-
ing, a tool that automatically does grammar check-
ing would be of great use to clinicians. Addition-
ally, we see several uses of automatic grammar de-
tection. For example, we can use the statistics of
grammatical errors as features in building classifiers
that predict language impairment. Furthermore, we
could also use the statistics of these grammatical er-
rors to come up with a measure of language develop-
ment that takes into account both grammatical com-
petence and grammatical deficiencies.
In this paper, we use existing NLP techniques to
automatically detect grammatical errors from child
language transcripts. Since children with Language
Impairment (LI) have a greater problem with correct
usage of verbs compared to Typically Developing
(TD) children (Rice et al, 1995), we focus mainly
on verb related errors. We compare rule based sys-
tems to statistical systems and investigate the use
of different features. We found the statistical sys-
tems performed better than the rule based systems
for most error categories.
2 Related Work
While there has been considerable work (Sagae et
al., 2007) done on annotating child language tran-
scripts for grammatical relations, as far as we know,
there has been no work done on automatic gram-
mar checking of child language transcripts. Most
of the existing work in automatic grammar check-
ing has been done on written text. Spoken language
on the other hand, presents challenges such as dis-
fluencies and false restarts which are not present in
written text. We believe that the specific research
challenges that are encountered in detecting and cor-
recting child language transcripts warrant a more de-
tailed examination.
Caines and Buttery (2010) focused on identify-
ing sentences with the missing auxiliary verb in the
progressive aspect constructions. They used logistic
regression to predict the presence of zero auxiliary
occurrence in the spoken British National Corpus
(BNC). An example of a zero auxiliary construction
is ?You talking to me??. They first identified con-
structions with the progressive aspect and annotated
the constructions for the following features: sub-
ject person, subject case, perfect aspect, presence of
negation and use of pronouns. Their model identi-
fied zero auxiliary constructions with 96.9% accu-
racy. They also demonstrated how their model can
be integrated into existing parsing tools, thereby in-
creasing the number of successful parses for zero
auxiliary constructions by 30%.
Lee and Seneff (2008) described a system for verb
error correction using template matching on parse
trees in two ways. Their work focused on correct-
ing the error types related to subject-verb agreement,
auxiliary agreement and complementation. They
considered the irregularities in parse trees caused
by verb error forms and used n-gram counts to fil-
ter proposed corrections. They used the AQUAINT
Corpus of English News Text to detect the irregular-
ities in the parse trees caused by verb error forms.
They reported an accuracy of 98.93% for verb er-
rors related to subject-verb agreement, and 98.94%
for verb errors related to auxiliary agreement and
complementation. Bowden and Fox (2002) devel-
oped a system to detect and explain errors made by
non-native English speakers. They used classifica-
tion and pattern matching rules instead of thorough
parsing. Their system searched for the verb-related
errors and noun-related errors one by one in one sen-
tence by narrowing down the classification of errors.
Lee and Seneff (2006) developed a system to auto-
matically correct grammatical errors related to arti-
cles, verbs, prepositions and nouns.
Leacock et al (2010) discuss automated gram-
matical error detection for English language learn-
ers. They focus on errors that language learners find
most difficult - constructions that contain preposi-
tions, articles, and collocations. They discuss the
existing systems in place for automated grammati-
cal error detection and correction for these and other
classes of errors in a number of languages. Addi-
88
Label Meaning Example
0 No error I like it.
1 Missing auxiliary verb You talking to me?
2 Missing copulae She lovely.
3 Subject-auxiliary verb agreement You is talking to me.
4 Incorrect auxiliary verb used e.g. using does instead of is She does dead girl.
5 Missing verb She her a book.
6 Wrong verb usage including subject-verb disagreement He love dogs.
7 Missing preposition The book is the table.
8 Missing article She ate apple.
9 Missing subject before verb I know loves me.
10 Missing infinitive marker ?to? I give it her.
11 Other errors not covered in 1-10 The put.
Table 1: Different types of errors considered in this study
tionally, they touch on error annotations and system
evaluation for grammatical error detection.
3 Data
For the purpose of our experiments, we used the Par-
adise dataset (Paradise et al, 2005). This dataset
contains 677 transcripts corresponding to 677 chil-
dren aged six that were collected in the course of
a study of the relationship of otitis media and child
development. The only household language spoken
by these children was English. The transcripts in
the Paradise set consist of conversations between a
child and his/her caretaker. We retained only the
child?s utterances and removed all other utterances.
The Paradise dataset (considering only the child?s
utterances) contains a total of 108,711 utterances,
394,290 words, and an average Mean Length of Ut-
terance of 3.64. Gabani (2009) used scores on the
Peabody Picture Vocabulary Test (Dunn, 1965), total
percentage phonemes repeated correctly on a non-
word repetition task and mean length of utterance
in morphemes to label these transcripts for language
impairment. A transcript was labeled as having been
produced by a child with LI if the child scored 1.5
or more standard deviations below the mean of the
entire sample on at least two of the three tests. Of
the 677 transcripts, 623 were labeled as TD and 54
as LI.
We manually annotated each utterance in the tran-
scripts for 10 different types of errors. Table 1 gives
the different types of errors we considered along
with examples. We focused on these 10 different
types of errors since children with LI have problems
with the usage of verbs in particular. The list of er-
rors we arrived at was based on the errors we ob-
served in the transcripts. Since an utterance could
have more than one error, we annotated each ut-
terance in the transcript for all the errors present
in the utterance. While annotating the utterances,
we observed that there were utterances that could
correspond to multiple types of error. For exam-
ple, consider the following sentence: ?She go to
school?. The error in this sentence could be an er-
ror of a missing auxiliary and a wrong verb form
in which case the correct sentence would be ?She is
going to school?; or it could be a missing modal, in
which case the correct form would be ?She will go to
school?; or it could just be a subject-verb disagree-
ment in which case ?She goes to school? would be
the correct form. Therefore, although we know that
the utterance definitely has an error, it is not always
possible to assign a single error. We also observed
several utterances had both a missing subject and a
missing auxiliary verb error. For example, instead of
saying ?I am going to play?, some children say ?Go-
ing to play?, which misses both the subject and aux-
iliary verb. In this case, the utterance was annotated
as having two errors: missing subject and missing
auxiliary. Finally, single word utterances were la-
beled as being correct.
Table 2 gives the distribution of the errors in the
corpus and percentage of TD and LI population that
89
No Error Type Percentage
(Count)
% of LI children
making error
% of TD children
making error
1 Missing auxiliary 8.43% (641) 7% 5%
2 Missing copulae 36.67% (2788) 77.78% 45%
3 Subject-auxiliary agreement 6.31% (480) 40.74% 35%
4 Incorrect auxiliary verb used 0.71%(54) 11.47% 3%
5 Missing verb 5% (380) 29.63% 10%
6 Wrong verb usage 14.59% (1109) 68.5% 50%
7 Missing preposition 5% (380) 7.4% 5%
8 Missing article 3.97% (302) 29.63% 35%
9 Missing subject 7.69% (585) 3.7% 5%
10 Missing infinitive marker ?To? 1.58% (120) 7.5% 11.67%
11 Other errors 10.05% (764) 56.7% 23.2%
Table 2: Statistics of Errors
made the error at least once in the entire transcript.
As we can see from Table 2, 36.67% of the errors in
the corpus are due to missing copulae. Wrong verb
usage was the next most common error contributing
to 14.59% of the errors in the corpus. We observed
that there was a higher percentage of children with
LI that made errors on all error categories except for
errors related to missing article and missing subject.
We observed that on average, the transcripts belong-
ing to children with LI had fewer utterances as com-
pared to transcripts belonging to TD children. Ad-
ditionally, children with LI used many single word
and two word utterances.
One annotator labeled the entire corpus for gram-
matical errors. To calculate inter-annotator agree-
ment, we randomly selected 386 utterances anno-
tated by the first annotator with different error types.
The second annotator was provided these utterances
along with the labels given by the first annotator1.
In case of a disagreement, the second annotator pro-
vided a different label/labels. The annotator agree-
ment using the average Cohen?s Kappa coeffiecient
was 77.7%. Out of the 386 utterances, there were
43 disagreements between the annotators. We found
that for some error categories such as the missing
auxiliary, there was high inter-annotator agreement
of 95.32%, whereas for other categories such as
wrong verb usage and missing articles, there was
1We will perform independent annotation of the errors and
calculate inter-annotator agreement based on these independent
annotations
less agreement (64.2% and 65.3% respectively). In
particular, we found low inter-annotator agreement
on utterances that have errors that could be assigned
to multiple categories.
4 Experiments
The transcripts were parsed using the Charniak
parser (Charniak, 2000). Since the Paradise dataset
consists of children?s utterances, and many of them
have not mastered the language, we observed that
processing these transcripts is challenging. As is
prevalent in spoken language corpora, these tran-
scripts had disfluencies, false restarts and incom-
plete utterances, which sometimes pose problems to
the parser.
We conducted experiments in detecting errors re-
lated to the usage of the -ing participle, subject-
auxiliary agreement, missing copulae, missing
verb, subject-verb agreement and missing infinitive
marker ?to?. For each of these categories, we con-
structed one rule based classifier using regular ex-
pressions based on the parse tree structure, an alter-
nating decision tree classifier that used rules as fea-
tures and a naive Bayes multinomial classifier that
used a variety of features. For every category, we
performed 10 fold cross validation using all the ut-
terances. We used the naive Bayes multinomial clas-
sifier and the alternating decision tree classifier from
the WEKA toolkit (Hall et al, 2009). Table 3 gives
the results using the three classifiers for the different
categories of errors, where (P/R) F1 stands for (Pre-
90
Error Rule Based System
(P/R)F1
Decision Tree Clas-
sifier using Rules as
features (P/R)F1
Naive Bayes Classifier
using a variety of fea-
tures (P/R)F1
Usage of -ing participle (0.984/0.978) 0.981 (0.986/1) 0.993 (0.736/0.929) 0.821
Missing copulae (0.885/0.9) 0.892 (0.912/0.94) 0.926 (0.82/0.86) 0.84
Missing verb (0.875/0.932) 0.903 (0.92/0.89) 0.905 (0.87/0.91) 0.9
Subject-auxiliary agree-
ment
(0.855/0.932) 0.888 (0.95/0.84) 0.892 (0.89/0.934) 0.912
Subject-verb agreement (0.883/0.945) 0.892 (0.92/0.877) 0.898 (0.91/0.914) 0.912
Missing infinitive marker
?To?
(0.97/0.954) 0.962 (0.94/0.84) 0.887 (0.95/0.88) 0.914
Overall (0.935/0.923) 0.929 (0.945/0.965) 0.955 (0.956/0.978) 0.967
Table 3: Detection of errors using rule based system, alternating decision tree classifier and naive Bayes classifier
No Feature Type
1 Verb Adjective Bigram
2 Auxiliary Noun Bigram
3 Auxiliary Progressive-verb Bigram
4 Pronoun Auxiliary Bigram
5 Wh-Pronoun Progressive verb Bigram
6 Progressive-verb Wh-adverb Bigram
7 Adverb Auxiliary Skip-1
8 Pronoun Auxiliary Skip-1
9 Wh-adverb Progressive-verb Skip-1
10 Auxiliary Preposition Skip-2
Table 4: Top most bigram features useful for detecting
misuse of -ing participle
cision/Recall) F1-measure. Below we describe the
different experiments we conducted.
4.1 Misuse of the -ing Participle
The -ing participle can be used as a progressive as-
pect, a verb complementation, or a prepositional
complementation. In the progressive aspect, it is
necessary that the progressive verb be preceded by
an auxiliary verb. When used as a verb comple-
mentation, the -ing participle should be preceded by
a verb and similiarly when used as a prepositional
complement, the -ing participle should be preceded
by a preposition.
Rule based system
The -ing participle is denoted by the VBG tag in the
Penn tree bank notation. VP and PP correspond to
the verb phrase and prepositional phrase structures
respectively. The rules that we formed were as fol-
lows:
1. Check that the utterance has a VBG tag (if it
does not have a VBG tag, it does not contain an
-ing participle).
2. If none of the following conditions are met,
there is an error in the usage of -ing participle:
(a) The root of the subtree that contains the
-ing participle should be a VP with the
head being a verb if used as a verb com-
plementation
(b) The root of the subtree that contains the
-ing participle should be a PP if used as a
prepositional complement
(c) The root of the subtree that contains the
-ing participle should be a VP with the
head being an auxiliary verb if used as a
progressive aspect
Predictive model
The features that we considered were:
1. Bigrams from POS tags
2. Skip bigrams from POS tags
We used the skip bigrams to account for the
fact that there could be other POS tags between
an auxiliary verb and the progressive aspect of
the verb such as adverbs. A skip-n bigram is
a sequence of 2 POS tags with a distance of n
between them. We used skip-1 and skip-2 bi-
grams in this study.
91
Analysis
As we can see from Table 3, the alternating decision
tree classifier with rules as features gave the best re-
sults with an F1-measure of 0.993. Table 4 gives
the topmost 10 features extracted using feature se-
lection. We got the best results when we used the
reduced set of features as opposed to using all bi-
grams and skip-1 and skip-2 bigrams. We also used
the results reported by (Caines and Buttery, 2010)
to see if their method was successful in identifying
zero auxiliary constructs on our corpus. When we
used logistic regression with the coefficients and fea-
tures used by (Caines and Buttery, 2010), we got a
recall of 0%. When we trained the logistic regres-
sion model on our data with their features, we got a
precision of 1.09%, recall of 53.6% and F1-measure
of 2.14%. This leads us to conclude that the features
that were used by them are not suitable for child lan-
guage transcripts. Additionally, we also observed
that based on the features they used, in some cases
it is difficult to distinguish zero auxiliary constructs
from those with auxiliary constructs. For example,
?You talking to me?? and ?Are you talking to me??
would have the same values for their features, al-
though the former is a zero auxiliary construct and
the latter is not.
4.2 Identifying Missing Copulae
A copular verb is a verb that links a subject to its
complement. In English, the most common copular
verb is ?be?. Examples of sentences that contain a
copular verb is ?She is lovely? and ?The child who
fell sick was healthy earlier?. An example of a sen-
tence that misses a copular verb is ?She lovely?.
Rule based system
The rule that we used was as follows:
If an Adjective Phrase follows a noun phrase, or
a Noun phrase follows a noun phrase, the likelihood
that the utterance is missing a copular verb is quite
high. However, there are exceptions to such rules,
for example, ?Apple Pie?. We formed additional
rules to identify such utterances and examined their
parse trees to determine the function of the two noun
phrases.
Predictive model
The features we used were as follows:
1. Does the utterance contain a noun phrase fol-
lowed by a noun phrase?
2. Does the utterance contain a noun phrase fol-
lowed by an adjective phrase?
3. Is the parent a verb phrase?
4. Is the parent a prepositional phrase?
5. Is the parent the root of the parse tree?
6. Is there an auxiliary verb or a verb between the
noun phrase and/or adjective phrase?
Analysis
As we can see from Table 3, the alternating deci-
sion tree classifier performed the best with an F1-
measure of 0.926. Our rules capture simple con-
structs that are used by young children. The majority
of the utterances that missed a copulae consisted of
noun phrase and an adjective phrase or a noun phrase
and a noun phrase. Hence, the rules based system
performed the best. Some of the false positives were
due to utterances like ?She an apple? where it is un-
likely that the missing verb is a copular verb.
4.3 Identifying Missing Verbs
Errors of this type occur when a sentence is miss-
ing the verb. For example, the sentence ?You can
an apple? lacks the main verb after the modal verb
?can?. Similarly, ?I did not it? lacks a main verb af-
ter ?did not?. For the purpose of this experiment, we
consider only utterances that contain a modal or an
auxiliary verb but do not have a main verb. We also
consider utterances that use the verb ?do? and detect
the main missing verb in such cases.
Rule based system
The rule we used was to check if the utterance con-
tains an auxiliary verb or a modal verb but not a main
verb. In this case, the utterance is definitely missing
a main verb. In order to identify utterances where the
words ?did?, ?do? and ?does? are auxiliary verbs, we
use the following procedure: If the negation ?not?
is present after did/do/does, then did/do/does is an
auxiliary verb and needs to be followed by a main
verb. In the case of the utterance being a question,
the presence of did/do/does at the beginning of the
utterances indicates the use as an auxiliary verb. In
92
such a case, we need to check for the presence of a
main verb. The same holds for the other auxiliary
verbs.
Predictive model
We used the following as features:
1. Is an auxiliary verb present?
2. Is a modal verb present?
3. Is a main verb present after the auxiliary verb?
4. Is a main verb present after the modal verb?
5. Type of utterance - interrogative, declarative
6. Is a negation (not) present?
Analysis
As we can see from Table 3, the alternating decision
tree classifier using rules as features gave the best
result with an F1-measure of 0.905. At present, we
handle only a subset of missing verbs and specif-
ically those verbs that contain an auxiliary verb.
Since most of the utterances are simple constructs,
the alternating decision tree classifier performs well.
4.4 Identifying Subject-auxiliary Agreement
In the case of the subject-auxiliary agreement and
subject-verb agreement, the first verb in the verb
phrase has to agree with the subject unless the first
verb is a modal verb. In the sentence ?The girls has
bought a nice car?, since the subject ?The girls? is
a plural noun phrase, the auxiliary verb should be in
the plural form. While considering the number and
person of the subject, we take into account whether
the subject is an indefinite pronoun or contains a
conjunction since special rules apply to these cases.
Indefinite pronouns are words which replace nouns
without specifying the nouns they replace. Some in-
definite pronouns such as all, any and more take both
singular and plural forms. On the other hand, indefi-
nite pronouns like somebody and anyone always take
the singular form.
Rule based system
The rule we used to identify subject-auxiliary agree-
ment was as follows:
1. Extract the number (singular, plural) of the sub-
ject and the auxiliary verb in the verb phrase.
2. If the number of the subject and auxiliary verb
do not match, there is a subject-auxiliary agree-
ment error.
Predictive model
The features were as follows:
1. Number of subject - singular or plural
2. Type of noun phrase - pronoun or other noun
phrase
3. Person of noun phrase - first, second, third
4. Presence of a main verb in the utterance (we are
looking at the agreement only for the auxiliary
verb)
Analysis
As we can see from Table 3, the naive Bayes multi-
nomial classifier performed the best with an F1-
measure of 0.912. We found that our system did
not detect the subject-auxiliary agreement correctly
if there was an error in the subject such as number
agreement.
4.5 Identifying Subject-verb Agreement
In order to achieve subject-verb agreement, the num-
ber and person of the subject and verb must agree.
The subject-verb agreement applies to the first verb
in the verb phrase. We consider cases wherein the
first verb is a main verb or contains a modal verb.
An example of a sentence that has subject-verb dis-
agreement is ?The boy have an ice cream?. The
number and person of the subject ?The boy? and the
verb ?have? do not match.
Rule based system
The rule we used to identify subject-verb agreement
was as follows:
1. Extract the number (singular, plural) and per-
son (first, second, third) of the subject and the
first verb in the verb phrase.
2. If the verb is not a modal verb and the num-
ber and person of the subject and verb do not
match, there is a subject-verb agreement error.
Predictive model
We used the following features to be used in a statis-
tical setup:
93
1. Type of sentence - interrogative or declarative
2. Number of subject - singular or plural
3. Person of subject if pronoun - first, second or
third
4. Number of verb - singular or plural
5. Person of verb - first, second or third
6. Type of verb - modal, main
Analysis
We found that our system did not detect errors in
cases where there was a number disagreement. For
example, in the sentence ?The two dog is playing?,
our system based on the POS tag would assume
that the subject is singular and therefore there is no
subject-verb error. One way to improve this would
be to detect number disagreement in the subject and
correct it before detecting the subject-verb agree-
ment.
4.6 Identifying Missing Infinitive Marker ?To?
Errors of this type occur when the sentence lacks the
infinitive marker ?to?. An example of such a sen-
tence would be ?She loves sleep?. In this case, ?She
loves to sleep? would be the correct form. On the
other hand, this statement is ambiguous since sleep
could be used as a noun sense or a verb sense. We
concentrated on identifying utterances that have the
progressive verb followed by the verb in the infini-
tive form. Examples of such sentences are: ?She is
going cry?. In this case, we can see that the sentence
is missing the ?to?.
Rule based system
If the utterance contains a progressive verb followed
by a verb in its infinitive form, it is missing the in-
finitive marker ?to?.
Predictive model
The features we used are:
1. Presence of a progressive verb followed by the
infinitive
2. Presence of infinitive marker ?to? before the in-
finitive
Analysis
The naive Bayes multinomial classifier performed
the best with an F1-measure of 0.967. We encoun-
tered exceptions with words like ?saying?. An ex-
ample of such a sentence would be ?He was saying
play?. Most of our false positives were due to sen-
tences such as this. We considered a subset of utter-
ances in which the infinitive was used along with the
progressive verb. The missing infinitive marker ?to?
is also found in other utterances such as ?I would
love to swim? in which case we have two verbs that
are in the base form - ?love? and ?swim?.
4.7 Combining the Classifiers
Finally, we perform sentence level binary classifica-
tion - does the sentence have a grammatical error?
Since an utterance can contain more than one error,
we serially apply the binary classifiers that we de-
scribed above for each error category. If any one of
the classifiers reports an error in the utterance, we
flag the utterance as having a grammatical error. For
evaluation, as long as the utterance had any gram-
matical error, we considered the decision to be cor-
rect. As we can see from Table 3, the best result
for detecting the overall errors was obtained by se-
rially applying the classifiers that used the features
that were not rule based.
5 Conclusions and Future Work
In this paper, we described a study of grammati-
cal errors in child language transcripts. Our study
showed that a higher percentage of children with
LI made at least one mistake than TD children on
most error categories. We created different systems
including rule based systems that used parse tree
template matching and classifiers to detect errors re-
lated to missing verbs, subject-auxiliary agreement,
subject-verb agreement, missing infinitive marker
?to?, missing copulae and wrong usage of -ing par-
ticiple. In all cases, we had a recall higher than 84%.
When combining the classifiers to detect sentences
with grammatical errors, the classifiers that used fea-
tures other than rules performed the best with an F1-
measure of 0.967.
The error categories that we detect at present are
restricted in their scope to specific kind of errors.
In future, we plan to enhance our systems to de-
94
tect other grammatical errors such as missing arti-
cles, missing prepositions and missing main verbs
in utterances that do not have an auxiliary verb. Fur-
thermore, we will investigate methods to address is-
sues in child language transcripts due to incomplete
utterances and disfluencies.
At present, we treat sentences that conform to
formal English language as correct. We could en-
hance our systems to look at dialect specific con-
structs and grammatical errors made across differ-
ent demographics. For example, African American
children have a different dialect and do not always
follow the formal English language while speaking.
Therefore, in the context of detecting language im-
pairment, it would be interesting to see whether both
TD children and LI children make the same errors
that are otherwise considered the norm in the dialect
they speak.
Acknowledgments
The authors thank Chris Dollaghan for sharing the
Paradise data, and Thamar Solorio for discussions.
This research is partly supported by an NSF award
IIS-1017190.
References
Mari I. Bowden and Richard K. Fox. 2002. A Diagnostic
Approach to the Detection of Syntactic Errors in En-
glish for Non-Native Speakers. Technical report, The
University of Texas-Pan American.
Andrew Caines and Paula Buttery. 2010. You talking to
me?: A predictive model for zero auxiliary construc-
tions. In Proceedings of the 2010 Workshop on NLP
and Linguistics: Finding the Common Ground, pages
43?51.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of the 1st North American
chapter of the Association for Computational Linguis-
tics conference, pages 132?139.
Lloyd M. Dunn. 1965. Peabody picture vocabulary test.
American Guidance Service Circle Pines, MN.
Jens Eeg-Olofsson and Ola Knuttson. 2003. Automatic
grammar checking for second language learners-the
use of prepositions. In Proceedings of NoDaLiDa.
Keyur Gabani. 2009. Automatic identification of lan-
guage impairment in monolingual English-speaking
children. Master?s thesis, The University Of Texas At
Dallas.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: An update.
ACM SIGKDD Explorations Newsletter, 11(1):10?18.
Claudia Leacock, Martin Chodorow, Michael Gamon,
and Joel Tetreault. 2010. Automated Grammatical Er-
ror Detection for Language Learners. Synthesis Lec-
tures on Human Language Technologies, 3(1):1?134.
John Lee and Stephanie Seneff. 2006. Automatic gram-
mar correction for second-language learners. In Pro-
ceedings of INTERSPEECH-2006, pages 1978?1981.
John Lee and Stephanie Seneff. 2008. Correcting misuse
of verb forms. In Proceedings of ACL-08:HLT, pages
174?182.
Jack L. Paradise, Thomas F. Campbell, Christine A.
Dollaghan, Heidi M. Feldman, Bernard S. Bernard,
D. Kathleen Colborn, Howard E. Rockette, Janine E.
Janosky, Dayna L. Pitcairn, Marcia Kurs-Lasky, et al
2005. Developmental outcomes after early or delayed
insertion of tympanostomy tubes. New England Jour-
nal of Medicine, 353(6):576?586.
Mabel L. Rice, Kenneth Wexler, and Patricia L. Cleave.
1995. Specific language impairment as a period of
extended optional infinitive. Journal of Speech and
Hearing Research, 38(4):850.
Kenji Sagae, Eric Davis, Alon Lavie, Brian MacWhin-
ney, and Shuly Wintner. 2007. High-accuracy annota-
tion and parsing of CHILDES transcripts. In Proceed-
ings of the Workshop on Cognitive Aspects of Compu-
tational Language Acquisition, pages 25?32.
Hollis S. Scarborough. 1990. Index of productive syntax.
Applied Psycholinguistics, 11(01):1?22.
95
Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, ACL-HLT 2011, pages 161?167,
24 June, 2011, Portland, Oregon, USA c?2011 Association for Computational Linguistics
A Cross-corpus Study of Unsupervised Subjectivity Identification
based on Calibrated EM
Dong Wang Yang Liu
The University of Texas at Dallas
{dongwang,yangl}@hlt.utdallas.edu
Abstract
In this study we investigate using an unsu-
pervised generative learning method for sub-
jectivity detection in text across different do-
mains. We create an initial training set using
simple lexicon information, and then evaluate
a calibrated EM (expectation-maximization)
method to learn from unannotated data. We
evaluate this unsupervised learning approach
on three different domains: movie data, news
resource, and meeting dialogues. We also per-
form a thorough analysis to examine impact-
ing factors on unsupervised learning, such as
the size and self-labeling accuracy of the ini-
tial training set. Our experiments and analysis
show inherent differences across domains and
performance gain from calibration in EM.
1 Introduction
Subjectivity identification is to identify whether an
expression contains opinion or sentiment. Auto-
matic subjectivity identification can benefit many
natural language processing (NLP) tasks. For ex-
ample, information retrieval systems can provide af-
fective or informative articles separately (Pang and
Lee, 2008). Summarization systems may want to
summarize factual and opinionated content differ-
ently (Murray and Carenini, 2008). In this paper,
we perform subjectivity detection at sentence level,
which is more appropriate for some subsequent pro-
cessing such as opinion summarization.
Previous work has shown that when enough la-
beled data is available, supervised classification
methods can achieve high accuracy for subjectivity
detection in some domains. However, it is often ex-
pensive to create such training data. On the other
hand, a lot of unannotated data is readily available
in various domains. Therefore an interesting and
important problem is to develop semi-supervised or
unsupervised learning methods that can learn from
an unannotated corpus. In this study, we use an un-
supervised learning approach where we first use a
knowledge-based method to create an initial train-
ing set, and then apply a calibrated EM method
to learn from an unannotated corpus. Our experi-
ments show significant differences among the three
domains: movie, news article, and meeting dialog.
This can be explained by the inherent difference of
the data, especially the task difficulty and classifier?s
performance for a domain. We demonstrate that for
some domains (e.g., movie data) the unsupervised
learning methods can rival the supervised approach.
2 Related Work
In the early age, knowledge-based methods were
widely used for subjectivity detection. They used
a lexicon or patterns and rules to predict whether a
target is subjective or not. These methods tended
to yield a high precision and low recall, or low
precision and high recall (Kim and Hovy, 2005).
Recently, machine learning approaches have been
adopted more often (Ng et al, 2006). There are
limitations in both methods. In knowledge-based
approaches, a predefined subjectivity lexicon may
not adapt well to different domains. While in ma-
chine learning approach, human labeling efforts are
required to create a large training set.
To overcome the above drawbacks, unsupervised
or semi-supervised methods have been explored in
sentiment analysis. For polarity classification, some
previous work used spectral techniques (Dasgupta
and Ng, 2009) or co-training (Li et al, 2010) to
mine the reviews in a semi-supervised manner. For
subjectivity identification, Wiebe and Riloff (Wiebe
and Riloff, 2005) applied a rule-based method to
create a training set first and then used it to train
a naive Bayes classifier. Melville et al (Melville
et al, 2009) used a pooling multinomial method to
combine lexicon derived probability and statistical
probability.
Our work is similar to the study in (Wiebe and
Riloff, 2005) in that we both use a rule-based
method to create an initial training set and learn from
161
unannotated corpus. However, there are two key dif-
ferences. First, unlike the self-training method they
used, we use a calibrated EM iterative learning ap-
proach. Second, we compare the results on three dif-
ferent corpora in order to evaluate the domain/genre
effect of the unsupervised method. Our cross-
corpus study shows how the unsupervised learning
approach performs in different domains and helps us
understand what are the factors impacting the learn-
ing methods.
3 Data
We use three data sets from different domains:
movie, news resource, and meeting conversations.
The first two are from written text domain and have
been widely used in many previous studies for sen-
timent analysis (Pang and Lee, 2004; Raaijmakers
and Kraaij, 2008). The third one is from speech
transcripts. It has been used in a few recent stud-
ies (Raaijmakers et al, 2008; Murray and Carenini,
2009), but not as much as those text data. The fol-
lowing provides more details of the data.
? The first corpus is movie data (Pang and Lee,
2004). It contains 5,000 subjective sentences
collected from movie reviews and 5,000 objec-
tive sentences collected from movie plot sum-
maries. The sentences in each collection are
randomly ordered.
? The second one is extracted from MPQA cor-
pus (version 2.0) (Wilson and Wiebe, 2003),
which is collected from news articles. This data
has been annotated with subjective information
at phrase level. We adopted the same rules as in
(Riloff and Wiebe, 2003) to create the sentence
level label: if a sentence has at least one pri-
vate state of strength medium or higher, then
the sentence is labeled SUBJECTIVE, other-
wise it is labeled OBJECTIVE. We randomly
extracted 5,000 subjective and 5,000 objective
sentences from this corpus to make it compara-
ble with the movie data.
? The third data set is from AMI meeting cor-
pus. It has been annotated using the scheme
described in (Wilson, 2008). There are 3 main
categories of annotations regarding sentiments:
subjective utterances, subjective questions, and
objective polar utterances. We consider the
union of subjective utterance and subjective
question as subjective and the rest as objective.
The subjectivity classification task is done at
the dialog act (DA) levels. We label each DA
using the label of the utterance that has over-
lap with it. We create a balanced data set us-
ing this corpus, containing 9,892 DAs in to-
tal. This number is slightly less than those for
movie and MPQA data because of the available
data size in this corpus. The data is also ran-
domly ordered without considering the role of
the speaker and which meeting it belongs to.
Table 1 summarizes statistics for the three data
sets. We can see that sentences in meeting dialogs
(AMI data) are generally shorter than the other do-
mains, and that sentences in news domain (MPQA)
are longer, and also have a larger variance. In ad-
dition, the inter-annotator agreement on AMI data
is quite low, which shows it is even difficult for hu-
man to determine whether an utterance contains sen-
timent in meeting conversations.
Movie MPQA AMI
min 3 1 3
sent length max 100 246 67
mean 20.37 22.38 8.78
variance 75.26 147.18 34.26
vocabulary size 15,847 13,414 3,337
Inter-annotator agreement N/A 0.77 0.56
Table 1: Statistics for the three data sets: movie, MPQA, and
AMI data. The inter-annotator agreement on movie data is not
available because it is not annotated by human.
4 Unsupervised Subjectivity Detection
In this section, we describe our unsupervised learn-
ing process that uses a knowledge-based method to
create an initial training set, and then uses a cali-
brated EM approach to incorporate unannotated data
into the learning process. We use a naive Bayes clas-
sifier as the base supervised classifier with a bag-of-
words model.
4.1 Create Initial Training Set
A lexicon-based method is used to create an initial
training set, since it can often achieve high precision
rate (though low recall) for subjectivity detection.
We use a subjectivity lexicon (Wilson et al, 2005)
to calculate the subjectivity score for each sentence.
162
This lexicon contains 8,221 entries that are catego-
rized into strong and weak subjective clues.
For each word w, we assign a subjectivity score
sub(w): 1 to strong subjective clues, 0.5 to weak
clues, and 0 for any other word. Then the subjec-
tivity score of a sentence is the sum of the values of
all the words in the sentence, normalized by the sen-
tence length. We noticed that for sentences labeled
as SUBJECTIVE in the three corpora, the subjective
clues appear more frequently in movie data than the
other two corpora. Thus we perform different nor-
malization for the three data sets to obtain the sub-
jectivity score for each sentence, sub(s): Equation
1 for the movie data, and Equation 2 for MPQA and
AMI data.
sub(s) =
?
w?s
sub(w)/sent length (1)
sub(s) =
?
w?s
sub(w)/log(sent length) (2)
We label the topm sentences with the highest sub-
jective scores as SUBJECTIVE, and label m sen-
tences with the lowest scores as OBJECTIVE. These
2m sentences form the initial training set for the it-
erative learning methods.
4.2 Calibrated EM Naive Bayes
Expectation-Maximization (EM) naive Bayes
method is a semi-supervised algorithm proposed in
(Nigam et al, 2000) for learning from both labeled
and unlabeled data. In the implementation of EM,
we iterate the E-step and M-step until model param-
eters converge or a predefined iteration number is
reached. In E-step, we use naive Bayes classifier to
estimate the posterior probabilities of each sentence
si belonging to each class cj (SUBJECTIVE and
OBJECTIVE), P (cj |si):
P (cj |si) =
P (cj)
?|si|
k=1 P (wk|cj)
?
cl?C
P (cl)
?|si|
k=1 P (wk|cl)
(3)
The M-step uses the probabilistic results from
the E-step to recalculate the parameters in the naive
Bayes classifier, the probability of word wt in class
cj and the prior probability of class cj :
P (wt|cj) =
0.1 +
?
si?S
N(wt, si)P (cj |si)
0.1? |V |+
?|V |
k=1
?
si?S
N(wk, si)P (cj |si)
(4)
P (cj) =
0.1 +
?
si?S
P (cj |si)
0.1? |C|+ |S|
(5)
S is the set of sentences. N(wt, si) is the count of
word wt in a sentence si. We use additive smooth-
ing with ? = 0.1 for probability parameter estima-
tion. |C| is the number of classes, which is 2 in our
case, and |V| is the vocabulary size, obtained from
the entire data set.
In the first iteration, we assign P (cj |si) using the
pseudo training data generated based on lexicon in-
formation. If a sentence is labeled SUBJECTIVE,
then P (sub|si) is 1 and P (obj|si) is 0; for the sen-
tences with OBJECTIVE labels, P (sub|si) is 0 and
P (obj|si) is 1.
In our work, we use a variant of standard EM:
calibrated EM, introduced by (Tsuruoka and Tsujii,
2003). The basic idea of this approach is to shift
the probability values of unlabeled data to the ex-
tent such that the class distribution of unlabeled data
is identical to the distribution in labeled data (bal-
anced class in our case). In our approach, before
model training (?M-step?) in each iteration, we ad-
just the posterior probability of each sentence in the
following steps:
? Transform the posterior probabilities through
the inverse function of the sigmoid function.
The outputs are real values.
? Sort them and use the median of all the values
as the border value. This is because our data is
balanced.
? Subtract this border value from the transformed
values.
? Transform the new values back into probability
values using a sigmoid function.
Note that there is a caveat here. We are assum-
ing we know the class distribution, based on labeled
training data or human knowledge. This is often a
reasonable assumption. In addition, we are assum-
ing that this class distribution is the same for the
unlabeled data. If this is not true, then the distri-
bution adjustment performed in calibrated EM may
hurt system performance.
5 Empirical Evaluation
In this section, we evaluate our unsupervised learn-
ing method and analyze various impacting factors.
163
In preprocessing, we removed the punctuation and
numbers from the data and performed word stem-
ming. To measure performance, we use classifica-
tion accuracy.
5.1 Unsupervised Learning Results
In experiments of unsupervised learning, we per-
form 5-fold cross validation. We divide the cor-
pus into 5 parts with equal size (each with balanced
class distribution). In each run we reserve one part
as the test set. From the remaining data, we use
the lexicon-based method to create the initial train-
ing data, containing 1,000 SUBJECTIVE and 1,000
OBJECTIVE sentences. The rest is used as unla-
beled data to perform iterative learning. The final
model is then applied to the reserved test set. Fig-
ure 1 shows the learning curves of calibrated EM on
movie, MPQA and AMI data respectively.
556065707580859095
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
itera
tion
accuracy(%)
556065707580859095
mov
ie
MPQ
A
AMI
Figure 1: Calibrated EM results using unsupervised setting
(2,000 self-labeled initial samples) on movie, MPQA, and AMI
data.
On movie data, calibrated EM improves the per-
formance significantly (p<0.005), compared to that
based on the initial training set (iteration 0). It takes
only a few iterations for the EM method to converge
and at the end of the iteration, it achieves 90.15%
accuracy, which rivals the fully supervised learn-
ing performance (91.31% when using all the 8,000
labeled sentences for training). On MPQA data,
this method yields some improvement (p<0.1) com-
pared to the initial point. But there is a peak accu-
racy in the first couple of iterations, and then perfor-
mance starts dropping thereafter. On AMI data, the
performance degrades after the first iteration.
5.2 Analysis and Discussion
5.2.1 Effect of initial set
For unsupervised learning, our first question is
how the accuracy and size of the initial training set
affect performance. We calculate the self-labeling
accuracy for the initial set using the lexicon based
method. Table 2 shows the labeling accuracy when
using different initial size, measured for SUBJEC-
TIVE and OBJECTIVE class separately. In addi-
tion, we present the classification performance on
the test set when using the naive Bayes classifier
trained from the initial set. Each size in the table
represents the total number of sentences in the ini-
tial set.
Table 2 shows that when the size is 2,000 (as we
used in previous experiments), the accuracy for both
classes on MPQA are even better than on movies,
even though we have seen that iterative learning
methods perform much better on movies, suggest-
ing that the initial data set accuracy is not the reason
for the worse performance on MPQA than movies.
It also shows that on movie data, as the initial size
increases, the accuracy of the pseudo training set de-
creases, which is as expected (the top ranked self-
labeled samples are more confident and accurate).
However, this is not the case on MPQA and AMI
data. There is no obvious drop of accuracy, rather in
many cases accuracy even increases when the initial
size increases. It shows that on these two corpora,
our lexicon-based method does not perform very
well because the most highly ranked sentences ac-
cording to the subjective lexicon are not those most
subjective sentences.
size 100 200 1000 2000 3000
movie
sub 95.20 92.20 82.48 79.24 77.13
obj 82.20 82.00 80.88 79.04 77.31
Acc Test 59.93 71.63 77.62 79.24 79.64
MPQA
sub 83.20 85.60 85.76 85.18 82.53
obj 87.60 86.60 87.64 87.46 85.92
Acc Test 60.45 63.83 66.98 68.75 70.05
AMI
sub 49.60 53.40 65.96 66.98 67.05
obj 71.60 71.00 68.56 69.04 69.89
Acc Test 50.51 53.81 60.53 60.39 60.46
Table 2: Initial pseudo training accuracy for SUBJECTIVE
(sub) and OBJECTIVE (obj) class, and performance on the test
using this initial training set (Acc Test). Results (all in %) are
shown for different initial data size.
From the results on the test set, we find that when
164
the size is smaller, such as containing 100 or 200
samples, the accuracy on test set is lower than using
a bigger initial set. This is mainly because there is
not sufficient data for model training. For AMI data,
this is also due to the low accuracy in the training set.
When the initial size is large enough, the improve-
ment from a larger training set is not as substantial,
for example, using 1,000, 2,000, or 3,000 sentences.
On AMI data, there is almost no difference among
the three sets. There is a tradeoff between the two
factors, self-labeling accuracy and the data size. Of-
ten an improvement in one aspect causes degrada-
tion of the other. A reasonable starting point needs
to be chosen considering both factors. Overall, it
shows that the performance on test set can benefit
more from using a larger initial training set, though
it may be noisy.
In order to further investigate the impact of self-
labeled initial data set, we perform standard semi-
supervised learning using reference labels in the
initial data set. The learning curve of this semi-
supervised setting is shown in Figure 2.
63687378838893
0
1
2
3
4
5
6
7
8
91
01
11
21
31
41
51
61
71
81
9
iteration
a c c u r a c y ( % )
63687378838893
mov
ie
MPQ
A
AMI
Figure 2: Calibrated EM results using semi-supervised learn-
ing (2,000 labeled seed) on movie, MPQA, and AMI data.
On movie data, calibrated EM yields better per-
formance over that based on the initial training data
(iteration 0). We can see that calibrated EM con-
verges very fast and achieves very high performance
in the first iteration. On MPQA and AMI data, cali-
brated EM increases the accuracy at the first iteration
but then degrades thereafter. This shows that incor-
porating unlabeled data in training is helpful, how-
ever, more EM iterations do not yield further gain.
We noticed that on AMI data, even when the ini-
tial set has 100% accuracy (i.e., semi-supervised set-
ting), it still fails to yield any performance gain on
AMI data. It shows that the low accuracy of initial
training set does not explain the poor performance
of unsupervised learning method. Therefore, we
conducted another set of experiments which use the
same semi-supervised setting but start from different
initial training sizes. We observed that on MPQA
and AMI data, calibrated EM is able to increase the
accuracy only when the initial training set is small
(less than 100 instances) and the performance at the
start point is poor. We believe this is related to the
data property and the assumptions used in EM. Sim-
ilar patterns have been found in some previous stud-
ies (Chapelle et al, 2006). They attribute this to the
incorrect model assumption, i.e., when the modeling
assumptions for a particular classifier do not match
the characteristics of the distribution of the data, un-
labeled data may degrade the performance of classi-
fiers.
5.2.2 Effect of calibration
Figure 3 compares calibrated EM with standard
EM using unsupervised learning on the three do-
mains. We can see that calibrated EM outperforms
standard EM, with a larger improvement on MPQA
and AMI data. When using standard EM, we find
that there is a larger difference between the number
of instances in the two classes based on the model?s
prediction on MPQA and AMI data than movie data.
For example, in one run using EM, in the first iter-
ation the ratio of the two classes is 2.21, 1.88, and
1.23 for MPQA, AMI, and movie data respectively.
Calibrated EM is more effective on the two domains
because it adjusts the posterior probability of each
sample according to the class distribution in the data,
making it more accurate in training the model in the
next iteration.
5.2.3 Error analysis
There are two points worth discussing based on
our error analysis.
A. Domain difference.
Much of the difference we have observed can be
attributed to the genre difference. In movie reviews,
often a person expresses his/her favor (or not) of the
movie explicitly, making the task relatively easy for
automatic subjectivity classification. MPQA data
is collected from news resource, where subjectiv-
ity mostly means an attitude or a judgment. Take
165
  
7880828486889092
7880828486889092
mov
ie_EM
mov
ie_cali_EM
6869707172
a c c u r a c y ( % )
6869707172
MPQ
A_EM
MPQ
A_cali_EM
575859606162
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
itera
tion
56575859606162
AMI
_EM
AMI
_cali_EM
Figure 3: Comparison of standard EM and calibrated EM.
the following sentence as an example: ?The United
States is prepared to fight terrorism alone?. It is la-
beled as SUBJECTIVE because it expresses a deter-
mination. However, it may also be interpreted as an
objective statement.
The AMI corpus consists of meeting conversa-
tions. The free-style dialogues are very different
from the style in review and news articles. There are
many incomplete sentences and disfluencies. More
importantly, the meaning of a sentence is often con-
text dependent. In the examples shown below, the
two sentences look very similar, however, the first
sentence is labeled as ?OBJECTIVE?, and the sec-
ond one as ?SUBJECTIVE?. This is because of the
different context and speaker information ? the sec-
ond sentence expresses agreement, but the first ex-
ample is just a sequence of discourse marker words.
? Alright yeah okay
? Yeah okay, true, true.
We notice that many of the classification errors in
AMI occur in very short sentences, like in the ex-
ample shown above. These short sentences are very
ambiguous for subjectivity classification.
B. Limitation of the bag-of-word model.
Our analysis also showed that some sentences are
difficult to classify if simply using surface words. In
the following, we show some examples of system
errors.
False negatives: subjective sentences recognized as
objective
? Johnson has, in his first film, set himself a task he is
not nearly up to. (movie data)
? The news from Israel is almost earth-shattering.
(MPQA)
? We can stick with what we already get. (AMI)
False positives: objective sentences recognized as
subjective
? Cathy (Julianne Moore) is the perfect 50s house-
wife, living the perfect 50s life: healthy kids, suc-
cessful husband, social prominence. (movie data)
? The committee Wednesday opened a formal de-
bate on human rights questions, including alterna-
tive approaches for improving the effective enjoy-
ment of human rights and fundamental freedoms.
(MPQA)
? um uh you know apple been really successful with
this surgical white kind of business or this sleek
kind of (AMI)
In the first three examples, there are no explicit
subjective clues, resulting in false negative errors.
The subjective word ?earth-shattering? is not in-
cluded in subjective lexicon and rarely used in the
corpus. The last three examples contain several sub-
jective words, and are therefore labeled as subjec-
tive. These are the problems with the current word
based approaches.
6 Conclusion and Future Work
This paper investigates an unsupervised learning
procedure for subjectivity identification at sentence
level. We use a lexicon-based method to create ini-
tial training data and then apply a calibrated EM to
utilize unlabeled corpus. We evaluate this method
across three different data sets and observe signif-
icant difference. It yields good performance on
movie data but does not achieve much performance
gain on MPQA corpus, while on AMI corpus it fails
to yield improvement. Our analysis showed that per-
formance of the base classifier has a substantial im-
pact on iterative learning methods. In addition, we
found that calibrated EM outperforms the standard
EM method when the class distribution based on
classifier?s hypotheses does not match the real one.
Our iterative learning approach uses a naive
Bayes classifier that may not have accurate posterior
probabilities. Therefore in our future work, we will
evaluate using other base models. Our cross-corpus
analysis shows poor performance of subjectivity de-
tection in AMI data. We plan to explore more in-
formation from multiparty dialogs to help improve
performance for that domain.
166
7 Acknowledgment
The authors thank Theresa Wilson for sharing annotation
for the AMI corpus and helping with data processing for
that data. Part of this work is supported by an NSF award
CNS-1059226.
References
O. Chapelle, B. Scho?lkopf, and A. Zien, editors. 2006.
Semi-supervised learning. MIT Press.
Sajib Dasgupta and Vincent Ng. 2009. Mine the easy,
classify the hard: a semi-supervised approach to auto-
matic sentiment classification. In Proceedings of ACL-
IJCNLP, pages 701?709.
Soo-Min Kim and Eduard Hovy. 2005. Automatic de-
tection of opinion bearing words and sentences. In
Proceedings of ACL.
Shoushan Li, Chu-Ren Huang, Guodong Zhou, and
Sophia Yat Mei Lee. 2010. Employing per-
sonal/impersonal views in supervised and semi-
supervised sentiment classification. In Proceedings of
ACL, pages 414?423.
Prem Melville, Wojciech Gryc, and Richard D.
Lawrence. 2009. Sentiment analysis of blogs by com-
bining lexical knowledge with text classification. In
Proceedings of ACM SIGKDD, pages 1275?1284.
Gabriel Murray and Giuseppe Carenini. 2008. Summa-
rizing spoken and written conversations. In Proceed-
ings of EMNLP, pages 773?782.
Gabriel Murray and Giuseppe Carenini. 2009. Detecting
subjectivity in multiparty speech. In Proceedings of
Interspeech.
Vincent Ng, Sajib Dasgupta, and S. M. Niaz Arifin. 2006.
Examining the role of linguistic knowledge sources in
the automatic identification and classification of re-
views. In Proceedings of COLING/ACL, pages 611?
618.
Kamal Nigam, Andrew Kachites McCallum, Sebastian
Thrun, and Tom Mitchell. 2000. Text classification
from labeled and unlabeled documents using EM. Ma-
chine Learning, 39:103?134.
Bo Pang and Lilian Lee. 2004. A sentimental educa-
tion: sentiment analysis using subjectivity summariza-
tion based on minimum cuts. In Proceedings of ACL,
pages 271?278.
Bo Pang and Lillian Lee. 2008. Using very simple statis-
tics for review search: An exploration. In Proceedings
of COLING, pages 73?76.
Stephan Raaijmakers and Wessel Kraaij. 2008. A Shal-
low approach to subjectivity classification. In Pro-
ceedings of ICWSM.
Stephan Raaijmakers, Khiet Truong, and Theresa Wilson.
2008. Multimodal subjectivity analysis of multiparty
conversation. In Proceedings of EMNLP, pages 466?
474.
Ellen Riloff and Janyce Wiebe. 2003. Learning extrac-
tion patterns for subjective expressions. In Proceed-
ings of EMNLP, pages 105?112.
Yoshimasa Tsuruoka and Jun?ichi Tsujii. 2003. Train-
ing a naive bayes classifier via the EM algorithm
with a class distribution constraint. In Proceedings
of NAACL, pages 127?134.
Janyce Wiebe and Ellen Riloff. 2005. Creating subjec-
tive and objective sentence classifiers from unanno-
tated texts. In Proceedings of CICLing, pages 486?
497.
Theresa Wilson and Janyce Wiebe. 2003. Annotating
opinions in the world press. In Proceedings of SIG-
dial, pages 13?22.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In Proceedings of HLT-EMNLP,
pages 347?354.
Theresa Wilson. 2008. Annotating subjective content in
meetings. In Proceedings of LREC.
167
Proceedings of the 15th Conference on Computational Natural Language Learning: Shared Task, pages 76?80,
Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational Linguistics
ETS: An Error Tolerable System for Coreference Resolution
Hao Xiong , Linfeng Song , Fandong Meng , Yang Liu , Qun Liu and Yajuan Lu?
Key Lab. of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences
P.O. Box 2704, Beijing 100190, China
{xionghao,songlinfeng,mengfandong,yliu,liuqun,lvyajuan}@ict.ac.cn
Abstract
This paper presents our error tolerable sys-
tem for coreference resolution in CoNLL-
2011(Pradhan et al, 2011) shared task (closed
track). Different from most previous reported
work, we detect mention candidates based on
packed forest instead of single parse tree, and
we use beam search algorithm based on the
Bell Tree to create entities. Experimental re-
sults show that our methods achieve promising
results on the development set.
1 Introduction
Over last decades, there has been increasing inter-
est on coreference resolution within NLP commu-
nity. The task of coreference resolution is to iden-
tify expressions in a text that refer to the same dis-
course entity. This year, CoNLL1 holds a shared
task aiming to model unrestricted coreference in
OntoNotes.2 The OntoNotes project has created a
large-scale, accurate corpus for general anaphoric
coreference that covers entities and events not lim-
ited to noun phrases or a limited set of entity types.
And Pradhan et al (2007) have ever used this corpus
for similar unrestricted coreference task.
Our approach to this year?s task could be divided
into two steps: mention identification and creation
of entities. The first stage is conducted on the anal-
ysis of parse trees produced by input data. The of-
ficial data have provided gold and automatic parse
trees for each sentences in training and development
1http://conll.bbn.com/
2http://www.bbn.com/ontonotes/
set. However, according to statistics, almost 3%
mentions have no corresponding constituents in au-
tomatic parse trees. Since only automatic parse trees
will be provided in the final test set, the effect of
parsing errors are inevitable. To alleviate this issue,
based on given automatic parse trees, we modify a
state-of-the-art parser (Charniak and Johnson, 2005)
to generate packed forest, and determine mention
candidates among all constituents from both given
parse tree and packed forest. The packed forest is a
compact representation of all parse trees for a given
sentence. Readers can refer to (Mi et al, 2008) for
detailed definitions.
Once the mentions are identified, the left step is
to group mentions referring to same object into sim-
ilar entity. This problem can be viewed as binary
classification problem of determining whether each
mention pairs corefer. We use a Maximum Entropy
classifier to predict the possibility that two mentions
refer to the similar entity. And mainly following the
work of Luo et al (2004), we use a beam search
algorithm based on Bell Tree to obtain the global
optimal classification.
As this is the first time we participate competi-
tion of coreference resolution, we mainly concen-
trate on developing fault tolerant capability of our
system while omitting feature engineering and other
helpful technologies.
2 Mention Detection
The first step of the coreference resolution tries to
recognize occurrences of mentions in documents.
Note that we recognize mention boundaries only on
development and test set while generating training
76
Figure 1: Left side is parse tree extracted from develop-
ment set, and right side is a forest. ?my daughter? is a
mention in this discourse, however it has no correspond-
ing constituent in parse tree, but it has a corresponding
constituent NP0 in forest.
instances using gold boundaries provided by official
data.
The first stage of our system consists of following
three successive steps:
? Extracting constituents annotated with NP,
NNP, PRP, PRP$ and VBD POS tags from sin-
gle parse tree.
? Extracting constituents with the same tags as
the last step from packed forest.
? Extracting Named Entity recognized by given
data.
It is worth mentioning that above three steps will
produce duplicated mentions, we hence collect all
mentions into a list and discard duplicated candi-
dates. The contribution of using packed forest is that
it extends the searching space of mention candidates.
Figure 1 presents an example to explain the advan-
tage of employing packed forest to enhance the men-
tion detection process. The left side of Figure 1 is
the automatic parse tree extracted from development
set, in which mention ?my daughter? has no corre-
sponding constituent in its parse tree. Under nor-
mal strategy, such mention will not be recognized
and be absent in the clustering stage. However, we
find that mention has its constituent NP0 in packed
forest. According to statistics, when using packed
forest, only 0.5% mentions could not be recognized
while the traditional method is 3%, that means the
theoretical upper bound of our system reaches 99%
compared to baseline?s 97%.
Since the requirement of this year?s task is
to model unrestricted coreference, intuitively, we
should not constraint in recognizing only noun
phrases but also adjective phrase, verb and so on.
However, we find that most mentions appeared in
corpus are noun phrases, and our experimental re-
sults indicate that considering constituents annotated
with above proposed POS tags achieve the best per-
formance.
3 Determining Coreference
This stage is to determine which mentions belong to
the same entity. We train a Maximum Entropy clas-
sifier (Le, 2004) to decide whether two mentions are
coreferent. We use the method proposed by Soon, et
al.?s to generate the training instances, where a posi-
tive instance is formed between current mention Mj
and its closest preceding antecedent Mi, and a neg-
ative instance is created by paring Mj with each of
the intervening mentions, Mi+1, Mi+2,...,Mj?1.
We use the following features to train our classi-
fier.
Features in Soon et al?s work (Soon et al, 2001)
Lexical features
IS PREFIX: whether the string of one mention is
prefix of the other;
IS SUFFIX: whether the string of one mention is
suffix of the other;
ACRONYM: whether one mention is the acronym
of the other;
Distance features
SENT DIST: distance between the sentences con-
taining the two mentions;
MEN DIST: number of mentions between two
mentions;
Grammatical features
IJ PRONOUN: whether both mentions are pro-
noun;
I NESTED: whether mention i is nested in an-
other mention;
J NESTED: whether mention j is nested in an-
other mention;
Syntax features
HEAD: whether the heads of two mentions have
the same string;
HEAD POS: whether the heads of two mentions
have the same POS;
HEA POS PAIRS: pairs of POS of the two men-
tions? heads;
77
Semantic features
WNDIST: distance between two mentions in
WordNet;
I ARG0: whether mention i has the semantic role
of Arg0;
J ARG0: whether mention j has the semantic role
of Arg0;
IJ ARGS: whether two mentions have the seman-
tic roles for similar predicate;
In the submitted results, we use the L-BFGS pa-
rameter estimation algorithm with gaussian prior
smoothing (Chen and Rosenfeld, 1999). We set the
gaussian prior to 2 and train the model in 100 itera-
tions.
3.1 Creation of Entities
This stage aims to create the mentions detected in
the first stage into entities, according to the predic-
tion of classifier. One simple method is to use a
greedy algorithm, by comparing each mention to its
previous mentions and refer to the one that has the
highest probability. In principle, this algorithm is
too greedy and sometimes results in unreasonable
partition (Ng, 2010). To address this problem, we
follow the literature (Luo et al, 2004) and propose
to use beam search to find global optimal partition.
Intuitively, creation of entities can be casted as
partition problem. And the number of partitions
equals the Bell Number (Bell, 1934), which has a
?closed? formula B(n) = 1e
??
k=0
kn
k! . Clearly, this
number is very huge when n is large, enumeration of
all partitions is impossible, so we instead designing
a beam search algorithm to find the best partition.
Formally, the task is to optimize the following ob-
jective,
y? = argmax
??P
?
e??
Prob(e) (1)
where P is all partitions, Prob(e) is the cost of
entity e. And we can use the following formula to
calculate the Prob(e),
Prob(e) =
?
i?e,j?e
pos(mi,mj)
+
?
i?e,j /?e
neg(mi,mj)
(2)
where pos(mi,mj) is the score predicted by clas-
sifier that the possibility two mentions mi and mj
group into one entity, and neg(mi,mj) is the score
that two mentions are not coreferent.
Theoretically, we can design a dynamic algorithm
to obtain the best partition schema. Providing there
are four mentions from A to D, and we have ob-
tained the partitions of A, B and C. To incorporate
D, we should consider assigning D to each entity of
every partition, and generate the partitions of four
mentions. For detailed explanation, the partitions
of three mentions are [A][B][C], [AB][C], [A][BC]
and [ABC], when considering the forth mention D,
we generate the following partitions:
? [A][B][C][D], [AD][B][C], [A][BD][C],
[A][B][CD]
? [AB][C][D], [ABD][C],[AB][CD]
? [A][BC][D], [AD][BC], [A][BCD]
? [ABC][D], [ABCD]
The score of partition [AD][B][C] can be
calculated by score([A][B][C]) + pos(A,D) +
neg(B,D) + neg(C,D). Since we can computer
pos and neg score between any two mentions in
advance, this problem can be efficiently solved by
dynamic algorithm. However, in practice, enumer-
ating the whole partitions is intractable, we instead
exploiting a beam with size k to store the top k parti-
tions of current mention size, according to the score
the partition obtain. Due to the scope limitation, we
omit the detailed algorithm, readers can refer to Luo
et al (2004) for detailed description, since our ap-
proach is almost similar to theirs.
4 Experiments
4.1 Data Preparation
The shared task provided data includes information
of lemma, POS, parse tree, word sense, predicate
arguments, named entity and so on. In addition to
those information, we use a modified in house parser
to generate packed forest for each sentence in devel-
opment set, and prune the packed forest with thresh-
old p=3 (Huang, 2008). Since the OntoNotes in-
volves multiple genre data, we merge all files and
78
Mention MUC BCUBED CEAFM CEAFE BLANC
baseline 58.97% 44.17% 63.24% 45.08% 37.13% 62.44%
baseline gold 59.18% 44.48% 63.46% 45.37% 37.47% 62.36%
sys forest 59.07% 44.4% 63.39% 45.29% 37.41% 62.41%
sys btree 59.44% 44.66% 63.77% 45.62% 37.82% 62.47%
sys forest btree 59.71% 44.97% 63.95% 45.91% 37.96% 62.52%
Table 1: Experimental results on development set (F score).
Mention MUC BCUBED CEAFM CEAFE BLANC
sys1 54.5% 39.15% 63.91% 45.32% 37.16% 63.18%
sys2 53.06% 35.55% 59.68% 38.24% 32.03% 50.13%
Table 2: Experimental results on development set with different training division (F score).
take it as our training corpus. We use the sup-
plied score toolkit 3 to compute MUC, BCUBED,
CEAFM, CEAFE and BLANC metrics.
4.2 Experimental Results
We first implement a baseline system (baseline)
that use single parse tree for mention detection
and greedy algorithm for creation of entities. We
also run the baseline system using gold parse tree,
namely baseline gold. To investigate the contribu-
tion of packed forest, we design a reinforced sys-
tem, namely sys forest. And another system, named
as sys btree, is used to see the contribution of beam
search with beam size k=10. Lastly, we combine
two technologies and obtain system sys forest btree.
Table 1 shows the experimental results on devel-
opment data. We find that the system using beam
search achieve promising improvement over base-
line. The reason for that has been discussed in last
section. We also find that compared to baseline,
sys forest and baseline gold both achieve improve-
ment in term of some metrics. And we are glad to
find that using forest, the performance of our sys-
tem is approaching the system based on gold parse
tree. But even using the gold parse tree, the im-
provement is slight. 4 One reason is that we used
some lexical and grammar features which are dom-
3http://conll.bbn.com/download/scorer.v4.tar.gz
4Since under task requirement, singleton mentions are fil-
tered out, it is hard to recognize the contribution of packed for-
est to mention detection, while we may incorrectly resolve some
mentions into singletons that affects the score of mention detec-
tion.
inant during prediction, and another explanation is
that packed forest enlarges the size of mentions but
brings difficulty to resolve them.
To investigate the effect of different genres to de-
velop set, we also perform following compared ex-
periments:
? sys1: all training corpus + WSJ development
corpus
? sys2: WSJ training corpus + WSJ development
corpus
Table 2 indicates that knowledge from other genres
can help coreference resolution. Perhaps the reason
is the same as last experiments, where syntax diver-
sity affects the task not very seriously.
5 Conclusion
In this paper, we describe our system for CoNLL-
2011 shared task. We propose to use packed for-
est and beam search to improve the performance of
coreference resolution. Multiple experiments prove
that such improvements do help the task.
6 Acknowledgement
The authors were supported by National Natural
Science Foundation of China, Contracts 90920004.
We would like to thank the anonymous reviewers
for suggestions, and SHUGUANG COMPUTING
PLATFORM for supporting experimental platform.
79
References
E.T. Bell. 1934. Exponential numbers. The American
Mathematical Monthly, 41(7):411?419.
E. Charniak and M. Johnson. 2005. Coarse-to-fine n-
best parsing and maxent discriminative reranking. In
Proceedings of the 43rd Annual Meeting on Associ-
ation for Computational Linguistics, pages 173?180.
Association for Computational Linguistics.
Stanley F. Chen and Ronald Rosenfeld. 1999. A gaussian
prior for smoothing maximum entropy models. Tech-
nical report, CMU-CS-99-108.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of
ACL-08: HLT, pages 586?594, Columbus, Ohio, June.
Z. Le. 2004. Maximum entropy modeling toolkit for
Python and C++.
X. Luo, A. Ittycheriah, H. Jing, N. Kambhatla, and
S. Roukos. 2004. A mention-synchronous corefer-
ence resolution algorithm based on the bell tree. In
Proceedings of the 42nd Annual Meeting on Associa-
tion for Computational Linguistics, pages 135?es. As-
sociation for Computational Linguistics.
H. Mi, L. Huang, and Q. Liu. 2008. Forestbased transla-
tion. In Proceedings of ACL-08: HLT, pages 192?199.
Citeseer.
Vincent Ng. 2010. Supervised noun phrase coreference
research: The first fifteen years. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 1396?1411, Uppsala, Swe-
den, July. Association for Computational Linguistics.
Sameer Pradhan, Lance Ramshaw, Ralph Weischedel,
Jessica MacBride, and Linnea Micciulla. 2007. Unre-
stricted Coreference: Identifying Entities and Events
in OntoNotes. In in Proceedings of the IEEE Inter-
national Conference on Semantic Computing (ICSC),
September 17-19.
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, and Nianwen Xue.
2011. Conll-2011 shared task: Modeling unrestricted
coreference in ontonotes. In Proceedings of the Fif-
teenth Conference on Computational Natural Lan-
guage Learning (CoNLL 2011), Portland, Oregon,
June.
W.M. Soon, H.T. Ng, and D.C.Y. Lim. 2001. A ma-
chine learning approach to coreference resolution of
noun phrases. Computational Linguistics, 27(4):521?
544.
80
Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 152?156,
Atlanta, Georgia, June 13 2013. c?2013 Association for Computational Linguistics
Simple Yet Powerful Native Language Identification on TOEFL11 
 
 
Ching-Yi Wu Po-Hsiang Lai Yang Liu     Vincent Ng 
University of Texas at Dallas Emerging Technology Lab Samsung R&D - Dallas University of Texas at Dallas 
800 W Campbell Rd 1301 Lookout Drive 800 W Campbell Rd 
Richardson, TX, USA Plano, TX, USA Richardson, TX, USA 
cxw120631@utdallas.edu s.lai@samsung.com yangl@hlt.utdallas.edu 
vince@hlt.utdallas.edu 
 
 
 
 
 
 
Abstract 
Native language identification (NLI) is the 
task to determine the native language of the 
author based on an essay written in a second 
language.  NLI is often treated as a classifica-
tion problem.  In this paper, we use the 
TOEFL11 data set which consists of more 
data, in terms of the amount of essays and 
languages, and less biased across prompts, i.e., 
topics, of essays.  We demonstrate that even 
using word level n-grams as features, and sup-
port vector machine (SVM) as a classifier can 
yield nearly 80% accuracy. We observe that 
the accuracy of a binary-based word level n-
gram representation (~80%) is much better 
than the performance of a frequency-based 
word level n-gram representation (~20%).  
Notably, comparable results can be achieved 
without removing punctuation marks, suggest-
ing a very simple baseline system for NLI. 
1 Introduction 
Native language identification (NLI) is an emerg-
ing field in the natural language processing com-
munity and machine learning community (Koppel 
et al, 2005; Blanchard et al, 2013). It is a task to 
identify the native language (L1) of an author 
based on his/her texts written in a second language.  
The application of NLI can bring many benefits, 
such as providing a learner adaptive feedback of 
their writing errors based on the native language 
for educational purposes (Koppel et al, 2005; 
Blanchard et al, 2013).  
NLI can be viewed as a classification problem.  
In a classification problem, a classifier is first 
trained using a set of training examples.  Each 
training example is represented as a set of features, 
along with a class label.  After a classifier is 
trained, the classifier is evaluated using a testing 
set (Murphy, 2012). Good data representation often 
yields a better classification performance (Murphy, 
2012).  Often time, the simpler representations 
might produce better performance.  In this work, 
we demonstrate that a binary-based word level n-
gram representation yields much better perform-
ance than a frequency-based word level n-gram 
representation.  In addition, we observed that re-
moving punctuation marks in an essay does not 
make too much difference in a classification per-
formance. 
The contributions of this paper are to demon-
strate the usefulness of a binary-based word level 
n-gram representation, and a very simple baseline 
system without the need of removing punctuation 
marks and stop words. 
This paper is organized as the following.  In 
Section 2, we present related literatures.  
TOEFL11 data set is introduced in Section 3.  In 
Section 4, our features and system design are de-
scribed.  The results are presented in Section 5, 
followed by conclusion in Section 6. 
 
 
152
2 Related Work 
The work by Koppel et al (2005) is the first study 
to investigate native language identification.  They 
use the International Corpus of Learner English 
(ICLE).  They set up this task as a classification 
problem studied in machine learning community.  
They use three types of features: function words, 
character n-gram, errors and idiosyncrasies, e.g. 
spelling and grammatical errors.   For errors and 
idiosyncrasies, they used Microsoft Office Word to 
detect those errors.  Their features were evaluated 
on a subset of the ICLE corpus, including essays 
sampled from five native languages (Russian, 
Czech, Bulgarian, French and Spanish) with 10-
fold cross validation.  They achieve an accuracy of 
80.2% by combining all of the features and using a 
support vector machine as the classification algo-
rithm. In addition, Tsur and Rappoport (2007) 
show that using character n-gram only on the ICLE 
can yield an accuracy of 66%.   
The work from Kochmar (2011) identifies an 
author?s native language using error analysis.  She 
suggests that writers with different native lan-
guages generate different grammatical error pat-
terns. Instead of using ICLE, this work uses a 
different corpus, English learner essays from the 
Cambridge Learner Corpus. She uses SVM on 
manually annotated spelling and grammatical er-
rors along with lexical features. 
Most of the systems described in NLI literature 
reach good performance in predicting an author?s 
native language, using character n-gram and part of 
speech n-gram as features (Blanchard et al, 2013).  
In recent years, various studies have started to look 
into complex features in order to improve the per-
formance.  Wong and Dras (2009) use contrastive 
analysis, a systematic analysis of structural simi-
larities and differences in a pair of languages.  A 
writer?s native language influences the target lan-
guage they aim to learn. They explore the impact 
of three English as Second Language (ESL) error 
types, subject-verb disagreement, noun-number 
disagreement and determiner errors, and use a sub-
set of ICLE with 7 languages.   However, although 
the determiner error feature seems useful, when it 
is combined with a baseline model of lexical fea-
tures, the classification performance is not signifi-
cantly improved (Wong and Dras, 2009). 
Wong and Dras (2011) use complex features 
such as production rules from two parsers and 
reranking features into the classification frame-
work, incorporating lexical features of Koppel et al 
(2005).  They achieve a classification performance 
of 81.71% on the 7-native-languages NLI, slightly 
better than 80.2% accuracy of the original Koppel 
et al (2005). 
Note that although the International Corpus of 
Learner English (ICLE) is used in most of the NLI 
studies, ICLE has been known to have fewer es-
says, and a skewed distribution toward topics of 
essays (Blanchard et al, 2013).  In addition, even 
though there are 16 native languages in ICLE, as 
each language has different numbers of essays, 
most work often uses different subsets of 7 native 
languages, which makes comparison harder across 
different studies (Blanchard et al, 2013). The NLI 
shared task 2013 provides a new data set, namely 
the TOEFL11 (Blanchard et al, 2013), which ad-
dresses these issues.  As previously discussed, 
complex features do not necessarily improve clas-
sification accuracy.  In this work, we use 
TOEFL11 to investigate the classification per-
formance using simple word n-gram based features.  
3 Data  
In this work, we use TOEFL11 as our corpus.  
TOEFL11 is a new data set for NLI (Blanchard et 
al., 2013). There are 11 native languages, including 
Arabic (ARA), Chinese (CHI), French (French), 
German (GER), Hindi (HIN), Italian (ITA), Japa-
nese (JPN), Korean (KOR), Spanish (SPA), Telugu 
(TEL), and Turkish (TUR).  Authors write essays 
based on 8 different topics in English.  There are 
1,100 essays for each language, and sampled from 
8 different topics, i.e., prompts.    Each essay is 
also annotated with an English proficiency level 
(low/medium/high) determined by assessment spe-
cialists.  Among 12,100 essays, there are 9,900 
essays in the training set, 1,100 essays in the de-
velopment set, i.e., validation set in machine learn-
ing, and 1,100 essays in the testing set.  In the 
training set and the development set, there are 
equal numbers of essays from each of the 11 native 
languages. By using TOEFL11, it makes our 
analysis less biased toward a specific topic of es-
says (Blanchard et al, 2013).  
 
 
 
153
4 NIL System Design 
In this section, we describe our NLI system, the 
features, and the classifier we use. 
4.1 Data Preprocessing 
Each essay is tokenized, and then capitalizations 
are removed.  Note that we did not remove English 
stop words, which might be useful to discriminate 
the native language for a writer.  For example, 
function words, which belong to stop words, such 
as ?the?, ?at?, ?which?, have been proven to be ef-
fective to distinguish native language for writers 
(Koppel et al, 2005).  There are two settings: ei-
ther punctuation marks are removed or kept.   
When punctuation marks are kept, they are viewed 
the same as word in constructing n-grams.  For 
example, in the sentence ?NLI is fun.?, ?fun .? is 
viewed as a bigram. 
4.2 Features 
In our system, word level n-grams are used to rep-
resent an essay.  Previous studies have shown that 
word level n-grams are useful in determining the 
native language of a writer (Bykh and Meurers, 
2012).  One reasonable hypothesis is that non-
native English writers with the same native lan-
guages tend to choose more similar words to ex-
press the same or similar concepts.  In addition, the 
combination of a sequence of words might also be 
affected by the different native language of writers.  
Therefore, word n-gram is useful to distinguish the 
native language of a writer.  Even though some 
previous studies have looked into using word level 
n-grams as features, how to use word level n-
grams has not been explored too much yet on 
TOEFL11 corpus.  To our knowledge, the most 
recent study by Blanchard et al (2013) started to 
research the effect of different forms of word level 
n-gram representations. 
There could be many ways to represent an essay 
by word level n-grams.  One possible representa-
tion of an essay is to use the frequency of a spe-
cific word n-gram, i.e., the number of times a 
specific word n-gram appears in an essay divided 
by the number of times all word n-grams appear in 
an essay.  In this representation, an essay is a vec-
tor whose elements are the frequency of different 
word n-grams in the essay.  Another possible rep-
resentation is to use binary representation, i.e., 1 
indicates this word n-gram is in this essay, 0 indi-
cates this word n-gram is not in this essay.  One 
interesting question to ask is:  
Which representation can be more informative 
to distinguish the native language of writers of es-
says? 
 Here we compare the performance of a fre-
quency-based word level n-gram representation 
and a binary-based word level n-gram representa-
tion. We included all word level n-grams in the 
training set, without any frequency cutoff.  For 
both binary-based and frequency-based representa-
tions, we run the experiments on the two settings:  
punctuation marks are either removed or kept. 
In addition to word level n-grams, since 
TOEFL11 also consists of English proficiency lev-
els evaluated by assessment experts, we also in-
cluded it to test whether this feature might improve 
the classification performance.  All of the features 
used in our system are summarized in Table 1.  
Besides each feature described above, we have also 
combined different features to test whether various 
combinations of features might improve the accu-
racy performance.  Here, we simply aggregated 
different features, for example, all word level uni-
grams, combined with all word level bigrams. 
4.3 Classifier 
Previous literatures have used various methods 
such as Na?ve Bayse, logistic regression and sup-
port vector machine on NLI problem.  As it has 
been shown that when representing an essay in 
order to perform a classification task, it often re-
sults in an essay being represented in a very high 
dimensional space.  Since support vector machine 
(SVM) is known to be adaptive when the feature 
dimension is high, we chose SVM as our classifi-
cation algorithm.   We also compared the results 
from Na?ve Bayse for an experimental purpose and 
found that SVM is better. We use SVM-Light for 
our system (Joachims, 1999).  We then train our 
SVM classifier on the training set (n=9900), and 
test the trained classifier on the testing set 
(n=1100). 
 
 
 
 
 
 
154
5 Results and Discussions 
5.1 Results 
Table 1 and Table 2 show the accuracies on the 
testing set for the different feature sets, when punc-
tuation marks are removed or kept respectively.  
As the results demonstrated, the accuracies of word 
level bigram are better than unigram using a bi-
nary-based representation.  When combining word 
level unigram and bigram, the accuracy is im-
proved in a binary-based representation.  This is 
consistent when punctuations are either removed or 
kept.  This observation is consistent with the exist-
ing NLI literatures: when combining word n-grams, 
it seems to improve the accuracy of the classifier, 
compared with a word n-gram alone. But we do 
not observe too much difference when punctuation 
marks are removed or kept, using both unigram 
and bigram. In fact, including punctuation marks 
lead to high accuracies in many scenarios, espe-
cially in unigram in a frequency-based representa-
tion, suggesting the usage of punctuation marks 
varies across native languages.   
 
Features 
Performance of  
Binary Word n-
gram Representa-
tion 
Performance of 
Freq. Word n-
gram Representa-
tion 
word unigram 70.91% 25.36% 
word bigram 76.00% 17.64% 
word unigram 
and  
word bigram 
79.73% 23.36% 
Table 1 Accuracy of Different Feature Sets, without 
Punctuation Marks 
 
Features 
Performance of 
Binary Word n-
gram Representa-
tion 
Performance of 
Freq. Word n-
gram Representa-
tion 
word unigram 70.18% 30.00% 
word bigram 77.09% 18.73% 
word unigram 
and  
word bigram 
79.45% 28.73% 
Table 2 Accuracy of Different Feature Sets, with 
Punctuation Marks 
 
Table 3 shows the confusion matrix of classifi-
cation performance, using unigram and bigram, in 
a binary-based representation when punctuation 
marks are removed. We observe that some of na-
tive languages, such as German, Italian, and Chi-
nese, lead to better classification accuracy than for 
Korean, Spanish, and Arabic. 
 
 ARA CHI FRE GER HIN ITA JPN KOR SPA TEL TUR Preci-sion 
Re-
call 
F-
measure 
ARA 75 1 5 3 1 3 1 1 3 4 3 78.9 75.0 76.9
CHI 3 86 0 0 1 0 5 4 0 0 1 81.9 86.0 83.9
FRE 1 1 79 7 3 4 2 0 1 0 2 77.5 79.0 78.2
GER 3 1 2 87 1 1 1 0 2 0 2 79.8 87.0 83.3
HIN 1 2 1 2 77 0 0 0 5 10 2 74.0 77.0 75.5
ITA 0 0 6 4 0 85 0 0 3 0 2 83.3 85.0 84.2
JPN 2 2 1 0 0 1 86 3 2 0 3 77.5 86.0 81.5
KOR 0 8 2 1 1 0 14 72 1 1 0 82.8 72.0 77.0
SPA 4 0 6 3 4 6 1 1 70 1 4 78.7 70.0 74.1
TEL 1 0 0 1 15 0 0 0 0 82 1 83.7 82.0 82.8
TUR 5 4 0 1 1 2 1 6 2 0 78 79.6 78.0 78.8
Average Performance: 79.7%.   Precision, Recall, F-measures are in %. 
Table 3 Confusion Matrix on Testing Set 
5.2 Binary Based of Word N-Gram Repre-
sentation 
We observe that the accuracy of a binary-based 
word level n-gram representation in our system is 
significantly better than a frequency-based repre-
sentation.  This is similar to the result reported by 
Blanchard et al, (2013) in TOEFL11 corpus.  The 
differences between their system and ours are that 
the system developed by Blanchard et al, (2013) 
used logistic regression with L1-regularzation, in-
stead of SVM and they did not remove all punctua-
tion marks and special characters.   
This might imply that a frequency-based word 
n-gram representation do not capture the character-
istics of the data. This might be because the data 
resides in a high dimension space, and the frequen-
cies of word level n-grams would be skewed.  In a 
future study, one might investigate a better repre-
sentation form and other complex features that 
have a stronger interpretative power of the data.  
5.3 Effects of Proficiency Level 
In our results, we have included English profi-
ciency level (low/medium/high) as a feature pro-
vided by assessment experts.  However, we did not 
find a strong improvement in accuracies, for ex-
ample, 79.13% using a binary-based word level n-
grams when punctuation marks removed.  We 
think this might be because only one feature will 
155
not dramatically change the accuracies.  This may 
be due to the fact word n-grams have already con-
tributed a large amount of features.  
6 Conclusion 
In this paper, we used a new data set, TOEFL11 to 
investigate NLI. In the most existing literatures, 
ICLE corpus was used. However, ICLE has fewer 
data and is known to be biased to topics of essays.  
The newly released corpus, TOEFL11 addresses 
these two drawbacks, which is useful for NLI 
community.  Support vector machine (SVM) was 
used as a classifier in our system.  We have dem-
onstrated that a binary-based word level n-gram 
representation has resulted in a significantly better 
performance compared to a frequency-based n-
gram representation.  We observed that there is not 
much difference in classification accuracies when 
punctuation removed or kept, when combining 
both unigram and bigram.  Interestingly, a fre-
quency-based word unigram with punctuation 
marks outperforms than the case without punctua-
tion marks, suggesting the potential of utilizing 
punctuation marks in NLI.  In addition, English 
proficiency level has also been included in our fea-
ture set, but did not yield a significant improve-
ment in accuracy.  As most of the essays are 
represented in a high dimension space using word 
level n-grams, we are looking into feature selection 
to reduce dimensionality and how to represent 
those features in order to improve accuracy, as 
well as other features.  
References  
Blanchard, D., Tetreault, J., Higgins, D., Cahill, A., and 
Chodorow, M. 2013. TOEFL11: A Corpus of Non-
Native English.  Educational Testing Service.  
Bykh, S. and Meurers, D. 2012. Native Language Iden-
tification using Recurring n-grams - Investigating 
Abstraction and Domain Dependence. In Proceed-
ings of COLING 2012, 425-440, Mumbai, India. The 
COLING 2012 Organizing Committee. 
Joachims, T. 1999. Making large-Scale SVM Learning 
Practical. Advances in Kernel Methods - Support 
Vector Learning, B. Sch?lkopf and C. Burges and A. 
Smola (ed.), MIT-Press.  
Kochmar, E. 2011. Identification of a writer?s native 
language by error analysis. Master?s thesis, Univer-
sity of Cambridge. 
Koppel, M., Schler, J., and Zigdon, K. 2005. Automati-
cally determining an anonymous author?s native lan-
guage. In ISI, 209?217. 
Murphy, K. P. 2012. Machine learning: a probabilistic 
perspective. MIT Press.  
Tsur, O. and Rappoport, A. 2007. Using classifier fea-
tures for studying the effect of native language on the 
choice of written second language words. In Pro-
ceedings of the Workshop on Cognitive Aspects of 
Computational Language Acquisition, 9?16, Prague, 
Czech Republic. Association for Computational Lin-
guistics. 
Wong, S.-M. J. and Dras, M. 2009. Contrastive analysis 
and native language identification. In Proceedings of 
the Australasian Language Technology Association 
Workshop 2009, 53?61, Sydney, Australia. 
Wong, S.-M. J. and Dras, M. 2011. Exploiting parse 
structures for native language identification. In Pro-
ceedings of the 2011 Conference on Empirical Meth-
ods in Natural Language Processing, 1600?1610, 
Edinburgh, Scotland, UK. Association for Computa-
tional Linguistics. 
156
Proceedings of the 2013 Workshop on Biomedical Natural Language Processing (BioNLP 2013), pages 89?97,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Exploring word class n-grams to measure
language development in children
Gabriela Ram??rez de la Rosa and Thamar Solorio
University of Alabama at Birmingham
Birmingham, AL 35294, USA
gabyrr,solorio@cis.uab.edu
Manuel Montes-y-Go?mez
INAOE
Sta. Maria Tonantzintla, Puebla, Mexico
mmontesg@ccc.inaoep.mx
Yang Liu
The University of Texas at Dallas
Richardson, TX 75080, USA
yangl@hlt.utdallas.edu
Aquiles Iglesias
Temple University
Philadelphia, PA 19140, USA
iglesias@temple.edu
Lisa Bedore and Elizabeth Pen?a
The University of Texas at Austin
Austin, TX 78712, USA
lbedore,lizp@mail.utexas.edu
Abstract
We present a set of new measures designed
to reveal latent information of language
use in children at the lexico-syntactic
level. We used these metrics to analyze
linguistic patterns in spontaneous narra-
tives from children developing typically
and children identified as having a lan-
guage impairment. We observed signif-
icant differences in the z-scores of both
populations for most of the metrics. These
findings suggest we can use these metrics
to aid in the task of language assessment
in children.
1 Introduction
The analysis of spontaneous language samples is
an important task across a variety of fields. For in-
stance, in language assessment this task can help
to extract information regarding language profi-
ciency (e.g. is the child typically developing or
language impaired). In second language acqui-
sition, language samples can help determine if
a child?s proficiency is similar to that of native
speakers.
In recent years, we have started seeing a grow-
ing interest in the exploration of NLP techniques
for the analysis of language samples in the clinical
setting. For example, Sahakian and Snyder (2012)
propose a set of linguistic measures for age pre-
diction in children that combines three traditional
measures from language assessment with a set of
five data-driven measures from language samples
of 7 children. A common theme in this emerg-
ing line of research is the study of the syntax in
those language samples. For instance, to annotate
data to be used in the study of language develop-
ment (Sagae et al, 2005), or to build models to
map utterances to their meaning, similar to what
children do during the language acquisition stage
(Kwiatkowski et al, 2012). In addition, language
samples are also used for neurological assessment,
as for example in (Roark et al, 2007; Roark et
al., 2011) where they explored features such as
Yngve and Frazier scores, together with features
derived from automated parse trees to model syn-
tactic complexity and surprisal. Similar features
are used in the classification of language samples
to discriminate between children developing typ-
ically and children suffering from autism or lan-
guage impairment (Prud?hommeaux et al, 2011).
In a similar line of research, machine learning and
features inspired by NLP have been explored for
the prediction of language status in bilingual chil-
dren (Gabani et al, 2009; Solorio et al, 2011).
More recent work has looked at the feasibility of
scoring coherence in story narratives (Hassanali et
al., 2012a) and also on the inclusion of coherence
89
as an additional feature to boost prediction accu-
racy of language status (Hassanali et al, 2012b).
The contribution of our work consists on new
metrics based on n-grams of Part of Speech (POS)
tags for assessing language development in chil-
dren that combine information at the lexical and
syntactic levels. These metrics are designed to
capture the lexical variability of specific syntac-
tic constructions and thus could help to describe
the level of language maturity in children. For in-
stance, given two lists of examples of the use of
determiner + noun: ?the dog, the frog, the tree?
and ?this dog, a frog, these trees? we want to be
able to say that the second one has more lexical
variability than the first one for that grammatical
pattern.
Our approach to compute these new metrics
does not require any special treatment on the tran-
scripts or special purpose parsers beyond a POS
tagger. On the contrary, we provide a set of mea-
sures that in addition to being easy to interpret by
practitioners, are also easy to compute.
2 Background and Motivation
To establish language proficiency, clinical re-
searchers and practitioners rely on a variety of
measures, such as number of different words,
type-token ratio, distribution of part-of-speech
tags, and mean length of sentences and words per
minute (Lu, 2012; Yoon and Bhat, 2012; Chen and
Zechner, 2011; Yang, 2011; Miller et al, 2006), to
name a few. Most of these metrics can be cate-
gorized as low-level metrics since they only con-
sider rates of different characteristics at the lexi-
cal level. These measures are helpful in the so-
lution of several problems, for example, building
automatic scoring models to evaluate non-native
speech (Chen and Zechner, 2011). They can also
be used as predictors of the rate of growth of En-
glish acquisition in specific populations, for in-
stance, in typically developing (TD) and language
impaired (LI) bilingual children (Rojas and Igle-
sias, 2012; Gutie?rrez-Clellen et al, 2012). Among
the most widely used metrics are mean length of
utterance (MLU), a measure of syntactic complex-
ity (Bedore et al, 2010), and measures of lexi-
cal productivity, such as the number of different
words (NDW) and the child?s ratio of functional
words to content words (F/C) (Sahakian and Sny-
der, 2012).
MLU, NDW, F/C and some other low-level
measures have demonstrated to be valuable in the
assessment of language ability considering that
practitioners often only need to focus on produc-
tivity, diversity of vocabulary, and sentence or-
ganization. Although useful, these metrics only
provide superficial measures of the children?s lan-
guage skills that fail to capture detailed lexico-
syntactic information. For example, in addition to
knowing that a child is able to use specific verb
forms in the right context, such as, third person
singular present tense or regular past tense, knowl-
edge about what are the most common patterns
used by a child, or how many different lexical
forms for noun + verb are present in the child?s
speech is needed because answering these ques-
tions provides more detailed information about the
status of grammatical development. To fill in this
need, we propose a set of measures that aim to cap-
ture language proficiency as a function of lexical
variability in syntactic patterns. We analyze the
information provided by our proposed metrics on
a set of spontaneous story retells and evaluate em-
pirically their potential use in language status pre-
diction.
3 Proposed measures
To present the different metrics we propose in this
study we begin with the definition of the following
concepts:
A syntactic pattern p is an n-gram of part-of-
speech tags denoted as p = ?t1 t2 ... tn?, where
ti indicates the part-of-speech tag corresponding
to the word at position i. For simplicity we use
tpi to indicate the tag at position i from pattern p.
Two examples of syntactic patterns of length two
are ?DT NN? and ?DT JJ? 1.
A lexical form f is an n-gram of words. It is de-
fined as f = ?w1 w2 ... wn?, where wi is the word
at position i. Similarly to the previous definition,
we use wfi to indicate the word at position i in a
lexical form f .
A lexical form f corresponds to a syntactic
pattern p if and only if |f | is equal to |p| and
?ktag(w
f
k ) = t
p
k, where tag() is a function that re-
turns the part-of-speech of its argument. The set of
lexical forms in a given transcript corresponding to
a syntactic pattern p is denoted by LF p. Two ex-
amples of lexical forms from the syntactic pattern
?DT NN? are ?the cat? and ?the frog?.
1We use the Penn Treebank POS tagset
90
DT the (62), a (17), all (8), no(2), that (1)
NN frog (16), boy(7), dog (6), boat (4), name (3), place (2), house (2), water (2), rabbit (2), noise (2), stick (1), tree
(1), bye(1), floor (1), um (1), baby (1), forest (1), room (1), foot (1), rock (1), squirrel (1), back (1), rabb (1),
card (1), one (1), present (1), dress (1), box (1), family (1)
VBD saw (7), dropped (4), said (4), started (4), looked (3), kicked (3), called (3), found (2), took (2), got (2), jumped
(2), heard (2), thought (1), turned (1), fell (1), waked (1), stood (1), wa (1), touched (1), told (1), scared (1), tur
(1), haded (1), opened (1), shh (1)
DT NN the frog (3), the dog (2), the place (2), the water (2), the boat (2), a noise (2), the forest (1), the rock (1), a tree
(1), a present (1), a um (1), the card (1), the box (1), the rabb (1), the floor (1), the back (1), no one (1)
DT VBD all started (2), all heard (1)
Table 1: Example of 5 syntactic patterns with their lists of lexical forms and the number of repetitions
of each of them. This information corresponds to an excerpt of an example transcript. DT is the part-of-
speech tag for determiner, NN for noun, and VBD for verb in past tense.
The bag-of-words associated to a syntactic pat-
tern p is denoted as W p. This set is composed
of all the words from the lexical forms that corre-
spond to the syntactic pattern p. It is formally de-
fined as follows: W p = {w|w ? f, f ? LF p}.
For example, the bag-of-words of the syntactic
pattern ?DT NN? with lexical forms ?the cat? and
?the frog? is {the, cat, frog}.
Table 1 shows five syntactic patterns of a tran-
script?s fragment. For each syntactic pattern in the
transcript we show the list of its lexical forms and
their frequency. We will use this example in the
description of the measures in the following sub-
sections.
3.1 Number of different lexical forms
(NDLF)
Analogous to the number of different words
(NDW), where words in the transcript are consid-
ered atomic units, we propose a metric where the
atomic units are lexical forms. Then, we measure
the number of different lexical forms used for each
syntactic pattern in the transcript. Formally, given
a syntactic pattern p and its set of lexical forms
LF p, the number of different lexical forms is com-
puted as follows:
NDLF(p) = |LF p| (1)
This measure gives information about the num-
ber of different ways the child can combine words
in order to construct a fragment of a speech that
corresponds to a specific grammatical pattern. Re-
search in language assessment has shown that
when children are in the early acquisition stages
of certain grammatical constructions they will use
the patterns as ?fixed expressions?. As children
master these constructions they are able to use
these grammatical devices in different contexts,
but also with different surface forms. Thereby, we
could use this measure to discriminate the syntac-
tic patterns the child has better command of from
those that might still be problematic and used in-
frequently or with a limited combination of sur-
face forms. For example, from the information
on Table 1 we see that NDLF(DT NN) = 17, and
NDLF(DT VBD) = 2. This seems to indicate that
the child has a better command of the grammatical
construction determiner + noun (DT NN) and can
thus produce more different lexical forms of this
pattern than determiner + verb (DT + VBD). But
also, we may use this measure to identify rare pat-
terns, that are unlikely to be found in a typically
developing population.
3.2 Lexical forms distribution (LFdist)
Following the idea of lexical forms as atomic
units, NDLF allows to know the different lexical
forms present in the transcripts. But we do not
know the distribution of use of each lexical form
for a specific syntactic pattern. In other words,
NDLF tells us the different surface forms observed
for each syntactic pattern, but it does not measure
the frequency of use of each of these lexical forms,
nor whether each of these forms are used at similar
rates. We propose to use LFdist to provide infor-
mation about the distribution of use for LF p, the
set of lexical forms observed for the syntactic pat-
tern p. We believe that uniform distributions can
be indicative of syntactic structures that the child
has mastered, while uneven distributions can re-
veal structures that the child has only memorized
(i.e. the child uses a fixed and small set of lex-
ical forms). To measure this distribution we use
the entropy of each syntactic pattern. In particu-
lar, given a syntactic pattern p and its set of lexical
forms LF p, the lexical form distribution is com-
puted as follows:
91
LFdist(p) = ?
?
fi?LF p
prob(fi) log prob(fi)
(2)
where
prob(fi) =
count(fi)
?
fk?LF p count(fk)
(3)
and count() is a function that returns the fre-
quency of its argument. Larger values of LFdist
indicate a greater difficulty in the prediction of
the lexical form that is being used under a spe-
cific grammatical pattern. For instance, in the ex-
ample of Table 1, LFdist(DT VBD) = 0.91 and
LFdist(DT NN) = 3.97. This indicates that the
distribution in the use of lexical forms for deter-
miner + noun is more uniform than the use of
lexical forms for determiner + verb, which im-
plies that for determiner + verb there are some
lexical forms that are more frequently used than
others2. Syntactic patterns with small values of
LFdist could flag grammatical constructions the
child does not feel comfortable manipulating and
thus might still be in the acquisition stage of lan-
guage learning.
3.3 Lexical variation (LEX)
Until now we are considering lexical forms as
atomic units. This could lead to overestimating
the real lexical richness in the sample, in particu-
lar for syntactic patterns of length greater than 1.
To illustrate this consider the syntactic pattern p =
?DT NN? and suppose we have the following set
of lexical forms for p = {?the frog?, ?a frog?, ?a
dog?, ?the dog?}. The value for NDLF (p) = 4.
But how many of these eight words are in fact dif-
ferent? That is the type of distinction we want to
make with the next proposed measure: LEX, that
is also an adaptation of type-token ratio (Lu, 2012)
used in the area of communication disorders but
computed over each grammatical pattern. For this
example, we want to be able to find that the lex-
ical variation of ?DT NN? is 0.5 (because there
are only four different words out of eight). For-
mally, given a syntactic pattern p, its set of lexical
forms LF p, and the bag-of-words W p, the lexical
variation is defined as shown in Equation 4.
2We recognize that this is an oversimplification of the en-
tropy measure since the number of outcomes will most likely
be different for each syntactic pattern.
LEX(p) =
|W p|
|LF p| ? n
(4)
Note that |LF p| = NDLF(p), and n is the
length of the syntactic pattern p. In Table 1 the lex-
ical variation of the pattern ?determiner + noun?
(DT+NN) is equal to 0.58 ( 2017?2 ), and for deter-
miner + verb (DT+VBD) is equal to 0.75 ( 32?2 ).
That means 58% of total words used under the pat-
tern ?DT+NN? are different, in comparison with
the 75% for ?DT+VBD?. In general, the closer the
value of LEX is to 1, there is less overlap between
the words in the lexical forms for that pattern.
Our hypothesis behind this measure is that for the
same syntactic pattern TD children may have less
overlap of words than children with LI, e.g. less
overlap indicates the use of a more diverse set of
words.
3.4 Lexical use of syntactic knowledge
(LexSyn)
With LEX we hope to accomplish the character-
ization of lexical richness of syntactic patterns
assuming that each part-of-speech has a similar
number of possible lexical forms. We assume as
well that less overlap in the words used for the
same grammatical pattern represents a more devel-
oped language than that with more overlap. How-
ever the definition of LEX overlooks a well known
fact about language: different word classes have
a different range of possibilities as their lexical
forms. Consider open class items, such as nouns
and verbs, where the lexicon is large and keeps
growing. In contrast, closed class items, such as
prepositions and determiners are fixed and have a
very small number of lexical forms. Therefore it
seems unfair to assign equal weight to the overlap
of words for these different classes. To account
for this phenomenon, we propose a new measure
that includes the information about the syntactic
knowledge that the child shows for each part of
speech. That is, we weigh the level of overlap
for specific grammatical constructions according
to the lexicon for the specific word classes in-
volved. Since we limit our analysis to the language
sample at hand, we define the ceiling of the lexi-
cal richness of a specific word class to be the to-
tal number of different surface forms found in the
transcript. In particular, given a syntactic pattern
p = ?t1 t2 ... tn?, with its set of lexical forms
LF p, the lexical use of syntactic knowledge is de-
fined as:
92
LexSyn(p) =
1
n
n?
i=1
|wfi |f ? LF
p|
NDLF(tpi )
(5)
where the numerator is the size of the set of
words in the i-th position in all the lexical forms.
Note that this measure does not make sense for
syntactic patterns of length < 2. Instead, syn-
tactic patterns of length 1 were used to identify
the syntactic knowledge of the child by using the
NDLF of each POS in p. In the example of Ta-
ble 1, LexSyn(DT NN) = 0.59. This value corre-
sponds to the sum of the number of different de-
terminers used in position 1 for LF p divided by
the total number of different determiners that this
child produced in the sample (for this case, the
number of determiners that this child produced is
given by NDLF(DT), that is 5), plus the number
of different nouns used under this syntactic pat-
tern over the total number of nouns produced by
the child (NDLF(NN)=29). The complete calcula-
tion of LexSyn(DT NN) = 12 ?(
3
5+
17
29) = 0.59.
This contrasts with the value of LexSyn for the pat-
tern ?determiner + verb?, LexSyn(DT VBD) =
1
2 ? (
1
5 +
2
25) = 0.14 that seems to indicate that the
child has more experience combining determiners
and nouns than determiners and verbs. Perhaps
this child has had limited exposure to other pat-
terns combining determiner and verb, or this pat-
tern is at a less mature stage in the linguistic reper-
toire of the child.
Children with LI tend to exhibit a less devel-
oped command of syntax than their TD cohorts.
Syntactic patterns with large values of LexSyn
show a high versatility in the use of those syntactic
patterns. However, since the syntactic reference is
taken from the same child, this versatility is rela-
tive only to what is observed in that single tran-
script. For instance, suppose that the total num-
ber of different determiners observed in the child?s
transcript is 1. Then any time the child uses that
determiner in a syntactic pattern, the knowledge of
this class, according to our metric, will be 100%,
which is correct, but this might not be enough to
determine if the syntactic knowledge of the child
for this grammatical class corresponds to age ex-
pectations for a typically developing child. In or-
der to improve the measurement of the lexical use
of syntactic knowledge we propose the measure
LexSynEx, that instead of using the information
of the same child to define the coverage of use for
a specific word class, it uses the information ob-
served for a held out set of transcripts from TD
children. This variation allows the option of mov-
ing the point of reference to a specific cohort, ac-
cording to what is needed.
4 Data set
The data used in this research is part of an ongoing
study of language impairment in Spanish-English
speaking children (Pen?a et al, 2003). From this
study we used a set of 175 children with a mean
age of about 70 months. Language status of these
children was determined via expert judgment by
three bilingual certified speech-language pathol-
ogists. At the end of the data collection period,
the experts reviewed child records in both lan-
guages including language samples, tests proto-
cols, and parent and teacher questionnaire data.
They made independent judgments about chil-
dren?s lexical, morphosyntactic, and narrative per-
formance in each language. Finally, they made an
overall judgment about children?s language abil-
ity using a 6 point scale (severely language im-
paired to above normal impairment). If at least two
examiners rated children?s language ability with
mild, moderate or severe impairment they were as-
signed to the LI group. Percent agreement among
the three examiners was 90%. As a result of this
process, 20 children were identified by the clinical
researchers as having LI, while the remaining 155
were identified as typically developing (TD).
The transcripts were gathered following stan-
dard procedures for collection of spontaneous lan-
guage samples in the field of communication dis-
orders. Using a wordless picture book, the chil-
dren were asked to narrate the story. The two
books used were ?A boy, a dog, and a frog? (Mayer,
1967) and ?Frog, where are you?? (Mayer, 1969).
For each child in the sample, 4 transcripts of story
narratives were collected, 2 in each language. In
this study we use only the transcripts where En-
glish was the target language.
5 Procedure
The purpose of the following analysis is to inves-
tigate the different aspects in the child?s language
that can be revealed by the proposed metrics. All
our measures are based on POS tags. We used the
Charniak parser (Charniak, 2000) to generate the
POS tags of the transcripts. For all the results re-
ported here we removed the utterances from the
interrogators and use all utterances by the chil-
93
dren. From the 155 TD instances, we randomly se-
lected 20, that together with the 20 instances with
LI form the test set. The remaining 135 TD in-
stances were used as the normative population, our
training set.
After the POS tagging process, we extracted the
set of syntactic patterns with length equal to 1, 2, 3
and 4 that appear in at least 80% of the transcripts
in the training set. The 80% threshold was chosen
with the goal of preserving the content that is most
likely to represent the TD population.
6 Analysis of the proposed measures and
implications
Figure 1 shows 5 plots corresponding to each of
our proposed measures. Each graph shows a com-
parison between the average values of the TD and
the LI populations. The x-axis in the graphs rep-
resents all the syntactic patterns gathered from the
training set that appeared on the test data, and the
y-axis represents the difference in the z-score val-
ues of each measure from the test set. The x-axis
is sorted in descending order according to the z-
score differences between values of TD and LI.
The most relevant discovery is that NDFL,
LFdist, LexSyn and LexSynEx show a wider gap
in the z-scores between the TD and LI popula-
tions for most of the syntactic patterns analyzed.
This difference is easy to note visually as most of
the TD patterns tend to have larger values, while
the ones for children with LI have lower scores.
Therefore, it seems our measures are indeed cap-
turing relevant information that characterizes the
language of the TD population.
Analyzing LEX from Figure 1, we see that most
of the LEX values are positive, for both TD and
LI instances, and we cannot observe marked dif-
ferences between them. That might be a con-
sequence of assuming all word classes can have
an equivalent number of different lexical forms.
Once we weigh each POS tag in the pattern by the
word forms the child has used (as in LexSyn and
LexSynEx), noticeable differences across the two
groups emerge. When we include syntactic knowl-
edge of a group of children (as in LexSynEx), those
similarities disappear. This behavior highlights the
need for a combined lexico-syntactic measure that
can describe latent information about language us-
age in children.
For building an intervention plan that helps to
improve child language skills, practitioners could
LFdist
verb (3rd person singular present)
verb (past tense) + personal pronoun
personal pronoun + auxiliary verb + adverb
verb (gerund)
NDLF
there + auxiliary verb
personal pronoun + auxiliary verb + adverb
adjective + noun
verb (3rd person singular present)
LexSyn
verb (past tense) + personal pronoun
personal pronoun + verb (past tense) + personal pronoun
personal pronoun + auxiliary verb + adverb
there + auxiliary verb
LexSynEx
personal pronoun + auxiliary verb + adverb
personal pronoun + verb (past tense) + personal pronoun
verb (past tense) + personal pronoun
there + auxiliary verb
Table 2: List of syntactic patterns with the biggest
difference between LI and TD in 4 measures:
LFdist, NDLF, and LexSyn and LexSynEx.
use the knowledge of specific grammatical con-
structions that need to be emphasized ?those that
seem to be problematic for the LI group. These
structures can be identified by pulling the syntac-
tic patterns with the largest difference in z-scores
from the TD population. Table 2 shows a list of
syntactic patterns with small values for LI and the
largest differences between LI and TD instances
in the test set. As the table indicates, most of the
syntactic patterns have length greater than 1. This
is not surprising since we aimed for developing
measures of higher-order analysis that can com-
plement the level of information provided by com-
monly used metrics in language assessment (as in
the case of MLU, NDW or F/C). The table also
shows that while each measure identifies a differ-
ent subset of syntactic patterns as relevant, some
syntactic patterns emerge in all the metrics. For
instance, personal pronoun + auxiliary verb + ad-
verb and there + auxiliary verb. This repetition
highlights the importance of those grammatical
constructions. But the differences also show that
the metrics complement each other. In general,
the syntactic patterns in the list represent complex
grammatical constructions where children with LI
are showing a less advanced command of language
use.
Table 3 shows some statistics about the lexical
forms present under pronoun + verb (3rd person
singular present) + verb (gerund or present par-
ticiple) (PP VBZ VBG) in all our data set. The last
94
-0.6
-0.4
-0.2
 0
 0.2
 0.4
 0.6
 0.8
 1
 1.2
z-s
cor
e
Syntactic patterns
Difference (TD-LI)
(a) NDLF
-0.6
-0.4
-0.2
 0
 0.2
 0.4
 0.6
 0.8
 1
 1.2
z-s
cor
e
Syntactic patterns
Difference (TD-LI)
(b) LFdist
-0.6
-0.4
-0.2
 0
 0.2
 0.4
 0.6
 0.8
 1
 1.2
z-s
cor
e
Syntactic patterns
Difference (TD-LI)
(c) LEX
-0.6
-0.4
-0.2
 0
 0.2
 0.4
 0.6
 0.8
 1
 1.2
z-s
cor
e
Syntactic patterns
Difference (TD-LI)
(d) LexSyn
-0.6
-0.4
-0.2
 0
 0.2
 0.4
 0.6
 0.8
 1
 1.2
z-s
cor
e
Syntactic patterns
Difference (TD-LI)
(e) LexSynEx
Figure 1: Performance comparison of the proposed measures for the TD and LI groups. Each data point
represents the difference in z-scores between the average values of the TD and LI instances in the test
set.
row in that table presents an example of the lexi-
cal forms used by two children. Note that for the
child with LI, there is only one lexical form: he is
touching. On the other hand, the TD child is using
the grammatical pattern with six different surface
forms. Clinical practitioners can take this infor-
mation and design language tasks that emphasize
the use of ?PP VBZ VBG? constructions.
6.1 Analysis of correlations among measures
To analyze the level of overlap between our mea-
sures we computed correlation coefficients among
them. The results are shown in Table 4.
The results from the correlation analysis are not
that surprising. They show that closely related
measures are highly to moderately correlated. For
instance, LEX and eLEX have a correlation of
TD LI
number of PP 6 5
number of VBZ 3 2
number of VBG 7 4
Example (instances: she is putting he is touching
td-0156 and li-3022) she is going
he is pushing
she is looking
she is carrying
she is playing
Table 3: Statistics of the surface forms for the
grammatical pattern PP VBZ VBG.
0.69, and LexSynEx and LexSyn have a correla-
tion of 0.61. NDLF and LFdist showed a posi-
tive correlation score of 0.81. This high correla-
tion hints to the fact that as the number of lexical
forms increases, so does the gap between their fre-
95
LFdist NDLF LEX eLEX LexSyn LexSynEx
LFdist 1.00
NDLF 0.81 1.00
LEX -0.53 -0.31 1.00
eLEX -0.54 -0.43 0.69 1.00
LexSyn 0.07 0.02 -0.23 -0.10 1.00
LexSynEx -0.02 -0.03 -0.08 -0.03 0.61 1.00
Table 4: Correlation matrix for the proposed metrics.
quency of use. While this may be a common phe-
nomenon of language use, it does not have a neg-
ative effect since the same effect will be observed
in both groups of children and we care to see the
differences in performance between a TD and an
LI population.
For all other pairs of measures, the correlation
scores were in the range of [?0.5, 0.1]. It was in-
teresting to note that LexSyn showed the lowest
correlation with the rest of the measures (between
[?0.11, 0.01]).
Correlation coefficients between our metrics
and MLU, NDW, and F/C were computed sepa-
rately for syntactic patterns of different lengths.
However all the different matrices showed the
same correlation patterns. We found a high cor-
relation between MLU and NDW, but low cor-
relation with all our proposed measures, except
for one case: NDW and LexSyn seemed to be
highly correlated (?-0.7). Interestingly, we noted
that despite the high correlation between MLU and
NDW, MLU and LexSyn showed weak correlation
(?-0.4). Overall, the findings from this analysis
support the use of our metrics as complimentary
measures for child language assessment.
7 Conclusions and future work
We proposed a set of new measures that were de-
veloped to characterize the lexico-syntactic vari-
ability of child language. Each measure aims to
find information that is not captured by traditional
measures used in communication disorders.
Our study is still preliminary in nature and re-
quires an in depth evaluation and analysis with a
larger pool of subjects. However the results pre-
sented are encouraging. The set of experiments
we discussed showed that TD and LI children have
significant differences in performance according
to our metrics and thus these metrics can be used to
enrich models of language trajectories in child lan-
guage acquisition. Another potential use of met-
rics similar to those proposed here is the design of
targeted intervention practices.
The scripts to compute the metrics as described
in this paper are available to the research commu-
nity by contacting the authors. However, the sim-
plicity of the metrics makes it easy for anyone to
implement, and it certainly makes it easy for clin-
ical researchers to interpret.
Our proposed metrics are a contribution to the
set of already known metrics for language assess-
ment. The goal of these new metrics is not to
replace existing ones, but to complement what is
already available with concise information about
higher-order syntactic constructions in the reper-
toire of TD children.
We are interested in evaluating the use of our
metrics in a longitudinal study. We believe they
are a promising framework to represent language
acquisition trajectories.
Acknowledgments
This research was partially funded by NSF under
awards 1018124 and 1017190. The first author
also received partial funding from CONACyT.
References
Lisa M. Bedore, Elizabeth D. Pen?a, Ronald B. Gillam,
and Tsung-Han Ho. 2010. Language sample mea-
sures and language ability in Spanish-English bilin-
gual kindergarteners. Journal of Communication
Disorders, 43:498?510.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of the 1st North
American chapter of the Association for Computa-
tional Linguistics conference, NAACL 2000, pages
132?139, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Miao Chen and Klaus Zechner. 2011. Computing
and evaluating syntactic complexity features for au-
tomated scoring of spontaneous non-native speech.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies - Volume 1, HLT ?11, pages
722?731, Stroudsburg, PA, USA. Association for
Computational Linguistics.
96
Keyur Gabani, Melissa Sherman, Thamar Solorio,
Yang Liu, Lisa M. Bedore, and Elizabeth D. Pen?a.
2009. A corpus-based approach for the prediction
of language impairment in monolingual English and
Spanish-English bilingual children. In Proceedings
of Human Language Technologies: The 2009 An-
nual Conference of the North American Chapter
of the Association for Computational Linguistics,
NAACL ?09, pages 46?55, Stroudsburg, PA, USA.
Association for Computational Linguistics.
V. Gutie?rrez-Clellen, G. Simon-Cereijido, and
M. Sweet. 2012. Predictors of second language
acquisition in Latino children with specific language
impairment. American Journal of Speech Language
Pathology, 21(1):64?77.
Khairun-nisa Hassanali, Yang Liu, and Thamar
Solorio. 2012a. Coherence in child language narra-
tives: A case study of annotation and automatic pre-
diction of coherence. In Proceedings of 3rd Work-
shop on Child, Computer and Interaction (WOCCI
2012).
Khairun-nisa Hassanali, Yang Liu, and Thamar
Solorio. 2012b. Evaluating NLP features for auto-
matic prediction of language impairment using child
speech transcripts. In Interspeech.
Tom Kwiatkowski, Sharon Goldwater, Luke Zettel-
moyer, and Mark Steedman. 2012. A probabilis-
tic model of syntactic and semantic acquisition from
child-directed utterances and their meanings. In
Proceedings of the 13th Conference of the European
Chapter of the Association for Computational Lin-
guistics, pages 234?244, Avignon, France. Associa-
tion for Computational Linguistics.
Xiaofei Lu. 2012. The relationship of lexical richness
to the quality of ESL learners? oral narratives. The
Modern Language Journal, 96(2):190?208.
Mercer Mayer. 1967. A boy, a dog, and a frog. Dial
Press.
Mercer Mayer. 1969. Frog, where are you? Dial
Press.
Jon F. Miller, John Heilmann, Ann Nockerts, Aquiles
Iglesias, Leah Fabiano, and David J. Francis. 2006.
Oral language and reading in bilingual children.
Learning Disabilities Research and Practice, 21:30?
43.
Elizabeth D. Pen?a, Lisa M. Bedore, Ronald B. Gillam,
and Thomas Bohman. 2003. Diagnostic markers
of language impairment in bilingual children. Grant
awarded by the NIDCH, NIH.
Emily T. Prud?hommeaux, Brian Roark, Lois M.
Black, and Jan van Santen. 2011. Classification of
atypical language in autism. In Proceedings of the
2nd Workshop on Cognitive Modeling and Compu-
tational Linguistics, pages 88?96, Portland, Oregon,
USA, June. Association for Computational Linguis-
tics.
Brian Roark, Margaret Mitchell, and Kristy Holling-
shead. 2007. Syntactic complexity measures for
detecting mild cognitive impairment. In Biologi-
cal, translational, and clinical language processing,
pages 1?8, Prague, Czech Republic, June. Associa-
tion for Computational Linguistics.
Brian Roark, Margaret Mitchell, John-Paul Hosom,
Kristy Hollingshead, and Jeffrey Kaye. 2011. Spo-
ken language derived measures for detecting mild
cognitive impairment. IEEE Transcations on Au-
dio, Speech, and Language Processing, 19(7):2081?
2090, September.
Rau?l Rojas and Aquiles Iglesias. 2012. The language
growth of Spanish-speaking English language learn-
ers. Child Development.
Kenji Sagae, Alon Lavie, and Brian MacWhinney.
2005. Automatic measurement of syntactic devel-
opment in child language. In Proceedings of the
43rd Annual Meeting of the Association for Com-
putational Linguistics, ACL ?05, pages 197?204,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Sam Sahakian and Benjamin Snyder. 2012. Automat-
ically learning measures of child language develop-
ment. In ACL, pages 95?99. The Association for
Computational Linguistics.
Thamar Solorio, Melissa Sherman, Y. Liu, Lisa
Bedore, Elizabeth Pen?a, and A. Iglesias. 2011. An-
alyzing language samples of Spanish-English bilin-
gual children for the automated prediction of lan-
guage dominance. Natural Language Engineering,
pages 367?395.
Charles Yang. 2011. A statistical test for grammar.
In Proceedings of the 2nd Workshop on Cognitive
Modeling and Computational Linguistics, pages 30?
38, Portland, Oregon, USA, June. Association for
Computational Linguistics.
Su-Youn Yoon and Suma Bhat. 2012. Assessment of
ESL learners? syntactic competence based on sim-
ilarity measures. In EMNLP-CoNLL, pages 600?
608. Association for Computational Linguistics.
97
Proceedings of the 2013 Workshop on Biomedical Natural Language Processing (BioNLP 2013), pages 111?115,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Using Latent Dirichlet Allocation for Child Narrative Analysis
Khairun-nisa Hassanali and Yang Liu
The University of Texas at Dallas
Richardson, TX, USA
nisa,yangl@hlt.utdallas.edu
Thamar Solorio
University of Alabama at Birmingham
Birmingham, AL, USA
solorio@uab.edu
Abstract
Child language narratives are used for lan-
guage analysis, measurement of language
development, and the detection of lan-
guage impairment. In this paper, we ex-
plore the use of Latent Dirichlet Alloca-
tion (LDA) for detecting topics from nar-
ratives, and use the topics derived from
LDA in two classification tasks: automatic
prediction of coherence and language im-
pairment. Our experiments show LDA is
useful for detecting the topics that corre-
spond to the narrative structure. We also
observed improved performance for the
automatic prediction of coherence and lan-
guage impairment when we use features
derived from the topic words provided by
LDA.
1 Introduction
Language sample analysis is a common technique
used by speech language researchers to measure
various aspects of language development. These
include speech fluency, syntax, semantics, and co-
herence. For such analysis, spontaneous narratives
have been widely used. Narrating a story or a per-
sonal experience requires the narrator to build a
mental model of the story and use the knowledge
of semantics and syntax to produce a coherent nar-
rative. Children learn from a very early age to nar-
rate stories. The different processes involved in
generating a narrative have been shown to provide
insights into the language status of children.
There has been some prior work on child lan-
guage sample analysis using NLP techniques. Sa-
hakian and Snyder (2012) used a set of linguistic
features computed on child speech samples to cre-
ate language metrics that included age prediction.
Gabani et al (2011) combined commonly used
measurements in communication disorders with
several NLP based features for the prediction of
Language Impairment (LI) vs. Typically Develop-
ing (TD) children. The features they used included
measures of language productivity, morphosyntac-
tic skills, vocabulary knowledge, sentence com-
plexity, probabilities from language models, stan-
dard scores, and error patterns. In their work, they
explored the use of language models and machine
learning methods for the prediction of LI on two
types of child language data: spontaneous and nar-
rative data.
Hassanali et al (2012a) analyzed the use of
coherence in child language and performed auto-
matic detection of coherence from child language
transcripts using features derived from narrative
structure such as the presence of critical narrative
components and the use of narrative elements such
as cognitive inferences and social engagement de-
vices. In another study, Hassanali et al (2012b)
used several coherence related features to auto-
matically detect language impairment.
LDA has been used in the field of narrative anal-
ysis. Wallace et al (2012) adapted LDA to the task
of multiple narrative disentanglement, in which
the aim was to tease apart narratives by assigning
passages from a text to the subnarratives that they
belong to. They achieved strong empirical results.
In this paper, we explore the use of LDA for
child narrative analysis. We aim to answer two
questions: Can we apply LDA to children nar-
ratives to identify meaningful topics? Can we
represent these topics automatically and use them
for other tasks, such as coherence detection and
language impairment prediction? Our results are
promising. We found that using LDA topic model-
ing can infer useful topics, and incorporating fea-
tures derived from such automatic topics improves
the performance of coherence classification and
language impairment detection over the previously
reported results.
111
Coherence Scale TD LI Total
Coherent 81 6 87
Incoherent 18 13 31
Total 99 19 118
Table 1: Number of TD and LI children on a 2-
scale coherence level
2 Data
For the purpose of the experiments, we used the
Conti-Ramsden dataset (Wetherell et al, 2007a;
Wetherell et al, 2007b) from the CHILDES
database (MacWhinney, 2000). This dataset con-
sists of transcripts belonging to 118 adolescents
aged 14 years. The adolescents were given the
wordless picture story book ?Frog, where are
you?? and asked to narrate the story based on the
pictures. The storybook is about the adventures of
a boy who goes searching for his missing pet frog.
Even though our goal is to perform child narrative
analysis, we used this dataset from adoloscents
since it was publicly available, and was annotated
for language impairment and coherence. Of the
118 adolescents, 99 adolescents belonged to the
TD group and 19 adolescents belonged to the lan-
guage impaired group. Hassanali et al (2012a)
annotated this dataset for coherence. A transcript
was annotated as coherent, as long as there was no
difficulty in understanding the narrative, and in-
coherent otherwise. Table 1 gives the TD and LI
distribution on a 2-scale coherence level. Figure
1 shows an example of a transcript produced by a
TD child.
Figure 1: Sample transcript from a TD child
3 Narrative Topic Analysis Using LDA
Latent Dirichlet Allocation (LDA) (Blei et al,
2003) has been used in NLP to model topics within
a collection of documents. In this study, we use
LDA to detect topics in narratives. Upon exam-
ining the transcripts, we observed that each topic
was described in about 3 to 4 utterances. We there-
fore segmented the narratives into chunks of 3 ut-
terances, with the assumption that each segment
corresponds roughly to one topic.
We used the software by Blei et al1 to perform
LDA. Prior to performing LDA, we removed the
stop words from the transcripts. We chose ? to
be 0.8 and K to be 20, where ? is the parameter
of the Dirichlet prior on the per-document topic
distributions and K denotes the number of topics
considered in the model.
We chose to use the transcripts of TD children
for generating the topics, because the transcripts of
TD children have fewer disfluencies, incomplete
utterances, and false starts. As we can observe
from Table 1, a higher percentage of TD children
produced coherent narratives when compared to
children with LI.
Table 2 gives the topic words for the top 10
topics extracted using LDA. The topics in Table
2 were manually labeled after examination of the
topic words extracted using LDA. We found that
some of the topics extracted by LDA corresponded
to subtopics. For example, searching for the frog
in the house has subtopics of the boy searching
for the frog in room and the dog falling out of the
window, which were part of the topics covered by
LDA. The subtopics are marked in italics in Table
2.
The following narrative components were iden-
tified as important features for the automatic pre-
diction of coherence by Hassanali et al (2012a).
1. Instantiation: introduce the main characters
of the story: the boy, the frog, and the dog,
and the frog goes missing
2. 1st episode: search for the frog in the house
3. 2nd episode: search for the frog in the tree
4. 3rd episode: search for the frog in the hole in
the ground
5. 4th episode: search for the frog near the rock
6. 5th episode: search for the frog behind the
log
7. Resolution: boy finds the frog in the river and
takes a frog home
Upon examining the topics extracted by LDA, we
observed that all the components mentioned above
1http://www.cs.princeton.edu/ blei/lda-c/index.html
112
Topic
No
Topic Words Used by TD Population Topic Described
1 went,frog,sleep,glass,put,caught,jar,yesterday,out,house Introduction
2 frog,up,woke,morning,called,gone,escaped,next,kept,realized Frog goes missing
3 window,out,fell,dog,falls,broke,quickly,opened,told,breaking Dog falls out of window
4 tree,bees,knocked,running,popped,chase,dog,inside,now,flying Dog chases the bees
5 deer,rock,top,onto,sort,big,up,behind,rocks,picked Deer behind the rock
6 searched,boots,room,bedroom,under,billy,even, floor,tilly,tried Search for frog in room
7 dog,chased,owl,tree,bees,boy,came,hole,up,more Boy is chased by owl from a
tree with beehives
8 jar,gone,woke,escaped,night,sleep,asleep,dressed,morning,frog Frog goes missing
9 deer,top,onto,running,ways,up,rocks,popped,suddenly,know Boy runs into the deer
10 looking,still,dog,quite,cross,obviously,smashes,have,annoyed Displeasure of boy with dog
Table 2: Top 10 topic words extracted by LDA on the story telling task. Subtopics are shown in italics.
were present in these topics. Many of the LDA
topics corresponded to a picture or two in the sto-
rybook.
4 Using LDA Topics for Coherence and
Language Impairment Classification
We extended the use of LDA for two tasks,
namely: the automatic evaluation of coherence
and the automatic evaluation of language impair-
ment. For the experiments below, we used the
WEKA toolkit (Hall et al, 2009) and built sev-
eral models using the naive Bayes, Bayesian net
classifier, Logistic Regression, and Support Vec-
tor Machine (SVM) classifier. Of all these classi-
fiers, the naive Bayes classifier performed the best,
and we report the results using the naive Bayes
classifier in Tables 3 and 4. We performed all the
experiments using leave-one-out cross-validation,
wherein we excluded the test transcript that be-
longed to a TD child from the training set when
generating topics using LDA.
4.1 Automatic Evaluation of Coherence
We treat the automatic evaluation of coherence
as a classification task. A transcript could either
be classified as coherent or incoherent. We use
the results of Hassanali et al (2012a) as a base-
line. They used the presence of narrative episodes,
and the counts of narrative quality elements such
as cognitive inferences and social engagement de-
vices as features in the automatic prediction of co-
herence. We add the features that we automati-
cally extracted using LDA.
We checked for the presence of at least six of
the ten topic words or their synonyms per topic in
a window of 3 utterances. If the topic words were
present, we took this as a presence of a topic; oth-
erwise we denoted it as an absence of a topic. In
total, there were 20 topics that we extracted using
LDA, which is higher compared to the 8 narrative
structure topics that were annotated for by Has-
sanali et al (2012a).
Table 3 gives the results for the automatic clas-
sification of coherence. As we observe in Table
3, there is an improvement in performance over
the baseline. We attribute this to the inclusion of
subtopics that were extracted using LDA.
4.2 Automatic Evaluation of Language
Impairment
We extended the use of LDA to create a summary
of the narratives. For the purpose of generating the
summary, we considered only the narratives gen-
erated by TD children in the training set. We gen-
erated a summary, by choosing 5 utterances cor-
responding to each topic that was generated using
LDA, thereby yielding a summary that consisted
of 100 utterances.
We observed that different words were used to
represent the same concept. For example, ?look?
and ?search? were used to represent the concept
of searching for the frog. Since the narration was
based on a picture storybook, many of the children
used different terms to refer to the same animal.
For example, ?the deer? in the story has been inter-
preted to be ?deer?, ?reindeer?, ?moose?, ?stag?,
?antelope? by different children. We created an
extended topic vocabulary using Wordnet to in-
clude words that were semantically similiar to the
topic keywords. In addition, for an utterance to be
113
Feature Set
Coherent Incoherent Accuracy
(%)Precision Recall F-1 Precision Recall F-1
Narrative (Hassanali et al,
2012a) (baseline)
0.869 0.839 0.854 0.588 0.645 0.615 78.814
Narrative + automatic topic
features
0.895 0.885 0.89 0.688 0.71 0.699 83.898
Table 3: Automatic classification of coherence on a 2-scale coherence level
in the summary, we put in the additional constraint
that neighbouring utterances within a window of
3 utterances also talk about the same topic. We
used this summary for constructing unigram and
bigram word features for the automatic prediction
of LI.
The features we constructed for the prediction
of LI were as follows:
1. Bigrams of the words in the summary
2. Presence or absence of the words in the sum-
mary regardless of the position
3. Presence or absence of the topics detected by
LDA in the narratives
4. Presence or absence of the topic words that
were detected using LDA
We used both the topics detected and the pres-
ence/absence of topic words as features since the
same topic word could be used across several top-
ics. For example, the words ?frog?, ?dog?, ?boy?,
and ?search? are common across several topics.
We refer to the above features as ?new features?.
Table 4 gives the results for the automatic pre-
diction of LI using different features. As we can
observe, the performance improves to 0.872 when
we add the new features to Gabani?s and the nar-
rative structure features. When we use the new
features by themselves to predict language impair-
ment, the performance is the worst. We attribute
this to the fact that other feature sets are richer
since these features take into account aspects such
as syntax and narrative structure.
We performed feature analysis on the new fea-
tures to see what features contributed the most.
The top scoring features were the presence or ab-
sence of the topics detected by LDA that corre-
sponded to the introduction of the narrative, the
resolution of the narrative, the search for the frog
in the room, and the search for the frog behind
the log. The following bigram features generated
from the summary contributed the most: ?deer
Feature P R F-1
Gabani?s (Gabani et
al., 2011)
0.824 0.737 0.778
Narrative (Hassanali et
al., 2012a)
0.385 0.263 0.313
New features 0.308 0.211 0.25
Narrative + Gabani?s 0.889 0.842 0.865
Narrative + Gabani?s +
new features
0.85 0.895 0.872
Table 4: Automatic classification of language im-
pairment
rock?, ?lost frog?, and ?boy hole?. Using a subset
of these best features did not improve the perfor-
mance when we added them to the narrative fea-
tures and Gabani?s features.
5 Conclusions
In this paper, we explored the use of LDA in the
context of child language analysis. We used LDA
to extract topics from child language narratives
and used these topic keywords to create a sum-
mary of the narrative and an extended vocabu-
lary. The topics extracted using LDA not only
covered the main components of the narrative but
also covered subtopics too. We then used the LDA
topic words and the summary to create features
for the automatic prediction of coherence and lan-
guage impairment. Due to higher coverage of the
LDA topics as compared to manual annotation, we
found an increase in performance of both auto-
matic prediction of coherence and language im-
pairment with the addition of the new features. We
conclude that the use of LDA to model topics and
extract summaries is promising for child language
analysis.
Acknowledgements
This research is supported by NSF awards IIS-
1017190 and 1018124.
114
References
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. the Journal of ma-
chine Learning research, 3:993?1022.
Keyur Gabani, Thamar Solorio, Yang Liu, Khairun-
nisa Hassanali, and Christine A. Dollaghan. 2011.
Exploring a corpus-based approach for detect-
ing language impairment in monolingual English-
speaking children. Artificial Intelligence in
Medicine, 53(3):161?170.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: An update.
ACM SIGKDD Explorations Newsletter, 11(1):10?
18.
Khairun-nisa Hassanali, Yang Liu, and Thamar
Solorio. 2012a. Coherence in child language nar-
ratives: A case study of annotation and automatic
prediction of coherence. In Proceedings of WOCCI
2012 - 3rd Workshop on Child, Computer and Inter-
action.
Khairun-nisa Hassanali, Yang Liu, and Thamar
Solorio. 2012b. Evaluating NLP features for au-
tomatic prediction of language impairment using
child speech transcripts. In Proceedings of INTER-
SPEECH.
Brian MacWhinney. 2000. The CHILDES project:
Tools for analyzing talk, Volume I: Transcription for-
mat and programs. Lawrence Erlbaum Associates.
Sam Sahakian and Benjamin Snyder. 2012. Automat-
ically learning measures of child language develop-
ment. In Proceedings of the 50th Annual Meeting
of the Association for Computational Linguistics:
Short Papers-Volume 2, pages 95?99. Association
for Computational Linguistics.
Bryon C. Wallace. 2012. Multiple narrative disentan-
glement: Unraveling infinite jest. In Proceeding of
the 2012 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, pages 1?10.
Danielle Wetherell, Nicola Botting, and Gina Conti-
Ramsden. 2007a. Narrative in adolescent specific
language impairment (SLI): a comparison with peers
across two different narrative genres. International
Journal of Language & Communication Disorders,
42(5):583?605.
Danielle Wetherell, Nicola Botting, and Gina Conti-
Ramsden. 2007b. Narrative skills in adolescents
with a history of SLI in relation to non-verbal IQ
scores. Child Language Teaching and Therapy,
23(1):95.
115
Proceedings of the ACL 2014 Workshop on Semantic Parsing, pages 12?16,
Baltimore, Maryland USA, June 26 2014.
c?2014 Association for Computational Linguistics
Large-scale CCG Induction from the Groningen Meaning Bank
Sebastian Beschke
?
, Yang Liu
?
and Wolfgang Menzel
?
?
Department of Informatics, University of Hamburg, Germany
{beschke,menzel}@informatik.uni-hamburg.de
?
State Key Laboratory of Intelligent Technology and Systems
Tsinghua National Laboratory for Information Science and Technology
Department of Computer Science and Technology, Tsinghua University, Beijing, China
liuyang2011@tsinghua.edu.cn
Abstract
In present CCG-based semantic parsing
systems, the extraction of a semantic
grammar from sentence-meaning exam-
ples poses a computational challenge. An
important factor is the decomposition of
the sentence meaning into smaller parts,
each corresponding to the meaning of a
word or phrase. This has so far limited
supervised semantic parsing to small, spe-
cialised corpora. We propose a set of
heuristics that render the splitting of mean-
ing representations feasible on a large-
scale corpus, and present a method for
grammar induction capable of extracting a
semantic CCG from the Groningen Mean-
ing Bank.
1 Introduction
Combinatory Categorial Grammar (CCG) forms
the basis of many current approaches to semantic
parsing. It is attractive for semantic parsing due
to its unified treatment of syntax and semantics,
where the construction of the meaning representa-
tion directly follows the syntactic analysis (Steed-
man, 2001). However, the supervised induction of
semantic CCGs?the inference of a CCG from a
corpus of sentence-meaning pairs?has so far only
been partially solved. While approaches are avail-
able that work on small corpora focused on spe-
cific domains (such as Geoquery and Freebase QA
for question answering (Zelle and Mooney, 1996;
Cai and Yates, 2013)), we are not aware of any
approach that allows the extraction of a seman-
tic CCG from a wide-coverage corpus such as the
Groningen Meaning Bank (GMB) (Basile et al.,
2012). This work attempts to fill this gap.
Analogous to the work of Kwiatkowski et al.
(2010), we view grammar induction as a series of
splitting steps, each of which essentially reverses
a CCG derivation step. However, we diverge
from their approach by applying novel heuristics
for searching the space of possible splits. The
combination of alignment consistency and single-
branching recursion turns out to produce a man-
ageable number of lexical items for most sen-
tences in the GMB, while statistical measures
and manual inspection suggest that many of these
items are also plausible.
2 Searching the space of CCG
derivations
Our search heuristics are embedded into a very
general splitting algorithm, Algorithm 1. Given
a sentence-meaning pair, it iterates over all possi-
ble sentence-meaning splits in two steps. First, a
split index in the sentence is chosen along with a
binary CCG-combinator to be reversed (the syn-
tactic split). Then, the meaning representation is
split accordingly to reverse the application of the
selected combinator (the semantic split). E. g., for
the forward application combinator, the meaning
representation z is split into f, g so that z = fg
(modulo ?, ?, ? conversions). By identifying f
with the left half l of the sentence and g with the
right half r, we obtain two new phrase-meaning
pairs, which are then split recursively.
This algorithm combines two challenging
search problems. Recursive syntactic splitting
searches the space of syntactic CCG derivations
that yield the sentence, which is exponential in the
length of the sentence. Semantic splitting, given
the flexibility of ?-calculus, has infinitely many
solutions. The crucial question is how to prune
the parts of the search space that are unlikely to
lead to good results.
Our strategy to address this problem is to apply
heuristics that constrain the results returned by se-
mantic splitting. By yielding no results on certain
inputs, this at the same time constrains the syntac-
tic search space. The following descriptions there-
12
fore relate to the implementation of the SEMSPLIT
function.
Algorithm 1 A general splitting algorithm. C is
the set of binary CCG combinators. The SEM-
SPLIT function returns possible splits of a meaning
representation according to the reverse application
of a combinator.
function SPLIT(x, z)
if |x| = 1 then
return {(x, z)}
else
G? ?
for 0 < i ? |x| ? 1 and c ? C do
l? x
0
. . . x
i?1
r ? x
i
. . . x
|x|?1
S ? SEMSPLIT(c, z)
for (f, g) ? S do
G? G ? SPLIT(l, f)
? SPLIT(r, g)
end for
end for
return G
end if
end function
2.1 Alignment consistency
The first heuristic we introduce is borrowed from
the field of statistical machine translation. There,
alignments between words of two languages are
used to identify corresponding phrase pairs, as
in the well-known GHKM algorithm (Galley et
al., 2004). In order to apply the same strategy
to meaning representations, we represent them as
their abstract syntax trees. Following Li et al.
(2013), we can then align words in the sentence
and nodes in the meaning representation to iden-
tify components that correspond to each other.
This allows us to impose an extra constraint on
the generation of splits: We require that nodes in f
not be aligned to any words in the right sentence-
half r, and conversely, that nodes in g not be
aligned to words in l.
Alignment consistency helps the search to fo-
cus on more plausible splits by grouping elements
of the meaning representation with the words that
evoked them. However, by itself it does not signif-
icantly limit the search space, as it is still possible
to extract infinitely many semantic splits from any
sentence at any splitting index.
Example: Given the word-to-meaning
?x
?y
?
mia(y)love(x, y)vincent(x)
lovesVincent Mia
Figure 1: An example word-to-meaning align-
ment. Splits across any of the alignment edges are
prohibited. E. g., we cannot produce a split whose
meaning representation contains both vincent and
mia.
alignment from Figure 1, a split that is
excluded by the alignment criterion is:
(Vincent : ?g.?x.?y.vincent(x) ? love(x, y) ?
g(y)), (loves Mia : ?y.mia(y)). This is because
the node ?love? (in f ) is aligned to the word
?loves? (in r).
2.2 Single-branching recursive splitting
The second heuristic is best described as a search
strategy over possible semantic splits. In the fol-
lowing presentation, we presume that alignment
consistency is being enforced. Again, it is helpful
to view the meaning representation as an abstract
syntax tree.
Recall that our goal is to find two expressions
f, g to be associated with the sentence halves l, r.
In a special case, this problem is easily solved: If
we can find some split node X which governs all
nodes aligned to words in r, but no nodes aligned
to words in l, we can simply extract the sub-tree
rooted at X and replace it with a variable. E. g.,
z = a(bc) can be split into f = ?x.a(xc) and
g = b, which can be recombined by application.
However, requiring the existence of exactly two
such contiguous components can be overly restric-
tive, as Figure 2 illustrates. Instead, we say that we
decompose z into a hierarchy of components, with
a split node at the root of each component. These
components are labelled as f - and g-components
in an alternating fashion.
In this hierarchy, the members of an f -
component are not allowed to have alignments to
words in l. A corresponding requirement holds for
13
@@
ed
@
@
cb
a
x
0
x
1
x
2
x
3
x
4
Figure 2: Illustration of single-branching recur-
sion: Assume that the leaves of the meaning rep-
resentation a(bc)(de) are aligned as given to the
words x
0
. . . x
4
, and that we wish to split the sen-
tence at index 2. The indicated split partitions the
meaning representation into three hierarchically
nested components and yields f = ?x.xc(de) and
g = ?y.a(by), which can be recombined using ap-
plication.
g-components.
The single-branching criterion states that all
split nodes lie on a common path from the root,
or in other words, every component is the parent
of at most one sub-component.
In comparison to more flexible strategies,
single-branching recursive splitting has the advan-
tage of requiring a minimum of additionally gen-
erated structure. For every component, we only
need to introduce one new bound variable for the
body plus one for every variable that occurs free
under the split node.
Together with the alignment consistency cri-
terion, single-branching recursive splitting limits
the search space sufficiently to make a full search
tractable in many cases.
2.3 Other heuristics
The following heuristics seem promising but are
left to be explored in future work.
Min-cut splitting In this strategy, we place no
restriction on which split nodes are chosen. In-
stead, we require that the overall count of split
nodes is minimal, which is equivalent to saying
that the edges cut by the split form a minimum cut
separating the nodes aligned to the left and right
halves of the sentence, respectively. This strat-
egy has the advantage of being able to handle any
alignment/split point combination, but requires a
more complex splitting pattern and thus more ad-
ditional structure than single-branching recursion.
Syntax-driven splitting Since CCG is based
on the assumption that semantic and syntactic
derivations are isomorphic, we might use syntac-
tic annotations to guide the search of the deriva-
tion space and only consider splits along con-
stituent boundaries. Syntactic annotations might
be present in the data or generated by standard
tools. However, initial tests have shown that this
requirement is too restrictive when combined with
our two main heuristics.
Obviously, an effective combination of heuris-
tics needs to be found. One particular configura-
tion which seems promising is alignment consis-
tency combined with min-cut splitting (which is
more permissive than single-branching recursion)
and syntax-driven splitting (which adds an extra
restriction).
3 Discussion
We present some empirical observations about the
behaviour of the above-mentioned heuristics. Our
observations are based on a grammar extracted
from the GMB. A formal evaluation of our system
in the context of a full semantic parsing system is
left for future work.
3.1 Implementation
Currently, our system implements single-
branching recursive splitting along with alignment
consistency. We extracted the word-to-meaning
alignments from the CCG derivations annotated
in the GMB, but kept only alignment edges
to predicate nodes. Sentence grammars were
extracted by generating an initial item for each
sentence and feeding it to the SPLIT procedure.
In addition to alignment consistency and single-
branching recursion, we enforce three simple cri-
teria to rule out highly implausible items: The
count of arrows in an extracted meaning represen-
tation?s type is limited to eight, the number of split
nodes is limited to three, and the number of free
variables in extracted components is also limited
to three.
A major limitation of our implementation is that
it currently only considers the application combi-
nator during splitting. We take this as a main rea-
son for the limited granularity we observe in our
output. Generalisation of the splitting implemen-
tation to other combinators such as composition is
therefore necessary before performing any serious
evaluation.
14
3.2 Manual inspection
Manual inspection of the generated grammars
leads to two general observations.
Firstly, many single-word items present in the
CCG annotations of the GMB are recovered.
While this behaviour is not required, it is en-
couraging, as these items exhibit a relatively sim-
ple structure and would be expected to generalise
well.
At the same time, many multi-word phrases
remain in the data that cannot be split further,
and are therefore unlikely to generalise well. We
have identified two likely causes for this phe-
nomenon: The missing implementation of a com-
position combinator, and coarse alignments.
Composition splits would enable the splitting of
items which do not decompose well (i. e., do not
pass the search heuristics in use) under the appli-
cation combinator. Since composition occurs fre-
quently in GMB derivations, it is to be expected
that its lack noticeably impoverishes the quality of
the extracted grammar.
The extraction of alignments currently in use in
our implementation works by retracing the CCG
derivations annotated in the GMB, and thus es-
tablishing a link between a word and the set of
meaning representation elements introduced by it.
However, our current implementation only han-
dles the most common derivation nodes and oth-
erwise cuts this retracing process short, making
alignments to the entire phrase governed by an in-
termediate node. This may cause the correspond-
ing part of the search to be pruned due to a search
space explosion. We plan to investigate using a
statistical alignment tool instead, possibly using
supplementary heuristics for determining aligned
words and nodes. As an additional advantage, this
would remove the need for annotated CCG deriva-
tions in the data.
3.3 Statistical observations
From the total 47 230 sentences present in the
GMB, our software was able to extract a sentence
grammar for 43 046 sentences. Failures occurred
either because processing took longer than 20 min-
utes, because the count of items extracted for a
single sentence surpassed 10 000, or due to pro-
cessing errors.
On average, 825 items were extracted per sen-
tence with a median of 268. After removing dupli-
cate items, the combined grammar for the whole
GMB consisted of about 32 million items. While
the running time of splitting is still exponential
and gets out of hand on some examples, most sen-
tences are processed within seconds.
Single-word items were extracted for 46% of
word occurrences. Ideally, we would like to ob-
tain single-word items for as many words as pos-
sible, as those items have the highest potential to
generalise to unseen data. For those occurrences
where no single-word item was extracted, the me-
dian length of the smallest extracted item was 12,
with a maximum of 49.
4 Conclusion
We have presented a method for bringing the in-
duction of semantic CCGs to a larger scale than
has been feasible so far. Using the heuristics of
alignment consistency and single-branching recur-
sive splitting, we are able to extract a grammar
from the full GMB. Our observations suggest a
mixed outcome: We obtain desirable single-word
items for only about half of all word occurrences.
However, due to the incompleteness of the im-
plementation and the lack of a formal evaluation,
these observations do not yet permit any conclu-
sions. In future work, we will address both of
these shortcomings.
5 Final remarks
The software implementing the presented func-
tionality is available for download
1
.
This work has been supported by the Ger-
man Research Foundation (DFG) as part of the
CINACS international graduate research group.
References
Valerio Basile, Johan Bos, Kilian Evang, and Noortje
Venhuizen. 2012. Developing a large semantically
annotated corpus. In Proceedings of LREC?12, Is-
tanbul, Turkey.
Qingqing Cai and Alexander Yates. 2013. Large-scale
semantic parsing via schema matching and lexicon
extension. In Proceedings of ACL 2013, Sofia, Bul-
garia.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation rule?
In Proceedings of HLT-NAACL 2004, Boston, Mas-
sachusetts, USA.
1
http://nats-www.informatik.
uni-hamburg.de/User/SebastianBeschke
15
Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwa-
ter, and Mark Steedman. 2010. Inducing probabilis-
tic CCG grammars from logical form with higher-
order unification. In Proceedings of EMNLP 2010,
Cambridge, Massachusetts, USA.
Peng Li, Yang Liu, and Maosong Sun. 2013. An ex-
tended GHKM algorithm for inducing ?-scfg. In
Proceedings of AAAI 2013, Bellevue, Washington,
USA.
Mark Steedman. 2001. The Syntactic Process. MIT
Press, January.
John M. Zelle and Raymond J. Mooney. 1996. Learn-
ing to parse database queries using inductive logic
programming. In Proceedings of AAAI-96, Portland,
Oregon, USA.
16
