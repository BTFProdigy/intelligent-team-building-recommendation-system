Coling 2008: Companion volume ? Posters and Demonstrations, pages 79?82
Manchester, August 2008
Generation under Space Constraints 
C?cile Paris, Nathalie Colineau, 
Andrew Lampert 
CSIRO ? ICT Centre 
Locked Bag 17 
North Ryde, NSW 1670, Australia 
FirstName.LastName@csiro.au 
Joan Giralt Duran 
 
Barcelona School of Informatics 
Technical University of Catalonia 
Barcelona, Spain 
joangi@gmail.com 
 
 
Abstract 
Reasoning about how much to generate 
when space is limited is a challenge for 
generation systems. This paper presents 
two algorithms that exploit the discourse 
structure to decide which content to drop 
when there are space restrictions, in the 
context of producing documents from 
pre-authored text fragments. We analyse 
the effectiveness of both algorithms and 
show that the second is near optimal.  
1 Introduction 
Many organisations employ content management 
systems to store information, typically at the 
paragraph level. The use of such systems enables 
the application of NLG techniques without the 
cost of acquiring a knowledge base or forming 
text from first principles. But it brings its own 
challenge: how to produce a coherent and well 
structured text satisfying specific space 
requirements when a system has no control over 
the text at the sentence level? 
The ability to reason about space constraints 
becomes more pressing as the amount of 
available information increases and delivery 
channels become more diverse in terms of space 
requirements (e.g., web browsers, email, PDAs). 
We, as humans, address this problem by short-
ening our sentences or restricting the content we 
include. We achieve the former by manipulating 
vocabulary and syntax. This requires careful at-
tention to the text at sentence level and often 
does not reclaim significant amount of space.  
                                                 
? CSIRO 2008. Licensed under the Creative Com-
mons Attribution-Noncommercial-Share Alike 3.0 
Unported license (http://creativecommons.org/ 
licenses/by-nc-sa/3.0/). Some rights reserved. 
We achieve the latter by dropping those pieces of 
content whose contribution to the communicative 
goal is most limited. This approach can reduce a 
text?s length significantly but requires an 
understanding of the text?s discourse structure. 
In our application domain, we answer people?s 
information needs by retrieving content from a 
repository of pre-authored text fragments and 
delivering that content via a variety of media 
(e.g., web, paper, email), each with their own 
space constraints. In this paper, we show how we 
exploit the discourse structure to determine what 
should be realised to best fit some specific space. 
In particular, we present two algorithms that per-
form this reasoning and analyse their compara-
tive performance. 
2 Related Work 
NLG systems have exploited the discourse struc-
ture for a number of tasks ? e.g., to generate ap-
propriate cue phrases (e.g., Scott and de Souza, 
1990) or reason about layout (e.g., Bateman et 
al., 2001). Our system uses the discourse 
structure to reason about how much to realise to 
fit a specific space. It produces one discourse tree 
that is then realised for different delivery chan-
nels, each with its own space requirements.  
Like other systems (e.g., Moore and Paris, 
1993), our system specifies the RST relations 
(Mann and Thompson, 1988) that hold between 
text spans during discourse planning. It then ex-
ploits the RST principle of nuclearity to decide 
what to realise. The intuition is that nuclei are 
important while satellites can be dropped.  This 
intuition has been exploited in some systems to 
produce summaries (e.g., Sparck-Jones, 1993; 
Marcu, 1998). Our purpose is different, however. 
We do not aim to produce a summary but a text 
that fits into some space requirements, 
79
  
 
Figure 1. A brochure generated by our system 
potentially only slightly shortening a text. Our 
task brings new challenges, e.g., filling the space 
optimally and producing a balanced text.  
Our system exploits the notion that some rela-
tions are more important than others. O?Donnell 
(1997) used this principle to produce documents 
of variable length. In his approach, sentence 
fragments were manually marked up with RST 
and the text was manipulated at or below the sen-
tence level.  In our work, we cannot manipulate 
text at sentence level nor manually mark up the 
documents to be shortened. 
3 Reasoning about Space Constraints 
We focus on applications in which the generated 
text is delivered through several channels. One 
such application is SciFly, which produces 
tailored brochures about our organisation. Given 
a query from a user (topic(s) of interest), the 
system consults a staff directory and a repository 
of text fragments to gather relevant information. 
The fragments, written by our marketing team, 
are self contained and comprised of one or two 
paragraphs. SciFly integrates all the relevant 
information into a coherent whole (see Figure 1) 
using the meta-data describing each fragment?s 
content. A text fragment can be used with differ-
ent rhetorical relations in different brochures. 
The system produces a 2-page paper brochure, a 
web output, and a PDF version of the paper bro-
chure is emailed to the user with a summary in 
the email body. All outputs are generated from 
the same discourse structure by our algorithm. 
Our need to deliver the brochure via multiple 
channels led us to design algorithms that reason 
about the content to be expressed and the space 
available for each channel. The system follows a 
two-stage approach: during discourse planning, 
content and organisation are selected, and a dis-
course tree is built. The tree includes the top 
level communicative goal, intermediate goals 
and the rhetorical relations that exist between 
text spans, encoding both the purpose of each 
fragment and how they relate to each other. Then, 
at the presentation stage, the system reasons 
about this structure  to decide what to realise 
when there is too much content for some channel.  
We implemented and tested two algorithms 
for this reasoning. Both algorithms embody the 
principle of nuclearity and exploit the notion that 
some relations are more important than others. 
An importance value is assigned to relations 
based on their contribution to the communicative 
goal. Table 1 shows our assignments, which are 
based on judgments from our marketing staff. 
To explain the algorithms, we represent the 
discourse tree using an abstract view, as shown 
in Figure 2. Each node is a communicative goal. 
White nodes indicate nuclei. Satellites are shaded 
in grey corresponding to the importance of the 
rhetorical relation linking them to the nucleus. 
The number inside each node is the approximate 
amount of content that node produces (in lines). 
80
Shading Discourse Relations Importance 
Black Illustration, Background, 
Circumstance, Elaboration 
Low  
Low-Medium 
Dark 
Grey 
Context, Motivation, 
Evidence, Summary , 
Justification,  
Medium 
Light 
Grey 
Preparation, Enablement Medium-High 
High 
Table 1. Importance score for some relations1 
Each node is the root of a subtree (empty if the 
node is a leaf) which generates some content. In 
both algorithms, the system computes for each 
node an approximation of the space required for 
that content in number of lines (an approximation 
as it depends on style, line-wrapping and other 
formatting attributes in the final output). This is 
computed bottom-up in an iterative manner by 
looking at the retrieved content at each node. 
 
Figure 2. Discourse tree with space annotations 
3.1 Simple Algorithm  
The first algorithm is simple. It checks whether 
the top level node would result in too much 
content given the space requirements of the 
output channel (e.g., lines of content per page). If 
yes, the system traverses the tree, selects satel-
lites with the lowest importance value and drops 
them with their sub-trees. The algorithm repeats 
this process until the total amount of content fits 
the required space. We deployed the SciFly 
system with this algorithm at a trade fair in 2005 
and 2006 and measured the experience visitors 
had with the system. On average, people rated 
the system positively but noted that there was 
sometimes a lot of blank space in the brochures, 
when they felt that more information could have 
been included. This is because our simple 
algorithm drops many sub-trees at once, thus 
potentially deleting a lot of content in each step. 
This led us to our enhanced algorithm. 
                                                 
1 In our system, we consider 5 levels of importance. We 
have merged levels here to avoid too many shades of grey. 
3.2 Enhanced Algorithm 
We redesigned the algorithm to take into account 
the depth of a node in addition to its rhetorical 
status. We assign each node a weight, computed 
by adding the weight of the node?s parent and the 
penalty score of the rhetorical relation, which is 
(inversely) related to its importance score. Pen-
alty scores range from 1 to 6, in increments of 1: 
A nucleus has a score of 1 to take the tree depth 
into account, high importance relations have a 
score of 2, and low importance relations have a 
score of 6. In a discourse tree, a child node is 
always heavier than its parent. The larger the 
weight, the less important the node is to the 
overall comunicative goal. The system orders the 
nodes by their weight, and the heaviest nodes are 
dropped first. Thus, nodes deeper in the tree and 
linked by discourse relations with lower 
importance get removed first. Nodes are dropped 
one by one, until the top level node has an 
amount of content that satisfies the space 
constraint. This provides finer control over the 
amount of realised content and avoids the 
limitation of the first algorithm. 
 
Figure 3. Ordered list of (satellite) cluster nodes 
Sometimes, a discourse structure contains 
parallel sub-structures (e.g., bulletted points) 
that, if pruned unevenly, result in unbalanced text 
that seems odd. In such cases, the discourse 
structure typically contains several sub-trees with 
the same structure. In SciFly, such parallel 
structures occur when describing a list of 
projects. These are generated during discourse 
planning by a plan containing a foreach 
statement, e.g., (foreach project in project-list 
(describe project)). To address this situation, the 
system annotates all sub-structures issued from 
such a foreach statement. When constructing the 
ordered list of satellites, nodes at the same depth 
in the sub-structures are clustered together, as 
shown in Figure 3, taking into account their 
relationship to the nucleus. When dropping 
81
nodes, the whole cluster is deleted toegether, 
rather than node by node. So, in Figure 3, the 
whole cluster of weight 10 is dropped first, then 
the cluster of weight 8, etc. This prevents one 
sub-structure from being pruned more than its 
sibling structures and ensures the resulting 
brochure is balanced.  
4 Evaluation 
We evaluated the algorithms to assess their 
comparative effectiveness, based on a test set of 
1507 automatically generated brochures about 
randomly selected topics. We observed that 
82.5% of the brochures generated with the 
enhanced algorithm filled over 96% of the 
available space (leaving at most 8 lines of empty 
space), compared to 29% of brochures generated 
with the simple algorithm. In addition, 96.5% of 
the brochures generated with the new algorithm 
filled at least 90% of the space, compared with 
44.5% of brochures with the simple algorithm.  
We also found that 75% of brochures included 
more content using the enhanced algorithm (an 
average of 32 additional lines), but 12% of the 
brochures contained less content. We examined 
the latter in detail and found that, for these cases, 
the difference was on average 4 lines, and that 
the reduction was due to our treatment of parallel 
discourse structures, thus representing a desirable 
loss of content to create balanced brochures. 
We also performed a user evaluation to verify 
that the improvement in space usage had not de-
creased users? satisfaction. We asked users to 
compare pairs of brochures (simple algorithm vs. 
enhanced algorithm), indicating their preference 
if any. Seventeen users participated in the 
evaluation and were presented with seven pairs 
of brochures. To control any order effect, the 
pairs were randomly presented from user to user, 
and, in each pair, each brochure was randomly 
assigned a left-right configuration. Participants 
mostly preferred the brochures from the en-
hanced algorithm, or found the brochures equiva-
lent, thus showing that our more effective use of 
space had not decreased users? satisfaction.   
Overall, our results show that our enhanced 
algorithm is close to optimal in terms of 
conveying a message appropriately while filling 
up the space and producing a coherent and 
balanced text.  
5 Conclusions 
Reasoning about how much to generate when 
space is limited presents an important challenge 
for generation systems. Most systems either 
control their generation process to avoid 
producing large amounts of text at the onset, or 
control the generation at the sentence level. In 
our application, we cannot resort to any of these 
approaches as we generate text reusing existing 
text fragments and need to produce one discourse 
tree with all the appropriate available content and 
then select what to realise to output on several 
delivery channels. To satisfy space constraints, 
we implemented and tested two algorithms that 
embody the notions of nuclearity and importance 
of information to decide which content to keep 
and which to withhold. Our approach produces 
documents that fill most of the available space 
while maintaining users? satisfaction. 
Acknowledgements 
We thank M. Raji for her work on the user ex-
periment, the members of our group and K. 
Vander Linden for their input, and everyone who 
participated in our evaluation. 
References 
John Bateman, T. Kamps, K. K. Reichenberger, K. 
and J. Kleinz. 2001. Constructive text, diagram and 
layout generation for information presentation: the 
DArt_bio system. Computational Linguistics, 27 
(3): 409?449. 
William C. Mann and Sandra A. Thompson. 1988. 
Rhetorical Structure Theory: Toward a functional 
theory of text organisation. Text 8(3):243?281. 
Daniel Marcu. 1998. To build text summaries of high 
quality, nuclearity is not sufficient. In Working 
Notes of the AAAI-98 Spring Symposium on Intelli-
gent Text Summarization, Stanford, CA, 1?8. 
Johanna D. Moore and C?cile L. Paris. 1993. Planning 
Text for Advisory Dialogues: Capturing Intentional 
and Rhetorical Information. Computational 
Linguistics, 19 (4):651?694, Cambridge, MA. 
Donia R. Scott and Clarisse S. de Souza. 1990. 
Getting the message across in RST-based text 
generation. In Dale, Mellish & Zock (eds). Current 
Research in NLG. London: Academic Press. 119?
128. 
Mick O?Donnell (1997). Variable Length On-Line 
Document Generation. Proceedings of EWNLG. 
Gerhard-Mercator University, Duisburg, Germany.  
Karen Spark Jones (1993). What might be in a 
summary? Information Retrieval 93: Von der 
Modellierung zur Anwendung. (Ed: Knorz, Krause 
and Womse-Hacker), Konstanz: Universitatsverlag 
Konstanz, 9?26. 
82
Proceedings of the Fourth International Natural Language Generation Conference, pages 127?129,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Evaluations of NLG Systems: common corpus and tasks or common 
dimensions and metrics?   
C?cile Paris, Nathalie Colineau and Ross Wilkinson 
CSIRO ICT Centre 
Locked Bag 17, North Ryde 
NSW 1670, Australia 
{Cecile.Paris, Nathalie.Colineau, Ross.Wilkinson}@csiro.au 
 
  
 
Abstract 
In this position paper, we argue that a 
common task and corpus are not the only 
ways to evaluate Natural Language Gen-
eration (NLG) systems. It might be, in 
fact, too narrow a view on evaluation and 
thus not be the best way to evaluate these 
systems.  The aim of a common task and 
corpus is to allow for a comparative 
evaluation of systems, looking at the sys-
tems? performances. It is thus a ?system-
oriented? view of evaluation. We argue 
here that, if we are to take a system ori-
ented view of evaluation, the community 
might be better served by enlarging the 
view of evaluation, defining common 
dimensions and metrics to evaluate sys-
tems and approaches. We also argue that 
end-user (or usability) evaluations form 
another important aspect of a system?s 
evaluation and should not be forgotten. 
1 Introduction 
For this special session, a specific question was 
asked: what would a shared task and shared cor-
pus be that would enable us to perform compara-
tive evaluations of alternative techniques in natu-
ral language generation (NLG)? In this position 
paper, we question the appropriateness of this 
specific question and suggest that the community 
might be better served by (1) looking at a differ-
ent question: what are the dimensions and met-
rics that would allow us to compare various 
techniques and systems and (2) not forgetting but 
encouraging usability evaluations of specific ap-
plications. 
The purpose of defining a shared task and a 
shared corpus is to compare the performance of 
various systems. It is thus a system-oriented view 
of evaluation, as opposed to an end-user oriented 
(or usability) view of evaluation.  It is, however, 
potentially a narrow view of a system-oriented 
evaluation, as it looks at the performance of an 
NLG system within a very specific context ? thus 
essentially looking at the performance of a spe-
cific application.  We argue here that (1), even if 
we take a system-oriented view of evaluation, the 
evaluation of NLG systems should not be limited 
to their performance in a specific context but 
should take other system?s characteristics into 
account, and that (2) end-user evaluations are 
crucial.  
2 Enlarging the view of system-oriented 
evaluations 
The comparison of NLG systems should not 
be limited to a particular task in a specific con-
text. Most systems are designed for specific ap-
plications in specific domains and tend to be 
tuned for these applications.  Evaluating them in 
a context of a specific common evaluation task 
might de-contextualise them and might encour-
age fine-tuning for this task, which might not be 
useful in general. Furthermore, the evaluation of 
a system should not be limited to its performance 
in a specific context but should address charac-
teristics such as: 
? Cost of building (time and effort); 
? Ease of extension, maintainability and cus-
tomisability to handle new requirements 
(time, effort and expertise required); 
? Cost of porting to a new domain or applica-
tion (time, effort and expertise required);  
? Cost of data capture if required (how expen-
sive, expertise required); 
? Coverage issues (users, tasks, dimensions of 
context; and 
? Ease of integration with other software. 
These dimensions are important if we want the 
technology to be adopted and if we want poten-
127
tial users of the technology to be able to make an 
informed choice as to what approach to choose 
when.  
Most NLG systems are built around a specific 
application. Using them in the context of a dif-
ferent application or domain might be difficult. 
While one can argue that basic techniques do not 
differ from one application to another, the cost of 
the modifications required and the expertise and 
skills needed may not be worth the trouble. It 
may be simply cheaper and more convenient to 
rebuild everything. However, firstly, this might 
not be an option, and, secondly, it may increase 
the cost of using an NLG approach to such an 
extent as to make it unaffordable. In addition, 
applications evolve over time and often require a 
quick deployment. It is thus increasingly desir-
able to be able to change (update) an application, 
enabling it to respond appropriately to the new 
situations which it must now handle: this may 
require the ability to handle new situations (e.g., 
generate new texts) or the ability to respond dif-
ferently than originally envisaged to known 
situations. This is important for at least two rea-
sons: 
(1) We are designers not domain experts. 
Although we usually carry out a do-
main/corpus/task analysis beforehand to acquire 
the domain knowledge and understand the users? 
needs in terms of the text to be generated, it is 
almost impossible to become a domain expert 
and know what is the most appropriate in each 
situation. Thus, the design of a specific applica-
tion should allow the experts to take on control 
and ensure the application is configured appro-
priately. This imposes the additional constraint 
that an application should be maintainable di-
rectly by a requirement specialist, an author, ex-
pert or potentially the reader/listener; 
(2) Situations are dynamic ? what is satis-
factory today may be unsatisfactory tomorrow. 
We must be prepared to take on board new re-
quirements as they come in. 
These requirements, of course, come at a cost. 
With this in mind, then, we believe that there is 
another side to system-oriented evaluation which 
we, as designers of NLG systems, need to con-
sider: the ease or cost of developing flexible ap-
plications that can be easily configured and 
maintained to meet changing requirements.  As a 
start towards this goal, we attempted to look 
more precisely at one of the characteristics men-
tioned above, the cost of maintaining and extend-
ing an application, attempting to understand what 
we should take into account to evaluate a system 
on that dimension. We believe asking the follow-
ing questions might be useful. When there are 
new requirements: 
(1) What changes are needed and do the 
modifications require the development of new 
resources, the implementation of additional func-
tionality to the underlying architecture, or both? 
(2) Who can do it and what is the expertise 
required? ? NLG systems are now quite complex 
and require a lot of expertise that may be shared 
among several individuals (e.g., software engi-
neering, computational linguistics, domain ex-
pertise, etc.). 
(3) How hard it is? ? How much effort and 
time would be required to modify/update the sys-
tem to the new requirements? 
In asking these questions, we believe it is also 
useful to decouple a specific system and its un-
derlying architecture, and ask the appropriate 
questions to both. 
3 Usability Evaluations of NLG Systems 
When talking about evaluation of NLG systems, 
we should also remember that usability 
evaluations are crucial, as they can confirm the 
usefulness of a system for its purpose and look at 
the impact of the generated text on its intended 
audience. There has been an increasing number 
of such evaluations ? e.g., (Reiter et al, 2001, 
Paris et al, 2001, Colineau et al, 2002, 
Kushniruk et al, 2002, Elhadad et al, 2005) ? 
and we should continue to encourage them as 
well as develop and share methodologies (and 
pitfalls) for performing these evaluations. It is 
interesting, in fact, to note that communities that 
have emphasized common task and corpus 
evaluations, such as the IR community, are now 
turning their attention to stakeholder-based 
evaluations such as task-based evaluations. In 
looking at ways to evaluate NLG systems, we 
might again enlarge our view beyond 
reader/listener-oriented usability evaluations, as 
readers are not the only persons potentially 
affected by our technology. When doing our 
evaluations, then, we must also consider other 
parties. Considering NLG systems as information 
systems, we might consider the following 
stakeholders beyond the reader: 
? The creators of the information: for some 
applications, this may refer to the person 
creating the resources or the information re-
quired for the NLG system. This might be, 
for example, the people writing the frag-
ments of text that will be later assembled 
128
automatically. Or it might include the person 
who will author the discourse rules or the 
templates required. With respect to these 
people, we might ask questions such as: 
?Does employing this NLG system/approach 
save them time??, ?Is it easy for them to up-
date the information??1 
? The ?owners? of the information. We refer 
here to the organisation choosing to employ 
an NLG system. Possible questions here 
might be: ?Does the automatically generated 
text achieve its purpose with respect to the 
organisation??, ?Can the organisation convey 
similar messages with the automated system? 
(e.g., branding issues). 
4 Discussion 
In this short position paper, we have argued that 
we need to enlarge our view of evaluation to en-
compass both usability evaluation (and include 
users beyond readers/listeners) and system-
oriented evaluations. While we recognise that it 
is crucial to have ways to compare systems and 
approaches (the main advantage of having a 
common corpus and task), we suggest that we 
should look for ways to enable these compari-
sons without narrowing our view on evaluation 
and de-contextualising the systems under consid-
eration. We have presented some possible di-
mensions on which approaches and systems 
could be evaluated. While we understand how to 
perform usability evaluations, we believe that an 
important question is whether it is possible to 
agree on dimensions for system-oriented evalua-
tions and on ?metrics? for these dimensions, to 
allow us to evaluate the different applications 
and approaches, and allow potential users of the 
technology to choose the appropriate one for 
their needs. In our own work, we exploit an NLG 
architecture to develop adaptive hypermedia ap-
plications (Paris et al, 2004), and some of our 
goals (Colineau et al, 2006) are to: 
? Articulate a comprehensive framework for 
the evaluation of approaches to building 
tailored information delivery systems and 
specific applications built using these ap-
proaches. 
? Identify how an application or an ap-
proach measures along some dimensions 
                                                 
1 We realise that, for some NLG applications, there 
might be no authors if all the data exploited by the 
system comes from underlying existing sources, e.g., 
weather or stock data or existing textual resources. 
(in particular for system-oriented evalua-
tion). 
We believe these are equally important for the 
evaluation of NLG systems. 
Acknowledgements 
We would like to thank the reviewers of the 
paper for their useful comments.  
 References 
Colineau, N., Paris, C. & Vander Linden, K. 2002. An 
Evaluation of Procedural Instructional Text.  In the 
Proceedings of the International Natural Language 
Generation Conference (INLG) 2002, NY. 
Colineau, N., Paris, C. & Wilkinson, R. 2006.  To-
wards Measuring the Cost of Changing Adaptive 
Hypermedia Systems.   In Proceedings of the In-
ternational Conference on Adaptive Hypermedia 
and Adaptive Web-based Systems (AH2006). 259-
263, Dublin, Ireland.  LNCS 4018. 
Elhadad, N. McKeown, K. Kaufman, D. & Jordan, D. 
2005. Facilitating physicians' access to information 
via tailored text summarization. In AMIA Annual 
Symposium, 2005, Washington DC. 
Kushniruk, A., Kan, MY, McKeown, K., Klavans, J., 
Jordan, D., LaFlamme, M. & Patel, V. 2002. Us-
ability evaluation of an experimental text summari-
zation system and three search engines: Implica-
tions for the reengineering of health care interfaces. 
In Proceedings of the American Medical Informat-
ics Association Annual Symposium (AMIA 2002). 
Paris, C., Wan, S., Wilkinson, R. & Wu, M. 2001. 
Generating Personalised Travel Guides? And who 
wants them? In Proceedings of the 2001 Interna-
tional Conference on User Modelling (UM?01), 
Sondhofen, Germany. 
Paris, C., Wu, M., Vander Linden, K., Post, M. & Lu, 
S. 2004. Myriad: An Architecture for Contextual-
ised Information Retrieval and Delivery. In Pro-
ceedings of the International Conference on Adap-
tive Hypermedia and Adaptive Web-based Systems 
(AH2004). 205-214, The Netherlands. 
Reiter, E., Robertson, R., Lennox A. S. & Osman, L. 
(2001). Using a randomised controlled clinical trial 
to evaluate an NLG system. In Proceedings of 
ACL'01, Toulouse, France, 434-441. 
129
