Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 134?137,
Sydney, July 2006. c?2006 Association for Computational Linguistics
On Closed Task of Chinese Word Segmentation: An Improved CRF 
Model Coupled with Character Clustering and  
Automatically Generated Template Matching 
Richard Tzong-Han Tsai, Hsieh-Chuan Hung, Cheng-Lung Sung,  
Hong-Jie Dai, and Wen-Lian Hsu 
Intelligent Agent Systems Lab 
Institute of Information Science, Academia Sinica 
No. 128, Sec. 2, Academia Rd., 115 Nankang, Taipei, Taiwan, R.O.C. 
{thtsai,yabt,clsung,hongjie,hsu}@iis.sinica.edu.tw 
 
  
 
Abstract 
This paper addresses two major prob-
lems in closed task of Chinese word 
segmentation (CWS): tagging sentences 
interspersed with non-Chinese words, 
and long named entity (NE) identifica-
tion. To resolve the former, we apply K-
means clustering to identify non-Chinese 
characters, and then adopt a two-tagger 
architecture: one for Chinese text and the 
other for non-Chinese text. For the latter 
problem, we apply postprocessing to our 
CWS output using automatically gener-
ated templates. The experiment results 
show that, when non-Chinese characters 
are sparse in the training corpus, our 
two-tagger method significantly im-
proves the segmentation of sentences 
containing non-Chinese words. Identifi-
cation of long NEs and long words is 
also enhanced by template-based post-
processing. In the closed task of 
SIGHAN 2006 CWS, our system 
achieved F-scores of 0.957, 0.972, and 
0.955 on the CKIP, CTU, and MSR cor-
pora respectively.  
1 Introduction 
Unlike Western languages, Chinese does not 
have explicit word delimiters. Therefore, word 
segmentation (CWS) is essential for Chinese 
text processing or indexing. There are two main 
problems in the closed CWS task. The first is to 
identify and segment non-Chinese word se-
quences in Chinese documents, especially in a 
closed task (described later). A good CWS sys-
tem should be able to handle Chinese texts pep-
pered with non-Chinese words or phrases. Since 
non-Chinese language morphologies are quite 
different from that of Chinese, our approach 
must depend on how many non-Chinese words 
appear, whether they are connected with each 
other, and whether they are interleaved with 
Chinese words. If we can distinguish non-
Chinese characters automatically and apply dif-
ferent strategies, the segmentation performance 
can be improved. The second problem in closed 
CWS is to correctly identify longer NEs. Most 
ML-based CWS systems use a five-character 
context window to determine the current charac-
ter?s tag. In the majority of cases, given the con-
straints of computational resources, this com-
promise is acceptable. However, limited by the 
window size, these systems often handle long 
words poorly. 
In this paper, our goal is to construct a general 
CWS system that can deal with the above prob-
lems. We adopt CRF as our ML model. 
2 Chinese Word Segmentation System 
2.1 Conditional Random Fields 
Conditional random fields (CRFs) are undirected 
graphical models trained to maximize a condi-
tional probability (Lafferty et al, 2001). A lin-
ear-chain CRF with parameters ?={?1, ?2, ?} 
defines a conditional probability for a state se-
quence y = y1 ?yT , given that an input sequence 
x = x1 ?xT  is 
???
????
?= ??
=
??
T
t k
ttkk tyyfZ
P
1
1 ),,,(exp
1
)|( xxy
x
?  ,(1)                          
where Zx is the normalization factor that makes 
the probability of all state sequences sum to one; 
fk(yt-1, yt, x, t) is often a binary-valued feature 
function and ?k is its weight. The feature 
134
functions can measure any aspect of a state 
transition, yt-1?yt, and the entire observation 
sequence, x, centered at the current time step, t. 
For example, one feature function might have 
the value 1 when yt-1 is the state B, yt is the state 
I, and tx  is the character ???. 
2.2 Character Clustering 
In many cases, Chinese sentences may be inter-
spersed with non-Chinese words. In a closed 
task, there is no way of knowing how many lan-
guages there are in a given text. Our solution is 
to apply a clustering algorithm to find homoge-
neous characters belonging to the same character 
clusters. One general rule we adopted is that a 
language?s characters tend to appear together in 
tokens. In addition, character clusters exhibit 
certain distinct properties. The first property is 
that the order of characters in some pairs can be 
interchanged. This is referred to as exchange-
ability. The second property is that some charac-
ters, such as lowercase characters, can appear in 
any position of a word; while others, such as 
uppercase characters, cannot. This is referred to 
as location independence. According to the gen-
eral rule, we can calculate the pairing frequency 
of characters in tokens by checking all tokens in 
the corpus. Assuming the alphabet is ?, we first 
need to represent each character as a |?|-
dimensional vector. For each character ci, we use 
vj to represent its j-dimension value, which is 
calculated as follows: 
r
jiijj ffv )],)[min(1( ?? ?+= ?             (2), 
where fij denotes the frequency with which ci and 
cj appear in the same word when ci?s position 
precedes that of cj. We take the minimum value 
of fij and fji because even when ci and cj have a 
high co-occurrence frequency, if either fij or fji is 
low, then one order does not occur often, so vj?s 
value will be low. We use two parameters to 
normalize vj within the range 0 to 1; ? is used to 
enlarge the gap between non-zero and zero fre-
quencies, and ? is used to weaken the influence 
of very high frequencies. 
Next, we apply the K-means algorithm to 
generate candidate cluster sets composed of K 
clusters (Hartigan et al, 1979). Different K?s, 
??s, and ??s are used to generate possible charac-
ter cluster sets. Our K-means algorithm uses the 
cosine distance. 
After obtaining the K clusters, we need to se-
lect the N1 best character clusters among them. 
Assuming the angle between the cluster centroid 
vector and (1, 1, ... , 1) is ?, the cluster with the 
largest cosine ? will be removed. This is because 
characters whose co-occurrence frequencies are 
nearly all zero will be transformed into vectors 
very close to (?, ?, ... , ?); thus, their centroids 
will also be very close to (?, ?, ... , ?), leading to 
unreasonable clustering results. 
After removing these two types of clusters, 
for each character c in a cluster M, we calculate 
the inverse relative distance (IRDist) of c using 
(3): 
??
?
?
?
??
?
?
?
=
?
),cos(
),(cos
log)IRDist(
mc
mc
c
i
i   ,            (3) 
where mi stands for the centroid of cluster Mi, 
and m stands for the centroid of M.  
We then calculate the average inverse dis-
tance for each cluster M. The N1 best clusters are 
selected from the original K clusters.   
The above K-means clustering and character 
cluster selection steps are executed iteratively 
for each cluster set generated from K-means 
clustering with different K?s, ??s, and ??s.  
After selecting the N1 best clusters for each 
cluster set, we pool and rank them according to 
their inner ratios. Each cluster?s inner ratio is 
calculated by the following formula: 
?
?
?
?
= ?
ji
ji
cc
ji
Mcc
ji
cc
cc
M
,
,
),occurence(co
 ),occurence(co
)inner(
 ,   (4) 
where co-occurrence(ci, cj) denotes the fre-
quency with which characters  ci and cj co-occur 
in the same word. 
To ensure that we select a balanced mix of 
clusters, for each character in an incoming clus-
ter M, we use Algorithm 1 to check if the fre-
quency of each character in C?M is greater 
than a threshold ?. 
 
Algorithm 1 Balanced Cluster Selection 
Input: A set of character clusters P={M1 ,  . . .  , MK} 
          Number of selections N2, 
Output: A set of clusters Q={ '1M  ,  . . .  , 
'
2N
M }. 
 
1: C={} 
2: sort the clusters in P by their inner ratios; 
3: while |C|<=N2 do 
4:     pick the cluster M that has highest inner ratio; 
5:     for each character c in M do 
6:          if the frequency of c in C?M is over thresh-
old ? 
7:                 P?P?M; 
8:                 continue; 
9 :        else 
135
10:               C?C?M; 
11:               P?P?M; 
12:        end; 
13:   end; 
14: end 
 
The above algorithm yields the best N1 clus-
ters in terms of exchangeability. Next, we exe-
cute the above procedures again to select the 
best  N2 clusters based on their location inde-
pendence and exchangeability. However, for 
each character ci, we use vj to denote the value of 
its j-th dimension. We calculate vj as follows: 
r
jijiijijj ffffv )]',,',)[min(1(
' ?? ?+= ,      (5) 
where ijf  stands for the frequency with which ci 
and cj appear in the same word when ci is the 
first character; and f?ij stands for the frequency 
with which ci and cj co-occur in the same word 
when ci precedes cj  but not in the first position. 
We choose the minimum value from ijf , f?ij, jif , 
and f?ji  because if ci and cj both appear in the 
first position of a word and their order is ex-
changeable, the four frequency values, including 
the minimum value, will all be large enough. 
Type Cluster Inner (K, ?, ?) 
,.0123456789 0.94 (10, 0.60, 0.16)
EX 
 
-/ABCDEFGHIKLMNOPR 
STUVWabcdefghiklmnoprst 
uvwxy 
0.93 (10, 0.70, 0.16)
??ABCDEFGHIKLMNO 
PRSTUVWabcdefghiklmno 
prstvwxy 
0.84 (10, 0.50, 0.25)EL 
?????????? 0.76 (10, 0.50, 0.26)
Table 1. Clustering Results of the CTU corpus 
Our next goal is to create the best hybrid of 
the above two cluster sets. The set selected for 
exchangeability is referred to as the EX set, 
while the set selected for both exchangeability 
and location independence is referred to as the 
EL set. We create a development set and use the 
best first strategy to build the optimal cluster set 
from EX?EL. The EX and EL for the CTU 
corpus are shown in Table 1. 
2.3 Handling Non-Chinese Words 
Non-Chinese characters suffer from a serious 
data sparseness problem, since their frequencies 
are much lower than those of Chinese characters. 
In bigrams containing at least one non-Chinese 
character (referred as non-Chinese bigrams), the 
problem is more serious. Take the phrase ???  
20?? (about 20 years old) for example. ?2? is 
usually predicted as I, (i.e., ???? is connected 
with ?2?) resulting in incorrect segmentation, 
because the frequency of ?2? in the I class is 
much higher than that of ?2? in the B class, even 
though the feature C-2C-1=???? has a high 
weight for assigning ?2? to the B class. 
Traditional approaches to CWS only use one 
general tagger (referred as the G tagger) for 
segmentation. In our system, we use two CWS 
taggers. One is a general tagger, similar to the 
traditional approaches; the other is a specialized 
tagger designed to deal with non-Chinese words. 
We refer to the composite tagger (the general 
tagger plus the specialized tagger) as the GS 
tagger. 
Here, we refer to all characters in the selected 
clusters as non-Chinese characters. In the devel-
opment stage, the best-first feature selector de-
termines which clusters will be used. Then, we 
convert each sentence in the training data and 
test data into a normalized sentence. Each non-
Chinese character c is replaced by a cluster rep-
resentative symbol ?M, where c is in the cluster 
M. We refer to the string composed of all ?M as 
F. If the length of F is more than that of W, it 
will be shortened to W. The normalized sentence 
is then placed in one file, and the non-Chinese 
character sequence is placed in another. Next, 
we use the normalized training and test file for 
the general tagger, and the non-Chinese se-
quence training and test file for the specialized 
tagger. Finally, the results of these two taggers 
are combined. 
The advantage of this approach is that it re-
solves the data sparseness problem in non-
Chinese bigrams. Consider the previous example 
in which ? stands for the numeral cluster. Since 
there is a phrase ??? 8??  in the training data, 
C-1C0= ?? 8? is still an unknown bigram using 
the G tagger. By using the GS tagger, however, 
??? 20?? and ??? 8?? will be converted 
as ??? ???? and ??? ???, respectively. 
Therefore, the bigram feature C-1C0=?? ?? is no 
longer unknown. Also, since ? in ?? ?? is 
tagged as B, (i.e., ??? and ??? are separated), 
??? and ??? will be separated in  ??? ????. 
2.4 Generating and Applying Templates 
Template Generation 
We first extract all possible word candidates 
from the training set. Given a minimum word 
length L, we extract all words whose length is 
greater than or equal to L, after which we align 
all word pairs. For each pair, if more than fifty 
136
percent of the characters are identical, a template 
will be generated to match both words in the pair. 
Template Filtering 
We have two criteria for filtering the extracted 
templates. First, we test the matching accuracy 
of each template t on the development set. This 
is calculated by the following formula: 
strings matched all of #
separators no with strings matched of #
)( =tA . 
In our system, templates whose accuracy is 
lower than the threshold ?1 are discarded. For the 
remaining templates, we apply two different 
strategies. According to our observations of the 
development set, most templates whose accu-
racy is less than ?2 are ineffective. To refine such 
templates, we employ the character class infor-
mation generated by character clustering to im-
pose a class limitation on certain template slots. 
This regulates the potential input and improves 
the precision. Consider a template with one or 
more wildcard slots. If any string matched with 
these wildcard slots contains characters in dif-
ferent clusters, this template is also discarded.  
Template-Based Post-Processing (TBPP) 
After the generated templates have been filtered, 
they are used to match our CWS output and 
check if the matched tokens can be combined 
into complete words. If a template?s accuracy is 
greater than ?2, then all separators within the 
matched strings will be eliminated; otherwise, 
for a template t with accuracy between ?1 and ?2, 
we eliminate all separators in its matched string 
if no substring matched with t?s wildcard slots 
contains characters in different clusters. Resul-
tant words of less than three characters in length 
are discarded because CRF performs well with 
such words. 
3 Experiment 
3.1 Dataset 
We use the three larger corpora in SIGHAN 
Bakeoff 2006: a Simplified Chinese corpus pro-
vided by Microsoft Research Beijing, and two 
Traditional Chinese corpora provided by Aca-
demia Sinica in Taiwan and the City University 
of Hong Kong respectively. Details of each cor-
pus are listed in Table 2. 
Training Size Test SizeCorpus 
Types Words Types Words
CKIP 141 K 5.45 M 19 K 122 K
City University (CTU) 69 K 1.46 M 9 K 41 K
Microsoft Research (MSR) 88 K 2.37 M 13 K 107 K
Table 2. Corpora Information 
3.2 Results 
Table 3 lists the best combination of n-gram fea-
tures used in the G tagger. 
Uni-gram Bigram  
C-2, C-1, C0, C1 C-2C-1, C-1C0, C0C1, C-3C-1, C-2C0, C-1C1
Table 3. Best Combination of N-gram Features 
Table 4 compares the baseline G tagger and the 
enhanced GST tagger. We observe that the GST 
tagger outperforms the G tagger on all three cor-
pora. 
Conf R P F ROOV RIV 
CKIP-g 0.958 0.949 0.954 0.690 0.969 
CKIP-gst 0.961 0.953 0.957 0.658 0.974 
CTU-g 0.966 0.967 0.966 0.786 0.973 
CTU-gst 0.973 0.972 0.972 0.787 0.981 
MSR-g 0.949 0.957 0.953 0.673 0.959 
MSR-gst 0.953 0.956 0.955 0.574 0.966 
Table 4 Performance Comparison of the G Tag-
ger and the GST Tagger  
4 Conclusion  
The contribution of this paper is two fold. First, 
we successfully apply the K-means algorithm to 
character clustering and develop several cluster 
set selection algorithms for our GS tagger. This 
significantly improves the handling of sentences 
containing non-Chinese words as well as the 
overall performance. Second, we develop a post-
processing method that compensates for the 
weakness of ML-based CWS on longer words. 
References 
Hartigan, J. A., & Wong, M. A. (1979). A K-means 
Clustering Algorithm. Applied Statistics, 28, 100-
108. 
Lafferty, J., McCallum, A., & Pereira, F. (2001). 
Conditional Random Fields: Probabilistic Models 
for Segmenting and Labeling Sequence Data. Pa-
per presented at the ICML-01. 
 
 
137
Coling 2010: Poster Volume, pages 223?231,
Beijing, August 2010
Abstract 
Global ranking, a new information re-
trieval (IR) technology, uses a ranking 
model for cases in which there exist re-
lationships between the objects to be 
ranked. In the ranking task, the ranking 
model is defined as a function of the 
properties of the objects as well as the 
relations between the objects. Existing 
global ranking approaches address the 
problem by ?learning to rank?. In this 
paper, we propose a global ranking 
framework that solves the problem via 
data fusion. The idea is to take each re-
trieved document as a pseudo-IR sys-
tem. Each document generates a pseu-
do-ranked list by a global function. The 
data fusion algorithm is then adapted to 
generate the final ranked list. Taking a 
biomedical information extraction task, 
namely, interactor normalization task 
(INT), as an example, we explain how 
the problem can be formulated as a 
global ranking problem, and demon-
strate how the proposed fusion-based 
framework outperforms baseline me-
thods. By using the proposed frame-
work, we improve the performance of 
the top 1 INT system by 3.2% using 
the official evaluation metric of the 
BioCreAtIvE challenge. In addition, by 
employing the standard ranking quality 
measure, NDCG, we demonstrate that 
                                                 
* Corresponding author 
the proposed framework can be cas-
caded with different local ranking 
models and improve their ranking re-
sults. 
1 Introduction 
Information Retrieval (IR) involves finding 
documents that are relevant to a given query in 
a large corpus. The task is usually formulated 
as a ranking problem. When a user submits a 
query, the IR system retrieves all documents 
that contain at least one query term, calculates 
a ranking score for each of the documents us-
ing a ranking model, and sorts the documents 
according to the ranking scores. The scores 
represent the relevance, importance, and/or 
diversity of the retrieved documents. Thus, the 
quality of a search engine can be determined 
by the accuracy of the ranking results.  
Recently, a machine learning technology 
called learning to rank has been applied exten-
sively to the task. Several state-of-the-art ma-
chine learning-based ranking algorithms have 
been proposed, e.g., RankSVM and RankNet. 
These algorithms differ substantially in terms 
of the ranking models and optimization tech-
niques employed, but most of them can be re-
garded as ?local ranking? approaches in the 
sense that each model is defined on a single 
document without considering the possible 
relations to other documents to be ranked. In 
many applications, this is only a loose approx-
imation as there is always relational informa-
tion among documents. For example, in some 
cases, users may prefer that two similar docu-
ments have similar relevance scores; even 
Global Ranking via Data Fusion 
Hong-Jie Dai1,2 Po-Ting Lai3 Richard Tzong-Han Tsai3* Wen-Lian Hsu1,2* 
1Department of Computer Science, National Tsing-Hua University,  
2Institute of Information Science, Academia Sinica,  
3Department of Computer Science & Engineering, Yuan Ze University 
hongjie@iis.sinica.edu.tw 
s951416@mail.yzu.edu.tw  
thtsai@saturn.yzu.edu.tw 
hsu@iis.sinica.edu.tw 
223
though one of the documents is not as relevant 
to the given query as the other; this problem is 
similar to Pseudo Relevance Feedback (Kwok, 
1984). In other cases, web pages from the 
same site form a sitemap hierarchy in which a 
parent document should be ranked higher than 
its child documents (referred to as Topic Dis-
tillation at TREC (Chowdhury, 2007)). To util-
ize all available information, more advanced 
ranking algorithms define a ranking model as a 
function of all the documents to be ranked, i.e., 
a global ranking model (Qin et al, 2008a; Qin 
et al, 2008b). 
Unlike conventional ranking and learning to 
rank models, such as BM25 and RankSVM, 
whose ranking functions are defined on a 
query and document pair, global ranking mod-
els utilize both content information and rela-
tion information. Qin et al (2008) proposed 
the first supervised learning framework for the 
global ranking problem. They formulated the 
problem as an optimization problem that in-
volves finding an objective function to minim-
ize the trade-off between local consistence and 
global consistence and implemented it on 
SVM. Subsequently, they defined the global 
ranking problem formally in (Qin et al, 2008) 
and solved it by employing continuous condi-
tional random fields (CRF). 
In this paper, we propose a new framework 
for the global ranking problem. The major dif-
ference between our work and that of Qin et al 
(2008a; 2008b) is that we do not compile a 
feature vector of relational information directly 
to construct a new machine-learned ranking 
model for global ranking. Instead, we use the 
ranking results generated by the original rank-
ing model and then employ an algorithm with 
the relational information to transform the 
global ranking problem into a data fusion prob-
lem; that is also known as a rank aggregate 
problem. The proposed framework is flexible 
and can be cascaded with conventional ranking 
models or learning to rank models. 
The remainder of this paper is organized as 
follows. In Section 2, we present a formal de-
finition of global ranking. In Section 3, we de-
scribe the proposed framework and consider 
three fusion algorithms that can be used with 
our framework. We also explain how the algo-
rithms can be adapted to solve the global rank-
ing problem. In Section 4, we introduce a bio-
medical text mining task called the interactor 
normalization task (INT) (Krallinger et al, 
2009) and show why it should be formulated 
as a global ranking problem. In Section 5, we 
report extensive experiments conducted on the 
INT dataset released by BioCreAtIvE 
(Krallinger et al, 2009). Section 6 contains 
some concluding remarks. 
2 Global Ranking Problem 
The global ranking problem was first defined 
formally by Qin et al (2008). In this paper, we 
propose a new global ranking framework 
based on their definition. Although we devel-
oped the framework independently, we adopt 
Qin et al?s terminology. 
Let   denote a query. In addition, let 
        
      
       
    
   
  denote the docu-
ments retrieved by  , and let      
   
      
       
    
   
  denote the ranking scores 
assigned to the documents. Here,      
represents the number of documents retrieved 
by  . Note that the numbers of documents va-
ries according to different queries. We assume 
that      is determined by a ranking model. 
If a ranking model is defined on a single 
document, i.e., in the form of 
  
        
               , 
it is called a ?local ranking? model.  
Let       
      
          
   
 
 be a set of 
real-value functions defined on   
   
,   
   
, and  
     (                ). The functions 
           
represents the relations between documents. 
Equation 2 is defined according to the re-
quirements of different tasks. For example, for 
the Pseudo Relevance Feedback problem, Qin 
et al (2008) defined Equation 2 as the similari-
ties between any two documents in their CRF-
based model. 
If a ranking model takes all the documents 
as its input and exploits both local and global 
information (Equation 2) in the documents, i.e., 
in the form of 
            , 
it is called a ?global ranking? approach. 
(1) 
(2) 
224
 3 Fusion-based Global Ranking 
Framework 
It is usually difficult to develop a global rank-
ing algorithm that can fully utilize all the local 
and global information in documents to pro-
duce a document rank and also consider the 
score ranks. One example of a global ranking 
algorithm that satisfied these criteria is the one 
proposed in (Qin et al, 2008) in which the 
modified CRF algorithm handles context (local) 
features and relational (global) features in the 
documents. Without solving a ranking problem 
directly, however, the modified CRF algorithm 
is more like a regression algorithm since it op-
timizes the CRF parameters in a maximum 
likelihood estimate without considering the 
score ranks. With respect to the ranking feature, 
in this section, we describe our framework 
based on the idea of data fusion for solving the 
global ranking problem. 
3.1 Framework Description 
The flow chart of the proposed framework is 
illustrated in Figure 1. The first step is the 
same as that of the traditional local ranking 
model. Given a query, the local ranking model 
  
   
 defined in Equation 1 is used to calculate 
the ranking score for each document, and re-
turn a document list sorted according to the 
local scores.  
The second step transforms the global rank-
ing problem into a data fusion problem. Our 
idea is to take each retrieved document as a 
pseudo-IR system, and the pseudo-ranking 
model,    
   
, used by each system is the func-
tion defined in Equation 2. For each pseudo-IR 
system,   
   
, the pseudo-ranking model for a 
document   
   
 is defined as follows:  
 
   
        
         
      
           
          . 
There are totally      pseudo-IR systems, 
which generate      pseudo-ranked lists. As a 
result, the global ranking problem is trans-
formed into a data fusion problem, that is to 
aggregate the pseudo-ranked lists. Figure 2 
shows the steps of the transformation algo-
rithm. 
The final step is to adapt fusion algorithms 
to aggregate the pseudo-ranked lists. A canoni-
cal data fusion task is called meta-search 
(Aslam and Montague, 2001; Fox and Shaw, 
1994; Lee, 1997; Nuray and Can, 2006), which 
aggregates Web search query results from sev-
eral engines into a more accurate ranking. The 
origin of research on data fusion can be traced 
back to (Borda, 1781). In recent years, the 
process has been used in many new applica-
tions, including aggregating data from micro-
array experiments to discover cancer-related 
genes (Pihura et al, 2008), integration of re-
sults from multiple mRNA studies (Lin and 
Ding, 2008), and similarity searches across 
datasets and information merging (Adler et al, 
2009; Zhao et al, 2010).  
Liu et al (2007) classified data fusion tech-
nologies into two categories: order-based fu-
sion and score-based fusion. In the first catego-
ry, the orders of the entities in individual rank-
ing lists are used by the fusion algorithm. In 
the second category, the entities in individual 
ranking lists are assigned scores and the fusion 
algorithm uses the scores. In the following 
sub-sections, we adapt three fusion algorithms 
Step 3
Step 2
Step 1
Local Ranking Model
Document Set
Query
Local Ranked 
Document List
Transformation 
Algorithm
Pseudo-IR 
System
1
Pseudo-IR 
System
2
Pseudo-IR 
System
n
(q)
...
Fusion 
Algorithm
Pseudo-
Ranked List
1
Pseudo-
Ranked List
2
Pseudo-
Ranked List
n
(q)
...
Global Ranked 
Document List
Pseudo-
Ranking Model
1
Pseudo-Ranking 
Model
2
Pseudo-Ranking 
Model
n
(q)
 
Figure 1. The Proposed Framework for 
Global Ranking. 
(3) 
225
for the proposed framework. The first is the 
Borda-fuse model (Aslam and Montague, 
2001), an order-based fusion approach based 
on an optimal voting procedure. The second is 
a linear combination (LC) model (Vogt and 
Cottrell, 1999), which is a score-based fusion 
approach. 
3.2 Borda-fuse 
The Borda-fuse model (Aslam and Montague, 
2001) is based on a political election strategy 
called the Borda Count. For our framework, 
the rationale behind the strategy is as follows. 
Each pseudo-IR system   
   
 is an analogy for 
a voter; and each voter ranks a fixed set of      
documents in order of preference (Equation 3). 
For each voter, the top ranked document is 
given      points, the second ranked document 
is given     -  points, and so on. If some doc-
uments left unranked by the voter, the remain-
ing points are divided equally among the un-
ranked documents. The documents are ranked 
in descending order of the total points. 
In our framework, we implement two Bor-
da-fuse-based models. The first is the modified 
Borda-fuse (MBF) model. In MBF, the number 
of points given for a voter's first and subse-
quent preferences is determined by the number 
of documents they have actually ranked, rather 
than the total number of ranked. Because the 
ranking model,    
   
, used by the pseudo-IR 
system may only retrieve  documents where 
  is smaller than     , we penalize systems 
that do not rank a full document set by reduc-
ing the number of points their vote distributes 
among the documents. In other words, if there 
are ten documents, but the pseudo-IR system 
only retrieves five, then the first document will 
only receive 5 points; the second will receive 4 
points, and so on. 
The second is the weighted Borda-fuse 
(WBF) model. The original Borda-fuse model 
reflects a democratic election in which each 
voter has equal weight. However, in many cas-
es, we prefer some voters because they are 
more reliable. We employ a simple weighting 
scheme that multiplies the points assigned to a 
document determined by system   
   
 by a 
weight 
  
   . 
3.3 LC Model 
The LC model has been used by many IR re-
searchers with varying degrees of success 
(Bartell et al, 1994; Knaus et al, 1995; Vogt 
and Cottrell, 1999; Vogt and Cottrell, 1998). In 
our framework, it is defined as follows. Given 
a query  , a document   
   
, the weights 
                     for  
    individual 
pseudo-IR systems, and jth pseudo-IR sys-
tem?s ranking score     
   
, the LC model cal-
culates the ranking score   of   
   
 against all 
pseudo-IR systems as follows: 
      
            
        
    
This score is then used to rank the documents. 
For example, for two pseudo-IR systems, this 
reduces to: 
          
           
          
   
 
Compared with MBF, Equation 4 requires 
both relevance scores and training data to de-
 function transform (    : the documents retrieved 
with query  ) 
{generate pseudo-ranked lists for     } 
 # a dictionary that maps the pseudo-IR systems to 
# their corresponding pseudo-ranked lists 
1. pseudoRankedLists = {} 
2. for   
   
 in     : 
     # a dictionary that maps the relation score (real 
    # value) to a list of documents. 
3.     relation = {} 
     for   
   
 in     : 
4.         relation[    
      
         ].append(  
   
) 
     # relation.keys() returns all keys stored in the  
 # dictionary relation. The key of relation is the 
 # relation score. 
5.     Sort relation.keys() in decreasing order 
     # a dictionary that maps a new rank to a list of 
    # documents. 
6.     pseudoRankedList = {} 
7.     newRank = 0 
     for score in sorted relation.keys(): 
         # relation[score] returns the document list  
         # corresponding to the given score 
         for doc in relation[score]: 
8.             pseudoRankedList[1+newRank] 
                                          .append(doc) 
9.         newRank = newRank + 1 
10.     pseudoRankedLists [  
   
] = pseudoRankedList 
 return pseudoRankedLists 
Figure 2. The Dependent Ranked List Gen-
eration Algorithm (represented using python 
syntax). 
(4) 
226
termine the weight   given to each pseudo-IR 
system. 
4 Case Study 
In this section, we describe the task examined 
in our study. We also explain how we formu-
late the task as a global ranking problem. The 
experiments results are detailed in Section 5. 
4.1 Interactor Normalization Task 
The interactor normalization task (INT) is a 
complicated text mining task that involves the 
following steps: (1) It recognizes gene men-
tions in a full text article. (2) It maps the rec-
ognized gene mentions to corresponding 
unique database identifiers which is similar to 
the word sense disambiguation task in compu-
tational linguistics. (3) It generates a ranked 
list of the identifiers according to their impor-
tance in the article and their probability of 
playing the interactor role in protein-protein 
interactions (PPIs). Such ranked lists are useful 
for human curators and can speed up PPI data-
base curation. 
Dai et al (2010) won first place in the Bio-
CreAtIvE II.5 INT challenge (Mardis et al, 
2009) by using a SVM-based local ranking 
model in which they treat gene mentions? iden-
tifiers in an article as the document set, and the 
query is a constant string ?interactor?. Based 
on their feature sets and evaluation results, we 
can find that their local ranking model tends to 
rank focus genes higher (Dai et al, 2010). 
However, the primary objective of INT is to 
generate a ranked list of interaction gene iden-
tifiers. According to (Jenssen et al, 2001), co-
mentioned genes are usually related in some 
way. For example, if two gene mentions fre-
quently occur alongside each other in the same 
sentence in an article, they probably have an 
association and influence each other?s rank. 
Take a low-ranked interactor that is only men-
tioned twice in an article as an example. If 
both mentions are next to the highest-ranked 
interactor in the article, then the low-ranked 
interactor?s rank should be boosted significant-
ly. Therefore, the ranking task for each article 
can be formulated as a global ranking problem; 
the global ranking algorithm should consider 
both the local information from Dai et al?s 
model and the global information from the as-
sociations among identifiers. 
4.2 Global Ranking in INT 
Let   be a constant ?interactor.? The identifier 
set generated by an INT system for a full-text 
article is analogous to the document set 
        
      
       
    
   
 . Here      denotes 
the number of identifiers. Note that the number 
of identifiers varies for different articles. Let 
        
      
       
    
   
  denote the ranking 
scores assigned to the identifiers given by a 
local ranking model. In this study, we used the 
INT system and SVM-based local ranking 
model released by (Dai et al, 2010) to gener-
ate the identifier set and ranking scores. 
To obtain the global information, we con-
sider the co-occurrence of identifiers and em-
ploy mutual information (MI) to measure the 
association between two identifiers as follows: 
                     
                      . 
In the above formula, the identifier probabili-
ties       and       are estimated by counting 
the number of occurrences in an article norma-
lized by  , i.e., the number of sentences con-
taining identifiers. The joint probability, 
        , is estimated by the number of times 
   co-occurs with    in a window of   words 
normalized by  . Note that, in practice, other 
advanced approaches can be used to calculate 
the association score. 
For the proposed framework, each identifier 
  
   
 is a pseudo-IR system with MI as its 
pseudo-ranking model    
   
. The identifiers 
that co-occur with   
   
 become candidates on 
  
   
?s pseudo-ranked list. 
5 Experiments 
In the following sub-sections, we introduce the 
dataset used in the experiments, describe the 
evaluation methods, report the results of the 
experiments conducted to compare the perfor-
mance of different methods, and discuss the 
efficiency of the proposed global ranking 
framework. 
227
5.1 Dataset 
We used the BioCreAtIvE II.5 Elsevier corpus 
released by BioCreAtIvE II.5 challenge in the 
experiments. The corpus contains 1,190 full-
text journal articles selected mainly from 
FEBS Letters. Following the same format as 
the BioCreAtIvE II.5 INT challenge, we used 
articles published in 2008 (61 articles) as our 
training set and articles published in 2007 or 
earlier (61 articles) as our test set. 
5.2 A Fusion-based Global Ranking 
Framework for INT 
Before applying the proposed framework, we 
preprocess the articles in the dataset to identify 
all gene mentions, and map them to their cor-
responding identifiers. After preprocessing, 
each full-text article is associated with a list of 
identifiers (Step 1 in Figure 1). The transform 
and fusion algorithm is then applied on each 
article (Steps 2 and 3 in Figure 1). 
To apply the WBF and LC models, we need 
to determine the weight assigned to each pseu-
do-IR system. To obtain the weight, we calcu-
late the precision of each rank of the ranked 
lists generated by Dai et al?s INT system. Fig-
ure 3 shows the precision of ranks 1 to 15 cal-
culated by applying three-fold cross validation 
on the INT training set. We observe that the 
precision declines as the rank increases, which 
implies that the higher ranks predicted by their 
SVM-based local ranking model are more reli-
able than the lower ranks. 
5.3 Evaluation Metrics 
Our evaluations focus on two comparisons: the 
first compares the ranking of the proposed 
framework with the original local ranking 
model by using the area under the curve of the 
interpolated precision/recall (iP/R) curve. This 
is the evaluation metric used in the BioCreA-
tIvE II.5 challenge and is a common way to 
depict the degradation of precision as one tra-
verses the retrieved results by plotting interpo-
lated precision numbers against percentage 
recall. The area under the iP/R function     is 
defined as follows: 
                              
 
   
                  
, 
where   is the total number of correct identifi-
ers and    is the highest interpolated precision 
for the correct identifier   at   , the recall for 
that hit. The interpolated precision    is calcu-
lated for each recall   by taking the highest 
precision at   or any     . 
In the second comparison, we use a standard 
quality measure in IR to estimate the ranking 
performance of local ranking models and the 
proposed framework. We adopt Normalized 
Discounted Cumulative Gain (NDCG) to 
measure the performance. The NDCG score of 
a ranking is computed based on DCG (Dis-
counted Cumulative Gain) as follows: 
             
    
       
 
   , 
where   is the rank position, and            
is the relevance grade of the  th identifier in 
the ranked result set. In our experiment, 
       corresponds to an interaction iden-
tifier, and        corresponds to other iden-
tifiers. NDCG is then computed as follows: 
        
      
       
, 
where      denotes the results of a perfect 
ranking. The NDCG values for all articles are 
averaged to obtain the average performance of 
the proposed framework. 
5.4 INT Test Set Performance 
Figure 4 shows the Area_iPR scores of four 
configurations. In the baseline configuration 
(Local/Rank1), the SVM-based local ranking 
model released by Dai et al is employed. In 
the configuration Global+LC, Global+MBF, 
and Global+WBF, the proposed global ranking 
framework is cascaded with the local ranking 
model and with three data fusion models: the 
LC model, the modified Borda-fuse (MBF) 
model, and the weighted Borda-fuse model. 
The figure also shows the Area_iPR scores of 
 
Figure 3. Precision of Different Ranks. 
0
0.2
0.4
0.6
0.8
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15
P
re
c
is
io
n
Rank
228
the top three teams and the average Area_iPR 
score of all BioCreAtIvE II.5 INT participants 
(Average).  
The results show that under the global rank-
ing framework, Area_iPR performance is im-
proved in addition to Global+MBF. The high-
est Area_iPR (Global+LC: 46.7%) is 3.2% 
higher than the Rank 1 score in the BioCreA-
tIvE II.5 INT challenge. According to our 
analysis, before global ranking, identifiers 
whose feature values rarely appear in the train-
ing set are often ranked incorrectly because 
their feature values are under-estimated by the 
ranking model. However, if the identifiers co-
occur with higher-ranked identifiers whose 
feature values appear frequently, the proposed 
framework is very likely to increase their ranks. 
This results in an improved Area_iPR score. 
5.5 Global Ranking Performance 
 
To illustrate the effectiveness of the proposed 
global ranking framework and assess its per-
formance when it is cascaded with other con-
ventional ranking models, we implement a 
simple term frequency-based ranking function, 
which is based on the identifier frequency in 
an article as another local ranking model. If 
two or more identifiers have the same frequen-
cy, two heuristic rules are employed sequen-
tially to rank them: (1) the identifier with the 
highest frequency in the Results section of the 
article, and (2) the identifier mentioned first in 
the article.  
Table 1 shows the NDCG percentage gain 
of different ranking models. It compares the 
ranked list generated by our global ranking 
framework and by the local ranking models. 
We observe that (1) irrespective of whether the 
local ranking model is a conventional model or 
a learning to rank model, Global+LC and 
Global+WBF models achieve NDCG gains 
over the original rankings of the local ranking 
models; (2) the results show that our global 
ranking framework can improve the perfor-
mance by only exploiting MI analysis. Howev-
er, it is expected that employing more ad-
vanced relation extraction methods to deter-
mine the global information (Equation 3) 
would yield more reliable pseudo-ranked lists 
and lead to a further improvement in the final 
ranking; and (3) similar to the results in Sec-
tion 5.4, the performance of Global+MBF does 
not improve. Global+MBF has a negative 
NDCG gain and the Area_iPR decreases by 
2.61%. We believe this is due to MBF gives 
equal weight to each pseudo-IR system. As 
mentioned in Section 4.1, the document set in 
INT is comprised of the identifiers of the gene 
mentions derived by Dai et al?s system. Un-
fortunately, there must be incorrect identifiers 
(the errors may be due to their gene mention 
recognition or identifier mapping processes). 
As in the meta-search, the best performance is 
often achieved by weighting the input systems 
unequally. Reasonable weights allow the algo-
rithm to concentrate on good feedback from 
pseudo-IR systems and ignore poor feedback. 
As shown by the average precision results in 
Figure 3, the identifiers (corresponding to the 
pseudo-IR systems in our framework) in the 
higher ranks are more reliable; however, MBF 
cannot use this information, which leads to a 
negative NDCG gain and a lower Area_iPR 
score. 
6 Conclusion 
We have presented a new global ranking 
framework based on data fusion technology. 
Our approach solves the global ranking prob-
lem in three stages: the first stage ranks the 
document set by the original local ranking 
model; the second stage transforms the prob-
Based on Global Ranking NDCG1 NDCG3 NDCG5 
Local Ranking 
/Rank1 
Global+LC +0.908 +1.323 -0.003 
Global+MBF -3.279 -1.034 -0.020 
Global+WBF -0.016 +3.630 +2.071 
Freq Global+LCf +1.639 +3.152 +2.817 
Global+MBFf -6.860 -4.275 -4.839 
Global+WBFf +2.549 +2.390 +3.043 
Table 1. The NDCG Gain (%) of Different 
Ranking Models. 
 
Figure 4. The Area_iPR Results of Different 
Ranking Models 
0.22 0.27 0.32 0.37 0.42 0.47 0.52
Global+WBF
Global+MBF
Global+LC
Local/Rank1
Hakenberg/Rank 2
S? tre/Rank 3
Average
229
lem into a data fusion task by using global in-
formation, and the final stage adapts fusion 
algorithms to solve the ranking problem. The 
framework is flexible and it can be combined 
with other mature ranking models and fusion 
algorithms. We also show how the BioCreA-
tIvE INT can be formulated as a global ranking 
problem and solved by the proposed frame-
work. Experiments on the INT dataset demon-
strate the effectiveness of the proposed frame-
work and its superior performance over other 
ranking models. 
In our future work, we will address the fol-
lowing issues: (1) the use of advanced data 
fusion algorithms in the proposed framework; 
(2) assessing the performance of the proposed 
framework on other tasks, such as Pseudo Re-
levance Feedback and Topic Distillation; and 
(3) design an advanced supervised learning 
relation extraction algorithm to replace MI in 
INT to evaluate the system performance. 
References 
Adler, P., R. Kolde, M. Kull, A. Tkachenko, H. 
Peterson, J. Reimand and J. Vilo (2009). Mining 
for coexpression across hundreds of datasets 
using novel rank aggregation and visualization 
methods. Genome Biology 10(R139). 
Aslam, J. A. and M. Montague (2001). Models for 
metasearch. Proceedings of the 24th annual 
international ACM SIGIR conference on 
Research and development in information 
retrieval, New Orleans, Louisiana, United States, 
ACM. 
Bartell, B. T., G. W. Cottrell and R. K. Belew 
(1994). Automatic combination of multiple 
ranked retrieval systems. Proceedings of the 
17th annual international ACM SIGIR 
conference on Research and development in 
information retrieval, Dublin, Ireland Springer-
Verlag New York, Inc. 
Borda, J. (1781). M?moire sur les ?lections au 
scrutin. Histoire del'Acad e?mie Royale des 
Sciences 2: 13. 
Chowdhury, G. (2007). TREC: Experiment and 
Evaluation in Information Retrieval. Online 
Information Review 31(5): 462. 
Dai, H.-J., P.-T. Lai and R. T.-H. Tsai (2010). 
Multi-stage gene normalization and SVM-based 
ranking for protein interactor extraction in full-
text articles. IEEE TRANSACTIONS ON 
COMPUTATIONAL BIOLOGY AND 
BIOINFORMATICS 14 May. 2010. IEEE 
computer Society Digital Library. IEEE 
Computer Society. 
Fox, E. A. and J. A. Shaw (1994). Combination of 
Multiple Searches. 1994, Proceedings of the 
Second Text REtrieval Conference (TREC 2)  
Jenssen, T.-K., A. Lagreid, J. Komorowski and E. 
Hovig (2001). A literature network of human 
genes for high-throughput analysis of gene 
expression. Nature Genetics 28(1): 21-28. 
Knaus, D., E. Mittendorf and P. Sch?uble (1995). 
Improving a basic retrieval method by links and 
passage level evidence. NIST Special 
Publication 500-225: Overview of the Third Text 
REtrieval Conference (TREC-3). 
Krallinger, M., F. Leitner and A. Valencia (2009). 
The BioCreative II.5 challenge overview. 
Proceedings of the BioCreative II.5 Workshop 
2009 on Digital Annotations, Madrid, Spain. 
Kwok, K. L. (1984). A document-document 
similarity measure based on cited titles and 
probability theory, and its application to 
relevance feedback retrieval. SIGIR'84. 
Lee, J. H. (1997). Analyses of multiple evidence 
combination. Proceedings of the 20th annual 
international ACM SIGIR conference on 
Research and development in information 
retrieval, Philadelphia, Pennsylvania, United 
States, ACM. 
Lin, S. and J. Ding (2008). Integration of Ranked 
Lists via Cross Entropy Monte Carlo with 
Applications to mRNA and microRNA Studies. 
Biometrics 65(1): 9-18. 
Liu, Y.-T., T.-Y. Liu, T. Qin, Z.-M. Ma and H. Li 
(2007). Supervised rank aggregation. 
Proceedings of the 16th international conference 
on World Wide Web, Banff, Alberta, Canada, 
ACM. 
Mardis, S., F. Leitner and L. Hirschman (2009). 
BioCreative II.5: Evaluation and ensemble 
system performance. Proceedings of the 
BioCreative II.5 Workshop 2009 on Digital 
Annotations, Madrid, Spain. 
Nuray, R. and F. Can (2006). Automatic ranking of 
information retrieval systems using data fusion. 
Inf. Process. Manage. 42(3): 595-614. 
Pihura, V., S. Dattaa and S. Datta (2008). Finding 
common genes in multiple cancer types through 
meta?analysis of microarray experiments: A 
230
rank aggregation approach Genomics 92(6): 
400-403  
Qin, T., T.-Y. Liu, X.-D. Zhang, D.-S. Wang and H. 
Li (2008). Global Ranking Using Continuous 
Conditional Random Fields. Proceedings of the 
Twenty-Second Annual Conference on Neural 
Information Processing Systems  (NIPS 2008), 
Vancouver, Canada. 
Qin, T., T. Liu, X. Zhang, D. Wang, W. Xiong and 
H. Li (2008). Learning to rank relational objects 
and its application to web search, ACM. 
Vogt, C. and G. Cottrell (1999). Fusion via a linear 
combination of scores. Information Retrieval 
1(3): 151-173. 
Vogt, C. C. and G. W. Cottrell (1998). Predicting 
the performance of linearly combined IR systems. 
Proceedings of the 21st annual international 
ACM SIGIR conference on Research and 
development in information retrieval, Melbourne, 
Australia ACM. 
Zhao, Z., J. Wang, S. Sharma, N. Agarwal, H. Liu 
and Y. Chang (2010). An Integrative Approach 
to Identifying Biologically Relevant Genes. 
Proceedings of SIAM International Conference 
on Data Mining (SDM). 
 
 
231
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 663?667,
Dublin, Ireland, August 23-24, 2014.
TMUNSW: Disorder Concept Recognition and Normalization in  
Clinical Notes for SemEval-2014 Task 7 
 
 
Jitendra 
Jonnagaddala 
Translational Cancer 
Research Network,  
University of  
New South Wales,  
Sydney 2031,  
Australia 
z3339253@unsw.
edu.au 
Manish Kumar 
Krishagni Solutions 
Pty Ltd, 
Armadale 6112,  
Australia 
manish.ku-
mar@krishagni
.com 
Hong-Jie Dai* Enny  
Rachmani 
Chien-Yeh Hsu 
Graduate Institute of Biomedical Informatics, 
College of Medical Science and Technology 
Taipei Medical University, 
Taipei City 110, Taiwan 
{hjdai, d610101005, 
cyhsu}@tmu.edu.tw 
Abstract 
We present our participation in Task 7 of 
SemEval shared task 2014. The goal of 
this particular task includes the identifica-
tion of disorder named entities and the 
mapping of each disorder to a unique Uni-
fied Medical Language System concept 
identifier, which were referred to as Task 
A and Task B respectively. We partici-
pated in both of these subtasks and used 
YTEX as a baseline system. We further 
developed a supervised linear chain Con-
ditional Random Field  model based on 
sets of features to predict disorder men-
tions. To take benefit of results from both 
systems we merged these results. Under 
strict condition our best run evaluated at 
0.549 F-measure for Task A and an accu-
racy of 0.489 for Task B on test dataset. 
Based on our error analysis we conclude 
that recall of our system can be signifi-
cantly increased by adding more features 
to the Conditional Random Field model 
and by using another type of tag represen-
tation or frame matching algorithm to deal 
with the disjoint entity mentions. 
                                                 
 
1 Introduction 
Clinical notes are rich sources of valuable pa-
tient?s information. These clinical notes are often 
plain text records containing important entity 
mentions such as clinical findings, procedures and 
disease mentions (Jimeno et al., 2008). Using au-
tomated tools to extract the aforementioned infor-
mation can undoubtedly help researchers and cli-
nicians with better decision making. An important 
subtask of information extraction called named 
entity recognition (NER) can recognize the 
boundary of named entity mention and classify it 
into a certain semantic group.  
The focus of the SemEval-2104 task 7 is recogni-
tion and normalization of disorder entities men-
tioned in clinical notes. As such, this task was fur-
ther divided into two parts: first, task A which in-
cludes recognition of mention of concepts that be-
long to UMLS (Unified Medical Language Sys-
tem) semantic group disorders (Bodenreider, 
2004). The concepts considered in Task A include 
the following eleven UMLS semantic types: Con-
genital Abnormality; Acquired Abnormality; In-
jury or Poisoning; Pathologic Function; Disease 
or Syndrome; Mental or Behavioral Dysfunction; 
Cell or Molecular Dysfunction; Experimental 
Model of Disease; Anatomical Abnormality; Ne-
oplastic Process; and Signs and Symptoms. Sec-
ond, task B referred to as task of normalization in-
volves the mapping of each disorder mention to a 
UMLS concept unique identifier (CUI).The map-
ping was limited to UMLS CUI of SNOMED clin-
ical term codes (Spackman, Campbell, & C?, 
1997). We participated in both tasks and devel-
         
       This work is licensed under a Creative Commons Attrib-
ution 4.0 International Licence. Page numbers and proceed-
ings footer are added by the organisers. Licence details: 
http://creativecommons.org/licenses/by/4.0/  
       *Corresponding author 
663
oped a disorder concept recognition/normaliza-
tion system based on several openly available 
tools and machine learning algorithms. 
2 Methods 
2.1 System Design 
For both task A and B, YTEX (Garla et al., 
2011) system was employed as a baseline system. 
We chose to use YTEX since it is specifically de-
signed for processing clinical notes with improve-
ments to cTAKES?s dictionary lookup algorithm 
and word sense disambiguation feature. The pre-
processing involves sentence detection, tokeniza-
tion and part-of-speech (POS) tagging(Fiscus, 
1997). Based on the tokenized tokens, several fea-
tures along with the corresponding part-of-speech 
tags were extracted for the supervised learning al-
gorithm?conditional random field (CRF) model 
(Lafferty, McCallum, & Pereira, 2001). After 
training, the CRF model was used for recognizing 
disorder mentions. Furthermore the recognized 
disorder concepts were sent to MetaMap 
(Aronson & Lang, 2010) to look for their corre-
sponding CUIs for generating normalized results. 
The results were finally merged with the output of 
YTEX. A high level diagram of the developed 
system is schematized in Figure 1.  
2.2 Disorder Concept Recognition 
The task A involves detecting boundaries of en-
tity that belongs to UMLS semantic group, disor-
ders. We used the sequence tagging tool based on 
Mallet?s implementation of the supervised linear 
chain CRF model to perform this task. We fol-
lowed the traditional BIO format to formulate the 
disorder concept recognition task as a sequential 
labelling task, wherein each token was assigned a 
label such as B is indicated the Beginning of en-
tity, I is indicated the Inside an entity, or O is in-
dicated the Outside of an entity. Thus, the model 
assigns each of the word into one of the above 
three labels. We investigated various types of fea-
tures proposed in previous works (Jiang et al., 
2011; Li, Kipper-Schuler, & Savova, 2008; Tang, 
Cao, Wu, Jiang, & Xu, 2013), like semantic fea-
ture which includes UMLS semantic group and 
semantic type, to develop our classifier. We also 
investigated various word features like POS, cap-
italization, and ?position of word? in the sentence. 
We also used ?previous word?, ?next word? and 
?label of these words? as a feature for developing 
our classifier. 
2.3 Disorder Concept Normalization 
Each disorder concept recognized by our recogni-
tion system was passed to a local installation of 
MetaMap using MetaMap Java API to obtain its 
candidate CUI. To increase the recall, we merged 
results from both YTEX and MetaMap systems. 
Output from YTEX baseline system was merged 
to the output from our CRF model with MetaMap. 
This method was used because it was observed 
that our CRF/MetaMap model has higher preci-
sion while YTEX baseline system has higher re-
call. 
3 Results  
3.1 Datasets 
For Task A and Task B, the training and devel-
opment datasets provided by the SemEval task 7 
organizers were used. Both were derived from 
ShARe corpus containing de-identified  plain text 
clinical notes from MIMIC II database (Suominen 
et al., 2013).  These clinical notes were manually 
annotated for disorder mention and normalized to 
 
Fig. 1: TMUNSW system design for SemEval-2014 Task 7. 
664
an UMLS CUI when possible. The corpus con-
sisted of four types of clinical notes: discharge 
summaries, electrocardiogram, echocardiogram, 
and radiology reports. As the dataset, we included 
different types of clinical notes, further we trained 
a CRF model for each type and evaluated its per-
formance on the corresponding development data. 
However, test set from task organizers contained 
discharge summaries only. Hence, the model de-
veloped for discharge summary was selected for 
evaluation on the test set. 
3.2 Evaluation Metrics  
The official evaluation script provided by or-
ganizers of the shared task was used to evaluate 
our system ability to correct an identify spans of 
text that belongs to semantic group disorders and 
to normalize them to the corresponding CUIs.  
We calculated the evaluation measures under 
two settings-strict and relaxed. The strict setting 
matches exact boundaries with the gold standard, 
while relaxed setting matches overlapping bound-
aries in the gold standard. The evaluation 
measures were calculated using the commonly 
used evaluation measures including recall (R), 
precision (P), and F-measure (F) (Powers, 2007). 
3.3 System Configurations 
We used YTEX V0.8 with cTAKES V2.5.0 as 
the baseline system for performance comparison. 
All default settings for YTEX, including the con-
cept window of the length 10, were adopted. We 
submit two runs for both tasks. For Task A, the 
first run, denoted as Run0, used the developed 
CRF model to recognize the disorder concepts. 
The second run was denoted as Run1, which 
merged the results of CRF model with YTEX. 
Similarly, for Task B, Run0 used the MetaMap 
2012 version to normalize the candidate disorder 
concepts recognized by our CRF model. For 
Run1, we merged normalized annotation results 
of YTEX with Run0. 
3.4 System Performance Comparison 
We performed a ten-fold cross validation on 
the combination of the training and development 
datasets for examining the recognition and nor-
malization performance of the developed CRF 
model combined with MetaMap (Run0), and com-
pared with the YTEX as the baseline system. Ta-
ble 1 summarized the results for Task A and B. 
 The results showed, for both tasks, Run0 sig-
nificantly outperformed YTEX in the strict setting. 
The higher F-score of Run0 can be attributed by 
the fact that Run0 is developed based on the re-
leased corpus and the machine learning algorithm 
which is better suited for NER task as compared 
to the rule based YTEX system. In the relaxed set-
ting, for Task A, Run0 also has significantly 
higher F-score than the YTEX baseline system. 
However, in case of Task B accuracy of YTEX is 
significantly greater than Run0. We believe that 
the higher accuracy of the baseline system can be 
attributed by the word sense disambiguation fea-
ture within YTEX. 
 
Task 
A 
YTEX Run0 
Strict Relaxed Strict Relaxed 
P 0.524 0.917 0.771 0.978 
R 0.469 0.670 0.615 0.811 
F 0.495 0.774 0.682 0.884 
Task  
B 
YTEX Run0 
Strict Relaxed Strict Relaxed 
Accuracy 0.469 1.000 0.684 0.752 
Table 1. Summary of Training Set Evaluation Re-
sults. 
3.5 Official Evaluation Results 
Table 2 shows the official evaluation results 
of the submitted two configurations, Run0 and 
Run1. Under the strict setting, Run1 achieves the 
better performance with an F-measure of 0.549 for 
Task A and an accuracy of 0.489 for Task B on 
test dataset. Our best run for Task A was ranked 
15 out of 21 participants, while for Task B it was 
ranked 9 out of 18 participants. 
 
Task 
A 
Run0 Run1 
Strict Relaxed Strict Relaxed 
P 0.622 0.899 0.524 0.914 
R 0.429 0.652 0.576 0.765 
F 0.508 0.756 0.549 0.833 
Task 
B 
Run0 Run1 
Strict Relaxed Strict Relaxed 
Accuracy 0.358 0.834 0.489 0.849 
Table 2. Summary of Test Set Evaluation Results.  
 
Table 2 shows that Run1 has higher F-score 
than Run0 because of its high recall. On the other 
hand, Run0 achieves significantly higher preci-
sion compared to Run1 for Task A. The result is 
in accordance with our expectation, because Run1 
integrated the results from YTEX to improve the 
recall of Run0 at the cost of the decrease in preci-
sion. The trade-off seems acceptable because it 
can significantly improve the accuracy in normal-
izing disorder concepts. 
665
4 Discussion 
We performed error analysis on development da-
taset and found that the lower recall of Run0 de-
rived from the miss of many disjoint entities 
(where the tokens comprising the entity string are 
non-adjacent), which cannot be captured by the 
current BIO tag set. For example, consider the 
sentence ?Abdomen is soft, nontender, non-
distended, negative bruits.? For this sentence the 
gold annotations contain three entities as ?Abdo-
men bruits-CUI= C0221755?, ?Abdomen 
nontender-CUI=CUI-less? and ?nondistended-
CUI=CUI-less?. In the current BIO formulation, 
all of the above three disjoint entities cannot be 
correctly recognized. There are also abbreviations 
which were rarely seen in the training dataset but 
appeared more in the development/test sets. So 
when we test our developed model on test set the 
abbreviations which are not part of training and 
development set must have been missed by our 
system. We believe that by incorporating medical 
abbreviations database into our model develop-
ment, the performance of our overall system 
would have been better. Also, the precision in 
Task A of Run1 was lower than Run0 because of 
some disjoint annotations.  
5 Conclusion  
We present a clinical NER system based on Mal-
let?s implementation of CRF and a hybrid normal-
ization system using MetaMap and YTEX. We 
developed our system with limited features due to 
the time constraint. We can conclude from error 
analysis that recall of this system could be signif-
icantly increased by adding more features to it. 
We plan to extend our system in future by using 
another type of tag representation or frame-based 
pattern matching algorithm to handle disjoint 
named entities. Similarly missing abbreviations 
can be handled by employing external resources 
such as abbreviation recognition tools. 
Acknowledgements 
This project was supported by a Cancer Institute 
NSW?s translational cancer research centre pro-
gram grant, and the research grant TMU101-AE1-
B55 of Taipei Medical University. 
References 
Aronson, A. R., & Lang, F. M. (2010). An overview of 
MetaMap: historical perspective and recent 
advances. J Am Med Inform Assoc, 17(3), 229-236. 
doi: 10.1136/jamia.2009.002733 
Bodenreider, O. (2004). The unified medical language 
system (UMLS): integrating biomedical 
terminology. Nucleic acids research, 32(suppl 1), 
D267-D270.  
Fiscus, J. G. (1997). A post-processing system to yield 
reduced word error rates: Recognizer output voting 
error reduction (ROVER). Paper presented at the 
Automatic Speech Recognition and Understanding, 
1997. Proceedings., 1997 IEEE Workshop on. 
Garla, V., Re, V. L., Dorey-Stein, Z., Kidwai, F., 
Scotch, M., Womack, J., . . . Brandt, C. (2011). The 
Yale cTAKES extensions for document 
classification: architecture and application. Journal 
of the American Medical Informatics Association, 
18(5), 614-620.  
Jiang, M., Chen, Y., Liu, M., Rosenbloom, S. T., Mani, 
S., Denny, J. C., & Xu, H. (2011). A study of 
machine-learning-based approaches to extract 
clinical entities and their assertions from discharge 
summaries. Journal of the American Medical 
Informatics Association, 18(5), 601-606.  
Jimeno, A., Jimenez-Ruiz, E., Lee, V., Gaudan, S., 
Berlanga, R., & Rebholz-Schuhmann, D. (2008). 
Assessment of disease named entity recognition on 
a corpus of annotated sentences. BMC 
Bioinformatics, 9 Suppl 3, S3. doi: 10.1186/1471-
2105-9-S3-S3 
Lafferty, J., McCallum, A., & Pereira, F. (2001). 
Conditional random fields: Probabilistic models for 
segmenting and labeling sequence data. Paper 
presented at the Proceedings of the 18th 
International Conference on Machine Learning 
(ICML).  
Li, D., Kipper-Schuler, K., & Savova, G. (2008). 
Conditional random fields and support vector 
machines for disorder named entity recognition in 
clinical texts. Paper presented at the Proceedings of 
the workshop on current trends in biomedical 
natural language processing. 
Powers, D. M. W. (2007). Evaluation: From Precision, 
Recall and F-Factor to ROC, Informedness, 
Markedness & Correlation. Journal of Machine 
Learning Technologies, 2(1), 37-63. doi: citeulike-
article-id:10061513 
Spackman, K. A., Campbell, K. E., & C?, R. (1997). 
SNOMED RT: a reference terminology for health 
care. Paper presented at the Proceedings of the 
AMIA annual fall symposium. 
Suominen, H., Salanter?, S., Velupillai, S., Chapman, 
W. W., Savova, G., Elhadad, N., . . . Jones, G. J. 
(2013). Overview of the ShARe/CLEF eHealth 
Evaluation Lab 2013 Information Access 
Evaluation. Multilinguality, Multimodality, and 
Visualization (pp. 212-231): Springer. 
666
Tang, B., Cao, H., Wu, Y., Jiang, M., & Xu, H. (2013). 
Recognizing clinical entities in hospital discharge 
summaries using Structural Support Vector 
Machines with word representation features. BMC 
medical informatics and decision making, 13(1), 1-
10.  
  
667
