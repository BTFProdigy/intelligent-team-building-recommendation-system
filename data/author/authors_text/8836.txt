Proceedings of Ninth Meeting of the ACL Special Interest Group in Computational Morphology and Phonology, pages 48?56,
Prague, June 2007. c?2007 Association for Computational Linguistics
Inducing Sound Segment Differences using Pair Hidden Markov Models
Martijn Wieling
Alfa-Informatica
University of Groningen
wieling@gmail.com
Therese Leinonen
Alfa-Informatica
University of Groningen
t.leinonen@rug.nl
John Nerbonne
Alfa-Informatica
University of Groningen
j.nerbonne@rug.nl
Abstract
Pair Hidden Markov Models (PairHMMs)
are trained to align the pronunciation tran-
scriptions of a large contemporary collec-
tion of Dutch dialect material, the Goeman-
Taeldeman-Van Reenen-Project (GTRP, col-
lected 1980?1995). We focus on the ques-
tion of how to incorporate information about
sound segment distances to improve se-
quence distance measures for use in di-
alect comparison. PairHMMs induce seg-
ment distances via expectation maximisa-
tion (EM). Our analysis uses a phonologi-
cally comparable subset of 562 items for all
424 localities in the Netherlands. We evalu-
ate the work first via comparison to analyses
obtained using the Levenshtein distance on
the same dataset and second, by comparing
the quality of the induced vowel distances to
acoustic differences.
1 Introduction
Dialectology catalogues the geographic distribution
of the linguistic variation that is a necessary condi-
tion for language change (Wolfram and Schilling-
Estes, 2003), and is sometimes successful in iden-
tifying geographic correlates of historical develop-
ments (Labov, 2001). Computational methods for
studying dialect pronunciation variation have been
successful using various edit distance and related
string distance measures, but unsuccessful in us-
ing segment differences to improve these (Heeringa,
2004). The most successful techniques distinguish
consonants and vowels, but treat e.g. all the vowel
differences as the same. Ignoring the special treat-
ment of vowels vs. consonants, the techniques re-
gard segments in a binary fashion?as alike or
different?in spite of the overwhelming consensus
that some sounds are much more alike than others.
There have been many attempts to incorporate more
sensitive segment differences, which do not neces-
sarily perform worse in validation, but they fail to
show significant improvement (Heeringa, 2004).
Instead of using segment distances as these are
(incompletely) suggested by phonetic or phonolog-
ical theory, we can also attempt to acquire these
automatically. Mackay and Kondrak (2005) in-
troduce Pair Hidden Markov Models (PairHMMs)
to language studies, applying them to the problem
of recognising ?cognates? in the sense of machine
translation, i.e. pairs of words in different languages
that are similar enough in sound and meaning to
serve as translation equivalents. Such words may
be cognate in the sense of historical linguistics, but
they may also be borrowings from a third language.
We apply PairHMMs to dialect data for the first time
in this paper. Like Mackay and Kondrak (2005) we
evaluate the results both on a specific task, in our
case, dialect classification, and also via examination
of the segment substitution probabilities induced by
the PairHMM training procedures. We suggest us-
ing the acoustic distances between vowels as a probe
to explore the segment substitution probabilities in-
duced by the PairHMMs.
Naturally, this validation procedure only makes
sense if dialects are using acoustically more similar
sounds in their variation, rather than, for example,
48
randomly varied sounds. But why should linguistic
and geographic proximity be mirrored by frequency
of correspondence? Historical linguistics suggests
that sound changes propagate geographically, which
means that nearby localities should on average share
the most changes. In addition some changes are con-
vergent to local varieties, increasing the tendency
toward local similarity. The overall effect in both
cases strengthens the similarity of nearby varieties.
Correspondences among more distant varieties are
more easily disturbed by intervening changes and
decreasing strength of propagation.
2 Material
In this study the most recent Dutch dialect data
source is used: data from the Goeman-Taeldeman-
Van Reenen-project (GTRP; Goeman and Taelde-
man, 1996). The GTRP consists of digital tran-
scriptions for 613 dialect varieties in the Netherlands
(424 varieties) and Belgium (189 varieties), gath-
ered during the period 1980?1995. For every vari-
ety, a maximum of 1876 items was narrowly tran-
scribed according to the International Phonetic Al-
phabet. The items consisted of separate words and
word groups, including pronominals, adjectives and
nouns. A more detailed overview of the data collec-
tion is given in Taeldeman and Verleyen (1999).
Since the GTRP was compiled with a view to
documenting both phonological and morphological
variation (De Schutter et al, 2005) and our pur-
pose here is the analysis of variation in pronunci-
ation, many items of the GTRP are ignored. We
use the same 562 item subset as introduced and dis-
cussed in depth by Wieling et al (2007). In short,
the 1876 item word list was filtered by selecting
only single word items, plural nouns (the singular
form was preceded by an article and therefore not in-
cluded), base forms of adjectives instead of compar-
ative forms and the first-person plural verb instead
of other forms. We omit words whose variation is
primarily morphological as we wish to focus on pro-
nunciation.
Because the GTRP transcriptions of Belgian vari-
eties are fundamentally different from transcriptions
of Netherlandic varieties (Wieling et al, 2007), we
will focus our analysis on the 424 varieties in the
Netherlands. The geographic distribution of these
Leeuwarden
Veendam
Aalsmeer
Utrecht
Delft
Urk
Putten
Coevorden
Oldenzaal
Middelburg Goirle
Venlo
Figure 1. Distribution of GTRP localities.
varieties is shown in Figure 1. Furthermore, note
that we will not look at diacritics, but only at the
phonetic symbols (82 in total).
3 The Pair Hidden Markov Model
In this study we will use a Pair Hidden Markov
Model (PairHMM), which is essentially a Hidden
Markov Model (HMM) adapted to assign similar-
ity scores to word pairs and to use these similarity
scores to compute string distances. In general an
HMM generates an observation sequence (output)
by starting in one of the available states based on the
initial probabilities, going from state to state based
on the transition probabilities while emitting an out-
put symbol in each state based on the emission prob-
ability of that output symbol in that state. The prob-
ability of an observation sequence given the HMM
can be calculated by using well known HMM algo-
rithms such as the Forward algorithm and the Viterbi
algorithms (e.g., see Rabiner, 1989).
The only difference between the PairHMM and
the HMM is that it outputs a pair of symbols in-
stead of only one symbol. Hence it generates two
(aligned) observation streams instead of one. The
PairHMM was originally proposed by Durbin et al
(1998) and has successfully been used for aligning
49
Figure 2. Pair Hidden Markov Model. Image cour-
tesy of Mackay and Kondrak (2005).
biological sequences. Mackay and Kondrak (2005)
adapted the algorithm to calculate similarity scores
for word pairs in orthographic form, focusing on
identifying translation equivalents in bilingual cor-
pora.
Their modified PairHMM has three states repre-
senting the basic edit operations: a substitution state
(M), a deletion state (X) and an insertion state (Y). In
the substitution state two symbols are emitted, while
in the other two states a gap and a symbol are emit-
ted, corresponding with a deletion and an insertion,
respectively. The model is shown in Figure 2. The
four transition parameters are specified by ?, ?, ?
and ? . There is no explicit start state; the proba-
bility of starting in one of the three states is equal to
the probability of going from the substitution state to
that state. In our case we use the PairHMM to align
phonetically transcribed words. A possible align-
ment (including the state sequence) for the two ob-
servation streams [mO@lk@] and [mEl@k] (Dutch di-
alectal variants of the word ?milk?) is given by:
m O @ l k @
m E l @ k
M M X M Y M X
We have several ways to calculate the similarity
score for a given word pair when the transition and
emission probabilities are known. First, we can use
the Viterbi algorithm to calculate the probability of
the best alignment and use this probability as a sim-
ilarity score (after correcting for length; see Mackay
and Kondrak, 2005). Second, we can use the For-
ward algorithm, which takes all possible alignments
into account, to calculate the probability of the ob-
servation sequence given the PairHMM and use this
probability as a similarity score (again corrected for
length; see Mackay, 2004 for the adapted PairHMM
Viterbi and Forward algorithms).
A third method to calculate the similarity score is
using the log-odds algorithm (Durbin et al, 1998).
The log-odds algorithm uses a random model to rep-
resent how likely it is that a pair of words occur to-
gether while they have no underlying relationship.
Because we are looking at word alignments, this
means an alignment consisting of no substitutions
but only insertions and deletions. Mackay and Kon-
drak (2005) propose a randommodel which has only
insertion and deletion states and generates one word
completely before the other, e.g.
m O @ l k @
m E l @ k
X X X X X X Y Y Y Y Y
The model is described by the transition proba-
bility ? and is displayed in Figure 3. The emis-
sion probabilities can be either set equal to the inser-
tion and deletion probabilities of the word similarity
model (Durbin et al, 1998) or can be specified sepa-
rately based on the token frequencies in the data set
(Mackay and Kondrak, 2005).
The final log-odds similarity score of a word pair
is calculated by dividing the Viterbi or Forward
probability by the probability generated by the ran-
dom model, and subsequently taking the logarithm
of this value. When using the Viterbi algorithm
the regular log-odds score is obtained, while using
the Forward algorithm yields the Forward log-odds
score (Mackay, 2004). Note that there is no need
for additional normalisation; by dividing two mod-
els we are already implicitly normalising.
Before we are able to use the algorithms de-
scribed above, we have to estimate the emission
probabilities (i.e. insertion, substitution and dele-
tion probabilities) and transition probabilities of the
model. These probabilities can be estimated by us-
ing the Baum-Welch expectation maximisation al-
gorithm (Baum et al, 1970). The Baum-Welch algo-
50
Figure 3. Random Pair Hidden Markov Model. Im-
age courtesy of Mackay and Kondrak (2005).
rithm iteratively reestimates the transition and emis-
sion probabilities until a local optimum is found and
has time complexity O(TN2), where N is the num-
ber of states and T is the length of the observa-
tion sequence. The Baum-Welch algorithm for the
PairHMM is described in detail in Mackay (2004).
3.1 Calculating dialect distances
When the parameters of the complete model have
been determined, the model can be used to calculate
the alignment probability for every word pair. As in
Mackay and Kondrak (2005) and described above,
we use the Forward and Viterbi algorithms in both
their regular (normalised for length) and log-odds
form to calculate similarity scores for every word
pair. Subsequently, the distance between two dialec-
tal varieties can be obtained by calculating all word
pair scores and averaging them.
4 The Levenshtein distance
The Levenshtein distance was introduced by Kessler
(1995) as a tool for measuring linguistic distances
between language varieties and has been success-
fully applied in dialect comparison (Nerbonne et al,
1996; Heeringa, 2004). For this comparison we use
a slightly modified version of the Levenshtein dis-
tance algorithm, which enforces a linguistic syllab-
icity constraint: only vowels may match with vow-
els, and consonants with consonants. The specific
details of this modification are described in more de-
tail in Wieling et al (2007).
We do not normalise the Levenshtein distance
measurement for length, because Heeringa et al
(2006) showed that results based on raw Levenshtein
distances are a better approximation of dialect dif-
ferences as perceived by the dialect speakers than
results based on the normalised Levenshtein dis-
tances. Finally, all substitutions, insertions and dele-
tions have the same weight.
5 Results
To obtain the best model probabilities, we trained
the PairHMM with all data available from the 424
Netherlandic localities. For every locality there were
on average 540 words with an average length of 5
tokens. To prevent order effects in training, every
word pair was considered twice (e.g., wa ? wb and
wb?wa). Therefore, in one training iteration almost
100 million word pairs had to be considered. To be
able to train with these large amounts of data, a par-
allel implementation of the PairHMM software was
implemented. After starting with more than 6700
uniform initial substitution probabilities, 82 inser-
tion and deletion probabilities and 5 transition prob-
abilities, convergence was reached after nearly 1500
iterations, taking 10 parallel processors each more
than 10 hours of computation time.
In the following paragraphs we will discuss the
quality of the trained substitution probabilities as
well as comment on the dialectological results ob-
tained with the trained model.
5.1 Trained substitution probabilities
We are interested both in how well the overall se-
quence distances assigned by the trained PairHMMs
reveal the dialectological landscape of the Nether-
lands, and also in how well segment distances in-
duced by the Baum-Welch training (i.e. based on
the substitution probabilities) reflect linguistic real-
ity. A first inspection of the latter is a simple check
on howwell standard classifications are respected by
the segment distances induced.
Intuitively, the probabilities of substituting a
vowel with a vowel or a consonant with a conso-
nant (i.e. same-type substitution) should be higher
than the probabilities of substituting a vowel with a
consonant or vice versa (i.e. different-type substitu-
tion). Also the probability of substituting a phonetic
51
symbol with itself (i.e. identity substitution) should
be higher than the probability of a substitution with
any other phonetic symbol. To test this assumption,
we compared the means of the above three substi-
tution groups for vowels, consonants and both types
together.
In line with our intuition, we found a higher prob-
ability for an identity substitution as opposed to
same-type and different-type non-identity substitu-
tions, as well as a higher probability for a same-type
substitution as compared to a different-type substitu-
tion. This result was highly significant in all cases:
vowels (all p?s ? 0.020), consonants (all p?s <
0.001) and both types together (all p?s < 0.001).
5.2 Vowel substitution scores compared to
acoustic distances
PairHMMs assign high probabilities (and scores)
to the emission of segment pairs that are more
likely to be found in training data. Thus we expect
frequent dialect correspondences to acquire high
scores. Since phonetic similarity effects alignment
and segment correspondences, we hypothesise that
phonetically similar segment correspondences will
be more usual than phonetically remote ones, more
specifically that there should be a negative correla-
tion between PairHMM-induced segment substitu-
tion probabilities presented above and phonetic dis-
tances.
We focus on segment distances among vowels,
because it is straightforward to suggest a measure
of distance for these (but not for consonants). Pho-
neticians and dialectologists use the two first for-
mants (the resonant frequencies created by different
forms of the vocal cavity during pronunciation) as
the defining physical characteristics of vowel qual-
ity. The first two formants correspond to the ar-
ticulatory vowel features height and advancement.
We follow variationist practice in ignoring third and
higher formants. Using formant frequencies we can
calculate the acoustic distances between vowels.
Because the occurrence frequency of the pho-
netic symbols influences substitution probability, we
do not compare substitution probabilities directly to
acoustic distances. To obtain comparable scores, the
substitution probabilities are divided by the product
of the relative frequencies of the two phonetic sym-
bols used in the substitution. Since substitutions in-
volving similar infrequent segments now get a much
higher score than substitutions involving similar, but
frequent segments, the logarithm of the score is used
to bring the respective scores into a comparable
scale.
In the program PRAAT we find Hertz values
of the first three formants for Dutch vowels pro-
nounced by 50 male (Pols et al, 1973) and 25 fe-
male (Van Nierop et al, 1973) speakers of stan-
dard Dutch. The vowels were pronounced in a /hVt/
context, and the quality of the phonemes for which
we have formant information should be close to the
vowel quality used in the GTRP transcriptions. By
averaging over 75 speakers we reduce the effect of
personal variation. For comparison we chose only
vowels that are pronounced as monophthongs in
standard Dutch, in order to exclude interference of
changing diphthong vowel quality with the results.
Nine vowels were used: /i, I, y, Y, E, a, A, O, u/.
We calculated the acoustic distances between all
vowel pairs as a Euclidean distance of the formant
values. Since our perception of frequency is non-
linear, using Hertz values of the formants when cal-
culating the Euclidean distances would not weigh
F1 heavily enough. We therefore transform frequen-
cies to Bark scale, in better keeping with human per-
ception. The correlation between the acoustic vowel
distances based on two formants in Bark and the log-
arithmical and frequency corrected PairHMM sub-
stitution scores is r = ?0.65 (p < 0.01). But
Lobanov (1971) and Adank (2003) suggested using
standardised z-scores, where the normalisation is
applied over the entire vowel set produced by a given
speaker (one normalisation per speaker). This helps
in smoothing the voice differences between men and
women. Normalising frequencies in this way re-
sulted in a correlation of r = ?0.72 (p < 0.001)
with the PairHMM substitution scores. Figure 4 vi-
sualises this result. Both Bark scale and z-values
gave somewhat lower correlations when the third
formant was included in the measures.
The strong correlation demonstrates that the
PairHMM scores reflect phonetic (dis)similarity.
The higher the probability that vowels are aligned
in PairHMM training, the smaller the acoustic dis-
tance between two segments. We conclude therefore
that the PairHMM indeed aligns linguistically corre-
sponding segments in accord with phonetic similar-
52
Figure 4. Predicting acoustic distances based on
PairHMM scores. Acoustic vowel distances are cal-
culated via Euclidean distance based on the first two
formants measured in Hertz, normalised for speaker.
r = ?0.72
ity. This likewise confirms that dialect differences
tend to be acoustically slight rather than large, and
suggests that PairHMMs are attuned to the slight
differences which accumulate locally during lan-
guage change. Also we can be more optimistic
about combining segment distances and sequence
distance techniques, in spite of Heeringa (2004,
Ch. 4) who combined formant track segment dis-
tances with Levenshtein distances without obtaining
improved results.
5.3 Dialectological results
To see how well the PairHMM results reveal the di-
alectological landscape of the Netherlands, we cal-
culated the dialect distances with the Viterbi and
Forward algorithms (in both their normalised and
log-odds version) using the trained model parame-
ters.
To assess the quality of the PairHMM results,
we used the LOCAL INCOHERENCE measurement
which measures the degree to which geographically
close varieties also represent linguistically similar
varieties (Nerbonne and Kleiweg, 2005). Just as
Mackay and Kondrak (2005), we found the over-
all best performance was obtained using the log-
odds version of Viterbi algorithm (with insertion and
deletion probabilities based on the token frequen-
cies).
Following Mackay and Kondrak (2005), we also
experimented with a modified PairHMM obtained
by setting non-substitution parameters constant.
Rather than using the transition, insertion and dele-
tion parameters (see Figure 2) of the trained model,
we set these to a constant value as we are most
interested in the effects of the substitution param-
eters. We indeed found slightly increased perfor-
mance (in terms of LOCAL INCOHERENCE) for the
simplified model with constant transition parame-
ters. However, since there was a very high corre-
lation (r = 0.98) between the full and the simplified
model and the resulting clustering was also highly
similar, we will use the Viterbi log-odds algorithm
using all trained parameters to represent the results
obtained with the PairHMM method.
5.4 PairHMM vs. Levenshtein results
The PairHMM yielded dialectological results quite
similar to those of Levenshtein distance. The LOCAL
INCOHERENCE of the two methods was similar, and
the dialect distance matrices obtained from the two
techniques correlated highly (r = 0.89). Given that
the Levenshtein distance has been shown to yield re-
sults that are consistent (Cronbach?s ? = 0.99) and
valid when compared to dialect speakers judgements
of similarity (r ? 0.7), this means in particular that
the PairHMMs are detecting dialectal variation quite
well.
Figure 5 shows the dialectal maps for the results
obtained using the Levenshtein algorithm (top) and
the PairHMM algorithm (bottom). The maps on the
left show a clustering in ten groups based on UP-
GMA (Unweighted Pair Group Method with Arith-
metic mean; see Heeringa, 2004 for a detailed expla-
nation). In these maps phonetically close dialectal
varieties are marked with the same symbol. How-
ever note that the symbols can only be compared
within a map, not between the two maps (e.g., a di-
alectal variety indicated by a square in the top map
does not need to have a relationship with a dialec-
tal variety indicated by a square in the bottom map).
Because clustering is unstable, in that small differ-
ences in input data can lead to large differences in
the classifications derived, we repeatedly added ran-
dom small amounts of noise to the data and iter-
atively generated the cluster borders based on the
53
Figure 5. Dialect distances for Levenshtein method (top) and PairHMM method (bottom). The maps on
the left show the ten main clusters for both methods, indicated by distinct symbols. Note that the shape of
these symbols can only be compared within a map, not between the top and bottom maps. The maps in the
middle show robust cluster borders (darker lines indicate more robust cluster borders) obtained by repeated
clustering using random small amounts of noise. The maps on the right show for each locality a vector
towards the region which is phonetically most similar. See section 5.4 for further explanation.
54
noisy input data. Only borders which showed up
during most of the 100 iterations are shown in the
map. The maps in the middle show the most ro-
bust cluster borders; darker lines indicate more ro-
bust borders. The maps on the right show a vector at
each locality pointing in the direction of the region
it is phonetically most similar to.
A number of observations can be made on the
basis of these maps. The most important observa-
tion is that the maps show very similar results. For
instance, in both methods a clear distinction can
be seen between the Frisian varieties (north) and
their surroundings as well as the Limburg varieties
(south) and their surroundings. Some differences
can also be observed. For instance, at first glance
the Frisian cities among the Frisian varieties are sep-
arate clusters in the PairHMM method, while this
is not the case for the Levenshtein method. Since
the Frisian cities differ from their surroundings a
great deal, this point favours the PairHMM. How-
ever, when looking at the deviating vectors for the
Frisian cities in the two vector maps, it is clear that
the techniques again yield similar results. Note that
a more detailed description of the results using the
Levenshtein distance on the GTRP data can be found
in Wieling et al (2007).
Although the PairHMM method is much more so-
phisticated than the Levenshtein method, it yields
very similar results. This may be due to the fact
that the data sets are large enough to compensate for
the lack of sensitivity in the Levenshtein technique,
and the fact that we are evaluating the techniques at
a high level of aggregation (average differences in
540-word samples).
6 Discussion
The present study confirms Mackay and Kondrak?s
(2004) work showing that PairHMMs align linguis-
tic material well and that they induce reasonable seg-
ment distances at the same time. We have extended
that work by applying PairHMMs to dialectal data,
and by evaluating the induced segment distances via
their correlation with acoustic differences. We noted
above that it is not clear whether the dialectological
results improve on the simple Levenshtein measures,
and that this may be due to the level of aggregation
and the large sample sizes. But we would also like
to test PairHMMs on a data set for which more sen-
sitive validation is possible, e.g. the Norwegian set
for which dialect speakers judgements of proximity
is available (Heeringa et al, 2006); this is clearly a
point at which further work would be rewarding.
At a more abstract level, we emphasise that the
correlation between acoustic distances on the one
hand and the segment distances induced by the
PairHMMs on the other confirm both that align-
ments created by the PairHMMs are linguistically
responsible, and also that this linguistic structure in-
fluences the range of variation. The segment dis-
tances induced by the PairHMMs reflect the fre-
quency with which such segments need to be aligned
in Baum-Welch training. It would be conceivable
that dialect speakers used all sorts of correspon-
dences to signal their linguistic provenance, but they
do not. Instead, they tend to use variants which are
linguistically close at the segment level.
Finally, we note that the view of diachronic
change as on the one hand the accumulation of
changes propagating geographically, and on the
other hand as the result of a tendency toward local
convergence suggests that we should find linguis-
tically similar varieties nearby rather than further
away. The segment correspondences PairHMMs in-
duce correspond to those found closer geographi-
cally.
We have assumed a dialectological perspective
here, focusing on local variation (Dutch), and using
similarity of pronunciation as the organising varia-
tionist principle. For the analysis of relations among
languages that are further away from each other?
temporally and spatially?there is substantial con-
sensus that one needs to go beyond similarity as a
basis for postulating grouping. Thus phylogenetic
techniques often use a model of relatedness aimed
not at similarity-based grouping, but rather at creat-
ing a minimal genealogical tree. Nonetheless sim-
ilarity is a satisfying basis of comparison at more
local levels.
Acknowledgements
We are thankful to Greg Kondrak for providing the
source code of the PairHMM training and testing al-
gorithms. We thank the Meertens Instituut for mak-
ing the GTRP data available for research and espe-
55
cially Boudewijn van den Berg for answering our
questions regarding this data. We would also like to
thank Vincent van Heuven for phonetic advice and
Peter Kleiweg for providing support and the soft-
ware we used to create the maps.
References
Patti Adank. 2003. Vowel normalization - a perceptual-
acoustic study of Dutch vowels. Wageningen: Ponsen
& Looijen.
Leonard E. Baum, Ted Petrie, George Soules, and Nor-
man Weiss. 1970. A maximization technique oc-
curring in the statistical analysis of probabilistic func-
tions of Markov Chains. The Annals of Mathematical
Statistics, 41(1):164?171.
Georges De Schutter, Boudewijn van den Berg, Ton Goe-
man, and Thera de Jong. 2005. Morfologische at-
las van de Nederlandse dialecten - deel 1. Amster-
dam University Press, Meertens Instituut - KNAW,
Koninklijke Academie voor Nederlandse Taal- en Let-
terkunde.
Richard Durbin, Sean R. Eddy, Anders Krogh, and
Graeme Mitchison. 1998. Biological Sequence Anal-
ysis : Probabilistic Models of Proteins and Nucleic
Acids. Cambridge University Press, July.
Ton Goeman and Johan Taeldeman. 1996. Fonologie en
morfologie van de Nederlandse dialecten. een nieuwe
materiaalverzameling en twee nieuwe atlasprojecten.
Taal en Tongval, 48:38?59.
Wilbert Heeringa, Peter Kleiweg, Charlotte Gooskens,
and John Nerbonne. 2006. Evaluation of string dis-
tance algorithms for dialectology. In John Nerbonne
and Erhard Hinrichs, editors, Linguistic Distances,
pages 51?62, Shroudsburg, PA. ACL.
Wilbert Heeringa. 2004. Measuring Dialect Pronunci-
ation Differences using Levenshtein Distance. Ph.D.
thesis, Rijksuniversiteit Groningen.
Brett Kessler. 1995. Computational dialectology in Irish
Gaelic. In Proceedings of the seventh conference on
European chapter of the Association for Computa-
tional Linguistics, pages 60?66, San Francisco, CA,
USA. Morgan Kaufmann Publishers Inc.
William Labov. 2001. Principles of Linguistic Change.
Vol.2: Social Factors. Blackwell, Malden, Mass.
Boris M. Lobanov. 1971. Classification of Russian vow-
els spoken by different speakers. Journal of the Acous-
tical Society of America, 49:606?608.
Wesley Mackay and Grzegorz Kondrak. 2005. Com-
puting word similarity and identifying cognates with
Pair Hidden Markov Models. In Proceedings of the
9th Conference on Computational Natural Language
Learning (CoNLL), pages 40?47, Morristown, NJ,
USA. Association for Computational Linguistics.
Wesley Mackay. 2004. Word similarity using Pair Hid-
den Markov Models. Master?s thesis, University of
Alberta.
John Nerbonne and Peter Kleiweg. 2005. Toward a di-
alectological yardstick. Accepted for publication in
Journal of Quantitative Linguistics.
John Nerbonne, Wilbert Heeringa, Erik van den Hout, Pe-
ter van der Kooi, Simone Otten, and Willem van de
Vis. 1996. Phonetic distance between Dutch dialects.
In Gert Durieux, Walter Daelemans, and Steven Gillis,
editors, CLIN VI: Proc. from the Sixth CLIN Meet-
ing, pages 185?202. Center for Dutch Language and
Speech, University of Antwerpen (UIA), Antwerpen.
Louis C. W. Pols, H. R. C. Tromp, and R. Plomp. 1973.
Frequency analysis of Dutch vowels from 50 male
speakers. The Journal of the Acoustical Society of
America, 43:1093?1101.
Lawrence R. Rabiner. 1989. A tutorial on Hidden
Markov Models and selected applications in speech
recognition. Proceedings of the IEEE, 77(2):257?286.
Johan Taeldeman and Geert Verleyen. 1999. De FAND:
een kind van zijn tijd. Taal en Tongval, 51:217?240.
D. J. P. J. Van Nierop, Louis C. W. Pols, and R. Plomp.
1973. Frequency analysis of Dutch vowels from 25
female speakers. Acoustica, 29:110?118.
Martijn Wieling, Wilbert Heeringa, and John Nerbonne.
2007. An aggregate analysis of pronunciation in the
Goeman-Taeldeman-Van Reenen-Project data. Taal
en Tongval. submitted, 12/2006.
Walt Wolfram and Natalie Schilling-Estes. 2003. Dialec-
tology and linguistic diffusion. In Brian D. Joseph and
Richard D. Janda, editors, The Handbook of Historical
Linguistics, pages 713?735. Blackwell, Malden, Mas-
sachusetts.
56
Proceedings of the EACL 2009 Workshop on Language Technology and Resources for Cultural Heritage,
Social Sciences, Humanities, and Education ?LaTeCH ? SHELT&R 2009, pages 18?25,
Athens, Greece, 30 March 2009. c?2009 Association for Computational Linguistics
Multiple sequence alignments in linguistics
Jelena Prokic?
University of Groningen
The Netherlands
j.prokic@rug.nl
Martijn Wieling
University of Groningen
The Netherlands
m.b.wieling@rug.nl
John Nerbonne
University of Groningen
The Netherlands
j.nerbonne@rug.nl
Abstract
In this study we apply and evaluate an
iterative pairwise alignment program for
producing multiple sequence alignments,
ALPHAMALIG (Alonso et al, 2004), us-
ing as material the phonetic transcriptions
of words used in Bulgarian dialectological
research. To evaluate the quality of the
multiple alignment, we propose two new
methods based on comparing each column
in the obtained alignments with the cor-
responding column in a set of gold stan-
dard alignments. Our results show that the
alignments produced by ALPHAMALIG
correspond well with the gold standard
alignments, making this algorithm suitable
for the automatic generation of multiple
string alignments. Multiple string align-
ment is particularly interesting for histor-
ical reconstruction based on sound corre-
spondences.
1 Introduction
Our cultural heritage is studied today not only in
museums, libraries, archives and their digital por-
tals, but also through the genetic and cultural lin-
eaments of living populations. Linguists, popula-
tion geneticists, archaeologists, and physical and
cultural anthropologists are all active in research-
ing cultural heritage on the basis of material that
may or may not be part of official cultural heritage
archives. The common task is that of understand-
ing the histories of the peoples of the world, espe-
cially their migrations and contacts. To research
and understand linguistic cultural heritage we re-
quire instruments which are sensitive to its signals,
and, in particular sensitive to signals of common
provenance. The present paper focuses on pronun-
ciation habits which have been recognized to bear
signals of common provenance for over two hun-
dred years (since the work of Sir William Jones).
We present work in a research line which seeks to
submit pronunciation data to phylogenetic analy-
sis (Gray and Atkinson, 2003) and which requires
an alignment of the (phonological) segments of
cognate words. We focus in this paper on evalu-
ating the quality of multi-aligned pronunciations.
In bioinformatics, sequence alignment is a way
of arranging DNA, RNA or protein sequences in
order to identify regions of similarity and deter-
mine evolutionary, functional or structural simi-
larity between the sequences. There are two main
types of string alignment: pairwise and multiple
string alignment. Pairwise string alignment meth-
ods compare two strings at a time and cannot di-
rectly be used to obtain multiple string alignment
methods (Gusfield, 1997, 343-344). In multiple
string alignment all strings are aligned and com-
pared at the same time, making it a good technique
for discovering patterns, especially those that are
weakly preserved and cannot be detected easily
from sets of pairwise alignments. Multiple string
comparison is considered to be the holy grail of
molecular biology (Gusfield, 1997, 332):
It is the most critical cutting-edge tool for ex-
tracting and representing biologically important,
yet faint or widely dispersed, commonalities
from a set of strings.
Multiple string comparison is not new in lin-
guistic research. In the late 19th century the
Neogrammarians proposed the hypothesis of the
regularity of sound change. According to THE
NEOGRAMMARIAN HYPOTHESIS sound change
occurs regularly and uniformly whenever the ap-
propriate phonetic environment is encountered
(Campbell, 2004). Ever since, the understand-
ing of sound change has played a major role in
the comparative method that is itself based on the
simultaneous comparison of different languages,
i.e. lists of cognate terms from the related lan-
guages. The correct analysis of sound changes
18
requires the simultaneous examination of corre-
sponding sounds in order to compare hypotheses
about their evolution. Alignment identifies which
sounds correspond. Historical linguists align the
sequences manually, while we seek to automate
this process.
In recent years there has been a strong fo-
cus in historical linguistics on the introduction
of quantitative methods in order to develop tools
for the comparison and classification of lan-
guages. For example, in his PhD thesis, Kondrak
(2002) presents algorithms for the reconstruction
of proto-languages from cognates. Warnow et al
(2006) applied methods taken from phylogenet-
ics on Indo-European phonetic data in order to
model language evolution. Heeringa and Joseph
(2007) applied the Levensthein algorithm to the
Dutch pronunciation data taken from Reeks Ned-
erlandse Dialectatlassen and tried to reconstruct a
?proto-language? of Dutch dialects using the pair-
wise alignments.
Studies in historical linguistics and dialectome-
try where string comparison is used as a basis for
calculating the distances between language vari-
eties will profit from tools to multi-align strings
automatically and to calculate the distances be-
tween them. Good multiple alignment is of ben-
efit to all those methods in diachronic linguistics
such as the comparative reconstruction method
or the so-called CHARACTER-BASED METHODS
taken from phylogenetics, which have also been
successfully applied in linguistics (Gray and Jor-
dan, 2000; Gray and Atkinson, 2003; Atkinson
et al, 2005; Warnow et al, 2006). The multi-
alignment systems can help historical linguistics
by reducing the human labor needed to detect the
regular sound correspondences and cognate pairs
of words. They also systematize the linguistic
knowledge in intuitive alignments, and provide a
basis for the application of the quantitative meth-
ods that lead to a better understanding of language
variation and language change.
In this study we apply an iterative pairwise
alignment program for linguistics, ALPHAMA-
LIG, on phonetic transcriptions of words used in
dialectological research. We automatically multi-
align all transcriptions and compare these gener-
ated alignments with manually aligned gold stan-
dard alignments. At the same time we propose
two methods for the evaluation of the multiple se-
quence alignments (MSA).
The structure of this paper is as follows. An
example of a multiple alignment and a discus-
sion of the advantages over pairwise alignment
is given in the next section, after which we dis-
cuss our data set in section 3. Section 4 explains
the iterative pairwise alignment algorithm and the
program ALPHAMALIG. Section 5 discusses the
gold standard and two baselines, while section 6
discusses the novel evaluation procedures. The re-
sults are given in section 7 and we end this paper
with a discussion in section 8.
2 Example of Multiple Sequence
Alignment
In this section we will give an example of the au-
tomatically multi-aligned strings from our data set
and point out some important features of the si-
multaneous comparison of more than two strings.
village1 j "A - - - -
village2 j "A z e - -
village3 - "A s - - -
village4 j "A s - - -
village5 j "A z e k a
village6 j "E - - - -
village7 - "6 s - - -
Figure 1: Example of multiple string alignment
In Figure 1 we have multi-aligned pronuncia-
tions of the word az ?I? automatically generated
by ALPHAMALIG. The advantages of this kind
of alignment over pairwise alignment are twofold:
? First, it is easier to detect and process corre-
sponding phones in words and their alterna-
tions (like ["A] and ["E] and ["6] in the second
column in Figure 1).
? Second, the distances/similarities between
strings can be different in pairwise compari-
son as opposed to multiple comparison. This
is so because multi-aligned strings, unlike
pairwise aligned strings, contain information
on the positions where phones were inserted
or deleted in both strings. For example,
in Figure 1 the pairwise alignment of the
pronunciations from village 1 and village 3
would be:
village1 j "A -
village3 - "A s
19
These two alignments have one matching el-
ement out of three in total, which means
that the similarity between them is 1/3 =
0.33. At the same time the similarity be-
tween these two strings calculated based on
the multi-aligned strings in Figure 1 would
be 4/6 = 0.66. The measurement based on
multi-alignment takes the common missing
material into account as well.
3 Data set
The data set used in this paper consists of pho-
netic transcriptions of 152 words collected from
197 sites evenly distributed all over Bulgaria. It
is part of the project Buldialect?Measuring lin-
guistic unity and diversity in Europe.1 Pronun-
ciations of almost all words were collected from
all the sites and for some words there are mul-
tiple pronunciations per site. Phonetic transcrip-
tions include various diacritics and suprasegmen-
tals, making the total number of unique characters
(types) in the data set 98.2
4 Iterative pairwise alignment
Multiple alignment algorithms iteratively merge
two multiple alignments of two subsets of strings
into a single multiple alignment that is union of
those subsets (Gusfield, 1997). The simplest ap-
proach is to align the two strings that have the
minimum distance over all pairs of strings and it-
eratively align strings having the smallest distance
to the already aligned strings in order to generate
a new multiple alignment. Other algorithms use
different initializations and different criteria in se-
lecting the new alignments to merge. Some begin
with the longest (low cost) alignment instead of
the least cost absolutely. A string with the smallest
edit distance to any of the already merged strings
is chosen to be added to the strings in the multiple
alignment. In choosing the pair with the minimal
distance, all algorithms are greedy, and risk miss-
ing optimal alignments.
ALPHAMALIG is an iterative pairwise align-
ment program for bilingual text alignment. It uses
the strategy of merging multiple alignments of
subsets of strings, instead of adding just one string
1The project is sponsored by Volkswagen Stiftung.
More information can be found at http://sfs.uni-
tuebingen.de/dialectometry.
2The data is publicly available and can be found at
http://www.bultreebank.org/BulDialects/index.html.
at the time to the already aligned strings.3 It was
originally developed to align corresponding words
in bilingual texts, i.e. with textual data, but it func-
tions with any data that can be represented as a
sequence of symbols of a finite alphabet. In addi-
tion to the input sequences, the program needs to
know the alphabet and the distances between each
token pair and each pair consisting of a token and
a gap.
In order to perform multiple sequence align-
ments of X-SAMPA word transcriptions we modi-
fied ALPHAMALIG slightly so it could work with
the tokens that consist of more than one symbol,
such as ["e], ["e:] and [t_S]. The distances be-
tween the tokens were specified in such a way that
vowels can be aligned only with vowels and con-
sonants only with consonants. The same tokens
are treated as identical and the distance between
them is set to 0. The distance between any token
in the data set to a gap symbol has the same cost
as replacing a vowel with a vowel or a consonant
with a consonant. Except for this very general lin-
guistic knowledge, no other data-specific informa-
tion was given to the program. In this research we
do not use any phonetic features in order to define
the segments more precisely and to calculate the
distances between them in a more sensitive way
than just making a binary ?match/does-not-match-
distinction?, since we want to keep the system lan-
guage independent and robust to the highest pos-
sible degree.
5 Gold standard and baseline
In order to evaluate the performance of AL-
PHAMALIG, we compared the alignments ob-
tained using this program to the manually aligned
strings, our gold standard, and to the alignments
obtained using two very simple techniques that
are described next: simple baseline and advanced
baseline.
5.1 Simple baseline
The simplest way of aligning two strings would be
to align the first element from one string with the
first element from the other string, the second el-
ement with the second and so on. If two strings
are not of equal length, the remaining unaligned
tokens are aligned with the gap symbol which rep-
3More information on ALPHAMALIG can be found
at http://alggen.lsi.upc.es/recerca/align/alphamalig/intro-
alphamalig.html.
20
resents an insertion or a deletion. This is the align-
ment implicit in Hamming distance, which ignores
insertions and deletions.
By applying this simple method, we obtained
multiple sequence alignments for all words in our
data set. An example of such a multiple sequence
alignment is shown in Figure 2. These align-
ments were used to check how difficult the mul-
tiple sequence alignment task is for our data and
how much improvement is obtained using more
advanced techniques to multi-align strings.
j "A - -
j "A z e
"A S - -
Figure 2: Simple baseline
5.2 Advanced baseline
Our second baseline is more advanced than the
first and was created using the following proce-
dure:
1. for each word the longest string among all
pronunciations is located
2. all strings are pairwise aligned against the
longest string using the Levensthein algo-
rithm (Heeringa, 2004). We refer to both se-
quences in a pairwise alignment as ALIGN-
MENT LINES. Note that alignment lines in-
clude hyphens indicating the places of inser-
tions and deletions.
3. the alignment lines?all of equal length?are
extracted
4. all extracted alignment lines are placed below
each other to form the multiple alignment
An example of combining pairwise alignments
against the longest string (in this case [j"aze]) is
shown in Figure 3.
5.3 Gold standard
Our gold standard was created by manually cor-
recting the advanced baseline alignments de-
scribed in the previous section. The gold stan-
dard results and both baseline results consist of
152 files with multi-aligned strings, one for each
word. The pronunciations are ordered alphabeti-
cally according to the village they come from. If
there are more pronunciations per site, they are all
present, one under the other.
j "A z e
j "A - -
j "A z e
- "A S -
j "A - -
j "A z e
- "A S -
Figure 3: Advanced baseline. The top two align-
ments each contain two alignment lines, and the
bottom one contains three.
6 Evaluation
Although multiple sequence alignments are
broadly used in molecular biology, there is still no
widely accepted objective function for evaluating
the goodness of the multiple aligned strings
(Gusfield, 1997). The quality of the existing
methods used to produce multiple sequence
alignments is judged by the ?biological meaning
of the alignments they produce?. Since strings
in linguistics cannot be judged by the biological
criteria used in string evaluation in biology, we
were forced to propose evaluation methods that
would be suitable for the strings in question. One
of the advantages we had was the existence of
the gold standard alignments, which made our
task easier and more straightforward?in order to
determine the quality of the multi-aligned strings,
we compare outputs of the different algorithms to
the gold standard. Since there is no off-the-shelf
method that can be used for comparison of multi-
aligned strings to a gold standard, we propose
two novel methods?one sensitive to the order of
columns in two alignments and another that takes
into account only the content of each column.
6.1 Column dependent method
The first method we developed compares the con-
tents of the columns and also takes the column se-
quence into account. The column dependent eval-
uation (CDE) procedure is as follows:
? Each gold standard column is compared to
the most similar column out of two neigh-
boring columns of a candidate multiple align-
ment. The two neighboring columns depend
on the previous matched column j and have
indices j +1 and j +2 (at the start j = 0). It
is possible that there are columns in the can-
didate multiple alignment which remain un-
matched, as well as columns at the end of the
gold standard which remain unmatched.
21
? The similarity of a candidate column with a
gold standard column is calculated by divid-
ing the number of correctly placed elements
in every candidate column by the total num-
ber of elements in the column. A score of
1 indicates perfect overlap, while a score of
0 indicates the columns have no elements in
common.
? The similarity score of the whole multiple
alignment (for a single word) is calculated by
summing the similarity score of each candi-
date column and dividing it by the total num-
ber of matched columns plus the total num-
ber of unmatched columns in both multiple
alignments.
? The final similarity score between the set of
gold standard alignments with the set of can-
didate multiple alignments is calculated by
averaging the multiple alignment similarity
scores for all strings.
As an example consider the multiple alignments
in Figure 4, with the gold standard alignment (GS)
on the left and the generated alignment (GA) on
the right.
w rj "E m e
v r "e m i
u rj "e m i
v rj "e m i
w - rj "E m e
v - r "e m i
- u rj "e m i
v - rj "e m i
Figure 4: GS and ALPHAMALIG multiple string
alignments, the gold standard alignment left, the
ALPHAMALIG output right.
The evaluation starts by comparing the first col-
umn of the GS with the first and second column
of the GA. The first column of the GA is the best
match, since the similarity score between the first
columns is 0.75 (3 out of 4 elements match). In
similar fashion, the second column of the GS is
compared with the second and the third column of
the GA and matched with the third column of GA
with a similarity score of 1 (all elements match).
The third GS column is matched with the fourth
GA column, the fourth GS column with the fifth
GA column and the fifth GS column with the sixth
GA column (all three having a similarity score of
1). As a consequence, the second column of the
GA remains unmatched. In total, five columns are
matched and one column remains unmatched. The
total score of the GA equals:
(0.75 + 1 + 1 + 1 + 1)
(5 + 1)
= 0.792
It is clear that this method punishes unmatched
columns by increasing the value of the denomina-
tor in the similarity score calculation. As a conse-
quence, swapped columns are punished severely,
which is illustrated in Figure 5.
"o rj @ j -
"o rj @ - u
"o rj @ f -
"o rj @ - j
"o rj @ u -
"o rj @ - f
Figure 5: Two alignments with swapped columns
In the alignments in Figure 5, the first three
columns of GS would be matched with the first
three columns of GA with a score of 1, the fourth
would be matched with the fifth, and two columns
would be left unmatched: the fifth GS column and
the fourth GA column yielding a total similarity
score of 4/6 = 0.66. Especially in this case this is
undesirable, as both sequences of these columns
represent equally reasonable multiple alignment
and should have a total similarity score of 1.
We therefore need a less strict evaluation method
which does not insist on the exact ordering. An
alternative method is introduced and discussed in
the following section.
6.2 Modified Rand Index
In developing an alternative evaluation we pro-
ceeded from the insight that the columns of a mul-
tiple alignment are a sort of PARTITION of the el-
ements of the alignment strings, i.e., they consti-
tute a set of disjoint multi-sets whose union is the
entire multi-set of segments in the multiple align-
ment. Each column effectively assigns its seg-
ments to a partition, which clearly cannot overlap
with the elements of another column (partition).
Since every segment must fall within some col-
umn, the assignment is also exhaustive.
Our second evaluation method is therefore
based on the modified Rand index (Hubert and
Arabie, 1985). The modified Rand index is used
in classification for comparing two different parti-
tions of a finite set of objects. It is based on the
Rand index (Rand, 1971), one of the most popular
measures for comparing the degree to which parti-
tions agree (in classification).
Given a set of n elements S = o1, ...on and two
partitions of S, U and V , the Rand index R is de-
fined as:
22
R =
a + b
a + b + c + d
where:
? a: the number of pairs of elements in S that
are in the same set (column) in U and in the
same set in V
? b: the number of pairs of elements in S that
are in different sets (columns) inU and in dif-
ferent sets in V
? c: the number of pairs of elements in S that
are in the same set in U and in different sets
in V
? d: the number of pairs of elements in S that
are in different sets in U and in the same set
in V
Consequently, a and b are the number of pairs of
elements on which two classifications agree, while
c and d are the number of pairs of elements on
which they disagree. In our case classifications
agree about concrete segment tokens only in the
cases where they appear in the same columns in
the alignments.
The value of Rand index ranges between 0 and
1, with 0 indicating that the two partitions (multi-
alignments) do not agree on any pair of points and
1 indicating that the data partitions are exactly the
same.4 A problem with the Rand index is that it
does not return a constant value (zero) if two par-
titions are picked at random. Hubert and Arabie
(1985) suggested a modification of the Rand in-
dex (MRI) that corrects this property. It can be
expressed in the general form as:
MRI =
Rand index? Expected index
Maximum index? Expected index
The expected index is the expected number of
pairs which would be placed in the same set in U
and in the same set in V by chance. The maximum
index represents the maximum number of objects
that can be put in the same set in U and in the
same set in V . The MRI value ranges between ?1
and 1, with perfect overlap being indicated by 1
and values ? 0 indicating no overlap. For a more
detailed explanation of the modified Rand index,
please refer to Hubert and Arabie (1985).
4In dialectometry, this index was used by Heeringa et al
(2002) to validate dialect clustering methods.
We would like to emphasize that it is clear that
the set of columns of a multi-alignment have more
structure than a partition sec, in particular because
the columns (subpartitions) are ordered, unlike the
subpartitions in a partition. But we shall compen-
sate for this difference by explicitly marking order.
"o [1] rj [2] @ [3] j [4] -
"o [5] rj [6] @ [7] - u [8]
"o [9] rj [10] @ [11] f [12] -
Figure 6: Annotated alignment
In our study, each segment token in each tran-
scription was treated as a different object (see Fig-
ure 6), and every column was taken to be a sub-
partition to which segment tokens are assigned.
Both alignments in Figure 5 have 12 phones that
are put into 5 groups. We ?tag? each token sequen-
tially in order to distinguish the different tokens of
a single segment from each other, but note that the
way we do this also introduces an order sensitivity
in the measure. The two partitions obtained are:
GS1 = {1,5,9}
GS2 = {2,6,10}
GS3 = {3,7,11}
GS4 = {4,12}
GS5 = {8}
GA1 = {1,5,9}
GA2 = {2,6,10}
GA3 = {3,7,11}
GA4 = {8}
GA5 = {4,12}
Using the modified Rand index the quality of
each column is checked, regardless of whether the
columns are in order. The MRI for the alignments
in Figure 5 will be 1, because both alignments
group segment tokens in the same way. Even
though columns four and five are swapped, in both
classifications phones [j] and [f] are grouped to-
gether, while sound [u] forms a separate group.
The MRI itself only takes into account the
quality of each column separately since it sim-
ply checks whether the same elements are together
in the candidate alignment as in the gold-standard
alignment. It is therefore insensitive to the order-
ing of columns. While it may have seemed coun-
terintuitive linguistically to proceed from an order-
insensitive measure, the comparison of ?tagged to-
kens? described above effectively reintroduces or-
der sensitivity.
In the next section we describe the results of ap-
plying both evaluation methods on the automati-
cally generated multiple alignments.
23
7 Results
After comparing all files of the baseline algo-
rithms and ALPHAMALIG against the gold stan-
dard files according to the column dependent eval-
uation method and the modified Rand index, the
average score is calculated by summing up all
scores and dividing them by the number of word
files (152).
The results are given in Table 1 and also in-
clude the number of words with perfect multi-
alignments (i.e. identical to the gold standard).
Using CDE, ALPHAMALIG scored 0.932 out of
1.0 with 103 perfectly aligned files. The result
for the simple baseline was 0.710 with 44 per-
fectly aligned files. As expected, the result for
the advanced baseline was in between these two
results?0.869 with 72 files that were completely
identical to the GS files. Using MRI to eval-
uate the alignments generated we obtained gen-
erally higher scores for all three algorithms, but
with the same ordering. ALPHAMALIG scored
0.982, with 104 perfectly aligned files. The ad-
vanced baseline had a lower score of 0.937 and
74 perfect alignments. The simple baseline per-
formed worse, scoring 0.848 and having 44 per-
fectly aligned files.
The scores of the CDE evaluation method are
lower than the MRI scores, which is due to the first
method?s problematic sensitivity to column order-
ing in the alignments. It is clear that in both evalu-
ation methods ALPHAMALIG outperforms both
baseline alignments by a wide margin.
It is important to notice that the scores for the
simple baseline are reasonably high, which can
be explained by the structure of our data set.
The variation of word pronunciations is relatively
small, making string alignment easier. However,
ALPHAMALIG obtained much higher scores us-
ing both evaluation methods.
Additional qualitative error analysis reveals that
the errors of ALPHAMALIG are mostly caused by
the vowel-vowel consonant-consonant alignment
restriction. In the data set there are 21 files that
contain metathesis. Since vowel-consonant align-
ments were not allowed in ALPHAMALIG, align-
ments produced by this algorithm were different
from the gold standard, as illustrated in Figure 7.
The vowel-consonant restriction is also respon-
sible for wrong alignments in some words where
metathesis is not present, but where the vowel-
consonant alignment is still preferred over align-
v l "7 k
v "7 l k
v l "7 - k
v - "7 l k
Figure 7: Two alignments with metathesis
ing vowels and/or consonants with a gap (see for
example Figure 4).
The other type of error present in the AL-
PHAMALIG alignments is caused by the fact
that all vowel-vowel and consonant-consonant dis-
tances receive the same weight. In Figure 8
the alignment of word bjahme ?were? produced
by ALPHAMALIG is wrong because instead of
aligning [mj] with [m] and [m] it is wrongly
aligned with [x] and [x], while [x] is aligned with
[S] instead of aligning it with [x] and [x].
b "E S u x - m e -
bj "A - - x - m i -
b "e x - mj - - 7 -
Figure 8: Alignment error produced by AL-
PHAMALIG
8 Discussion and future work
In this study we presented a first attempt to auto-
matically multi-align phonetic transcriptions. The
algorithmwe used to generate alignments has been
shown to be very reliable, produce alignments of
good quality, with less than 2% error at the seg-
ment level. In this study we used only very sim-
ple linguistic knowledge in order to align strings.
The only restriction we imposed was that a vowel
should only be aligned with a vowel and a con-
sonant only with a consonant. The system has
shown to be very robust and to produce good qual-
ity alignments with a very limited information on
the distances between the tokens. However, in the
future we would like to apply this algorithm using
more detailed segment distances, so that we can
work without vowel-consonant restrictions. Using
more detailed language specific feature system for
each phone, we believe we may be able to improve
the produced alignments further. This especially
holds for the type of errors illustrated in Figure 8
where it is clear that [mj] is phonetically closer to
[m] than to [x] sound.
As our data set was relatively simple (indicated
by the reasonable performance of our simple base-
line algorithm), we would very much like to eval-
uate ALPHAMALIG against a more complex data
24
CDE CDE perfect columns MRI MRI perfect columns
Simple baseline 0.710 44 0.848 44
Advanced baseline 0.869 72 0.937 74
ALPHAMALIG 0.932 103 0.982 104
Table 1: Results of evaluating outputs of the different algorithms against the GS
set and try to replicate the good results we ob-
tained here. On one hand, high performance of
both baseline algorithms show that our task was
relatively easy. On the other hand, achieving per-
fect alignments will be very difficult, if possible at
all.
Additionally, we proposed two methods to eval-
uate multiple aligned strings in linguistic research.
Although these systems could be improved, both
of them are giving a good estimation of the qual-
ity of the generated alignments. For the examined
data, we find MRI to be better evaluation tech-
nique since it overcomes the problem of swapped
columns.
In this research we tested and evaluated AL-
PHAMALIG on the dialect phonetic data. How-
ever, multiple sequence alignments can be also
applied on the sequences of sentences and para-
graphs. This makes multiple sequence alignment
algorithm a powerful tool for mining text data in
social sciences, humanities and education.
Acknowledgements
We are thankful to Xavier Messeguer of the Tech-
nical University of Catalonia who kindly sup-
plied us with the source code of ALPHAMALIG.
We also thank Therese Leinonen and Sebastian
K?rschner of the University of Groningen and Es-
teve Valls i Alecha of the University of Barcelona
for their useful feedback on our ideas.
References
Laura Alonso, Irene Castellon, Jordi Escribano, Xavier
Messeguer, and Lluis Padro. 2004. Multiple
Sequence Alignment for characterizing the linear
structure of revision. In Proceedings of the 4th In-
ternational Conference on Language Resources and
Evaluation.
Quentin Atkinson, Geoff Nicholls, David Welch, and
Russell Gray. 2005. From words to dates: water
into wine, mathemagic or phylogenetic inference.
Transcriptions of the Philological Society, 103:193?
219.
Lyle Campbell. 2004. Historical Linguistics: An In-
troduction. Edinburgh University Press, second edi-
tion.
Russel D. Gray and Quentin D. Atkinson. 2003.
Language-tree divergence times support the Ana-
tolian theory of Indo-European origin. Nature,
426:435?339.
Russel D. Gray and Fiona M. Jordan. 2000. Lan-
guage trees support the express-train sequence of
Austronesian expansion. Nature, 405:1052?1055.
Dan Gusfield. 1997. Algorithms on Strings, Trees and
Sequences: Computer Science and Computational
Biology. Cambridge University Press.
Wilbert Heeringa and Brian Joseph. 2007. The rela-
tive divergence of Dutch dialect pronunciations from
their common source: An exploratory study. In John
Nerbonne, T. Mark Ellison, and Grzegorz Kondrak,
editors, Proceedings of the Ninth Meeting of the ACL
Special Interest Group in Computational Morphol-
ogy and Phonology.
Wilbert Heeringa, John Nerbonne, and Peter Kleiweg.
2002. Validating dialect comparison methods. In
Wolfgang Gaul and Gunter Ritter, editors, Classifi-
cation, Automation, and New Media. Proceedings of
the 24th Annual Conference of the Gesellschaft f?r
Klassifikation e. V., University of Passau, March 15-
17, 2000, pages 445?452. Springer, Berlin, Heidel-
berg and New York.
Wilbert Heeringa. 2004. Measuring Dialect Pronunci-
ation Differences using Levenshtein Distance. Ph.D.
thesis, Rijksuniversiteit Groningen.
Lawrence Hubert and Phipps Arabie. 1985. Compar-
ing partitions. Journal of Classification, 2:193?218.
Grzegorz Kondrak. 2002. Algorithms for Language
Reconstruction. PhD Thesis, University of Toronto.
William M. Rand. 1971. Objective criteria for the
evaluation of clustering methods. Journal of Amer-
ican Statistical Association, 66(336):846?850, De-
cember.
Tandy Warnow, Steven N. Evans, Donald Ringe, and
Luay Nakhleh. 2006. A stochastic model of lan-
guage evolution that incorporates homoplasy and
borrowing. In Peter Forster and Colin Renfrew, ed-
itors, Phylogenetic Methods and the Prehistory of
Languages. MacDonald Institute for Archaeological
Research, Cambridge.
25
Proceedings of the EACL 2009 Workshop on Language Technology and Resources for Cultural Heritage,
Social Sciences, Humanities, and Education ?LaTeCH ? SHELT&R 2009, pages 26?34,
Athens, Greece, 30 March 2009. c?2009 Association for Computational Linguistics
Evaluating the pairwise string alignment of pronunciations
Martijn Wieling
University of Groningen
The Netherlands
m.b.wieling@rug.nl
Jelena Prokic?
University of Groningen
The Netherlands
j.prokic@rug.nl
John Nerbonne
University of Groningen
The Netherlands
j.nerbonne@rug.nl
Abstract
Pairwise string alignment (PSA) is an im-
portant general technique for obtaining a
measure of similarity between two strings,
used e.g., in dialectology, historical lin-
guistics, transliteration, and in evaluating
name distinctiveness. The current study
focuses on evaluating different PSA meth-
ods at the alignment level instead of via
the distances it induces. About 3.5 million
pairwise alignments of Bulgarian phonetic
dialect data are used to compare four al-
gorithms with a manually corrected gold
standard. The algorithms evaluated in-
clude three variants of the Levenshtein al-
gorithm as well as the Pair Hidden Markov
Model. Our results show that while all
algorithms perform very well and align
around 95% of all alignments correctly,
there are specific qualitative differences in
the (mis)alignments of the different algo-
rithms.
1 Introduction
Our cultural heritage is not only accessible
through museums, libraries, archives and their
digital portals, it is alive and well in the varied
cultural habits practiced today by the various peo-
ples of the world. To research and understand this
cultural heritage we require instruments which are
sensitive to its signals, and, in particular sensitive
to signals of common provenance. The present
paper focuses on speech habits which even today
bear signals of common provenance in the vari-
ous dialects of the world?s languages, and which
have also been recorded and preserved in major
archives of folk culture internationally. We present
work in a research line which seeks to develop
digital instruments capable of detecting common
provenance among pronunciation habits, focusing
in this paper on the issue of evaluating the quality
of these instruments.
Pairwise string alignment (PSA) methods, like
the popular Levenshtein algorithm (Levenshtein,
1965) which uses insertions (alignments of a seg-
ment against a gap), deletions (alignments of a gap
against a segment) and substitutions (alignments
of two segments) often form the basis of deter-
mining the distance between two strings. Since
there are many alignment algorithms and specific
settings for each algorithm influencing the dis-
tance between two strings (Nerbonne and Klei-
weg, 2007), evaluation is very important in deter-
mining the effectiveness of the distance methods.
Determining the distance (or similarity) be-
tween two phonetic strings is an important aspect
of dialectometry, and alignment quality is impor-
tant in applications in which string alignment is
a goal in itself, for example, determining if two
words are likely to be cognate (Kondrak, 2003),
detecting confusable drug names (Kondrak and
Dorr, 2003), or determining whether a string is
the transliteration of the same name from another
writing system (Pouliquen, 2008).
In this paper we evaluate string distance mea-
sures on the basis of data from dialectology. We
therefore explain a bit more of the intended use of
the pronunciation distance measure.
Dialect atlases normally contain a large num-
ber of pronunciations of the same word in various
places throughout a language area. All pairs of
pronunciations of corresponding words are com-
pared in order to obtain a measure of the aggre-
gate linguistic distance between dialectal varieties
(Heeringa, 2004). It is clear that the quality of the
measurement is of crucial importance.
Almost all evaluation methods in dialectometry
focus on the aggregate results and ignore the in-
dividual word-pair distances and individual align-
ments on which the distances are based. The fo-
cus on the aggregate distance of 100 or so word
26
pairs effectively hides many differences between
methods. For example, Heeringa et al (2006) find
no significant differences in the degrees to which
several pairwise string distance measures correlate
with perceptual distances when examined at an ag-
gregate level. Wieling et al (2007) and Wieling
and Nerbonne (2007) also report almost no differ-
ence between different PSA algorithms at the ag-
gregate level. It is important to be able to evaluate
the different techniques more sensitively, which is
why this paper examines alignment quality at the
segment level.
Kondrak (2003) applies a PSA algorithm to
align words in different languages in order to de-
tect cognates automatically. Exceptionally, he
does provide an evaluation of the string alignments
generated by different algorithms. But he restricts
his examination to a set of only 82 gold standard
pairwise alignments and he only distinguishes cor-
rect and incorrect alignments and does not look at
misaligned phones.
In the current study we introduce and evaluate
several alignment algorithms more extensively at
the alignment level. The algorithms we evaluate
include the Levenshtein algorithm (with syllabic-
ity constraint), which is one of the most popular
alignment methods and has successfully been used
in determining pronunciation differences in pho-
netic strings (Kessler, 1995; Heeringa, 2004). In
addition we look at two adaptations of the Lev-
enshtein algorithm. The first adaptation includes
the swap-operation (Wagner and Lowrance, 1975),
while the second adaptation includes phonetic seg-
ment distances, which are generated by applying
an iterative pointwise mutual information (PMI)
procedure (Church and Hanks, 1990). Finally we
include alignments generated with the Pair Hid-
den Markov Model (PHMM) as introduced to lan-
guage studies by Mackay and Kondrak (2005).
They reported that the Pair Hidden Markov Model
outperformed ALINE, the best performing algo-
rithm at the alignment level in the aforementioned
study of Kondrak (2003). The PHMM has also
successfully been used in dialectology by Wieling
et al (2007).
2 Dataset
The dataset used in this study consists of 152
words collected from 197 sites equally distributed
over Bulgaria. The transcribed word pronuncia-
tions include diacritics and suprasegmentals (e.g.,
intonation). The total number of different phonetic
types (or segments) is 98.1
The gold standard pairwise alignment was au-
tomatically generated from a manually corrected
gold standard set of N multiple alignments (see
Prokic? et al, 2009 ) in the following way:
? Every individual string (including gaps) in
the multiple alignment is aligned with ev-
ery other string of the same word. With 152
words and 197 sites and in some cases more
than one pronunciations per site for a cer-
tain word, the total number of pairwise align-
ments is about 3.5 million.
? If a resulting pairwise alignment contains a
gap in both strings at the same position (a
gap-gap alignment), these gaps are removed
from the pairwise alignment. We justify this,
reasoning that no alignment algorithm may
be expected to detect parallel deletions in a
single pair of words. There is no evidence for
this in the single pair.
To make this clear, consider the multiple align-
ment of three Bulgarian dialectal variants of the
word ?I? (as in ?I am?):
j "A s
"A z i
j "A
Using the procedure above, the three generated
pairwise alignments are:
j "A s j "A s "A z i
"A z i j "A j "A
3 Algorithms
Four algorithms are evaluated with respect to the
quality of their alignments, including three vari-
ants of the Levenshtein algorithm and the Pair
Hidden Markov Model.
3.1 The VC-sensitive Levenshtein algorithm
The Levenshtein algorithm is a very efficient dy-
namic programming algorithm, which was first in-
troduced by Kessler (1995) as a tool for computa-
tionally comparing dialects. The Levenshtein dis-
tance between two strings is determined by count-
ing the minimum number of edit operations (i.e.
insertions, deletions and substitutions) needed to
transform one string into the other.
1The dataset is available online at the website
http://www.bultreebank.org/BulDialects/
27
For example, the Levenshtein distance between
[j"As] and ["Azi], two Bulgarian dialectal variants
of the word ?I? (as in ?I am?), is 3:
j"As delete j 1
"As subst. s/z 1
"Az insert i 1
"Azi
3
The corresponding alignment is:
j "A s
"A z i
1 1 1
The Levenshtein distance has been used fre-
quently and successfully in measuring linguis-
tic distances in several languages, including Irish
(Kessler, 1995), Dutch (Heeringa, 2004) and Nor-
wegian (Heeringa, 2004). Additionally, the Lev-
enshtein distance has been shown to yield aggre-
gate results that are consistent (Cronbach?s ? =
0.99) and valid when compared to dialect speak-
ers judgements of similarity (r ? 0.7; Heeringa et
al., 2006).
Following Heeringa (2004), we have adapted
the Levenshtein algorithm slightly, so that it does
not allow alignments of vowels with consonants.
We refer to this adapted algorithm as the VC-
sensitive Levenshtein algorithm.
3.2 The Levenshtein algorithm with the swap
operation
Because metathesis (i.e. transposition of sounds)
occurs relatively frequently in the Bulgarian di-
alect data (in 21 of 152 words), we extend the
VC-sensitive Levenshtein algorithm as described
in section 3.1 to include the swap-operation (Wag-
ner and Lowrance, 1975), which allows two ad-
jacent characters to be interchanged. The swap-
operation is also known as a transposition, which
was introduced with respect to detecting spelling
errors by Damerau (1964). As a consequence the
Damerau distance refers to the minimum number
of insertions, deletions, substitutions and transpo-
sitions required to transform one string into the
other. In contrast to Wagner and Lowrance (1975)
and in line with Damerau (1964) we restrict the
swap operation to be only allowed for string X
and Y when xi = yi+1 and yi = xi+1 (with xi
being the token at position i in string X):
xi xi+1
yi yi+1
>< 1
Note that a swap-operation in the alignment is in-
dicated by the symbol ?><?. The first number fol-
lowing this symbol indicates the cost of the swap-
operation.
Consider the alignment of [vr"7] and [v"7r],2
two Bulgarian dialectal variants of the word ?peak?
(mountain). The alignment involves a swap and
results in a total Levenshtein distance of 1:
v r "7
v "7 r
>< 1
However, the alignment of the transcription [vr"7]
with another dialectal transcription [v"ar] does not
allow a swap and yields a total Levenshtein dis-
tance of 2:
v r "7
v "a r
1 1
Including just the option of swapping identical
segments in the implementation of the Leven-
shtein algorithm is relatively easy. We set the
cost of the swap operation to one3 plus twice the
cost of substituting xi with yi+1 plus twice the
cost of substituting yi with xi+1. In this way the
swap operation will be preferred when xi = yi+1
and yi = xi+1, but not when xi 6= yi+1 and/or
yi 6= xi+1. In the first case the cost of the swap
operation is 1, which is less than the cost of the
alternative of two substitutions. In the second case
the cost is either 3 (if xi 6= yi+1 or yi 6= xi+1) or
5 (if xi 6= yi+1 and yi 6= xi+1), which is higher
than the cost of using insertions, deletions and/or
substitutions.
Just as in the previous section, we do not allow
vowels to align with consonants (except in the case
of a swap).
3.3 The Levenshtein algorithm with
generated segment distances
The VC-sensitive Levenshtein algorithm as de-
scribed in section 3.1 only distinguishes between
vowels and consonants. However, more sensi-
tive segment distances are also possible. Heeringa
(2004) experimented with specifying phonetic
segment distances based on phonetic features and
2We use transcriptions in which stress is marked on
stressed vowels instead of before stressed syllables. We fol-
low in this the Bulgarian convention instead of the IPA con-
vention.
3Actually the cost is set to 0.999 to prefer an alignment
involving a swap over an alternative alignment involving only
regular edit operations.
28
also based on acoustic differences derived from
spectrograms, but he did not obtain improved re-
sults at the aggregate level.
Instead of using segment distances as these are
(incompletely) suggested by phonetic or phono-
logical theory, we tried to determine the sound
distances automatically based on the available
data. We used pointwise mutual information
(PMI; Church and Hanks, 1990) to obtain these
distances. It generates segment distances by as-
sessing the degree of statistical dependence be-
tween the segments x and y:
PMI(x, y) = log2
(
p(x, y)
p(x) p(y)
)
(1)
Where:
? p(x, y): the number of times x and y occur
at the same position in two aligned strings
X and Y , divided by the total number of
aligned segments (i.e. the relative occurrence
of the aligned segments x and y in the whole
dataset). Note that either x or y can be a gap
in the case of insertion or deletion.
? p(x) and p(y): the number of times x (or y)
occurs, divided by the total number of seg-
ment occurrences (i.e. the relative occurrence
of x or y in the whole dataset). Dividing by
this term normalizes the empirical frequency
with respect to the frequency expected if x
and y are statistically independent.
The greater the PMI value, the more segments tend
to cooccur in correspondences. Negative PMI val-
ues indicate that segments do not tend to cooccur
in correspondences, while positive PMI values in-
dicate that segments tend to cooccur in correspon-
dences. The segment distances can therefore be
generated by subtracting the PMI value from 0 and
adding the maximum PMI value (i.e. lowest dis-
tance is 0). In that way corresponding segments
obtain the lowest distance.
Based on the PMI value and its conversion to
segment distances, we developed an iterative pro-
cedure to automatically obtain the segment dis-
tances:
1. The string alignments are generated using the
VC-sensitive Levenshtein algorithm (see sec-
tion 3.1).4
4We also used the Levenshtein algorithm without the
vowel-consonant restriction to generate the PMI values, but
this had a negative effect on the performance.
2. The PMI value for every segment pair is cal-
culated according to (1) and subsequently
transformed to a segment distance by sub-
tracting it from zero and adding the maxi-
mum PMI value.
3. The Levenshtein algorithm using these seg-
ment distances is applied to generate a new
set of alignments.
4. Step 2 and 3 are repeated until the alignments
of two consecutive iterations do not differ
(i.e. convergence is reached).
The potential merit of using PMI-generated seg-
ment distances can be made clear by the following
example. Consider the strings [v"7n] and [v"7?k@],
Bulgarian dialectal variants of the word ?outside?.
The VC-sensitive Levenshtein algorithm yields
the following (correct) alignment:
v "7 n
v "7 ? k @
1 1 1
But also the alternative (incorrect) alignment:
v "7 n
v "7 ? k @
1 1 1
The VC-sensitive Levenshtein algorithm gener-
ates the erroneous alignment because it has no way
to identify that the consonant [n] is nearer to the
consonant [?] than to the consonant [k]. In con-
trast, the Levenshtein algorithm which uses the
PMI-generated segment distances only generates
the correct first alignment, because the [n] occurs
relatively more often aligned with [?] than with
[k] so that the distance between [n] and [?] will
be lower than the distance between [n] and [k].
The idea behind this procedure is similar to Ris-
tad?s suggestion to learn segment distances for edit
distance using an expectation maximization algo-
rithm (Ristad and Yianilos, 1998). Our approach
differs from their approach in that we only learn
segment distances based on the alignments gener-
ated by the VC-sensitive Levenshtein algorithm,
while Ristad and Yianilos (1998) learn segment
distances by considering all possible alignments of
two strings.
3.4 The Pair Hidden Markov Model
The Pair Hidden Markov Model (PHMM) also
generates alignments based on automatically gen-
erated segment distances and has been used suc-
29
Figure 1: Pair Hidden Markov Model. Image
courtesy of Mackay and Kondrak (2005).
cessfully in language studies (Mackay and Kon-
drak, 2005; Wieling et al, 2007).
A Hidden Markov Model (HMM) is a proba-
bilistic finite-state transducer that generates an ob-
servation sequence by starting in an initial state,
going from state to state based on transition prob-
abilities and emitting an output symbol in each
state based on the emission probabilities in that
state for that output symbol (Rabiner, 1989). The
PHMM was originally proposed by Durbin et al
(1998) for aligning biological sequences and was
first used in linguistics by Mackay and Kondrak
(2005) to identify cognates. The PHMM differs
from the regular HMM in that it outputs two ob-
servation streams (i.e. a series of alignments of
pairs of individual segments) instead of only a se-
ries of single symbols. The PHMM displayed in
Figure 1 has three emitting states: the substitution
(?match?) state (M) which emits two aligned sym-
bols, the insertion state (Y) which emits a symbol
and a gap, and the deletion state (X) which emits
a gap and a symbol.
The following example shows the state se-
quence for the pronunciations [j"As] and ["Azi] (En-
glish ?I?):
j "A s
"A z i
X M M Y
Before generating the alignments, all probabil-
ities of the PHMM have to be estimated. These
probabilities consist of the 5 transition probabili-
ties shown in Figure 1: , ?, ?, ?XY and ?M . In
addition there are 98 emission probabilities for the
insertion state and the deletion state (one for ev-
ery segment) and 9604 emission probabilities for
the substitution state. The probability of starting in
one of the three states is set equal to the probability
of going from the substitution state to that particu-
lar state. The Baum-Welch expectation maximiza-
tion algorithm (Baum et al, 1970) can be used to
iteratively reestimate these probabilities until a lo-
cal optimum is found.
To prevent order effects in training, every word
pair is considered twice (e.g., wa ? wb and wb ?
wa). The resulting insertion and deletion probabil-
ities are therefore the same (for each segment), and
the probability of substituting x for y is equal to
the probability of substituting y for x, effectively
yielding 4802 distinct substitution probabilities.
Wieling et al (2007) showed that using Dutch
dialect data for training, sensible segment dis-
tances were obtained; acoustic vowel distances
on the basis of spectrograms correlated signifi-
cantly (r = ?0.72) with the vowel substitution
probabilities of the PHMM. Additionally, proba-
bilities of substituting a symbol with itself were
much higher than the probabilities of substitut-
ing an arbitrary vowel with another non-identical
vowel (mutatis mutandis for consonants), which
were in turn much higher than the probabilities of
substituting a vowel for a consonant.
After training, the well known Viterbi algorithm
can be used to obtain the best alignments (Rabiner,
1989).
4 Evaluation
As described in section 2, we use the generated
pairwise alignments from a gold standard of multi-
ple alignments for evaluation. In addition, we look
at the performance of a baseline of pairwise align-
ments, which is constructed by aligning the strings
according to the Hamming distance (i.e. only al-
lowing substitutions and no insertions or deletions;
Hamming, 1950).
The evaluation procedure consists of comparing
the alignments of the previously discussed algo-
rithms including the baseline with the alignments
of the gold standard. For the comparison we use
the standard Levenshtein algorithm without any
restrictions. The evaluation proceeds as follows:
1. The pairwise alignments of the four algo-
rithms, the baseline and the gold standard are
generated and standardized (see section 4.1).
When multiple equal-scoring alignments are
30
generated by an algorithm, only one (i.e. the
final) alignment is selected.
2. In each alignment, we convert each pair of
aligned segments to a single token, so that ev-
ery alignment of two strings is converted to a
single string of segment pairs.
3. For every algorithm these transformed strings
are aligned with the transformed strings of
the gold standard using the standard Leven-
shtein algorithm.
4. The Levenshtein distances for all these
strings are summed up resulting in the total
distance between every alignment algorithm
and the gold standard. Only if individual
segments match completely the segment dis-
tance is 0, otherwise it is 1.
To illustrate this procedure, consider the following
gold standard alignment of [vl"7k] and [v"7lk], two
Bulgarian dialectal variants of the word ?wolf?:
v l "7 k
v "7 l k
Every aligned segment pair is converted to a single
token by adding the symbol ?/? between the seg-
ments and using the symbol ?-? to indicate a gap.
This yields the following transformed string:
v/v l/"7 "7/l k/k
Suppose another algorithm generates the follow-
ing alignment (not detecting the swap):
v l "7 k
v "7 l k
The transformed string for this alignment is:
v/v l/- "7/"7 -/l k/k
To evaluate this alignment, we align this string to
the transformed string of the gold standard and ob-
tain a Levenshtein distance of 3:
v/v l/"7 "7/l k/k
v/v l/- "7/"7 -/l k/k
1 1 1
By repeating this procedure for all alignments and
summing up all distances, we obtain total dis-
tances between the gold standard and every align-
ment algorithm. Algorithms which generate high-
quality alignments will have a low distance from
the gold standard, while the distance will be higher
for algorithms which generate low-quality align-
ments.
4.1 Standardization
The gold standard contains a number of align-
ments which have alternative equivalent align-
ments, most notably an alignment containing an
insertion followed by a deletion (which is equal
to the deletion followed by the insertion), or an
alignment containing a syllabic consonant such as
["?
"
], which in fact matches both a vowel and a
neighboring r-like consonant and can therefore be
aligned with either the vowel or the consonant. In
order to prevent punishing the algorithms which
do not match the exact gold standard in these
cases, the alignments of the gold standard and all
alignment algorithms are transformed to one stan-
dard form in all relevant cases.
For example, consider the correct alignment of
[v"iA] and [v"ij], two Bulgarian dialectal variations
of the English plural pronoun ?you?:
v "i A
v "i j
Of course, this alignment is as reasonable as:
v "i A
v "i j
To avoid punishing the first, we transform all in-
sertions followed by deletions to deletions fol-
lowed by insertions, effectively scoring the two
alignments the same.
For the syllabic consonants we transform all
alignments to a form in which the syllabic con-
sonant is followed by a gap and not vice versa.
For instance, aligning [v"?
"
x] with [v"Arx] (English:
?peak?) yields:
v "?
"
x
v "A r x
Which is transformed to the equivalent alignment:
v "?
"
x
v "A r x
5 Results
We will report both quantitative results using the
evaluation method discussed in the previous sec-
tion, as well as the qualitative results, where we
focus on characteristic errors of the different align-
ment algorithms.
5.1 Quantitative results
Because there are two algorithms which use gen-
erated segment distances (or probabilities) in their
alignments, we first check if these values are sen-
sible and comparable to each other.
31
5.1.1 Comparison of segment distances
With respect to the PMI results (convergence
was reached after 7 iterations, taking less than
5 CPU minutes), we indeed found sensible re-
sults: the average distance between identical sym-
bols was significantly lower than the distance be-
tween pairs of different vowels and consonants
(t < ?13, p < .001). Because we did not allow
vowel-consonants alignments in the Levenshtein
algorithm, no PMI values were generated for those
segment pairs.
Just as Wieling et al (2007), we found sen-
sible PHMM substitution probabilities (conver-
gence was reached after 1675 iterations, taking
about 7 CPU hours): the probability of matching
a symbol with itself was significantly higher than
the probability of substituting one vowel for an-
other (similarly for consonants), which in turn was
higher than the probability of substituting a vowel
with a consonant (all t?s > 9, p < .001).
To allow a fair comparison between the PHMM
probabilities and the PMI distances, we trans-
formed the PHMM probabilities to log-odds
scores (i.e. dividing the probability by the rela-
tive frequency of the segments and subsequently
taking the log). Because the residues after the
linear regression between the PHMM similarities
and PMI distances were not normally distributed,
we used Spearman?s rank correlation coefficient
to assess the relationship between the two vari-
ables. We found a highly significant Spearman?s
? = ?.965 (p < .001), which means that the re-
lationship between the PHMM similarities and the
PMI distances is very strong. When looking at the
insertions and deletions we also found a significant
relationship: Spearman?s ? = ?.736 (p < .001).
5.1.2 Evaluation against the gold standard
Using the procedure described in section 4, we cal-
culated the distances between the gold standard
and the alignment algorithms. Besides reporting
the total number of misaligned tokens, we also di-
vided this number by the total number of aligned
segments in the gold standard (about 16 million)
to get an idea of the error rate. Note that the error
rate is 0 in the perfect case, but might rise to nearly
2 in the worst case, which is an alignment consist-
ing of only insertions and deletions and therefore
up to twice as long as the alignments in the gold
standard. Finally, we also report the total number
of alignments (word pairs) which are not exactly
equal to the alignments of the gold standard.
The results are shown in Table 1. We can
clearly see that all algorithms beat the baseline
and align about 95% of all string pairs correctly.
While the Levenshtein PMI algorithm aligns most
strings perfectly, it misaligns slightly more indi-
vidual segments than the PHMM and the Leven-
shtein algorithm with the swap operation (i.e. it
makes more segment alignment errors per word
pair). The VC-sensitive Levenshtein algorithm
in general performs slightly worse than the other
three algorithms.
5.2 Qualitative results
Let us first note that it is almost impossible for
any algorithm to achieve a perfect overlap with the
gold standard, because the gold standard was gen-
erated from multiple alignments and therefore in-
corporates other constraints. For example, while a
certain pairwise alignment could appear correct in
aligning two consonants, the multiple alignment
could show contextual support (from pronuncia-
tions in other varieties) for separating the conso-
nants. Consequently, all algorithms discussed be-
low make errors of this kind.
In general, the specific errors of the VC-
sensitive Levenshtein algorithm can be separated
into three cases. First, as we illustrated in section
3.3, the VC-sensitive Levenshtein algorithm has
no way to distinguish between aligning a conso-
nant with one of two neighboring consonants and
sometimes chooses the wrong one (this also holds
for vowels). Second, it does not allow alignments
of vowels with consonants and therefore cannot
detect correct vowel-consonant alignments such as
correspondences of [u] with [v] initially. Third,
for the same reason the VC-sensitive Levenshtein
algorithm is also not able to detect metathesis of
vowels with consonants.
The misalignments of the Levenshtein algo-
rithm with the swap-operation can also be split in
three cases. It suffers from the same two prob-
lems as the VC-sensitive Levenshtein algorithm in
choosing to align a consonant incorrectly with one
of two neighboring consonants and not being able
to align a vowel with a consonant. Third, even
though it aligns some of the metathesis cases cor-
rectly, it also makes some errors by incorrectly ap-
plying the swap-operation. For example, consider
the alignment of [s"irjIni] and [s"irjnI], two Bul-
garian dialectal variations of the word ?cheese?, in
which the swap-operation is applied:
32
Algorithm Misaligned segments (error rate) Incorrect alignments (%)
Baseline (Hamming algorithm) 2510094 (0.1579) 726844 (20.92%)
VC-sens. Levenshtein algorithm 490703 (0.0309) 191674 (5.52%)
Levenshtein PMI algorithm 399216 (0.0251) 156440 (4.50%)
Levenshtein swap algorithm 392345 (0.0247) 161834 (4.66%)
Pair Hidden Markov Model 362423 (0.0228) 160896 (4.63%)
Table 1: Comparison to gold standard alignments. All differences are significant (p < 0.01).
s "i rj I n i
s "i rj n I
0 0 0 >< 1 1
However, the two I?s are not related and should not
be swapped, which is reflected in the gold standard
alignment:
s "i rj I n i
s "i rj n I
0 0 0 1 0 1
The incorrect alignments of the Levenshtein
algorithm with the PMI-generated segment dis-
tances are mainly caused by its inability to align
vowels with consonants and therefore, just as the
VC-sensitive Levenshtein algorithm, it fails to de-
tect metathesis. On the other hand, using seg-
ment distances often solves the problem of select-
ing which of two plausible neighbors a consonant
should be aligned with.
Because the PHMM employs segment substi-
tution probabilities, it also often solves the prob-
lem of aligning a consonant to one of two neigh-
bors. In addition, the PHMM often correctly
aligns metathesis involving equal as well as sim-
ilar symbols, even realizing an improvement over
the Levenshtein swap algorithm. Unfortunately,
many wrong alignments of the PHMM are also
caused by allowing vowel-consonant alignments.
Since the PHMM does not take context into ac-
count, it also aligns vowels and consonants which
often play a role in metathesis when no metathesis
is involved.
6 Discussion
This study provides an alternative evaluation of
string distance algorithms by focusing on their ef-
fectiveness in aligning segments. We proposed,
implemented, and tested the new procedure on a
substantial body of data. This provides a new per-
spective on the quality of distance and alignment
algorithms as they have been used in dialectology,
where aggregate comparisons had been at times
frustratingly inconclusive.
In addition, we introduced the PMI weight-
ing within the Levenshtein algorithm as a sim-
ple means of obtaining segment distances, and
showed that it improves on the popular Leven-
shtein algorithm with respect to alignment accu-
racy.
While the results indicated that the PHMM mis-
aligned the fewest segments, training the PHMM
is a lengthy process lasting several hours. Con-
sidering that the Levenshtein algorithm with the
swap operation and the Levenshtein algorithm
with the PMI-generated segment distances are
much quicker to (train and) apply, and that they
have only slightly lower performance with respect
to the segment alignments, we actually prefer us-
ing those methods. Another argument in favor of
using one of these Levenshtein algorithms is that
it is a priori clearer what type of alignment errors
to expect from them, while the PHMM algorithm
is less predictable and harder to comprehend.
While our results are an indication of the good
quality of the evaluated algorithms, we only evalu-
ated the algorithms on a single dataset for which a
gold standard was available. Ideally we would like
to verify these results on other datasets, for which
gold standards consisting of multiple or pairwise
alignments are available.
Acknowledgements
We are grateful to Peter Kleiweg for extending the
Levenshtein algorithm in the L04 package with the
swap-operation. We also thank Greg Kondrak for
providing the original source code of the Pair Hid-
den Markov Models. Finally, we thank Therese
Leinonen and Sebastian Ku?rschner of the Univer-
sity of Groningen and Esteve Valls i Alecha of the
University of Barcelona for their useful feedback
on our ideas.
33
References
Leonard E. Baum, Ted Petrie, George Soules, and Nor-
man Weiss. 1970. A maximization technique occur-
ring in the statistical analysis of probabilistic func-
tions of Markov Chains. The Annals of Mathemati-
cal Statistics, 41(1):164?171.
Kenneth W. Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicog-
raphy. Computational Linguistics, 16(1):22?29.
Fred J. Damerau. 1964. A technique for computer de-
tection and correction of spelling errors. Communi-
cations of the ACM, 7:171?176.
Richard Durbin, Sean R. Eddy, Anders Krogh, and
Graeme Mitchison. 1998. Biological Sequence
Analysis: Probabilistic Models of Proteins and Nu-
cleic Acids. Cambridge University Press, United
Kingdom, July.
Richard Hamming. 1950. Error detecting and error
correcting codes. Bell System Technical Journal,
29:147?160.
Wilbert Heeringa, Peter Kleiweg, Charlotte Gooskens,
and John Nerbonne. 2006. Evaluation of string dis-
tance algorithms for dialectology. In John Nerbonne
and Erhard Hinrichs, editors, Linguistic Distances,
pages 51?62, Shroudsburg, PA. ACL.
Wilbert Heeringa. 2004. Measuring Dialect Pronunci-
ation Differences using Levenshtein Distance. Ph.D.
thesis, Rijksuniversiteit Groningen.
Brett Kessler. 1995. Computational dialectology in
Irish Gaelic. In Proceedings of the seventh con-
ference on European chapter of the Association for
Computational Linguistics, pages 60?66, San Fran-
cisco, CA, USA. Morgan Kaufmann Publishers Inc.
Grzegorz Kondrak and Bonnie Dorr. 2003. Identifica-
tion of Confusable Drug Names: A New Approach
and Evaluation Methodology. Artificial Intelligence
in Medicine, 36:273?291.
Grzegorz Kondrak. 2003. Phonetic Alignment and
Similarity. Computers and the Humanities, 37:273?
291.
Vladimir Levenshtein. 1965. Binary codes capable of
correcting deletions, insertions and reversals. Dok-
lady Akademii Nauk SSSR, 163:845?848.
Wesley Mackay and Grzegorz Kondrak. 2005. Com-
puting word similarity and identifying cognates with
Pair Hidden Markov Models. In Proceedings of
the 9th Conference on Computational Natural Lan-
guage Learning (CoNLL), pages 40?47, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
John Nerbonne and Peter Kleiweg. 2007. Toward a di-
alectological yardstick. Journal of Quantitative Lin-
guistics, 14:148?167.
Bruno Pouliquen. 2008. Similarity of names across
scripts: Edit distance using learned costs of N-
Grams. In Bent Nordstro?m and Aarne Ranta, ed-
itors, Proceedings of the 6th international Con-
ference on Natural Language Processing (Go-
Tal?2008), volume 5221, pages 405?416.
Jelena Prokic?, Martijn Wieling, and John Nerbonne.
2009. Multiple sequence alignments in linguistics.
In Piroska Lendvai and Lars Borin, editors, Proceed-
ings of the EACL 2009 Workshop on Language Tech-
nology and Resources for Cultural Heritage, Social
Sciences, Humanities, and Education.
Lawrence R. Rabiner. 1989. A tutorial on Hidden
Markov Models and selected applications in speech
recognition. Proceedings of the IEEE, 77(2):257?
286.
Eric Sven Ristad and Peter N. Yianilos. 1998. Learn-
ing string-edit distance. IEEE Transactions on Pat-
tern Analysis and Machine Intelligence, 20:522?
532.
Robert Wagner and Roy Lowrance. 1975. An exten-
sion of the string-to-string correction problem. Jour-
nal of the ACM, 22(2):177?183.
Martijn Wieling and John Nerbonne. 2007. Dialect
pronunciation comparison and spoken word recog-
nition. In Petya Osenova, editor, Proceedings of
the RANLPWorkshop on Computational Phonology,
pages 71?78.
Martijn Wieling, Therese Leinonen, and John Ner-
bonne. 2007. Inducing sound segment differences
using Pair Hidden Markov Models. In Mark Ellison
John Nerbonne and Greg Kondrak, editors, Comput-
ing and Historical Phonology: 9th Meeting of the
ACL Special Interest Group for Computational Mor-
phology and Phonology, pages 48?56.
34
Proceedings of the 2009 Workshop on Graph-based Methods for Natural Language Processing, ACL-IJCNLP 2009, pages 14?22,
Suntec, Singapore, 7 August 2009.
c?2009 ACL and AFNLP
Bipartite spectral graph partitioning to co-cluster varieties and sound
correspondences in dialectology
Martijn Wieling
University of Groningen
The Netherlands
m.b.wieling@rug.nl
John Nerbonne
University of Groningen
The Netherlands
j.nerbonne@rug.nl
Abstract
In this study we used bipartite spectral
graph partitioning to simultaneously clus-
ter varieties and sound correspondences
in Dutch dialect data. While cluster-
ing geographical varieties with respect to
their pronunciation is not new, the simul-
taneous identification of the sound corre-
spondences giving rise to the geographi-
cal clustering presents a novel opportunity
in dialectometry. Earlier methods aggre-
gated sound differences and clustered on
the basis of aggregate differences. The de-
termination of the significant sound corre-
spondences which co-varied with cluster
membership was carried out on a post hoc
basis. Bipartite spectral graph clustering
simultaneously seeks groups of individual
sound correspondences which are associ-
ated, even while seeking groups of sites
which share sound correspondences. We
show that the application of this method
results in clear and sensible geographi-
cal groupings and discuss the concomitant
sound correspondences.
1 Introduction
Exact methods have been applied successfully to
the analysis of dialect variation for over three
decades (S?eguy, 1973; Goebl, 1982; Nerbonne
et al, 1999), but they have invariably functioned
by first probing the linguistic differences between
each pair of a range of varieties (sites, such as
Whitby and Bristol in the UK) over a body of
carefully controlled material (say the pronuncia-
tion of the vowel in the word ?put?). Second, the
techniques AGGREGATE over these linguistic dif-
ferences, in order, third, to seek the natural groups
in the data via clustering or multidimensional scal-
ing (MDS) (Nerbonne, 2009).
Naturally techniques have been developed to
determine which linguistic variables weigh most
heavily in determining affinity among varieties.
But all of the following studies separate the deter-
mination of varietal relatedness from the question
of its detailed linguistic basis. Kondrak (2002)
adapted a machine translation technique to deter-
mine which sound correspondences occur most
regularly. His focus was not on dialectology, but
rather on diachronic phonology, where the regular
sound correspondences are regarded as strong ev-
idence of historical relatedness. Heeringa (2004:
268?270) calculated which words correlated best
with the first, second and third dimensions of an
MDS analysis of aggregate pronunciation differ-
ences. Shackleton (2004) used a database of ab-
stract linguistic differences in trying to identify
the British sources of American patterns of speech
variation. He applied principal component analy-
sis to his database to identify the common com-
ponents among his variables. Nerbonne (2006)
examined the distance matrices induced by each
of two hundred vowel pronunciations automati-
cally extracted from a large American collection,
and subsequently applied factor analysis to the co-
variance matrices obtained from the collection of
vowel distance matrices. Proki?c (2007) analyzed
Bulgarian pronunciation using an edit distance
algorithm and then collected commonly aligned
sounds. She developed an index to measure how
characteristic a given sound correspondence is for
a given site.
To study varietal relatedness and its linguistic
basis in parallel, we apply bipartite spectral graph
partitioning. Dhillon (2001) was the first to use
spectral graph partitioning on a bipartite graph
of documents and words, effectively clustering
groups of documents and words simultaneously.
Consequently, every document cluster has a direct
connection to a word cluster; the document clus-
tering implies a word clustering and vice versa. In
14
his study, Dhillon (2001) also demonstrated that
his algorithm worked well on real world examples.
The usefulness of this approach is not only lim-
ited to clustering documents and words simulta-
neously. For example, Kluger et al (2003) used
a somewhat adapted bipartite spectral graph parti-
tioning approach to successfully cluster microar-
ray data simultaneously in clusters of genes and
conditions.
In summary, the contribution of this paper is to
apply a graph-theoretic technique, bipartite spec-
tral graph partitioning, to a new sort of data,
namely dialect pronunciation data, in order to
solve an important problem, namely how to rec-
ognize groups of varieties in this sort of data while
simultaneously characterizing the linguistic basis
of the group. It is worth noting that, in isolat-
ing the linguistic basis of varietal affinities, we
thereby hope to contribute technically to the study
of how cognitive and social dynamics interact in
language variation. Although we shall not pursue
this explicitly in the present paper, our idea is very
simple. The geographic signal in the data is a re-
flection of the social dynamics, where geographic
distance is the rough operationalization of social
contact. In fact, dialectometry is already success-
ful in studying this. We apply techniques to extract
(social) associations among varieties and (linguis-
tic) associations among the speech habits which
the similar varieties share. The latter, linguistic
associations are candidates for cognitive explana-
tion. Although this paper cannot pursue the cogni-
tive explanation, it will provide the material which
a cognitive account might seek to explain.
The remainder of the paper is structured as fol-
lows. Section 2 presents the material we studied,
a large database of contemporary Dutch pronunci-
ations. Section 3 presents the methods, both the
alignment technique used to obtain sound corre-
spondences, as well as the bipartite spectral graph
partitioning we used to simultaneously seek affini-
ties in varieties as well as affinities in sound corre-
spondences. Section 4 presents our results, while
Section 5 concludes with a discussion and some
ideas on avenues for future research.
2 Material
In this study we use the most recent broad-
coverage Dutch dialect data source available: data
from the Goeman-Taeldeman-Van Reenen-project
(GTRP; Goeman and Taeldeman, 1996; Van den
Berg, 2003). The GTRP consists of digital tran-
scriptions for 613 dialect varieties in the Nether-
lands (424 varieties) and Belgium (189 varieties),
gathered during the period 1980?1995. For every
variety, a maximum of 1876 items was narrowly
transcribed according to the International Phonetic
Alphabet. The items consist of separate words and
phrases, including pronominals, adjectives and
nouns. A detailed overview of the data collection
is given in Taeldeman and Verleyen (1999).
Because the GTRP was compiled with a view
to documenting both phonological and morpho-
logical variation (De Schutter et al, 2005) and
our purpose here is the analysis of sound corre-
spondences, we ignore many items of the GTRP.
We use the same 562 item subset as introduced
and discussed in depth in Wieling et al (2007).
In short, the 1876 item word list was filtered by
selecting only single word items, plural nouns
(the singular form was preceded by an article and
therefore not included), base forms of adjectives
instead of comparative forms and the first-person
plural verb instead of other forms. We omit words
whose variation is primarily morphological as we
wish to focus on sound correspondences. In all va-
rieties the same lexeme was used for a single item.
Because the GTRP transcriptions of Belgian
varieties are fundamentally different from tran-
scriptions of Netherlandic varieties (Wieling et al,
2007), we will restrict our analysis to the 424
Netherlandic varieties. The geographic distribu-
tion of these varieties including province names
is shown in Figure 1. Furthermore, note that we
will not look at diacritics, but only at the 82 dis-
tinct phonetic symbols. The average length of ev-
ery item in the GTRP (without diacritics) is 4.7
tokens.
3 Methods
To obtain the clearest signal of varietal differ-
ences in sound correspondences, we ideally want
to compare the pronunciations of each variety with
a single reference point. We might have used the
pronunciations of a proto-language for this pur-
pose, but these are not available. There are also no
pronunciations in standard Dutch in the GTRP and
transcribing the standard Dutch pronunciations
ourselves would likely have introduced between-
transcriber inconsistencies. Heeringa (2004: 274?
276) identified pronunciations in the variety of
Haarlem as being the closest to standard Dutch.
15
Figure 1: Distribution of GTRP localities includ-
ing province names
Because Haarlem was not included in the GTRP
varieties, we chose the transcriptions of Delft (also
close to standard Dutch) as our reference tran-
scriptions. See the discussion section for a con-
sideration of alternatives.
3.1 Obtaining sound correspondences
To obtain the sound correspondences for every site
in the GTRP with respect to the reference site
Delft, we used an adapted version of the regular
Levenshtein algorithm (Levenshtein, 1965).
The Levenshtein algorithm aligns two (pho-
netic) strings by minimizing the number of edit
operations (i.e. insertions, deletions and substitu-
tions) required to transform one string into the
other. For example, the Levenshtein distance be-
tween [lEIk@n] and [likh8n], two Dutch variants of
the word ?seem?, is 4:
lEIk@n delete E 1
lIk@n subst. i/I 1
lik@n insert h 1
likh@n subst. 8/@ 1
likh8n
4
The corresponding alignment is:
l E I k @ n
l i k h 8 n
1 1 1 1
When all edit operations have the same cost,
multiple alignments yield a Levenshtein distance
of 4 (i.e. by aligning the [i] with the [E] and/or by
aligning the [@] with the [h]). To obtain only the
best alignments we used an adaptation of the Lev-
enshtein algorithm which uses automatically gen-
erated segment substitution costs. This procedure
was proposed and described in detail by Wieling
et al (2009) and resulted in significantly better in-
dividual alignments than using the regular Leven-
shtein algorithm.
In brief, the approach consists of obtaining ini-
tial string alignments by using the Levenshtein al-
gorithm with a syllabicity constraint: vowels may
only align with (semi-)vowels, and consonants
only with consonants, except for syllabic conso-
nants which may also be aligned with vowels. Af-
ter the initial run, the substitution cost of every
segment pair (a segment can also be a gap, rep-
resenting insertion and deletion) is calculated ac-
cording to a pointwise mutual information proce-
dure assessing the statistical dependence between
the two segments. I.e. if two segments are aligned
more often than would be expected on the basis of
their frequency in the dataset, the cost of substi-
tuting the two symbols is set relatively low; oth-
erwise it is set relatively high. After the new seg-
ment substitution costs have been calculated, the
strings are aligned again based on the new seg-
ment substitution costs. The previous two steps
are then iterated until the string alignments remain
constant. Our alignments were stable after 12 iter-
ations.
After obtaining the final string alignments, we
use a matrix to store the presence or absence of
each segment substitution for every variety (with
respect to the reference place). We therefore ob-
tain an m ? n matrix A of m varieties (in our
case 423; Delft was excluded as it was used as our
reference site) by n segment substitutions (in our
case 957; not all possible segment substitutions
occur). A value of 1 in A (i.e. A
ij
= 1) indicates
the presence of segment substitution j in variety i,
while a value of 0 indicates the absence. We ex-
perimented with frequency thresholds, but decided
against applying one in this paper as their applica-
tion seemed to lead to poorer results. We postpone
a consideration of frequency-sensitive alternatives
to the discussion section.
16
3.2 Bipartite spectral graph partitioning
An undirected bipartite graph can be represented
by G = (R,S,E), where R and S are two sets
of vertices and E is the set of edges connecting
vertices from R to S. There are no edges between
vertices in a single set. In our case R is the set of
varieties, while S is the set of sound segment sub-
stitutions (i.e. sound correspondences). An edge
between r
i
and s
j
indicates that the sound segment
substitution s
j
occurs in variety r
i
. It is straight-
forward to see that matrix A is a representation of
an undirected bipartite graph.
Spectral graph theory is used to find the prin-
cipal properties and structure of a graph from its
graph spectrum (Chung, 1997). Dhillon (2001)
was the first to use spectral graph partitioning on
a bipartite graph of documents and words, effec-
tively clustering groups of documents and words
simultaneously. Consequently, every document
cluster has a direct connection to a word cluster. In
similar fashion, we would like to obtain a cluster-
ing of varieties and corresponding segment substi-
tutions. We therefore apply the multipartitioning
algorithm introduced by Dhillon (2001) to find k
clusters:
1. Given the m ? n variety-by-segment-
substitution matrix A as discussed previ-
ously, form
A
n
= D
1
?1/2
AD
2
?1/2
with D
1
and D
2
diagonal matrices such that
D
1
(i, i) = ?
j
A
ij
and D
2
(j, j) = ?
i
A
ij
2. Calculate the singular value decomposition
(SVD) of the normalized matrix A
n
SVD(A
n
) = U ?? ? V
T
and take the l = dlog
2
ke singular vectors,
u
2
, . . . ,u
l + 1
and v
2
, . . . ,v
l + 1
3. Compute Z =
[
D
1
?1/2
U
[2,...,l+1]
D
2
?1/2
V
[2,...,l+1]
]
4. Run the k-means algorithm on Z to obtain
the k-way multipartitioning
To illustrate this procedure, we will co-cluster
the following variety-by-segment-substitution ma-
trix A in k = 2 groups.
[2]:[I] [d]:[w] [-]:[@]
Vaals (Limburg) 0 1 1
Sittard (Limburg) 0 1 1
Appelscha (Friesland) 1 0 1
Oudega (Friesland) 1 0 1
We first construct matrices D
1
and D
2
. D
1
contains the total number of edges from every va-
riety (in the same row) on the diagonal, while D
2
contains the total number of edges from every seg-
ment substitution (in the same column) on the di-
agonal. Both matrices are show below.
D
1
=
?
?
?
?
2 0 0 0
0 2 0 0
0 0 2 0
0 0 0 2
?
?
?
?
D
2
=
?
?
2 0 0
0 2 0
0 0 4
?
?
We can now calculate A
n
using the formula dis-
played in step 1 of the multipartitioning algorithm:
A
n
=
?
?
?
?
0 .5 .35
0 .5 .35
.5 0 .35
.5 0 .35
?
?
?
?
Applying the SVD to A
n
yields:
U =
?
?
?
?
?.5 .5 .71
?.5 .5 .71
?.5 ?.5 0
?.5 ?.5 0
?
?
?
?
? =
?
?
1 0 0
0 .71 0
0 0 0
?
?
V =
?
?
?.5 ?.71 ?.5
?.5 .71 ?.5
?.71 0 .71
?
?
To cluster in two groups, we look at the second
singular vectors (i.e. columns) of U and V and
compute the 1-dimensional vector Z:
Z =
[
.35 .35 ?.35 ?.35 ?.5 .5 0
]
T
Note that the first four values correspond with the
places (Vaals, Sittard, Appelscha and Oudega) and
the final three values correspond to the segment
substitutions ([2]:[I], [d]:[w] and [-]:[@]).
After running the k-means algorithm on Z, the
items are assigned to one of two clusters as fol-
lows:
[
1 1 2 2 2 1 1
]
T
17










 
 
 

Figure 2: Visualizations of co-clustering varieties (y-axis) and segments substitutions (x-axis) in 2 (left),
3 (middle) and 4 (right) clusters
The clustering shows that Appelscha and
Oudega are clustered together and linked to the
clustered segment substitution of [2]:[I] (cluster
2). Similarly, Vaals and Sittard are clustered to-
gether and linked to the clustered segment substi-
tutions [d]:[w] and [-]:[@] (cluster 1). Note that the
segment substitution [-]:[@] (an insertion of [@]) is
actually not meaningful for the clustering of the
varieties (as can also be observed in A), because
the bottom value of the second column of V cor-
responding to this segment substitution is 0. It
could therefore just as likely be grouped in clus-
ter 2. Nevertheless, the k-means algorithm always
assigns every item to a single cluster.
In the following section we will report the re-
sults on clustering in two, three and four groups.
1
4 Results
After running the multipartitioning algorithm
2
we
obtained a two-way clustering in k clusters of va-
rieties and segment substitutions. Figure 2 tries
to visualize the simultaneous clustering in two
dimensions. A black dot is drawn if the vari-
ety (y-axis) contains the segment substitution (x-
axis). The varieties and segments are sorted in
such a way that the clusters are clearly visible (and
marked) on both axes.
To visualize the clustering of the varieties, we
created geographical maps in which we indicate
1
We also experimented with clustering in more than four
groups, but the k-means clustering algorithm did not give sta-
ble results for these groupings. It is possible that the random
initialization of the k-means algorithm caused the instability
of the groupings, but since we are ignoring the majority of
information contained in the alignments it is more likely that
this causes a decrease in the number of clusters we can reli-
ably detect.
2
The implementation of the multipartitioning algo-
rithm was obtained from http://adios.tau.ac.il/
SpectralCoClustering
the cluster of each variety by a distinct pattern.
The division in 2, 3 and 4 clusters is shown in Fig-
ure 3.
In the following subsections we will discuss
the most important geographical clusters together
with their simultaneously derived sound corre-
spondences. For brevity, we will only focus on
explaining a few derived sound correspondences
for the most important geographical groups. The
main point to note is that besides a sensible geo-
graphical clustering, we also obtain linguistically
sensible results.
Note that the connection between a cluster of
varieties and sound correspondences does not nec-
essarily imply that those sound correspondences
only occur in that particular cluster of varieties.
This can also be observed in Figure 2, where
sound correspondences in a particular cluster of
varieties also appear in other clusters (but less
dense).
3
The Frisian area
The division into two clusters clearly separates the
Frisian language area (in the province of Fries-
land) from the Dutch language area. This is the
expected result as Heeringa (2004: 227?229) also
measured Frisian as the most distant of all the
language varieties spoken in the Netherlands and
Flanders. It is also expected in light of the fact that
Frisian even has the legal status of a different lan-
guage rather than a dialect of Dutch. Note that the
separate ?islands? in the Frisian language area (see
Figure 3) correspond to the Frisian cities which
are generally found to deviate from the rest of the
Frisian language area (Heeringa, 2004: 235?241).
3
In this study, we did not focus on identifying the most
important sound correspondences in each cluster. See the
Discussion section for a possible approach to rank the sound
correspondences.
18
Figure 3: Clustering of varieties in 2 clusters (left), 3 clusters (middle) and 4 clusters (right)
A few interesting sound correspondences be-
tween the reference variety (Delft) and the Frisian
area are displayed in the following table and dis-
cussed below.
Reference [2] [2] [a] [o] [u] [x] [x] [r]
Frisian [I] [i] [i] [E] [E] [j] [z] [x]
In the table we can see that the Dutch /a/ or /2/
is pronounced [i] or [I] in the Frisian area. This
well known sound correspondence can be found
in words such as kamers ?rooms?, Frisian [kIm@s]
(pronunciation from Anjum), or draden ?threads?
and Frisian [trIdn] (Bakkeveen). In addition, the
Dutch (long) /o/ and /u/ both tend to be realized
as [E] in words such as bomen ?trees?, Frisian
[bjEm@n] (Bakkeveen) or koeien ?cows?, Frisian
[kEi] (Appelscha).
We also identify clustered correspondences of
[x]:[j] where Dutch /x/ has been lenited, e.g. in
geld (/xElt/) ?money?, Frisian [jIlt] (Grouw), but
note that [x]:[g] as in [gElt] (Franeker) also oc-
curs, illustrating that sound correspondences from
another cluster (i.e. the rest of the Netherlands)
can indeed also occur in the Frisian area. An-
other sound correspondence co-clustered with the
Frisian area is the Dutch /x/ and Frisian [z] in
zeggen (/zEx@/) ?say? Frisian [siz@] (Appelscha).
Besides the previous results, we also note some
problems. First, the accusative first-person plu-
ral pronoun ons ?us? lacks the nasal in Frisian, but
the correspondence was not tallied in this case be-
cause the nasal consonant is also missing in Delft.
Second, some apparently frequent sound corre-
spondences result from historical accidents, e.g.
[r]:[x] corresponds regularly in the Dutch:Frisian
pair [dor]:[trux] ?through?. Frisian has lost the fi-
nal [x], and Dutch has either lost a final [r] or
experienced metathesis. These two sorts of ex-
amples might be treated more satisfactorily if we
were to compare pronunciations not to a standard
language, but rather to a reconstruction of a proto-
language.
The Limburg area
The division into three clusters separates the
southern Limburg area from the rest of the Dutch
and Frisian language area. This result is also in
line with previous studies investigating Dutch di-
alectology; Heeringa (2004: 227?229) found the
Limburg dialects to deviate most strongly from
other different dialects within the Netherlands-
Flanders language area once Frisian was removed
from consideration.
Some important segment correspondences for
Limburg are displayed in the following table and
discussed below.
Reference [r] [r] [k] [n] [n] [w]
Limburg [?] [K] [x] [?] [K] [f]
Southern Limburg uses more uvular versions
of /r/, i.e. the trill [?], but also the voiced uvular
fricative [K]. These occur in words such as over
?over, about?, but also in breken ?to break?, i.e.
both pre- and post-vocalically. The bipartite clus-
19
tering likewise detected examples of the famous
?second sound shift?, in which Dutch /k/ is lenited
to /x/, e.g. in ook ?also? realized as [ox] in Epen
and elsewhere. Interestingly, when looking at
other words there is less evidence of lenition in the
words maken ?to make?, gebruiken ?to use?, koken
?to cook?, and kraken ?to crack?, where only two
Limburg varieties document a [x] pronunciation of
the expected stem-final [k], namely Kerkrade and
Vaals. The limited linguistic application does ap-
pear to be geographically consistent, but Kerkrade
pronounces /k/ as [x] where Vaals lenites further to
[s] in words such as ruiken ?to smell?, breken ?to
break?, and steken ?to sting?. Further, there is no
evidence of lenition in words such as vloeken ?to
curse?, spreken ?to speak?, and zoeken ?to seek?,
which are lenited in German (fluchen, sprechen,
suchen).
Some regular correspondences merely reflected
other, and sometimes more fundamental differ-
ences. For instance, we found correspondences
between [n] and [?] or [K] for Limburg , but this
turned out to be a reflection of the older plurals
in -r. For example, in the word wijf ?woman?,
plural wijven in Dutch, wijver in Limburg dialect.
Dutch /w/ is often realized as [f] in the word tarwe
?wheat?, but this is due to the elision of the final
schwa, which results in a pronunciation such as
[ta?@f], in which the standard final devoicing rule
of Dutch is applicable.
The Low Saxon area
Finally, the division in four clusters also separates
the varieties from Groningen and Drenthe from
the rest of the Netherlands. This result differs
somewhat from the standard scholarship on Dutch
dialectology (see Heeringa, 2004), according to
which the Low Saxon area should include not only
the provinces of Groningen and Drenthe, but also
the province of Overijssel and the northern part
of the province of Gelderland. It is nonetheless
the case that Groningen and Drenthe normally are
seen to form a separate northern subgroup within
Low Saxon (Heeringa, 2004: 227?229).
A few interesting sound correspondences are
displayed in the following table and discussed be-
low.
Reference [@] [@] [@] [-] [a]
Low Saxon [m] [N] [?] [P] [e]
The best known characteristic of this area, the
so-called ?final n? (slot n) is instantiated strongly
in words such as strepen, ?stripes?, realized as
[strepm] in the northern Low Saxon area. It would
be pronounced [strep@] in standard Dutch, so the
differences shows up as an unexpected correspon-
dence of [@] with [m], [N] and [?].
The pronunciation of this area is also distinctive
in normally pronouncing words with initial glottal
stops [P] rather than initial vowels, e.g. af ?fin-
ished? is realized as [POf] (Schoonebeek). Further-
more, the long /a/ is often pronounced [e] as in
kaas ?cheese?, [kes] in Gasselte, Hooghalen and
Norg.
5 Discussion
In this study, we have applied a novel method
to dialectology in simultaneously determining
groups of varieties and their linguistic basis (i.e.
sound segment correspondences). We demon-
strated that the bipartite spectral graph partitioning
method introduced by Dhillon (2001) gave sensi-
ble clustering results in the geographical domain
as well as for the concomitant linguistic basis.
As mentioned above, we did not have transcrip-
tions of standard Dutch, but instead we used tran-
scriptions of a variety (Delft) close to the stan-
dard langueage. While the pronunciations of most
items in Delft were similar to standard Dutch,
there were also items which were pronounced dif-
ferently from the standard. While we do not be-
lieve that this will change our results significantly,
using standard Dutch transcriptions produced by
the transcribers of the GTRP corpus would make
the interpretation of sound correspondences more
straightforward.
We indicated in Section 4 that some sound cor-
respondences, e.g. [r]:[x], would probably not oc-
cur if we used a reconstructed proto-language as
a reference instead of the standard language. A
possible way to reconstruct such a proto-language
is by multiple aligning (see Proki?c, 2009) all pro-
nunciations of a single word and use the most fre-
quent phonetic symbol at each position in the re-
constructed word. It would be interesting to see if
using such a reconstructed proto-language would
improve the results by removing sound correspon-
dences such as [r]:[x].
In this study we did not investigate methods
to identify the most important sound correspon-
dences. A possible option would be to create a
ranking procedure based on the uniqueness of the
sound correspondences in a cluster. I.e. the sound
20
correspondence is given a high importance when
it only occurs in the designated cluster, while the
importance goes down when it also occurs in other
clusters).
While sound segment correspondences function
well as a linguistic basis, it might also be fruitful
to investigate morphological distinctions present
in the GTRP corpus. This would enable us to
compare the similarity of the geographic distribu-
tions of pronunciation variation on the one hand
and morphological variation on the other.
As this study was the first to investigate the ef-
fectiveness of a co-clustering approach in dialec-
tometry, we focused on the original bipartite spec-
tral graph partitioning algorithm (Dhillon, 2001).
Investigating other approaches such as bicluster-
ing algorithms for biology (Madeira and Oliveira,
2004) or an information-theoretic co-clustering
approach (Dhillon et al, 2003) would be highly
interesting.
It would likewise be interesting to attempt to
incorporate frequency, by weighting correspon-
dences that occur frequently more heavily than
those which occur only infrequently. While it
stands to reason that more frequently encoun-
tered variation would signal dialectal affinity more
strongly, it is also the case that inverse fre-
quency weightings have occasionally been applied
(Goebl, 1982), and have been shown to function
well. We have the sense that the last word on this
topic has yet to be spoken, and that empirical work
would be valuable.
Our paper has not addressed the interaction be-
tween cognitive and social dynamics directly, but
we feel it has improved our vantage point for un-
derstanding this interaction. In dialect geogra-
phy, social dynamics are operationalized as geog-
raphy, and bipartite spectral graph partitioning has
proven itself capable of detecting the effects of so-
cial contact, i.e. the latent geographic signal in the
data. Other dialectometric techniques have done
this as well.
Linguists have rightly complained, however,
that the linguistic factors have been neglected in
dialectometry (Schneider, 1988:176). The current
approach does not offer a theoretical framework to
explain cognitive effects such as phonemes corre-
sponding across many words, but does enumerate
them clearly. This paper has shown that bipartite
graph clustering can detect the linguistic basis of
dialectal affinity. If deeper cognitive constraints
are reflected in that basis, then we are now in an
improved position to detect them.
Acknowledgments
We would like to thank Assaf Gottlieb for shar-
ing the implementation of the bipartite spectral
graph partitioning method. We also would like to
thank Peter Kleiweg for supplying the L04 pack-
age which was used to generate the maps in this
paper. Finally, we are grateful to Jelena Proki?c and
the anonymous reviewers for their helpful com-
ments on an earlier version of this paper.
References
Fan Chung. 1997. Spectral graph theory. American
Mathematical Society.
Georges De Schutter, Boudewijn van den Berg, Ton
Goeman, and Thera de Jong. 2005. Morfologische
Atlas van de Nederlandse Dialecten (MAND) Deel
1. Amsterdam University Press, Meertens Instituut
- KNAW, Koninklijke Academie voor Nederlandse
Taal- en Letterkunde, Amsterdam.
Inderjit Dhillon, Subramanyam Mallela, and Dhar-
mendra Modha. 2003. Information-theoretic co-
clustering. In Proceedings of the ninth ACM
SIGKDD international conference on Knowledge
discovery and data mining, pages 89?98. ACM New
York, NY, USA.
Inderjit Dhillon. 2001. Co-clustering documents and
words using bipartite spectral graph partitioning. In
Proceedings of the seventh ACM SIGKDD interna-
tional conference on Knowledge discovery and data
mining, pages 269?274. ACM New York, NY, USA.
Hans Goebl. 1982. Dialektometrie: Prinzipien
und Methoden des Einsatzes der Numerischen
Taxonomie im Bereich der Dialektgeographie.
?
Osterreichische Akademie der Wissenschaften,
Wien.
Ton Goeman and Johan Taeldeman. 1996. Fonolo-
gie en morfologie van de Nederlandse dialecten.
Een nieuwe materiaalverzameling en twee nieuwe
atlasprojecten. Taal en Tongval, 48:38?59.
Wilbert Heeringa. 2004. Measuring Dialect Pronunci-
ation Differences using Levenshtein Distance. Ph.D.
thesis, Rijksuniversiteit Groningen.
Yuval Kluger, Ronen Basri, Joseph Chang, and Mark
Gerstein. 2003. Spectral biclustering of microarray
data: Coclustering genes and conditions. Genome
Research, 13(4):703?716.
Grzegorz Kondrak. 2002. Determining recur-
rent sound correspondences by inducing translation
21
models. In Proceedings of the Nineteenth Inter-
national Conference on Computational Linguistics
(COLING 2002), pages 488?494, Taipei. COLING.
Vladimir Levenshtein. 1965. Binary codes capable of
correcting deletions, insertions and reversals. Dok-
lady Akademii Nauk SSSR, 163:845?848.
Sara Madeira and Arlindo Oliveira. 2004. Bicluster-
ing algorithms for biological data analysis: a survey.
IEEE/ACM Transactions on Computational Biology
and Bioinformatics, 1(1):24?45.
John Nerbonne, Wilbert Heeringa, and Peter Kleiweg.
1999. Edit distance and dialect proximity. In David
Sankoff and Joseph Kruskal, editors, Time Warps,
String Edits and Macromolecules: The Theory and
Practice of Sequence Comparison, 2nd ed., pages v?
xv. CSLI, Stanford, CA.
John Nerbonne. 2006. Identifying linguistic struc-
ture in aggregate comparison. Literary and Lin-
guistic Computing, 21(4):463?476. Special Issue,
J.Nerbonne & W.Kretzschmar (eds.), Progress in
Dialectometry: Toward Explanation.
John Nerbonne. 2009. Data-driven dialectology. Lan-
guage and Linguistics Compass, 3(1):175?198.
Jelena Proki?c, Martijn Wieling, and John Nerbonne.
2009. Multiple sequence alignments in linguistics.
In Lars Borin and Piroska Lendvai, editors, Lan-
guage Technology and Resources for Cultural Her-
itage, Social Sciences, Humanities, and Education,
pages 18?25.
Jelena Proki?c. 2007. Identifying linguistic structure
in a quantitative analysis of dialect pronunciation.
In Proceedings of the ACL 2007 Student Research
Workshop, pages 61?66, Prague, June. Association
for Computational Linguistics.
Edgar Schneider. 1988. Qualitative vs. quantitative
methods of area delimitation in dialectology: A
comparison based on lexical data from georgia and
alabama. Journal of English Linguistics, 21:175?
212.
Jean S?eguy. 1973. La dialectom?etrie dans l?atlas lin-
guistique de gascogne. Revue de Linguistique Ro-
mane, 37(145):1?24.
Robert G. Shackleton, Jr. 2005. English-american
speech relationships: A quantitative approach. Jour-
nal of English Linguistics, 33(2):99?160.
Johan Taeldeman and Geert Verleyen. 1999. De
FAND: een kind van zijn tijd. Taal en Tongval,
51:217?240.
Boudewijn van den Berg. 2003. Phonology & Mor-
phology of Dutch & Frisian Dialects in 1.1 million
transcriptions. Goeman-Taeldeman-Van Reenen
project 1980-1995, Meertens Instituut Electronic
Publications in Linguistics 3. Meertens Instituut
(CD-ROM), Amsterdam.
Martijn Wieling, Wilbert Heeringa, and John Ner-
bonne. 2007. An aggregate analysis of pronuncia-
tion in the Goeman-Taeldeman-Van Reenen-Project
data. Taal en Tongval, 59:84?116.
Martijn Wieling, Jelena Proki?c, and John Nerbonne.
2009. Evaluating the pairwise alignment of pronun-
ciations. In Lars Borin and Piroska Lendvai, editors,
Language Technology and Resources for Cultural
Heritage, Social Sciences, Humanities, and Educa-
tion, pages 26?34.
22
Proceedings of the 2010 Workshop on Graph-based Methods for Natural Language Processing, ACL 2010, pages 33?41,
Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational Linguistics
Hierarchical spectral partitioning of bipartite graphs to cluster dialects
and identify distinguishing features
Martijn Wieling
University of Groningen
The Netherlands
m.b.wieling@rug.nl
John Nerbonne
University of Groningen
The Netherlands
j.nerbonne@rug.nl
Abstract
In this study we apply hierarchical spectral
partitioning of bipartite graphs to a Dutch
dialect dataset to cluster dialect varieties
and determine the concomitant sound cor-
respondences. An important advantage of
this clustering method over other dialec-
tometric methods is that the linguistic ba-
sis is simultaneously determined, bridging
the gap between traditional and quantita-
tive dialectology. Besides showing that
the results of the hierarchical clustering
improve over the flat spectral clustering
method used in an earlier study (Wieling
and Nerbonne, 2009), the values of the
second singular vector used to generate the
two-way clustering can be used to identify
the most important sound correspondences
for each cluster. This is an important ad-
vantage of the hierarchical method as it
obviates the need for external methods to
determine the most important sound corre-
spondences for a geographical cluster.
1 Introduction
For almost forty years quantitative methods have
been applied to the analysis of dialect variation
(Se?guy, 1973; Goebl, 1982; Nerbonne et al,
1999). Until recently, these methods focused
mostly on identifying the most important dialectal
groups using an aggregate analysis of the linguis-
tic data.
One of these quantitative methods, clustering,
has been applied frequently to dialect data, espe-
cially in an effort to compare computational anal-
yses to traditional views on dialect areas (Davis
and Houck, 1995; Clopper and Pisoni, 2004; Hee-
ringa, 2004; Moisl and Jones, 2005; Mucha and
Haimerl, 2005; Prokic? and Nerbonne, 2009).
While viewing dialect differences at an ag-
gregate level certainly gives a more comprehen-
sive view than the analysis of a single subjec-
tively selected feature, the aggregate approach has
never fully convinced traditional linguists of its
use as it fails to identify the linguistic distinc-
tions among the identified groups. Recently, how-
ever, Wieling and Nerbonne (2009; 2010) an-
swered this criticism by applying a promising
graph-theoretic method, the spectral partitioning
of bipartite graphs, to cluster varieties and simulta-
neously determine the linguistic basis of the clus-
ters.
The spectral partitioning of bipartite graphs has
been a popular method for the task of co-clustering
since its introduction by Dhillon in 2001. Besides
being used in the field of information retrieval
for co-clustering words and documents (Dhillon,
2001), this method has also proven useful in the
field of bioinformatics, successfully co-clustering
genes and conditions (Kluger et al, 2003).
Wieling and Nerbonne (2009) used spectral par-
titioning of bipartite graphs to co-cluster dialect
varieties and sound correspondences with respect
to a set of reference pronunciations. They reported
a fair geographical clustering of the varieties in
addition to sensible sound correspondences. In a
follow-up study, Wieling and Nerbonne (2010) de-
veloped an external method to rank the sound cor-
respondences for each geographic cluster, which
also conformed largely to the subjectively selected
?interesting? sound correspondences in their ear-
lier study (Wieling and Nerbonne, 2009).
In all the aforementioned studies, the spectral
graph partitioning method was used to generate a
flat clustering. However, Shi and Malik (2000)
indicated that a hierarchical clustering obtained
by repeatedly grouping in two clusters should be
preferred over the flat clustering approach as ap-
proximation errors are reduced. More importantly,
genealogical relationships between languages (or
dialects) are generally expected to have a hierar-
chical structure due to the dynamics of language
33
Figure 1: Distribution of GTRP varieties including
province names
change in which early changes result in separate
varieties which then undergo subsequent changes
independently (Jeffers and Lehiste, 1979).
In this study, we will apply the hierarchical
spectral graph partitioning method to a Dutch di-
alect dataset. Besides comparing the results to
the flat clustering obtained by Wieling and Ner-
bonne (2009), we will also show that identifying
the most important sound correspondences is in-
herent to the method, alleviating the need for an
external ranking method (e.g., see Wieling and
Nerbonne, 2010).
While the current study applies the hierarchical
clustering and (novel) ranking method to pronun-
ciation data, we would also like to point out that
these methods are not restricted to this type of data
and can readily be applied to other domains such
as information retrieval and bioinformatics where
other spectral methods (e.g., principal component
analysis) have already been applied successfully
(e.g., see Furnas et al, 1988 and Jolicoeur and
Mosimann, 1960).
2 Material
In this study, we use the same dataset as dis-
cussed by Wieling and Nerbonne (2009). In short,
the Goeman-Taeldeman-Van Reenen-project data
(GTRP; Goeman and Taeldeman, 1996; Van den
9DDOV
6LWDUG
$SSHOVFKD
2XGHJD
Figure 2: Example of a bipartite graph of varieties
and sound correspondences
Berg, 2003) is the most recent Dutch dialect
dataset digitally available consisting of 1876 pho-
netically transcribed items for 613 dialect varieties
in the Netherlands and Flanders. We focus on
a subset of 562 words selected by Wieling et al
(2007) for all 424 Netherlandic varieties. We do
not include the Belgian varieties, as the transcrip-
tions did not use the same number of tokens as
used for the Netherlandic transcriptions. The geo-
graphic distribution of the GTRP varieties includ-
ing province names is shown in Figure 1.
3 Methods
The spectral graph partitioning method we apply
requires as input an undirected bipartite graph. A
bipartite graph is a graph consisting of two sets of
vertices where each edge connects a vertex from
one set to a vertex in the other set. Vertices within
a set are not connected. An example of a bipartite
graph is shown in Figure 2. The vertices on the left
side represent the varieties, while the vertices on
the right side represent the sound correspondences
(each individual sound is surrounded by a set of
square brackets). An edge between a variety and
a sound correspondence indicates that the sound
correspondence occurs in that variety with respect
to a specific reference variety.
As we are interested in clustering dialect vari-
eties and detecting their underlying linguistic ba-
sis, our bipartite graph consists of dialect varieties
and for each variety the presence of sound corre-
spondences compared to a reference variety (indi-
cated by an edge; see Figure 2). Because we do
not have pronunciations of standard (or historical)
Dutch, we use the pronunciations of the city Delft
as our reference, since they are close to standard
34
Dutch (Wieling and Nerbonne, 2009) and allow a
more straightforward interpretation of the sound
correspondences than those of other varieties.
3.1 Obtaining sound correspondences
We obtain the sound correspondences by aligning
the pronunciations of Delft against the pronuncia-
tions of all other dialect varieties using the Leven-
shtein algorithm (Levenshtein, 1965). The Leven-
shtein algorithm generates an alignment by mini-
mizing the number of edit operations (insertions,
deletions and substitutions) needed to transform
one string into the other. For example, the Lev-
enshtein distance between [bInd@n] and [bEind@],
two Dutch dialect pronunciations of the word ?to
bind?, is 3:
bInd@n insert E 1
bEInd@n subst. i/I 1
bEind@n delete n 1
bEind@
3
The corresponding alignment is:
b I n d @ n
b E i n d @
1 1 1
When all edit operations have the same cost, it
is clear that the vowel [I] in the alignment above
can be aligned with either the vowel [E] or the
vowel [i]. To improve the initial alignments, we
use an empirically derived segment distance table
obtained by using the pointwise mutual informa-
tion (PMI) procedure as introduced by Wieling et
al. (2009).1 They showed that applying the PMI
procedure resulted in much better alignments than
using several other alignment procedures.
The initial step of the PMI procedure consists
of obtaining a starting set of alignments. In our
case we obtain these by using the Levenshtein
algorithm with a syllabicity constraint: vowels
may only align with (semi-)vowels, and conso-
nants only with consonants, except for syllabic
consonants which may also be aligned with vow-
els. Subsequently, the substitution cost of every
segment pair (a segment can also be a gap, rep-
resenting an insertion or a deletion) can be calcu-
lated according to a pointwise mutual information
procedure assessing the statistical dependence be-
tween the two segments:
1The PMI procedure is implemented in the dialectom-
etry package RUG/L04 which can be downloaded from
http://www.let.rug.nl/ kleiweg/L04.
PMI(x, y) = log2
(
p(x, y)
p(x) p(y)
)
Where:
? p(x, y) is estimated by calculating the num-
ber of times x and y occur at the same posi-
tion in two aligned strings X and Y , divided
by the total number of aligned segments (i.e.
the relative occurrence of the aligned seg-
ments x and y in the whole data set). Note
that either x or y can be a gap in the case of
insertion or deletion.
? p(x) and p(y) are estimated as the number
of times x (or y) occurs, divided by the total
number of segment occurrences (i.e. the rel-
ative occurrence of x or y in the whole data
set). Dividing by this term normalizes the co-
occurrence frequency with respect to the fre-
quency expected if x and y are statistically
independent.
In short, this procedure adapts the distance be-
tween two sound segments based on how likely it
is that they are paired in the alignments. If two
sounds are seen more (less) often together than we
would expect based on their relative frequency in
the dataset, their PMI score will be positive (neg-
ative). Higher scores indicate that segments tend
to co-occur in correspondences more often, while
lower scores indicate the opposite. New segment
distances (i.e. segment substitution costs) are ob-
tained by subtracting the PMI score from 0 and
adding the maximum PMI score (to enforce that
the minimum distance is 0). Based on the adapted
segment distances we generate new alignments
and we repeat this procedure until the alignments
remain constant.
We extract the sound correspondences from the
final alignments and represent the bipartite graph
by a matrix A having 423 rows (all varieties, ex-
cept Delft) and 957 columns (all occurring sound
correspondences). We do not include frequency
information in this matrix, but use binary values to
indicate the presence (1) or absence (0) of a sound
correspondence with respect to the reference pro-
nunciation.2 To reduce the effect of noise, we only
2We decided against using (the log) of the frequencies,
as results showed that this approach gave too much weight
to uninformative high-frequent substitutions of two identical
sounds.
35
regard a sound correspondence as present in a vari-
ety when it occurs in at least three aligned pronun-
ciations. Consequently, we reduce the number of
sound correspondences (columns of A) by more
than half to 477.
3.2 Hierarchical spectral partitioning of
bipartite graphs
Spectral graph theory is used to find the princi-
pal properties and structure of a graph from its
graph spectrum (Chung, 1997). Wieling and Ner-
bonne (2009) used spectral partitioning of bipar-
tite graphs as introduced by Dhillon (2001) to
co-cluster varieties and sound correspondences,
enabling them to obtain a geographical cluster-
ing with a simultaneously derived linguistic basis
(i.e. the clustered sound correspondences). While
Wieling and Nerbonne (2009) focused on the
flat clustering approach, we will use the hierar-
chical approach by iteratively clustering in two
groups. This approach is preferred by Shi and Ma-
lik (2000), because approximation errors are re-
duced compared to the flat clustering approach.
The hierarchical spectral partitioning algorithm,
following Dhillon (2001), proceeds as follows:
1. Given the 423 ? 477 variety-by-segment-
correspondence matrix A as discussed pre-
viously, form
An = D1?1/2AD2?1/2
with D1 and D2 diagonal matrices such that
D1(i, i) = ?jAij and D2(j, j) = ?iAij
2. Calculate the singular value decomposition
(SVD) of the normalized matrix An
SVD(An) = U?V T
and take the singular vectors u2 and v2
3. Compute z2 =
[
D1?1/2 u2
D2?1/2 v2
]
4. Run the k-means algorithm on z2 to obtain
the bipartitioning
5. Repeat steps 1 to 4 on both clusters separately
to create the hierarchical clustering
The following example (taken from Wieling and
Nerbonne, 2010) shows how we can co-cluster the
graph of Figure 2 in two groups. The matrix rep-
resentation of this graph is as follows:
[2]:[I] [-]:[@] [d]:[w]
Appelscha (Friesland) 1 1 0
Oudega (Friesland) 1 1 0
Vaals (Limburg) 0 1 1
Sittard (Limburg) 0 1 1
The first step is to construct matrices D1 and
D2 which contain the total number of edges from
every variety (D1) and every sound correspon-
dence (D2) on the diagonal. Both matrices are
shown below.
D1 =
?
?
?
?
2 0 0 0
0 2 0 0
0 0 2 0
0 0 0 2
?
?
?
? D2 =
?
?
2 0 0
0 4 0
0 0 2
?
?
The normalized matrix An can be calculated
using the formula displayed in step 1 of the hierar-
chical bipartitioning algorithm:
An =
?
?
?
?
.5 .35 0
.5 .35 0
0 .35 .5
0 .35 .5
?
?
?
?
Applying the singular value decomposition to An
yields:
U =
?
?
?
?
?.5 .5 .71 0
?.5 .5 ?.71 0
?.5 ?.5 0 ?.71
?.5 ?.5 0 .71
?
?
?
?
? =
?
?
?
?
1 0 0
0 .71 0
0 0 0
0 0 0
?
?
?
?
V T =
?
?
?.5 ?.71 ?.5
.71 0 ?.71
?.5 .71 ?.5
?
?
Finally, we look at the second singular vector of
U (second column) and V T (second row; i.e. sec-
ond column of V ) and compute the 1-dimensional
vector z2:
z2 =
[
.35 .35 ?.35 ?.35 .5 0 ?.5
]T
The first four values correspond with the places
Appelscha, Oudega, Vaals and Sittard, while the
36
last three values correspond to the segment substi-
tutions [2]:[I], [-]:[@] and [d]:[w].
After running the k-means algorithm (with ran-
dom initialization) on z2, the items are assigned to
one of two clusters as follows:
[
1 1 2 2 1 1 2
]T
This clustering shows that Appelscha and
Oudega are grouped together (corresponding to
the first and second item of the vector, above) and
linked to the clustered segment substitutions of
[2]:[I] and [-]:[@] (cluster 1). Also, Vaals and Sit-
tard are clustered together and linked to the clus-
tered segment substitution [d]:[w] (cluster 2). The
segment substitution [-]:[@] (an insertion of [@]) is
actually not meaningful for the clustering of the
varieties (as can be seen in A), because the middle
value of V T corresponding to this segment substi-
tution equals 0. It could therefore just as likely be
grouped cluster 2. Nevertheless, the k-means al-
gorithm always assigns every item to one cluster.3
3.3 Determining the importance of sound
correspondences
Wieling and Nerbonne (2010) introduced a post
hoc method to rank each sound correspondence
[a]:[b] based on the representativenessR in a clus-
ter ci (i.e. the proportion of varieties v in cluster ci
containing the sound correspondence):
R(a, b, ci) =
|v in ci containing [a]:[b]|
|v in ci|
and the distinctiveness D (i.e. the number of vari-
eties v within as opposed to outside cluster ci con-
taining the sound correspondence normalized by
the relative size of the cluster):
D(a, b, ci) =
O(a, b, ci)? S(ci)
1? S(ci)
Where the relative occurrence O and the relative
size S are given by:
O(a, b, ci) =
|v in ci containing [a]:[b]|
|v containing [a]:[b]|
S(ci) =
|v in ci|
|all v?s|
3Note that we could also have decided to drop this sound
correspondence. However using our ranking approach (see
Secion 3.3) already ensures that the uninformative sound cor-
respondences are ranked very low.
The importance I is then calculated by averaging
the distinctiveness and representativeness:
I(a, b, ci) =
R(a, b, ci) +D(a, b, ci)
2
An extensive explanation of this method can be
found in Wieling and Nerbonne (2010).
As we now only use a single singular vector
to determine the partitioning (in contrast to the
study of Wieling and Nerbonne, 2010 where they
used multiple singular vectors to determine the
flat clustering), we will investigate if the values
of the singular vector v2 reveal information about
the importance (as defined above) of the individ-
ual sound correspondences. We will evaluate these
values by comparing them to the importance val-
ues on the basis of the representativeness and dis-
tinctiveness (Wieling and Nerbonne, 2010).
4 Results
In this section, we will report the results of apply-
ing the hierarchical spectral partitioning method to
our Dutch dialect dataset. In addition, we will also
compare the geographical clustering to the results
obtained by Wieling and Nerbonne (2009).
We will only focus on the four main clusters
each consisting of at least 10 varieties. While
our method is able to detect smaller clusters in
the data, we do not believe these to be sta-
ble. We confirmed this by applying three well-
known distance-based clustering algorithms (i.e.
UPGMA, WPGMA and Ward?s Method; Prokic?
and Nerbonne, 2009) to our data which also only
agreed on four main clusters. In addition, Wieling
and Nerbonne (2009) reported reliable results on a
maximum of 4 clusters.
4.1 Geographical results
Figure 3 shows a dendrogram visualizing the ob-
tained hierarchy as well as a geographic visualiza-
tion of the clustering. For comparison, Figure 4
shows the visualization of four clusters based on
the flat clustering approach of Wieling and Ner-
bonne (2009).
It is clear that the geographical results of the
hierarchical approach (Figure 3) are comparable
to the results of the flat clustering approach (Fig-
ure 4) of Wieling and Nerbonne (2009).4 How-
4Note that the results of the flat clustering approach were
based on all 957 sound correspondences. No noise-reducing
frequency threshold was applied there, as this was reported to
lead to poorer results (Wieling and Nerbonne, 2009).
37
Figure 3: Geographic visualization of the clus-
tering including dendrogram. The shades of grey
in the dendrogram correspond with the map (e.g.,
the Limburg varieties can be found at the bottom-
right).
ever, despite the Frisian area (top-left) being iden-
tical, we clearly observe that both the Low Saxon
area (top-right) and the Limburg area (bottom-
right) are larger in the map based on the hierar-
chical approach. As this better reflects the tradi-
tional Dutch dialect landscape (Heeringa, 2004),
the hierarchical clustering method seems to be
an improvement over the flat clustering method.
Also the hierarchy corresponds largely with the
one found by Heeringa (2004, Chapter 9), identi-
fying Frisian, Limburg and Low Saxon as separate
groups.
4.2 Most important sound correspondences
To see whether the values of the singular vector v2
can be used as a substitute for the external ranking
method, we correlated the absolute values of the
Figure 4: Geographic visualization of the flat clus-
tering reported in Wieling and Nerbonne (2009).
The shades of grey are identical to the shades of
grey in Figure 3.
singular vector with the importance values based
on the distinctiveness and representativeness. For
the sound correspondences of the Frisian area we
obtained a high Spearman?s rank correlation co-
efficient ? of .92 (p < .001). For the Low Saxon
area and the Limburg area we obtained similar val-
ues (? = .87, p < .001 and ? = .88, p < .001,
respectively). These results clearly show that the
values of the second singular vector v2 can be
used as a good substitute for the external ranking
method.
Frisian area
The following table shows the five most important
sound correspondences for the Frisian area.
Rank 1 2 3 4 5
Reference - [x] [f] [x] [a]
Frisian [S] [j] - [z] [i]
While we have limited overlap (only [x]:[z]; oc-
curing in e.g. zeggen ?say? Dutch [zEx@], Frisian
[siz@]) with the sound correspondences selected
and discussed by Wieling and Nerbonne (2010)
who used the flat clustering method without a fre-
quency threshold (also causing some of the differ-
ences), we observe more overlap with the subjec-
38
tively selected sound correspondences in Wieling
and Nerbonne (2009; [x]:[j] in e.g. geld ?money?
Dutch [xElt], Frisian [jIlt]; and [a]:[i] in e.g. kaas
?cheese? Dutch [kas], Frisian [tsis]). In addition,
we detected two novel sound correspondences
([f]:[-] and [-]:[S]).
We commonly find the correspondence [-]:[S]
in the infinitive form of verbs such as wachten
?wait? Dutch [waxt@], Frisian [waxtS@]; vechten
?fight? Dutch [vExt@], Frisian [vExtS@]; or spuiten
?spray? Dutch [sp?Yt@], Frisian [spoYtS@], but it
also appears e.g. in Dutch tegen ?against? [teix@],
Frisian [tSIn]. The [f]:[-] correspondence is found
in words like sterven ?die? standard Dutch [stERf@],
Frisian [stER@].
Low Saxon area
The most important sound correspondences of the
Low Saxon area are shown in the table below.
Rank 1 2 3 4 5
Reference [k] [v] [@] [f] [p]
Low Saxon [P] [b] [m] [b] [P]
These sound correspondences overlap to a large
extent with the most important sound correspon-
dences identified and discussed by Wieling and
Nerbonne (2010). The correspondence [k]:[P] can
be found in words like planken ?boards?, Dutch
[plANk@], Low Saxon [plANPN
"
], while the corre-
spondence [v]:[b] is found in words like bleven
?remain? Dutch [blEv@n], Low Saxon [blibm
"
].
The final overlapping correspondence [f]:[b] can
be observed in words like proeven ?test? Dutch
[pruf@], Low Saxon [proybm
"
].
The sound correspondence [@]:[m] was dis-
cussed and selected by Wieling and Ner-
bonne (2009) as an interesting sound correspon-
dence, occurring in words like strepen ?stripes?
Dutch [strep@], Low Saxon [strepm
"
].
The new correspondence [p]:[P] occurs in
words such as lampen ?lamps? standard Dutch
[lamp@], Aduard (Low Saxon) [lamPm
"
], but also
postvocalically, as in gapen ?yawn?, standard
Dutch [xap@], Aduard (Low Saxon) [xoPm
"
]. It
is obviously related to the [k]:[P] correspondence
discussed above.
Limburg area
The most important sound correspondences for the
Limburg area are displayed in the table below.
Rank 1 2 3 4 5
Reference [r] [s] [o] [n] [r]
Limburg [x] [Z] - [x] [?]
For this area, we observe limited overlap with
the most important sound correspondences based
on distinctiveness and representativeness (Wieling
and Nerbonne, 2010; only [n]:[x] overlaps, occur-
ing in words like kleden ?cloths? Dutch [kled@n],
Limburg [klEId@x]), as well as with the subjec-
tively selected interesting sound correspondences
(Wieling and Nerbonne, 2009; only [r]:[?] over-
laps, which occurs in words like breken ?to break?
Dutch [brek@], Limburg [b?Ek@]).
The sound correspondence [o]:[-] can be found
in wonen ?living?, pronounced [woun@] in our
reference variety Delft and [wun@] in Limburg.
As the standard Dutch pronunciation is actually
[won@], this correspondence is caused by the
choice of our reference variety, which is unfortu-
nately not identical to standard Dutch.
The other two sound correspondences are more
informative. The sound correspondence [r]:[x] can
be found in words like vuur ?fire? Dutch [fyr],
Limburg [vy@x] and is similar to the sound cor-
respondence [r]:[?] discussed above. The other
correspondence [s]:[Z] occurs when comparing the
standard-like Delft variety to Limburg varietes in
words such as zwijgen ?to be silent? [sweix@], Lim-
burg [ZwiG@]; or zwemmen ?swim? [swEm@], Lim-
burg [Zw8m@].
Hierarchical versus flat clustering
In general, then, the sound correspondences un-
covered by the hierarchical version of the spectral
clustering technique turn out to be at least as in-
teresting as those uncovered by the flat clustering,
which leads us to regard the hierarchical cluster-
ing technique as defensible in this respect. Since
dialectologists are convinced that dialect areas are
organized hierarchically, we are naturally inclined
toward hierarchical clustering techniques as well.
We note additionally that the using the values of
the second singular vector is an adequate substitu-
tion of the external ranking method based on dis-
tinctiveness and representativeness, which means
that the present paper also marks a step forward in
simplifying the methodology.
5 Discussion
In this study we showed that using hierarchi-
cal spectral partitioning of bipartite graphs results
39
in an improved geographical clustering over the
flat partitioning method and also results in sen-
sible concomitant sound correspondences. One
of the reasons for the improvement of the geo-
graphical clustering could be the approximation
errors which arise when going from the real val-
ued solution to the discrete valued solution, and
which increase with every additional singular vec-
tor used (Shi and Malik, 2000).
In addition, we showed that using the values of
the second singular vector obviates the need for
an external ranking method (e.g., see Wieling and
Nerbonne, 2010) to identify the most important
sound correspondences.
Since the spectral partitioning of bipartite
graphs appears to be identifying significant (rep-
resentative and distinctive) correspondences well
? both in the flat clustering design and in the
(present) hierarchical scheme, several further op-
portunities become worthy of exploration. First,
we might ask if we can automatically identify
a threshold of significance for such correspon-
dences, as to-date we have only sought to verify
significance, not to exclude marginally significant
elements. Second, while we have applied the tech-
nique exclusively to data for which the correspon-
dence consists of a comparison of dialect data to
(near) standard data, the analysis of historical data,
in which varieties are compared to an earlier form,
is within reach. As the first step, we should wish to
compare data to a well-established historical pre-
decessor as further steps might require genuine re-
construction, still beyond anyone?s reach (as far as
we know). Third, the technique would be more
generally applicable if it did not require agree-
ing on a standard, or pole of comparison. This
sounds difficult, but multi-alignment techniques
might bring it within reach (Prokic? et al, 2009).
It is intriguing to note that Nerbonne (in press)
found only sporadic correspondences using fac-
tor analysis on data which incorporated frequency
of correspondence, and we have likewise found
frequency-weighted data less successful as a ba-
sis for spectral bipartite clustering. Shackleton
(2007), Wieling and Nerbonne (2010) and the cur-
rent paper are more successful using data which
lacks information about the frequency of occur-
rence of sounds and/or sound correspondences.
The question arises as to whether this is general
and why this is so. Is it due to the skewness of fre-
quency distributions, in which a suitable normal-
ization might be attempted? Or is it simply more
straightforward to focus on the absolute presence
or absence of a sound or sound correspondence?
While sound correspondences function well as
a linguistic basis, it might also be interesting to
investigate morphological distinctions present in
the GTRP corpus. This would enable us to com-
pare the similarity of the geographic distributions
of pronunciation variation and morphological vari-
ation.
Finally, while we only tested this method on a
single dataset, it would be interesting to see if our
results and conclusions also hold when applied to
more and different datasets. We also realize that
the evaluation in this study is rather qualitative, but
we intend to develop more quantitative evaluation
methods for future studies.
Acknowledgements
We thank Gertjan van Noord and Tim Van de
Cruys for their comments during a presentation
about the flat spectral graph partitioning method,
which instigated the search for an inherent rank-
ing method.
References
Fan Chung. 1997. Spectral graph theory. American
Mathematical Society.
Cynthia G. Clopper and David B. Pisoni. 2004. Some
acoustic cues for the perceptual categorization of
American English regional dialects. Journal of Pho-
netics, 32(1):111?140.
L.M. Davis and C.L. Houck. 1995. What Determines a
Dialect Area? Evidence from the Linguistic Atlas of
the Upper Midwest. American Speech, 70(4):371?
386.
Inderjit Dhillon. 2001. Co-clustering documents and
words using bipartite spectral graph partitioning. In
Proceedings of the seventh ACM SIGKDD interna-
tional conference on Knowledge discovery and data
mining, pages 269?274. ACM New York, NY, USA.
George Furnas, Scott Deerwester, Susan Dumais,
Thomas Landauer, Richard Harshman, Lynn
Streeter, and Karen Lochbaum. 1988. Information
retrieval using a singular value decomposition
model of latent semantic structure. In Proceedings
of the 11th annual international ACM SIGIR confer-
ence on Research and development in information
retrieval, pages 465?480. ACM.
Hans Goebl. 1982. Dialektometrie: Prinzipien
und Methoden des Einsatzes der Numerischen
Taxonomie im Bereich der Dialektgeographie.
40
O?sterreichische Akademie der Wissenschaften,
Wien.
Ton Goeman and Johan Taeldeman. 1996. Fonolo-
gie en morfologie van de Nederlandse dialecten.
Een nieuwe materiaalverzameling en twee nieuwe
atlasprojecten. Taal en Tongval, 48:38?59.
Wilbert Heeringa. 2004. Measuring Dialect Pronunci-
ation Differences using Levenshtein Distance. Ph.D.
thesis, Rijksuniversiteit Groningen.
Robert Jeffers and Ilse Lehiste. 1979. Principles and
methods for historical linguistics. MIT Press, Cam-
bridge.
Pierre Jolicoeur and James E. Mosimann. 1960. Size
and shape variation in the painted turtle. A principal
component analysis. Growth, 24:339?354.
Yuval Kluger, Ronen Basri, Joseph Chang, and Mark
Gerstein. 2003. Spectral biclustering of microarray
data: Coclustering genes and conditions. Genome
Research, 13(4):703?716.
Vladimir Levenshtein. 1965. Binary codes capable of
correcting deletions, insertions and reversals. Dok-
lady Akademii Nauk SSSR, 163:845?848.
Hermann Moisl and Val Jones. 2005. Cluster anal-
ysis of the newcastle electronic corpus of tyneside
english: A comparison of methods. Literary and
Linguistic Computing, 20(supp.):125?146.
Hans-Joachim Mucha and Edgard Haimerl. 2005.
Automatic validation of hierarchical cluster anal-
ysis with application in dialectometry. In Claus
Weihs and Wolgang Gaul, editors, Classification?
the Ubiquitous Challenge. Proc. of the 28th Meet-
ing of the Gesellschaft fu?r Klassifikation, Dortmund,
March 9?11, 2004, pages 513?520, Berlin. Springer.
John Nerbonne, Wilbert Heeringa, and Peter Kleiweg.
1999. Edit distance and dialect proximity. In David
Sankoff and Joseph Kruskal, editors, Time Warps,
String Edits and Macromolecules: The Theory and
Practice of Sequence Comparison, 2nd ed., pages v?
xv. CSLI, Stanford, CA.
John Nerbonne. in press. Various Variation Aggre-
gates in the LAMSAS South. In C. Davis and M. Pi-
cone, editors, Language Variety in the South III.
University of Alabama Press, Tuscaloosa.
Jelena Prokic? and John Nerbonne. 2009. Recognizing
groups among dialects. In John Nerbonne, Charlotte
Gooskens, Sebastian Kurschner, and Rene van Be-
zooijen, editors, International Journal of Humani-
ties and Arts Computing, special issue on Language
Variation.
Jelena Prokic?, Martijn Wieling, and John Nerbonne.
2009. Multiple sequence alignments in linguistics.
In Lars Borin and Piroska Lendvai, editors, Lan-
guage Technology and Resources for Cultural Her-
itage, Social Sciences, Humanities, and Education,
pages 18?25.
Jean Se?guy. 1973. La dialectome?trie dans l?atlas lin-
guistique de gascogne. Revue de Linguistique Ro-
mane, 37(145):1?24.
Robert G. Shackleton, Jr. 2007. Phonetic variation in
the traditional english dialects. Journal of English
Linguistics, 35(1):30?102.
Jianbo Shi and Jitendra Malik. 2000. Normalized cuts
and image segmentation. IEEE Transactions on pat-
tern analysis and machine intelligence, 22(8):888?
905.
Boudewijn van den Berg. 2003. Phonology & Mor-
phology of Dutch & Frisian Dialects in 1.1 million
transcriptions. Goeman-Taeldeman-Van Reenen
project 1980-1995, Meertens Instituut Electronic
Publications in Linguistics 3. Meertens Instituut
(CD-ROM), Amsterdam.
Martijn Wieling and John Nerbonne. 2009. Bipartite
spectral graph partitioning to co-cluster varieties and
sound correspondences in dialectology. In Mono-
jit Choudhury, Samer Hassan, Animesh Mukher-
jee, and Smaranda Muresan, editors, Proc. of the
2009 Workshop on Graph-based Methods for Nat-
ural Language Processing, pages 26?34.
Martijn Wieling and John Nerbonne. 2010. Bipartite
spectral graph partitioning for clustering dialect va-
rieties and detecting their linguistic features. Com-
puter Speech and Language. Accepted to appear in
a special issue on network models of social and cog-
nitive dynamics of language.
Martijn Wieling, Wilbert Heeringa, and John Ner-
bonne. 2007. An aggregate analysis of pronuncia-
tion in the Goeman-Taeldeman-Van Reenen-Project
data. Taal en Tongval, 59:84?116.
Martijn Wieling, Jelena Prokic?, and John Nerbonne.
2009. Evaluating the pairwise alignment of pronun-
ciations. In Lars Borin and Piroska Lendvai, editors,
Language Technology and Resources for Cultural
Heritage, Social Sciences, Humanities, and Educa-
tion, pages 26?34.
41
Proceedings of the Ninth Workshop on Innovative Use of NLP for Building Educational Applications , pages 163?173,
Baltimore, Maryland USA, June 26, 2014.
c?2014 Association for Computational Linguistics
Assessing the Readability of Sentences: Which Corpora and Features?
Felice Dell?Orletta

, Martijn Wieling
??
, Andrea Cimino

, Giulia Venturi

and Simonetta Montemagni


Istituto di Linguistica Computazionale ?Antonio Zampolli? (ILC?CNR)
ItaliaNLP Lab - www.italianlp.it
{name.surname}@ilc.cnr.it
?
Department of Humanities Computing, University of Groningen, The Netherlands
?
Department of Quantitative Linguistics, University of T?ubingen, Germany
wieling@gmail.com
Abstract
The paper investigates the problem of
sentence readability assessment, which is
modelled as a classification task, with a
specific view to text simplification. In par-
ticular, it addresses two open issues con-
nected with it, i.e. the corpora to be used
for training, and the identification of the
most effective features to determine sen-
tence readability. An existing readabil-
ity assessment tool developed for Italian
was specialized at the level of training cor-
pus and learning algorithm. A maximum
entropy?based feature selection and rank-
ing algorithm (grafting) was used to iden-
tify to the most relevant features: it turned
out that assessing the readability of sen-
tences is a complex task, requiring a high
number of features, mainly syntactic ones.
1 Introduction
Over the last ten years, work on automatic read-
ability assessment employed sophisticated NLP
techniques (such as syntactic parsing and statisti-
cal language modeling) to capture highly complex
linguistic features, and used statistical machine
learning to build readability assessment tools. A
variety of different NLP?based approaches has
been proposed so far in the literature, differing
at the level of the number of identified readabil-
ity classes, the typology of features taken into ac-
count, the intended audience of the texts under
evaluation, or the application within which read-
ability assessment is carried out, etc.
Research focused so far on readability assess-
ment at the document level. However, as pointed
out by Skory and Eskenazi (2010), methods devel-
oped perform well when the task is characterizing
the readability level of an entire document, while
they are unreliable for short texts, including single
sentences. Yet, for specific applications, assessing
the readability level of individual sentences would
be desirable. This is the case, for instance, for text
simplification: in current approaches, text read-
ability is typically assessed with respect to the en-
tire document, while text simplification is carried
out at the sentence level, as e.g. done in Alu??sio
et al. (2010), Bott and Saggion (2011) and Inui et
al. (2003). By decoupling the readability assess-
ment and simplification processes, the impact of
simplification operations on the overall readabil-
ity level of a given text may not always be clear.
With sentence?based readability assessment, this
is expected to be no longer a problem. Sentence
readability assessment thus represents an open is-
sue in the literature which is worth being further
explored. To our knowledge, the only attempts
in this direction are represented by Dell?Orletta et
al. (2011) and Sj?oholm (2012) for the Italian and
Swedish languages respectively, followed more
recently by Vajjala and Meurers (2014) dealing
with English.
In this paper, we tackle the challenge of assess-
ing the readability of individual sentences as a first
step towards text simplification. The task is mod-
elled as a classification task, with the final aim
of shedding light on two open issues connected
with it, namely the reference corpora to be used
for training (i.e. collections of sentences classified
according to their readability level), and the iden-
tification of the most effective features to deter-
mine sentence readability. For what concerns the
former, sentence readability assessment poses the
remarkable issue of classifying sentences accord-
ing to their difficulty: if all sentences occurring in
simplified texts can be assumed to be easy?to?read
sentences, the reverse does not necessarily hold
since not all sentences occurring in complex texts
are to be assumed difficult?to?read. This fact has
important implications at the level of the composi-
tion of the corpora to be used for training. The sec-
163
ond issue is concerned with whether and to what
extent the features playing a significant role in the
assessment of readability at the sentence level co-
incide with those exploited at the level of docu-
ment. In particular, the following research ques-
tions are addressed:
1. in assessing sentence readability, is it bet-
ter to use a small gold standard training cor-
pus of manually classified sentences or a
much bigger training corpus automatically
constructed from readability?tagged docu-
ments possibly containing misclassified sen-
tences?
2. which are the features maximizing sentence
readability assessment?
3. to what extent do important features for sen-
tence readability classification match those
playing a role in the document readability
classification?
We will try to answer these questions by work-
ing on Italian, which is a less?resourced language
as far as readability is concerned. To this end,
READ?IT (Dell?Orletta et al., 2011; Dell?Orletta
et al., 2014), which represents the first NLP?based
readability assessment tool for Italian, was spe-
cialized in different respects, namely at the level of
the training corpus and of the learning algorithm;
to investigate questions 2. and 3. above, a maxi-
mum entropy?based feature selection and ranking
algorithm (i.e. grafting) was selected. The specific
target audience of readers addressed in this study
is represented by people characterised by low lit-
eracy skills and/or by mild cognitive impairment.
The paper is organized as follows: Section 2 de-
scribes the background literature, Section 3 intro-
duces our approach to the task, in terms of used
corpora, features and learning algorithm. Finally,
Sections 4 and 5 describe the experimental setting
and discuss achieved results.
2 Background
In spite of the acknowledged need of perform-
ing readability assessment at the sentence level,
so far very few attempts have been made to sys-
tematically investigate the issues and challenges
concerned with the readability assessment of sen-
tences (as opposed to documents). The first two
studies in this direction focused on languages
other than English, namely Italian (Dell?Orletta
et al., 2011) and Swedish (Sj?oholm, 2012). In
both cases, the authors start from the assump-
tion that while all sentences occurring in simpli-
fied texts can be assumed to be easy?to?read sen-
tences, the reverse is not true, since not all sen-
tences occurring in complex texts are difficult?to?
read. This has important consequences at the level
of the evaluation of sentence classification results:
i.e. erroneous readability assessments within the
class of difficult?to?read texts may either corre-
spond to those easy?to?read sentences occurring
within complex texts or represent real classifi-
cation errors. To overcome this problem in the
readability assessment of individual sentences, a
notion of distance with respect to easy-to-read
sentences was introduced by Dell?Orletta et al.
(2011). Focusing on English, a similar issue is
addressed more recently by Vajjala and Meur-
ers (2014) who developed a binary sentence clas-
sifier trained on Wikipedia and Simple English
Wikipedia: they showed that the low accuracy ob-
tained by their classifier stems from the incorrect
assumption that all Wikipedia sentences are more
complex than the Simple Wikipedia ones.
Besides readability, sentence?based analyses
are reported in the literature for related tasks: for
instance, in a text simplification scenario by Drn-
darevi?c et al. (2013), Alu??sio et al. (2008),
?
Stajner
and Saggion (2013) and Barlacchi and Tonelli
(2013); or to predict writing quality level by Louis
and Nenkova (2013). Sheikha and Inkpen (2012)
report the results of both document? and sentence?
based classification in the different but related task
of assessing formal vs. informal style of a docu-
ment/sentence. For students learning English, An-
dersen et al. (2013) made a self?assessment and
tutoring system available which was able to assign
a quality score for each individual sentence they
write: this provides automated feedback on learn-
ers? writing.
A further important issue, largely investigated
in previous readability assessment studies, is the
identification of linguistic factors playing a role
in assessing the readability of documents. If tra-
ditional readability metrics (see e.g., Kincaid et
al. (1975)) typically rely on raw text characteris-
tics, such as word and sentence length, the new
NLP?based readability indices exploit wider sets
of features ranging across different linguistic lev-
els. Starting from Schwarm and Ostendorf (2005)
and Heilman et al. (2007), the role of syntactic
164
features in this task was considered, and more re-
cently, the role of discourse features (e.g., dis-
course topic, discourse cohesion and coherence)
has also been taken into account (see e.g., Barzi-
lay and Lapata (2008), Pitler and Nenkova (2008),
Kate et al. (2010), Feng et al. (2010) and Tonelli
et al. (2012)). Many of these studies also explored
the usefulness of features belonging to individual
levels of linguistic description in predicting text
readability. For example, Feng et al. (2010) sys-
tematically evaluated a wide range of features and
compared the results of different statistical classi-
fiers trained on different classes of features. Sim-
ilarly, the correlation between level?specific fea-
tures has been calculated by Pitler and Nenkova
(2008) with respect to human readability judg-
ments, and by Franc?ois and Fairon (2012) with
respect to readability levels. In both cases, the
classes of features which turned out to be highly
correlated with readability judgments were used
in a readability assessment tool to test their effi-
cacy. Note, however, that in all cases the predic-
tive power of the selected features was evaluated
at the document level only.
3 Our Approach
In this section, we introduce the main ingredi-
ents of our approach to sentence readability as-
sessment, corpora used for training and testing,
selected features and the learning and feature se-
lection algorithm.
3.1 Corpora
We relied on two different corpora: a newspaper
corpus, La Repubblica (henceforth, Rep), and an
easy?to?read newspaper, Due Parole (henceforth,
2Par). 2Par includes articles specifically written
by Italian linguists experts in text simplification
for an audience of adults with a rudimentary lit-
eracy level or with mild intellectual disabilities
(Piemontese, 1996), which represents the target
audience of this study. The two corpora ? selected
as representative of complex vs. simplified texts
within the journalistic genre ? differ significantly
with respect to the distribution of features typi-
cally correlated with text complexity (Dell?Orletta
et al., 2011) and thus represent reliable training
datasets. However, whereas such a distinction is
valid as far as documents are concerned, it appears
to be a simplistic generalization when the focus is
on sentences. In other words, whereas we can con-
sider all sentences of 2Par as easy?to?read, not all
Rep sentences are expected to be difficult?to?read.
From this it follows that whereas the internal com-
position of 2Par is homogeneous at the sentence
level, this is not the case for Rep.
To overcome this asymmetry and in particular
to assess the impact of the noise in the Rep train-
ing corpus, we constructed different training sets
differing in size and internal composition, going
from a noisy set which assumes all Rep sentences
to be difficult?to?read to a clean but smaller set
in which the easy?to?read sentences occurring in
Rep were manually filtered out. These training
sets were used in different experiments whose re-
sults are reported in Section 4.2.
The corpus containing only difficult?to?read
sentences was manually built by annotating Rep
sentences according to their readability (i.e. easy
vs. difficult). The annotation process was car-
ried out by two annotators with a background in
computational linguistics. In order to assess the
reliability of their judgements, we started with a
small annotation experiment: the two annotators
were provided with the same 5 articles from the
Rep corpus (for a total of 107 sentences) and were
asked to extract the difficult?to?read sentences (as
opposed to both easy?to?read and not?easy?to?
classify sentences). The first annotator carried out
the task in 5 minutes and 46 seconds, while the
second annotator took 9 minutes and 8 seconds.
The two annotators agreed on the classification of
81 difficult?to?read sentences out of 107 consid-
ered ones (in particular, the first annotator iden-
tified 90 difficult?to?read?sentences and the sec-
ond one 93 sentences). The agreement between
the two annotators was calculated in terms of pre-
cision, by taking one of the annotation sets as the
gold standard and the other as response: on aver-
age, we obtained a precision of 0.88 in the retrieval
of sentences definitely classified as difficult?to?
read. Given the high level of agreement, the two
annotators were asked to select difficult sentences
from two sets of distinct Rep articles. This re-
sulted in a set of 1,745 difficult?to?read sentences
which were used together with a random selection
of easy?to?read sentences from 2Par for training
and testing.
1
1
The collection can be downloaded from
www.italianlp.it/?page id=22.
165
Feature Ranking position Feature Ranking position
Sent. class. Doc. class. Sent. class. Doc. class.
Raw text features:
[1] Sentence length 1 1 [2] Word length 2 2
Lexical features:
[3] Word types in the Basic Italian Vocabu-
lary
14 42 [6] ?High availability words? 21 22
[4] ?Fundamental words? 10 9 [7] TTR (form) 7
[5] ?High usage words? 22 38 [8] TTR (lemma) 53
Morpho?syntactic features:
[9] Adjective 46 [26] Aux. verb ? inf. mood 64
[10] Adverb 29 59 [27] Aux. verb ? part. mood 51
[11] Article 49 25 [28] Aux. verb ? subj. mood 55
[12] Conjunction 40 [29] Main verb ? cond. mood 40 43
[13] Determiner 43 54 [30] Main verb ? ger. mood 48 48
[14] Interjection [31] Main verb ? imp. mood 37 57
[15] Noun 12 19 [32] Main verb ? indic. mood 16 11
[16] Number 65 44 [33] Main verb ? inf. mood 13 13
[17] Predeterminer [34] Main verb ? part. mood 26 28
[18] Preposition 61 [35] Main verb ? subj. mood 46 32
[19] Pronoun 27 30 [36] Modal verb - inf. mood 54 56
[20] Punctuation 35 [37] Modal verb ? cond. mood 41 36
[21] Residual [38] Modal verb ? imp. mood
[22] Verb 63 34 [39] Modal verb ? indic. mood 18 23
[23] Lexical density 34 33 [40] Modal verb ? part. mood
[24] Aux. verb ? cond. mood 59 60 [41] Modal verb ? subj. mood 60 58
[25] Aux. verb ? indic. mood 17 17
Syntactic features:
[42] Argument 62 [65] Sentence root 35 62
[43] Auxiliary 70 [66] Subject 39 52
[44] Clitic 63 [67] Subordinate clause 64
[45] Complement 28 29 [68] Temporal complement 45 55
[46] Concatenation 66 [69] Temporal modifier
[47] Conjunct in a disjunctive compound 58 67 [70] Temporal predicate
[48] Conjunct linked by a copulative con-
junction
38 37 [71] Parse tree depth 5 4
[49] Copulative conjunction 31 39 [72] Embedded complement ?chains? 8 24
[50] Determiner 50 26 [73] Verbal Root 6 3
[51] Direct object 44 27 [74] Arity of verbal predicates 3 15
[52] Disjunctive conjunction 57 68 [75] Pre?verbal subject 4 12
[53] Indirect complement/object 66 [76] Post?verbal subject 25 16
[54] Locative complement 52 51 [77] Pre?verbal object 36 41
[55] Locative modifier [78] Post?verbal object 9 21
[56] Locative predicate [79] Main clauses 23 14
[57] Modal verb 61 [80] Subordinate clauses 42 45
[58] Modifier 20 47 [81] Subordinate clauses in pre?verbal posi-
tion
32 10
[59] Negative 56 69 [82] Subordinate clauses in post?verbal po-
sition
19 20
[60] Passive subject [83] ?Chains? of embedded subordinate
clauses
11 5
[61] Predicative complement 49 [84] Finite complement clauses 30 18
[62] Preposition [85] Infinitive clauses 53 50
[63] Punctuation 24 31 [86] Length of dependency links 15 8
[64] Relative modifier 47 65 [87] Maximum length of dependency links 7 6
Table 1: Typology of features and ranking position in sentence and document readability assessment
experiments. Only about 14 features are needed for an adequate model of document readability, whereas
this number increases to 30 for sentence readability (marked in boldface). Features which were not
selected during ranking have no rank.
3.2 Linguistic Features
The set of features used in the experiments re-
ported in this paper is wide, spanning across dif-
ferent levels of linguistic analysis. They can
be broadly classified into four main classes, as
reported in Table 1: raw text features, lexical
features, morpho?syntactic features and syntactic
features, shortly described below.
2
2
For an exhaustive discussion including the motivations
underlying this selection of features, the interested reader is
Raw text features (Features [1?2] in Table 1)
refer to those features typically used within tra-
ditional readability metrics and include sentence
length, calculated as the average number of words
per sentence, and word length, calculated as the
average number of characters per words.
The cover category of lexical features (Features
[3?8] in Table 1) includes features referring to
referred to Dell?Orletta et al. (2011, 2014) where these fea-
tures were successfully used for assessing the readability of
Italian texts.
166
both the internal composition of the vocabulary
and the lexical richness of the text. For what con-
cerns the former, the Basic Italian Vocabulary by
De Mauro (2000) was taken as a reference re-
source, including a list of 7000 words highly fa-
miliar to native speakers of Italian. In particular,
we consider: a) the percentage of all unique words
(types) on this reference list occurring in the text,
and b) the internal distribution of the occurring ba-
sic Italian vocabulary words into the usage classi-
fication classes of ?fundamental words? (very fre-
quent words), ?high usage words? (frequent words)
and ?high availability words? (relatively lower fre-
quency words referring to everyday life). Lexical
richness of texts is monitored by computing the
Type/Token Ratio (TTR), which refers to the ratio
between the number of lexical types and the num-
ber of tokens within a text. Due to its sensitivity
to sample size, this feature is computed for text
samples of equivalent length.
The set of morpho?syntactic features (Features
[9?41] in Table 1) is aimed at capturing differ-
ent aspects of the linguistic structure affecting in
one way or another the readability of a text. They
range from the probability distribution of part?
of?speech (POS) types, to the lexical density of
the text, calculated as the ratio of content words
(verbs, nouns, adjectives and adverbs) to the to-
tal number of lexical tokens in a text. This class
also includes features referring to the distribution
of verbs by mood and/or tense, which can be seen
as a language?specific feature exploiting the pre-
dictive power of the Italian rich morphology.
The set of syntactic features (Features [42?87]
in Table 1) captures different aspects of the syntac-
tic structure which are taken as reliable indicators
for automatic readability assessment, namely:
? the unconditional probability of syntactic de-
pendency types, e.g. subject, direct object,
modifier, etc. (Features 42?70);
? parse tree depth features (71?72), going from
the depth of the whole parse tree, calculated
in terms of the longest path from the root
of the dependency tree to some leaf, to a
more specific feature referring to the aver-
age depth of embedded complement ?chains?
governed by a nominal head and including
either prepositional complements or nominal
and adjectival modifiers;
? verbal predicate features (73?78) aimed at
capturing different aspects of the behaviour
of verbal predicates: they range from the
number of verbal roots with respect to num-
ber of all sentence roots occurring in a text,
to more specific features such as the arity
of verbs, meant as the number of instanti-
ated dependency links sharing the same ver-
bal head (covering both arguments and modi-
fiers) and the relative ordering of subject and
object with respect to the verbal head;
? as subordination is widely acknowledged
to be an index of structural complexity
in language, subordination features (79?
85) include: the distribution of subordinate
vs. main clauses; for subordinates, the dis-
tribution of infinitives vs finite complement
clauses, their relative ordering with respect
to the main clause and the average depth of
?chains? of embedded subordinate clauses;
? the length of dependency links is another
characteristic connected with the syntactic
complexity of sentences. Features 86?87
measure dependency length in terms of the
words occurring between the syntactic head
and the dependent: they focus on all depen-
dency links vs. maximum dependency links
only.
3.3 Model Training and Feature Ranking
Given the twofold goal of this study, i.e. re-
liably assessing sentence readability and finding
the most predictive features undelying it, we used
GRAFTING (Perkins et al., 2003), as this approach
allows to train a maximum entropy model while si-
multaneously including incremental feature selec-
tion. The method uses a gradient?based heuristic
to select the most promising feature (to add to the
set of selected features S), and then performs a full
weight optimization over all features in S. This
process is repeated until a certain stopping crite-
rion is reached. As the grafting approach we use
integrates the l
1
regularization (preventing overfit-
ting), features are only included (i.e. have a non-
zero weight) when the reduction of the objective
function is greater than a certain treshold. In our
case, the l
1
prior we use was selected on the basis
of evaluating maximum entropy models with vary-
ing l
1
values (range: 1e-11, 1e-10, ..., 0.1, 1) via
10?fold cross validation. We used TINYEST
3
, a
3
http://github.com/danieldk/tinyest
167
grafting-capable maximum entropy parameter es-
timator for ranking tasks (de Kok, 2011; de Kok,
2013), to select the features and estimate their
weights. Whereas our task is not a ranking task,
but rather a binary classification problem, we were
able to model it as a ranking task by assigning a
high score (1) to difficult?to?read sentences and a
low score (0) to easy?to?read sentences. Conse-
quently, a sentence having a score < 0.5 was in-
terpreted as an easy?to?read sentence, whereas a
sentence which was assigned a score ? 0.5 was
interpreted to be a difficult?to?read sentence.
4 Experiments and Results
4.1 Experimental Setup
In all experiments, the corpora were automatically
tagged by the part?of?speech tagger described
in Dell?Orletta (2009) and dependency?parsed by
the DeSR parser (Attardi, 2006) using Support
Vector Machines as learning algorithm. We de-
vised two different experiments, aimed at explor-
ing the research questions investigated in this pa-
per. To this end, READ?IT was adapted by inte-
grating a specialized training corpus and a maxi-
mum entropy?based feature selection and ranking
algorithm (i.e. grafting).
Experiment 1
This experiment, investigating the first research
question, is aimed at identifying what is the most
effective training data for sentence readability as-
sessment. In particular, the goal is to compare
the results on the basis of using a small set of
gold standard data with respect to a (potentially
larger, but) noisy data set (i.e. without manual re-
vision) where every Rep sentence was assumed to
be difficult?to?read. In particular, the comparison
involved four datasets:
? a collection of gold standard data consisting
of 1,310 easy?to?read sentences randomly
extracted from the 2Par corpus and 1,310
manually selected difficult?to-read sentences
from the Rep corpus;
? a large and unbalanced collection of uncor-
rected data consisting of the whole 2Par cor-
pus (3,910 easy?to?read sentences) and the
whole Rep corpus (8,452 sentences, classi-
fied a priori as difficult?to?read);
? a balanced collection of uncorrected sen-
tences, consisting of 3,910 sentences from
2Par and 3,910 sentences from Rep;
? a balanced collection of uncorrected sen-
tences having the same size as the gold stan-
dard dataset, namely 1,310 sentences from
2Par and 1,310 sentences from Rep.
To assess similarities and differences at the level
of the different corpora used for training in this
experiment, in Table 2 we report a selection of
linguistic features (see Section 3.2) characterizing
the four datasets with respect to the whole 2Par
corpus.We can observe that 2Par differs from all
four Rep corpora for all reported features, and that
the four Rep corpora show similar trends. Inter-
estingly, however, the Rep Gold corpus is almost
always the most distant one from 2Par (i.e. at the
level of sentence length, word length, distribution
of adjectives and subjects, average length of de-
pendency links and parse tree depth).
On the basis of the four Rep datasets, four mod-
els were built which we evaluated using a held?
out test set consisting of 435 sentences from 2Par
and 435 manually classified difficult?to?read sen-
tences from Rep. Using the grafting method, we
calculated the classification score for each sen-
tence in our test set on the basis of an increasing
number of features (ranging from 1 to all non-zero
weighted features for the specific dataset): sen-
tences with a score below 0.5 were classified as
easy?to?read, whereas sentences having a score
greater or equal to 0.5 were classified as difficult?
to?read. This procedure was repeated for each of
the four models.
Experiment 2
The second experiment is aimed at answering our
second and third research questions, focusing on
the features relevant for sentence readability, and
the relationship of those features with document
readability classification. For this purpose, we
compared sentence? and document?based read-
ability classification results. In particular, we com-
pared the features used by the sentence?based
readability model trained on the gold standard
data and the features used by the document?based
model trained on Rep and 2Par. With respect
to the document classification, we used a cor-
pus of 638 documents (319 extracted from 2Par
representating easy?to?read texts, and 319 ex-
tracted from Rep representing difficult?to?read
texts) with 20% of the documents constituting the
held?out test set.
168
Features Rep Unbalan. large Rep Balan. small Rep Balan. large Rep Gold 2Par
Sentence length 24.98 26.03 25.26 28.14 18.66
Word length 5.14 5.24 5.14 5.28 5
?Fundamental words? 75.05% 75.08% 74.83% 74.99% 76.38
Adjective 6.19% 6.25% 6.36% 6.42% 6.03%
Noun 25.65% 27.09% 25.74% 26.10% 29.13%
Subject 4.62% 4.75% 4.64% 4.42% 6%
Max. length of dependency links 9.73 10.13 9.85 10.98 7.67
Parse tree depth 6.18 6.57 6.30 6.83 5.2
Table 2: Distribution of some linguistic features in Rep and 2Par training data
Accuracy Precision (all ft)
Training data 2 ft 10 ft 30 ft 50 ft all ft Easy Difficult
Unbalanced large 50 63.7 74.9 78.4 78.9 (85 ft) 69.2 88.5
Balanced small 64 67.9 79.2 80.8 82.5 (82 ft) 82.5 82.5
Balanced large 63.9 70.6 79.7 81.0 82.3 (85 ft) 83.0 81.6
Gold data 65.6 69.8 79.9 81.3 83.7 (66 ft) 84.8 82.5
Table 3: Sentence classification results using four training datasets and a varying number of features
4.2 Which Training Corpus for Sentence
Classification?
Table 3 reports the results for the sentence classi-
fication task using the four training datasets de-
scribed above. Results are reported in terms of
both overall accuracy (calculated as the proportion
of correct answers against all answers) and preci-
sion within each readability class (when using all
features), defined as the number of easy or diffi-
cult sentences correctly identified as such (in their
respective columns).
Accuracy was computed for all training models
tested using an increasing number of features (2,
10, 30, 50 and all features) as resulting from the
GRAFTING?based ranking and detailed in Table 1.
Note that the first two features correspond in all
cases to the traditional readability features of sen-
tence length and word length. The classification
model trained on the small gold standard dataset
turned out to almost always outperform all other
models: it achieved the best accuracy (83.7%) us-
ing a relatively small number of features (66), and
also for a fixed number of features (i.e. 2, 30
and 50). Only when using the top?10 features,
the uncorrected balanced large dataset slightly out-
performed the gold standard dataset. The accu-
racy when using the unbalanced dataset for train-
ing was always significantly (p < 0.05) worse (us-
ing McNemar?s test) than the accuracy based on
the other training data. The only other significant
difference existed between the balanced small and
large dataset for 10 features. All other differences
are non?significant.
It is also interesting to note that in the results
reported in column 2 ft of Table 3 a significant
difference is observed when comparing the accu-
racy achieved using the unbalanced large data set
with that achieved with the gold standard data: i.e.
about 15.5 percentage points of difference for the
2 ft model against 3 ? 6% using higher numbers
of features. This result originates from the fact that
the unbalanced corpus contains to a larger extent
sentences which are short and complex at the same
time whose correct readability assessment requires
linguistically?grounded features (see below).
The last two columns of Table 3 report preci-
sion results for easy? vs. difficult?to?read sen-
tences for each of the four training datasets (all
features). It is clear that for the class of difficult?
to?read sentences the highest precision (88.5%) is
obtained when using the whole 2Par and Rep cor-
pora for training (i.e. unbalanced large), whereas
for the class of easy?to?read sentences the best
precision results (84.8%) are obtained with the
system trained on the gold standard dataset. In-
terestingly, the worst precision results (69.2%) are
reported for the class of easy?to?read sentences
with the unbalanced large training data set.
These results suggest that the advantages of us-
ing the gold standard data over the uncorrected
training data sets are limited. From this it fol-
lows that treating the whole Rep corpus as a col-
lection of difficult?to?read sentences is not com-
pletely unjustified: this is in line with the satisfac-
tory results reported by Dell?Orletta et al. (2011)
where Rep was used for training a sentence read-
169
ability classifier without any manual filtering of
sentences. Nevertheless, the results of this ex-
periment demonstrate that readability assessment
accuracy and in particular the precision in identi-
fying easy?to?read sentences can be improved by
using a manually selected training dataset. Bal-
ancing the size of larger but potentially noisy (i.e.
without manual revision) data sets appears to cre-
ate a positive trade?off between accuracy and pre-
cision for both classes, thus representing a viable
alternative to the construction of a gold standard
dataset.
4.3 Sentence vs. Document Classification:
which and how many features?
To identify the typology of features needed for
sentence readability assessment and compare them
to those needed for assessing document read-
ability, we compared the results obtained by the
grafting?based feature selection in the sentence
classification task (using the gold standard dataset
for training, see Table 3) to those obtained in the
document classification task whose accuracy on
the test set is reported in Table 4 for increasing
numbers of features selected via GRAFTING.
Train. data 2 ft 10 ft 30 ft 50 ft 70 ft (all)
Rep - 2Par 80 93.3 96.6 96.6 95
Table 4: Accuracy of document classification for
a varying number of features
By comparing the document classification re-
sults with respect to those obtained for sentences,
it can be noticed that the best accuracy is achieved
using a set of 30 features: in contrast to sentence
classification where adding features keeps increas-
ing the performance, more features do not appear
to help for document classification. Sentence read-
ability classification thus seems to be a more com-
plex task, requiring a higher amount of features.
This trend emerges more clearly in Figures 1(a)
and 1(b), where the classification results on the
training set (using 10?fold cross?validation) and
the held?out test set are visualized for increas-
ing amounts of features selected via GRAFTING.
As Figure 1(a) shows, the document classifica-
tion task requires about 14 features after which
the performance appears to stabilize (97.4% accu-
racy for the ten?fold cross-validation and 96.7%
for the held?out test set). In contrast, Figure
1(b) shows that sentence classification requires at
least 30 features (83.4% accuracy for the ten?fold
cross-validation and 79.9% for the test set).
Noticeable differences can also be observed in
the typology of features playing a prominent role
in the two tasks. For each feature taken into ac-
count, Table 1 reports its ranking as resulting from
sentence? and document?based classification ex-
periments (columns ?Sent. class.? and ?Doc.
class.? respectively). Note that in interpreting
the rank associated with each feature it should
be considered that in sentence? and document?
classification the number of required features is
significantly different, i.e. 30 and 14 respectively:
this is to say that approximately the same rank as-
sociated to the same feature does not entail a com-
parable role across the two classification tasks.
As already pointed out, for both sentences and
documents raw text features (i.e. Sentence length
and Word length) turned out to be the top features,
leading however to significantly different results:
i.e. 80% accuracy for documents vs. 65% for
sentences. Among the remaining features, graft-
ing results show that syntactic features do play
a central role in both sentence? and document?
based readability assessment: many of these are
highly ranked, with some differences. Syntactic
features playing a similar role in both readabil-
ity classification tasks include: Verbal root [73],
Parse tree depth [71], ?Chains? of embedded sub-
ordinate clauses [83] and Max. length of depen-
dency links [87], covering important aspects of
syntactic complexity such as depth of the syntactic
dependency (sub?)tree and length of dependency
links. Features that are mainly useful for sentence
readability turned out to be Arity of verbal pred-
icates [74], Pre?verbal subject [75], Post?verbal
object [78] and Embedded complement ?chains?
[72], which can all be seen as representing local
features referring to sentence parts. The feature
Subordinate clauses in pre?verbal position [81],
focusing on the global distribution of pre?verbal
subordinate clauses within the document, is rele-
vant for document classification only. It is interest-
ing to note that features capturing different facets
of the same phenomenon can play quite a different
role for assessing the readability of sentences vs.
documents: this is the case of dependency length,
measured in terms of the words occurring between
the syntactic head and the dependent, where fea-
ture [86] refers to the average length of all de-
pendency links and [87] to the average length of
170
(a) Document classification (b) Sentence classification
Figure 1: Document vs Sentence classification results
maximum dependency links from each sentence.
Whereas [86] plays a similar role for sentences
and documents (in both cases it is a middle rank
feature), [87] is a global feature playing a more
prominent role in document classification.
At the morpho?syntactic level, the feature rank-
ing is more comparable. However, it is interest-
ing to note that very few morpho?syntactic fea-
tures were selected by the feature selection pro-
cess: this is particularly true for document classi-
fication. This can follow from the fact that these
features can be considered as proxies of the syn-
tactic structure which in these experiments was
represented through specific features: in this situ-
ation, the grafting process preferred syntactic fea-
tures over morpho?syntactic ones, in spite of the
lower accuracy of the dependency parser with re-
spect to the part?of?speech tagger. Interestingly,
this result is in contrast with what reported by
Falkenjack and J?onsson (2014) for what concerns
document readability assessment, who claim that
an optimal subset of text features for readability
based document classification does not need fea-
tures induced via parsing. Among the morpho?
syntactic features, it appears that verbal features
play an important role: this can follow both by the
language dealt with which is a a morphologically
rich language, and by the fact that these features
do not have a counterpart at the syntactic level.
Lexical features show a much more mixed re-
sult. Type?Token Ratio (TTR) is only important
for document classification, whereas most of the
other features are important for sentence readabil-
ity, but not for document readability (with the ex-
ception of the presence of ?fundamental words? of
the Basic Italian Vocabulary).
5 Discussion
In this study we have focused on three research
questions. First, we asked which type of train-
ing corpus is best to assess sentence readability.
Whereas we found that using a set of manually
selected complex sentences was better than using
a simple corpus?based distinction, the extra ef-
fort needed to construct the training corpus might
not be worthwhile as observed improvements were
quite modest. However, we did not consider a
more sensitive measure of the difficulty of a sen-
tence (such as a number ranging between 0 and
1), and this might be able to offer a more sub-
stantial improvement (at the cost of needing more
time to create the training material). Of course,
when the goal is to identify the best features for
assessing sentence readability, it does make sense
to have high?quality training data to prevent se-
lecting inadequate features. The second research
question involved identifying which features were
most useful for assessing sentence readability. Be-
sides raw text features, syntactic but also morpho?
syntactic features turned out to play a central role
to achieve adequate performance. The third re-
search question investigated the overlap between
the features needed for document and sentence
readability classification. Whereas there certainly
was overlap between the top features (with dif-
ferent levels of performance), most of the fea-
tures had a different rank across the two tasks,
with local features being more predictive for sen-
tence classification and global ones for documents.
This suggests that the sentence readability task is
more complex than assessing document readabil-
ity, given that there is much less information avail-
able for a sentence than for a document.
171
Acknowledgments
The research reported in this paper was carried out
in the framework of the Short Term Mobility pro-
gram of international exchanges funded by CNR
(Italy). We thank Dani?el de Kok for his help in
applying TINYEST to our data and Giulia Benotto
for her help in manual revision of training data.
References
Sandra M. Alu??sio, Lucia Specia, Thiago A.S. Pardo,
Erick G. Maziero, and Renata P.M. Fortes. 2008.
Towards brazilian portuguese automatic text simpli-
fication systems. In Proceedings of the Eighth ACM
Symposium on Document Engineering, pages 240?
248.
Sandra Alu??sio, Lucia Specia, Caroline Gasperin, and
Carolina Scarton. 2010. Readability assessment for
text simplification. In Proceedings of the NAACL
HLT 2010 Fifth Workshop on Innovative Use of NLP
for Building Educational Applications, pages 1?9.
?istein E. Andersen, Helen Yannakoudakis, Fiona
Barker, and Tim Parish. 2013. Developing and
testing a self-assessment and tutoring system. In
Proceedings of the Eighth Workshop on Innovative
Use of NLP for Building Educational Applications,
pages 32?41.
Giuseppe Attardi. 2006. Experiments with a multi-
language non-projective dependency parser. In Pro-
ceedings of the Tenth Conference on Computational
Natural Language Learning (CoNLL-X ?06), New
York City, New York, pages 166?170.
Gianni Barlacchi and Sara Tonelli. 2013. Ernesta: A
sentence simplification tool for children?s stories in
italian. In Proceedings of the 14th Conferences on
Computational Linguistics and Natural Language
Processing (CICLing 2013), pages 476?487.
Regina Barzilay and Mirella Lapata. 2008. Model-
ing local coherence: An entity-based approach. vol-
ume 34.
Stefan Bott and Horacio Saggion. 2011. An un-
supervised alignment algorithm for text simplifica-
tion corpus construction. In Proceedings of the
Workshop on Monolingual Text-To-Text Generation,
pages 20?26.
Dani?el de Kok. 2011. Discriminative features in
reversible stochastic attribute-value grammars. In
Proceedings of the EMNLP Workshop on Language
Generation and Evaluation, pages 54?63. Associa-
tion for Computational Linguistics.
Dani?el de Kok. 2013. Reversible Stochastic Attribute-
Value Grammars. Ph.D. thesis, Rijksuniversiteit
Groningen.
Tullio De Mauro. 2000. Il dizionario della lingua ital-
iana. Paravia, Torino.
Felice Dell?Orletta, Simonetta Montemagni, and Giu-
lia Venturi. 2011. Read-it: Assessing readability
of italian texts with a view to text simplification. In
Proceedings of the Workshop on Speech and Lan-
guage Processing for Assistive Technologies (SLPAT
2011), pages 73?83.
Felice Dell?Orletta, Simonetta Montemagni, and Giulia
Venturi. 2014. Assessing document and sentence
readability in less resourced languages and across
textual genres. In International Journal of Applied
Linguistics (ITL). Special Issue on Readability and
Text Simplification. To appear.
Felice Dell?Orletta. 2009. Ensemble system for part-
of-speech tagging. In Proceedings of Evalita?09,
Evaluation of NLP and Speech Tools for Italian,
Reggio Emilia, December.
Biljana Drndarevi?c, Sanja
?
Stajner, Stefan Bott, Susana
Bautista, and Horacio Saggion. 2013. Automatic
text simplification in spanish: A comparative evalu-
ation of complementing modules. In Computational
Linguistics and Intelligent Text Processing, pages
488?500. Springer Berlin Heidelberg.
Johan Falkenjack and Arne J?onsson. 2014. Classify-
ing easy-to-read texts without parsing. In Proceed-
ings of the Proceedings of the 3rd Workshop on Pre-
dicting and Improving Text Readability for Target
Reader Populations (PITR), Gothenburg, Sweden.
Association for Computational Linguistics.
Lijun Feng, Martin Jansche, Matt Huenerfauth, and
No?emie Elhadad. 2010. A comparison of features
for automatic readability assessment. In Proceed-
ings of the 23rd International Conference on Com-
putational Linguistics (COLING 2010), pages 276?
284.
Thomas Franc?ois and C?edrick Fairon. 2012. An ?AI
readability? formula for french as a foreign lan-
guage. In Proceedings of the 2012 Joint Confer-
ence on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, Jeju Island, Korea, pages 466?477.
Michael J. Heilman, Kevyn Collins, and Jamie Callan.
2007. Combining lexical and grammatical features
to improve readability measures for first and second
language texts. In Proceedings of the Human Lan-
guage Technology Conference, pages 460?467.
Kentaro Inui, Atsushi Fujita, Tetsuro Takahashi, Ryu
Iida, and Tomoya Iwakura. 2003. Text simplifica-
tion for reading assistance: A project note. In Pro-
ceedings of the Second International Workshop on
Paraphrasing, pages 9?16.
Rohit J. Kate, Xiaoqiang Luo, Siddharth Patwardhan,
Martin Franz, Radu Florian, Raymond J. Mooney,
Salim Roukos, and Chris Welty. 2010. Learning to
172
predict readability using diverse linguistic features.
In oceedings of the 23rd International Conference
on Computational Linguistics, pages 546?554.
J. Peter Kincaid, Lieutenant Robert P. Fishburne,
Richard L. Rogers, and Brad S. Chissom. 1975.
Derivation of new readability formulas for navy
enlisted personnel. In Research Branch Report,
Millington, TN: Chief of Naval Training, pages 8?
75.
Annie Louis and Ani Nenkova. 2013. A corpus of sci-
ence journalism for analysing writing quality. vol-
ume 4.
Simon Perkins, Kevin Lacker, and James Theiler.
2003. Grafting: Fast, incremental feature selection
by gradient descent in function space. The Journal
of Machine Learning Research, 3:1333?1356.
Maria Emanuela Piemontese. 1996. Capire e farsi
capire. Teorie e tecniche della scrittura controllata.
Tecnodid, Napoli.
Emily Pitler and Ani Nenkova. 2008. Revisiting
readability: A unified framework for predicting text
quality. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Process-
ing, pages 186?195.
Sarah E. Schwarm and Mari Ostendorf. 2005. Reading
level assessment using support vector machines and
statistical language models. In Proceedings of the
43rd Annual Meeting on Association for Computa-
tional Linguistics (ACL 05), pages 523?530.
Fadi Abu Sheikha and Diana Inkpen. 2012. Learning
to classify documents according to formal and infor-
mal style. volume 8.
Johan Sj?oholm. 2012. Probability as readability: A
new machine learning approach to readability as-
sessment for written Swedish. LiU Electronic Press,
Master thesis.
Adam Skory and Maxine Eskenazi. 2010. Predicting
cloze task quality for vocabulary training. In Pro-
ceedings of the NAACL HLT 2010 Fifth Workshop
on Innovative Use of NLP for Building Educational
Applications, pages 49?56.
Sara Tonelli, Ke Tran Manh, and Emanuele Pianta.
2012. Making readability indices readable. In Pro-
ceedings of the First Workshop on Predicting and
Improving Text Readability for Target Reader Popu-
lations, pages 40?48.
Sowmya Vajjala and Detmar Meurers. 2014. On as-
sessing the reading level of individual sentences for
text simplification. In Proceedings of the 14th Con-
ference of the European Chapter of the Association
for Computational Linguistics (EACL-14), Gothen-
burg, Sweden. Association for Computational Lin-
guistics.
Sanja
?
Stajner and Horacio Saggion. 2013. Readabil-
ity indices for automatic evaluation of text simplifi-
cation systems: A feasibility study for spanish. In
Proceedings of the International Joint Conference
on Natural Language Processing.
173
