A t ra inab le  method  fo r  ext rac t ing  Ch inese  ent i ty  names  
and  the i r  re la t ions  
Yimin Zhang & Zhou Joe F 
Intel China Research Center 
Kerry Center 6F1 
No. 1 Guanghua Road, Chaoyang District 
Beijing 100020, P.R. China 
Abstract 
In this paper we propose a trainable 
method for extracting Chinese entity names 
and their relations. We view the entire 
problem as series of classification problems 
and employ memory-based learning (MBL) 
to resolve them. Preliminary results show 
that this method is efficient, flexible and 
promising to achieve better performance 
than other existing methods. 
1. Introduction 
Entity names and their relations form 
the main content of a document. By grasping 
the entity names and their relations from a 
document, we will be able to understand the 
document to some extent. 
In the field of information extraction, 
much work has been done to automatically 
learn patterns \[1\] from training corpus in 
order to extract entity names and their 
relations from Engfish documents. But for 
Chinese, researchers primarily use man- 
made rules and keyword sets to identify 
entity names \[2\]. Building rules is often 
complex, error-prone and time-consuming, 
and usually requires detailed knowledge of 
system internals. Some researchers also use 
statistical method \[3\], but the training needs 
lots of human annotated data and only local 
context information can be used. With this in 
the view, we have sought a more efficient 
and flexible trainable method to resolve this 
problem. 
Section 2 first gives a general outline of 
the trainable method we have defined to 
extract Chinese entity names and their 
relations, then describes person name 
extraction, entity name classification and 
relation extraction in detail. Section 3 
describes the preliminary experimental 
results of our method. Section 4 contains our 
remarks and discussion on possible 
extensions of the proposed work. 
2. General Outline of the Method 
we view the problem of Chinese entity 
names and their relations as a series of 
classification problems, such as person ame 
boundary classification, entity name 
classification, noun phrase boundary 
classification and relation classification. If
those classification problems are clarified, 
then we will be able to resolve the related 
extraction problem. For example, if we can 
correctly classify (or identify) the beginning 
or the ending boundaries of a person name 
appeared in a document, we will be able to 
extract the person ame. 
The process can be divided into two 
stages. The first stage is the learning process 
in which several classifiers are built from the 
training data. The second stage is the 
extracting process in which Chinese entity 
names and their relations are extracted using 
the classifiers learned. The learning 
algorithm used in the learning process is 
memory-based learning (MBL) \[4\]. MBL 
entails a classification based supervised 
learning approach. The approach as been 
named differently in a variety of contexts, 
such as similarity-based, example-based, 
analogical, case-based, instance-based, and 
lazy learning, we  selected MBL as our 
learning algorithm because it suits well the 
domains with a large number of features 
66 
from heterogeneous ources and can 
remember exceptional and low frequency 
cases that are useful to extrapolate from \[5\]. 
In addition, we can customize the learner 
using different weighting functions 
according to linguistic bias. 
The main steps for the learning process 
are: 
Step 1: Prepare training data in which all 
noun phrases, entity names and their 
relations are manually annotated. 
Step 2: Segmenting, tagging, and partial 
parsing the training data. 
Step 3: Extract the training sets (instance 
base) from the parsed training data. Four 
training sets are extracted for different asks 
with each related to Chinese person names, 
entity names, noun phrases, or relations 
between entity names in the training data. 
The main features used in an example can be 
either local context features, e.g. dependency 
relation feature, or global context features, 
e.g. the features of a word in the whole 
document, or surface linguistic features, e.g. 
character feature and word feature, or deep 
linguistic features like semantic feature. 
Step 4: Use MBL algorithm to obtain IG- 
Tree \[4\] for the four training sets. IG-Tree is 
a compressed representation f the training 
set that can be processed quickly in 
classification process. In our case, the 
resulted IG-Trees are PersonName-IG-Tree, 
EntityName-IG-Tree, NP-IG-Tree, and 
Relation-IG-Tree. 
The main steps for extracting process are: 
Step 1: Segmenting, tagging and partial 
parsing the input Chinese documents. 
Step 2: Identify Chinese people names using 
PersonName-IG-Tree. 
Step 3: Identify Chinese organization ames 
using the same method as described in \[2\]. 
Step 4: Identify other entity names (location, 
time, number) using the same method as 
described in \[2\]. 
Step 5: Identify Chinese noun phrases (NP 
chunking) using NP-IG-Tree. 
Step 6: Use entity names and noun phrases 
extracted to perform partial parsing again to 
fix the parsing errors. 
Step 7: Use EntityName-IG-tree to classify 
the noun phrases extracted. This step will 
identify entity names that are missed in the 
previous teps. 
Step 8: Use Relafion-IG-Tree to identify 
relations between the extracted entity names. 
For a better understanding of the 
algorithm, we will describe in detail the 
person name extraction, the entity name 
classification, and the relation extraction in 
the next subsection. Please note that we are 
not going to discuss NP chunking further 
since it is beyond the main theme of this 
paper. 
2.1 Person  Name ext ract ion  
Chinese person names can be divided 
into two categories, local Chinese person 
names that consist of Chinese surnames and 
given names and transliterated person names 
that are sound translations of foreign names. 
The length of a local person name ranges 
from 2 to 6 characters, while the length of a 
transliterated person name is unrestricted. 
After segmentation, person names are 
usually divided into several words. The task 
i s  to extract the word sequences that are 
person name components. With this in view, 
we convert the person name extraction 
problem to an equivalent classification 
problem, i.e. classifying word sequences 
existing in the results of segmentation i to 
two classes, namely Person-Name and Not- 
Person-Name. 
To classify a word sequence we need to 
use a number of features. 
(1) Word features: the beginning 
word/tag of the sequence, the ending 
word/tag of the sequence. 
(2) Local context features: the n-th 
(n<=3) word/tag before/after the sequence; 
the verb before/after the sequence. 
(3) Context dependency features: the 
dependency relations of the word sequence 
and the dependency relations of the first 
67 
word before/after the sequence. The main 
dependency relations include verb-object 
(the relation between a verb and its noun 
object), subject-verb (the relation between a
verb and its subject), subject-adj (the 
relation between an adjective and its 
subject), adv-verb (the relation between a 
verb and its adverbial modifier), adv-adj (the 
relation between an adjective and its 
adverbial modifier), modifier-head (the 
relation between a noun and its modifier. In 
Chinese, the modifier can be adjective, 
noun, verb or other phrases). 
The word features and local context 
features can be directly extracted from the 
parsing results of the training data. The 
extraction of dependency features needs 
more explanation. We employ the 
collocation information obtained from a 
large corpus \[6\] to help the Chinese partial 
parser do the parsing. In most cases, 
dependency relations can be taken directly 
from the parsing results. But, there are some 
instances that the parser does not function 
well resulting in flat parsing trees. Under 
these circumstances, we resort to some 
simple heuristics such as linear order for 
dependency relations, thus making our 
method robust enough to extract most of the 
dependency relations. 
To make the learning process more 
efficient, we use Boolean features in the 
tralning set, so every feature described 
above is translated into several Boolean 
features. For example, for the feature lth- 
Next-Word (the first word after the word 
sequence), its value is the top 500 words 
(ordered by frequency) that can appear after 
a person name. We translate it into 500 
features with every feature name like lth- 
Next-Word-XX in which XX is one of the 
500 words. The feature value 1 means that 
the XX appear next to the word sequence in 
the instance. The translated examples have 
about several thousands of Boolean features, 
which will be a big challenge for machine 
learning algorithms like C4.5 and CN2, but 
for MBL this is not a big problem. 
Furthermore, we use sparse array 
representation to make the storage 
requirement much lower. 
For every word sequence in the training 
data that meets with one of the following 
three requirements, we extract that word 
sequence, including its class and all its 
features described above: 
(1) Begin with a surnarne, plus 1 or 2 
characters. 
(2) Begin with two surnames, plus 1 or 2 
characters. 
(3) Begin with a character included in 
the first character set of transliterated person 
names (extracted from training data), plus 
several characters. The name may not 
surpass a normal word (that is included in a 
list of 5000 most frequently used Chinese 
words), because these normal words rarely 
occurred in a transliterated name. 
For example, if in the training data, three 
words "W1 W2 W3" are annotated as a 
person name, then we will extract a Person- 
Name "Wl W2 W3" and Not-Person-Name 
"W1 W2". 
After all the examples are extracted from 
the training data, they are fed to MBL 
Learner to get the PersonName-IG-Tree. 
In the extracting process, we do the same 
as in the learning process to extract all 
examples, but the class of every example is 
unknown to us in advance. With the 
PersonName-IG-Tree, we can derive the 
class of every example, and then all word 
sequences classified as PersonName are 
extracted. 
When a person name appears more than 
once in a document, we can rely on cache 
mechanism, similar to those described in \[2\], 
to solve ambiguous cases. For example, a
person name "~:J~3~" appears more than once 
in a document. We first erroneously extract 
"~!~gj.~" from a sentence "~ i~1~-~ 
~ 5~ ~-~ - - 'q" ;~ ~i~ ' t~  I~!1 ~ i~ ~ 131 ", then 
correctly extract "~31U' from another 
sentence "~3,~-~- - :~" .  Now "~:\]~gj-~" 
and "~!5~" are both in the cache, the cache 
mechanism will be able to correct the first 
68 
error based on the heuristics that, if an 
extracted person name is a substring of 
another extracted person name and they do 
not appear in one sentence, then only the 
substring is the correct person name. 
To better understand the process of 
extracting person names, we describe a few 
intuitive xamples below. 
(1) Sample 1 
The sentence: "~, .~:~~. -~. -~,~"  
The segmentation result: "~, .~  ~! 
Here both "~\]~-~-" and "~.~"  are 
person name candidates for extraction. 
Because no dependency relations are found 
for the next word "-~-" and most training 
examples with this feature are classified as 
Not-Person-Name in training data, so "~:\]~.~" 
is also classified as Not-Person-Name. Both 
the previous word -,~,~.~:.~.~e~ ~:rm,, and the next 
word "$.,k~" are positive evidences that 
make it certain that ""~!~..~_" shduld be 
classified as a person name, therefore our 
algorithm correctly extracted it from this 
sentence. 
(2) Sample 2 
The sentence: ' "~ , : i~ l J~\ ]~. i l~ l \ ] "  
The segmentation result: " .~:~ :~ l J  
Our a lgor i thmcorrect ly  classified 
"\]t~Jl~ll" as not a person name. The 
reason is that in the training data all word 
sequences whose previous word is "~"  is 
a not a person name. 
These examples how that our method 
performs disambiguation well thanks to the 
MBL learner ' s  capability of catching 
exceptions. 
2.2 Entity Name Classification 
The task of entity name classification is
to classify the given noun phrases into 
several categories, such as organization 
name, product name, location, etc., as well 
as person names that are missed in the 
previous extraction. 
In addition to the features used in person 
name extraction, more features are needed. 
Some of these features are equivalent to the 
features used in Crystal \[8\], such as subject- 
auxiliary-noun, e.g. the relation between 
"I~I~I~'~N\] '' and "~.~"  in the sentence 
Some features are specific to Chinese. 
Semantic features are also included to make 
the learned classifier more powerful. The 
semantic features of a word can be taken 
from a widely used Chinese thesaurus \[7\] 
that classifies Chinese words into 12 broad 
categories, 94 middle categories, and 1428 
small categories. There are about 70 
thousands words in the thesaurus. 
Unlike other inductive learning systems 
in the field of information extraction, such as 
Crystal, we use a general machine learning 
algorithm to do the learning. The most 
relevant earlier work is the experiment 
described in \[8\] using the machine learning 
algorithm C4.5. Though their experiment 
showed that the performance of C4.5 based 
method was comparable to Crystal, they 
abandoned this method due to the time 
complexity of C4.5 when ,dealing with large 
number of features. MBL is similar to C4.5 
in that both are general machine learning 
algorithms. Their differences lie in that 
? MBL is a lazy learning algorithm that keeps 
all training data in memory and only 
abstracts at classification time by 
extrapolating a class from the most similar 
items in memory, therefore, its time 
complexity is much lower than C4.5, 
especially when training data contains large 
number of features and examples. Actually, 
in Soderland's analysis \[8\], MBL's time 
complexity is only slightly greater than that 
of Crystal. Though the instance-based 
algorithm like MBL may require large 
memory, the advanced hardware technology 
available today can overcome this problem. 
A sparse vector representation will also 
lower the memory requirement. Taken all 
these into consideration, MBL is well suited 
for entity name extraction and relation 
extraction. Furthermore, the simple instance 
representation and weight function make 
69 
MBL-based method more flexible and 
extensible. Any useful features can be added 
to the system without any modification to 
the algorithm. In Crystal, however, adding 
more features may affect the correctness of 
weight function used in finding similar 
examples. Our method can employ global 
features, i.e. features beyond the sentence 
level. We treat a NP and all its occurrences 
(including its anaphofical references) in one 
text as one single example and all context 
words that are in some dependency relations 
to this NP as this example's features. Thus, 
we can resolve more complicated cases than 
Crystal. For example, if a NP is in subj-verb 
relation with verb "~,~" , it can be a person 
name or an organization ame. But, if we 
know all verbs that have subj-verb relations 
with this NP, then we will know the exact 
class this NP belongs to. 
The steps for entity name classification 
are similar to the steps in person name 
extraction. Our method is quite impressive 
in that it can learn a lot of context features to 
classify the entity names, e.g. it correctly 
classifies "/~ 1\]~" in "~ t\]~ l~?J.~.gj-~" (Qiming's 
father) as a person name. Such a person 
name cannot be recognized in person name 
extraction because it does not begin with a 
surname or first character of transliterated 
person names. 
2.3 Relation extraction 
This task is to identify relation classes 
between entity names. Our current classes 
include employee-of, location-of, product- 
of, and no-relation. The relations we can 
extract are by no means restricted to this set. 
We can expand the set if training data are 
provided. 
The features for this task include features 
used in Soderland's experiment \[8\]. These 
features are equivalent to the syntactic- 
lexical or syntactic-semantic constraints 
used in Crystal. The feature name begins 
with the name of the syntax position (SUB J, 
OBJ, PP-OBJ etc.), followed by the name of 
the constraint and the actual term or class 
name. For example, "\]l~,~,~,~ '' in the 
subject position would include the features: 
SUBJ-Terms-\]\[~l~ i, 
SUBJ-Terms-~,~ 
SUBJ-Mod-Terms-Ii~l~ //the terms in the 
modifier of the subject 
SUB J-Head-Terms-,e~, ~ 
SUB-Classes-Employee // the semantic 
categories of the subject 
SUB-Mod-Classes-Organization 
SUB-Head-Classes-Organization 
More features are introduced in our 
method, such as the linear order of entity 
names, the word(s) between the entity 
names, the relative position of the entity 
names (in one sentence or in neighboring 
sentences), etc. These features will make our 
method more robust han Crystal. 
For every two related entity names in the 
training data, we identify a training example 
and extract it. After all the examples are 
extracted from the training data, they are fed 
to MBL Learner to get the Relation-IG-Tree. 
In the extracting process, we do the same 
as in the learning process to extract all pairs 
of entity names. Then using the Relation-IG- 
Tree, we can derive the relation between 
every pair of entity names. 
To better understand the process 
of relation extraction, we describe a couple 
of examples below. 
(1) Sample 1 
The input text: jl~j~,l\]~l~\[\]t~ff3 \[ \]  ~ l~ lC J  
IT ~ i i~:~,~-$U~,  ..- 
In the entity extraction, we have extracted 
"~ l~ l~ l~"  as a company name and 
"IT~lj~'f@~.~,:~-" as a product name. In the 
training data, some training examples have 
similar sentence patterns, e.g. "Company 
Name (~J / ;~)  ...Product Name ~J~\ ]~" ,  
and most of the time there are product-of 
relation between the two entity names. 
Based on this evidence, a product-of 
relation can be identified between 
"~\ ] :~1~"  and "IT~i~'~t:~-~-". 
(2) Sample 2 
70 
The input text: ~':t~;,~/~t~j~J2~,~'~k~t~ 
~l~.~..,,~. :;i~:t~_, ~L~tIJ!:~I,~TCL~I~\[\] 
In the entity extraction, we have 
extracted "~: t :~"  as a person name and 
"TCL~,~I~" as a company name. Now we 
want to test if these two entity names have 
an employee-of relation. As can be seen in 
the training data, if a person name and a 
company name appear in neighbonng 
sentences, and no other person names and 
company names are found in between, they 
tend to have a employee-of relation. Based 
on this evidence, an employee-of relation 
can be identified between "~: : \ [ :~"  and 
"TCL~\ [ \ ] " .  Current systems, such as 
Crystal, would find it difficult to resolve 
because these two entity names appear in 
different sentences. 
3. System Evaluation 
To test our method we prepare a 
manually annotated corpus comprised of 
about 200 business news. All the entity 
names (about 500 person names and 300 
organization names), noun phrases, and 
relations (i.e. employee-of, product-of, 
location-of) in the corpus were manually 
annotated. Ten pairs of training set and 
testing set were randomly selected from the 
corpus with each set equivalent to half size 
of the entire corpus. We ran our learning and 
extracting processes on all the data sets and 
calculated the mean recall and precision 
rates. The results are showed in Table. 1. 
Table 1: Evaluation for extracting Chinese ntity 
names and their relations 
Person Name 
Organization 
Name 
Employee-Of 
Recall 
86.3% 
73.4% 
75.6% 
Precision 
83.2% 
89.3% 
92.3% 
Product-Of 56.2% 87.1% 
Location-Of 67.2% 75.6% 
As can been seen, our performance in 
person name and organization name 
extraction is  comparable to  other systems 
\[2,3\] considenng the relatively small size of 
the training corpus. Based on our survey, our 
work on extracting entity relations is 
unprecedented for Chinese, therefore we are 
unable to establish a benchmark. But, the 
extraction of emloyee-of relation looks quite 
good. Detailed analysis reveals that our 
method can handle well some instances 
where co-reference resolution is needed 
because we introduced cross-sentence 
features. The method did poorly on product- 
of relation extraction due to the errors in 
noun phrases chunking. With a better NP 
chunking module, the performance can be 
improved. 
4. Conclusion 
In this paper we presented a trainable 
method for extracting Chinese entity names 
and their relations. The method provides a 
unified framework based on MBL. Our 
preliminary experiment demonstrates 
that this trainable method is efficient and 
flexible. Any linguistic features, either 
surface or deep, can be easily added into 
the system. Preliminary experiments 
have shown that our performance is 
comparable to or better than other 
existing trainable methods, such as 
HMM and Crystal. Our work, however, 
is still in its preliminary stage. More 
thorough evaluation is required using 
larger testing corpora. Some algorithmic 
extensions are also expected so as to 
improve the performance, including 
automatic feature selection, coreference 
resolution, etc. 
Reference 
\[1\] S. Soderland, D. Fisher, J. Aseltine, and W. 
Lehnert. CRYSTAL: Inducing a conceptual 
dictionary. In Proceedings of the Fourteenth 
International Joint Conference on Artificial 
Intelligence, Montreal, Canada, August 1995. 
\[2\] H.-H. Chen, Y.-W. Ding, S.-C. Tsai, G.-W. 
Bian, Description of the NTU System used for 
MET-2, Message Understanding Conference 
71 
Proceedings(MUC-7), Washington, DC. 
Available at http://www.muc.saic.com 
/proceedings/muc_7 proceedings/ntumet2 .pdf. 
1998. 
\[3\] Shihong Yu, Shuanhu Bai and Paul Wu,  
Description o f the  Kent Ridge Digital Labs 
System Used for MUC-7, Message 
Understanding Conference Proceedings, 
Washington, DC. Available at 
http://www.muc.saic.com/proceedings/muc_7_p 
? roceedings/kent_ridge.pdf. 1998. 
\[4\] Walter Daelemans, Jakub Zavrel, Ko van der 
Sloot, and Antal van den Bosch. TiMBL: 
Tilburg Memory Based Learner, version 3.0, 
Reference Guide Reference: ILK Technical 
Report 00-01. Available at 
http:llilk.kub.nll-ilklpaperslilkOOO l.ps.gz., 2000. 
\[5\] Walter Daelemans, Antal van den Bosch, 
Jakub Zavrel, Jorn Veenstra, Sabine Buchholz, 
and Bertjan Busser. Rapid development of NLP 
modules with memory-based learning, In 
Proceedings of ELSNET in Wonderland, pp. 
105-113. Utrecht: ELSNET, 1998. 
\[6\] D. Lin. Extracting Collocations from Text 
Corpora. First Workshop on Computational 
Terminology, Montreal, Canada, August, 1998. 
\[7\] Mei Jiaju, ((Tong Yi Ci Ci Lin)> (Chinese), 
Shanghai Dictionary Publishing Press, Shanghai, 
1983. 
\[8\] S. Soderland. "CRYSTAL: Learning 
Domain-specific Text Analysis Rules", 
Technical Report, Center for Intelligent 
Information Retrieval, University of 
Massachusetts, Available from http://www- 
nlp.cs.umass.edu/ciir-pubs/te-43.pdf, 1996. 
72 
A Real-time Integration of Concept-based Search and 
Summarization on Chinese Websites 
Joe F Zhou and Weiquan Liu 
lntel China Research Center 
100020 Beijing, China 
~/oe.f.zhou; louis.liu}@intel.com 
Abstract 
This paper introduces an intuitive search 
environment for casual and novice Chinese 
users over Internet. The system consists of four 
components, a concept network, a query 
reformulation model, a standard search engine, 
and an automatic summarizer. When the user 
enters one or more fairly general and vague 
terms, the search engine returns an initial 
answer set, and at the same time pipes the query 
to the concept network that connects thousands 
of conceptual nodes, each referring to a specific 
concept for a given domain and pointing to a 
number of associated conceptual terms. If the 
concept is located in the network, the related 
conceptual terms are displayed. The user has the 
option of using one or more of these specific 
terms to reformulate he next round of searches. 
Such search iterations continue until the user' s 
ultimate information seeking goal is reached. 
For each search iteration, auto summarizer 
presents the main theme of the document 
retrieved and an optional text-to-speech engine 
can read out the output summary if the user 
prefers. 
1. Introduction 
Internet is changing the world, and at the same 
time changing people' s information seeking 
behaviour. Traditionally, information searchers 
are trained professionals working in libraries or 
other special technical or scientific fields. They 
have developed a variety of  techniques and 
heuristics for addressing information seeking 
difficulties in the environment typically 
dominated by Boolean query formula. These 
Boolean information retrieval systems are 
normally commercial and non-interactive 
systems and the searches conducted in such 
settings are exact-match and set-based retrieval 
from databases of indexed citations and 
abstracts of documents (Koenemann and Belkin, 
1996). With the dramatic explosion of 
information sources over Internet, current user 
population isno longer estricted to' professional 
searchers. Practically it includes everyone in 
life. The majority of these users, however, are 
either casual or novice or both. Casual users, 
such as browsers of news stories, look for 
interesting information rather than information 
relevant o a specific need (Stadnyk and Kass, 
1992). Novice users may have a specific 
information topic, but due to little or no training 
in search and retrieval, they don' t know how 
to make best use of the available operators and 
tools. On the whole, query formulation is one 
problem facing both types of information users 
(Turtle, 1994). Internet users all have difficulty 
in mapping their intent to any logical query 
structure. They prefer limiting their searches to 
one or a simple list of terms, while seeking help 
from the system to guide them to achieve their 
ultimate information goal. 
In this paper we introduce an integrated system 
that combines a standard search system with a 
query reformulation model, a pre-constructed 
concept network, an automatic document 
summarizer, and an optional text-to-speech 
(TTS) engine. Together, these intelligent 
components provide intuitive human 
intelligence to the Chinese users over Internet. 
The integrated system not only guides and 
navigates the user to perform searches in a 
humanly conversational way, but also makes 
and delivers the retrieved information back to 
the user in a simple, succinct, and easily 
comprehensible manner. 
2. System Configuration 
Figure 1 and Figure 2 present the overall system 
configuration and data flow of the integrated 
system. The system consists of four main 
components: a concept net, a query 
148 
reformulation model, a standard search engine, 
and a summarizer. There is also an optional 
component, i.e., if the user chooses, she can 
launch a text-to-speech (TTS) engine to read out 
the automatically generated summary. 
The concept net is a network of conceptual 
terms. It is normally constructed for a specific 
domain with certain amount of human 
intervention. A link connects each pair of 
related concepts in the network, specifying the 
semantic relationship between them. For the 
current economic news domain, the main types 
of relationships include, but not limited to, 
canonical form of, synonym of, hyponym of, 
hypernym of, part of, product of, member of, 
etc. 
The system accepts users' queries expressed in
Chinese natural language. The query may 
contain the terms that are already stored in the 
concept network. Unlike professional searchers, 
Most of Chinese web users have little training in 
information retrieval or have no prior 
knowledge or experience in it. They may not 
even know where to search and how to search. 
To perform an initial search, they tend to use 
one or more very general or vague terms. Under 
such circumstances the system guidance and 
navigation are extremely important. One unique 
functionality of this integrated system is to 
intuitively lead a casual or novice user from a 
more general search to a more specific search 
until the user becomes satisfied with the 
returned information. 
For each search conducted, the query 
reformulation model looks up the concept 
network for more specific terms that are relevant 
to the more general terms in the earlier query. 
For example, if a general term is a company' s 
name, then its subsidiaries, its products, its stock 
symbol, its industrial code, etc. are considered 
to be specific information about the company. 
The query reformulation model either replaces 
or expands the original query with these related 
terms and formulates them into a standardized 
format. Search operators, such as AND, OR, 
NOT, NEAR, etc. are used to connect the terms 
selected. Assigned to each individual term is a 
different weight so as to reshape a new search 
emphasis. 
The standard search engine performs the search 
against the targeted database using the 
reformulated query with N relevant documents 
returned in an order of  the relevance to the 
query (N is a number defined by the user and it 
is 10 by default). At the same time, the concept 
terms in the original query and the 
corresponding specific terms extracted from the 
concept network are also displayed. The web 
interface is designed in a way that makes each 
of the specific terms searchable. The user has 
the choice to select any of these specific terms 
to form a new query. The search engine takes 
the new query to perform the next round of 
search, actually a more specific search based on 
the user' s intention. The iteration continues - 
the more specific search while using more 
specific terms, the closer the user will be to his 
desired information - though he can stop 
anytime after each search iteration to examine 
the retrieved ocuments. 
A text summarizer automatically generates the 
summaries for the documents each search 
iteration returns (Liu and Zhou, 2000). Together 
with the output summary, a selection panel that 
includes the key-word list (average 3 to 6 
words), the headline, and the leading text 
(usually the first 100 characters) of the 
document is displayed. The selection panel 
provides the user with an ability of examining 
the retrieved information more efficiently. By 
glancing over the key words, the user should be 
able to grasp the main idea of the document. He 
can make a decision whether to skip the 
document or continue to learn more about it. If 
his choice is the latter, he can move up to the 
headline or the leading text that offer more 
about he document content. He can, of course, 
move further to look at the summary that is 
supposedly a mini-document of the original. I f  
the user decides to move further to read the 
entire document, then he will click on the 
hyperlinked title or headline of the document. 
The integrated system will go directly online, 
usually a specific website, to grab the document 
for the user. 
Associated with the summarizer is an optional 
text-to-speech (TTS) engine. The user can 
choose either to read the document himself or 
get relaxed by simply listening to the system' s 
voice output. 
149 
Query 
__~ Query 
reformulation 
I Standard 
t~ search engine Summarizer 
Fig. 1 System configuration 
Interface 
User submits anew query 
by selecting one or more 
specific terms displayed 
on the interface (web 
page) 
User enters an initial query 
.I Reformulate the L query \]~' 
q, ,  
I 
Search over the \[ 
I database 
, 
I 
I Display search result / 
Summarize individual 
Text-to-speech i 
conversion : 
Fig. 2 System Data Flow 
3. An Example 
Summarization 
Search and 
In this section we use an example to illustrate 
how the integrated system works. 
Suppose that here is a Chinese user who has no 
experience in search over Internet, neither has 
any idea regarding where or how to do it. He 
just heard some news about F~ (Legend, one 
of the biggest computer manufacturing 
companies in China) and wants to find it out 
from Internet. When we make our integrated 
search and summarization system available to 
him, the only word in his mind is the name of 
the company ~,~ (Legend). So, he enters this 
fairly general term as his fast search query and 
presses the Go button (see Figure 3 (a)). 
Figure 3 (b) shows what the integrated system 
returns to the user in response to his initial 
150 
search. In addition to the top 10 most relevant 
documents, two more specific terms are 
extracted from the pre-constructed concept 
network, i.e., t I~ \ [ \ ]  (Legend Corporation) 
and I~,flgJ\]~ (Legend computer). In the 
concept network these two terms are found to be 
closely associated with the initial search 
termF~,~., (Legend). When the user examines 
the top relevant document returned, he fmds that 
a selection panel is displayed on the interface 
that is associated with this (or each) returned 
document. If looking up from the last row, he 
notices three key words that are extracted from 
the document, i.e., ~ \ [ \ ]  (Legend 
Corporation), ?d,-T:~_~ B\](electronic company), 
~1~ ~)~- (internationally well-known 
brand). By putting these three key words 
together the user should be able to grasp the 
main idea expressed in the document 
(something like "Legend Corporation is an 
electronic company who has some 
internationally well-known brand names" ). I f  
the user wants to know more about the 
document, he can move up to click the button 
f l~3~J  (automatic summarization) or read 
the leading text (in this case the first 50 
characters from the document are extracted). If
the user wants to refer to the entire document, 
he can do it by clicking on the headline. The 
integrated system will go live to Internet o 
retrieve the entire original text. 
But, what if the user is not satisfied with the 
current answer set? Then, he has an option of 
kicking off another ound of  search. To do that 
he may want to narrow down his search by 
choosing between ~o~,~-,~ \[\] (Legend 
Corporation) and I~f~ f l~  (Legend 
computer) - the two relatively speaking, more 
specific terms associated with his initial query 
I~  (Legend). Let us suppose that the user 
decides to select 1~?3.~ \[\]  (Legend 
Corporation) to conduct a new but more 
specific search. The search can be activated by 
simply clicking on this selected term. Figure 3 
(e) shows the results of this search iteration. 
Again, in addition to the top 10 most relevant 
documents returned, more than 20 related terms 
are extracted in this round from the concept 
network that are considered to be conceptually 
relevant o the search termID~,~..,~\[\] (Legend 
Corporation). These terms represent even more 
specific concepts comparing to the two specific 
terms returned in the first round. A detailed 
examination of these terms reveals that they 
represent the following conceptual categories: 
first, more specific news or information about 
the company, such as ~\ [ \ ] IT~3 l r  (the head of 
Chinese 1T), ?P~-T~'~f3~~" (No. 1 of the 
first 100 most powerfil electronic companies in 
China), \ ]~L~ (Joint venture of Legend 
and CA) and ~ t ~  (Happy Family, a 
software kit); the products of Legend 
Corporation, such as F~*~;{~ (Legend 
motherboard), ~,~..~'f~ (Legend software), 
~ ~  (Legend Chinese card); the 
management of Legend company, such as 
F~,~,~J~ (president of Legend), I~,~,~lJ,~,~ 
(vice-president of Legend); and other related 
corporations, such as ~i~:.~ll~\[~ (business 
corporation), ~f13~ (electronic 
corporation), I.~L~ \[\] (industrial 
corporation), and ~ /~ \[\] (Haier 
Corporation). At this point, the user has another 
option. He can go through the selection panel 
associated with each retrieved ocument to see 
which document contains the information he 
wants. If so, his search iteration will terminate. 
If  not, he can reformulate his next search by 
selecting any one or more terms from the 
specific term list. In this particular case, the user 
may go for the first choice since the top 
document retrieved turns out to be identical for 
these two search iterations conducted so far. The 
document entitled J.~ZJ~ll/kJl~!~3-~qF0~ 
~1~I~~ (Reorganization of Legend 
Corporation to welcome the joining of WTO) 
receives the relevance rate of 100% in both 
searches. This is probably the news about 
Legend Corporation the user is looking for. 
As mentioned above, even if the user has 
obtained the information he desires, he still can 
reformulate or expand his next search for more 
specific news about ~ (Legend Corporation). 
Also, the user can launch a completely new 
search for another company. Remember 
~J~/F~\[\] (Haier Corporation), another most 
powerful company in China, appears in the 
specific term list. By clicking on this term, the 
user will be able to locate the latest news or 
other information about a new company. 
151 
Chinese  Web News Summadzer  ant i  R, .Enter  a genera l  te rm,  say ,  
"Legend", the name of a 
; ........... 0"  ...................... 7 - . " la rge  computer company in 
~.~ ................. ~- . j  to begin search 
(a) User enters initial query 
 ,~ ' i  ......................... m~ ........... II II I II I IIIIIIII IIIIIIIIIII II ~1oI~ 
Two more specific ten~s that 
are associated with Legend. 
.e. Le~,end Corp. and Legend 
mpuwr are extracted fi'om 
~ 7 ~ 1 ~ ~  the concept net are displayed. 
. . . . . . . . . . . . . . . . . . . . . . . . .  They can be selected by the 
'~-' ~ h ~ w ~  user to conduct another search 
(b) Results of the first search iteration 
152 
The leading text 
"... More specific terms are 
displayed when "Legend 
Corp." is selected 
Title of  the I st relevant 
document about "Legend 
Corp." 
(c) Results of the second search iteration 
\['~1~ ~/  \ ] / . ~ = ~ ' t  - = i" ~-~1 ~1~, . . . . . .  ~ . . . . . . . . . .  ~ | \ [~  , i i l  . . . . . . . . . . .  l.~.I { | r  . . . . . . . . . .  . . . . . . . . . .  m ~ 
I 
~e 
this 
he TTS 
at the 
:ally 
single 
(d) summary for a single document 
Fig. 3 Snapshots of the interactive interface 
153 
4. Conclusion 
This paper presents an intuitive search system 
for casual or novice Chinese users on specific 
websites. Instead of describing the algorithmic 
portions of each component, we focus on 
illustrating the overall system design and how 
the integrated system works and serves the end- 
users. The presented system provides human 
intelligence to the user who desires to seek 
relevant information over Interact. It does not 
assume that the user receives any training in 
search and retrieval or has any prior experience 
in using Internet. The user can start his search 
with a general and vague term or idea. The 
integrated system will guide the user from a 
general search to a series of more specific 
searches until he is fully satisfied with the 
information returned. Our survey states that the 
existing search systems, especially those in the 
Chinese market, are still dominated by key term 
search mechanism. This integrated system 
presents a new paradigm. It provides a concept- 
driven search environment that allows the user 
to manipulate the semantic relationships 
between the original query terms with its 
associated terms. We have so far built two 
applications based on two specific domains. The 
preliminary results have demonstrated that the 
integration of a concept network, a query 
reformulator, a standard search algorithm, an 
auto summarizer, and an optional TTS engine 
indeed suits the current information seeking 
behavior and make search activities in websites 
more intuitive, as well as productive. 
References 
Brandow et al Brandow R. Mitze K. and Rau L 
F. Automatic Condensation of Electronic 
Publication by Sentence Selection. Information 
Processing & Management, 31(5): 675-68, 1995 
Koenemann J. and Belkin N., A Case For 
Interaction: A Study of Interactive Information 
Retrieval Behavior and Effectiveness. In the 
proceedings ofACM/SIGIR, 1996. 
Stadnyk I. and Kass R. Modeling Users" 
Interests in Information Filters. 
Communications of  the ACM, 35(12):49- 
50, 1992. 
Turtle, H., Natural Language vs. Boolean 
Query Evaluation: A Comparison of  
Retrieval Performance. In the proceedings 
of  ACM/SIGIR, 1994. 
Liu, W. and Zhou J. Building a Chinese Text 
Summarizer with Phrasal Chunks and Domain 
Knowledge. In the Proceedings of Rocling' 
2000. Taipei, 2000. 
154 
