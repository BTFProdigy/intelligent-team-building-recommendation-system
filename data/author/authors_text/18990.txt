Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 210?214,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Finding middle ground? Multi-objective Natural Language Generation
from time-series data
Dimitra Gkatzia, Helen Hastie, and Oliver Lemon
School of Mathematical and Computer Sciences, Heriot-Watt University, Edinburgh
{dg106, h.hastie, o.lemon}@hw.ac.uk
Abstract
A Natural Language Generation (NLG)
system is able to generate text from non-
linguistic data, ideally personalising the
content to a user?s specific needs. In some
cases, however, there are multiple stake-
holders with their own individual goals,
needs and preferences. In this paper, we
explore the feasibility of combining the
preferences of two different user groups,
lecturers and students, when generating
summaries in the context of student feed-
back generation. The preferences of each
user group are modelled as a multivariate
optimisation function, therefore the task
of generation is seen as a multi-objective
(MO) optimisation task, where the two
functions are combined into one. This ini-
tial study shows that treating the prefer-
ences of each user group equally smooths
the weights of the MO function, in a way
that preferred content of the user groups is
not presented in the generated summary.
1 Introduction
Summarisation of time-series data refers to the
task of automatically generating summaries from
attributes whose values change over time. Content
selection is the task of choosing what to say, i.e.
what information to be included in a report (Re-
iter and Dale, 2000). Here, we consider the task
of automatically generating feedback summaries
for students describing their performance during
the lab of a computer science module over the
semester. This work is motivated by the fact that
different user groups have different preferences of
the content that should be conveyed in a summary,
as shown by Gkatzia et al. (2013).
Various factors can influence students? learning,
such as difficulty of the material (Person et al.,
1995), workload (Craig et al., 2004), attendance
in lectures (Ames, 1992) etc. These factors change
over time and can be interdependent. The different
stakeholders (i.e. lecturers and students) have dif-
ferent perceptions regarding what constitutes good
feedback. Therefore, when generating feedback,
we should take into account all preferences in or-
der to be able to produce feedback summaries that
are acceptable by both user groups.
Stakeholders often have conflicting goals, needs
and preferences, for example managers with em-
ployees or doctors with patients and relatives. In
our data, for instance, lecturers tend to comment
on the hours that a student studied, whereas the
students disprefer this content. Generating the
same summary for both groups allows for mean-
ingful further discussion with common ground.
Previous work on NLG systems that address
more than one user group use different versions of
a system for each different user group (Gatt et al.,
2009) or make use of User Models (Janarthanam
and Lemon, 2010; Thompson et al., 2004; Zuk-
erman and Litman, 2001). Here, we explore a
method that adapts to both expert preferences and
users simultaneously (i.e. lecturer and students
preferences), by applying Multi-Objective opti-
misation (MOO). MOO can be applied to situa-
tions where optimal decisions are sought in the
presence of trade-offs between conflicting objec-
tives (Chankong and Haimes, 1983). We explore
whether balancing the preferences of two user
groups can result in an adaptive system that is ac-
ceptable by all users. At the same time, the pro-
gramming effort is reduced as only one system
needs to be developed. Moreover, by pooling all
available data together, there is less need for an
extensive data collection.
In the next section, we present three systems:
one tuned for lecturers, one for students, and one
that attempts to find middle ground. In Section 3,
we describe an evaluation of these three systems
and in Section 4 we discuss the results. Finally, in
210
Section 5, directions for future work are discussed.
2 Methodology
Reinforcement Learning (RL) is a machine learn-
ing technique that defines how an agent learns
to take optimal sequences of actions so as to
maximize a cumulative reward (Sutton and Barto,
1998). Here we extend the framework proposed
by Gkatzia et al. (2013) whereby the content selec-
tion is seen as a Markov Decision problem and the
goal of the agent is to learn to take the sequence
of actions that leads to optimal content selection.
A Temporal Difference learning method (Sutton
and Barto, 1998) was used to train an agent for
content selection. Firstly, we will describe the
data in general. Secondly, we refer to the RL
system that adapts to lecturers? preferences as de-
scribed by Gkatzia et al. (2013). Thirdly, we will
describe how we collected data and developed a
methodology that adapts to students? preferences
and finally how we combined the knowledge of
both steps to develop an MO system. The three
systems (Lecturer-adapted, Student-adapted, MO)
share the same architecture but the difference lies
in the reward functions used for training.
2.1 The Data
For this study, the dataset described by Gkatzia
et al. (2013) was used. Table 1 shows an exam-
ple of this dataset that describes a student?s learn-
ing habits and a corresponding feedback summary
provided by a lecturer. The dataset is composed
of 37 similar instances. Each instance consists of
time-series information about the student?s learn-
ing routine and the selected templates that lectur-
ers used to provide feedback to this student. A
template is a quadruple consisting of an id, a fac-
tor (Table 1), a reference type (trend, weeks, aver-
age, other) and surface text. For instance, a tem-
plate can be (1, marks, trend, ?Your marks were
<trend>over the semester?). The lexical choice
for <trend>(i.e. increasing or decreasing) de-
pends on the values of time-series data. There
is a direct mapping between the values of factor
and reference type and the surface text. The time-
series attributes are listed in Table 1 (bottom left).
2.2 Time-series summarisation systems
Actions and states: The state consists of the time-
series data and the selected templates. In order to
explore the state space the agent selects a time-
series attribute (e.g. marks, deadlines etc.) and
then decides whether to talk about it or not. The
states and actions are similar for all systems.
Lecturer-adapted reward function
The reward function is derived from analysis with
linear regression of the provided dataset and is the
following cumulative multivariate function:
Reward
LECT
= a+
n
?
i=1
b
i
? x
i
+ c ? length
where X = {x
1
, x
2
, ..., x
n
} is the vector of
combinations of the data trends observed in the
time-series data and a particular reference type of
the factor. The value of x
i
is given by the function:
x
i
=
?
?
?
?
?
?
?
?
?
?
?
1, if the combination of a factor trend
and a particular reference type is
included in the feedback
0, if not.
The coefficients represent the preference level of
a factor to be selected and how to be conveyed
in the summary. Important factors are associated
with high positive coefficients and the unimpor-
tant ones with negative coefficients. In the train-
ing phase, the agent selects a factor and then de-
cides whether to talk about it or not. If it decides
to refer to a factor, the selection of the template is
performed deterministically, i.e. it selects the tem-
plate that results in higher reward. Length rep-
resents the number of factors selected for gener-
ation.
Student-adapted reward function
The Student-adapted system uses the same RL al-
gorithm as the Lecturer-adapted one. The differ-
ence lies in the reward function. The reward func-
tion used for training is of a similar style as the
Lecturer-adapted reward function. This function
was derived by manipulating the student ratings in
a previous experiment and estimating the weights
using linear regression in a similar way as Walker
et al. (1997) and Rieser et al. (2010).
Multi-objective function
The function used for the multi-objective method
is derived by weighting the sum of the individual
reward functions.
R
MO
= 0.5 ? R
LECT
+ 0.5 ? R
STUDENT
To reduce the confounding variables, we kept
the ordering of content in all systems the same.
3 Evaluation
The output of the above-mentioned three systems
were evaluated both in simulation and with real
211
Raw Data
factors week 2 week 3 ... week 10
marks 5 4 ... 5
hours studied 1 2 ... 3
... ... ... ... ...
Trends from Data
factors factor trend
(1) marks trend other
(2) hours studied trend increasing
(3) understandability trend decreasing
(4) difficulty trend decreasing
(5) deadlines trend increasing
(6) health issues trend other
(7) personal issues trend decreasing
(8) lectures attended trend other
(9) revision trend decreasing
Summary
Your overall performance was excellent
during the semester. Keep up the good
work and maybe try some more challeng-
ing exercises. Your attendance was vary-
ing over the semester. Have a think about
how to use time in lectures to improve your
understanding of the material. You spent 2
hours studying the lecture material on
average. You should dedicate more time
to study. You seem to find the material
easier to understand compared to the
beginning of the semester. Keep up the
good work! You revised part of the learn-
ing material. Have a think about whether
revising has improved your performance.
Table 1: Top left: example of the time-series raw data for feedback generation. Bottom left: example of
described trends. Right box: a target summary generated by an expert (bold signifies the chosen content).
users. Example summaries of all systems are pre-
sented in Table 2.
3.1 Evaluation in Simulation
26 summaries were produced by each system. The
output of each system was evaluated with the three
reward functions. Table 3 shows the results.
As expected, all systems score highly when
evaluated with the reward function for which
they were trained, with the second highest reward
scored from the MO function. Table 2 illustrates
this with the MO Policy clearly between the other
two policies. Moreover, the MO function reduces
the variability between summaries as is also re-
flected in the standard deviation given in Table 3.
We used BLEU (4-grams) (Papineni et al.,
2002) to measure the similarities between the
feedback summaries generated by the three sys-
tems. BLEU score is between 0-1 with values
closer to 1 indicating texts are more similar. Our
results demonstrate that the summaries generated
by the three systems are quite different (BLEU
score between 0.33 and 0.36). This shows that the
framework presented here is capable of producing
quite different summaries based on the various re-
ward functions.
3.2 Evaluation with real users
The goal of the evaluation is to determine whether
the end-user can pick up on the above-mentioned
differences in the feedback and rank them accord-
ing to their preferences. The output of the three
systems was ranked by 19 lecturers and 48 first-
year Computer Science students. Time-series data
of three students were presented on graphs to each
participant. They were also shown 3 feedback
summaries and they were asked to rank them in
terms of preference.
As we can see from Table 4, the two user groups
significantly preferred the output of the system
which was trained for their preferences (Mann-
Whitney U test, p < 0.05). Interestingly, lecturers
found both the outputs produced by the Lecturer-
adapted system and the Student-adapted system
significantly preferable (p < 0.05) to the output
produced by the MO system. In contrast, students
significantly preferred the output generated by the
Student-adapted system over the other two. Fi-
nally, both user groups rated the MO system 3rd,
but there is not a significant difference between
the student ratings for the MO system and the
Lecturer-adapted system.
4 Discussion
It is interesting to examine the weights derived
from the multiple-linear regression to determine
the preferences of the different user groups. For
instance, lecturers? most preferred content is
hours studied, therefore the reward function gives
high scores to summaries that mention the hours
212
Lecturer-adapted Student-adapted Multi-objective
Make sure you revise the learning
material and try to do the lab ex-
ercises again. You dedicated more
time studying the lecture material in
the beginning of the semester com-
pared to the end of the semester.
Have a think about what is prevent-
ing you from studying. Your under-
standing of the material could be
improved. Try going over the teach-
ing material again. You have had
other deadlines during weeks 5, 6,
8, 9 and 10. You may want to plan
your studying and work ahead. You
did not face any health problems
during the semester.
You found the lab exercises very
challenging. Make sure that you
have understood the taught material
and don?t hesitate to ask for clari-
fication. You dedicated more time
studying the lecture material in
the beginning of the semester com-
pared to the end of the semester.
Have a think about what is prevent-
ing you from studying. Your un-
derstanding of the material could
be improved. Try going over the
teaching material again. Revising
material during the semester will
improve your performance in the
lab.
Your attendance was varying over the
semester. Have a think about how to
use time in lectures to improve your un-
derstanding of the material. You found
the lab exercises very challenging. Make
sure that you have understood the taught
material and don?t hesitate to ask for
clarification. You dedicated more time
studying the lecture material in the be-
ginning of the semester compared to the
end of the semester. Have a think about
what is preventing you from studying.
You did not face any health problems
during the semester. You revised part
of the learning material. Have a think
whether revising has improved your per-
formance.
Table 2: Example outputs from the three different systems (bold signifies the chosen content).
Time-Series Summarisation Systems Lecturer Function Student Function MO Function
Lecturer-adapted system 243.82 (70.35) 51.99 (89.87) 114.12 (49.58)
Student-adapted system 72.54 (106.97) 213.75 (59.45) 127.76 (52.09)
MO system 123.67 (72.66) 153.79 (56.61) 164.84 (83.89)
Table 3: Average rewards (and standard deviation) assigned to summaries produced by the 3 systems.
Bold signifies higher reward.
Summarisation
Systems
Lecturer?s Rat-
ing
Student?s
Rating
Lecturer-adapted 1st (2.15)* 3rd (1.97)
Student-adapted 1st (2.01)* 1st* (2.22)
MO 2nd, 3rd (1.81) 3rd (1.79)
Table 4: Mode of the ratings for each user group
(*Mann-Whitney U test, p < 0.05, when compar-
ing each system to the MO system).
that a student studied in all cases (i.e. when the
hours studied increased, decreased, or remained
stable). This, however, does not factor heavily into
the student?s reward function.
Secondly, lecturers find it useful to give some
advice to students who faced personal issues dur-
ing the semester, such as advising them to talk to
their mentor. Students, on the other hand, like
reading about personal issues only when the num-
ber of issues they faced was increasing over the
semester, perhaps as this is the only trend that may
affect their performance. Students seem to mostly
prefer a feedback summary that mentions the un-
derstandability of the material when it increases
which is positive feedback. Finally, the only factor
that both groups agree on is that health issues is
negatively weighted and therefore not mentioned.
The MO reward function attempts to balance
the preferences of the two user groups. Therefore,
for this function, the coefficient for mentioning
health issues is also negative, however the other
coefficients are smoothed providing neither strong
negative or positive coefficients. This means that
there is less variability (see Table 3) but that per-
haps this function meets neither group?s criteria.
5 Conclusion and Future Work
In conclusion, we presented a framework for de-
veloping and evaluating various reward functions
for time-series summarisation of feedback. This
framework has been validated in that both simula-
tion and subjective studies show that each group
does indeed prefer feedback generated using a
highly tuned reward function, with lecturers being
slightly more open to variation. Further investiga-
tion is required as to whether it is indeed possible
to find middle ground between these two groups.
Choices for one group may be negatively rated
by the other and it might not be possible to find
middle ground but it is worth investigating further
other methods of reward function derivation using
stronger feature selection methods, such as Princi-
pal Component Analysis.
213
References
Carole Ames. 1992. Classrooms: Goals, structures,
and student motivation. Journal of Educational Psy-
chology, 84(3):p261?71.
Chankong and Haimes. 1983. Multiobjective decision
making theory and methodology. In New York: El-
sevier Science Publishing.
Scotty D. Craig, Arthur C. Graesser, Jeremiah Sullins,
and Barry Gholson. 2004. Affect and learning: an
exploratory look into the role of affect in learning
with autotutor. In Journal of Educational Media,
29:241-250.
Albert Gatt, Francois Portet, Ehud Reiter, James
Hunter, Saad Mahamood, Wendy Moncur, and So-
mayajulu Sripada. 2009. From data to text in the
neonatal intensive care unit: Using NLG technology
for decision support and information management.
In Journal of AI Communications, 22:153-186.
Dimitra Gkatzia, Helen Hastie, Srinivasan Ja-
narthanam, and Oliver Lemon. 2013. Generating
student feedback from time-series data using Rein-
forcement Learning. In 14th European Workshop in
Natural Language Generation.
Srinivasan Janarthanam and Oliver Lemon. 2010.
Adaptive referring expression generation in spoken
dialogue systems: Evaluation with real users. In
11th Annual Meeting of the Special Interest Group
on Discourse and Dialogue.
K Papineni, S Roukos, T. Ward, and W. J Zhu. 2002.
BLEU: a method for automatic evaluation of ma-
chine translation. In 40th Annual meeting of the As-
sociation for Computational Linguistics.
Natalie K. Person, Roger J. Kreuz, Rolf A. Zwaan, and
Arthur C. Graesser. 1995. Pragmatics and peda-
gogy: Conversational rules and politeness strategies
may inhibit effective tutoring. In Journal of Cogni-
tion and Instruction, 13(2):161-188.
Ehud Reiter and Robert Dale. 2000. Building natural
language generation systems. InCambridge Univer-
sity Press.
Verena Rieser, Oliver Lemon, and Xingkun Liu. 2010.
Optimising information presentation for spoken dia-
logue systems. In 48th Annual Meeting of the Asso-
ciation for Computational Linguistics.
Richart Sutton and Andrew Barto. 1998. Reinforce-
ment learning. In MIT Press.
Cynthia A. Thompson, Mehmet H. Goker, and Pat Lan-
gley. 2004. A personalised system for conversa-
tional recommendations. In Journal of Artificial In-
telligence Research 21, 333-428.
Marilyn Walker, Diane Litman, Candace Kamm, and
Alicia Abella. 1997. PARADISE: A framework for
evaluating spoken dialogue agents. In 35th Annual
meeting of the Association for Computational Lin-
guistics.
Ingrid Zukerman and Diane Litman. 2001. Natu-
ral language processing and user modeling: Syner-
gies and limitations. In User Modeling and User-
Adapted Interaction, 11(1-2), 129-158.
214
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1231?1240,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Comparing Multi-label Classification with Reinforcement Learning for
Summarisation of Time-series Data
Dimitra Gkatzia, Helen Hastie, and Oliver Lemon
School of Mathematical and Computer Sciences, Heriot-Watt University, Edinburgh
{dg106, h.hastie, o.lemon}@hw.ac.uk
Abstract
We present a novel approach for automatic
report generation from time-series data, in
the context of student feedback genera-
tion. Our proposed methodology treats
content selection as a multi-label (ML)
classification problem, which takes as in-
put time-series data and outputs a set of
templates, while capturing the dependen-
cies between selected templates. We show
that this method generates output closer to
the feedback that lecturers actually gener-
ated, achieving 3.5% higher accuracy and
15% higher F-score than multiple simple
classifiers that keep a history of selected
templates. Furthermore, we compare a
ML classifier with a Reinforcement Learn-
ing (RL) approach in simulation and using
ratings from real student users. We show
that the different methods have different
benefits, with ML being more accurate for
predicting what was seen in the training
data, whereas RL is more exploratory and
slightly preferred by the students.
1 Introduction
Summarisation of time-series data refers to the
task of automatically generating text from vari-
ables whose values change over time. We con-
sider the task of automatically generating feed-
back summaries for students describing their per-
formance during the lab of a Computer Science
module over the semester. Students? learning can
be influenced by many variables, such as difficulty
of the material (Person et al, 1995), other dead-
lines (Craig et al, 2004), attendance in lectures
(Ames, 1992), etc. These variables have two im-
portant qualities. Firstly, they change over time,
and secondly they can be dependent on or inde-
pendent of each other. Therefore, when generating
feedback, we need to take into account all vari-
ables simultaneously in order to capture potential
dependencies and provide more effective and use-
ful feedback that is relevant to the students.
In this work, we concentrate on content selec-
tion which is the task of choosing what to say,
i.e. what information is to be included in a report
(Reiter and Dale, 2000). Content selection deci-
sions based on trends in time-series data determine
the selection of the useful and important variables,
which we refer to here as factors, that should be
conveyed in a summary. The decisions of factor
selection can be influenced by other factors that
their values are correlated with; can be based on
the appearance or absence of other factors in the
summary; and can be based on the factors? be-
haviour over time. Moreover, some factors may
have to be discussed together in order to achieve
some communicative goal, for instance, a teacher
might want to refer to student?s marks as a moti-
vation for increasing the number of hours studied.
We frame content selection as a simple classifi-
cation task: given a set of time-series data, decide
for each template whether it should be included
in a summary or not. In this paper, with the term
?template? we refer to a quadruple consisting of an
id, a factor (bottom left of Table 1), a reference
type (trend, weeks, average, other) and surface
text. However, simple classification assumes that
the templates are independent of each other, thus
the decision for each template is taken in isolation
from the others, which is not appropriate for our
domain. In order to capture the dependencies in
the context, multiple simple classifiers can make
the decisions for each template iteratively. After
each iteration, the feature space grows by 1 fea-
ture, in order to include the history of the previous
template decisions. Here, we propose an alterna-
tive method that tackles the challenge of interde-
pendent data by using multi-label (ML) classifica-
tion, which is efficient in taking data dependencies
1231
Raw Data
factors week 2 week 3 ... week 10
marks 5 4 ... 5
hours studied 1 2 ... 3
... ... ... ... ...
Trends from Data
factors trend
(1) marks (M) trend other
(2) hours studied (HS) trend increasing
(3) understandability (Und) trend decreasing
(4) difficulty (Diff) trend decreasing
(5) deadlines (DL) trend increasing
(6) health issues (HI) trend other
(7) personal issues (PI) trend decreasing
(8) lectures attended (LA) trend other
(9) revision (R) trend decreasing
Summary
Your overall performance was excellent
during the semester. Keep up the good
work and maybe try some more challeng-
ing exercises. Your attendance was vary-
ing over the semester. Have a think about
how to use time in lectures to improve your
understanding of the material. You spent 2
hours studying the lecture material on
average. You should dedicate more time
to study. You seem to find the material
easier to understand compared to the
beginning of the semester. Keep up the
good work! You revised part of the learn-
ing material. Have a think whether revis-
ing has improved your performance.
Table 1: The table on the top left shows an example of the time-series raw data for feedback generation.
The table on the bottom left shows an example of described trends. The box on the right presents a target
summary (target summaries have been constructed by teaching staff).
into account and generating a set of labels (in our
case templates) simultaneously (Tsoumakas et al,
2010). ML classification requires no history, i.e.
does not keep track of previous decisions, and thus
has a smaller feature space.
Our contributions to the field are as follows: we
present a novel and efficient method for tackling
the challenge of content selection using a ML clas-
sification approach; we applied this method to the
domain of feedback summarisation; we present a
comparison with an optimisation technique (Rein-
forcement Learning), and we discuss the similari-
ties and differences between the two methods.
In the next section, we refer to the related work
on Natural Language Generation from time-series
data and on Content Selection. In Section 4.2, we
describe our approach and we carry out a compar-
ison with simple classification methods. In Sec-
tion 5, we present the evaluation setup and in Sec-
tion 6 we discuss the results, obtained in simula-
tion and with real students. Finally, in Section 8,
directions for future work are discussed.
2 Related Work
Natural Language Generation from time-series
data has been investigated for various tasks such
as weather forecast generation (Belz and Kow,
2010; Angeli et al, 2010; Sripada et al, 2004),
report generation from clinical data (Hunter et al,
2011; Gatt et al, 2009), narrative to assist children
with communication needs (Black et al, 2010) and
audiovisual debrief generation from sensor data
from Autonomous Underwater Vehicles missions
(Johnson and Lane, 2011).
The important tasks of time-series data sum-
marisation systems are content selection (what to
say), surface realisation (how to say it) and infor-
mation presentation (Document Planning, Order-
ing, etc.). In this work, we concentrate on content
selection. Previous methods for content selection
include Reinforcement Learning (Rieser et al,
2010); multi-objective optimisation (Gkatzia et
al., 2014); Gricean Maxims (Sripada et al, 2003);
Integer Linear Programming (Lampouras and An-
droutsopoulos, 2013); collective content selection
(Barzilay and Lapata, 2004); interest scores as-
signed to content (Androutsopoulos et al, 2013); a
combination of statistical and template-based ap-
proaches to NLG (Kondadadi et al, 2013); statis-
tical acquisition of rules (Duboue and McKeown,
2003) and the Hidden Markov model approach for
Content Selection and ordering (Barzilay and Lee,
2004).
Collective content selection (Barzilay and La-
pata, 2004) is similar to our proposed method in
that it is a classification task that predicts the tem-
plates from the same instance simultaneously. The
difference between the two methods lies in that the
1232
collective content selection requires the considera-
tion of an individual preference score (which is de-
fined as the preference of the entity to be selected
or omitted, and it is based on the values of entity
attributes and is computed using a boosting algo-
rithm) and the identification of links between the
entities with similar labels. In contrast, ML clas-
sification does not need the computation of links
between the data and the templates. ML classi-
fication can also apply to other problems whose
features are correlated, such as text classification
(Madjarov et al, 2012), when an aligned dataset is
provided.
ML classification algorithms have been divided
into three categories: algorithm adaptation meth-
ods, problem transformation and ensemble meth-
ods (Tsoumakas and Katakis, 2007; Madjarov
et al, 2012). Algorithm adaptation approaches
(Tsoumakas et al, 2010) extend simple classifi-
cation methods to handle ML data. For exam-
ple, the k-nearest neighbour algorithm is extended
to ML-kNN by Zhang and Zhou (2007). ML-
kNN identifies for each new instance its k nearest
neighbours in the training set and then it predicts
the label set by utilising the maximum a posteri-
ori principle according to statistical information
derived from the label sets of the k neighbours.
Problem transformation approaches (Tsoumakas
and Katakis, 2007) transform the ML classifica-
tion task into one or more simple classification
tasks. Ensemble methods (Tsoumakas et al, 2010)
are algorithms that use ensembles to perform ML
learning and they are based on problem transfor-
mation or algorithm adaptation methods. In this
paper, we applied RAkEL (Random k-labelsets)
(Tsoumakas et al, 2010): an ensemble problem
transformation method, which constructs an en-
semble of simple-label classifiers, where each one
deals with a random subset of the labels.
Finally, our domain for feedback generation is
motivated by previous studies (Law et al, 2005;
van den Meulen et al, 2010) who show that text
summaries are more effective in decision making
than graphs therefore it is advantageous to provide
a summary over showing users the raw data graph-
ically. In addition, feedback summarisation from
time-series data can be applied to the field of In-
telligent Tutoring Systems (Gross et al, 2012).
3 Data
The dataset consists of 37 instances referring to
the activities of 26 students. For a few students
there is more than 1 instance. An example of one
such instance is presented in Table 1. Each in-
stance includes time-series information about the
student?s learning habits and the selected tem-
plates that lecturers used to provide feedback to
this student. The time-series information includes
for each week of the semester: (1) the marks
achieved at the lab; (2) the hours that the stu-
dent spent studying; (3) the understandability of
the material; (4) the difficulty of the lab exercises
as assessed by the student; (5) the number of other
deadlines that the student had that week; (6) health
issues; (7) personal issues; (8) the number of lec-
tures attended; and (9) the amount of revision that
the student had performed. The templates describe
these factors in four different ways:
1. <trend>: referring to the trend of a fac-
tor over the semester (e.g. ?Your performance
was increasing...?),
2. <weeks>: explicitly describing the factor
value at specific weeks (e.g. ?In weeks 2, 3
and 9...?),
3. <average>: considering the average of a
factor value (e.g. ?You dedicated 1.5 hours
studying on average...?), and
4. <other>: mentioning other relevant infor-
mation (e.g. ?Revising material will improve
your performance?).
For the corpus creation, 11 lecturers selected the
content to be conveyed in a summary, given the
set of raw data (Gkatzia et al, 2013). As a result,
for the same student there are various summaries
provided by the different experts. This character-
istic of the dataset, that each instance is associated
with more than one solution, additionally moti-
vates the use of multi-label classification, which
is concerned with learning from examples, where
each example is associated with multiple labels.
Our analysis of the dataset showed that there
are significant correlations between the factors, for
example, the number of lectures attended (LA)
correlates with the student?s understanding of the
material (Und), see Table 2. As we will discuss
further in Section 5.1, content decisions are in-
fluenced by the previously generated content, for
example, if the lecturer has previously mentioned
health issues, mentioning hours studied has a high
probability of also being mentioned.
1233
Factor (1) M (2) HS (3) Und (4) Diff (5) DL (6) HI (7) PI (8) LA (9) R
(1) M 1* 0.52* 0.44* -0.53* -0.31 -0.30 -0.36* 0.44* 0.16
(2) HS 0.52* 1* 0.23 -0.09 -0.11 0.11 -0.29 0.32 0.47*
(3) Und 0.44* 0.23 1* -0.54* 0.03 -0.26 0.12 0.60* 0.32
(4) Diff -0.53* -0.09 -0.54* 1* 0.16 -0.06 0.03 -0.19 0.14
(5) DL -0.31 -0.11 0.03 0.16 1* 0.26 0.24 -0.44* 0.14
(6) HI -0.30 -0.11 -0.26 -0.06 0.26 1* 0.27 -0.50* 0.15
(7) PI -0.36* -0.29 0.12 0.03 0.24 0.27 1* -0.46* 0.34*
(8) LA 0.44* 0.32 0.60* -0.19 -0.44* -0.50* -0.46* 1* -0.12
(9) R 0.16 0.47* 0.03 0.14 0.14 0.15 0.34* -0.12 1*
Table 2: The table presents the Pearson?s correlation coefficients of the factors (* means p<0.05).
4 Methodology
In this section, the content selection task and the
suggested multi-label classification approach are
presented. The development and evaluation of the
time-series generation system follows the follow-
ing pipeline (Gkatzia et al, 2013):
1. Time-Series data collection from students
2. Template construction by Learning and
Teaching (L&T) expert
3. Feedback summaries constructed by lectur-
ers; random summaries rated by lecturers
4. Development of time-series generation sys-
tems (Section 4.2, Section 5.3): ML system,
RL system, Rule-based and Random system
5. Evaluation: (Section 5)
- Offline evaluation (Accuracy and Reward)
- Online evaluation (Subjective Ratings)
4.1 The Content Selection Task
Our learning task is formed as follows: given a
set of 9 time-series factors, select the content that
is most appropriate to be included in a summary.
Content is regarded as labels (each template rep-
resents a label) and thus the task can be thought of
as a classification problem. As mentioned, there
are 4 ways to refer to a factor: (1) describing the
trend, (2) describing what happened in every time
stamp, (3) mentioning the average and (4) making
another general statement. Overall, for all factors
there are 29 different templates
1
. An example of
the input data is shown in Table 1. There are two
decisions that need to be made: (1) whether to talk
about a factor and (2) in which way to refer to it.
Instead of dealing with this task in a hierarchical
way, where the algorithm will first learn whether
to talk about a factor and then to decide how to
1
There are fewer than 36 templates, because for some fac-
tors there are less than 4 possible ways of referring to them.
refer to it, we transformed the task in order to re-
duce the learning steps. Therefore, classification
can reduce the decision workload by deciding ei-
ther in which way to talk about it, or not to talk
about a factor at all.
4.2 The Multi-label Classification Approach
Traditional single-label classification is the task of
identifying which label one new observation is as-
sociated with, by choosing from a set of labels L
(Tsoumakas et al, 2010). Multi-label classifica-
tion is the task of associating an observation with
a set of labels Y ? L (Tsoumakas et al, 2010).
One set of factor values can result in various
sets of templates as interpreted by the different
experts. A ML classifier is able to make deci-
sions for all templates simultaneously and cap-
ture these differences. The RAndom k-labELsets
(RAkEL) (Tsoumakas et al, 2010) was applied
in order to perform ML classification. RAkEL is
based on Label Powerset (LP), a problem transfor-
mation method (Tsoumakas et al, 2010). LP ben-
efits from taking into consideration label correla-
tions, but does not perform well when trained with
few examples as in our case (Tsoumakas et al,
2010). RAkEL overcomes this limitation by con-
structing a set of LP classifiers, which are trained
with different random subsets of the set of labels
(Tsoumakas et al, 2010).
The LP method transforms the ML task, into
one single-label multi-class classification task,
where the possible set of predicted variables for
the transformed class is the powerset of labels
present in the original dataset. For instance, the set
of labels L = {temp
0
, temp
1
, ...temp
28
} could be
transformed to {temp
0,1,2
, temp
28,3,17,
...}. This
algorithm does not perform well when consider-
ing a large number of labels, due to the fact that
the label space grows exponentially (Tsoumakas
1234
Classifier Accuracy Precision Recall F score
(10-fold)
Decision Tree (no history) *75.95% 67.56 75.96 67.87
Decision Tree (with predicted history) **73.43% 65.49 72.05 70.95
Decision Tree (with real history) **78.09% 74.51 78.11 75.54
Majority-class (single label) **72.02% 61.73 77.37 68.21
RAkEL (multi-label) (no history) 76.95% 85.08 85.94 85.50
Table 3: Average, precision, recall and F-score of the different classification methods (T-test, * denotes
significance with p<0.05 and ** significance with p<0.01, when comparing each result to RAkEL).
et al, 2010). RAkEL tackles this problem by con-
structing an ensemble of LP classifiers and train-
ing each one on a different random subset of the
set of labels (Tsoumakas et al, 2010).
4.2.1 The Production Phase of RAkEL
The algorithm was implemented using the MU-
LAN Open Source Java library (Tsoumakas et
al., 2011), which is based on WEKA (Witten and
Frank, 2005). The algorithm works in two phases:
1. the production of an ensemble of LP algo-
rithms, and
2. the combination of the LP algorithms.
RAkEL takes as input the following parameters:
(1) the numbers of iterations m (which is devel-
oper specified and denotes the number of models
that the algorithm will produce), (2) the size of la-
belset k (which is also developer specified), (3) the
set of labels L, and (4) the training set D. During
the initial phase it outputs an ensemble of LP clas-
sifiers and the corresponding k-labelsets. A pseu-
docode for the production phase is shown below:
Algorithm 1 RAkEL production phase
1 : I n p u t : i t e r a t i o n s m, k l a b e l s e t s ,
l a b e l s L , t r a i n i n g d a t a D
2 : f o r i =0 t o m
3 : S e l e c t random k? l a b e l s e t from L
4 : T r a i n an LP on D
5 : Add LP t o ensemble
6 : end f o r
7 : Outpu t : t h e ensemble o f LPs
wi th c o r r e s p o n d i n g k? l a b e l s e t s
4.2.2 The Combination Phase
During the combination phase, the algorithm takes
as input the results of the production phase, i.e.
the ensemble of LPs with the corresponding k-
labelsets, the set of labels L, and the new instance
x and it outputs the result vector of predicted la-
bels for instance x. During run time, RAkEL es-
timates the average decision for each label in L
and if the average is greater than a threshold t (de-
termined by the developer) it includes the label in
the predicted labelset. We used the standard pa-
rameter values of t, k and m (t = 0.5, k = 3 and
m equals to 58 (2*29 templates)). In future, we
could perform parameter optimisation by using a
technique similar to (Gabsdil and Lemon, 2004).
5 Evaluation
Firstly, we performed a preliminary evaluation on
classification methods, comparing our proposed
ML classification with multiple iterated classifica-
tion approaches. The summaries generated by the
ML classification system are then compared with
the output of a RL system and two baseline sys-
tems in simulation and with real students.
5.1 Comparison with Simple Classification
We compared the RAkEL algorithm with single-
label (SL) classification. Different SL classifiers
were trained using WEKA: JRip, Decision Trees,
Naive Bayes, k-nearest neighbour, logistic regres-
sion, multi-layer perceptron and support vector
machines. It was found out that Decision Trees
achieved on average 3% higher accuracy. We,
therefore, went on to use Decision Trees that use
generation history in three ways.
Firstly, for Decision Tree (no history), 29
decision-tree classifiers were trained, one for each
template. The input of these classifiers were the
9 factors and each classifier was trained in order
to decide whether to include a specific template or
not. This method did not take into account other
selected templates ? it was only based on the time-
series data.
Secondly, for Decision Tree (with predicted
history), 29 classifiers were also trained, but this
time the input included the previous decisions
made by the previous classifiers (i.e. the history)
1235
as well as the set of time-series data in order to
emulate the dependencies in the dataset. For in-
stance, classifier n was trained using the data from
the 9 factors and the template decisions for tem-
plates 0 to n? 1.
Thirdly, for Decision Tree (with real his-
tory), the real, expert values were used rather
than the predicted ones in the history. The
above-mentioned classifiers are compared with,
the Majority-class (single label) baseline, which
labels each instance with the most frequent tem-
plate.
The accuracy, the weighted precision, the
weighted recall, and the weighted F-score of the
classifiers are shown in Table 3. It was found that
in 10-fold cross validation RAkEL performs sig-
nificantly better in all these automatic measures
(accuracy = 76.95%, F-score = 85.50%). Remark-
ably, ML achieves more than 10% higher F-score
than the other methods (Table 3). The average
accuracy of the single-label classifiers is 75.95%
(10-fold validation), compared to 73.43% of clas-
sification with history. The reduced accuracy of
the classification with predicted history is due to
the error in the predicted values. In this method,
at every step, the predicted outcome was used in-
cluding the incorrect decisions that the classifier
made. The upper-bound accuracy is 78.09% cal-
culated by using the expert previous decisions and
not the potentially erroneous predicted decisions.
This result is indicative of the significance of the
relations between the factors showing that the pre-
dicted decisions are dependent due to existing cor-
relations as discussed in Section 1, therefore the
system should not take these decisions indepen-
dently. ML classification performs better because
it does take into account these correlations and de-
pendencies in the data.
5.2 The Reinforcement Learning System
Reinforcement Learning (RL) is a machine learn-
ing technique that defines how an agent learns to
take optimal actions so as to maximise a cumu-
lative reward (Sutton and Barto, 1998). Content
selection is seen as a Markov Decision problem
and the goal of the agent is to learn to take the se-
quence of actions that leads to optimal content se-
lection. The Temporal Difference learning method
was used to train an agent for content selection.
Actions and States: The state consists of the
time-series data and the selected templates. In or-
der to explore the state space the agent selects a
factor (e.g. marks, deadlines etc.) and then decides
whether to talk about it or not.
Reward Function: The reward function reflects
the lecturers? preferences on summaries and is
derived through linear regression analysis of a
dataset containing lecturer constructed summaries
and ratings of randomly generated summaries.
Specifically, it is the following cumulative multi-
variate function:
Reward = a+
n
?
i=1
b
i
? x
i
+ c ? length
where X = {x
1
, x
2
, ..., x
n
} describes the com-
binations of the data trends observed in the time-
series data and a particular template. a, b and c are
the regression coefficients, and their values vary
from -99 to 221. The value of x
i
is given by the
function:
x
i
=
?
?
?
?
?
?
?
?
?
?
?
1, the combination of a factor trend
and a template type is included
in a summary
0, if not.
The RL system differs from the classification
system in the way it performs content selection.
In the training phase, the agent selects a factor and
then decides whether to talk about it or not. If the
agent decides to refer to a factor, the template is
selected in a deterministic way, i.e. from the avail-
able templates it selects the template that results in
higher expected cumulative future reward.
5.3 The Baseline Systems
We compared the ML system and the RL system
with two baselines described below by measuring
the accuracy of their outputs, the reward achieved
by the reward function used for the RL system,
and finally we also performed evaluation with stu-
dent users. In order to reduce the confounding
variables, we kept the ordering of content in all
systems the same, by adopting the ordering of the
rule-based system. The baselines are as follows:
1. Rule-based System: generates summaries
based on Content Selection rules derived by work-
ing with a L&T expert and a student (Gkatzia et
al., 2013).
2. Random System: initially, selects a factor
randomly and then selects a template randomly,
until it makes decisions for all factors.
1236
Time-Series Accuracy Reward Rating Mode (mean) Data Source
Summarisation Systems
Multi-label Classification 85% 65.4 7 (6.24) Lecturers? constructed summaries
Reinforcement Learning **66% 243.82 8 (6.54) Lecturers? ratings & summaries
Rule-based **65% 107.77 7, 8 (5.86) L&T expert
Random **45.2% 43.29 *2 (*4.37) Random
Table 4: Accuracy, average rewards (based on lecturers? preferences) and averages of the means of the
student ratings. Accuracy significance (Z-test) with RAkEL at p<0.05 is indicated as * and at p<0.01
as **. Student ratings significance (Mann Whitney U test) with RAkEL at p<0.05 is indicated as *.
6 Results
Each of the four systems described above gener-
ated 26 feedback summaries corresponding to the
26 student profiles. These summaries were evalu-
ated in simulation and with real student users.
6.1 Results in Simulation
Table 4 presents the accuracy, reward, and mode
of student rating of each algorithm when used to
generate the 26 summaries. Accuracy was esti-
mated as the proportion of the correctly classified
templates to the population of templates. In or-
der to have a more objective view on the results,
the score achieved by each algorithm using the
reward function was also calculated. ML clas-
sification achieved significantly higher accuracy,
which was expected as it is a supervised learning
method. The rule-based system and the RL sys-
tem have lower accuracy compared to the ML sys-
tem. There is evidently a mismatch between the
rules and the test-set; the content selection rules
are based on heuristics provided by a L&T Expert
rather than by the same pool of lecturers that cre-
ated the test-set. On the contrary, the RL is trained
to optimise the selected content and not to repli-
cate the existing lecturer summaries, hence there
is a difference in accuracy.
Accuracy measures how similar the generated
output is to the gold standard, whereas the reward
function calculates a score regarding how good
the output is, given an objective function. RL is
trained to optimise for this function, and therefore
it achieves higher reward, whereas ML is trained
to learn by examples, therefore it produces out-
put closer to the gold standard (lecturer?s produced
summaries). RL uses exploration and exploitation
to discover combinations of content that result in
higher reward. The reward represents predicted
ratings that lecturers would give to the summary.
The reward for the lecturers? produced summaries
is 124.62 and for the ML method is 107.77. The
ML classification system performed worse than
this gold standard in terms of reward, which is ex-
pected given the error in predictions (supervised
methods learn to reproduce the gold standard).
Moreover, each decision is rewarded with a dif-
ferent value as some combinations of factors and
templates have greater or negative regression coef-
ficients. For instance, the combination of the fac-
tors ?deadlines? and the template that corresponds
to <weeks> is rewarded with 57. On the other
hand, when mentioning the <average> difficulty
the summary is ?punished? with -81 (see descrip-
tion of the reward function in Section 5.2). Conse-
quently, a single poor decision in the ML classifi-
cation can result in much less reward.
6.2 Subjective Results with Students
37 first year computer science students partici-
pated in the study. Each participant was shown
a graphical representation of the time-series data
of one student and four different summaries gen-
erated by the four systems (see Figure 1). The or-
der of the presented summaries was randomised.
They were asked to rate each feedback summary
on a 10-point rating scale in response to the fol-
lowing statement: ?Imagine you are the following
student. How would you evaluate the following
feedback summaries from 1 to 10??, where 10 cor-
responds to the most preferred summary and 1 to
the least preferred.
The difference in ratings between the ML clas-
sification system, the RL system and the Rule-
based system is not significant (see Mode (mean)
in Table 4, p>0.05). However, there is a trend to-
wards the RL system. The classification method
reduces the generation steps, by making the de-
cision of the factor selection and the template se-
lection jointly. Moreover, the training time for the
classification method is faster (a couple of seconds
compared to over an hour). Finally, the student
1237
Figure 1: The Figure show the evaluation setup. Students were presenting with the data in a graphical
way and then they were asked to evaluate each summary in a 10-point Rating scale. Summaries displayed
from left to right: ML system, RL, rule-based and random.
significantly prefer all the systems over the ran-
dom.
7 Summary
We have shown that ML classification for sum-
marisation of our time-series data has an accuracy
of 76.95% and that this approach significantly out-
performs other classification methods as it is able
to capture dependencies in the data when mak-
ing content selection decisions. ML classifica-
tion was also directly compared to a RL method.
It was found that although ML classification is
almost 20% more accurate than RL, both meth-
ods perform comparably when rated by humans.
This may be due to the fact that the RL optimi-
sation method is able to provide more varied re-
sponses over time rather than just emulating the
training data as with standard supervised learn-
ing approaches. Foster (2008) found similar re-
sults when performing a study on generation of
emphatic facial displays. A previous study by
Belz and Reiter (2006) has demonstrated that au-
tomatic metrics can correlate highly with human
ratings if the training dataset is of high quality.
In our study, the human ratings correlate well to
the average scores achieved by the reward func-
tion. However, the human ratings do not correlate
well to the accuracy scores. It is interesting that
the two methods that score differently on various
automatic metrics, such as accuracy, reward, pre-
cision, recall and F-score, are evaluated similarly
by users.
The comparison shows that each method can
serve different goals. Multi-label classification
generates output closer to gold standard whereas
RL can optimise the output according to a reward
function. ML classification could be used when
the goal of the generation is to replicate phenom-
ena seen in the dataset, because it achieves high
accuracy, precision and recall. However, opti-
misation methods can be more flexible, provide
more varied output and can be trained for different
goals, e.g. for capturing preferences of different
users.
1238
8 Future Work
For this initial experiment, we evaluated with stu-
dents and not with lecturers, since the students are
the recipients of feedback. In future, we plan to
evaluate with students? own data under real cir-
cumstances as well as with ratings from lecturers.
Moreover, we plan to utilise the results from this
student evaluation in order to train an optimisation
algorithm to perform summarisation according to
students? preferences. In this case, optimisation
would be the preferred method as it would not be
appropriate to collect gold standard data from stu-
dents. In fact, it would be of interest to investi-
gate multi-objective optimisation techniques that
can balance the needs of the lecturers to convey
important content to the satisfaction of students.
9 Acknowledgements
The research leading to this work has re-
ceived funding from the EC?s FP7 programme:
(FP7/2011-14) under grant agreement no. 248765
(Help4Mood).
References
Carole Ames. 1992. Classrooms: Goals, structures,
and student motivation. Journal of Educational Psy-
chology, 84(3):261?71.
Ion Androutsopoulos, Gerasimos Lampouras, and
Dimitrios Galanis. 2013. Generating natural lan-
guage descriptions from owl ontologies: the nat-
ural owl system. Atrificial Intelligence Research,
48:671?715.
Gabor Angeli, Percy Liang, and Dan Klein. 2010. A
simple domain-independent probabilistic approach
to generation. In Conference on Empirical Methods
in Natural Language Processing (EMNLP).
Regina Barzilay and Mirella Lapata. 2004. Collec-
tive content selection for concept-to-text generation.
In Conference on Human Language Technology and
Empirical Methods in Natural Language Processing
(HLT-EMNLP).
Regina Barzilay and Lillian Lee. 2004. Catching the
drift: Probabilistic content models, with applications
to generation and summarization. In Human Lan-
guage Technology Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics (HLT-NAACL).
Anja Belz and Eric Kow. 2010. Extracting parallel
fragments from comparable corpora for data-to-text
generation. In 6th International Natural Language
Generation Conference (INLG).
Anja Belz and Ehud Reiter. 2006. Comparing auto-
matic and human evaluation of nlg systems. In 11th
Conference of the European Chapter of the Associa-
tion for Computational Linguistics (ACL).
Rolf Black, Joe Reddington, Ehud Reiter, Nava
Tintarev, and Annalu Waller. 2010. Using NLG and
sensors to support personal narrative for children
with complex communication needs. In NAACL
HLT 2010 Workshop on Speech and Language Pro-
cessing for Assistive Technologies.
Scotty D. Craig, Arthur C. Graesser, Jeremiah Sullins,
and Barry Gholson. 2004. Affect and learning:
an exploratory look into the role of affect in learn-
ing with autotutor. Journal of Educational Media,
29:241?250.
Pable Duboue and K.R. McKeown. 2003. Statistical
acquisition of content selection rules for natural lan-
guage generation. In Conference on Human Lan-
guage Technology and Empirical Methods in Natu-
ral Language Processing (EMNLP).
Mary Ellen Foster. 2008. Automated metrics that
agree with human judgements on generated output
for an embodied conversational agent. In 5th Inter-
national Natural Language Generation Conference
(INLG).
Malte Gabsdil and Oliver Lemon. 2004. Combining
acoustic and pragmatic features to predict recogni-
tion performance in spoken dialogue systems. In
42nd Annual Meeting of the Association for Com-
putational Linguistics (ACL).
Albert Gatt, Francois Portet, Ehud Reiter, James
Hunter, Saad Mahamood, Wendy Moncur, and So-
mayajulu Sripada. 2009. From data to text in the
neonatal intensive care unit: Using NLG technology
for decision support and information management.
AI Communications, 22: 153-186.
Dimitra Gkatzia, Helen Hastie, Srinivasan Ja-
narthanam, and Oliver Lemon. 2013. Generating
student feedback from time-series data using Rein-
forcement Learning. In 14th European Workshop in
Natural Language Generation (ENLG).
Dimitra Gkatzia, Helen Hastie, and Oliver Lemon.
2014. Finding Middle Ground? Multi-objective
Natural Language Generation from time-series data.
In 14th Conference of the European Chapter of the
Association for Computational Linguistics (EACL)
(to appear).
Sebastian Gross, Bassam Mokbel, Barbara Hammer,
and Niels Pinkwart. 2012. Feedback provision
strategies in intelligent tutoring systems based on
clustered solution spaces. In J. Desel, J. M. Haake,
and C. Spannagel, editors, Tagungsband der 10. e-
Learning Fachtagung Informatik (DeLFI), number
P-207 in GI Lecture Notes in Informatics, pages 27?
38. GI.
1239
Jim Hunter, Yvonne Freer, Albert Gatt, Yaji Sripada,
Cindy Sykes, and D Westwater. 2011. Bt-nurse:
Computer generation of natural language shift sum-
maries from complex heterogeneous medical data.
American Medical Informatics Association, 18:621-
624.
Nicholas Johnson and David Lane. 2011. Narrative
monologue as a first step towards advanced mis-
sion debrief for AUV operator situational aware-
ness. In 15th International Conference on Advanced
Robotics.
Ravi Kondadadi, Blake Howald, and Frank Schilder.
2013. A statistical nlg framework for aggregated
planning and realization. In 51st Annual Meet-
ing of the Association for Computational Linguistics
(ACL).
Gerasimos Lampouras and Ion Androutsopoulos.
2013. Using integer linear programming in concept-
to-text generation to produce more compact texts. In
51st Annual Meeting of the Association for Compu-
tational Linguistics (ACL).
Anna S. Law, Yvonne Freer, Jim Hunter, Robert H.
Logie, Neil McIntosh, and John Quinn. 2005. A
comparison of graphical and textual presentations of
time series data to support medical decision making
in the neonatal intensive care unit. Journal of Clini-
cal Monitoring and Computing, pages 19: 183?194.
Gjorgji Madjarov, Dragi Kocev, Dejan Gjorgjevikj, and
Saso Dzeroski. 2012. An extensive experimen-
tal comparison of methods for multi-label learning.
Pattern Recognition, 45(9):3084?3104.
Natalie K. Person, Roger J. Kreuz, Rolf A. Zwaan, and
Arthur C. Graesser. 1995. Pragmatics and peda-
gogy: Conversational rules and politeness strategies
may inhibit effective tutoring. Journal of Cognition
and Instruction, 13(2):161-188.
Ehud Reiter and Robert Dale. 2000. Building natu-
ral language generation systems. Cambridge Uni-
versity Press.
Verena Rieser, Oliver Lemon, and Xingkun Liu. 2010.
Optimising information presentation for spoken dia-
logue systems. In 48th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL).
Somayajulu Sripada, Ehud Reiter, Jim Hunter, and Jin
Yu. 2003. Generating english summaries of time se-
ries data using the gricean maxims. In 9th ACM in-
ternational conference on Knowledge discovery and
data mining (SIGKDD).
Somayajulu Sripada, Ehud Reiter, I Davy, and
K Nilssen. 2004. Lessons from deploying NLG
technology for marine weather forecast text gener-
ation. In PAIS session of ECAI-2004:760-764.
Richart Sutton and Andrew Barto. 1998. Reinforce-
ment learning. MIT Press.
Grigorios Tsoumakas and Ioannis Katakis. 2007.
Multi-label classification: An overview. Inter-
national Journal Data Warehousing and Mining,
3(3):1?13.
Grigorios Tsoumakas, Ioannis Katakis, and Ioannis
Vlahavas. 2010. Random k-labelsets for multi-
label classification. IEEE Transactions on Knowl-
edge and Data Engineering, 99(1):1079?1089.
Grigorios Tsoumakas, Eleftherios Spyromitros-
Xioufis, Josef Vilcek, and Ioannis Vlahavas.
2011. Mulan: A java library for multi-label
learning. Journal of Machine Learning Research,
12(1):2411?2414.
Marian van den Meulen, Robert Logie, Yvonne Freer,
Cindy Sykes, Neil McIntosh, and Jim Hunter. 2010.
When a graph is poorer than 100 words: A com-
parison of computerised natural language genera-
tion, human generated descriptions and graphical
displays in neonatal intensive care. In Applied Cog-
nitive Psychology, 24: 77-89.
Ian Witten and Eibe Frank. 2005. Data mining: Practi-
cal machine learning tools and techniques. Morgan
Kaufmann Publishers.
Min-Ling Zhang and Zhi-Hua Zhou. 2007. Ml-knn: A
lazy learning approach to multi-label learning. Pat-
tern Recognition, 40(7):2038?2048.
1240
Proceedings of the 14th European Workshop on Natural Language Generation, pages 115?124,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
Generating student feedback from time-series data using Reinforcement
Learning
Dimitra Gkatzia, Helen Hastie, Srinivasan Janarthanam and Oliver Lemon
Department of Mathematical and Computer Sciences
Heriot-Watt University
Edinburgh, Scotland
{dg106, h.hastie, sc445, o.lemon} @hw.ac.uk
Abstract
We describe a statistical Natural Language
Generation (NLG) method for summarisa-
tion of time-series data in the context of
feedback generation for students. In this
paper, we initially present a method for
collecting time-series data from students
(e.g. marks, lectures attended) and use ex-
ample feedback from lecturers in a data-
driven approach to content selection. We
show a novel way of constructing a reward
function for our Reinforcement Learning
agent that is informed by the lecturers?
method of providing feedback. We eval-
uate our system with undergraduate stu-
dents by comparing it to three baseline
systems: a rule-based system, lecturer-
constructed summaries and a Brute Force
system. Our evaluation shows that the
feedback generated by our learning agent
is viewed by students to be as good as the
feedback from the lecturers. Our findings
suggest that the learning agent needs to
take into account both the student and lec-
turers? preferences.
1 Introduction
Data-to-text generation refers to the task of auto-
matically generating text from non-linguistic data
(Reiter and Dale, 2000). The goal of this work is
to develop a method for summarising time-series
data in order to provide continuous feedback to
students across the entire semester. As a case
study, we took a module in Artificial Intelligence
and asked students to fill out a very short diary-
type questionnaire on a weekly basis. Questions
included, for example, number of deadlines, num-
ber of classes attended, severity of personal issues.
These data were then combined with the marks
from the weekly lab reflecting the students? per-
formance. As data is gathered each week in the
lab, we now have a set of time-series data and our
goal is to automatically create feedback. The goal
is to present a holistic view through these diary en-
tries of how the student is doing and what factors
may be affecting performance.
Feedback is very important in the learning pro-
cess but very challenging for academic staff to
complete in a timely manner given the large num-
ber of students and the increasing pressures on
academics? time. This is where automatic feed-
back can play a part, providing a tool for teachers
that can give insight into factors that may not be
immediately obvious (Porayska-Pomsta and Mel-
lish, 2013). As reflected in NSS surveys1, stu-
dents are not completely satisfied with how feed-
back is currently delivered. The 2012 NSS survey,
for all disciplines reported an 83% satisfaction rate
with courses, with 70% satisfied with feedback.
This has improved from recent years (in 2006 this
was 60% for feedback) but shows that there is
still room for improvement in how teachers deliver
feedback and its content.
In the next section (Section 2) a discussion of
the related work is presented. In Section 3, a de-
scription of the methodology is given as well as
the process of the data collection from students,
the template construction and the data collection
with lecturers. In Section 4, the Reinforcement
Learning implementation is described. In Section
5, the evaluation results are presented, and finally,
in Sections 6 and 7, a conclusion and directions
for future work are discussed.
2 Related Work
Report generation from time-series data has been
researched widely and existing methods have been
used in several domains such as weather forecasts
(Belz and Kow, 2010; Angeli et al, 2010; Sripada
et al, 2004), clinical data summarisation (Hunter
1http://www.thestudentsurvey.com/
115
et al, 2011; Gatt et al, 2009), narrative to assist
children with communication needs (Black et al,
2010) and audiovisual debriefs from sensor data
from Autonomous Underwater Vehicles missions
(Johnson and Lane, 2011).
The two main challenges for time-series data
summarisation are what to say (Content Selec-
tion) and how to say it (Surface Realisation). In
this work we concentrate on the former. Previ-
ous methods for content selection include Gricean
Maxims (Sripada et al, 2003); collective con-
tent selection (Barzilay and Lapata, 2004); and
the Hidden Markov model approach for content
selection and ordering (Barzilay and Lee, 2004).
NLG systems tend to be very domain-specific
and data-driven systems that seek to simultane-
ously optimize both content selection and sur-
face realisation have the potential to be more
domain-independent, automatically optimized and
lend themselves to automatic generalization (An-
geli et al, 2010; Rieser et al, 2010; Dethlefs
and Cuayahuitl, 2011). Recent work on report
generation uses statistical techniques from Ma-
chine Translation (Belz and Kow, 2010), super-
vised learning (Angeli et al, 2010) and unsuper-
vised learning (Konstas and Lapata, 2012).
Here we apply Reinforcement Learning meth-
ods (see Section 4 for motivation) which have been
successfully applied to other NLG tasks, such as
Temporal Expressions Generation (Janarthanam
et al, 2011), Lexical Choice (Janarthanam and
Lemon, 2010), generation of adaptive restaurant
summaries in the context of a dialogue system
(Rieser et al, 2010) and generating instructions
(Dethlefs and Cuayahuitl, 2011).
3 Methodology
Figure 1: Methodology for data-driven feedback
report generation
Figure 1 shows graphically our approach to the de-
velopment of a generation system. Firstly, we col-
lected data from students including marks, demo-
graphic details and weekly study habits. Next, we
created templates for surface realisation with the
help of a Teaching and Learning expert. These
templates were used to generate summaries that
were rated by lecturers. We used these ratings to
train the learning agent. The output of the learning
agent (i.e. automatically generated feedback re-
ports) were finally evaluated by the students. Each
of these steps are discussed in turn.
3.1 Time-series Data Collection from
Students
The data were collected during the weekly lab ses-
sions of a Computer Science module which was
taught to third year Honours and MSc students
over the course of a 10 week semester. We re-
cruited 26 students who were asked to fill in a
web-based diary-like questionnaire. Initially, we
asked students to provide some demographic de-
tails (age, nationality, level of study). In addition,
students provided on a weekly basis, information
for nine factors that could influence their perfor-
mance. These nine factors were motivated from
the literature and are listed here in terms of effort
(Ames, 1992), frustration (Craig et al, 2004) , dif-
ficulty (Person et al, 1995; Fox, 1993) and per-
formance (Chi et al, 2001). Effort is measured
by three factors: (1) how many hours they studied;
(2) the level of revision they have done; (3) as well
as the number of lectures (of this module) they at-
tended. Frustration is measured by (4) the level
of understandability of the content; (5) whether
they have had other deadlines; and whether they
faced any (6) health and/or (7) personal issues and
at what severity. The difficulty of the lab exercises
is measured by (8) the students? perception of dif-
ficulty. Finally, (9) marks achieved by the students
in each weekly lab was used as a measure of their
performance.
3.2 Data Trends
Initially, the data were processed so as to iden-
tify the existing trend of each factor during the
semester, (e.g. number of lectures attending de-
creases). The tendencies of the data are estimated
using linear least-squares regression, with each
factor annotated as INCREASING, DECREAS-
ING or STABLE. In addition, for each student we
perform a comparison between the average of each
116
Type Description Examples
AVERAGE describes the factor data by either averaging the values given by
the student,
?You spent 2 hours studying the lecture material
on average?. (HOURS STUDIED)
or by comparing the student?s average with the class average
(e.g. if above the mean value for the class, we say that the ma-
terial is challenging).
?You found the lab exercises very challenging?.
(DIFFICULTY)
TREND discusses the trend of the data, e.g. increasing, decreasing or
stable.
?Your workload is increasing over the
semester?. (DEADLINES)
WEEKS talks about specific events that happened in one or more weeks. ?You have had other deadlines during weeks 5,
6 and 9?. (DEADLINES)
OTHER all other expressions that are not directly related to data. ?Revising material during the semester will im-
prove your performance?. (REVISION)
Table 1: The table explains the different template types.
factor and the class average of the same factor.
3.3 Template Generation
The wording and phrasing used in the templates to
describe the data were derived from working with
and following the advice of a Learning and Teach-
ing (L&T) expert. The expert provided consulta-
tion on how to summarise the data. We derived 4
different kinds of templates for each factor: AV-
ERAGE, TREND, WEEKS and OTHER based on
time-series data on plotted graphs. A description
of the template types is shown in Table 1.
In addition, the L&T expert consulted on how
to enhance the templates so that they are ap-
propriate for communicating feedback accord-
ing to the guidelines of the Higher Education
Academy (2009), for instance, by including moti-
vating phrases such as ?You may want to plan your
study and work ahead?.
3.4 Data Collection from Lecturers
The goal of the Reinforcement Learning agent is
to learn to generate feedback at least as well as
lecturers. In order to achieve this, a second data
collection was conducted with 12 lecturers partic-
ipating.
The data collection consisted of three stages
where lecturers were given plotted factor graphs
and were asked to:
1. write a free style text summary for 3 students
(Figure 2);
2. construct feedback summaries using the tem-
plates for 3 students (Figure 3);
3. rate random feedback summaries for 2 stu-
dents (Figure 4).
We developed the experiment using the Google
Web Toolkit for Web Applications, which facil-
itates the development of client-server applica-
tions. The server side hosts the designed tasks and
stores the results in a datastore. The client side is
responsible for displaying the tasks on the user?s
browser.
In Task 1, the lecturers were presented with the
factor graphs of a student (one graph per factor)
and were asked to provide a free-text feedback
summary for this student. The lecturers were en-
couraged to pick as many factors as they wanted
and to discuss the factors in any order they found
useful. Figure 2 shows an example free text sum-
mary for a high performing student where the lec-
turer decided to talk about lab marks and under-
standability. Each lecturer was asked to repeat this
task 3 times for 3 randomly picked students.
In Task 2, the lecturers were again asked to con-
struct a feedback summary but this time they were
given a range of sentences generated from the tem-
plates (as described in Section 2.3). They were
asked to use these to construct a feedback report.
The number of alternative utterances generated for
each factor varies depending on the factor and the
given data. In some cases, a factor can have 2 gen-
erated utterances and in other cases up to 5 (with
a mean of 3 for each factor) and they differenti-
ate in the style of trend description and wording.
Again the lecturer was free to choose which fac-
tors to talk about and in which order, as well as
to decide on the template style he/she prefers for
the realisation through the template options. Fig-
ure 3 shows an example of template selection for
the same student as in Figure 2.
In Task 3, the lecturers were presented with the
plotted factor graphs plus a corresponding feed-
back summary that was generated by randomly
choosing n factors and their templates, and were
asked to rate it in a scale between 0-100 (100 for
the best summary). Figure 4 shows an example of
117
Figure 2: The interface of the 1st task of the data collection: the lecturer consults the factor graphs and
provides feedback in a free text format.
Figure 3: The interface of the 2nd task of data collection: the lecturer consults the graphs and constructs
a feedback summary from the given templates (this graph refers to the same student as Figure 2).
a randomly generated summary for the same stu-
dent as in Figure 2.
4 Learning a Time-Series Generation
Policy
Reinforcement Learning (RL) is a machine learn-
ing technique that defines how an agent learns to
take optimal actions in a dynamic environment so
as to maximize a cumulative reward (Sutton and
Barto, 1998). In our framework, the task of con-
tent selection of time-series data is presented as a
Markov Decision problem. The goal of the agent
is to learn to choose a sequence of actions that
obtain the maximum expected reward in the long
run. In this section, we describe the Reinforce-
ment Learning setup for learning content selection
118
Figure 4: The interface of the 3rd task of data col-
lection: the lecturer consults the graphs and rates
the randomly generated feedback summary (this
graph refers to the same student as Figures 2 and
3).
from time-series data for feedback report gener-
ation. Summarisation from time-series data is an
open challenge and we aim to research other meth-
ods in the future, such as supervised learning, evo-
lutionary algorithms etc.
4.1 Actions and States
In this learning setup, we focused only on select-
ing the correct content, i.e. which factors to talk
about. The agent selects a factor and then decides
whether to talk about it or not. The state consists
of a description of the factor trends and the num-
ber of templates that have been selected so far. An
example of the initial state of a student can be:
<marks increased, lectures attended stable,
hours studied increased, understandability stable,
difficulty increased, health issues stable, per-
sonal issues stable, revision increased, 0>
The agent explores the state space by selecting a
factor and then by deciding whether to talk about
it or not. If the agent decides to talk about the
selected factor, it chooses the template in a greedy
way, i.e. it chooses for each factor the template
that results in a higher reward. After an action has
been selected, it is deleted from the action space.
4.1.1 Ordering
In order to find out in which order the lectur-
ers describe the factors, we transformed the feed-
back summaries into n-grams of factors. For in-
stance, a summary that talks about the student?s
performance, the number of lectures that he/she
attended, potential health problems and revision
done can be translated into the following ngram:
start, marks, lectures attended, health issues, re-
vision, end. We used the constructed n-grams to
compute the bigram frequency of the tokens in or-
der to identify which factor is most probable to be
referred to initially, which factors follow particu-
lar factors and which factor is usually talked about
in the end. It was found that the most frequent or-
dering is: start, marks, hours studied, understand-
ability, difficulty, deadlines, health issues, per-
sonal issues, lectures attended, revision, end.
4.2 Reward Function
The goal of the reward function is to optimise the
way lecturers generate and rate feedback. Given
the expert annotated summaries from Task 1, the
constructed summaries from Task 2 and the ratings
from Task 3, we derived the multivariate reward
function:
Reward = a +
n?
i=1
bi ? xi + c ? length
where X = {x1, x2, ..., xn} represents the
combinations between the data trends observed in
the time-series data and the corresponding lectur-
ers? feedback (i.e. whether they included a factor
to be realised or not and how). The value xi for
factor i is defined by the function:
xi =
?
?????
?????
1, the combination i of a factor trend
and a template type is included in
the feedback
0, if not.
For instance, the value of x1 is 1 if marks were
increased and this trend is realised in the feedback,
otherwise it is 0. In our domain n = 90 in order to
cover all the different combinations. The length
stands for the number of factors selected, a is the
intercept, bi and c are the coefficients for xi and
length respectively.
In order to model the reward function, we used
linear regression to compute the weights from the
data gathered from the lecturers. Therefore, the
reward function is fully informed by the data pro-
vided by the experts. Indeed, the intercept a, the
vector weights b and the weight c are learnt by
making use of the data collected by the lecturers
from the 3 tasks discussed in Section 3.4.
The reward function is maximized (Reward
= 861.85) for the scenario (i.e. each student?s
data), content selection and preferred template
style shown in Table 2 (please note that this sce-
nario was not observed in the data collection).
119
Factor Trend Template
difficulty stable NOT MENTIONED
hours studied stable TREND
understandability stable NOT MENTIONED
deadlines increase WEEKS
health issues stable WEEKS
personal issues stable WEEKS
lectures att. stable WEEKS
revision stable OTHER
marks increase TREND
Table 2: The table shows the scenario at which the
reward function is maximised.
The reward function is minimized (Reward =
-586.0359) for the scenario shown in Table 3
(please note that this scenario also was not ob-
served in the data collection).
Factor Trend Template
difficulty increase AVERAGE
hours studied stable NOT MENTIONED
understandability decrease AVERAGE
deadlines * *
health issues increase TREND
personal issues stable TREND
lectures att. stable NOT MENTIONED
revision stable AVERAGE
marks stable TREND
Table 3: The table shows the scenario at which the
reward function is minimised (* denotes multiple
options result in the same minimum reward).
4.3 Training
We trained a time-series generation policy
for 10,000 runs using the Tabular Temporal-
Difference Learning (Sutton and Barto, 1998).
During the training phase, the learning agent gen-
erated feedback summaries. When the construc-
tion of the summary begins, the length of the sum-
mary is 0. Each time that the agent adds a template
(by selecting a factor), the length is incremented,
thus changing the state. It repeats the process until
it decides for all factors whether to talk about them
or not. The agent is finally rewarded at the end of
the process using the Reward function described
in Section 3.2. Initially, the learning agent selects
factors randomly, but gradually learns to identify
factors that are highly rewarding for a given data
scenario. Figure 5 shows the learning curve of the
agent.
Figure 5: Learning curve for the learning agent.
The x-axis shows the number of summaries pro-
duced and y- axis the total reward received for
each summary.
5 Evaluation
We evaluated the system using the reward func-
tion and with students. In both these evaluations,
we compared feedback reports generated using
our Reinforcement Learning agent with four other
baseline systems. Here we present a brief descrip-
tion of the baseline systems.
Baseline 1: Rule-based system. This system
selects factors and templates for generation using a
set of rules. These hand-crafted rules were derived
from a combination of the L&T expert?s advice
and a student?s preferences and is therefore a chal-
lenging baseline and represents a middle ground
between the L&T expert?s advice and a student?s
preferences. An example rule is: if the mark aver-
age is less than 50% then refer to revision.
Baseline 2: Brute Force system. This system
performs a search of the state space, by exploring
randomly as many different feedback summaries
as possible. The Brute Force algorithm is shown
below:
Algorithm 1 Brute Force algorithm
I n p u t d a t a : D
f o r n = 0 . . . 1 0 , 0 0 0
c o n s t r u c t randomly f e e d b a c k [ n ]
a s s i g n getReward [ n ]
i f ge tReward [ n]>getReward [ n?1]
b e s t F e e d b a c k = f e e d b a c k [ n ]
e l s e
b e s t F e e d b a c k = f e e d b a c k [ n?1]
r e t u r n b e s t F e e d b a c k
In each run the algorithm constructs a feedback
summary, then it calculates its reward, using the
same reward function used for the Reinforcement
Learning approach, and if the reward of the new
feedback is better than the previous, it keeps the
120
new one as the best. It repeats this process for
10,000 times for each scenario. Finally, the algo-
rithm returns the summary that scored the highest
ranking.
Baseline 3: Lecturer-produced summaries.
These are the summaries produced by the lectur-
ers, as described in Section 2.4, for Task 2 using
template-generated utterances.
Baseline 4: Random system: The Random
system constructs feedback summaries by select-
ing factors and templates randomly as described in
Task 3 (in Section 3.4).
5.1 Evaluation with Reward Function
Table 4 presents the results of the evaluation per-
formed using the Reward Function, comparing
the learned policy with the four baseline systems.
Each system generated 26 feedback summaries.
On average the learned policy scores significantly
higher than any other baseline for the given sce-
narios (p <0.05 in a paired t-test).
Time-Series Summarisation Systems Reward
Learned 243.82
Baseline 1: Rule-based 107.77
Baseline 2: Brute Force 241.98
Baseline 3: Lecturers 124.62
Baseline 4: Random 43.29
Table 4: The table summarises the average re-
wards that are assigned to summaries produced
from the different systems.
5.2 Evaluation with Students
A subjective evaluation was conducted using 1st
year students of Computer Science as participants.
We recruited 17 students, who were all English na-
tive speakers. The participants were shown 4 feed-
back summaries in a random order, one generated
by the learned policy, one from the rule-based sys-
tem (Baseline 1), one from the Brute Force system
(Baseline 2) and one summary produced by a lec-
turer using the templates (Baseline 3). Given the
poor performance of the Random system in terms
of reward, Baseline 4 was omitted from this study.
Overall there were 26 different scenarios, as de-
scribed in Section 3.1. All summaries presented
to a participant were generated from the same sce-
nario. The participants then had to rank the sum-
maries in order of preference: 1 for the most pre-
ferred and 4 for the least preferred. Each partici-
pant repeated the process for 4.5 scenarios on aver-
age (the participant was allowed to opt out at any
stage). The mode values of the rankings of the
preferences of the students are shown in Table 5.
The web-based system used for the evaluation is
shown in Figure 6.
System Mode of Rankings
Learned 3rd
Baseline 3: Lecturers 3rd
Baseline 1: Rule-based 1st
Baseline 2: Brute Force 4th
Table 5: The table shows the mode value of the
rankings of the preference of the students.
We ran a Mann-Whitney?s U test to evaluate the
difference in the responses of our 4-point Likert
Scale question between the Learned system and
the other three baselines. It was found that, for
the given data, the preference of students for the
feedback generated by the Learned system is as
good as the feedback produced by the experts, i.e.
there is no significant difference between the mean
value of the rankings of the Learned system and
the lecturer-produced summaries (p = 0.8) (Base-
line 3).
The preference of the users for the Brute Force
system does not differ significantly from the sum-
maries generated by the Learned system (p =
0.1335). However, the computational cost of the
Brute Force is higher because each time that the
algorithm sees a new scenario it has to run ap-
proximately 3k times to reach a good summary (as
seen in Figure 7) and about 10k to reach an optimal
one, which corresponds to 46 seconds. This delay
would prohibit the use of such a system in time-
critical situations (such as defence) and in live sys-
tems such as tutoring systems. In addition, the
processing time would increase with more compli-
cated scenarios and if we want to take into account
the ordering of the content selection and/or if we
have more variables. In contrast, the RL method
needs only to be trained once.
Finally, the users significantly preferred the
summaries produced by the Rule-based system
(Baseline 1) to the summaries produced by the
Learned system. This is maybe because of the fact
that in the rule-based system some knowledge of
the end user?s preferences (i.e. students) was taken
into account in the rules which was not the case
in the other three systems. This fact suggests that
121
Figure 6: The interface for the evaluation: the students viewed the four feedback summaries and ranked
them in order of preference. From left to right, the summaries as generated by: an Expert (Baseline 3),
the Rule based system (Baseline 1), the Brute Force algorithm (Baseline 2), the Learned system.
Figure 7: The graphs shows the number of cycles
that the Brute Force algorithm needs to achieve
specific rewards.
students? preferences should be taken into account
as they are the receivers of the feedback. This can
also be generalised to other areas, where the ex-
perts and the end users are not the same group
of people. As the learned policy was not trained
to optimise for the evaluation criteria, in future,
we will explore reward functions that bear in mind
both the expert knowledge and the student?s pref-
erences.
6 Conclusion
We have presented a statistical learning approach
to summarisation from time-series data in the area
of feedback reports. In our reports, we took into
account the principles of good feedback provision
as instructed by the Higher Education Academy.
We also presented a method for data gathering
from students and lecturers and show how we can
use these data to generate feedback by presenting
the problem as a Markov Decision Process and
optimising it using Reinforcement Learning tech-
niques. We also showed a way of constructing a
data-driven reward function that can capture de-
pendencies between the time-series data and the
realisation phrases, in a similar way that the lec-
turers do when providing feedback. Finally, our
evaluation showed that the learned report genera-
tion policy generates reports as well as lecturers.
7 Future Work
We aim to conduct further qualitative research in
order to explore what factors and templates stu-
dents find useful to be included in the feedback
and inform our reward function with this informa-
tion as well as what we have observed in the lec-
turer data collection. This way, we hope, not only
to gain insights into what is important to students
and lecturers but also to develop a data-driven ap-
proach that, unlike the rule-based system, does not
require expensive and difficult-to-obtain expert in-
put from Learning and Teaching experts. In ad-
dition, we want to compare RL techniques with
supervised learning approaches and evolutionary
algorithms. Finally, we want to unify content se-
122
lection and surface realisation, therefore we will
extend the action space in order to include actions
for template selection.
8 Acknowledgements
The research leading to this work has re-
ceived funding from the EC?s FP7 programme:
(FP7/2011-14) under grant agreement no. 248765
(Help4Mood).
References
Carole Ames. 1992. Classrooms: Goals, Structures,
and Student Motivation. Journal of Educational Psy-
chology, 84(3):p261-71.
Gabor Angeli, Percy Liang and Dan Klein. 2010. A
simple domain-independent probabilistic approach
to generation. EMNLP ?10: Proceedings of the 2010
Conference on Empirical Methods in Natural Lan-
guage Processing.
Regina Barzilay and Mirella Lapata. 2004. Collec-
tive content selection for concept-to-text generation.
HLT ?05: Proceedings of the conference on Hu-
man Language Technology and Empirical Methods
in Natural Language Processing.
Regina Barzilay and Lillian Lee. 2004. Catching the
drift: Probabilistic content models, with applications
to generation and summarization. HLT-NAACL
2004: Proceedings of the Human Language Tech-
nology Conference of the North American Chapter
of the Association for Computational Linguistics.
Anja Belz and Eric Kow. 2010. Extracting parallel
fragments from comparable corpora for data-to-text
generation. INLG ?10: Proceedings of the 6th Inter-
national Natural Language Generation Conference.
Rolf Black, Joe Reddington, Ehud Reiter, Nava
Tintarev, and Annalu Waller. 2010. Using NLG and
Sensors to Support Personal Narrative for Children
with Complex Communication Needs. SLPAT ?10:
Proceedings of the NAACL HLT 2010 Workshop on
Speech and Language Processing for Assistive Tech-
nologies.
Michelene T.H. Chi, Stephanie A. Siler, Heisawn
Jeong, Takashi Yamauchi, Robert G. Hausmann.
2001. Learning from human tutoring. Journal of
Cognitive Science, 25(4):471-533.
Scotty D. Craig, Arthur C. Graesser, Jeremiah Sullins,
Barry Gholson. 2004. Affect and learning: an ex-
ploratory look into the role of affect in learning with
AutoTutor. Journal of Educational Media, 29:241-
250.
Nina Dethlefs and Heriberto Cuayahuitl. 2011.
Combining hierarchical reinforcement learning and
bayesian networks for natural language generation
in situated dialogue. ENLG ?11: Proceedings of the
13th European Workshop on Natural Language Gen-
eration.
Barbara Fox. 1993. The Human Tutorial Dialogue
Project: Issues in the Design of Instructional Sys-
tems. Lawrence Erlbaum Associates, Hillsdale,
New Jersey.
Albert Gatt, Francois Portet, Ehud Reiter, James
Hunter, Saad Mahamood,Wendy Moncur, and So-
mayajulu Sripada. 2009. From Data to Text in the
Neonatal Intensive Care Unit: Using NLG Technol-
ogy for Decision Support and Information Manage-
ment. Journal of AI Communications, 22:153-186.
Higher Education Academy. 2009. Providing individ-
ual written feedback on formative and summative
assessments. http://www.heacademy.
ac.uk/assets/documents/resources/
database/id353_senlef_guide.pdf.
Last modified September 16.
Jim Hunter, Yvonne Freer, Albert Gatt, Yaji Sripada,
Cindy Sykes, and D Westwater. 2011. BT-Nurse:
Computer Generation of Natural Language Shift
Summaries from Complex Heterogeneous Medical
Data. Journal of the American Medical Informatics
Association,18:621-624.
Srinivasan Janarthenam, Helen Hastie, Oliver Lemon,
Xingkun Liu. 2011. ?The day after the day after to-
morrow?? A machine learning approach to adaptive
temporal expression generation: training and evalu-
ation with real users. SIGDIAL ?11: Proceedings of
the SIGDIAL 2011 Conference.
Srinivasan Janarthanam and Oliver Lemon. 2010.
Adaptive Referring Expression Generation in Spo-
ken Dialogue Systems: Evaluation with Real Users.
SIGDIAL ?10: Proceedings of the 11th Annual
Meeting of the Special Interest Group on Discourse
and Dialogue.
Nicholas A. R. Johnson and David M. Lane. 2011.
Narrative Monologue as a First Step Towards Ad-
vanced Mission Debrief for AUV Operator Situa-
tional Awareness. In the 15th International Confer-
ence on Advanced Robotics.
Ioannis Konstas and Mirella Lapata. 2012. Unsuper-
vised concept-to-text generation with hypergraphs.
NAACL HLT ?12: Proceedings of the 2012 Confer-
ence of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies.
Natalie K. Person, Roger J. Kreuz, Rolf A. Zwaan and
Arthur C. Graesser. 1995. Pragmatics and Peda-
gogy: Conversational Rules and Politeness Strate-
gies May Inhibit Effective Tutoring. Journal of Cog-
nition and Instruction, 13(2):161-188.
Kaska Porayska-Pomsta and Chris Mellish. 2013.
Modelling human tutors? feedback to inform natural
language interfaces for learning. International Jour-
nal of Human-Computer Studies,71(6):703724.
123
Ehud Reiter and Robert Dale. 2000. Building Natural
Language Generation systems. Cambridge Univer-
sity Press.
Verena Rieser, Oliver Lemon, and Xingkun Liu. 2010.
Optimising Information Presentation for Spoken Di-
alogue Systems. ACL ?10: Proceedings of the 48th
Annual Meeting of the Association for Computa-
tional Linguistics.
Somayajulu Sripada, Ehud Reiter, I Davy, and K
Nilssen. 2004. Lessons from Deploying NLG Tech-
nology for Marine Weather Forecast Text Gener-
ation. In Proceedings of PAIS session of ECAI-
2004:760-764.
Somayajulu Sripada, Ehud Reiter, Jim Hunter, and Jin
Yu. 2003. Generating English Summaries of Time
Series Data using the Gricean Maxims. KDD ?03:
Proceedings of the ninth ACM SIGKDD interna-
tional conference on Knowledge discovery and data
mining.
Richart Sutton and Andrew Barto. 1998. Reinforce-
ment Learning. MIT Press.
124
Proceedings of the 8th International Natural Language Generation Conference, pages 138?142,
Philadelphia, Pennsylvania, 19-21 June 2014. c?2014 Association for Computational Linguistics
Multi-adaptive Natural Language Generation using Principal Component
Regression
Dimitra Gkatzia, Helen Hastie, and Oliver Lemon
School of Mathematical and Computer Sciences, Heriot-Watt University, Edinburgh
{dg106, h.hastie, o.lemon}@hw.ac.uk
Abstract
We present FeedbackGen, a system that
uses a multi-adaptive approach to Natu-
ral Language Generation. With the term
?multi-adaptive?, we refer to a system
that is able to adapt its content to dif-
ferent user groups simultaneously, in our
case adapting to both lecturers and stu-
dents. We present a novel approach to
student feedback generation, which simul-
taneously takes into account the prefer-
ences of lecturers and students when de-
termining the content to be conveyed in
a feedback summary. In this framework,
we utilise knowledge derived from rat-
ings on feedback summaries by extract-
ing the most relevant features using Prin-
cipal Component Regression (PCR) anal-
ysis. We then model a reward function
that is used for training a Reinforcement
Learning agent. Our results with stu-
dents suggest that, from the students? per-
spective, such an approach can generate
more preferable summaries than a purely
lecturer-adapted approach.
1 Introduction
Summarisation of time-series data refers to the
task of automatically generating reports from at-
tributes whose values change over time. Content
selection is the task of choosing what to say, i.e.
what information is to be included in a report (Re-
iter and Dale, 2000). We consider the task of auto-
matically generating feedback summaries for stu-
dents describing their performance during the lab
of a computer science module over the semester.
Various factors can influence students? learn-
ing such as difficulty of the material (Person et
al., 1995), workload (Craig et al., 2004), atten-
dance in lectures (Ames, 1992), etc. These fac-
tors change over time and can be interdependent.
In addition, different stakeholders often have con-
flicting goals, needs and preferences, for example
managers with employees, or doctors with patients
and relatives, or novice and expert users. In our
data, for instance, lecturers tend to comment on
the hours that the student studied, whereas the stu-
dents disprefer this content. In our previous work,
we showed that lecturers and students have dif-
ferent perceptions regarding what constitutes good
feedback (Gkatzia et al., 2013). Here, we present a
novel approach to generation by adapting its con-
tent to two user groups simultaneously. Producing
the same summary for two groups is important as
it allows for shared context and meaningful further
discussion and reduces development time.
2 Related Work
Previous work on NLG systems that address more
than one user group employs different versions of
a system for each different user group (Gatt et al.,
2009; Hunter et al., 2011; Mahamood and Reiter,
2011), makes use of User Models (Janarthanam
and Lemon, 2010; Thompson et al., 2004; Zuker-
man and Litman, 2001) or personalises the output
to individual users using rules (Reiter et al., 1999).
Our proposed system adapts the output to the pref-
erences of more than one user type1, lecturers and
students, but instead of developing many different
systems or using User Models that describe differ-
ent users, it attempts to model the middle ground
between the preferences.
In order to identify the users? preferences, we
apply Principal Components Regression (PCR
(Jolliffe, 1982)) analysis to two datasets that con-
tain lecturers? and students? ratings and identify
the most important variables from the principal
components, which are then included in a reward
function. This hand-crafted reward function is
used for training an RL agent for summarisation
1Our approach is different to multi-objective optimisa-
tion.
138
Raw Data
factors week 2 week 3 ... week 10
marks 5 4 ... 5
hours studied 1 2 ... 3
... ... ... ... ...
Trends from Data
factors trend
(1) marks trend other
(2) hours studied trend increasing
(3) understandability trend decreasing
(4) difficulty trend decreasing
(5) deadlines trend increasing
(6) health issues trend other
(7) personal issues trend decreasing
(8) lectures attended trend other
(9) revision trend decreasing
Summary
Your overall performance was excellent
during the semester. Keep up the good
work and maybe try some more challeng-
ing exercises. Your attendance was vary-
ing over the semester. Have a think about
how to use time in lectures to improve your
understanding of the material. You spent 2
hours studying the lecture material on
average. You should dedicate more time
to study. You seem to find the material
easier to understand compared to the
beginning of the semester. Keep up the
good work! You revised part of the learn-
ing material. Have a think whether revis-
ing has improved your performance.
Table 1: The table on the top left shows an example of the time-series data. The table on the bottom left
shows an example of described trends. The box on the right presents a target summary.
of time-series data. Our previous work showed
that when comparing RL and supervised learning
in the context of student feedback generation, stu-
dents preferred the output generated by the RL
system (Gkatzia et al., 2014a). Therefore, here, we
used RL rather than a supervised learning method.
The work described here builds on work reported
in (Gkatzia et al., 2014b), which uses as a reward
function the average of the Lecturer-adapted and
Student-adapted reward functions. However, that
method seems to cancel out the preferences of the
two groups whereas PCR is able to identify rele-
vant content for both groups.
In the next section, we describe the data used,
and the methodology for the multi-adaptive NLG,
as well as two alternative systems. In Section 4,
we describe the comparison of these three systems
in a subjective evaluation and present the results in
Section 5. A discussion follows in Section 6 and
finally, future work is discussed in Section 7.
3 Methodology
Reinforcement Learning is a machine learning
technique that defines how an agent learns to take
optimal sequences of actions so as to maximize a
cumulative reward (Sutton and Barto, 1998). In
our framework, the task of summarisation of time-
series data is modelled as a Markov Decision Pro-
cess, where the decisions on content selection cor-
respond to a sequence of actions (see Section 3.2).
Temporal Difference (TD) learning (Sutton and
Barto, 1990) is used for training three agents in
a simulated environment to learn to make optimal
content selection decisions:
1. by adapting to both groups simultaneously,
2. by adapting to lecturers,
3. by adapting to students.
3.1 The Data
For this study, the dataset described in (Gkatzia et
al., 2013) was used. Table 1 presents an exam-
ple of this dataset that describes a student?s learn-
ing factors and an aligned feedback summary pro-
vided by a lecturer. The dataset is composed of
37 similar instances. Each instance consists of
time-series information about the student?s learn-
ing routine and the selected templates that lec-
turers used to provide feedback to this particu-
lar student. A template is a quadruple consist-
ing of an id, a factor (bottom left of Ta-
ble 1), a reference type (trend, week, aver-
age, other) and surface text. For instance,
a template can be (1, marks, trend, ?Your marks
were <trend>over the semester?). The lexical
choice for <trend>(i.e. increasing or decreasing)
depends on the values of time-series data. There
is a direct mapping between the values of factor
139
and reference type and the surface text. The time-
series factors are listed in Table 1.
3.2 Actions and states
The state consists of the time-series data and the
number of factors which have so far been selected
to be talked about (the change of the value of this
variable consequently introduces a state change).
In order to explore the state space the agent se-
lects a time-series factor (e.g. marks, deadlines
etc.) and then decides whether to talk about it or
not, until all factors have been considered.
3.3 Reward function
The reward function is the following cumulative
multivariate function:
Reward = a+
n?
i=1
bi ? xi + c ? length
where X = {x1, x2, ..., xn} describes the cho-
sen combinations of the factor trends observed in
the time-series data and a particular template (i.e.
the way of mentioning a factor). a, b and c are the
correlation coefficients and length describes the
number of factors selected to be conveyed in the
feedback summary. The value of xi is given by
the function:
xi =
?
?
?
1, the combination of a factor trend
and a template type is included
0, if not.
The coefficients represent the level of preference
for a factor to be selected and the way it is con-
veyed in the summary. In the training phase, the
agent selects a factor and then decides whether to
talk about it or not. If the agent decides to refer
to a factor, the selection of the template is then
performed in a deterministic way, i.e. it selects the
template that results in higher reward.
Each rated summary is transformed into a vec-
tor of 91 binary features. Each feature describes
both (1) the trend of a factor (e.g. marks increas-
ing, see also Table 1) and (2) the way that this
factor could be conveyed in the summary (e.g.
one possible way is referring to average, another
possible way is referring to increasing/decreasing
trend). If both conditions are met, the value of
the feature is 1, otherwise 0. The 91 binary fea-
tures describe all the different possible combina-
tions. For both the Lecturer-adapted and Student-
adapted systems, the reward function is derived
from a linear regression analysis of the provided
dataset, similarly to Walker et al. (1997) and
Rieser et al. (2010).
3.3.1 Multi-adaptive Reward Function
In order to derive a reward function that finds a
balance between the two above mentioned sys-
tems, we use PCR to reduce the dimensionality
of the data and thus reduce the introduced noise.
Through PCR we are able to reduce the number
of features and identify components of factors that
are deemed important to both parties to be used in
the reward function.
PCR is a method that combines Principal Com-
ponent Analysis (PCA) (Jolliffe, 1986) with lin-
ear regression. PCA is a technique for reducing
the dataset dimensionality while keeping as much
of the variance as possible. In PCR, PCA is ini-
tially performed to identify the principal compo-
nents, in our case, the factors that contribute the
most to the variance. Then, regression is applied
to these principal components to obtain a vector
of estimated coefficients. Finally, this vector is
transformed back into the general linear regres-
sion equation. After performing this analysis on
both datasets (students and lecturers), we choose
the most important (i.e. the ones that contribute
the most to the variance) commoncomponents or
features resulting in 18 features which were used
in the reward function. We then design a hand-
crafted reward function taking into account this
PCR analysis. The five most important features
are shown in Table 2.
factor trend way it is mentioned
(1) marks stable average
(2) hours studied decreasing trend
(3) health issues decreasing weeks
(4) lectures attended stable average
(5) personal issues increasing trend
Table 2: The top 5 features out of the 18 selected
through PCR analysis.
4 Evaluation
FeedbackGen is evaluated with real users against
two alternative systems: one that adapts to lectur-
ers? preferences and one that adapts to students?
preferences. The output of the three systems is
ranked by 30 computer science students from a va-
riety of years of study. Time-series data of three
students are presented on graphs to each partici-
pant, along with three feedback summaries (each
one generated by a different system), in random
order, and they are asked to rank them in terms of
preference.
140
Student-adapted {Ranking: 1st*} FeedbackGen {Ranking: 2nd*} Lecturer-adapted {Ranking: 3rd*}
You did well at weeks 2, 3, 6, 8, 9 and 10,
but not at weeks 4, 5 and 7. Have a think
about how you were working well and
try to apply it to the other labs. Your at-
tendance was varying over the semester.
Have a think about how to use time in lec-
tures to improve your understanding of
the material. You found the lab exercises
not very challenging. You could try out
some more advanced material and exer-
cises. You dedicated more time study-
ing the lecture material in the beginning
of the semester compared to the end of
the semester. Have a think about what
is preventing you from studying. Revis-
ingmaterial during the semester will im-
prove your performance in the lab.
Your overall performance was
very good during the semester.
Keep up the good work and maybe
try some more challenging exer-
cises. You found the lab exer-
cises not very challenging. You
could try out some more advanced
material and exercises. You dedi-
cated more time studying the lec-
ture material in the beginning of
the semester compared to the end
of the semester. Have a think about
what is preventing you from study-
ing. You have had other dead-
lines during weeks 6 and 8. You
may want to plan your studying and
work ahead.
Your overall performance was very
good during the semester. Keep up the
good work and maybe try some more
challenging exercises. You found the
lab exercises not very challenging. You
could try out some more advanced mate-
rial and exercises. You dedicated more
time studying the lecture material in the
beginning of the semester compared to
the end of the semester. Have a think
about what is preventing you from study-
ing. You have had other deadlines during
weeks 6 and 8. You may want to plan
your studying and work ahead. You did
not face any health problems during the
semester. You did not face any personal
issues during the semester.
Table 3: The table presents example outputs from the three different systems in order of highest ranked
(bold signifies the chosen template content, * denotes significance with p <0.05 after comparing each
system with each other using Mann Whitney U test).
5 Results
Table 3 shows three summaries that have been
generated by the different systems. As we can see
from Table 3, students significantly prefer the out-
put of the system that is trained for their prefer-
ences. In contrast, students significantly dispre-
fer the system that is trained for lecturers? pref-
erences. Finally, they rank as second the system
that captures the preferences of both lecturers and
students, which shows that it might be feasible to
find middle ground between the preferences of two
user groups. Significance testing is done using
a Mann Whitney U test (p <0.05), performing a
pair-wise comparison.
6 Discussion
The weights derived from the linear regression
analysis vary from the Lecturer-adapted func-
tion to the Student-adapted function. For in-
stance, the lecturers? most preferred content is
hours studied. This, however, does not factor
heavily into the student?s reward function, apart
from the case where hours studied are de-
creasing or remain stable (see also Table 2).
Students like reading about
personal issues when the number of issues
they faced was increasing over the semester. On
the other hand, lecturers find it useful to give
advice to all students who faced personal issues
during the semester, hencepersonal issues
are included in the top 18 features (Table 2).
Moreover, students seem to mostly prefer a feed-
back summary that mentions the understandability
of the material when it increases, which is positive
feedback.
As reflected in Table 2, the analysis of PCR
showed that both groups found it useful to refer
to the average of marks when they remain stable.
In addition, both groups found understandability
when it increases useful, for a variety of reasons,
for example lecturers might find it useful to en-
courage students whereas students might prefer to
receive positive feedback. Both groups also agree
on hours studied as described earlier. On the
other hand, both groups find mentioning the stu-
dents? difficulty when it decreases as positive.
7 Future Work
In the future, we plan to evaluate our methodol-
ogy with lecturers and a larger sample of students
across different disciplines. Moreover, we aim to
port our methodology to a different domain, and
try to find the middle ground between the pref-
erences of novices and expert users when sum-
marising medical data while providing first aid.
Finally, we want to compare the methodology pre-
sented here to a multi-objective optimisation ap-
proach (Fonseca and Flemming, 1993), where the
preferences of each user group will be modelled as
two different optimisation functions.
Acknowledgements
The research leading to this work has re-
ceived funding from the EC?s FP7 programme:
(FP7/2011-14) under grant agreement no. 248765
(Help4Mood).
141
References
Carole Ames. 1992. Classrooms: Goals, structures,
and student motivation. Journal of Educational Psy-
chology, 84(3):p261?71.
Scotty D. Craig, Arthur C. Graesser, Jeremiah Sullins,
and Barry Gholson. 2004. Affect and learning: an
exploratory look into the role of affect in learning
with autotutor.
Carlos Fonseca and Peter Flemming. 1993. Genetic
algorithms for multiobjective optimization: Formu-
lation, discussion and generalization. In 5th Inter-
national Conference on Genetic Algorithms.
Albert Gatt, Francois Portet, Ehud Reiter, James
Hunter, Saad Mahamood, Wendy Moncur, and So-
mayajulu Sripada. 2009. From data to text in the
neonatal intensive care unit: Using NLG technology
for decision support and information management.
AI Communications, 22: 153-186.
Dimitra Gkatzia, Helen Hastie, Srinivasan Ja-
narthanam, and Oliver Lemon. 2013. Generating
student feedback from time-series data using Rein-
forcement Learning. In 14th European Workshop in
Natural Language Generation (ENLG).
Dimitra Gkatzia, Helen Hastie, and Oliver Lemon.
2014a. Comparing multi-label classification with
reinforcement learning for summarisation of time-
series data. In 52nd Annual Meeting of the Associa-
tion for Computational Linguistics (ACL).
Dimitra Gkatzia, Helen Hastie, and Oliver Lemon.
2014b. Finding Middle Ground? Multi-objective
Natural Language Generation from Time-series
data. In 14th Conference of the European Chap-
ter of the Association for Computational Linguistics
(EACL).
Jim Hunter, Yvonne Freer, Albert Gatt, Yaji Sripada,
Cindy Sykes, and D Westwater. 2011. Bt-nurse:
Computer generation of natural language shift sum-
maries from complex heterogeneous medical data.
American Medical Informatics Association, 18:621-
624.
Srinivasan Janarthanam and Oliver Lemon. 2010.
Adaptive referring expression generation in spoken
dialogue systems: Evaluation with real users. In
11th Annual Meeting of the Special Interest Group
on Discourse and Dialogue (SIGDIAL).
Ian T. Jolliffe. 1982. A note on the use of principal
components in regression. Journal of the Royal Sta-
tistical Society, Series C: 31(3):300?303.
Ian Jolliffe. 1986. Principal Component Analysis.
Springer-Verlag.
Saad Mahamood and Ehud Reiter. 2011. Generating
Affective Natural Language for Parents of Neona-
tal Infants. In 13th European Workshop in Natural
Language Generation (ENLG).
Natalie K. Person, Roger J. Kreuz, Rolf A. Zwaan, and
Arthur C. Graesser. 1995. Pragmatics and peda-
gogy: Conversational rules and politeness strategies
may inhibit effective tutoring. Journal of Cognition
and Instruction, 13(2):161-188.
Ehud Reiter and Robert Dale. 2000. Building natu-
ral language generation systems. Cambridge Uni-
versity Press.
Ehud Reiter, Roma Robertson, and Liesl Osman. 1999.
Types of knowledge required to personalise smoking
cessation letters. Artificial Intelligence in Medicine:
Proceedings of the Joint European Conference on
Artificial Intelligence in Medicine and Medical De-
cision Making.
Verena Rieser, Oliver Lemon, and Xingkun Liu. 2010.
Optimising information presentation for spoken dia-
logue systems. In 48th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL).
Richard Sutton and Andrew Barto. 1990. Time deriva-
tive models of pavlovian reinforcement. Learning
and Computational Neuroscience: Foundations of
Adaptive Networks, pages 497?537.
Richart Sutton and Andrew Barto. 1998. Reinforce-
ment learning. MIT Press.
Cynthia A. Thompson, Mehmet H. Goker, and Pat Lan-
gley. 2004. A personalised system for conversa-
tional recommendations. Journal of Artificial Intel-
ligence Research, 21(1).
Marilyn Walker, Diane J Litman, Candace Kamm, and
Alicia Abella. 1997. PARADISE: A framework
for evaluating spoken dialogue agents. In 8th con-
ference on European chapter of the Association for
Computational Linguistics (EACL).
Ingrid Zukerman and Diane Litman. 2001. Natu-
ral language processing and user modeling: Syner-
gies and limitations. In User Modeling and User-
Adapted Interaction, 11(1-2).
142
