The University of Ja?n Word Sense Disambiguation System
*
 
Manuel Garc?a-Vega 
Universidad de Ja?n 
Av. Madrid 35 
Ja?n, Spain, 23071 
mgarcia@ujaen.es 
Miguel A. Garc?a-Cumbreras 
Universidad de Ja?n 
Ja?n, Spain, 23071 
Av. Madrid 35, 23071 
magc@ujaen.es 
M. Teresa Mart?n-Valdivia 
Universidad de Ja?n 
Av. Madrid 35 
Ja?n, Spain, 23071 
maite@ujaen.es 
L. Alfonso Ure?a-L?pez 
Universidad de Ja?n 
Av. Madrid 35 
Ja?n, Spain, 23071 
laurena@ujaen.es 
                                                             
*
 This paper has been partially supported by the Spanish Government (MCYT) Project number TIC2003-07158-
C04-04 
 
 
Abstract 
This paper describes the architecture and re-
sults of the University of Ja?n system pre-
sented at the SENSEVAL-3 for the English-
lexical-sample and English-All-Words tasks. 
The system is based on a neural network ap-
proach. We have used the Learning Vector 
Quantization, which is a supervised learning 
algorithm based on the Kohonen neural model. 
1 Introduction 
Our system for SENSEVAL-3 uses a supervised 
learning algorithm for word sense disambiguation. 
The method suggested trains a neural network us-
ing the Learning Vector Quantization (LVQ) algo-
rithm, integrating several semantic relations of 
WordNet (Fellbaum, 1998) and SemCor corpus 
(Miller et al, 1993). The University of Ja?n system 
has been used in English-lexical-sample and Eng-
lish-All-Words tasks. 
2 Experimental Environment 
The presented disambiguator uses the Vector 
Space Model (VSM) as an information representa-
tion model. Each sense of a word is represented as 
a vector in an n-dimensional space where n is the 
number of words in all its contexts. 
The accuracy of the disambiguator depends es-
sentially on the word weights. We use the LVQ 
algorithm to adjust them. The input vector weights 
are calculated as shown by (Salton and McGill, 
1983) with the standard tf?idf, where the documents 
are the paragraphs. They are presented to the LVQ 
network and, after training, the output vectors 
(called prototype or codebook vectors) are ob-
tained, containing the adjusted weights for all 
senses of each word. 
Any word to disambiguate is represented with a 
vector in the same way. This representation must 
be compared with all the trained word sense vec-
tors by applying the cosine similarity rule: 
 
 
ik
ik
ik
sim
xw
xw
xw
?
?
),( =  [1] 
 
The sense corresponding to the vector of highest 
similarity is selected as the disambiguated sense. 
To train the neural network we have integrated 
semantic information from two linguistic re-
sources: SemCor corpus and WordNet lexical da-
tabase.  
2.1 SemCor  
Firstly, the SemCor (the Brown Corpus labeled 
with the WordNet senses) was fully used (the 
Brown-1, Brown-2 and Brown-v partitions). We 
used the paragraph as a contextual semantic unit 
                                             Association for Computational Linguistics
                        for the Semantic Analysis of Text, Barcelona, Spain, July 2004
                 SENSEVAL-3: Third International Workshop on the Evaluation of Systems
and each context was included in the training vec-
tor set. 
The SENSEVAL-3 English tasks have used the 
WordNet 1.7.1 sense inventory, but the SemCor is 
tagged with an earlier version of WordNet (spe-
cifically WordNet version 1.6).  
 
Figure 1. SemCor context for ?climb?. 
 
Therefore it was necessary to update the SemCor 
word senses. We have used the automatically 
mapped version of Semcor with the WordNet 1.7.1 
senses found in WordNet site
1
.  
Figure 2. WordNet artificial paragraph 
 
                                                             
1
 http://www.cogsci.princeton.edu/~wn/ 
Figure 1 shows the common format for the all 
the resource input paragraphs. For each word, the 
pos and sense are described, e.g. ?climb\2#1? is the 
verb ?climb? with sense 1. In addition, it has 158 
different words in its context and all of them are 
shown like the pair word-frequency. 
2.2 WordNet  
'Semantic relations from WordNet 1.7.1 were 
considered, in particular synonymy, antonymy, 
hyponymy, homonymy, hyperonymy, meronymy, 
and coordinate terms to generate artificial para-
graphs with words along each relation. 
For example, for a word with 7 senses, 7 artifi-
cial paragraphs with the synonyms of the 7 senses 
were added, 7 more with all its hyponyms, and so 
on. 
Figure 2 shows these artificial paragraphs for the 
?climb? verb. 
3 Learning Vector Quantization 
The LVQ algorithm (Kohonen, 1995) performs 
supervised learning, which uses a set of inputs with 
their correctly annotated outputs adjusting the 
model when an error is committed between the 
model outputs and the known outputs. 
The LVQ algorithm is a classification method 
based on neural competitive learning, which allows 
the definition of a group of categories on the input 
data space by reinforced learning, either positive 
(reward) or negative (punishment). In competitive 
learning, the output neurons compete to become 
active. Only a single output neuron is active at any 
one time. 
The general application of LVQ is to adjust the 
weights of labels to high dimensional input vec-
tors, which is technically done by representing the 
labels as regions of the data space, associated with 
adjustable prototype or codebook vectors. Thus, a 
codebook vector, w
k
, is associated for each class, 
k. This is particularly useful for pattern classifica-
tion problems. 
The learning algorithm is very simple. First, the 
learning rate and the codebook vectors are initial-
ised. Then, the following procedure is repeated for 
all the training input vectors until a stopping crite-
rion is satisfied:  
- Select a training input pattern, x, with class 
d, and present it to the network 
climb\2#1 158 a_hundred\5 1 ab-
sorb\2 1 advance\2 1 ... walk\2 1 
want\1 1 warn\2 1 warped\5 1 way\1 
2 west\3 1 whip\1 2 whir\1 1 
wraithlike\5 1 
climb\2#1 45 abruptly\4 1 absence\1 
1 ... stop\2 1 switch_off\2 1 
there\4 1 tube\1 1 two\5 1 unex-
pectedly\4 1 water\1 1 
... 
climb\2#2 33 adjust\2 1 almost\4 1 
arrange\2 1 ... procedure\1 1 re-
vetment\1 1 run\2 1 sky\1 1 
snatch\2 1 spread\2 1 stand\2 1 
truck\1 1 various\5 1 wait\2 1 
wing\1 1 
... 
climb\2#3 3 average\2 1 feel\2 1 
report\2 1 
climb\2#1 10 arise\2 1 come_up\2 1 
go\2 1 go_up\2 1 lift\2 1 locomote\2 
1 move\2 1 move_up\2 1 rise\2 1 
travel\2 1 
climb\2#1 3 climb_up\2 1 go_up\2 1 
mount\2 1 
climb\2#1 5 mountaineer\2 1 ramp\2 1 
ride\2 1 scale\2 1 twine\2 1 
climb\2#2 7 clamber\2 1 scramble\2 1 
shin\2 1 shinny\2 1 skin\2 1 sput-
ter\2 1 struggle\2 1 
... 
climb\2#5 2 go up\2 1 rise\2 1 
- Calculate the Euclidean distance between 
the input vector and each codebook vector 
||x-w
k
|| 
- Select the codebook vector, w
c
, that is 
closest to the input vector, x.  
 
 
{ }
kc
wxwx ?=?
k
min
 [2] 
 
This codebook vector is the winner neu-
ron and only this neuron updates its 
weights according the learning equation 
(equation 3). If the class of the input pat-
tern, x, matches the class of the winner 
codebook vector, w
c
 (the classification has 
been correct), then the codebook vector is 
moved closer to the pattern (reward), oth-
erwise it is moved further away. 
Let x(t) be a input vector at time t, and w
k
(t) the 
codebook vector for the class k at time t. The fol-
lowing equation defines the basic learning process 
for the LVQ algorithm. 
 
 [ ]  )()()()()1( tttstt
ccc
wxww ???+=+ ?  [3] 
 
Figure 3. Codebook vectors for ?climb? domain 
 
where s = 0, if k ? c; s = 1, if x(t) and w
c
(t) be-
long to the same class (c = d); and s = -1, if they do 
not (c ? d). ?(t) is the learning rate, and 0<?(t)<1 
is a monotically decreasing function of time. It is 
recommended that ?(t) should initially be rather 
small, say, smaller than 0.1 (Kohonen, 1995) and 
?(t) continues decreasing to a given threshold, u, 
very close to 0. 
The codebook vectors for the LVQ were initial-
ized to zero and every training vector was intro-
duced into the neural network, modifying the 
prototype vector weights depending on the correct-
ness in the winner election. 
All training vectors were introduced several 
times, updating the weights according to learning 
equation. ?(t) is a monotonically decreasing func-
tion and it represents the learning rate factor, be-
ginning with 0.1 and decreasing lineally: 
 
 ( ) ( ) ( )
P
tt
0
1
?
?? ?=+  [4] 
 
where P is the number of iterations performed in 
the training. The number of iterations has been 
fixed at 25 because at this point the network is 
stabilized. 
The LVQ must find the winner sense by calcu-
lating the Euclidean distances between the code-
book vectors and input vector. The shortest 
distance points to the winner and its weights must 
be updated. 
4 English Tasks 
The training corpus generated from SemCor and 
WordNet has been used to train the neural net-
works. All contexts of every word to disambiguate 
constitute a domain. Each domain represents a 
word and its senses. Figure 3 shows the codebook 
vectors generated after training process for ?climb? 
domain. 
We have generated one network per domain and 
after the training process, we have as many do-
mains as there are words to disambiguate adjusted. 
The network architecture per domain is shown in 
Figure 4. The number of input units is the number 
of different terms in all contexts of the given do-
main and the number of output units is the number 
of different senses. 
The disambiguator system has been used in Eng-
lish lexical sample and English all words tasks. 
For the English lexical sample task, we have 
used the available SENSEVAL-3 corpus to train 
the neural networks. We have also used the con-
texts generated using SemCor and WordNet for 
each word in SENSEVAL-3 corpus. For the Eng-
climb\2#1 1921 a\1#0 0.01883 
aarseth\1#0 0.03259 abelard\1#0 ... 
yorkshire\1#0 0.03950 young\3#0 
0.00380 zero\1#0 0.01449 
climb\2#2 235 act\1#0 -0.11558 
alone\4#0 -0.07754 ... windy\3#0 -
0.00922 worker\1#0 -0.02738 year\1#0 
-0.03715 zacchaeus\1#0 -0.02344 
climb\2#3 1148 abchasicus\1#0 
0.04127 able\3#0 -0.00945 ... 
young\3#0 -0.00275 zero\1#0 -0.00010
climb\2#4 258 age\1#0 -0.04180 air-
space\1#0 -0.02862 alone\4#0 -
0.01920 apple\1#0 -0.04242 ... 
world\1#0 -0.14184 year\1#0 -0.04113
young\3#0 -0.04831 zero\1#0 -0.06230
... 
lish all word task, we have only used the complete 
contexts of both SemCor and WordNet resources. 
The corpus has been tagged and lemmatized using 
the Tree-tagger (Schmid, 1994). 
Figure 4. The network architecture 
 
Once the training has finished, the testing be-
gins. The test is very simple. We establish the 
similarity between a given vector of the corpus 
evaluation with all the codebook vectors of its do-
main, and the highest similarity value corresponds 
to the disambiguated sense (winner sense). If it is 
not possible to  find a sense (it is impossible to ob-
tain  the cosine similarity value), we assign by de-
fault the most frequent sense (e.g. the first sense in 
WordNet). 
The official results achieved by the University of 
Ja?n system are presented in Table 1 for English 
lexical sample task, and in Table 2 for English all 
words. 
 
ELS Precision Recall Coverage 
Fine-grained 0.613 0.613 99.95% 
Coarse-grained 0.695 0.695 99.95% 
Table 1. Official results for ELS. 
 
EAW Precision Recall Coverage 
With U 0.590 0.590 100% 
Without U 0.601 0.588 97.795% 
Table 2. Official results for EAW. 
5 Conclusion 
This paper presents a new approach based on 
neural networks to disambiguate the word senses. 
We have used the LVQ algorithm to train a neural 
network to carry out the English lexical sample and 
English all words tasks. We have integrated two 
linguistic resources in the corpus provided by the 
organization: WordNet and SemCor.  
 
References 
Fellbaum, C. 1998. WordNet: An Electronic Lexi-
cal Database. The MIT Press 
Kohonen, T. 1995. Self-Organization and Associa-
tive Memory. 2nd Ed, Springer.Verlag, Berl?n. 
Kohonen, T., J. Hynninen, J. Kangas, J. Laak-
sonen, K. Torkkola. 1996. Technical Report, 
LVQ_PAK: The Learning Vector Quantization 
Program Package. Helsinki University of Tech-
nology, Laboratory of Computer and Information 
Science, FIN-02150 Espoo, Finland. 
Miller G., C. Leacock, T. Randee, R. Bunker. 
1993. A Semantic Concordance. Proc. of the 3rd 
DARPA Workshop on Human Language Tech-
nology. 
Salton, G. & McGill, M.J. 1983. Introduction to 
Modern Information Retrieval. McGraw-Hill, 
New York. 
Schmid, H., 1994. Probabilistic Part-of-Speech 
Tagging Using Decision Trees. In Proceedings 
of International Conference on New Methods in 
Language Processing. 
 
 
 
 
T
1 
T
2 
T
3 
T
M 
... 
Sense
1
 
Sense
1
 
Sense
N
 
... 
The R2D2 Team at SENSEVAL-3?
Sonia Va?zquez, Rafael Romero
Armando Sua?rez and Andre?s Montoyo
Dpto. de Lenguajes y Sistemas. Informa?ticos
Universidad de Alicante, Spain
{svazquez,romero}@dlsi.ua.es
{armando,montoyo}@dlsi.ua.es
Manuel Garc??a, M. Teresa Mart??n ?
M. ?Angel Garc??a and L. Alfonso Uren?a
Dpto. de Informa?tica
Universidad de Jae?n, Spain
{mgarcia,maite}@ujaen.es
{magc,laurena}@ujaen.es
Davide Buscaldi, Paolo Rosso ?
Antonio Molina, Ferra?n Pla? and Encarna Segarra
Dpto. de Sistemas Informa?ticos y Computacio?n
Univ. Polit. de Valencia, Spain
{dbuscaldi,prosso}@dsic.upv.es
{amolina,fpla,esegarra}@dsic.upv.es
Abstract
The R2D2 systems for the English All-Words and
Lexical Sample tasks at SENSEVAL-3 are based on
several supervised and unsupervised methods com-
bined by means of a voting procedure. Main goal
was to take advantage of training data when avail-
able, and getting maximum coverage with the help
of methods that not need such learning examples.
The results reported in this paper show that super-
vised and unsupervised methods working in par-
allel, and a simple sequence of preferences when
comparing the answers of such methods, is a feasi-
ble method. . .
The whole system is, in fact, a cascade of deci-
sions of what label to assign to a concrete instance
based on the agreement of pairs of systems, when
it is possible, or selecting the available answer from
one of them. In this way, supervised are preferred to
unsupervised methods, but these last ones are able
to tag such words that not have available training
data.
1 Introduction
Designing a system for Natural Language Process-
ing (NLP) requires a large knowledge on language
structure, morphology, syntax, semantics and prag-
matic nuances. All of these different linguistic
knowledge forms, however, have a common asso-
ciated problem, their many ambiguities, which are
difficult to resolve.
In this paper we concentrate on the resolution
of the lexical ambiguity that appears when a given
word in a context has several different meanings.
? This paper has been partially supported by the Spanish
Government (CICyT) under project number TIC-2003-7180
and the Valencia Government (OCyT) under project number
CTIDIB-2002-151
This specific task is commonly referred as Word
Sense Disambiguation (WSD). This is a difficult
problem that is receiving a great deal of attention
from the research community because its resolu-
tion can help other NLP applications as Machine
Translation (MT), Information Retrieval (IR), Text
Processing, Grammatical Analysis, Information Ex-
traction (IE), hypertext navigation and so on.
The R2D2 Team has participated in two tasks:
English all-words and lexical sample. We use sev-
eral different systems both supervised and unsuper-
vised. The supervised methods are based on Max-
imum Entropy (ME) (Lau et al, 1993; Berger et
al., 1996; Ratnaparkhi, 1998), neural network using
the Learning Vector Quantization algorithm (Koho-
nen, 1995) and Specialized Hidden Markov Mod-
els (Pla, 2000). The unsupervised methods are Rel-
evant Domains (RD) (Montoyo et al, 2003) and
the CIAOSENSO WSD system which is based on
Conceptual Density (Agirre and Rigau, 1995), fre-
quency of WordNet (Miller et al, 1993a) senses and
WordNet Domains (Magnini and Cavaglia, 2000).
In the following section we will show a more
complete description of the systems. Next, how
such methods were combined in two voting sys-
tems, and the results obtained in SENSEVAL-3. Fi-
nally, some conclusions will be presented.
2 Systems description
In this section the systems that have participated at
SENSEVAL-3 will be described.
2.1 Maximum Entropy
ME modeling provides a framework for integrating
information for classification from many heteroge-
neous information sources (Manning and Schu?tze,
1999). ME probability models have been success-
                                             Association for Computational Linguistics
                        for the Semantic Analysis of Text, Barcelona, Spain, July 2004
                 SENSEVAL-3: Third International Workshop on the Evaluation of Systems
fully applied to some NLP tasks, such as POS tag-
ging or sentence boundary detection (Ratnaparkhi,
1998). ME have been also applied to WSD (van
Halteren et al, 2001; Montoyo and Sua?rez, 2001;
Sua?rez and Palomar, 2002), and as meta-learner in
(Ilhan et al, 2001).
Our ME-based system has been shown competi-
tive (Ma`rquez et al, 2003) when compared to other
supervised systems such as Decision Lists, Support
Vector Machines, and AdaBoost. The features that
were defined to train the system are those described
in Figure 1.
? the target word itself
? lemmas of content-words at positions ?1, ?2, ?3
? words at positions ?1, ?2,
? words at positions ?1, ?2, ?3
? content-words at positions ?1, ?2, ?3
? POS-tags of words at positions ?1, ?2, ?3
? lemmas of collocations at positions (?2,?1),
(?1,+1), (+1,+2)
? collocations at positions (?2,?1), (?1,+1),
(+1,+2)
? lemmas of nouns at any position in context, occur-
ring at least m% times with a sense
? grammatical relation of the target word
? the word that the target word depends on
? the verb that the target word depends on
? the target word belongs to a multi-word, as identi-
fied by the parser
Figure 1: Features Used for the Supervised Learn-
ing of the ME system
Because the ME system needs annotated data
for the training, Semcor (Miller et al, 1993b) was
used for the English All-Words task, the system
was trained using Semcor (Miller et al, 1993b), and
parsed by Minipar (Lin, 1998). Only those words
that have 10 examples or more in Semcor were pro-
cessed in order to obtain a ME classifier.
For the Spanish Lexical Sample task, the train-
ing data from SENSEVAL-3 was the source of la-
beled examples. We did not use any parser, just the
lemmatization and POS-tagging information sup-
plied into the training data itself.
2.2 UPV-SHMM-AW
The upv-shmm-aw WSD system is a supervised ap-
proach based on Specialized Hidden Markov Mod-
els (SHMM).
Basically, a SHMM consists of changing the
topology of a Hidden Markov Model in order to get
a more accurate model which includes more infor-
mation. This is done by means of an initial step
previous to the learning process. It consists of the
redefinition of the input vocabulary and the output
tags. This redefinition is done by means of two pro-
cesses which transform the training set: the selec-
tion process chooses which input features (words,
lemmas, part-of-speech tags, ...) are relevant to the
task, and the specialization process redefines the
output tags by adding information from the input.
This specialization produces some changes in the
model topology, in order to allow the model to bet-
ter capture some contextual restrictions and to get a
more accurate model.
We used as training data the part of the Sem-
Cor corpus that is semantically annotated and su-
pervised for nouns, verbs, adjectives and adverbs,
and the test data set provided by SENSEVAL-2.
We used 10% of the training corpus as a develop-
ment data set in order to determine the best selection
and specialization criteria.
In the experiments, we used WordNet1.6 (Miller
et al, 1993a) as a dictionary that supplies all the
possible semantic senses for a given word. Our sys-
tem disambiguated all the polysemic lemmas, that
is, the coverage of our system was 100%. For un-
known words (words that did not appear in the train-
ing data set), we assigned the first sense in WordNet.
2.3 Relevant Domains
This is an unsupervised WSD method based on the
WordNet Domains lexical resource (Magnini and
Cavaglia, 2000). The underlaying working hypoth-
esis is that domain labels, such as ARCHITEC-
TURE, SPORT and MEDICINE provide a natural
way to establish semantic relations between word
senses, that can be used during the disambiguation
process. This resource has already been used on
Word Sense Disambiguation (Magnini and Strappa-
rava, 2000), but it has not made use of glosses infor-
mation. So our approach make use of a new lexical
resource obtained from glosses information named
Relevant Domains.
First step is to obtain the Relevant Domains re-
source from WordNet glosses. For this task is nec-
essary a previous part-of-speech tagging of Word-
Net glosses (each gloss has associated a domain la-
bel). So we extract all nouns, verbs, adjectives and
adverbs from glosses and assign them their associ-
ated domain label. With this information and using
the Association Ratio formula(w=word,D=domain
label), in (1), we obtain the Relevant Domains re-
source.
AR(w,D) = Pr(w|D)log2Pr(w|D)Pr(w) (1)
The final result is for each word, a set of domain
labels sorted by Association Ratio, for example,
for word plant? its Relevant Domains are: genetics
0.177515, ecology 0.050065, botany 0.038544 . . . .
Once obtained Relevant Domains the disam-
biguation process is carried out. We obtain from
the text source the context words that co-occur with
the word to be disambiguated (context could be
a sentence or a window of words). We obtain a
context vector from Relevant Domains and context
words (in case of repeated domain labels, they are
weighted). Furthermore we need a sense vector ob-
tained in the same way as context vector from words
of glosses of each word sense. We select the cor-
rect sense using the cosine measure between con-
text vector and sense vectors. So the selected sense
is that for which the cosine with the context vector
is closer to one.
2.4 LVQ-JA ?EN-ELS
The LVQ-JA ?EN-ELS system (Garc??a-Vega et al,
2003) is based on a supervised learning algorithm
for WSD. The method trains a neural network using
the Learning Vector Quantization (LVQ) algorithm
(Kohonen, 1995), integrating Semcor and several
semantic relations of WordNet.
The Vector Space Model (VSM) is used as an in-
formation representation model. Each sense of a
word is represented as a vector in an n-dimensional
space where n is the number of words in all its con-
texts.
We use the LVQ algorithm to adjust the word
weights. The input vector weights are calculated
as shown by (Salton and McGill, 1983) with the
standard (tf ? idf). They are presented to the LVQ
network and, after training, the output vectors are
obtained, containing the adjusted weights for all
senses of each word.
Any word to disambiguate is represented with a
vector in the same way. This representation must be
compared with all the trained sense vectors of the
word by applying the cosine similarity rule:
sim(wk, xi) = wk ? xi| wk | ? | xi | (2)
The sense corresponding to the vector of highest
similarity is selected as the disambiguated sense.
To train the neural network we have inte-
grated semantic information from two linguistic re-
sources: SemCor1.6 corpus and WordNet1.7.1 lex-
ical database. From Semcor1.6 we used the para-
graph as a contextual semantic unit and each con-
text was included in the training vector set. From
WordNet1.7.1 some semantic relations were consid-
ered, specifically, synonymy, antonymy, hyponymy,
homonymy, hyperonymy, meronymy, and coordi-
nate terms. This information was introduced to the
training set through the creation of artificial para-
graphs with the words of each relation. So, for a
word with 7 senses, 7 artificial paragraphs with the
synonyms of the 7 senses were added, 7 more with
all its hyponyms, and so on.
The learning algorithm is very simple. First, the
learning rate and the codebook vectors are initial-
ized. Then, the following procedure is repeated for
all the training input vectors until a stopping crite-
rion is satisfied:
- Select a training input pattern, x, with class d,
and present it to the network
- Calculate the Euclidean distance between the in-
put vector and each codebook vector || x? wk ||
- Select the codebook vector, wc, that is closest to
the input vector, x, like the winner sense.
- The winner neuron updates its weights accord-
ing the learning equation:
wc(t+ 1) = wc(t) + s ? ?(t) ? [x(t)? wc(t)] (3)
where s = 0, if k 6= c; s = 1, if x(t) and wc(t)
belong to the same class (c = d); and s = ?1, if
they do not (c 6= d). ?(t) is the learning rate, and
0 < ?(t) < 1 is a monotically decreasing func-
tion of time. It is recommended that ?(t) should
already initially be rather small, say, smaller than
0.1 (Kohonen, 1995) and ?(t) continues decreasing
to a given threshold, u, very close to 0.
2.5 CIAOSENSO
The CIAOSENSO WSD system is an unsupervised
system based on Conceptual Density, the frequency
of WordNet sense, and WordNet Domains. Concep-
tual Density is a measure of the correlation among
the sense of a given word and its context. The
noun sense disambiguation is performed by means
of a formula combining the Conceptual Density
with WordNet sense frequency (Rosso et al, 2003).
The context window used in both the English all-
words and lexical sample tasks is of 4 nouns. Ad-
ditional weights are assigned to those senses hav-
ing the same domain as the context nouns? senses.
Each weight is proportional to the frequency of such
senses, and is calculated as MDW (f, i) = 1/f ?1/i
where f is an integer representing the frequency
of the sense of the word to be disambiguated and
i gives the same information for the context word.
Example: If the word to be disambiguated is doc-
tor, the domains for senses 1 and 4 are, respec-
tively, Medicine and School. Therefore, if one of
the context words is university, the resulting weight
for doctor(4) and university(3) is 1/4 ? 1/3.
The sense disambiguation of an adjective is per-
formed only on the basis of the above weights.
Given one of its senses, we extract the synsets ob-
tained by the similar to, pertainym and attribute
relationships. For each of them, we calculate the
MDW with respect to the senses of the context
noun. The weight assigned to the adjective sense
is the average between these MDWs. The se-
lected sense is the one having the maximum average
weight.
The sense disambiguation of a verb is done nearly
in the same way, but taking into consideration only
the MDWs with the context words. In the all-words
task the context words are the noun before and af-
ter the verb, whereas in the lexical sample task the
context words are four (two before and two after the
verb), without regard to their morphological cate-
gory. This has been done in order to improve the
recall in the latter task, for which the test corpus is
made up mostly by verbs.
The sense disambiguation of adverbs (in both
tasks) is carried out in the same way of the disam-
biguation of verbs for the lexical sample task.
3 Tasks Processing
We have selected several combinations of such sys-
tems described before for two voting systems, one
for the Lexical-Sample task and the other for the
All-Words task.
3.1 English Lexical Sample Task
At the English Lexical Sample task we combined
the answers of four systems: Relevant Domains,
CIAOSENSO, LVQ-JA ?EN-ELS and Maximum En-
tropy.
The four methods worked in parallel and their
sets of answers were the input of a majority voting
procedure. This procedure selected those answers
with more systems agreements. In case of tie we
gave priority to supervised systems.
With this voting system we obtained around a
63% precision and a 52% recall.
3.2 English All Words Task
For this task we used a voting system combining
the results of Relevant Domains, Maximum En-
tropy, CIAOSENSO and UPV-SHMM-AW. So we
obtained the final results after 10 steps.
Step 1, we selected those answers with agree-
ment between ME and UPV-SHMM-AW (super-
vised systems).
Step 2, from no agreement in step 1 we selected
those answers with agreement between ME and Rel-
evant Domains.
Step 3, from no agreement in step 2 we selected
those answers with agreement between ME and
CIAOSENSO.
Step 4, from no agreement in step 3 we se-
lected those answers with agreement between
CIAOSENSO and UPV-SHMM-AW.
Step 5, from no agreement in step 4 we se-
lected those answers with agreement between UPV-
SHMM-AW and Relevant Domains.
Step 6, from no agreement in step 5 we selected
those answers with agreement between Relevant
Domains and CIAOSENSO.
Step 7, from no agreement in step 6 we selected
Maximum Entropy answers.
Step 8, from the remaining unlabeled instances
we selected UPV-SHMM-AW answers.
Step 9, from the remaining unlabeled instances
we selected Relevant Domains answers.
Step 10, from the remaining unlabeled instances
we selected CIAOSENSO answers.
Last step was labeling with the most frequent
sense in WordNet those instances that had been not
tagged by any system, but in view of the final results
only two instances had not answer and we didn?t
find them in WordNet.
With this voting system preference was given to
supervised systems over unsupervised systems.
We obtained around a 63% precision and a 63%
recall.
4 Conclusions
This paper presents the main characteristics of
the Maximum Entropy, LVQ-JAEN-ELS, UPV-
SHMM-AW, Relevant Domains and CIAOSENSO
systems within the framework of SENSEVAL-3 En-
glish Lexical Sample and All Words tasks. These
systems are combined with a voting technique ob-
taining a promising results for English All Words
and English Lexical Sample tasks.
References
Eneko Agirre and German Rigau. 1995. A pro-
posal for word sense disambiguation using Con-
ceptual Distance. In Proceedings of the Interna-
tional Conference ?Recent Advances in Natural
Language Processing? (RANLP95).
Adam L. Berger, Stephen A. Della Pietra, and Vin-
cent J. Della Pietra. 1996. A maximum entropy
approach to natural language processing. Com-
putational Linguistics, 22(1):39?71.
Manuel Garc??a-Vega, Mar??a Teresa Mart??n-
Valdivia, and Luis Alfonso Uren?a. 2003.
Aprendizaje competitivo lvq para la desam-
biguacio?n le?xica. Revista de la Sociedad
Espaola para el Procesamiento del Lenguaje
Natural, 31:125?132.
H. Tolga Ilhan, Sepandar D. Kamvar, Dan Klein,
Christopher D. Manning, and Kristina Toutanova.
2001. Combining Heterogeneous Classifiers for
Word-Sense Disambiguation. In Judita Preiss
and David Yarowsky, editors, Proceedings of the
2nd International Workshop on Evaluating Word
Sense Disambiguation Systems (SENSEVAL-2),
pages 87?90, Toulouse, France, July. ACL-
SIGLEX.
T. Kohonen. 1995. Self-organization and associa-
tive memory. 2nd Ed. Springer Verlag, Berlin.
R. Lau, R. Rosenfeld, and S. Roukos. 1993.
Adaptative statistical language modeling using
the maximum entropy principle. In Proceedings
of the Human Language Technology Workshop,
ARPA.
Dekang Lin. 1998. Dependency-based evaluation
of minipar. In Proceedings of the Workshop on
the Evaluation of Parsing Systems, First Inter-
national Conference on Language Resources and
Evaluation, Granada, Spain.
Bernardo Magnini and Gabriela Cavaglia. 2000.
Integrating Subject Field Codes into WordNet. In
M. Gavrilidou, G. Crayannis, S. Markantonatu,
S. Piperidis, and G. Stainhaouer, editors, Pro-
ceedings of LREC-2000, Second International
Conference on Language Resources and Evalu-
ation, pages 1413?1418, Athens, Greece.
Bernardo Magnini and C. Strapparava. 2000. Ex-
periments in Word Domain Disambiguation for
Parallel Texts. In Proceedings of the ACL Work-
shop on Word Senses and Multilinguality, Hong
Kong, China.
Christopher D. Manning and Hinrich Schu?tze.
1999. Foundations of Statistical Natural Lan-
guage Processing. The MIT Press, Cambridge,
Massachusetts.
Llu??s Ma`rquez, Fco. Javier Raya, John Car-
roll, Diana McCarthy, Eneko Agirre, David
Mart??nez, Carlo Strapparava, and Alfio
Gliozzo. 2003. Experiment A: several all-words
WSD systems for English. Technical Report
WP6.2, MEANING project (IST-2001-34460),
http://www.lsi.upc.es/?nlp/meaning/meaning.html.
George A. Miller, Richard Beckwith, Christiane
Fellbaum, Derek Gross, and Katherine J. Miller.
1993a. Five Papers on WordNet. Special Issue of
the International journal of lexicography, 3(4).
George A. Miller, C. Leacock, R. Tengi, and
T. Bunker. 1993b. A Semantic Concordance. In
Proceedings of ARPA Workshop on Human Lan-
guage Technology, pages 303?308, Plainsboro,
New Jersey.
Andre?s Montoyo and Armando Sua?rez. 2001.
The University of Alicante word sense disam-
biguation system. In Judita Preiss and David
Yarowsky, editors, Proceedings of the 2nd In-
ternational Workshop on Evaluating Word Sense
Disambiguation Systems (SENSEVAL-2), pages
131?134, Toulouse, France, July. ACL-SIGLEX.
Andre?s Montoyo, Sonia Va?zquez, and German
Rigau. 2003. Me?todo de desambiguacio?n le?xica
basada en el recurso le?xico Dominios Rele-
vantes. Procesamiento del Lenguaje Natural, 30,
september.
F. Pla. 2000. Etiquetado Le?xico y Ana?lisis
Sinta?ctico Superficial basado en Modelos Es-
tad??sticos. Tesis doctoral, Departamento de Sis-
temas Informa?ticos y Computacio?n. Universidad
de Polite?cnica de Valencia, Septiembre.
Adwait Ratnaparkhi. 1998. Maximum Entropy
Models for Natural Language Ambiguity Resolu-
tion. Ph.D. thesis, University of Pennsylvania.
P. Rosso, F. Masulli, D. Buscaldi, F. Pla, and
A. Molina. 2003. Automatic noun disambigua-
tion. LNCS, Springer Verlag, 2588:273?276.
G. Salton and M.J. McGill. 1983. Introduction
to modern information retrieval. McGraw-Hill,
New York.
Armando Sua?rez and Manuel Palomar. 2002.
A maximum entropy-based word sense disam-
biguation system. In Hsin-Hsi Chen and Chin-
Yew Lin, editors, Proceedings of the 19th In-
ternational Conference on Computational Lin-
guistics, pages 960?966, Taipei, Taiwan, August.
COLING 2002.
H. van Halteren, J. Zavrel, and W. Daelemans.
2001. Improving accuracy in wordclass tag-
ging through combination of machine learning
systems. Computational Linguistics, 27(2):199?
230.
Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 78?82,
Prague, June 2007. c?2007 Association for Computational Linguistics
Combining Lexical-Syntactic Information with Machine Learning for
Recognizing Textual Entailment
Arturo Montejo-Ra?ez, Jose Manuel Perea, Fernando Mart??nez-Santiago,
Miguel A?ngel Garc??a-Cumbreras, Maite Mart??n-Valdivia, Alfonso Uren?a-Lo?pez
Dpto. de Informa?tica, Universidad de Jae?n
Campus de las Lagunillas s/n, 23071 - Jae?n
{amontejo, jmperea, dofer, magc, maite, laurena}@ujaen.es
Abstract
This document contains the description of
the experiments carried out by SINAI group.
We have developed an approach based on
several lexical and syntactic measures inte-
grated by means of different machine learn-
ing models. More precisely, we have eval-
uated three features based on lexical sim-
ilarity and 11 features based on syntactic
tree comparison. In spite of the relatively
straightforward approach we have obtained
more than 60% for accuracy. Since this
is our first participation we think we have
reached a good result.
1 Approach description
We fill face the textual entailment recognition us-
ing Machine Learning methods, i.e. identifying fea-
tures that characterize the relation between hypothe-
sis and associated text and generating a model using
existing entailment judgements that will allow us to
provide a new entailment judgement agains unseen
pairs text-hypothesis. This approach can be split into
the two processes shown in Figures 1 and 2.
In a more formal way, given a text t and an hy-
pothesis h we want to define a function e which takes
these two elements as arguments and returns and an-
swer to the entailment question:
e(t, h) =
{ Y ES if h is entailed by t
NO otherwise (1)
Now the question is to find that ideal function
Figure 1: Training processes
Figure 2: Classification processes
e(t, h). We will approximate this function using a
binary classifier:
e?(t, h) = bc(f,m) (2)
where
bc is a binary classifier
f is a set of features
m is the learned model for the classifier
Therefore, it only remains to select a binary clas-
sifier and a feature extraction method. We have per-
formed two experiments with different choices for
both decisions. These two experiments are detailed
below.
78
1.1 Lexical similarity
This experiment approaches the textual entailment
task being based on the extraction of a set of lexical
measures that show the existing similarity between
the hypothesis-text pairs. Our approach is similar
to (Ferrandez et al, 2007) but we make matching
between similar words too while (Ferrandez et al,
2007) apply exact matching (see below).
The first step previous to the calculation of the
different measures is to preprocess the pairs using
the English stopwords list. Next we have used the
GATE1 architecture to obtain the stems of tokens.
Once obtained stems, we have applied four different
measures or techniques:
? Simple Matching: this technique consists of
calculating the semantic distance between each
stem of the hypothesis and text. If this dis-
tance exceeds a threshold, both stems are con-
sidered similar and the similarity weight value
increases in one. The accumulated weight is
normalized dividing it by the number of ele-
ments of the hypothesis. In this experiment we
have considered the threshold 0.5. The values
of semantic distance measure range from 0 to
1. In order to calculate the semantic distance
between two tokens (stems), we have tried sev-
eral measures based on WordNet (Alexander
Budanitsky and Graeme Hirst, 2001). Lin?s
similarity measure (Lin, 1998) was shown to
be best overall measures. It uses the notion of
information content and the same elements as
Jiang and Conrath?s approach (Jiang and Con-
rath, 1997) but in a different fashion:
simL(c1, c2) = 2? log p(lso(c1, c2))log p(c1) + log p(c2)
where c1 and c2 are synsets, lso(c1,c2) is
the information content of their lowest super-
ordinate (most specific common subsumer) and
p(c) is the probability of encountering an in-
stance of a synset c in some specific corpus
(Resnik, 1995). The Simple Matching tech-
nique is defined in the following equation:
SIMmatching =
?
i?H similarity(i)
|H|
1http://gate.ac.uk/
where H is the set that contains the elements of
the hypothesis and similarity(i) is defined like:
similarity(i) =
{ 1 if ?j ? TsimL(i, j) > 0.5
0 otherwise
? Binary Matching: this measure is the same
that the previous one but modifying the simi-
larity function:
similarity(i) =
{ 1 if ?j ? T i = j
0 otherwise
? Consecutive Subsequence Matching: this
technique relies on forming subsequences of
consecutive stems in the hypothesis and match-
ing them in the text. The minimal size of the
consecutive subsequences is two and the max-
imum is the maximum size of the hypothesis.
Every correct matching increases in one the fi-
nal weight. The sum of the obtained weights of
the matching between subsequences of a cer-
tain size or length is normalized by the number
of sets of consecutive subsequences of the hy-
pothesis created for this length. These weights
are accumulated and normalized by the size of
the hypothesis less one. The Consecutive Sub-
sequence Matching technique is defined in the
following equations:
CSSmatching =
?|H|
i=2 f(SHi)
|H| ? 1
where SHi is the set that contains the subse-
quences of the hypothesis with i size or length
and f(SHi) is defined like:
f(SHi) =
?
j?SHi matching(j)
|H| ? i+ 1
where
matching(i) =
{ 1 if ?k ? STi k = j
0 otherwise
where STi represents the set that contains the
subsequences with i size from text.
? Trigrams: this technique relies on forming tri-
grams of words in the hypothesis and match-
ing them in the text. A trigram is a group of
79
three words. If a hypothesis trigram matches in
text, then the similarity weight value increases
in one. The accumulated weight is normalized
dividing it by the number of trigrams of the hy-
pothesis.
1.2 Syntactic tree comparison
Some features have been extracted from pairs
hypothesis-text related to the syntactic information
that some parser can produce. The rationale be-
hind it consists in measuring the similarity between
the syntactic trees of both hypothesis and associated
text. To do that, terms appearing in both trees are
identified (we call this alignment) and then, graph
distances (number of nodes) between those terms in
both trees are compared, producing certain values as
result.
In our experiments, we have applied the
COLLINS (Collins, 1999) parser to generate the
syntactic tree of both pieces of text. In Figure 3 the
output of the syntactic parsing for a sample pair is
shown. This data is the result of the syntactical anal-
ysis performed by the mentioned parser. A graph
based view of the tree corresponding to the hypoth-
esis is drawn in Figure 4. This graph will help us to
understand how certain similarity measures are ob-
tained.
Figure 3: Syntactic trees of sample hypothesis and
its associated text
<t>
(TOP (S (LST (LS 0302) (. .)) (NP (JJ Next) (NN year))
(VP (VBZ is) (NP (NP (DT the) (JJ 50th) (NN anniversary))
(PP (IN of) (NP (NP (DT the) (NNP Normandy) (NN invasion)
(, ,)) (NP (NP (DT an)(NN event)) (SBAR (IN that) (S (VP
(MD would) (RB n?t) (VP (VB have) (VP (VBN been) (ADJP
(JJ possible)) (PP (IN without) (NP (NP (DT the) (NNP
Liberty) (NN ships.)) (SBAR (S (NP (DT The) (NNS
volunteers)) (VP (VBP hope) (S (VP (TO to) (VP (VB raise)
(NP (JJ enough) (NN money)) (S (VP (TO to) (VP (VB sail)
(NP (DT the) (NNP O?Brien)) (PP (TO to) (NP (NNP France)))
(PP (IN for)(NP (DT the) (JJ big) (NNP D-Day) (NN celebration)
(. .))))))))))))))))))))))))))
</t>
<h>
(TOP (S (NP (NP (CD 50th) (NNP Anniversary)) (PP (IN of)
(NP (NNP Normandy) (NNP Landings)))) (VP (VBZ lasts) (NP
(DT a) (NN year) (. .)))))
</h>
From the sample above, the terms normandy, year
and anniversary appear in both pieces of text. We
say that these terms are ?aligned?. Therefore, for
the three possible pairs of aligned terms we can com-
pute the distance, in nodes, to go from one term to
the other at each tree. Then, the difference of these
Figure 4: Syntact tree of sample hypothesis
distances is computed and some statistics are gener-
ated. We can summarize the process of computing
this differences in the algorithm detailed in Figure 6.
Figure 5: Tree comparison process
For instance, in the tree represented in Figure 4
we can see that we have to perform 5 steps to go
from node Anniversary to node Normandy. Since
there are no more possible occurrences of these two
terms, then the minimal distance between them is
5. This value is also measured on the tree corre-
80
sponding to the text, and the absolute difference be-
tween these two minimal distances is stored in order
to compute final feature weights consisting in basic
statistical values. The algorithm to obtain the distri-
bution of distance differences is detailed in Figure 6.
Figure 6: Extraction of features based on syntactic
distance
Input:
a syntactic tree of the hypothesis Sh
a syntactic tree of the text St
Output :
the set of distance differences
Dd = {ddij : ti, tj ? T}
Pseudo code:
T ? aligned terms between Sh and St
Dd ? ?
for i = 1..n do
for j = i+ 1..n do
disth ? minimal distance between
nodes ti and tj in Sh
distt ? minimal distance between
nodes ti and tj in St
ddij ? |disth ? distt|
Dd ? {ddij} ?Dd
end-for
end-for
The statistics generated from the resulting list of
distances differences Dd are the following:
1. The number of aligned terms (3 in the given
example).
2. The number of matched POS values of aligned
terms, that is, if the term appears with the same
POS label in both texts (in the example An-
niversary differs in the POS label assigned).
3. The number of unmatched POS labels of
aligned terms.
4. The average distance in nodes through the syn-
tactic tree to go from one aligned term to an-
other.
5. The minimal distance difference found.
Table 1: Results with TiMBL and BBR classifiers
(Exp5 is the only official result reported in this pa-
per).
Experiment Classifier Accuracy
Exp1 BBR 0.6475
Exp2 BBR 0.64625
Exp3 BBR 0.63875
Exp4 TiMBL 0.6062
Exp5 TiMBL 0.6037
Exp6 TiMBL 0.57
6. The maximal distance difference found.
7. The standard deviation of distance differences.
In a similar way, differences in the depth level of
nodes for aligned terms are also calculated. From
the example exposed the following values were
computed:
* Aligned 3
* MatchedPOS 2
* UnmatchedPOS 1
* AvgDistDiff 0.0392156863
* MinDistDiff 0.0000000000
* MaxDistDiff 0.0588235294
* StdevDistDiff 0.0277296777
* AvgDepthDiff 2.0000000000
* MinDepthDiff 1.0000000000
* MaxDepthDiff 3.0000000000
* StdevDepthDiff 0.8164965809
2 Experiments and results
The algorithms used as binary classifiers are two:
Bayesian Logistic Regression (BBR)2 and TiMBL
(Daelemans et al, 1998). Both algorithms have been
trained with the devel data provided by the organiza-
tion of the Pascal challange. As has been explained
in previous sections, a model is generated via the
supervised learning process. This model m is then
feed into the classification variant of the algorithm,
which will decide whether a new hypothesis sample
is entailed by the given text or not.
The experiments and results are shown in Table 1:
where:
? Exp1 uses four features: three lexical similari-
ties (SIMmatching + CSSmatching + Trigrams)
and Syntactic tree comparison.
2http://www.stat.rutgers.edu/?madigan/BBR/ [available at
March 27, 2007]
81
? Exp2 uses five features: four lexical similari-
ties (SIMmatching + CSSmatching + Trigrams
+ BINmatching) and Syntactic tree compari-
son.
? Exp3 uses only three lexical similarities
(SIMmatching + CSSmatching + Trigrams).
? Exp4 uses the four lexical similarities
(SIMmatching + CSSmatching + Trigrams +
BINmatching)
? Exp5 uses only three lexical similarities
(SIMmatching + CSSmatching + Trigrams).
? Exp6 uses four features: three lexical similari-
ties (SIMmatching + CSSmatching + Trigrams)
and Syntactic tree comparison.
As we expected, the best result we have obtained
is by means of the integration of the whole of the
features available. More surprising is the good result
obtained by using lexical features only, even better
than experiments based on syntactical features only.
On the other hand, we expected that the integration
of both sort of features improve significatively the
performance of the system, but the improvement re-
spect of lexical features is poor (less than 2%). .
Similar topics share similar vocabulary, but not sim-
ilar syntax at all. Thus, we think we should to inves-
tigate semantic features better than the syntactical
ones.
3 Conclusions and future work
In spite of the simplicity of the approach, we have
obtained remarkable results: each set of features has
reported to provide relevant information concerning
to the entailment judgement determination. On the
other hand, these two approaches can be merged into
one single system by using different features all to-
gether and feeding with them several binary classi-
fiers that could compose a voting system. We will
do that combining TiMBL, SVM and BBR.We ex-
pect to improve the performance of the entailment
recognizer by this integration.
Finally, we want to implement a hierarchical ar-
chitecture based on constraint satisfaction networks.
The constraints will be given by the set of avail-
able features and the maintenance of the integration
across the semantic interpretation process.
4 Acknowledgements
This work has been partially financed by the
TIMOM project (TIN2006-15265-C06-03) granted
by the Spanish Government Ministry of Science and
Technology and the RFC/PP2006/Id 514 granted by
the University of Jae?n.
References
Alexander Budanitsky and Graeme Hirst. 2001. Seman-
tic distance in wordnet: An experimental, application-
oriented evaluation of five measures.
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania.
Walter Daelemans, Jakub Zavrel, Ko van der Sloot, and
Antal van den Bosch. 1998. Timbl: Tilburg memory
based learner, version 1.0, reference guide.
Oscar Ferrandez, Daniel Micolo, Rafael Mu noz, and
Manuel Palomar. 2007. Te?cnicas le?xico-sinta?cticas
para reconocimiento de inmplicacio?n textual. . Tec-
nolog??as de la Informaco?n Multilingu?e y Multimodal.
In press.
Jay J. Jiang and David W. Conrath. 1997. Semantic
similarity based on corpus statistics and lexical taxon-
omy. In Proceedings of International Conference on
Research in Computational Linguistics, Taiwan.
Dekang Lin. 1998. An information-theoretic definition
of similarity. In Proceedings of the 15th International
Conference on Machine Learning.
Philip Resnik. 1995. Using information content to evalu-
ate semantic similarity. In Proceedings of the 14th In-
ternational Joint Conference on Artificial Intelligence,
Montreal.
82
Proceedings of the 2th Workshop of Natural Language Processing for Improving Textual Accessibility (NLP4ITA), pages 11?19,
Atlanta, Georgia, 14 June 2013. c?2013 Association for Computational Linguistics
Open Book: a tool for helping ASD users? semantic comprehension
Eduard Barbu
University of Jae?n
Paraje de Las Lagunillas
Jae?n, 23071, Spain
ebarbu@ujaen.es
Maria Teresa Mart??n-Valdivia
University of Jae?n
Paraje de Las Lagunillas
Jae?n, 23071, Spain
maite@ujaen.es
Luis Alfonso Uren?a-Lo?pez
University of Jae?n
Paraje de Las Lagunillas
Jae?n, 23071, Spain
laurena@ujaen.es
Abstract
Persons affected by Autism Spectrum Dis-
orders (ASD) present impairments in so-
cial interaction. A significant percentile of
them have inadequate reading comprehension
skills. In the ongoing FIRST project we build
a multilingual tool called Open Book that
helps the ASD people to better understand the
texts. The tool applies a series of automatic
transformations to user documents to identify
and remove the reading obstacles to compre-
hension. We focus on three semantic compo-
nents: an Image component that retrieves im-
ages for the concepts in the text, an idiom de-
tection component and a topic model compo-
nent. Moreover, we present the personaliza-
tion component that adapts the system output
to user preferences.
1 Introduction
Autism Spectrum Disorders are widespread and af-
fect every 6 people in 10000 according to Autism
Europe site1. The disorder is chiefly characterized
by impairments in social interaction and by repet-
itive and stereotyped behaviour (Attwood, 2007).
People affected by ASD are not able to communi-
cate properly because they lack an adequate theory
of mind (Baron-Cohen, 2001). Therefore, they are
not able to infer the other persons? mental states:
beliefs, emotions or desires. This lack of empathy
prevents the people with ASD to have a fulfilled so-
cial life. Their inability to understand others leads
to the incapacity to communicate their wishes and
desires and to social marginalization.
1http://www.autismeurope.org/
The FIRST project seeks to make a small step
towards integration of ASD people in the informa-
tion society by addressing their reading comprehen-
sion ability. It is well known that many of the ASD
people have a wide range of language difficulties.
Psychological studies showed that they have prob-
lems understanding less common words (Gillispie,
2008), have difficulty comprehending polysemous
words (Fossett and Mirenda, 2006) and have trou-
bles dealing with figurative language (Douglas et al,
2011). The absence of good comprehension skills
impedes the ASD students to participate in curricu-
lum activities or to properly interact with their col-
leagues in chats or blogs. To enhance the reading
comprehension of ASD people we are developing a
software tool. It is built by partners in Academia and
Industry in close collaboration with teams of psy-
chologists and clinicians. It operates in a multilin-
gual setting and is able to process texts in English,
Spanish and Bulgarian languages. Based on litera-
ture research and on a series of studies performed
in the United Kingdom, Spain and Bulgaria with a
variety of autistic patients ranging from children to
adults the psychologists identified a series of obsta-
cles in reading comprehensions that the tool should
remove. From a linguistic point of view they can
be classified in syntactic obstacles (difficulty in pro-
cessing relative clauses, for example) and semantic
obstacles (difficulty in understanding rare or special-
ized terms or in comprehension of idioms, for exam-
ple). The tool applies a series of automatic transfor-
mations to user documents to identify and remove
the reading obstacles to comprehension. It also as-
sists the carers , persons that assist the ASD people
in every day life tasks, to correct the results of auto-
11
matic processing and prepare the documents for the
users. This paper will focus on three essential soft-
ware components related to semantic processing: a
software component that adds images to concepts
in the text, a software component that identifies id-
iomatic expressions and a component that computes
the topics of the document. Moreover, we present
the personalization component that adapts the sys-
tem output to user preferences. The rest of the paper
has the following structure: the next section briefly
presents other similar tools on the market. Section
3 presents a simple procedure for identifying the
obstacles ASD people have in reading comprehen-
sions. Section 4 shows the architecture of the seman-
tic processing components and the personalization
component. The last section draws the conclusions
and comments on the future work. Before present-
ing the main part of the article we make a brief note:
throughout the paper we will use whenever possible
the term ?user? instead of ASD people or patients.
2 Related Work
A number of software tools were developed to sup-
port the learning of ASD people. Probably the
most known one is Mind Reading2, a tool that
teaches human emotions using a library of 412 ba-
sic human emotions illustrated by images and video.
Other well known software is VAST-Autism3, a tool
that supports the understanding of linguistic units:
words, phrase and sentences by combining spoken
language and images. ?Stories about me? is an IPad
application4 that allows early learners to compose
stories about themselves. All these tools and others
from the same category are complementary to Open
Book. However, they are restricted to pre-stored
texts and not able to accommodate new pieces of
information. The main characteristics that sets aside
our tool is its scalability and the fact that it is the only
tool that uses NLP techniques to enhance text com-
prehension. Even if the carers correct the automatic
processing output, part of their work is automatized.
2http://www.jkp.com/mindreading/index.php
3http://a4cwsn.com/2011/03/vast-autism-1-core/
4http://www.limitedcue.com/our-apps/
3 Obstacles in text comprehension
Most of the automatic operations executed by the
Open Book tool are actually manually performed by
the carers. They simplify the parts of the text that are
difficult to understand. We compared the texts be-
fore and after the manual simplification process and
registered the main operations. The main simplifica-
tion operations ordered by frequency performed by
carers for 25 Spanish documents belonging to dif-
ferent genders: rent contracts, newspaper articles,
children literature, health care advices, are the fol-
lowing:
1. Synonymous (64 Operations). A noun or an ad-
jective is replaced by its less complex synonym.
2. Sentence Splitting (40 Operations). A long sen-
tence is split in shorter sentences or in a bullet
list.
3. Definition (34 Operations). A difficult term is
explained using Wikipedia or a dictionary.
4. Near Synonymous (33 Operations). The term
is replaced by a near synonym.
5. Image (27 Operations) A concept is illustrated
by an image.
6. Explanation (24 Operations). A sentence is
rewritten using different words.
7. Deletion (17 Operations). Parts of the sentence
are removed.
8. Coreference(17 Operations). A coreference
resolution is performed.
9. Syntactic Operation (9 Operations). A trans-
formation on the syntactic parse trees is per-
formed.
10. Figurative Language (9 Operations). An idiom
or metaphor is explained.
11. Summarization (3 Operations). The content of
a sentence or paragraph is summarized.
The most frequent operations with the exception
of Sentence Splitting are semantic in nature: replac-
ing a word with a synonym, defining the difficult
12
terms. The only obstacle that cannot be tackled au-
tomatically is Explanation. The Explanation entails
interpretation of the sentence or paragraph and can-
not be reduced to simpler operations.
A similar inventory has been done in English.
Here the most frequent operation are Sentence Split-
ting, Synonyms and Definition. The operations are
similar across English and Spanish but their ordering
differs slightly.
4 The Semantic System
In this paper we focus on three semantic compo-
nents meant to augment the reading experience of
the users. The components enhance the meaning
of documents assigning images to the representa-
tive and difficult concepts, detecting and explaining
the idiomatic expressions or computing the topics to
which the documents belong.
In addition to these components we present an-
other component called Personalization. Strictly
speaking, the personalization is not related to se-
mantic processing per se but, nevertheless, it has
an important role in the final system. Its role
is to aggregate the output of all software compo-
nents,including the three ones mentioned above, and
adapt it according to user?s needs.
All the input and output documents handled by
NLP components are GATE (Cunningham et al,
2011) documents. There are three reasons why
GATE documents are preferred: reusability, extensi-
bility and flexibility. A GATE document is reusable
because there are many software components devel-
oped both in academy and industry, most of them
collected in repositories by University of Sheffield,
that work with this format. A GATE document is
extensible because new components can add their
annotations without modifying previous annotations
or the content of the document. Moreover, in case
there is no dependence between the software com-
ponents the annotations can be added in parallel. Fi-
nally, a GATE document is flexible because it al-
lows the creation of various personalization work-
flows based on the specified attributes of the anno-
tations. The GATE document format is inspired by
TIPSTER architecture design5 and contains in ad-
dition to the text or multimedia content annotations
5http://www.itl.nist.gov/iaui/894.02/related projects/tipster/
grouped in Annotation Sets and features. The GATE
format requires that an annotation has the following
mandatory features: an id, a type and a span. The
span defines the starting and the ending offsets of
the annotation in the document text.
Each developed software component adds its an-
notations in separate name annotation sets. The
components are distributed and exposed to the out-
side world as SOAP web services. Throughout the
rest of the paper we will use interchangeably the
terms: component, software component and web
service.
For each semantic component we discuss:
? The reasons for its development. In general,
there are two reasons for the development of a
certain software component: previous studies
in the literature and studies performed by our
psychologists and clinicians. In this paper we
will give only motivations from previous stud-
ies because the discussion of our clinicians and
psychologist studies are beyond the purpose of
this paper.
? Its architecture. We present both the foreseen
characteristics of the component and what was
actually achieved at this stage but we focus on
the latter.
? The annotations it added. We discuss all the
features of the annotations added by each com-
ponent.
4.1 The Image Web Service
In her landmark book, ?Thinking in Pictures: My
Life with Autism?, Temple Grandin (1996), a scien-
tist affected by ASD, gives an inside testimony for
the importance of pictures in the life of ASD peo-
ple:
?Growing up, I learned to convert abstract ideas
into pictures as a way to understand them. I visu-
alized concepts such as peace or honesty with sym-
bolic images. I thought of peace as a dove, an Indian
peace pipe, or TV or newsreel footage of the signing
of a peace agreement. Honesty was represented by
an image of placing one?s hand on the Bible in court.
A news report describing a person returning a wallet
with all the money in it provided a picture of honest
behavior.?
13
Grandin suggests that not only the ASD people
need images to understand abstract concepts but that
most of their thought process is visual. Other studies
document the importance of images in ASD: Kana
and colleagues (2006) show that the ASD people use
mental imagery even for comprehension of low im-
agery sentences. In an autobiographic study Grandin
(2009) narrates that she uses language to retrieve
pictures from the memory in a way similar to an im-
age retrieval system.
The image component assigns images to concepts
in the text and to concepts summarizing the meaning
of the paragraphs or the meaning of the whole doc-
ument. Currently we are able to assign images to
the concepts in the text and to the topics computed
for the document. Before retrieving the images from
the database we need a procedure for identifying
the difficult concepts. The research literature helps
with this task, too. It says that our users have diffi-
culty understanding less common words (Lopez and
Leekam, 2003) and that they need word disambigua-
tion (Fossett and Mirenda, 2006).
From an architectural point of view the Image
Web Service incorporates three independent sub-
components:
? Document Indexing. The Document Index-
ing sub-component indexes the document con-
tent for fast access and stores all offsets of the
indexing units. The indexed textual units are
words or combinations of words (e.g., terms).
? Difficult Concepts Detection. The difficult
concepts are words or terms (e.g. named enti-
ties) disambiguated against comprehensive re-
sources: like Wordnet and Wikipedia. This
sub-component formalizes the notion ?difficult
to understand? for the users. It should be based
on statistical procedures for identifying rare
terms as well as on heuristics for evaluating the
term complexity from a phonological point of
view. For the time being the sub-component
searches in the document a precompiled list of
terms.
? Image Retrieval. This sub-component re-
trieves the images corresponding to difficult
concepts from image databases or from web
searching engines like Google and Bing.
The Image Web Service operates in automated
mode or in on-demand mode. In the automated
mode a document received by the Image Web Ser-
vice is processed according to the working flow in
Figure 1. In the on-demand mode the user high-
lights the concepts (s)he considers difficult and the
web service retrieves the corresponding image or set
of images. The difference between the two modes
of operations is that in the on-demand mode the dif-
ficult concept detection is performed manually.
Once the GATE document is received by the sys-
tem it is tokenized, POS (Part of Speech) tagged
and lemmatized (if these operations were not already
performed by other component) by a layer that is not
presented in Figure 1. Subsequently, the document
content is indexed by Document Indexing subcom-
ponent. For the time being the terms of the doc-
ument are disambiguated against Wordnet. The Im-
age Retrieval component retrieves the corresponding
images from the image database.
The current version uses the ImageNet Database
(Deng et al, 2009) as image database. The Ima-
geNet database pairs the synsets in Princeton Word-
net with images automatically retrieved from Web
and cleaned with the aid of Mechanical Turk. Be-
cause the wordnets for Spanish and Bulgarian are ei-
ther small or not publicly available future versions of
the Web Service will disambiguate the terms against
Wikipedia articles and retrieve the image illustrating
the article title. All annotations are added in ?Im-
ageAnnotationSet?. An annotation contains the fol-
lowing features:
? Image Disambiguation Confidence is the con-
fidence of the WSD (Word Sense Disambigua-
tion) algorithm in disambiguating a concept.
? Image URL represents the URL address of the
retrieved image
? Image Retrieval Confidence is the confidence
of assigning an image to a disambiguated con-
cept.
In the on-demand mode the images are also re-
trieved from Google and Bing Web Services and
the list of retrieved images is presented to the carer
and/or to the users. The carer or user selects the im-
age and inserts it in the appropriate place in the doc-
ument.
14
Figure 1: The Image Web Service.
4.2 The Idiom Detection Web Service
In the actual linguistic discourse and lexicographical
practice the term ?idiom? is applied to a fuzzy cat-
egory defined by prototypical examples: ?kick the
bucket?, ?keep tabs on?, etc. Because we cannot
provide definitions for idioms we venture to spec-
ify three important properties that characterize them
(Nunberg et al, 1994) :
? Conventionality.The meaning of idioms are not
compositional.
? Inflexibility. Idioms appear in a limited range
of syntactic constructions.
? Figuration. The line between idioms and
other figurative language is somewhat blurred
because other figurative constructions like
metaphors: ?take the bull by the horns? or hy-
perboles: ?not worth the paper it?s printed on?
are also considered idioms.
The figurative language in general and the id-
ioms in particular present particular problems for
our users as they are not able to grasp the meaning
of these expressions (Douglas et al, 2011). To facil-
itate the understanding of idiomatic expressions our
system identifies the expressions and provide defini-
tions for them.
The actual Idiom Web Service finds idiomatic ex-
pressions in the user submitted documents by simple
text matching. The final version of Idiom Web Ser-
vice will use a combination of trained models and
hand written rules for idiom detection. Moreover, it
is also envisaged that other types of figurative lan-
guage like metaphors could be detected. At the mo-
ment the detection is based on precompiled lists of
idioms and their definitions. Because the compo-
nent works by simple text matching, it is language
independent. Unlike the actual version of the Idiom
Web Service the final version should be both lan-
guage and domain dependent. The architecture of
this simple component is presented in Figure 2 .
Figure 2: The Idiom Web Service.
The GATE input document is indexed by the doc-
ument indexing component for providing fast ac-
cess to its content. For each language we compiled
list of idioms from web sources, dictionaries and
Wikipedia. All idiom annotations are added in the
?IdiomAnnotationSet?. An annotation contains the
following features:
? Idiom Confidence represents the confidence the
algorithm assigns to a particular idiom detec-
tion.
? Definition represents the definition for the ex-
tracted idiom.
4.3 The Topic Models Web Service
The mathematical details of the topics models are
somewhat harder to grasp but the main intuition be-
hind is easily understood. Consider an astrobiology
document. Most likely it will talk about at least three
topics: biology, computer models of life and astron-
omy. It will contain words like: cell, molecules, life
related to the biology topic; model, computer, data,
number related to computer models of life topic and
star, galaxy, universe, cluster related with astronomy
topic. The topic models are used to organize vast
15
collections of documents based on the themes or dis-
courses that permeate the collection. From a practi-
cal point of view the topics can be viewed as clus-
ters of words (those related to the three topics in the
example above are good examples) that frequently
co-occur in the collection. The main assumption be-
hind Latent Dirichlet Allocation (LDA) (Blei et al,
2003), the simplest topic model technique, is that
the documents in the collections were generated by a
random process in which the topics are drawn from
a given distribution of topics and words are drawn
from the topics themselves. The task of LDA and
other probabilistic topic models is to construct the
topic distribution and the topics (which are basically
probability distributions over words) starting with
the documents in the collection.
The Topic Models Web Service is based on an
implementation of LDA. It assigns topics to the
user submitted documents, thus informing about the
themes traversing the documents and facilitating the
browsing of the document repository. The topics
themselves perform a kind of summarization of doc-
uments showing, before actual reading experience,
what the document is about.
The architecture of the Topic Models Web Service
is presented in Figure 3.
Figure 3: The Topic Model Web Service.
Once a document is received it is first dispatched
to the Feature Extraction Module where it is POS
tagged and lemmatized and the relevant features are
extracted. As for training models, the features are
all nouns, name entities and verbs in the document.
Then the Topic Inferencer module loads the appro-
priate domain model and performs the inference and
assigns the new topics to the document. There are
three domains/genders that the users of our system
are mainly interested in: News, Health Domain and
Literature. For each of these domains we train topic
models in each of the three languages of the project.
Of course the system is easily extensible to other do-
mains. Adding a new model is simply a matter of
loading it in the system and modifying a configura-
tion file.
The output of the Web System is a document in
the GATE format containing the most important top-
ics and the most significant words in the topics. The
last two parameters can be configured (by default
they are set to 3 and 5 respectively). Unlike the an-
notations for the previous components the annota-
tion for Topic Model Web Service are not added for
span of texts in the original document. This is be-
cause the topics are not necessarily words belonging
to the original document. Strictly speaking the top-
ics are attributes of the original document and there-
fore they are added in the ?GateDocumentFeatures?
section. An example of an output document contain-
ing the section corresponding to the document topics
is given in Figure 4.
Figure 4: The GATE Document Representation of the
Computed Topic Model.
Currently we trained three topic models cor-
responding to the three above mentioned do-
mains/genres for the Spanish language:
? News. The corpus of news contains more
than 500.000 documents downloaded from the
web pages of the main Spanish newspapers (El
Mundo, El Pais, La Razon, etc. . . ). The topic
model is trained using a subset of 50.000 docu-
ments and 400 topics. The optimum number of
documents and topics will be determined when
16
the users test the component. However, one
constraint on the number of documents to use
for model training is the time required to per-
form the inference: if the stored model is too
big then the inference time can exceed the time
limit the users expect.
? Health Domain. The corpus contains 7168
Spanish documents about general health is-
sues (healthy alimentation, description of the
causes and treatments of common diseases,
etc.) downloaded from medlineplus portal. The
topic model is trained with all documents and
100 topics. In the future we will extend both
the corpus and the topic model.
? Literature. The corpus contains literature in
two genders: children literature (121 Spanish
translation of Grimm brothers stories) and 336
Spanish novels. Since for the time being the
corpus is quite small we train a topic model
with 20 topics just for the system testing pur-
poses.
For the English and the Bulgarian language we
have prepared corpora for each domain but we have
not trained a topic model yet. To create the training
model all corpora should be POS tagged, lemma-
tized and the name entities recognized. The features
for training the topic model are all nouns, name en-
tities and verbs in the corpora.
4.4 Personalization
The role of the Personalization Web Service is to
adapt the output of the system to the user?s expe-
rience. This is achieved by building both static and
dynamic user profiles. The static user profiles con-
tain a number of parameters that can be manually
set. Unlike the static profiles, the dynamic ones con-
tain a series of parameters whose values are learnt
automatically. The system registers a series of ac-
tions the users or carers perform with the text. For
example, they can accept or reject the decisions per-
formed by other software components. Based on
editing operations a dynamic user profile will be
built incrementally by the system. Because at this
stage of the project the details of the dynamic pro-
file are not yet fully specified we focus on the static
profile in this section.
The architecture of the Personalization compo-
nent is presented in Figure 5.
Figure 5: The Personalization Web Service.
In addition to the web services presented in the
previous sections (The Idiom Web Service and The
Image Web Service) the Personalization Web Ser-
vice receives input from Anaphora Web Service and
Syntax Simplification Web Service. The Anaphora
component resolves the pronominal anaphora and
the Syntax Simplification component identifies and
eliminates difficult syntactic constructions. The Per-
sonalization component aggregates the input from
all web services and based on the parameters speci-
fied in the static profile (the wheel in Figure 5) trans-
forms the aggregate document according to the user
preferences. The personalization parameters in the
static profile are the following:
1. Image Disambiguation Confidence. The image
annotation is dropped when the corresponding
concept disambiguation confidence is less than
the threshold.
2. Image Retrieval Confidence. The image an-
notation is dropped when the assigned image
is retrieved with a confidence lower than the
threshold.
3. Idiom Confidence. The idiom annotation is
dropped when the assigned idiom confidence is
less than the threshold.
4. Anaphora Confidence. The pronominal
anaphora annotations are dropped when the
anaphor is solved with a confidence less than
the threshold.
5. Anaphora Complexity. The parameter assess
the complexity of anaphors. If the anaphora
17
complexity score is less than the specified
threshold it drops the resolved pronominal
anaphora.
6. Syntactic Complexity. It drops all annotations
for which the syntactic complexity is less than
the threshold.
The user can also reject the entire output of a cer-
tain web service if he does not need the functionality.
For example, the user can require to display or not
the images, to resolve or not the anaphora, to sim-
plify the sentences or not, etc. In case the output of
a certain web service is desired the user can spec-
ify the minimum level of confidence accepted. Any
annotation that has a level of confidence lower than
the specified threshold will be dropped. In addition
to the parameters related to document content the
static profile includes parameters related to graphi-
cal appearance (e.g. fonts or user themes) that are
not discussed here.
5 Conclusions and further work
In this paper we presented three semantic compo-
nents to aid ASD people to understand the texts.
The Image Component finds, disambiguates and as-
signs Images to difficult terms in the text or re-
lated to the text. It works in two modes: auto-
mated or on-demand. In the automated mode a doc-
ument is automatically enriched with images. In
the on-demand mode the user highlights the con-
cepts (s)he considers difficult and the web service
retrieves the corresponding images. Further devel-
opment of this component will involve disambigua-
tion against Wikipedia and retrieval of images from
the corresponding articles. The Idiom Component
finds idioms and other figurative language expres-
sions in the user documents and provides definitions
for them. Further versions of the component will
go beyond simple matching and will identify other
categories of figurative language. The Topic Mod-
els component helps organizing the repository col-
lection by computing topics for the user documents.
Moreover it also offers a summarization of the doc-
ument before the actual reading experience. Finally
the Personalization component adapts the system
output to the user experience. Future versions of the
component will define dynamic user profiles in addi-
tion to the static user profiles in the current version.
Our hope is that the Open Book tool will be useful
for other parts of populations that have difficulties
with syntactic constructions or semantic processing,
too.
Acknowledgments
We want to thank the three anonymous reviewers
whose suggestions helped improve the clarity of this
paper. This work is partially funded by the European
Commission under the Seventh (FP7 - 2007-2013)
Framework Program for Research and Technologi-
cal Development through the FIRST project (FP7-
287607). This publication reflects only the views
of the authors, and the Commission cannot be held
responsible for any use which may be made of the
information contained therein.
References
Tony Attwood. 2007. The complete guide to Asperger
Syndrome. Jessica Kingsley Press.
Simon Baron-Cohen. 2001. Theory of mind and autism:
a review. Int Rev Ment Retard, 23:169?184.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. J. Mach. Learn.
Res., 3:993?1022, March.
Hamish Cunningham, Diana Maynard, Kalina
Bontcheva, Valentin Tablan, Niraj Aswani, Ian
Roberts, Genevieve Gorrell, Adam Funk, An-
gus Roberts, Danica Damljanovic, Thomas Heitz,
Mark A. Greenwood, Horacio Saggion, Johann
Petrak, Yaoyong Li, and Wim Peters. 2011. Text
Processing with GATE (Version 6).
Jia Deng, Wei Dong, R. Socher, Li-Jia Li, Kai Li, and
Li Fei-Fei. 2009. ImageNet: A large-scale hierarchi-
cal image database. In Computer Vision and Pattern
Recognition, 2009. CVPR 2009. IEEE Conference on,
pages 248?255. IEEE, June.
K.H. Douglas, K.M. Ayres, J. Langone, and V.B. Bram-
lett. 2011. The effectiveness of electronic text and
pictorial graphic organizers to improve comprehension
related to functional skills. Journal of Special Educa-
tion Technology, 26(1):43?57.
Brenda Fossett and Pat Mirenda. 2006. Sight word
reading in children with developmental disabilities:
A comparison of paired associate and picture-to-text
matching instruction. Research in Developmental Dis-
abilities, 27(4):411?429.
William Matthew Gillispie. 2008. Semantic Process-
ing in Children with Reading Comprehension Deficits.
Ph.D. thesis, University of Kansas.
18
Temple Grandin. 1996. Thinking In Pictures: and Other
Reports from My Life with Autism. Vintage, October.
Temple Grandin. 2009. How does visual thinking work
in the mind of a person with autism? a personal ac-
count. Philosophical Transactions of the Royal So-
ciety B: Biological Sciences, 364(1522):1437?1442,
May.
Rajesh K. Kana, Timothy A. Keller, Vladimir L.
Cherkassky, Nancy J. Minshew, and Marcel Adam
Just. 2006. Sentence comprehension in autism:
Thinking in pictures with decreased functional con-
nectivity.
B. Lopez and S. R. Leekam. 2003. Do children with
autism fail to process information in context ? Journal
of child psychology and psychiatry., 44(2):285?300,
February.
Geoffrey Nunberg, Ivan Sag, and Thomas Wasow. 1994.
Idioms. Language.
19
