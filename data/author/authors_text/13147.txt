Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1002?1012,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Domain Adaptation of Rule-Based Annotators
for Named-Entity Recognition Tasks
Laura Chiticariu Rajasekar Krishnamurthy Yunyao Li Frederick Reiss Shivakumar Vaithyanathan
IBM Research ? Almaden
650 Harry Road, San Jose, CA 95120, USA
{chiti, rajase, yunyaoli, frreiss}@us.ibm.com, shiv@almaden.ibm.com
Abstract
Named-entity recognition (NER) is an impor-
tant task required in a wide variety of ap-
plications. While rule-based systems are ap-
pealing due to their well-known ?explainabil-
ity,? most, if not all, state-of-the-art results
for NER tasks are based on machine learning
techniques. Motivated by these results, we ex-
plore the following natural question in this pa-
per: Are rule-based systems still a viable ap-
proach to named-entity recognition? Specif-
ically, we have designed and implemented
a high-level language NERL on top of Sys-
temT, a general-purpose algebraic informa-
tion extraction system. NERL is tuned to the
needs of NER tasks and simplifies the pro-
cess of building, understanding, and customiz-
ing complex rule-based named-entity annota-
tors. We show that these customized annota-
tors match or outperform the best published
results achieved with machine learning tech-
niques. These results confirm that we can
reap the benefits of rule-based extractors? ex-
plainability without sacrificing accuracy. We
conclude by discussing lessons learned while
building and customizing complex rule-based
annotators and outlining several research di-
rections towards facilitating rule development.
1 Introduction
Named-entity recognition (NER) is the task of iden-
tifying mentions of rigid designators from text be-
longing to named-entity types such as persons, orga-
nizations and locations (Nadeau and Sekine, 2007).
While NER over formal text such as news articles
and webpages is a well-studied problem (Bikel et
al., 1999; McCallum and Li, 2003; Etzioni et al,
2005), there has been recent work on NER over in-
formal text such as emails and blogs (Huang et al,
2001; Poibeau and Kosseim, 2001; Jansche and Ab-
ney, 2002; Minkov et al, 2005; Gruhl et al, 2009).
The techniques proposed in the literature fall under
three categories: rule-based (Krupka and Hausman,
2001; Sekine and Nobata, 2004), machine learning-
based (O. Bender and Ney, 2003; Florian et al,
2003; McCallum and Li, 2003; Finkel and Manning,
2009; Singh et al, 2010) and hybrid solutions (Sri-
hari et al, 2001; Jansche and Abney, 2002).
1.1 Motivation
Although there are well-established rule-based sys-
tems to perform NER tasks, most, if not all, state-of-
the-art results for NER tasks are based on machine
learning techniques. However, the rule-based ap-
proach is still extremely appealing due to the associ-
ated transparency of the internal system state, which
leads to better explainability of errors (Siniakov,
2010). Ideally, one would like to benefit from the
transparency and explainability of rule-based tech-
niques, while achieving state-of-the-art accuracy.
A particularly challenging aspect of rule-based
NER in practice is domain customization ? cus-
tomizing existing annotators to produce accurate re-
sults in new domains. In machine learning-based
systems, adapting to a new domain has tradition-
ally involved acquiring additional labeled data and
learning a new model from scratch. However, recent
work has proposed more sophisticated approaches
that learn a domain-independent base model, which
can later be adapted to specific domains (Florian et
1002
BASEBALL - MAJOR LEAGUE STANDINGS AFTER TUESDAY 'S GAMES NEW YORK 1996-08-28?
?AMERICAN LEAGUE EASTERN DIVISION W L PCT GB NEW YORK 74 57 .565 -BALTIMORE 70 61 .534 4 BOSTON 68 65 .511 7 
?TEXAS AT KANSAS CITYBOSTON AT CALIFORNIANEW YORK AT SEATTLE
?
BASEBALL - ORIOLES WIN , YANKEESLOSE . BALTIMORE 1996-08-27
?In Seattle , Jay Buhner 's eighth-inning single snapped a tie as the Seattle Mariners edged the New York Yankees 2-1 in the opener of a three-game series .New York starter Jimmy Key left the game in the first inning after Seattle shortstop Alex Rodriguez lined a shot off his left elbow .
?
Document d1 Document d2
Customization Requirement : City, County or State names within sports articles may refer to a sports team  or to the location itself.Customization Solution (CS) :Within sports articles, Identify all occurrences of city/county/state as Organizations, Except when a contextual clue indicates that the reference is to the location
Organization Location
Figure 1: Example Customization Requirement
al., 2004; Blitzer et al, 2006; Jiang and Zhai, 2006;
Arnold et al, 2008; Wu et al, 2009). Implement-
ing a similar approach for rule-based NER typically
requires a significant amount of manual effort to (a)
identify the explicit semantic changes required for
the new domain (e.g., differences in entity type def-
inition), (b) identify the portions of the (complex)
core annotator that should be modified for each dif-
ference and (c) implement the required customiza-
tion rules without compromising the extraction qual-
ity of the core annotator. Domain customization of
rule-based NER has not received much attention in
the recent literature with a few exceptions (Petasis et
al., 2001; Maynard et al, 2003; Zhu et al, 2005).
1.2 Problem Statement
In this paper, we explore the following natural ques-
tion: Are rule-based systems still a viable approach
to named-entity recognition? Specifically, (a) Is it
possible to build, maintain and customize rule-based
NER annotators that match the state-of-the-art re-
sults obtained using machine-learning techniques?
and (b) Can this be achieved with a reasonable
amount of manual effort?
1.3 Contributions
In this paper, we address the challenges mentioned
above by (i) defining a taxonomy of the different
types of customizations that a rule developer may
perform when adapting to a new domain (Sec. 2), (ii)
identifying a set of high-level operations required
for building and customizing NER annotators, and
(iii) exposing these operations in a domain-specific
NER rule language, NERL, developed on top of Sys-
// Core rules identify Organization and Location candidates
// Begin customization// Identify articles covering sports event from article title CR1 <SportsArticle>  Evaluate Regular Expressions <R1>// Identify locations in sports articlesCR2 Retain <Location> As <LocationMaybeOrg> If ContainedWithin <SportsArticle>// City/County/State references (e.g., New York) may refer to the sports team in that cityCR3 Retain <LocationMaybeOrg> If Matches Dictionaries<?cities.dict?,?counties.dict?,?states.dict?>
// Some city references in sports articles may refer to the city (e.g., In Seattle )// These references should not be reclassified as OrganizationCR4 Discard <LocationMaybeOrg> If Matches Regular Expression <R2>on Left Context 2 Tokens
// City references to sports teams are added to Organization and removed from LocationCR5 Augment <Organization> With <LocationMaybeOrg>// End customization
// Continuation of core rules// Remove Locations that overlap with OrganizationsDiscard <Location> If Overlaps Concepts <Organization>Figure 2: Example Customization Rules in NERL
temT (Chiticariu et al, 2010), a general-purpose
algebraic information extraction system (Sec. 3).
NERL is specifically geared towards building and
customizing complex NER annotators and makes it
easy to understand a complex annotator that may
comprise hundreds of rules. It simplifies the iden-
tification of what portions need to be modified for
a given customization requirement. It also makes
individual customizations easier to implement, as il-
lustrated by the following example.
Suppose we have to customize a domain-
independent rule-based NER annotator for the
CoNLL corpus (Tjong et al, 2003). Consider the
two sports-related news articles in Fig. 1 from the
corpus, where city names such as ?New York? or
?Seattle? can refer to either a Location or an Orga-
nization (the sports team based in that city). In the
domain-independent annotator, city names were al-
ways identified as Location, as this subtle require-
ment was not considered during rule development.
A customization to address this issue is shown in
Fig. 1, which can be implemented in NERL with five
rules (Fig. 2). This customization (explained in de-
tail in Sec. 3) improved the F?=1 score for Organi-
zation and Location by approximately 9% and 3%,
respectively (Sec. 4).
We used NERL to customize a domain-
independent rule-based NER annotator for three
different domains ? CoNLL03 (Tjong et al, 2003),
Enron (Minkov et al, 2005) and ACE05 (NIST,
2005). Our experimental results (Sec. 4.3) demon-
strate that the customized annotators have extraction
quality better than the best-known results for
1003
Affects Single Affects Multiple
Entity Type Entity Types
Identify New Instances CS , CDD , CDSD BR
Modify Existing instances CEB , CDD CATA, CG
Table 1: Categorizing NER Customizations
individual domains, which were achieved with
machine learning techniques. The fact that we are
able to achieve such results across multiple domains
answers our earlier question and confirms that
we can reap the benefits of rule-based extractors?
explainability without sacrificing accuracy.
However, we found that even using NERL, the
amount of manual effort and expertise required in
rule-based NER may still be significant. In Sec. 5,
we report on the lessons learned and outline several
interesting research directions towards simplifying
rule development and facilitating the adoption of the
rule-based approach towards NER.
2 Domain Customization for NER
We consider NER tasks following the broad defini-
tion put forth by (Nadeau and Sekine, 2007), for-
mally defined as follows:
Definition 1 Named entity recognition is the task of
identifying and classifying mentions of entities with
one or more rigid designators, as defined by (Kripke,
1982).
For instance, the identification of proper nouns
representing persons, organizations, locations, prod-
uct names, proteins, drugs and chemicals are consid-
ered as NER tasks.
Based on our experience of customizing NER an-
notators for multiple domains, we categorize the
customizations involved into two main categories as
listed below. This categorization motivates the de-
sign of NERL (Sec. 3).
Data-driven (CDD): The most common NER cus-
tomization is data-driven, where the customizations
mostly involve the addition of new patterns and
dictionary entries, driven by observations from the
training data in the new domain. An example is
the addition of a new rule to identify locations from
the beginning of news articles (e.g., ?BALTIMORE
1995-08-27? and ?MURCIA , Spain 1996-09-10?).
Application-driven: What is considered a valid
named entity and its corresponding type can vary
across application domains. The most common di-
mensions on which the definition of a named entity
can vary are:
Entity Boundary (CEB): Different application do-
mains may have different definitions of where the
same entity starts or ends. For example, a Person
may (CoNLL03) or may not (Enron) include gener-
ational markers (e.g. ?Jr.? in ?Bush Jr.? or ?IV? in
?Henry IV?).
Ambiguous Type Assignment (CATA): The exact
type of a given named entity can be ambiguous.
Different applications may assign different types
for the same named entity. For instance, all in-
stances of ?White House? may be considered as Lo-
cation (CoNLL03), or be assigned as Facility or Or-
ganization based on their context (ACE05). In fact,
even within the same application domain, entities
typically considered as of the same type may be as-
signed differently. For example, given ?New York
beat Seattle? and ?Ethiopia beat Uganda?, both
?New York? and ?Ethiopia? are teams referred by their
locations. However, (Tjong et al, 2003) considers
the former, which corresponds to a city, as an Orga-
nization, and the latter, which corresponds to a coun-
try, as a Location.
Domain-Specific Definition (CDSD): Whether a
given term is even considered a named entity may
depend on the specific domain. As an example, con-
sider the text ?Commercialization Meeting - SBeck,
BHall, BSuperty, TBusby, SGandhi-Gupta?. Infor-
mal names such as ?SBeck? and ?BHall? may be con-
sidered as valid person names (Enron).
Scope(CS): Each type of named entity usually con-
tains several subtypes. For the same named en-
tity task, different applications may choose to in-
clude different sets of subtypes. For instance,
roads and buildings are considered part of Location
in CoNLL03, while they are not included in ACE05.
Granularity(CG): Name entity types are hierarchi-
cal. Different applications may define NER tasks
at different granularities. For instance, in ACE05,
Organization and Location entity types were split
into four entity types (Organization, Location, Geo-
Political Entity and Facility).
The different customizations are summarized as
shown in Tab. 1, based on the following criteria: (i)
whether the customization identifies new instances
or modifies existing instances; and (ii) whether the
1004
customization affects single or multiple entities. For
instance, CS identifies new instances for a single en-
tity type, as it adds instances of a new subtype for an
existing entity type. Note that BR in the table de-
notes the rules used to build the core annotator.
3 Named Entity Rule Language
3.1 Grammar vs. Algebraic NER
Traditionally, rule-based NER systems were based
on the popular CPSL cascading grammar specifi-
cation (Appelt and Onyshkevych, 1998). CPSL is
designed so that rules that adhere to the standard
can be executed efficiently with finite state transduc-
ers. Accordingly, the standard defines a rigid left-to-
right execution model where a region of text can be
matched by at most one rule according to a fixed rule
priority, and where overlapping annotations are dis-
allowed in the output of each grammar phase.
While it simplifies the design of CPSL engines,
the rigidity of the rule matching semantics makes
it difficult to express operations frequently used in
rule-based information extraction. These limitations
have been recognized in the literature, and several
extensions have been proposed to allow more flex-
ible matching semantics, and to allow overlapping
annotations (Cunningham et al, 2000; Boguraev,
2003; Drozdzynski et al, 2004). However, even
with these extensions, common operations such as
filtering annotations (e.g. CR4 in Fig. 2), are dif-
ficult to express in grammars and often require an
escape to custom procedural code.
Recently, several declarative algebraic languages
have been proposed for rule-based IE systems, no-
tably AQL (Chiticariu et al, 2010) and Xlog (Shen
et al, 2007). These languages are not constrained
by the requirement that all rules map onto finite state
transducers, and therefore can express a significantly
richer semantics than grammar-based languages. In
particular, the AQL rule language as implemented in
SystemT (Chiticariu et al, 2010) can express many
common operations used in rule-based information
extraction without requiring custom code. In addi-
tion, the separation of extraction semantics from ex-
ecution enables SystemT?s rule optimizer and effi-
cient runtime engine. Indeed, as shown in (Chiti-
cariu et al, 2010), SystemT can deliver an order of
magitude higher annotation throughput compared to
a state-of-the-art CPSL-based IE system.
Since AQL is a general purpose information ex-
traction rule language, similar to CPSL and JAPE,
it exposes an expressive set of capabilities that go
beyond what is required for NER tasks. These ad-
ditional capabilities can make AQL rules more ver-
bose than is necessary for implementing rules in the
NER domain. For example, Fig. 3 shows how the
same customization rule CR4 from Fig. 2 can be
implemented in JAPE or in AQL. Notice how im-
plementing even a single customization may lead to
defining complex rules (e.g. JAPE-R1, AQL-R1)
and sometimes even using custom code (e.g. JAPE-
R2). As illustrated by this example, the rules in AQL
and JAPE tend to be complex since some operations
? e.g., filtering the outputs of one rule based on the
outputs of another rule ? that are common in NER
rule sets require multiple rules in AQL or multiple
grammar phases in JAPE.
To make NER rules easier to develop and to
understand, we designed and implemented Named
Entity Rule Language (NERL) on top of SystemT.
NERL is a declarative rule language designed specif-
ically for named entity recognition. The design of
NERL draws on our experience with building and
customizing multiple complex NER annotators. In
particular, we have identified the operations required
in practice for such tasks, and expose these opera-
tions as built-in constructs in NERL. In doing so, we
ensure that frequently performed operations can be
expressed succinctly, so as not to complicate the rule
set unnecessarily. As a result, NERL rules for named
entity recognition tasks are significantly more com-
pact and easy to understand than the equivalent AQL
rules. At the same time, NERL rules can easily be
compiled to AQL, allowing our NER rule develop-
ment framework to take advantage of the capabilities
of the SystemT rule optimizer and efficient runtime
execution engine.
3.2 NERL
For the rest of this section, we focus on describ-
ing the types of rules supported in NERL. In Sec. 4,
we shall demonstrate empirically that NERL can be
successfully employed in building and customizing
complex NER annotators.
A NERL rule has the following form:
IntConcept ? RuleBody(IntConcept1, IntConcept2, . . .)
1005
// Some city references in sports articles may refer to the city (e.g., In Seattle )// These references should not be reclassified as OrganizationCR4 Discard <LocationMaybeOrg> If Matches Regular Expression <R2> on Left Context 2 Tokens
Rule in NERL
JAPE Phase 1Rule : AmbiguousLocationContext
({Token}[2]):context({AmbiguousLoc}): annot
  :annot.AmbiguousLoc = {lc = context.string}
JAPE Phase 2Rule : RetainValidLocation
({AmbiguousLoc.lc =~ R2}):ambiguousloc -->{  // rule to discard ambiguous locationsAnnotationSet loc = bindings.get(?ambiguousloc");
outputAS.removeAll(loc); }
Rule : RetainValidLocation({Token}[2]):context({AmbiguousLoc}):loc    -->{   // Action part in Java to test R2 on left context // and delete annotationAnnotationSet loc = bindings.get(?loc");AnnotationSet context = bindings.get(?context");int begOffset = context.firstNode().getOffset().intValue(); int endOffset = context.lastNode().getOffset().intValue(); String mydocContent = doc.getContent().toString(); String contextString =
mydocContent.substring(begOffset, endOffset);if (Pattern.matches(?R2?, contextString)) {
outputAS.removeAll(loc); }}
create view LocationMaybeOrgInvalid as
select LMO.value as valuefrom LocationMaybeOrg LMO 
where MatchesRegex(/R2/,LeftContextTok(LMO.value,2));
create view LocationMaybeOrgValid as(select LMO.value as value from LocationMaybeOrg LMO)
minus(select LMOI.value as value from LocationMaybeOrgInvalid LMOI);
Two Alternative Rule sets in JAPE Equivalent Rule set in AQL
JAPE-R1 JAPE-R2 AQL-R1
Figure 3: Single Customization Rule expressed in NERL, JAPE and AQL
Intuitively, a NERL rule creates an intermediate con-
cept or named entity (IntConcept for short) by ap-
plying a NERL rule on the input text and zero or
more previously defined intermediate concepts.
NERL Rule Types The types of rules supported in
NERL are summarized in Tab. 2. In what follows,
we illustrate these types by means of examples.
Feature definition (FD): FD rules identify basic
features from text (e.g., FirstName, LastName and
CapsWord features for identifying person names).
Candidate definition (CD): CD rules identify com-
plete occurrences of the target entity. For instance,
the Sequence rule ?LastName followed by ?,? fol-
lowed by FirstName? identifies person annotations
as a sequence of three tokens, where the first and
third tokens occur in dictionaries containing last and
first names.
Candidate Refinement (CR): CR rules are used to
refine candidates generated for different annotation
types. E.g., the Filter rule CR3 in Fig. 2 retains Loca-
tionMaybeOrg annotations that appear in one of sev-
eral dictionaries.
Consolidation (CO): CO rules are used to resolve
overlapping candidates generated by multiple CD
rules. For instance, consider the text ?Please see
the following request from Dr. Kenneth Lim of the
BAAQMD.?. A CD rule may identify ?Dr. Kenneth
Lim? as a person, while another CD rule may identify
?Kenneth Lim? as a candidate person. A consolidation
rule is then used to merge these two annotations to
produce a single annotation for ?Dr. Kenneth Lim?.
NERL Examples Within these categories, three
types of rules deserve special attention, as they cor-
respond to frequently used operations and are specif-
ically designed to ensure compactness of the rule-
set. In contrast, as discussed earlier (Fig. 3), each of
these operations require several rules and possibly
custom code in existing rule-based IE systems.
DynamicDict: The DynamicDict rule is used to create
customized gazetteers on the fly. The following ex-
ample shows the need for such a rule: While ?Clin-
ton? does not always refer to a person?s last name
(Clinton is the name of several cities in USA), in
documents containing a full person name with ?Clin-
ton? as a last name (e.g., ?Hillary Clinton?) it is rea-
sonable to annotate all references to the (possibly)
ambiguous word ?Clinton? as a person. This goal
can be accomplished using the rule <Create Dynamic
Dictionary using Person with length 1 to 2 tokens>,
which creates a gazetteer on a per-document basis.
Filter: The Filter rule is used to discard/retain cer-
tain intermediate annotations based on predicates on
the annotation text and its local context. Example
filtering predicates include
? Discard C If Matches Regular Expression R
? Retain C If Contains Dictionary D on Local Context LC
? Discard C If Overlaps Concepts C1, C2, . . .
ModifySpan: The ModifySpan rule is used to expand
or trim the span of a candidate annotation. For
instance, an Entity Boundary customization to in-
clude generational markers as part of a Person anno-
tation can be implemented using a ModifySpan rule
<Expand Person Using Dictionary ?generation.dict? on
RightContext 2 Tokens>.
Using NERL Tab. 2 shows how different types of
rules are used during rule building and customiza-
tions. Since BR and CS involve identifying one
1006
Rule Category Syntax BR CDD CG
CS CDSD CEB CATA
Dictionary FD Evaluate Dictionaries < D1, D2, . . . > with flags? X X
Regex FD Evaluate Regular Expressions < R1, R2, . . . > with flags? X X
PoS FD Evaluate Part of Speech < P1, P2, . . . > with language < L >? X X
DynamicDict FD Create Dynamic Dictionary using IntConcept with flags? X X
Sequence CD IntConceptorString multiplicity?
(followed by IntConceptorString multiplicity?)+ X X
Filter CR Discard/Retain IntConcept(As IntConcept)?
If SatisfiesPredicate on LocalContext X X X
ModifySpan CR Trim/Expand IntConcept Using Dictionary < D >
on LocalContext X X
Augment CO Augment IntConcept With IntConcept X X
Consolidate CO Consolidate IntConcept using ConsolidationPolicy X X
Table 2: Description of rules supported in NERL
or more entity (sub)types from scratch, all types
of rules are used. CDD and CDSD identify addi-
tional instances for an existing type and therefore
mainly rely on FD and CD rules. On the other hand,
the customizations that modify existing instances
(CEB ,CATA,CG) require CR and CO rules.
Revisiting the example in Fig. 2, CR rules were
used to implement a fairly sophisticated customiza-
tion in a compact fashion, as follows. Rule CR1
first identifies sports articles using a regular expres-
sion based on the article title. Rule CR2 marks
Locations within these articles as LocationMaybeOrg
and Rule CR3 only retains those occurrences that
match a city, county or state name (e.g., ?Seattle?).
Rule CR4 identifies occurrences that have a contex-
tual clue confirming that the mention was to a lo-
cation (e.g., ?In? or ?At?). These occurrences are al-
ready classified correctly as Location and do not need
to be changed. Finally, CR5 adds the remaining am-
biguous mentions to Organization, which would be
deleted from Location by a subsequent core rule.
4 Development and Customization of NER
extractors with NERL
Using NERL, we have developed CoreNER, a
domain-independent generic library for multiple
NER extraction tasks commonly encountered in
practice, including Person, Organization, Location,
EmailAddress, PhoneNumber, URL, and DateTime, but
we shall focus the discussion on the first three tasks
(see Tab. 3 for entity definitions), since they are the
most challenging. In this section, we first overview
the process of developing CoreNER (Sec. 4.1). We
then describe how we have customized CoreNER
for three different domains (Sec. 4.2), and present
a quality comparison with best published results ob-
tained with state-of-the-art machine learning tech-
niques (Sec. 4.3). The tasks we consider are not re-
stricted to documents in a particular language, but
due to limited availability of non-English corpora
and extractors for comparison, our evaluation uses
English-language text. In Sec. 5 we shall elaborate
on the difficulties encountered while building and
customizing CoreNER using NERL and the lessons
we learned in the process.
4.1 Developing CoreNER
We have built our domain independent CoreNER li-
brary using a variety of formal and informal text
(e.g. web pages, emails, blogs, etc.), and informa-
tion from public data sources such as the US Census
Bureau (Census, 2007) and Wikipedia.
The development process proceeded as follows.
We first collected dictionaries for each entity
type from different resources, followed by man-
ual cleanup when needed to categorize entries col-
lected into ?strong? and ?weak? dictionaries. For
instance, we used US Census data to create several
name dictionaries, placing ambiguous entries such
as ?White? and ?Price? in a dictionary of ambigu-
ous last names, while unambiguous entries such as
?Johnson? and ?Williams? went to the dictionary for
strict last names. Second, we developed FD and
CD rules to identify candidate entities based on the
way named entities generally occur in text. E.g.,
<Salutation CapsWord CapsWord> and <FirstName
1007
Type Subtypes
PER individual
LOC
Address, Boundary, Land-Region-Natural, Region-General,
Region-International, Airport, Buildings-Grounds, Path, Plant,
Subarea-Facility, Continent, Country-or-District, Nation,
Population-Center, State-or-Province
ORG Commercial, Educational, Government, Media, Medical-ScienceNon-Governmental
Table 3: NER Task Types and Subtypes
LastName> for Person, and <CapsWord{1,3} OrgSuf-
fix> and <CapsWord{1,2} Industry> for Organization.
We then added CR and CO rules to account for
contextual clues and overlapping annotations (e.g.,
Delete Person annotations appearing within an Orga-
nization annotation).
The final CoreNER library consists of 104 FD (in-
volving 68 dictionaries, 33 regexes and 3 dynamic
dictionaries), 74 CD, 123 CR and 102 CO rules.
4.2 Customizing CoreNER
In this section we describe the process of customiz-
ing our domain-independent CoreNER library for
several different datasets. We start by discussing our
choice of datasets to use for customization.
Datasets For a rigorous evaluation of CoreNER?s
customizability, we require multiple datasets satis-
fying the following criteria: First, the datasets must
cover diverse sources and styles of text. Second,
the set of the most challenging NER tasks Person,
Organization and Location (see Tab. 3) considered
in CoreNER should be applicable to them. Finally,
they should be publicly available and preferably
have associated published results, against which we
can compare our experimental results. Towards this
end, we chose the following public datasets.
? CoNLL03 (Tjong et al, 2003): a collection of
Reuters news stories. Consists of formal text.
? Enron (Minkov et al, 2005): a collection of
emails with meeting information from the Enron
dataset. Contains predominantly informal text.
? ACE05 (NIST, 2005)1 a collection of broadcast
news, broadcast conversations and newswire re-
ports. Consists of both formal and informal text.
Customization Process The goal of customization
1The evaluation test set is not publicly available. Thus, fol-
lowing the example of (Florian et al, 2006), the publicly avail-
able set is split into a 80%/20% data split, with the last 20% of
the data in chronological order selected as test data.
is to refine the original CoreNER (hence referred
to as CoreNERorig) in order to improve its extrac-
tion quality on the training set (in terms of F?=1)
for each dataset individually. In addition, a devel-
opment set is available for CoNLL03 (referred to as
CoNLL03dev), therefore we seek to improve F?=1 on
CoNLL03dev as well.
The customization process for each dataset pro-
ceeded as follows. First, we studied the entity defini-
tions and identified their differences when compared
with the definitions used for CoreNERorig (Tab. 3).
We then added rules to account for the differences.
For example, the definition of Organization in the
CoNLL03 dataset contained a sports organization
subtype, which was not considered when develop-
ing CoreNER. Therefore, we have used public data
sources (e.g., Wikipedia) to collect and curate dic-
tionaries of major sports associations and sport clubs
from around the world. The new dictionaries, along
with regular expressions identifying sports teams in
sports articles were used for defining FD and CD
rules such as CR1 (Fig. 2). Finally, CR and CO rules
were added to filter invalid candidates and augment
the Organization type with the new sports subtype
(similar in spirit to rules CR4 and CR5 in Fig. 2).
In addition to the train and development sets, the
customization process for CoNLL03 also involved
unlabeled data from the corpus as follows. 1) Since
data-driven rules (CDD) are often created based on a
few instances from the training data, testing them on
the unlabeled data helped fine tune the rules for pre-
cision. 2) CoNLL03 is largely dominated by sports
news, but only a subset of all sports were represented
in the train dataset. Using the unlabeled data, we
were able to add CDD rules for five additional types
of sports, resulting in 0.31% improvement in F?=1
score on CoNLL03dev. 3) Unlabeled data was also
useful in identifying domain-specific gazetteers by
using simple extraction rules followed by a man-
ual cleanup phase. For instance, for CoNLL03 we
collected five gazetteers of organization and person
names from the unlabeled data, resulting in 0.45%
improvement in recall for CoNLL03dev.
The quality of the customization on the train col-
lections is shown in Tab. 5. The total number of
rules added during customization for each of the
three domains is listed in Tab. 4. Notice how rules
of all four types are used both in the development
1008
FD CD CR CO
CoreNERorig 104 74 123 102
CoreNERconll 179 56 284 71
CoreNERenron 13 10 9 1
CoreNERace 83 35 117 26
Table 4: Rules added during customization
Precision Recall F?=1
CoreNERconll 97.64 95.60 96.61
CoreNERenron 91.15 92.58 91.86
CoreNERace 92.32 91.22 91.77
Table 5: Quality of customization on train datasets (%)
of the domain independent NER annotator, and dur-
ing customizations for different domains. A total of
8 person weeks were spent on customizations, and
we believe this effort is quite reasonable by rule-
based extraction standards. For example, (Maynard
et al, 2003) reports that customizing the ANNIE
domain independent NER annotator developed us-
ing the JAPE grammar-based rule language for the
ACE05 dataset required 6 weeks (and subsequent
tuning over the next 6 months), resulting in im-
proving the quality to 82% for this dataset. As we
shall discuss shortly, with similar manual effort, we
were able to achieve results outperforming state-of-
art published results on three different datasets, in-
cluding ACE05. However, one may rightfully ar-
gue that the process is still too lengthy impeding the
widespread deployment of rule-based NER extrac-
tion. We elaborate on the effort involved and the
lessons learned in the process in Sec. 5.
4.3 Evaluation of Customization
We now present an experimental evaluation of the
customizability of CoreNER. The main goals are
to investigate: (i) the feasibility of CoreNER cus-
tomization for different application domains; (ii)
the effectiveness of such customization compared to
state-of-the-art results; (iii) the impact of different
types of customization (Tab. 1); and (iv) how often
different categories of NERL rules (Tab. 2) are used
during customization.
We measured the effectiveness of customization
using the improvement in extraction quality of the
customized CoreNER over CoreNERorig. As shown
in Tab. 6, customization significantly improved
Precision Recall F?=1
CoNLL03dev
CoreNERorig 83.81 61.77 71.12
CoreNERconll 96.49 93.76 95.11
Improvement 12.68 31.99 13.99
CoNLL03test
CoreNERorig 77.21 54.87 64.15
CoreNERconll 93.89 89.75 91.77
Improvement 15.68 34.88 27.62
Enron
CoreNERorig 85.06 69.55 76.53
CoreNERenron 88.41 82.39 85.29
Improvement 3.35 12.84 8.76
ACE2005
CoreNERorig 57.23 57.41 57.32
CoreNERace 90.11 87.82 88.95
Improvement 32.88 30.41 31.63
Table 6: Overall Improvement due to Customization (%)
Precision Recall F?=1
LOC CoreNERconll 97.17 95.37 96.26
CoNLL03dev
Florian 96.59 95.65 96.12
ORG CoreNERconll 93.70 88.67 91.11Florian 90.85 89.63 90.24
PER CoreNERconll 97.79 95.87 96.82Florian 96.08 97.12 96.60
LOC CoreNERconll 93.11 91.61 92.35
CoNLL03test
Florian 90.59 91.73 91.15
ORG CoreNERconll 92.25 85.31 88.65Florian 85.93 83.44 84.67
PER CoreNERconll 96.32 92.39 94.32Florian 92.49 95.24 93.85
Enron PER CoreNERenron 87.27 81.82 84.46Minkov 81.1 74.9 77.9
Table 7: Comparison with state-of-the-art results(%)
F?=1 score for CoreNERorig across all datasets. 2
We note that the extraction quality of
CoreNERorig was low on CoNLL03 and ACE05
mainly due to differences in entity type definitions.
In particular, sports organizations, which occurred
frequently in the CoNLL03 collection, were not
considered during the development of CoreNERorig,
while in ACE05, ORG and LOC entity types were
split into four entity types (Organization, Location,
Geo-Political Entity and Facility). Customizations
such as CS and CG address the above changes
in named-entity type definition and substantially
improve the extraction quality of CoreNERorig.
Next, we compare the extraction quality of the
2CoNLL03dev and CoNLL03test correspond to the develop-
ment and test sets for CoNLL03 respectively.
1009
customized CoreNER for CoNLL03 and Enron3 with
the corresponding best published results by (Florian
et al, 2003) and (Minkov et al, 2005). Tab. 7 shows
that our customized CoreNER outperforms the cor-
responding state-of-the-art numbers for all the NER
tasks on both CoNLL03 and Enron. 4 These results
demonstrate that high-quality annotators can be built
by customizing CoreNERorig using NERL, with the
final extraction quality matching that of state-of-the-
art machine learning-based extractors.
It is worthwhile noting that the best pub-
lished results for CoNLL03 (Florian et al, 2003)
were obtained by using four different classifiers
(Robust Risk Minimization, Maximum Entropy,
Transformation-based learning, and Hidden Markov
Model) and trying six different classifier combi-
nation methods. Compared to the best published
result obtained by combining the four classifiers,
the individual classifiers performed between 2.5-
7.6% worse for Location, 5.6-15.2% for Organiza-
tion and 3.9-14.0% for Person5. Taking this into
account, the extraction quality advantage of cus-
tomized CoreNER is significant when compared
with the individual state-of-the-art classifiers.
Impact of Customizations by Type. While cus-
tomizing CoreNER for the three datasets, all types
of changes described in Sec. 2 were performed. We
measured the impact of each type of customization
by comparing the extraction quality of CoreNERorig
with CoreNERorig enhanced with all the customiza-
tions of that type. From the results for CoNLL03
(Tab. 8), we make the following observations.
? Customizations that identify additional subtypes
of entities (CS) or modify existing instances for
multiple types (CATA) have significant impact.
This effect can be especially high when the miss-
ing subtype appears very often in the new do-
main (E.g., over 50% of the organizations in
CoNLL03 are sports teams).
? Data-driven customizations (CDD) rely on the
aggregated impact of many rules. While individ-
ual rules may have considerable impact on their
3We cannot meaningfully compare our results against previ-
ously published results for ACE05, which is originally used for
mention detection while CoreNER considers only NER tasks.
4For Enron the comparison is reported only for Person, as
labeled data is available only for that type.
5Extended version obtained via private communication.
# rules added Precision Recall F?=1
CEB 3
CoNLL03dev
LOC ?0.21 ?0.22 ?0.22
ORG ?1.35 ?0.38 ?0.59
PER - - -
CoNLL03test
LOC ?0.30 ?0.36 ?0.33
ORG ?0.54 ?0.12 ?0.20
PER - - -
CATA 5
CoNLL03dev
LOC ?7.18 ?0.87 ?3.19
ORG ?1.37 ?10.67 ?9.04
PER ?0.04 - ?0.01
CoNLL03test
LOC ?7.73 ?1.20 ?3.77
ORG ?1.37 ?11.62 ?14.18
PER - - -
CDSD 2
CoNLL03dev
LOC ?0.85 - ?0.45
ORG ?1.00 ?0.07 ?0.01
PER - - -
CoNLL03test
LOC ?0.04 ?0.12 ?0.12
ORG ?0.64 - ?0.04
PER - - -
CS 149
CoNLL03dev
LOC ?1.63 ?0.21 ?0.85
ORG ?11.44 ?40.79 ?39.73
PER ?0.13 - ?0.05
CoNLL03test
LOC ?3.71 ?0.18 ?2.05
ORG ?9.2 ?36.24 ?37.96
PER ?0.58 - ?0.2
CDD 431
CoNLL03dev
LOC ?0.94 ?10.18 ?3.99
ORG ?9.63 ?11.93 ?14.71
PER ?6.12 ?28.5 ?18.84
CoNLL03test
LOC ?1.66 ?6.72 ?1.64
ORG ?8.84 ?12.40 ?15.90
PER ?9.15 ?31.48 ?22.21
Table 8: Impact by customization type on CoNLL03(%)
own (e.g., identifying all names that appear as
part of a player list increases the recall of PER by
over 6% on both CoNLL03dev and CoNLL03test),
the overall impact relies on the accumulative ef-
fect of many small improvements.
? Certain customizations (CEB and CDSD) pro-
vide smaller quality improvements, both per rule
and in aggregate.
5 Lessons Learned
Our experimental evaluation shows that rule-based
annotators can achieve quality comparable to that of
state-of-the-art machine learning techniques. In this
section we discuss three important lessons learned
regarding the human effort involved in developing
such rule-based extractors.
Usefulness of NERL We found NERL very helpful
in that it provided a higher-level abstraction catered
specifically towards NER tasks, thus hiding the com-
plexity inherent in a general-purpose IE rule lan-
guage. In doing so, NERL restricts the large space
of operations possible within a general-purpose lan-
guage to the small number of predefined ?templates?
1010
listed in Tab. 2. (We have shown empirically that our
choice of NERL rules is sufficient to achieve high
accuracy for NER tasks.) Therefore, NERL simpli-
fies development and maintenance of complex NER
extractors, since one does not need to worry about
multiple AQL statements or JAPE grammar phases
for implementing a single conceptual operation such
as filtering (see Fig. 3).
Is NERL Sufficient? Even using NERL, building
and customizing NER rules remains a labor inten-
sive process. Consider the example of designing the
filter rule CR4 from Fig. 3. First, one must exam-
ine multiple false positive Location entities to even
decide that a filter rule is appropriate. Second, one
must understand how those false positives were pro-
duced, and decide accordingly on the particular con-
cept to be used as filter (LocationMaybeOrg in this
case). Finally, one needs to decide how to build the
filter. Tab. 9 lists all the attributes that need to be
specified for a Filter rule, along with examples of
the search space for each rule attribute.
Rule Attributes Examples of Search Space
Location Intermediate Concept to filter
Predicate Type Matches Regex, Contains Dictionary, . . .
Predicate Parameter Regular Expressions, Dictionary Entries, . . .
Context Type Entity text, Left or Right context
Context Parameter k tokens, l characters
Table 9: Search space explored while adding a Filter rule
This search space problem is not unique to filter
rules. In fact, most rules in Tab. 2 have two or more
rule attributes. Therefore, designing an individual
NERL rule remains a time-consuming ?trial and er-
ror? process, in which multiple ?promising? combi-
nations are implemented and evaluated individually
before deciding on a satisfactory final rule.
Tooling for NERL The fact that NERL is a high-
level language exposing a restricted set of operators
can be exploited to reduce the human effort involved
in building NER annotators by enabling the follow-
ing tools:
Annotation Provenance Tools tracking prove-
nance (Cheney et al, 2009) for NERL rules can
help in explaining exactly which sequence of rules
is responsible for producing a given false positive,
thereby enabling one to quickly identify ?misbe-
haved? rules. For instance, one can quickly narrow
down the choices for the location where the filter
rule CR4 (Fig. 2) should be applied based on the
provenance of the false positives. Similarly, tools
for explaining false positives in the spirit of (Huang
et al, 2008), are also conceivable.
Automatic Parameter Learning The most time-
consuming part in building a rule often is to decide
the value of its parameters, especially for FD and
CR rules. For instance, while defining a CR rule,
one has to choose values for the Predicate parame-
ter and the Context parameter (see Tab. 9). Some
parameter values can be learned ? for example, dic-
tionaries (Riloff, 1993) and regular expressions (Li
et al, 2008).
Automatic Rule Refinement Tools automatically
suggesting entire customization rules to a complex
NERL program in the spirit of (Liu et al, 2010) can
further reduce human effort in building NER anno-
tators. With the help of such tools, one only needs
to consider good candidate NERL rules suggested
by the system without having to go through the
conventional manual ?trial and error? process.
6 Conclusion
In this paper, we described NERL, a high-level rule
language for building and customizing NER annota-
tors. We demonstrated that a complex NER annota-
tor built using NERL can be effectively customized
for different domains, achieving extraction quality
superior to the state-of-the-art numbers. However,
our experience also indicates that the process of de-
signing the rules themselves is still manual and time-
consuming. Finally, we discuss how NERL opens
up several interesting research directions towards the
development of sophisticated tooling for automating
some of the rule development tasks.
References
D. E. Appelt and B. Onyshkevych. 1998. The common
pattern specification language. In TIPSTER workshop.
A. Arnold, R. Nallapati, and W. W. Cohen. 2008.
Exploiting feature hierarchy for transfer learning in
named entity recognition. In ACL.
D. M. Bikel, R. L. Schwartz, and R. M. Weischedel.
1999. An algorithm that learns what?s in a name. In
Machine Learning, volume 34, pages 211?231.
J. Blitzer, R. Mcdonald, and F. Pereira. 2006. Domain
adaptation with structural correspondence learning. In
EMNLP.
1011
B. Boguraev. 2003. Annotation-based finite state pro-
cessing in a large-scale nlp arhitecture. In RANLP.
Census. 2007. U.S. Census Bureau.
http://www.census.gov.
J. Cheney, L. Chiticariu, and W. Tan. 2009. Provenance
in databases: Why, how, and where. Foundations and
Trends in Databases, 1(4):379?474.
L. Chiticariu, R. Krishnamurthy, Y. Li, S. Raghavan,
F. Reiss, and S. Vaithyanathan. 2010. SystemT: An
algebraic approach to declarative information extrac-
tion. In ACL.
H. Cunningham, D. Maynard, and V. Tablan. 2000.
JAPE: a Java Annotation Patterns Engine (Second Edi-
tion). Research Memorandum CS?00?10, Department
of Computer Science, University of Sheffield.
W. Drozdzynski, H. Krieger, J. Piskorski, U. Scha?fer, and
F. Xu. 2004. Shallow processing with unification and
typed feature structures ? foundations and applica-
tions. Ku?nstliche Intelligenz, 1:17?23.
O. Etzioni, M. Cafarella, D. Downey, A. Popescu,
T. Shaked, S. Soderland, D. Weld, and A. Yates. 2005.
Unsupervised named-entity extraction from the web:
an experimental study. Artif. Intell., 165(1):91?134.
J. R. Finkel and C. D. Manning. 2009. Nested named
entity recognition. In EMNLP.
R. Florian, A. Ittycheriah, H. Jing, and T. Zhang. 2003.
Named entity recognition through classifier combina-
tion. In CoNLL.
R. Florian, H. Hassan, A. Ittycheriah, H. Jing, N. Kamb-
hatla, X. Luo, N. Nicolov, and S. Roukos. 2004. A
statistical model for multilingual entity detection and
tracking. In NAACL-HLT.
R. Florian, H. Jing, N. Kambhatla, and I. Zitouni. 2006.
Factorizing complex models: A case study in mention
detection. In ACL.
D. Gruhl, M. Nagarajan, J. Pieper, C. Robson, and
A. Sheth. 2009. Context and domain knowledge en-
hanced entity spotting in informal text. In ISWC.
J. Huang, G. Zweig, and M. Padmanabhan. 2001. Infor-
mation extraction from voicemail. In ACL.
Jiansheng Huang, Ting Chen, AnHai Doan, and Jeffrey F.
Naughton. 2008. On the Provenance of Non-Answers
to Queries over Extracted Data. PVLDB, 1(1):736?
747.
M. Jansche and S. Abney. 2002. Information extraction
from voicemail transcripts. In EMNLP.
J. Jiang and C. Zhai. 2006. Exploiting domain structure
for named entity recognition. In NAACL-HLT.
Saul Kripke. 1982. Naming and Necessity. Harvard Uni-
versity Press.
G. R. Krupka and K. Hausman. 2001. IsoQuest Inc.: De-
scription of the NetOwlTM extractor system as used
for MUC-7. In MUC-7.
Y. Li, R. Krishnamurthy, S. Raghavan, S. Vaithyanathan,
and H. V. Jagadish. 2008. Regular expression learning
for information extraction. In EMNLP.
B. Liu, L. Chiticariu, V. Chu, H. V. Jagadish, and F. Reiss.
2010. Automatic Rule Refinement for Information
Extraction. PVLDB, 3.
D. Maynard, K. Bontcheva, and H. Cunningham. 2003.
Towards a semantic extraction of named entities. In
RANLP.
A. McCallum and W. Li. 2003. Early results for named
entity recognition with conditional random fields, fea-
ture induction and web-enhanced lexicons. In CoNLL.
E. Minkov, R. C. Wang, and W. W. Cohen. 2005. Ex-
tracting personal names from emails: Applying named
entity recognition to informal text. In HLT/EMNLP.
D. Nadeau and S. Sekine. 2007. A survey of named
entity recognition and classification. Linguisticae In-
vestigationes, 30(1):3?26.
NIST. 2005. The ace evaluation plan.
F. J. Och O. Bender and H. Ney. 2003. Maximum en-
tropy models for named entity recognition. In CoNLL.
G. Petasis, F. Vichot, F. Wolinski, G. Paliouras,
V. Karkaletsis, and C. Spyropoulos. 2001. Using
machine learning to maintain rule-based named-entity
recognition and classification systems. In ACL.
T. Poibeau and L. Kosseim. 2001. Proper name ex-
traction from non-journalistic texts. In Computational
Linguistics in the Netherlands, pages 144?157.
E. Riloff. 1993. Automatically constructing a dictionary
for information extraction tasks. In KDD.
S. Sekine and C. Nobata. 2004. Definition, dictionaries
and tagger for extended named entity hierarchy. In
Conference on Language Resources and Evaluation.
W. Shen, A. Doan, J. F. Naughton, and R. Ramakrishnan.
2007. Declarative information extraction using data-
log with embedded extraction predicates. In VLDB.
S. Singh, D. Hillard, and C. Leggeteer. 2010. Minimally-
supervised extraction of entities from text advertise-
ments. In NAACL-HLT.
P. Siniakov. 2010. GROPUS - an adaptive rule-based al-
gorithm for information extraction. Ph.D. thesis, Freie
Universitat Berlin.
R. Srihari, C. Niu, and W. Li. 2001. A hybrid approach
for named entity and sub-type tagging. In ANLP.
E. F. Tjong, K. Sang, and F. De Meulder. 2003. Intro-
duction to the CoNLL-2003 shared task: Language-
independent named entity recognition. In CoNLL.
D. Wu, W. S. Lee, N. Ye, and H. L. Chieu. 2009. Domain
adaptive bootstrapping for named entity recognition.
In EMNLP.
J. Zhu, V. Uren, and E. Motta. 2005. Espotter: Adaptive
named entity recognition for web browsing. In WM.
1012
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 128?138, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Towards Efficient Named-Entity Rule Induction for Customizability
Ajay Nagesh1,2
1IITB-Monash Research Academy
ajaynagesh@cse.iitb.ac.in
Ganesh Ramakrishnan
2IIT Bombay
ganesh@cse.iitb.ac.in
Laura Chiticariu
IBM Research - Almaden
chiti@us.ibm.com
Rajasekar Krishnamurthy
IBM Research - Almaden
rajase@us.ibm.com
Ankush Dharkar
SASTRA University
ankushdharkar@cse.sastra.edu
Pushpak Bhattacharyya
IIT Bombay
pb@cse.iitb.ac.in
Abstract
Generic rule-based systems for Information
Extraction (IE) have been shown to work
reasonably well out-of-the-box, and achieve
state-of-the-art accuracy with further domain
customization. However, it is generally rec-
ognized that manually building and customiz-
ing rules is a complex and labor intensive pro-
cess. In this paper, we discuss an approach
that facilitates the process of building cus-
tomizable rules for Named-Entity Recognition
(NER) tasks via rule induction, in the Annota-
tion Query Language (AQL). Given a set of
basic features and an annotated document col-
lection, our goal is to generate an initial set
of rules with reasonable accuracy, that are in-
terpretable and thus can be easily refined by
a human developer. We present an efficient
rule induction process, modeled on a four-
stage manual rule development process and
present initial promising results with our sys-
tem. We also propose a simple notion of ex-
tractor complexity as a first step to quantify
the interpretability of an extractor, and study
the effect of induction bias and customization
of basic features on the accuracy and complex-
ity of induced rules. We demonstrate through
experiments that the induced rules have good
accuracy and low complexity according to our
complexity measure.
1 Introduction
Named-entity recognition (NER) is the task of iden-
tifying mentions of rigid designators from text be-
longing to named-entity types such as persons, orga-
nizations and locations (Nadeau and Sekine, 2007).
Generic NER rules have been shown to work reason-
ably well-out-of-the-box, and with further domain
customization (Chiticariu et al2010b), achieve
quality surpassing state-of-the-art results. Table 1
System Dataset F?=1
Generic Customized
GATE ACE2002 57.8 82.2
ACE 2005 57.32 88.95
SystemT CoNLL 2003 64.15 91.77
Enron 76.53 85.29
Table 1: Quality of generic vs. customized rules.
summarizes the quality of NER rules out-of-the-box
and after domain customization in the GATE (Cun-
ningham et al2011) and SystemT (Chiticariu et
al., 2010a) systems, as reported in (Maynard et al
2003) and (Chiticariu et al2010b) respectively.
Rule-based systems are widely used in enterprise
settings due to their explainability. Rules are trans-
parent, which leads to better explainability of errors.
One can easily identify the cause of a false positive
or negative, and refine the rules without affecting
other correct results identified by the system. Fur-
thermore, rules are typically easier to understand by
an IE developer and can be customized for a new
domain without requiring additional labeled data.
Typically, a rule-based NER system consists of a
combination of four categories of rules (Chiticariu et
al., 2010b): (1) Basic Feature (BF) rules to identify
components of an entity such as first name and last
name. (2) Candidate definition (CD) rules to iden-
tify complete occurrences of an entity by combining
the output of multiple BF rules, e.g., first name fol-
lowed by last name is a person candidate. (3) Candi-
date refinement (CR) rules to refine candidates gen-
erated by CD rules. E.g., discard candidate persons
contained within organizations. (4) Consolidation
rules (CO) to resolve overlapping candidates gener-
ated by multiple CD and CR rules.
A well-known drawback that influences the
adoptability of rule-based NER systems is the man-
128
ual effort required to build the rules. A common ap-
proach to address this problem is to build a generic
NER extractor and then customize it for specific do-
mains. While this approach partially alleviates the
problem, substantial manual effort (in the order of
several person weeks) is still required for the two
stages as reported in (Maynard et al2003; Chiti-
cariu et al2010b). In this paper, we present initial
work towards facilitating the process of building a
generic NER extractor using induction techniques.
Specifically, given as input an annotated docu-
ment corpus, a set of BF rules, and a default CO
rule for each entity type, our goal is to generate a
set of CD and CR rules such that the resulting ex-
tractor constitutes a good starting point for further
refinement by a developer. Since the generic NER
extractor has to be manually customized, a major
challenge is to ensure that the generated rules have
good accuracy, and, at the same time, that they are
not too complex, and consequently interpretable.
The main contributions in this paper are
1. An efficient system for NER rule induction, us-
ing a highly expressive rule language (AQL) as
the target language. The first phase of rule in-
duction uses a combination of clustering and
relative least general generalization (RLGG)
techniques to learn CD rules. The second phase
identifies CR rules using a propositional rule
learner like JRIP to learn accurate composi-
tions of CD rules.
2. Usage of induction biases to enhance the inter-
pretability of rules. These biases capture the
expertise gleaned from manual rule develop-
ment and constrain the search space in our in-
duction system.
3. Definition of an initial notion of extractor com-
plexity to quantify the interpretability of an ex-
tractor and to guide the process of adding in-
duction biases to favor learning less complex
extractors. This is to ensure that the rules are
easily customizable by the developer.
4. Scalable induction process through usage of
SystemT, a state-of-the-art IE system which
serves as a highly efficient theorem prover for
AQL, and performance optimizations such as
clustering of examples and parallelizing vari-
ous modules (E.g.: propositional rule learning).
Roadmap We first describe preliminaries on Sys-
temT and AQL (Section 3) and define the target lan-
guage for our induction algorithm and the notion of
rule complexity (Section 4). We then present our
approach for inducing CD and CR rules, and dis-
cuss induction biases that would favor interpretabil-
ity (Section 5), and discuss the results of an empir-
ical evaluation (Section 6). We conclude with av-
enues for improvement in the future (Section 7).
2 Related Work
Existing approaches to rule induction for IE focus
on rule-based systems based on the cascading gram-
mar formalism exemplified by the Common Pat-
tern Specification Language (CPSL) (Appelt and
Onyshkevych, 1998), where rules are specified as
a sequence of basic features that describe an en-
tity, with limited predicates in the context of an
entity mention. Patel et al2009) and Soderland
(1999) elaborate on top-down techniques for induc-
tion of IE rules, whereas (Califf and Mooney, 1997;
Califf and Mooney, 1999) discuss a bottom-up IE
rule induction system that uses the relative least gen-
eral generalization (RLGG) of examples1. However,
in all these systems, the expressivity of the rule-
representation language is restricted to that of cap-
turing sequence information. As discussed in Sec-
tion 3, contextual clues and higher level rule inter-
actions such as filtering and join are very difficult,
if not impossible to express in such representations
without resorting to custom code. Learning higher
level interactions between rules has received little
attention. Our technique for learning higher level in-
teractions is similar to the induction of ripple down
rules (Gaines and Compton, 1995), which, to the
best of our knowledge, has not been previously ap-
plied to IE. A framework for refining AQL extractors
based on an annotated document corpus described
in (Liu et al2010). We present complementary
techniques for inducing an initial extractor that can
be automatically refined in this framework.
3 Preliminaries
SystemT is a declarative IE system based on an al-
gebraic framework. In SystemT, developers write
rules in AQL. To represent annotations in a docu-
1Our work also makes use of RLGGs but computes these
generalizations for clusters of examples, instead of pairs.
129
Figure 1: Example Person extractor in AQL
ment, AQL uses a simple relational data model with
three types: a span is a region of text within a docu-
ment identified by its ?begin? and ?end? positions; a
tuple is a fixed-size list of spans; a relation, or view,
is a multi-set of tuples, where every tuple in the view
must be of the same size.
Figure 1 shows a portion of a Person extractor
written in AQL. The basic building block of AQL
is a view. A view is a logical description of a set of
tuples in terms of (i) the document text (denoted as a
special view called Document), and (ii) the contents
of other views, as specified in the from clauses of
each statement. Figure 1 also illustrates five of the
basic constructs that can be used to define a view,
and which we explain next. The complete specifica-
tion can be found in the AQL manual (IBM, 2012).
In the paper, we will use ?rules? and ?views? inter-
changeably.
The extract statement specifies basic character-
level extraction primitives such as regular expression
and dictionary matching over text, creating a tuple
for each match. As an example, rule R1 uses the ex-
tract statement to identify matches (Caps spans) of a
regular expression for capitalized words.
The select statement is similar to the SQL select
statement but it contains an additional consolidate
on clause (explained further), along with an exten-
sive collection of text-specific predicates. Rule R5
illustrates a complex example: it selects First spans
immediately followed within zero tokens by a Last
span, where the latter is also a Caps span. The
two conditions are specified using two join predi-
cates: FollowsTok and Equals respectively. For each
triplet of First, Last and Caps spans satisfying the two
predicates, the CombineSpans built-in scalar func-
tion in the select clause constructs larger PersonFirst-
Last spans that begin at the begin position of the First
span, and end at the end position of the Last (also
Caps) span.
The union all statement merges the outputs of two
or more statements. For example, rule R6 unions
person candidates identified by rules R4 and R5.
The minus statement subtracts the output of one
statement from the output of another. For example,
rule R8 defines a view PersonAll by filtering out Per-
sonInvalid tuples from the set of PersonCandidate tu-
ples. Notice that rule R7 used to define the view Per-
sonInvalid illustrates another join predicate of AQL
called Overlaps, which returns true if its two argu-
ment spans overlap in the input text. Therefore, at
a high level, rule R8 removes person candidates that
overlap with an Organization span. (The Organization
extractor is not depicted in the figure.)
The consolidate clause of a select statement re-
moves selected overlapping spans from the indicated
column of the input tuples, according to the spec-
ified policy (for instance, ?ContainedWithin?). For
example, rule R9 retains PersonAll spans that are not
contained in other PersonAll spans.
Internally, SystemT compiles an AQL extractor
into an executable plan in the form of a graph of
operators. The formal definition of these operators
takes the form of an algebra (Reiss et al2008), sim-
ilar to relational algebra, but with extensions for text
processing. The decoupling between AQL and the
operator algebra allows for greater rule expressiv-
ity because the rule language is not constrained by
the need to compile to a finite state transducer, as in
grammar systems based on the CPSL standard. In
fact, join predicates such as Overlaps, as well as fil-
ter operations (minus) are extremely difficult to ex-
130
press in CPSL systems such as GATE without an
escape to custom code (Chiticariu et al2010b). In
addition, the decoupling between the AQL specifi-
cation of ?what? to extract from ?how? to extract
it, allows greater flexibility in choosing an efficient
execution strategy among the many possible opera-
tor graphs that may exist for the same AQL extrac-
tor. Therefore, extractors written in AQL achieve
orders of magnitude higher throughput (Chiticariu
et al2010a).
4 Induction Target Language
Our goal is to automatically generate NER extrac-
tors with good quality, and at the same time, man-
ageable complexity, so that the extractors can be fur-
ther refined and customized by the developer. To this
end, we focus on inducing extractors using the sub-
set of AQL constructs described in Section 3. We
note that we have chosen a small subset of AQL con-
structs that are sufficient to implement several com-
mon operations required for NER. However, AQL is
a much more expressive language, and incorporating
additional constructs is subject to our future work.
In this section we describe the building blocks of
our target language, and propose a simple definition
for measuring the complexity of an extractor.
Target Language. The components of the target
language are as follows, and summarized in Table 2.
Basic features (BF): BF views are specified using the
extract statement, such as rulesR1 toR3 in Figure 1.
In this paper, we assume as input a set of basic fea-
tures, consisting of dictionaries and regular expres-
sions.
Candidate definition (CD): CD views are expressed
using the select statement to combine BF views with
join predicates (e.g., Equals, FollowsTok or Over-
laps), and the CombineSpans scalar function to con-
struct larger candidate spans from input spans. Rules
R4 and R5 in Figure 1 are example CD rules. In
general, a CD view is defined as: ?Select all spans
constructed from view1, view2, . . ., viewn, such that all
join predicates are satisfied?.
Candidate refinement (CR): CR views are used to
discard spans output by the CD views that may be
incorrect. In general, a CR view is defined as: ?From
the list of spans of viewvalid subtract all those spans that
belong to viewinvalid?. viewvalid is obtained by join-
ing all the positive CD clues on the Equals predicate
and viewinvalid is obtained by joining all the nega-
tive overlapping clues with the Overlaps predicate
and subsequently ?union?ing all the negative clues.
(e.g., similar in spirit to rules R6, R7 and R8 in Fig-
ure 1, except that the subtraction is done from a sin-
gle view and not the union of multiple views).
Consolidation (CO): Finally, a select statement with
a fixed consolidate clause is used for each entity type
to remove overlapping spans from CR views. An
example CO view is defined by rule R9 in Figure 1.
Extractor Complexity. Since our goal is to gener-
ate extractors with manageable complexity, we must
introduce a quantitative measure of extractor com-
plexity, in order to (1) judge the complexity of the
extractors generated by our system, and (2) reduce
the search space considered by the induction system.
To this end, we define a simple complexity score
that is a function of the number of rules, and the
number of input views to each rule of the extrac-
tor. In particular, we define the length of rule R,
denoted as L(R), as the number of input views in
the from clause(s) of the view. For example, in Fig-
ure 1, we have L(R4) = 2 and L(R5) = 3, since
R4 and R5 have two, and respectively three views
in the from clause. Furthermore, L(R8) = 2 since
each of the two inner statements of R8 has one from
clause with a single input view. The complexity of
BF rules (e.g., R1 to R3) and CO rules (e.g., R9) is
always 1, since these types of rules have a single in-
put view. We define the complexity of extractor E,
denoted as C(E) as the sum of lengths of all rules of
E. For example, the complexity of the Person extrac-
tor from Figure 1 is 15, plus the length of all rules
involved in defining Organization, which are omitted
from the figure.
Our simple notion of rule length is motivated
by existing literature in the area of database sys-
tems (Abiteboul et al1995), where the size of a
conjunctive query is determined only by the number
of atoms in the body of the query (e.g., items in the
FROM clause), and it is independent on the number
of join variables (i.e., items in the WHERE clause),
or the size of the head of the query (e.g., items in the
SELECT clause). As such, our notion of complexity
is rather coarse, and we shall discuss its shortcom-
ings in detail in Section 6.2. However, we shall show
that the complexity score significantly reduces the
search space of our induction techniques leading to
131
Phase name AQL statements Prescription Rule Type
Basic Features extract Off-the-shelf, Learning using prior
work (Riloff, 1993; Li et al2008)
Basic Features Definition
Phase 1 (Clustering and
RLGG)
select Bottom-up learning (LGG), Top-down refine-
ment
Development of Candidate
Rules
Phase 2 (Propositional Rule
Learning)
select, union
all, minus
RIPPER, Lightweight Rule Induction Candidate Rules Filtering
Consolidation consolidate,
union all
Manually identified consolidation rules, based
on domain knowledge
Consolidation rules
Table 2: Phases in induction, the language constructs invoked in each phase, the prescriptions for inducing rules in the
phase and the corresponding type of rule in manual rule development.
simpler and smaller extractors, and therefore consti-
tutes a good basis for more comprehensive measures
of interpretability in the future.
5 Induction of Rules
Since the goal is to generate rules that can be cus-
tomized by humans, the overall structure of the in-
duced rules must be similar in spirit to what a devel-
oper following best practices would write. Hence,
the induction process is divided into multiple phases.
Figure 2 shows the correspondence between the
phases of induction and the types of rules. In Ta-
ble 2, we summarize the phases of our induction al-
gorithm, along with the subset of AQL constructs
that comprise the language of the rules learnt in that
phase, the possible methods prescribed for inducing
the rules and their correspondence with the stages in
the manual rule development.
Our induction system generates rules for two of
the four categories, namely CD and CR rules as
highlighted in Figure 2. We assume that we are
given the BFs in the form of dictionaries and reg-
ular expressions. Prior work on learning dictionar-
ies (Riloff, 1993) and regular expressions (Li et al
2008) could be leveraged to semi-automate the pro-
cess of defining the basic features.
We represent each example, in conjunction with
relevant background knowledge in the form first
order horn clauses. This background knowledge
will serve as input to our induction system. The
first phase of induction uses a combination of
clustering and relative least general generalization
(RLGG) (Nienhuys-Cheng andWolf, 1997; Muggle-
ton and Feng, 1992) techniques. At the end of this
phase, we have a number of CD rules. In the sec-
ond phase, we begin by forming a structure called
the span-view table. Broadly speaking, this is an
attribute-value table formed by all the views induced
in the first phase along with the textual spans gener-
ated by them. The attribute-value table is used as
input to a propositional rule learner such as JRIP
to learn accurate compositions of a useful (as deter-
mined by the learning algorithm) subset of the CD
rules. This forms the second phase of our system.
The rules learnt from this phase are the CR rules.
At various phases, several induction biases are intro-
duced to enhance the interpretability of rules. These
biases capture the expertise gleaned from manual
rule development and constrain the search space in
our induction system.
The hypothesis language of our induction sys-
tem is Annotation Query Language (AQL) and we
use SystemT as the theorem prover. SystemT pro-
vides a very fast rule execution engine and is cru-
cial in our induction system as we test multiple hy-
potheses in the search for the more promising ones.
AQL provides a very expressive rule representation
language that is proven to be capable of encoding
all the paradigms that any rule-based representa-
tion can encode. The dual advantages of rich rule-
representation and execution efficiency are the main
motivation behind our choice.
We discuss our induction procedure in detail next.
5.1 Basic Features and Background Knowledge
We assume that we are provided with a set of dictio-
naries and regular expressions for defining all our
basic create view statements. R1, R2 and R3 in
Figure 1 are such basic view definitions. The ba-
sic views are compiled and executed in SystemT
over the training document collection and the re-
sulting spans are represented by equivalent predi-
cates in first order logic. Essentially, each train-
ing example is represented as a definite clause,
132
Figure 2: Correspondence between Manual Rule devel-
opment and Rule Induction.
that includes in its body, the basic view-types en-
coded as background knowledge predicates. To es-
tablish relationships between different background
knowledge predicates for each training example, we
define some additional ?glue predicates? such as
contains and before.
5.2 Induction of Candidate Definition Rules
Clustering Module. We obtain non-overlapping
clusters of examples within each type, by comput-
ing similarities between their representations as def-
inite clauses. We present the intuition behind this
approach in Figure 3 which illustrates the process
of taking two examples and finding their generaliza-
tion. It is worthwhile to look at generalizations of
instances that are similar. For instance, two token
person names such as Mark Waugh and Mark Twain
are part of a single cluster. However, we would not
be able to generalize a two-token name (e.g., Mark
Waugh) with another name consisting of initials fol-
lowed by a token (e.g., M. Waugh). Using a wrap-
per around the hierarchical agglomerative cluster-
ing implemented in LingPipe2, we cluster examples
and look at generalizations only within each cluster.
Clustering also helps improve efficiency by reduc-
ing the computational overhead, since otherwise, we
would have to consider generalizations of all pairs of
examples (Muggleton and Feng, 1992).
RLGG computation. We compute our CD
rules as the relative least general generalization
(RLGG) (Nienhuys-Cheng and Wolf, 1997; Mug-
gleton and Feng, 1992) of examples in each clus-
ter. Given a set of clauses in first order logic,
their RLGG is the least generalized clause in the
2http://alias-i.com/lingpipe/demos/tutorial/cluster/read-
me.html
Figure 3: Relative Least General Generalization
subsumption lattice of the clauses relative to the
background knowledge (Nienhuys-Cheng and Wolf,
1997). RLGG is associative, and we use this fact
to compute RLGGs of sets of examples in a clus-
ter. The RLGG of two bottom clauses as computed
in our system and its translation to an AQL view is
illustrated in Figure 3. We filter out noisy RLGGs
and convert the selected RLGGs into the equivalent
AQL views. Each such AQL view is treated as a
CD rule. We next discuss the process of filtering-
out noisy RLGGs. We interchangeably refer to the
RLGGs and the clusters they represent .
Iterative Clustering and RLGG filtering. Since
clustering is sub-optimal, we expected some clusters
in a single run of clustering to have poor RLGGs, ei-
ther in terms of complexity or precision. We there-
fore use an iterative clustering approach, based on
the separate-and-conquer (Fu?rnkranz, 1999) strat-
egy. In each iteration, we pick the clusters with the
best RLGGs and remove all examples covered by
those RLGGs. The best RLGGs must have preci-
sion and number of examples covered above a pre-
specified threshold.
5.3 Induction of Candidate Refinement Rules
Span-View Table. The CD views from phase 1
along with the textual spans they generate, yield the
span-view table. The rows of the table correspond
to the set of spans returned by all the CD views. The
columns correspond to the set of CD view names.
Each span either belongs to one of the named en-
tity types (PER, ORG or LOC) or is none of them
(NONE); the type information constitutes its class
label (see Figure 4 for an illustrated example). The
cells in the table correspond to either a match (M) or
a no-match (N) or partial/overlapping match (O) of
a span generated by a CD view. This attribute-value
table is used as input to a propositional rule learner
133
Figure 4: Span-View Table
like JRIP to learn compositions of CD views.
Propositional Rule Learning. Based on our study
of different propositional rule learners, we decided
to use RIPPER (Fu?rnkranz and Widmer, 1994) im-
plemented as the JRIP classifier in weka (Witten et
al., 2011). Some considerations that favor JRIP are
(i) absence of rule ordering, (ii) ease of conversion
to AQL and (iii) amenability to add induction biases
in the implementation.
A number of syntactic biases were introduced in
JRIP to aid in the interpretability of the induced
rules. We observed in our manually developed rules
that CR rules for a type involve interaction between
CDs for the same type and negations (not-overlaps,
not matches) of CDs of the other types. This bias
was incorporated by constraining a JRIP rule to con-
tain only positive features (CDs) of the same type
(say PER) and negative features (CDs) of only other
types (ORG and LOC, in this case).
The output of the JRIP algorithm is a set of
rules, one set for each of PER, ORG and LOC.
Here is an example rule: PER-CR-Rule ? (PerCD
= m) AND (LocCD != o) which is read as : ?If a
span matches PerCD and does not overlap with LocCD,
then that span denotes a PER named entity?. Here
PerCD is {[FirstName ? CapsPerson][LastName
? CapsPerson]} 3 and LocCD is {[CapsPlace ?
CitiesDict]}. This rule filters out wrong person
annotations like ?Prince William? in Prince William
Sound. (This is the name of a location but has over-
lapped with a person named entity.) In AQL, this
effect can be achieved most elegantly by the minus
(filter) construct. Such an AQL rule will filter all
those occurrences of Prince William from the list of
3Two consecutive spans where the 1st is FirstName and
CapsPerson and the 2nd is LastName and CapsPerson.
persons that overlap with a city name.
Steps such as clustering, computation of RLGGs,
JRIP, and theorem proving using SystemT were par-
allelized. Once the CR views for each type of
named entity are learnt, many forms of consolida-
tions (COs) are possible, both within and across
types. A simple consolidation policy that we have
incorporated in the system is as follows: union all
the rules of a particular type, then perform a con-
tained within consolidation, resulting in the final set
of consolidated views for each named entity type.
6 Experiments
We evaluate our system on CoNLL03 (Tjong
Kim Sang and De Meulder, 2003), a collection
of Reuters news stories. We used the CoNLL03
training set for induction and report results on the
CoNLL03 test collection.
The basic features (BFs) form the primary input to
our induction system. We experimented with three
sets of BFs:
Initial set(E1): The goal in this setup is to induce
an initial set of rules based on a small set of reason-
able BFs. We use a conservative initial set consisting
of 15 BFs (5 regular expressions and 10 dictionar-
ies).
Enhanced set (E2): Based on the results of E1,
we identify a set of additional domain independent
BFs4. Five views were added to the existing set in
E1 (1 regular expression and 4 dictionaries). The
goal is to observe whether our approach yields rea-
sonable accuracies compared to generic rules devel-
oped manually.
Domain customized set (E3): Based on the
knowledge of the domain of the training dataset
(CoNLL03), we introduced a set of features specific
to this dataset. These included sports-related person,
organization and location dictionaries5. These views
were added to the existing set in E2. The intended
goal is to observe what are the best possible accura-
cies that could be achieved with BFs customized to
a particular domain.
The set of parameters for iterative clustering on
which the accuracies reported are : the precision
threshold for the RLGGs of the clusters was 70%
4E.g., the feature preposition dictionary was added in E2 to
help identify organization names such as Bank of England.
5Half of the documents in CoNLL03 are sports-related.
134
Train Test
Type P R F P R F C(E)
E1 (Initial set)
PER 88.5 41.4 56.4 92.5 39.4 55.3 144
ORG 89.1 7.3 13.4 85.9 5.2 9.7 22
LOC 91.6 54.5 68.3 87.3 55.3 67.8 105
Overall 90.2 35.3 50.7 89.2 33.3 48.5 234
E2 (Enhanced set)
PER 84.7 52.9 65.1 87.5 49.9 63.5 233
ORG 88.2 7.8 14.3 85.8 5.9 11.0 99
LOC 92.1 58.6 71.7 88.6 59.1 70.9 257
Overall 88.6 40.7 55.8 88.0 38.2 53.3 457
E3 (Domain customized set)
PER 89.9 57.3 70.0 91.7 56.0 69.5 430
ORG 86.9 50.9 64.2 86.9 47.5 61.4 348
LOC 90.8 67.0 77.1 84.3 67.3 74.8 356
Overall 89.4 58.7 70.9 87.3 57.0 68.9 844
Table 3: Results on CoNLL03 dataset with different basic
feature sets
and the number of examples covered by each RLGG
was 5. We selected the top 5 clusters from each iter-
ation whose RLGGs crossed this threshold. If there
were no such clusters then we would lower the preci-
sion threshold to 35% (half of the threshold). When
no new clusters were formed, we ended the itera-
tions.
6.1 Experiments and Results
Effect of Augmenting Basic Features. Table 3
shows the accuracy and complexity of rules induced
with the three basic feature sets E1, E2 and E3,
respectively 6. The overall F-measure on the test
dataset is 48.5% with E1, it increases to around
53.3% with E2 and is highest at 68.9% with E3.
As we increase the number of BFs, the accuracies
of the induced extractors increases, at the cost of
an increase in complexity. In particular, the re-
call increases significantly across the board, and is
more prominent between E2 and E3, where the ad-
ditional domain specific features result in recall in-
crease from 5.9% to 47.5% for ORG. The precision
increases slightly for PER, but decreases slightly for
LOC and ORG with the addition of domain specific
features.
Comparison with manually developed rules. We
compared the induced extractors with the manually
developed extractors of (Chiticariu et al2010b),
heretofore referred to as manual extractors. (For a
detailed analysis, we obtained the extractors from
6These are the results for the configuration with bias.
the authors). Table 4 shows the accuracy and com-
plexity of the induced rules with E2 and E3 and the
manual extractors for the generic domain and, re-
spectively, customized for the CoNLL03 domain.
(In the table, ignore the column Induced (without
bias), which is discussed later). Our technique
compares reasonably with the manually constructed
generic extractor for two of the three entity types;
and on precision for all entity types, especially since
our system generated the rules in 1 hour, whereas the
development of manual rules took much longer 7.
Additional work is required to match the manual
customized extractor?s performance, primarily due
to shortcomings in our current target language. Re-
call that our framework is limited to a small subset
of AQL constructs for expressing CD and CR rules,
and there is a single consolidation rule. In particu-
lar, advanced constructs such as dynamic dictionar-
ies are not supported, and the set of predicates to the
Filter construct supported in our system is restricted
to predicates over other concepts, which is only a
subset of those used in (Chiticariu et al2010b).
The manual extractors also contain a larger number
of rules covering many different cases, improving
the accuracy, but also leading to a higher complex-
ity score. To better analyze the complexity, we also
computed the average rule length for each extrac-
tor by dividing the complexity score by the number
of AQL views of the extractor. The average rule
length is 1.78 and 1.87 for the induced extractors
with E2 and E3, respectively, and 1.9 and 2.1 for the
generic and customized extractors of (Chiticariu et
al., 2010b), respectively. The average rule length in-
creases from the generic extractor to the customized
extractor in both cases. On average, however, an in-
dividual induced rule is slightly smaller than a man-
ually developed rule.
Effect of Bias. The goal of this experiment is to
demonstrate the importance of biases in the induc-
tion process. The biases added to the system are
broadly of two types: (i) Partition of basic features
based on types (ii) Restriction on the type of CD
views that can appear in a CR view. 8 Without
7 (Chiticariu et al2010b) mentions that customization for 3
domains required 8 person weeks. It is reasonable to infer that
developing the generic rules took comparable effort.
8For e.g., person CR view can contain only person CD views
as positive clues and CD views of other types as negative clues.
135
(i) many semantically similar basic features (espe-
cially, regular expressions) would match a given to-
ken, leading to an increase in the length of a CD
a rule. For example, in the CD rule [FirstName-
Dict][CapsPerson ? CapsOrg]} (?A FirstNameDict
span followed by a CapsPerson span that is also a Cap-
sOrg span?), CapsPerson and CapsOrg are two very
similar regular expressions identifying capitalized
phrases that look like person, and respectively, orga-
nization names, with small variations (e.g., the for-
mer may allow special characters such as ?-?). In-
cluding both BFs in a CD rule leads to a larger rule
that is unintuitive for a developer. The former bias
excludes such CD rules from consideration.
The latter type of bias prevents CD rules of one
type to appear as positive clues for a CR rule of
a different type. For instance, without this bias,
one of the CR rules obtained was Per ? (OrgCD
= m) AND (LocCD != o) (?If a span matches OrgCD
and does not overlap with LocCD, then that span
denotes a PER named entity?. Here OrgCD was
{[CapsOrg][CapsOrg]} and LocCD was {[CapsLoc
? CitiesDict]}. The inclusion of an Organization
CD rule as a positive clue for a Person CR rule is
unintuitive for a developer.
Table 4, shows the effect (for E2 and E3) on the
test dataset of disabling and enabling bias during
the induction of CR rules using JRIP. Adding bias
improves the precision of the induced rules. With-
out bias, however, the system is less constrained in
its search for high recall rules, leading to slightly
higher overall F measure. This comes at the cost
of an increase in extractor complexity and average
rule length. For example, for E2, the average rule
length decreases from 2.17 to 1.78 after adding the
bias. Overall, our results show that biases lead to
less complex extractors with only a very minor ef-
fect on accuracy, thus biases are important factors
contributing to inducing rules that are understand-
able and may be refined by humans.
Comparison with other induction systems. We
also experimented with two other induction systems,
Aleph9 and ALP10, a package that implements one
of the reportedly good information extraction algo-
rithms (Ciravegna, 2001). While induction in Aleph
9A system for inductive logic programming. See
http://www.cs.ox.ac.uk/activities/machlearn/Aleph/aleph.html
10http://code.google.com/p/alpie/
was performed with the same target language as in
our approach, the target language of ALP is JAPE,
which has been shown (Chiticariu et al2010b) to
lack in some of the constructs (such as minus) that
AQL provides and which form a part of our tar-
get language (especially the rule refinement phase).
However, despite experimenting with all possible
parameter configurations for each of these (in each
of E1, E2 and E3 settings), the accuracies obtained
were substantially (30-50%) worse and the extrac-
tor complexity was much (around 60%) higher when
compared to our system (with or without bias). Ad-
ditionally, Aleph takes close to three days for induc-
tion, whereas both ALP and our system require less
than an hour.
6.2 Discussion
Weak and Strong CDs reflected in CRs. In
our experiments, we found that varying the pre-
cision and complexity thresholds while inducing
the CDs (c.f Section 5) affected the F1 of the fi-
nal extractor only minimally. But reducing the
precision threshold generally improved the preci-
sion of the final extractor, which seemed counter-
intuitive at first. We found that CR rules learned
by JRIP consist of a strong CD rule (high preci-
sion, typically involving a dictionary) and a weak
CD rule (low precision, typically involving only
regular expressions). The strong CD rule always
corresponded to a positive clue (match) and the
weak CD rule corresponded to the negative clue
(overlaps or not-matches). This is illustrated in
the following CR rule: PER ? (PerCD = m) AND
(OrgCD != o) where (PerCD is {[CapsPersonR]
[CapsPersonR ? LastNameDict]} and (OrgCD is
{[CapsOrgR][CapsOrgR][CapsOrgR]}. This is
posited to be the way the CR rule learner operates
? it tries to learn conjunctions of weak and strong
clues so as to filter one from the other. Therefore,
setting a precision threshold too high limited the
number of such weak clues and the ability of the CR
rule learner to find such rules.
Interpretability. Measuring interpretability of rules
is a difficult problem. In this work, we have taken
a first step towards measuring interpretability using
a coarse grain measure in the form of a simple no-
tion of complexity score. The complexity is very
helpful in comparing alternative rule sets based on
136
Chiticariu et al010b Induced (With Bias) Induced (Without Bias)
P R F C(E) P R F C(E) P R F C(E)
Generic (E2) PER 82.2 60.3 69.5 945 87.5 49.9 63.5 233 85.8 53.7 66.0 476
ORG 75.7 17.5 28.5 1015 85.8 5.9 11.0 99 74.1 15.7 25.9 327
LOC 72.2 86.1 78.6 921 88.6 59.1 70.9 257 85.9 61.5 71.7 303
Overall 75.9 54.6 63.5 1015 88.0 38.2 53.3 457 84.2 43.5 57.4 907
Customised (E3) PER 96.3 92.2 94.2 2154 91.7 56.0 69.5 430 90.7 60.3 72.4 359
ORG 91.1 85.1 88.0 2154 86.9 47.5 61.4 348 90.4 46.8 61.7 397
LOC 93.3 91.7 92.5 2154 84.3 67.3 74.8 356 83.9 69.1 75.8 486
Overall 93.5 89.6 91.5 2160 87.3 57.0 68.9 844 87.8 58.7 70.4 901
Table 4: Comparison of induced rules (with and without bias) and manually developed rules. (CoNLL03 test dataset)
the number of rules, and the size of each rule, but
exhibits a number of shortcomings described next.
First, it disregards other components of a rule be-
sides its from clause, for example, the number of
items in the select clause, or the where clause. Sec-
ond, rule developers use semantically meaningful
view names such as those shown in Figure 1 to help
them recall the semantics of a rule at a high-level, an
aspect that is not captured by the complexity mea-
sure. Automatic generation of meaningful names
for induced views is an interesting direction for fu-
ture work. Finally, the overall structure of an extrac-
tors is not considered. In simple terms, an extrac-
tor consisting of 5 rules of size 1 is indistinguish-
able from an extractor consisting of a single rule
of size 5, and it is arguable which of these extrac-
tors is more interpretable. More generally, the ex-
tent of this shortcoming is best explained using an
example. When informally examining the rules in-
duced by our system, we found that CD rules are
similar in spirit to those written by rule develop-
ers. On the other hand, the induced CR rules are
too fine-grained. In general, rule developers group
CD rules with similar semantics, then write refine-
ment rules at the higher level of the group, as op-
posed to the lower level of individual CD views. For
example, one may write multiple CD rules for can-
didate person names of the form ?First??Last?, and
multiple CD rules of the form ?Last?, ?First?. One
would then union together the candidates from each
of the two groups into two different views, e.g., Per-
FirstLast and PerLastCommaFirst, and write filter
rules at the higher level of these two views, e.g.,
?Remove PerLastCommaFirst spans that overlap with a
PerFirstLast span?. In contrast, our induction algo-
rithm considers CR rules consisting of combinations
of CD rules directly, leading to many semantically
similar CR rules, each operating over small parts of
a larger semantic group (see rule in Section 6.1).
This results in repetition, and qualitatively less in-
terpretable rules, since humans prefer higher levels
of abstraction and generalization. This nuance is not
captured by the complexity score which may deem
an extractor consisting of many rules, where many
of the rules operate at higher levels of groups of can-
didates to be more complex than a smaller extrac-
tor with many fine-grained rules. Indeed, as shown
before, the complexity of the induced extractors is
much smaller compared to that of manual extrac-
tors, although the latter follow the semantic group-
ing principle and are considered more interpretable.
7 Conclusion
We presented a system for efficiently inducing
named entity annotation rules in the AQL language.
The design of our approach is aimed at producing
accurate rules that can be understood and refined
by humans, by placing special emphasis on low
complexity and efficient computation of the induced
rules, while mimicking a four stage approach used
for manually constructing rules. The induced rules
have good accuracy and low complexity according
to our complexity measure. While our complexity
measure informs the biases in our system and leads
to simpler, smaller extractors, it captures extrac-
tor interpretability only to a certain extent. There-
fore, we believe more work is required to devise a
more comprehensive quantitative measure for inter-
pretability, and refine our techniques in order to in-
crease the interpretability of induced rules. Other
interesting directions for future work are introduc-
ing more constructs in our framework, and applying
our techniques to other languages.
137
References
S. Abiteboul, R. Hull, and V. Vianu. 1995. Foundations
of Databases. Addison Wesley Publishing Co.
Douglas E. Appelt and Boyan Onyshkevych. 1998. The
common pattern specification language. In TIPSTER
workshop.
Mary Elaine Califf and Raymond J. Mooney. 1997. Ap-
plying ilp-based techniques to natural language infor-
mation extraction: An experiment in relational learn-
ing. In IJCAI Workshop on Frontiers of Inductive
Logic Programming.
Mary Elaine Califf and Raymond J. Mooney. 1999. Re-
lational learning of pattern-match rules for information
extraction. In AAAI.
Laura Chiticariu, Rajasekar Krishnamurthy, Yunyao Li,
Sriram Raghavan, Frederick R. Reiss, and Shivaku-
mar Vaithyanathan. 2010a. Systemt: an algebraic ap-
proach to declarative information extraction. In ACL.
Laura Chiticariu, Rajasekar Krishnamurthy, Yunyao
Li, Frederick Reiss, and Shivakumar Vaithyanathan.
2010b. Domain adaptation of rule-based annotators
for named-entity recognition tasks. In EMNLP.
Fabio Ciravegna. 2001. (lp)2, an adaptive algorithm for
information extraction from web-related texts. In In
Proceedings of the IJCAI-2001 Workshop on Adaptive
Text Extraction and Mining.
Hamish Cunningham, Diana Maynard, Kalina
Bontcheva, Valentin Tablan, Niraj Aswani, Ian
Roberts, Genevieve Gorrell, Adam Funk, An-
gus Roberts, Danica Damljanovic, Thomas Heitz,
Mark A. Greenwood, Horacio Saggion, Johann
Petrak, Yaoyong Li, and Wim Peters. 2011. Text
Processing with GATE (Version 6).
J. Fu?rnkranz and G. Widmer. 1994. Incremental reduced
error pruning. pages 70?77.
Johannes Fu?rnkranz. 1999. Separate-and-conquer rule
learning. Artif. Intell. Rev., 13(1):3?54, February.
B. R. Gaines and P. Compton. 1995. Induction of ripple-
down rules applied to modeling large databases. J. In-
tell. Inf. Syst., 5:211?228, November.
IBM, 2012. IBM InfoSphere BigInsights - An-
notation Query Language (AQL) reference.
http://publib.boulder.ibm.com/
infocenter/bigins/v1r3/topic/com.
ibm.swg.im.infosphere.biginsights.
doc/doc/biginsights_aqlref_con_
aql-overview.html.
Yunyao Li, Rajasekar Krishnamurthy, Sriram Raghavan,
Shivakumar Vaithyanathan, and H. V. Jagadish. 2008.
Regular expression learning for information extrac-
tion. In EMNLP.
Bin Liu, Laura Chiticariu, Vivian Chu, H. V. Jagadish,
and Frederick R. Reiss. 2010. Automatic rule refine-
ment for information extraction. Proc. VLDB Endow.,
3:588?597.
Diana Maynard, Kalina Bontcheva, and Hamish Cun-
ningham. 2003. Towards a semantic extraction of
named entities. In In Recent Advances in Natural Lan-
guage Processing.
Stephen Muggleton and C. Feng. 1992. Efficient induc-
tion in logic programs. In ILP.
D. Nadeau and S. Sekine. 2007. A survey of named
entity recognition and classification. Linguisticae In-
vestigationes, 30:3?26.
Shan-Hwei Nienhuys-Cheng and Ronald de Wolf. 1997.
Foundations of Inductive Logic Programming.
Anup Patel, Ganesh Ramakrishnan, and Pushpak Bhat-
tacharyya. 2009. Incorporating linguistic expertise
using ilp for named entity recognition in data hungry
indian languages. In ILP.
Frederick Reiss, Sriram Raghavan, Rajasekar Krishna-
murthy, Huaiyu Zhu, and Shivakumar Vaithyanathan.
2008. An algebraic approach to rule-based informa-
tion extraction. In ICDE.
Ellen Riloff. 1993. Automatically constructing a dictio-
nary for information extraction tasks. In AAAI.
Stephen Soderland. 1999. Learning information extrac-
tion rules for semi-structured and free text. Mach.
Learn., 34:233?272.
Erik F. Tjong Kim Sang and Fien De Meulder. 2003.
Introduction to the conll-2003 shared task: language-
independent named entity recognition. In HLT-
NAACL.
Ian H.Witten, Eibe Frank, andMark A. Hall. 2011. Data
Mining: Practical Machine Learning Tools and Tech-
niques. Morgan Kaufmann, Amsterdam, 3rd edition.
138
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 827?832,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Rule-based Information Extraction is Dead!
Long Live Rule-based Information Extraction Systems!
Laura Chiticariu
IBM Research - Almaden
San Jose, CA
chiti@us.ibm.com
Yunyao Li
IBM Research - Almaden
San Jose, CA
yunyaoli@us.ibm.com
Frederick R. Reiss
IBM Research - Almaden
San Jose, CA
frreiss@us.ibm.com
Abstract
The rise of ?Big Data? analytics over unstruc-
tured text has led to renewed interest in infor-
mation extraction (IE). We surveyed the land-
scape of IE technologies and identified a major
disconnect between industry and academia:
while rule-based IE dominates the commercial
world, it is widely regarded as dead-end tech-
nology by the academia. We believe the dis-
connect stems from the way in which the two
communities measure the benefits and costs of
IE, as well as academia?s perception that rule-
based IE is devoid of research challenges. We
make a case for the importance of rule-based
IE to industry practitioners. We then lay out a
research agenda in advancing the state-of-the-
art in rule-based IE systems which we believe
has the potential to bridge the gap between
academic research and industry practice.
1 Introduction
The recent growth of ?Big Data? analytics over large
quantities of unstructured text has led to increased
interest in information extraction technologies from
both academia and industry (Mendel, 2013).
Most recent academic research in this area starts
from the assumption that statistical machine learn-
ing is the best approach to solving information ex-
traction problems. Figure 1 shows empirical ev-
idence of this trend drawn from a survey of re-
cent published research papers. We examined the
EMNLP, ACL, and NAACL conference proceedings
from 2003 through 2012 and identified 177 different
EMNLP research papers on the topic of entity ex-
traction. We then classified these papers into three
categories, based on the techniques used: purely
Commercial*Vendors*(2013)*NLP*Papers*(200392012)*
100%$
50%$
0%$
3.5%*
21%$
75%$
Rule,$Based$
Hybrid$
Machine$Learning$Based$
45%*
22%$
33%$
Implementa@ons*of*En@ty*Extrac@on*
Large*Vendors*
67%*
17%$
17%$
All*Vendors*
Figure 1: Fraction of NLP conference papers from
EMNLP, ACL, and NAACL over 10 years that use ma-
chine learning versus rule-based techniques to perform
entity extraction over text (left); the same breakdown for
commercial entity extraction vendors one year after the
end of this 10-year period (right). The rule-based ap-
proach, although largely ignored in the research commu-
nity, dominates the commercial market.
rule-based, purely machine learning-based, or a hy-
brid of the two. We focus on entity extraction, as it
is a classical IE task, and most industrial IE systems
offer this feature.
The left side of the graph shows the breakdown
of research papers according to this categorization.
Only six papers relied solely on rules to perform the
extraction tasks described. The remainder relied en-
tirely or substantially on statistical techniques. As
shown in Figure 2, these fractions were roughly con-
stant across the 10-year period studied, indicating
that attitudes regarding the relative importance of the
different techniques have remained constant.
We found that distinguishing ?hybrid? systems
827
En@ty*Extrac@on*Papers*by*Year*
0%$
25%$
50%$
75%$
100%$
Year$of$PublicaAon$
FracAo
n$of$N
LP$Pap
ers$ Hybrid$
Machine$Learning$Based$
Rule,$Based$
Figure 2: The conference paper data (left-hand bar) from
Figure 1, broken down by year of publication. The rel-
ative fractions of the three different techniques have not
changed significantly over time.
from pure machine learning systems was quite chal-
lenging. The papers that use a mixture of rule-
based and machine learning techniques were gener-
ally written so as to obfuscate the use of rules, em-
phasizing the machine learning aspect of the work.
Authors hid rules behind euphemisms such as ?de-
pendency restrictions? (Mausam et al, 2012), ?en-
tity type constraints? (Yao et al, 2011), or ?seed dic-
tionaries? (Putthividhya and Hu, 2011).
In the commercial world, the situation is largely
reversed. The right side of Figure 1 shows the result
of a parallel survey of commercial entity extraction
products from 54 different vendors listed in (Yuen
and Koehler-Kruener, 2012). We studied analyst
reports and product literature, then classified each
product according to the same three categories. Ta-
ble 1 shows the 41 products considered in the study
1. We conducted this industry survey in 2013, one
year after the ten-year run of NLP papers we stud-
ied. One would expect the industrial landscape to
reflect the research efforts of the previous 10 years,
as mature technology moved from academia to in-
dustry. Instead, results of this second survey showed
the opposite effect, with rule-based systems com-
prising the largest fraction of those surveyed. Only
1/3 of the vendors relied entirely on machine learn-
ing. Among public companies and private compa-
1Other products do not offer entity extraction, or we did not
find sufficient evidence to classify the technology.
Table 1: Vendors and products considered in the study.
ai-one NathanApp
Attensity Command Center
Basis Technology Rosette
Clarabridge Analyze
Daedalus Stilus NER
GATE Information Extraction
General Sentiment
HP Autonomy IDOL Eduction
IBM InfoSphere BigInsights Text
Analytics
IBM InfoSphere Streams Text An-
alytics
IBM SPSS Text Analytics for Sur-
veys
IntraFind iFinder NAMER
IxReveal uHarmonize
Knime
Language Computer Cicero LITE
Lexanalytics Salience
alias-i LingPipe
Marklogic Analytics & Business Intelli-
gence
MeshLabs eZi CORE
Microsoft FAST Search Server
MotiveQuest
Nice Systems NiceTrack Open Source In-
telligence
OpenAmplify Insights
OpenText Content Analytics
Pingar
Provalis Research WordStat
Rapid-I Text Processing Extension
Rocket AeroText
salesforce.com Radian 6
SAP HANA Text Analysis
SAS Text Analytics
Serendio
Smartlogic Semaphore Classification
and Text Mining Server
SRA International NetOwl Text Analytics
StatSoft STATISTICA Text Miner
Temis Luxid Content Enrichment
Platform
Teradata (integration w/ Attensity)
TextKernel Extract!
Thompson Reuters OpenCalais
Veda Semantics Entity Identifier
ZyLab Text Mining&Analytics
828
Table 2: Pros and Cons
Pros Cons
R
ul
e-
ba
se
d
? Declarative ? Heuristic
? Easy to comprehend ? Requires tedious
? Easy to maintain manual labor
? Easy to incorporate
domain knowledge
? Easy to trace and fix
the cause of errors
M
L
-b
as
ed
? Trainable ? Requires labeled data
? Adaptable ? Requires retraining
? Reduces manual for domain adaptation
effort ? Requires ML expertise
to use or maintain
? Opaque
nies with more than $100 million in revenue, the sit-
uation is even more skewed towards rule-based sys-
tems, with large vendors such as IBM, SAP, and Mi-
crosoft being completely rule-based.
2 Explaining the Disconnect
What is the source of this disconnect between re-
search and industry? There does not appear to be
a lack of interaction between the two communities.
Indeed, many of the smaller companies we surveyed
were founded by NLP researchers, and many of the
larger vendors actively publish in the NLP literature.
We believe that the disconnect arises from a differ-
ence in how the two communities measure the costs
and benefits of information extraction.
Table 2 summarizes the pros and cons of machine
learning (ML) and rule-based IE technologies (Atz-
mueller and Kluegl, 2008; Grimes, 2011; Leung et
al., 2011; Feldman and Rosenfeld, 2006; Guo et
al., 2006; Krishnan et al, 2005; Yakushiji et al,
2006; Kluegl et al, 2009). On the surface, both
academia and commercial vendors acknowledge es-
sentially the same pros and cons for the two ap-
proaches. However, the two communities weight the
pros and cons significantly differently, leading to the
drastic disconnect in Figure 1.
Evaluating the benefits of IE. Academic papers
evaluate IE performance in terms of precision and
recall over standard labeled data sets. This simple,
clean, and objective measure is useful for judging
competitions, but the reality of the business world is
much more fluid and less well-defined.
In a business context, definitions of even basic en-
tities like ?product? and ?revenue? vary widely from
one company to another. Within any of these ill-
defined categories, some entities are more important
to get right than others. For example, in electronic
legal discovery, correctly identifying names of ex-
ecutives is much more important than finding other
types of person names.
In real-world applications, the output of extrac-
tion is often the input to a larger process, and it
is the quality of the larger process that drives busi-
ness value. This quality may derive from an aspect
of extracted output that is only loosely correlated
with overall precision and recall. For example, does
extracted sentiment, when broken down and aggre-
gated by product, produce an unbiased estimate of
average sentiment polarity for each product?
To be useful in a business context, IE must func-
tion well with metrics that are ill-defined and sub-
ject to change. ML-based IE models, which require
a careful up-front definition of the IE task, are poor
fit for these metrics. The commercial world greatly
values rule-based IE for its interpretability, which
makes IE programs easier to adopt, understand, de-
bug, and maintain in the face of changing require-
ments (Kluegl et al, 2009; Atzmueller and Kluegl,
2008). Furthermore, rule-based IE programs are val-
ued for allowing one to easily incorporate domain
knowledge, which is essential for targeting specific
business problems (Grimes, 2011). As an example,
an application may pose simple requirements to its
entity recognition component to output only full per-
son names, and not include salutation. With a rule-
based system, such a requirement translates to re-
moving a few rules. On the other hand, a ML-based
approach requires a complete retrain.
Evaluating the costs of IE. In a business setting,
the most significant costs of using information ex-
traction are the labor cost of developing or adapting
extractors for a particular business problem, and the
hardware cost of compute resources required by the
system.
NLP researchers generally have a well-developed
sense of the labor cost of writing extraction rules,
viewing this task as a ?tedious and time-consuming
process? that ?is not really practical? (Yakushiji et
al., 2006). These criticisms are valid, and, as we
829
point out in the next section, they motivate a research
effort to build better languages and tools.
But there is a strong tendency in the NLP lit-
erature to ignore the complex and time-consuming
tasks inherent in solving an extraction problem using
machine learning. These tasks include: defining the
business problem to be solved in strict mathematical
terms; understanding the tradeoffs between different
types of models in the context of the NLP task def-
inition; performing feature engineering based on a
solid working understanding of the chosen model;
and gathering extensive labeled data ? far more
than is needed to measure precision and recall ?
often through clever automation.
All these steps are time-consuming; even highly-
qualified workers with postgraduate degrees rou-
tinely fail to execute them effectively. Not sur-
prisingly, in industry, ML-based systems are often
deemed risky to adopt and difficult to understand
and maintain, largely due to model opaqueness (Fry,
2011; Wagstaff, 2012; Malioutov and Varshney,
2013). The infeasibility of gathering labeled data in
many real-world scenarios further increases the risk
of committing to a ML-based solution.
A measure of the system?s scalability and run-
time efficiency, hardware costs are a function of two
metrics: throughput and memory footprint. These
figures, while extremely important for commercial
vendors, are typically not reported in NLP litera-
ture. Nevertheless, our experience in practice sug-
gests that ML-based approaches are much slower,
and require more memory compared to rule-based
approaches, whose throughput can be in the order
of MB/second/core for complex extraction tasks like
NER (Chiticariu et al, 2010).
The other explanation. Finally, we believe that the
most notable reason behind the academic commu-
nity?s steering away from rule-based IE systems is
the (false) perception of lack of research problems.
The general attitude is one of ?What?s the research
in rule-based IE? Just go ahead and write the rules.?
as indicated by anecdotal evidence and only implic-
itly stated in the literature, where any usage of rules
is significantly underplayed as explained earlier. In
the next section, we strive to debunk this perception.
3 Bridging the Gap
As NLP researchers who also work regularly with
business customers, we have become increasingly
worried about the gap in perception between infor-
mation extraction research and industry. The recent
growth of Big Data analytics has turned IE into big
business (Mendel, 2013). If current trends continue,
the business world will move ahead with unprinci-
pled, ad-hoc solutions to customers? business prob-
lems, while researchers pursue ever more complex
and impractical statistical approaches that become
increasingly irrelevant. Eventually, the gap between
research and practice will become insurmountable,
an outcome in neither community?s best interest.
The academic NLP community needs to stop
treating rule-based IE as a dead-end technology. As
discussed in Section 2, the domination of rule-based
IE systems in the industry is well-justified. Even in
their current form, with ad-hoc solutions built on
techniques from the early 1980?s, rule-based sys-
tems serve the industry needs better than the lat-
est ML techniques. Nonetheless, there is an enor-
mous untapped opportunity for researchers to make
the rule-based approach more principled, effective,
and efficient. In the remainder of this section, we
lay out a research agenda centered around captur-
ing this opportunity. Specifically, taking a systemic
approach to rule-based IE, one can identify a set of
research problems by separating rule development
and deployment. In particular, we believe research
should focus on: (a) data models and rule language,
(b) systems research in rule evaluation and (c) ma-
chine learning research for learning problems in this
richer target language.
Define standard IE rule language and data
model. If research on rule-based IE is to move
forward in a principled way, the community needs
a standard way to express rules. We believe that
the NLP community can replicate the success of
the SQL language in connecting data management
research and practice. SQL has been successful
largely due to: (1) expressivity: the language pro-
vides all primitives required for performing basic
manipulation of structured data, (2) extensibility: the
language can be extended with new features without
fundamental changes to the language, (3) declara-
tivity: the language allows the specification of com-
830
putation logic without describing its control flow,
thus allowing developers to code what the program
should accomplish, rather than how to accomplish it.
An earlier attempt in late 1980?s to formal-
ize a rule language resulted in the Common Pat-
tern Specification Language (CPSL) (Appelt and
Onyshkevych, 1998). While CPSL did not suc-
ceed due to multiple drawbacks, including expres-
sivity limitations, performance limitations, and its
lack of support for core operations such as part of
speech (Chiticariu et al, 2010), CPSL did gain some
traction, e.g., it powers the JAPE language of the
GATE open-source NLP system (Cunningham et al,
2011). Meanwhile, a number of declarative IE lan-
guages developed in the database community, in-
cluding AQL (Chiticariu et al, 2010; Li et al, 2011),
xLog (Shen et al, 2007), and SQL extensions (Wang
et al, 2010; Jain et al, 2009), have shown that for-
malisms of rule-based IE systems are possible, as
exemplified by (Fagin et al, 2013). However, they
largely remain unknown in the NLP community.
We believe now is the right time to establish a
standard IE rule language, drawing from existing
proposals and experience over the past 30 years. To-
wards this goal, IE researchers need to answer the
following questions: What is the right data model to
capture text, annotations over text, and their proper-
ties? Can we establish a standard declarative exten-
sible rule language for processing data in this model
with a clear set of constructs that is sufficiently ex-
pressive to solve most IE tasks encountered so far?
Systems research based on standard IE rule lan-
guage. Standard IE data model and language en-
ables the development of systems implementing the
standard. One may again wonder, ?Where is the re-
search in that?? As in the database community, ini-
tial research should focus on systemic issues such
as data representation and speeding up rule evalua-
tion via automatic performance optimization. Once
baseline systems are established, system-related re-
search would naturally diverge in several directions,
such as extending the language with new primitives
(and corresponding optimizations), and exploring
modern hardware.
ML research based on standard IE rule language.
A standard rule language and corresponding execu-
tion engine enables researchers to use the standard
language as the expressivity of the output model,
and define learning problems for this target lan-
guage, including learning basic primitives such as
regular expressions and dictionaries, or complete
rule sets. (One need not worry about choosing the
language, nor runtime efficiency.) With an expres-
sive rule language, a major challenge is to prevent
the system from generating arbitrarily complex rule
sets, which would be difficult to understand or main-
tain. Some interesting research directions include
devising proper measures for rule complexity, con-
straining the search space such that the learnt rules
closely resemble those written by humans, active
learning techniques to cope with scarcity of labeled
data, and visualization tools to assist rule develop-
ers in exploring and choosing between different au-
tomatically generated rules. Finally, it is conceiv-
able that some problems will not fit in the target
language, and therefore will need alternative solu-
tions. However, the community would have shown
? objectively ? that the problem is not learnable
with the available set of constructs, thus motivating
follow-on research on extending the standard with
new primitives, if possible, or developing novel hy-
brid IE solutions by leveraging the standard IE rule
language together with ML technology.
4 Conclusion
While rule-based IE dominates the commercial
world, it is widely considered obsolete by the
academia. We made a case for the importance
of rule-based approaches to industry practitioners.
Drawing inspiration from the success of SQL and
the database community, we proposed directions
for addressing the disconnect. Specifically, we call
for the standardization of an IE rule language and
outline an ambitious research agenda for NLP re-
searchers who wish to tackle research problems of
wide interest and value in the industry.
Acknowledgments
We would like to thank our colleagues, Howard
Ho, Rajasekar Krishnamurthy, and Shivakumar
Vaithyanathan, as well as the anonymous reviewers
for their thoughtful and constructive comments.
831
References
Douglas E. Appelt and Boyan Onyshkevych. 1998. The
Common Pattern Specification Language. In Proceed-
ings of a workshop held at Baltimore, Maryland: Oc-
tober 13-15, 1998, TIPSTER ?98, pages 23?30.
Martin Atzmueller and Peter Kluegl. 2008. Rule-Based
Information Extraction for Structured Data Acquisi-
tion using TextMarker. In LWA.
Laura Chiticariu, Rajasekar Krishnamurthy, Yunyao Li,
Sriram Raghavan, Frederick Reiss, and Shivakumar
Vaithyanathan. 2010. SystemT: An Algebraic Ap-
proach to Declarative Information Extraction. In ACL.
Hamish Cunningham, Diana Maynard, Kalina
Bontcheva, Valentin Tablan, Niraj Aswani, Ian
Roberts, Genevieve Gorrell, Adam Funk, An-
gus Roberts, Danica Damljanovic, Thomas Heitz,
Mark A. Greenwood, Horacio Saggion, Johann
Petrak, Yaoyong Li, and Wim Peters. 2011. Text
Processing with GATE (Version 6), Chapter 8: JAPE:
Regular Expressions over Annotations.
Ronald Fagin, Benny Kimelfeld, Frederick Reiss, and
Stijn Vansummeren. 2013. Spanners: a formal frame-
work for information extraction. In PODS.
Ronen Feldman and Benjamin Rosenfeld. 2006. Boost-
ing Unsupervised Relation Extraction by Using NER.
In EMNLP, pages 473?481.
C. Fry. 2011. Closing the Gap between Analytics and
Action. INFORMS Analytics Mag., 4(6):405.
Seth Grimes. 2011. Text/Content Analyt-
ics 2011: User Perspectives on Solutions.
http://www.medallia.com/resources/item/
text-analytics-market-study/ .
Hong Lei Guo, Li Zhang, and Zhong Su. 2006.
Empirical study on the performance stability of
named entity recognition model across domains.
In EMNLP, pages 509?516.
Alpa Jain, Panagiotis Ipeirotis, and Luis Gravano.
2009. Building query optimizers for information
extraction: the sqout project. SIGMOD Record,
37(4):28?34.
Peter Kluegl, Martin Atzmueller, and Frank Puppe.
2009. TextMarker: A Tool for Rule-Based Infor-
mation Extraction. In UIMA@GSCL Workshop,
pages 233?240.
Vijay Krishnan, Sujatha Das, and Soumen
Chakrabarti. 2005. Enhanced answer type
inference from questions using sequential mod-
els. In HLT, pages 315?322.
Cane Wing-ki Leung, Jing Jiang, Kian Ming A.
Chai, Hai Leong Chieu, and Loo-Nin Teow.
2011. Unsupervised Information Extraction with
Distributional Prior Knowledge. In EMNLP,
pages 814?824.
Yunyao Li, Frederick Reiss, and Laura Chiticariu.
2011. Systemt: A declarative information extrac-
tion system. In ACL.
Dmitry M. Malioutov and Kush R. Varshney. 2013.
Exact rule learning via boolean compressed sens-
ing. In ICML.
Mausam, Michael Schmitz, Stephen Soderland,
Robert Bart, and Oren Etzioni. 2012. Open Lan-
guage Learning for Information Extraction. In
EMNLP-CoNLL, pages 523?534.
Thomas Mendel. 2013. Business
Intelligence and Big Data Trends
2013. http://www.hfsresearch.com/
Business-Intelligence-and-Big-Data-Trends-2013
(accessed March 28th, 2013).
Duangmanee Putthividhya and Junling Hu. 2011.
Bootstrapped Named Entity Recognition for
Product Attribute Extraction. In EMNLP, pages
1557?1567.
Warren Shen, AnHai Doan, Jeffrey F. Naughton, and
Raghu Ramakrishnan. 2007. Declarative Infor-
mation Extraction Using Datalog with Embedded
Extraction Predicates. In VLDB, pages 1033?
1044.
Kiri Wagstaff. 2012. Machine learning that matters.
In ICML.
Daisy Zhe Wang, Eirinaios Michelakis, Michael J.
Franklin, Minos N. Garofalakis, and Joseph M.
Hellerstein. 2010. Probabilistic Declarative In-
formation Extraction. In ICDE.
Akane Yakushiji, Yusuke Miyao, Tomoko Ohta,
Yuka Tateisi, and Jun?ichi Tsujii. 2006. Auto-
matic construction of predicate-argument struc-
ture patterns for biomedical information extrac-
tion. In EMNLP, pages 284?292.
Limin Yao, Aria Haghighi, Sebastian Riedel, and
Andrew McCallum. 2011. Structured Relation
Discovery using Generative Models. In EMNLP,
pages 1456?1466.
Daniel Yuen and Hanns Koehler-Kruener. 2012.
Who?s Who in Text Analytics, September.
832
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 128?137,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
SystemT: An Algebraic Approach to Declarative Information Extraction
Laura Chiticariu Rajasekar Krishnamurthy Yunyao Li
Sriram Raghavan Frederick R. Reiss Shivakumar Vaithyanathan
IBM Research ? Almaden
San Jose, CA, USA
{chiti,sekar,yunyaoli,rsriram,frreiss,vaithyan}@us.ibm.com
Abstract
As information extraction (IE) becomes
more central to enterprise applications,
rule-based IE engines have become in-
creasingly important. In this paper, we
describe SystemT, a rule-based IE sys-
tem whose basic design removes the ex-
pressivity and performance limitations of
current systems based on cascading gram-
mars. SystemT uses a declarative rule
language, AQL, and an optimizer that
generates high-performance algebraic ex-
ecution plans for AQL rules. We com-
pare SystemT?s approach against cascad-
ing grammars, both theoretically and with
a thorough experimental evaluation. Our
results show that SystemT can deliver re-
sult quality comparable to the state-of-the-
art and an order of magnitude higher an-
notation throughput.
1 Introduction
In recent years, enterprises have seen the emer-
gence of important text analytics applications like
compliance and data redaction. This increase,
combined with the inclusion of text into traditional
applications like Business Intelligence, has dra-
matically increased the use of information extrac-
tion (IE) within the enterprise. While the tradi-
tional requirement of extraction quality remains
critical, enterprise applications also demand ef-
ficiency, transparency, customizability and main-
tainability. In recent years, these systemic require-
ments have led to renewed interest in rule-based
IE systems (Doan et al, 2008; SAP, 2010; IBM,
2010; SAS, 2010).
Until recently, rule-based IE systems (Cunning-
ham et al, 2000; Boguraev, 2003; Drozdzynski
et al, 2004) were predominantly based on the
cascading grammar formalism exemplified by the
Common Pattern Specification Language (CPSL)
specification (Appelt and Onyshkevych, 1998). In
CPSL, the input text is viewed as a sequence of an-
notations, and extraction rules are written as pat-
tern/action rules over the lexical features of these
annotations. In a single phase of the grammar, a
set of rules are evaluated in a left-to-right fash-
ion over the input annotations. Multiple grammar
phases are cascaded together, with the evaluation
proceeding in a bottom-up fashion.
As demonstrated by prior work (Grishman and
Sundheim, 1996), grammar-based IE systems can
be effective in many scenarios. However, these
systems suffer from two severe drawbacks. First,
the expressivity of CPSL falls short when used
for complex IE tasks over increasingly pervasive
informal text (emails, blogs, discussion forums
etc.). To address this limitation, grammar-based
IE systems resort to significant amounts of user-
defined code in the rules, combined with pre-
and post-processing stages beyond the scope of
CPSL (Cunningham et al, 2010). Second, the
rigid evaluation order imposed in these systems
has significant performance implications.
Three decades ago, the database community
faced similar expressivity and efficiency chal-
lenges in accessing structured information. The
community addressed these problems by introduc-
ing a relational algebra formalism and an associ-
ated declarative query language SQL. The ground-
breaking work on System R (Chamberlin et al,
1981) demonstrated how the expressivity of SQL
can be efficiently realized in practice by means of
a query optimizer that translates an SQL query into
an optimized query execution plan.
Borrowing ideas from the database community,
we have developed SystemT, a declarative IE sys-
tem based on an algebraic framework, to address
both expressivity and performance issues. In Sys-
temT, extraction rules are expressed in a declar-
ative language called AQL. At compilation time,
128
({First} {Last} ) :full :full.Person
({Caps}  {Last} ) :full :full.Person
({Last} {Token.orth = comma} {Caps | First}) : reverse
:reverse.Person
({First}) : fn  :fn.Person
({Last}) : ln  :ln.Person
({Lookup.majorType = FirstGaz})  : fn  :fn.First
({Lookup.majorType = LastGaz}) : ln  :ln.Last
({Token.orth = upperInitial} | 
{Token.orth = mixedCaps } ) :cw  :cw.Caps
Rule Patterns
50
20
10
10
10
50
50
10
Priority
P2R1
P2R2
P2R3
P2R4
P2R5
P1R1
P1R2
P1R3
RuleId
Input
First
Last
Caps
Token
Output
Person
Input
Lookup
Token
Output
First
Last
Caps
TypesPhase
P2
P1
P2R3        ({Last} {Token.orth = comma} {Caps | First}) : reverse   :reverse.Person
Last followed by Token whose orth attribute has value 
comma followed by Caps or First
Rule part Action part
Create Person
annotation
Bind match 
to variables
Syntax:
Gazetteers containing first names and last names
Figure 1: Cascading grammar for identifying Person names
SystemT translates AQL statements into an al-
gebraic expression called an operator graph that
implements the semantics of the statements. The
SystemT optimizer then picks a fast execution
plan from many logically equivalent plans. Sys-
temT is currently deployed in a multitude of real-
world applications and commercial products1.
We formally demonstrate the superiority of
AQL and SystemT in terms of both expressivity
and efficiency (Section 4). Specifically, we show
that 1) the expressivity of AQL is a strict superset
of CPSL grammars not using external functions
and 2) the search space explored by the SystemT
optimizer includes operator graphs correspond-
ing to efficient finite state transducer implemen-
tations. Finally, we present an extensive experi-
mental evaluation that validates that high-quality
annotators can be developed with SystemT, and
that their runtime performance is an order of mag-
nitude better when compared to annotators devel-
oped with a state-of-the-art grammar-based IE sys-
tem (Section 5).
2 Grammar-based Systems and CPSL
A cascading grammar consists of a sequence of
phases, each of which consists of one or more
rules. Each phase applies its rules from left to
right over an input sequence of annotations and
generates an output sequence of annotations that
the next phase consumes. Most cascading gram-
mar systems today adhere to the CPSL standard.
Fig. 1 shows a sample CPSL grammar that iden-
tifies person names from text in two phases. The
first phase, P1, operates over the results of the tok-
1A trial version is available at
http://www.alphaworks.ibm.com/tech/systemt
Rule skipped 
due to priority 
semantics
CPSL
Phase P1
Last(P1R2) Last(P1R2)
? Mark        Scott     ,            Howard         Smith   ?
First(P1R1) First(P1R1) First(P1R1) Last(P1R2)
CPSL
Phase P2
? Mark        Scott     ,        Howard          Smith    ?
Person(P2R1)
Person (P2R4)
Person(P2R4)
Person (P2R5)
Person(P2R4)
? Mark        Scott     ,            Howard         Smith   ?
First(P1R1) First(P1R1) First(P1R1) Last(P1R2)
JAPE
Phase P1
(Brill) Caps(P1R3) Last(P1R2) Last(P1R2)
Caps(P1R3) Caps(P1R3)
Caps(P1R3)
? Mark        Scott     ,        Howard          Smith    ?
Person(P2R1)
Person (P2R4, P2R5)
JAPE
Phase P2
(Appelt)
Person(P2R1)
Person (P2R2)
Some discarded 
matches omitted
for clarity
? Tomorrow, we will meet Mark Scott, Howard Smith and ?Document d1
Rule fired
Legend
3 persons
identified
2 persons
identified
(a)
(b)
Figure 2: Sample output of CPSL and JAPE
enizer and gazetteer (input types Token and Lookup,
respectively) to identify words that may be part of
a person name. The second phase, P2, identifies
complete names using the results of phase P1.
Applying the above grammar to document d1
(Fig. 2), one would expect that to match ?Mark
Scott? and ?Howard Smith? as Person. However,
as shown in Fig. 2(a), the grammar actually finds
three Person annotations, instead of two. CPSL has
several limitations that lead to such discrepancies:
L1. Lossy sequencing. In a CPSL grammar,
each phase operates on a sequence of annotations
from left to right. If the input annotations to a
phase may overlap with each other, the CPSL en-
gine must drop some of them to create a non-
overlapping sequence. For instance, in phase P1
(Fig. 2(a)), ?Scott? has both a Lookup and a To-
ken annotation. The system has made an arbitrary
choice to retain the Lookup annotation and discard
the Token annotation. Consequently, no Caps anno-
tations are output by phase P1.
L2. Rigid matching priority. CPSL specifies
that, for each input annotation, only one rule can
actually match. When multiple rules match at the
same start position, the following tie-breaker con-
ditions are applied (in order): (a) the rule match-
ing the most annotations in the input stream; (b)
the rule with highest priority; and (c) the rule de-
clared earlier in the grammar. This rigid match-
ing priority can lead to mistakes. For instance,
as illustrated in Fig. 2(a), phase P1 only identi-
fies ?Scott? as a First. Matching priority causes
the grammar to skip the corresponding match for
?Scott? as a Last. Consequently, phase P2 fails to
identify ?Mark Scott? as one single Person.
L3. Limited expressivity in rule patterns. It is
not possible to express rules that compare annota-
tions overlapping with each other. E.g., ?Identify
129
[A-Z]{\w|-}+
DocumentInput Tuple
?
we will meet Mark 
Scott, ?
Output Tuple 2 Span 2Document
Span 1Output Tuple 1 Document
Regex
Caps
Figure 3: Regular Expression Extraction Operator
words that are both capitalized and present in the
FirstGaz gazetteer? or ?Identify Person annotations
that occur within an EmailAddress?.
Extensions to CPSL
In order to address the above limitations, several
extensions to CPSL have been proposed in JAPE,
AFst and XTDL (Cunningham et al, 2000; Bogu-
raev, 2003; Drozdzynski et al, 2004). The exten-
sions are summarized as below, where each solu-
tion Si corresponds to limitation Li.
? S1. Grammar rules are allowed to operate on
graphs of input annotations in JAPE and AFst.
? S2. JAPE introduces more matching regimes
besides the CPSL?s matching priority and thus
allows more flexibility when multiple rules
match at the same starting position.
? S3. The rule part of a pattern has been ex-
panded to allow more expressivity in JAPE,
AFst and XTDL.
Fig. 2(b) illustrates how the above extensions
help in identifying the correct matches ?Mark Scott?
and ?Howard Smith? in JAPE. Phase P1 uses a match-
ing regime (denoted by Brill) that allows multiple
rules to match at the same starting position, and
phase P2 uses CPSL?s matching priority, Appelt.
3 SystemT
SystemT is a declarative IE system based on an
algebraic framework. In SystemT, developers
write rules in a language called AQL. The system
then generates a graph of operators that imple-
ment the semantics of the AQL rules. This decou-
pling allows for greater rule expressivity, because
the rule language is not constrained by the need to
compile to a finite state transducer. Likewise, the
decoupled approach leads to greater flexibility in
choosing an efficient execution strategy, because
many possible operator graphs may exist for the
same AQL annotator.
In the rest of the section, we describe the parts
of SystemT, starting with the algebraic formalism
behind SystemT?s operators.
3.1 Algebraic Foundation of SystemT
SystemT executes IE rules using graphs of op-
erators. The formal definition of these operators
takes the form of an algebra that is similar to the
relational algebra, but with extensions for text pro-
cessing.
The algebra operates over a simple relational
data model with three data types: span, tuple, and
relation. In this data model, a span is a region of
text within a document identified by its ?begin?
and ?end? positions; a tuple is a fixed-size list of
spans. A relation is a multiset of tuples, where ev-
ery tuple in the relation must be of the same size.
Each operator in our algebra implements a single
basic atomic IE operation, producing and consum-
ing sets of tuples.
Fig. 3 illustrates the regular expression ex-
traction operator in the algebra, which per-
forms character-level regular expression match-
ing. Overall, the algebra contains 12 different op-
erators, a full description of which can be found
in (Reiss et al, 2008). The following four oper-
ators are necessary to understand the examples in
this paper:
? The Extract operator (E) performs character-
level operations such as regular expression and
dictionary matching over text, creating a tuple
for each match.
? The Select operator (?) takes as input a set of
tuples and a predicate to apply to the tuples. It
outputs all tuples that satisfy the predicate.
? The Join operator (??) takes as input two sets
of tuples and a predicate to apply to pairs of
tuples from the input sets. It outputs all pairs
of input tuples that satisfy the predicate.
? The consolidate operator (?) takes as input a
set of tuples and the index of a particular col-
umn in those tuples. It removes selected over-
lapping spans from the indicated column, ac-
cording to the specified policy.
3.2 AQL
Extraction rules in SystemT are written in AQL,
a declarative relational language similar in syn-
tax to the database language SQL. We chose SQL
as a basis for our language due to its expres-
sivity and its familiarity. The expressivity of
SQL, which consists of first-order logic predicates
130
Figure 4: Person annotator as AQL query
over sets of tuples, is well-documented and well-
understood (Codd, 1990). As SQL is the pri-
mary interface to most relational database sys-
tems, the language?s syntax and semantics are
common knowledge among enterprise application
programmers. Similar to SQL terminology, we
call a collection of AQL rules an AQL query.
Fig. 4 shows portions of an AQL query. As
can be seen, the basic building block of AQL is
a view: A logical description of a set of tuples in
terms of either the document text (denoted by a
special view called Document) or the contents of
other views. Every SystemT annotator consists
of at least one view. The output view statement in-
dicates that the tuples in a view are part of the final
results of the annotator.
Fig. 4 also illustrates three of the basic con-
structs that can be used to define a view.
? The extract statement specifies basic
character-level extraction primitives to be
applied directly to a tuple.
? The select statement is similar to the SQL
select statement but it contains an additional
consolidate on clause, along with an exten-
sive collection of text-specific predicates.
? The union all statement merges the outputs
of one or more select or extract statements.
To keep rules compact, AQL also provides a
shorthand sequence pattern notation similar to the
syntax of CPSL. For example, the CapsLast
view in Figure 4 could have been written as:
create view CapsLast as
extract pattern <C.name> <L.name>
from Caps C, Last L;
Internally, SystemT translates each of these ex-
tract pattern statements into one or more select
and extract statements.
AQL SystemT
Optimizer
SystemT
Runtime
Compiled
Operator
Graph
Figure 5: The compilation process in SystemT
Figure 6: Execution strategies for the CapsLast rule
in Fig. 4
SystemT has built-in multilingual support in-
cluding tokenization, part of speech and gazetteer
matching for over 20 languages using Language-
Ware (IBM, 2010). Rule developers can utilize
the multilingual support via AQL without hav-
ing to configure or manage any additional re-
sources. In addition, AQL allows user-defined
functions to be used in a restricted context in or-
der to support operations such as validation (e.g.
for extracted credit card numbers), or normaliza-
tion (e.g., compute abbreviations of multi-token
organization candidates that are useful in gener-
ating additional candidates). More details on AQL
can be found in the AQL manual (SystemT, 2010).
3.3 Optimizer and Operator Graph
Grammar-based IE engines place rigid restrictions
on the order in which rules can be executed. Due
to the semantics of the CPSL standard, systems
that implement the standard must use a finite state
transducer that evaluates each level of the cascade
with one or more left to right passes over the entire
token stream.
In contrast, SystemT places no explicit con-
straints on the order of rule evaluation, nor does
it require that intermediate results of an annota-
tor collapse to a fixed-size sequence. As shown in
Fig. 5, the SystemT engine does not execute AQL
directly; instead, the SystemT optimizer compiles
AQL into a graph of operators. By tying a collec-
tion of operators together by their inputs and out-
puts, the system can implement a wide variety of
different execution strategies. Different execution
strategies are associated with different evaluation
costs. The optimizer chooses the execution strat-
egy with the lowest estimated evaluation cost.
131
Fig. 6 presents three possible execution strate-
gies for the CapsLast rule in Fig. 4. If the opti-
mizer estimates that the evaluation cost of Last is
much lower than that of Caps, then it can deter-
mine that Plan C has the lowest evaluation cost
among the three, because Plan C only evaluates
Caps in the ?left? neighborhood for each instance
of Last. More details of our algorithms for enumer-
ating plans can be found in (Reiss et al, 2008).
The optimizer in SystemT chooses the best ex-
ecution plan from a large number of different al-
gebra graphs available to it. Many of these graphs
implement strategies that a transducer could not
express: such as evaluating rules from right to left,
sharing work across different rules, or selectively
skipping rule evaluations. Within this large search
space, there generally exists an execution strategy
that implements the rule semantics far more effi-
ciently than the fastest transducer could. We refer
the reader to (Reiss et al, 2008) for a detailed de-
scription of the types of plan the optimizer consid-
ers, as well as an experimental analysis of the per-
formance benefits of different parts of this search
space.
Several parallel efforts have been made recently
to improve the efficiency of IE tasks by optimiz-
ing low-level feature extraction (Ramakrishnan et
al., 2006; Ramakrishnan et al, 2008; Chandel et
al., 2006) or by reordering operations at a macro-
scopic level (Ipeirotis et al, 2006; Shen et al,
2007; Jain et al, 2009). However, to the best of
our knowledge, SystemT is the only IE system
in which the optimizer generates a full end-to-end
plan, beginning with low-level extraction primi-
tives and ending with the final output tuples.
3.4 Deployment Scenarios
SystemT is designed to be usable in various de-
ployment scenarios. It can be used as a stand-
alone system with its own development and run-
time environment. Furthermore, SystemT ex-
poses a generic Java API that enables the integra-
tion of its runtime environment with other applica-
tions. For example, a specific instantiation of this
API allows SystemT annotators to be seamlessly
embedded in applications using the UIMA analyt-
ics framework (UIMA, 2010).
4 Grammar vs. Algebra
Having described both the traditional cascading
grammar approach and the declarative approach
Figure 7: Supporting Complex Rule Interactions
used in SystemT, we now compare the two in
terms of expressivity and performance.
4.1 Expressivity
In Section 2, we described three expressivity lim-
itations of CPSL grammars: Lossy sequencing,
rigid matching priority, and limited expressivity in
rule patterns. As we noted, cascading grammar
systems extend the CPSL specification in various
ways to provide workarounds for these limitations.
In SystemT, the basic design of the AQL lan-
guage eliminates these three problems without the
need for any special workaround. The key design
difference is that AQL views operate over sets of
tuples, not sequences of tokens. The input or out-
put tuples of a view can contain spans that overlap
in arbitrary ways, so the lossy sequencing prob-
lem never occurs. The annotator will retain these
overlapping spans across any number of views un-
til a view definition explicitly removes the over-
lap. Likewise, the tuples that a given view pro-
duces are in no way constrained by the outputs of
other, unrelated views, so the rigid matching prior-
ity problem never occurs. Finally, the select state-
ment in AQL allows arbitrary predicates over the
cross-product of its input tuple sets, eliminating
the limited expressivity in rule patterns problem.
Beyond eliminating the major limitations of
CPSL grammars, AQL provides a number of other
information extraction operations that even ex-
tended CPSL cannot express without custom code.
Complex rule interactions. Consider an exam-
ple document from the Enron corpus (Minkov et
al., 2005), shown in Fig. 7, which contains a list
of person names. Because the first person in the
list (?Skilling?) is referred to by only a last name,
rule P2R3 in Fig. 1 incorrectly identifies ?Skilling,
Cindy? as a person. Consequently, the output of
phase P2 of the cascading grammar contains sev-
eral mistakes as shown in the figure. This problem
132
went to the Switchfoot concert at the Roxy. It was pretty fun,? The lead singer/guitarist 
was really good, and even though there was another guitarist  (an Asian guy), he ended up 
playing most of the guitar parts, which was really impressive. The biggest surprise though is 
that I actually liked the opening bands. ?I especially liked the first band
Consecutive review snippets are within 25 tokens
At least 4 occurrences of MusicReviewSnippet or GenericReviewSnippet
At least 3 of them should be MusicReviewSnippets
Review ends with one of these.
Start with 
ConcertMention
Complete review is
within 200 tokens
ConcertMention
MusicReviewSnippet
GenericReviewSnippet
Example Rule
Informal Band Review
Figure 8: Extracting informal band reviews from web logs
occurs because CPSL only evaluates rules over
the input sequence in a strict left-to-right fashion.
On the other hand, the AQL query Q1 shown in
the figure applies the following condition: ?Al-
ways discard matches to Rule P2R3 if they overlap
with matches to rules P2R1 or P2R2? (even if the
match to Rule P2R3 starts earlier). Applying this
rule ensures that the person names in the list are
identified correctly. Obtaining the same effect in
grammar-based systems would require the use of
custom code (as recommended by (Cunningham
et al, 2010)).
Counting and Aggregation. Complex extraction
tasks sometimes require operations such as count-
ing and aggregation that go beyond the expressiv-
ity of regular languages, and thus can be expressed
in CPSL only using external functions. One such
task is that of identifying informal concert reviews
embedded within blog entries. Fig. 8 describes, by
example, how these reviews consist of reference
to a live concert followed by several review snip-
pets, some specific to musical performances and
others that are more general review expressions.
An example rule to identify informal reviews is
also shown in the figure. Notice how implement-
ing this rule requires counting the number of Mu-
sicReviewSnippet and GenericReviewSnippet annotations
within a region of text and aggregating this occur-
rence count across the two review types. While
this rule can be written in AQL, it can only be ap-
proximated in CPSL grammars.
Character-Level Regular Expression CPSL
cannot specify character-level regular expressions
that span multiple tokens. In contrast, the extract
regex statement in AQL fully supports these ex-
pressions.
We have described above several cases where
AQL can express concepts that can only be ex-
pressed through external functions in a cascad-
ing grammar. These examples naturally raise the
question of whether similar cases exist where a
cascading grammar can express patterns that can-
not be expressed in AQL.
It turns out that we can make a strong statement
that such examples do not exist. In the absence
of an escape to arbitrary procedural code, AQL is
strictly more expressive than a CPSL grammar. To
state this relationship formally, we first introduce
the following definitions.
We refer to a grammar conforming to the CPSL
specification as a CPSL grammar. When a CPSL
grammar contains no external functions, we refer
to it as a Code-free CPSL grammar. Finally, we
refer to a grammar that conforms to one of the
CPSL, JAPE, AFst and XTDL specifications as an
expanded CPSL grammar.
Ambiguous Grammar Specification An ex-
panded CPSL grammar may be under-specified in
some cases. For example, a single rule contain-
ing the disjunction operator (|) may match a given
region of text in multiple ways. Consider the eval-
uation of Rule P2R3 over the text fragment ?Scott,
Howard? from document d1 (Fig. 1). If ?Howard?
is identified both as Caps and First, then there are
two evaluations for Rule P2R3 over this text frag-
ment. Since the system has to arbitrarily choose
one evaluation, the results of the grammar can be
non-deterministic (as pointed out in (Cunning-
ham et al, 2010)). We refer to a grammar G as
an ambiguous grammar specification for a docu-
ment collection D if the system makes an arbitrary
choice while evaluating G over D.
Definition 1 (UnambigEquiv) A query Q is Un-
ambigEquiv to a cascading grammar G if and only
if for every document collection D, where G is not
an ambiguous grammar specification for D, the
results of the grammar invocation and the query
evaluation are identical.
We now formally compare the expressivity of
AQL and expanded CPSL grammars. The detailed
proof is omitted due to space limitations.
Theorem 1 The class of extraction tasks express-
ible as AQL queries is a strict superset of that ex-
pressible through expanded code-free CPSL gram-
mars. Specifically,
(a) Every expanded code-free CPSL grammar can
be expressed as an UnambigEquiv AQL query.
(b) AQL supports information extraction opera-
tions that cannot be expressed in expanded code-
free CPSL grammars.
133
Proof Outline: (a) A single CPSL grammar can
be expressed in AQL as follows. First, each rule
r in the grammar is translated into a set of AQL
statements. If r does not contain the disjunct (|)
operator, then it is translated into a single AQL
select statement. Otherwise, a set of AQL state-
ments are generated, one for each disjunct opera-
tor in rule r, and the results merged using union
all statements. Then, a union all statement is used
to combine the results of individual rules in the
grammar phase. Finally, the AQL statements for
multiple phases are combined in the same order as
the cascading grammar specification.
The main extensions to CPSL supported by ex-
panded CPSL grammars (listed in Sec. 2) are han-
dled as follows. AQL queries operate on graphs
on annotations just like expanded CPSL gram-
mars. In addition, AQL supports different match-
ing regimes through consolidation operators, span
predicates through selection predicates and co-
references through join operators.
(b) Example operations supported in AQL that
cannot be expressed in expanded code-free CPSL
grammars include (i) character-level regular ex-
pressions spanning multiple tokens, (ii) count-
ing the number of annotations occurring within a
given bounded window and (iii) deleting annota-
tions if they overlap with other annotations start-
ing later in the document. 2
4.2 Performance
For the annotators we test in our experiments
(See Section 5), the SystemT optimizer is able to
choose algebraic plans that are faster than a com-
parable transducer-based implementation. The
question arises as to whether there are other an-
notators for which the traditional transducer ap-
proach is superior. That is, for a given annota-
tor, might there exist a finite state transducer that
is combinatorially faster than any possible algebra
graph? It turns out that this scenario is not possi-
ble, as the theorem below shows.
Definition 2 (Token-Based FST) A token-based
finite state transducer (FST) is a nondeterministic
finite state machine in which state transitions are
triggered by predicates on tokens. A token-based
FST is acyclic if its state graph does not contain
any cycles and has exactly one ?accept? state.
Definition 3 (Thompson?s Algorithm)
Thompson?s algorithm is a common strategy
for evaluating a token-based FST (based on
(Thompson, 1968)). This algorithm processes the
input tokens from left to right, keeping track of the
set of states that are currently active.
Theorem 2 For any acyclic token-based finite
state transducer T , there exists an UnambigEquiv
operator graph G, such that evaluating G has the
same computational complexity as evaluating T
with Thompson?s algorithm starting from each to-
ken position in the input document.
Proof Outline: The proof constructs G by struc-
tural induction over the transducer T . The base
case converts transitions out of the start state into
Extract operators. The inductive case adds a Se-
lect operator to G for each of the remaining state
transitions, with each selection predicate being the
same as the predicate that drives the corresponding
state transition. For each state transition predicate
that T would evaluate when processing a given
document, G performs a constant amount of work
on a single tuple. 2
5 Experimental Evaluation
In this section we present an extensive comparison
study between SystemT and implementations of
expanded CPSL grammar in terms of quality, run-
time performance and resource requirements.
TasksWe chose two tasks for our evaluation:
? NER : named-entity recognition for Person,
Organization, Location, Address, PhoneNumber,
EmailAddress, URL and DateTime.
? BandReview : identify informal reviews in
blogs (Fig. 8).
We chose NER primarily because named-entity
recognition is a well-studied problem and standard
datasets are available for evaluation. For this task
we use GATE and ANNIE for comparison3. We
chose BandReview to conduct performance evalu-
ation for a more complex extraction task.
Datasets. For quality evaluation, we use:
? EnronMeetings (Minkov et al, 2005): collec-
tion of emails with meeting information from
the Enron corpus4 with Person labeled data;
? ACE (NIST, 2005): collection of newswire re-
ports and broadcast news/conversations with
Person, Organization, Location labeled data5.
3To the best of our knowledge, ANNIE (Cunningham et
al., 2002) is the only publicly available NER library imple-
mented in a grammar-based system (JAPE in GATE).
4http://www.cs.cmu.edu/ enron/
5Only entities of type NAM have been considered.
134
Table 1: Datasets for performance evaluation.
Dataset Description of the Content Number of Document size
documents range average
Enronx Emails randomly sampled from the Enron corpus of average size xKB (0.5 < x < 100)2 1000 xKB +/? 10% xKB
WebCrawl Small to medium size web pages representing company news, with HTML tags removed 1931 68b - 388.6KB 8.8KB
FinanceM Medium size financial regulatory filings 100 240KB - 0.9MB 401KB
FinanceL Large size financial regulatory filings 30 1MB - 3.4MB 1.54MB
Table 2: Quality of Person on test datasets.
Precision (%) Recall (%) F1 measure (%)
(Exact/Partial) (Exact/Partial) (Exact/Partial)
EnronMeetings
ANNIE 57.05/76.84 48.59/65.46 52.48/70.69
T-NE 88.41/92.99 82.39/86.65 85.29/89.71
Minkov 81.1/NA 74.9/NA 77.9/NA
ACE
ANNIE 39.41/78.15 30.39/60.27 34.32/68.06
T-NE 93.90/95.82 90.90/92.76 92.38/94.27
Table 1 lists the datasets used for performance
evaluation. The size of FinanceLis purposely
small because GATE takes a significant amount of
time processing large documents (see Sec. 5.2).
Set Up. The experiments were run on a server
with two 2.4 GHz 4-core Intel Xeon CPUs and
64GB of memory. We use GATE 5.1 (build 3431)
and two configurations for ANNIE: 1) the default
configuration, and 2) an optimized configuration
where the Ontotext Japec Transducer6 replaces the
default NE transducer for optimized performance.
We refer to these configurations as ANNIE and
ANNIE-Optimized, respectively.
5.1 Quality Evaluation
The goal of our quality evaluation is two-fold:
to validate that annotators can be built in Sys-
temT with quality comparable to those built in
a grammar-based system; and to ensure a fair
performance comparison between SystemT and
GATE by verifying that the annotators used in the
study are comparable.
Table 2 shows results of our comparison study
for Person annotators. We report the classical
(exact) precision, recall, and F1 measures that
credit only exact matches, and corresponding par-
tial measures that credit partial matches in a fash-
ion similar to (NIST, 2005). As can be seen, T-
NE produced results of significantly higher quality
than ANNIE on both datasets, for the same Person
extraction task. In fact, on EnronMeetings, the F1
measure of T-NE is 7.4% higher than the best pub-
lished result (Minkov et al, 2005). Similar results
6http://www.ontotext.com/gate/japec.html
a) Throughput on Enron
0
100
200
300
400
500
600
700
0 20 40 60 80 100
Average document size (KB)
Th
ro
u
gh
pu
t (
K
B
/s
ec
)
ANNIE
ANNIE-Optimized
T-NE
x
b) Memory Utilization on Enron
0
200
400
600
0 20 40 60 80 100
Average document size (KB)
A
v
g 
H
ea
p 
si
ze
 
(M
B
) ANNIE
ANNIE-Optimized
T-NE
Error bars show
25th and 75th
percentile 
x
Figure 9: Throughput (a) and memory consump-
tion (b) comparisons on Enronx datasets.
can be observed for Organization and Location on
ACE (exact numbers omitted in interest of space).
Clearly, considering the large gap between
ANNIE?s F1 and partial F1 measures on both
datasets, ANNIE?s quality can be improved via
dataset-specific tuning as demonstrated in (May-
nard et al, 2003). However, dataset-specific tun-
ing for ANNIE is beyond the scope of this paper.
Based on the experimental results above and our
previous formal comparison in Sec. 4, we believe
it is reasonable to conclude that annotators can be
built in SystemT of quality at least comparable to
those built in a grammar-based system.
5.2 Performance Evaluation
We now focus our attention on the throughput and
memory behavior of SystemT, and draw a com-
parison with GATE. For this purpose, we have con-
figured both ANNIE and T-NE to identify only the
same eight types of entities listed for NER task.
Throughput. Fig. 9(a) plots the throughput of
the two systems on multiple Enronx datasets with
average document sizes of between 0.5KB and
100KB. For this experiment, both systems ran
with a maximum Java heap size of 1GB.
135
Table 3: Throughput and mean heap size.
ANNIE ANNIE-Optimized T-NE
Dataset ThroughputMemoryThroughput Memory ThroughputMemory
(KB/s) (MB) (KB/s) (MB) (KB/s) (MB)
WebCrawl 23.9 212.6 42.8 201.8 498.9 77.2
FinanceM 18.82 715.1 26.3 601.8 703.5 143.7
FinanceL 19.2 2586.2 21.1 2683.5 954.5 189.6
As shown in Fig. 9(a), even though the through-
put of ANNIE-Optimized (using the optimized trans-
ducer) increases two-fold compared to ANNIE un-
der default configuration, T-NE is between 8 and
24 times faster compared to ANNIE-Optimized. For
both systems, throughput varied with document
size. For T-NE, the relatively low throughput on
very small document sizes (less than 1KB) is due
to fixed overhead in setting up operators to pro-
cess a document. As document size increases, the
overhead becomes less noticeable.
We have observed similar trends on the rest
of the test collections. Table 3 shows that T-
NE is at least an order of magnitude faster than
ANNIE-Optimized across all datasets. In partic-
ular, on FinanceL T-NE?s throughput remains
high, whereas the performance of both ANNIE and
ANNIE-Optimized degraded significantly.
To ascertain whether the difference in perfor-
mance in the two systems is due to low-level com-
ponents such as dictionary evaluation, we per-
formed detailed profiling of the systems. The pro-
filing revealed that 8.2%, 16.2% and respectively
14.2% of the execution time was spent on aver-
age on low-level components in the case of ANNIE,
ANNIE-Optimized and T-NE, respectively, thus lead-
ing us to conclude that the observed differences
are due to SystemT?s efficient use of resources at
a macroscopic level.
Memory utilization. In theory, grammar based
systems can stream tuples through each stage
for minimal memory consumption, whereas Sys-
temT operator graphs may need to materialize in-
termediate results for the full document at certain
points to evaluate the constraints in the original
AQL. The goal of this study is to evaluate whether
this potential problem does occur in practice.
In this experiment we ran both systems with a
maximum heap size of 2GB, and used the Java
garbage collector?s built-in telemetry to measure
the total quantity of live objects in the heap over
time while annotating the different test corpora.
Fig. 9(b) plots the minimum, maximum, and mean
heap sizes with the Enronx datasets. On small doc-
uments of size up to 15KB, memory consumption
is dominated by the fixed size of the data struc-
tures used (e.g., dictionaries, FST/operator graph),
and is comparable for both systems. As docu-
ments get larger, memory consumption increases
for both systems. However, the increase is much
smaller for T-NE compared to that for both AN-
NIE and ANNIE-Optimized. A similar trend can be
observed on the other datasets as shown in Ta-
ble 3. In particular, for FinanceL, both ANNIE and
ANNIE-Optimized required 8GB of Java heap size to
achieve reasonable throughput7 , in contrast to T-
NE which utilized at most 300MB out of the 2GB
of maximum Java heap size allocation.
SystemT requires much less memory than
GATE in general due to its runtime, which monitors
data dependencies between operators and clears
out low-level results when they are no longer
needed. Although a streaming CPSL implemen-
tation is theoretically possible, in practice mecha-
nisms that allow an escape to custom code make it
difficult to decide when an intermediate result will
no longer be used, hence GATE keeps most inter-
mediate data in memory until it is done analyzing
the current document.
The BandReview Task. We conclude by briefly dis-
cussing our experience with the BandReview task
from Fig. 8. We built two versions of this anno-
tator, one in AQL, and the other using expanded
CPSL grammar. The grammar implementation
processed a 4.5GB collection of 1.05 million blogs
in 5.6 hours and output 280 reviews. In contrast,
the SystemT version (85 AQL statements) ex-
tracted 323 reviews in only 10 minutes!
6 Conclusion
In this paper, we described SystemT, a declar-
ative IE system based on an algebraic frame-
work. We presented both formal and empirical
arguments for the benefits of our approach to IE.
Our extensive experimental results show that high-
quality annotators can be built using SystemT,
with an order of magnitude throughput improve-
ment compared to state-of-the-art grammar-based
systems. Going forward, SystemT opens up sev-
eral new areas of research, including implement-
ing better optimization strategies and augmenting
the algebra with additional operators to support
advanced features such as coreference resolution.
7GATE ran out of memory when using less than 5GB of
Java heap size, and thrashed when run with 5GB to 7GB
136
References
Douglas E. Appelt and Boyan Onyshkevych. 1998.
The common pattern specification language. In TIP-
STER workshop.
Branimir Boguraev. 2003. Annotation-based finite
state processing in a large-scale nlp arhitecture. In
RANLP, pages 61?80.
D. D. Chamberlin, A. M. Gilbert, and Robert A. Yost.
1981. A history of System R and SQL/data system.
In vldb.
Amit Chandel, P. C. Nagesh, and Sunita Sarawagi.
2006. Efficient batch top-k search for dictionary-
based entity recognition. In ICDE.
E. F. Codd. 1990. The relational model for database
management: version 2. Addison-Wesley Longman
Publishing Co., Inc., Boston, MA, USA.
H. Cunningham, D. Maynard, and V. Tablan. 2000.
JAPE: a Java Annotation Patterns Engine (Sec-
ond Edition). Research Memorandum CS?00?10,
Department of Computer Science, University of
Sheffield, November.
H. Cunningham, D. Maynard, K. Bontcheva, and
V. Tablan. 2002. GATE: A framework and graphical
development environment for robust NLP tools and
applications. In Proceedings of the 40th Anniver-
sary Meeting of the Association for Computational
Linguistics, pages 168 ? 175.
Hamish Cunningham, Diana Maynard, Kalina
Bontcheva, Valentin Tablan, Marin Dimitrov, Mike
Dowman, Niraj Aswani, Ian Roberts, Yaoyong
Li, and Adam Funk. 2010. Developing language
processing components with gate version 5 (a user
guide).
AnHai Doan, Luis Gravano, Raghu Ramakrishnan, and
Shivakumar Vaithyanathan. 2008. Special issue on
managing information extraction. SIGMOD Record,
37(4).
Witold Drozdzynski, Hans-Ulrich Krieger, Jakub
Piskorski, Ulrich Scha?fer, and Feiyu Xu. 2004.
Shallow processing with unification and typed fea-
ture structures ? foundations and applications.
Ku?nstliche Intelligenz, 1:17?23.
Ralph Grishman and Beth Sundheim. 1996. Message
understanding conference - 6: A brief history. In
COLING, pages 466?471.
IBM. 2010. IBM LanguageWare.
P. G. Ipeirotis, E. Agichtein, P. Jain, and L. Gravano.
2006. To search or to crawl?: towards a query opti-
mizer for text-centric tasks. In SIGMOD.
Alpa Jain, Panagiotis G. Ipeirotis, AnHai Doan, and
Luis Gravano. 2009. Join optimization of informa-
tion extraction output: Quality matters! In ICDE.
Diana Maynard, Kalina Bontcheva, and Hamish Cun-
ningham. 2003. Towards a semantic extraction of
named entities. In Recent Advances in Natural Lan-
guage Processing.
Einat Minkov, Richard C. Wang, and William W. Co-
hen. 2005. Extracting personal names from emails:
Applying named entity recognition to informal text.
In HLT/EMNLP.
NIST. 2005. The ACE evaluation plan.
Ganesh Ramakrishnan, Sreeram Balakrishnan, and
Sachindra Joshi. 2006. Entity annotation based on
inverse index operations. In EMNLP.
Ganesh Ramakrishnan, Sachindra Joshi, Sanjeet Khai-
tan, and Sreeram Balakrishnan. 2008. Optimization
issues in inverted index-based entity annotation. In
InfoScale.
Frederick Reiss, Sriram Raghavan, Rajasekar Kr-
ishnamurthy, Huaiyu Zhu, and Shivakumar
Vaithyanathan. 2008. An algebraic approach to
rule-based information extraction. In ICDE, pages
933?942.
SAP. 2010. Inxight ThingFinder.
SAS. 2010. Text Mining with SAS Text Miner.
Warren Shen, AnHai Doan, Jeffrey F. Naughton, and
Raghu Ramakrishnan. 2007. Declarative informa-
tion extraction using datalog with embedded extrac-
tion predicates. In vldb.
SystemT. 2010. AQL Manual.
http://www.alphaworks.ibm.com/tech/systemt.
Ken Thompson. 1968. Regular expression search al-
gorithm. pages 419?422.
UIMA. 2010. Unstructured Information Management
Architecture.
http://uima.apache.org.
137
Proceedings of the ACL-HLT 2011 System Demonstrations, pages 109?114,
Portland, Oregon, USA, 21 June 2011. c?2011 Association for Computational Linguistics
SystemT: A Declarative Information Extraction System
Yunyao Li
IBM Research - Almaden
650 Harry Road
San Jose, CA 95120
yunyaoli@us.ibm.com
Frederick R. Reiss
IBM Research - Almaden
650 Harry Road
San Jose, CA 95120
frreiss@us.ibm.com
Laura Chiticariu
IBM Research - Almaden
650 Harry Road
San Jose, CA 95120
chiti@us.ibm.com
Abstract
Emerging text-intensive enterprise applica-
tions such as social analytics and semantic
search pose new challenges of scalability and
usability to Information Extraction (IE) sys-
tems. This paper presents SystemT, a declar-
ative IE system that addresses these challenges
and has been deployed in a wide range of en-
terprise applications. SystemT facilitates the
development of high quality complex annota-
tors by providing a highly expressive language
and an advanced development environment.
It also includes a cost-based optimizer and a
high-performance, flexible runtime with mini-
mummemory footprint. We present SystemT
as a useful resource that is freely available,
and as an opportunity to promote research in
building scalable and usable IE systems.
1 Introduction
Information extraction (IE) refers to the extraction
of structured information from text documents. In
recent years, text analytics have become the driv-
ing force for many emerging enterprise applications
such as compliance and data redaction. In addition,
the inclusion of text has also been increasingly im-
portant for many traditional enterprise applications
such as business intelligence. Not surprisingly, the
use of information extraction has dramatically in-
creased within the enterprise over the years. While
the traditional requirement of extraction quality re-
mains critical, enterprise applications pose several
two challenges to IE systems:
1.Scalability: Enterprise applications operate
over large volumes of data, often orders of
magnitude larger than classical IE corpora. An
IE system should be able to operate at those
scales without compromising its execution ef-
ficiency or memory consumption.
2.Usability: Building an accurate IE system is
an inherently labor intensive process. There-
fore, the usability of an enterprise IE system in
terms of ease of development and maintenance
is crucial for ensuring healthy product cycle
and timely handling of customer complains.
Traditionally, IE systems have been built from in-
dividual extraction components consisting of rules
or machine learning models. These individual com-
ponents are then connected procedurally in a pro-
gramming language such as C++, Perl or Java. Such
procedural logic towards IE cannot meet the increas-
ing scalability and usability requirements in the en-
terprise (Doan et al, 2006; Chiticariu et al, 2010a).
Three decades ago, the database community faced
similar scalability and expressivity challenges in
accessing structured information. The community
addressed these problems by introducing a rela-
tional algebra formalism and an associated declar-
ative query language SQL. Borrowing ideas from
the database community, several systems (Doan and
others, 2008; Bohannon and others, 2008; Jain et al,
2009; Krishnamurthy et al, 2008; Wang et al, 2010)
have been built in recent years taking an alternative
declarative approach to information extraction. In-
stead of using procedural logic to implement the ex-
traction task, declarative IE systems separate the de-
scription of what to extract from how to extract it,
allowing the IE developer to build complex extrac-
109
Development Environment
Optimizer
Rules(XQL)
ExecutionEngine
SampleDocuments
RuntimeEnvironmentunti environ ent
InputDocumentStream
AnnotatedDocumentStream
Plan(Algebra)
UserInterface
Publish
Figure 1: Overview of SystemT
tion programs without worrying about performance
considerations.
In this demonstration, we showcase one such
declarative IE system called SystemT, designed
to address the scalability and usability challenges.
We illustrate how SystemT, currently deployed in
a multitude of real-world applications and com-
mercial products, can be used to develop and
maintain IE annotators for enterprise applica-
tions. A free version of SystemT is available at
http://www.alphaworks.ibm.com/tech/systemt.
2 Overview of SystemT
Figure 1 depicts the architecture of SystemT. The sys-
tem consists of two major components: the Development
Environment and the Runtime Environment. The Sys-
temT Development Environment supports the iterative
process of constructing and refining rules for information
extraction. The rules are specified in a declarative lan-
guage called AQL (F.Reiss et al, 2008). The Develop-
ment Environment provides facilities for executing rules
over a given corpus of representative documents and vi-
sualizing the results of the execution. Once a developer
is satisfied with the results that her rules produce on these
documents, she can publish her annotator.
Publishing an annotator is a two-step process. First,
given an AQL annotator, there can be many possible
graphs of operators, or execution plans, each of which
faithfully implements the semantics of the annotator.
Some of the execution plans are much more efficient than
others. The SystemT Optimizer explores the space of
the possible execution plans to choose the most efficient
one. This execution plan is then given to the SystemT
Runtime to instantiate the corresponding physical oper-
ators. Once the physical operators are instantiated, the
create view Phone asextract regex /\d{3}-\d{4}/ on D.text as numberfrom Document D;
create view Person asextract dictionary ?firstNames.dict? on D.text as namefrom Document D;
create view PersonPhoneAll asselect CombineSpans(P.name, Ph.number) as matchfrom Person P, Phone Phwhere FollowsTok(P.name, Ph.number, 0, 5);
create view PersonPhone asselect R.name as namefrom PersonPhoneAll Rconsolidate on R.name;
output view PersonPhone; 
Figure 2: An AQL program for a PersonPhone task.
SystemT Runtime feeds one document at a time through
the graph of physical operators and outputs a stream of
annotated documents.
The decoupling of the Development and Runtime en-
vironments is essential for the flexibility of the system. It
facilitates the incorporating of various sophisticated tools
to enable annotator development without sacrificing run-
time performance. Furthermore, the separation permits
the SystemT Runtime to be embedded into larger appli-
cations with minimum memory footprint. Next, we dis-
cuss individual components of SystemT in more details
(Sections 3 ? 6), and summarize our experience with the
system in a variety of enterprise applications (Section 7).
3 The Extraction Language
In SystemT, developers express an information extrac-
tion program using a language called AQL. AQL is a
declarative relational language similar in syntax to the
database language SQL, which was chosen as a basis for
our language due to its expressivity and familiarity. An
AQL program (or an AQL annotator) consists of a set of
AQL rules.
In this section, we describe the AQL language and
its underlying algebraic operators. In Section 4, we ex-
plain how the SystemT optimizer explores a large space
of possible execution plans for an AQL annotator and
chooses one that is most efficient.
3.1 AQL
Figure 2 illustrates a (very) simplistic annotator of rela-
tionships between persons and their phone number. At a
high-level, the annotator identifies person names using a
simple dictionary of first names, and phone numbers us-
ing a regular expression. It then identifies pairs of Person
and Phone annotations, where the latter follows the
110
former within 0 to 5 tokens, and marks the corre-
sponding region of text as a PersonPhoneAll annota-
tion. The final output PersonPhone is constructed by
removing overlapping PersonPhoneAll annotations.
AQL operates over a simple relational data model
with three data types: span, tuple, and view. In this
data model, a span is a region of text within a doc-
ument identified by its ?begin? and ?end? positions,
while a tuple is a list of spans of fixed size. A view
is a set of tuples. As can be seen from Figure 2,
each AQL rule defines a view. As such, a view is the
basic building block in AQL: it consists of a logical
description of a set of tuples in terms of the docu-
ment text, or the content of other views. The input
to the annotator is a special view called Document
containing a single tuple with the document text.
The AQL annotator tags some views as output views,
which specify the annotation types that are the final
results of the annotator.
The example in Figure 2 illustrates two of the
basic constructs of AQL. The extract statement
specifies basic character-level extraction primitives,
such as regular expressions or dictionaries (i.e.,
gazetteers), that are applied directly to the docu-
ment, or a region thereof. The select statement
is similar to the corresponding SQL statement, but
contains an additional consolidate on clause
for resolving overlapping annotations, along with an
extensive collection of text-specific predicates.
To keep rules compact, AQL also allows a short-
hand pattern notation similar to the syntax of the
CPSL grammar standard (Appelt and Onyshkevych,
1998). For example, the PersonPhoneAll view
in Figure 2 can also be expressed as shown below.
Internally, SystemT translates each of these extract
pattern statements into one or more select and ex-
tract statements.
create view PersonPhoneAll as
extract pattern
<P.name> <Token>{0,5} <Ph.number>
from Person P, Phone Ph;
SystemT has built-in multilingual support in-
cluding tokenization, part of speech and gazetteer
matching for over 20 languages using IBM Lan-
guageWare. Annotator developers can utilize the
multilingual support via AQLwithout having to con-
figure or manage any additional resources. In ad-
dition, AQL allows user-defined functions in a re-
firstNames.dict
DocumentInput Tuple
?I?ve seen John and Martin, ?
Output Tuple 2 Span 2Document Span 1Output Tuple 1 Document
Dictionary
Person
(?Anna?, ?John?, ?Martin?, ?)
Figure 3: Dictionary Extraction Operator
stricted context in order to support operations such
as validation or normalization. More details on AQL
can be found in the AQL manual (Chiticariu et al,
2010b).
3.2 Algebraic Operators in SystemT
SystemT executes AQL rules using graphs of op-
erators. These operators are based on an algebraic
formalism that is similar to the relational algebra
formalism, but with extensions for text processing.
Each operator in the algebra implements a single
basic atomic IE operation, producing and consum-
ing sets of tuples (i.e., views).
Fig. 3 illustrates the dictionary extraction operator
in the algebra, which performs character-level dic-
tionary matching. A full description of the 12 differ-
ent operators of the algebra can be found in (F.Reiss
et al, 2008). Three of the operators are listed below.
? The Extract operator (E) performs character-
level operations such as regular expression and
dictionary matching over text, producing one tu-
ple for each match.
? The Select operator (?) takes as input a set of
tuples and a predicate to apply to the tuples, and
outputs all tuples that satisfy the predicate.
? The Join operator (??) takes as input two sets of
tuples and a predicate to apply to pairs of tuples.
It outputs all pairs satisfying the predicate.
Other operators include PartOfSpeech for part-
of-speech detection, Consolidate for removing
overlapping annotations, Block and Group for
grouping together similar annotations occurring
within close proximity to each other, as well as ex-
pressing more general types of aggregation, Sort for
sorting, and Union and Minus for expressing set
union and set difference, respectively.
111
Person Phone
Plan BPlan A Find matches of Person, then discard matches that are not followed by a Phone
? ??dict
Find matches of Person and Phone, then identify pairs that are within 0 to 5 tokens of each other
Plan CFind matches of Phone, then discard matches that are not followed by a Person
?
?
regex
Figure 4: Execution strategies for PersonPhoneAll in
Fig. 2
4 The Optimizer
Grammar-based IE engines such as (Boguraev,
2003; Cunningham et al, 2000) place rigid restric-
tions on the order in which rules can be executed.
Such systems that implement the CPSL standard or
extensions of it must use a finite state transducer to
evaluate each level of the cascade with one or more
left to right passes over the entire input token stream.
In contrast, SystemT uses a declarative approach
based on rules that specify what patterns to extract,
as opposed to how to extract them. In a declarative
IE system such as SystemT the specification of an
annotator is completely separate from its implemen-
tation. In particular, the system does not place ex-
plicit constraints on the order of rule evaluation, nor
does it require that intermediate results of an anno-
tator collapse to a fixed-size sequence.
As shown in Fig. 1, the SystemT engine does
not execute AQL directly; instead, the SystemT
Optimizer compiles AQL into a graph of operators.
Given a collection of AQL views, the optimizer gen-
erates a large number of different operator graphs,
all of which faithfully implement the semantics of
the original views. Even though these graphs always
produce the same results, the execution strategies
that they represent can have very different perfor-
mance characteristics. The optimizer incorporates
a cost model which, given an operator graph, esti-
mates the CPU time required to execute the graph
over an average document in the corpus. This cost
model allows the optimizer to estimate the cost of
each potential execution strategy and to choose the
one with the fastest predicted running time.
Fig. 4 presents three possible execution strategies
for the PersonPhoneAll rule in Fig. 2. If the opti-
mizer estimates that the evaluation cost of Person is
much lower than that of Phone, then it can determine
that Plan B has the lowest evaluation cost among
the three, because Plan B only evaluates Phone in
the ?right? neighborhood for each instance of Per-
son. More details of our algorithms for enumerating
plans can be found in (F.Reiss et al, 2008).
The optimizer in SystemT chooses the best exe-
cution plan from a large number of different algebra
graphs available. Depending on the execution plan
generated by the optimizer, SystemT may evaluate
views out of order, or it may skip evaluating some
views entirely. It may share work among views or
combine multiple equivalent views together. Even
within the context of a single view, the system can
choose among several different execution strategies
without affecting the semantics of the annotator.
This decoupling is possible because of the declar-
ative approach in SystemT, where the AQL rules
specify only what patterns to extract and not how to
extract them. Notice that many of these strategies
cannot be implemented using a transducer. In fact,
we have formally proven that within this large search
space, there generally exists an execution strategy
that implements the rule semantics far more effi-
ciently than the fastest transducer could (Chiticariu
et al, 2010b). This approach also allows for greater
rule expressivity, because the rule language is not
constrained by the need to compile to a finite state
transducer, as in traditional CPSL-based systems.
5 The Runtime
The SystemT Runtime is a compact, small memory
footprint, high-performance Java-based runtime en-
gine designed to be embedded in a larger system.
The runtime engine works in two steps. First, it
instantiates the physical operators in the compiled
operator graph generated by the optimizer. Second,
once the first step has been completed, the runtime
feeds documents through the operator graph one at a
time, producing annotations.
SystemT exposes a generic Java API for the inte-
gration of its runtime environment with other appli-
cations. Furthermore, SystemT provides two spe-
cific instantiations of the Java API: a UIMA API and
a Jaql function that allow the SystemT runtime to
be seamlessly embedded in applications using the
UIMA analytics framework (UIMA, 2010), or de-
ployed in a Hadoop-based environment. The latter
112
allows SystemT to be embedded as a Map job in a
map-reduce framework, thus enabling the system to
scale up and process large volumes of documents in
parallel.
5.1 Memory Consumption
Managing memory consumption is very important
in information extraction systems. Extracting struc-
tured information from unstructured text requires
generating and traversing large in-memory data
structures, and the size of these structures deter-
mines how large a document the system can process
with a given amount of memory.
Conventional rule-based IE systems cannot
garbage-collect their main-memory data structures
because the custom code embedded inside rules can
change these structures in arbitrary ways. As a re-
sult, the memory footprint of the rule engine grows
continuously throughout processing a given docu-
ment.
In SystemT, the AQL view definitions clearly
specify the data dependencies between rules. When
generating an execution plan for an AQL annota-
tor, the optimizer generates information about when
it is safe to discard a given set of intermediate re-
sults. The SystemT Runtime uses this information
to implement garbage collection based on reference-
counting. This garbage collection significantly re-
duces the system?s peak memory consumption, al-
lowing SystemT to handle much larger documents
than conventional IE systems.
6 The Development Environment
The SystemT Development Environment assists a
developer in the iterative process of developing,
testing, debugging and refining AQL rules. Be-
sides standard editor features present in any well-
respected IDE for programming languages such as
syntax highlighting, the Development Environment
also provides facilities for visualizing the results of
executing the rules over a sample document collec-
tion as well as explaining in detail the provenance of
any output annotation as the sequence of rules that
have been applied in generating that output.
7 Evaluation
As discussed in Section 1, our goal in building Sys-
temT was to address the scalability and usability
Application Type Type of Platform
brand management server-side
business insights server-side
client-side mashups client-side
compliance server-side
search (email, web, patent) server-side
security server-side
server-side mashups server-side
Table 1: Types of applications using SystemT
challenges posed by enterprise applications. As
such, our evaluation focuses on these two dimen-
sions.
7.1 Scalability
Table 1 presents a diverse set of enterprise applica-
tions currently using SystemT. SystemT has been
deployed in both client-side applications with strict
memory constraints, as well as on applications on
the cloud, where it can process petabytes of data
in parallel. The focus on scalability in the design
of SystemT is essential for its flexible execution
model. First of all, efficient execution plans are
generated automatically by the SystemT Optimizer
based on sample document collections. This en-
sures that the same annotator can be executed effi-
ciently for different types of document collections.
In fact, our previous experimental study shows that
the execution plan generated by the SystemT opti-
mizer can be 20 times or more faster than a manu-
ally constructed plan (F.Reiss et al, 2008). Further-
more, the Runtime Environment of SystemT results
in compact memory footprint and allows SystemT
to be embedded in applications with strict memory
requirements as small as 10MB.
In our recent study over several document col-
lections of different sizes, we found that for the
same set of extraction tasks, the SystemT through-
put is at least an order of magnitude higher than
that of a state-of-the-art grammar-based IE system,
with much lower memory footprint (Chiticariu et al,
2010b). The high throughput and low memory foot-
print of SystemT allows it to satisfy the scalability
requirement of enterprise applications.
7.2 Usability
Table 2 lists different types of annotators built us-
ing SystemT for a wide range of domains. Most,
113
Domain Sample Annotators Built
blog Sentiment, InformalReview
email ConferenceCall, Signature, Agenda, DrivingDirection, PersonPhone, PersonAddress, PersonEmailAddress
financial Merger, Acquisition, JointVenture, EarningsAnnouncement, AnalystEarningsEstimate, DirectorsOfficers, CorporateActions
generic Person, Location, Organization, PhoneNumber, EmailAddress, URL, Time, Date
healthcare Disease, Drug, ChemicalCompound
web Homepage, Geography, Title, Heading
Table 2: List of Sample Annotators Built Using SystemT for Different Domains
if not all, of these annotators are already deployed
in commercial products. The emphasis on usability
in the design of SystemT has been critical for its
successful deployment in various domains. First of
all, the declarative approach taken by SystemT al-
lows developers to build complex annotators without
worrying about performance. Secondly, the expres-
siveness of the AQL language has greatly eased the
burden of annotator developers when building com-
plex annotators, as complex semantics such as dupli-
cate elimination and aggregation can be expressed in
a concise fashion (Chiticariu et al, 2010b). Finally,
the Development Environment further facilitates an-
notator development, where the clean semantics of
AQL can be exploited to automatically construct ex-
planations of incorrect results to help a developer in
identifying specific parts of the annotator responsi-
ble for a given mistake. SystemT has been suc-
cessfully used by enterprise application developers
in building high quality complex annotators, without
requiring extensive training or background in natural
language processing.
8 Demonstration
This demonstration will present the core function-
alities of SystemT. In particular, we shall demon-
strate the iterative process of building and debug-
ging an annotator in the Development Environment.
We will then showcase the execution plan automati-
cally generated by the Optimizer based on a sample
document collection, and present the output of the
Runtime Environment using the execution plan. In
our demonstration we will first make use of a simple
annotator, as the one shown in Fig. 2, to illustrate
the main constructs of AQL. We will then showcase
the generic state-of-the-art SystemT Named Enti-
ties Annotator Library (Chiticariu et al, 2010c) to
illustrate the quality of annotators that can be built
in our system.
References
D. E. Appelt and B. Onyshkevych. 1998. The common
pattern specification language. In TIPSTER workshop.
B. Boguraev. 2003. Annotation-based finite state pro-
cessing in a large-scale nlp arhitecture. In RANLP.
P. Bohannon et al 2008. Purple SOX Extraction Man-
agement System. SIGMOD Record, 37(4):21?27.
L. Chiticariu, Y. Li, S. Raghavan, and F. Reiss. 2010a.
Enterprise information extraction: Recent develop-
ments and open challenges. In SIGMOD.
Laura Chiticariu, Rajasekar Krishnamurthy, Yunyao Li,
Sriram Raghavan, Frederick R. Reiss, and Shivaku-
mar Vaithyanathan. 2010b. Systemt: an algebraic ap-
proach to declarative information extraction. ACL.
Laura Chiticariu, Rajasekar Krishnamurthy, Yunyao
Li, Frederick Reiss, and Shivakumar Vaithyanathan.
2010c. Domain adaptation of rule-based annotators
for named-entity recognition tasks. EMNLP.
H. Cunningham, D. Maynard, and V. Tablan. 2000.
JAPE: a Java Annotation Patterns Engine (Second Edi-
tion). Research Memorandum CS?00?10, Department
of Computer Science, University of Sheffield.
A. Doan et al 2008. Information extraction challenges
in managing unstructured data. SIGMOD Record,
37(4):14?20.
A. Doan, R. Ramakrishnan, and S. Vaithyanathan. 2006.
Managing Information Extraction: State of the Art and
Research Directions. In SIGMOD.
F.Reiss, S. Raghavan, R. Krishnamurthy, H. Zhu, and
S. Vaithyanathan. 2008. An algebraic approach to
rule-based information extraction. In ICDE.
A. Jain, P. Ipeirotis, and L. Gravano. 2009. Building
query optimizers for information extraction: the sqout
project. SIGMOD Rec., 37:28?34.
R. Krishnamurthy, Y. Li, S. Raghavan, F. Reiss,
S. Vaithyanathan, and H. Zhu. 2008. SystemT: a sys-
tem for declarative information extraction. SIGMOD
Record, 37(4):7?13.
D. Z. Wang, E. Michelakis, M. J. Franklin, M. Garo-
falakis, and J. M. Hellerstein. 2010. Probabilistic
declarative information extraction. In ICDE.
114
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 109?114,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
WizIE: A Best Practices Guided Development Environment
for Information Extraction
Yunyao Li Laura Chiticariu Huahai Yang Frederick R. Reiss Arnaldo Carreno-fuentes
IBM Research - Almaden
650 Harry Road
San Jose, CA 95120
{yunyaoli,chiti,hyang,frreiss,acarren}@us.ibm.com
Abstract
Information extraction (IE) is becoming a crit-
ical building block in many enterprise appli-
cations. In order to satisfy the increasing text
analytics demands of enterprise applications,
it is crucial to enable developers with general
computer science background to develop high
quality IE extractors. In this demonstration,
we present WizIE, an IE development envi-
ronment intended to reduce the development
life cycle and enable developers with little or
no linguistic background to write high qual-
ity IE rules. WizIE provides an integrated
wizard-like environment that guides IE devel-
opers step-by-step throughout the entire devel-
opment process, based on best practices syn-
thesized from the experience of expert devel-
opers. In addition, WizIE reduces the manual
effort involved in performing key IE develop-
ment tasks by offering automatic result expla-
nation and rule discovery functionality. Pre-
liminary results indicate that WizIE is a step
forward towards enabling extractor develop-
ment for novice IE developers.
1 Introduction
Information Extraction (IE) refers to the problem of
extracting structured information from unstructured
or semi-structured text. It has been well-studied by
the Natural Language Processing research commu-
nity for a long time. In recent years, IE has emerged
as a critical building block in a wide range of enter-
prise applications, including financial risk analysis,
social media analytics and regulatory compliance,
among many others. An important practical chal-
lenge driven by the use of IE in these applications
is usability (Chiticariu et al, 2010c): specifically,
how to enable the ease of development and mainte-
nance of high-quality information extraction rules,
also known as annotators, or extractors.
Developing extractors is a notoriously labor-
intensive and time-consuming process. In order to
ensure highly accurate and reliable results, this task
is traditionally performed by trained linguists with
domain expertise. As a result, extractor develop-
ment is regarded as a major bottleneck in satisfying
the increasing text analytics demands of enterprise
applications. Hence, reducing the extractor devel-
opment life cycle is a critical requirement. Towards
this goal, we have built WizIE, an IE development
environment designed primarily to (1) enable devel-
opers with little or no linguistic background to write
high quality extractors, and (2) reduce the overall
manual effort involved in extractor development.
Previous work on improving the usability of IE
systems has mainly focused on reducing the manual
effort involved in extractor development (Brauer et
al., 2011; Li et al, 2008; Li et al, 2011a; Soder-
land, 1999; Liu et al, 2010). In contrast, the fo-
cus of WizIE is on lowering the extractor develop-
ment entry barrier by means of a wizard-like en-
vironment that guides extractor development based
on best practices drawn from the experience of
trained linguists and expert developers. In doing so,
WizIE also provides natural entry points for differ-
ent tools focused on reducing the effort required for
performing common tasks during IE development.
Underlying our WizIE are a state-of-the-art
IE rule language and corresponding runtime en-
gine (Chiticariu et al, 2010a; Li et al, 2011b). The
runtime engine and WizIE are commercially avail-
109
Profile Extractor  Test  ExtractorDevelop  ExtractorInput Documents Label Text/Clues
Task Analysis Rule Development Performance Tuning Delivery
Export Extractor
Figure 1: Best Practices for Extractor Development
able as part of IBM InfoSphere BigInsights (IBM,
2012).
2 System Overview
The development process for high-quality, high-
performance extractors consists of four phases, as
illustrated in Fig. 1. First, in the Task Analysis
phase, concrete extraction tasks are defined based
on high-level business requirements. For each ex-
traction task, IE rules are developed during the Rule
Development phase. The rules are profiled and fur-
ther fine-tuned in the Performance Tuning phase, to
ensure high runtime performance. Finally, in theDe-
livery phase, the rules are packaged so that they can
be easily embedded in various applications.
WizIE is designed to assist and enable both novice
and experienced developers by providing an intu-
itive wizard-like interface that is informed by the
best practices in extractor development throughout
each of these phases. By doing so, WizIE seeks
to provide the key missing pieces in a conventional
IE development environment (Cunningham et al,
2002; Li et al, 2011b; Soundrarajan et al, 2011),
based on our experience as expert IE developers, as
well as our interactions with novice developers with
general computer science background, but little text
analytics experience, during the development of sev-
eral enterprise applications.
3 The Development Environment
In this section, we present the general functionality
of WizIE in the context of extraction tasks driven
by real business use cases from the media and en-
tertainment domain. We describe WizIE in details
and show how it guides and assists IE developers in
a step-by-step fashion, based on best practices.
3.1 Task Analysis
The high-level business requirement of our run-
ning example is to identify intention to purchase
for movies from online forums. Such information
is of great interest to marketers as it helps pre-
dict future purchases (Howard and Sheth, 1969).
During the first phrase of IE development (Fig. 2),
WizIE guides the rule developer in turning such a
high-level business requirement into concrete ex-
traction tasks by explicitly asking her to select and
manually examine a small number 1 of sample doc-
uments, identify and label snippets of interest in the
sample documents, and capture clues that help to
identify such snippets.
The definition and context of the concrete extrac-
tion tasks are captured by a tree structure called the
extraction plan (e.g. right panel in Fig. 2). Each
leaf node in an extraction plan corresponds to an
atomic extraction task, while the non-leaf nodes de-
note higher-level tasks based on one or more atomic
extraction tasks. For instance, in our running ex-
ample, the business question of identifying intention
of purchase for movies has been converted into the
extraction task of identifying MovieIntent mentions,
which involves two atomic extraction tasks: identi-
fying Movie mentions and Intent mentions.
The extraction plan created, as we will describe
later, plays a key role in the IE development process
in WizIE. Such tight coupling of task analysis with
actual extractor development is a key departure from
conventional IE development environments.
3.2 Rule Development
Once concrete extraction tasks are defined,
WizIE guides the IE developer to write actual rules
based on best practices. Fig. 3(a) shows a screenshot
of the second phase of building an extractor, the
Rule Development phase. The Extraction Task panel
on the left provides information and tips for rule
development, whereas the Extraction Plan panel
on the right guides the actual rule development
for each extraction task. As shown in the figure,
the types of rules associated with each label node
fall into three categories: Basic Features, Can-
1The exact sample size varies by task type.
110
Figure 2: Labeling Snippets and Clues of Interest
didate Generation and Filter&Consolidate. This
categorization is based on best practices for rule
development (Chiticariu et al, 2010b). As such,
the extraction plan groups together the high-level
specification of extraction tasks via examples, and
the actual implementation of those tasks via rules.
The developer creates rules directly in the Rule
Editor, or via the Create Statement wizard, acces-
sible from the Statements node of each label in the
Extraction Plan panel:
The wizard allows the user to select a type for
the new rule, from predefined sets for each of the
three categories. The types of rules exposed in each
category are informed by best practices. For ex-
ample, the Basic Features category includes rules
for defining basic features using regular expressions,
dictionaries or part of speech information, whereas
the Candidate Generation category includes rules for
combining basic features into candidate mentions by
means of operations such as sequence or alternation.
Once the developer provides a name for the new rule
(view) and selects its type, the appropriate rule tem-
plate (such as the one illustrated below) is automat-
ically generated in an appropriate file on disk and
displayed in the editor, for further editing 2.
Once the developer completes an iteration of rule
development, WizIE guides her in testing and refin-
ing the extractor, as shown in Fig. 3(b). The An-
notation Explorer at the bottom of the screen gives
a global view of the extraction results, while other
panels highlight individual results in the context of
the original input documents. The Annotation Ex-
plorer enables filtering and searching results, and
comparing results with those from a previous iter-
ation. WizIE also provides a facility for manually
labeling a document collection with ?ground truth?
annotations, then comparing the extraction results
with the ground truth in order to formally evalu-
ate the quality of the extractor and avoid regressions
during the development process.
An important differentiator of WizIE compared
with conventional IE development environments is
a suite of sophisticated tools for automatic result ex-
planation and rule discovery. We briefly describe
them next.
Provenance Viewer. When the user clicks on an ex-
tracted result, the Provenance Viewer shows a com-
plete explanation of how that result has been pro-
2Details on the rule syntax can be found in (IBM, )
111
AB
Figure 3: Extractor Development: (a) Developing, and (b) Testing.
duced by the extractor, in the form of a graph that
demonstrates the sequence of rules and individual
pieces of text responsible for that result. Such expla-
nations are critical to enable the developer to under-
stand why a false positive is generated by the sys-
tem, and identify problematic rule(s) that could be
refined in order to correct the mistake. An example
explanation for an incorrect MovieIntent mention ?I
just saw Mission Impossible? is shown below.
As can be seen, the MovieIntent mention is gener-
ated by combining a SelfRef (matching first person
pronouns) with a MovieName mention, and in turn,
the latter is obtained by combining several MovieN-
ameCandidate mentions. With this information, the
developer can quickly determine that the SelfRef and
MovieName mentions are correct, but their combina-
tion in MovieIntentCandidate is problematic. She can
then proceed to refine the MovieIntentCandidate rule,
for example, by avoiding any MovieIntentCandidate
mentions containing a past tense verb form such as
saw, since past tense in not usually indicative of in-
tent (Liu et al, 2010).
Pattern Discovery. Negative contextual clues such
as the verb ?saw? above are useful for creating rules
that filter out false positives. Conversely, positive
clues such as the phrase ?will see? are useful for
creating rules that separate ambiguous matches from
high-precision matches. WizIE?s Pattern Discovery
component facilitates automatic discovery of such
clues by mining available sample data for common
patterns in specific contexts (Li et al, 2011a). For
example, when instructed to analyze the context be-
tween SelfRef and MovieName mentions, Pattern Dis-
covery finds a suite of common patterns as shown
in Fig. 4. The developer can analyze these patterns
and choose those suitable for refining the rules. For
example, patterns such as ?have to see? can be seen
as positive clues for intent, whereas phrases such as
?took ... to see? or ?went to see? are negative clues,
and can be used for filtering false positives.
Regular Expression Generator. WizIE also en-
ables the discovery of regular expression patterns.
The Regular Expression Generator takes as input a
112
Figure 4: Pattern Discovery
Figure 5: Regular Expression Generator
set of sample mentions and suggests regular expres-
sions that capture the samples, ranging from more
specific (higher accuracy) to more general expres-
sions (higher coverage). Figure 5 shows two reg-
ular expressions automatically generated based on
mentions of movie ratings, and how the developer is
subsequently assisted in understanding and refining
the generated expression. In our experience, regu-
lar expressions are complex concepts that are diffi-
cult to develop for both expert and novice develop-
ers. Therefore, such a facility to generate expres-
sions based on examples is extremely useful.
3.3 Performance Tuning
Once the developer is satisfied with the quality of the
extractor, WizIE guides her in measuring and tuning
its runtime performance, in preparation for deploy-
ing the extractor in a production environment. The
Profiler observes the execution of the extractor on
a sample input collection over a period of time and
records the percentage of time spent executing each
rule, or performing certain runtime operations. After
the profiling run completes, WizIE displays the top
25 most expensive rules and runtime operations, and
the overall throughput (amount of input data pro-
cessed per unit of time). Based on this information,
the developer can hand-tune the critical parts of the
extractor, rerun the Profiler, and validate an increase
in throughput. She would repeat this process until
satisfied with the extractor?s runtime performance.
3.4 Delivery and Deployment
Once satisfied with both the result quality and
runtime performance, the developer is guided by
WizIE?s Export wizard through the process of ex-
porting the extractor in a compiled executable form.
The generated executable can be embedded in an ap-
plication using a Java API interface. WizIE can also
wrap the executable plan in a pre-packaged applica-
tion that can be run in a map-reduce environment,
then deploy this application on a Hadoop cluster.
4 Evaluation
A preliminary user study was conducted to evalu-
ate the effectiveness of WizIE in enabling novice IE
developers. The study included 14 participants, all
employed at a major technology company. In the
pre-study survey, 10 of the participants reported no
prior experience with IE tasks, two of them have
seen demonstrations of IE systems, and two had
brief involvement in IE development, but no expe-
rience with WizIE. For the question ?According to
your understanding, how easy is it to build IE appli-
cations in general ??, the median rating was 5, on a
113
scale of 1 (very easy) to 7 (very difficult).
The study was conducted during a 2-day training
session. In Day 1, participants were given a thor-
ough introduction to IE, shown example extractors,
and instructed to develop extractors without WizIE.
Towards the end of Day 1, participants were asked
to solve an IE exercise: develop an extractor for
the high-level requirement of identifying mentions
of company revenue by division from the company?s
official press releases. WizIE was introduced to the
participants in Day 2 of the training, and its fea-
tures were demonstrated and explained with exam-
ples. Participants were then asked to complete the
same exercise as in Day 1. Authors of this demon-
stration were present to help participants during the
exercises in both days. At the end of each day, par-
ticipants filled out a survey about their experience.
In Day 1, none of the participants were able to
complete the exercise after 90 minutes. In the sur-
vey, one participant wrote ?I am in sales so it is all
difficult?; another participant indicated that ?I don?t
think I would be able to recreate the example on my
own from scratch?. In Day 2, most participants were
able to complete the exercise in 90 minutes or less
usingWizIE. In fact, two participants created extrac-
tors with accuracy and coverage of over 90%, when
measured against the ground truth. Overall, the par-
ticipants were much more confident about creating
extractors. One participant wrote ?My first impres-
sion is very good?. On the other hand, another par-
ticipant asserted that ?The nature of the task is still
difficult?. They also found that WizIE is useful and
easy to use, and it is easier to build extractors with
the help of WizIE.
In summary, our preliminary results indicate that
WizIE is a step forward towards enabling extractor
development for novice IE developers. In order to
formally evaluate WizIE, we are currently conduct-
ing a formal study of using WizIE to create extrac-
tors for several real business applications.
5 Demonstration
In this demonstration we showcase WizIE?s step-by-
step approach to guide the developer in the iterative
process of IE rule development, from task analysis
to developing, tuning and deploying the extractor
in a production environment. Our demonstration is
centered around the high-level business requirement
of identifying intent to purchase movies from blogs
and forum posts as described in Section 3. We start
by demonstrating the process of developing two rel-
atively simple extractors for identifying MovieIntent
and MovieRating mentions. We then showcase com-
plex state-of-the-art extractors for identifying buzz
and sentiment for the media and entertainment do-
main, to illustrate the quality and runtime perfor-
mance of extractors built with WizIE.
References
F. Brauer, R. Rieger, A. Mocan, and W. M. Barczynski.
2011. Enabling information extraction by inference of
regular expressions from sample entities. In CIKM.
L. Chiticariu, R. Krishnamurthy, Y. Li, S. Raghavan,
F. Reiss, and S. Vaithyanathan. 2010a. SystemT: an
algebraic approach to declarative information extrac-
tion. ACL.
L. Chiticariu, R. Krishnamurthy, Y. Li, F. Reiss, and
S. Vaithyanathan. 2010b. Domain adaptation of rule-
based annotators for named-entity recognition tasks.
EMNLP.
L. Chiticariu, Y. Li, S. Raghavan, and F. Reiss. 2010c.
Enterprise Information Extraction: Recent Develop-
ments and Open Challenges. In SIGMOD (Tutorials).
H. Cunningham, D. Maynard, K. Bontcheva, and
V. Tablan. 2002. Gate: an architecture for develop-
ment of robust hlt applications. In ACL.
J.A. Howard and J.N. Sheth. 1969. The Theory of Buyer
Behavior. Wiley.
IBM. InfoSphere BigInsights - Annotation Query Lan-
guage (AQL) reference. http://ibm.co/kkzj1i.
IBM. 2012. InfoSphere BigInsights. http://ibm.co/jjbjfa.
Y. Li, R. Krishnamurthy, S. Raghavan, S. Vaithyanathan,
and H. V. Jagadish. 2008. Regular expression learning
for information extraction. In EMNLP.
Y. Li, V. Chu, S. Blohm, H. Zhu, and H. Ho. 2011a. Fa-
cilitating pattern discovery for relation extraction with
semantic-signature-based clustering. In CIKM.
Y. Li, F. Reiss, and L. Chiticariu. 2011b. SystemT: A
Declarative Information Extraction System. In ACL
(Demonstration).
B. Liu, L. Chiticariu, V. Chu, H. V. Jagadish, and F. Reiss.
2010. Automatic Rule Refinement for Information
Extraction. PVLDB, 3(1):588?597.
S. Soderland. 1999. Learning information extrac-
tion rules for semi-structured and free text. Machine
Learning, 34(1-3):233?272, February.
B. R. Soundrarajan, T. Ginter, and S. L. DuVall. 2011.
An interface for rapid natural language processing de-
velopment in UIMA. In ACL (Demonstrations).
114
