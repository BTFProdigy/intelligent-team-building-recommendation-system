A Preliminary Look into
the Use of Named Entity Information 
for Bioscience Text Tokenization
Robert Arens
Department of Computer Science
University of Iowa
Iowa City, Iowa, USA
robertarens@uiowa.edu
Abstract
Tokenization in the bioscience domain is often
difficult. New terms, technical terminology, and
nonstandard orthography, all common in
bioscience text, contribute to this difficulty.
This paper will introduce the tasks of
tokenization, normalization before introducing
BAccHANT, a system built for bioscience text
normalization. Casting tokenization /
normalization as a problem of punctuation
classification motivates using machine learning
methods in the implementation of this system.
The evaluation of BAccHANT's performance
included error analysis of the system's
performance inside and outside of named
entities (NEs) from the GENIA corpus, which
led to the creation of a normalization system
trained solely on data from inside NEs,
BAccHANT-N. Evaluation of this new system
indicated that normalization systems trained on
data inside NEs perform better than systems
trained both inside and outside NEs, motivating
a merging of tokenization and named entity
tagging processes as opposed to the standard
pipelining approach.
1 Introduction
For the purposes of this paper, a token can be defined as
the smallest discrete unit of meaning in a document
relevant to the task at hand, the smallest entity of
information that cannot be further reduced in form and
still carry that information. This definition of a token is
dependent on both the type of information we wish to
extract from a document, and the nature of the document
itself; that is, tokenization is task-specific. For example,
tokenizing technical reports in order to search them by
keyword may require a conservative tokenization
scheme; e.g. a document containing the term "2.0-
gigahertz processor" would not want to tokenize "2.0"
away from "gigahertz" for fear that the document would
be missed if the user searched for that exact phrase.
However, if the same set of documents was being
tokenized to build a database of processor speeds, "2.0"
would need to be tokenized away from "gigahertz" in
order to store its speed
Tokenization is often straightforward; discovering
the words in the sentence, "I saw a cat." is not difficult,
as the tokens are bounded by punctuation, including
space characters. However, discovering the words in the
sentence, "I studied E. coli in a 2.5% solution." presents
some problems. The period following ?E.? is being used
to indicate an acronym instead of a sentence boundary,
and must be recognized as such. Even if we were not to
concern ourselves with sentence boundaries, deciding
that any period simply ends a token, the sentence would
again present a problem since we would not want to
tokenize the 2 from the 5 in "2.5".
The difficulty in tokenization stems from ambiguous
punctuation. In order to tokenize, one must be able to
tell with certainty when a piece of punctuation ends a
token. 
The bioscience domain presents additional
difficulties to tokenizing. Bioscience literature contains
technical terminology and includes ambiguous
punctuation, similar to the E. coli sentence above. The
domain is dynamic, with thousands of researchers
adding to the literature (the MEDLINE database adds
approximately 400,000 new entries consisting of journal
articles and abstracts per year (MEDLINE Fact Sheet,
2002)). Bioscience literature contains heterogeneous
orthographics; for example, the literature contains the
terms "NF-kappaB", "NF-kappa B", and "NF-kappa-B,"
and while each refers to the same protein, tokenizers
using spaces and dashes as breaking criteria will return a
different tokenization of each term though one standard
tokenization would be preferable.
The problem of nonstandard orthography is of
particular importance for document retrieval. Consider
the NF-kappaB example above; if a document repository
contains documents with different orthographic versions
of NF-kappaB, a researcher searching for NF-kappaB
would have to search for all possible orthographic
variations and would miss documents containing
unanticipated orthography. Normalization attempts to
solve this problem by removing orthographic variation
from tokens, bringing them to one normalized form. For
example, if all three versions of NF-kappaB had all
spaces and dashes removed, all three would look like
?NFkappaB,? and a document retrieval system would
find all instances in a search. A related strategy, query
expansion, attempts to solve the same problem by
accounting for as many orthographic variants of the user
query as possible, and searching for all of them.
Normalization acts as a special case of tokenization by
deciding which instances of punctuation break a token,
and removing all punctuation that does not break the
token in order to bring it to a normalized form.
The remainder of this paper will consider work
relevant to tokenization and normalization for the
bioscience domain. Casting tokenization, and by
extension normalization, as a classification problem
motivates the creation of BAccHANT, a machine
learning system designed to normalize bioscience text.
Evaluation of this system includes an evaluation of the
system's performance inside and outside of named
entities, and results from this evaluation motivate the
creation of a new system, BAccHANT-N, trained solely
on date from inside NEs. The improvement in
performance of BAccHANT-N over BAccHANT when
normalizing inside NE text indicates that named entity
information is useful for bioscience text tokenization
tasks, motivating future work in systems that perform
tokenization and NE tagging concurrently.
2 Related work
As noted in Habert et al (1998), standard methods for
evaluating the quality of tokens produced by
tokenization systems do not exist. Though a necessary
first step to tasks such as document retrieval, sentence
boundary finding, parsing, etc., there exists work
involving these tasks that take tokenization for granted
(e.g. Chang, Schutze and Altman (2002), Seki and
Mostafa (2003)), mention tokenization without detailing
the tokenization scheme (e.g. Fukuda et al (1998)), or
indicate use of a tokenization system without
mentioning its performance (e.g. Bennet et al (1999),
Yamamoto et al (2003)). To the author's knowledge,
there exists no work analyzing the impact of
tokenization performance on bioinformatics tasks.
Tokenization methods for bioinformatics tasks range
from simple to complex. Bennet et al (1999) tokenized
for noun phrase extraction, tokenizing based on
whitespace, with additional modification to take
?specialized nomenclature? into account. Yamamoto et
al. (2003) developed a morphological analyzer for
protein name tagging which tokenized, part-of-speech
tagged, and stemmed documents. Seki and Mostafa
(2003) essentially tokenized by dictionary lookup for
protein name extraction, using hand-crafted rules and
filtering to identify protein name candidates to check
against their dictionary. 
Relevant work on normalization can be found in the
proceedings of the 2003 Text REtrieval Conference
(TREC) Genomics track competition. The competition
involved two tasks. The first task was for gene or
protein X, find all MEDLINE references that focus on
the basic biology of the gene/protein from the
designated organism. Basic biology includes isolation,
structure, genetics and function of genes/proteins in
normal and disease states. The second task was to
extract GeneRIF statements from records from the
MEDLINE biomedical and health abstract repository.
Kayaalp et al (2003) normalized by converting all
letters to lower case, and expanded queries by
identifying terms with both alphabetic and numerical
characters and searching for hyphenated variants, i.e.
JAK2 and JAK-2. de Bruijn and Martin (2003) used
morphological query expansion along with a relevance
feedback engine. Osborne et al used a number of query
expansion strategies, including appending parenthetical
information, acronym expansions, words following
hyphens, lower and uppercase versions of terms, etc.
de Brujin and Martin (2003) and Osborne et al
(2003) both indicate that query expansion was beneficial
to the performance of their systems. However, no
authors gave performance measures for their query
expansion methods independent of their final systems.
To the author's knowledge, there exists no work
analyzing the performance of normalization systems for
bioscience literature. 
Named entities are ?proper names and quantities of
interest? (Chinchor (1998)) in a document. Named entity
tagging involves discovering and marking these entities
in a document, e.g. finding all proteins in a document
and labeling them as such. Having biomedical
documents tagged with NEs allows for better
information extraction, archival, searching, etc. of those
documents. The GENIA corpus (Kim et al (2003)) is a
corpus of 2000 MEDLINE abstracts tagged for parts of
speech and hand-tagged for NEs. NE tags in the GENIA
corpus are based on an ontology, consisting of amino
acids, proteins, organisms and their tissues, cells, and
other.
3 Methodology
From a machine learning perspective, one way to look at
a tokenization task, including normalization, is as a
classification problem. As stated before, the problem of
tokenization is that of ambiguous punctuation ? one
must be able to tell whether or not a piece of
punctuation should be included in a token. A document
can be tokenized by classifying each piece of
punctuation in the document as part of a token or as a
token boundary. Removing the pieces of punctuation
classified as part of the token will normalize the token.
Possible features for classifying punctuation may
include the piece of punctuation itself, character or
characters to the left/right of the punctuation, type of
character[s] to the left/right of the punctuation (i.e.
uppercase, lowercase, number, etc.), the length of the
sentence or article the term occurs in, the type of
sentence or article the term occurs in, etc. 
The system presented here, BAccHANT (Bioscience
And Health Article Normalizing Tokenizer), was
created to normalize MEDLINE text for the TREC
Genomics track, as presented earlier. It classifies pieces
of punctuation in bioscience text based on the
surrounding characters, determining whether the
punctuation is a token boundary or needs to be removed
for normalization.
The features chosen for BAccHANT were the
following: piece of punctuation being classified (Punc),
character to the left of the punctuation (CL), type of
character to the left of the punctuation (TL), character to
the right of the punctuation (CR), type of character to
the right of the punctuation (TR), and whether the
punctuation should be removed for normalization, or
break the token (Class). These features were chosen by
the author. Feature selection using information gain
ratio indicated that all five should be used.
Feature Values
Punc .  ,  -  (  )  /  ;  [  ]  :  \  {  }  <space>
CL / CR <the character itself>
TL / TR lower, cap, num, space, other
Class remove, break
Table 1: The features and their possible values.
Values for Punc and CL/CR are self-explanatory.
Values for TL/TR are as follows:
* lower: Character is lowercase
* cap: Character is a capital letter
* num: Character is a number
* space: Character is whitespace (space, tab, etc.)
* other: Character is none of the above
Values for Class are as follows:
* remove: The punctuation should be removed
* break: The punctuation should break the token
The 'remove' class is of chief importance for the
normalization task, since classifying a piece of
punctuation as 'remove' means the punctuation will be
removed for normalization. 
Sample feature vectors:
* "NF-kappaB"  ==  ['F', -, 'k', cap, lower, remove]
* "T cells" ==  ['T',  , 'c',cap, lower, remove]
* "alpha gene" == ['a',  , 'g',lower, lower, break]
* "yATF-binding" == ['F', -, 'b', cap, lower, break]
The training / testing set for BAccHANT was
constructed from 67 MEDLINE abstracts, hand
tokenized by the author using the tokenization scheme
presented in the appendix. A domain expert1 was
available for determining difficult tokenizations. The 67
abstracts yielded 17253 pieces of punctuation.
Distributions follow. The feature vectors created from
the set were used to create a decision tree, implemented
using the Weka tool set (Witten and Frank). The tree
used reduced error pruning to increase accuracy.
Punctuation
Type Total remove break
<space> 14476 463 14013
- 1103 737 366
. 637 12 625
, 577 6 571
( 186 8 178
) 186 7 179
/ 45 7 38
: 18 0 18
[ 9 6 2
] 9 7 2
; 7 2 5
Totals 17253 1255 15998
Table 2: Punctuation distribution of the MEDLINE
train/test set
4 Evaluation
The baseline used for evaluation was to simply break on
every instance of punctuation; that is, assume no
punctuation needs to be removed. This achieves an
accuracy of 92.73%, where accuracy is the percentage of
correctly classified punctuation. This baseline was
chosen for its high accuracy; however, as it is a simple
majority class baseline which always predicts 'break',
giving it a precision score of 1, a recall score of 0, and
an f-measure of 0 for the 'remove' class. 
BAccHANT was trained and tested using 10-fold
cross-validation. It achieved an accuracy of 96.60%,
which was a statistically significant improvement over
the baseline (all significance testing was done using a
two-tailed t-test with a p-value of 0.05). More detailed
results follow.
1 Dr. Vladimir Leontiev, University of Iowa, Department
of Anatomy and Cell Biology
Class
remove break
Precision 0.832 0.974
Recall 0.668 0.989
F-Measure 0.741 0.982
Table 3: Precision, recall, and f-measure
The 'break' classification reached high precision and
recall. This is unsurprising as 96.7% of all <space>
punctuation classified as 'break', and <space>
punctuation made up 83.9% of all punctuation. Commas
and periods were similarly easy to classify as 'break'. Of
more interest is the 'remove' classification, as this class
indicates punctuation to be normalized. The recall was
not as good as was hoped, with BAccHANT discovering
roughly 2 out of every 3 instances present, though it
correctly classified roughly 5 out of 6 instances it found
We suspected that punctuation was being used
differently inside of named entities vs. outside of NEs.
To investigate this suspicion, we tested BAccHANT on
NE data from the GENIA corpus. The testing set created
from GENIA consisted wholly of character data from
inside NEs. The set contained 5798 instances of
punctuation. Punctuation distribution for the GENIA
corpus test set follows.
Punctuation
Type Total remove break
<space> 4849 157 4692
- 304 192 112
. 237 4 233
, 187 2 185
( 62 3 59
) 62 3 59
/ 14 4 10
: 2 0 2
[ 0 0 0
] 0 0 0
; 0 0 0
Totals 5798 365 5433
Table 4: Punctuation distribution in the GENIA
corpus test set
The accuracy of BAccHANT on this test set was
90%. More detailed results for the 'remove' class follow.
BAccHANT  performance
Test Set All text GENIA corpus
Accuracy 0.966 0.900
Precision 0.832 0.546
Recall 0.688 0.453
F-Measure 0.741 0.500
Table 5: Accuracy, precision, recall, and F-measure
for BAccHANT tested on all text vs. inside NEs.
Precision, recall and f-measure are given for the
'remove' class
Further testing revealed that accuracy outside NEs
was near 99%. The statistically significant degradation
in performance of BAccHANT inside NEs vs.
performance both inside and outside NEs indicates that
data inside named entities is more difficult to normalize
than data outside named entities.
These results seem to indicate that a normalization
system trained solely on data inside NEs could perform
better than a system trained on both named and non-
named data when normalizing NEs. A new
normalization system trained on NE data, BAccHANT-
N, was built to test this.
The new system was trained and tested using the
GENIA corpus test set. BAccHANT-N was created
similarly to BAccHANT, with identical features, and
implemented as a decision tree using reduced error
pruning. It was trained and tested using 10-fold cross-
validation and achieved an accuracy of 96.5%. More
detailed results follow.
Class
remove break
Precision 0.833 0.980
Recall 0.789 0.985
F-Measure 0.811 0.983
Table 6: Precision, recall, and F-measure for
BAccHANT-N tested on named entity data.
Below is a results summary table, giving accuracy
for both classes, and precision, recall, and f-measure for
the 'remove' class across all systems presented.
BAccHANT-N showed statistically significant
improvement over BAccHANT when normalizing
named entity data. These results show that a system
trained on data inside NEs shows improvement in
performance over a system trained on data from inside
and outside NEs.
Baseline BAccHANT
Training
set
All Text Named
Entities
Test set All NE All NE All NE
Accuracy 0.927 0.914 0.966 0.900 0.965
Precision 1 1 0.832 0.546 0.833
Recall 0 0 0.688 0.453 0.789
F-Measure 0 0 0.741 0.500 0.811
Table 7: Results summary across all systems.
Precision, recall, and f-measure are given for the
'remove' class.
5 Future Work
Currently, BAccHANT looks only at one character to
either side of the piece of punctuation to be classified.
By expanding the number of characters examined from
one to a certain number of characters (a window),
accuracy should increase. Since BAccHANT decision
tree learns based on context, greater context may allow
for better learning, and a window of characters will
expand context.
Also, a window of characters will introduce new
features to learn from. Since a decision tree's features
determine how it learns from context, adding better
features to the decision tree may help the tree learn
better. Examples of new features include:
* Mixed case - does the window include both
uppercase and lowercase characters? 
* Mixed type - does the window include a mix of
letters, numbers, and other character types?
* Boundary size - is there a definite token boundary
within the character window, and if so, how far into the
window is the boundary?
Error analysis of BAccHANT on named entity
tagged data led to the creation of a normalization system
trained on data from inside NEs which performed better
than BAccHANT, and hence would be a better choice
for normalizing inside NEs. However, this normalizer
would necessarily need to be run on named entity tagged
data, as it has not been trained to deal with text outside
of NEs. To accomplish this, a system to simultaneously
tag named entities and normalize at the same time would
be desirable. This could be accomplished via
hierarchical hidden Markov models (Fine et. al., 1998).
A system of this type involves "tiering" hidden Markov
models within each other. This model could be used to
statistically compute the most likely name for a section
of text, and then normalize appropriately in one pass. As
hidden Markov models have been used both for name-
finding (Bikel et al (1997)) and tokenization (Cutting et
al. (1992)), this seems to be a promising research
possibility.
6 Conclusion
This paper has introduced a system to normalize
bioscience and health articles based on learning features
surrounding punctuation which may need to be removed
for normalization. The system performed significantly
better than the baseline system.
By analyzing the system's performance on named
entity data from the GENIA corpus, it was discovered
that named entities seemed to be more difficult to
normalize than surrounding non-named text. This
finding led to the creation of another normalization
system trained on named entity data, which showed
significant improvement over the first system when
tested on named entities. This improvement seems to
indicate that a system which would compute named
entities in parallel with normalization would be useful.
References
Nuala A. Bennet, Qin He, Kevin Powell, and Bruce R.
Schatz. 1999. Extracting noun phrases for all of
MEDLINE. In Proceedings of the AMIA Symposium,
671-65
Daniel M. Bikel, Scott Miller, Richard Schwartz and
Ralph Weischedel. 1997. Nymble: a High-
Performance Learning Name-finder. In Proceedings
of the Conference on Applied Natural Language
Processing, 1997.
Stephen Blott, Cathal Gurrin, Gareth J. F. Jones, Alan F.
Smeaton, and Thomas Sodring. 2003. On the Use of
MeSH Headings to Improve Retrieval Effectiveness.
In Proceedings of The 12th Text Retrieval
Conference, Gaithersburg, Md, November 2003.
Eric W. Brown, Andrew Dolbey, Lawrence Hunter.
2003. IBM Research and the University of Colorado
TREC 2003 Genomics Track. In Proceedings of The
12th Text Retrieval Conference, Gaithersburg, Md,
November 2003.
Berry de Brujin, and Joel Martin. 2003. Finding Gene
Function Using LitMiner. In Proceedings of The 12th
Text Retrieval Conference, Gaithersburg, Md,
November 2003.
Jeffery T. Chang, Hinrich Scuhtze, and Russ B. Altman.
2002. Creating an Online Dictionary of
Abbreviations from MEDLINE. Journal of American
Medical Informatics Association, 9(6): 612-620.
Nancy A. Chinchor. 1998. Overview of MUC-7/MET-2.
In Proceedings of the Seventh Message
Understanding Conference (MUC-7).
Doug Cutting, Julian Kupiec, Jan Pedersen, and
Penelope Sibun. 1992. A practical part-of-speech
tagger. In Proceedings of the Third Conference on
Applied Natural Language Processing. 133-140
Shai Fine, Yoram Singer, and Naftali Tishby. 1998. The
hierarchical hidden Markov model: Analysis and
applications. Machine Learning, 32(1):41-62
Ken-ichiro Fukuda, Tatsuhiko Tsunoda, Ayuchi
Tamura, and Toshihisa Takagi. 1998. Toward
Information Extraction: Identifying Protein Names
from Biological Papers. In Proceedings of the Pacific
Symposium on Biocomputing '98 (PSB'98).
B. Habert, G. Adda, M. Adda-Decker, P. Boula de
Mareuil, S. Ferrari, O. Ferret, G. Illouz, and P.
Paroubek. 1998. Towards Tokenization Evaluation.
In Proceedings of LREC-98, 427-431.
William R. Hersh and Ravi T. Bhupatiraju. 2003. TREC
Genomics Track Overview. In Proceedings of The
12th Text Retrieval Conference, Gaithersburg, Md,
November 2003.
Lynette Hirschman, Alexander A. Morgan, and
Alexander S. Yeh. 2002. Rutabaga by any other
name: extracting biological names. Journal of
Biomedical Informatics. 35(4): 247-259.
MEDLINE Fact Sheet. (2002). Retrieved November 2,
2003 from
http://www.nlm.nih.gov/pubs/factsheets/medline.html
Mehmet Kayaalp, Alan R. Aronson, Susanne M.
Humphrey, Nicholas C. Ide, Lorraine K. Tanabe,
Lawrence H. Smith, Dina Demner, Russell R. Loane,
James G. Mork, and Olivier Bodenreidera. 2003.
Methods for accurate retrieval of MEDLINE citations
in functional genomics. In Proceedings of The 12th
Text Retrieval Conference, Gaithersburg, Md,
November 2003.
Jun'ichi Kazama, Takaki Makino, Yoshihiro Ohta,
Jun'ichi Tsujii. 2002. Tuning Support Vector
Machines for Biomedical Named Entity Recognition.
In Proceedings of the Workshop on Natural
Language Processing in the Biomedical Domain. 1-8.
Jin-Dong Kim, Tomoko Ohta, Yuka Tateisi and Jun-ichi
Tsujii. 2003. GENIA corpus ? a semantically
annotated corpus for bio-textmining. Bioinformatics,
19(1):180-182.
Andrei Mikheev. 2003. Text Segmentation. In R.
Mitkov (Ed.), The Oxford Handbook of
Computational Linguistics (pp. 201-218). New York:
Oxford University Press, Inc.
Miles Osborne, Jeffrey Chang, Mark Cumiskey, Nipun
Mehra, Veronica Rotemberg, Gail Sinclair, Matthew
Smillie, Russ B. Altman, and Bonnie Webber. 2003.
Edinburgh-Stanford TREC 2003 Genomics Track:
Notebook Paper. In Proceedings of The 12th Text
Retrieval Conference, Gaithersburg, Md, November
2003.
David D. Palmer. 1994. Satz - An Adaptive Sentence
Segmentation System. M.S. Thesis and UC-Berkeley
Technical Report UCB/CSD 94-846.
Kazuhiro Seki and Javed Mostafa. 2003. An Approach
to Protein Name Extraction using Heuristics and a
Dictionary. Retrieved November 11, 2003, from
lair.indiana.edu/research/capris/papers/lair03-04.pdf
Lorraine Tanabe and W. John Wilbur. 2002. Tagging
Gene and Protein Names in Full Text Articles. In
Proceedings of the Workshop on Natural Language
Processing in the Biomedical Domain. 9-13.
Ian H. Witten and Eibe Frank. 1999. Data Mining:
Practical Machine Learning Tools and Techniques
with Java Implementations. San Francisco: Morgan
Kaufman Publishers.
Kaoru Yamamoto, Taku Kudo, Akihiko Konagaya, and
Yuji Matsumoto. 2003. Protein Name Tagging for
Biomedical Annotation in Text. In Proceedings of
the ACL 2003 Workshop on Natural Language
Processing in Biomedicine, pp. 65-72.
Appendix - Hand tokenizing MEDLINE
abstracts for normalization
The goal of this tokenization scheme is to process
plain-text MEDLINE abstracts into a tokenized gold
standard. The format will be one token per line, with
breaking punctuation occupying a line by itself.
The rule of thumb for tokenizing in this fashion is,
include only punctuation critical for the unique naming
of proteins, genes, compounds, etc. found in bioscience
literature. Else, the punctuation should be broken on. 
Expanded forms of acronyms present an ambiguity
problem for tokenization. While we want to keep the
acronym ?NF-kappa B? as one token, its expanded form
?nuclear factor-kappa beta? should be tokenized on all
punctuation. While the heterogeneous orthography of
?NF-kappa B? must be taken into account since ?NF-
kappaB? and ?NF-kappa-B? both appear in the
literature, the literature does not contain instances of
?nuclearfactor-kappa beta? or ?kappabeta?. 
Dashes represent the greatest punctuation ambiguity
in the literature, with two out of three instances being
removed for normalization. In particular, break if:
* there is a prefix before the dash, as in ?anti-DNA?
or ?non-IL?.
* the dash indicates a number range, as in ?1-3
hours?.
* the token candidate following the dash is some
kind of modifying noun, gerund or adjective as in ?REL-
binding? or ?Duffy-negative?.
* there are multiple dashes stringing a number of
tokens together, as in ?neck-spine-torso axis?.
* the dash indicates a negative number.
The gold standards used for training and testing are
available from the author by request, or by download at:
http://que.info-science.uiowa.edu/~bob/name-gold
(data from inside named entities)
http://que.info-science.uiowa.edu/~bob/all-gold
(data from inside and outside named entities)
Proceedings of the Second ACL Workshop on Effective Tools and Methodologies for Teaching NLP and CL, pages 28?31,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Web-based Interfaces for Natural Language Processing Tools
Marc Light? and Robert Arens? and Xin Lu?
?Linguistics Department
?School of Library and Information Science
??Computer Science Department
University of Iowa
Iowa, USA 52242
{marc-light,robert-arens,xin-lu}@uiowa.edu
Abstract
We have built web interfaces to a number
of Natural Language Processing technolo-
gies. These interfaces allow students to
experiment with different inputs and view
corresponding output and inner workings
of the systems. When possible, the in-
terfaces also enable the student to mod-
ify the knowledge bases of the systems
and view the resulting change in behav-
ior. Such interfaces are important because
they allow students without computer sci-
ence background to learn by doing. Web
interfaces also sidestep issues of platform
dependency in software packages, avail-
able computer lab times, etc. We discuss
our basic approach and lessons learned.
1 Introduction
The Problem: Natural language processing (NLP)
technology is relevant to non-computer scientists:
our classes are populated by students from neuro-
science, speech pathology, linguistics, teaching of
foreign languages, health informatics, etc. To effec-
tively use NLP technology, it is helpful understand,
at some level, how it works. Hands-on experimen-
tation is an effective method for gaining such under-
standing. Unfortunately, to be able to experiment,
non-computer scientists often need to acquire some
programming skills and knowledge of the Unix op-
erating system. This can be time consuming and
tedious and can distract students from their central
goal of understanding how a technology works and
how best to employ it for their interests.
In addition, getting a technology to run on a set
lab machines can be problematic: the programs may
be developed for a different platform, e.g., a pro-
gram was developed for Linux but the lab machines
run MSWindows. Another hurdle is that machine
administrators are often loath to install applications
that they perceive as non-standard. Finally, lab times
can be restrictive and thus it is preferable to enable
students to use computers to which they have easy
access.
Our Solution: We built web interfaces to many
core NLP modules. These interfaces not only al-
low students to use a technology but also allow stu-
dents to modify and extend the technology. This en-
ables experimentation. We used server-side script-
ing languages to build such web interfaces. These
programs take input from a web browser, feed it to
the technology in question, gather the output from
the technology and send it back to the browser for
display to the student. Access to web browsers is
nearly ubiquitous and thus the issue of lab access is
side-stepped. Finally, the core technology need only
run on the web server platform. Many instructors
have access to web servers running on different plat-
forms and, in general, administering a web server is
easier than maintaining lab machines.
An Example: Finite state transduction is a core
NLP technology and one that students need to un-
derstand. The Cass partial parsing system (Abney,
1997) makes use of a cascade of FSTs. To use this
system, a student creates a grammar. This grammar
is compiled and then applied to sentences provided
28
Figure 1: Web interface to Cass
Figure 2: Cass Output
29
by the student. Prior to our work, the only interface
to Cass involved the Unix command line shell. Fig-
ure 3 shows an example session with the command
line interface. It exemplifies the sort of interface that
users must master in order to work with current hu-
man language technology.
1 emacs input.txt &
2 emacs grammar.txt &
3 source /usr/local/bin/setupEnv
3 reg gram.txt
4 Montytagger.py inTagged input.txt
5 cat inTagged |
6 wordSlashTagInput.pl |
7 cass -v -g gram.txt.fsc > cassOut
8 less cassOut
Figure 3: Cass Command Line Interface
A web-based interface hides many of the details, see
Figure 1 and Figure 2. For example, the use of an
ASCII-based text editor such as emacs become un-
necessary. In addition, the student does not need
to remembering flags such as -v -g and does not
need to know how to use Unix pipes, |, and out-
put redirection, >. None of this knowledge is ter-
ribly difficult but the amount accumulates quickly
and such information does not help the student un-
derstand how Cass works.
2 What we have built
To date, we have built web interfaces to nine NLP-
related technologies:
? the Cass parser (Abney, 1997),
? the MontyTagger Brill-style part-of-speech tag-
ger (Liu, 2004),
? the NLTK statistical part-of-speech tagger,
? a NLTK context-free grammar parser (Loper
and Bird, 2002),
? the Gsearch context-free grammar parser (Cor-
ley et al, 2001),
? the SenseRelate word sense disambiguation
system (Pedersen et al, 2005),
? a Perl Regular expression evaluator,
? a linguistic feature annotator,
? and a decision tree classifier (Witten and Frank,
1999).
These interfaces have been used in an introduction
to computational linguistics course and an introduc-
tion to creating and using corpora course. Prior to
the interface construction, no hands-on lab assign-
ments were given; instead all assignments were pen-
cil and paper. The NLP technologies listed above
were chosen because they fit into the material of the
course and because of their availability.
2.1 Allowing the student to process input
The simplest type of interface allows students to pro-
vide input and displays corresponding output. All
the interfaces above provide this ability. They all
start with HTML forms to collect input. In the sim-
plest case, PHP scripts process the forms, placing
input into files and then system calls are made to
run the NLP technology. Finally, output files are
wrapped in HTML and displayed to the user. The
basic PHP program remains largely unchanged from
one NLP technology to the next. In most cases, it
suffices to use the server file system to pass data
back and forth to the NLP program ? PHP pro-
vides primitives for creating and removing unique
temporary files. In only one case was it necessary to
use a semaphore on a hard-coded filename. We also
experimented with Java server pages and Perl CGI
scripts instead of PHP.
2.2 Allowing the student to modify knowledge
resources
The web interfaces to the Cass parser, Gsearch, and
MontyTagger allow the student to provide their cor-
responding knowledge base. For Cass and Gsearch,
an additional text box is provided for the grammars
they require. The rule sequence and lexicon that the
MontyTagger uses can be large and thus unwieldy
for a textarea form input element. We solved
the problem by preloading the textareas with a
?standard? rule sequence and lexicon which the stu-
dent can then modify. We also provided the ability to
upload the rule sequences and lexicon as files. One
problem with the file upload method is that it assume
that the students can generate ASCII-only files with
30
the appropriate line break character. This assump-
tion is often false.
An additional problem with allowing students
to modify knowledge resources is providing use-
ful feedback when these student-provided resources
contain syntax or other types of errors. At this point
we simply capture the stderr output of the pro-
gram and display it.
Finally, with some systems such as Spew
(Schwartz, 1999), and The Dada Engine (Bulhak,
1996), allowing web-based specification of knowl-
edge bases amounts to allowing the student to exe-
cute arbitrary code on the server machine, an obvi-
ous security problem.
2.3 Allowing the student to examine internal
system processing
Displaying system output with a web interface is rel-
atively easy; however, showing the internal work-
ings of a system is more challenging with a web
interface. At this point, we have only displayed
traces of steps of an algorithm. For example, the
NLTK context-free grammar parser interface pro-
vides a trace of the steps of the parsing algorithm.
One possible solution would be to generate Flash
code to animate a system?s processing.
2.4 Availability
The web pages are currently available at que.info-
science.uiowa.edu/?light/classes/compLing/ How-
ever, it is not our intent to provide server cycles for
the community but rather to provide the PHP scripts
open source so that others can run the interfaces
on their own servers. An instructor at another
university has already made use of our code.
3 Lessons learned
? PHP is easier to work with than Java Server
Pages and CGI scripts;
? requiring users to paste input into text boxes is
superior to allowing user to upload files (for se-
curity reasons and because it is easier to control
the character encoding used);
? getting debugging information back to the stu-
dent is very important;
? security is an issue since one is allowing users
to initiate computationally intensive processes;
? it is still possible for students to claim the inter-
face does not work for them (even though we
used no client-side scripting).
? Peer learning is less likely than in a lab set-
ting; however, we provided a web forum and
this seems to alleviated the problem somewhat.
4 Summary
At the University of Iowa, many students, who want
to learn about natural language processing, do not
have the requisite Unix and programming skills to
do labs using command line interfaces. In addition,
our lab machines run MSWindows, the instructors
do not administer the machines, and there are restric-
tive lab hours. Thus, until recently assignments con-
sisted of pencil-and-paper problems. We have built
web-based interfaces to a number of NLP modules
that allow students to use, modify, and learn.
References
Steven Abney. 1997. Partial parsing via finite-state cas-
cades. Natural Language Engineering, 2(4).
Andrew Bulhak. 1996. The dada engine.
http://dev.null.org/dadaengine/.
S. Corley, M. Corley, F. Keller, M. Crocker, and
S. Trewin. 2001. Finding Syntactic Structure in Un-
parsed Corpora: The Gsearch Corpus Query System.
Computers and the Humanities, 35:81?94.
Hugo Liu. 2004. Montylingua: An end-to-end natural
language processor with common sense. homepage.
Edward Loper and Steven Bird. 2002. Nltk: The natural
language toolkit. In Proc. of the ACL-02 Workshop
on Effective Tools and Methods for Teaching Natural
Language Processing and Computational Linguistics.
Ted Pedersen, Satanjeev Banerjee, and Siddharth Pat-
wardhan. 2005. Maximizing Semantic Relatedness to
Perform Word Sense Disambiguation. Supercomput-
ing institute research report umsi 2005/25, University
of Minnesota.
Randal Schwartz. 1999. Random sentence generator.
Linux Magazine, September.
Ian H. Witten and Eibe Frank. 1999. Data Mining: Prac-
tical Machine Learning Tools and Techniques with
Java Implementations. Morgan Kaufmann.
31
