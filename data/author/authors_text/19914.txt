Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 2052?2063, Dublin, Ireland, August 23-29 2014.
Quality Estimation of English-French Machine Translation:
A Detailed Study of the Role of Syntax
Rasoul Kaljahi
??
, Jennifer Foster
?
, Raphael Rubino
??
, Johann Roturier
?
?
NCLT, School of Computing, Dublin City University, Ireland
{rkaljahi, jfoster, rrubino}@computing.dcu.ie
?
Symantec Research Labs, Dublin, Ireland
johann roturier@symantec.com
Abstract
We investigate the usefulness of syntactic knowledge in estimating the quality of English-French
translations. We find that dependency and constituency tree kernels perform well but the error
rate can be further reduced when these are combined with hand-crafted syntactic features. Both
types of syntactic features provide information which is complementary to tried-and-tested non-
syntactic features. We then compare source and target syntax and find that the use of parse trees
of machine translated sentences does not affect the performance of quality estimation nor does
the intrinsic accuracy of the parser itself. However, the relatively flat structure of the French
Treebank does appear to have an adverse effect, and this is significantly improved by simple
transformations of the French trees. Finally, we provide further evidence of the usefulness of
these transformations by applying them in a separate task ? parser accuracy prediction.
1 Introduction
Quality Estimation (QE) for Machine Translation (MT) involves judging the correctness of the output
of an MT system given an input and no reference translation (Blatz et al., 2003; Ueffing et al., 2003;
Specia et al., 2009). An accurate QE-for-MT system would mean that reliable decisions could be made
regarding whether to publish a machine translation as is or to re-direct it to a translator, either for post-
editing or to be translated from scratch. The scores produced by a QE system can also be used to choose
between translations, in a system combination framework or in n-best list reranking. The work presented
here takes place in the context of a wider study, the aim of which is to develop an English-French QE
system so that technical support material that is produced on a daily basis by a company?s English-
speaking customers can be translated automatically into French and made available with confidence to
the company?s French-speaking customer base.
It is reasonable to assume that syntactic features are useful in QE for MT as a way of capturing
the syntactic complexity of the source sentence, the grammaticality of the target translation and the
syntactic symmetry between the source sentence and its translation. This assumption has been borne out
by previous research which has demonstrated the usefulness of syntactic features for English-Spanish
QE (Hardmeier et al., 2012; Rubino et al., 2012). We focus more closely on understanding the role
of syntax by comparing the use of hand-crafted features and tree kernels (Collins and Duffy, 2002;
Moschitti, 2006), and by teasing apart the contribution of target and source syntax.
We find that both tree kernels and manually engineered features produce statistically significantly
better results than a strong set of non-syntactic features provided as a baseline by the organisers of the
2012 WMT shared task on QE for MT (Callison-Burch et al., 2012), and that both types of syntactic
features can be combined fruitfully with this baseline. Furthermore, we show that it is worthwhile to
combine tree kernels with hand-crafted features. Our tree kernel features are the complete set of tree
fragments of both the constituency and dependency trees of the source and target sentences. Our hand-
crafted feature set consists of an initial set of 489 constituency and dependency features which are then
reduced to a set of 144 with no significant loss in performance.
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
2052
We then show that source (English) constituency trees significantly outperform target (French) transla-
tion constituency trees in this task. We hypothesise that this is happening because a) the French parser has
a lower accuracy compared to the English, or b) the target trees sentences are harder to parse, represent-
ing, as they do, potentially ill-formed machine translations which may result in noisier parse trees which
are harder to learn from. If the first hypothesis were true, we would expect to see a drop in the accuracy
of our QE system when we use lower-accuracy parses. We do not observe this. If the second hypothesis
were true, we would expect to observe that the target trees were also less useful than the source trees in
the opposite translation direction (French-English). Instead, we find that the target (English) constituency
trees significantly outperform the source (French) constituency trees, suggesting that the difference be-
tween source and target that we observe in the original English-French experiment is related neither to
intrinsic parser accuracy nor to translation direction but rather to the languages/treebanks.
We explore the extent to which the difference between French and English constituency trees is due
to the relatively flatter structure of the French treebank. We use simple transformation heuristics to
introduce more nodes into the French trees and significantly improve the performance. We also apply
these heuristics in a second task, parser accuracy prediction. This task is similar to QE for MT except
we are predicting the quality of a parse tree in the absence of a reference parse tree. We also find here
that the modified trees also outperform the original trees, suggesting that one must proceed with caution
when using French Treebank tree fragments in a machine-learning task.
The paper?s novel contributions are as follows:
1. Evidence that syntactic information is useful in English-French QE for MT and further evidence
that it is useful in QE for MT in general
2. A comparison of two methods of representing syntactic information in QE
3. A more comprehensive set of syntactic features than has been previously been used in QE for MT
4. A comparison of the role of source and target syntax in English-French QE for MT
5. A set of heuristics that can be applied to French Treebank trees resulting in performance improve-
ments in the tasks of both QE for MT and parser accuracy prediction
The rest of this paper is organised as follows: we discuss related work in using syntax in QE in
Section 2, we describe the data in Section 3, and we then go on to describe the QE framework and the
systems built in Section 4. We follow this with an investigation of the role of source and target syntax in
Section 5 before presenting our heuristics to modify the French constituency trees in Section 6.
2 Related Work
Features extracted from parser output have been used before in training QE for MT systems. Quirk
(2004) uses a single syntax-based feature which indicates whether a full parse for the source sentence
could be found. Hardmeier et al. (2012) employ tree kernels to predict the 1-to-5 post-editing cost of a
machine-translated sentence. They use tree kernels derived from syntactic constituency and dependency
trees of the source side (English) and only dependency trees of the translation side (Spanish). The tree
kernels are used both alone and combined with non-syntactic features. The combined setting ranked
second in the 2012 shared task on QE for MT (Callison-Burch et al., 2012). Rubino et al. (2012) explore
a variety of syntactic features extracted from the output of both a hand-crafted broad-coverage gram-
mar/parser and a statistical constituency parser on the WMT 2012 data set. They find that the syntactic
features make an important contribution to the overall system. In a framework for combining QE and
automatic metrics to evaluate MT output, Specia and Gim?enez (2010) use part-of-speech (POS) tag lan-
guage model probabilities of the MT output 3-grams as features for QE and features built upon syntactic
chunks, dependencies and constituent structure to build automatic MT evaluation metrics. Avramidis
(2012) builds a series of models for estimating post-editing effort using syntactic features such as parse
probabilities and syntactic label frequency. In a similar vein, Gamon et al. (2005) use POS tag trigrams,
CFG rules and features derived from a semantic analysis of the MT output to classify it as fluent or
disfluent.
2053
In this work, we compare the use of tree kernels and hand-crafted features extracted from the con-
stituency and dependency trees of the source and target sides of a translation pair, as well as comparing
the role of source and target syntax. In addition, we conduct a more in-depth analysis of these approaches
and compare the utility of syntactic information extracted from the source side and target sides of the
translation.
3 Data
While there is evidence to suggest that predicting human evaluation scores is superior to predicting
automatic metrics in QE for ME (Quirk, 2004), it has also been shown that human judgements are not
necessarily consistent (Snover et al., 2006). A more practical consideration is that human evaluation
exists for just a few language pairs and domains. To the best of our knowledge, the only available
English-to-French data set which contains human judgements of translation quality are as follows:
? CESTA (Hamon et al., 2007), which is selected from the Official Journal of the European Commis-
sion and also from the health domain. In addition to the domain (and style) difference to newswire
(the domain on which our parsers are trained), a major stumbling block which prevents us from
using this data set is its small size: only 1135 segments have been evaluated manually.
? WMT 2007 (Callison-Burch et al., 2007), which contains only 302 distinct source segments (each
with approx. 5 translations) only half of which is in the news domain.
? FAUST
1
, which is out-of-domain and difficult to apply to our setting as the evaluations and post-
edits are user feedbacks, often in the form of phrases/fragments.
Thus, we instead attempt to predict automatic metric scores as there is a sufficient amount of parallel
text for our language pair and domain. We use BLEU
2
(Papineni et al., 2002), TER
3
(Snover et al., 2006)
and METEOR
4
(Denkowski and Lavie, 2011), which are the most-widely used MT evaluation metrics.
All metrics are applied at the segment level.
5
We randomly select 4500 parallel segments from the News development data sets released for the
WMT13 translation task (Bojar et al., 2013). In order to be independent of any one translation system,
we translate the data set with the following three systems and randomly choose 1500 distinct segments
from each:
? ACCEPT
6
: a phrase-based Moses system trained on training sets of WMT12 releases of Europarl
and News Commentary plus data from Translators Without Borders (TWB)
? SYSTRAN: a proprietary rule-based system
? Bing
7
: an online translation system
The data set is randomly split into 3000 training, 500 development and 1000 test segments. We use the
development set for tuning model parameters and building hand-crafted feature sets, and the test set for
testing model performance and analyses purposes.
4 Syntax-based QE
One way to employ syntactic information in a machine-learning task is to manually compile a set of
features that can be extracted automatically from a parse tree. An example of one such feature is the
label of the root of the tree. Another method is to directly use these trees in a tree kernel (Collins and
Duffy, 2002; Moschitti, 2006). This approach allows exponentially-sized feature spaces (e.g. all subtrees
1
http://www.faust-fp7.eu/faust/Main/DataReleases
2
Version 13a of MTEval script was used at the segment level.
3
TER COMpute 0.7.25: http://www.cs.umd.edu/
?
snover/tercom/
4
METEOR 1.4: http://www.cs.cmu.edu/
?
alavie/METEOR/
5
We present 1-TER to be more easily comparable to BLEU and METEOR. There is no upper bound for TER scores unlike
the other two metrics. Scores higher than 1 occur when the number of errors is higher than the segment length. To avoid this,
scores higher than 1 are cut-off to 1 before being converted to 1-TER.
6
http://www.accept.unige.ch/Products/D_4_1_Baseline_MT_systems.pdf
7
http://www.bing.com/translator
2054
of a tree) to be efficiently modelled using dynamic programming and has shown to be effective in many
natural language processing tasks including parsing and named entity recognition (Collins and Duffy,
2002), semantic role labelling (Moschitti, 2006), sentiment analysis (Wiegand and Klakow, 2010) and
QE for MT (Hardmeier et al., 2012). Although there can be overlap between the information captured by
the two approaches, each can capture information that the other one cannot. In addition, while tree ker-
nels involve minimal feature engineering, hand-crafted features offer more flexibility. Moschitti (2006)
shows that combining the two is beneficial. We use both hand-crafted features and tree kernels, applied
separately and combined together.
For parsing the English and French data into their constituency structures, a PCFG-LA parser
8
is
used. We train the English parser on the training section of the Wall Street Journal (WSJ) section of the
Penn Treebank (PTB) (Marcus et al., 1993). The French parser is trained on the training section of the
French Treebank (FTB) (Abeill?e et al., 2003). We obtain dependency parses by converting the English
constituency parses using the Stanford converter (de Marneffe and Manning, 2008) and the French
parses using Const2Dep (Candito et al., 2010). We evaluate the performance of the QE models using
Root Mean Square Error (RMSE) and Pearson correlation coefficient (r). To compute the statistical
significance of the performance differences between QE models, we use paired bootstrap resampling
following Koehn (2004). We randomly resample (with replacement) a set of N instances from the
predictions of each of the two given systems, where N is the size of the test set. We repeat this sampling
N times and count the number of times each of the two settings is better in terms of each measure (RMSE
and Pearson r). If a setting is better more than 95% of the time, we consider it statistically significant
at p < 0.05.
In the following sections, we first describe our baseline systems and then the quality estimation sys-
tems build using tree kernels, hand-crafted features and a combination of both.
4.1 Baseline QE Systems
In order to verify the usefulness of syntax-based QE, we build two baselines. The first baseline (BM) uses
the mean of the segment-level evaluation scores in the training set for all instances. In the second baseline
(BW), the 17 baseline features of the WMT12 QE Shared Task are used. BW is considered a strong baseline
as the system that used only these features was ranked higher than many of the participating systems.
We use support vector regression implemented in the SVMLight toolkit
9
to build BW. The Radial Basis
Function (RBF) kernel is used. The results for both baselines are presented in the first two rows of
Table 1. Since BW is a stronger baseline than BM, we will compare all syntax-based systems to BW only.
4.2 Syntax-based QE with Tree Kernels
Tree kernels are kernel functions that compute the similarity between two instances of data represented
as trees based on the number of common fragments between them. Therefore, the need for explicitly en-
coding an instance in terms of manually-designed and extracted features is eliminated, while benefitting
from a very high-dimensional feature space. Moschitti (2006) introduces an efficient implementation
of tree kernels within a support vector machine framework. Instead of extracting all possible tree frag-
ments, the algorithm compares only tree fragments rooted in two similar nodes. This algorithm is made
available through SVMLight-TK software
10
, which is used in this work.
In order to extract tree kernels from dependency trees, the labels on the arcs must be removed. Fol-
lowing Tu et al. (2012), the nodes in the resulting tree representation are word forms and dependency
relations, omitting POS tag information. An example is shown in Figure 1. A word is a child of its
dependency relation to its head. The dependency relation in turn is the child of the head word. This
continues until the root of the tree.
Based on preliminary experiments on our development set, we use subset tree kernels, where the tree
fragments are subtrees rooted at any node in the tree so that no production rule expanding a node in the
8
https://github.com/CNGLdlab/LORG-Release. The Lorg parser is very similar to the Berkeley parser (Petrov
et al., 2006), the main difference being its unknown word handling mechanism (Attia et al., 2010).
9
http://svmlight.joachims.org/
10
http://disi.unitn.it/moschitti/Tree-Kernel.htm
2055
BLEU 1-TER METEOR
RMSE r RMSE r RMSE r
BM 0.1626 0 0.1965 0 0.1657 0
BW 0.1601 0.1766 0.1949 0.1565 0.1625 0.2047
TK 0.1581 0.2437 0.1888 0.2774 0.1595 0.2715
BW+TK 0.1570 0.2696 0.1879 0.2939 0.1576 0.3111
HC 0.1603 0.1998 0.1913 0.2365 0.1610 0.2516
BW+HC 0.1587 0.2418 0.1899 0.2611 0.1585 0.2964
SyQE 0.1577 0.2535 0.1887 0.2797 0.1594 0.2743
BW+SyQE 0.1568 0.2802 0.1879 0.2937 0.1576 0.3127
Table 1: QE performances measured by RMSE and Pearson r; BM: Mean baseline, BW: WMT 17 base-
line features, TK: tree kernels, HC: hand-crafted features, SyQE: full syntax-based systems (TK+HC).
Statistically significantly better scores compared to their counterpart (upper row in the row block) are in
bold.
rootcamecc      advmod      nsubj      punctAnd       then           era            .                      det      amod                        the    American 
Figure 1: Tree Kernel Representation of Dependency Structure for And then the American era came.
subtree is split. Unlike subtree kernels, subset tree kernels allow tree fragments with non-terminals as
leaves. We tune the C parameter for Pearson r on the development set, with all other parameters left as
default.
We build a system with all four parse trees for every training instance, which includes the constituency
and dependency trees of the source and target side of the translation. The third row of Table 1 shows
the performance of this system which is named TK. The results achieved using this system represent a
statistically significant improvement over the BW baseline results. In order to examine their complemen-
tarity, we combine these tree kernels and the baseline features (BW+TK) in the fourth row of Table 1.
This combined system performs better than the two individual systems.
While BLEU prediction is the most accurate (lowest RMSE), METEOR prediction appears to be the
easiest to learn (highest Pearson r). TER prediction seems to be more difficult than BLEU and METEOR
prediction, especially in terms of prediction error. This is probably related to the distribution of each of
these metric scores in our data set. The standard deviations (?) of BLEU, TER and METEOR scores are
0.1620, 0.1943 and 0.1652 respectively. The substantially higher ? of TER scores makes them harder to
predict accurately leading to higher prediction error.
4.3 Syntax-based QE with Hand-crafted Features
We design a set of constituency and dependency feature types, some of which have previously been used
by the works described in Section 2 and some introduced here. Each feature type contains at least two
features, one extracted from the source and the other from the translation. Numerical feature types can
be further instantiated by extracting the ratio and differences between the source and target side feature
values. Some feature types are parametric meaning that they can be varied by changing the value of a pa-
rameter. For example, the non-terminal label is a parameter for the non-terminal-label-count
2056
Constituency
?1 Label of the root node of the constituency tree
2 Height of the constituency tree which is the number of edges from root node to the farthest terminal (leaf) node
?3 Number of nodes in the constituency tree
4 Log probability of the constituency parse assigned by the parser
?5 Parseval F
1
score of the tree with respect to a tree produced by the Stanford parser (Klein and Manning, 2003)
?6 Right hand side of the CFG production rule expanding the root node
7 All non-lexical and lexical CFG production rules expanding the tree nodes
?8 Average arity of the non-lexical CFG production rules expanding the constituency tree nodes
9 Counts of each non-terminal label in the tree
?10 POS unigrams, 3-grams and 5-grams
11 POS n-gram scores against language models trained on the POS tags of the respective treebanks using the SRILM
toolkit (http://www.speech.sri.com/projects/srilm/) with Witten-Bell smoothing
?12 Counts of each 12 universal POS tags (Petrov et al., 2012)
?13 Location of the first verb in the sentence in terms of the token distance from the beginning
?14 Average number of POS n-grams in each n-gram frequency quartile of the POS corpora of the respective treebanks
Dependency
?1 POS tag of the top node (dependent of the dummy root node) of the dependency tree
?2 Number of dependents of the top node
?3 Sequence of all dependency relations which modify the top node
?4 Sequence of the POS tags of the dependents of the top node
?5 Average number of dependents per node
?6 Height of the tree computed in the same way for the constituency tree
?7 3- and 5-gram sequences of dependency relations of the tokens to their head
?8 Number of most frequent dependency relations in our News training set
?9 Dependency relation n-gram scores against language models trained on the respective treebanks for each language
?10 Average number of dependency relation n-grams in each n-gram frequency quartile of the respective treebanks
?11 Pairs of tokens and their dependency relations to their head
Table 2: Constituency and dependency feature types
feature type. Therefore, it instantiates as several features, one for each non-terminal-label.
As in BW, we use support vector machines (SVM) to build the QE systems using these hand-crafted
features. We keep only those features which fire for more than a threshold which is set empirically on
the development set. Table 2 lists our syntax-based feature types and their descriptions. Those that have,
to the best of our knowledge, not been used in QE for MT before are marked with an asterisk.
The total number of feature-value pairs in the full feature set is 489. Since this feature set is large
and contains many sparse features, we attempt to reduce it through ablation experiments in which we
directly compare the effect of leaving out features that we suspect may be redundant. For example, we
investigate whether either the ratio or difference of the source and target numerical features or both of
them are redundant by building three systems, one without ratio features, one without difference features
and one with neither. This process is also carried out for log probability and perplexity features, original
and universal POS-tag-based features, n-gram and language model score features, lexical and non-lexical
CFG rules, and n-gram orders (i.e. 3-gram vs. 5-gram features). This process proved useful: we found,
for example, that either 3- or 5-grams worked better than both together and features based on universal
POS tags better than those based on original POS tags.
The final reduced feature set contains 144 features-value pairs. We build one QE system with all 489
features HC-all and one with the reduced set of 144 features HC . Table 3 compares the performance on
the development and test set. The system with the reduced feature set performs consistently better than
the HC-all system on the development set, mostly with statistically significant differences. However,
on the test set, the performance degrades albeit not statistically significantly. Considering a more than
70% reduction in feature set size, this relatively small degradation is tolerable. We use the reduced
feature set as our hand-crafted feature set for the rest of the work.
Compared to TK in Table 1 (third and fourth versus fifth and sixth rows), the performances are lower
for all MT metrics, though not statistically significantly. It is worth noting that we observed an opposite
2057
BLEU 1-TER METEOR
RMSE r RMSE r RMSE r
Development Set
HC-all 0.1567 0.3026 0.1851 0.2746 0.1575 0.2996
HC 0.1540 0.3398 0.1819 0.3263 0.1547 0.3452
Test Set
HC-all 0.1603 0.2108 0.1902 0.2510 0.1607 0.2493
HC 0.1603 0.1998 0.1913 0.2365 0.1610 0.2516
Table 3: QE performance with all hand-crafted syntactic features HC-all and the reduced feature set
HC. Statistically significantly better scores compared to their counterpart (upper row) are in bold.
RMSE r
TK-CD-ST 0.1581 0.2437
TK-CD-S 0.1584 0.2294
TK-CD-T 0.1597 0.2101
TK-C-S 0.1583 0.2312
TK-C-T 0.1608 0.1479
TK-D-S 0.1598 0.1869
TK-D-T 0.1598 0.2102
Table 4: BLEU prediction performances with tree kernels of only source S or translation T side trees.
The scores in bold are statistically better than their counterparts in the same row block. The original
result with source and target combined is provided for reference in the first row.
behaviour on the development set, where hand-crafted features largely outperform tree kernels. This
suggests that the tree kernels are more generalisable. We also combine these features with the WMT
17 baseline features (BW+HC). This combination also improves over both syntax-based and baseline
systems, confirming again the usefulness of syntactic information in addition to surface features.
We combine tree kernels and hand-crafted features to build a full syntax-based QE system (SyQE),
which improves over both TK and HC (Table 1) . The improvements for TER and METEOR prediction
are slight but statistically significant for BLEU prediction. This system is also combined with BW in
BW+SyQE (the last row of Table 1), resulting in statistically significant gains for all metrics.
5 Source and Target Syntax in Syntax-based QE
We now turn our attention to the parts played by source and target syntax in QE for MT. To save space,
we present only the BLEU scores for the tree kernel systems. Table 4 shows the results achieved by
systems built using either the source or target side of the translations.
At a glance, it can be seen that the source side constituency tree kernels outperform the target side
ones, while the opposite is the case for dependency tree kernels. The differences for constituency trees
are however substantially bigger. When both constituency and dependency trees are combined, the source
side trees perform better (TK-CD-S vs. TK-CD-T).
The following three hypotheses could explain this difference between TK-C-S and TK-C-T:
1. The Role of Parser Accuracy: The fact that French parsing models do not reach the high Parseval
F1s achieved by English parsing models could explain the difference in usefulness between the
French and English consistuency trees. On the standard parsing test sets, the English parsing model
achieves an F1 of 89.6 and the French an F
1
of 83.4.
2. Parsing Machine Translation Output: The difference between the source and target could be
happening because the target side is machine translation output and (presumably) represents a lower
2058
Sombre  Matter  Affects  de  vol  Probes   spatiale
NP
NP
NP
PP
SENT
NC
ET ET P
ET ET ET
Figure 2: Parse tree of the machine translation of Dark Matter Affects Flight of Space Probes to French
quality set of sentences than the source (see Figure 2 for an example of a parse tree for a poor
translation).
3. Differences in Annotation Strategies: The difference between the source and target could be due
to the idiosyncrasies of the underlying treebanks which is not carried over via the conversion tools
to the dependency structure.
Hypotheses 1 and 2 relate the usefulness of parse trees in QE to the intrinsic quality of the parse trees.
French constituency trees are less accurate than English ones, either because the French parsing model
is not as accurate as the English one (Hypothesis 1) or because the possibly ungrammatical nature of the
French parsing input adversely affects the quality of the parse tree (Hypothesis 2). Although this low
quality would be expected to affect the dependency trees in the same way since they are directly derived
from the consistency trees, this is not the case and it appears that the problematic aspects of the French
parses are abstracted away from the dependency trees.
To test the first hypothesis, we investigate the role of parser accuracy in QE. For both languages, we
substitute the standard parsing models used in all our prior experiments with ?lower-accuracy? mod-
els trained using only a fraction of the training data (following Quirk and Corston-Oliver (2006)). The
English parsing model achieves an F
1
of 72.5 and the French an F
1
of 66.5, representing drops of ap-
proximately 17 points from the original models. The RMSE and Pearson r of the new QE model are
0.1583 and 0.2350 compared to 0.1581 and 0.2437 of the one trained with original trees (see also the
third row of Table 1). These results show that the use of these lower-accuracy models has only a minimal
and statistically insignificant effect on QE performance, suggesting that intrinsic parser accuracy is not
the reason why the target constituency trees are less useful than the source constituency trees.
11
To investigate the second hypothesis, we switch the translation direction to French-to-English. There-
fore, we now parse the well-formed French input sentences and the machine-translated English segments.
If the second hypothesis were true, the target side parse trees in this direction would still underperform
the source side ones. The results are shown in Table 5. All the systems using target trees outperform
those using source trees. The difference between source and target in the models that use constituency
trees is especially substantial and statistically significant. Thus, it is apparent that the suspected lower
quality of constituency parse trees of MT output is not the reason for the lower QE performance.
We now seek the answer in our third hypothesis, i.e. in the difference between the annotation schemes
of the PTB and the FTB. One major difference, noted by, for example, Schluter and van Genabith (2007),
is that the FTB has a relatively flatter structure. It lacks a verb phrase (VP) node and phrases modifying
the verb are the sibling of the verb nucleus. We investigate this further in the next section.
6 Modifying French Parse Trees
In order to test whether the annotation strategy is a reason for the lower performance of French con-
stituency tree kernels, we apply a set of three heuristics which introduce more structure to the French
parse trees (1&2) or simply make them more PTB-like (3):
? Heuristic 1 automatically adds a VP node above the verb node (VN) and at most 3 of its immediate
adjacent nodes if they are noun or prepositional phrases (NP or PP).
11
See (Kaljahi et al., 2013) for a more detailed exploration of the role of parser accuracy in QE for MT.
2059
RMSE r
TK-FE/CD-ST 0.1561 0.2334
TK-FE/CD-S 0.1574 0.1830
TK-FE/CD-T 0.1559 0.2423
TK-FE/C-S 0.1581 0.1578
TK-FE/C-T 0.1556 0.2336
TK-FE/D-S 0.1577 0.1655
TK-FE/D-T 0.1579 0.1886
Table 5: BLEU prediction performances with tree kernels for Fr-En direction (FE) (C: constituency, D:
dependency, S: source, T: translation)
RMSE r
TK-C-T 0.1608 0.1479
TK-C-T
m
0.1591 0.2143
TK-CD-ST 0.1581 0.2437
TK-CD-ST
m
0.1574 0.2609
Table 6: QE with tree kernels using original and modified French trees (
m
)
? Heuristic 2 stratifies some of the production rules in the tree by grouping together every two equal
adjacent POS tags under a new node with a tag made of the POS tag suffixed with St.
? Heuristic 3 moves coordinated nodes (the immediate left sibling of the COORD node) under COORD.
Figure 3 shows examples of the application of each of these methods. We apply these heuristics to
the parsed MT output in the English-French translation direction and rebuild the tree kernel system with
translation side constituency trees (TK-C-T) and the full tree kernel system (TK-CD-ST) with the mod-
ified trees. The results are presented in Table 6. Despite the possibility of introducing linguistic errors,
these heuristics yield a statistically significant improvement in QE performance. Unsurprisingly, the
changes are bigger for the system with only translation side constituency trees as in the full system there
are three other tree types involved. These results suggest that the structure of the French constituency
trees is a factor in the lower performance of its tree kernels in QE.
12
The gain achieved by applying these heuristics is related to the fact that there are more similar frag-
ments extracted from the modified structure which are useful for the tree kernel system. For example, in
the original top left tree in Figure 3, there is no chance that a fragment consisting only of VN and NP ?
a very common structure and thus useful in calculating tree similarity ? will be extracted by the subset
tree kernel. The reason is that this kernel type does not allow the production rule to be split (in this case
the rule expanding the S node). However, after applying Heuristic 1, the fragment equivalent to VP ->
VN NP production rule can be easily extracted. Among the three heuristics, the first one contributes the
largest part of the improvement; the other two have a very slight effect according to the results of their
individual application, though they contribute to the overall performance when all three are combined.
The success of using modified French trees in improving tree kernel performance may of course de-
pend on the data set and even the task in hand, and may not be generalisable. We next explore this
question by applying the modification to a different task and a different data set.
6.1 Parser Accuracy Prediction
The task we choose is parser accuracy prediction, the aim of which is to predict the accuracy of a parse
tree without a reference (QE for parsing). The task was previously explored for English by Ravi et al.
12
We also see a slightly smaller improvement for the hand-crafted features using the modified French trees. The combina-
tion of tree kernels and hand-crafted features with the modified trees leads to a statistically significant improvement over the
combination with the original trees.
2060
Figure 3: Application of tree modification heuristics on example French translation parse trees
RMSE r
PAP 0.1239 0.4035
PAP
m
0.1233 0.4197
Table 7: Parser Accuracy Prediction (PAP) performance with tree kernels using original and modified
French trees (
m
)
(2008). We build a tree kernel model to predict the accuracy of French parses. To train the system, we
parse the training section of FTB with our French parser and score them using F
1
. We use the FTB
development set to tune the SVM C parameter and test the model on the FTB test set. Two parser
accuracy prediction models are then built using this setting, one with the original parse trees and the
second with the modified parse trees produced using the three heuristics listed above. The results are
presented in Table 7.
Both RMSE and Pearson r improve with the modified trees, where the r improvement is statistically
significant. Although the improvement we observe is not as large as the one we observed for the QE for
MT task, the results add weight to our claim that the structure of the FTB trees should be optimised for
use in tree kernel learning.
7 Conclusion
We analysed the utility of syntactic information in QE of English-French MT and found it useful both
individually and combined with standard QE features. We found that tree kernels are a convenient and
effective way of encoding syntactic knowledge but that our hand-crafted feature set also brings additional,
useful information. As a result of comparing the role of source and target syntax, we also found that the
constituent structure in the FTB could be amended to be more useful in QE for MT and parser accuracy
prediction. Now that we have explored the role of syntax in this project, our next step is try to further
improve our QE system by adding semantic information. However, there are many other ways in which
the research in this paper could be further extended. Our focus is on the language pair English-French
and the QE task but it would certainly be interesting to perform a similar analysis on the role of syntax
in QE for other language pairs, or to investigate the impact of French tree modification on other tasks.
2061
Acknowledgments
This research has been supported by the Irish Research Council Enterprise Partnership Scheme (EP-
SPG/2011/102 and EPSPD/2011/135) and the computing infrastructure of the Centre for Next Gener-
ation Localisation at Dublin City University. We are grateful to Djam?e Seddah for useful discussions
about the French Treebank. We also thank the reviewers for their helpful comments.
References
Anne Abeill?e, Lionel Cl?ement, and Franc?ois Toussenel. 2003. Building a treebank for French. In Treebanks:
Building and Using Syntactically Annotated Corpora. Kluwer Academic Publishers.
Mohammed Attia, Jennifer Foster, Deirdre Hogan, Joseph Le Roux, Lamia Tounsi, and Josef van Genabith. 2010.
Handling unknown words in statistical latent-variable parsing models for Arabic, English and French. In Pro-
ceedings of the 1st Workshop on Statistical Parsing of Morphologically Rich Languages.
Eleftherios Avramidis. 2012. Quality estimation for machine translation output using linguistic analysis and
decoding features. In Proceedings of WMT.
John Blatz, Erin Fitzgerald, George Foster, Simona Gandrabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis, and
Nicola Ueffing. 2003. Confidence estimation for machine translation. In JHU/CLSP Summer Workshop Final
Report.
Ond?rej Bojar, Christian Buck, Chris Callison-Burch, Christian Federmann, Barry Haddow, Philipp Koehn, Christof
Monz, Matt Post, Radu Soricut, and Lucia Specia. 2013. Findings of the 2013 workshop on statistical machine
translation. In Proceedings of the 8th WMT.
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn, Christof Monz, and Josh Schroeder. 2007. (meta-)
evaluation of machine translation. In Proceedings of the Second Workshop on Statistical Machine Translation,
pages 136?158.
Chris Callison-Burch, Philipp Koehn, Christof Monz, Matt Post, Radu Soricut, and Lucia Specia. 2012. Findings
of the 2012 workshop on statistical machine translation. In Proceedings of the Seventh WMT.
Marie Candito, Benot Crabb, and Pascal Denis. 2010. Statistical French dependency parsing: treebank conversion
and first results. In Proceedings of LREC.
Michael Collins and Nigel Duffy. 2002. New ranking algorithms for parsing and tagging: Kernels over discrete
structures, and the voted perceptron. In Proceedings of the ACL.
Marie-Catherine de Marneffe and Christopher D. Manning. 2008. The Stanford typed dependencies representa-
tion. In Proceedings of the COLING Workshop on Cross-Framework and Cross-Domain Parser Evaluation.
Michael Denkowski and Alon Lavie. 2011. Meteor 1.3: Automatic metric for reliable optimization and evaluation
of machine translation systems. In Proceedings of WMT.
Michael Gamon, Anthony Aue, and Martine Smets. 2005. Sentence-level MT evaluation without reference trans-
lations: beyond language modeling. In EAMT.
Olivier Hamon, Antony Hartley, Andr?ei Popescu-Belis, and Khalid Choukri. 2007. Assessing human and auto-
mated quality judgments in the french MT evaluation campaign CESTA. In Proceedings of the MT Summit.
Christian Hardmeier, Joakim Nivre, and J?org Tiedemann. 2012. Tree kernels for machine translation quality
estimation. In Proceedings of the WMT.
Rasoul Samed Zadeh Kaljahi, Jennifer Foster, Raphael Rubino, Johann Roturier, and Fred Hollowood. 2013.
Parser accuracy in quality estimation of machine translation: A tree kernel approach. In Proceedings of IJCNLP.
Dan Klein and Christopher D. Manning. 2003. Accurate Unlexicalized Parsing. In Proceedings of the ACL.
Philipp Koehn. 2004. Statistical significance tests for machine translation evaluation. In Proceedings of EMNLP.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. 1993. Building a large annotated corpus of
English: the Penn Treebank. Computational Linguistics, 19(2):313?330.
Alessandro Moschitti. 2006. Making tree kernels practical for natural language learning. In Proceedings of EACL.
2062
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evaluation
of machine translation. In Proceedings of the ACL.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 2006. Learning accurate, compact and interpretable
tree annotation. In Proceedings of the 21st COLING-ACL.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012. A universal part-of-speech tagset. In Proceedings of
LREC.
Chris Quirk and Simon Corston-Oliver. 2006. The impact of parse quality on syntactically-informed statistical
machine translation. In Proceedings of EMNLP.
Chris Quirk. 2004. Training a sentence-level machine translation confidence measure. In Proceedings of LREC.
Sujith Ravi, Kevin Knight, and Radu Soricut. 2008. Automatic prediction of parser accuracy. In Proceedings of
EMNLP.
Raphael Rubino, Jennifer Foster, Joachim Wagner, Johann Roturier, Rasoul Kaljahi, and Fred Hollowood. 2012.
DCU-Symantec submission for the WMT 2012 quality estimation task. In Proceedings of WMT.
Natalie Schluter and Josef van Genabith. 2007. Preparing, restructuring, and augmenting a french treebank:
Lexicalised parsers or coherent treebanks? In Proceedings of the 10th Conference of the Pacific Association for
Computational Linguistics.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A study of transla-
tion edit rate with targeted human annotation. In Proceedings of AMTA.
Lucia Specia and Jes?us Gim?enez. 2010. Combining confidence estimation and reference-based metrics for seg-
ment level mt evaluation. In Proceedings of AMTA.
Lucia Specia, Marco Turchi, Nicola Cancedda, Marc Dymetman, and Nello Cristianini. 2009. Estimating the
sentence-level quality of machine translation systems. In EAMT, pages 28?35.
Zhaopeng Tu, Yifan He, Jennifer Foster, Josef van Genabith, Qun Liu, and Shouxun Lin. 2012. Identifying high-
impact sub-structures for convolution kernels in document-level sentiment classification. In Proceedings of the
ACL.
Nicola Ueffing, Klaus Macherey, and Hermann Ney. 2003. Confidence measures for statistical machine transla-
tion. In Machine Translation Summit IX.
Michael Wiegand and Dietrich Klakow. 2010. Convolution kernels for opinion holder extraction. In Proceedings
of NAACL-HLT.
2063
Proceedings of the ACL 2010 Student Research Workshop, pages 91?96,
Uppsala, Sweden, 13 July 2010. c?2010 Association for Computational Linguistics
 
 
Adapting Self-training for Semantic Role Labeling 
 
Rasoul Samad Zadeh Kaljahi 
FCSIT, University of Malaya 
50406, Kuala Lumpur, Malaysia. 
rsk7945@perdana.um.edu.my 
 
  
 
Abstract 
Supervised semantic role labeling (SRL) sys-
tems trained on hand-crafted annotated corpo-
ra have recently achieved state-of-the-art per-
formance. However, creating such corpora is 
tedious and costly, with the resulting corpora 
not sufficiently representative of the language. 
This paper describes a part of an ongoing work 
on applying bootstrapping methods to SRL to 
deal with this problem. Previous work shows 
that, due to the complexity of SRL, this task is 
not straight forward. One major difficulty is 
the propagation of classification noise into the 
successive iterations. We address this problem 
by employing balancing and preselection me-
thods for self-training, as a bootstrapping algo-
rithm. The proposed methods could achieve 
improvement over the base line, which do not 
use these methods. 
1 Introduction 
Semantic role labeling has been an active re-
search field of computational linguistics since its 
introduction by Gildea and Jurafsky (2002). It 
reveals the event structure encoded in the sen-
tence, which is useful for other NLP tasks or ap-
plications such as information extraction, ques-
tion answering, and machine translation (Surdea-
nu et al, 2003). Several CoNLL shared tasks 
(Carreras and Marquez, 2005; Surdeanu et al, 
2008) dedicated to semantic role labeling affirm 
the increasing attention to this field. 
One important supportive factor of studying 
supervised statistical SRL has been the existence 
of hand-annotated semantic corpora for training 
SRL systems. FrameNet (Baker et al, 1998) was 
the first such resource, which made the emer-
gence of this research field possible by the se-
minal work of Gildea and Jurafsky (2002). How-
ever, this corpus only exemplifies the semantic 
role assignment by selecting some illustrative 
examples for annotation. This questions its suita-
bility for statistical learning. Propbank was 
started by Kingsbury and Palmer (2002) aiming 
at developing a more representative resource of 
English, appropriate for statistical SRL study. 
Propbank has been used as the learning 
framework by the majority of SRL work and 
competitions like CoNLL shared tasks. However, 
it only covers the newswire text from a specific 
genre and also deals only with verb predicates. 
All state-of-the-art SRL systems show a dra-
matic drop in performance when tested on a new 
text domain (Punyakanok et al, 2008). This 
evince the infeasibility of building a comprehen-
sive hand-crafted corpus of natural language use-
ful for training a robust semantic role labeler. 
A possible relief for this problem is the utility 
of semi-supervised learning methods along with 
the existence of huge amount of natural language 
text available at a low cost. Semi-supervised me-
thods compensate the scarcity of labeled data by 
utilizing an additional and much larger amount 
of unlabeled data via a variety of algorithms. 
Self-training (Yarowsky, 1995) is a semi-
supervised algorithm which has been well stu-
died in the NLP area and gained promising re-
sult. It iteratively extend its training set by labe-
ling the unlabeled data using a base classifier 
trained on the labeled data. Although the algo-
rithm is theoretically straightforward, it involves 
a large number of parameters, highly influenced 
by the specifications of the underlying task. Thus 
to achieve the best-performing parameter set or 
even to investigate the usefulness of these algo-
rithms for a learning task such as SRL, a tho-
rough experiment is required. This work investi-
gates its application to the SRL problem. 
2 Related Work  
The algorithm proposed by Yarowsky (1995) for 
the problem of word sense disambiguation has 
been cited as the origination of self-training. In 
that work, he bootstrapped a ruleset from a 
91
  
small number of seed words extracted from 
an online dictionary using a corpus of unan-
notated English text and gained a compara-
ble accuracy to fully supervised approaches. 
Subsequently, several studies applied the algo-
rithm to other domains of NLP. Reference reso-
lution (Ng and Cardie 2003), POS tagging (Clark 
et al, 2003), and parsing (McClosky et al, 2006) 
were shown to be benefited from self-training. 
These studies show that the performance of self-
training is tied with its several parameters and 
the specifications of the underlying task. 
In SRL field, He and Gildea (2006) used self-
training to address the problem of unseen frames 
when using FrameNet as the underlying training 
corpus. They generalized FrameNet frame ele-
ments to 15 thematic roles to control the com-
plexity of the process. The improvement gained 
by the progress of self-training was small and 
inconsistent. They reported that the NULL label 
(non-argument) had often dominated other labels 
in the examples added to the training set. 
Lee et al (2007) attacked another SRL learn-
ing problem using self-training. Using Propbank 
instead of FrameNet, they aimed at increasing 
the performance of supervised SRL system by 
exploiting a large amount of unlabeled data 
(about 7 times more than labeled data). The algo-
rithm variation was similar to that of He and Gil-
dea (2006), but it only dealt with core arguments 
of the Propbank. They achieved a minor im-
provement too and credited it to the relatively 
poor performance of their base classifier and the 
insufficiency of the unlabeled data. 
3 SRL System 
To have enough control over entire the system 
and thus a flexible experimental framework, we 
developed our own SRL system instead of using 
a third-party system. The system works with 
PropBank-style annotation and is described here. 
Syntactic Formalism: A Penn Treebank con-
stituent-based approach for SRL is taken. Syn-
tactic parse trees are produced by the reranking 
parser of Charniak and Johnson (2005). 
Architecture: A two-stage pipeline architec-
ture is used, where in the first stage less-probable 
argument candidates (samples) in the parse tree 
are pruned, and in the next stage, final arguments 
are identified and assigned a semantic role. 
However, for unlabeled data, a preprocessing 
stage identifies the verb predicates based on the 
POS tag assigned by the parser. The joint argu-
ment identification and classification is chosen to 
decrease the complexity of self-training process. 
Features: Features are listed in table 1. We 
tried to avoid features like named entity tags to 
less depend on extra annotation. Features marked 
with * are used in addition to common features 
in the literature, due to their impact on the per-
formance in feature selection process. 
Classifier: We chose a Maximum Entropy 
classifier for its efficient training time and also 
its built-in multi-classification capability. More-
over, the probability score that it assigns to labels 
is useful in selection process in self-training. The 
Maxent Toolkit1 was used for this purpose. 
                                                 
1http://homepages.inf.ed.ac.uk/lzhang10/maxent_tool
kit.html 
Feature Name Description 
Phrase Type Phrase type of the constitu-
ent 
Position+Predicate 
Voice 
Concatenation of constitu-
ent position relative to verb 
and verb voice 
Predicate Lemma  Lemma of the predicate 
Predicate POS POS tag of the predicate 
Path Tree path of non-terminals 
from predicate to constitu-
ent 
Head Word 
Lemma 
Lemma of the head word 
of the constituent 
Content Word  
Lemma 
Lemma of the content 
word of the constituent 
Head Word POS POS tag of the head word 
of the constituent 
Content Word POS POS tag of the head word 
of the constituent 
Governing Category The first VP or S ancestor 
of a NP constituent 
Predicate 
Subcategorization 
Rule expanding the predi-
cate's parent 
Constituent 
Subcategorization * 
Rule expanding the consti-
tuent's parent 
Clause+VP+NP 
Count in Path 
Number of clauses, NPs 
and VPs in the path 
Constituent and  
Predicate Distance 
Number of words between 
constituent and predicate 
Compound Verb 
Identifier 
Verb predicate structure 
type: simple, compound, or 
discontinuous compound 
Head Word Loca-
tion in Constituent * 
Location of head word in-
side the constituent based 
on the number of words in 
its right and left 
Table 1: Features 
92
  
4 Self-training  
4.1 The Algorithm  
While the general theme of the self-training algo-
rithm is almost identical in different implementa-
tions, variations of it are developed based on the 
characteristics of the task in hand, mainly by cus-
tomizing several involved parameters. Figure 1 
shows the algorithm with highlighted parameters. 
The size of seed labeled data set L and unla-
beled data U, and their ratio are the fundamental 
parameters in any semi-supervised learning. The 
data used in this work is explained in section 5.1. 
In addition to performance, efficiency of the 
classifier (C) is important for self-training, which 
is computationally expensive. Our classifier is a 
compromise between performance and efficien-
cy. Table 2 shows its performance compared to 
the state-of-the-art (Punyakanok et al 2008) 
when trained on the whole labeled training set. 
Stop criterion (S) can be set to a pre-
determined number of iterations, finishing all of 
the unlabeled data, or convergence of the process 
in terms of improvement. We use the second op-
tion for all experiments here. 
In each iteration, one can label entire the 
unlabeled data or only a portion of it. In the latter 
case, a number of unlaleled examples (p) are 
selected and loaded into a pool (P). The selection 
can be based on a specific strategy, known as 
preselection (Abney, 2008) or simply done 
according to the original order of the unlabeled 
data. We investigate preselection in this work. 
After labeling the p unlabeled data, training 
set is augmented by adding the newly labeled 
data. Two main parameters are involved in this 
step: selection of labeled examples to be added to 
training set and addition of them to that set. 
Selection is the crucial point of self-training, 
in which the propagation of labeling noise into 
upcoming iterations is the major concern. One 
can select all of labeled examples, but usually 
only a number of them (n), known as growth 
size, based on a quality measure is selected. This 
measure is often the confidence score assigned 
by the classifier. To prevent poor labelings 
diminishing the quality of training set, a 
threshold (t) is set on this confidence score. 
Selection is also influenced by other factors, one 
of which being the balance between selected 
labels, which is explored in this study and 
explained in detail in the section 4.3. 
The selected labeled examples can be retained 
in unlabeled set to be labeled again in next 
iterations (delibility) or moved so that they are 
labeled only once (indelibility). We choose the 
second approach here. 
4.2 Preselection 
While using a pool can improve the efficiency of 
the self-training process, there can be two other 
motivations behind it, concerned with the per-
formance of the process. 
One idea is that when all data is labeled, since 
the growth size is often much smaller than the 
labeled size, a uniform set of examples preferred 
by the classifier is chosen in each iteration. This 
leads to a biased classifier like the one discussed 
in previous section. Limiting the labeling size to 
a pool and at the same time (pre)selecting diver-
gence examples into it can remedy the problem. 
The other motivation is originated from the 
fact that the base classifier is relatively weak due 
to small seed size, thus its predictions, as the 
measure of confidence in selection process, may 
not be reliable. Preselecting a set of unlabeled 
examples more probable to be correctly labeled 
by the classifier in initial steps seems to be a use-
ful strategy against this fact.  
We examine both ideas here, by a random pre-
selection for the first case and a measure of sim-
plicity for the second case. Random preselection 
is built into our system, since we use randomized 
1- Add the seed example set L to currently 
empty training set T. 
2- Train the base classifier C with training 
set T. 
3- Iterate the following steps until the stop 
criterion S is met. 
a- Select p examples from U into pool 
P. 
b- Label pool P with classifier C 
c- Select n labeled examples with the 
highest confidence score whose score 
meets a certain threshold t and add to 
training set T. 
d- Retrain the classifier C with new 
training set. 
Figure 1: Self-training Algorithm 
 WSJ Test Brown Test 
P R F1 P R F1 
Cur 77.43 68.15 72.50 69.14 57.01 62.49
Pun 82.28 76.78 79.44 73.38 62.93 67.75
Table 2: Performances of the current system (Cur) 
and the state-of-the-art (Punyakanok et al, 2008) 
93
  
training data. As the measure of simplicity, we 
propose the number of samples extracted from 
each sentence; that is we sort unlabeled sen-
tences in ascending order based on the number of 
samples and load the pool from the beginning. 
4.3 Selection Balancing 
Most of the previous self-training problems in-
volve a binary classification. Semantic role labe-
ling is a multi-class classification problem with 
an unbalanced distribution of classes in a given 
text. For example, the frequency of A1 as the 
most frequent role in CoNLL training set is 
84,917, while the frequency of 21 roles is less 
than 20. The situation becomes worse when the 
dominant label NULL (for non-arguments) is 
added for argument identification purpose in a 
joint architecture. This biases the classifiers to-
wards the frequent classes, and the impact is 
magnified as self-training proceeds. 
In previous work, although they used a re-
duced set of roles (yet not balanced), He and 
Gildea (2006) and Lee et al (2007), did not dis-
criminate between roles when selecting high-
confidence labeled samples. The former study 
reports that the majority of labels assigned to 
samples were NULL and argument labels ap-
peared only in last iterations.  
To attack this problem, we propose a natural 
way of balancing, in which instead of labeling 
and selection based on argument samples, we 
perform a sentence-based selection and labeling. 
The idea is that argument roles are distributed 
over the sentences. As the measure for selecting 
a labeled sentence, the average of the probabili-
ties assigned by the classifier to all argument 
samples extracted from the sentence is used. 
5 Experiments and Results  
In these experiments, we target two main prob-
lems addressed by semi-supervised methods: the 
performance of the algorithm in exploiting unla-
beled data when labeled data is scarce and the 
domain-generalizability of the algorithm by us-
ing an out-of-domain unlabeled data. 
We use the CoNLL 2005 shared task data and 
setting for testing and evaluation purpose. The 
evaluation metrics include precision, recall, and 
their harmonic mean, F1. 
5.1 The Data 
The labeled data are selected from Propbank 
corpus prepared for CoNLL 2005 shared task. 
Our learning curve experiments on varying size 
of labeled data shows that the steepest increase in 
F1 is achieved by 1/10th of CoNLL training data. 
Therefore, for training a base classifier as high-
performance as possible, while simulating the 
labeled data scarcity with a reasonably small 
amount of it, 4000 sentence are selected random-
ly from the total 39,832 training sentences as 
seed data (L). These sentences contain 71,400 
argument samples covering 38 semantic roles out 
of 52 roles present in the total training set. 
We use one unlabeled training set (U) for in-
domain and another for out-of-domain experi-
ments. The former is the remaining portion of 
CoNLL training data and contains 35,832 sen-
tences (698,567 samples). The out-of-domain set 
was extracted from Open American National 
Corpus 2  (OANC), a 14-million words multi-
genre corpus of American English. The whole 
corpus was preprocessed to prune some proble-
matic sentences. We also excluded the biomed 
section due to its large size to retain the domain 
balance of the data. Finally, 304,711 sentences 
with the length between 3 and 100 were parsed 
by the syntactic parser. Out of these, 35,832 sen-
tences were randomly selected for the experi-
ments reported here (832,795 samples). 
Two points are worth noting about the results 
in advance. First, we do not exclude the argu-
ment roles not present in seed data when evaluat-
ing the results. Second, we observed that our 
predicate-identification method is not reliable, 
since it is solely based on POS tags assigned by 
parser which is error-prone. Experiments with 
gold predicates confirmed this conclusion. 
5.2 The Effect of Balanced Selection 
Figures 2 and 3 depict the results of using unba-
lanced and balanced selection with WSJ and 
OANC data respectively. To be comparable with 
previous work (He and Gildea, 2006), the growth 
size (n) for unbalanced method is 7000 samples 
and for balanced method is 350 sentences, since 
each sentence roughly contains 20 samples. A 
probability threshold (t) of 0.70 is used for both 
cases. The F1 of base classifier, best-performed 
classifier, and final classifier are marked. 
When trained on WSJ unlabeled set, the ba-
lanced method outperforms the other in both 
WSJ (68.53 vs. 67.96) and Brown test sets (59.62 
vs. 58.95). A two-tail t-test based on different 
random selection of training data confirms the 
statistical significance of this improvement at 
p<=0.05 level. Also, the self-training trend is 
                                                 
2 http://www.americannationalcorpus.org/OANC 
94
  
more promising with both test sets. When trained 
on OANC, the F1 degrades with both methods as 
self-training progress. However, for both test 
sets, the best classifier is achieved by the ba-
lanced selection (68.26 vs. 68.15 and 59.41 vs. 
58.68). Moreover, balanced selection shows a 
more normal behavior, while the other degrades 
the performance sharply in the last iterations 
(due to a swift drop of recall). 
Consistent with previous work, with unba-
lanced selection, non-NULL-labeled unlabeled 
samples are selected only after the middle of the 
process. But, with the balanced method, selection 
is more evenly distributed over the roles.  
A comparison between the results on Brown 
test set with each of unlabeled sets shows that in-
domain data generalizes even better than out-of-
domain data (59.62 vs. 59.41 and also note the 
trend). One apparent reason is that the classifier 
cannot accurately label the out-of-domain unla-
beled data successively used for training. The 
lower quality of our out-of-domain data can be 
another reason for this behavior. Furthermore, 
the parser we used was trained on WSJ, so it ne-
gatively affected the OANC parses and conse-
quently its SRL results. 
5.3 The Effect of Preselection 
Figures 4 and 5 show the results of using pool 
with random and simplicity-based preselection 
with WSJ and OANC data respectively. The pool 
size (p) is 2000, and growth size (n) is 1000 sen-
tences. The probability threshold (t) used is 0.5. 
Comparing these figures with the previous 
figures shows that preselection improves the self-
training trend, so that more unlabeled data can 
still be useful. This observation was consistent 
with various random selection of training data.  
Between the two strategies, simplicity-based 
method outperforms the random method in both 
self-training trend and best classifier F1 (68.45 
vs. 68.25 and 59.77 vs. 59.3 with WSJ and 68.33 
vs. 68 with OANC), though the t-test shows that 
the F1 difference is not significant at p<=0.05. 
This improvement does not apply to the case of 
using OANC data when tested with Brown data 
 
Figure 2: Balanced (B) and Unbalanced (U) Selection 
with WSJ Unlabeled Data 
67.96 67.77
67.95
68.53 68.1
58.95
57.99
58.58
59.62
59.09
57
59
61
63
65
67
69
0 7000 14000 21000 28000 35000
F1
Number?of?Unlabeled?Sentences
WSJ?test?(U) WSJ?test?(B)
Brown?test?(U) Brown?test?(B)
Figure 3: Balanced (B) and Unbalanced (U) Selection 
with OANC Unlabeled Data 
68.15
65.75
67.95
68.26
67.14
58.68
55.64
58.58
59.41
58.41
55
57
59
61
63
65
67
69
0 7000 14000 21000 28000 35000
F1
Number?of?Unlabeled?Sentences
WSJ?test?(U) WSJ?test?(B)
Brown?test?(U) Brown?test?(B)
Figure 4: Random (R) and Simplicity (S) Pre-selection 
with WSJ Unlabeled Data 
68.25 68.14
67.95
68.45 68.44
59.3 58.5558.58
59.77 59.34
57
59
61
63
65
67
69
0 5000 10000 15000 20000 25000 30000 35000
F1
Number?of?Unlabeled?Sentences
WSJ?test?(R) WSJ?test?(S)
Brown?test?(R) Brown?test?(S)
Figure 5: Random (R) and Simplicity (S) Pre-selection 
with OANC Unlabeled Data 
68
67.39
67.95
68.33
67.45
59.38 59.17
58.58
59.27
59.08
57
59
61
63
65
67
69
0 5000 10000 15000 20000 25000 30000 35000
F1
Number?of?Unlabeled?Sentences
WSJ?test?(R) WSJ?test?(S)
Brown?test?(R) Brown?test?(S)
95
  
(59.27 vs. 59.38), where, however,  the differ-
ence is not statistically significant. The same 
conclusion to the section 5.2 can be made here. 
6 Conclusion and Future Work  
This work studies the application of self-training 
in learning semantic role labeling with the use of 
unlabeled data. We used a balancing method for 
selecting newly labeled examples for augmenting 
the training set in each iteration of the self-
training process. The idea was to reduce the ef-
fect of unbalanced distribution of semantic roles 
in training data. We also used a pool and ex-
amined two preselection methods for loading 
unlabeled data into it.  
These methods showed improvement in both 
classifier performance and self-training trend. 
However, using out-of-domain unlabeled data for 
increasing the domain generalization ability of 
the system was not more useful than using in-
domain data. Among possible reasons are the 
low quality of the used data and the poor parses 
of the out-of-domain data. 
Another major factor that may affect the self-
training behavior here is the poor performance of 
the base classifier compared to the state-of-the-
art (see Table 2), which exploits more compli-
cated SRL architecture. Due to high computa-
tional cost of self-training approach, bootstrap-
ping experiments with such complex SRL ap-
proaches are difficult and time-consuming. 
Moreover, parameter tuning process shows 
that other parameters such as pool-size, growth 
number and probability threshold are very effec-
tive. Therefore, more comprehensive parameter 
tuning experiments than what was done here is 
required and may yield better results. 
We are currently planning to port this setting 
to co-training, another bootstrapping algorithm. 
One direction for future work can be adapting the 
architecture of the SRL system to better match 
with the bootstrapping process. Another direction 
can be adapting bootstrapping parameters to fit 
the semantic role labeling complexity. 
References 
Abney, S. 2008. Semisupervised Learning for Compu-
tational Linguistics. Chapman and Hall, London. 
Baker, F., Fillmore, C. and Lowe, J. 1998. The Berke-
ley FrameNet project. In Proceedings of COLING-
ACL, pages 86-90. 
Charniak, E. and Johnson, M. 2005. Coarse-to-fine n-
best parsing and MaxEnt discriminative reranking. 
In Proceedings of the 43rd Annual Meeting of the 
ACL, pages 173-180. 
Carreras, X. and Marquez, L. 2005. Introduction to 
the CoNLL-2005 shared task: Semantic role labe-
ling. In Proceedings of the 9th Conference on Nat-
ural Language Learning (CoNLL), pages. 152-164. 
Clark S., Curran, R. J. and Osborne M. 2003. Boot-
strapping POS taggers using Unlabeled Data. In 
Proceedings of the 7th Conference on Natural 
Language Learning At HLT-NAACL 2003, pages 
49-55. 
Gildea, D. and Jurafsky, D. 2002. Automatic labeling 
of semantic roles. CL, 28(3):245-288. 
He, S. and Gildea, H. 2006. Self-training and Co-
training for Semantic Role Labeling: Primary Re-
port. TR 891, University of Colorado at Boulder 
Kingsbury, P. and Palmer, M. 2002. From Treebank 
to PropBank. In Proceedings of the 3rd Interna-
tional Conference on Language Resources and 
Evaluation (LREC-2002). 
Lee, J., Song, Y. and Rim, H. 2007. Investigation of 
Weakly Supervised Learning for Semantic Role 
Labeling. In Proceedings of the Sixth international 
Conference on Advanced Language Processing 
and Web information Technology (ALPIT 2007), 
pages 165-170. 
McClosky, D., Charniak, E., and Johnson, M. 2006. 
Effective self-training for parsing. In Proceedings 
of the Main Conference on Human Language 
Technology Conference of the North American 
Chapter of the ACL, pages 152-159. 
Ng, V. and Cardie, C. 2003. Weakly supervised natu-
ral language learning without redundant views. In 
Proceedings of the 2003 Conference of the North 
American Chapter of the ACL on Human Lan-
guage Technology, pages 94-101. 
Punyakanok, V., Roth, D. and Yi, W. 2008. The Im-
portance of Syntactic Parsing and Inference in Se-
mantic Role Labeling. CL, 34(2):257-287. 
Surdeanu, M., Harabagiu, S., Williams, J. and Aar-
seth, P. 2003. Using predicate argument structures 
for information extraction. In Proceedings of the 
41st Annual Meeting of the ACL, pages 8-15. 
Surdeanu, M., Johansson, R., Meyers, A., Marquez, 
L. and Nivre, J. 2008. The CoNLL 2008 shared 
task on joint parsing of syntactic and semantic de-
pendencies. In Proceedings of the 12th Conference 
on Natural Language Learning (CoNLL), pages 
159-177. 
Yarowsky, E. 1995. Unsupervised Word Sense Dis-
ambiguation Rivaling Supervised Methods. In pro-
ceeding of the 33rd Annual Meeting of ACL, pages 
189-196. 
96
Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*SEM 2014), pages 87?92,
Dublin, Ireland, August 23-24 2014.
Semantic Role Labelling with minimal resources:
Experiments with French
Rasoul Kaljahi
??
, Jennifer Foster
?
, Johann Roturier
?
?
NCLT, School of Computing, Dublin City University, Ireland
{rkaljahi, jfoster}@computing.dcu.ie
?
Symantec Research Labs, Dublin, Ireland
johann roturier@symantec.com
Abstract
This paper describes a series of French se-
mantic role labelling experiments which
show that a small set of manually anno-
tated training data is superior to a much
larger set containing semantic role labels
which have been projected from a source
language via word alignment. Using uni-
versal part-of-speech tags and dependen-
cies makes little difference over the orig-
inal fine-grained tagset and dependency
scheme. Moreover, there seems to be no
improvement gained from projecting se-
mantic roles between direct translations
than between indirect translations.
1 Introduction
Semantic role labelling (SRL) (Gildea and Juraf-
sky, 2002) is the task of identifying the predicates
in a sentence, their semantic arguments and the
roles these arguments take. The last decade has
seen considerable attention paid to statistical SRL,
thanks to the existence of two major hand-crafted
resources for English, namely, FrameNet (Baker
et al., 1998) and PropBank (Palmer et al., 2005).
Apart from English, only a few languages have
SRL resources and these resources tend to be of
limited size compared to the English datasets.
French is one of those languages which suffer
from a scarcity of hand-crafted SRL resources.
The only available gold-standard resource is a
small set of 1000 sentences taken from Europarl
(Koehn, 2005) and manually annotated with Prop-
bank verb predicates (van der Plas et al., 2010b).
This dataset is then used by van der Plas et al.
(2011) to evaluate their approach to projecting the
SRLs of English sentences to their translations
This work is licensed under a Creative Commons Attribution
4.0 International Licence. Page numbers and proceedings
footer are added by the organisers. Licence details: http:
//creativecommons.org/licenses/by/4.0/
in French. They additionally build a large, ?ar-
tificial? or automatically labelled dataset of ap-
proximately 1M Europarl sentences by projecting
the SRLs from English sentences to their French
translations and use it for training an SRL system.
We build on the work of van der Plas et al.
(2010b) by answering the following questions: 1)
How much artificial data is needed to train an
SRL system? 2) Is it better to use direct trans-
lations than indirect translations, i.e. is it better
to use for projection a source-target pair where
the source represents the original sentence and the
target represents its direct translation as opposed
to a source-target pair where the source and tar-
get are both translations of an original sentence
in a third language? 3) Is it better to use coarse-
grained syntactic information (in the form of uni-
versal part-of-speech tags and universal syntactic
dependencies) than to use fine-grained syntactic
information? We find that SRL performance lev-
els off after only 5K training sentences obtained
via projection and that direct translations are no
more useful than indirect translations. We also
find that it makes very little difference to French
SRL performance whether we use universal part-
of-speech tags and syntactic dependencies or more
fine-grained tags and dependencies.
The surprising result that SRL performance lev-
els off after just 5K training sentences leads us
to directly compare the small hand-crafted set of
1K sentences to the larger artificial training set.
We use 5-fold cross-validation on the small dataset
and find that the SRL performance is substantially
higher (>10 F
1
in identification and classification)
when the hand-crafted annotations are used.
2 Related Work
There has been relatively few works in French
SRL. Lorenzo and Cerisara (2012) propose a clus-
tering approach for verb predicate and argument
labelling (but not identification). They choose
87
VerbNet style roles (Schuler, 2006) and manu-
ally annotate sentences with them for evaluation,
achieving an F
1
of 78.5.
Gardent and Cerisara (2010) propose a method
for semi-automatically annotating the French de-
pendency treebank (Candito et al., 2010) with
Propbank core roles (no adjuncts). They first
manually augment TreeLex (Kup?s?c and Abeill?e,
2008), a syntactic lexicon of French, with seman-
tic roles of syntactic arguments of verbs (i.e. verb
subcategorization). They then project this anno-
tation to verb instances in the dependency trees.
They evaluate their approach by performing error
analysis on a small sample and suggest directions
for improvement. The annotation work is however
at its preliminary stage and no data is published.
As mentioned earlier, van der Plas et al. (2011)
use word alignments to project the SRLs of the
English side of EuroParl to its French side result-
ing in a large artificial dataset. This idea is based
on the Direct Semantic Transfer hypothesis which
assumes that a semantic relationship between two
words in a sentence can be transferred to any
two words in the translation which are aligned to
these source-side words. Evaluation on their 1K
manually-annotated dataset shows that a syntactic-
semantic dependency parser trained on this artifi-
cial data set performs significantly better than di-
rectly projecting the labelling from its English side
? a promising result because, in a real-world sce-
nario, the English translations of the French data
to be annotated do not necessarily exist.
Pad?o and Lapata (2009) also make use of word
alignments to project SRLs from English to Ger-
man. The word alignments are used to compute
the semantic similarity between syntactic con-
stituents. In order to determine the extent of se-
mantic correspondence between English and Ger-
man, they manually annotate a set of parallel sen-
tences and find that about 72% of the frames and
92% of the argument roles exist in both sides, ig-
noring their lexical correspondence.
3 Datasets, SRL System and Evaluation
We use the two datasets described in (van der Plas
et al., 2011) and the delivery report of the Clas-
sic project (van der Plas et al., 2010a). These
are the gold standard set of 1K sentences which
was annotated by manually identifying each verb
predicate, finding its equivalent English frameset
in PropBank and identifying and labelling its ar-
guments based on the description of the frame-
set (henceforth known as Classic1K), and the syn-
thetic dataset consisting of more than 980K sen-
tences (henceforth known as Classic980K), which
was created by word aligning an English-French
parallel corpus (Europarl) using GIZA++ (Och
and Ney, 2003) and projecting the French SRLs
from the English SRLs via the word alignments.
The joint syntactic-semantic parser described in
(Titov et al., 2009) was used to produce the En-
glish SRLs and the dependency parses of the
French side were produced using the ISBN parser
described in (Titov and Henderson, 2007).
We use LTH (Bj?orkelund et al., 2009), a
dependency-based SRL system, in all of our ex-
periments. This system was among the best-
performing systems in the CoNLL 2009 shared
task (Haji?c et al., 2009) and is straightforward to
use. It comes with a set of features tuned for each
shared task language (English, German, Japanese,
Spanish, Catalan, Czech, Chinese). We compared
the performance of the English and Spanish fea-
ture sets on French and chose the former due to its
higher performance (by 1 F
1
point).
To evaluate SRL performance, we use the
CoNLL 2009 shared task scoring script
1
, which
assumes a semantic dependency between the argu-
ment and predicate and the predicate and a dummy
root node and then calculates the precision (P), re-
call (R) and F
1
of identification of these dependen-
cies and classification (labelling) of them.
4 Experiments
4.1 Learning Curve
The ultimate goal of SRL projection is to build a
training set which partially compensates for the
lack of hand-crafted resources. van der Plas et
al. (2011) report encouraging results showing that
training on their projected data is beneficial over
directly obtaining the annotation via projection
which is not always possible. Although the quality
of such automatically-generated training data may
not be comparable to the manual one, the possi-
bility of building much bigger data sets may pro-
vide some advantages. Our first experiment inves-
tigates the extent to which the size of the synthetic
training set can improve performance.
We randomly select 100K sentences from Clas-
sic980K, shuffle them and split them into 20 sub-
1
https://ufal.mff.cuni.cz/
conll2009-st/eval09.pl
88
0
1000
0
2000
0
3000
0
4000
0
5000
0
6000
0
7000
0
8000
0
9000
0
1000
0020
30
40
50
60
70
80
PrecisionRecallF1
Figure 1: Learning curve with 100K training data
of projected annotations
0
1000
0
2000
0
3000
0
4000
0
5000
0
6000
0
7000
0
8000
0
9000
0
1000
0020
30
40
50
60
70
80
PrecisionRecallF1
Figure 2: Learning curve with 100K training data
of projected annotations on only direct translations
sets of 5K sentences. We then split the first 5K into
10 sets of 500 sentences. We train SRL models
on the resulting 29 subsets using LTH. The per-
formance of the models evaluated on Classic1K
is presented in Fig. 1. Surprisingly, the best F
1
(58.7) is achieved by only 4K sentences, and af-
ter that the recall (and consequently F
1
) tends to
drop though precision shows a positive trend, sug-
gesting that the additional sentences bring little in-
formation. The large gap between precision and
recall is also interesting, showing that the projec-
tions do not have wide semantic role coverage.
2
4.2 Direct Translations
Each sentence in Europarl was written in one of
the official languages of the European Parliament
and translated to all of the other languages. There-
fore both sides of a parallel sentence pair can be in-
direct translations of each other. van der Plas et al.
(2011) suggest that translation divergence may af-
2
Note that our results are not directly comparable with
(van der Plas et al., 2011) because they split Classic1K into
development and test sets, while we use the whole set for
testing. We do not have access to their split.
fect automatic projection of semantic roles. They
therefore select for their experiments only those
276K sentences from the 980K which are direct
translations between English and French. Moti-
vated by this idea, we replicate the learning curve
in Fig. 1 with another set of 100K sentences ran-
domly selected from only the direct translations.
The curve is shown in Fig. 2. There is no no-
ticeable difference between this and the graph in
Fig. 1, suggesting that the projections obtained via
direct translations are not of higher quality.
4.3 Impact of Syntactic Annotation
Being a dependency-based semantic role labeller,
LTH employs a large set of features based on syn-
tactic dependency structure. This inspires us to
compare the impact of different types of syntactic
annotations on the performance of this system.
Based on the observations from the previous
sections, we choose two different sizes of training
sets. The first set contains the first 5K sentences
from the original 100K, as we saw that more than
this amount tends to diminish performance. The
second set contains the first 50K from the original
100K, the purpose of which is to check if changing
the parses affects the usefulness of adding more
data. We will call these data sets Classic5K and
Classic50K respectively.
Petrov et al. (2012) create a set of 12 univer-
sal part-of-speech (POS) tags which should in the-
ory be applicable to any natural language. It is
interesting to know whether these POS tags are
more useful for SRL than the original set of the 29
more fine-grained POS tags used in French Tree-
bank which we have used so far. To this end, we
convert the original POS tags of the data to uni-
versal POS tags and retrain and evaluate the SRL
models. The results are given in the second row of
Table 1 (OrgDep+UniPOS). The first row of the
table (Original) shows the performance using
the original annotation. Even though the scores
increase in most cases ? due mostly to a rise in
recall ? the changes are small. It is worth noting
that identification seems to benefit more from the
universal POS tags.
Similar to universal POS tags, McDonald et al.
(2013) introduce a set of 40 universal dependency
types which generalize over the dependency struc-
ture specific to several languages. For French, they
provide a new treebank, called uni-dep-tb,
manually annotating 16,422 sentences from vari-
89
5K 50K
Identification Classification Identification Classification
P R F
1
P R F
1
P R F
1
P R F
1
Original 85.95 59.64 70.42 71.34 49.50 58.45 86.67 58.07 69.54 72.44 48.54 58.13
OrgDep+UniPOS 86.71 60.46 71.24 71.11 49.58 58.43 86.82 58.71 70.05 72.30 48.90 58.34
StdUniDep+UniPOS 86.14 59.76 70.57 70.60 48.98 57.84 86.38 58.90 70.04 71.61 48.83 58.07
CHUniDep+UniPOS 85.98 59.21 70.13 70.66 48.66 57.63 86.47 58.26 69.61 71.74 48.34 57.76
Table 1: SRL performance using different syntactic parses with Classic 5K and 50K training sets
ous domains. We now explore the utility of this
new dependency scheme in SRL.
The French universal dependency treebank
comes in two versions, the first using the stan-
dard dependency structure based on basic Stanford
dependencies (de Marneffe and Manning, 2008)
where content words are the heads except in cop-
ula and adposition constructions, and the second
which treats content words as the heads for all
constructions without exemption. We use both
schemes in order to verify their effect on SRL.
In order to obtain universal dependencies for
our data, we train parsing models with Malt-
Parser (Nivre et al., 2006) using the entire
uni-dep-tb.
3
We then parse our data us-
ing these MaltParser models. The input POS
tags to the parser are the universal POS tags
used in OrgDep+UniPOS. We train and evalu-
ate new SRL models on these data. The results
are shown in the third and fourth rows of Table
1. StdUniDept+UniPOS is the setting using
standard dependencies and CHUDep+UPOS using
content-head dependencies.
According to the third and fourth rows in Table
1, content-head dependencies are slightly less use-
ful than standard dependencies. The general ef-
fect of universal dependencies can be compared to
those of original ones by comparing these results
to OrgDep+UniPOS - the use of universal de-
pendencies appears to have only a modest (nega-
tive) effect. However, we must be careful of draw-
ing too many conclusions because in addition to
the difference in dependency schemes, the training
data used to train the parsers as well as the parsers
themselves are different.
Overall, we observe that the universal annota-
tions can be reliably used when the fine-grained
annotation is not available. This can be especially
3
Based on our preliminary experiments on the pars-
ing performance, we use LIBSVM as learning algorithm,
nivreeager as parsing algorithm for the standard depen-
dency models and stackproj for the content-head ones.
Identification Classification
P R F
1
P R F
1
1K 83.76 83.00 83.37 68.40 67.78 68.09
5K 85.94 59.62 70.39 71.30 49.47 58.40
1K+5K 85.74 66.53 74.92 71.48 55.46 62.46
SelfT 83.82 83.66 83.73 67.91 67.79 67.85
Table 2: Average scores of 5-fold cross-validation
with Classic 1K (1K), 5K (5K), 1K plus 5K
(1K+5K) and self-training with 1K seed and 5K
unlabeled data (SelfT)
useful for languages which lack such resources
and require techniques such as cross-lingual trans-
fer to replace them.
4.4 Quality vs. Quantity
In Section 4.1, we saw that adding more data an-
notated through projection did not elevate SRL
performance. In other words, the same perfor-
mance was achieved using only a small amount
of data. This is contrary to the motivation for cre-
ating synthetic training data, especially when the
hand-annotated data already exist, albeit in a small
size. In this section, we compare the performance
of SRL models trained using manually-annotated
data with SRL models trained using 5K of artifi-
cial or synthetic training data. We use the original
syntactic annotations for both datasets.
To this end, we carry out a 5-fold cross-
validation on Classic1K. We then evaluate the
Classic5K model, on each of the 5 test sets gen-
erated in the cross-validation. The average scores
of the two evaluation setups are compared. The
results are shown in Table 2.
While the 5K model achieves higher precision,
its recall is far lower resulting in dramatically
lower F
1
. This high precision and low recall is due
to the low confidence of the model trained on pro-
jected data suggesting that a considerable amount
of information is not transferred during the projec-
tion. This issue can be attributed to the fact that the
90
Classic projection uses intersection of alignments
in the two translation directions, which is the most
restrictive setting and leaves many source predi-
cates and arguments unaligned.
We next add the Classic5K projected data to
the manually annotated training data in each fold
of another cross-validation setting and evaluate
the resulting models on the same test sets. The
results are reported in the third row of the Ta-
ble 2 (1K+5K). As can be seen, the low qual-
ity of the projected data significantly degrades the
performance compared to when only manually-
annotated data are used for training.
Finally, based on the observation that the qual-
ity of labelling using manually annotated data is
higher than using the automatically projected data,
we replicate 1K+5K with the 5K data labelled us-
ing the model trained on the training subset of 1K
at each cross-validation fold. In other words, we
perform a one-round self-training with this model.
The performance of the resulting model evaluated
in the same cross-validation setting is given in the
last row of Table 2 (SelfT).
As expected, the labelling obtained by mod-
els trained on manual annotation are more useful
than the projected ones when used for training new
models. It is worth noting that, unlike with the
1K+5K setting, the balance between precision and
recall follows that of the 1K model. In addition,
some of the scores are the highest among all re-
sults, although the differences are not significant.
4.5 How little is too little?
In the previous section we saw that using a manu-
ally annotated dataset with as few as 800 sentences
resulted in significantly better SRL performance
than using projected annotation with as many as
5K sentences. This unfortunately indicates the
need for human labour in creating such resources.
It is interesting however to know the lower bound
of this requirement. To this end, we reverse our
cross-validation setting and train on 200 and test
on 800 sentences. We then compare to the 5K
models evaluated on the same 800 sentence sets
at each fold. The results are presented in Table 3.
Even with only 200 manually annotated sentences,
the performance is considerably higher than with
5K sentences of projected annotations. However,
as one might expect, compared to when 800 sen-
tences are used for training, this small model per-
forms significantly worse.
Identification Classification
P R F
1
P R F
1
1K 82.34 79.61 80.95 64.14 62.01 63.06
5K 85.95 59.64 70.42 71.34 49.50 58.45
Table 3: Average scores of 5-fold cross-validation
with Classic 1K (1K) and 5K (5K) using 200 sen-
tences for training and 800 for testing at each fold
5 Conclusion
We have explored the projection-based approach
to SRL by carrying out experiments with a large
set of French semantic role labels which have been
automatically transferred from English. We have
found that increasing the number of these artificial
projections that are used in training an SRL sys-
tem does not improve performance as might have
been expected when creating such a resource. In-
stead it is better to train directly on what little gold
standard data is available, even if this dataset con-
tains only 200 sentences. We suspect that the dis-
appointing performance of the projected dataset
originates in the restrictive way the word align-
ments have been extracted. Only those alignments
that are in the intersection of the English-French
and French-English word alignment sets are re-
tained resulting in low SRL recall. Recent prelim-
inary experiments show that less restrictive align-
ment extraction strategies including extracting the
union of the two sets or source-to-target align-
ments lead to a better recall and consequently F
1
both when used for direct projection to the test
data or for creating the training data and then ap-
plying the resulting model to the test data.
We have compared the use of universal POS
tags and dependency labels to the original, more
fine-grained sets and shown that there is only a
little difference. However, it remains to be seen
whether this finding holds for other languages or
whether it will still hold for French when SRL per-
formance can be improved. It might also be in-
teresting to explore the combination of universal
dependencies with fine-grained POS tags.
Acknowledgments
This research has been supported by the Irish
Research Council Enterprise Partnership Scheme
(EPSPG/2011/102) and the computing infrastruc-
ture of the CNGL at DCU. We thank Lonneke van
der Plas for providing us the Classic data. We also
thank the reviewers for their helpful comments.
91
References
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The berkeley framenet project. In Proceed-
ings of the 36th ACL, pages 86?90.
Anders Bj?orkelund, Love Hafdell, and Pierre Nugues.
2009. Multilingual semantic role labeling. In Pro-
ceedings of the Thirteenth Conference on Compu-
tational Natural Language Learning: Shared Task,
pages 43?48.
Marie Candito, Benot Crabb?e, and Pascal Denis. 2010.
Statistical french dependency parsing: treebank
conversion and first results. In Proceedings of
LREC?2010.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The stanford typed dependencies
representation. In Proceedings of the COLING
Workshop on Cross-Framework and Cross-Domain
Parser Evaluation.
Claire Gardent and Christophe Cerisara. 2010. Semi-
Automatic Propbanking for French. In TLT9 -
The Ninth International Workshop on Treebanks and
Linguistic Theories.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational Linguis-
tics, 28(3):245?288.
Jan Haji?c, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Ant`onia Mart??, Llu??s
M`arquez, Adam Meyers, Joakim Nivre, Sebastian
Pad?o, Jan
?
St?ep?anek, Pavel Stra?n?ak, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The conll-2009
shared task: Syntactic and semantic dependencies
in multiple languages. In Proceedings of the Thir-
teenth Conference on Computational Natural Lan-
guage Learning: Shared Task, pages 1?18.
Philipp Koehn. 2005. Europarl: A Parallel Corpus
for Statistical Machine Translation. In Conference
Proceedings: the tenth Machine Translation Sum-
mit, pages 79?86.
Anna Kup?s?c and Anne Abeill?e. 2008. Growing
treelex. In Proceedings of the 9th International Con-
ference on Computational Linguistics and Intelli-
gent Text Processing, CICLing?08, pages 28?39.
Alejandra Lorenzo and Christophe Cerisara. 2012.
Unsupervised frame based semantic role induction:
application to french and english. In Proceedings of
the ACL 2012 Joint Workshop on Statistical Parsing
and Semantic Processing of Morphologically Rich
Languages, pages 30?35.
Ryan McDonald, Joakim Nivre, Yvonne Quirmbach-
Brundage, Yoav Goldberg, Dipanjan Das, Kuz-
man Ganchev, Keith Hall, Slav Petrov, Hao
Zhang, Oscar T?ackstr?om, Claudia Bedini, N?uria
Bertomeu Castell?o, and Jungmee Lee. 2013. Uni-
versal dependency annotation for multilingual pars-
ing. In Proceedings of the 51st Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 2: Short Papers), pages 92?97.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006.
Maltparser: A data-driven parser-generator for de-
pendency parsing. In In Proceedings of LREC.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Sebastian Pad?o and Mirella Lapata. 2009. Cross-
lingual annotation projection of semantic roles. J.
Artif. Int. Res., 36(1):307?340.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated cor-
pus of semantic roles. Computational Linguistics,
31(1):71?106.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012.
A universal part-of-speech tagset. In Proceedings of
LREC, May.
Karin Kipper Schuler. 2006. VerbNet: A Broad-
Coverage, Comprehensive Verb Lexicon. Ph.D. the-
sis, University of Pennsylvania.
Ivan Titov and James Henderson. 2007. A latent vari-
able model for generative dependency parsing. In
Proceedings of the 10th International Conference on
Parsing Technologies, pages 144?155.
Ivan Titov, James Henderson, Paola Merlo, and
Gabriele Musillo. 2009. Online projectivisation for
synchronous parsing of semantic and syntactic de-
pendencies. In In Proceedings of the Internation
Joint Conference on Artificial Intelligence (IJCAI),
pages 1562?1567.
Lonneke van der Plas, James Henderson, and Paola
Merlo. 2010a. D6. 2: Semantic role annotation of a
french-english corpus.
Lonneke van der Plas, Tanja Samard?zi?c, and Paola
Merlo. 2010b. Cross-lingual validity of propbank
in the manual annotation of french. In Proceedings
of the Fourth Linguistic Annotation Workshop, LAW
IV ?10, pages 113?117.
Lonneke van der Plas, Paola Merlo, and James Hen-
derson. 2011. Scaling up automatic cross-lingual
semantic role annotation. In Proceedings of the 49th
Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 299?304.
92
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 392?397,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
DCU-Symantec at the WMT 2013 Quality Estimation Shared Task
Raphael Rubino??, Joachim Wagner??, Jennifer Foster?,
Johann Roturier? Rasoul Samad Zadeh Kaljahi?? and Fred Hollowood?
?NCLT, School of Computing, Dublin City University, Ireland
?Center for Next Generation Localisation, Dublin, Ireland
?Symantec Research Labs, Dublin, Ireland
?{rrubino, jwagner, jfoster}@computing.dcu.ie
?{johann roturier, fhollowood}@symantec.com
Abstract
We describe the two systems submit-
ted by the DCU-Symantec team to Task
1.1. of the WMT 2013 Shared Task on
Quality Estimation for Machine Transla-
tion. Task 1.1 involve estimating post-
editing effort for English-Spanish trans-
lation pairs in the news domain. The
two systems use a wide variety of fea-
tures, of which the most effective are the
word-alignment, n-gram frequency, lan-
guage model, POS-tag-based and pseudo-
references ones. Both systems perform at
a similarly high level in the two tasks of
scoring and ranking translations, although
there is some evidence that the systems are
over-fitting to the training data.
1 Introduction
The WMT 2013 Quality Estimation Shared Task
involve both sentence-level and word-level qual-
ity estimation (QE). The sentence-level task con-
sist of three subtasks: scoring and ranking transla-
tions with regard to post-editing effort (Task 1.1),
selecting among several translations produced by
multiple MT systems for the same source sentence
(Task 1.2), and predicting post-editing time (Task
1.3). The DCU-Symantec team enter two systems
to Task 1.1. Given a set of source English news
sentences and their Spanish translations, the goals
are to predict the HTER score of each translation
and to produce a ranking based on HTER for the
set of translations. A set of 2,254 sentence pairs
are provided for training.
On the ranking task, our system DCU-SYMC
alltypes is second placed out of thirteen sys-
tems and our system DCU-SYMC combine is
ranked fifth, according to the Delta Average met-
ric. According to the Spearman rank correlation,
our systems are the joint-highest systems. In the
scoring task, the DCU-SYMC alltypes system
is placed sixth out of seventeen systems accord-
ing to Mean Absolute Error (MAE) and third ac-
cording to Root Mean Squared Error (RMSE). The
DCU-SYMC combine system is placed fifth ac-
cording to MAE and second according to RMSE.
In this system description paper, we describe the
features, the learning methods used, the results for
the two submitted systems and some other systems
we experiment with.
2 Features
Our starting point for the WMT13 QE shared task
was the feature set used in the system we submit-
ted to the WMT12 QE task (Rubino et al, 2012).
This feature set, comprising 308 features in to-
tal, extended the 17 baseline features provided by
the task organisers to include 6 additional sur-
face features, 6 additional language model fea-
tures, 17 additional features derived from the
MT system components and the n-best lists, 138
features obtained by part-of-speech tagging and
parsing the source sentences and 95 obtained by
part-of-speech tagging the target sentences, 21
topic model features, 2 features produced by a
grammar checker1 and 6 pseudo-source (or back-
translation) features.
We made the following modifications to this
2012 feature set:
? The pseudo-source (or back-translation) fea-
tures were removed, as they did not con-
tribute useful information to our system last
year.
? The language model and n-gram frequency
feature sets were extended in order to cover
1 to 5 gram sequences, as well as source and
target ratios for these feature values.
? The word-alignment feature set was also
extended by considering several thresholds
1http://www.languagetool.org/
392
when counting the number of target words
aligned with source words.
? We extracted 8 additional features from the
decoder log file, including the number of dis-
carded hypotheses, the total number of trans-
lation options and the number of nodes in the
decoding graph.
? The set of topic model features was reduced
in order to keep only those that were shown
to be effective on three quality estimation
datasets (the details can be found in (Rubino
et al (to appear), 2013)). These features en-
code the difference between source and target
topic distributions according to several dis-
tance/divergence metrics.
? Following Soricut et al (2012), we employed
pseudo-reference features. The source sen-
tences were translated with three different
MT systems: an in-house phrase-based SMT
system built using Moses (Koehn et al,
2007) and trained on the parallel data pro-
vided by the organisers, the rule-based sys-
tem Systran2 and the online, publicly avail-
able, Bing Translator3. The obtained trans-
lations are compared to the target sentences
using sentence-level BLEU (Papineni et al,
2002), TER (Snover et al, 2006) and the Lev-
enshtein distance (Levenshtein, 1966).
? Also following Soricut et al (2012), one-
to-one word-alignments, with and without
Part-Of-Speech (POS) agreement, were in-
cluded as features. Using the alignment in-
formation provided by the decoder, we POS
tagged the source and target sentences with
TreeTagger (Schmidt, 1994) and the publicly
available pre-trained models for English and
Spanish. We mapped the tagsets of both lan-
guages by simplifying the initial tags and ob-
tain a reduced set of 8 tags. We applied that
simplification on the tagged sentences before
checking for POS agreement.
3 Machine Learning
In this section, we describe the learning algo-
rithms and feature selection used in our experi-
ments, leading to the two submitted systems for
the shared task.
2Systran Enterprise Server version 6
3http://www.bing.com/translator
3.1 Primary Learning Method
To estimate the post-editing effort of translated
sentences, we rely on regression models built us-
ing the Support Vector Machine (SVM) algorithm
for regression -SVR, implemented in the LIB-
SVM toolkit (Chang and Lin, 2011). To build
our final regression models, we optimise SVM
hyper-parameters (C, ? and ) using a grid-search
method with 5-fold cross-validation for each pa-
rameter triplet. The parameters leading to the best
MAE, RMSE and Pearson?s correlation coefficient
(r) are kept to build the model.
3.2 Feature Selection on Feature Types
In order to reduce the feature and obtain more
compact models, we apply feature selection on
each of our 15 feature types. Examples of feature
types are language model features or topic model
features. For each feature type, we apply a feature
subset evaluation method based on the wrapper
paradigm and using the best-first search algorithm
to explore the feature space. The M5P (Wang
and Witten, 1997) regression tree algorithm im-
plemented in the Weka toolkit (Hall et al, 2009)
is used with default parameters to train and eval-
uate a regression model for each feature subset
obtained with best-first search. A 10-fold cross-
validation is performed for each subset and we
keep the features leading to the best RMSE. We
use M5P regression trees instead of -SVR be-
cause grid-search with the latter is too computa-
tionally expensive to be applied so many times.
Using feature selection in this way, we obtain 15
reduced feature sets that we combine to form the
DCU-SYMC alltypes system, containing 102
features detailed in Table 1.
3.3 Feature Binarisation
In order to aid the SVM learner, we also experi-
ment with binarising our feature set, i.e. convert-
ing our features with various feature value ranges
into features whose values are either 1 or 0. Again,
we employ regression tree learning. We train
regression trees with M5P and M5P-R4 (imple-
mented in the Weka toolkit) and create a binary
feature for each regression rule found in the trees
(ignoring the leaf nodes). For example, a binary
feature indicating whether the Bing TER score is
less than or equal to 55.685 is derived from the
4We experiment with J48 decision trees as well, but this
method did not outperform regression tree methods.
393
Backward LM
Source 1-gram perplexity.
Source & target 1-grams perplexity ratio.
Source & target 3-grams and 4-gram perplexity ratio.
Target Syntax
Frequency of tags: ADV, FS, DM, VLinf, VMinf, semicolon, VLger, NC, PDEL, VEfin, CC, CCNEG, PPx, ART, SYM,
CODE, PREP, SE and number of ambiguous tags
Frequency of least frequent POS 3-gram observed in a corpus.
Frequency of least frequent POS 4-gram and 6-gram with sentence padding (start and end of sentence tags) observed in a
corpus.
Source Syntax
Features from three probabilistic parsers. (Rubino et al, 2012).
Frequency of least frequent POS 2-gram, 4-gram and 9-gram with sentence padding observed in a corpus.
Number of analyses found and number of words, using a Lexical Functional Grammar of English as described in Rubino
et al (2012).
LM
Source unigram perplexity.
Target 3-gram and 4-gram perplexity with sentence padding.
Source & target 1-gram and 5-gram perplexity ratio.
Source & target unigram log-probability.
Decoder
Component scores during decoding.
Number of phrases in the best translation.
Number of translation options.
N -gram Frequency
Target 2-gram in second and third frequency quartiles.
Target 3-gram and 5-gram in low frequency quartiles.
Number of target 1-gram seen in a corpus.
Source & target 1-grams in highest and second highest frequency quartile.
One-to-One Word-Alignment
Count of O2O word alignment, weighted by target sentence length.
Count of O2O word alignment with POS agreement, weighted by count of O2O, by source length, by target length.
Pseudo-Reference
Moses translation TER score.
Bing translation number of words and TER score.
Systran sBLEU, number of substitutions and TER score.
Surface
Source number of punctuation marks and average words occurrence in source sentence.
Target number of punctuation marks, uppercased letters and binary value if the last character of the sentence is a punctuation
mark.
Ratio of source and target sentence lengths, average word length and number of punctuation marks over sentence lengths.
Topic Model
Cosine distance between source and target topic distributions.
Jensen-Shannon divergence between source and target topic distributions.
Word Alignment
Averaged number of source words aligned per target words with p(s|t) thresholds: 1.0, 0.75, 0.5, 0.25, 0.01
Averaged number of source words aligned per target words with p(s|t) = 0.01 weighted by target words frequency
Averaged number of target words aligned per source word with p(t|s) = 0.01 weighted by source words frequency
Ratio of source and target averaged aligned words with thresholds: 1.0 and 0.1, and with threshold: 0.75, 0.5, 0.25 weighted
by words frequency
Table 1: Features selected with the wrapper approach using best-first search and M5P. These features are
included in the submitted system alltypes.
394
Feature to which threshold t is applied t (?)
Target 1-gram backward LM log-prob. ?35.973
Target 3-gram backward LM perplexity 7144.99
Probabilistic parsing feature 3.756
Probabilistic parsing feature 57.5
Frequency of least frequent POS 6-gram 0.5
Source 3-gram LM log-prob. 65.286
Source 4-gram LM perplexity with padding 306.362
Target 2-gram LM perplexity 176.431
Target 4-gram LM perplexity 426.023
Target 4-gram LM perplexity with padding 341.801
Target 5-gram LM perplexity 112.908
Ratio src&trg 5-gram LM log-prob. 1.186
MT system component score ?50
MT system component score ?0.801
Source 2-gram frequency in low quartile 0.146
Ratio src&trg 2-gram in high freq. quartile 0.818
Ratio src&trg 3-gram in high freq. quartile 0.482
O2O word alignment 15.5
Pseudo-ref. Moses Levenshtein 19
Pseudo-ref. Moses TER 21.286
Pseudo-ref. Bing TER 16.905
Pseudo-ref. Bing TER 23.431
Pseudo-ref. Bing TER 37.394
Pseudo-ref. Bing TER 55.685
Pseudo-ref. Systran sBLEU 0.334
Pseudo-ref. Systran TER 36.399
Source average word length 4.298
Target uppercased/lowercased letters ratio 0.011
Ratio src&trg average word length 1.051
Source word align., p(s|t) > 0.75 11.374
Source word align., p(s|t) > 0.1 485.062
Source word align., p(s|t) > 0.75 weighted 0.002
Target word align., p(t|s) > 0.01 weighted 0.019
Word align. ratio p > 0.25 weighted 1.32
Table 2: Features selected with the M5P-R M50
binarisation approach. For each feature, the cor-
responding rule indicates the binary feature value.
These features are included in the submitted sys-
tem combine in addition to the features presented
in Table 1.
regression rule Bing TER score ? 55.685.
The primary motivation for using regression
tree learning in this way was to provide a quick
and convenient method for binarising our feature
set. However, we can also perform feature selec-
tion using this method by experimenting with vari-
ous minimum leaf sizes (Weka parameter M ). We
plot the performance of the M5P and M5P-R (opti-
mising towards correlation) over the parameter M
and select the best three values of M . To experi-
ment with the effect of smaller and larger feature
sets, we further include parameters of M that (a)
lead to an approximately 50% bigger feature set
and (b) to an approximately 50% smaller feature
set.
Our DCU-SYMC combine system was built
by combining the DCU-SYMC alltypes fea-
ture set, reduced using the best-first M5P wrap-
per approach as described in subsection 3.2, with
a binarised set produced using an M5P regres-
sion tree with a minimum of 50 nodes per leaf.
This latter configuration, containing 34 features
detailed in Table 2, was selected according to the
evaluation scores obtained during cross-validation
on the training set using -SVR, as described in
the next section. Finally, we run a greedy back-
ward feature selection algorithm wrapping -SVR
on both DCU-SYMC alltypes and DCU-SYMC
combine in order to optimise our feature sets for
the SVR learning algorithm, removing 6 and 2 fea-
tures respectively.
4 System Evaluation and Results
In this section, we present the results obtained with
-SVR during 5-fold cross-validation on the train-
ing set and the final results obtained on the test
set. We selected two systems to submit amongst
the different configurations based on MAE, RMSE
and r. As several systems reach the same perfor-
mance according to these metrics, we use the num-
ber of support vectors (noted SV) as an indicator
of training data over-fitting. We report the results
obtained with some of our systems in Table 3.
The results show that the submitted sys-
tems DCU-SYMC alltypes and DCU-SYMC
combine lead to the best scores on cross-
validation, but they do not outperform the system
combining the 15 feature types without feature se-
lection (15 types). This system reaches the best
scores on the test set compared to all our systems
built on reduced feature sets. This indicates that
we over-fit and fail to generalise from the training
data.
Amongst the systems built using reduced fea-
ture sets, the M5P-R M80 system, based on the
tree binarisation approach using M5P-R, yields
the best results on the test set on 3 out of 4 offi-
cial metrics. These results indicate that this sys-
tem, trained on 16 features only, tends to estimate
HTER scores more accurately on the unseen test
data. The results of the two systems based on
the M5P-R binarisation method are the best com-
pared to all the other systems presented in this
Section. This feature binarisation and selection
method leads to robust systems with few features:
31 and 16 for M5P-R M50 and M5P-R M80 re-
spectively. Even though these systems do not lead
to the best results, they outperform the two sub-
mitted systems on one metric used to evaluate the
395
Cross-Validation Test
System nb feat MAE RMSE r SV MAE RMSE DeltaAvg Spearman
15 types 442 0.106 0.138 0.604 1194.6 0.126 0.156 0.108 0.625
M5P M50 34 0.106 0.138 0.600 1417.8 0.135 0.167 0.102 0.586
M5P M130 4 0.114 0.145 0.544 750.6 0.142 0.173 0.079 0.517
M5P-R M50 31 0.106 0.137 0.610 655.4 0.135 0.166 0.100 0.591
M5P-R M80 16 0.107 0.139 0.597 570.6 0.134 0.165 0.106 0.597
alltypes? 96 0.104 0.135 0.624 1130.6 0.135 0.171 0.101 0.589
combine? 134 0.104 0.134 0.629 689.8 0.134 0.166 0.098 0.588
Table 3: Results obtained with different regression models, during cross-validation on the training set
and on the test set, depending on the feature selection method. Systems marked with ? were submitted
for the shared task.
scoring task and two metrics to evaluate the rank-
ing task.
On the systems built using reduced feature sets,
we observe a difference of approximately 0.03pt
absolute between the MAE and RMSE scores ob-
tained during cross-validation and those on the test
set. Such a difference can be related to train-
ing data over-fitting, even though the feature sets
obtained with the tree binarisation methods are
small. For instance, the system M5P M130 is
trained on 4 features only, but the difference be-
tween cross-validation and test MAE scores is
similar to the other systems. We see on the fi-
nal results that our feature selection methods is an
over-fitting factor: by selecting the features which
explain well the training set, the final model tends
to generalise less. The selected features are suited
for the specificities of the training data, but are less
accurate at predicting values on the unseen test set.
5 Discussion
Training data over-fitting is clearly shown by the
results presented in Table 3, indicated by the per-
formance drop between results obtained during
cross-validation and the ones obtained on the test
set. While this drop may be related to data over-
fitting, it may also be related to the use of differ-
ent machine learning methods for feature selec-
tion (M5P and M5P-R) and for building the fi-
nal regression models (-SVR). In order to ver-
ify this aspect, we build two regression models
using M5P, based on the feature sets alltypes
and combine. Results are presented in Table 4
and show that, for the alltypes feature set, the
RMSE, DeltaAvg and Spearman scores are im-
proved using M5P compared to SVM. For the
combine feature set, the scoring results (MAE
and RMSE) are better using SVM, while the rank-
ing results are similar for both machine learning
methods.
The performance drop between the results on
the training data (or a development set) and the
test data was also observed by the highest ranked
participants in the WMT12 QE shared task. To
compare our system without feature selection to
the winner of the previous shared task, we eval-
uate the 15 types system in Table 3 using the
WMT12 QE dataset. The results are presented in
Table 5. We can see that similar MAEs are ob-
tained with our feature set and the WMT12 QE
winner, whereas our system gets a higher RMSE
(+0.01). For the ranking scores, our system is
worse using the DeltaAvg metric while it is bet-
ter on Spearman coefficient.
6 Conclusion
We presented in this paper our experiments for the
WMT13 Quality Estimation shared task. Our ap-
proach is based on the extraction of a large ini-
tial feature set, followed by two feature selection
methods. The first one is a wrapper approach us-
ing M5P and a best-first search algorithm, while
the second one is a feature binarisation approach
using M5P and M5P-R. The final regression mod-
els were built using -SVR and we selected two
systems to submit based on cross-validation re-
sults.
We observed that our system reaching the best
scores on the test set was not a system trained on
a reduced feature set and it did not yield the best
cross-validation results. This system was trained
using 442 features, which are the combination of
15 different feature types. Amongst the systems
built on reduced sets, the best results are obtained
396
System nb feat MAE RMSE DeltaAvg Spearman
alltypes 96 0.135 0.165 0.104 0.604
combine 134 0.139 0.169 0.098 0.587
Table 4: Results obtained with the two feature sets contained in our submitted systems using M5P to
build the regression models instead of -SVR.
System nb feat MAE RMSE DeltaAvg Spearman
WMT12 winner 15 0.61 0.75 0.63 0.64
15 types 442 0.61 0.76 0.60 0.65
Table 5: Results obtained on WMT12 QE dataset with our best system (15 types) compared to WMT12
QE highest ranked team, in the Likert score prediction task.
using the feature binarisation approach M5P-R
80, which contains 16 features selected from our
initial set of features. The tree-based feature bina-
risation is a fast and flexible method which allows
us to vary the number of features by optimising the
leaf size and leads to acceptable results with a few
selected features.
Future work involves a deeper analysis of the
over-fitting effect and an investigation of other
methods in order to outperform the non-reduced
feature set. We are also interested in finding a ro-
bust way to optimise the leaf size parameter for
our tree-based feature binarisation method, with-
out using cross-validation on the training set with
an SVM algorithm.
Acknowledgements
The research reported in this paper has been
supported by the Research Ireland Enterprise
Partnership Scheme (EPSPG/2011/102 and EP-
SPD/2011/135) and Science Foundation Ireland
(Grant 12/CE/I2267) as part of the Centre for
Next Generation Localisation (www.cngl.ie)
at Dublin City University.
References
Chih-Chung Chang and Chih-Jen Lin. 2011.
LIBSVM: A Library for Support Vector Ma-
chines. ACM Transactions on Intelligent Sys-
tems and Technology, 2:27:1?27:27. Soft-
ware available at http://www.csie.ntu.
edu.tw/?cjlin/libsvm.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H Witten.
2009. The WEKA Data Mining Software: an
Update. ACM SIGKDD Explorations Newsletter,
11(1):10?18.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, et al 2007. Moses: Open source
toolkit for statistical machine translation. In ACL,
pages 177?180.
Vladimir I Levenshtein. 1966. Binary codes capable
of correcting deletions, insertions and reversals. In
Soviet physics doklady, volume 10, page 707.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In ACL, pages 311?
318.
Raphael Rubino, Jennifer Foster, Joachim Wagner, Jo-
hann Roturier, Rasul Samad Zadeh Kaljahi, and Fred
Hollowood. 2012. DCU-Symantec Submission for
the WMT 2012 Quality Estimation Task. In Pro-
ceedings of the Seventh WMT, pages 138?144.
Raphael Rubino et al (to appear). 2013. Topic Models
for Translation Quality Estimation for Gisting Pur-
poses. In Proceeding of MT Summit XIV.
Helmut Schmidt. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of the
International Conference on New Methods in Natu-
ral Language Processing.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In AMTA, pages 223?231.
Radu Soricut, Nguyen Bach, and Ziyuan Wang. 2012.
The SDL Language Weaver Systems in the WMT12
Quality Estimation Shared Task. In Proceedings of
the Seventh WMT, pages 145?151.
Yong Wang and Ian H Witten. 1997. Inducing Model
Trees for Continuous Classes. In Proceedings of
ECML, pages 128?137. Prague, Czech Republic.
397
Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 67?77,
October 25, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Syntax and Semantics in Quality Estimation of Machine Translation
Rasoul Kaljahi
??
, Jennifer Foster
?
, Johann Roturier
?
?
NCLT, School of Computing, Dublin City University, Ireland
{rkaljahi, jfoster}@computing.dcu.ie
?
Symantec Research Labs, Dublin, Ireland
{johann roturier}@symantec.com
Abstract
We employ syntactic and semantic infor-
mation in estimating the quality of ma-
chine translation from a new data set
which contains source text from English
customer support forums and target text
consisting of its machine translation into
French. These translations have been both
post-edited and evaluated by professional
translators. We find that quality estima-
tion using syntactic and semantic informa-
tion on this data set can hardly improve
over a baseline which uses only surface
features. However, the performance can
be improved when they are combined with
such surface features. We also introduce
a novel metric to measure translation ade-
quacy based on predicate-argument struc-
ture match using word alignments. While
word alignments can be reliably used,
the two main factors affecting the per-
formance of all semantic-based methods
seems to be the low quality of seman-
tic role labelling (especially on ill-formed
text) and the lack of nominal predicate an-
notation.
1 Introduction
The problem of evaluating machine translation
output without reference translations is called
quality estimation (QE) and has recently been the
centre of attention (Bojar et al., 2014) following
the seminal work of Blatz et al. (2003). Most
QE studies have focused on surface and language-
model-based features of the source and target. The
quality of translation is however closely related to
the syntax and semantics of the languages, the for-
mer concerning fluency and the latter adequacy.
While there have been some attempts to utilize
syntax in this task, semantics has been paid less
attention. In this work, we aim to exploit both
syntax and semantics in QE, with a particular fo-
cus on the latter. We use shallow semantic analy-
sis obtained via semantic role labelling (SRL) and
employ this information in QE in various ways in-
cluding statistical learning using both tree kernels
and hand-crafted features. We also design a QE
metric which is based on the Predicate-Argument
structure Match (PAM ) between the source and its
translation. The semantic-based system is then
combined with the syntax-based system to evalu-
ate the full power of structural linguistic informa-
tion. We also combine this system with a baseline
system consisting of effective surface features.
A second contribution of the paper is the release
of a new data set for QE.
1
This data set comprises
a set of 4.5K sentences chosen from customer sup-
port forum text. The machine translation of the
sentences are not only evaluated in terms of ade-
quacy and fluency, but also manually post-edited
allowing various metrics of interest to be applied
to measure different aspects of quality. All exper-
iments are carried out on this data set.
The rest of the paper is organized as follows:
after reviewing the related work, the data is de-
scribed and the semantic role labelling approach
is explained. The baseline is then introduced, fol-
lowed by the experiments with tree kernels, hand-
crafted features, the PAM metric and finally the
combination of all methods. The paper ends with
a summary and suggestions for future work.
2 Related Work
Syntax has been exploited in QE in various ways
including tree kernels (Hardmeier et al., 2012;
Kaljahi et al., 2013; Kaljahi et al., 2014b),
parse probabilities and syntactic label frequency
(Avramidis, 2012), parseability (Quirk, 2004) and
POS n-gram scores (Specia and Gim?enez, 2010).
1
The data will be made publicly available - see http://
www.computing.dcu.ie/mt/confidentmt.html
67
Turning to the role of semantic knowledge in
QE and MT evaluation in general, Pighin and
M`arquez (2011) propose a method for ranking two
translation hypotheses that exploits the projection
of SRL from a sentence to its translation using
word alignments. They first project the SRL of a
source corpus to its parallel corpus and then build
two translation models: 1) translations of proposi-
tion labelling sequences in the source to its projec-
tion in the target and 2) translations of argument
role fillers in the source to their counterparts in
the target. The source SRL is then projected to
its machine translation and the above models are
forced to translate source proposition labelling se-
quences to the projected ones. Finally the confi-
dence scores of these translations and their reach-
ability are used to train a classifier which selects
the better of the two translation hypotheses with
an accuracy of 64%. Factors hindering their clas-
sifier are word alignment limitations and low SRL
recall due to the lack of a verb or the loss of a
predicate during translation.
In MT evaluation, where reference translations
are available, Gim?enez and M`arquez (2007) use
semantic roles in building several MT evaluation
metrics which measure the full or partial lexical
match between the fillers of same semantic roles in
the hypothesis and translation, or simply the role
label matches between them. They conclude that
these features can only be useful in combination
with other features and metrics reflecting different
aspects of the quality.
Lo and Wu (2011) introduce HMEANT, a man-
ual MT evaluation metric based on predicate-
argument structure matching which involves two
steps of human engagement: 1) semantic role an-
notation of the reference and machine translation,
2) evaluating the translation of predicates and ar-
guments. The metric calculates the F
1
score of
the semantic frame match between the reference
and machine translation based on this evaluation.
To keep the costs reasonable, the first step is car-
ried out by amateur annotators who were mini-
mally trained with a simplified list of 10 thematic
roles. On a set of 40 examples, the metric is
meta-evaluated in terms of correlation with human
judgements of translation adequacy ranking, and a
correlation as high as that of HTER is reported.
Lo et al. (2012) propose MEANT, a variant of
HMEANT, which automatizes its manual steps
using 1) automatic SRL systems for (only) verb
predicates, 2) automatic alignment of predicates
and their arguments in the reference and ma-
chine translation based on their lexical similarity.
Once the predicates and arguments are aligned,
their similarities are measured using a variety of
methods such as cosine distance and even Me-
teor and BLEU. In computation of the final score,
the similarity scores replace the counts of correct
and partial translations used in HMEANT. This
metric outperforms several automatic metrics in-
cluding BLEU, Meteor and TER, but it signifi-
cantly under-performs HMEANT and HTER. Fur-
ther analysis shows that automatizing the second
step does not affect the performance of MEANT.
Therefore, it seems to be the lower accuracy of the
semantic role labelling that is responsible.
Bojar and Wu (2012) identify a set of flaws
with HMEANT and propose solutions for them.
The most important problems stem from the su-
perficial SRL annotation guidelines. These prob-
lems are exacerbated in MEANT due to the auto-
matic nature of the two steps. More recently, Lo
et al. (2014) extend MEANT to ranking transla-
tions without a reference by using phrase transla-
tion probabilities for aligning semantic role fillers
of the source and its translation.
3 Data
We randomly select 4500 segments from a large
collection of Symantec English Norton forum
text.
2
In order to be independent of any one MT
system, we translate these segments into French
with the following three systems and randomly
choose 1500 distinct segments from each.
? ACCEPT
3
: a phrase-based Moses system
trained on training sets of WMT12 releases
of Europarl and News Commentary plus
Symantec translation memories
? SYSTRAN: a proprietary rule-based system
augmented with domain-specific dictionaries
? Bing
4
: an online translation system
These translations are evaluated in two ways.
The first method involves light post-editing by
a professional human translator who is a native
2
http://community.norton.com
3
http://www.accept.unige.ch/Products/
D_4_1_Baseline_MT_systems.pdf
4
http://www.bing.com/translator(on24-
Feb-2014)
68
Adequacy Fluency
5 All meaning Flawless Language
4 Most of meaning Good Language
3 Much of meaning Non-native Language
2 Little meaning Disfluent Language
1 None of meaning Incomprehensible
Table 2: Adequacy/fluency score interpretation
French speaker.
5
Each sentence translation is then
scored against its post-edit using BLEU
6
(Papineni
et al., 2002), TER (Snover et al., 2006) and
METEOR (Denkowski and Lavie, 2011), which are
the most widely used MT evaluation metrics. Fol-
lowing Snover et al. (2006), we consider this way
of scoring MT output to be a variation of human-
targeted scoring, where no reference translation
is provided to the post-editor, so we call them
HBLEU, HTER and HMETEOR. The average scores
for the entire data set together with their standard
deviations are presented in Table 1.
7
In the second method, we asked three profes-
sional translators, who are again native French
speakers, to assess the quality of MT output in
terms of adequacy and fluency in a 5-grade scale
(LDC, 2002). The interpretation of the scores is
given in Table 2. Each evaluator was given the
entire data set for evaluation. We therefore col-
lected three sets of scores and averaged them to
obtain the final scores. The averages of these
scores for the entire data set together with their
standard deviations are presented in Table 1. To
be easily comparable to human-targeted scores,
we scale these scores to the [0,1] range, i.e. ad-
equacy/fluency scores of 1 and 5 are mapped to 0
and 1 respectively and all the scores in between
are accordingly scaled.
The average Kappa inter-annotator agreement
for adequacy scores is 0.25 and for fluency scores
0.19. However, this measurement does not dif-
ferentiate between small and large differences in
agreement. In other words, the difference between
5
The post-editing guidelines are based on the
TAUS/CNGL guidelines for achieving ?good enough?
quality downloaded from https://evaluation.
taus.net/images/stories/guidelines/taus-
cngl-machine-translation-postediting-
guidelines.pdf.
6
Version 13a of MTEval script was used at the segment
level which performs smoothing.
7
Note that HTER scores have no upper limit and can be
higher than 1 when the number of errors is higher than the
segment length. In addition, the higher HTER indicates lower
translation quality. To be comparable to the other scores, we
cut-off them at 1 and convert to 1-HTER.
1-HTER HBLEU HMeteor Adq Flu
1-HTER - - - - -
HBLEU 0.9111 - - - -
HMeteor 0.9207 0.9314 - - -
Adq 0.6632 0.7049 0.6843 - -
Flu 0.6447 0.7213 0.6652 0.8824 -
Table 3: Pearson r between pairs of metrics on the
entire 4.5K data set
scores of 5 and 4 is the same as the difference
between 5 and 2. To account for this, we use
weighted Kappa instead. Specifically, we consider
two scores of difference 1 to represent 75% agree-
ment instead of 100%. All the other differences
are considered to be a disagreement. The aver-
age weighted Kappa computed in this way is 0.65
for adequacy and 0.63 for fluency. Though the
weighting used is quite strict, the weighted Kappa
values are in the substantial agreement range.
Once we have both human-targeted and manual
evaluation scores together, it is interesting to know
how they are correlated. We calculate the Pearson
correlation coefficient r between each pair of the
five scores and present them in Table 3. HBLEU
has the highest correlation with both adequacy and
fluency scores among the human-targeted metrics.
HTER on the other hand has the lowest correla-
tion. Moreover, HBLEU is more correlated with
fluency than with adequacy which is the opposite
to HMeteor. This is expected according to the
definition of BLEU and Meteor. There is also
a high correlation between adequacy and fluency
scores. Although this could be related to the fact
that both scores are from the same evaluators, it
indicates that if either the fluency and adequacy of
the MT output is low or high, the other tends to be
the same.
The data is split into train, development and test
sets of 3000, 500 and 1000 sentences respectively.
4 Semantic Role Labelling
The type of semantic information we use in this
work is the predicate-argument structure or se-
mantic role labelling of the sentence. This infor-
mation needs to be extracted from both sides of the
translation, i.e. English and French. Though the
SRL of English has been well-studied (M`arquez
et al., 2008) thanks to the existence of two major
hand-crafted resources, namely FrameNet (Baker
et al., 1998) and PropBank (Palmer et al., 2005),
French is one of the under-studied languages in
69
1-HTER HBLEU HMeteor Adequacy Fluency
Average 0.6976 0.5517 0.7221 0.6230 0.4096
Standard Deviation 0.2446 0.2927 0.2129 0.2488 0.2780
Table 1: Average and standard deviation of the evaluation scores for the entire data set
this respect mainly due to a lack of such resources.
The only available gold standard resource is a
small set of 1000 sentences taken from Europarl
(Koehn, 2005) and manually annotated with Prop-
bank verb predicates (van der Plas et al., 2010).
van der Plas et al. (2011) attempt to tackle this
scarcity by automatically projecting SRL from the
English side of a large parallel corpus to its French
side. Our preliminary experiments (Kaljahi et al.,
2014a), however, show that SRL models trained
on the small manually annotated corpus have a
higher quality than ones trained on the much larger
projected corpus. We therefore use the 1K gold
standard set to train a French SRL model. For En-
glish, we use all the data provided in the CoNLL
2009 shared task (Haji?c et al., 2009).
We use LTH (Bj?orkelund et al., 2009), a
dependency-based SRL system, for both the En-
glish and French data. This system was among
the best performing systems in the CoNLL 2009
shared task and is straightforward to use. It comes
with a set of features tuned for each shared task
language (English, German, Japanese, Spanish,
Catalan, Czech, Chinese). We compared the per-
formance of the English and Spanish feature sets
on French and chose the former due to its higher
performance (by 1 F
1
point).
It should be noted that the English SRL data
come with gold standard syntactic annotation. On
the other hand, for our QE data set, such anno-
tation is not available. Our preliminary experi-
ments show that, since the SRL system heavily
relies on syntactic features, the performance con-
siderably drops when the syntactic annotation of
the test data is obtained using a different parser
than that of the training data. We therefore re-
place the parses of the training data with those ob-
tained automatically by first parsing the data us-
ing the Lorg PCFG-LA parser
8
(Attia et al., 2010)
and then converting them to dependencies using
Stanford converter (de Marneffe and Manning,
2008). The POS tags are also replaced with those
output by the parser. For the same reason, we re-
8
https://github.com/CNGLdlab/LORG-
Release.
place the original POS tagging of the French 1K
data with those obtained by the MElt tagger (De-
nis and Sagot, 2012).
The English SRL achieves 77.77 and 67.02 la-
belled F
1
points when trained only on the training
section of PropBank and tested on the WSJ and
Brown test sets respectively.
9
The French SRL is
evaluated using 5-fold cross-validation on the 1K
data set and obtains an F
1
average of 67.66. When
applied to the QE data set, these models identify
9133, 8875 and 8795 propositions on its source
side, post-edits and MT output respectively.
5 Baseline
We compare the results of our experiments to a
baseline built using the 17 baseline features of the
WMT QE shared task (Bojar et al., 2014). These
features provide a strong baseline and have been
used in all three years of the shared task. We
use support vector regression implemented in the
SVMLight toolkit
10
with Radial Basis Function
(RBF) kernel to build this baseline. To extract
these features, a parallel English-French corpus
is required to build a lexical translation table us-
ing GIZA++ (Och and Ney, 2003). We use the
Europarl English-French parallel corpus (Koehn,
2005) plus around 1M segments of Symantec
translation memory.
Table 4 shows the performance of this system
(WMT17) on the test set measured by Root Mean
Square Error (RMSE) and Pearson correlation co-
efficient (r). We only report the results on predict-
ing four of the metrics introduced above, omitting
HMeteor due to space constraints. C and ? pa-
rameters are tuned on the development set with re-
spect to r. The results show a significant differ-
ence between manual and human-targeted metric
prediction. The higher r for the former suggests
that the patterns of these scores are easier to learn.
The RMSE seems to follow the standard deviation
9
Although the English SRL data are annotated for noun
predicates as well as verb predicates, since the French data
has only verb predicate annotations, we only consider verb
predicates for English.
10
http://svmlight.joachims.org/
70
of the scores as the same ranking is seen in both.
6 Tree Kernels
Tree kernels (Moschitti, 2006) have been success-
fully used in QE by Hardmeier et al. (2012) and
in our previous work (Kaljahi et al., 2013; Kal-
jahi et al., 2014b), where syntactic trees are em-
ployed. Tree kernels eliminate the burden of man-
ual feature engineering by efficiently utilizing all
subtrees of a tree. We employ both syntactic and
semantic information in learning quality scores,
using the SVMLight-TK
11
, a support vector ma-
chine (SVM) implementation of tree kernels.
We implement a syntactic tree kernel QE sys-
tem with constituency and dependency trees of
the source and target side, following our previous
work (Kaljahi et al., 2013; Kaljahi et al., 2014b).
The performance of this system (TKSyQE) is
shown in Table 4. Unlike our previous results,
where the syntax-based system significantly out-
performed the WMT17 baseline, TKSyQE can only
beat the baseline in HTER and fluency prediction,
with neither difference being statistically signifi-
cant and it is below the baseline for HBLEU and
adequacy prediction.
12
It should be noted that in
our previous work, a WMT News data set was
used as the QE data set which, unlike our new data
set, is well-formed and in the same domain as the
parsers? training data. The discrepancy between
our new and old results suggests that the perfor-
mance is strongly dependent on the data set.
Unlike syntactic parsing, semantic role la-
belling does not produce a tree to be directly used
in the tree kernel framework. There can be var-
ious ways to accomplish this goal. We first try
a method inspired by the PAS format introduced
by Moschitti et al. (2006). In this format, a fixed
number of nodes are gathered under a dummy root
node as slots of one predicate and 6 arguments of
a proposition (one tree per predicate). Each node
dominates an argument label or a dummy label for
the predicate, which in turn dominates the POS
tag of the argument or the predicate lemma. If a
proposition has more than 6 arguments they are
ignored, if it has fewer than 6 arguments, the extra
slots are attached to a dummy null label. Note that
these trees are derived from the dependency-based
SRL of both the source and target side (Figure
11
http://disi.unitn.it/moschitti/Tree-
Kernel.htm
12
We use paired bootstrap resampling Koehn (2004) for
statistical significance testing.
1-HTER HBLEU Adq Flu
RMSE
WMT17 0.2310 0.2696 0.2219 0.2469
TKSyQE 0.2267 0.2721 0.2258 0.2431
D-PAS 0.2489 0.2856 0.2423 0.2652
D-PST 0.2409 0.2815 0.2383 0.2606
C-PST 0.2400 0.2809 0.2410 0.2615
CD-PST 0.2394 0.2795 0.2373 0.2578
TKSSQE 0.2269 0.2722 0.2253 0.2425
Pearson r
WMT17 0.3661 0.3806 0.4710 0.4769
TKSyQE 0.3693 0.3559 0.4306 0.5013
D-PAS 0.1774 0.1843 0.2770 0.3252
D-PST 0.2136 0.2450 0.3169 0.3670
C-PST 0.2319 0.2541 0.2966 0.3616
CD-PST 0.2311 0.2714 0.3303 0.3923
TKSSQE 0.3682 0.3537 0.4351 0.5046
Table 4: RMSE and Pearson r of the 17 base-
line features (WMT17) and tree kernel systems;
TKSyQE: syntax-based tree kernels, D-PAS:
dependency-based PAS tree kernels of Moschitti
et al. (2006), D-PST, C-PST and CD-PST:
dependency-based, constituency-based proposi-
tion subtree kernels and their combination,
TKSSQE: syntactic-semantic tree kernels
1(a)). The results are shown in Table 4 (D-PAS).
The performance is statistically significantly lower
than the baseline.
13
In order to encode more information in the trees,
we propose another format in which proposition
subtrees (PST) of the sentence are gathered un-
der a dummy root node. A dependency PST (Fig-
ure 1(b)) is formed by the predicate label under
the root dominating its lemma and all its argu-
ments roles. Each of these nodes in turn dominates
three nodes: the argument word form (the predi-
cate word form for the case of a predicate lemma),
its syntactic dependency relation to its head and its
POS tag. We preserve the order of arguments and
predicate in the sentence.
14
This system is named
D-PST in Table 4. Tree kernels in this format sig-
nificantly outperform D-PAS. However, the per-
formance is still far lower than the baseline.
The above formats are based on dependency
trees. We try another PST format derived from
constituency trees. These PSTs (Figure 1(c)) are
the lowest common subtrees spanning the predi-
cate node and its argument nodes and are gath-
ered under a dummy root node. The argument role
13
Note that the only lexical information in this format is
the predicate lemma. We tried replacing the POS tags with
argument word forms, which led to a slight degradation.
14
This format is chosen among several other variations due
to its higher performance.
71
(a) D-PAS (b) D-PST (c) C-PST (d) D-TKSSQE (e) C-TKSSQE
Figure 1: Semantic tree kernel formats for the sentence: Can anyone help?
labels are concatenated with the syntactic non-
terminal category of the argument node. Predi-
cates are not marked. However, our dependency-
based SRL is required to be converted into a
constituency-based format. While constituency-
to-dependency conversion is straightforward us-
ing head-finding rules (Surdeanu et al., 2008),
the other way around is not. We therefore ap-
proximate the conversion using a heuristic we call
(D2C).
15
As shown in Table 4, the system built us-
ing these PSTs C-PST improves over D-PST for
human-targeted metric prediction, but not man-
ual metric prediction. However, when they are
combined in CD-PST, we can see improvement
over the highest scores of both systems, except
for HTER prediction for Pearson r. The fluency
prediction improvement is statistically significant.
The other changes are not statistically significant.
An alternative approach to formulating seman-
tic tree kernels is to augment syntactic trees with
semantic information. We augment the trees in
TKSyQE with semantic role labels. We attach se-
mantic roles to dependency labels of the argument
nodes in the dependency trees as in Figure 1(d).
For constituency trees, we use the D2C heuristic
to elevate roles up the terminal nodes and attach
the labels to the syntactic non-terminal category
of the node as in Figure 1(e). The performance
of the resulting system, TKSSQE, is shown in Ta-
ble 4. It substantially outperforms its counterpart,
CD-PST, all differences being statistically signif-
icant. However, compared to the plain syntactic
tree kernels (TKSyQE), the changes are slight and
inconsistent, rendering the augmentation not use-
ful. We consider this system to be our syntactic-
15
This heuristic (D2C) recursively elevates the argument
role already assigned to a terminal node (based on the
dependency-based argument position) to the parent node as
long as 1) the argument node is not a root node or is not
tagged as a POS (possessive), 2) the role is not an AM-NEG,
AM-MOD or AM-DIS adjunct, and 3) the argument does not
dominate its predicate?s node or another argument node of the
same proposition.
1-HTER HBLEU Adq Flu
RMSE
WMT17 0.2310 0.2696 0.2219 0.2469
HCSyQE 0.2435 0.2797 0.2334 0.2479
HCSeQE 0.2482 0.2868 0.2416 0.2612
Pearson r
WMT17 0.3661 0.3806 0.4710 0.4769
HCSyQE 0.2572 0.3080 0.3961 0.4696
HCSeQE 0.1794 0.1636 0.2972 0.3577
Table 5: RMSE and Pearson r of the 17 baseline
features (WMT17) and hand-crafted features
semantic tree kernel system.
7 Hand-crafted Features
In our previous work (Kaljahi et al., 2014b), we
experiment with a set of hand-crafted syntactic
features extracted from both constituency and de-
pendency trees on a different data set. We apply
the same feature set on the new data set here. The
results are reported in Table 5. The performance of
this system (HCSyQE) is significantly lower than
the baseline. This is opposite to what we ob-
serve with the same feature set on a different data
set, again showing that the role of data is funda-
mental in understanding system performance. The
main difference between these two data sets is that
the former is extracted from a well-formed text in
the news domain, the same domain on which our
parsers and SRL system have been trained, while
the new data set does not necessarily contain well-
formed text nor is it from the same domain.
We design another set of feature types aiming
at capturing the semantics of the source and trans-
lation via predicate-argument structure. The fea-
ture types are listed in Table 6. Feature types
1 to 8 each contain two features, one extracted
from the source and the other from the transla-
tion. To compute argument span sizes (feature
types 4 and 5), we use the constituency conver-
sion of SRL obtained using the D2C heuristic in-
troduced in Section 6. The proposition label se-
72
1 Number of propositions
2 Number of arguments
3 Average number of arguments per proposition
4 Sum of span sizes of arguments
5 Ratio of sum of span sizes of arguments to sentence
length
6 Proposition label sequences
7 Constituency label sequences of proposition elements
8 Dependency label sequences of proposition elements
9 Percentage of predicate/argument word alignment
mapping types
Table 6: Semantic feature types
quence (feature type 6) is the concatenation of ar-
gument roles and predicate labels of the propo-
sition with their preserved order (e.g. A0-go.01-
A4). Similarly, constituency and dependency la-
bel sequences (feature types 4 and 5) are extracted
by replacing argument and predicate labels with
their constituency and dependency labels respec-
tively. Feature type 9 consists of three features
based on word alignment of source and target
sentences: number of non-aligned, one-to-many-
aligned and many-to-one-aligned predicates and
arguments. The word alignments are obtained us-
ing the grow-diag-final-and heuristic as
they performed slightly better than other types.
16
As in the baseline system, we use SVMs to build
the QE systems using these hand-crafted features.
The nominal features are binarized to be usable by
SVM. However, the set of possible feature values
can be large, leading to a large number of binary
features. For example, there are more than 5000
unique proposition label sequences in our data.
Not only does this high dimensionality reduce the
efficiency of the system, it can also affect its per-
formance as these features are sparse. To tackle
this issue, we impose a frequency cutoff on these
features: we keep only frequent features using a
threshold set empirically on the development set.
Table 5 shows the performance of the system
(HCSeQE) built with these features. The semantic
features perform substantially lower than the syn-
tactic features and thus the baseline, especially in
predicting human-targeted scores. Since these fea-
tures are chosen from a comprehensive set of se-
mantic features, and as they should ideally capture
adequacy better than general features, a probable
reason for their low performance is the quality of
16
It should be noted that a number of features in addition
to those presented here have been tried, e.g. the ratio and dif-
ference of the source and target values of numerical features.
However, through manual feature selection, we have removed
features which do not appear to contribute much.
the underlying syntactic and semantic analysis.
8 Predicate-Argument Match (PAM)
Translation adequacy measures how much of the
source meaning is preserved in the translated text.
Predicate-argument structure or semantic role la-
belling expresses a substantial part of the meaning.
Therefore, the matching between the predicate-
argument structure of the source and its transla-
tion could be an important clue to the translation
adequacy, independent of the language pair used.
We attempt to exploit predicate-argument match
(PAM) to create a metric that measures the trans-
lation adequacy.
The algorithm to compute PAM score starts
by aligning the predicates and arguments of the
source side to its target side using word align-
ments.
17
It then treats the problem as one of SRL
scoring, similar to the scoring scheme used in the
CoNLL 2009 shared task (Haji?c et al., 2009). As-
suming the source side SRL as a reference, it com-
putes unlabelled precision and recall of the target
side SRL with respect to it:
UPrec =
# aligned preds and their args
# target side preds and args
URec =
# aligned preds and their args
# source side preds and args
Labelled precision and recall are calculated in
the same way except that they also require argu-
ment label agreement. UF
1
and LF
1
are the har-
monic means of unlabelled and labelled scores re-
spectively. Inspired by the observation that most
source sentences with no identified proposition are
short and can be assumed to be easier to translate,
and based on experiments on the dev set, we assign
a score of 1 to such sentences. When no proposi-
tion is identified in the target side while there is a
proposition in the source, we assign a score of 0.5.
We obtain word alignments using the Moses
toolkit (Hoang et al., 2009), which can gener-
ate alignments in both directions and combine
them using a number of heuristics. We try in-
tersection, union, source-to-target only, as well
as the grow-diag-final-and heuristic, but
only the source-to-target results are reported here
as they slightly outperform the others.
Table 7 shows the RMSE and Pearson r for
each of the unlabelled and labelled F
1
against ade-
17
We also tried lexical and phrase translation tables for this
purpose in addition to word alignments but they do not out-
perform word alignments.
73
1-HTER HBLEU Adq Flu
RMSE
1 UF
1
0.3175 0.3607 0.3108 0.4033
LF
1
0.4247 0.3903 0.3839 0.3586
Pearson r
UF
1
0.2328 0.2179 0.2698 0.2865
LF
1
0.1784 0.1835 0.2225 0.2688
Table 7: RMSE and Pearson r of PAM unlabelled
and labelled F
1
scores as estimation of the MT
evaluation metrics
1-HTER HBLEU Adq Flu
RMSE
PAM 0.2414 0.2833 0.2414 0.2661
HCSeQE 0.2482 0.2868 0.2416 0.2612
HCSeQE
pam
0.2445 0.2822 0.2370 0.2575
Pearson r
PAM 0.2292 0.2195 0.2787 0.3210
HCSeQE 0.1794 0.1636 0.2972 0.3577
HCSeQE
pam
0.2387 0.2368 0.3571 0.3908
Table 8: RMSE and Pearson r of PAM scores as
features, alone and combined (PAM)
quacy and also fluency scores on the test data set.
18
According to the results, the unlabelled F
1
(UF
1
)
is a closer estimation than the labelled one. Its
Pearson correlation scores are overall competitive
to the hand-crafted semantic features (HCSeQE in
Table 5): they are better for the automatic metric
cases but lower for manual ones. However, the
RMSE scores are considerably larger. Overall, the
performance is not comparable to the baseline and
other well performing systems. We investigate the
reasons behind this result in the next section.
Another way to employ the PAM scores in QE
is to use them in a statistical framework. We build
a SVM model using all 6 PAM scores The per-
formance of this system (PAM) on the test set is
shown in Table 8. The performance is consider-
ably higher than when the PAM scores are used
directly as estimations. Interestingly, compared to
the 47 semantic hand-crafted features (HCSeQE),
this small feature set performs better in predicting
human-targeted metrics.
We add these features to our set of hand-
crafted features in Section 7 to yield a new sys-
tem (HCSeQE
pam
in Table 8). All scores improve
compared to the stronger of the two components.
However, only the manual metric prediction im-
provements are statistically significant. The per-
formance is still not close to the baseline.
18
Precision and recall scores were also tried. Precision
proved to be the weakest estimator, whereas recall scores
were highest for some settings.
8.1 Analyzing PAM
Ideally, PAM scores should capture the adequacy
of translation with a high accuracy. The results
are however far from ideal. There are two fac-
tors involved in the PAM scoring procedure, the
quality of which can affect its performance: 1)
predicate-argument structure of the source and
target side of the translation, 2) alignment of
predicate-argument structures of source and target.
The SRL systems for both English and French
are trained on edited newswire. On the other
hand, our data is neither from the same domain nor
edited. The problem is exacerbated on the trans-
lation target side, where our French SRL system
is trained on only a small data set and applied to
machine translation output. To discover the con-
tribution of each of these factors in the accuracy
of PAM, we carry out a manual analysis. We ran-
domly select 10% of the development set (50 sen-
tences) and count the number of problems of each
of these two categories.
We find only 8 cases in which a wrong word
alignment misleads PAM scoring. On the other
hand, there are 219 cases of SRL problems, in-
cluding predicate and argument identification and
labelling: 82 cases (37%) in the source and 138
cases (63%) in the target.
We additionally look for the cases where a
translation divergence causes predicate-argument
mismatch in the source and translation. For ex-
ample, without sacrificing is translated into sans
impact sur (without impact on), a case of transpo-
sition, where the source side verb predicate is left
unaligned thus affecting the PAM score. We find
only 9 such cases in the sample, which is similar
to the proportion of word alignment problems.
As mentioned in the previous section, PAM
scoring has to assign default values for cases in
which there is no predicate in the source or tar-
get. This can be another source of estimation error.
In order to verify its effect, we find such cases in
the development set and manually categorize them
based on the reason causing the sentence to be left
without predicates. There are 79 (16%) source and
96 (19%) target sentences for which the SRL sys-
tems do not identify any predicate, out of which
64 cases have both sides without any predicate.
Among such source sentences, 20 (25%) have no
predicate due to a predicate identification error of
the SRL system, 57 (72%) because of the sentence
structure (e.g. copula verbs which are not labelled
74
1-HTER HBLEU Adq Flu
RMSE
WMT17 0.2310 0.2696 0.2219 0.2469
SyQE 0.2255 0.2711 0.2248 0.2419
SeQE 0.2249 0.2710 0.2242 0.2404
SSQE 0.2246 0.2696 0.2230 0.2402
SSQE+WMT17 0.2225 0.2673 0.2202 0.2379
Pearson r
WMT17 0.3661 0.3806 0.4710 0.4769
SyQE 0.3824 0.3650 0.4393 0.5087
SeQE 0.3884 0.3648 0.4447 0.5182
SSQE 0.3920 0.3768 0.4538 0.5196
SSQE+WMT17 0.4144 0.3953 0.4771 0.5331
Table 9: RMSE and Pearson r of the 17 baseline
features (WMT17) and system combinations
as predicates in the SRL training data, titles, etc.),
and the remaining 2 due to spelling errors mislead-
ing the SRL system. Among the target side sen-
tences, most of the cases are due to the sentence
structure (65 or 68%) and only 14 (15%) cases are
caused by an SRL error. In 13 cases, no verb pred-
icate in the source is translated correctly. Among
the remaining cases, two are due to untranslated
spelling errors in the source and the other two due
to tokenization errors misleading the SRL system.
These numbers show that the main reason lead-
ing to the sentences without verbal predicates is
the sentence structure. This problem can be al-
leviated by employing nominal predicates in both
sides. While this is possible for the English side,
there is currently no French resource where nomi-
nal predicates have been annotated.
9 Combining Systems
We now combine the systems we have built so
far (Table 9). We first combine syntax-based
and semantic-based systems individually. SyQE
is the combination of the syntactic tree kernel
system (TKSyQE) and the hand-crafted features
(HCSyQE). Likewise, SeQE is the combination
of the semantic tree kernel system (TKSSQE) and
the semantic hand-crafted features including PAM
features (HCSeQE
pam
). These two systems are
combined in SSQE but without syntactic tree ker-
nels (TKSyQE) to avoid redundancy with TKSSQE
as these are the augmented syntactic tree kernels.
We finally combine SSQE with the baseline.
SyQE significantly improves over its tree ker-
nel and hand-crafted components. It also outper-
forms the baseline in HTER and fluency predic-
tion, but is beaten by it in HBLEU and adequacy
prediction. None of these differences are statis-
tically significant however. SeQE also performs
better than the stronger of its components. Except
for adequacy prediction, the other improvements
are statistically significant. This system performs
slightly better than SyQE. Its comparison to the
baseline is the same as that of SyQE, except that
its superiority to the baseline in fluency prediction
is statistically significant.
The full syntactic-semantic system (SSQE) also
improves over its syntactic and semantic compo-
nents. However, the improvements are not statisti-
cally significant. Compared to the baseline, HTER
and fluency prediction perform better, the latter
being statistically significant. HBLEU prediction
is around the same as the baseline, but adequacy
prediction performance is lower, though not statis-
tically significantly.
Finally, when we combine the syntactic-
semantic system with the baseline system, the
combination continues to improve further. Com-
pared to the stronger component however, only the
HTER and fluency prediction improvements are
statistically significant.
10 Conclusion
We introduced a new QE data set drawn from cus-
tomer support forum text, machine translated and
both post-edited and manually evaluated for ad-
equacy and fluency. We used syntactic and se-
mantic QE systems via both tree kernels and hand-
crafted features. We found it hard to improve over
a baseline, albeit strong, using such information
which is extracted by applying parsers and seman-
tic role labellers on out-of-domain and unedited
text. We also defined a metric for estimating the
translation adequacy based on predicate-argument
structure match between source and target. This
metric relies on automatic word alignments and
semantic role labelling. We find that word align-
ment and translation divergence only have minor
effects on the performance of this metric, whereas
the quality of semantic role labelling is the main
hindering factor. Another major issue affecting the
performance of PAM is the unavailability of nom-
inal predicate annotation.
Our PAM scoring method is based on only word
matches as there are no constituent SRL resources
available for French ? perhaps constituent-based
arguments can make a more accurate comparison
between the source and target predicate-argument
structure possible.
75
Acknowledgments
This research has been supported by the Irish
Research Council Enterprise Partnership Scheme
(EPSPG/2011/102) and the computing infrastruc-
ture of the CNGL at DCU. We thank the reviewers
for their helpful comments.
References
Mohammed Attia, Jennifer Foster, Deirdre Hogan,
Joseph Le Roux, Lamia Tounsi, and Josef van Gen-
abith. 2010. Handling unknown words in statistical
latent-variable parsing models for Arabic, English
and French. In Proceedings of the 1st Workshop
on Statistical Parsing of Morphologically Rich Lan-
guages.
Eleftherios Avramidis. 2012. Quality estimation for
Machine Translation output using linguistic analysis
and decoding features. In Proceedings of the 7th
WMT.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley Framenet project. In Proceed-
ings of the 36th ACL.
Anders Bj?orkelund, Love Hafdell, and Pierre Nugues.
2009. Multilingual semantic role labeling. In Pro-
ceedings of the Thirteenth Conference on Computa-
tional Natural Language Learning: Shared Task.
John Blatz, Erin Fitzgerald, George Foster, Simona
Gandrabur, Cyril Goutte, Alex Kulesza, Alberto
Sanchis, and Nicola Ueffing. 2003. Confidence
estimation for Machine Translation. In JHU/CLSP
Summer Workshop Final Report.
Ond?rej Bojar and Dekai Wu. 2012. Towards a
predicate-argument evaluation for MT. In Proceed-
ings of the Sixth Workshop on Syntax, Semantics and
Structure in Statistical Translation.
Ondrej Bojar, Christian Buck, Christian Federmann,
Barry Haddow, Philipp Koehn, Johannes Leveling,
Christof Monz, Pavel Pecina, Matt Post, Herve
Saint-Amand, Radu Soricut, Lucia Specia, and Ale?s
Tamchyna. 2014. Findings of the 2014 workshop
on Statistical Machine Translation. In Proceedings
of the 9th WMT.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The Stanford typed dependencies
representation. In Proceedings of the COLING
Workshop on Cross-Framework and Cross-Domain
Parser Evaluation.
Pascal Denis and Beno??t Sagot. 2012. Coupling an
annotated corpus and a lexicon for state-of-the-art
pos tagging. Lang. Resour. Eval., 46(4):721?736.
Michael Denkowski and Alon Lavie. 2011. Meteor
1.3: Automatic metric for reliable optimization and
evaluation of machine translation systems. In Pro-
ceedings of the 6th WMT.
Jes?us Gim?enez and Llu??s M`arquez. 2007. Linguistic
features for automatic evaluation of heterogenous mt
systems. In Proceedings of the Second Workshop on
Statistical Machine Translation.
Jan Haji?c, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Ant`onia Mart??, Llu??s
M`arquez, Adam Meyers, Joakim Nivre, Sebastian
Pad?o, Jan
?
St?ep?anek, Pavel Stra?n?ak, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The conll-2009
shared task: Syntactic and semantic dependencies
in multiple languages. In Proceedings of the Thir-
teenth Conference on Computational Natural Lan-
guage Learning: Shared Task, pages 1?18.
Christian Hardmeier, Joakim Nivre, and J?org Tiede-
mann. 2012. Tree kernels for machine translation
quality estimation. In Proceedings of the Seventh
WMT.
Hieu Hoang, Philipp Koehn, and Adam Lopez. 2009.
A unified framework for phrase-based, hierarchical,
and syntax-based statistical machine translation. In
Proceedings of the International Workshop on Spo-
ken Language Translation (IWSLT).
Rasoul Kaljahi, Jennifer Foster, Raphael Rubino, Jo-
hann Roturier, and Fred Hollowood. 2013. Parser
accuracy in quality estimation of machine transla-
tion: a tree kernel approach. In International Joint
Conference on Natural Language Processing (IJC-
NLP).
Rasoul Kaljahi, Jennifer Foster, and Johann Roturier.
2014a. Semantic role labelling with minimal re-
sources: Experiments with french. In Third Joint
Conference on Lexical and Computational Seman-
tics (*SEM).
Rasoul Kaljahi, Jennifer Foster, Raphael Rubino, and
Johann Roturier. 2014b. Quality estimation of
english-french machine translation: A detailed study
of the role of syntax. In International Conference on
Computational Linguistics (COLING).
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
EMNLP.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. In Conference Pro-
ceedings: the tenth Machine Translation Summit.
LDC. 2002. Linguistic data annotation specification:
Assessment of fluency and adequacy in chinese-
english translations. Technical report.
Chi-kiu Lo and Dekai Wu. 2011. Meant: An inex-
pensive, high-accuracy, semi-automatic metric for
evaluating translation utility via semantic frames. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies - Volume 1.
Chi-kiu Lo, Anand Karthik Tumuluru, and Dekai Wu.
2012. Fully automatic semantic mt evaluation. In
Proceedings of the Seventh WMT.
76
Chi-kiu Lo, Meriem Beloucif, Markus Saers, and
Dekai Wu. 2014. Xmeant: Better semantic mt eval-
uation without reference translations. In Proceed-
ings of the 52nd Annual Meeting of the Association
for Computational Linguistics (Volume 2: Short Pa-
pers), June.
Llu??s M`arquez, Xavier Carreras, Kenneth C.
Litkowski, and Suzanne Stevenson. 2008. Se-
mantic role labeling: An introduction to the special
issue. Comput. Linguist., 34(2):145?159, June.
Alessandro Moschitti, Daniele Pighin, and Roberto
Basili. 2006. Tree kernel engineering for propo-
sition re-ranking. In Proceedings of Mining and
Learning with Graphs (MLG).
Alessandro Moschitti. 2006. Making tree kernels prac-
tical for natural language learning. In Proceedings
of EACL.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Comput. Linguist., 29(1):19?51.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated cor-
pus of semantic roles. Computational Linguistics,
31(1):71?106.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
ACL, pages 311?318.
Daniele Pighin and Llu??s M`arquez. 2011. Automatic
projection of semantic structures: An application to
pairwise translation ranking. In Proceedings of the
Fifth Workshop on Syntax, Semantics and Structure
in Statistical Translation, pages 1?9.
Chris Quirk. 2004. Training a sentence-level machine
translation confidence measure. In Proceedings of
LREC.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of AMTA.
Lucia Specia and Jes?us Gim?enez. 2010. Combining
confidence estimation and reference-based metrics
for segment level MT evaluation. In Proceedings of
AMTA.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s M`arquez, and Joakim Nivre. 2008. The
conll-2008 shared task on joint parsing of syntactic
and semantic dependencies. In Proceedings of the
Twelfth Conference on Computational Natural Lan-
guage Learning.
Lonneke van der Plas, Tanja Samard?zi?c, and Paola
Merlo. 2010. Cross-lingual validity of propbank
in the manual annotation of french. In Proceedings
of the Fourth Linguistic Annotation Workshop, LAW
IV ?10.
Lonneke van der Plas, Paola Merlo, and James Hen-
derson. 2011. Scaling up automatic cross-lingual
semantic role annotation. In Proceedings of the 49th
Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies.
77
