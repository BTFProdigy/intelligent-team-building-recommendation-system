Probabilistic Sentence Reduction Using Support Vector Machines
Minh Le Nguyen, Akira Shimazu, Susumu Horiguchi
Bao Tu Ho and Masaru Fukushi
Japan Advanced Institute of Science and Technology
1-8, Tatsunokuchi, Ishikawa, 923-1211, JAPAN
{nguyenml, shimazu, hori, bao, mfukushi}@jaist.ac.jp
Abstract
This paper investigates a novel application of sup-
port vector machines (SVMs) for sentence reduction.
We also propose a new probabilistic sentence reduc-
tion method based on support vector machine learn-
ing. Experimental results show that the proposed
methods outperform earlier methods in term of sen-
tence reduction performance.
1 Introduction
The most popular methods of sentence reduc-
tion for text summarization are corpus based
methods. Jing (Jing 00) developed a method
to remove extraneous phrases from sentences
by using multiple sources of knowledge to de-
cide which phrases could be removed. However,
while this method exploits a simple model for
sentence reduction by using statistics computed
from a corpus, a better model can be obtained
by using a learning approach.
Knight and Marcu (Knight and Marcu 02)
proposed a corpus based sentence reduction
method using machine learning techniques.
They discussed a noisy-channel based approach
and a decision tree based approach to sentence
reduction. Their algorithms provide the best
way to scale up the full problem of sentence re-
duction using available data. However, these al-
gorithms require that the word order of a given
sentence and its reduced sentence are the same.
Nguyen and Horiguchi (Nguyen and Horiguchi
03) presented a new sentence reduction tech-
nique based on a decision tree model without
that constraint. They also indicated that se-
mantic information is useful for sentence reduc-
tion tasks.
The major drawback of previous works on
sentence reduction is that those methods are
likely to output local optimal results, which may
have lower accuracy. This problem is caused by
the inherent sentence reduction model; that is,
only a single reduced sentence can be obtained.
As pointed out by Lin (Lin 03), the best sen-
tence reduction output for a single sentence is
not approximately best for text summarization.
This means that ?local optimal? refers to the
best reduced output for a single sentence, while
the best reduced output for the whole text is
?global optimal?. Thus, it would be very valu-
able if the sentence reduction task could gener-
ate multiple reduced outputs and select the best
one using the whole text document. However,
such a sentence reduction method has not yet
been proposed.
Support Vector Machines (Vapnik 95), on the
other hand, are strong learning methods in com-
parison with decision tree learning and other
learning methods (Sholkopf 97). The goal of
this paper is to illustrate the potential of SVMs
for enhancing the accuracy of sentence reduc-
tion in comparison with previous work. Accord-
ingly, we describe a novel deterministic method
for sentence reduction using SVMs and a two-
stage method using pairwise coupling (Hastie
98). To solve the problem of generating mul-
tiple best outputs, we propose a probabilistic
sentence reduction model, in which a variant of
probabilistic SVMs using a two-stage method
with pairwise coupling is discussed.
The rest of this paper will be organized as
follows: Section 2 introduces the Support Vec-
tor Machines learning. Section 3 describes the
previous work on sentence reduction and our
deterministic sentence reduction using SVMs.
We also discuss remaining problems of deter-
ministic sentence reduction. Section 4 presents
a probabilistic sentence reduction method using
support vector machines to solve this problem.
Section 5 discusses implementation and our ex-
perimental results; Section 6 gives our conclu-
sions and describes some problems that remain
to be solved in the future.
2 Support Vector Machine
Support vector machine (SVM)(Vapnik 95) is a
technique of machine learning based on statisti-
cal learning theory. Suppose that we are given
l training examples (xi, yi), (1 ? i ? l), where
xi is a feature vector in n dimensional feature
space, yi is the class label {-1, +1 } of xi. SVM
finds a hyperplane w.x + b = 0 which correctly
separates the training examples and has a max-
imum margin which is the distance between two
hyperplanes w.x+ b ? 1 and w.x+ b ? ?1. The
optimal hyperplane with maximum margin can
be obtained by solving the following quadratic
programming.
min 12 ?w?+ C0
l?
i
?i
s.t. yi(w.xi + b) ? 1? ?i
?i ? 0
(1)
where C0 is the constant and ?i is a slack vari-
able for the non-separable case. In SVM, the
optimal hyperplane is formulated as follows:
f(x) = sign
( l?
1
?iyiK(xi, x) + b
)
(2)
where ?i is the Lagrange multiple, and
K(x?, x??) is a kernel function, the SVM calcu-
lates similarity between two arguments x? and
x??. For instance, the Polynomial kernel func-
tion is formulated as follow:
K(x?, x??) = (x?.x??)p (3)
SVMs estimate the label of an unknown ex-
ample x whether the sign of f(x) is positive or
not.
3 Deterministic Sentence Reduction
Using SVMs
3.1 Problem Description
In the corpus-based decision tree approach, a
given input sentence is parsed into a syntax tree
and the syntax tree is then transformed into a
small tree to obtain a reduced sentence.
Let t and s be syntax trees of the original sen-
tence and a reduced sentence, respectively. The
process of transforming syntax tree t to small
tree s is called ?rewriting process? (Knight and
Marcu 02), (Nguyen and Horiguchi 03). To
transform the syntax tree t to the syntax tree
s, some terms and five rewriting actions are de-
fined.
An Input list consists of a sequence of words
subsumed by the tree t where each word in the
Input list is labelled with the name of all syntac-
tic constituents in t. Let CSTACK be a stack
that consists of sub trees in order to rewrite a
small tree. Let RSTACK be a stack that con-
sists of sub trees which are removed from the
Input list in the rewriting process.
? SHIFT action transfers the first word from the
Input list into CSTACK. It is written mathe-
matically and given the label SHIFT.
? REDUCE(lk,X) action pops the lk syntactic
trees located at the top of CSTACK and com-
bines them in a new tree, where lk is an integer
and X is a grammar symbol.
? DROP X action moves subsequences of words
that correspond to syntactic constituents from
the Input list to RSTACK.
? ASSIGN TYPE X action changes the label of
trees at the top of the CSTACK. These POS
tags might be different from the POS tags in
the original sentence.
? RESTORE X action takes the X element in
RSTACK and moves it into the Input list,
where X is a subtree.
For convenience, let configuration be a status
of Input list, CSTACK and RSTACK. Let cur-
rent context be the important information in a
configuration. The important information are
defined as a vector of features using heuristic
methods as in (Knight and Marcu 02), (Nguyen
and Horiguchi 03).
The main idea behind deterministic sentence
reduction is that it uses a rule in the current
context of the initial configuration to select a
distinct action in order to rewrite an input sen-
tence into a reduced sentence. After that, the
current context is changed to a new context and
the rewriting process is repeated for selecting
an action that corresponds to the new context.
The rewriting process is finished when it meets
a termination condition. Here, one rule corre-
sponds to the function that maps the current
context to a rewriting action. These rules are
learned automatically from the corpus of long
sentences and their reduced sentences (Knight
and Marcu 02), (Nguyen and Horiguchi 03).
3.2 Example
Figure 1 shows an example of applying a se-
quence of actions to rewrite the input sentence
(a, b, c, d, e), when each character is a word. It
illustrates the structure of the Input list, two
stacks, and the term of a rewriting process based
on the actions mentioned above. For example,
in the first row, DROP H deletes the sub-tree
with its root node H in the Input list and stores
it in the RSTACK. The reduced tree s can be
obtained after applying a sequence of actions
as follows: DROP H; SHIFT; ASSIGN TYPE K;
DROP B; SHIFT; ASSIGN TYPE H; REDUCE 2
F; RESTORE H; SHIFT; ASSIGN TYPE D; RE-
DUCE 2G. In this example, the reduced sentence
is (b, e, a).
Figure 1: An Example of the Rewriting Process
3.3 Learning Reduction Rules Using
SVMs
As mentioned above, the action for each config-
uration can be decided by using a learning rule,
which maps a context to an action. To obtain
such rules, the configuration is represented by
a vector of features with a high dimension. Af-
ter that, we estimate the training examples by
using several support vector machines to deal
with the multiple classification problem in sen-
tence reduction.
3.3.1 Features
One important task in applying SVMs to text
summarization is to define features. Here, we
describe features used in our sentence reduction
models.
The features are extracted based on the cur-
rent context. As it can be seen in Figure 2, a
context includes the status of the Input list and
the status of CSTACK and RSTACK. We de-
fine a set of features for a current context as
described bellow.
Operation feature
The set of features as described in (Nguyen and
Horiguchi 03) are used in our sentence reduction
models.
Original tree features
These features denote the syntactic constituents
Figure 2: Example of Configuration
that start with the first unit in the Input list.
For example, in Figure 2 the syntactic con-
stituents are labels of the current element in the
Input list from ?VP? to the verb ?convince?.
Semantic features
The following features are used in our model as
semantic information.
? Semantic information about current words
within the Input list; these semantic types
are obtained by using the named entities such
as Location, Person, Organization and Time
within the input sentence. To define these
name entities, we use the method described in
(Borthwick 99).
? Semantic information about whether or not the
word in the Input list is a head word.
? Word relations, such as whether or not a word
has a relationship with other words in the sub-
categorization table. These relations and the
sub-categorization table are obtained using the
Commlex database (Macleod 95).
Using the semantic information, we are able to
avoid deleting important segments within the
given input sentence. For instance, the main
verb, the subject and the object are essential
and for the noun phrase, the head noun is essen-
tial, but an adjective modifier of the head noun
is not. For example, let us consider that the
verb ?convince? was extracted from the Com-
lex database as follows.
convince
NP-PP: PVAL (?of?)
NP-TO-INF-OC
This entry indicates that the verb ?convince?
can be followed by a noun phrase and a preposi-
tional phrase starting with the preposition ?of?.
It can be also followed by a noun phrase and a
to-infinite phrase. This information shows that
we cannot delete an ?of? prepositional phrase
or a to-infinitive that is the part of the verb
phrase.
3.3.2 Two-stage SVM Learning using
Pairwise Coupling
Using these features we can extract training
data for SVMs. Here, a sample in our training
data consists of pairs of a feature vector and
an action. The algorithm to extract training
data from the training corpus is modified using
the algorithm described in our pervious work
(Nguyen and Horiguchi 03).
Since the original support vector machine
(SVM) is a binary classification method, while
the sentence reduction problem is formulated as
multiple classification, we have to find a method
to adapt support vector machines to this prob-
lem. For multi-class SVMs, one can use strate-
gies such as one-vs all, pairwise comparison or
DAG graph (Hsu 02). In this paper, we use the
pairwise strategy, which constructs a rule for
discriminating pairs of classes and then selects
the class with the most winning among two class
decisions.
To boost the training time and the sentence
reduction performance, we propose a two-stage
SVM described below.
Suppose that the examples in training data
are divided into five groups m1,m2, ...,m5 ac-
cording to their actions. Let Svmc be multi-
class SVMs and let Svmc-i be multi-class SVMs
for a group mi. We use one Svmc classifier to
identify the group to which a given context e
should be belong. Assume that e belongs to
the group mi. The classifier Svmc-i is then used
to recognize a specific action for the context e.
The five classifiers Svmc-1, Svmc-2,..., Svmc-5
are trained by using those examples which have
actions belonging to SHIFT, REDUCE, DROP,
ASSIGN TYPE and RESTORE.
Table 1 shows the distribution of examples in
five data groups.
3.4 Disadvantage of Deterministic
Sentence Reductions
The idea of the deterministic algorithm is to
use the rule for each current context to select
the next action, and so on. The process termi-
nates when a stop condition is met. If the early
steps of this algorithm fail to select the best ac-
Table 1: Distribution of example data on five
data groups
Name Number of examples
SHIFT-GROUP 13,363
REDUCE-GROUP 11,406
DROP-GROUP 4,216
ASSIGN-GROUP 13,363
RESTORE-GROUP 2,004
TOTAL 44,352
tions, then the possibility of obtaining a wrong
reduced output becomes high.
One way to solve this problem is to select mul-
tiple actions that correspond to the context at
each step in the rewriting process. However,
the question that emerges here is how to deter-
mine which criteria to use in selecting multiple
actions for a context. If this problem can be
solved, then multiple best reduced outputs can
be obtained for each input sentence and the best
one will be selected by using the whole text doc-
ument.
In the next section propose a model for se-
lecting multiple actions for a context in sentence
reduction as a probabilistic sentence reduction
and present a variant of probabilistic sentence
reduction.
4 Probabilistic Sentence Reduction
Using SVM
4.1 The Probabilistic SVM Models
Let A be a set of k actions A =
{a1, a2...ai, ..., ak} and C be a set of n con-
texts C = {c1, c2...ci, ..., cn} . A probabilistic
model ? for sentence reduction will select an
action a ? A for the context c with probability
p?(a|c). The p?(a|c) can be used to score ac-
tion a among possible actions A depending the
context c that is available at the time of deci-
sion. There are several methods for estimating
such scores; we have called these ?probabilistic
sentence reduction methods?. The conditional
probability p?(a|c) is estimated using a variant
of probabilistic support vector machine, which
is described in the following sections.
4.1.1 Probabilistic SVMs using
Pairwise Coupling
For convenience, we denote uij = p(a = ai|a =
ai?aj , c). Given a context c and an action a, we
assume that the estimated pairwise class prob-
abilities rij of uij are available. Here rij can
be estimated by some binary classifiers. For
instance, we could estimate rij by using the
SVM binary posterior probabilities as described
in (Plat 2000). Then, the goal is to estimate
{pi}ki=1 , where pi = p(a = ai|c), i = 1, 2, ..., k.For this propose, a simple estimate of these
probabilities can be derived using the following
voting method:
pi = 2
?
j:j 6=i
I{rij>rji}/k(k ? 1)
where I is an indicator function and k(k? 1) is
the number of pairwise classes. However, this
model is too simple; we can obtain a better one
with the following method.
Assume that uij are pairwise probabilities of
the model subject to the condition that uij =
pi/(pi+pj). In (Hastie 98), the authors proposed
to minimize the Kullback-Leibler (KL) distance
between the rij and uij
l(p) =
?
i 6=j
nijrij log rijuij (4)
where rij and uij are the probabilities of a pair-
wise ai and aj in the estimated model and in
our model, respectively, and nij is the number
of training data in which their classes are ai or
aj . To find the minimizer of equation (6), they
first calculate
?l(p)
?pi =
?
i 6=j
nij(?rijpi +
1
pi + pj ).
Thus, letting ?l(p) = 0, they proposed to find
a point satisfying
?
j:j 6=i
nijuij =
?
j:j 6=i
nijrij ,
k?
i=1
pi = 1,
where i = 1, 2, ...k and pi > 0.
Such a point can be obtained by using an algo-
rithm described elsewhere in (Hastie 98). We
applied it to obtain a probabilistic SVM model
for sentence reduction using a simple method as
follows. Assume that our class labels belong to
l groups: M = {m1,m2...mi, ...,ml} , where l
is a number of groups and mi is a group e.g.,
SHIFT, REDUCE ,..., ASSIGN TYPE. Then
the probability p(a|c) of an action a for a given
context c can be estimated as follows.
p(a|c) = p(mi|c)? p(a|c,mi) (5)
where mi is a group and a ? mi. Here, p(mi|c)
and p(a|c,mi) are estimated by the method in
(Hastie 98).
4.2 Probabilistic sentence reduction
algorithm
After obtaining a probabilistic model p, we then
use this model to define function score, by which
the search procedure ranks the derivation of in-
complete and complete reduced sentences. Let
d(s) = {a1, a2, ...ad} be the derivation of a small
tree s, where each action ai belongs to a set of
possible actions. The score of s is the product
of the conditional probabilities of the individual
actions in its derivation.
Score(s) =
?
ai?d(s)
p(ai|ci) (6)
where ci is the context in which ai was decided.
The search heuristic tries to find the best re-
duced tree s? as follows:
s? = argmax? ?? ?
s?tree(t)
Score(s) (7)
where tree(t) are all the complete reduced trees
from the tree t of the given long sentence. As-
sume that for each configuration the actions
{a1, a2, ...an} are sorted in decreasing order ac-
cording to p(ai|ci), in which ci is the context
of that configuration. Algorithm 1 shows a
probabilistic sentence reduction using the top
K-BFS search algorithm. This algorithm uses
a breadth-first search which does not expand
the entire frontier, but instead expands at most
the top K scoring incomplete configurations in
the frontier; it is terminated when it finds M
completed reduced sentences (CL is a list of re-
duced trees), or when all hypotheses have been
exhausted. A configuration is completed if and
only if the Input list is empty and there is one
tree in the CSTACK. Note that the function
get-context(hi, j) obtains the current context of
the jth configuration in hi, where hi is a heap at
step i. The function Insert(s,h) ensures that the
heap h is sorted according to the score of each
element in h. Essentially, in implementation we
can use a dictionary of contexts and actions ob-
served from the training data in order to reduce
the number of actions to explore for a current
context.
5 Experiments and Discussion
We used the same corpus as described in
(Knight and Marcu 02), which includes 1,067
pairs of sentences and their reductions. To
evaluate sentence reduction algorithms, we ran-
domly selected 32 pairs of sentences from our
parallel corpus, which is refered to as the test
corpus. The training corpus of 1,035 sentences
extracted 44,352 examples, in which each train-
ing example corresponds to an action. The
SVM tool, LibSVM (Chang 01) is applied to
train our model. The training examples were
Algorithm 1 A probabilistic sentence reduction
algorithm
1: CL={Empty};
i = 0; h0={ Initial configuration}
2: while |CL| < M do
3: if hi is empty then4: break;5: end if6: u =min(|hi|, K)7: for j = 1 to u do8: c=get-context(hi, j)
9: Select m so that
m?
i=1
p(ai|c) < Q is maximal
10: for l=1 to m do11: parameter=get-parameter(al);12: Obtain a new configuration s by performing action al
with parameter
13: if Complete(s) then
14: Insert(s, CL)
15: else16: Insert(s, hi+1)17: end if18: end for19: end for20: i = i + 1
21: end while
divided into SHIFT, REDUCE, DROP, RE-
STORE, and ASSIGN groups. To train our
support vector model in each group, we used
the pairwise method with the polynomial ker-
nel function, in which the parameter p in (3)
and the constant C0 in equation (1) are 2 and
0.0001, respectively.
The algorithms (Knight and Marcu 02) and
(Nguyen and Horiguchi 03) served as the base-
line1 and the baseline2 for comparison with the
proposed algorithms. Deterministic sentence re-
duction using SVM and probabilistic sentence
reduction were named as SVM-D and SVMP, re-
spectively. For convenience, the ten top reduced
outputs using SVMP were called SVMP-10. We
used the same evaluation method as described
in (Knight and Marcu 02) to compare the pro-
posed methods with previous methods. For this
experiment, we presented each original sentence
in the test corpus to three judges who are spe-
cialists in English, together with three sentence
reductions: the human generated reduction sen-
tence, the outputs of the proposed algorithms,
and the output of the baseline algorithms.
The judges were told that all outputs were
generated automatically. The order of the out-
puts was scrambled randomly across test cases.
The judges participated in two experiments. In
the first, they were asked to determine on a scale
from 1 to 10 how well the systems did with re-
spect to selecting the most important words in
the original sentence. In the second, they were
asked to determine the grammatical criteria of
reduced sentences.
Table 2 shows the results of English language
sentence reduction using a support vector ma-
chine compared with the baseline methods and
with human reduction. Table 2 shows compres-
sion rates, and mean and standard deviation re-
sults across all judges, for each algorithm. The
results show that the length of the reduced sen-
tences using decision trees is shorter than using
SVMs, and indicate that our new methods out-
perform the baseline algorithms in grammatical
and importance criteria. Table 2 shows that the
Table 2: Experiment results with Test Corpus
Method Comp Gramma Impo
Baseline1 57.19% 8.60? 2.8 7.18? 1.92
Baseline2 57.15% 8.60? 2.1 7.42? 1.90
SVM-D 57.65% 8.76? 1.2 7.53? 1.53
SVMP-10 57.51% 8.80? 1.3 7.74? 1.39
Human 64.00% 9.05? 0.3 8.50? 0.80
first 10 reduced sentences produced by SVMP-
10 (the SVM probabilistic model) obtained the
highest performances. We also compared the
computation time of sentence reduction using
support vector machine with that in previous
works. Table 3 shows that the computational
times for SVM-D and SVMP-10 are slower than
baseline, but it is acceptable for SVM-D.
Table 3: Computational times of performing re-
ductions on test-set. Average sentence length
was 21 words.
Method Computational times (sec)
Baseline1 138.25
SVM-D 212.46
SVMP-10 1030.25
We also investigated how sensitive the pro-
posed algorithms are with respect to the train-
ing data by carrying out the same experi-
ment on sentences of different genres. We
created the test corpus by selecting sentences
from the web-site of the Benton Foundation
(http://www.benton.org). The leading sen-
tences in each news article were selected as the
most relevant sentences to the summary of the
news. We obtained 32 leading long sentences
and 32 headlines for each item. The 32 sen-
tences are used as a second test for our methods.
We use a simple ranking criterion: the more the
words in the reduced sentence overlap with the
words in the headline, the more important the
sentence is. A sentence satisfying this criterion
is called a relevant candidate.
For a given sentence, we used a simple
method, namely SVMP-R to obtain a re-
duced sentence by selecting a relevant candi-
date among the ten top reduced outputs using
SVMP-10.
Table 4 depicts the experiment results for
the baseline methods, SVM-D, SVMP-R, and
SVMP-10. The results shows that, when ap-
plied to sentence of a different genre, the per-
formance of SVMP-10 degrades smoothly, while
the performance of the deterministic sentence
reductions (the baselines and SVM determinis-
tic) drops sharply. This indicates that the prob-
abilistic sentence reduction using support vector
machine is more stable.
Table 4 shows that the performance of
SVMP-10 is also close to the human reduction
outputs and is better than previous works. In
addition, SVMP-R outperforms the determin-
istic sentence reduction algorithms and the dif-
ferences between SVMP-R?s results and SVMP-
10 are small. This indicates that we can ob-
tain reduced sentences which are relevant to the
headline, while ensuring the grammatical and
the importance criteria compared to the origi-
nal sentences.
Table 4: Experiment results with Benton Cor-
pus
Method Comp Gramma Impo
Baseline1 54.14% 7.61? 2.10 6.74? 1.92
Baseline2 53.13% 7.72? 1.60 7.02? 1.90
SVM-D 56.64% 7.86? 1.20 7.23? 1.53
SVMP-R 58.31% 8.25? 1.30 7.54? 1.39
SVMP-10 57.62% 8.60? 1.32 7.71? 1.41
Human 64.00% 9.01? 0.25 8.40? 0.60
6 Conclusions
We have presented a new probabilistic sentence
reduction approach that enables a long sentence
to be rewritten into reduced sentences based on
support vector models. Our methods achieves
better performance when compared with earlier
methods. The proposed reduction approach can
generate multiple best outputs. Experimental
results showed that the top 10 reduced sentences
returned by the reduction process might yield
accuracies higher than previous work. We be-
lieve that a good ranking method might improve
the sentence reduction performance further in a
text.
References
A. Borthwick, ?A Maximum Entropy Approach
to Named Entity Recognition?, Ph.D the-
sis, Computer Science Department, New York
University (1999).
C.-C. Chang and C.-J. Lin, ?LIB-
SVM: a library for support vec-
tor machines?, Software available at
http://www.csie.ntu.edu.tw/ cjlin/libsvm.
H. Jing, ?Sentence reduction for automatic
text summarization?, In Proceedings of the
First Annual Meeting of the North Ameri-
can Chapter of the Association for Compu-
tational Linguistics NAACL-2000.
T.T. Hastie and R. Tibshirani, ?Classification
by pairwise coupling?, The Annals of Statis-
tics, 26(1): pp. 451-471, 1998.
C.-W. Hsu and C.-J. Lin, ?A comparison of
methods for multi-class support vector ma-
chines?, IEEE Transactions on Neural Net-
works, 13, pp. 415-425, 2002.
K. Knight and D. Marcu, ?Summarization be-
yond sentence extraction: A Probabilistic ap-
proach to sentence compression?, Artificial
Intelligence 139: pp. 91-107, 2002.
C.Y. Lin, ?Improving Summarization Perfor-
mance by Sentence Compression ? A Pi-
lot Study?, Proceedings of the Sixth Inter-
national Workshop on Information Retrieval
with Asian Languages, pp.1-8, 2003.
C. Macleod and R. Grishman, ?COMMLEX
syntax Reference Manual?; Proteus Project,
New York University (1995).
M.L. Nguyen and S. Horiguchi, ?A new sentence
reduction based on Decision tree model?,
Proceedings of 17th Pacific Asia Conference
on Language, Information and Computation,
pp. 290-297, 2003
V. Vapnik, ?The Natural of Statistical Learning
Theory?, New York: Springer-Verlag, 1995.
J. Platt,? Probabilistic outputs for support vec-
tor machines and comparison to regularized
likelihood methods,? in Advances in Large
Margin Classifiers, Cambridege, MA: MIT
Press, 2000.
B. Scholkopf et al ?Comparing Support Vec-
tor Machines with Gausian Kernels to Radius
Basis Function Classifers?, IEEE Trans. Sig-
nal Procesing, 45, pp. 2758-2765, 1997.
A Sentence Reduction Using Syntax Control
Nguyen Minh Le
The Graduate School of
Information Science JAIST
Ishikawa, 923-1292, Japan
nguyenml@jaist.ac.jp
Susumu Horiguchi
The Graduate School of
Information Science JAIST
Ishikawa, 923-1292, Japan
hori@jaist.ac.jp
Abstract
This paper present a method based on the
behavior of nonnative speaker for reduc-
tion sentence in foreign language. We
demonstrate an algorithm using seman-
tic information in order to produce two
reduced sentences in two difference lan-
guages and ensure both grammatical and
sentence meaning of the original sentence
in reduced sentences. In addition, the or-
ders of reduced sentences are able to be
different from original sentences.
1 Introduction
Most of the researches in automatic summarization
were focused on extraction or identifying the im-
portant clauses and sentences, paragraphs in texts
(Inderject Mani and Mark Maybury, 1999). How-
ever, when humans produce summaries of docu-
ments, they used to create new sentences that are
grammatical, that cohere with one another, and cap-
ture the most salient parts of information in the orig-
inal document. Sentence reduction is the problem to
remove some redundant words or some phrases from
the original sentence by creating a new sentence in
which the gist meaning of the original sentence was
unchanged.
Methods of sentence reduction have been used
in many applications. Grefenstette (G.Grefenstette,
1998) proposed removing phrases in sentences to
produce a telegraphic text that can be used to pro-
vide audio scanning services for the blind. Dolan
(S.H. Olivers and W.B.Dolan, 1999) proposed re-
moving clauses in sentences before indexing docu-
ment for information retrieval. Those methods re-
move phrases based on their syntactic categories but
not rely on the context of words, phrases and sen-
tences around. Without using that information can
be reduced the accuracy of sentence reduction prob-
lem. Mani and Maybury also present a process of
writing a reduced sentence by reversing the original
sentence with a set of revised rules to improve the
performance of summarization. (Inderject Mani and
Mark Maybury, 1999).
Jing and McKeown(H. Jing, 2000) studied a new
method to remove extraneous phrase from sentences
by using multiple source of knowledge to decide
which phrase in the sentences can be removed. The
multiple sources include syntactic knowledge, con-
text information and statistic computed from a cor-
pus that consists of examples written by human pro-
fessional. Their method prevented removing some
phrases that were relative to its context around and
produced a grammatical sentence.
Recently, Knight and Marcu(K.Knight and
D.Marcu, 2002) demonstrated two methods for sen-
tence compression problem, which are similar to
sentence reduction one. They devised both noisy-
channel and decision tree approach to the prob-
lem. The noisy-channel framework has been used
in many applications, including speech recognition,
machine translation, and information retrieval. The
decision tree approach has been used in parsing sen-
tence. (D. Magerman, 1995)(Ulf Hermijakob and
J.Mooney, 1997) to define the rhetorical of text doc-
uments (Daniel Marcu, 1999).
Most of the previous methods only produce a
short sentence whose word order is the same as that
of the original sentence, and in the same language,
e.g., English.
When nonnative speaker reduce a long sentence
in foreign language, they usually try to link the
meaning of words within the original sentence into
meanings in their language. In addition, in some
cases, the reduced sentence and the original sen-
tence had their word order are difference. Therefore,
two reduced sentences are performed by non-native
speaker, one is the reduced sentence in foreign lan-
guage and another is in their language.
Following the behavior of nonnative speaker, two
new requirements have been arisen for sentence re-
duction problem as follows:
1) The word order of the reduced sentence may dif-
ferent from the original sentence.
2) Two reduced sentences in two difference lan-
guages can be generated.
With the two new perspectives above, sentence re-
duction task are useful for many applications such
as: information retrieval, query text summarization
and especially cross-language information retrieval.
To satisfy these new requirements, we proposed a
new algorithm using semantic information to simu-
late the behavior of nonnative-speaker. The seman-
tic information obtained from the original sentence
will be integrated into the syntax tree through syntax
control. The remainder of this paper will be orga-
nized as follows: Section 2 demonstrated a method
using syntactic control to reduced sentences. Sec-
tion 3 shows implementation and experiments. Sec-
tion 4 gives some conclusions and remained prob-
lems to be solved in future.
2 Sentence reduction using syntax control
2.1 Formulation
Let E and V be two difference languages. Given a
long sentence e : e1, e2, ..., en in the language E.
The task of sentence reduction into two languages
E and V is to remove or replace some redundant
words in the sentence e to generate two new sen-
tences e?1, e?2, ..., e?m and v1, v2, ..., vk in language E
and V so that their gist meanings are unchanged.
In practice, we used English language as a source
language and the target language are in English and
Vietnamese. However, the reader should understand
that our method can apply for any pair of languages.
In the following part we present an algorithm of sen-
tence reduction using syntax control with rich se-
mantic information.
2.2 Sentence reduction algorithm
We present an algorithm based on a semantic parsing
in order to generate two short sentences into differ-
ence languages. There are three steps in a reduction
algorithm using syntax control. In the first step, the
input sentence e will be parsed into a syntax tree t
through a syntax parser.
In the second step, the syntax tree will be added
rich semantic information by using a semantic
parser, in which each node of the syntax tree is asso-
ciated with a specific syntax control. The final step is
a process of generating two deference sentences into
language E and V language from the syntax tree t
that has been annotated with rich semantic informa-
tion.
2.2.1 Syntax parsing
First, We parse a sentence into a syntax tree. Our
syntax parser locates the subject, object, and head
word within a sentence. It also recognizes phrase
verbs, cue phases or expressions in English sen-
tences. These are useful information to reduce sen-
tence. The Figure 2 explains the equivalent of our
grammar symbol with English grammar symbol.
Figure 1 shows an example of our syntax pars-
ing for the sentence ?Like FaceLift, much of ATM?s
screen performance depends on the underlying ap-
plication?.
To reduce the ambiguity, we design a syntactic pars-
ing base on grammar symbols, which classified in
detail. Part of speech of words was extended to cope
with the ambiguity problem. For example, in Figure
2, ?noun? was dived into ?private noun? and ?gen-
eral noun?.
The bilingual dictionary was built including about
200,000 words in English and its meaning in Viet-
namese. Each English word entry includes several
meanings in Vietnamese and each meaning was as-
sociated with a symbol meaning. The set of sym-
bol meanings in each word entry is defined by using
WordNet database.(C. Fellbaum, 1998) The dictio-
nary also contained several phrases, expressions in
Figure 1: An example of syntax tree of ?Like
FaceLift, much of ATM?s screen performance de-
pends on the underlying application?
English and its equivalent to Vietnamese.
2.2.2 Semantic parsing using syntax control
After producing a syntax tree with rich informa-
tion, we continue to apply a semantic parsing for that
syntax tree.
Let N be an internal node of the syntax tree t and N
has k children nodes: n1, n2, ...nk .
The node N based on semantic information from
its n children nodes to consider what the remained
part in the reducing sentence should be.
When parsing semantic for the syntax tree t, each
N must be used the information of children nodes
to define its information. We call that information is
semantic-information of the node N and define it as
N.sem . In addition, each semantic-information of
a given node N was mapped with a meaning in the
Figure 2: Example of symbol Equivalent
target language.
For convince, we define SI is a set of semantic-
information and assume that the jth semantic-
information of the node nj is nj [i].
To understand what the meaning of the node N
should be, we have to know the meaning of each
children node and know how to combine them into
meanings for the node N .
Figure 3: Syntax control
Figure 3 shows two choices for sequence mean-
ings of the node N in a reduction process .
It is easy for human to understand exactly which
meaning of ni should be and then decoding them as
objects to memorize. With this basic idea, we design
a control language to do this task.
The k children nodes n1, n2, ...nk are associated
with a set of a syntax control to conduct the reducing
sentence process. The node N and its children are
associated with a set of rules. To present the set of
rules we used a simple syntax of a control language
as follows:
1) Syntax to present the order of children nodes and
nodes to be removed.
2) Syntax to constraint each meaning of a children
node with meanings of other children nodes.
3) Syntax to combine sequence meanings into
one symbol meaning (this process called a inherit
process from the node N to its children).
A syntax rule control will be encoded as one-
generation rules and a set of condition rules so that
the generation rule has to satisfy. With a specifica-
tion condition rule, we can define its generation rule
directly.
Condition rule
A condition rule is formulated as follows: if
nj1 .sem = v1 ? nj2 .sem = v2... ? njm .sem = vm
then N.sem = v with v and vj ? SI
Generation rule
A generation rule is a sequence of symbols in order
to transfer the internal node N into the internal node
of a reduced sentence. We used two generation
rules, one for E and other one for V . Given a
sequence symbols g : g1g2...gm , in which gi is an
integer or a string. The equation gi = j means the
children node be remained at position j in the target
node. If gi = ?v1v2...vl?, we have that string will in
the children node ni of the target node.
Figure 1 shows a syntax tree of the input sentence:
?Much of ATM?s performance depends on the un-
derlying application.?. In this syntax tree, the syntax
rule:?S1=Bng-daucau Subj cdgt Bng-cuoicau? will
be used the syntax control bellow to reduce
< Con > default < /Con >
< Gen > 1 2 < /Gen >
The condition rule is ?default? mean the generation
rule is applied to any condition rule. The generation
rule be ?1 2? mean only the node (Subj) in the
index 1 and the node (cdgt) in the index 2 of the
rule ?S1=Bng-daucau Subj cdgt Bng-cuoicau? are
remained in the reduced sentence.
If the syntax control is changed to
< Con > Subj = HUMAN < /Con >
< Gen > 1 2 < /Gen >
This condition rule means that only the case the
semantic information in the children node ?Subj?
is ?HUMAN? the generation rule ?1 2? is applied
for reduction process. Using the default condition
rule the reduced sentences to be generated as
follows.
Original sentence: Like FaceLift, much of ATM?s
screen performance depends on the underlying
application.
Reduced sentence in English: Much of ATM?s per-
formance depends on the underlying application.
Reduced sentence in Vietnamese: Nhieu hieu suat
cua ATM phu thuoc vao nhung ung dung tiem an.
In order to generating reduced sentence in Viet-
namese language, the condition rule and generation
is also designed. This process is used the same way
as transfer translation method.
Because the gist meaning of a short sentence is un-
changed in comparing with the original sentence, the
gist meaning of a node after applying the syntax con-
trol will be unchanged. With this assumption, we
can reuse the syntax control for translating the origi-
nal sentence into other languages (English into Viet-
namese) for translating the reduced sentence. There-
fore, our sentence reduction program can produce
two reduced sentences in two difference languages.
Our semantic parsing used that set of rules to select
suitable rules for the current context. The problem
of selecting a set of suitable rules for the current con-
text of the current node N is to find the most likely
condition rule among the set of syntax control rules
that associated with it. Thus, semantic parsing using
syntax control problem can be described mathemat-
ically as follows:
Given a sequence of children nodes n1, n2, ..., nk
of a node N , each node ni consist of a list of mean-
ing, in which each meaning was associated with a
symbol meaning. The syntax rule for the node N
was associated with a set of condition rules. In ad-
dition, one condition rule is mapped with a specifi-
cation generation rule.
Find the most condition rules for that node se-
quences.
This problem can be solved by using a variant of the
Viterbi algorithm (A.J. Viterbi, 1967).
Firstly, we define each semantic-information of a
children node with all index condition rules. Sec-
ondly, we try to find all sequences that come from
the same condition rules.
Algorithm 1 A definition of condition rules algo-
rithm. FindRule(N )
Require: Input: N is a node
Ensure: A syntax control for a rule
{Initialization step:}
1: for i = 1 to k do
2: for j = 1 to Ki do
3: Set stack s[i]=all index rules in the set of
condition rules satisfy ni.sem = ni[j]
4: end for
5: for i = 1 to K1 do
6: Cost[0][i] = 1;
7: Back[0][i] = 0;
8: end for
9: end for
{Interaction step:}
10: for i = 1 to k do
11: for j=1 to Ki do
12: Cost[i][j] = maxCost[i ? 1][l] ?
V alue(s[i][j], s[i? 1][l]) with l = 1,Ki
Back[i][j]= all the index gave max
13: end for
14: end for
{Identification step:}
15: Set a list LS= all index rules gave max values
Cost[k][j] with j = 1,Kk.
16: Update all semantic-information of each condi-
tion rule in the list LS to node N .
17: Function Value (i, j)
Begin
If i==j return 2;
Else return 1;
End
After defining a set of semantic-information for
each internal node, we have a frame of semantic
parsing algorithm as shown in Algorithm 2. Our se-
mantic parsing using syntax control is fast because
of finding syntax control rule for each node tree is
applied dynamic programming.
Algorithm 2 Semantic parsing algorithm
Require: Given a syntax tree , a set of syntax con-
trol for each node of the syntax tree.
Ensure: a syntax tree with rich semantic informa-
tion
{SemanticParsingTree}
1: if N is leaf then
2: Update all symbol-meaning in word entry
3: else
4: FindRules(N);
5: end if{main procedure}
6: SemanticParsingNode(root);
2.2.3 Generation reduced sentences
The input of this process is a syntax tree which
associated with rich information after applying the
semantic parsing process. Browsing the syntax tree
following bottom-up process, in which, a node tree
can be generated a short sub-sentence by using the
corresponding generation rule. Because we have
two generation rules for each node tree, so we have
two reduced sentences in two difference languages.
3 Experiments and Discussion
3.1 Experiment Data
We used the same corpus(K.Knight and D.Marcu,
2002) with 1067 pair of sentence and its reduction.
We manually changed the order of some reduced
sentences in that corpus while keep their meaning.
We manually build a set of syntax control for that
corpus for our reduction algorithm using syntax con-
trol. The set of semantic symbols was described
such as, HUMAN, ANIMAL, THINGS, etc. We
make 100 pair of sentences with the order of a reduc-
tion sentence is different from its original sentence.
Afterward, those sentences are to be combined with
the corpus above in order to confirm that our method
can deal with the changeable word order problem.
3.2 Experiment Method
To evaluate our reduction algorithms, we randomly
selected 32 pair of sentences from our parallel
corpus, which will refer to as the Test corpus.
We used 1035 sentence pairs for training with
the reduction based decision tree algorithm. We
used test corpus to confirm that our methods us-
ing semantic-information will outperform than the
decision tree method without semantic-information
(K.Knight and D.Marcu, 2002). We presented each
original sentence in the test corpus to three judges
who are Vietnamese and specialize in English, to-
gether with three sentence reductions of it: The hu-
man generated reduction sentence, the outputs of the
sentence reduction based syntax control and the out-
put of the baseline algorithm. The judges were told
that all outputs were generated automatically. The
order of the outputs was scrambled randomly across
test cases. The judges participated in two experi-
ments. In the first experiment, they were asked to
determine on a scale from 1 to 10 how well the sys-
tems did with respect to selecting the most important
words in the original sentence. In the second exper-
iment, they were asked to determine on a scale from
1 to 10 how grammatical the outputs were. The out-
puts of our methods include both reduced sentences
in English and Vietnamese. In the third experiment,
we tested on the randomly of 32 sentences from 100
sentences whose had word order between input and
output are different.
3.3 Experiment Results
Using the first and the second experiment method,
we had two table results as follows.
Table 1: Experiment results with outputs in English
Method comp Grammatically Importance
Baseline 57.19 8.6? 2.8 7.18? 1.92
Syn.con 6.5 8.7? 1.2 7.3? 1.6
Human 53.33 9.05? 0.3 8.5? 0.8
Table 2: Experiment results with outputs in Viet-
namese
Method comp Grammatically Importance
Baseline x x x
Syn.con 67 6.5? 1.7 6? 1.3
Human 63 8.5? 0.3 8.7? 0.7
Using the third experiments method we achieved
a result to be shown in Table5.
Table 3: Experiment results with the changeable or-
der
Method comp Grammatically Importance
Baseline 56.2 7.4? 3.1 6.5? 1.3
Syn.con 66 8.4? 2.1 7.2? 1.7
Human 53.33 9.2? 0.3 8.5? 0.8
3.4 Discussion
Table 1 shows the compression of three reduc-
tion methods in comparing with human for English
language. The grammatically of semantic control
achieved a high results because we used the syntax
control from human expert. The sentence reduction
decision based is yielded a smallest result. We sus-
pect that the requirement of word order may affect
the grammatically. Table 1 and Table 3 also indi-
cates that our new method achieved the importance
of words are outperform than the baseline algorithm
due to semantic information. This was because our
method using semantic information to avoid deleting
important words. Following our point, the base line
method should integrate with semantic information
within the original sentence to enhance the accuracy.
Table 2 shows the outputs of our method into Viet-
namese language, the baseline method cannot gener-
ate the output into Vietnamese language. The syntax
control method achieved a good enough results in
both grammatically and importance aspects.
The comparison row in the Table 1 and the Table 2
also reported that the baseline yields a shorter output
than syntax control method.
Table 3 shows that when we selected randomly 32
sentence pairs from 100 pairs of sentences those had
words order between input and output are different,
we have the syntax method change a bit while the
baseline method achieved a low result. This is due
to the syntax control method using rule knowledge
based while the baseline was not able to learn with
that corpus that.
4 Conclusions
We have presented an algorithm that allows rewrit-
ing a long sentence into two reduced sentences in
two difference languages. We compared our meth-
ods with the other methods to show advantages as
well as limits of the method. We claim that the se-
mantic information of the original sentence through
using syntax control is very useful for sentence re-
duction problem.
We proposed a method for sentence reduction
using semantic information and syntactic parsing
so-called syntax control approach. Our method
achieved a higher accuracy and the outputted reduc-
tion sentences in two different languages e.g. En-
glish and Vietnamese. Thus, it is closed to the out-
puts of non-native speaker in reduction manner.
Investigate machine learning to generate syntax
control rules automatically from corpus available are
promising to enhance the accuracy of sentence re-
duction using syntax control .
Acknowledgements
We would like to thank to Dr. Daniel Marcus about
the data corpus for sentence reduction task. This re-
search was supported in part by the international re-
search project grant, JAIST.
References
Indeject Mani and Mark Maybury. 1999. Advances in
Automatic Text Summarization. The MIT press.
G.Grefenstette. 1998. Producing intelligent telegraphic
text reduction to provide an audio scanning service for
the blind. In Working notes of the AAAI Spring Sym-
posium on Intelligent Text summarization, pp.111-118.
S.H.Olivers and W.B.Dolan. 1999. Less is more; elimi-
nating index terms from subordinate clauses. In Pro-
ceedings of the 37th Annual Meeting of the Association
for Computational Linguistic, pp.349-356.
H.Jing. 2000. Sentence reduction for automatic text
summarization. In Proceeding of the First Annual
Meeting of the North American Chapter of the Asso-
ciation for Computational Linguistics NAACL-2000.
Kevin Knight and Daniel Marcu. 2002. Summarization
beyond sentence extraction: A Probabilistic approach
to sentence compression. Artificial Intelligent, 139:
91?107.
D. Magerman. 1995. Statistical decision tree models for
parsing. In Proceedings of the 33rd Annual Meeting of
the Association for Computational Linguistic, pp.276-
283.
Ulf Hermijakob and Raymond J. Mooney. 1997. Learn-
ing parse and translation decision from examples with
rich context. In Proceeding of ACL/EACL?97, pp 482-
489.
Daniel Marcu. 1999. A decision- based approach to
Rhetorical parsing. In Proc. Of ACL?99, pp.365-372.
C.Fellbaum. 1998. WORDNET: An Electronic Lexical
Database. The Mit Press.
A.J.Viterbi. 1967. Error bounds for convolution codes
and an asymptotically optimal decoding algorithm.
IEEE Trans on Information Theory,13: 260?269.
