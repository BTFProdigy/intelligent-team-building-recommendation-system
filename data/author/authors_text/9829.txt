Proceedings of NAACL HLT 2009: Short Papers, pages 161?164,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Exploiting Named Entity Classes in CCG Surface Realization
Rajakrishnan Rajkumar Michael White
Department of Linguistics
The Ohio State University
Columbus, OH, USA
{raja,mwhite,espinosa}@ling.osu.edu
Dominic Espinosa
Abstract
This paper describes how named entity (NE)
classes can be used to improve broad cover-
age surface realization with the OpenCCG re-
alizer. Our experiments indicate that collaps-
ing certain multi-word NEs and interpolating
a language model where NEs are replaced by
their class labels yields the largest quality in-
crease, with 4-grams adding a small additional
boost. Substantial further benefit is obtained
by including class information in the hyper-
tagging (supertagging for realization) compo-
nent of the system, yielding a state-of-the-
art BLEU score of 0.8173 on Section 23 of
the CCGbank. A targeted manual evaluation
confirms that the BLEU score increase corre-
sponds to a significant rise in fluency.
1 Introduction
Hogan et al (2007) have recently shown that better
handling of named entities (NEs) in broad coverage
surface realization with LFG can lead to substan-
tial improvements in BLEU scores. In this paper,
we confirm that better NE handling can likewise im-
prove broad coverage surface realization with CCG,
even when employing a more restrictive notion of
named entities that better matches traditional real-
ization practice. Going beyond Hogan et al (2007),
we additionally show that NE classes can be used
to improve realization quality through better lan-
guage models and better hypertagging (supertagging
for realization) models, yielding a state-of-the-art
BLEU score of 0.8173 on Section 23 of the CCG-
bank.
A question addressed neither by Hogan et al
nor anyone else working on broad coverage surface
realization recently is whether reported increases
in BLEU scores actually correspond to observable
improvements in quality. We view this situation
as problematic, not only because Callison-Burch
et al (2006) have shown that BLEU does not al-
ways rank competing systems in accord with hu-
man judgments, but also because surface realiza-
tion scores are typically much higher than those in
MT?where BLEU?s performance has been repeat-
edly assessed?even when using just one reference.
Thus, in this paper, we present a targeted manual
evaluation confirming that our BLEU score increase
corresponds to a significant rise in fluency, a practice
we encourage others to adopt.
2 CCG Surface Realization
CCG (Steedman, 2000) is a unification-based cat-
egorial grammar formalism defined almost en-
tirely in terms of lexical entries that encode sub-
categorization as well as syntactic features (e.g.
number and agreement). OpenCCG is a pars-
ing/generation library which includes a hybrid
symbolic-statistical chart realizer (White, 2006). A
vital component of the realizer is the hypertagger
(Espinosa et al, 2008), which predicts lexical cat-
egory assignments using a maxent model trained on
contexts within a directed graph structure represent-
ing the logical form (LF) input; features and rela-
tions in the graph as well as parent child relation-
ships are the main features used to train the model.
The realizer takes as input an LF description (see
Figure 1 of Espinosa et al, 2008), but here we also
161
use LFs with class information on some elementary
predications (e.g. @x:MONEY($ 10,000)). Chart re-
alization proceeds in iterative beta-best fashion, with
a progressively wider hypertagger beam width. If no
complete realization is found within the time limit,
fragments are greedily assembled. Alternative real-
izations are ranked using integrated n-gram scoring;
n-gram models help in choosing word order and, to
a lesser extent, making lexical choices.
3 Collapsing Named Entities
An error analysis of the OpenCCG baseline output
reveals that out of 2331 NEs annotated by the BBN
corpus, 238 are not realized correctly. For exam-
ple, multi-word NPs like Texas Instruments Japan
Ltd. are realized as Japan Texas Instruments Ltd..
Inspired by Hogan et al?s (2007)?s Experiment 1,
we decided to use the BBN corpus NE annotation
(Weischedel and Brunstein, 2005) to collapse cer-
tain classes of NEs. But unlike their experiment
where all the NEs annotated by the BBN corpus are
collapsed, we chose to collapse into single tokens
only NEs whose exact form can be reasonably ex-
pected to be specified in the input to the realizer.
For example, while some quantificational or com-
paratives phrases like more than $ 10,000 are anno-
tated as MONEY in the BBN corpus, in our view
only $ 10,000 should be collapsed into an atomic
unit, with more than handled compositionally ac-
cording to the semantics assigned to it by the gram-
mar. Thus, after transferring the BBN annotations to
the CCGbank corpus, we (partially) collapsed NEs
which are CCGbank constituents according to the
following rules: (1) completely collapse the PER-
SON, ORGANIZATION, GPE, WORK OF ART
major class type entitites; (2) ignore phrases like
three decades later, which are annotated as DATE
entities; and (3) collapse all phrases with POS tags
CD or NNP(S) or lexical items % or $, ensuring that
all prototypical named entities are collapsed.
4 Exploiting NE Classes
Going beyond Hogan et al (2007) and collaps-
ing experiments, we also experiment with NE
classes in language models and hypertagging mod-
els. BBN annotates both major types and subtypes
(DATE:AGE, DATE:DATE etc). For all our experi-
ments, we use both of these.
4.1 Class replaced n-gram models
For both the original CCGbank as well as the col-
lapsed corpus, we created language model training
data with semantic classes replacing actual words,
in order to address data sparsity issues caused by
rare words in the same semantic class. For exam-
ple, in the collapsed corpus, the Section 00 sen-
tence Pierre Vinken , 61 years old , will join the
board as a nonexecutive director Nov. 29 . be-
comes PERSON , DATE:AGE DATE:AGE old ,
will join the ORG DESC:OTHER as a nonexecutive
PER DESC DATE:DATE DATE:DATE . During re-
alization, word forms are generated, but are then re-
placed by their semantic classes and scored using
the semantic class replaced n-gram model, similar
to (Oh and Rudnicky, 2002). As the specific words
may still matter, the class replaced model is interpo-
lated at the word level with an ordinary, word-based
language model, as well as with a factored language
model over POS tags and supertags.
4.2 Class features in hypertagging
We also experimented with a hypertagging model
trained over the collapsed corpus, where the seman-
tic classes of the elementary lexical predications,
along with the class features of their adjacent nodes,
are added as features.
5 Evaluation
5.1 Hypertagger evaluation
As Table 2 indicates, the hypertagging model does
worse in terms of per-logical predication accuracy
& per-whole-graph accuracy on the collapsed cor-
pus. To some extent this is not surprising, as collaps-
ing eliminates many easy tagging cases; however, a
full explanation is still under investigation. Note that
class information does improve performance some-
what on the collapsed corpus.
5.2 Realizer evaluation
For a both the original CCGbank and the col-
lapsed corpus, we extracted a section 02?21 lexico-
grammars and used it to derive LFs for the devel-
opment and test sections. We used the language
models in Table 1 to score realizations and for the
162
Condition Expansion
LM baseline-LM: word 3g+ pos 3g*stag 3g
HT baseline Hypertagger
LM4 LM with 4g word
LMC LM with class-rep model interpolated
LM4C LM with both
HTC HT with classes on nodes as extra feats
Table 1: Legend for Experimental Conditions
Corpus Condition Tags/pred Pred Graph
Uncollapsed HT 1.0 93.56% 39.14%
HT 1.5 98.28% 78.06%
Partly HT 1.0 92.22% 35.04%
Collapsed HTC 1.0 92.89% 38.31%
HT 1.5 97.87% 73.14%
HTC 1.5 98.02% 75.30%
Table 2: Hypertagger testing on Section 00 of the uncol-
lapsed corpus (1896 LFs & 38104 predicates) & partially
collapsed corpus (1895 LFs & 35370 predicates)
collapsed corpus, we also tried a class-based hyper-
tagging model. Hypertagger ?-values were set for
each corpus and for each hypertagging model such
that the predicted tags per pred was the same at each
level. BLEU scores were calculated after removing
the underscores between collapsed NEs.
5.3 Results
Our baseline results are much better than those pre-
viously reported with OpenCCG in large part due to
improved grammar engineering efforts and bug fix-
ing. Table 3 shows development set results which
indicate that collapsing appears to improve realiza-
tion on the whole, as evidenced by the small increase
in BLEU scores. The class-replaced word model
provides a big boost on the collapsed corpus, from
0.7917 to 0.7993, much more than 4-grams. Adding
semantic classes to the hypertagger improves its ac-
curacy and gives us another half BLEU point in-
crease. Standard test set results, reported in Table 4,
confirm the overall increase, from 0.7940 to 0.8173.
In analyzing the Section 00 results, we found that
with the collapsed corpus, NE errors were reduced
from 238 to 99, which explains why the BLEU
score increases despite the drop in exact matches and
grammatically complete realizations from the base-
line. A semi-automatic analysis reveals that most
of the corrections involve proper names that are no
longer mangled. Correct adjective ordering is also
achieved in some cases; for example, Dutch publish-
Corpus Condition %Exact %Complete BLEU
Uncollapsed LM+HT 29.27 84.02 0.7900
(98.6% LM4+HT 29.14 83.61 0.7899
coverage) LMC+HT 30.64 83.70 0.7937
LM4C+HT 30.85 83.65 0.7946
Partly collapsed LM+HT 28.28 82.48 0.7917
(98.6% LM4+HT 28.68 82.54 0.7929
coverage) LMC+HT 30.74 82.33 0.7993
LM4C+HT 31.06 82.33 0.7995
LM4C+HTC 32.01 83.17 0.8042
Table 3: Section 00 blind testing results
Condition %Exact %Complete BLEU
LM+HT 29.38 82.53 0.7940
LM4C+HTC 33.74 85.04 0.8173
Table 4: Section 23 results: LM+HT baseline on origi-
nal corpus (97.8% coverage), LM4C+HTC best case on
collapsed corpus (94.8% coverage)
ing group is enforced by the class-replaced models,
while all the other models realize this as publishing
Dutch group. Additionally, the class-replaced model
sometimes helps with animacy marking on relative
pronouns, as in Mr. Otero , who . . . instead of Mr.
Otero , which . . . . (Note that our input LFs do not
directly specify the choice of function words such
as case-marking prepositions, relative pronouns and
complementizers, and thus class-based scoring can
help to select the correct surface word form.)
5.4 Targeted manual evaluation
While the language models employing NE classes
certainly improve some examples, others are made
worse, and some are just changed to different, but
equally acceptable paraphrases. For this reason, we
carried out a targeted manual evaluation to confirm
the BLEU results.
5.4.1 Procedure
Along the lines of (Callison-Burch et al, 2006),
two native speakers (two of the authors) provided
ratings for a random sample of 49 realizations that
differed between the baseline and best conditions on
the collapsed corpus. Note that the selection pro-
cedure excludes exact matches and thus focuses on
sentences whose realization quality may be lower
on average than in an arbitrary sample. Sentences
were rated in the context of the preceding sentence
(if any) for both fluency and adequacy in compari-
son to the original sentence. The judges were not
163
LEU scoreB
 2
 2.5
 3
 3.5
 4
 4.5
 5
 0.66  0.68  0.7  0.72  0.74  0.76  0.78  0.8
AdequacyFluency
Baseline
BestH
um
an 
Sco
re
Figure 1: BLEU scores plotted against human judge-
ments of fluency and adequacy
aware of the condition (best/baseline) while doing
the rating. Ratings of the two judges were averaged
for each item.
5.4.2 Results
In the human evaluation, the best system?s mean
scores were 4.4 for adequacy and 3.61 for fluency,
compared with the baseline?s scores of 4.35 and 3.36
respectively. Figure 1 shows these results including
the standard error for each measurement, with the
BLEU scores for this specific test set. The sample
size was sufficient to show that the increase in flu-
ency from 3.36 to 3.61 represented a significant dif-
ference (paired t-test, 1-tailed, p = 0.015), while the
adequacy scores did not differ significantly.
5.4.3 Brief comparison to related systems
While direct comparisons cannot really be made
when inputs vary in their semantic depth and speci-
ficity, we observe that our all-sentences BLEU score
of 0.8173 exceeds that of Hogan et al (2007), who
report a top score of 0.6882 (though with coverage
near 100%). Nakanishi et al (2005) and Langkilde-
Geary (2002) report scores of 0.7733 and 0.7570, re-
spectively, though the former is limited to sentences
of length 20 or less, and the latter?s coverage is much
lower.
6 Conclusion and Future Work
In this paper, we have shown how named entity
classes can be used to improve the OpenCCG re-
alizer?s language models and hypertagging models,
helping to achieve a state-of-the-art BLEU score of
0.8173 on CCGbank Section 23. We have also con-
firmed the increase in quality through a targeted
manual evaluation, a practice we encourage others
working on surface realization to adopt. In future
work, we plan to investigate the unexpected drop in
hypertagger performance on our NE-collapsed cor-
pus, which we conjecture may be resolved by taking
advantage of Vadas and Curran?s (2008) corrections
to the CCGbank?s NP structures.
7 Acknowledgements
This work was supported in part by NSF IIS-
0812297 and by an allocation of computing time
from the Ohio Supercomputer Center. Our thanks
also to Josef Van Genabith, the OSU Clippers group
and the anonymous reviewers for helpful comments
and discussion.
References
Chris Callison-Burch, Miles Osborne, and Philipp
Koehn. 2006. Re-evaluating the role of BLEU in ma-
chine translation research. In Proc. EACL.
Dominic Espinosa, Michael White, and Dennis Mehay.
2008. Hypertagging: Supertagging for surface real-
ization with CCG. In Proc. ACL-08:HLT.
Deirdre Hogan, Conor Cafferkey, Aoife Cahill, and Josef
van Genabith. 2007. Exploiting multi-word units
in history-based probabilistic generation. In Proc.
EMNLP-CoNLL.
Irene Langkilde-Geary. 2002. An empirical verification
of coverage and correctness for a general-purpose sen-
tence generator. In Proc. INLG-02.
Hiroko Nakanishi, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic methods for disambiguation of an
HPSG-based chart generator. In Proc. IWPT-05.
Alice H. Oh and Alexander I. Rudnicky. 2002. Stochas-
tic natural language generation for spoken dialog sys-
tems. Computer, Speech & Language, 16(3/4):387?
407.
Mark Steedman. 2000. The syntactic process. MIT
Press, Cambridge, MA, USA.
David Vadas and James R. Curran. 2008. Parsing noun
phrase structure with CCG. In Proc. ACL-08:HLT.
Ralph Weischedel and Ada Brunstein. 2005. BBN pro-
noun coreference and entity type corpus. Technical
report, BBN.
Michael White. 2006. Efficient Realization of Coordi-
nate Structures in Combinatory Categorial Grammar.
Research on Language and Computation, 4(1):39?75.
164
Proceedings of ACL-08: HLT, pages 183?191,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Hypertagging: Supertagging for Surface Realization with CCG
Dominic Espinosa and Michael White and Dennis Mehay
Department of Linguistics
The Ohio State University
Columbus, OH, USA
{espinosa,mwhite,mehay}@ling.osu.edu
Abstract
In lexicalized grammatical formalisms, it is
possible to separate lexical category assign-
ment from the combinatory processes that
make use of such categories, such as pars-
ing and realization. We adapt techniques
from supertagging ? a relatively recent tech-
nique that performs complex lexical tagging
before full parsing (Bangalore and Joshi,
1999; Clark, 2002) ? for chart realization
in OpenCCG, an open-source NLP toolkit for
CCG. We call this approach hypertagging, as
it operates at a level ?above? the syntax, tag-
ging semantic representations with syntactic
lexical categories. Our results demonstrate
that a hypertagger-informed chart realizer can
achieve substantial improvements in realiza-
tion speed (being approximately twice as fast)
with superior realization quality.
1 Introduction
In lexicalized grammatical formalisms such as Lex-
icalized Tree Adjoining Grammar (Schabes et al,
1988, LTAG), Combinatory Categorial Grammar
(Steedman, 2000, CCG) and Head-Driven Phrase-
Structure Grammar (Pollard and Sag, 1994, HPSG),
it is possible to separate lexical category assign-
ment ? the assignment of informative syntactic cat-
egories to linguistic objects such as words or lex-
ical predicates ? from the combinatory processes
that make use of such categories ? such as pars-
ing and surface realization. One way of performing
lexical assignment is simply to hypothesize all pos-
sible lexical categories and then search for the best
combination thereof, as in the CCG parser in (Hock-
enmaier, 2003) or the chart realizer in (Carroll and
Oepen, 2005). A relatively recent technique for lex-
ical category assignment is supertagging (Bangalore
and Joshi, 1999), a preprocessing step to parsing that
assigns likely categories based on word and part-of-
speech (POS) contextual information. Supertagging
was dubbed ?almost parsing? by these authors, be-
cause an oracle supertagger left relatively little work
for their parser, while speeding up parse times con-
siderably. Supertagging has been more recently ex-
tended to a multitagging paradigm in CCG (Clark,
2002; Curran et al, 2006), leading to extremely ef-
ficient parsing with state-of-the-art dependency re-
covery (Clark and Curran, 2007).
We have adapted this multitagging approach to
lexical category assignment for realization using the
CCG-based natural language toolkit OpenCCG.1 In-
stead of basing category assignment on linear word
and POS context, however, we predict lexical cat-
egories based on contexts within a directed graph
structure representing the logical form (LF) of a
proposition to be realized. Assigned categories are
instantiated in OpenCCG?s chart realizer where, to-
gether with a treebank-derived syntactic grammar
(Hockenmaier and Steedman, 2007) and a factored
language model (Bilmes and Kirchhoff, 2003), they
constrain the English word-strings that are chosen to
express the LF. We have dubbed this approach hy-
pertagging, as it operates at a level ?above? the syn-
tax, moving from semantic representations to syn-
tactic categories.
We evaluate this hypertagger in two ways: first,
1http://openccg.sourceforge.net.
183
we evaluate it as a tagger, where the hypertagger
achieves high single-best (93.6%) and multitagging
labelling accuracies (95.8?99.4% with category per
lexical predication ratios ranging from 1.1 to 3.9).2
Second, we compare a hypertagger-augmented ver-
sion of OpenCCG?s chart realizer with the pre-
existing chart realizer (White et al, 2007) that sim-
ply instantiates the chart with all possible CCG cat-
egories (subject to frequency cutoffs) for each in-
put LF predicate. The hypertagger-seeded realizer
runs approximately twice as fast as the pre-existing
OpenCCG realizer and finds a larger number of
complete realizations, resorting less to chart frag-
ment assembly in order to produce an output within
a 15 second per-sentence time limit. Moreover, the
overall BLEU (Papineni et al, 2002) and METEOR
(Lavie and Agarwal, 2007) scores, as well as num-
bers of exact string matches (as measured against to
the original sentences in the CCGbank) are higher
for the hypertagger-seeded realizer than for the pre-
existing realizer.
This paper is structured as follows: Section 2 pro-
vides background on chart realization in OpenCCG
using a corpus-derived grammar. Section 3 de-
scribes our hypertagging approach and how it is in-
tegrated into the realizer. Section 4 describes our
results, followed by related work in Section 5 and
our conclusions in Section 6.
2 Background
2.1 Surface Realization with OpenCCG
The OpenCCG surface realizer is based on Steed-
man?s (2000) version of CCG elaborated with
Baldridge and Kruijff?s multi-modal extensions for
lexically specified derivation control (Baldridge,
2002; Baldridge and Kruijff, 2003) and hybrid
logic dependency semantics (Baldridge and Kruijff,
2002). OpenCCG implements a symbolic-statistical
chart realization algorithm (Kay, 1996; Carroll et al,
1999; White, 2006b) combining (1) a theoretically
grounded approach to syntax and semantic composi-
tion with (2) factored language models (Bilmes and
Kirchhoff, 2003) for making choices among the op-
tions left open by the grammar.
In OpenCCG, the search for complete realizations
2Note that the multitagger is ?correct? if the correct tag is
anywhere in the multitag set.
he h2
aa1
heh3
<Det>
<Arg0> <Arg1>
<TENSE>pres
<NUM>sg
<Arg0>
w1 want.01
m1
<Arg1>
<GenRel>
<Arg1>
<TENSE>pres
p1point
h1have.03
make.03
Figure 1: Semantic dependency graph from the CCGbank
for He has a point he wants to make [. . . ]
makes use of n-gram language models over words
represented as vectors of factors, including surface
form, part of speech, supertag and semantic class.
The search proceeds in one of two modes, anytime
or two-stage (packing/unpacking). In the anytime
mode, a best-first search is performed with a con-
figurable time limit: the scores assigned by the n-
gram model determine the order of the edges on
the agenda, and thus have an impact on realization
speed. In the two-stage mode, a packed forest of
all possible realizations is created in the first stage;
in the second stage, the packed representation is un-
packed in bottom-up fashion, with scores assigned
to the edge for each sign as it is unpacked, much
as in (Langkilde, 2000). Edges are grouped into
equivalence classes when they have the same syn-
tactic category and cover the same parts of the in-
put logical form. Pruning takes place within equiv-
alence classes of edges. Additionally, to realize a
wide range of paraphrases, OpenCCG implements
an algorithm for efficiently generating from disjunc-
tive logical forms (White, 2006a).
To illustrate the input to OpenCCG, consider the
semantic dependency graph in Figure 1, which is
taken from section 00 of a Propbank-enhanced ver-
sion of the CCGbank (Boxwell and White, 2008).
In the graph, each node has a lexical predica-
tion (e.g. make.03) and a set of semantic features
(e.g. ?NUM?sg); nodes are connected via depen-
dency relations (e.g. ?ARG0?). Internally, such
184
graphs are represented using Hybrid Logic Depen-
dency Semantics (HLDS), a dependency-based ap-
proach to representing linguistic meaning developed
by Baldridge and Kruijff (2002). In HLDS, hy-
brid logic (Blackburn, 2000) terms are used to de-
scribe dependency graphs. These graphs have been
suggested as representations for discourse structure,
and have their own underlying semantics (White,
2006b).
To more robustly support broad coverage surface
realization, OpenCCG has recently been enhanced
to greedily assemble fragments in the event that the
realizer fails to find a complete realization. The frag-
ment assembly algorithm begins with the edge for
the best partial realization, i.e. the one that covers
the most elementary predications in the input logi-
cal form, with ties broken according to the n-gram
score. (Larger fragments are preferred under the
assumption that they are more likely to be gram-
matical.) Next, the chart and agenda are greedily
searched for the best edge whose semantic coverage
is disjoint from those selected so far; this process re-
peats until no further edges can be added to the set
of selected fragments. In the final step, these frag-
ments are concatenated, again in a greedy fashion,
this time according to the n-gram score of the con-
catenated edges: starting with the original best edge,
the fragment whose concatenation on the left or right
side yields the highest score is chosen as the one to
concatenate next, until all the fragments have been
concatenated into a single output.
2.2 Realization from an Enhanced CCGbank
White et al (2007) describe an ongoing effort to en-
gineer a grammar from the CCGbank (Hockenmaier
and Steedman, 2007) ? a corpus of CCG deriva-
tions derived from the Penn Treebank ? suitable for
realization with OpenCCG. This process involves
converting the corpus to reflect more precise anal-
yses, where feasible, and adding semantic represen-
tations to the lexical categories. In the first step, the
derivations in the CCGbank are revised to reflect the
desired syntactic derivations. Changes to the deriva-
tions are necessary to reflect the lexicalized treat-
ment of coordination and punctuation assumed by
the multi-modal version of CCG that is implemented
in OpenCCG. Further changes are necessary to sup-
port semantic dependencies rather than surface syn-
tactic ones; in particular, the features and unifica-
tion constraints in the categories related to semanti-
cally empty function words such complementizers,
infinitival-to, expletive subjects, and case-marking
prepositions are adjusted to reflect their purely syn-
tactic status.
In the second step, a grammar is extracted from
the converted CCGbank and augmented with logi-
cal forms. Categories and unary type changing rules
(corresponding to zero morphemes) are sorted by
frequency and extracted if they meet the specified
frequency thresholds.
A separate transformation then uses around two
dozen generalized templates to add logical forms
to the categories, in a fashion reminiscent of (Bos,
2005). The effect of this transformation is illustrated
below. Example (1) shows how numbered seman-
tic roles, taken from PropBank (Palmer et al, 2005)
when available, are added to the category of an ac-
tive voice, past tense transitive verb, where *pred*
is a placeholder for the lexical predicate; examples
(2) and (3) show how more specific relations are in-
troduced in the category for determiners and the cat-
egory for the possessive ?s, respectively.
(1) s1 :dcl\np2/np3 =?
s1 :dcl,x1\np2 :x2/np3 :x3 : @x1(*pred* ?
?TENSE?pres ? ?ARG0?x2 ? ?ARG1?x3)
(2) np1/n1 =?
np1 :x1/n1 :x1 : @x1(?DET?(d ? *pred*))
(3) np1/n1\np2 =?
np1 :x1/n1 :x1\np2 :x2 : @x1(?GENOWN?x2)
After logical form insertion, the extracted and
augmented grammar is loaded and used to parse the
sentences in the CCGbank according to the gold-
standard derivation. If the derivation can be success-
fully followed, the parse yields a logical form which
is saved along with the corpus sentence in order to
later test the realizer. The algorithm for following
corpus derivations attempts to continue processing if
it encounters a blocked derivation due to sentence-
internal punctuation. While punctuation has been
partially reanalyzed to use lexical categories, many
problem cases remain due to the CCGbank?s re-
liance on punctuation-specific binary rules that are
not supported in OpenCCG.
185
Currently, the algorithm succeeds in creating log-
ical forms for 97.7% of the sentences in the devel-
opment section (Sect. 00) of the converted CCG-
bank, and 96.1% of the sentences in the test section
(Sect. 23). Of these, 76.6% of the development log-
ical forms are semantic dependency graphs with a
single root, while 76.7% of the test logical forms
have a single root. The remaining cases, with multi-
ple roots, are missing one or more dependencies re-
quired to form a fully connected graph. These miss-
ing dependencies usually reflect inadequacies in the
current logical form templates.
2.3 Factored Language Models
Following White et al (2007), we use factored tri-
gram models over words, part-of-speech tags and
supertags to score partial and complete realiza-
tions. The language models were created using the
SRILM toolkit (Stolcke, 2002) on the standard train-
ing sections (2?21) of the CCGbank, with sentence-
initial words (other than proper names) uncapital-
ized. While these models are considerably smaller
than the ones used in (Langkilde-Geary, 2002; Vell-
dal and Oepen, 2005), the training data does have
the advantage of being in the same domain and
genre (using larger n-gram models remains for fu-
ture investigation). The models employ interpolated
Kneser-Ney smoothing with the default frequency
cutoffs. The best performing model interpolates a
word trigrammodel with a trigrammodel that chains
a POS model with a supertag model, where the POS
model conditions on the previous two POS tags, and
the supertag model conditions on the previous two
POS tags as well as the current one.
Note that the use of supertags in the factored lan-
guage model to score possible realizations is distinct
from the prediction of supertags for lexical category
assignment: the former takes the words in the local
context into account (as in supertagging for parsing),
while the latter takes features of the logical form into
account. It is this latter process which we call hyper-
tagging, and to which we now turn.
3 The Approach
3.1 Lexical Smoothing and Search Errors
In White et al?s (2007) initial investigation of scal-
ing up OpenCCG for broad coverage realization,
test set grammar complete
oracle / best
dev (00) dev 49.1% / 47.8%
train 37.5% / 22.6%
Table 1: Percentage of complete realizations using an or-
acle n-gram model versus the best performing factored
language model.
all categories observed more often than a thresh-
old frequency were instantiated for lexical predi-
cates; for unseen words, a simple smoothing strategy
based on the part of speech was employed, assign-
ing the most frequent categories for the POS. This
approach turned out to suffer from a large number
of search errors, where the realizer failed to find a
complete realization before timing out even in cases
where the grammar supported one. To confirm that
search errors had become a significant issue, White
et al compared the percentage of complete realiza-
tions (versus fragmentary ones) with their top scor-
ing model against an oracle model that uses a simpli-
fied BLEU score based on the target string, which is
useful for regression testing as it guides the best-first
search to the reference sentence. The comparison
involved both a medium-sized (non-blind) grammar
derived from the development section and a large
grammar derived from the training sections (the lat-
ter with slightly higher thresholds). As shown in
Table 1, with the large grammar derived from the
training sections, many fewer complete realizations
are found (before timing out) using the factored lan-
guage model than are possible, as indicated by the
results of using the oracle model. By contrast, the
difference is small with the medium-sized grammar
derived from the development section. This result is
not surprising when one considers that a large num-
ber of common words are observed to have many
possible categories.
In the next section, we show that a supertag-
ger for CCG realization, or hypertagger, can reduce
the problem of search errors by focusing the search
space on the most likely lexical categories.
3.2 Maximum Entropy Hypertagging
As supertagging for parsing involves studying a
given input word and its local context, the concep-
186
tual equivalent for a lexical predicate in the LF is to
study a given node and its local graph structure. Our
implementation makes use of three general types of
features: lexicalized features, which are simply the
names of the parent and child elementary predica-
tion nodes, graph structural features, such as the
total number of edges emanating from a node, the
number of argument and non-argument dependents,
and the names of the relations of the dependent
nodes to the parent node, and syntactico-semantic
attributes of nodes, such as the tense and number.
For example, in the HLDS graph shown in Figure 1,
the node representing want has two dependents, and
the relational type of make with respect to want is
ARG1.
Clark (2002) notes in his parsing experiments that
the POS tags of the surrounding words are highly in-
formative. As discussed below, a significant gain in
hypertagging accuracy resulted from including fea-
tures sensitive to the POS tags of a node?s parent, the
node itself, and all of its arguments and modifiers.
Predicting these tags requires the use of a separate
POS tagger, which operates in a manner similar to
the hypertagger itself, though exploiting a slightly
different set of features (e.g., including features cor-
responding to the four-character prefixes and suf-
fixes of rare logical predication names). Follow-
ing the (word) supertagging experiments of (Cur-
ran et al, 2006) we assigned potentially multiple
POS tags to each elementary predication. The POS
tags assigned are all those that are some factor ?
of the highest ranked tag,3 giving an average of 1.1
POS tags per elementary predication. The values of
the corresponding feature functions are the POS tag
probabilities according to the POS tagger. At this
ambiguity level, the POS tagger is correct ? 92% of
the time.
Features for the hypertagger were extracted from
semantic dependency graphs extracted from sections
2 through 21 of the CCGbank. In total, 37,168
dependency graphs were derived from the corpus,
yielding 468,628 feature parameters.
The resulting contextual features and gold-
standard supertag for each predication were then
used to train a maximum entropy classifier model.
3I.e., all tags t whose probabilities p(t) ? ? ? p?, where p?
is the highest ranked tag?s probability.
Maximum entropy models describe a set of proba-
bility distributions of the form:
p(o | x) =
1
Z(x)
? exp
( n?
i=1
?ifi(o, x)
)
where o is an outcome, x is a context, the fi are
feature functions, the ?i are the respective weights
of the feature functions, and Z(x) is a normalizing
sum over all competing outcomes. More concretely,
given an elementary predication labeled want (as in
Figure 1), a feature function over this node could be:
f(o, x) =
{ 1, if o is (s[dcl]\np)/(s[adj]\np) and
number of LF dependents(x) = 2
0, otherwise.
We used Zhang Le?s maximum entropy toolkit4
for training the hypertagging model, which uses an
implementation of Limited-memory BFGS, an ap-
proximate quasi-Newton optimization method from
the numerical optimization literature (Liu and No-
cedal, 1989). Using L-BFGS allowed us to include
continuous feature function values where appropri-
ate (e.g., the probabilities of automatically-assigned
POS tags). We trained each hypertagging model to
275 iterations and our POS tagging model to 400 it-
erations. We used no feature frequency cut-offs, but
rather employed Gaussian priors with global vari-
ances of 100 and 75, respectively, for the hypertag-
ging and POS tagging models.
3.3 Iterative ?-Best Realization
During realization, the hypertagger serves to prob-
abilistically filter the categories assigned to an ele-
mentary predication, as well as to propose categories
for rare or unseen predicates. Given a predication,
the tagger returns a ?-best list of supertags in order
of decreasing probability. Increasing the number of
categories returned clearly increases the likelihood
that the most-correct supertag is among them, but at
a corresponding cost in chart size. Accordingly, the
hypertagger begins with a highly restrictive value for
?, and backs off to progressively less-restrictive val-
ues if no complete realization could be found using
the set of supertags returned. The search is restarted
4http://homepages.inf.ed.ac.uk/s0450736/
maxent toolkit.html.
187
Table 2: Hypertagger accuracy on Sections 00 and 23.
Results (in percentages) are for per-logical-predication
(PR) and per-whole-graph (GRPH) tagging accurcies.
Difference between best-only and baselines (b.l.) is sig-
nificant (p < 2 ? 10?16) by McNemar?s ?2 test.
Sect00 Sect23
? TagsPred PR GRPH PR GRPH
b.l. 1 1 68.7 1.8 68.7 2.3
b.l. 2 2 84.3 9.9 84.4 10.9
1.0 1 93.6 40.4 93.6 38.2
0.16 1.1 95.8 55.7 96.2 56.8
0.05 1.2 96.6 63.8 97.3 66.0
0.0058 1.5 97.9 74.8 98.3 76.9
1.75e-3 1.8 98.4 78.9 98.7 81.8
6.25e-4 2.2 98.7 82.5 99.0 84.3
1.25e-4 3.2 99.0 85.7 99.3 88.5
5.8e-5 3.9 99.1 87.2 99.4 89.9
from scratch with the next ? value, though in prin-
ciple the same chart could be expanded. The iter-
ative, ?-best search for a complete realization uses
the realizer?s packing mode, which can more quickly
determine whether a complete realization is possi-
ble. If the halfway point of the overall time limit
is reached with no complete realization, the search
switches to best-first mode, ultimately assembling
fragments if no complete realization can be found
during the remaining time.
4 Results and Discussion
Several experiments were performed in training and
applying the hypertagger. Three different models
were created using 1) non-lexicalized features only,
2) all features excluding POS tags, 3) all, 3) all
features except syntactico-semantic attributes such
as tense and number and 4) all features available.
Models trained on these feature subsets were tested
against one another on Section 00, and then the best
performing model was run on both Section 00 and
23.
4.1 Feature Ablation Testing
The the whole feature set was found in feature abla-
tion testing on the development set to outperform all
other feature subsets significantly (p < 2.2 ? 10?16).
These results listed in Table 3. As we can see, taking
Table 3: Hypertagger feature ablation testing results on
Section 00. The full feature set outperforms all others sig-
nificantly (p < 2.2 ? 10?16). Results for per-predication
(PR) and per-whole-graph (GRPH) tagging percentage
accuracies are listed. (Key: no-POS=no POS features;
no-attr=no syntactico-semantic attributes such as tense
and number; non-lex=non-lexicalized features only (no
predication names).
FEATURESET PR GRPH
full 93.6 40.37
no-POS 91.3 29.5
no-attr 91.8 31.2
non-lex 91.5 28.7
away any one class of features leads to drop in per-
predication tagging accuracy of at least 1.8% and a
drop per-whole-graph accuracy of at least 9.2%. As
expected from previous work in supertagging (for
parsing), POS features resulted in a large improve-
ment in overall accuracy (1.8%). Although the POS
tagger by itself is only 92% accurate (as a multi-
tagger of 1.1 POSword average ambiguity) ? well be-
low the state-of-the-art for the tagging of words ?
its predictions are still quite valuable to the hyper-
tagger.
4.2 Best Model Hypertagger Accuracy
The results for the full feature set on Sections 00
and 23 are outlined in Table 2. Included in this
table are accuracy data for a baseline dummy tag-
ger which simply assigns the most-frequently-seen
tag(s) for a given predication and backs off to the
overall most frequent tag(s) when confronted with
an unseen predication. The development set (00)
was used to tune the ? parameter to obtain reason-
able hypertag ambiguity levels; the model was not
otherwise tuned to it. The hypertagger achieves high
per-predication and whole-graph accuracies even at
small ambiguity levels.
4.3 Realizer Performance
Tables 4 and 5 show how the hypertagger improves
realization performance on the development and test
sections of the CCGbank. As Table 4 indicates, us-
ing the hypertagger in an iterative beta-best fash-
ion more than doubles the number of grammati-
cally complete realizations found within the time
188
Table 5: Realization quality metrics exact match, BLEU and METEOR, on complete realizations only and overall,
with and without hypertagger, on Sections 00 and 23.
Sec- Hyper- Complete Overall
tion tagger BLEU METEOR Exact BLEU METEOR
00 with 0.8137 0.9153 15.3% 0.6567 0.8494
w/o 0.6864 0.8585 11.3% 0.5902 0.8209
23 with 0.8149 0.9162 16.0% 0.6701 0.8557
w/o 0.6910 0.8606 12.3% 0.6022 0.8273
Table 4: Percentage of grammatically complete realiza-
tions, runtimes for complete realizations and overall run-
times, with and without hypertagger, on Sections 00 and
23.
Sec- Hyper- Percent Complete Overall
tion tagger Complete Time Time
00 with 47.4% 1.2s 4.5s
w/o 22.6% 8.7s 9.5s
23 with 48.5% 1.2s 4.4s
w/o 23.5% 8.9s 9.6s
limit; on the development set, this improvement eli-
mates more than the number of known search errors
(cf. Table 1). Additionally, by reducing the search
space, the hypertagger cuts overall realization times
by more than half, and in the cases where complete
realizations are found, realization times are reduced
by a factor of four, down to 1.2 seconds per sentence
on a desktop Linux PC.
Table 5 shows that increasing the number of com-
plete realizations also yields improved BLEU and
METEOR scores, as well as more exact matches. In
particular, the hypertagger makes possible a more
than 6-point improvement in the overall BLEU score
on both the development and test sections, and a
more than 12-point improvement on the sentences
with complete realizations.
As the effort to engineer a grammar suitable for
realization from the CCGbank proceeds in paral-
lel to our work on hypertagging, we expect the
hypertagger-seeded realizer to continue to improve,
since a more complete and precise extracted gram-
mar should enable more complete realizations to be
found, and richer semantic representations should
simplify the hypertagging task. Even with the cur-
rent incomplete set of semantic templates, the hy-
pertagger brings realizer performance roughly up to
state-of-the-art levels, as our overall test set BLEU
score (0.6701) slightly exceeds that of Cahill and
van Genabith (2006), though at a coverage of 96%
instead of 98%. We caution, however, that it remains
unclear how meaningful it is to directly compare
these scores when the realizer inputs vary consider-
ably in their specificity, as Langkilde-Geary?s (2002)
experiments dramatically illustrate.
5 Related Work
Our approach follows Langkilde-Geary (2002) and
Callaway (2003) in aiming to leverage the Penn
Treebank to develop a broad-coverage surface re-
alizer for English. However, while these earlier,
generation-only approaches made use of converters
for transforming the outputs of Treebank parsers to
inputs for realization, our approach instead employs
a shared bidirectional grammar, so that the input to
realization is guaranteed to be the same logical form
constructed by the parser. In this regard, our ap-
proach is more similar to the ones pursued more re-
cently by Carroll, Oepen and Velldal (2005; 2005;
2006), Nakanishi et al (2005) and Cahill and van
Genabith (2006) with HPSG and LFG grammars.
While we consider our approach to be the first to
employ a supertagger for realization, or hypertagger,
the approach is clearly reminiscent of the LTAG tree
models of Srinivas and Rambow (2000). The main
difference between the approaches is that ours con-
sists of a multitagging step followed by the bottom-
up construction of a realization chart, while theirs
involves the top-down selection of the single most
likely supertag for each node that is grammatically
189
compatible with the parent node, with the proba-
bility conditioned only on the child nodes. Note
that although their approach does involve a subse-
quent lattice construction step, it requires making
non-standard assumptions about the TAG; in con-
trast, ours follows the chart realization tradition of
working with the same operations of grammatical
combination as in parsing, including a well-defined
notion of semantic composition. Additionally, as
our tagger employs maximum entropy modeling, it
is able to take into account a greater variety of con-
textual features, including those derived from parent
nodes.
In comparison to other recent chart realization ap-
proaches, Nakanishi et al?s is similar to ours in that
it employs an iterative beam search, dynamically
changing the beam size in order to cope with the
large search space. However, their log-linear selec-
tion models have been adapted from ones used in
parsing, and do not condition choices based on fea-
tures of the input semantics to the same extent. In
particular, while they employ a baseline maximum
likelihood model that conditions the probability of
a lexical entry upon its predicate argument struc-
ture (PAS) ? that is, the set of elementary predi-
cations introduced by the lexical item ? this prob-
ability does not take into account other elements of
the local context, including parents and modifiers,
and their lexical predicates. Similarly, Cahill and
van Genabith condition the probability of their lex-
ical rules on the set of feature-value pairs linked to
the RHS of the rule, but do not take into account any
additional context. Since their probabilistic mod-
els involve independence assumptions like those in
a PCFG, and since they do not employ n-grams for
scoring alternative realizations, their approach only
keeps the single most likely edge in an equivalence
class, rather than packing them into a forest. Car-
roll, Oepen and Velldal?s approach is like Nakanishi
et al?s in that they adapt log-linear parsing models
to the realization task; however, they employ manu-
ally written grammars on much smaller corpora, and
perhaps for this reason they have not faced the need
to employ an iterative beam search.
6 Conclusion
We have introduced a novel type of supertagger,
which we have dubbed a hypertagger, that assigns
CCG category labels to elementary predications in
a structured semantic representation with high accu-
racy at several levels of tagging ambiguity in a fash-
ion reminiscent of (Bangalore and Rambow, 2000).
To our knowledge, we are the first to report tag-
ging results in the semantic-to-syntactic direction.
We have also shown that, by integrating this hy-
pertagger with a broad-coverage CCG chart real-
izer, considerably faster realization times are possi-
ble (approximately twice as fast as compared with
a realizer that performs simple lexical look-ups)
with higher BLEU, METEOR and exact string match
scores. Moreover, the hypertagger-augmented real-
izer finds more than twice the number of complete
realizations, and further analysis revealed that the
realization quality (as per modified BLEU and ME-
TEOR) is higher in the cases when the realizer finds
a complete realization. This suggests that further
improvements to the hypertagger will lead to more
complete realizations, hence more high-quality re-
alizations. Finally, further efforts to engineer a
grammar suitable for realization from the CCGbank
should provide richer feature sets, which, as our fea-
ture ablation study suggests, are useful for boosting
hypertagging performance, hence for finding better
and more complete realizations.
Acknowledgements
The authors thank the anonymous reviewers, Chris
Brew, Detmar Meurers and Eric Fosler-Lussier for
helpful comments and discussion.
References
Jason Baldridge and Geert-Jan Kruijff. 2002. Coupling
CCG and Hybrid Logic Dependency Semantics. In
Proc. ACL-02.
Jason Baldridge and Geert-Jan Kruijff. 2003. Multi-
Modal Combinatory Categorial Grammar. In Proc.
ACL-03.
Jason Baldridge. 2002. Lexically Specified Derivational
Control in Combinatory Categorial Grammar. Ph.D.
thesis, School of Informatics, University of Edinburgh.
Srinivas Bangalore and Aravind K. Joshi. 1999. Su-
190
pertagging: An Approach to Almost Parsing. Com-
putational Linguistics, 25(2):237?265.
Srinivas Bangalore and Owen Rambow. 2000. Exploit-
ing a probabilistic hierarchical model for generation.
In Proce. COLING-00.
Jeff Bilmes and Katrin Kirchhoff. 2003. Factored lan-
guage models and general parallelized backoff. In
Proc. HLT-03.
Patrick Blackburn. 2000. Representation, reasoning, and
relational structures: a hybrid logic manifesto. Logic
Journal of the IGPL, 8(3):339?625.
Johan Bos. 2005. Towards wide-coverage semantic in-
terpretation. In Proc. IWCS-6.
Stephen Boxwell and Michael White. 2008. Projecting
Propbank roles onto the CCGbank. In Proc. LREC-08.
To appear.
Aoife Cahill and Josef van Genabith. 2006. Robust
PCFG-based generation using automatically acquired
LFG approximations. In Proc. COLING-ACL ?06.
Charles Callaway. 2003. Evaluating coverage for large
symbolic NLG grammars. In Proc. IJCAI-03.
John Carroll and Stefan Oepen. 2005. High efficiency
realization for a wide-coverage unification grammar.
In Proc. IJCNLP-05.
John Carroll, Ann Copestake, Dan Flickinger, and Vic-
tor Poznan?ski. 1999. An efficient chart generator for
(semi-) lexicalist grammars. In Proc. ENLG-99.
Stephen Clark and James Curran. 2007. Wide-coverage
efficient statistical parsing with CCG and log-linear
models. Computational Linguistics, 33(4).
Stephen Clark. 2002. Supertagging for combinatory
categorial grammar. In Proceedings of the 6th Inter-
national Workshop on Tree Adjoining Grammars and
Related Frameworks (TAG+6), pages 19?24, Venice,
Italy.
James R. Curran, Stephen Clark, and David Vadas.
2006. Multi-tagging for lexicalized-grammar pars-
ing. In Proceedings of the Joint Conference of the
International Committee on Computational Linguis-
tics and the Association for Computational Linguis-
tics (COLING/ACL-06), pages 697?704, Sydney, Aus-
tralia.
Julia Hockenmaier and Mark Steedman. 2007. CCG-
bank: A Corpus of CCG Derivations and Dependency
Structures Extracted from the Penn Treebank. Com-
putational Linguistics, 33(3):355?396.
Julia Hockenmaier. 2003. Data and Models for Sta-
tistical Parsing with Combinatory Categorial Gram-
mar. Ph.D. thesis, University of Edinburgh, Edin-
burgh, Scotland.
Martin Kay. 1996. Chart generation. In Proc. ACL-96.
Irene Langkilde-Geary. 2002. An empirical verification
of coverage and correctness for a general-purpose sen-
tence generator. In Proc. INLG-02.
Irene Langkilde. 2000. Forest-based statistical sentence
generation. In Proc. NAACL-00.
Alon Lavie and Abhaya Agarwal. 2007. METEOR: An
automatic metric for MT evaluation with high levels
of correlation with human judgments. In Proceedings
of Workshop on Statistical Machine Translation at the
45th Annual Meeting of the Association of Computa-
tional Linguistics (ACL-2007), Prague.
D C Liu and Jorge Nocedal. 1989. On the limited mem-
ory method for large scale optimization. Mathematical
Programming B, 45(3).
Hiroko Nakanishi, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic methods for disambiguation of an
HPSG-based chart generator. In Proc. IWPT-05.
Martha Palmer, Dan Gildea, and Paul Kingsbury. 2005.
The proposition bank: A corpus annotated with se-
mantic roles. Computational Linguistics, 31(1).
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Computa-
tional Linguistics (ACL), Philadelphia, PA.
Carl J Pollard and Ivan A Sag. 1994. Head-Driven
Phrase Structure Grammar. University Of Chicago
Press.
Yves Schabes, Anne Abeille?, and Aravind K. Joshi.
1988. Parsing strategies with ?lexicalized? grammars:
Application to tree adjoining grammars. In Proceed-
ings of the 12th International Conference on Compu-
tational Linguistics (COLING-88), Budapest.
Mark Steedman. 2000. The Syntactic Process. MIT
Press, Cambridge, Massachusetts, USA.
Andreas Stolcke. 2002. SRILM ? An extensible lan-
guage modeling toolkit. In Proc. ICSLP-02.
Erik Velldal and Stephan Oepen. 2005. Maximum en-
tropy models for realization ranking. In Proc. MT
Summit X.
Erik Velldal and Stephan Oepen. 2006. Statistical rank-
ing in tactical generation. In Proceedings of the 2006
Conference on Empirical Methods in Natural Lan-
guage Processing, Sydney, Australia, July.
Michael White, Rajakrishnan Rajkumar, and Scott Mar-
tin. 2007. Towards broad coverage surface realiza-
tion with CCG. In Proc. of the Workshop on Using
Corpora for NLG: Language Generation and Machine
Translation (UCNLG+MT).
Michael White. 2006a. CCG chart realization from dis-
junctive inputs. In Proceedings, INLG 2006.
Michael White. 2006b. Efficient realization of coordi-
nate structures in Combinatory Categorial Grammar.
Research on Language and Computation, 4(1):39?75.
191
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 564?574,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Further Meta-Evaluation of Broad-Coverage Surface Realization
Dominic Espinosa and Rajakrishnan Rajkumar and Michael White and Shoshana Berleant
Department of Linguistics
The Ohio State University
Columbus, Ohio, USA
{espinosa,raja,mwhite,berleant}@ling.ohio-state.edu
Abstract
We present the first evaluation of the utility of
automatic evaluation metrics on surface real-
izations of Penn Treebank data. Using outputs
of the OpenCCG and XLE realizers, along
with ranked WordNet synonym substitutions,
we collected a corpus of generated surface re-
alizations. These outputs were then rated and
post-edited by human annotators. We eval-
uated the realizations using seven automatic
metrics, and analyzed correlations obtained
between the human judgments and the auto-
matic scores. In contrast to previous NLG
meta-evaluations, we find that several of the
metrics correlate moderately well with human
judgments of both adequacy and fluency, with
the TER family performing best overall. We
also find that all of the metrics correctly pre-
dict more than half of the significant system-
level differences, though none are correct in
all cases. We conclude with a discussion of the
implications for the utility of such metrics in
evaluating generation in the presence of varia-
tion. A further result of our research is a cor-
pus of post-edited realizations, which will be
made available to the research community.
1 Introduction and Background
In building surface-realization systems for natural
language generation, there is a need for reliable
automated metrics to evaluate the output. Unlike
in parsing, where there is usually a single gold-
standard parse for a sentence, in surface realization
there are usually many grammatically-acceptable
ways to express the same concept. This parallels
the task of evaluating machine-translation (MT) sys-
tems: for a given segment in the source language,
there are usually several acceptable translations into
the target language. As human evaluation of trans-
lation quality is time-consuming and expensive, a
number of automated metrics have been developed
to evaluate the quality of MT outputs. In this study,
we investigate whether the metrics developed for
MT evaluation tasks can be used to reliably evaluate
the outputs of surface realizers, and which of these
metrics are best suited to this task.
A number of surface realizers have been devel-
oped using the Penn Treebank (PTB), and BLEU
scores are often reported in the evaluations of these
systems. But how useful is BLEU in this con-
text? The original BLEU study (Papineni et al,
2001) scored MT outputs, which are of generally
lower quality than grammar-based surface realiza-
tions. Furthermore, even for MT systems, the
usefulness of BLEU has been called into question
(Callison-Burch et al, 2006). BLEU is designed to
work with multiple reference sentences, but in tree-
bank realization, there is only a single reference sen-
tence available for comparison.
A few other studies have investigated the use of
such metrics in evaluating the output of NLG sys-
tems, notably (Reiter and Belz, 2009) and (Stent et
al., 2005). The former examined the performance of
BLEU and ROUGE with computer-generated weather
reports, finding a moderate correlation with human
fluency judgments. The latter study applied sev-
eral MT metrics to paraphrase data from Barzilay
and Lee?s corpus-based system (Barzilay and Lee,
2003), and found moderate correlations with human
adequacy judgments, but little correlation with flu-
ency judgments. Cahill (2009) examined the perfor-
mance of six MT metrics (including BLEU) in evalu-
ating the output of a LFG-based surface realizer for
564
German, also finding only weak correlations with
the human judgments.
To study the usefulness of evaluation metrics such
as BLEU on the output of grammar-based surface
realizers used with the PTB, we assembled a cor-
pus of surface realizations from three different re-
alizers operating on Section 00 of the PTB. Two
human judges evaluated the adequacy and fluency
of each of the realizations with respect to the ref-
erence sentence. The realizations were then scored
with a number of automated evaluation metrics de-
veloped for machine translation. In order to investi-
gate the correlation of targeted metrics with human
evaluations, and gather other acceptable realizations
for future evaluations, the judges manually repaired
each unacceptable realization during the rating task.
In contrast to previous NLG meta-evaluations, we
found that several of the metrics correlate moder-
ately well with human judgments of both adequacy
and fluency, with the TER family performing best.
However, when looking at statistically significant
system-level differences in human judgments, we
found that some of the metrics get some of the rank-
ings correct, but none get them all correct, with dif-
ferent metrics making different ranking errors. This
suggests that multiple metrics should be routinely
consulted when comparing realizer systems.
Overall, our methodology is similar to that of
previous MT meta-evaluations, in that we collected
human judgments of system outputs, and com-
pared these scores with those assigned by auto-
matic metrics. A recent alternative approach to para-
phrase evaluation is ParaMetric (Callison-Burch et
al., 2008); however, it requires a corpus of annotated
(aligned) paraphrases (which does not yet exist for
PTB data), and is arguably focused more on para-
phrase analysis than paraphrase generation.
The plan of the paper is as follows: Section 2 dis-
cusses the preparation of the corpus of surface real-
izations. Section 3 describes the human evaluation
task and the automated metrics applied. Sections 4
and 5 present and discuss the results of these evalua-
tions. We conclude with some general observations
about automatic evaluation of surface realizers, and
some directions for further research.
2 Data Preparation
We collected realizations of the sentences in Sec-
tion 00 of the WSJ corpus from the following three
sources:
1. OpenCCG, a CCG-based chart realizer (White,
2006)
2. The XLE Generator, a LFG-based system de-
veloped by Xerox PARC (Crouch et al, 2008)
3. WordNet synonym substitutions, to investigate
how differences in lexical choice compare to
grammar-based variation.1
Although all three systems used Section 00 of
the PTB, they were applied with various parame-
ters (e.g., language models, multiple-output versus
single-output) and on different input structures. Ac-
cordingly, our study does not compare OpenCCG to
XLE, or either of these to the WordNet system.
2.1 OpenCCG realizations
OpenCCG is an open source parsing/realization
library with multimodal extensions to CCG
(Baldridge, 2002). The OpenCCG chart realizer
takes logical forms as input and produces strings
by combining signs for lexical items. Alternative
realizations are scored using integrated n-gram
and perceptron models. For robustness, fragments
are greedily assembled when necessary. Realiza-
tions were generated from 1,895 gold standard
logical forms, created by constrained parsing of
development-section derivations. The following
OpenCCG models (which differ essentially in the
way the output is ranked) were used:
1. Baseline 1: Output ranked by a trigram word
model
2. Baseline 2: Output ranked using three language
models (3-gram words + 3-gram words with
named entity class replacement + factored lan-
guage model of words, POS tags and CCG su-
pertags)
1Not strictly surface realizations, since they do not involve
an abstract input specification, but for simplicity we refer to
them as realizations throughout.
565
3. Baseline 3: Perceptron with syntax features and
the three LMs mentioned above
4. Perceptron full-model: n-best realizations
ranked using perceptron with syntax features
and the three n-gram models, as well as dis-
criminative n-grams
The perceptron model was trained on sections 02-
21 of the CCGbank, while a grammar extracted from
section 00-21 was used for realization. In addition,
oracle supertags were inserted into the chart during
realization. The purpose of such a non-blind test-
ing strategy was to evaluate the quality of the output
produced by the statistical ranking models in isola-
tion, rather than focusing on grammar coverage, and
avoid the problems associated with lexical smooth-
ing, i.e. lexical categories in the development sec-
tion not being present in the training section.
To enrich the variation in the generated realiza-
tions, dative-alternation was enforced during real-
ization by ensuring alternate lexical categories of the
verb in question, as in the following example:
(1) the executives gave [the chefs] [a stand-
ing ovation]
(2) the executives gave [a standing ovation]
[to the chefs]
2.2 XLE realizations
The corpus of realizations generated by the XLE
system contained 42,527 surface realizations of ap-
proximately 1,421 section 00 sentences (an aver-
age of 30 per sentence), initially unranked. The
LFG f-structures used as input to the XLE genera-
tor were derived from automatic parses, as described
in (Riezler et al, 2002). The realizations were
first tokenized using Penn Treebank conventions,
then ranked using perplexities calculated from the
same trigram word model used with OpenCCG. For
each sentence, the top 4 realizations were selected.
The XLE generator provides an interesting point
of comparison to OpenCCG as it uses a manually-
developed grammar with inputs that are less abstract
but potentially noisier, as they are derived from au-
tomatic parses rather than gold-standard ones.
2.3 WordNet synonymizer
To produce an additional source of variation, the
nouns and verbs of the sentences in section 00 of
the PTB were replaced with all of their WordNet
synonyms. Verb forms were generated using verb
stems, part-of-speech tags, and the morphg tool.2
These substituted outputs were then filtered using
the n-gram data which Google Inc. has made avail-
able.3 Those without any 5-gram matches centered
on the substituted word (or 3-gram matches, in the
case of short sentences) were eliminated.
3 Evaluation
From the data sources described in the previous sec-
tion, a corpus of realizations to be evaluated by the
human judges was constructed by randomly choos-
ing 305 sentences from section 00, then selecting
surface realizations of these sentences using the fol-
lowing algorithm:
1. Add OpenCCG?s best-scored realization.
2. Add other OpenCCG realizations until all four
models are represented, to a maximum of 4.
3. Add up to 4 realizations from either the XLE
system or the WordNet pool, chosen randomly.
The intent was to give reasonable coverage of all
realizer systems discussed in Section 2 without over-
loading the human judges. ?System? here means
any instantiation that emits surface realizations, in-
cluding various configurations of OpenCCG (using
different language models or ranking systems), and
these can be multiple-output, such as an n-best list,
or single-output (best-only, worst-only, etc.). Ac-
cordingly, more realizations were selected from the
OpenCCG realizer because 5 different systems were
being represented. Realizations were chosen ran-
domly, rather than according to sentence types or
other criteria, in order to produce a representative
sample of the corpus. In total, 2,114 realizations
were selected for evaluation.
2http://www.informatics.sussex.ac.uk/
research/groups/nlp/carroll/morph.html
3http://www.ldc.upenn.edu/Catalog/docs/
LDC2006T13/readme.txt
566
3.1 Human judgments
Two human judges evaluated each surface realiza-
tion on two criteria: adequacy, which represents the
extent to which the output conveys all and only the
meaning of the reference sentence; and fluency, the
extent to which it is grammatically acceptable. The
realizations were presented to the judges in sets con-
taining a reference sentence and the 1-8 outputs se-
lected for that sentence. To aid in the evaluation of
adequacy, one sentence each of leading and trailing
context were displayed. Judges used the guidelines
given in Figure 1, based on the scales developed
by the NIST Machine Translation Evaluation Work-
shop.
In addition to rating each realization on the two
five-point scales, each judge also repaired each out-
put which he or she did not judge to be fully ade-
quate and fluent. An example is shown in Figure 2.
These repairs resulted in new reference sentences for
a substantial number of sentences. These repaired
realizations were later used to calculate targeted ver-
sions of the evaluation metrics, i.e., using the re-
paired sentence as the reference sentence. Although
targeted metrics are not fully automatic, they are of
interest because they allow the evaluation algorithm
to focus on what is actually wrong with the input,
rather than all textual differences. Notably, targeted
TER (HTER) has been shown to be more consistent
with human judgments than human annotators are
with one another (Snover et al, 2006).
3.2 Automatic evaluation
The realizations were also evaluated using seven au-
tomatic metrics:
? IBM?s BLEU, which scores a hypothesis by
counting n-gram matches with the reference
sentence (Papineni et al, 2001), with smooth-
ing as described in (Lin and Och, 2004)
? The NIST n-gram evaluation metric, similar to
BLEU, but rewarding rarer n-gram matches, and
using a different length penalty
? METEOR, which measures the harmonic mean
of unigram precision and recall, with a higher
weight for recall (Banerjee and Lavie, 2005)
? TER (Translation Edit Rate), a measure of the
number of edits required to transform a hy-
pothesis sentence into the reference sentence
(Snover et al, 2006)
? TERP, an augmented version of TER which
performs phrasal substitutions, stemming, and
checks for synonyms, among other improve-
ments (Snover et al, 2009)
? TERPA, an instantiation of TERP with edit
weights optimized for correlation with ade-
quacy in MT evaluations
? GTM (General Text Matcher), a generaliza-
tion of the F-measure that rewards contiguous
matching spans (Turian et al, 2003)
Additionally, targeted versions of BLEU, ME-
TEOR, TER, and GTM were computed by using the
human-repaired outputs as the reference set. The
human repair was different from the reference sen-
tence in 193 cases (about 9% of the total), and we
expected this to result in better scores and correla-
tions with the human judgments overall.
4 Results
4.1 Human judgments
Table 1 summarizes the dataset, as well as the mean
adequacy and fluency scores garnered from the hu-
man evaluation. Overall adequacy and fluency judg-
ments were high (4.16, 3.63) for the realizer sys-
tems on average, and the best-rated realizer systems
achieved mean fluency scores above 4.
4.2 Inter-annotator agreement
Inter-annotator agreement was measured using the
?-coefficient, which is commonly used to measure
the extent to which annotators agree in category
judgment tasks. ? is defined as P (A)?P (E)1?P (E) , where
P (A) is the observed agreement between annota-
tors and P (E) is the probability of agreement due
to chance (Carletta, 1996). Chance agreement for
this data is calculated by the method discussed in
Carletta?s squib. However, in previous work in
MT meta-evaluation, Callison-Burch et al (2007),
assume the less strict criterion of uniform chance
agreement, i.e. 15 for a five-point scale. They also
567
Score Adequacy Fluency
5 All the meaning of the reference Perfectly grammatical
4 Most of the meaning Awkward or non-native; punctuation errors
3 Much of the meaning Agreement errors or minor syntactic problems
2 Meaning substantially different Major syntactic problems, such as missing words
1 Meaning completely different Completely ungrammatical
Figure 1: Rating scale and guidelines
Ref. It wasn?t clear how NL and Mr. Simmons would respond if Georgia Gulf spurns them again
Realiz. It weren?t clear how NL and Mr. Simmons would respond if Georgia Gulf again spurns them
Repair It wasn?t clear how NL and Mr. Simmons would respond if Georgia Gulf again spurns them
Figure 2: Example of repair
introduce the notion of ?relative? ?, which measures
how often two or more judges agreed that A > B,
A = B, or A < B for two outputs A and B, irre-
spective of the specific values given on the five-point
scale; here, uniform chance agreement is taken to be
1
3 . We report both absolute and relative ? in Table 2,
using actual chance agreement rather than uniform
chance agreement.
The ? scores of 0.60 for adequacy and 0.63 for flu-
ency across the entire dataset represent ?substantial?
agreement, according to the guidelines discussed in
(Landis and Koch, 1977), better than is typically
reported for machine translation evaluation tasks;
for example, Callison-Burch et al (2007) reported
?fair? agreement, with ? = 0.281 for fluency and
? = 0.307 for adequacy (relative). Assuming the
uniform chance agreement that the previously cited
work adopts, our inter-annotator agreements (both
absolute and relative) are still higher. This is likely
due to the generally high quality of the realizations
evaluated, leading to easier judgments.
4.3 Correlation with automatic evaluation
To determine how well the automatic evaluation
methods described in Section 3 correlate with the
human judgments, we averaged the human judg-
ments for adequacy and fluency, respectively, for
each of the rated realizations, and then computed
both Pearson?s correlation coefficient and Spear-
man?s rank correlation coefficient between these
scores and each of the metrics. Spearman?s corre-
lation makes fewer assumptions about the distribu-
tion of the data, but may not reflect a linear rela-
tionship that is actually present. Both are frequently
reported in the literature. Due to space constraints,
we show only Spearman?s correlation, although the
TER family scored slightly better on Pearson?s coef-
ficient, relatively.
The results for Spearman?s correlation are given
in Table 3. Additionally, the average scores for ad-
equacy and fluency were themselves averaged into
a single score, following (Snover et al, 2009), and
the Spearman?s correlation of each of the automatic
metrics with these scores are given in Table 4. All
reported correlations are significant at p < 0.001.
4.4 Bootstrap sampling of correlations
For each of the sub-corpora shown in Table 1, we
computed confidence intervals for the correlations
between adequacy and fluency human scores with
selected automatic metrics (BLEU, HBLEU, TER,
TERP, and HTER) as described in (Koenh, 2004). We
sampled each sub-corpus 1000 times with replace-
ment, and calculated correlations between the rank-
ings induced by the human scores and those induced
by the metrics for each reference sentence. We then
used these coefficients to estimate the confidence in-
terval, after excluding the top 25 and bottom 25 co-
efficients, following (Lin and Och, 2004). The re-
sults of this for the BLEU metric are shown in Table
5. We determined which correlations lay within the
95% confidence interval of the best performing met-
ric in each row of Table Table 3; these figures are
italicized.
568
5 Discussion
5.1 Human judgments of systems
The results for the four OpenCCG perceptron mod-
els mostly confirm those reported in (White and Ra-
jkumar, 2009), with one exception: the B-3 model
was below B-2, though the P-B (perceptron-best)
model still scored highest. This may have been due
to differences in the testing scenario. None of the
differences in adequacy scores among the individ-
ual systems are significant, with the exception of the
WordNet system. In this case, the lack of word-
sense disambiguation for the substituted words re-
sults in a poor overall adequacy score (e.g., wage
floor ? wage story). Conversely, it scores highest
for fluency, as substituting a noun or verb with a syn-
onym does not usually introduce ungrammaticality.
5.2 Correlations of human judgments with MT
metrics
Of the non-human-targeted metrics evaluated, BLEU
and TER/TERP demonstrate the highest correla-
tions with the human judgments of fluency (r =
0.62, 0.64). The TER family of evaluation metrics
have been observed to perform very well in MT-
evaluation tasks, and although the data evaluated
here differs from typical MT data in some impor-
tant ways, the correlation of TERP with the human
judgments is substantial. In contrast with previous
MT evaluations where TERP performs considerably
better than TER, these scored close to equal on our
data, possibly because TERP?s stem, synonym, and
paraphrase matching are less useful when most of
the variation is syntactic.
The correlations with BLEU and METEOR are
lower than those reported in (Callison-Burch et al,
2007); in that study, BLEU achieved adequacy and
fluency correlations of 0.690 and 0.722, respec-
tively, and METEOR achieved 0.701 and 0.719. The
correlations for these metrics might be expected to
be lower for our data, since overall quality is higher,
making the metrics? task more difficult as the out-
puts involve subtler differences between acceptable
and unacceptable variation.
The human-targeted metrics (represented by the
prefixed H in the data tables) correlated even more
strongly with the human judgments, compared to the
non-targeted versions. HTER demonstrated the best
correlation with realizer fluency (r = 0.75).
For several kinds of acceptable variation involv-
ing the rearrangement of constituents (such as da-
tive shift), TERP gives a more reasonable score than
BLEU, due to its ability to directly evaluate phrasal
shifts. The following realization was rated 4.5 for
fluency, and was more correctly ranked by TERP
than BLEU:
(3) Ref: The deal also gave Mitsui access to
a high-tech medical product.
(4) Realiz.: The deal also gave access to a
high-tech medical product to Mitsui.
For each reference sentence, we compared the
ranking of its realizations induced from the human
scores to the ranking induced from the TERP score,
and counted the rank errors by the latter, infor-
mally categorizing them by error type (see Table
7). In the 50 sentences with the highest numbers of
rank errors, 17 were affected by punctuation differ-
ences, typically involving variation in comma place-
ment. Human fluency judgments of outputs with
only punctuation problems were generally high, and
many realizations with commas inserted or removed
were rated fully fluent by the annotators. However,
TERP penalizes such insertions or deletions. Agree-
ment errors are another frequent source of rank-
ing errors for TERP. The human judges tended to
harshly penalize sentences with number-agreement
or tense errors, whereas TERP applies only a single
substitution penalty for each such error. We expect
that with suitable optimization of edit weights to
avoid over-penalizing punctuation shifts and under-
penalizing agreement errors, TERP would exhibit an
even stronger correlation with human fluency judg-
ments.
None of the evaluation metrics can distinguish an
acceptable movement of a word or constituent from
an unacceptable movement, with only one reference
sentence. A substantial source of error for both
TERP and BLEU is variation in adverbial placement,
as shown in (7).
Similar errors are seen with prepositional phrases
and some commonly-occurring temporal adverbs,
which typically admit a number of variations in
placement. Another important example of accept-
able variation which these metrics do not generally
rank correctly is dative alternation:
569
(7)
Ref. We need to clarify what exactly is wrong with it.
Realiz. Flu. TERP BLEU
We need to clarify exactly what is wrong with it. 5 0.1 0.5555
We need to clarify exactly what ?s wrong with it. 5 0.2 0.4046
We need to clarify what , exactly , is wrong with it. 5 0.2 0.5452
We need to clarify what is wrong with it exactly. 4.5 0.1 0.6756
We need to clarify what exactly , is wrong with it. 4 0.1 0.7017
We need to clarify what , exactly is wrong with it. 4 0.1 0.7017
We needs to clarify exactly what is wrong with it. 3 0.103 0.346
(5) Ref. When test booklets were passed
out 48 hours ahead of time, she says she
copied questions in the social studies sec-
tion and gave the answers to students.
(6) Realiz. When test booklets were passed
out 48 hours ahead of time , she says she
copied questions in the social studies sec-
tion and gave students the answers.
The correlations of each of the metrics with the
human judgments of fluency for the realizer systems
indicate at least a moderate relationship, in contrast
with the results reported in (Stent et al, 2005) for
paraphrase data, which found an inverse correlation
for fluency, and (Cahill, 2009) for the output of a sur-
face realizer for German, which found only a weak
correlation. However, the former study employed
a corpus-based paraphrase generation system rather
than grammar-driven surface realizers, and the re-
sulting paraphrases exhibited much broader varia-
tion. In Cahill?s study, the outputs of the realizer
were almost always grammatically correct, and the
automated evaluation metrics were ranking marked-
ness instead of grammatical acceptability.
5.3 System-level comparisons
In order to investigate the efficacy of the metrics
in ranking different realizer systems, or competing
realizations from the same system generated using
different ranking models, we considered seven dif-
ferent ?systems? from the whole dataset of realiza-
tions. These consisted of five OpenCCG-based re-
alizations (the best realization from three baseline
models, and the best and the worst realization from
the full perceptron model), and two XLE-based sys-
tems (the best and the worst realization, after rank-
ing the outputs of the XLE realizer with an n-gram
model). The mean of the combined adequacy and
fluency scores of each of these seven systems was
compared with that of every other system, result-
ing in 21 pairwise comparisons. Then Tukey?s HSD
test was performed to determine the systems which
differed significantly in terms of the average ade-
quacy and fluency rating they received.4 The test
revealed five pairwise comparisons where the scores
were significantly different.
Subsequently, for each of these systems, an over-
all system-level score for each of the MT metrics
was calculated. For the five pairwise comparisons
where the adequacy-fluency group means differed
significantly, we checked whether the metric ranked
the systems correctly. Table 8 shows the results of
a pairwise comparison between the ranking induced
by each evaluation metric, and the ranking induced
by the human judgments. Five of the seven non-
targeted metrics correctly rank more than half of the
systems. NIST, METEOR, and GTM get the most
comparisons right, but neither NIST nor GTM cor-
rectly rank the OpenCCG-baseline model 1 with re-
spect to the XLE-best model. TER and TERP get two
of the five comparisons correct, and they incorrectly
rank two of the five OpenCCG model comparisons,
as well as the comparison between the XLE-worst
and OpenCCG-best systems.
For the targeted metrics, HNIST is correct for all
five comparisons, while neither HBLEU nor HME-
TEOR correctly rank all the OpenCCG models. On
the other hand, HTER and HGTM incorrectly rank the
XLE-best system versus OpenCCG-based models.
In summary, some of the metrics get some of the
rankings correct, but none of the non-targeted met-
rics get al of them correct. Moreover, different met-
rics make different ranking errors. This argues for
4This particular test was chosen since it corrects for multiple
post-hoc analyses conducted on the same data-set.
570
the use of multiple metrics in comparing realizer
systems.
6 Conclusion
Our study suggests that although the task of evalu-
ating the output from realizer systems differs from
the task of evaluating machine translations, the au-
tomatic metrics used to evaluate MT outputs deliver
moderate correlations with combined human fluency
and adequacy scores when used on surface realiza-
tions. We also found that the MT-evaluation met-
rics are useful in evaluating different versions of the
same realizer system (e.g., the various OpenCCG re-
alization ranking models), and finding cases where
a system is performing poorly. As in MT-evaluation
tasks, human-targeted metrics have the highest cor-
relations with human judgments overall. These re-
sults suggest that the MT-evaluation metrics are use-
ful for developing surface realizers. However, the
correlations are lower than those reported for MT
data, suggesting that they should be used with cau-
tion, especially for cross-system evaluation, where
consulting multiple metrics may yield more reliable
comparisons. In our study, the targeted version of
TERP correlated most strongly with human judg-
ments of fluency.
In future work, the performance of the TER family
of metrics on this data might be improved by opti-
mizing the edit weights used in computing its scores,
so as to avoid over-penalizing punctuation move-
ments or under-penalizing agreement errors, both
of which were significant sources of ranking errors.
Multiple reference sentences may also help mitigate
these problems, and the corpus of human-repaired
realizations that has resulted from our study is a step
in this direction, as it provides multiple references
for some cases. We expect the corpus to also prove
useful for feature engineering and error analysis in
developing better realization models.5
Acknowledgements
We thank Aoife Cahill and Tracy King for providing
us with the output of the XLE generator. We also
thank Chris Callison-Burch and the anonymous re-
viewers for their helpful comments and suggestions.
5The corpus can be downloaded from http://www.
ling.ohio-state.edu/?mwhite/data/emnlp10/.
This material is based upon work supported by
the National Science Foundation under Grant No.
0812297.
References
Jason Baldridge. 2002. Lexically Specified Derivational
Control in Combinatory Categorial Grammar. Ph.D.
thesis, University of Edinburgh.
S. Banerjee and A. Lavie. 2005. METEOR: An auto-
matic metric for MT evaluation with improved corre-
lation with human judgments. In Proceedings of the
ACL Workshop on Intrinsic and Extrinsic Evaluation
Measures for Machine Translation and/or Summariza-
tion, pages 65?72.
R. Barzilay and L. Lee. 2003. Learning to paraphrase:
An unsupervised approach using multiple-sequence
alignment. In proceedings of HLT-NAACL, volume
2003, pages 16?23.
Aoife Cahill. 2009. Correlating human and automatic
evaluation of a german surface realiser. In Proceed-
ings of the ACL-IJCNLP 2009 Conference Short Pa-
pers, pages 97?100, Suntec, Singapore, August. Asso-
ciation for Computational Linguistics.
C. Callison-Burch, M. Osborne, and P. Koehn. 2006. Re-
evaluating the role of BLEU in machine translation re-
search. In Proceedings of EACL, volume 2006, pages
249?256.
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,
Christof Monz, and Josh Schroeder. 2007. (meta-)
evaluation of machine translation. In StatMT ?07: Pro-
ceedings of the Second Workshop on Statistical Ma-
chine Translation, pages 136?158, Morristown, NJ,
USA. Association for Computational Linguistics.
C. Callison-Burch, T. Cohn, and M. Lapata. 2008. Para-
metric: An automatic evaluation metric for paraphras-
ing. In Proceedings of the 22nd International Con-
ference on Computational Linguistics-Volume 1, pages
97?104. Association for Computational Linguistics.
J. Carletta. 1996. Assessing agreement on classification
tasks: the kappa statistic. Computational linguistics,
22(2):249?254.
Dick Crouch, Mary Dalrymple, Ron Kaplan, Tracy King,
John Maxwell, and Paula Newman. 2008. Xle docu-
mentation. Technical report, Palo Alto Research Cen-
ter.
Philip Koenh. 2004. Statistical significance tests for ma-
chine translation evaluation. In Proceedings of the
2009 Conference on Empirical Methods in Natural
Language Processing.
J.R. Landis and G.G. Koch. 1977. The measurement of
observer agreement for categorical data. Biometrics,
33(1):159?174.
571
Chin-Yew Lin and Franz Josef Och. 2004. Orange: a
method for evaluating automatic evaluation metrics for
machine translation. In COLING ?04: Proceedings
of the 20th international conference on Computational
Linguistics, page 501, Morristown, NJ, USA. Associ-
ation for Computational Linguistics.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2001.
Bleu: a method for automatic evaluation of machine
translation. Technical report, IBM Research.
E. Reiter and A. Belz. 2009. An investigation into the
validity of some metrics for automatically evaluating
natural language generation systems. Computational
Linguistics, 35(4):529?558.
Stefan Riezler, Tracy H. King, Ronald M. Kaplan,
Richard Crouch, John T. III Maxwell, and Mark John-
son. 2002. Parsing the wall street journal using
a lexical-functional grammar and discriminative esti-
mation techniques. In Proceedings of 40th Annual
Meeting of the Association for Computational Lin-
guistics, pages 271?278, Philadelphia, Pennsylvania,
USA, July. Association for Computational Linguistics.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea
Micciulla, and John Makhoul. 2006. A study of trans-
lation edit rate with targeted human annotation. In In
Proceedings of Association for Machine Translation in
the Americas, pages 223?231.
M. Snover, N. Madnani, B.J. Dorr, and R. Schwartz.
2009. Fluency, adequacy, or HTER?: exploring dif-
ferent human judgments with a tunable MT metric.
In Proceedings of the Fourth Workshop on Statistical
Machine Translation, pages 259?268. Association for
Computational Linguistics.
Amanda Stent, Matthew Marge, and Mohit Singhai.
2005. Evaluating evaluation methods for generation in
the presence of variation. In Proceedings of CICLing.
J.P. Turian, L. Shen, and I.D. Melamed. 2003. Evalua-
tion of machine translation and its evaluation. recall
(C? R), 100:2.
Michael White and Rajakrishnan Rajkumar. 2009. Per-
ceptron reranking for CCG realization. In Proceedings
of the 2009 Conference on Empirical Methods in Nat-
ural Language Processing, pages 410?419, Singapore,
August. Association for Computational Linguistics.
Michael White. 2006. Efficient Realization of Coordi-
nate Structures in Combinatory Categorial Grammar.
Research on Language and Computation, 4(1):39?75.
572
Type System #Refs #Paraphrases Average Paraphrases/Ref #Exact Matches Adq Flu
Single output OpenCCG Baseline 1 296 296 1.0 72 4.17 3.65
OpenCCG Baseline 2 296 296 1.0 82 4.34 3.94
OpenCCG Baseline 3 296 296 1.0 76 4.31 3.86
OpenCCG Perceptron Best 296 1.0 1.0 112 4.37 4.09
OpenCCG Perceptron Worst 117 117 1.0 5 4.34 3.36
XLE Best 154 154 1.0 24 4.41 4.07
XLE Worst 157 157 1.0 13 4.08 3.73
Multiple output OpenCCG-Perceptron All 296 767 2.6 158 4.45 3.91
OpenCCG All 296 1131 3.8 162 4.20 3.61
XLE All 174 557 3.2 54 4.17 3.81
Wordnet Subsitutions 162 486 3.0 0 3.66 4.71
Realizer All 296 1628 5.0 169 4.16 3.63
All 296 2114 7.1 169 4.05 3.88
Table 1: Descriptive statistics
System Adq Flu
p(A) p(E) ? p(A) p(E) ?
OpenCCG-Abs 0.73 0.47 0.48 0.70 0.24 0.61
OpenCCG-Rel 0.76 0.47 0.54 0.76 0.34 0.64
XLE-Abs 0.68 0.42 0.44 0.69 0.27 0.58
XLE-Rel 0.73 0.45 0.50 0.69 0.37 0.50
Wordnet-Abs 0.57 0.25 0.43 0.77 0.66 0.33
Wordnet-Rel 0.74 0.34 0.61 0.73 0.60 0.33
Realizer-Abs 0.70 0.44 0.47 0.69 0.24 0.59
Realizer-Rel 0.74 0.41 0.56 0.73 0.33 0.60
All-Abs 0.67 0.38 0.47 0.71 0.29 0.59
All-Rel 0.74 0.36 0.60 0.75 0.34 0.63
Table 2: Corpora-wise inter-annotator agreement (absolute and relative ? values shown)
Sys N B M G TP TA T HT HN HB HM HG
OpenCCG-Adq 0.27 0.39 0.35 0.18 0.39 0.34 0.4 0.43 0.3 0.43 0.43 0.23
OpenCCG-Flu 0.49 0.55 0.4 0.42 0.6 0.46 0.6 0.72 0.58 0.69 0.57 0.53
XLE-Adq 0.52 0.51 0.55 0.31 0.5 0.5 0.5 0.52 0.47 0.51 0.61 0.4
XLE-Flu 0.56 0.56 0.48 0.37 0.55 0.5 0.55 0.61 0.54 0.61 0.51 0.51
Wordnet-Adq 0.17 0.14 0.24 0.15 0.37 0.26 0.22 0.64 0.52 0.56 0.32 0.6
Wordnet-Flu 0.26 0.21 0.24 0.24 0.22 0.27 0.26 0.34 0.32 0.34 0.3 0.34
Realizer-Adq 0.47 0.6 0.57 0.42 0.59 0.57 0.6 0.62 0.49 0.62 0.65 0.48
Realizer-Flu 0.51 0.62 0.52 0.5 0.63 0.53 0.64 0.75 0.59 0.73 0.65 0.63
All-Adq 0.37 0.37 0.33 0.32 0.42 0.31 0.43 0.53 0.44 0.48 0.44 0.45
All-Flu 0.21 0.62 0.51 0.32 0.61 0.55 0.6 0.7 0.33 0.71 0.62 0.48
Table 3: Spearman?s correlations among NIST (N), BLEU (B), METEOR (M), GTM (G), TERp (TP), TERpa (TA),
TER (T), human variants (HN, HB, HM, HT, HG) and human judgments (-Adq: adequacy and -Flu: Fluency); Scores
which fall within the 95 %CI of the best are italicized.
Sys N B M G TP TA T HT HN HB HM HG
OpenCCG 0.49 0.57 0.42 0.4 0.61 0.46 0.62 0.73 0.58 0.7 0.59 0.51
XLE 0.63 0.64 0.59 0.39 0.62 0.58 0.63 0.69 0.6 0.68 0.63 0.54
Wordnet 0.21 0.14 0.21 0.19 0.38 0.25 0.23 0.65 0.56 0.57 0.31 0.63
Realizer 0.55 0.68 0.57 0.5 0.68 0.58 0.69 0.78 0.61 0.77 0.7 0.63
All 0.34 0.58 0.47 0.38 0.61 0.48 0.61 0.75 0.48 0.73 0.61 0.58
Table 4: Spearman?s correlations among NIST (N), BLEU (B), METEOR (M), GTM (G), TERp (TP), TERpa (TA),
TER (T), human variants (HN, HB, HM, HT, HG) and human judgments (combined adequacy and fluency scores)
573
System Adq Flu
Sp 95%L 95%U Sp 95%L 95%U
Realizer 0.60 0.58 0.63 0.62 0.59 0.65
XLE 0.51 0.47 0.56 0.56 0.51 0.61
OpenCCG 0.39 0.35 0.42 0.55 0.52 0.59
All 0.37 0.34 0.4 0.62 0.6 0.64
Wordnet 0.14 0.06 0.21 0.21 0.13 0.28
Table 5: Spearman?s correlation analysis (bootstrap sampling) of the BLEU scores of various systems with human
adequacy and fluency scores
Sys HJ N B M G TP TA T HT HN HB HM HG HJ1-HJ2
OpenCCG HJ-1 0.44 0.52 0.39 0.36 0.56 0.43 0.58 0.75 0.58 0.72 0.62 0.52 0.76
HJ-2 0.5 0.58 0.43 0.4 0.62 0.46 0.63 0.7 0.55 0.68 0.56 0.49
XLE HJ-1 0.6 0.6 0.55 0.37 0.57 0.55 0.58 0.69 0.63 0.68 0.64 0.54 0.75
HJ-2 0.6 0.6 0.56 0.39 0.6 0.55 0.61 0.64 0.54 0.61 0.57 0.51
Wordnet HJ-1 0.2 0.18 0.26 0.16 0.37 0.28 0.24 0.7 0.59 0.64 0.35 0.65 0.72
HJ-2 0.25 0.16 0.23 0.19 0.37 0.25 0.25 0.59 0.52 0.51 0.32 0.56
Realizer HJ-1 0.51 0.65 0.56 0.49 0.64 0.56 0.66 0.8 0.62 0.78 0.72 0.64 0.82
HJ-2 0.55 0.68 0.56 0.5 0.67 0.57 0.68 0.74 0.58 0.73 0.66 0.6
All HJ-1 0.32 0.53 0.45 0.37 0.57 0.44 0.57 0.77 0.5 0.74 0.62 0.59 0.79
HJ-2 0.35 0.58 0.46 0.37 0.61 0.47 0.6 0.71 0.44 0.69 0.57 0.54
Table 6: Spearman?s correlations of NIST (N), BLEU (B), METEOR (M), GTM (G), TERp (TP), TERpa (TA), human
variants (HT, HN, HB, HM, HG), and individual human judgments (combined adq. and flu. scores)
Factor Count
Punctuation 17
Adverbial shift 16
Agreement 14
Other shifts 8
Conjunct rearrangement 8
Complementizer ins/del 5
PP shift 4
Table 7: Factors influencing TERP ranking errors for 50 worst-ranked realization groups
Metric Score Errors
nist 4 C1-XB
bleu 3 XB-PW C1-XB
meteor 4 XW-PB
ter 2 PW-PB XW-PB C1-PB
terp 2 PW-PB XW-PB C1-PB
terpa 3 XW-PB C1-PB
gtm 4 C1-XB
hnist 5
hbleu 3 PW-PB XW-PB
hmeteor 2 PW-PB XW-PB C1-PB
hter 3 XB-PW C1-XB
hgtm 3 XB-PW C1-XB
Table 8: Metric-wise ranking performance in terms of agreement with a ranking induced by combined adequacy and
fluency scores; each metric gets a score out of 5 (i.e. number of system-level comparisons that emerged significant as
per the Tukey?s HSD test)
Legend: Perceptron Best (PB); Perceptron Worst (PW); XLE Best (XB); XLE Worst (XW); OpenCCG baseline mod-
els 1 to 3 (C1 ... C3)
574
