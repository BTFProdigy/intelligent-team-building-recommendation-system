Proceedings of the BioNLP Workshop on Linking Natural Language Processing and Biology at HLT-NAACL 06, pages 41?48,
New York City, June 2006. c?2006 Association for Computational Linguistics
Human Gene Name Normalization using Text Matching with Automatically
Extracted Synonym Dictionaries
Haw-ren Fang
Department of Computer Science, University of Maryland
College Park, MD 20742, USA
hrfang@cs.umd.edu
Kevin Murphy and Yang Jin and Jessica S. Kim and Peter S. White?
Division of Oncology, Children?s Hospital of Philadelphia
Philadelphia, PA 19104, USA
{murphy,jin,kim,white}@genome.chop.edu
Abstract
The identification of genes in biomedi-
cal text typically consists of two stages:
identifying gene mentions and normaliza-
tion of gene names. We have created an
automated process that takes the output
of named entity recognition (NER) sys-
tems designed to identify genes and nor-
malizes them to standard referents. The
system identifies human gene synonyms
from online databases to generate an ex-
tensive synonym lexicon. The lexicon is
then compared to a list of candidate gene
mentions using various string transforma-
tions that can be applied and chained in
a flexible order, followed by exact string
matching or approximate string matching.
Using a gold standard of MEDLINE ab-
stracts manually tagged and normalized
for mentions of human genes, a com-
bined tagging and normalization system
achieved 0.669 F-measure (0.718 preci-
sion and 0.626 recall) at the mention level,
and 0.901 F-measure (0.957 precision and
0.857 recall) at the document level for
documents used for tagger training.
1 Introduction
Gene and protein name identification and recogni-
tion in biomedical text are challenging problems.
A recent competition, BioCreAtIvE, highlighted the
? To whom correspondence should be addressed.
two tasks inherent in gene recognition: identifying
gene mentions in text (task 1A) (Yeh et al, 2005)
and normalizing an identified gene list (task 1B)
(Hirschman et al, 2005). This competition resulted
in many novel and useful approaches, but the results
clearly identified that more important work is neces-
sary, especially for normalization, the subject of the
current work.
Compared with gene NER, gene normalization
is syntactically easier because identification of the
textual boundaries of each mention is not required.
However, gene normalization poses significant se-
mantic challenges, as it requires detection of the ac-
tual gene intended, along with reporting of the gene
in a standardized form (Crim et al, 2005). Several
approaches have been proposed for gene normal-
ization, including classification techniques (Crim et
al., 2005; McDonald et al, 2004), rule-based sys-
tems (Hanisch et al, 2005), text matching with dic-
tionaries (Cohen, 2005), and combinations of these
approaches. Integrated systems for gene identifica-
tion typically have three stages: identifying candi-
date mentions in text, identifying the semantic in-
tent of each mention, and normalizing mentions by
associating each mention with a unique gene identi-
fier (Morgan et al, 2004). In our current work, we
focus upon normalization, which is currently under-
explored for human gene names. Our objective is
to create systems for automatically identifying hu-
man gene mentions with high accuracy that can be
used for practical tasks in biomedical literature re-
trieval and extraction. Our current approach relies
on a manually created and tuned set of rules.
41
2 Automatically Extracted Synonym
Dictionaries
Even when restricted to human genes, biomedical
researchers mention genes in a highly variable man-
ner, with a minimum of adherence to the gene nam-
ing standard provided by the Human Gene Nomen-
clature Committee (HGNC). In addition, frequent
variations in spelling and punctuation generate ad-
ditional non-standard forms. Extracting gene syn-
onyms automatically from online databases has sev-
eral benefits (Cohen, 2005). First, online databases
contain highly accurate annotations from expert
curators, and thus serve as excellent information
sources. Second, refreshing of specialized lexicons
from online sources provides a means to obtain new
information automatically and with no human in-
tervention. We thus sought a way to rapidly col-
lect as many human gene identifiers as possible.
All the statistics used in this section are from on-
line database holdings last extracted on February 20,
2006.
2.1 Building the Initial Dictionaries
Nineteen online websites and databases were ini-
tially surveyed to identify a set of resources that col-
lectively contain a large proportion of all known hu-
man gene identifiers. After examination of the 19 re-
sources with a limited but representative set of gene
names, we determined that only four databases to-
gether contained all identifiers (excluding resource-
specific identifiers used for internal tracking pur-
poses) used by the 19 resources. We then built an
automated retrieval agent to extract gene synonyms
from these four online databases: The HGNC Ge-
new database, Entrez Gene, Swiss-Prot, and Stan-
ford SOURCE. The results were collected into a sin-
gle dictionary. Each entry in the dictionary con-
sists of a gene identifier and a corresponding offi-
cial HGNC symbol. For data from HGNC, with-
drawn entries were excluded. Retrieving gene syn-
onyms from SOURCE required a list of gene identi-
fiers to query SOURCE, which was compiled by the
retrieval agent from the other sources (i.e., HGNC,
Entrez Gene and Swiss-Prot). In total, there were
333,297 entries in the combined dictionary.
2.2 Rule-Based Filter for Purification
Examination of the initial dictionary showed that
some entries did not fit our definition of a gene iden-
tifier, usually because they were peripheral (e.g., a
GenBank sequence identifier) or were describing a
gene class (e.g., an Enzyme Commission identifier
or a term such as ?tyrosine kinase?). A rule-based
filter was imposed to prune these uninformative syn-
onyms. The rules include removing identifiers under
these conditions:
1. Follows the form of a GenBank or EC acces-
sion ID (e.g., 1-2 letters followed by 5-6 dig-
its).
2. Contains at most 2 characters and 1 letter but
not an official HGNC symbol (e.g., P1).
3. Matches a description in the OMIM morbid
list1 (e.g., Tangier disease).
4. Is a gene EC number.2
5. Ends with ?, family ??, where ? is a capital letter
or a digit.
6. Follows the form of a DNA clone (e.g., 1-4 dig-
its followed by a single letter, followed by 1-2
digits).
7. Starts with ?similar to? (e.g., similar to zinc fin-
ger protein 533).
Our filter pruned 9,384 entries (2.82%).
2.3 Internal Update Across the Dictionaries
We used HGNC-designated human gene symbols as
the unique identifiers. However, we found that cer-
tain gene symbols listed as ?official? in the non-
HGNC sources were not always current, and that
other assigned symbols were not officially desig-
nated as such by HGNC. To remedy these issues, we
treated HGNC as the most reliable source and Entrez
Gene as the next most reliable, and then updated our
dictionary as follows:
1ftp://ftp.ncbi.nih.gov/repository/OMIM/morbidmap
2EC numbers are removed because they often represent gene
classes rather than specific instances.
42
? In the initial dictionary, some synonyms are
associated with symbols that were later with-
drawn by HGNC. Our retrieval agent extracted
a list of 5,048 withdrawn symbols from HGNC,
and then replaced any outdated symbols in the
dictionary with the official ones. Sixty with-
drawn symbols were found to be ambiguous,
but we found none of them appearing as sym-
bols in our dictionary.
? If a symbol used by Swiss-Prot or SOURCE
was not found as a symbol in HGNC or En-
trez Gene, but was a non-ambiguous synonym
in HGNC or Entrez Gene, then we replaced
it by the corresponding symbol of the non-
ambiguous synonym.
Among the 323,913 remaining entries, 801 entries
(0.25%) had symbols updated. After removing du-
plicate entries (42.19%), 187,267 distinct symbol-
synonym pairs representing 33,463 unique genes
were present. All tasks addressed in this section
were performed automatically by the retrieval agent.
3 Exact String Matching
We initially invoked several string transformations
for gene normalization, including:
1. Normalization of case.
2. Replacement of hyphens with spaces.
3. Removal of punctuation.
4. Removal of parenthesized materials.
5. Removal of stop words3.
6. Stemming, where the Porter stemmer was em-
ployed (Porter, 1980).
7. Removal of all spaces.
The first four transformations are derived from
(Cohen et al, 2002). Not all the rules we ex-
perimented with demonstrated good results for hu-
man gene name normalization. For example, we
found that stemming is inappropriate for this task.
To amend potential boundary errors of tagged men-
tions, or to match the variants of the synonyms, four
3ftp://ftp.cs.cornell.edu/pub/smart/English.stop
mention reductions (Cohen et al, 2002) were also
applied to the mentions or synonyms:
1. Removal of the first character.
2. Removal of the first word.
3. Removal of the last character.
4. Removal of the last word.
To provide utility, a system was built to allow
for transformations and reductions to be invoked
flexibly, including chaining of rules in various se-
quences, grouping of rules for simultaneous invo-
cation, and application of transformations to ei-
ther or both the candidate mention input and the
dictionary. For example, the mention ?alpha2C-
adrenergic receptor? in PMID 8967963 matches
synonym ?Alpha-2C adrenergic receptor? of gene
ADRA2C after normalizing case, replacing hyphens
by spaces, and removing spaces. Each rule can be
built into an invoked sequence deemed by evaluation
to be optimal for a given application domain.
A normalization step is defined here as the pro-
cess of finding string matches after a sequence of
chained transformations, with optional reductions
of the mentions or synonyms. We call a normal-
ization step safe if it generally makes only minor
changes to mentions. On the contrary, a normaliza-
tion step is called aggressive if it often makes sub-
stantial changes. However, a normalization step safe
for long mentions may not be safe for short ones.
Hence, our system was designed to allow a user to
set optional parameters factoring the minimal men-
tion length and/or the minimal normalized mention
length required to invoke a match.
A normalization system consists of multiple nor-
malization steps in sequence. Transformations are
applied sequentially and a match searched for; if
no match is identified for a particular step, the al-
gorithm proceeds to the next transformation. The
normalization steps and the optional conditions are
well-encoded in our program, which allows for a
flexible system specified by the sequences of the step
codes. Our general principle is to design a normal-
ization system that invokes safe normalization steps
first, and then gradually moves to more aggressive
43
ones. As the process lengthens, the precision de-
creases while the recall increases. The balance be-
tween precision and recall desired for a particular
application can be defined by the user.
Specifically, given string s, we use T (s) to de-
note the transformed string. All the 7 transformation
rules listed at the beginning of this subsection are
idempotent, since T (T (s)) = T (s). Two transfor-
mations, denoted by T1 and T2, are called commuta-
tive, if T1(T2(s)) = T2(T1(s)). The first four trans-
formations listed form a set of commutative rules.
Knowledge of these properties helps design a nor-
malization system.
Recall that NER systems, such as those required
for BioCreAtIvE task 1B, consist of two stages. For
our applications of interest, the normalization in-
put is generated by a gene tagger (McDonald and
Pereira, 2005), followed by the normalization sys-
tem described here as the second stage. In the sec-
ond stage, more synonyms do not necessarily imply
better performance, because less frequently used or
less informative synonyms may result in ambigu-
ous matches, where a match is called ambiguous
if it associates a mention with multiple gene iden-
tifiers. For example, from the Swiss-Prot dictio-
nary we know the gene mention ?MDR1? in PMID
8880878 is a synonym uniquely representing the
ABCB1 gene. However, if we include synonyms
from HGNC, it results in an ambiguous match be-
cause the TBC1D9 gene also uses the synonym
?MDR1?.
We investigated the rules separately, designed the
initial normalization procedure, and tuned our sys-
tem at the end. To evaluate the efficacy of our com-
piled dictionary and its sources, we determined the
accuracy of our system with all transformations and
reductions invoked sequentially, and without any ef-
forts to optimize the sequence (see section 6 for eval-
uation details). The goal in this experiment was to
evaluate the effectiveness of each vocabulary source
alone and in combination. Our experimental re-
sults at the mention level are summarized in Ta-
ble 1. The best two-staged system achieved a preci-
sion of 0.725 and recall of 0.704 with an F-measure
of 0.714, by using only HGNC and Swiss-Prot en-
tries.
As errors can be derived from the tagger or the
normalization alone or in combination, we also as-
Table 1: Results of Gene Normalization Using Exact
String Matching
Steps Recall Precision F-measure
(1) HGNC 0.762 0.511 0.611
(2) Entrez Gene 0.686 0.559 0.616
(3) Swiss-Prot 0.722 0.622 0.669
(4) SOURCE 0.743 0.431 0.545
(1)+(2) 0.684 0.564 0.618
(1)+(3) 0.725 0.704 0.714
(2)+(3) 0.665 0.697 0.681
(1)+(2)+(3) 0.667 0.702 0.684
(1)+(2)+(3)+(4) 0.646 0.707 0.675
sessed the performance of our normalization pro-
gram alone by directly normalizing the mentions in
the gold standard file used for evaluation (i.e., as-
suming the tagger is perfect). Our normalization
system achieved 0.824 F-measure (0.958 precision
and 0.723 recall) in this evaluation.
4 Approximate String Matching
Approximate string matching techniques have been
well-developed for entity identification. Given two
strings, a distance metric generates a score that re-
flects their similarity. Various string distance met-
rics have been developed based upon edit-distance,
string tokenization, or a hybrid of the two ap-
proaches (Cohen et al, 2003). Given a gene men-
tion, we consider the synonym(s) with the high-
est score to be a match if the score is higher than
a defined threshold. Our program also allows op-
tional string transformations and provides a user-
defined parameter for determining the minimal men-
tion length for approximate string matching. The
decision on the method chosen may be affected by
several factors, such as the application domain, fea-
tures of the strings representing the entity class, and
the particular data sets used. For gene NER, vari-
ous scoring methods have been favored (Crim et al,
2005; Cohen et al, 2003; Wellner et al, 2005).
Approximate string matching is usually consid-
ered more aggressive than exact string matching
with transformations; hence, we applied it as the last
step of our normalization sequence. To assess the
usefulness of approximate string matching, we be-
gan with our best dictionary subset in Subsection 3
44
(i.e., using HGNC and SwissProt), and applied ap-
proximate string matching as an additional normal-
ization step.
 0.35
 0.4
 0.45
 0.5
 0.55
 0.6
 0.65
 0.7
 0.75
 0  0.2  0.4  0.6  0.8  1
Pr
ec
is
io
n
q-gram Match Ratio
 0.7
 0.71
 0.72
 0.73
 0.74
 0.75
 0.76
 0.77
 0.78
 0  0.2  0.4  0.6  0.8  1
R
ec
al
l
q-gram Match Ratio
Jaro
JaroWinkler
SmithWaterman
TFIDF
UnsmoothedJS
Jaccard
Figure 1: Performance of Approximate String
Matching for Gene Normalization.
We selected six existing distance metrics that ap-
peared to be useful for human gene normalization:
Jaro, JaroWinkler, SmithWaterman, TFIDF, Un-
smoothedJS, and Jaccard. Our experiment showed
that TFIDF, UnsmoothedJS and Jaccard outper-
formed the others for human gene normalization in
our system, as shown in Figure 1. By incorpo-
rating approximate string matching using either of
these metrics into our system, overall performance
was slightly improved to 0.718 F-measure (0.724
precision and 0.713 recall) when employing a high
threshold (0.95). However, in most scenarios, ap-
proximate matching did not considerably improve
recall and had a non-trivial detrimental effect upon
precision.
5 Ambiguation Analysis
Gene identifier ambiguity is inherent in synonym
dictionaries as well as being generated during nor-
malization steps that transform mention strings.
5.1 Ambiguity in Synonym Dictionaries
If multiple gene identifiers share the same synonym,
it results in ambiguity. Table 2 shows the level of
ambiguity between and among the four sources of
gene identifiers used by our dictionary. The rate
of ambiguity ranges from 0.89% to 2.83%, which
is a rate comparable with that of mouse (1.5%)
and Drosophila (3.6%) identifiers (Hirschman et al,
2005).
 1
 10
 100
 1000
 10000
 100000
 1e+06
 1  10
# 
Sy
no
ny
m
s
Degree of Ambiguity
HGNC
Entrez Gene
Swiss-Prot
SOURCE
Total
Figure 2: Distribution of ambiguous synonyms in
the human gene dictionary.
Figure 2 is a log-log plot showing the distribu-
tion of ambiguous synonyms, where the degree is
the number of gene identifiers that a synonym is as-
sociated with. Comparing Figure 2 with (Hirschman
et al, 2005, Figure 3), we noted that on average, hu-
man gene synonyms are less ambiguous than those
of the three model organisms.
Another type of ambiguity is caused by gene sym-
bols or synonyms being common English words or
other biological terms. Our dictionary contains 11
gene symbols identical to common stop words4: T,
AS, DO, ET, IF, RD, TH, ASK, ITS, SHE and
WAS.
5.2 Ambiguous Matches in Gene
Normalization
We call a match ambiguous if it associates a men-
tion with multiple gene identifiers. Although the
4ftp://ftp.cs.cornell.edu/pub/smart/English.stop
45
Table 2: Statistics for Dictionary Sources
Dictionary # Symbols # Synonyms Ratio Max. # of Synonyms # with One Ambiguity
per Gene Definition Rate
HGNC 22,838 78,706 3.446 10 77,389 1.67%
Entrez Gene 33,007 109,127 3.306 22 106,034 2.83%
Swiss-Prot 12,470 61,743 4.951 17 60,536 1.95%
SOURCE 17,130 66,682 3.893 13 66,086 0.89%
Total 33,469 181,061 5.410 22 176,157 2.71%
normalization procedure may create ambiguity, if a
mention matches multiple synonyms, it may not be
strictly ambiguous. For example, the gene mention
?M creatine kinase? in PMID 1690725 matches the
synonyms ?Creatine kinase M-type? and ?Creatine
kinase, M chain? in our dictionary using the TFIDF
scoring method (with score 0.866). In this case, both
synonyms are associated with the CKM gene, so the
match is not ambiguous. However, even if a mention
matches only one synonym, it can be ambiguous, be-
cause the synonym is possibly ambiguous.
Figure 3 shows the result of an experiment con-
ducted upon 200,000 MEDLINE abstracts, where
the degree of ambiguity is the number of gene iden-
tifiers that a mention is associated with. The maxi-
mum, average, and standard deviation of the ambi-
guity degrees are 20, 1.129 and 0.550, respectively.
The overall ambiguity rate of all matched mentions
was 8.16%, and the rate of ambiguity is less than
10% at each step. Successful disambiguation can
increase the true positive match rate and therefore
improve performance but is beyond the scope of the
current investigation.
 1
 10
 100
 1000
 10000
 100000
 1e+06
 2  4  6  8  10  12  14  16  18  20
# 
M
en
tio
ns
# Matched Genes
Figure 3: Distribution of Ambiguous Genes in
200,000 MEDLINE Abstracts.
6 Application and Evaluation of an
Optimized Normalizer
Finally, we were interested in determining the effec-
tiveness of an optimized system based upon the gene
normalization system described above, and also cou-
pled with a state-of-the-art gene tagger. To de-
termine the optimal results of such a system, we
created a corpus of 100 MEDLINE abstracts that
together contained 1,094 gene mentions for 170
unique genes (also used in the evaluations above).
These documents were a subset of those used to train
the tagger, and thus measure optimal, rather than
typical MEDLINE, performance (data for a gener-
alized evaluation is forthcoming). This corpus was
manually annotated to identify human genes, ac-
cording to a precise definition of gene mentions that
an NER gene system would be reasonably expected
to tag and normalize correctly. Briefly, the definition
included only human genes, excluded multi-protein
complexes and antibodies, excluded chained men-
tions of genes (e.g., ?HDAC1- and -2 genes?), and
excluded gene classes that were not normalizable
to a specific symbol (e.g., tyrosine kinase). Docu-
ments were dual-pass annotated in full and then ad-
judicated by a 3rd expert. Adjudication revealed a
very high level of agreement between annotators.
To optimize the rule set for human gene normal-
ization, we evaluated up to 200 cases randomly cho-
sen from all MEDLINE files for each rule, where
invocation of that specific rule alone resulted in a
match. Most of the transformations worked per-
fectly or very well. Stemming and removal of the
first or last word or character each demonstrated
poor performance, as genes and gene classes were
often incorrectly converted to other gene instances
(e.g., ?CAP? and ?CAPS? are distinct genes). Re-
46
moval of stop words generated a high rate of false
positives. Rules were ranked according to their pre-
cision when invoked separately. A high-performing
sequence was ?0 01 02 03 06 016 026 036?, with 0
referring to case-insensitivity, 1 being replacement
of hyphens with spaces, 2 being removal of punc-
tuation, 3 being removal of parenthesized materials,
and 6 being removal of spaces; grouped digits indi-
cate simultaneous invocation of each specified rule
in the group. Table 3 indicates the cumulative accu-
racy achieved at each step5. A formalized determi-
nation of an optimal sequence is in progress. Ap-
proximate matching did not considerably improve
recall and had a non-trivial detrimental effect upon
precision.
Table 3: Results of Gene Normalization after Each
Step of Exact String Matching
Steps Recall Precision F-measure
0 0.628 0.698 0.661
01 0.649 0.701 0.674
02 0.654 0.699 0.676
03 0.665 0.702 0.683
06 0.665 0.702 0.683
016 0.718 0.685 0.701
026 0.718 0.685 0.701
036 0.718 0.685 0.701
The normalization sequence ?0 01 02 03 06 016
026 036? was then utilized for two separate evalua-
tions. First, we used the actual textual mentions of
each gene from the gold standard files as input into
our optimized normalization sequence, in order to
determine the accuracy of the normalization process
alone. We also used a previously developed CRF
gene tagger (McDonald and Pereira, 2005) to tag the
gold standard files, and then used the tagger?s output
as input for our normalization sequence. This sec-
ond evaluation determined the accuracy of a com-
bined NER system for human gene identification.
Depending upon the application, evaluation can
be determined more significant at either at the men-
tion level (redundantly), where each individual men-
tion is evaluated independently for accuracy, or as in
5The last two steps did not generate new matches using our
gold standard file and therefore the scores were unchanged.
These rule sets may improve performance in other cases.
the case of BioCreAtIvE task 1B, at the document
level (non-redundantly), where all mentions within a
document are considered to be equivalent. For pure
information extraction tasks, mention level accuracy
is a relevant performance indicator. However, for ap-
plications such as information extraction-based in-
formation retrieval (e.g., the identification of docu-
ments mentioning a specific gene), document-level
accuracy is a relevant gauge of system performance.
For normalization alone, at the mention level
our optimized normalization system achieved 0.882
precision, 0.704 recall, and 0.783 F-measure. At
the document level, the normalization results were
1.000 precision, 0.994 recall, and 0.997 F-measure.
For the combined NER system, the performance
was 0.718 precision, 0.626 recall, and 0.669 F-
measure at the mention level. At the document level,
the NER system results were 0.957 precision, 0.857
recall, and 0.901 F-measure. The lower accuracy of
the combined system was due to the fact that both
the tagger and the normalizer introduce error rates
that are multiplicative in combination.
7 Conclusions and Future Work
In this article we present a gene normalization sys-
tem that is intended for use in human gene NER, but
that can also be readily adapted to other biomedi-
cal normalization tasks. When optimized for human
gene normalization, our system achieved 0.783 F-
measure at the mention level.
Choosing the proper normalization steps depends
on several factors, such as (for genes) the organism
of interest, the entity class, the accuracy of identify-
ing gene mentions, and the reliability of the under-
lying dictionary. While the results of our normalizer
compare favorably with previous efforts, much fu-
ture work can be done to further improve the perfor-
mance of our system, including:
1. Performance of identifying gene mentions.
Only approximately 50 percent of gene men-
tions identified by our tagger were normaliz-
able. While this is mostly due to the fact that
the tagger identifies gene classes that cannot
be normalized to a gene instance, a significant
subset of gene instance mentions are not being
normalized.
2. Reliability of the dictionary. Though we have
47
investigated a sizable number of gene identifier
sources, the four representative sources used
for compiling our gene dictionary are incom-
plete and often not precise for individual terms.
Some text mentions were not normalizable due
the the incompleteness of our dictionary, which
limited the recall.
3. Disambiguation. A small portion (typi-
cally 7%-10%) of the matches were ambigu-
ous. Successful development of disambigua-
tion tools can improve the performance.
4. Machine-learning. It is likely possible that op-
timized rules can be used as probabilistic fea-
tures for a machine-learning-based version of
our normalizer.
Gene normalization has several potential applica-
tions, such as for biomedical information extraction,
database curation, and as a prerequisite for relation
extraction. Providing a proper synonym dictionary,
our normalization program is amenable to generaliz-
ing to other organisms, and has already proven suc-
cessful in our group for other entity normalization
tasks. An interesting future study would be to deter-
mine accuracy for BioCreAtIvE data once mouse,
Drosophila, and yeast vocabularies are incorporated
into our system.
Acknowledgment
This work was supported in part by NSF grant
EIA-0205448, funds from the David Lawrence
Altschuler Chair in Genomics and Computational
Biology, and the Penn Genomics Institute. The au-
thors acknowledge Shannon Davis and Jeremy Laut-
man for gene dictionary assessment, Steven Carroll
for gene tagger implementation and results, Penn
BioIE annotators for annotation of the gold standard,
and Monica D?arcy and members of the Penn BioIE
team for helpful comments.
References
K. B. Cohen, A. E. Dolbey, G. K. Acquaah-Mensah, and
L. Hunter. 2002. Contrast and variability in gene
names. In ACL Workshop on Natural Language Pro-
cessing in the Biomedical Domain, pages 14?20.
W. W. Cohen, P. Ravikumar, and S. E. Fienberg. 2003.
A comparison of string distance metrices for name-
matching tasks. In Proceedings of IIWeb Workshop.
A. M. Cohen. 2005. Unsupervised gene/protein entity
normalization using automatically extracted dictionar-
ies. In Linking Biological Literature, Ontologies and
Databases: Mining Biological Semantics, Proceed-
ings of the BioLINK2005 Workshop, pages 17?24. MI:
Association for Computational Linguistics, Detroit.
J. Crim, R. McDonald, and F. Pereira. 2005. Automati-
cally annotating documents with normalized gene lists.
BMC Bioinformatics, 6(Suppl 1)(S13).
D. Hanisch, K. Fundel, H.-T. Mevissen, R. Zimmer, and
J. Fluck. 2005. Prominer: Rule-based protein and
gene entity recognition. BMC Bioinformatics, 6(Suppl
1)(S14).
L. Hirschman, M. Colosimo, A. Morgan, and A. Yeh.
2005. Overview of biocreative task 1b: Normalized
gene lists. BMC Bioinformatics, 6(Suppl 1)(S11).
R. McDonald and F. Pereira. 2005. Identifying gene
and protein mentions in text using conditional random
fields. BMC Bioinformatics, 6(Suppl 1)(S6).
R. McDonald, R. S. Winters, M. Mandel, Y. Jin, P. S.
White, and F. Pereira. 2004. An entity tagger for rec-
ognizing acquired genomic variations in cancer litera-
ture. Journal of Bioinformatics, 20(17):3249?3251.
A. A. Morgan, L. Hirschman, M. Colosimo, A. S. Yeh,
and J. B. Colombe. 2004. Gene name identification
and normalization using a model organism database.
Journal of Biomedical Informatics, 37(6):396?410.
M. F. Porter. 1980. An algorithm for suffix stripping.
Program, 14(3).
B Wellner, J. Castan?o, and J. Pustejovsky. 2005. Adap-
tive string similarity metrics for biomedical reference
resolution. In Proceedings of the ACL-ISMB Work-
shop on Linking Biological Literature, Ontologies and
Databases: Mining Biological Semantics, pages 9?16,
Detroit. Association for Computational Linguistics.
A. Yeh, A. Morgan, M. Colosimo, and L. Hirschman.
2005. Biocreative task 1a: Gene mention finding eval-
uation. BMC Bioinformatics, 6(Suppl 1)(S2).
48
Proceedings of the ACL 2014 Workshop on Semantic Parsing, pages 33?38,
Baltimore, Maryland USA, June 26 2014.
c?2014 Association for Computational Linguistics
Cooking with Semantics
Jon Malmaud
Brain and Cognitive Sciences
MIT
43 Vassar St.
Cambridge, MA
malmaud@mit.edu
Earl J. Wagner, Nancy Chang, Kevin Murphy
Google
1600 Amphitheatre Pkwy
Mountain View, CA
wag@google.com, ncchang@google.com,
kpmurphy@google.com
Abstract
We are interested in the automatic inter-
pretation of how-to instructions, such as
cooking recipes, into semantic representa-
tions that can facilitate sophisticated ques-
tion answering. Recent work has shown
impressive results on semantic parsing of
instructions with minimal supervision, but
such techniques cannot handle much of the
situated and ambiguous language used in
instructions found on the web. In this pa-
per, we suggest how to extend such meth-
ods using a model of pragmatics, based on
a rich representation of world state.
1 Introduction
Understanding instructional text found on the web
presents unique challenges and opportunities that
represent a frontier for semantic parsing. Cru-
cially, instructional language is situated: it as-
sumes a situational context within which the agent
(i.e., the reader) is to carry out a sequence of ac-
tions, as applied to objects that are (or become)
available in the immediate environment. These ac-
tions and objects may not be explicitly specified;
indeed, much instructional language is ambigu-
ous, underspecified and often even ungrammatical
relative to conventional usage.
In this ?vision paper?, we focus on interpreting
cooking recipes. While there are several services
that already support searching for recipes (such
as Google Recipe Search
1
, Yummly, Foodily, and
MyTaste), the faceted search capabilities they pro-
vide are limited to recipe meta-data such as ingre-
dients, genres, cooking time, portions, and nutri-
tion values. Some of this information is explicitly
marked up in machine-readable form
2
. However,
1
http://www.google.com/insidesearch/
features/recipes/
2
See e.g. http://microformats.org/wiki/
recipe-formats
Figure 1: Example recipes. Left: for a mixed
drink. Right: for guacamole dip.
the actual steps of the recipe are treated as an un-
structured blob of text. (The same problem ap-
plies to other instructional sites, such as ehow.
com, wikihow.com, answers.yahoo.com,
www.instructables.com, etc.) Interpreting
the steps of recipes (and instructions more gener-
ally) is the goal of this paper.
2 Challenges
This section surveys some of the linguistic chal-
lenges typical of the cooking domain, as illustrated
by the two recipes in Figure 1. These difficulties
can be classified broadly as problems arising from
the interpretation of arguments, actions and con-
trol structure.
Arguments: One particularly salient character-
istic of recipes is that they often feature arguments
that are omitted, underspecified or otherwise de-
pendent on the context. Arguments may be elided
in syntactic contexts where they are usually re-
quired (the so-called ?zero anaphora? problem),
especially when they are easily filled by an object
in the immediate context. For example, the item
to set aside in (1a) is the just-treated cocktail glass,
and the item to fill in (1b) and shake and then strain
in (1c) is the recently mentioned shaker. Note that
the context may include the ingredient list itself, as
illustrated by the elided argument(s) to be added
in the one-line recipe ?Add to a cocktail glass in
the order listed.? Arguments may be implicitly
available, based on either domain-specific expec-
tations of the initial context or the results of pre-
33
ceding steps. The ice in (1b) isn?t listed in the
corresponding recipes ingredient list, since many
common ingredients (water, ice, salt, pepper) are
assumed to be available in most kitchens. Some-
times, the argument may never have been directly
verbalized, but rather is the result of a previous ac-
tion. Thus in the recipe ?Pour ingredients over ice
and shake vigorously,? the object to shake is the
container (only implicitly available) along with its
contents ? which, once the ?pour? instruction is
executed, include both ice and the (listed) ingre-
dients. Note also that interpreting ?the remain-
ing ingredients? in (1b) requires an understand-
ing of which ingredients have yet to be used at
that point in the recipe. Arguments may be in-
directly available, by association with an explic-
itly available argument. Recipe 2 mentions avo-
cados in several explicit and implicit referring ex-
pressions; of these only the ?them? in (2a) may
be considered straightforward anaphoric reference
(to the just-cut avocados). Step (2b) involves a
metonymic reference to the ?skin and pits? where
the part-whole relation between these items and
the avocado is what makes the instruction inter-
pretable. Step (2c) once again mentions ?avoca-
dos?, but note that this now refers to the flesh of
the avocados, i.e., the implicit scooped-out object
from (2a). Arguments may be incompletely speci-
fied, especially with respect to amount. The exact
amount of sugar needed in (1a) is not mentioned,
for example. Similarly, the amount of ice needed
in (1b) depends on the size of the shaker and is not
precisely specified.
Actions: Like arguments, action interpretation
also depends on the situational context. For exam-
ple, actions may have ambiguous senses, mainly
due to the elided arguments noted above. The verb
?shake? in (1c), for example, yields a spurious in-
transitive reading. Actions may have argument-
dependent senses: certain verbs may resolve to
different motor actions depending on the affor-
dances of their arguments. For example, the ac-
tion intended by the verb ?garnish? in (1d) might
involve careful perching of the peel on the rim
of the glass; in other recipes, the same verb ap-
plied to nutmeg or cut fruit may be better inter-
preted as an add action. Actions may be omitted
or implied, in particular by the way certain argu-
ments are expressed. Most recipes involving eggs,
for example, do not explicitly mention the need to
crack them and extract their contents; this is a de-
fault preparatory step. Other ingredients vary in
how strongly they are associated with (implicit)
preparatory steps. For example, recipes calling
for ?1/4 avocado? may require that something like
steps (2a-b) be undertaken (and their results quar-
tered); the ?orange peel? of (1d) may likewise de-
pend on a separate procedure for extracting peel
from an orange.
Control structure: Instructions sometimes
provide more complex information about se-
quence, coordination and control conditions. Con-
ditions: An action may be specified as being per-
formed until some finish condition holds. In (2c),
the ?until smooth? condition?itself featuring an
elided avocado argument?controls how long the
blending action should continue. Other conditions
mentioned in recipes include ?Add crushed ice un-
til the glass is almost full?, ?Stir until the glass be-
gins to frost?, and ?Add salt to taste?. Sequence:
Though implicitly sequential, recipes occasion-
ally include explicit sequencing language. In the
recipe ?Add to a cocktail glass in the order listed?,
the order reflects that of the ingredient list. Other
recipes specify that certain steps can or should be
done ?ahead of time?, or else while other steps are
in progress. Alternatives: Recipes sometimes al-
low for some variability, by specifying alternative
options for specific ingredients (?Garnish with a
twist of lemon or lime?), appliances or utensils
(?Using a large fork (or a blender)...?), and even
actions (?Chop or mash the avocados?).
As should be clear from these examples, the in-
terpretation of a given step in a set of instructions
may hinge on many aspects of situated and proce-
dural knowledge, including at least: the physical
context (including the particular props and tools
assumed available); the incremental state result-
ing from successful execution of previous steps;
and general commonsense knowledge about the
affordances of specific objects or expected argu-
ments of specific actions (or more conveniently,
corpus-based verb-argument expectations that ap-
proximate such knowledge, see e.g., (Nyga and
Beetz, 2012)). All of these sources of knowl-
edge go significantly beyond those employed in
semantic parsing models for single utterances and
in non-procedural contexts.
3 Proposed approach
We propose to maintain a rich latent context that
persists while parsing an entire recipe, in contrast
34
Bowl    
Flour
Milk
Empty    
In(packet)
In(jug)
Has(milk)    
In(packet)
In(bowl)
Add:
   from: jug
   to: bowl
   what: milk
   manner: pouring
"Pour milk
into a bowl"
Has(milk, flour)    
In(bowl)
In(bowl)
Add:
   from: packet
   to: bowl
   what: flour
"Add flour"
Has(milk, flour)    
In(bowl)
In(bowl)
Mix:
   what: bowl
   manner: well
S0 S1
A1
WA1 WA2
"The paste
should be
smooth."
WS3
S2 S3
A2 A3
VA1
VS3
Video
Image
WA3
"Mix well"
Figure 2: Our proposed probabilistic model, showing a possible trace of observed and latent variables
after parsing each step of a pancake recipe. See text for description of notation.
to approaches that interpret each sentence inde-
pendently. This context represents the state of the
kitchen, and statements in the recipes are inter-
preted pragmatically with respect to the evolving
context. More precisely, our model has the over-
all structure of a discrete-time, partially observed,
object-oriented Markov Decision Process, as il-
lustrated in Figure 2. The states and actions are
both hidden. What we observe is text and/or im-
ages/video; our goal is to infer the posterior over
the sequence of actions (i.e., to recover the ?true?
recipe), given the noisy evidence.
States and actions. The world state S
t
is repre-
sented as a set of objects, such as ingredients and
containers, along with various predicates, encod-
ing the quantity, location, and condition (e.g., raw
or cooked, empty or full) of each object. Note that
previous work on situated semantic parsing often
uses grid world environments where the only flu-
ent is the agent?s location; in contrast, we allow
any object to undergo state transformations. In
particular, objects can be created and destroyed.
Each action A
t
is represented by a semantic
frame, corresponding to a verb with various ar-
guments or roles. This specifies how to trans-
form the state. We also allow for sequencing
and loop frames c.f., the ?robot control language?
in (Matuszek et al., 2013). We assume access
to a simple cooking simulator that can take in a
stream of low-level instructions to produce a new
state; this implements the world dynamics model
p(S
t
|S
t?1
,A
t
).
Text data. We assume that the text of the
t?th sentence, represented by WA
t
, describes the
t?th primitive action, A
t
. We represent the con-
ditional distribution p(A
t
|WA
t
,S
t?1
) as a log-
linear model, as in prior work on frame-semantic
parsing/ semantic role labeling (SRL) (Das et al.,
2014).
3
However, we extend this prior work by al-
lowing roles to be filled not just from spans from
the text, but also by objects in the latent state vec-
tor. We will use various pragmatically-inspired
features to represent the compatibility between
candidate objects in the state vector and roles in
the action frame, including: whether the object
has been recently mentioned or touched, whether
the object has the right affordances for the cor-
responding role (e.g., if the frame is ?mix?, and
the role is ?what?, the object should be mixable),
3
Although CCGs have been used in previous work on
(situated) semantic parsing, such as (Artzi and Zettlemoyer,
2013), we chose to use the simpler approach based on frames
because the nature of the language that occurs in recipes
is sufficiently simple (there are very few complex nested
clauses).
35
etc. More sophisticated models, based on model-
ing the belief state of the listener (e.g., (Goodman
and Stuhlm?uller, 2013; Vogel et al., 2013)) are also
possible and within the scope of future work.
In addition to imperative sentences, we some-
times encounter descriptive sentences that de-
scribe what the state should look like at a given
step (c.f., (Lau et al., 2009)). We let WS
t
denote a
sentence (possibly empty) describing the t?th state,
S
t
. The distribution p(S
t
|WS
t
) is a discriminative
probabilistic classifier of some form.
Visual data. Much instructional information is
available in the form of how-to videos. In addi-
tion, some textual instructions are accompanied by
static images. We would like to extend the model
to exploit such data, when available.
Let a video clip associated with an action at
time t be denoted by VA
t
. We propose to learn
p(A
t
|VA
t
) using supervised machine learning.
For features, we could use the output of standard
object detectors and their temporal trajectories, as
in (Yu and Siskind, 2013), bags of visual words
derived from temporal HOG descriptors as in (Das
et al., 2013), or features derived from RGB-D sen-
sors such as Kinect, as in (Song et al., 2013; Lei et
al., 2012).
There are many possible ways to fuse the in-
formation from vision and text, i.e., to com-
pute p(A
t
|VA
t
,WA
t
,S
t?1
). The simplest ap-
proach is to separately train the two conditionals,
p(A
t
|WA
t
,S
t?1
) and p(A
t
|VA
t
), and then train
another model to combine them, using a separate
validation set; this will learn the relative reliability
of the two sources of signal.
Learning and inference. We assume that we
have manually labeled the actions A
t
, and that the
initial state S
0
is fully observed (e.g., a list of in-
gredients, with all containers empty). If we ad-
ditional assume that the world dynamics model is
known
4
and deterministic, then we can uniquely
infer the sequence of states S
1:T
. This lets us use
standard supervised learning to fit the log-linear
model p(A
t
|WA
t
,S
t?1
).
In the future, we plan to relax the assumption
of fully labeled training data, and to allow for
learning from a distant supervision signal, simi-
lar to (Artzi and Zettlemoyer, 2013; Branavan et
al., 2009). For example, we can prefer a parse that
results in a final state in which all the ingredients
4
There has been prior work on learning world models
from text, see e.g., (Sil and Yates, 2011; Branavan et al.,
2012).
have been consumed, and the meal is prepared.
4 Preliminary results
We conducted a preliminary analysis to gauge the
feasibility and expected performance benefits of
our approach. We used the raw recipes provided
in the CMU Recipe Database (Tasse and Smith,
2008), which consists of 260 English recipes
downloaded from allrecipes.com. We then
applied a state-of-the art SRL system (Das et al.,
2014) to the corpus, using Propbank (Palmer et al.,
2005) as our frame repository. Figure 3 summa-
rizes our findings.
To judge the variance of predicates used in the
cooking domain, we computed the frequency of
each word tagged as a present-tense verb by a sta-
tistical part-of-speech tagger, filtering out a small
number of common auxiliary verbs. Our find-
ings suggest a relatively small number of verbs
account for a large percentage of observed instruc-
tions (e.g, ?add?, ?bake?, and ?stir?). The majority
of these verbs have corresponding framesets that
are usually correctly recognized, with some no-
table exceptions. Further, the most common ob-
served framesets have a straightforward mapping
to our set of kitchen state transformations, such
as object creation via combination (?add?, ?mix?,
?combine?, ?stir in?), location transfers (?place?,
?set?), and discrete state changes over a small
space of features (?cook?, ?cut?, ?cool?, ?bake?).
To gain a preliminary understand of the limi-
tations of the current SRL system and the possi-
ble performance benefits of our proposed system,
we hand-annotated five of our recipes as follows:
Each verb in the recipe corresponding to an action
was annotated with its best corresponding roleset
(if any). Each role in that roleset was marked as
either being explicitly present in the text, implic-
itly present in our latent kitchen model but not in
the text (and so in principle, fillable by our model),
or neither present in the text nor in our model. For
example, in?cover for forty minutes?, the frameset
?cover? has an explicit temporal role-filling (?for
forty minutes?) and an implicit role-filling (?the
pot? as the patient of ?cover?).
For each verb in the annotation, we checked if
the SRL system mapped that verb to the correct
roleset and if so, whether it filled the same seman-
tic roles as the annotator indicated were explicitly
present in the text. Overall, we found 54% recall
of the annotations by the SRL system. We quali-
36
add stir mix bake combine remove place pour cook coolVerb02040
6080100120
140
Frequency add.02 combine.01cook.01 mix.01 place.01 stir.02 remain.01 bring.01 serve.01 melt.01Roleset02040
6080100120
140
Frequency
Heat oil in a large pot >until hot@  brown chicken >in the pot@ . 
5emoYe  chicken >from the pot@  and set >the chicken@  aside.
6aute  onions until >the onions are@  soft, about  minutes.
Add broth, beans, half the pepper, and all the chicken >to the pot@  coYer  and simmer >the pot contents@  for  minutes.
Add parsley, cilantro, salt, and remaining pepper >to the pot@ , and simmer >the mixture@   more minutes.
Figure 3: Results. Top: Distribution of the ten most common verbs and framesets in 260 recipes from
allrecipes.com. Bottom: An example recipe annotation. Blue indicates propbank predicates. Bracketed
red indicates implicit propbank arguments not in the text, but in principle recognizable by our model.
Green indicates quantifier adjectives which our model could resolve to an exact quantity, given initial
ingredient amounts.
tatively notes several failure modes. Many errors
arise from not recognizing predicates represented
in the text as an imperative verb, likely because
PropBank contains few examples of such language
for the labeler to learn from. Other errors result
from ungrammatical constructs (e.g. in ?cook five
minutes?, the eliding of ?for? causes ?five min-
utes? to incorrectly parse as a direct argument).
Certain cooking-related verbs lack framesets en-
tirely, such as ?prebake?. Occasionally, the wrong
roleset is chosen. For example, in?Stir the mix-
ture? , ?Stir? is labeled as ?stir.02: cause (emo-
tional) reaction? rather than ?stir.01: mix with a
circular motion?.
We also analyzed the quantity and qualitative
trends in the human annotations that refer to roles
fillable from the latent kitchen model but not lit-
erally present in the text. Overall, 52% of verb
annotations referenced at least one such role. The
most common situation (occurring for 36% of all
annotated verbs) is the ?patient/direct object? role
is elided in the text but inferable from the world
state, as in ?simmer [the mixture] for 40 min-
utes?. The second most common is the ?location?
modifier role is elided in the text, as in ?Remove
chicken [from the pot]?. Overall, we believe our
proposed approach will improve the quality of the
SRL system, and thus the overall interpretability
of the recipes.
5 Possible applications
We believe that semantic parsing of recipes and
other instructional text could support a rich array
of applications, such as the following:
Deriving a ?canonical? recipe. It would be
useful to align different versions of the same
recipe to derive a ?canonical form? cf., (Druck and
Pang, 2012; Tenorth et al., 2013b).
Explaining individual steps. It would be help-
ful if a user could click on a confusing step in a
recipe and get a more detailed explanation and/or
an illustrative video clip.
Automatically interpreting software instruc-
tions. Going beyond the recipe domain, it would
be useful to develop a system which can interpret
instructions such as how to install software, and
then automatically execute them (i.e., install the
software for you). In practice, this may be too
hard, so we could allow the system to ask for hu-
man help if it gets stuck, cf. (Deits et al., 2013).
Robotics. (Tenorth et al., 2013a) suggest min-
ing natural language ?action recipes? as a way to
specify tasks for service robots. In the domain
of food recipes, there have already been several
demonstrations (e.g., (Beetz et al., 2011; Bollini et
al., 2013)) of robots automatically cooking meals
based on recipes.
Task assistance using augmented reality.
Imagine tracking the user as they follow some in-
structions using a device such as Google glass, and
offering help when needed. Such systems have
been developed before for specialized domains
like maintenance and repair of military hardware
5
,
but automatic parsing of natural language text po-
tentially opens this up to the consumer market.
(Note that there is already a recipe app for Google
Glass
6
, although it just displays a static list of in-
structions.)
5
For example, see http://graphics.cs.
columbia.edu/projects/armar/index.htm.
6
See http://www.glassappsource.com/
listing/all-the-cooks-recipes.
37
References
Y. Artzi and L. Zettlemoyer. 2013. Weakly supervised
learning of semantic parsers for mapping instruc-
tions to actions. Trans. Assoc. for Computational
Linguistics, 1:49?62.
M. Beetz, U. Klank, I. Kreese, A. Maldonado,
L. Mosenlechner, D. Pangercic, T. Ruhr, and
M. Tenorth. 2011. Robotic roommates making pan-
cakes. In Intl. Conf. on Humanoid Robots.
Mario Bollini, Stefanie Tellex, Tyler Thompson,
Nicholas Roy, and Daniela Rus. 2013. Interpreting
and executing recipes with a cooking robot. Experi-
mental Robotics.
SRK Branavan, H. Chen, L. Zettlemoyer, and R. Barzi-
lay. 2009. Reinforcement learning for mapping in-
structions to actions. In Association for Computa-
tional Linguistics.
S.R.K. Branavan, N. Kushman, T. Lei, and R. Barzilay.
2012. Learning High-Level Planning from Text. In
ACL.
P. Das, C. Xu, R. F. Doell, and J. J. Corso. 2013. A
thousand frames in just a few words: Lingual de-
scription of videos through latent topics and sparse
object stitching. In CVPR.
D. Das, D. Chen, A. Martins, N. Schneider, and
N. Smith. 2014. Frame-semantic parsing. Com-
putational Linguistics.
R. Deits, S. Tellex, P. Thaker, D. Simeonov, T. Kol-
lar, and N. Roy. 2013. Clarifying Commands
with Information-Theoretic Human-Robot Dialog.
J. Human-Robot Interaction.
G. Druck and B. Pang. 2012. Spice it Up? Mining
Renements to Online Instructions from User Gener-
ated Content. In ACL.
Noah D Goodman and Andreas Stuhlm?uller. 2013.
Knowledge and implicature: Modeling language un-
derstanding as social cognition. Topics in cognitive
science, 5(1):173?184.
TA Lau, Clemens Drews, and Jeffrey Nichols. 2009.
Interpreting Written How-To Instructions. IJCAI.
J. Lei, X. Ren, and D. Fox. 2012. Fine-grained kitchen
activity recognition using RGB-D. In Ubicomp.
C. Matuszek, E. Herbst, L. Zettlemoyer, and D. Fox.
2013. Learning to parse natural language commands
to a robot control system. Experimental Robotics,
pages 1?14.
D. Nyga and M. Beetz. 2012. Everything robots al-
ways wanted to know about housework (but were
afraid to ask). In Intl. Conf. on Intelligent Robots
and Systems.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
proposition bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1):71?106.
A. Sil and A. Yates. 2011. Extracting STRIPS Rep-
resentations of Actions and Events. In Recent Ad-
vances in NLP.
Young Chol Song, Henry Kautz, James Allen, Mary
Swift, Yuncheng Li, Jiebo Luo, and Ce Zhang.
2013. A markov logic framework for recognizing
complex events from multimodal data. In Proc. 15th
ACM Intl. Conf. Multimodal Interaction, pages 141?
148. ACM.
D. Tasse and N. Smith. 2008. SOUR CREAM: To-
ward Semantic Processing of Recipes. Technical
Report CMU-LTI-08-005, Carnegie Mellon Univer-
sity, Pittsburgh, PA.
M. Tenorth, A. Perzylo, R. Lafrenz, and M. Beetz.
2013a. Representation and exchange of knowl-
edge about actions, objects, and environments in the
roboearth framework. IEEE Trans. on Automation
Science and Engineering, 10(3):643?651.
M. Tenorth, J. Ziegltrum, and M. Beetz. 2013b. Auto-
mated alignment of specifications of everyday ma-
nipulation tasks. In IEEE Intl. Conf. on Intelligent
Robots and Systems.
A. Vogel, M. Bodoia, C. Potts, and D. Jurafsky. 2013.
Emergence of Gricean Maxims from Multi-Agent
Decision Theory. In NAACL.
Haonan Yu and JM Siskind. 2013. Grounded language
learning from video described with sentences. In As-
sociation for Computational Linguistics.
38
