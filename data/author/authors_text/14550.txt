Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 309?312,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Improving Blog Polarity Classification via Topic Analysis and Adaptive
Methods
Feifan Liu
University of Wisconsin, Milwaukee
liuf@uwm.edu
Dong Wang, Bin Li, Yang Liu
The University of Texas at Dallas
dongwang,yangl@hlt.utdallas.edu
Abstract
In this paper we examine different linguistic features
for sentimental polarity classification, and perform a
comparative study on this task between blog and re-
view data. We found that results on blog are much
worse than reviews and investigated two methods
to improve the performance on blogs. First we ex-
plored information retrieval based topic analysis to
extract relevant sentences to the given topics for po-
larity classification. Second, we adopted an adaptive
method where we train classifiers from review data
and incorporate their hypothesis as features. Both
methods yielded performance gain for polarity clas-
sification on blog data.
1 Introduction
Sentimental analysis is a task of text categorization that
focuses on recognizing and classifying opinionated text
towards a given subject. Different levels of sentimental
analysis has been performed in prior work, from binary
classes to more fine grained categories. Pang et al (2002)
defined this task as a binary classification task and ap-
plied it to movie reviews. More sentiment classes, such
as document objectivity and subjectivity as well as dif-
ferent rating scales on the subjectivity, have also been
taken into consideration (Pang and Lee, 2005; Boiy et
al., 2007). In terms of granularity, this task has been
investigated from building word level sentiment lexicon
(Turney, 2002; Moilanen and Pulman, 2008) to detecting
phrase-level (Wilson et al, 2005; Agarwal et al, 2009)
and sentence-level (Riloff and Wiebe, 2003; Hu and Liu,
2004) sentiment orientation. However, most previous
work has mainly focused on reviews (Pang et al, 2002;
Hu and Liu, 2004), news resources (Wilson et al, 2005),
and multi-domain adaptation (Blitzer et al, 2007; Man-
sour et al, 2008). Sentiment analysis on blogs (Chesley
et al, 2005; Kim et al, 2009) is still at its early stage.
In this paper we investigate binary polarity classifica-
tion (positive vs. negative). We evaluate the genre effect
between blogs and review data and show the difference of
feature effectiveness. We demonstrate improved polarity
classification performance in blogs using two methods:
(a) integrating topic relevance analysis to perform topic
specific polarity classification; (b) adopting an adaptive
method by incorporating multiple classifiers? hypotheses
from different review domains as features. Our manual
analysis also points out some challenges and directions
for further study in blog domain.
2 Features for Polarity Classification
For the binary polarity classification task, we use a super-
vised learning framework to determine whether a docu-
ment is positive or negative. We used a subjective lex-
icon, containing 2304 positive words and 4145 negative
words respectively, based on (Wilson et al, 2005). The
features we explored are listed below.
(i) Lexical features (LF)
We use the bag of words for the lexical features as they
have been shown very useful in previous work.
(ii) Polarized lexical features (PL)
We tagged each sentiment word in our data set with its
polarity tag based on the sentiment lexicon (?POS? for
positive, and ?NEG? for negative), along with its part-
of-speech tag. For example, in the sentence ?It is good,
and I like it?, ?good? is tagged as ?POS/ADJ?, ?like? is
tagged as ?POS/VRB?. Then we encode the number of
the polarized tags in a document as features.
(iii) Polarized bigram features (PB)
Contextual information around the polarized words
can be useful for sentimental analysis. A word may
flip the polarity of its neighboring sentiment words even
though this word itself is not necessarily a negative word.
For example, in ?Given her sudden celebrity with those
on the left...? (a sentence in a political blog), ?sudden?
preceding ?celebrity? implies the author?s negative atti-
tude towards ?her?. We combine the sentiment word?s
polarized tag and its following and preceding word or
its part-of-speech to comprise different bigram features
to represent this kind of contextual information. For ex-
309
ample, in ?I recommend this.?, ?recommend? is a posi-
tive verb, denoted as ?POS/VRB?, and the bigram fea-
tures including this tag and its previous word ?I? are
?I POS/VRB? and ?pron POS/VRB?.
(iv) Transition word features (T)
Transition words, such as ?although?, ?even though?,
serve as function words that may change the literal opin-
ion polarity in the current sentence. This information has
not been widely explored for sentiment analysis. In this
study, we compiled a transition word list containing 31
words. We use the co-occurring feature between a transi-
tion word and its nearby content words (noun, verb, ad-
jective and adverb) or polarized tags of sentiment words
within the same sentence, but not spanning over other
transition words. For example, in ?Although it is good?,
we use features like ?although is?,?although good? and
?although POS/ADJ?, where ?POS/ADJ? is the PL fea-
ture for word ?good?.
3 Feature Effectiveness on Blogs and
Reviews
The blog data we used is from the TREC Blog Track eval-
uation in 2006 and 2007. The annotation was conducted
for the 100 topics used in the evaluation (blogs are rele-
vant to a given topic and also opinionated). We use 6,896
positive and 5,300 negative blogs. For the review data,
we combined multiple review data sets from (Pang et al,
2002; Blitzer et al, 2007) together. It contains reviews
from movies and four product domains (kitchen, elec-
tronics, books, and dvd), each of which has 1000 neg-
ative and 1000 positive samples. For the data without
sentence information (e.g., blog data, some review data),
we generated sentences using the maximum entropy sen-
tence boundary detection tool1. We used TnT tagger to
obtain the part-of-speech tags for these data sets.
For classification, we use the maximum entropy clas-
sifier2 with a Gaussian prior of 1 and 100 iterations in
model training. For all the experiments below, we use
a 10-fold cross validation setup and report the average
classification accuracy. Table 1 shows classification re-
sults using various feature sets on blogs and review data.
We keep the lexical feature (LF) as a base feature, and
investigate the effectiveness of adding more different fea-
tures. We used Wilcox signed test for statistical signifi-
cance test. Symbols ??? and ??? in the table indicate the
significant level of 0.05 and 0.1 respectively, compared to
the baseline performance using LF feature setup.
For the review domain, most of the feature sets can sig-
nificantly improve the classification performance over the
baseline of using ?LF? features. ?PB? features yielded
more significant improvement than ?PL? or ?T? feature
categories. Combining ?PL? and ?T? features resulted in
some slight further improvement, achieving the best ac-
1http://stp.ling.uu.se/?gustav/java/classes/MXTERMINATOR.html
2http://homepages.inf.ed.ac.uk/s0450736/maxent toolkit.html
Feature Set Blogs Reviews
LF 72.07 81.67
LF+PL 70.93 81.93
LF+PB 72.44 83.62?
LF+T 72.17 81.76
LF+PL+PB 70.81 83.61?
LF+PL+T 72.74 82.13?
LF+PB+T 72.29 83.73?
LF+PL+PB+T 71.85 83.94?
Table 1: Polarity classification results (accuracy in %) using
different features for blogs and reviews.
curacy of 83.94%. We notice that incorporating our pro-
posed transition feature (T) always achieves some gain on
different feature settings, suggesting that those transition
features are useful for sentimental analysis on reviews.
From Table 1, we can see that overall the performance
on blogs is worse than on the review data. We hypoth-
esize this may be due to the large variation in terms of
contents and styles in blogs. Regarding the feature ef-
fectiveness, we also observe some differences between
blogs and reviews. Adding the polarized bigram feature
and transition feature (PB and T) individually can yield
some improvement; however, adding both of them did
not result in any further improvement ? performance de-
grades compared to LF+PB. Interestingly, although ?PL?
feature alone does not seem to help, by adding ?PL? and
?T? together, the performance achieved the best accuracy
of 72.74%. We also found that adding all the features
together hurts the performance, suggesting that different
features interact with each other and some do not com-
bine well (e.g., PB and T features). In addition, all the
improvements here are not statistically significant.
Note that for the blog data, we randomly split them for
the cross validation experiments regardless of the queries.
In order to better understand whether the poor results on
blog data is due to the effect of different queries, we per-
formed another experiment where for each query, we ran-
domly divided the corresponding blogs into training and
test splits. Only 66 queries were kept for this experi-
ments ? we did not include those queries that have fewer
than 10 relevant blogs. The results for the query balanced
split on blogs are shown in Figure 1. We also include re-
sults for the five individual review data sets in order to see
the topic effect. We present results using four represen-
tative feature sets chosen according to Table 1. For the
review data, we notice some difference across different
data sets, suggesting their inherent difference in terms of
task difficulty. We observe slight performance increase
for some feature sets using the query balanced setup for
blog data, but overall it is still much worse than the review
data. This shows that the query unbalanced training/test
split does not explain the performance gap between blogs
and reviews. This is consistent with (Zhang et al, 2007)
that found that a query-independent classifier performs
even better than query-dependent one. We expect that the
310
query unbalanced setup is more realistic, therefore, in the
following experiments, we continue with this setup.
7173
7577
7981
8385
87
Accur
acy(%
)
blog_data
blog_data_query_balancedreview_books
review_dvd
review_kitchen
review_movie
review_elec
Figure 1: Polarity classification results on query balanced blog
data and five individual review data sets.
4 Improving Blog Polarity Classification
To improve the performance of polarity classification on
blogs, we propose two methods: (a) extract only topic-
relevant segments from blogs for sentiment analysis; (b)
apply adaptive methods to leverage review data.
4.1 Using topic-relevant blog context
Generally a review is written towards one product or one
kind of service, but a blog may cover several topics with
possibly different opinions towards each topic. The blog
data we used is annotated based on some specific topics
in the TREC Blog Track evaluation. Take topic 870 in
the data as an example, ?Find opinions on alleged use
of steroids by baseball player Barry?. There is one blog
that talks about 5 different baseball players in issues of
using steroids. Since the reference opinion tag of a blog
is determined by polarity towards the given query topic, it
might be confusing for the classifier if we use the whole
blog to derive features. Recently topic analysis has been
used for polarity classification (Zhang et al, 2007; Titov
and McDonald, 2008; Wiegand and Klakow, 2009). We
take a different approach in this study.
In order to obtain a topic-relevant context, we retrieved
the top 10 relevant sentences corresponding to the given
topic using the Lemur toolkit3. Then we used these sen-
tences and their immediate previous and following sen-
tences for feature extraction in the same way as what
we did on the whole blog. In addition to using all the
words in the relevant context, we also investigated using
only content words since those are more topic indicative
than function words. We extracted content words (nouns,
verbs, adjectives and adverbs) from each blog in their
original order and apply the same feature extraction pro-
cess as for using all the words.
3http://www.lemurproject.org/lemur/
Table 2 shows the blog polarity classification results
using the whole blog vs. relevant context composed of
all the words or only content words. For the significance
test, the comparison was done for using relevant context
with all the words vs. using the whole blog; and us-
ing content words only vs. using all the words in rele-
vant context. Each comparison was with respect to the
same feature setup. We observe improved polarity classi-
fication performance when using sentence retrieval based
topic analysis to extract relevant context. Using all the
words in the topic relevant context, all the improvements
compared to using the original entire blog are statistically
significant at the level of 0.01. We also notice that un-
like on the entire blog document, the ?PL? features con-
tribute positively when combined with ?LF?. All the fea-
ture settings with ?PL? perform very well. The best ac-
curacy of 75.32% is achieved using feature combination
of ?LF+PL? or ?LF+PL+T?. This suggests that polarized
lexical features suffered from the off-topic content when
using the entire blog and are more useful within contexts
of certain topics.
When using content words only, we observe consistent
gain across all the feature sets. Three feature settings,
?LF+PB?,?LF+T? and ?LF+PL+PB+T?, achieve statisti-
cally significant further improvement (compared to using
all the words of relevant contexts). The best accuracy
(75.6%) is achieved by using the ?LF+PB? features.
Feature Set Whole Relevant Context
Blog All Words Content Words
LF 72.07 74.92? 75.14
LF+PL 70.93 75.32? 75.34
LF+PB 72.44 75.03? 75.6?
LF+T 72.17 75.01? 75.35?
LF+PL+PB 70.81 75.27? 75.35
LF+PL+T 72.74 75.32? 75.41
LF+PB+T 72.29 75.17? 75.42
LF+PL+PB+T 71.85 75.21? 75.45?
Table 2: Blog polarity classification results (accuracy in %) us-
ing topic relevant context composed of all the words or only
content words.
4.2 Adaptive methods using review data
Domain adaptation has been studied in some previous
work (e.g., (Blitzer et al, 2007; Mansour et al, 2008)).
In this paper, we evaluate two adaptive approaches in or-
der to leverage review data to improve blog polarity clas-
sification. In the first approach, in each of the 10-fold
cross-validation training, we pool the blog training data
(90% of the entire blog data) together with all the review
data from 5 different domains. In the second method, we
augment features with hypotheses obtained from classi-
fiers trained using other domain data. Specifically, we
first trained 5 classifiers from 5 review domain data sets
respectively, and encoded the hypotheses from different
classifiers as features for blog training (together with the
original features of the blog data). Results of these two
approaches are shown in Table 3. We use the topic rele-
311
vant context with content words only in this experiment,
and present results for different feature combinations (ex-
cept the baseline ?LF? setting). The significance test is
conducted in comparison to the results using only blog
data for training, for the same feature setting.
We find that the first approach does not yield any gain,
even though the added data is about the same size as
the blog data. It indicates that due to the large differ-
ence between the two genres, simply combining blogs
and reviews in training is not effective. However, we
can see that using augmented features in training signifi-
cantly improved the performance across different feature
sets. The best result is achieved using ?LF+T? features,
76.84% compared with the best accuracy of 75.6% when
using the blog data only (?LF+PB? features).
Feature Set Only Blog Pool Data Augment Features
LF+PL 75.34 75.05 76.12?
LF+PB 75.6 74.35 76.28?
LF+T 75.35 74.47 76.84?
LF+PL+PB 75.35 74.94 76.7?
LF+PL+T 75.41 74.85 76.32?
LF+PB+T 75.42 74.46 76.3?
LF+PL+PB+T 75.45 74.96 76.53?
Table 3: Results (accuracy in %) of blog polarity classification
using two methods leveraging review data.
4.3 Error analysis
Notice that after achieving some improvements the per-
formance on blogs is still much worse than on review
data. Thus we performed a manual error analysis for a
better understanding of the difficulties of sentiment anal-
ysis on blog data, and identified the following challenges.
(a) Idiomatic expressions. Compared to reviews, blog-
gers seem to use more idioms. For example, ?Of course
he has me over the barrel...? expresses negative opinion,
however, there are no superficially indicative features.
(b) Ironic writing style. Some bloggers prefer ironic
style especially when speaking against something or
somebody, whereas opinions are often expressed using
plain writing style in reviews. Simply using the surface
word level features is not able to model these properly.
(c) Background knowledge. In some political blogs,
the polarized expressions are implicit. Correctly recog-
nizing them requires background knowledge and deeper
language analysis techniques.
5 Conclusions and Future Work
In this paper, we have evaluated various features and the
domain effect on sentimental polarity classification. Our
experiments on blog and review data demonstrated dif-
ferent feature effectiveness and the overall poorer perfor-
mance on blogs than reviews. We found that the polarized
features and the transition word features we introduced
are useful for polarity classification. We also show that
by extracting topic-relevant context and considering only
content words, the system can achieve significantly better
performance on blogs. Furthermore, an adaptive method
using augmented features can effectively leverage data
from other domains, and yield improvement compared
to using in-domain training or training on combined data
from different domains. For our future work, we plan
to investigate other adaption methods, and try to address
some of the problems identified in our error analysis.
6 Acknowledgment
The authors thank the three anonymous reviewers for
their suggestions.
References
Apoorv Agarwal, Fadi Biadsy, and Kathleen McKeown. 2009.
Contextual phrase-level polarity analysis using lexical affect
scoring and syntactic n-grams. In Proc. of EACL.
John Blitzer, Mark Dredze, and Fernando Pereira. 2007. Bi-
ographies, bollywood, boom-boxes and blenders: Domain
adaptation for sentiment classification. In Proc. of ACL.
Erik Boiy, Pieter Hens, Koen Deschacht, and Marie-Francine
Moens. 2007. Automatic sentiment analysis in on-line text.
In Proc. of ELPUB.
Paula Chesley, Bruce Vincent, Li Xu, and Rohini K. Srihari.
2005. Using verbs and adjectives to automatically classify
blog sentiment. In Proc. of AAAI.
Minqing Hu and Bing Liu. 2004. Mining and summarizing
customer reviews. In Proc. of ACM SIGKDD.
Jungi Kim, Jin-Ji Li, and Jong-Hyeok Lee. 2009. Discovering
the discriminative views: Measuring term weights for senti-
ment analysis. In Proc. of ACL-IJCNLP.
Yishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh.
2008. Domain adaptation with multiple sources. In Proc.
of NIPS.
Karo Moilanen and Stephen Pulman. 2008. The good, the bad,
and the unknown: Morphosyllabic sentiment tagging of un-
seen words. In Proc. of ACL.
Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting class
relationships for sentiment categorization with respect to rat-
ing scales. In Proc. of ACL.
Bo Pang, Lilian Lee, and Shrivakumar Vaithyanathan. 2002.
Thumbs up? sentiment classification using machine learning
techniques. In Proc. of EMNLP.
Ellen Riloff and Janyce Wiebe. 2003. Learning extraction pat-
terns for subjective expressions. In Proc. of EMNLP.
Ivan Titov and Ryan McDonald. 2008. Modeling online re-
views with multi-grain topic models. In Proc. of WWW.
Peter D. Turney. 2002. Thumbs up or thumbs down? semantic
orientation applied to unsupervised classification of reviews.
In Proc. of ACL.
Michael Wiegand and Dietrich Klakow. 2009. Topic-Related
polarity classification of blog sentences. In Proc. of the 14th
Portuguese Conference on Artificial Intelligence: Progress
in Artificial Intelligence, pages 658?669.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005.
Recognizing contextual polarity in phrase-level sentiment
analysis. In Proc. of HLT-EMNLP.
Wei Zhang, Clement Yu, and Weiyi Meng. 2007. Opinion re-
trieval from blogs. In Proc. of CIKM, pages 831?840.
312
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 331?339,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
A Pilot Study of Opinion Summarization in Conversations
Dong Wang Yang Liu
The University of Texas at Dallas
dongwang,yangl@hlt.utdallas.edu
Abstract
This paper presents a pilot study of opinion
summarization on conversations. We create
a corpus containing extractive and abstrac-
tive summaries of speaker?s opinion towards
a given topic using 88 telephone conversa-
tions. We adopt two methods to perform ex-
tractive summarization. The first one is a
sentence-ranking method that linearly com-
bines scores measured from different aspects
including topic relevance, subjectivity, and
sentence importance. The second one is a
graph-based method, which incorporates topic
and sentiment information, as well as addi-
tional information about sentence-to-sentence
relations extracted based on dialogue struc-
ture. Our evaluation results show that both
methods significantly outperform the baseline
approach that extracts the longest utterances.
In particular, we find that incorporating di-
alogue structure in the graph-based method
contributes to the improved system perfor-
mance.
1 Introduction
Both sentiment analysis (opinion recognition) and
summarization have been well studied in recent
years in the natural language processing (NLP) com-
munity. Most of the previous work on sentiment
analysis has been conducted on reviews. Summa-
rization has been applied to different genres, such
as news articles, scientific articles, and speech do-
mains including broadcast news, meetings, conver-
sations and lectures. However, opinion summariza-
tion has not been explored much. This can be use-
ful for many domains, especially for processing the
increasing amount of conversation recordings (tele-
phone conversations, customer service, round-table
discussions or interviews in broadcast programs)
where we often need to find a person?s opinion or
attitude, for example, ?how does the speaker think
about capital punishment and why??. This kind of
questions can be treated as a topic-oriented opin-
ion summarization task. Opinion summarization
was run as a pilot task in Text Analysis Conference
(TAC) in 2008. The task was to produce summaries
of opinions on specified targets from a set of blog
documents. In this study, we investigate this prob-
lem using spontaneous conversations. The problem
is defined as, given a conversation and a topic, a
summarization system needs to generate a summary
of the speaker?s opinion towards the topic.
This task is built upon opinion recognition and
topic or query based summarization. However, this
problem is challenging in that: (a) Summarization in
spontaneous speech is more difficult than well struc-
tured text (Mckeown et al, 2005), because speech
is always less organized and has recognition errors
when using speech recognition output; (b) Senti-
ment analysis in dialogues is also much harder be-
cause of the genre difference compared to other do-
mains like product reviews or news resources, as re-
ported in (Raaijmakers et al, 2008); (c) In conversa-
tional speech, information density is low and there
are often off topic discussions, therefore presenting
a need to identify utterances that are relevant to the
topic.
In this paper we perform an exploratory study
on opinion summarization in conversations. We
compare two unsupervised methods that have been
331
widely used in extractive summarization: sentence-
ranking and graph-based methods. Our system at-
tempts to incorporate more information about topic
relevancy and sentiment scores. Furthermore, in
the graph-based method, we propose to better in-
corporate the dialogue structure information in the
graph in order to select salient summary utterances.
We have created a corpus of reasonable size in this
study. Our experimental results show that both
methods achieve better results compared to the base-
line.
The rest of this paper is organized as follows. Sec-
tion 2 briefly discusses related work. Section 3 de-
scribes the corpus and annotation scheme we used.
We explain our opinion-oriented conversation sum-
marization system in Section 4 and present experi-
mental results and analysis in Section 5. Section 6
concludes the paper.
2 Related Work
Research in document summarization has been well
established over the past decades. Many tasks have
been defined such as single-document summariza-
tion, multi-document summarization, and query-
based summarization. Previous studies have used
various domains, including news articles, scientific
articles, web documents, reviews. Recently there
is an increasing research interest in speech sum-
marization, such as conversational telephone speech
(Zhu and Penn, 2006; Zechner, 2002), broadcast
news (Maskey and Hirschberg, 2005; Lin et al,
2009), lectures (Zhang et al, 2007; Furui et al,
2004), meetings (Murray et al, 2005; Xie and Liu,
2010), voice mails (Koumpis and Renals, 2005).
In general speech domains seem to be more diffi-
cult than well written text for summarization. In
previous work, unsupervised methods like Maximal
Marginal Relevance (MMR), Latent Semantic Anal-
ysis (LSA), and supervised methods that cast the ex-
traction problem as a binary classification task have
been adopted. Prior research has also explored using
speech specific information, including prosodic fea-
tures, dialog structure, and speech recognition con-
fidence.
In order to provide a summary over opinions, we
need to find out which utterances in the conversa-
tion contain opinion. Most previous work in senti-
ment analysis has focused on reviews (Pang and Lee,
2004; Popescu and Etzioni, 2005; Ng et al, 2006)
and news resources (Wiebe and Riloff, 2005). Many
kinds of features are explored, such as lexical fea-
tures (unigram, bigram and trigram), part-of-speech
tags, dependency relations. Most of prior work used
classification methods such as naive Bayes or SVMs
to perform the polarity classification or opinion de-
tection. Only a handful studies have used conver-
sational speech for opinion recognition (Murray and
Carenini, 2009; Raaijmakers et al, 2008), in which
some domain-specific features are utilized such as
structural features and prosodic features.
Our work is also related to question answering
(QA), especially opinion question answering. (Stoy-
anov et al, 2005) applies a subjectivity filter based
on traditional QA systems to generate opinionated
answers. (Balahur et al, 2010) answers some spe-
cific opinion questions like ?Why do people criti-
cize Richard Branson?? by retrieving candidate sen-
tences using traditional QA methods and selecting
the ones with the same polarity as the question. Our
work is different in that we are not going to an-
swer specific opinion questions, instead, we provide
a summary on the speaker?s opinion towards a given
topic.
There exists some work on opinion summariza-
tion. For example, (Hu and Liu, 2004; Nishikawa et
al., 2010) have explored opinion summarization in
review domain, and (Paul et al, 2010) summarizes
contrastive viewpoints in opinionated text. How-
ever, opinion summarization in spontaneous conver-
sation is seldom studied.
3 Corpus Creation
Though there are many annotated data sets for the
research of speech summarization and sentiment
analysis, there is no corpus available for opinion
summarization on spontaneous speech. Thus for this
study, we create a new pilot data set using a sub-
set of the Switchboard corpus (Godfrey and Holli-
man, 1997).1 These are conversational telephone
speech between two strangers that were assigned a
topic to talk about for around 5 minutes. They were
told to find the opinions of the other person. There
are 70 topics in total. From the Switchboard cor-
1Please contact the authors to obtain the data.
332
pus, we selected 88 conversations from 6 topics for
this study. Table 1 lists the number of conversations
in each topic, their average length (measured in the
unit of dialogue acts (DA)) and standard deviation
of length.
topic #Conv. avg len stdev
space flight and exploration 6
165.5 71.40
capital punishment 24
gun control 15
universal health insurance 9
drug testing 12
universal public service 22
Table 1: Corpus statistics: topic description, number of
conversations in each topic, average length (number of
dialog acts), and standard deviation.
We recruited 3 annotators that are all undergrad-
uate computer science students. From the 88 con-
versations, we selected 18 (3 from each topic) and
let al three annotators label them in order to study
inter-annotator agreement. The rest of the conversa-
tions has only one annotation.
The annotators have access to both conversation
transcripts and audio files. For each conversation,
the annotator writes an abstractive summary of up
to 100 words for each speaker about his/her opin-
ion or attitude on the given topic. They were told to
use the words in the original transcripts if possible.
Then the annotator selects up to 15 DAs (no mini-
mum limit) in the transcripts for each speaker, from
which their abstractive summary is derived. The se-
lected DAs are used as the human generated extrac-
tive summary. In addition, the annotator is asked
to select an overall opinion towards the topic for
each speaker among five categories: strongly sup-
port, somewhat support, neutral, somewhat against,
strongly against. Therefore for each conversation,
we have an abstractive summary, an extractive sum-
mary, and an overall opinion for each speaker. The
following shows an example of such annotation for
speaker B in a dialogue about ?capital punishment?:
[Extractive Summary]
I think I?ve seen some statistics that say that, uh, it?s
more expensive to kill somebody than to keep them in
prison for life.
committing them mostly is, you know, either crimes of
passion or at the moment
or they think they?re not going to get caught
but you also have to think whether it?s worthwhile on
the individual basis, for example, someone like, uh, jeffrey
dahlmer,
by putting him in prison for life, there is still a possi-
bility that he will get out again.
I don?t think he could ever redeem himself,
but if you look at who gets accused and who are the
ones who actually get executed, it?s very racially related
? and ethnically related
[Abstractive Summary]
B is against capital punishment except under certain
circumstances. B finds that crimes deserving of capital
punishment are ?crimes of the moment? and as a result
feels that capital punishment is not an effective deterrent.
however, B also recognizes that on an individual basis
some criminals can never ?redeem? themselves.
[Overall Opinion]
Somewhat against
Table 2 shows the compression ratio of the extrac-
tive summaries and abstractive summaries as well as
their standard deviation. Because in conversations,
utterance length varies a lot, we use words as units
when calculating the compression ratio.
avg ratio stdev
extractive summaries 0.26 0.13
abstractive summaries 0.13 0.06
Table 2: Compression ratio and standard deviation of ex-
tractive and abstractive summaries.
We measured the inter-annotator agreement
among the three annotators for the 18 conversations
(each has two speakers, thus 36 ?documents? in to-
tal). Results are shown in Table 3. For the ex-
tractive or abstractive summaries, we use ROUGE
scores (Lin, 2004), a metric used to evaluate auto-
matic summarization performance, to measure the
pairwise agreement of summaries from different an-
notators. ROUGE F-scores are shown in the table
for different matches, unigram (R-1), bigram (R-2),
and longest subsequence (R-L). For the overall opin-
ion category, since it is a multiclass label (not binary
decision), we use Krippendorff?s ? coefficient to
measure human agreement, and the difference func-
tion for interval data: ?2ck = (c? k)
2 (where c, k are
the interval values, on a scale of 1 to 5 corresponding
to the five categories for the overall opinion).
We notice that the inter-annotator agreement for
extractive summaries is comparable to other speech
333
extractive summaries
R-1 0.61
R-2 0.52
R-L 0.61
abstractive summaries
R-1 0.32
R-2 0.13
R-L 0.25
overall opinion ? = 0.79
Table 3: Inter-annotator agreement for extractive and ab-
stractive summaries, and overall opinion.
summary annotation (Liu and Liu, 2008). The
agreement on abstractive summaries is much lower
than extractive summaries, which is as expected.
Even for the same opinion or sentence, annotators
use different words in the abstractive summaries.
The agreement for the overall opinion annotation
is similar to other opinion/emotion studies (Wil-
son, 2008b), but slightly lower than the level rec-
ommended by Krippendorff for reliable data (? =
0.8) (Hayes and Krippendorff, 2007), which shows
it is even difficult for humans to determine what
opinion a person holds (support or against some-
thing). Often human annotators have different inter-
pretations about the same sentence, and a speaker?s
opinion/attitude is sometimes ambiguous. Therefore
this also demonstrates that it is more appropriate to
provide a summary rather than a simple opinion cat-
egory to answer questions about a person?s opinion
towards something.
4 Opinion Summarization Methods
Automatic summarization can be divided into ex-
tractive summarization and abstractive summariza-
tion. Extractive summarization selects sentences
from the original documents to form a summary;
whereas abstractive summarization requires genera-
tion of new sentences that represent the most salient
content in the original documents like humans do.
Often extractive summarization is used as the first
step to generate abstractive summary.
As a pilot study for the problem of opinion sum-
marization in conversations, we treat this problem
as an extractive summarization task. This section
describes two approaches we have explored in gen-
erating extractive summaries. The first one is a
sentence-ranking method, in which we measure the
salience of each sentence according to a linear com-
bination of scores from several dimensions. The sec-
ond one is a graph-based method, which incorpo-
rates the dialogue structure in ranking. We choose to
investigate these two methods since they have been
widely used in text and speech summarization, and
perform competitively. In addition, they do not re-
quire a large labeled data set for modeling training,
as needed in some classification or feature based
summarization approaches.
4.1 Sentence Ranking
In this method, we use Equation 1 to assign a score
to each DA s, and select the most highly ranked ones
until the length constriction is satisfied.
score(s) = ?simsim(s,D) + ?relREL(s, topic)
+?sentsentiment(s) + ?lenlength(s)
?
i
?i = 1 (1)
? sim(s,D) is the cosine similarity between DA
s and all the utterances in the dialogue from
the same speaker, D. It measures the rele-
vancy of s to the entire dialogue from the tar-
get speaker. This score is used to represent the
salience of the DA. It has been shown to be an
important indicator in summarization for var-
ious domains. For cosine similarity measure,
we use TF*IDF (term frequency, inverse docu-
ment frequency) term weighting. The IDF val-
ues are obtained using the entire Switchboard
corpus, treating each conversation as a docu-
ment.
? REL(s, topic) measures the topic relevance of
DA s. It is the sum of the topic relevance of all
the words in the DA. We only consider the con-
tent words for this measure. They are identified
using TreeTagger toolkit.2 To measure the rel-
evance of a word to a topic, we use Pairwise
Mutual Information (PMI):
PMI(w, topic) = log2
p(w&topic)
p(w)p(topic)
(2)
2http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/De
cisionTreeTagger.html
334
where all the statistics are collected from the
Switchboard corpus: p(w&topic) denotes the
probability that word w appears in a dialogue
of topic t, and p(w) is the probability of w ap-
pearing in a dialogue of any topic. Since our
goal is to rank DAs in the same dialog, and
the topic is the same for all the DAs, we drop
p(topic) when calculating PMI scores. Be-
cause the value of PMI(w, topic) is negative,
we transform it into a positive one (denoted
by PMI+(w, topic)) by adding the absolute
value of the minimum value. The final rele-
vance score of each sentence is normalized to
[0, 1] using linear normalization:
RELorig(s, topic) =
?
w?s
PMI+(w, topic)
REL(s, topic) =
RELorig(s, topic)?Min
Max?Min
? sentiment(s) indicates the probability that ut-
terance s contains opinion. To obtain this,
we trained a maximum entropy classifier with
a bag-of-words model using a combination
of data sets from several domains, including
movie data (Pang and Lee, 2004), news articles
from MPQA corpus (Wilson and Wiebe, 2003),
and meeting transcripts from AMI corpus (Wil-
son, 2008a). Each sentence (or DA) in these
corpora is annotated as ?subjective? or ?objec-
tive?. We use each utterance?s probability of
being ?subjective? predicted by the classifier as
its sentiment score.
? length(s) is the length of the utterance. This
score can effectively penalize the short sen-
tences which typically do not contain much
important content, especially the backchannels
that appear frequently in dialogues. We also
perform linear normalization such that the final
value lies in [0, 1].
4.2 Graph-based Summarization
Graph-based methods have been widely used in doc-
ument summarization. In this approach, a document
is modeled as an adjacency matrix, where each node
represents a sentence, and the weight of the edge be-
tween each pair of sentences is their similarity (co-
sine similarity is typically used). An iterative pro-
cess is used until the scores for the nodes converge.
Previous studies (Erkan and Radev, 2004) showed
that this method can effectively extract important
sentences from documents. The basic framework we
use in this study is similar to the query-based graph
summarization system in (Zhao et al, 2009). We
also consider sentiment and topic relevance infor-
mation, and propose to incorporate information ob-
tained from dialog structure in this framework. The
score for a DA s is based on its content similarity
with all other DAs in the dialogue, the connection
with other DAs based on the dialogue structure, the
topic relevance, and its subjectivity, that is:
score(s) = ?sim
?
v?C
sim(s, v)
?
z?C sim(z, v)
score(v)
+?rel
REL(s, topic)
?
z?C REL(z, topic)
+?sent
sentiment(s)
?
z?C sentiment(z)
+?adj
?
v?C
ADJ(s, v)
?
z?C ADJ(z, v)
score(v)
?
i
?i = 1 (3)
where C is the set of all DAs in the dialogue;
REL(s, topic) and sentiment(s) are the same
as those in the above sentence ranking method;
sim(s, v) is the cosine similarity between two DAs
s and v. In addition to the standard connection be-
tween two DAs with an edge weight sim(s, v), we
introduce new connections ADJ(s, v) to model di-
alog structure. It is a directed edge from s to v, de-
fined as follows:
? If s and v are from the same speaker and within
the same turn, there is an edge from s to v and
an edge from v to s with weight 1/dis(s, v)
(ADJ(s, v) = ADJ(v, s) = 1/dis(s, v)),
where dis(s, v) is the distance between s and
v, measured based on their DA indices. This
way the DAs in the same turn can reinforce
each other. For example, if we consider that
335
one DA is important, then the other DAs in the
same turn are also important.
? If s and v are from the same speaker, and
separated only by one DA from another
speaker with length less than 3 words (usu-
ally backchannel), there is an edge from s to
v as well as an edge from v to s with weight 1
(ADJ(s, v) = ADJ(v, s) = 1).
? If s and v form a question-answer pair from two
speakers, then there is an edge from question s
to answer v with weight 1 (ADJ(s, v) = 1).
We use a simple rule-based method to deter-
mine question-answer pairs ? sentence s has
question marks or contains ?wh-word? (i.e.,
?what, how, why?), and sentence v is the im-
mediately following one. The motivation for
adding this connection is, if the score of a ques-
tion sentence is high, then the answer?s score is
also boosted.
? If s and v form an agreement or disagreement
pair, then there is an edge from v to s with
weight 1 (ADJ(v, s) = 1). This is also de-
termined by simple rules: sentence v contains
the word ?agree? or ?disagree?, s is the previ-
ous sentence, and from a different speaker. The
reason for adding this is similar to the above
question-answer pairs.
? If there are multiple edges generated from the
above steps between two nodes, then we use the
highest weight.
Since we are using a directed graph for the sen-
tence connections to model dialog structure, the re-
sulting adjacency matrix is asymmetric. This is dif-
ferent from the widely used graph methods for sum-
marization. Also note that in the first sentence rank-
ing method or the basic graph methods, summariza-
tion is conducted for each speaker separately. Ut-
terances from one speaker have no influence on the
summary decision for the other speaker. Here in our
proposed graph-based method, we introduce con-
nections between the two speakers, so that the adja-
cency pairs between them can be utilized to extract
salient utterances.
5 Experiments
5.1 Experimental Setup
The 18 conversations annotated by all 3 annotators
are used as test set, and the rest of 70 conversa-
tions are used as development set to tune the param-
eters (determining the best combination weights). In
preprocessing we applied word stemming. We per-
form extractive summarization using different word
compression ratios (ranging from 10% to 25%). We
use human annotated dialogue acts (DA) as the ex-
traction units. The system-generated summaries are
compared to human annotated extractive and ab-
stractive summaries. We use ROUGE as the eval-
uation metrics for summarization performance.
We compare our methods to two systems. The
first one is a baseline system, where we select the
longest utterances for each speaker. This has been
shown to be a relatively strong baseline for speech
summarization (Gillick et al, 2009). The second
one is human performance. We treat each annota-
tor?s extractive summary as a system summary, and
compare to the other two annotators? extractive and
abstractive summaries. This can be considered as
the upper bound of our system performance.
5.2 Results
From the development set, we used the grid search
method to obtain the best combination weights for
the two summarization methods. In the sentence-
ranking method, the best parameters found on the
development set are ?sim = 0, ?rel = 0.3, ?sent =
0.3, ?len = 0.4. It is surprising to see that the sim-
ilarity score is not useful for this task. The possible
reason is, in Switchboard conversations, what peo-
ple talk about is diverse and in many cases only topic
words (except stopwords) appear more than once. In
addition, REL score is already able to catch the topic
relevancy of the sentence. Thus, the similarity score
is redundant here.
In the graph-based method, the best parameters
are ?sim = 0, ?adj = 0.3, ?rel = 0.4, ?sent = 0.3.
The similarity between each pair of utterances is
also not useful, which can be explained with similar
reasons as in the sentence-ranking method. This is
different from graph-based summarization systems
for text domains. A similar finding has also been
shown in (Garg et al, 2009), where similarity be-
336
384348535863
0.1
0.15
0.2
0.25
com
pres
sion
 ratio
ROUGE-1(%)
max-
lengt
h
sente
nce-
rank
ing
grap
h
hum
an
(a) compare to reference extractive summary
1719212325272931
0.1
0.15
0.2
0.25
com
pres
sion
 ratio
ROUGE-1(%)
max-
lengt
h
sente
nce-
rank
ing
grap
h
hum
an
(b) compare to reference abstractive summary
Figure 1: ROUGE-1 F-scores compared to extractive
and abstractive reference summaries for different sys-
tems: max-length, sentence-ranking method, graph-
based method, and human performance.
tween utterances does not perform well in conversa-
tion summarization.
Figure 1 shows the ROUGE-1 F-scores compar-
ing to human extractive and abstractive summaries
for different compression ratios. Similar patterns are
observed for other ROUGE scores such as ROUGE-
2 or ROUGE-L, therefore they are not shown here.
Both methods improve significantly over the base-
line approach. There is relatively less improvement
using a higher compression ratio, compared to a
lower one. This is reasonable because when the
compression ratio is low, the most salient utterances
are not necessarily the longest ones, thus using more
information sources helps better identify important
sentences; but when the compression ratio is higher,
longer utterances are more likely to be selected since
they contain more content.
There is no significant difference between the two
methods. When compared to extractive reference
summaries, sentence-ranking is slightly better ex-
cept for the compression ratio of 0.1. When com-
pared to abstractive reference summaries, the graph-
based method is slightly better. The two systems
share the same topic relevance score (REL) and
sentiment score, but the sentence-ranking method
prefers longer DAs and the graph-based method
prefers DAs that are emphasized by the ADJ ma-
trix, such as the DA in the middle of a cluster of
utterances from the same speaker, the answer to a
question, etc.
5.3 Analysis
To analyze the effect of dialogue structure we in-
troduce in the graph-based summarization method,
we compare two configurations: ?adj = 0 (only us-
ing REL score and sentiment score in ranking) and
?adj = 0.3. We generate summaries using these two
setups and compare with human selected sentences.
Table 4 shows the number of false positive instances
(selected by system but not by human) and false neg-
ative ones (selected by human but not by system).
We use all three annotators? annotation as reference,
and consider an utterance as positive if one annotator
selects it. This results in a large number of reference
summary DAs (because of low human agreement),
and thus the number of false negatives in the system
output is very high. As expected, a smaller compres-
sion ratio (fewer selected DAs in the system output)
yields a higher false negative rate and a lower false
positive rate. From the results, we can see that gen-
erally adding adjacency matrix information is able
to reduce both types of errors except when the com-
pression ratio is 0.15.
The following shows an example, where the third
DA is selected by the system with ?adj = 0.3, but
not by ?adj = 0. This is partly because the weight
of the second DA is enhanced by the the question-
337
?adj = 0 ?adj = 0.3
ratio FP FN FP FN
0.1 37 588 33 581
0.15 60 542 61 546
0.2 100 516 90 511
0.25 137 489 131 482
Table 4: The number of false positive (FP) and false neg-
ative (FN) instances using the graph-based method with
?adj = 0 and ?adj = 0.3 for different compression ratios.
answer pair (the first and the second DA), and thus
subsequently boosting the score of the third DA.
A: Well what do you think?
B: Well, I don?t know, I?m thinking about from one to
ten what my no would be.
B: It would probably be somewhere closer to, uh, less
control because I don?t see, -
We also examined the system output and human
annotation and found some reasons for the system
errors:
(a) Topic relevance measure. We use the statis-
tics from the Switchboard corpus to measure the rel-
evance of each word to a given topic (PMI score),
therefore only when people use the same word in
different conversations of the topic, the PMI score of
this word and the topic is high. However, since the
size of the corpus is small, some topics only con-
tain a few conversations, and some words only ap-
pear in one conversation even though they are topic-
relevant. Therefore the current PMI measure cannot
properly measure a word?s and a sentence?s topic
relevance. This problem leads to many false neg-
ative errors (relevant sentences are not captured by
our system).
(b) Extraction units. We used DA segments as
units for extractive summarization, which can be
problematic. In conversational speech, sometimes
a DA segment is not a complete sentence because
of overlaps and interruptions. We notice that anno-
tators tend to select consecutive DAs that constitute
a complete sentence, however, since each individual
DA is not quite meaningful by itself, they are often
not selected by the system. The following segment
is extracted from a dialogue about ?universal health
insurance?. The two DAs from speaker B are not
selected by our system but selected by human anno-
tators, causing false negative errors.
B: and it just can devastate ?
A: and your constantly, -
B: ? your budget, you know.
6 Conclusion and Future Work
This paper investigates two unsupervised methods
in opinion summarization on spontaneous conver-
sations by incorporating topic score and sentiment
score in existing summarization techniques. In the
sentence-ranking method, we linearly combine sev-
eral scores in different aspects to select sentences
with the highest scores. In the graph-based method,
we use an adjacency matrix to model the dialogue
structure and utilize it to find salient utterances in
conversations. Our experiments show that both
methods are able to improve the baseline approach,
and we find that the cosine similarity between utter-
ances or between an utterance and the whole docu-
ment is not as useful as in other document summa-
rization tasks.
In future work, we will address some issues iden-
tified from our error analysis. First, we will in-
vestigate ways to represent a sentence?s topic rel-
evance. Second, we will evaluate using other ex-
traction units, such as applying preprocessing to re-
move disfluencies and concatenate incomplete sen-
tence segments together. In addition, it would be
interesting to test our system on speech recognition
output and automatically generated DA boundaries
to see how robust it is.
7 Acknowledgments
The authors thank Julia Hirschberg and Ani
Nenkova for useful discussions. This research is
supported by NSF awards CNS-1059226 and IIS-
0939966.
References
Alexandra Balahur, Ester Boldrini, Andre?s Montoyo, and
Patricio Mart??nez-Barco. 2010. Going beyond tra-
ditional QA systems: challenges and keys in opinion
question answering. In Proceedings of COLING.
Gu?nes Erkan and Dragomir R. Radev. 2004. LexRank:
graph-based lexical centrality as salience in text sum-
marization. Journal of Artificial Intelligence Re-
search.
338
Sadaoki Furui, Tomonori Kikuchi, Yousuke Shinnaka,
and Chior i Hori. 2004. Speech-to-text and speech-to-
speech summarization of spontaneous speech. IEEE
Transactions on Audio, Speech & Language Process-
ing, 12(4):401?408.
Nikhil Garg, Benoit Favre, Korbinian Reidhammer, and
Dilek Hakkani Tu?r. 2009. ClusterRank: a graph
based method for meeting summarization. In Proceed-
ings of Interspeech.
Dan Gillick, Korbinian Riedhammer, Benoit Favre, and
Dilek Hakkani-Tur. 2009. A global optimization
framework for meeting summarization. In Proceed-
ings of ICASSP.
John J. Godfrey and Edward Holliman. 1997.
Switchboard-1 Release 2. In Linguistic Data Consor-
tium, Philadelphia.
Andrew Hayes and Klaus Krippendorff. 2007. Answer-
ing the call for a standard reliability measure for cod-
ing data. Journal of Communication Methods and
Measures, 1:77?89.
Minqing Hu and Bing Liu. 2004. Mining and sum-
marizing customer reviews. In Proceedings of ACM
SIGKDD.
Konstantinos Koumpis and Steve Renals. 2005. Auto-
matic summarization of voicemail messages using lex-
ical and prosodic features. ACM - Transactions on
Speech and Language Processing.
Shih Hsiang Lin, Berlin Chen, and Hsin min Wang.
2009. A comparative study of probabilistic ranking
models for chinese spoken document summarization.
ACM Transactions on Asian Language Information
Processing, 8(1).
Chin-Yew Lin. 2004. ROUGE: a package for auto-
matic evaluation of summaries. In Proceedings of ACL
workshop on Text Summarization Branches Out.
Fei Liu and Yang Liu. 2008. What are meeting sum-
maries? An analysis of human extractive summaries
in meeting corpus. In Proceedings of SIGDial.
Sameer Maskey and Julia Hirschberg. 2005. Com-
paring lexical, acoustic/prosodic, structural and dis-
course features for speech summarization. In Pro-
ceedings of Interspeech.
Kathleen Mckeown, Julia Hirschberg, Michel Galley, and
Sameer Maskey. 2005. From text to speech summa-
rization. In Proceedings of ICASSP.
Gabriel Murray and Giuseppe Carenini. 2009. Detecting
subjectivity in multiparty speech. In Proceedings of
Interspeech.
Gabriel Murray, Steve Renals, and Jean Carletta. 2005.
Extractive summarization of meeting recordings. In
Proceedings of EUROSPEECH.
Vincent Ng, Sajib Dasgupta, and S.M.Niaz Arifin. 2006.
Examining the role of linguistic knowledge sources in
the automatic identification and classification of re-
views. In Proceedings of the COLING/ACL.
Hitoshi Nishikawa, Takaaki Hasegawa, Yoshihiro Mat-
suo, and Genichiro Kikui. 2010. Opinion summariza-
tion with integer linear programming formulation for
sentence extraction and ordering. In Proceedings of
COLING.
Bo Pang and Lilian Lee. 2004. A sentiment educa-
tion: sentiment analysis using subjectivity summariza-
tion based on minimum cuts. In Proceedings of ACL.
Michael Paul, ChengXiang Zhai, and Roxana Girju.
2010. Summarizing contrastive viewpoints in opinion-
ated text. In Proceedings of EMNLP.
Ana-Maria Popescu and Oren Etzioni. 2005. Extracting
product features and opinions from reviews. In Pro-
ceedings of HLT-EMNLP.
Stephan Raaijmakers, Khiet Truong, and Theresa Wilson.
2008. Multimodal subjectivity analysis of multiparty
conversation. In Proceedings of EMNLP.
Veselin Stoyanov, Claire Cardie, and Janyce Wiebe.
2005. Multi-perspective question answering using the
OpQA corpus. In Proceedings of EMNLP/HLT.
Janyce Wiebe and Ellen Riloff. 2005. Creating sub-
jective and objective sentence classifiers from unan-
notated texts. In Proceedings of CICLing.
Theresa Wilson and Janyce Wiebe. 2003. Annotating
opinions in the world press. In Proceedings of SIG-
Dial.
Theresa Wilson. 2008a. Annotating subjective content in
meetings. In Proceedings of LREC.
Theresa Wilson. 2008b. Fine-grained subjectivity and
sentiment analysis: recognizing the intensity, polarity,
and attitudes of private states. Ph.D. thesis, University
of Pittsburgh.
Shasha Xie and Yang Liu. 2010. Improving super-
vised learning for meeting summarization using sam-
pling and regression. Computer Speech and Lan-
guage, 24:495?514.
Klaus Zechner. 2002. Automatic summarization of
open-domain multiparty dialogues in dive rse genres.
Computational Linguistics, 28:447?485.
Justin Jian Zhang, Ho Yin Chan, and Pascale Fung. 2007.
Improving lecture speech summarization using rhetor-
ical information. In Proceedings of Biannual IEEE
Workshop on ASRU.
Lin Zhao, Lide Wu, and Xuanjing Huang. 2009. Using
query expansion in graph-based approach for query-
focused multi-document summarization. Journal of
Information Processing and Management.
Xiaodan Zhu and Gerald Penn. 2006. Summarization of
spontaneous conversations. In Proceedings of Inter-
speech.
339
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 166?170,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
A Two-step Approach to Sentence Compression of Spoken Utterances
Dong Wang, Xian Qian, Yang Liu
The University of Texas at Dallas
dongwang,qx,yangl@hlt.utdallas.edu
Abstract
This paper presents a two-step approach to
compress spontaneous spoken utterances. In
the first step, we use a sequence labeling
method to determine if a word in the utterance
can be removed, and generate n-best com-
pressed sentences. In the second step, we
use a discriminative training approach to cap-
ture sentence level global information from
the candidates and rerank them. For evalua-
tion, we compare our system output with mul-
tiple human references. Our results show that
the new features we introduced in the first
compression step improve performance upon
the previous work on the same data set, and
reranking is able to yield additional gain, espe-
cially when training is performed to take into
account multiple references.
1 Introduction
Sentence compression aims to preserve the most im-
portant information in the original sentence with
fewer words. It can be used for abstractive summa-
rization where extracted important sentences often
need to be compressed and merged. For summariza-
tion of spontaneous speech, sentence compression
is especially important, since unlike fluent and well-
structured written text, spontaneous speech contains
a lot of disfluencies and much redundancy. The fol-
lowing shows an example of a pair of source and
compressed spoken sentences1 from human annota-
tion (removed words shown in bold):
[original sentence]
1For speech domains, ?sentences? are not clearly defined.
We use sentences and utterances interchangeably when there is
no ambiguity.
and then um in terms of the source the things uh the
only things that we had on there I believe were whether...
[compressed sentence]
and then in terms of the source the only things that we
had on there were whether...
In this study we investigate sentence compres-
sion of spoken utterances in order to remove re-
dundant or unnecessary words while trying to pre-
serve the information in the original sentence. Sen-
tence compression has been studied from formal
text domain to speech domain. In text domain,
(Knight and Marcu, 2000) applies noisy-channel
model and decision tree approaches on this prob-
lem. (Galley and Mckeown, 2007) proposes to use a
synchronous context-free grammars (SCFG) based
method to compress the sentence. (Cohn and La-
pata, 2008) expands the operation set by including
insertion, substitution and reordering, and incorpo-
rates grammar rules. In speech domain, (Clarke and
Lapata, 2008) investigates sentence compression in
broadcast news using an integer linear programming
approach. There is only a few existing work in spon-
taneous speech domains. (Liu and Liu, 2010) mod-
eled it as a sequence labeling problem using con-
ditional random fields model. (Liu and Liu, 2009)
compared the effect of different compression meth-
ods on a meeting summarization task, but did not
evaluate sentence compression itself.
We propose to use a two-step approach in this pa-
per for sentence compression of spontaneous speech
utterances. The contributions of our work are:
? Our proposed two-step approach allows us to
incorporate features from local and global lev-
els. In the first step, we adopt a similar se-
quence labeling method as used in (Liu and
Liu, 2010), but expanded the feature set, which
166
results in better performance. In the second
step, we use discriminative reranking to in-
corporate global information about the com-
pressed sentence candidates, which cannot be
accomplished by word level labeling.
? We evaluate our methods using different met-
rics including word-level accuracy and F1-
measure by comparing to one reference com-
pression, and BLEU scores comparing with
multiple references. We also demonstrate that
training in the reranking module can be tailed
to the evaluation metrics to optimize system
performance.
2 Corpus
We use the same corpus as (Liu and Liu, 2010)
where they annotated 2,860 summary sentences in
26 meetings from the ICSI meeting corpus (Murray
et al, 2005). In their annotation procedure, filled
pauses such as ?uh/um? and incomplete words are
removed before annotation. In the first step, 8 anno-
tators were asked to select words to be removed to
compress the sentences. In the second step, 6 an-
notators (different from the first step) were asked
to pick the best one from the 8 compressions from
the previous step. Therefore for each sentence, we
have 8 human compressions, as well a best one se-
lected by the majority of the 6 annotators in the sec-
ond step. The compression ratio of the best human
reference is 63.64%.
In the first step of our sentence compression ap-
proach (described below), for model training we
need the reference labels for each word, which rep-
resents whether it is preserved or deleted in the com-
pressed sentence. In (Liu and Liu, 2010), they used
the labels from the annotators directly. In this work,
we use a different way. For each sentence, we still
use the best compression as the gold standard, but
we realign the pair of the source sentence and the
compressed sentence, instead of using the labels
provided by annotators. This is because when there
are repeated words, annotators sometimes randomly
pick removed ones. However, we want to keep the
patterns consistent for model training ? we always
label the last appearance of the repeated words as
?preserved?, and the earlier ones as ?deleted?. An-
other difference in our processing of the corpus from
the previous work is that when aligning the original
and the compressed sentence, we keep filled pauses
and incomplete words since they tend to appear to-
gether with disfluencies and thus provide useful in-
formation for compression.
3 Sentence Compression Approach
Our compression approach has two steps: in the
first step, we use Conditional Random Fields (CRFs)
to model this problem as a sequence labeling task,
where the label indicates whether the word should be
removed or not. We select n-best candidates (n = 25
in our work) from this step. In the second step we
use discriminative training based on a maximum En-
tropy model to rerank the candidate compressions,
in order to select the best one based on the quality
of the whole candidate sentence, which cannot be
performed in the first step.
3.1 Generate N-best Candidates
In the first step, we cast sentence compression as
a sequence labeling problem. Considering that in
many cases phrases instead of single words are
deleted, we adopt the ?BIO? labeling scheme, simi-
lar to the name entity recognition task: ?B? indicates
the first word of the removed fragment, ?I? repre-
sents inside the removed fragment (except the first
word), and ?O? means outside the removed frag-
ment, i.e., words remaining in the compressed sen-
tence. Each sentence with n words can be viewed as
a word sequence X1, X2, ..., Xn, and our task is to
find the best label sequence Y1, Y2, ..., Yn where Yi
is one of the three labels. Similar to (Liu and Liu,
2010), for sequence labeling we use linear-chain
first-order CRFs. These models define the condi-
tional probability of each labeling sequence given
the word sequence as:
p(Y |X) ?
exp
Pn
k=1(
P
j ?jfj(yk, yk?1, X) +
P
i ?igi(xk, yk, X))
where fj are transition feature functions (here first-
order Markov independence assumption is used); gi
are observation feature functions; ?j and ?i are their
corresponding weights. To train the model for this
step, we use the best reference compression to obtain
the reference labels (as described in Section 2).
In the CRF compression model, each word is rep-
resented by a feature vector. We incorporate most
of the features used in (Liu and Liu, 2010), includ-
ing unigram, position, length of utterance, part-of-
speech tag as well as syntactic parse tree tags. We
did not use the discourse parsing tree based features
because we found they are not useful in our exper-
iments. In this work, we further expand the feature
set in order to represent the characteristics of disflu-
encies in spontaneous speech as well as model the
adjacent output labels. The additional features we
167
introduced are:
? the distance to the next same word and the next
same POS tag.
? a binary feature to indicate if there is a filled
pause or incomplete word in the following 4-
word window. We add this feature since filled
pauses or incomplete words often appear after
disfluent words.
? the combination of word/POS tag and its posi-
tion in the sentence.
? language model probabilities: the bigram prob-
ability of the current word given the previous
one, and followed by the next word, and their
product. These probabilities are obtained from
the Google Web 1T 5-gram.
? transition features: a combination of the current
output label and the previous one, together with
some observation features such as the unigram
and bigrams of word or POS tag.
3.2 Discriminative Reranking
Although CRFs is able to model the dependency
of adjacent labels, it does not measure the quality
of the whole sentence. In this work, we propose
to use discriminative training to rerank the candi-
dates generated in the first step. Reranking has been
used in many tasks to find better global solutions,
such as machine translation (Wang et al, 2007),
parsing (Charniak and Johnson, 2005), and disflu-
ency detection (Zwarts and Johnson, 2011). We use
a maximum Entropy reranker to learn distributions
over a set of candidates such that the probability of
the best compression is maximized. The conditional
probability of output y given observation x in the
maximum entropy model is defined as:
p(y|x) = 1Z(x)exp
[?k
i=1 ?if(x, y)
]
where f(x, y) are feature functions and ?i are their
weighting parameters; Z(x) is the normalization
factor.
In this reranking model, every compression can-
didate is represented by the following features:
? All the bigrams and trigrams of words and POS
tags in the candidate sentence.
? Bigrams and trigrams of words and POS tags in
the original sentence in combination with their
binary labels in the candidate sentence (delete
the word or not). For example, if the origi-
nal sentence is ?so I should go?, and the can-
didate compression sentence is ?I should go?,
then ?so I 10?, ?so I should 100? are included
in the features (1 means the word is deleted).
? The log likelihood of the candidate sentence
based on the language model.
? The absolute difference of the compression ra-
tio of the candidate sentence with that of the
first ranked candidate. This is because we try
to avoid a very large or small compression ra-
tio, and the first candidate is generally a good
candidate with reasonable length.
? The probability of the label sequence of the
candidate sentence given by the first step CRFs.
? The rank of the candidate sentence in 25 best
list.
For discriminative training using the n-best can-
didates, we need to identify the best candidate from
the n-best list, which can be either the reference
compression (if it exists on the list), or the most
similar candidate to the reference. Since we have
8 human compressions and also want to evaluate
system performance using all of them (see exper-
iments later), we try to use multiple references in
this reranking step. In order to use the same train-
ing objective (maximize the score for the single best
among all the instances), for the 25-best list, if m
reference compressions exist, we split the list into
m groups, each of which is a new sample containing
one reference as positive and several negative can-
didates. If no reference compression appears in 25-
best list, we just keep the entire list and label the in-
stance that is most similar to the best reference com-
pression as positive.
4 Experiments
We perform a cross-validation evaluation where one
meeting is used for testing and the rest of them are
used as the training set. When evaluating the system
performance, we do not consider filled pauses and
incomplete words since they can be easily identi-
fied and removed. We use two different performance
metrics in this study.
? Word-level accuracy and F1 score based on the
minor class (removed words). This was used
in (Liu and Liu, 2010). These measures are ob-
tained by comparing with the best compression.
In evaluation we map the result using ?BIO? la-
bels from the first-step compression to binary
labels that indicate a word is removed or not.
168
? BLEU score. BLEU is a widely used metric
in evaluating machine translation systems that
often use multiple references. Since there is a
great variation in human compression results,
and we have 8 reference compressions, we ex-
plore using BLEU for our sentence compres-
sion task. BLEU is calculated based on the pre-
cision of n-grams. In our experiments we use
up to 4-grams.
Table 1 shows the averaged scores of the cross
validation evaluation using the above metrics for
several methods. Also shown in the table is the com-
pression ratio of the system output. For ?reference?,
we randomly choose one compression from 8 ref-
erences, and use the rest of them as references in
calculating the BLEU score. This represents human
performance. The row ?basic features? shows the
result of using all features in (Liu and Liu, 2010)
except discourse parsing tree based features, and us-
ing binary labels (removed or not). The next row
uses this same basic feature set and ?BIO? labels.
Row ?expanded features? shows the result of our ex-
panded feature set using ?BIO? label set from the
first step of compression. The last two rows show
the results after reranking, trained using one best ref-
erence or 8 reference compressions, respectively.
accuracy F1 BLEU ratio (%)
reference 81.96 69.73 95.36 76.78
basic features (Liu
and Liu, 2010)
76.44 62.11 91.08 73.49
basic features, BIO 77.10 63.34 91.41 73.22
expanded features 79.28 67.37 92.70 72.17
reranking
train w/ 1 ref 79.01 67.74 91.90 70.60
reranking
train w/ 8 refs 78.78 63.76 94.21 77.15
Table 1: Compression results using different systems.
Our result using the basic feature set is similar to
that in (Liu and Liu, 2010) (their accuracy is 76.27%
when compression ratio is 0.7), though the experi-
mental setups are different: they used 6 meetings as
the test set while we performed cross validation. Us-
ing the ?BIO? label set instead of binary labels has
marginal improvement for the three scores. From
the table, we can see that our expanded feature set is
able to significantly improve the result, suggesting
the effectiveness of the new introduced features.
Regarding the two training settings in reranking,
we find that there is no gain from reranking when
using only one best compression, however, train-
ing with multiple references improves BLEU scores.
This indicates the discriminative training used in
maximum entropy reranking is consistent with the
performance metrics. Another reason for the per-
formance gain for this condition is that there is less
data imbalance in model training (since we split the
n-best list, each containing fewer negative exam-
ples). We also notice that the compression ratio af-
ter reranking is more similar to the reference. As
suggested in (Napoles et al, 2011), it is not appro-
priate to compare compression systems with differ-
ent compression ratios, especially when considering
grammars and meanings. Therefore for the com-
pression system without reranking, we generated re-
sults with the same compression ratio (77.15%), and
found that using reranking still outperforms this re-
sult, 1.19% higher in BLEU score.
For an analysis, we check how often our sys-
tem output contains reference compressions based
on the 8 references. We found that 50.8% of sys-
tem generated compressions appear in the 8 refer-
ences when using CRF output with a compression
ration of 77.15%; and after reranking this number
increases to 54.8%. This is still far from the oracle
result ? for 84.7% of sentences, the 25-best list con-
tains one or more reference sentences, that is, there
is still much room for improvement in the reranking
process. The results above also show that the token
level measures by comparing to one best reference
do not always correlate well with BLEU scores ob-
tained by comparing with multiple references, which
shows the need of considering multiple metrics.
5 Conclusion
This paper presents a 2-step approach for sentence
compression: we first generate an n-best list for each
source sentence using a sequence labeling method,
then rerank the n-best candidates to select the best
one based on the quality of the whole candidate sen-
tence using discriminative training. We evaluate the
system performance using different metrics. Our re-
sults show that our expanded feature set improves
the performance across multiple metrics, and rerank-
ing is able to improve the BLEU score. In future
work, we will incorporate more syntactic informa-
tion in the model to better evaluate sentence quality.
We also plan to perform a human evaluation for the
compressed sentences, and use sentence compres-
sion in summarization.
169
6 Acknowledgment
This work is partly supported by DARPA un-
der Contract No. HR0011-12-C-0016 and NSF
No. 0845484. Any opinions expressed in this ma-
terial are those of the authors and do not necessarily
reflect the views of DARPA or NSF.
References
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proceedings of the 43rd Annual Meeting
on Association for Computational Linguistics, pages
173?180, Stroudsburg, PA, USA. Proceedings of ACL.
James Clarke and Mirella Lapata. 2008. Global infer-
ence for sentence compression an integer linear pro-
gramming approach. Journal of Artificial Intelligence
Research, 31:399?429, March.
Trevor Cohn and Mirella Lapata. 2008. Sentence com-
pression beyond word deletion. In Proceedings of
COLING.
Michel Galley and Kathleen R. Mckeown. 2007. Lex-
icalized Markov grammars for sentence compression.
In Proceedings of HLT-NAACL.
Kevin Knight and Daniel Marcu. 2000. Statistics-based
summarization-step one: Sentence compression. In
Proceedings of AAAI.
Fei Liu and Yang Liu. 2009. From extractive to abstrac-
tive meeting summaries: can it be done by sentence
compression? In Proceedings of the ACL-IJCNLP.
Fei Liu and Yang Liu. 2010. Using spoken utterance
compression for meeting summarization: a pilot study.
In Proceedings of SLT.
Gabriel Murray, Steve Renals, and Jean Carletta. 2005.
Extractive summarization of meeting recordings. In
Proceedings of EUROSPEECH.
Courtney Napoles, Benjamin Van Durme, and Chris
Callison-Burch. 2011. Evaluating Sentence Com-
pression: Pitfalls and Suggested Remedies. In Pro-
ceedings of the Workshop on Monolingual Text-To-Text
Generation, pages 91?97, Portland, Oregon, June. As-
sociation for Computational Linguistics.
Wen Wang, A. Stolcke, and Jing Zheng. 2007. Rerank-
ing machine translation hypotheses with structured
and web-based language models. In Proceedings of
IEEE Workshop on Speech Recognition and Under-
standing, pages 159?164, Kyoto.
Simon Zwarts and Mark Johnson. 2011. The impact of
language models and loss functions on repair disflu-
ency detection. In Proceedings of ACL.
170
Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, ACL-HLT 2011, pages 161?167,
24 June, 2011, Portland, Oregon, USA c?2011 Association for Computational Linguistics
A Cross-corpus Study of Unsupervised Subjectivity Identification
based on Calibrated EM
Dong Wang Yang Liu
The University of Texas at Dallas
{dongwang,yangl}@hlt.utdallas.edu
Abstract
In this study we investigate using an unsu-
pervised generative learning method for sub-
jectivity detection in text across different do-
mains. We create an initial training set using
simple lexicon information, and then evaluate
a calibrated EM (expectation-maximization)
method to learn from unannotated data. We
evaluate this unsupervised learning approach
on three different domains: movie data, news
resource, and meeting dialogues. We also per-
form a thorough analysis to examine impact-
ing factors on unsupervised learning, such as
the size and self-labeling accuracy of the ini-
tial training set. Our experiments and analysis
show inherent differences across domains and
performance gain from calibration in EM.
1 Introduction
Subjectivity identification is to identify whether an
expression contains opinion or sentiment. Auto-
matic subjectivity identification can benefit many
natural language processing (NLP) tasks. For ex-
ample, information retrieval systems can provide af-
fective or informative articles separately (Pang and
Lee, 2008). Summarization systems may want to
summarize factual and opinionated content differ-
ently (Murray and Carenini, 2008). In this paper,
we perform subjectivity detection at sentence level,
which is more appropriate for some subsequent pro-
cessing such as opinion summarization.
Previous work has shown that when enough la-
beled data is available, supervised classification
methods can achieve high accuracy for subjectivity
detection in some domains. However, it is often ex-
pensive to create such training data. On the other
hand, a lot of unannotated data is readily available
in various domains. Therefore an interesting and
important problem is to develop semi-supervised or
unsupervised learning methods that can learn from
an unannotated corpus. In this study, we use an un-
supervised learning approach where we first use a
knowledge-based method to create an initial train-
ing set, and then apply a calibrated EM method
to learn from an unannotated corpus. Our experi-
ments show significant differences among the three
domains: movie, news article, and meeting dialog.
This can be explained by the inherent difference of
the data, especially the task difficulty and classifier?s
performance for a domain. We demonstrate that for
some domains (e.g., movie data) the unsupervised
learning methods can rival the supervised approach.
2 Related Work
In the early age, knowledge-based methods were
widely used for subjectivity detection. They used
a lexicon or patterns and rules to predict whether a
target is subjective or not. These methods tended
to yield a high precision and low recall, or low
precision and high recall (Kim and Hovy, 2005).
Recently, machine learning approaches have been
adopted more often (Ng et al, 2006). There are
limitations in both methods. In knowledge-based
approaches, a predefined subjectivity lexicon may
not adapt well to different domains. While in ma-
chine learning approach, human labeling efforts are
required to create a large training set.
To overcome the above drawbacks, unsupervised
or semi-supervised methods have been explored in
sentiment analysis. For polarity classification, some
previous work used spectral techniques (Dasgupta
and Ng, 2009) or co-training (Li et al, 2010) to
mine the reviews in a semi-supervised manner. For
subjectivity identification, Wiebe and Riloff (Wiebe
and Riloff, 2005) applied a rule-based method to
create a training set first and then used it to train
a naive Bayes classifier. Melville et al (Melville
et al, 2009) used a pooling multinomial method to
combine lexicon derived probability and statistical
probability.
Our work is similar to the study in (Wiebe and
Riloff, 2005) in that we both use a rule-based
method to create an initial training set and learn from
161
unannotated corpus. However, there are two key dif-
ferences. First, unlike the self-training method they
used, we use a calibrated EM iterative learning ap-
proach. Second, we compare the results on three dif-
ferent corpora in order to evaluate the domain/genre
effect of the unsupervised method. Our cross-
corpus study shows how the unsupervised learning
approach performs in different domains and helps us
understand what are the factors impacting the learn-
ing methods.
3 Data
We use three data sets from different domains:
movie, news resource, and meeting conversations.
The first two are from written text domain and have
been widely used in many previous studies for sen-
timent analysis (Pang and Lee, 2004; Raaijmakers
and Kraaij, 2008). The third one is from speech
transcripts. It has been used in a few recent stud-
ies (Raaijmakers et al, 2008; Murray and Carenini,
2009), but not as much as those text data. The fol-
lowing provides more details of the data.
? The first corpus is movie data (Pang and Lee,
2004). It contains 5,000 subjective sentences
collected from movie reviews and 5,000 objec-
tive sentences collected from movie plot sum-
maries. The sentences in each collection are
randomly ordered.
? The second one is extracted from MPQA cor-
pus (version 2.0) (Wilson and Wiebe, 2003),
which is collected from news articles. This data
has been annotated with subjective information
at phrase level. We adopted the same rules as in
(Riloff and Wiebe, 2003) to create the sentence
level label: if a sentence has at least one pri-
vate state of strength medium or higher, then
the sentence is labeled SUBJECTIVE, other-
wise it is labeled OBJECTIVE. We randomly
extracted 5,000 subjective and 5,000 objective
sentences from this corpus to make it compara-
ble with the movie data.
? The third data set is from AMI meeting cor-
pus. It has been annotated using the scheme
described in (Wilson, 2008). There are 3 main
categories of annotations regarding sentiments:
subjective utterances, subjective questions, and
objective polar utterances. We consider the
union of subjective utterance and subjective
question as subjective and the rest as objective.
The subjectivity classification task is done at
the dialog act (DA) levels. We label each DA
using the label of the utterance that has over-
lap with it. We create a balanced data set us-
ing this corpus, containing 9,892 DAs in to-
tal. This number is slightly less than those for
movie and MPQA data because of the available
data size in this corpus. The data is also ran-
domly ordered without considering the role of
the speaker and which meeting it belongs to.
Table 1 summarizes statistics for the three data
sets. We can see that sentences in meeting dialogs
(AMI data) are generally shorter than the other do-
mains, and that sentences in news domain (MPQA)
are longer, and also have a larger variance. In ad-
dition, the inter-annotator agreement on AMI data
is quite low, which shows it is even difficult for hu-
man to determine whether an utterance contains sen-
timent in meeting conversations.
Movie MPQA AMI
min 3 1 3
sent length max 100 246 67
mean 20.37 22.38 8.78
variance 75.26 147.18 34.26
vocabulary size 15,847 13,414 3,337
Inter-annotator agreement N/A 0.77 0.56
Table 1: Statistics for the three data sets: movie, MPQA, and
AMI data. The inter-annotator agreement on movie data is not
available because it is not annotated by human.
4 Unsupervised Subjectivity Detection
In this section, we describe our unsupervised learn-
ing process that uses a knowledge-based method to
create an initial training set, and then uses a cali-
brated EM approach to incorporate unannotated data
into the learning process. We use a naive Bayes clas-
sifier as the base supervised classifier with a bag-of-
words model.
4.1 Create Initial Training Set
A lexicon-based method is used to create an initial
training set, since it can often achieve high precision
rate (though low recall) for subjectivity detection.
We use a subjectivity lexicon (Wilson et al, 2005)
to calculate the subjectivity score for each sentence.
162
This lexicon contains 8,221 entries that are catego-
rized into strong and weak subjective clues.
For each word w, we assign a subjectivity score
sub(w): 1 to strong subjective clues, 0.5 to weak
clues, and 0 for any other word. Then the subjec-
tivity score of a sentence is the sum of the values of
all the words in the sentence, normalized by the sen-
tence length. We noticed that for sentences labeled
as SUBJECTIVE in the three corpora, the subjective
clues appear more frequently in movie data than the
other two corpora. Thus we perform different nor-
malization for the three data sets to obtain the sub-
jectivity score for each sentence, sub(s): Equation
1 for the movie data, and Equation 2 for MPQA and
AMI data.
sub(s) =
?
w?s
sub(w)/sent length (1)
sub(s) =
?
w?s
sub(w)/log(sent length) (2)
We label the topm sentences with the highest sub-
jective scores as SUBJECTIVE, and label m sen-
tences with the lowest scores as OBJECTIVE. These
2m sentences form the initial training set for the it-
erative learning methods.
4.2 Calibrated EM Naive Bayes
Expectation-Maximization (EM) naive Bayes
method is a semi-supervised algorithm proposed in
(Nigam et al, 2000) for learning from both labeled
and unlabeled data. In the implementation of EM,
we iterate the E-step and M-step until model param-
eters converge or a predefined iteration number is
reached. In E-step, we use naive Bayes classifier to
estimate the posterior probabilities of each sentence
si belonging to each class cj (SUBJECTIVE and
OBJECTIVE), P (cj |si):
P (cj |si) =
P (cj)
?|si|
k=1 P (wk|cj)
?
cl?C
P (cl)
?|si|
k=1 P (wk|cl)
(3)
The M-step uses the probabilistic results from
the E-step to recalculate the parameters in the naive
Bayes classifier, the probability of word wt in class
cj and the prior probability of class cj :
P (wt|cj) =
0.1 +
?
si?S
N(wt, si)P (cj |si)
0.1? |V |+
?|V |
k=1
?
si?S
N(wk, si)P (cj |si)
(4)
P (cj) =
0.1 +
?
si?S
P (cj |si)
0.1? |C|+ |S|
(5)
S is the set of sentences. N(wt, si) is the count of
word wt in a sentence si. We use additive smooth-
ing with ? = 0.1 for probability parameter estima-
tion. |C| is the number of classes, which is 2 in our
case, and |V| is the vocabulary size, obtained from
the entire data set.
In the first iteration, we assign P (cj |si) using the
pseudo training data generated based on lexicon in-
formation. If a sentence is labeled SUBJECTIVE,
then P (sub|si) is 1 and P (obj|si) is 0; for the sen-
tences with OBJECTIVE labels, P (sub|si) is 0 and
P (obj|si) is 1.
In our work, we use a variant of standard EM:
calibrated EM, introduced by (Tsuruoka and Tsujii,
2003). The basic idea of this approach is to shift
the probability values of unlabeled data to the ex-
tent such that the class distribution of unlabeled data
is identical to the distribution in labeled data (bal-
anced class in our case). In our approach, before
model training (?M-step?) in each iteration, we ad-
just the posterior probability of each sentence in the
following steps:
? Transform the posterior probabilities through
the inverse function of the sigmoid function.
The outputs are real values.
? Sort them and use the median of all the values
as the border value. This is because our data is
balanced.
? Subtract this border value from the transformed
values.
? Transform the new values back into probability
values using a sigmoid function.
Note that there is a caveat here. We are assum-
ing we know the class distribution, based on labeled
training data or human knowledge. This is often a
reasonable assumption. In addition, we are assum-
ing that this class distribution is the same for the
unlabeled data. If this is not true, then the distri-
bution adjustment performed in calibrated EM may
hurt system performance.
5 Empirical Evaluation
In this section, we evaluate our unsupervised learn-
ing method and analyze various impacting factors.
163
In preprocessing, we removed the punctuation and
numbers from the data and performed word stem-
ming. To measure performance, we use classifica-
tion accuracy.
5.1 Unsupervised Learning Results
In experiments of unsupervised learning, we per-
form 5-fold cross validation. We divide the cor-
pus into 5 parts with equal size (each with balanced
class distribution). In each run we reserve one part
as the test set. From the remaining data, we use
the lexicon-based method to create the initial train-
ing data, containing 1,000 SUBJECTIVE and 1,000
OBJECTIVE sentences. The rest is used as unla-
beled data to perform iterative learning. The final
model is then applied to the reserved test set. Fig-
ure 1 shows the learning curves of calibrated EM on
movie, MPQA and AMI data respectively.
556065707580859095
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
itera
tion
accuracy(%)
556065707580859095
mov
ie
MPQ
A
AMI
Figure 1: Calibrated EM results using unsupervised setting
(2,000 self-labeled initial samples) on movie, MPQA, and AMI
data.
On movie data, calibrated EM improves the per-
formance significantly (p<0.005), compared to that
based on the initial training set (iteration 0). It takes
only a few iterations for the EM method to converge
and at the end of the iteration, it achieves 90.15%
accuracy, which rivals the fully supervised learn-
ing performance (91.31% when using all the 8,000
labeled sentences for training). On MPQA data,
this method yields some improvement (p<0.1) com-
pared to the initial point. But there is a peak accu-
racy in the first couple of iterations, and then perfor-
mance starts dropping thereafter. On AMI data, the
performance degrades after the first iteration.
5.2 Analysis and Discussion
5.2.1 Effect of initial set
For unsupervised learning, our first question is
how the accuracy and size of the initial training set
affect performance. We calculate the self-labeling
accuracy for the initial set using the lexicon based
method. Table 2 shows the labeling accuracy when
using different initial size, measured for SUBJEC-
TIVE and OBJECTIVE class separately. In addi-
tion, we present the classification performance on
the test set when using the naive Bayes classifier
trained from the initial set. Each size in the table
represents the total number of sentences in the ini-
tial set.
Table 2 shows that when the size is 2,000 (as we
used in previous experiments), the accuracy for both
classes on MPQA are even better than on movies,
even though we have seen that iterative learning
methods perform much better on movies, suggest-
ing that the initial data set accuracy is not the reason
for the worse performance on MPQA than movies.
It also shows that on movie data, as the initial size
increases, the accuracy of the pseudo training set de-
creases, which is as expected (the top ranked self-
labeled samples are more confident and accurate).
However, this is not the case on MPQA and AMI
data. There is no obvious drop of accuracy, rather in
many cases accuracy even increases when the initial
size increases. It shows that on these two corpora,
our lexicon-based method does not perform very
well because the most highly ranked sentences ac-
cording to the subjective lexicon are not those most
subjective sentences.
size 100 200 1000 2000 3000
movie
sub 95.20 92.20 82.48 79.24 77.13
obj 82.20 82.00 80.88 79.04 77.31
Acc Test 59.93 71.63 77.62 79.24 79.64
MPQA
sub 83.20 85.60 85.76 85.18 82.53
obj 87.60 86.60 87.64 87.46 85.92
Acc Test 60.45 63.83 66.98 68.75 70.05
AMI
sub 49.60 53.40 65.96 66.98 67.05
obj 71.60 71.00 68.56 69.04 69.89
Acc Test 50.51 53.81 60.53 60.39 60.46
Table 2: Initial pseudo training accuracy for SUBJECTIVE
(sub) and OBJECTIVE (obj) class, and performance on the test
using this initial training set (Acc Test). Results (all in %) are
shown for different initial data size.
From the results on the test set, we find that when
164
the size is smaller, such as containing 100 or 200
samples, the accuracy on test set is lower than using
a bigger initial set. This is mainly because there is
not sufficient data for model training. For AMI data,
this is also due to the low accuracy in the training set.
When the initial size is large enough, the improve-
ment from a larger training set is not as substantial,
for example, using 1,000, 2,000, or 3,000 sentences.
On AMI data, there is almost no difference among
the three sets. There is a tradeoff between the two
factors, self-labeling accuracy and the data size. Of-
ten an improvement in one aspect causes degrada-
tion of the other. A reasonable starting point needs
to be chosen considering both factors. Overall, it
shows that the performance on test set can benefit
more from using a larger initial training set, though
it may be noisy.
In order to further investigate the impact of self-
labeled initial data set, we perform standard semi-
supervised learning using reference labels in the
initial data set. The learning curve of this semi-
supervised setting is shown in Figure 2.
63687378838893
0
1
2
3
4
5
6
7
8
91
01
11
21
31
41
51
61
71
81
9
iteration
a c c u r a c y ( % )
63687378838893
mov
ie
MPQ
A
AMI
Figure 2: Calibrated EM results using semi-supervised learn-
ing (2,000 labeled seed) on movie, MPQA, and AMI data.
On movie data, calibrated EM yields better per-
formance over that based on the initial training data
(iteration 0). We can see that calibrated EM con-
verges very fast and achieves very high performance
in the first iteration. On MPQA and AMI data, cali-
brated EM increases the accuracy at the first iteration
but then degrades thereafter. This shows that incor-
porating unlabeled data in training is helpful, how-
ever, more EM iterations do not yield further gain.
We noticed that on AMI data, even when the ini-
tial set has 100% accuracy (i.e., semi-supervised set-
ting), it still fails to yield any performance gain on
AMI data. It shows that the low accuracy of initial
training set does not explain the poor performance
of unsupervised learning method. Therefore, we
conducted another set of experiments which use the
same semi-supervised setting but start from different
initial training sizes. We observed that on MPQA
and AMI data, calibrated EM is able to increase the
accuracy only when the initial training set is small
(less than 100 instances) and the performance at the
start point is poor. We believe this is related to the
data property and the assumptions used in EM. Sim-
ilar patterns have been found in some previous stud-
ies (Chapelle et al, 2006). They attribute this to the
incorrect model assumption, i.e., when the modeling
assumptions for a particular classifier do not match
the characteristics of the distribution of the data, un-
labeled data may degrade the performance of classi-
fiers.
5.2.2 Effect of calibration
Figure 3 compares calibrated EM with standard
EM using unsupervised learning on the three do-
mains. We can see that calibrated EM outperforms
standard EM, with a larger improvement on MPQA
and AMI data. When using standard EM, we find
that there is a larger difference between the number
of instances in the two classes based on the model?s
prediction on MPQA and AMI data than movie data.
For example, in one run using EM, in the first iter-
ation the ratio of the two classes is 2.21, 1.88, and
1.23 for MPQA, AMI, and movie data respectively.
Calibrated EM is more effective on the two domains
because it adjusts the posterior probability of each
sample according to the class distribution in the data,
making it more accurate in training the model in the
next iteration.
5.2.3 Error analysis
There are two points worth discussing based on
our error analysis.
A. Domain difference.
Much of the difference we have observed can be
attributed to the genre difference. In movie reviews,
often a person expresses his/her favor (or not) of the
movie explicitly, making the task relatively easy for
automatic subjectivity classification. MPQA data
is collected from news resource, where subjectiv-
ity mostly means an attitude or a judgment. Take
165
  
7880828486889092
7880828486889092
mov
ie_EM
mov
ie_cali_EM
6869707172
a c c u r a c y ( % )
6869707172
MPQ
A_EM
MPQ
A_cali_EM
575859606162
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
itera
tion
56575859606162
AMI
_EM
AMI
_cali_EM
Figure 3: Comparison of standard EM and calibrated EM.
the following sentence as an example: ?The United
States is prepared to fight terrorism alone?. It is la-
beled as SUBJECTIVE because it expresses a deter-
mination. However, it may also be interpreted as an
objective statement.
The AMI corpus consists of meeting conversa-
tions. The free-style dialogues are very different
from the style in review and news articles. There are
many incomplete sentences and disfluencies. More
importantly, the meaning of a sentence is often con-
text dependent. In the examples shown below, the
two sentences look very similar, however, the first
sentence is labeled as ?OBJECTIVE?, and the sec-
ond one as ?SUBJECTIVE?. This is because of the
different context and speaker information ? the sec-
ond sentence expresses agreement, but the first ex-
ample is just a sequence of discourse marker words.
? Alright yeah okay
? Yeah okay, true, true.
We notice that many of the classification errors in
AMI occur in very short sentences, like in the ex-
ample shown above. These short sentences are very
ambiguous for subjectivity classification.
B. Limitation of the bag-of-word model.
Our analysis also showed that some sentences are
difficult to classify if simply using surface words. In
the following, we show some examples of system
errors.
False negatives: subjective sentences recognized as
objective
? Johnson has, in his first film, set himself a task he is
not nearly up to. (movie data)
? The news from Israel is almost earth-shattering.
(MPQA)
? We can stick with what we already get. (AMI)
False positives: objective sentences recognized as
subjective
? Cathy (Julianne Moore) is the perfect 50s house-
wife, living the perfect 50s life: healthy kids, suc-
cessful husband, social prominence. (movie data)
? The committee Wednesday opened a formal de-
bate on human rights questions, including alterna-
tive approaches for improving the effective enjoy-
ment of human rights and fundamental freedoms.
(MPQA)
? um uh you know apple been really successful with
this surgical white kind of business or this sleek
kind of (AMI)
In the first three examples, there are no explicit
subjective clues, resulting in false negative errors.
The subjective word ?earth-shattering? is not in-
cluded in subjective lexicon and rarely used in the
corpus. The last three examples contain several sub-
jective words, and are therefore labeled as subjec-
tive. These are the problems with the current word
based approaches.
6 Conclusion and Future Work
This paper investigates an unsupervised learning
procedure for subjectivity identification at sentence
level. We use a lexicon-based method to create ini-
tial training data and then apply a calibrated EM to
utilize unlabeled corpus. We evaluate this method
across three different data sets and observe signif-
icant difference. It yields good performance on
movie data but does not achieve much performance
gain on MPQA corpus, while on AMI corpus it fails
to yield improvement. Our analysis showed that per-
formance of the base classifier has a substantial im-
pact on iterative learning methods. In addition, we
found that calibrated EM outperforms the standard
EM method when the class distribution based on
classifier?s hypotheses does not match the real one.
Our iterative learning approach uses a naive
Bayes classifier that may not have accurate posterior
probabilities. Therefore in our future work, we will
evaluate using other base models. Our cross-corpus
analysis shows poor performance of subjectivity de-
tection in AMI data. We plan to explore more in-
formation from multiparty dialogs to help improve
performance for that domain.
166
7 Acknowledgment
The authors thank Theresa Wilson for sharing annotation
for the AMI corpus and helping with data processing for
that data. Part of this work is supported by an NSF award
CNS-1059226.
References
O. Chapelle, B. Scho?lkopf, and A. Zien, editors. 2006.
Semi-supervised learning. MIT Press.
Sajib Dasgupta and Vincent Ng. 2009. Mine the easy,
classify the hard: a semi-supervised approach to auto-
matic sentiment classification. In Proceedings of ACL-
IJCNLP, pages 701?709.
Soo-Min Kim and Eduard Hovy. 2005. Automatic de-
tection of opinion bearing words and sentences. In
Proceedings of ACL.
Shoushan Li, Chu-Ren Huang, Guodong Zhou, and
Sophia Yat Mei Lee. 2010. Employing per-
sonal/impersonal views in supervised and semi-
supervised sentiment classification. In Proceedings of
ACL, pages 414?423.
Prem Melville, Wojciech Gryc, and Richard D.
Lawrence. 2009. Sentiment analysis of blogs by com-
bining lexical knowledge with text classification. In
Proceedings of ACM SIGKDD, pages 1275?1284.
Gabriel Murray and Giuseppe Carenini. 2008. Summa-
rizing spoken and written conversations. In Proceed-
ings of EMNLP, pages 773?782.
Gabriel Murray and Giuseppe Carenini. 2009. Detecting
subjectivity in multiparty speech. In Proceedings of
Interspeech.
Vincent Ng, Sajib Dasgupta, and S. M. Niaz Arifin. 2006.
Examining the role of linguistic knowledge sources in
the automatic identification and classification of re-
views. In Proceedings of COLING/ACL, pages 611?
618.
Kamal Nigam, Andrew Kachites McCallum, Sebastian
Thrun, and Tom Mitchell. 2000. Text classification
from labeled and unlabeled documents using EM. Ma-
chine Learning, 39:103?134.
Bo Pang and Lilian Lee. 2004. A sentimental educa-
tion: sentiment analysis using subjectivity summariza-
tion based on minimum cuts. In Proceedings of ACL,
pages 271?278.
Bo Pang and Lillian Lee. 2008. Using very simple statis-
tics for review search: An exploration. In Proceedings
of COLING, pages 73?76.
Stephan Raaijmakers and Wessel Kraaij. 2008. A Shal-
low approach to subjectivity classification. In Pro-
ceedings of ICWSM.
Stephan Raaijmakers, Khiet Truong, and Theresa Wilson.
2008. Multimodal subjectivity analysis of multiparty
conversation. In Proceedings of EMNLP, pages 466?
474.
Ellen Riloff and Janyce Wiebe. 2003. Learning extrac-
tion patterns for subjective expressions. In Proceed-
ings of EMNLP, pages 105?112.
Yoshimasa Tsuruoka and Jun?ichi Tsujii. 2003. Train-
ing a naive bayes classifier via the EM algorithm
with a class distribution constraint. In Proceedings
of NAACL, pages 127?134.
Janyce Wiebe and Ellen Riloff. 2005. Creating subjec-
tive and objective sentence classifiers from unanno-
tated texts. In Proceedings of CICLing, pages 486?
497.
Theresa Wilson and Janyce Wiebe. 2003. Annotating
opinions in the world press. In Proceedings of SIG-
dial, pages 13?22.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In Proceedings of HLT-EMNLP,
pages 347?354.
Theresa Wilson. 2008. Annotating subjective content in
meetings. In Proceedings of LREC.
167
