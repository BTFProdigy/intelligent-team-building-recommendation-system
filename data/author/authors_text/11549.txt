Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 149?152,
Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLP
Word to Sentence Level Emotion Tagging for Bengali Blogs  
 
 
Dipankar Das 
Department of Computer Science & 
Engineering, Jadavpur University, India 
dipankar.dipnil2005@gmail.com
Sivaji Bandyopadhyay 
Department of Computer Science & 
Engineering, Jadavpur University, India 
sivaji_cse_ju@yahoo.com 
 
 
Abstract 
 
In this paper, emotion analysis on blog texts 
has been carried out for a less privileged lan-
guage like Bengali. Ekman?s six basic emotion 
types have been selected for reliable and semi 
automatic word level annotation. An automatic 
classifier has been applied for recognizing six 
basic emotion types for different words in a 
sentence. Application of different scoring 
strategies to identify sentence level emotion 
tag based on the acquired word level emotion 
constituents have produced satisfactory per-
formance.  
1 Introduction 
Emotion is a private state that is not open to ob-
jective observation or verification. So, the identi-
fication of the emotional state of natural lan-
guage texts is really a challenging issue. Most of 
the related work has been conducted for English.   
    The approach in this paper is to assign emo-
tion tags on the Bengali blog sentences with one 
of the Ekman?s (1993) six basic emotion types 
such as happiness, sadness, anger, fear, surprise 
and disgust. The system consists of two phases, 
machine learning based word level emotion clas-
sification followed by assignment of sentence 
level emotion tags based on the word level con-
stituents using sense based scoring mechanism. 
The classifier accuracy has been measured 
through confusion matrix. Corpus based and 
sense based tag weights have been calculated for 
each of the six emotion tags and then these emo-
tion tag weights have been used to identify sen-
tence level emotion tag. The tuned reference 
ranges selected from the development set have 
proved effective on the test set.  
The rest of the paper is organized as follows. 
Section 2 describes the related work. Section 3 
briefly describes the resource preparation.  Ma-
chine learning based word level emotion tagging 
system framework and its evaluation results have 
been discussed in section 4. Section 5 describes 
the calculation of tag weights, sentence level 
emotion detection process based on the tag 
weights, evaluation strategies and results. Finally 
section 6 concludes the paper.  
2 Related Work 
(Mishne et al, 2006) used several supervised and 
unsupervised machine learning techniques on 
blog data for comparative evaluation. Importance 
of verbs and adjectives in identifying emotion 
has been explained in (Chesley et al, 2006). 
(Yang et al, 2007) has used Yahoo! Kimo Blog 
corpora containing emoticons associated with 
textual keywords to build emotion lexicons. 
(Chen et al, 2007) has experimented the emotion 
classification task on web blog corpora using 
Support Vector Machine (SVM) and Conditional 
Random Field (CRF) and the observed results 
have shown that the CRF classifiers outperform 
SVM classifiers in case of document level emo-
tion detection. 
3 Resource Preparation  
Bengali is a less computerized language and 
there is no existing emotion word list or Senti-
WordNet in Bengali. The English WordNet Af-
fect lists, (Strapparava et al, 2004) based on Ek-
man?s six basic emotion types have been updated 
with the synsets retrieved from the English Sen-
tiWordNet to have adequate number of emotion 
word entries.  
These lists have been converted to Bengali us-
ing English to Bengali bilingual dictionary 1 . 
These six lists have been termed as Emotion lists. 
A Bengali SentiWordNet is being developed by 
replacing each word entry in the synonymous set 
of the English SentiWordNet (Esuli et al, 2006) 
                                                 
1 http://home.uchicago.edu/~cbs2/banglainstruction.html 
149
by its equivalent Bengali meaning using the same 
English to Bengali bilingual dictionary.  
A knowledge base for the emoticons has been 
prepared by experts after minutely analyzing the 
Bengali blog data. Each image link of the emoti-
con in the raw corpus has been mapped into its 
corresponding textual entity in the tagged corpus 
with the proper emotion tags using the knowl-
edge base. The Bengali blog data have been col-
lected from the web blog archive 
(www.amarblog.com) containing 1300 sentences 
on 14 different topics and their corresponding 
user comments have been retrieved.   
4 Word Level Emotion Classification 
Primarily, the word level annotation has been 
semi-automatically carried out using Ekman?s six 
basic emotion tags. The assignment of emotion 
tag to a word has been done based on the type of 
the Emotion Word lists in which that word is pre-
sent. Other non-emotional words have been 
tagged with neutral type. 1000 sentences have 
been considered for training of the CRF based 
word level emotion classification module. Rest 
200 and 100 sentences, verified by language ex-
perts to perform evaluation have been considered 
as development and test data respectively.  
4.1 Feature Selection and Training  
The Conditional Random Field (CRF) 
(McCallum, 2001) framework has been used for 
training as well as for the classification of each 
word of a sentence into the above-mentioned six 
emotion tags and one neutral tag. By manually 
reviewing the Bengali blog data and different 
language specific characteristics, 10 active fea-
tures have been selected heuristically for our 
classification task. Each feature value is boolean 
in nature, with discrete value for intensity feature 
at the word level. 
x POS information: We are interested with 
the verb, noun, adjective and adverb words 
as these are emotion informative constitu-
ents. For this feature, total 1300 sentences 
has been passed through a Bengali part of 
speech tagger (Ekbal et al 2008) based on 
Support Vector Machine (SVM) tech-
nique. The POS tagger was developed 
with a tagset of 26 POS tags2, defined for 
the Indian languages. The POS tagger has 
demonstrated an overall accuracy of ap-
proximately 90%.  
                                                 
2http://shiva.iiit.ac.in/SPSAL2007/iiit_tagset_guidelines.pdf  
x First sentence in a topic: It has been ob-
served that first sentence of the topic gen-
erally contains emotion (Roth et.al., 2005). 
x SentiWordNet emotion word: A word 
appearing in the SentiWordNet (Bengali) 
contains an emotion. 
x Reduplication: The reduplicated words 
(e.g., bhallo bhallo [good good], khokhono 
khokhono [when when] etc.) in Bengali are 
most likely emotion words. 
x Question words: It has been observed 
that the question words generally contrib-
ute to the emotion in a sentence. 
x Colloquial / Foreign words: The collo-
quial words (e.g., kshyama [pardon] etc.) 
and foreign words (e.g. Thanks, gossya 
[anger] etc.) are highly rich with their 
emotional contents. 
x Special punctuation symbols: The sym-
bols (e.g. !, ?, @ etc ) appearing at the 
word / sentence level convey emotions.  
x Quoted sentence: The sentences espe-
cially remarks or direct speech always 
contain emotion. 
x Negative word: Negative words such as 
na (no), noy (not) etc. reverse the meaning 
of the emotion in a sentence. Such words 
are appropriately tagged. 
x Emoticons: The emoticons and their con-
secutive occurrences generally contribute 
as much as real sentiment to the words or 
sentences that precede or follow it.  
Features  Training       Testing 
Parts of Speech 
First Sentence  
Word in SentiWordNet 
Reduplication 
Question Words 
Coll. / Foreign Words 
Special Symbols  
Quoted Sentence 
Negative Words 
Emoticons 
432              221 
96                13 
684              157   
18                7 
23                11   
      35                9 
      16                4  
      22                8 
      67                27 
      87                33  
        Table 1: Frequencies of different features  
 
Different unigram and bi-gram context fea-
tures (word level as well as POS tag level) and 
their combination has been generated from the 
training corpus. The following sentence contains 
four features (Colloquial word (khyama), special 
150
symbol (!), quoted sentence and emotion word 
(????? [happy])) together and all these four fea-
tures are important to identify the emotion of this 
sentence. 
      k????   ??o!    ??? ??    ?????     ?????  
    (khyama) (dao)!   ?(tumi)  (bhalo)  (lok)?      
    (Forgive)!            ?(you)   (good)   (person)? 
4.2 Evaluation Results of the Word-level 
Emotion Classification   
Evaluation results of the development set have 
demonstrated an accuracy of 56.45%. Error 
analysis has been conducted with the help of 
confusion matrix as shown in Table 2. A close 
investigation of the evaluation results suggests 
that the errors are mostly due to the uneven dis-
tribution between emotion and non-emotion tags.   
 
Tags happy   sad   ang     dis    fear  sur    ntrl 
happy   
sad    
ang      
dis     
fear    
sur    
ntrl 
            0.01   0.05   0.0    0.0    0.0   0.03 
0.006             0.02   0.03  0.0    0.0   0.02 
0.0       0.03             0.0    0.02  0.0   0.01 
0.0       0.0     0.01            0.01  0.0   0.01 
0.0       0.0     0.0     0.0             0.0   0.01 
0.02     0.007 0.0     0.0    0.0            0.01 
0.0       0.0     0.0     0.0    0.0    0.0  
Table 2: Confusion matrix for development set  
 
The number of non-emotional or neutral type 
tags is comparatively higher than other emotional 
tags in a sentence. So, one solution to this unbal-
anced class distribution is to split the ?non-
emotion? (emo_ntrl) class into several subclasses. 
That is, given a POS tagset POS, we generate 
new emotion classes, ?emo_ntrl-C?|C?POS. We 
have 26 sub-classes, which correspond, to non-
emotion tags such as ?emo_ntrl-NN? (common 
noun), ?emo_ntrl-VFM? (verb finite main) etc. 
Evaluation results of the system with the inclu-
sion of this class splitting technique have shown 
the accuracies of 64.65% and 66.74% on the de-
velopment and test data respectively.   
5 Sentence Level Emotion Tagging 
This module has been developed to identify sen-
tence level emotion tags based on the word level 
emotion tags. 
5.1 Calculation of Emotion Tag weights 
Sense_Tag_Weight (STW): The tag weight has 
been calculated using SentiWordNet. We have 
selected the basic six words ?happy?, ?sad?, 
?anger?, ?disgust?, ?fear? ?surprise? as the seed 
words corresponding to each emotion type. The 
positive and negative scores in the English Sen-
tiWordNet for each synset in which each of these 
seed words appear have been retrieved and the 
average of the scores has been fixed as the 
Sense_Tag_Weight of that particular emotion tag.   
Corpus_Tag_Weight (CTW): This tag weight 
for each emotion tag has been calculated based 
on the frequency of occurrence of an emotion tag 
with respect to the total number of occurrences 
of all six types of emotion tags in the annotated 
corpus. 
 
Tag Types        CTW                     STW   
emo_happy 
emo_sad 
emo_ang 
emo_dis 
emo_fear 
emo_sur 
emo_ntrl 
      0.5112                     0.0125 
      0.2327              ( - ) 0.1022 
      0.0959              ( - ) 0.5 
      0.1032              ( - ) 0.075 
      0.0465                     0.0131 
      0.0371                     0.0625 
      0.0                           0.0 
Table 3: CTW and STW for each of six emotion 
tags with neutral tag 
5.2 Scoring Techniques 
The following two scoring techniques depending 
on two calculated tag weights (in section 5.1) 
have been adopted for selecting the best possible 
sentence level emotion tags.  
(1) Sense_Weight_Score (SWS): Each sen-
tence is assigned a Sense_Weight_Score (SWS) 
for each emotion tag which is calculated by di-
viding the total Sense_Tag_Weight (STW)of all 
occurrences of an emotion tag in the sentence by 
the total Sense_Tag_Weight (STW) of all types 
of emotion tags present in that sentence. The 
Sense_Weight_Score is calculated as  
SWSi = (STWi * Ni) / (? j=1 to 7 STWj * Nj) | i ? j 
where SWSi is the Sentence level 
Sense_Weight_Score for the emotion tag i in the 
sentence and Ni is the number of occurrences of 
that emotion tag in the sentence. STWi and STWj 
are the Sense_Tag_Weights for the emotion tags i 
and j respectively. Each sentence has been as-
signed with the sentence level emotion tag SETi 
for which SWSi is highest, i.e., 
SETi = [max i=1 to 6(SWSi)].  
(2) Corpus_Weight_Score (CWS): This meas-
ure is calculated in a similar manner by using the 
CTW of each emotion tag. The corresponding 
Bengali sentence is assigned with the emotion 
tag for which the sentence level CWS is highest. 
The scoring mechanism has been considered for 
verifying any domain related biasness of emotion 
and their influence in emotion detection process.  
151
5.3 Evaluation Results of Sentence Level 
Emotion Tagging 
Each sentence in the development and test sets 
have been annotated with positive or negative or 
neutral valence and with any of the six emotion 
tags. The SWS has been used in identifying va-
lence scores as there is no valence information 
carried by CWS. The sentences for which the 
total SWS produced positive, negative and zero 
(0) values have been tagged as positive, negative 
and neutral type. Any domain biasness through 
CWS has been re-evaluated through SWS also. 
We have taken the Bengali corpus from comic 
related background. So, during analysis on the 
development set, the CWS outperforms the SWS 
significantly in identifying happy, disgust, fear 
and surprise sentence level emotion tags. The 
other SETs have been identified through SWS as 
the CWS for these SETs are significantly less 
than their corresponding SWS as shown in Table 
5. The knowledge and information of the refer-
ence ranges (shown in Table 4) of SWS and 
CWS for assigning valence and six other emotion 
tags, acquired after tuning of development set, 
have been applied on the test set. The valence 
and emotion tag assignment process has been 
evaluated using accuracy measure on test data. 
The difference in the accuracies for the develop-
ment and test sets is negligible. It signifies that 
the best possible reference range for valence and 
other emotion tags have been selected. Results in 
Table 5 show that the system has performed sat-
isfactorily for valence identification as well as 
for sentence level emotion tagging.   
Table 4: Reference ranges 
6 Conclusion  
The hierarchical ordering of the word level to 
sentence level and from sentence level to docu-
ment level can be considered as the well favored 
route to track the document level emotional ori-
entation. The handling of negative words and 
metaphors and their impact in detecting sentence 
level emotion along with document level analysis 
are the future areas to be explored. 
Table 5: Accuracies (in %) of valence and six   
emotion tags in development set before and after 
applying the reference range and in test set 
References  
Andrea Esuli and Fabrizio Sebastiani. 2006. SENTI-
WORDNET: A Publicly Available Lexical Re-
source for Opinion Mining.LREC-06. 
Andrew McCallum, Fernando Pereira and John 
Lafferty. 2001. Conditional Random Fields: Prob-
abilistic Models for Segmenting and labeling Se-
quence Data. ISBN, 282 ? 289.    
A. Ekbal and S. Bandyopadhyay. 2008. Web-based 
Bengali News Corpus for Lexicon Development 
and POS Tagging. POLIBITS, 37(2008):20-29. 
Mexico. 
B. Vincent, L. Xu, P. Chesley and R. K. Srhari. 2006. 
Using verbs and adjectives to automatically clas-
sify blog sentiment.AAAI-CAAW-06. 
Carlo Strapparava, Rada Mihalcea .2007. SemEval-
2007 Task 14: Affective Text. 45th Aunual Meet-
ing of ACL. 
C. Yang, K. H.-Y. Lin, and H.-H. Chen. 2007. Build-
ing Emotion Lexicon from Weblog Corpora, 45th 
Annual Meeting of ACL, pp. 133-136. 
C. Yang, K. H.-Y. Lin, and H.-H. Chen.2007. Emo-
tion Classification from Web Blog Corpora, 
IEEE/WIC/ACM, 275-278. 
Cecilia Ovesdotter Alm, Dan Roth, Richard Sproat. 
2005. Emotions from text: machine learning for 
text-based emotion prediction. Human Language 
Technology and EMNLP, 579-586.Canada. 
G. Mishne and M. de Rijke. 2006. Capturing Global 
Mood Levels using Blog Posts, AAAI, Spring 
Symposium on Computational Approaches to 
Analysing Weblogs, 145-152. 
Paul Ekman. 1993. Facial expression and emotion. 
American Psychologist, 48(4):384?392. 
Category  Reference Range        
Valence (SWS) 
 
happy 
sad 
angry 
disgust 
fear 
surprise 
0 to 2.35 (+ve), 0 to -0.56 
(-ve) and  0.0  neutral)        
0.31 to 1 (CWS)           
-0.15 to -1.6 (SWS)     
-0.5 to -1.9 (SWS)       
0.18 to 1 (CWS)          
0.14 to 1.9 (CWS)       
0.15 to 1.76 (CWS)     
 
Category 
     
        Development         Test         
      Before        After 
CWS    SWS          
Valence  
happy 
sad 
angry 
disgust 
fear 
surprise 
  --        49.56    65.43     66.54 
54.15    10.33    63.88     64.28 
7.66      42.93    64.56     66.42 
15.47    53.44    61.48     60.28 
60.13    17.18    70.19     72.18 
55.57    11.54    66.04     67.14 
50.25    12.39    65.45     66.45 
152
Proceedings of the 7th Workshop on Asian Language Resources, ACL-IJCNLP 2009, pages 76?83,
Suntec, Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Bengali Verb Subcategorization Frame Acquisition - A Baseline Model 
 
Somnath Banerjee                      Dipankar Das                      Sivaji Bandyopadhyay 
Department of Computer Science & Engineering 
Jadavpur University, Kolkata-700032, India 
s.banerjee1980@gmail.com, dipankar.dipnil2005@gmail.com, 
sivaji_cse_ju@yahoo.com 
 
 
 
Abstract 
 
Acquisition of verb subcategorization frames 
is important as verbs generally take different 
types of relevant arguments associated with 
each phrase in a sentence in comparison to 
other parts of speech categories. This paper 
presents the acquisition of different subcate-
gorization frames for a Bengali verb Kara 
(do). It generates compound verbs in Bengali 
when combined with various noun phrases. 
The main hypothesis here is that the subcate-
gorization frames for a Bengali verb are same 
with the subcategorization frames for its 
equivalent English verb with an identical 
sense tag.  Syntax plays the main role in the 
acquisition of Bengali verb subcategorization 
frames. The output frames of the Bengali 
verbs have been compared with the frames of 
the equivalent English verbs identified using 
a Bengali-English bilingual lexicon. The 
flexible ordering of different phrases, addi-
tional attachment of optional phrases in Ben-
gali sentences make this frames acquisition 
task challenging. This system has demon-
strated precision and recall values of 77.11% 
and 88.23% respectively on a test set of 100 
sentences. 
1 Introduction  
A subcategorization frame is a statement of what 
types of syntactic arguments a verb (or an adjec-
tive) takes, such as objects, infinitives, that-
clauses, participial clauses, and subcategorized 
prepositional phrases (Manning,1993). The verb 
phrase in a sentence usually takes various types 
of subcategorization frames compared to phrases 
of other types and hence the acquisition of such 
frames for verbs are really challenging. 
A subcategorization dictionary obtained auto-
matically from corpora can be updated quickly 
and easily as different usages develop. Several 
large, manually developed subcategorization 
lexicons are available for English, e.g. the 
COMLEX Syntax (Macleod et al, 1994), AC-
QUILEX (Copestake, 1992) and the ANLT 
(Briscoe et al, 1987) dictionaries. VerbNet (VN) 
(Kipper-Schuler, 2005) is the largest online verb 
lexicon with explicitly stated syntactic and se-
mantic information based on Levin?s verb classi-
fication (Levin, 1993). It is a hierarchical do-
main-independent, broad-coverage verb lexicon 
with mappings to other lexical resources such as 
WordNet (Miller, 1990), XTAG (XTAG Re-
search Group, 2001) and FrameNet (Baker et al, 
1998). But, there is no existing subcategorization 
lexicon available for Bengali language. The sub-
categorization of verbs is an essential issue in 
parsing for the free phrase order languages such 
as Bengali. As there is no such existing parser 
available in Bengali, the acquisition as well as 
evaluation of the acquired subcategorization 
frames are difficult but crucial tasks. The main 
difference between English and Bengali sentence 
is the variation in the ordering of various 
phrases. The pivotal hypothesis here is that the 
subcategorization frames obtained for a Bengali 
verb are same with the subcategorization frames 
that may be acquired for its equivalent verb with 
an identical sense tag in English. 
The present work deals with the acquisition of 
verb subcategorization frames of a verb kara 
(do) from a Bengali newspaper corpus. This verb 
generates various types of compound verbs in 
combination with other preceding noun phrases 
in Bengali. The sentences containing these types 
of compound verb entries have been retrieved 
from the Bengali corpus. The Bengali verb sub-
categorization frame acquisition task has been 
carried out for the ten most frequent compound 
verbs that contain kara (do) as a component. The 
number of occurrences of other compound verbs 
76
is negligible in the corpus. So, for evaluation 
purpose, we have not considered those verbs. 
Each of the ten Bengali compound verbs has 
been searched in the Bengali-English bilingual 
lexicon1 and the equivalent English verb mean-
ings with its synonyms have been identified and 
retrieved. All possible subcategorization frames 
for each of the English synonyms of the Bengali 
verb have been acquired from the English 
VerbNet2. These frames have been mapped to the 
Bengali sentences that contain the compound 
verb. Evaluation results with a test set of 100 
sentences show the effectiveness of the model 
with precision, recall and F-Measure values of 
77.11%, 88.23% and 79.24% respectively. There 
are some frames that have not been identified 
due to their absence in the corpus. Linguists have 
suggested that these frames do appear in Bengali 
and hence can be acquired.   
The rest of the paper is organized as follows. 
Section 2 gives the description of the related 
works carried out in this area.  Section 3 de-
scribes the framework for the acquisition of sub-
categorization frames for ten compound Bengali 
verbs. Evaluation results of the system are dis-
cussed in section 4. Finally section 5 concludes 
the paper. 
2 Related Work 
One of the early works for identifying verbs that 
resulted in extremely low yields for subcategori-
zation frame acquisition is described in (Brent, 
1991). A rule based system for automatically 
acquiring six verb subcategorization frames and 
their frequencies from a large corpus is men-
tioned in (Ushioda et al, 1993). An open class 
vocabulary of 35,000 words was analyzed manu-
ally in (Briscoe and Carroll, 1997) for subcatego-
rization frames and predicate associations. The 
result was compared against associations in 
ANLT and COMLEX. Variations of subcatego-
rization frequencies across corpus type (written 
vs. spoken) have been studied in (Carroll and 
Rooth, 1998). A mechanism for resolving verb 
class ambiguities using subcategorization frames 
is reported in (Lapata and Brew, 1999). All these 
works deal with English. Several works on the 
term classification of verb diathesis roles or the 
lexical semantics of predicates in natural lan-
guage have been reported in ((McCarthy, 2001), 
                                                                                                 
1 http://home.uchicago.edu/~cbs2/banglainstruction.html 
2 http://verbs.colorado.edu/~mpalmer/projects/verbnet.html 
(Korhonen, 2002), (Stevenson and Merlo, 1999) 
and (Walde, 1998)).  
A cross lingual work on learning verb-
argument structure for Czech language is de-
scribed in (Sarkar and Zeman, 2000).  (Samanta-
ray, 2007) gives a method of acquiring different 
subcategorization frames for the purpose of ma-
chine aided translation system for Indian lan-
guages. The work on subcategorization frame 
acquisition of Japanese verbs using breadth-first 
algorithm is described in (Muraki et al, 1997). 
3     System Outline 
We have developed several modules for the ac-
quisition of verb subcategorization frames from 
the Bengali newspaper corpus. The modules con-
sist of POS tagging and chunking, Identification 
and Selection of Verbs, English Verb Determina-
tion, Frames Acquisition from VerbNet and 
Bengali Verb Subcategorization Frame Acquisi-
tion. 
3.1 POS Tagging and Chunking 
We have used a Bengali news corpus (Ekbal and 
Bandyopadhyay, 2008) developed from the web-
archives of a widely read Bengali newspaper.  A 
portion of the Bengali news corpus containing 
1500 sentences have been POS tagged using a 
Maximum Entropy based POS tagger (Ekbal et 
al., 2008). The POS tagger was developed with a 
tagset of 26 POS tags3, defined for the Indian 
languages. The POS tagger demonstrated an ac-
curacy of 88.2%. We have also developed a rule-
based chunker to chunk the POS tagged data 
with an overall accuracy of 89.4%. 
3.2 Identification and Selection of Verbs 
Our previous work (Das et.al., 2009) on the ac-
quisition of Bengali subcategorization frames 
from the same Bengali news corpus was carried 
out for the most frequent verb ?????? (dekha) 
(see) in that corpus. The next highest frequent 
verb in this corpus is ????? (kara) (do) which is 
a special verb in Bengali. However to the best of 
our knowledge, no frame acquisition task has 
been carried out yet for this Bengali verb. The 
single occurrence of ????? (kara) as a main verb 
in a sentence takes completely different subcate-
gorization frames in comparison with the ac-
quired frames for the compound verbs consisting 
of ????? (kara) as a component. Hence, we have 
 
3http://shiva.iiit.ac.in/SPSAL2007/iiit_tagset_guidelines.pdf  
 
77
concentrated our focus to acquire subcategoriza-
tion frames for the Bengali verb ????? (kara).   
For this purpose, we have manually analyzed 
the tagged and chunked data to identify the word 
????? (kara) that are tagged as main verb (VM) 
and belong to the verb group chunk (VG) in the 
corpus. The preceding noun phrase of ????? 
(kara) generally produces completely different 
verbs in Bengali (e.g. [???? ??? (tairi(NN) 
kara(VM))(make)], [?????? ??? (byabahar (NN) 
kara(VM))(use)] etc.).  
Bengali, like any other Indian languages, is 
morphologically very rich. Different suffixes 
may be attached to a verb depending on the vari-
ous features such as Tense, Aspect, and Person. 
The Bengali stemmer uses a suffix list to identify 
the stem form of the verb ????? (kara). Another 
table stores the stem form and the corresponding 
root form. Stemming process has correctly iden-
tified 234 occurrences of the verb ????? (kara) 
from its 241 occurrences in the corpus with an 
accuracy of 97.09%. The sentences where the 
verb ????? (kara) appears in any inflected form 
but has been tagged as main verb (VM) have 
been retrieved. These sentences have been con-
sidered for fine-grained analysis of verb subcate-
gorization frames. It is expected that the corpus 
will have adequate number of occurrences for 
each subcategorization frame of the verb. The 
passive occurrences of ????? (kara) such as 
???????? (karano), ????? (kariye) have been fil-
tered out and the sentences containing the pas-
sive entries of ????? have not been considered in 
the present work. 
The compound verb phrases with pattern such 
as {[XXX] (NN) [kara] (VM)} have been identi-
fied and retrieved from the Bengali POS tagged 
and chunked corpus. It has been observed that 
most of these compound verb phrases are indi-
vidually different verbs in Bengali. Around 182 
various kinds of verbs have been identified. Cer-
tain typical and distinct occurrences of ????? 
(kara) have also been identified. But, linguistic 
verification shows that these typical verbs are 
formed by attaching the verb ????? (kara) to an 
adjective or an adverb word, like ???? ??? 
(jhakjhak kara) , ???? ??? (taktak kara), ??? 
??? (sheet kara) etc. Such types of around 48 
verb entries have been identified and filtered out 
from the corpus. The rest 134 distinct types of 
Bengali compound verbs (CV) with ????? (kara) 
as a component have been considered as target 
verbs for analysis. 
We have identified the frequencies of these 
verbs in the corpus. It has to be mentioned that 
only a few verbs have an adequate number of 
sentences in the corpus. For this reason, only the 
top ten compound verbs that have the largest 
number of occurrences in the corpus have been 
selected. Table 1 represents the top 10 different 
Bengali compound verbs and their frequencies 
obtained from the corpus.  
 
  Table 1. Top 10 Bengali Compound Verbs and 
their frequencies obtained from the corpus 
Bengali Verbs Freq. 
???? ??? (tairi kara) (make) 23 
?????? ??? (byabahar kara) (use) 18 
??? ??? (bas kara) (live) 17 
??? ??? (kaj kara) (work) 15 
??g? ??? (sangraha kara) (collect) 13 
?n ??? (bandha kara) (shut) 13 
???????   ??? (chitkar kara) (shout) 3 
??? ??? (bhul kara) (mistake) 3 
??j??? ??? (jigyasa kara) (ask) 3 
?????k? ??? (parjabekkhan kara) 
(observe) 
3 
3.3 English Verb Determination  
The verb subcategorization frames for the 
equivalent English verbs (in the same sense) of a 
Bengali verb are the initial set of verb subcatego-
rization frames that have been considered as 
valid for that Bengali verb. The root forms of the 
target verbs appearing in different inflected 
forms in the Bengali corpus have been identified 
by the process described in section 3.2. The de-
termination of equivalent English verbs has been 
carried out using a Bengali-English bilingual 
lexicon. We have used the available Bengali-
English bilingual dictionary that has been for-
matted for the text processing tasks. Various syn-
tactical representations of a word entry in the 
lexicon have been analyzed to identify its syno-
nyms and meanings. The example of an entry in 
the bilingual lexicon for our target verb ????? 
(kara) is given as follows. 
<??? [kar?] v to do, to per-
form, to accomplish, to exe-
cute (??? ???); to build, to 
make (???? ???) ;.....> 
 
But, the various distinct verbs, with ????? 
(kara) as a component have individual separate 
78
entries in the bilingual dictionary. We have iden-
tified the equivalent English verbs from those 
Bengali verb entries in the dictionary. For exam-
ple,  
<???? ??? v. to build, to 
make; ?> 
<?????? ??? v. to apply, to 
use; to behave; to treat (a 
person), to behave towards; 
?> 
<??? ??? v. to work; to 
serve; to be effective ;?> 
 
Different synonyms for a verb having the 
same sense are separated using ?,? and different 
senses are separated using ?;? in the lexicon. The 
synonyms including different senses of the target 
verb have been extracted from the lexicon. This 
yields a resulting set called Synonymous Verb 
Set (SVS). For example, the English synonyms 
(apply, use) and synonym with another sense 
(behave) have been selected for Bengali verb 
??????? ???? (byabahar kara) and have been 
categorized as two different SVS for the Bengali 
verb ??????? ????. Two synonyms (make, build) 
for the Bengali verb ????? ???? (tairi kara) are 
thus present in the same SVS. Now, the task is to 
acquire all the possible existing frames for each 
member of the SVS from the VerbNet. The 
????? (kara) verb may also appear in passive 
form in Bengali sentences. For example,  
 ?????          ???        
(Ramke)NNP  (kaj)NN   
 ??????        ?????? 
(karano)VM  (hayechilo)VAUX 
 
The corresponding dictionary entry for the 
passive form of ????? (kara) is as follows. But in 
this work, we have concentrated only on those 
sentences where ????? (kara) appears in active 
form.  
<?????? [kar?n?] v to cause to 
do or perform or accomplish 
or execute or build or 
make?> 
3.4 Frames Acquisition from VerbNet  
VerbNet associates the semantics of a verb with 
its syntactic frames and combines traditional 
lexical semantic information such as thematic 
roles and semantic predicates, with syntactic 
frames and selectional restrictions. Verb entries 
in the same VerbNet class share common syntac-
tic frames, and thus they are believed to have the 
same syntactic behavior. The VerbNet files con-
taining the verbs with their possible subcategori-
zation frames and membership information is 
stored in XML file format. The Bengali verb ???? 
??? (tairi kora) (make) has no direct class in 
VerbNet. The verb ?make? and its synonymous 
verb ?build? are members of one of the sub-
classes of the build-26.1 class and ?make? is also 
a member of the dub-29.3 class. A snapshot of 
XML file for the build-26.1 class is given below.
..... 
<VNCLASS ID="build-26.1"  
.....<SUBCLASSES> 
   <VNSUBCLASS ID="build-26.1-1"> 
<MEMBERS> 
    <MEMBER name="build" 
wn="build%2:36:00"/> 
    <MEMBER name="make" 
wn="make%2:36:01 make%2:36:05 
..... 
make%2:42:13 make%2:36:10"/> 
.....  
</MEMBERS> 
..... 
<FRAME> 
    <DESCRIPTION descriptionNum-
ber="3.9" primary="NP-PP" secon-
dary="Asset-PP" xtag=""/>            
<EXAMPLES> 
    <EXAMPLE> The contractor 
builds houses for $100,000.     
    </EXAMPLE> 
    .....  
</EXAMPLES> 
.....</FRAME> 
..... 
The verbs in VerbNet that take same type of sub-
categorization frames are stored in the  <MEM-
BER> tag and the possible primary and secon-
dary subcategorization frames are kept in <DE-
SCRIPTION> tag with proper English examples 
for each frame. The example for each of the sub-
categorization frames for the English verb 
"make" has been given in the "build-26.1-1" sub-
class of the ?build-26.1? class in the VerbNet. 
The sentence tagged within <EXAMPLE>.. 
</EXAMPLE> shows that after the occurrence 
of the verb "build/make", one noun phrase (NP) 
and one prepositional phrase (PP) have occurred 
as the arguments of the verb. The frame cor-
responding to this sentence has been described as 
the primary frame "NP-PP" in the frame descrip-
tion <DESCRIPTION> tag. 
79
Sense wise separated SVS members occupy 
the membership of same class or subclass in 
VerbNet. It has been observed that the verbs 
?build? and ?make? are members of the same 
SVS (extracted from the Bengali-English bilin-
gual dictionary) and they are also members of the 
same subclass build-26.1-1. Therefore, both of 
the verbs take same subcategorization frames. 
 
SVS (VerbNet 
classes) 
Primary and Secondary 
Frames for a SVS 
Make (build-
26.1-1) 
Build (build-
26.1-1) 
NP-PP, NP, NP-NP, NP-
NP-PP, Asset-PP 
Asset-Subject 
Use (use-105, 
consume-66, fit-
54.3) 
Apply (use-105) 
NP-ADVP, NP-PP, NP-
TO-INF-VC, Basic 
Transitive, NP-ING-SC, 
Location Subject 
Alternation, NP-PP 
for-PP, Location-PP 
Behave (mas-
querade-29.6, 
29.6-1) 
PP, Basic Transitive 
as-PP, like-PP, in-PP 
Table 2. The SVS members and their subcatego-
rization frames for the corresponding Bengali 
verbs ???? ??? (tairi kara) and  
?????? ??? (byabahar kara) 
 
The xml files of VerbNet have been preproc-
essed to build up a general list that contains all 
members (verbs) and their possible subcategori-
zation frames (primary as well as secondary) in-
formation. This preprocessed list is searched to 
acquire the subcategorization frames for each 
member of the SVS of the ten Bengali verbs 
(identified in section 3.3).  As the verbs are clas-
sified according to their semantics in the 
VerbNet, the frames for the particular Bengali 
verb are assumed to be similar to the frames ob-
tained for the members of its SVS.  It has also 
been observed that the same verb with a different 
sense can belong to a separate class in the 
VerbNet. For example, the acquired frames (pri-
mary and secondary) for each member of the 
SVS of the target verbs (??????? ???? and ????? 
????) have been shown in Table 2. In this way, 
all possible subcategorization frames for each 
member of a SVS have been extracted from the 
generalized search list for our ten target verbs. 
3.5 Bengali Verb Subcategorization Frames 
Acquisition  
The acquired VerbNet frames have been mapped 
to the Bengali verb subcategorization frames by 
considering the position of the verb as well as its 
general co-existing nature with other phrases in 
Bengali sentences. 
    The syntax of ?NP-PP? frame for a Bengali 
sentence has been acquired by identifying the 
target verb followed by a NP chunk and a PREP 
chunk. The sentences containing prepositional 
frame ?PP? do not appear in the Bengali corpus, 
as there is no concept of preposition in Bengali. 
But, when we compare the sentences containing 
postpositional markers, i.e. PREP (postpositions) 
as a probable argument of the verb, the system 
gives the desired output.  
???       ????         ??????? 
(jar)PRP (theke)PREP (hat-
pakha)NN                      
 ??      ?c???        ???? 
(ar)CC   (achhadon)QF 
(toiri)NN                                
????????         ???k 
(korechilen)VM (Max)NN 
 
All the frames of a SVS corresponding to a 
Bengali verb have been considered. The Bengali 
verb ??????? ???? (byabahar kara) in the fol-
lowing sentence has taken the frame ?ADVP-
PRED? (the word with RB tag) from a different 
SVS.  
 ?????????            
(karmachari ra)NN 
 ?nh??? ??          
(bondhuttwapurno)RB                      
??????          ???? 
(byabahar)NN  (karen)VM 
   
Another form of ?ADVP-PRED? frame has 
been obtained by considering the Bengali mean-
ing of the corresponding English adverbial 
phrase. ?There? is an adverbial phrase taken by 
the ?live? verb in English. The corresponding 
representation in the equivalent Bengali verb is 
o????i (okhanei) as shown in the following sen-
tence. Hence, the frame has been identified. 
o????i        ???  
(okhanei)RB (bas)NN         
????        ??? 
(karte)VM  (habe)VAUX  
 
80
The NNPC (Compound proper noun), NNP 
(Proper noun), NNC (Compound common noun) 
and NN (Common noun) POS tags help to de-
termine the subjects, objects as well as the loca-
tive information related to the verb. In simple 
sentences the occurrence of these POS tags pre-
ceded by the PRP (Pronoun) or NNPC tags and 
followed by the verb gives similar frame syntax 
for ?Basic Transitive? frame of the VerbNet. 
Only the components like subject, object and a 
single verb in Bengali as well as in English sen-
tence can be signified as simple ?Basic Transi-
tive? frame. 
 ??            ???         
(se)PRP   NP((rakam)NN   
 ????i???       ???      ???       
(designer)NN) (kaj)NN (kare)VM 
 
The following example shows that the frame 
identified from the sentence is also a ?transitive 
frame? and the secondary frame component is a 
?material object? for that sentence. 
   e??       ??????                                 
The set of acquired subcategorization frames or 
the frame lexicon can be evaluated against a gold 
standard corpus obtained either through manual 
analysis of corpus data or from subcategorization 
frame entries in a large dictionary or from the 
output of the parser made for that language. As 
there is no parser available for the Bengali and 
also no existing dictionary for Bengali that con-
tains subcategorization frames, manual analysis 
from corpus data is the only method for evalua-
tion. The chunked sentences that contain the ten 
most frequent verbs have been evaluated manu-
ally to prepare the gold standard data.  
(ekti)QC (bagaze)NNP 
     ??g?          ????? 
 VGNF((sangroho)NN (korlam)VM) 
 
The PREP (postposition) followed by a NP 
phrase and the target verb gives similar syntax 
for a NP-PP frame but it has been noticed that 
the secondary frame here can be a component of 
?Location-PP?. 
????        ????                                          
We have identified 45 different kinds of verbs 
in the corpus. A detailed statistics of the verb 
????? (kara) is presented in Table 3. During the 
Bengali verb subcategorization frame acquisition 
process, it has been observed that the simple sen-
tences contain most of the frames that the Eng-
lish verb form usually takes in VerbNet. Analysis 
of a simple Bengali sentence to identify the verb 
subcategorization frames is easier in the absence 
of a parser than analyzing complex and com-
pound sentences. There are only three occur-
rences of ????? (kara) as auxiliary in the corpus. 
These are chunking errors as the verb ????? 
(kara) does not occur as auxiliary verb.  
(setu)NNP (theke)PREP 
    ????      u?d? 
NP((nana)JJ (udvid)NN)) 
  p????         ?????k?            
  (projati)JJ (porjobekkhon)NN 
????? 
(korlam)VM  
 
The sentences where the determiner (DEM) 
and a NP chunk follow the target verb the se-
quence (Target verb DEM NP) is considered as 
the frame of sentential complement "S" for that 
target verb. 
 ???        ?????? 
(Ram)NNP (chitkar)(NN)           
???           ??    ?? 
(korlo)VM(je)(DEM) (se)(PRP) 
 ??         ???o 
(ar)CC      (kokhono)NN 
????      ?? 
(asbe)VM (na)NEG  
 
The presence of JJ (Adjective) generally does 
not play any role in the acquisition process of 
verb subcategorization frames. There are some 
frames that did not have any instance in our cor-
pus. Such frames are ?Asset-PP?, ?After-PP?, 
?Location Subject Alternation? and ?NP-TO-
INF-VC? etc. A close linguistic analysis shows 
that these frames can also be acquired from the 
Bengali sentences. They have not occurred in the 
corpus that has been considered for the analysis 
in the present work. 
4 Evaluation 
The verb subcategorization frames acquisition 
process is evaluated using type precision (the 
percentage of subcategorization frame types that 
the system proposes are correct according to the 
gold standard), type recall (the percentage of 
subcategorization frame types in the gold stan-
dard that the system proposes) and F-measure:   
          
 
 
81
The system has been evaluated with 100 gold 
standard test sentences containing ten most fre-
quent verbs and the evaluation results are shown 
in Table 4. The recall of the system shows a sat-
isfactory performance in producing Bengali verb 
subcategorization frames but the precision value 
requires more improvement.   
 
  Information Freq. 
Number of sentences in the corpus      1500 
Number of different verbs in the 
corpus 
45 
Number of inflected forms of the 
verb ????? in the corpus 
49 
Total number of occurrences of the 
verb ????? (before stemming ) in the 
corpus  
241  
Total number of occurrences of the 
verb ????? (after stemming) in the 
corpus  
234 
Number of sentences where ????? 
occurs as a  Main Verb (VM) 
206 
Number of sentences where ????? 
occurs as a Simple Main Verb 
(SVM) 
2 
Number of sentences where ????? 
occurs as a Compound Main Verb 
(CVM) 
204 
Number of sentences where ????? 
occurs as a Passive Verb 
(??????)(done) 
25 
Number of sentences where ????? 
occurs as a  Auxiliary Verb (VAUX) 
3 
Number of simple sentences where 
????? occurs as a Simple Main Verb 
(SVM) 
0 
Number of simple sentences where 
?????  occurs as a Compound Main 
Verb (CVM) 
127 
Table 3. The frequency information of the verb 
????? (kara) acquired from the corpus 
 
Measures Results 
Recall 88.23% 
Precision 71.11% 
F-Measure 79.24 
Table 4. The Precision, Recall and F-Measure 
values of the system 
 
It has been noticed that the absence of other 
frames in the Bengali corpus is due to the free 
phrase ordering characteristics of Bengali Lan-
guage. The proper alignment of the phrases is 
needed to cope up with this language specific 
problem. The number of different frames ac-
quired for these ten verbs is shown in Table 5.  
 
Bengali Verbs Subcategory 
Frames 
No. of 
Frames  
???? ??? 
(toiri kora)  
NP-PP 
NP-NP 
15 
3 
?????? ??? 
(babohar kora) 
NP-ADVP 
NP-PP 
NP-ING-SC 
NP-PP 
Location-PP 
1 
2 
1 
1 
1 
??? ??? (bas 
kora) 
Basic  
Transitive 
PP 
ADVP-
PRED 
 
12 
1 
1 
??? ??? (kaj 
kora) 
PP 
NP-PP 
1 
11 
??g? ??? 
(sangroho 
kora) 
Transitive  
(Material 
obj) 
PP 
1 
 
 
2 
?n ??? 
(bondho kora) 
Basic 
Transitive 
NP-PP 
 
1 
1 
???????   ??? 
(chitkar kora) 
S 
PP 
1 
1 
??? ??? (bhul 
kora) 
Nil 0 
??j??? ??? 
(jigyasa kora) 
BT 1 
?????k? ??? 
(porjobekkhon 
kora) 
Transitive 
(Location-
PP) 
NP-PP 
1 
 
 
1 
Table 5. The frequencies of different frames ac-
quired from corpus 
5 Conclusion 
The acquisition of subcategorization frames for 
more number of verbs and clustering them will 
help us to build a verb lexicon for Bengali lan-
guage. We need to find out Bengali verb sub-
categorization frames that may not be supported 
for the corresponding English verb with identical 
sense. 
82
There is no restriction for domain dependency 
in this system. For the free-phrase-order lan-
guages like Bengali, the overall performance can 
be increased by proper assumptions, rules and 
implementation procedures. Verb morphological 
information, synonymous sets and their possible 
subcategorization frames are all important in-
formation to develop a full-fledged parser for 
Bengali. This system can be used for solving 
alignment problems in Machine Translation for 
Bengali as well as to identify possible argument 
selection for Question and Answering systems.  
References  
Anna Korhonen. 2002. Semantically motivated sub-
categorization acquisition. ACL Workshop on 
Unsupervised Lexical Acquisition. Philadelphia. 
Anoop Sarkar and Daniel Zeman. 2000. Automatic 
extraction of subcategorization frames for czech. 
COLING-2000.
A. Ekbal and S. Bandyopadhyay. 2008. A Web-based 
Bengali News Corpus for Named Entity Recogni-
tion. LRE Journal. Springer. 
A.Ekbal, R. Haque and S. Bandyopadhyay. 2008. 
Maximum Entropy Based Bengali Part of Speech 
Tagging. RCS Journal, (33): 67-78.  
Akira Ushioda, David A. Evans, Ted Gibson, Alex 
Waibel. 1993. The Automatic Acquisition of Fre-
quencies of Verb Subcategorization Frames from 
Tagged Corpora. Workshop on Acquisition of 
Lexical Knowledge from Text, 95-106. Colum-
bus, Ohio.  
B. K. Boguraev and E. J. Briscoe.1987. Large lexi-
cons for natural language processing utilising the 
grammar coding system of the Longman Diction-
ary of Contemporary English. Computational 
Linguistics, 13(4): 219-240.  
Christopher D. Manning. 1993. Automatic Acquisi-
tion of a Large Subcategorization Dictionary from 
Corpora. 31st Meeting of the ACL, 235-242. Co-
lumbus, Ohio. 
Collin F. Baker, Charles J. Fillmore, and John B. 
Lowe.1998. The Berkeley FrameNet project. 
COLING/ACL-98, 86-90. Montreal. 
Copestake A.1992. The ACQUILEX LKB: Represen-
tation Issues in the Semi-automatic Acquisition of 
Large Lexicons. ANLP. Trento, Italy. 
D.Das, A.Ekbal, and S.Bandyopadhyay. 2009. Ac-
quiring Verb Subcategorization Frames in Bengali 
from Corpora. ICCPOL-09, LNAI-5459, 386-
393.Hong Kong.  
 Dan Gusfield. 1997. Algorithms on Strings, Trees 
and Sequences. Cambridge University Press, 
Cambridge, UK. 
Diana McCarthy. 2001. Lexical Acquisition at the 
Syntax-Semantics Interface: Diathesis Alter-
nations, Subcategorization Frames and Selec-
tional Preferences. University of Sussex.  
Grishman, R., Macleod, C., and Meyers, A. 1994. 
Comlex syntax : building a computational lexicon. 
COLING-94, 268-272. Kyoto, Japan.   
George A. Miller. 1990. WordNet: An on-line lexical 
database. International Journal of Lexicogra-
phy, 3(4):235-312.  
Glenn Carroll, Mats Rooth. 1998. Valence induction 
with a head-lexicalized PCFG. EMNLP. Granada.  
Karin Kipper-Schuler.2005. VerbNet: A broad-
coverage, comprehensive verb lexicon. Ph.D. 
thesis, Computer and Information Science Dept., 
University of Pennsylvania, Philadelphia, PA. 
Kazunori Muraki, Shin'ichiro Kamei, Shinichi 
Doi.1997. A Left-to-right Breadth-first Algo-
rithm for. Subcategorization Frame Selection 
of Japanese Verbs. TMI.  
Levin, B. 1993. English Verb Classes and Alterna-
tion: A Preliminary Investigation. The Univer-
sity of  Chicago Press. 
Michael Brent.1991. Automatic acquisition of sub-
categorization frames from untagged text.   29th 
Meeting of the ACL, 209-214. California.  
Maria Lapata, Chris Brew.1999. Using subcategoriza-
tion to resolve verb class ambiguity. 
WVLC/EMNLP, 266-274.  
Suzanne Stevenson, Paola Merlo. 1999. Automatic 
Verb Classification using Distributions of Gram-
matical Features. EACL-99, 45-52. Norge.  
Sabine Schulte im Walde. 1998. Automatic Seman-
tic Classification of Verbs According to Their 
Alternation Behavior. Master's thesis,  Stuttgart. 
S.D. Samantaray.2007. A Data mining approach for 
resolving cases of Multiple Parsing in Machine 
Aided Translation of Indian Languages. ITNG'07 
? IEEE. 
Ted Briscoe, John Carroll.1997. Automatic Extraction 
of Subcategorization from Corpora. ANLP-ACL, 
356-363.  Washington, D.C. 
XTAG Research Group. 2001. A lexicalized tree ad-
joining grammar for English. IRCS. University of 
Pennsylvania. 
83
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 206?209,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
JU: A Supervised Approach to Identify Semantic Relations from Paired 
Nominals 
 
Santanu Pal         Partha Pakray             Dipankar Das          Sivaji Bandyopadhyay
Department of Computer Science & Engineering, Jadavpur University, Kolkata, India 
santanupersonal1@gmail.com,parthapakray@gmail.com,     
dipankar.dipnil2005@gmail.com,sivaji_cse_ju@yahoo.com 
Abstract 
This article presents the experiments carried 
out at Jadavpur University as part of the 
participation in Multi-Way Classification of 
Semantic Relations between Pairs of Nomi-
nals in the SemEval 2010 exercise. Separate 
rules for each type of the relations are iden-
tified in the baseline model based on the 
verbs and prepositions present in the seg-
ment between each pair of nominals. Inclu-
sion of WordNet features associated with 
the paired nominals play an important role 
in distinguishing the relations from each 
other. The Conditional Random Field (CRF) 
based machine-learning framework is 
adopted for classifying the pair of nominals.  
Application of dependency relations, 
Named Entities (NE) and various types of 
WordNet features along with several com-
binations of these features help to improve 
the performance of the system. Error analy-
sis suggests that the performance can be im-
proved by applying suitable strategies to 
differentiate each paired nominal in an al-
ready identified relation. Evaluation result 
gives an overall macro-averaged F1 score of 
52.16%.     
1 Introduction 
Semantic Relations describe the relations between 
concepts or meanings that are crucial but hard to 
identify. The present shared task aims to develop 
the systems for automatically recognizing semantic 
relations between pairs of nominals. Nine relations 
such as Cause-Effect, Instrument-Agency, Product-
Producer, Content-Container, Entity-Origin, En-
tity-Destination, Component-Whole, Member-
Collection and Message-Topic are given for Se-
mEval-2010 Task #8 (Hendrix et al, 2010). The 
relation that does not belong to any of the nine re-
lations is tagged as Other. The first five relations 
also featured in the previous SemEval-2007 Task 
#4.  
The present paper describes the approach of 
identifying semantic relations between pair of 
nominals. The baseline system is developed based 
on the verbs and prepositions present in the senten-
tial segment between the two nominals. Some 
WordNet (Miller, 1990) features are also used in 
the baseline for extracting the relation specific at-
tributes (e.g. Content type hypernym feature used 
for extracting the relation of Content-Container). 
The performance of the baseline system is limited 
due to the consideration of only the verb and 
preposition words in between the two nominals 
along with a small set of WordNet features. Hence, 
the Conditional Random Field (CRF) (McCallum 
et al, 2001) based framework is considered to ac-
complish the present task. The incorporation of 
different lexical features (e.g. WordNet hyponyms, 
Common-parents, distance), Named Entities (NE) 
and syntactic features (direct or transitive depend-
ency relations of parsing) has noticeably improved 
the performance of the system. It is observed that 
nominalization feature plays an effective role for 
identifying as well as distinguishing the relations. 
The test set containing 2717 sentences is evaluated 
against four different training sets. Some of the 
relations, e.g. Cause-Effect, Member-Collection 
perform well in comparison to other relations in all 
the four test results. Reviewing of the confusion 
matrices suggests that the system performance can 
be improved by reducing the errors that occur in 
distinguishing the two individual nominals in each 
relation. 
The rest of the paper is organized as follows. 
The pre-processing of resources and the baseline 
system are described in Section 2 and Section 3 
respectively. Development of CRF-based model is 
discussed in Section 4. Experimental results along 
206
with error analysis are specified in Section 5. Fi-
nally Section 6 concludes the paper. 
2 Resource Pre-Processing 
The annotated training corpus containing 8000 sen-
tences was made available by the respective task 
organizers. The objective is to evaluate the effec-
tiveness of the system in terms of identifying se-
mantic relations between pair of nominals. The 
rule-based baseline system is evaluated against the 
whole training corpus. But, for in-house experi-
ments regarding CRF based framework, the devel-
opment data is prepared by randomly selecting 500 
sentences from the 8000 training sentences. Rest 
7500 sentences are used for training of the CRF-
model. The format of one example entry in training 
file is as follows.  
"The system as described above has its greatest 
application in an arrayed <e1>configuration</e1> 
of antenna <e2>elements</e2>."  
Component-Whole (e2, e1)  
Comment: Not a collection: there is structure 
here, organisation. 
    Each of the training sentences is annotated by 
the paired nominals tagged as <e1> and <e2>. 
The relation of the paired nominals and a comment 
portion describing the detail of the input type fol-
lows the input sentence. 
The sentences are filtered and passed through 
Stanford Dependency Parser (Marneffe et al, 
2006) to identify direct as well as transitive de-
pendencies between the nominals. The direct de-
pendency is identified based on the simultaneous 
presence of both nominals, <e1> as well as <e2> 
in the same dependency relation whereas the tran-
sitive dependencies are verified if <e1> and <e2> 
are connected via one or more intermediate de-
pendency relations.  
Each of the sentences is passed through a Stan-
ford Named Entity Recognizer (NER)1 for identi-
fying the named entities. The named entities are 
the useful hints to separately identify the relations 
like Entity-Origin and Entity-Destination from 
other relations as the Origin and Destination enti-
ties are tagged by the NER frequently than other 
entities. 
Different seed lists are prepared for different 
types of verbs. For example, the lists for causal 
                                                          
1  http://nlp.stanford.edu/software/CRF-NER.shtml 
and motion verbs are developed by processing the 
XML files of English VerbNet (Kipper-Schuler, 
2005). The list of the causal and motion verbs are 
prepared by collecting the member verbs if their 
corresponding class contain the semantic type  
?CAUSE? or ?MOTION?. The other verb lists are 
prepared manually by reviewing the frequency of 
verbs in the training corpus. The WordNet stem-
mer is used to identify the root forms of the verbs.   
3 Baseline Model 
The baseline model is developed based on the 
similarity clues present in the phrasal pattern con-
taining verbs and prepositions. Different rules are 
identified separately for the nine different rela-
tions. A few WordNet features such as hypernym, 
meronym, distance and Common-Parents are 
added into the rule-based baseline model. Some of 
the relation specific rules are mentioned below. 
For example, if any of the nominals contain 
their meronym property as ?whole? and if the hy-
pernym tree for one of the nominals contains the 
word ?whole?, the relation is identified as a Com-
ponent-Whole relation.   But, the ordering of the 
nominals <e1> and <e2> is done based on the 
combination of ?has?, ?with? and ?of? with other 
word level components.  
The relations Cause-Effect, Entity-Destination 
are identified based on the causal verbs (cause, 
lead etc.) and motion verbs (go, run etc.) respec-
tively. One of the main criteria for extracting these 
relations is to verify the presence of causal and 
motion verbs in between the text segment of <e1> 
and <e2>. Different types of specific relaters (as, 
because etc.) are identified from the text segment 
as well. It is observed that such specific causal re-
laters help in distinguishing other relations from 
Cause-Effect.  
If one of the nominals is described as instrument 
type in its hypernym tree, the corresponding rela-
tion is identified as Instrument-Agency but the base 
level filtering criterion is applied if both the nomi-
nals belong to instrument type. On the other hand, 
if any of the nominals belong to the hypernym tree 
as content or container or hold type, it returns the 
relation Content-Container as a probable answer. 
Similarly, if both of them belong to the same type, 
the condition is fixed as false criterion for that par-
ticular category. The nominals identified as the 
part of collective nouns and associated with 
207
phrases like "of", "in", "from" between <e1> and 
<e2> contain the relation of Member-Collection. 
The relations e.g. Message-Topic uses seed list of 
verbs that satisfy the communication type in the 
hypernym tree and Product-Producer relation con-
cerns the hypernym feature as Product type. 
But, the identification of the proper ordering of 
the entities in the relation, i.e., whether the relation 
is valid between <e1, e2> or <e2, e1> is done by 
considering the passive sense of the sentence with 
the help of the keyword ?by? as well as by some 
passive dependency relations.  
The evaluation of the rule-based baseline sys-
tem on the 8000 training data gives an average F1-
score of 22.45%. The error analysis has shown that 
use of lexical features only is not sufficient to ana-
lyze the semantic relation between two nominals 
and the performance can be improved by adopting 
strategies for differentiating the nominals of a par-
ticular pair. 
4 CRF-based Model 
To improve the baseline system performance, 
CRF-based machine learning framework 
(McCallum et al, 2001) is considered for classify-
ing the semantic relations that exist among the or-
dered pair of nominals. Identification of appropri-
ate features plays a crucial role in any machine-
learning framework. The following features are 
identified heuristically by manually reviewing the 
corpus and based on the frequency of different 
verbs in different relations. 
? 11 WordNet features (Synset, Synonym, 
Gloss, Hyponym, Nominalization, Holo-
nym, Common-parents, WordNet distance, 
Sense ID, Sense count, Meronym) 
? Named Entities (NE) 
? Direct Dependency 
? Transitive Dependency 
? 9 separate verb list containing relation spe-
cific verbs, each for 9 different semantic 
relations  
Different singleton features and their combinations 
are generated from the training corpus. Instead of 
considering the whole sentence as an input to the 
CRF-based system, only the pairs of nominals are 
passed for classification. The previous and next 
token of the current token with respect to each of 
the relations are added in the template to identify 
their co-occurrence nature that in turn help in the 
classification process. Synsets containing synony-
mous verbs of the same and different senses are 
considered as individual features.   
4.1 Feature Analysis  
The importance of different features varies accord-
ing to the genre of the relations. For example, the 
Common-parents WordNet feature plays an effec-
tive role in identifying the Content-Container and 
Product-Producer relations. If the nominals in a 
pair share a common Sense ID and Sense Count  
then this is considered as a feature. The combina-
tion of multiple features in comparison with a sin-
gle feature generally shows a reasonable perform-
ance enhancement of the present classification sys-
tem. Evaluation on the development data for the 
various feature combinations has shown that the 
nominalization feature effectively performs for all 
the relations. WordNet distance feature is used for 
capturing the relations like Content-Container and 
Component-Whole. The direct and transitive de-
pendency syntactic features contribute in identify-
ing the relation as well as identify the ordering of 
the entities <e1> and <e2> in the relation. 
The Named-Entity (NE) relation plays an impor-
tant role in distinguishing the relations, e.g., Entity-
Origin and Entity-Destination from other relations. 
The person tagged NEs have been excluded from 
the present task as such NEs are not present in the  
Entity-Origin and Entity-Destination relations. It 
has been observed that the relation specific verbs 
supply useful clues to the training phrase for dif-
ferentiating relations among nominals.   
The system is trained on 7500 sentences and the 
evaluation is carried out on 500 development sen-
tences achieving an F1-Score of 57.56% F1-Score. 
The tuning on the development set has been carried 
out based on the performance produced by the 
individual features that effectively contains 
WordNet relations. In addition to that, the 
combination of dependency features with verb 
feature plays an contributory role on the system 
evaluation results. 
208
Table 1: Precision, Recall and F1-scores (in %) of semantic relations in (9+1) way directionality-based evaluation 
 
5 Experimental Results 
The active feature list is prepared after achieving 
the best possible F1-score of 61.82% on the devel-
opment set of 500 sentences. The final training of 
the CRF-based model is carried out on four differ-
ent sets containing 1000, 2000, 4000 and 8000 sen-
tences. These four training sets are prepared by 
extracting sentences from the beginning of the 
training corpus and the final evaluation is carried 
out on 2717 test sentences as provided by the or-
ganizers. The results on the four test sets termed as 
TD1, TD2, TD3 and TD4 are shown in Table 1. 
The error analysis is done based on the information 
present in the confusion matrices. The fewer occur-
rence of Entity-Destination (e2, e1) instance in the 
training corpus plays the negative role in identify-
ing the relation. Mainly, the strategy used for as-
signing the order among the entities, i.e., either 
<e1, e2> or <e2, e1> in the already identified re-
lations is the main cause of errors of the system. 
The Entity-Origin, Product-Producer and Mes-
sage-Topic relations suffer from overlapping prob-
lem with other relations. Each of the tested nomi-
nal pairs is tagged with more than one relation. 
But, selecting the first output tag produced by CRF 
is considered as the final relational tag for each of 
the nominal pairs. Hence, a distinguishing strategy 
needs to be adopted for fine-grained selection.  
6 Conclusion and Future Task 
In our approach to automatic classification of se-
mantic relations between nominals, the system 
achieves its best performance using the lexical fea-
ture such as nominalization of WordNet and syn-
tactic information such as dependency relations. 
These facts lead us to conclude that semantic fea-
tures from WordNet, in general, play a key role in 
the classification task. The present system aims for 
assigning class labels to discrete word level entities 
but the context feature is not taken into considera-
tion. The future task is to evaluate the performance 
of the system by capturing the context present be-
tween the pair of nominals.  
References  
Andrew McCallum, Fernando Pereira and John 
Lafferty. 2001. Conditional Random Fields: Prob-
abilistic Models for Segmenting and labeling Se-
quence Data. ICML-01, 282 ? 289. 
George A. Miller. 1990. WordNet: An on-line lexical 
database. International Journal of Lexicography, 
3(4): 235?312. 
Karin Kipper-Schuler. 2005. VerbNet. A broad-
coverage, comprehensive verb lexicon. Ph.D. thesis, 
University of Pennsylvania, Philadelphia, PA. 
Marie-Catherine de Marneffe, Bill MacCartney, and 
Christopher D. Manning. 2006. Generating Typed 
Dependency Parses from Phrase Structure Parses. 
(LREC 2006). 
Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva, 
Preslav Nakov, Diarmuid ?O S?eaghdha, Sebastian 
Padok , Marco Pennacchiotti, Lorenza Romano, Stan 
Szpakowicz. 2010. SemEval-2010 Task 8: Multi-
Way Classification of Semantic Relations Between 
Pairs of Nominals. 5th SIGLEX Workshop. 
TD1 TD2 TD3 TD4 Relations 
Prec. Recall F1 Prec. Recall F1 Prec. Recall F1 Prec. Recall F1 
Cause-Effect 76.33 65.85 70.70 78.55 65.85 71.64 79.86 68.90 73.98 79.26 72.26 75.60
Component-Whole 49.25 31.41 38.36 48.76 37.82 42.60 50.77 42.31 46.15 58.40 49.04 53.31
Content-Container 31.35 30.21 30.77 37.93 34.38 36.07 40.65 32.81 36.31 51.15 34.90 41.49
   Entity-Destination 37.58 62.67 46.98 43.43 63.36 51.53 43.09 63.01 51.18 
 
47.07 60.62 52.99
Entity-Origin 62.50 46.51 53.33 61.95 49.22 54.86 60.18 52.71 56.20 64.02 53.10 58.05
Instrument-Agency 19.46 23.08 21.11 21.18 27.56 23.96 26.43 23.72 25.00 32.48 24.36 27.84
Member-Collection 50.97 67.81 58.20 54.82 70.82 61.80 59.93 72.53 65.63 66.80 71.67 69.15
Message-Topic 41.70 41.38 41.54 50.23 42.15 45.83 52.81 46.74 49.59 57.78 49.81 53.50
Product-Producer 52.94 7.79 13.58 48.94 9.96 16.55 59.09 16.88 26.26 53.17 29.00 37.54
Other 21.10 27.09 23.72 24.48 33.70 28.36 26.28 37.44 30.88 26.64 42.07 32.62
Average F1 score 42.62 44.98 47.81 52.16 
209
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 370?374,
Dublin, Ireland, August 23-24, 2014.
JU_CSE: A Conditional Random Field (CRF) Based Approach to    
Aspect Based Sentiment Analysis 
 
Braja Gopal Patra, Soumik Mandal, Dipankar Das and Sivaji Bandyopadhyay 
Department of Computer Science & Engineering,  
Jadavpur University, Kolkata, India 
brajagopal.cse@gmail.com, mandal.soumik@gmail.com, 
dipankar.dipnil2005@gmail.com, sivaji_cse_ju@yahoo.com 
 
  
Abstract 
The fast upswing of online reviews and their 
sentiments on the Web became very useful 
information to the people. Thus, the opin-
ion/sentiment mining has been adopted as a 
subject of increasingly research interest in 
the recent years. Being a participant in the 
Shared Task Challenge, we have developed a 
Conditional Random Field based system to 
accomplish the Aspect Based Sentiment 
Analysis task. The aspect term in a sentence 
is defined as the target entity. The present 
system identifies aspect term, aspect catego-
ries and their sentiments from the Laptop 
and Restaurants review datasets provided by 
the organizers. 
1 Introduction 
In recent times, the research activities in the 
areas of Opinion Mining/Sentiment Analysis in 
natural language texts and other media are gain-
ing ground under the umbrella of subjectivity 
analysis and affect computing1. The reason may 
be the huge amount of available text data in So-
cial Web in the forms of news, reviews, blogs, 
chat and twitter etc. Majority of research efforts 
are being carried out for the identification of pos-
itive or negative polarity from the textual con-
tents like sentence, paragraph, or text span re-
gardless of the entities (e.g., laptops, restaurants) 
and their aspects (e.g., battery, screen; food, ser-
vice). 
                                                 
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details: 
http://creativecommons.org/licenses/by/4.0/ 
1http://www.saaip.org/ 
Aspect is a multinomial distribution over 
words that represent a more specific topic in re-
views (Jo and Oh, 2011). For example, in case of 
Laptop reviews, ?touchpad? is considered an 
aspect. Similarly, given a predefined entity, an 
aspect term describes a specific aspect of that 
entity (e.g., for the entity ?restaurant?, ?wine? 
can be an aspect term). Aspect term can be ap-
peared as a single word (e.g., ?menu?) or multi-
ple words (?side dish?). 
It is observed that for a particular entity, one 
or more number of aspect terms can be grouped 
into a single category (e.g., aspect terms 
?drinks?, ?main course? belongs to the same cat-
egory, ?food?).  
The main goal of the Aspect Based Sentiment 
Analysis (ABSA) (Pontiki et al., 2014) task is to 
identify the aspect terms and their categories 
from the given target entities as well as to identi-
fy the sentiments expressed towards each of the 
aspect terms. The datasets provided by the 
shared task organizers consist of customer re-
views with human-annotations. 
We have participated in all of the four tasks. A 
combination of Conditional Random Field (CRF) 
based machine learning algorithm and rule based 
techniques has been adopted for identifying the 
aspect term, aspect category and their senti-
ments. We have used several features like Part of 
Speech (POS), Stanford dependency relations2, 
WordNet information, and sentiment lexicon 
(SentiWordNet3) to accomplish these tasks. 
The rest of the paper is organized in the fol-
lowing manner. Section 2 provides the details of 
previous works. Section 3 provides an elabora-
tive description of the data used in the task. Fea-
tures used in these experiments are described in 
Section 4. The detailed setup of experimentation 
and analysis of the results are described in Sec-
                                                 
2 http://nlp.stanford.edu/software/lex-parser.shtml 
3 http://sentiwordnet.isti.cnr.it/ 
370
tion 5. Finally, conclusions and future directions 
are presented. 
2 Related Work 
It has been observed that most of the previous 
works on aspect detection were based on infor-
mation extraction, to find the most frequent noun 
phrases (Hu and Liu, 2004). This approach is 
generally useful in finding aspects which are 
strongly associated with a single noun. But, one 
principal disadvantage of this approach is that it 
cannot detect the aspect terms which are of low 
frequency and noun phrases (e.g., different 
names of dishes like Biryani, Dosa and Uttapam 
etc. for the aspect category, ?food?). The pro-
posed work of such problem involves semantic 
hierarchy, rule-based or combination of both 
(Popescu and Etzioni 2005). More recent ap-
proaches of aspect detection are based on topic 
modelling, that use Latent Dirichlet Allocation 
(LDA) (Brody and Elhadad, 2010). But, the 
standard Latent Dirichlet Allocation (LDA) is 
not exactly suitable for the task of aspect detec-
tion due to their inherent nature of capturing 
global topics in the data, rather than finding local 
aspects related to the predefined entity. This ap-
proach was further modified in Sentence-LDA 
(SLDA) and Aspect and Sentiment Unification 
Model (ASUM) (Jo and Oh, 2011). Similarly, the 
identification of focussed text spans for opinion 
topics and targets were identified in (Das and 
Bandyopadhyay, 2010). 
Snyder and Barzilay (2007) addressed the 
problem of identifying categories for multiple 
related aspect terms appeared in the text. For 
instance, in a restaurant review, such categories 
may include food, ambience and service etc. In 
our task, we call them as aspect or review cate-
gories. The authors implemented the Good Grief 
decoding algorithm on a corpus collected on res-
taurant review4, which outperforms over the fa-
mous PRank algorithm (Crammer and Singer, 
2001). 
Ganu et al., (2009) have classified the restau-
rant reviews collected from City search New 
York5 into six categories namely Food, Service, 
Price, Ambience, Anecdotes, and Miscellaneous. 
Sentiment associated with each category has also 
been identified and both the experiments were 
carried out using Support Vector Machine classi-
fiers. Finally, they implemented the regression 
based model containing MATLAB regression 
                                                 
4 http://people.csail.mit.edu/bsnyder/naacl07/ 
5 http://www.citysearch.com/guide/newyork-ny-metro 
function (mvregress) to give rating (1 to 5) to 
each review.  
To determine the sentiment or polarity of the 
aspect term and aspect category, we need a prior 
sentiment annotated lexicon. Several works have 
been conducted on building emotional corpora in 
different English languages such as SentiWord-
Net (Baccianella et al., 2010), WordNet Affect 
(Strapparava and Valitutti, 2004) (Patra et al., 
2013) etc. Among all these publicly available 
sentiment lexicons, SentiWordNet is one of the 
well-known and widely used ones (number of 
citations is higher than other resources6) that has 
been utilized in several applications such as sen-
timent analysis, opinion mining and emotion 
analysis.  
Several works have been performed on the au-
tomated opinion detection or polarity identifica-
tion from reviews (Yu and Hatzivassiloglou, 
2003; Hu and Liu, 2004). Yu and Hatzivass-
iloglou (2003) has focused on characterizing 
opinions and facts in a generic manner, without 
examining who the opinion holder is or what the 
opinion is about. Then, they have identified the 
polarity or sentiment of the fact using Naive 
Bayes classifier. Hu and Liu, (2004) has summa-
rized the customer review and then identified the 
sentiment of that review. They have achieved 
promising accuracy in case of identifying polari-
ty of the reviews.  
3 Data 
The sentences collected from the customer re-
views of Restaurants and Laptops are used in 
these tasks. The training data of Restaurant re-
views contains 3041 English sentences annotated 
with aspect terms and aspect categories along 
with their polarity. The training data of Laptop 
reviews contains 3045 sentences annotated with 
aspect terms along with their polarity. The test 
data contains 800 sentences from each of the re-
view sets.  
An example extracted from the corpus is as 
follows:  
But the staff was so horrible to us.  
Here, "staff" is the aspect term and its polarity 
is "negative". The aspect category is "service" 
and polarity of the aspect category is also "nega-
tive". 
                                                 
6 http://citeseerx.ist.psu.edu/index 
371
4 Feature Analysis 
In general, the feature selection always plays 
an important role in any machine learning 
framework and depends upon the data set used 
for the experiments. Based on a preliminary in-
vestigation of the dataset, we have identified 
some of the following features. Different combi-
nations of the features have also been used to get 
the best results from the classification task. 
Parts-of-Speech (POS): the aspect terms are 
basically represented by the noun phrases. On the 
other hand, the POS tag plays an important role 
in aspect term identification (Hu and Liu, 2004; 
Brody and Elhadad, 2010). Thus, we have used 
the Stanford CoreNLP7 tool to parse each of the 
review sentences to find out the part-of-speech 
tag of each word and included them as a feature 
in all of our experiments.  
POS Frequency: We have observed that the 
aspect terms surrounded by a noun or adjective 
are also denoted as aspect terms. Therefore, we 
have utilized this information in our system. For 
example, in the phrase ?external_JJ mouse_NN?. 
Here the word ?mouse? is an object and aspect 
term. The word ?external? is also tagged as as-
pect term. 
Before be verb: We have observed that the 
nouns occur before the ?be? verbs denote the 
aspect terms in most of the cases. e.g. ?The hard 
disk is noisy?. Here ?hark disk? is an aspect term 
and is followed by the ?be? verb "is". 
Inanimate words: In case of the Restaurant 
and Laptop reviews, we observed that many of 
the inanimate nouns occur as aspect terms. We 
have used the hyponym tree of RiTa.WordNet8 to 
identify the inanimate words. For example, in the 
following sentence, the words food, kitchen and 
menu are inanimate nouns occurred as aspect 
terms. 
?The food is uniformly exceptional, with a 
very capable kitchen which will proudly whip up 
whatever you feel like eating, whether it's on the 
menu or not.?  
Dependency Relation for finding Object: We 
have identified the object based dependency rela-
tions from parsed sentences, as we have observed 
that the words occupied in such relations are rep-
resented as aspect terms in many cases. ?dobj?, 
?obj? and ?xobj? are considered as the probable 
candidate relations for identifying the aspect 
                                                 
7
http://nlp.stanford.edu/software/corenlp.shtml 
8www.rednoise.org/rita/reference/RiWordNet.html 
terms. Here, the Stanford Parser9 has been used 
to get the dependency relations. 
Ontology Information (Liu, 2012): We have 
counted the aspect terms in the training data. The 
aspect terms occurred more than five times in the 
corpus are considered during our experiments. At 
first, we have tested this ontology information on 
the development set and observed that the aspect 
terms with frequency five or more also give bet-
ter results in the test set. 
Sentiment Words: We have used the senti-
ment words as a feature for the sentiment identi-
fication tasks (Liu, 2012; Brody and Elhadad, 
2010). Words are identified as positive, negative 
or neutral using SentiWordNet10. 
WordNet Information: The RiTa.WordNet 
package has been used to extract different prop-
erties of the words.  
For aspect category identification, we have 
matched the hypernym tree of each word with 
the four categories (service, price, food, and am-
bience). If the hypernym tree does not contain 
any of such words, we check the next level hy-
pernym tree of the words derived from hypernym 
of previous word. We have checked up to the 
second degree hypernym tree. We also searched 
hypernym tree of the synset of each word.  
Number of Sentence: It has been found that 
many reviews contain more than one sentence. 
Therefore, we have included the number of sen-
tence as a feature based on the output of Stanford 
Parser. We have split the output of Stanford 
Parser by the mark, ?(S?.  
In case of our experiments, the stop words are 
excluded. Total of 329 stop words was prepared 
manually.  
5 Experimentation and Result Analysis 
We have used the CRF++ 0.58 11 , an open 
source tool for implementing the machine learn-
ing framework for our experiments. CRF is well 
known for sequence labeling tasks (Lafferty et 
al., 2001). Similarly, in the present task, the as-
pect terms use the context information and are 
represented in sequences. Many of the aspect 
terms are multiword expressions such as ?hard 
disk?. We have created different templates for 
different subtasks to capture all the relations be-
tween different sequence related features.  
 
                                                 
9http://nlp.stanford.edu/software/lex-parser.shtml 
10http://sentiwordnet.isti.cnr.it/ 
11http://crfpp.googlecode.com/svn/trunk/doc/index.htm 
372
a. Classification of Aspect Term 
Features used in case of identifying aspect 
terms are POS, POS Frequency, Before be verb, 
Inanimate word, objects of the sentence, ontolo-
gy information. We have used several rules to 
identify these features. Then, we have used the 
CRF++ to identify the aspect terms. Some post 
processing techniques are also used in order to 
get better accuracy. The present system identifies 
only single word aspect terms. But it is found in 
the training data that many aspect terms consist 
of multiple words. Therefore, if there is a stop 
word in between two system identified aspect 
words, the stop word is also considered as a part 
of the aspect term. We have joined the aspect 
words along with the stop words to form a single 
but multiword aspect terms.  
Precisions, Recalls and F-scores are recorded 
for our system in Table 1. The maximum F-
scores achieved in the aspect term identification 
task for Laptop and Restaurant are 0.7455012 
and 0.84012544, respectively. Our system per-
forms better on Restaurant reviews than Laptop 
reviews.  
 Laptop Restaurant 
Precision 0.4938838 0.6481481 
Recall 0.7442396 0.8184855 
F-score 0.59375 0.72342515 
Table 1: JU_CSE system result for aspect 
term identification. 
b. Classification of Aspect Category 
Features used in this experiment are POS, De-
pendency relations for object and a few semantic 
relations of WordNet. In this subtask, we have 
also used aspect term knowledge as a feature. 
We identified the POS of the words using Stan-
ford CoreNLP tool and used the words which are 
not listed in our stop-word list. The objects are 
identified from the dependency relations. The 
hpernym trees of these words are searched up to 
second degree to find four aspect categories 
(service, price, food, and ambience). If we don?t 
find these four categories in the hypernym tree, 
we increase the frequency of anecdotes/ miscel-
laneous category. Frequency counts of these 
matched words are listed as a feature. The accu-
racy of the system for aspect categories in the 
Restaurant reviews are shown in Table 2.  
Maximum F-score achieved in this aspect cat-
egory identification is 0.8857715. The main 
problem faced in this task was to assign the an-
ecdotes/ miscellaneous category to the respective 
reviews. There are many cases in which the an-
ecdotes/miscellaneous categories occurred with 
other categories. In these cases, our system fails 
to identify the anecdotes/miscellaneous category.  
 
Restaurant 
Precision Recall F-score 
0.7307317 0.68029064 0.7046096 
Table 2: JU_CSE system result for aspect 
category identification. 
We have also observed that every review has 
at least one category. If any word of the review 
does not belong to any of the four categories, we 
assign these reviews with anecdotes/ miscellane-
ous category at the time of post processing.  
c. Classification of Sentiment of Aspect 
term and category 
Features used in these experiments are POS, 
Positive, Negative and Neutral words and num-
ber of sentences. Some reviews with multiple 
sentences contain different sentiments associated 
with different aspect terms. This observation also 
leads to conflict sentiment. Therefore, we have 
also included the aspect term and aspect catego-
ry information during sentiment identification. 
The accuracy of the system is given in the Table 
3. 
Accuracy 
? 
Aspect 
Term  
Sentiment 
Aspect 
Category 
Sentiment 
Laptop 0.5321101 NaN 
Restaurant 0.65547705 0.6409756 
Table 3: JU_CSE system result for aspect 
term and category sentiment identification. 
Our system performs moderate in case of sen-
timent identification. Mainly, the system was 
biased towards the positive tags. It is found that 
the number of positive tags in the training data 
was more as compared to others. We have ob-
served that a conflict tag occurs when an aspect 
term was present as both positive and negative. 
As the present system identifies the sentiment 
based on word level only, it was unable to detect 
the conflict tags. The feature, number of sentenc-
es fails to identify the conflict tags. Therefore, 
we need to find more suitable features for our 
system to improve the accuracy. 
373
6 Conclusion 
In this paper, we have presented a CRF based 
system for identifying the aspect terms, aspect 
categories and their sentiments. We believe that 
this problem will become increasingly important 
for common people. This task will not only be 
useful to common shoppers, but also crucial to 
product manufacturers and restaurateurs.  
Overall accuracies of our system were moder-
ate. In future, we will include more suitable fea-
tures to improve accuracy of our system. We also 
intend to explore different machine learning al-
gorithms for these tasks in future.  
Reference 
Benjamin Snyder and Regina Barzilay. 2007. Multi-
ple Aspect Ranking Using the Good Grief Algo-
rithm. In Proceedings of the Human Language 
Technologies: The Annual Conference of the North 
American Chapter of the Association for Computa-
tional Linguistics (NAACL-HLT 2007), pp. 300-
307. 
Bing Liu. 2012. Sentiment Analysis and Opinion 
Mining. Synthesis Lectures on Human Language 
Technologies 5, no. 1 (2012): 1-167. 
Braja G. Patra, Hiroya Takamura, Dipankar Das, 
Manabu Okumura, and Sivaji Bandyopadhyay. 
2013. Construction of Emotional Lexicon Using 
Potts Model. In Proceedings of the 6th Internation-
al Joint Conference on Natural Language Pro-
cessing (IJCNLP-2013), Nagoya, Japan, pp. 674?
679. 
Carlo Strapparava, and Alessandro Valitutti. 2004. 
WordNet Affect: an Affective Extension of Word-
Net. In LREC, vol. 4, pp. 1083-1086. 
Dipankar Das and Sivaji Bandyopadhyay. 2010. Ex-
tracting emotion topics from blog sentences: use of 
voting from multi-engine supervised classifiers. In 
Proceedings of the 2nd international workshop on 
Search and mining user-generated contents, pp. 
119-126. 
Ganu Gayatree, Noemie Elhadad, and Amelie Marian. 
2009. Beyond the stars: Improving rating predic-
tions using review text content. In Proceedings of 
the 12th International Workshop on the Web and 
Databases, Providence, Rhode Island. 
Hong Yu and Vasileios Hatzivassiloglou. 2003. To-
wards answering opinion questions: Separating 
facts from opinions and identifying the polarity of 
opinion sentences. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language 
Processing (EMNLP-2013), pp. 129-136.  
Koby Crammer and Yoram Singer. 2001. Pranking 
with ranking. In NIPS, vol. 14, pp. 641-647. 
John Lafferty, Andrew McCallum, Fernando C.N. 
Pereira. 2001. Conditional Random Fields: Proba-
bilistic Models for Segmenting and Labeling Se-
quence Data. In Proceedings of the 18th Interna-
tional Conference on Machine Learning (ICML 
2001), pp. 282-289.  
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the 10th 
ACM SIGKDD International Conference on 
Knowledge Discovery and Data Mining, pp. 168-
177. 
Ana-Maria Popescu and Oren Etzioni. 2005. Extract-
ing product features and opinions from reviews. In 
proceedings of the Human Language Technology 
Conference: Conference on Empirical Methods in 
Natural Language Processing (HLT-EMNLP). 
Morristown, NJ, USA, pp. 339?346. 
Samaneh Moghaddam and Martin Ester. 2010. Opin-
ion digger: an unsupervised opinion miner from 
unstructured product reviews. In Proceedings of the 
19th ACM international conference on Information 
and knowledge management, pp. 1825-1828. 
Samuel Brody and Noemie Elhadad. 2010. An unsu-
pervised aspect-sentiment model for online re-
views. In Proceedings of the Human Language 
Technologies: The 2010 Annual Conference of the 
North American Chapter of the Association for 
Computational Linguistics (HLT-NAACL). 
Soo-Min Kim and Eduard Hovy. 2006. Extracting 
opinions, opinion holders, and topics expressed in 
online news media text. In Proceedings of the 
Workshop on Sentiment and Subjectivity in Text, 
pp. 1-8. 
Stefano Baccianella, Andrea Esuli, and Fabrizio Se-
bastiani. 2010. SentiWordNet 3.0: An Enhanced 
Lexical Resource for Sentiment Analysis and 
Opinion Mining. In LREC, vol. 10, pp. 2200-2204. 
Yohan Jo and Alice H. Oh. 2011. Aspect and senti-
ment unification model for online review analysis. 
In Proceedings of the fourth ACM international 
conference on Web search and data mining.  
Zhongwu Zhai, Bing Liu, Hua Xu, and Peifa Jia. 
2011. Clustering product features for opinion min-
ing. In Proceedings of the fourth ACM internation-
al conference on Web search and data mining, pp. 
347-354. 
Maria Pontiki, Dimitrios Galanis, John Pavlopoulos, 
Haris Papageorgiou, Ion Androutsopoulos, and 
Suresh Manandhar. 2014. SemEval-2014 Task 4: 
Aspect Based Sentiment Analysis. In Proceedings 
of the 8th International Workshop on Semantic 
Evaluation (SemEval 2014), Dublin, Ireland. 
374
Proceedings of the 8th Workshop on Asian Language Resources, pages 47?55,
Beijing, China, 21-22 August 2010. c?2010 Asian Federation for Natural Language Processing
Labeling Emotion in Bengali Blog Corpus ? A Fine Grained 
Tagging at Sentence Level 
Dipankar Das 
Department of Computer Science  
& Engineering,  
Jadavpur University 
dipankar.dipnil2005@gmail.com 
Sivaji Bandyopadhyay 
Department of Computer Science  
& Engineering,  
Jadavpur University  
sivaji_cse_ju@yahoo.com 
 
Abstract 
Emotion, the private state of a human 
entity, is becoming an important topic 
in Natural Language Processing (NLP) 
with increasing use of search engines. 
The present task aims to manually an-
notate the sentences in a web based 
Bengali blog corpus with the emotional 
components such as emotional expres-
sion (word/phrase), intensity, associ-
ated holder and topic(s). Ekman?s six 
emotion classes (anger, disgust, fear, 
happy, sad and surprise) along with 
three types of intensities (high, general 
and low) are considered for the sen-
tence level annotation. Presence of dis-
course markers, punctuation marks, 
negations, conjuncts, reduplication, 
rhetoric knowledge and especially 
emoticons play the contributory roles 
in the annotation process. Different 
types of fixed and relaxed strategies 
have been employed to measure the 
agreement of the sentential emotions, 
intensities, emotional holders and top-
ics respectively. Experimental results 
for each emotion class at word level on 
a small set of the whole corpus have 
been found satisfactory.    
1 Introduction 
Human emotion described in texts is an impor-
tant cue for our daily communication but the 
identification of emotional state from texts is 
not an easy task as emotion is not open to any 
objective observation or verification (Quirk et 
al., 1985). Emails, weblogs, chat rooms, online 
forums and even twitter are considered as the 
affective communication substrates to analyze 
the reaction of emotional catalysts. Among 
these media, blog is one of the communicative 
and informative repository of text based emo-
tional contents in the Web 2.0 (Lin et al, 
2007).  
Rapidly growing web users from multilin-
gual communities focus the attention to im-
prove the multilingual search engines on the 
basis of sentiment or emotion. Major studies 
on Opinion Mining and Sentiment Analyses 
have been attempted with more focused per-
spectives rather than fine-grained emotions. 
The analyses of emotion or sentiment require 
some basic resource. An emotion-annotated 
corpus is one of the primary ones to start with.  
The proposed annotation task has been car-
ried out at sentence level. Three annotators 
have manually annotated the Bengali blog sen-
tences retrieved from a web blog archive1 with 
Ekman?s six basic emotion tags (anger (A),
disgust (D), fear (F), happy (H), sad (Sa) and 
surprise (Su)). The emotional sentences are 
tagged with three types of intensities such as 
high, general and low. The sentences of non-
emotional (neutral) and multiple (mixed) cate-
gories are also identified. The identification of 
emotional words or phrases and fixing the 
scope of emotional expressions in the sen-
tences are carried out in the present task. Each 
of the emoticons is also considered as individ-
ual emotional expressions. The emotion holder 
and relevant topics associated with the emo-
tional expressions are annotated considering 
the punctuation marks, conjuncts, rhetorical 
structures and other discourse information. The 
knowledge of rhetorical structure helps in re-
moving the subjective discrepancies from the 
                                                 
1 www.amarblog.com 
47
writer?s point of view. The annotation scheme 
is used to annotate 123 blog posts containing 
4,740 emotional sentences having single emo-
tion tag and 322 emotional sentences for mixed 
emotion tagss along with 7087 neutral sen-
tences in Bengali. Three types of standard 
agreement measures such as Cohen?s kappa 
() (Cohen, 1960; Carletta, 1996), Measure of 
Agreement on Set-valued Items (MASI) (Pas-
sonneau, 2004) and agr (Wiebe et al, 2005) 
metrics are employed for annotating the emo-
tion related components. The relaxed agree-
ment schemes like MASI and agr are specially 
considered for fixing the boundaries of emo-
tional expressions and topic spans in the emo-
tional sentences. The inter annotator agreement 
of some emotional components such as senten-
tial emotions, holders, topics show satisfactory 
performance but the sentences of mixed emo-
tion and intensities of general and low show 
the disagreement. A preliminary experiment 
for word level emotion classification on a 
small set of the whole corpus yielded satisfac-
tory results.
The rest of the paper is organized as fol-
lows. Section 2 describes the related work. The 
annotation of emotional expressions, sentential 
emotion and intensities are described in Sec-
tion 3. In Section 4, the annotation scheme for 
emotion holder is described. The issues of 
emotional topic annotation are discussed in 
Section 5. Section 6 describes the preliminary 
experiments carried out on the annotated cor-
pus. Finally, Section 7 concludes the paper.     
2 Related Work 
One of the most well known tasks of annotat-
ing the private states in texts is carried out by 
(Wiebe et al, 2005).  They manually annotated 
the private states including emotions, opinions, 
and sentiment in a 10,000-sentence corpus (the 
MPQA corpus) of news articles. The opinion 
holder information is also annotated in the 
MPQA corpus but the topic annotation task has 
been initiated later by (Stoyanov and Cardie, 
2008a). In contrast, the present annotation 
strategy includes the fine-grained emotion 
classes and specially handles the emoticons 
present in the blog posts. 
(Alm et al, 2005) have considered eight 
emotion categories (angry, disgusted, fearful, 
happy, sad, positively surprised, negatively 
surprised) to accomplish the emotion annota-
tion task at sentence level. They have manually 
annotated 1580 sentences extracted from 22 
Grimms? tales. The present approach discusses 
the issues of annotating unstructured blog text 
considering rhetoric knowledge along with the 
attributes, e.g. negation, conjunct, reduplica-
tion etc.  
Mishne (2005) experimented with mood 
classification in a blog corpus of 815,494 posts 
from Livejournal 
(http://www.livejournal.com), a free weblog 
service with a large community. (Mihalcea and 
Liu, 2006) have used the same data source for 
classifying the blog posts into two particular 
emotions ? happiness and sadness. The blog 
posts are self-annotated by the blog writers 
with happy and sad mood labels. In contrast, 
the present approach includes Ekman?s six 
emotions, emotion holders and topics to ac-
complish the whole annotation task. 
(Neviarouskaya et al, 2007) collected 160 
sentences labeled with one of the nine emo-
tions categories (anger, disgust, fear, guilt, in-
terest, joy, sadness, shame, and surprise) and a 
corresponding intensity value from a corpus of 
online diary-like blog posts. On the other hand, 
(Aman and Szpakowicz, 2007) prepare an 
emotion-annotated corpus with a rich set of 
emotion information such as category, inten-
sity and word or phrase based expressions. The 
present task considers all the above emotion 
information during annotation. But, the present 
annotation task additionally includes the com-
ponents like emotion holder, single or multiple 
topic spans. 
The emotion corpora for Japanese were built 
for recognizing emotions (Tokuhisa et al, 
2008). An available emotion corpus in Chinese 
is Yahoo!?s Chinese news 
(http://tw.news.yahoo.com), which is used for 
Chinese emotion classification of news readers 
(Lin, et al, 2007). The manual annotation of 
eight emotional categories (expect, joy, love, 
surprise, anxiety, sorrow, angry and hate) 
along with intensity, holder, word/phrase, de-
gree word, negative word, conjunction, rheto-
ric, punctuation and other linguistic expres-
sions are carried out at sentence, paragraph as 
well as document level on 1,487 Chinese blog 
documents (Quan and Ren, 2009). In addition 
48
to the above emotion entities, the present ap-
proach also includes the annotation of single or 
multiple emotion topics in a target span. 
Recent study shows that non-native English 
speakers support the growing use of the Inter-
net 2.  This raises the demand of linguistic re-
sources for languages other than English. Ben-
gali is the fifth popular language in the World, 
second in India and the national language in 
Bangladesh but it is less computerized com-
pared to English. To the best of our knowl-
edge, at present, there is no such available cor-
pus that is annotated with detailed linguistic 
expressions for emotion in Bengali or even for 
other Indian languages. Thus we believe that 
this corpus would help the development and 
evaluation of emotion analysis systems in 
Bengali. 
3 Emotion Annotation 
Random collection of 123 blog posts contain-
ing a total of 12,149 sentences are retrieved 
from Bengali web blog archive 3  (especially 
from comics, politics, sports and short stories) 
to prepare the corpus. No prior training was 
provided to the annotators but they were in-
structed to annotate each sentence of the blog 
corpus based on some illustrated samples of 
the annotated sentences. Specially for annotat-
ing the emotional expressions and topic(s) in 
emotional sentences, the annotators are free in 
selecting the texts spans. This annotation 
scheme is termed as relaxed scheme. For other 
emotional components, the annotators are 
given items with fixed text spans and in-
structed to annotation the items with definite 
tags. 
3.1 Identifying Emotional Expressions for 
Sentential Emotion and Intensity 
The identification of emotion or affect affixed 
in the text segments is a puzzle. But, the puzzle 
can be solved partially using some lexical 
clues (e.g. discourse markers, punctuation 
marks (sym), negations (NEG), conjuncts 
(CONJ), reduplication (Redup)), structural 
clues (e.g. rhetoric and syntactic knowledge) 
and especially some direct affective clues (e.g. 
                                                 
2 http://www.internetworldstats.com/stats.htm 
3 www.amarblog.com 
emoticons (emo_icon)). The identification of 
structural clues indeed requires the identifica-
tion of lexical clues.  
Rhetorical Structure Theory (RST) de-
scribes the various parts of a text, how they 
can be arranged and connected to form a whole 
text (Azar, 1999). The theory maintains that 
consecutive discourse elements, termed text 
spans, which can be in the form of clauses, 
sentences, or units larger than sentences, are 
related by a relatively small set (20?25) of rhe-
torical relations (Mann and Thompson, 1988). 
RST distinguishes between the part of a text 
that realizes the primary goal of the writer, 
termed as nucleus, and the part that provides 
supplementary material, termed satellite. The 
separation of nucleus from satellite is done 
based on punctuation marks (, ! @?), emoti-
cons, discourse markers (  jehetu [as], 	 
jemon [e.g.], 
 karon [because], 	 mane
[means]), conjuncts (e ebong [and], 
n 
kintu [but], a athoba [or]), causal verbs 
( ghotay [caused]) if they are explicitly 
specified in the sentences.  
Use of emotion-related words is not the sole 
means of expressing emotion. Often a 
sentence, which otherwise may not have an 
emotional word, may become emotion bearing 
depending on the context or underlying 
semantic meaning (Aman and Szpakowicz, 
2007). An empirical analysis of the blog texts 
shows two types of emotional expressions. The 
first category contains explicitly stated 
emotion word (EW) or phrases (EP) mentioned 
in the nucleus or in the satellite. Another 
category contains the implicit emotional clues 
that are identified based on the context or from 
the metaphoric knowledge of the expressions. 
Sometimes, the emotional expressions contain 
direct emotion words (EW) (

 koutuk 
[joke], 	 ananda [happy],  
ashcharjyo [surprise]), reduplication (Redup) 
(Proceedings of the Multiword Expressions: From Theory to Applications (MWE 2010), pages 37?45,
Beijing, August 2010
Automatic Extraction of Complex Predicates in Bengali  
Dipankar Das     Santanu Pal      Tapabrata Mondal       Tanmoy Chakraborty   
 
  Sivaji Bandyopadhyay 
Department of Computer Science and Engineering 
Jadavpur University 
dipankar.dipnil2005@gmail.com, 
santanupersonal1@gmail.com, 
tapabratamondal@gmail.com, its_tanmoy@yahoo.co.in, 
sivaji_cse_ju@yahho.com 
 
 
Abstract 
This paper presents the automatic ex-
traction of Complex Predicates (CPs) 
in Bengali with a special focus on 
compound verbs (Verb + Verb) and 
conjunct verbs (Noun /Adjective + 
Verb). The lexical patterns of com-
pound and conjunct verbs are extracted 
based on the information of shallow 
morphology and available seed lists of 
verbs. Lexical scopes of compound and 
conjunct verbs in consecutive sequence 
of Complex Predicates (CPs) have 
been identified. The fine-grained error 
analysis through confusion matrix 
highlights some insufficiencies of lexi-
cal patterns and the impacts of different 
constraints that are used to identify the 
Complex Predicates (CPs). System 
achieves F-Scores of 75.73%, and 
77.92% for compound verbs and 
89.90% and 89.66% for conjunct verbs 
respectively on two types of Bengali 
corpus.      
1 Introduction 
Complex Predicates (CPs) contain [verb] + 
verb (compound verbs) or [noun/ 
adjective/adverb] +verb (conjunct verbs) 
combinations in South Asian languages (Hook, 
1974). To the best of our knowledge, Bengali  
 
 
is not only a language of South Asia but also 
the sixth popular language in the World 1 , 
second in India and the national language of 
Bangladesh. The identification of Complex 
Predicates (CPs) adds values for building 
lexical resources (e.g. WordNet (Miller et al, 
1990; VerbNet (Kipper-Schuler, 2005)), 
parsing strategies and machine translation 
systems.  
Bengali is less computerized compared to 
English due to its morphological enrichment. 
As the identification of Complex Predicates 
(CPs) requires the knowledge of morphology, 
the task of automatically extracting the Com-
plex Predicates (CPs) is a challenge. Complex 
Predicates (CPs) in Bengali consists of two 
types, compound verbs (CompVs) and conjunct 
verbs (ConjVs). 
The compound verbs (CompVs) (e.g. ? ?? 
? ?? mere phela ?kill?, ???? ???? bolte laglo 
?started saying?) consist of two verbs. The first 
verb is termed as Full Verb (FV) that is present 
at surface level either as conjunctive participial 
form -e ?e or the infinitive form -?  ?te. The 
second verb bears the inflection based on 
Tense, Aspect and Person. The second verbs 
that are termed as Light Verbs (LV) are 
polysemous, semantically bleached and 
confined into some definite candidate seeds 
(Paul, 2010).  
On the other hand, each of the Bengali con-
junct verbs (ConjVs) (e.g. ???? ??? bharsha 
                                                 
1http://www.ethnologue.com/ethno_docs/distributio
n.asp?by=size 
37
kara ?to depend?, ???? ??? jhakjhak kara ?to 
glow?) consists of noun or adjective followed 
by a Light Verb (LV). The Light Verbs (LVs) 
bear the appropriate inflections based on 
Tense, Aspect and Person.   
According to the definition of multi-word 
expressions (MWEs)(Baldwin and Kim, 2010), 
the absence of conventional meaning of the 
Light Verbs in Complex Predicates (CPs) 
entails us to consider the Complex Predicates 
(CPs) as MWEs (Sinha, 2009). But, there are 
some typical examples of Complex Predicates 
(CPs), e.g. ? ?? ??? dekha kara ?see-do? that 
bear the similar lexical pattern as Full Verb 
(FV)+ Light Verb (LV) but both of the Full 
Verb (FV) and Light Verb (LV) loose their 
conventional meanings and generate a 
completely different meaning (?to meet? in this 
case).  
In addition to that, other types of predicates 
such as ???? ? ? niye gelo ?take-go? (took and 
went), ???? ? ? diye gelo ?give-go? (gave and 
went) follows the similar lexical patterns 
FV+LV as of Complex Predicates (CPs) but 
they are not mono-clausal. Both the Full Verb 
(FV) and Light Verb (LV) behave like 
independent syntactic entities and they belong 
to non-Complex Predicates (non-CPs). The 
verbs are also termed as Serial Verb (SV) 
(Mukherjee et al, 2006). 
Butt (1993) and Paul (2004) have also 
mentioned the following criteria that are used 
to check the validity of complex predicates 
(CPs) in Bengali. The following cases are the 
invalid criteria of complex predicates (CPs). 
1. Control Construction (CC): ????? ??? 
likhte bollo ?asked to write?, ????? ???? 
??? likhte badhyo korlo ?forced to 
write? 
2. Modal Control Construction (MCC): 
? ?? ??? jete hobe ?have to go? ? ?? ??? 
khete hobe ?have to eat? 
3. Passives (Pass) : ??? ??? dhora porlo 
?was caught?, ???? ?? mara holo ?was 
beaten? 
4. Auxiliary Construction (AC): ??? ??? 
bose ache ?is sitting?, ???? ??? niye chilo 
?had taken?. 
Sometimes, the successive sequence of the 
Complex Predicates (CPs) shows a problem of 
deciding the scopes of individual Complex 
Predicates (CPs) present in that sequence. For 
example the sequence, u?? ??? ? ???? uthe pore 
dekhlam ?rise-wear-see? (rose and saw) seems 
to contain two Complex Predicates (CPs) (u?? 
??? uthe pore ?rose? and ??? ? ???? pore 
dekhlam ?wore and see?). But there is actually 
one Complex Predicate (CP). The first one u?? 
??? uthe pore ?rose? is a compound verb 
(CompV) as well as a Complex Predicate (CP). 
Another one is ? ???? dekhlam ?saw? that is a 
simple verb. As the sequence is not mono-
clausal, the Complex Predicate (CP) u?? ??? 
uthe pore ?rose? associated with ? ???? dekhlam 
?saw? is to be separated by a lexical boundary. 
Thus the determination of lexical scopes of 
Complex Predicates (CPs) from a long con-
secutive sequence is indeed a crucial task.      
 The present task therefore not only aims to 
extract the Complex Predicates (CPs) 
containing compound and conjunct verbs but 
also to resolve the problem of deciding the 
lexical scopes automatically. The compound 
verbs (CompVs) and conjunct verbs (ConjVs) 
are extracted from two separate Bengali 
corpora based on the morphological 
information (e.g. participle forms, infinitive 
forms and inflections) and list of Light Verbs 
(LVs). As the Light Verbs (LVs) in the 
compound verbs (CompVs) are limited in 
number, fifteen predefined verbs (Paul, 2010) 
are chosen as Light Verbs (LVs) for framing 
the compound verbs (CompVs).  A manually 
prepared seed list that is used to frame the 
lexical patterns for conjunct verbs (ConjVs) 
contains frequently used Light Verbs (LVs).  
An automatic method is designed to identify 
the lexical scopes of compound and conjunct 
verbs in the long sequences of Complex 
Predicates (CPs). The identification of lexical 
scope of the Complex Predicates (CPs) 
improves the performance of the system as the 
number of identified Complex Predicates 
(CPs) increases.  
Manual evaluation is carried out on two 
types of Bengali corpus. The experiments are 
carried out on 800 development sentences 
from two corpora but the final evaluation is 
carried out on 1000 sentences. Overall, the 
system achieves F-Scores of 75.73%, and 
77.92% for compound verbs and 89.90% and 
89.66% for conjunct verbs respectively.  
38
The error analysis shows that not only the 
lexical patterns but also the augmentation of 
argument structure agreement (Das, 2009), the 
analysis of Non-MonoClausal Verb (NMCV) or 
Serial Verb, Control Construction (CC), 
Modal Control Construction (MCC), Passives 
(Pass) and Auxiliary Construction (AC) (Butt, 
1993; Paul, 2004) are also necessary to 
identify the Complex Predicates (CPs). The 
error analysis shows that the system suffers in 
distinguishing the Complex Predicates (CPs) 
from the above constraint constructions.  
The rest of the paper is organized as fol-
lows. Section 2 describes the related work 
done in this area. The automatic extraction of 
compound and conjunct verbs is described in 
Section 3. In Section 4, the identification of 
lexical scopes of the Complex Predicates 
(CPs) is mentioned. Section 5 discusses the 
results of evaluation along with error analysis. 
Finally, Section 6 concludes the paper. 
2 Related Work 
The general theory of complex predicate is 
discussed in Alsina (1996). Several attempts 
have been organized to identify complex 
predicates in South Asian languages (Abbi, 
1991; Bashir, 1993; Verma, 1993) with a spe-
cial focus to Hindi (Burton-Page, 1957; Hook, 
1974), Urdu (Butt, 1995), Bengali (Sarkar, 
1975; Paul, 2004), Kashmiri (Kaul, 1985) and 
Oriya (Mohanty, 1992). But the automatic ex-
traction of Complex Predicates (CPs) has been 
carried out for few languages, especially 
Hindi. 
The task described in (Mukherjee et al, 
2006) highlights the development of a database 
based on the hypothesis that an English verb is 
projected onto a multi-word sequence in Hindi. 
The simple idea of projecting POS tags across 
an English-Hindi parallel corpus considers the 
Complex Predicate types, adjective-verb (AV), 
noun-verb (NV), adverb-verb (Adv-V), and 
verb-verb (VV) composites. A similar task 
(Sinha, 2009) presents a simple method for 
detecting Complex Predicates of all kinds us-
ing a Hindi-English parallel corpus. His simple 
strategy exploits the fact that Complex Predi-
cate is a multi-word expression with a meaning 
that is distinct from the meaning of the Light 
Verb. In contrast, the present task carries the 
identification of Complex Predicates (CPs) 
from monolingual Bengali corpus based on 
morphological information and lexical pat-
terns. 
The analysis of V+V complex predicates 
termed as lexical compound verbs (LCpdVs) 
and the linguistic tests for their detection in 
Hindi are described in (Chakrabarti et al, 
2008). In addition to compound verbs, the pre-
sent system also identifies the conjunct verbs 
in Bengali. But, it was observed that the identi-
fication of Hindi conjunct verbs that contain 
noun in the first slot is puzzling and therefore a 
sophisticated solution was proposed in (Das, 
2009) based on the control agreement strategy 
with other overtly case marked noun phrases. 
The present task also agrees with the above 
problem in identifying conjunct verbs in Ben-
gali although the system satisfactorily identi-
fies the conjunct verbs (ConjVs). 
Paul (2003) develops a constraint-based 
mechanism within HPSG framework for com-
posing Indo-Aryan compound verb construc-
tions with special focus on Bangla (Bengali) 
compound verb sequences. Postulating seman-
tic relation of compound verbs, another work 
(Paul, 2009) proposed a solution of providing 
lexical link between the Full verb and Light 
Verb to store the Compound Verbs in Indo 
WordNet without any loss of generalization. 
To the best of our knowledge, ours is the first 
attempt at automatic extraction of Complex 
Predicates (CPs) in Bengali.  
3 Identification of Complex Predi-
cates (CPs) 
The compound verbs (CompVs) and conjunct 
verbs (ConjVs) are identified from the shallow 
parsed result using a lexical pattern matching 
technique. 
3.1 Preparation of Corpora 
Two types of Bengali corpus have been con-
sidered to carry out the present task. One cor-
pus is collected from a travel and tourism do-
main and another from an online web archive 
of Rabindranath Rachanabali 2 . Rabindra 
Rachanabali corpus is a large collection of 
short stories of Rabindranath Tagore. The for-
                                                 
2 www.rabindra-rachanabali.nltr.org 
39
mer EILMT travel and tourism corpus is ob-
tained from the consortium mode project ?De-
velopment of English to Indian Languages 
Machine Translation (EILMT 3) System?. The 
second type of corpus is retrieved from the 
web archive and pre-processed accordingly. 
Each of the Bengali corpora contains 400 and 
500 development and test sentences respec-
tively.   
The sentences are passed through an open 
source Bengali shallow parser 4. The shallow 
parser gives different morphological informa-
tion (root, lexical category of the root, gender, 
number, person, case, vibhakti, tam, suffixes 
etc.) that help in identifying the lexical patterns 
of Complex Predicates (CPs).  
3.2 Extracting Complex Predicates (CPs) 
Manual observation shows that the Complex 
Predicates (CPs) contain the lexical pattern 
{[XXX] (n/adj) [YYY] (v)} in the shallow 
parsed sentences where XXX and YYY repre-
sent any word. But, the lexical category of the 
root word of XXX is either noun (n) or adjec-
tive (adj) and the lexical category of the root 
word of YYY is verb (v). The shallow parsed 
sentences are pre-processed to generate the 
simplified patterns. An example of similar 
lexical pattern of the shallow parsed result and 
its simplified output is shown in Figure 1.  
 
((NP  a????  NN  <fs 
?f='a???? ,n,,sg,,d,?? ? ,?? ? '>  ))              
   
((VGF  ???????      VM       <fs 
?f='?r,v,,,5,,? ,? '>     )) 
a???? |no?n|a???? /NN/NP/ 
(a???? ^n^*^sg^*^d^?? ^?? ? )_ 
???????|v??b|???????/VM/VGF/              
(?r^v^*^*^1^*^? ^? ) 
         
 Figure 1. Example of a pre-processed shallow 
parsed result. 
 
                                                 
3 The EILMT project is funded by the Department of 
Information Technology (DIT), Ministry of Communica-
tions and Information Technology (MCIT), Government 
of India. 
4http://ltrc.iiit.ac.in/showfile.php?filename=downloads/sh
allow_parser.php 
The corresponding lexical categories of the 
root words a???? adhyan ?study? (e.g. noun 
for ?n?) and '?r  kar, ?do? (e.g. verb for ?v?) are 
shown in bold face in Figure 1. The f ollowing 
example is of conjunct verb (ConjV).  
The extraction of Bengali compound verbs 
(CompVs) is straightforward rather than con-
junct verbs (ConjVs). The lexical pattern of 
compound verb is {[XXX](v) [YYY] (v)} where 
the lexical or basic POS categories of the root 
words of  ?XXX? and ?YYY? are only verb. If 
the basic POS tags of the root forms of ?XXX? 
and ?YYY? are verbs (v) in shallow parsed sen-
tences, then only the corresponding lexical 
patterns are considered as the probable candi-
dates of compound verbs (CompVs).  
Example 1: 
??i??|v??b|??i??/VM/VGNF/? ? ^v^*^*^?ny^*^i??^i??)
#??????|v??b|??????/VM/VGF/(??^v^*^*^1^*^?^?) 
Example 1 is a compound verb (CompV) but 
Example 2 is not. In Example 2, the lexical 
category or the basic POS of the Full Verb 
(FV) is noun (n) and hence the pattern is dis-
carded as non-compound verb (non-CompV). 
Example 2: 
?k? |noun|?k? /NN/NP/(?k? ^n^*^*^*^*^*^pos
lcat="NM") #  
?????|verb|?????/VM/VGNF/(?r^v^*^*^any^*^i??
^i??) 
Bengali, like any other Indian languages, is 
morphologically very rich. Different suffixes 
may be attached to a Light Verb (LVs) (in this 
case [YYY]) depending on the various features 
such as Tense, Aspect, and Person.  
In case of extracting compound verbs 
(CompVs), the Light Verbs are identified from 
a seed list (Paul, 2004). The list of Light Verbs 
is specified in Table 1. The dictionary forms of 
the Light Verbs are stored in this list. As the 
Light Verbs contain different suffixes, the pri-
mary task is to identify the root forms of the 
Light Verbs (LVs) from shallow parsed result. 
Another table that stores the root forms and the 
corresponding dictionary forms of the Light 
Verbs is used in the present task. The table 
contains a total number of 378 verb entries 
including Full Verbs (FVs) and Light Verbs 
(LVs). The dictionary forms of the Light Verbs 
(LVs) are retrieved from the Table. 
On the other hand, the conjunctive particip-
ial form -e/i?? -e/iya or the infinitive form -
? /i?? ?te/ite are attached with the Full Verbs 
40
(FVs) (in this case [XXX]) in compound verbs 
(CompVs). i?? / iya and i??/ ite are also used 
for conjunctive participial form -e ?e or the 
infinitive form -?  ?te respectively in litera-
ture. The participial and infinitive forms are 
checked based on the morphological informa-
tion (e.g. suffixes of the verb) given in the 
shallow parsed results. In Example 1, the Full 
Verb (FV) contains -i?? -iya suffix. If the dic-
tionary forms of the Light Verbs (LVs) are pre-
sent in the list of Light Verbs and the Full 
Verbs (FVs) contain the suffixes of -e/i?? -
e/iya or ? /i?? ?te/ite, both verbs are combined 
to frame the patterns of compound verbs 
(CompVs). 
 
aSa ?come?          d?Ra ?stand? 
rakha ?keep?       ana ?bring?  
deoya ?give?        pOra ?fall?  
paTha ?send?       bERano ?roam? 
neoya ?take?        tola ?lift? 
bOSa ?sit?           oTha ?rise? 
jaoya ?go?           chaRa ?leave? 
phEla ?drop?       mOra ?die? 
 
Table 1. List of Light Verbs for compound 
verbs. 
The identification of conjunct verbs 
(ConjVs) requires the lexical pattern (Noun / 
Adjective + Light Verb) where a noun or an 
adjective is followed by a Light Verb (LV). The 
dictionary forms of the Light Verbs (LVs) that 
are frequently used as conjunct verbs (ConjVs) 
are prepared manually. The list of Light Verbs 
(LVs) is given in Table 2. The detection of 
Light Verbs (LVs) for conjunct verbs (ConjVs) 
is similar to the detection of the Light Verbs 
(LVs) for compound verbs (CompVs) as de-
scribed earlier in this section.  If the basic POS 
of the root of the first words ([XXX]) is either 
?noun? or ?adj? (n/adj) and the basic POS of 
the following word ([YYY]) is ?verb? (v), the 
patterns are considered as conjunct verbs 
(ConjVs). The Example 2 is an example of 
conjunct verb (ConjV). 
For example, ???? ??? (jhakjhak kara ?to 
glow?), ???? ??? (taktak ?to glow?), ?? ??? ??? 
(chupchap kara ?to silent?) etc are identified as 
conjunct verbs (ConjVs) where the basic POS 
of the former word is an adjective (adj) fol-
lowed by ??? kara ?to do?, a common Light 
Verb.  
deoya ?give?        kara  ?do?    
neoya ?take?        laga  ?start?            
paoya ?pay?         kata  ?cut?   
  
Table 2. List of Light Verbs for conjunct verbs. 
 
Example 3:  
  ????|?dj|???? /JJ/JJP/(???? ^?dj) # 
????|v??b|????/VM/VGF/(?r^v^*^*^5^*^?^?) 
But, the extraction of conjunct verbs 
(ConjVs) that have a ?noun+verb? construction 
is descriptively and theoretically puzzling 
(Das, 2009). The identification of lexical pat-
terns is not sufficient to recognize the com-
pound verbs (CompVs). For example, ?i ? ??? 
boi deoya ?give book? and ???? ? ??? bharsa 
deyoa ?to assure? both contain similar lexical 
pattern (noun+verb) and same Light Verb ? ??? 
deyoa. But, ???? ? ??? bharsa deyoa ?to assure? 
is a conjunct verb (ConjV) whereas ?i ? ??? boi 
deoya ?give book? is not a conjunct verb 
(ConjV). Linguistic observation shows that the 
inclusion of this typical category into conjunct 
verbs (ConjVs) requires the additional knowl-
edge of syntax and semantics.  
In connection to conjunct verbs (ConjVs), 
(Mohanty, 2010) defines two types of conjunct 
verbs (ConjVs), synthetic and analytic. A syn-
thetic conjunct verb is one in which both the 
constituents form an inseparable whole from 
the semantic point of view or semantically 
non-compositional in nature. On the other 
hand, an analytic conjunct verb is semantically 
compositional. Hence, the identification of 
conjunct verbs requires knowledge of seman-
tics rather than only the lexical patterns. 
It is to be mentioned that sometimes, the 
negative markers (?? no, ??i nai) are attached 
with the Light Verbs u?????  uthona ?do not get 
up? ? ?????  phelona ?do not throw?. Negative 
attachments are also considered in the present 
task while checking the suffixes of Light Verbs 
(LVs). 
4 Identification of Lexical Scope for 
Complex Predicates (CPs) 
The identification of lexical scopes of the 
Complex Predicates (CPs) from their succes-
sive sequences shows that multiple Complex 
41
Predicates (CPs) can occur in a long sequence. 
An automatic method is employed to identify 
the Complex Predicates (CPs) along with their 
lexical scopes. The lexical category or basic 
POS tags are obtained from the parsed sen-
tences. 
If the compound and conjunct verbs occur 
successively in a sequence, the left most two 
successive tokens are chosen to construct the 
Complex Predicate (CP). If successive verbs 
are present in a sequence and the dictionary 
form of the second verb reveals that the verb is 
present in the lists of compound Light Verbs 
(LV), then that Light Verb (LV) may be a part 
of a compound verb (CompV). For that reason, 
the immediate previous word token is chosen 
and tested for its basic POS in the parsed result. 
If the basic POS of the previous word is ?verb 
(v)? and any suffixes of either conjunctive par-
ticipial form -e/i?? -e/iya or the infinitive form 
-? /i?? ?te/ite is attached to the previous verb, 
the two successive verbs are grouped together 
to form a compound verb (CompV) and the 
lexical scope is fixed for the Complex Predi-
cate (CP).  
If the previous verb does not contain -e/i?? 
-e/iya or -? /i?? ?te/ite inflections, no com-
pound verb (CompV) is framed with these two 
verbs. But, the second Light Verb (LV) may be 
a part of another Complex Predicate (CP). This 
Light Verb (LV) is now considered as the Full 
Verb (FV) and its immediate next verb is 
searched in the list of compound Light Verbs 
(LVs) and the formation of compound verbs 
(CompVs) progresses similarly.  If the verb is 
not in the list of compound Light Verbs, the 
search begins by considering the present verb 
as Full Verb (FV) and the search goes in a 
similar way. 
The following examples are given to illus-
trate the formation of compound verbs 
(CompVs) and find the lexical scopes of the 
compound verbs (CompVs). 
 
???      ????       ????     ???      ? ??? 
(ami)       (chalte)      (giye)    (pore)    (gelam). 
I <fell down while walking>. 
 
Here, ?chalte giye pore gelam? is a verb 
group. The two left most verbs ???? ???? chalte 
giye are picked and the dictionary form of the 
second verb is searched in the list of com-
pound Light Verbs. As the dictionary form 
(jaoya ?go?) of the verb ???? giye is present in 
the list of compound Light Verbs (as shown in 
Table 1), the immediate previous verb ???? 
chalte is checked for inflections -e/i?? -e/iya 
or -? /i?? ?te/ite.  As the verb ???? chalte con-
tains the inflection -?  -te , the verb group ???? 
???? chalte giye is a compound verb (CompV) 
where ???? giye is a Light Verb and ???? chalte 
is the Full Verb with inflection (-?  -te).  Next 
verb group, ???   ? ??? pore gelam is identified 
as compound verb (CompV) in a similar way 
(??+ (-e) por+ (-e) + ? ??? gelam (jaoya ?go?)).  
Another example is given as follows.  
 
???    u??      ???       ? ????       ?   
(ami)   (uthe)      (pore)      (dekhlam)    (je)  
?? ??      e????        ? i 
(tumi)     (ekhane)       (nei) 
I <get up and saw> that you are not here 
 
Here, u?? ??? ? ???? uthe pore dekhlam is 
another verb group. The immediate next verb 
of u?? uthe is ??? pore that is chosen and its 
dictionary form is searched in the list of com-
pound Light Verbs (LV) similarly. As the dic-
tionary form (???  pOra) of the verb ??? pore 
is present in the list of Light Verbs and the 
verb u??   uthe contains the inflection -e ?e, 
the consecutive verbs frame a compound verb 
(CompV) u?? ??? where u?? uthe is a Full Verb 
with inflection -e ?e and ??? pore is a Light 
Verb. The final verb ? ????      dekhlam is 
chosen and as there is no other verb present, 
the verb ? ???? dekhlam is excluded from any 
formation of compound verb (CompV) by con-
sidering it as a simple verb.  
Similar technique is adopted for identifying 
the lexical scopes of conjunct verbs (ConjVs). 
The method seems to be a simple pattern 
matching technique in a left-to-right fashion 
but it helps in case of conjunct verbs (ConjVs). 
As the noun or adjective occur in the first slot 
of conjunct verbs (ConjVs) construction, the 
search starts from the point of noun or adjec-
tive. If the basic POS of a current token is ei-
ther ?noun? or ?adjective? and the dictionary 
form of the next token with the basic POS 
?verb (v)? is in the list of conjunct Light Verbs 
(LVs), then the two consecutive tokens are 
42
combined to frame the pattern of a conjunct 
verb (ConjV). 
For example, the identification of lexical 
scope of a conjunct verb (ConjV) from a se-
quence such as u????  ???? ? ??? uparjon korte 
gelam ?earn-do-go? (went to earn) identifies 
the conjunct verb (ConjV) u????  ???? uparjon 
korte. There is another verb group ???? ? ??? 
korte gelam that seems to be a compound verb 
(CompV) but is excluded by considering ?? ??? 
gelam as a simple verb. 
5 Evaluation 
The system is tested on 800 development sen-
tences and finally applied on a collection of 
500 sentences from each of the two Bengali 
corpora. As there is no annotated corpus avail-
able for evaluating Complex Predicates (CPs), 
the manual evaluation of total 1000 sentences 
from the two corpora is carried out in the pre-
sent task.  
The recall, precision and F-Score are con-
sidered as the standard metrics for the present 
evaluation. The extracted Complex Predicates 
(CPs) contain compound verb (CompV) and 
conjunct verbs (ConjVs). Hence, the metrics 
are measured for both types of verbs individu-
ally. The separate results for two separate cor-
pora are shown in Table 3 and Table 4 respec-
tively. The results show that the system identi-
fies the Complex Predicates (CPs) satisfacto-
rily from both of the corpus. In case of Com-
pound Verbs (CompVs), the precision value is 
higher than the recall. The lower recall value 
of Compound Verbs (CompVs) signifies that 
the system fails to capture the other instances 
from overlapping sequences as well as non-
Complex predicates (non-CPs).  
But, it is observed that the identification of 
lexical scopes of compound verbs (CompVs) 
and conjunct verbs (ConjVs) from long se-
quence of successive Complex Predicates 
(CPs) increases the number of Complex Predi-
cates (CPs) entries along with compound verbs 
(CompVs) and conjunct verbs (ConjVs). The 
figures shown in bold face in Table 3 and Ta-
ble 4 for the Travel and Tourism corpus and 
Short Story corpus of Rabindranath Tagore 
indicates the improvement of identifying lexi-
cal scopes of the Complex Predicates (CPs).  
In comparison to other similar language 
such as Hindi (Mukerjee et al, 2006) (the re-
ported precision and recall are 83% and 46% 
respectively), our results (84.66% precision 
and 83.67% recall) are higher in case of ex-
tracting Complex Predicates (CPs). The reason 
may be of resolving the lexical scope and han-
dling the morphosyntactic features using shal-
low parser.  
In addition to Non-MonoClausal Verb 
(NMCV) or Serial Verb, the other criteria 
(Butt, 1993; Paul, 2004) are used in our pre-
sent diagnostic tests to identify the complex 
predicates (CPs). The frequencies of 
Compound Verb (CompV), Conjunct Verb 
(ConjV) and the instances of other constraints 
of non Complex Predicates (non-CPs) are 
shown in Figure 2. It is observed that the num-
bers of instances of Conjunct Verb (ConjV), 
Passives (Pass), Auxiliary Construction (AC) 
and Non-MonoClausal Verb (NMCV) or Serial 
Verb are comparatively high than other in-
stances in both of the corpus. 
 
EILMT  Recall Precision F-
Score 
Compound  
Verb 
(CompV) 
65.92% 
70.31% 
 
80.11% 
82.06% 
72.32% 
75.73%
Conjunct 
Verb 
(ConjV) 
94.65% 
96.96% 
80.44% 
83.82% 
86.96% 
89.90%
 
Table 3. Recall, Precision and F-Score of the 
system for acquiring the CompVs and ConjVs 
from EILMT Travel and Tourism Corpus. 
 
Rabindra 
Rachana-
bali 
Recall Precision F-
Score 
Compound  
Verb 
(CompV) 
68.75% 
72.22% 
 
81.81% 
84.61% 
74.71% 
77.92%
Conjunct 
Verb 
(ConjV) 
94.11% 
95.23% 
83.92% 
84.71% 
88.72% 
89.66%
 
Table 4. Recall, Precision and F-Score of the 
system for acquiring the CompVs and ConjVs 
from Rabindra Rachanabali corpus. 
 
43
 
 CompV ConjV NMCV CC MCC Pass AC 
CompV 0.76 0.00 0.02 0.00 0.00 0.03 0.02 
ConjV 0.04 0.72 0.03 0.01 0.02 0.02 0.02 
NMCV 0.17 0.18 0.65 0.00 0.02 0.02 0.02 
CC 0.01 0.00 0.00 0.56 0.01 0.02 0.02 
MCC 0.00 0.00 0.00 0.07 0.65 0.00 0.02 
Pass 0.12 0.01 0.00 0.00 0.00 0.78 0.00 
AC 0.06 0.07 0.04 0.00 0.00 0.08 0.54 
Table 5. Confusion Matrix for CPs and constraints of non-CPs (in %).  
 
0
50
100
150
200
CompV
ConjV
CC MCC
Pass
AC NMCV
EILMTRabindra
  
Figure 2. The frequencies of Complex Predi-
cates (CPs) and different constrains of non-
Complex Predicates (non-CPs). 
 
The error analysis is conducted on both of 
the corpus. Considering both corpora as a 
whole single corpus, the confusion matrix is 
developed and shown in Table 5. The bold face 
figures in Table 5 indicate that the percentages 
of non-Complex Predicates (non-CPs) such as 
Non-MonoClausal Verbs (NMCV), Passives 
(Pass) and Auxiliary Construction (AC) that 
are identified as compound verbs (CompVs). 
The reason is the frequencies of the non-
Complex Predicates (non-CPs) that are rea-
sonably higher in the corpus.  In case of con-
junct verbs (ConjVs), the Non-MonoClausal 
Verbs (NMCV) and Auxiliary Construction 
(AC) occur as conjunct verbs (ConjVs).  The 
system also suffers from clausal detection that 
is not attempted in the present task. The Pas-
sives (Pass) and Auxiliary Construction (AC) 
requires the knowledge of semantics with ar-
gument structure knowledge. 
6 Conclusion 
In this paper, we have presented a study of 
Bengali Complex Predicates (CPs) with a spe-
cial focus on compound verbs, proposed auto-
matic methods for their extraction from a cor-
pus and diagnostic tests for their evaluation. 
The problem arises in case of distinguishing 
Complex Predicates (CPs) from Non-Mono-
Clausal verbs, as only the lexical patterns are 
insufficient to identify the verbs. In future task, 
the subcategorization frames or argument 
structures of the sentences are to be identified 
for solving the issues related to the errors of 
the present system.  
References 
Abbi, Anvita. 1991. Semantics of Explicator Com-
pound Verbs. In South Asian Languages, Lan-
guage Sciences, 13(2): 161-180. 
Alsina, Alex. 1996. Complex Predicates: Structure 
and Theory. Center for the Study of Language 
and Information Publications, Stanford, CA. 
Bashir, Elena. 1993. Causal chains and compound 
verbs. In M. K. Verma ed. (1993) Complex 
Predicates in South Asian Languages, Manohar 
Publishers and Distributors, New Delhi. 
Burton-Page, John. 1957. Compound and conjunct 
verbs in Hindi. Bulletin of the School of Oriental 
and African Studies, 19: 469-78. 
Butt, Miriam. 1995. The Structure of Complex 
Predicates in Urdu. Doctoral Dissertation, Stan-
ford University. 
Chakrabarti, Debasri, Mandalia Hemang, Priya 
Ritwik, Sarma Vaijayanthi, Bhattacharyya Push-
pak. 2008. Hindi Compound Verbs and their 
Automatic Extraction. International Conference 
on Computational Linguistics ?2008, pp. 27-30. 
44
Das, Pradeep Kumar. 2009. The form and function 
of Conjunct verb construction in Hindi. Global 
Association of Indo-ASEAN Studies, Daejeon, 
South Korea. 
Hook, Peter. 1974. The Compound Verbs in Hindi. 
The Michigan Series in South and South-east 
Asian Language and Linguistics. The University 
of Michigan. 
Kaul, Vijay Kumar. 1985. The Compound Verb in 
Kashmiri. Unpublished Ph.D. dissertation. Ku-
rukshetra University. 
Kipper-Schuler, Karin. 2005. VerbNet: A broad-
coverage,  comprehensive verb lexicon. Ph.D. 
thesis, Computer and Information Science Dept., 
University of Pennsylvania, Philadelphia,PA  
Miller, George, Richard Beckwith, Christiane Fell-
baum, Derek Gross and Katherine Miller. 1990. 
Five Papers on WordNet. CSL Report 43, Cogni-
tive Science Laboratory, Princeton University, 
Princeton. 
Mohanty, Gopabandhu. 1992. The Compound 
Verbs in Oriya. Ph. D. dissertation, Deccan Col-
lege Post-Graduate and Research Institute, Pune. 
Mohanty, Panchanan. 2010. WordNets for Indian 
Languages: Some Issues. Global WordNet Con-
ference-2010, pp. 57-64. 
Mukherjee, Amitabha, Soni Ankit and Raina Achla 
M. 2006. Detecting Complex Predicates in Hindi 
using POS Projection across Parallel Corpora. 
Multiword Expressions: Identifying and Exploit-
ing Underlying Properties Association for Com-
putational Linguistics, pp. 28?35, Sydney.  
Paul, Soma. 2010. Representing Compound Verbs 
in Indo WordNet. Golbal Wordnet Conference-
2010, pp. 84-91. 
Paul, Soma. 2004. An HPSG Account of Bangla 
Compound Verbs with LKB Implementation. 
Ph.D dissertation, University of Hyderabad, Hy-
derabad. 
Paul, Soma. 2003. Composition of Compound 
Verbs in Bangla. Multi-Verb constructions. 
Trondheim  Summer School. 
Sarkar, Pabitra. 1975. Aspects of Compound Verbs 
in Bengali. Unpublished M.A. dissertation, Chi-
cago University. 
Sinha, R. Mahesh, K. 2009. Mining Complex 
Predicates In Hindi Using A Parallel Hindi-
English Corpus. Multiword Expression Work-
shop, Association of Computational Linguistics-
International Joint Conference on Natural Lan-
guage Processing-2009, pp. 40-46, Singapore. 
Timothy, Baldwin, Su Nam Kim. 2010. Multiword 
Expressions. In Nitin Indurkhya and Fred J. 
Damerau (eds.) Handbook of Natural Language 
Processing, Second Edition, Chapman & 
Hall/CRC, London, UK, pp. 267-292. 
Verma, Manindra K.1993. Complex Predicates in 
South Asian Languages. Manohar Publishers and 
Distributors, New Delhi. 
 
45
Proceedings of the Workshop on Multiword Expressions: from Parsing and Generation to the Real World (MWE 2011), pages 8?13,
Portland, Oregon, USA, 23 June 2011. c?2011 Association for Computational Linguistics
Semantic Clustering: an Attempt to Identify Multiword Expressions in 
Bengali 
Tanmoy Chakraborty        Dipankar Das        Sivaji Bandyopadhyay 
Department of Computer Science and Engineering 
Jadavpur University, Kolkata 700 032, India 
its_tanmoy@yahoo.co.in, dipankar.dipnil2005@gmail.com  
sivaji_cse_ju@yahoo.com 
 
Abstract 
One of the key issues in both natural lan-
guage understanding and generation is the 
appropriate processing of Multiword Ex-
pressions (MWEs). MWE can be defined 
as a semantic issue of a phrase where the 
meaning of the phrase may not be obtained 
from its constituents in a straightforward 
manner. This paper presents an approach of 
identifying bigram noun-noun MWEs from 
a medium-size Bengali corpus by cluster-
ing the semantically related nouns and in-
corporating a vector space model for 
similarity measurement. Additional inclu-
sion of the English WordNet::Similarity 
module also improves the results consider-
ably. The present approach also contributes 
to locate clusters of the synonymous noun 
words present in a document. Experimental 
results draw a satisfactory conclusion after 
analyzing the Precision, Recall and F-score 
values.  
1 Introduction 
Over the past two decades or so, Multi-Word Ex-
pressions (MWEs) have been identified with an 
increasing amount of interest in the field of Com-
putational linguistics and Natural Language 
Processing (NLP). The term MWE is used to refer 
the various types of linguistic units and expres-
sions including idioms (kick the bucket, ?to die?), 
noun compounds (village community), phrasal 
verbs (find out, ?search?) and other habitual collo-
cations like conjunction (as well as), institutiona-
lized phrases (many thanks) etc. They can also be 
grossly defined as ?idiosyncratic interpretations 
that cross the word boundaries? (Sag et al, 2002).  
MWE is considered as a special issue of seman-
tics where the individual components of an expres-
sion often fail to keep their meanings intact within 
the actual meaning of the expression. This opaque-
ness in meaning may be partial or total depending 
on the degree of compositionality of the whole ex-
pression. In Bengali, an analogous scenario can be 
observed when dealing with the expressions like 
compound nouns (taser ghar, ?house of cards?, 
?fragile?), complex predicates such as conjunct 
verbs (anuvab kara, ?to feel?) and compound verbs 
(uthe para, ?to arise?), idioms (matir manus, ?down 
to the earth?), Named Entities (NEs) (Rabindra-
nath Thakur, ?Rabindranath Tagore?) etc.  
In this paper, we analyze MWEs from the pers-
pective of semantic interpretation. We have fo-
cused mainly on the fact that the individual 
meanings of the components are totally or partially 
diminished in order to form the actual semantics of 
the expression. A constellation technique has been 
employed to group all nouns that are somehow 
related to the meaning of the component of any 
expression in the corpus and hence to build cluster 
for that component. Two types of vector space 
based similarity techniques are applied to make a 
binary classification of the candidate nouns. The 
intuition was that more the similarity of the com-
ponents of an expression, less the probability of the 
candidate to become a MWE. We have also shown 
the results using WordNet::Similarity module.  
The remainder of the paper is organized as fol-
lows. In the next section, we review the related 
work on MWE and graph-clustering approach for 
detecting compositionality. Section 3 proposes a 
brief description of the semantic clustering ap-
proach. The system framework is elaborated in 
Section 4. Experimental results and the various 
observations derived from our research are dis-
cussed in Section 5. Finally, Section 6 concludes 
the paper. 
8
2 Related Work 
A number of research activities regarding MWE 
identification have been carried out in various lan-
guages like English, German and many other Eu-
ropean languages. The statistical co-occurrence 
measurements such as Mutual Information (MI) 
(Church and Hans, 1990), Log-Likelihood (Dun-
ning, 1993) and Salience (Kilgarriff and Rosenz-
weig, 2000) have been suggested for identification 
of MWEs. An unsupervised graph-based algorithm 
to detect the compositionality of MWEs has been 
proposed in (Korkontzelos and Manandhar 2009). 
In case of Indian languages, an approach in 
compound noun MWE extraction (Kunchukuttan 
and Damani, 2008) and a classification based ap-
proach for Noun-Verb collocations (Venkatapathy 
and Joshi, 2009) have been reported. In Bengali, 
the works on automated extraction of MWEs are 
limited in number. One method of automatic ex-
traction of Noun-Verb MWE in Bengali (Agarwal 
et al, 2004) has been carried out using significance 
function. In contrast, we have proposed a cluster-
ing technique to identify Bengali MWEs using se-
mantic similarity measurement. It is worth noting 
that the conducted experiments are useful for iden-
tifying MWEs for the electronically resource con-
strained languages.   
3 Semantic Clustering Approach 
Semantic clustering aims to cluster semantically 
related tokens present in a document. Identifying 
semantically related words for a particular token is 
carried out by looking the surrounding tokens and 
finding the synonymous words within a fixed con-
text window. Statistical idiomaticity demands fre-
quent occurrence of a particular expression as one 
or few occurrences of a particular word cannot in-
fer all its meaning. However, the semantics of a 
word may be obtained by analyzing its similarity 
sets called synset.  Higher value of the similarity 
coefficient between two synonymous sets of the 
multi-word components indicates more affinity of 
the components to each other.  
For individual component of a bigram expres-
sion, semantically related words of the documents 
are extracted by using a monolingual dictionary (as 
discussed in Section 4.4). Count of elements in an 
intersection of two synsets indicates the commo-
nality of the two sets and its absolute value stands 
for their commonality measure. Considering the 
common elements as the dimensions of the vector 
space, similarity based techniques are applied to 
measure the semantic affection of the two compo-
nents present in a bigram. 
4 System Framework 
4.1 Corpus Preparation and Candidate Selec-
tion 
The system uses a large number of Bengali articles 
written by the noted Indian Nobel laureate Rabin-
dranath Tagore 1 . We are primarily interested in 
single document term affinity rather than document 
information and document length normalization. 
Merging all of the articles, a medium size raw cor-
pus consisting of 393,985 tokens and 283,533 
types has been prepared. Basic pre-processing of 
the crawled corpus is followed by parsing with the 
help of an open source shallow parser2 developed 
for Bengali. Parts-of-Speech (POS), chunk, root, 
inflection and other morphological information for 
each token have been retrieved. Bigram noun se-
quence within a noun chunk is extracted and 
treated as candidates based on their POS, chunk 
categories and the heuristics described as follows.   
1. POS:   POS of each token is either ?NN? or         
?NNP? 
2. Chunk: w1 and w2 must be in the same ?NP?   
chunk 
3. Inflection: Inflection 3  of w1 must be                
?-    ????(null), ?-??(-r), ?-???(-er), ?-
??(-e), ?-??(-y) or ?-????(-yr) and for 
w2, any inflection is considered. 
4.2 Dictionary Restructuring 
To the best of our knowledge, no full-fledged 
WordNet resource is available for Bengali. Hence, 
the building of Bengali synsets from a monolingual 
Bengali dictionary not only aims to identify the 
meaning of a token, but also sets up the framework 
towards the development of Bengali WordNet. 
Each word present in the monolingual dictionary 
(Samsada Bengali Abhidhana)4 contains its POS, 
                                                        
1 http://www.rabindra-rachanabali.nltr.org 
2  http://ltrc.iiit.ac.in/analyzer/bengali 
3  Linguistic study (Chattopadhyay, 1992) reveals that for 
compound noun MWE, considerable inflections of first noun 
are only those which are mentioned above. 
4  http://dsal.uchicago.edu/dictionaries/biswas-bangala/ 
9
phonetics and synonymous sets. An automatic 
technique has been devised to identify the synsets 
of a particular word based on the clues (?,? comma 
and ?;? semi-colon) provided in the dictionary to 
distinguish words of similar and different sense 
from the synonymous sets. The symbol tilde (~) 
indicates that the suffix string followed by the tilde 
(~) notation makes another new word concatenat-
ing with the original entry word. A partial snapshot 
of the synsets for the Bengali word ????? (Ang-
shu) is shown in Figure 1. In Table 1, the frequen-
cies of different synsets according to their POS are 
shown. 
Dictionary Entry: 
??? [a??u] ??. 1 ????, ???, ???; ~ ? 
??. ?? , ??? ?? ; ???? ??? ???????? ??? 
???  ~ ??? ??. ????????, ?????????  
Synsets: 
??? ????/???/???_??.#25_1_1  
???? ??/???_??_??.#26_1_1  
???? ????_???_????????_???_??_??.#26_2_2 
?????? ????????/????????_??.#27_1_1 
Figure 1: A partial snapshot of the Bengali mono-
lingual dictionary entry (word and synsets) 
 
Total 
#Word  
Total 
#Synset 
Noun Adj- 
ective 
Pro- 
noun 
Verb 
33619 63403 28485 11023 235 1709 
Table 1: Total number of words, synsets and Fre-
quencies of different POS based synsets 
4.3 Generating Semantic Clusters of Nouns 
In the first phase, we have generated the synonym-
ous sets for all nouns present in the corpus using 
the synset based dictionary whereas in the second 
phase, the task is to identify the semantic distance 
between two nouns. The format of the dictionary 
can be thought of as follows:  
W1=n1
1, n2
1, n3
1,  ?????? = {ni
1} 
     . 
 . 
Wm=n1
m, n2
m, n3
m,  ?????. = {np
m} 
where, W1, W2, ?.,W
m are the dictionary word en-
tries and nj
m (for all j) are the elements of the syn-
sets of Wm. Now, each noun entry identified by the 
shallow parser in the document is searched in the 
dictionary. For example, if a noun N present the 
corpus becomes an entry of the synsets, W1, W
3
 and 
W5, the synset of N is as follows,  
            SynSet (N) = {Wl, W3, W5}??? (1) 
To identify the semantic similarity between two 
nouns, we have applied simple intersection rule. 
The number of common elements between the syn-
sets of the two noun words denotes the similarity 
between them. If Ni and Nj are the two noun words 
in the document and Wi and Wj are their corres-
ponding synsets, the similarity of the two words 
can be defined as, 
               Similarity (Ni,Nj) = |W
i ? Wj|???.(2) 
We have clustered all the nouns present in the 
document for a particular noun and have identified 
the similarity score for every pair of nouns ob-
tained using equation 2. 
4.4 Checking of Candidate Bigram as MWE  
The identification of candidates as MWE is done 
using the results obtained from the previous phase. 
The algorithm to identify the noun-noun bigram 
<M1 M2> as MWE is discussed below with an 
example shown in Figure 2. 
 
ALOGRITHM:  MWE-CHECKING 
    INPUT: Noun-noun bigram <M1 M2> 
    OUTPUT: Return true if MWE, or return false. 
1. Extract semantic clusters of M1 and M2 
2. Intersection of the clusters of both M1 and M2 
(Figure 2.1 shows the common synset entries of 
M1 and M2 using rectangle). 
3. For measuring the semantic similarity between 
M1 and M2: 
3.1. In an n-dimensional vector space (here 
n=2), the common entries act as the axes. Put 
M1 and M2 as two vectors and associated 
weights as their co-ordinates. 
3.2. Calculate cosine-similarity measurement 
and Euclidean distance (Figure 2.2). 
4. Final decision taken individually for two differ-
ent measurements- 
4.1 If cosine-similarity > m, return false;            
Else return true; 
  4.2 If Euclidean-distance >  p, return false; 
                Else return true; 
(Where m and p are the pre-defined cut-off values) 
 
 
    We have also employed English WordNet 5  to 
measure   the   semantic   similarity   between   two  
                                                        
5 http://www.d.umn.edu/tpederse/similarity.html 
10
 
 
 
 
 
 
 
 
 
Figure 2.1: Intersection of the clusters of the con-
stituents (left side); Figure 2.2: Similarity between 
two constituents Evaluation (right side) 
Bengali words translated into English. Word-
Net::Similarity is an open-source package for cal-
culating the lexical similarity between word (or 
sense) pairs based on various similarity measures. 
Basically, WordNet measures the relative distance 
between two nodes denoted by two words in the 
WordNet tree which can vary from -1 to 1 where    
-1 indicates total dissimilarity between two nodes. 
The equation used to calculate this distance is men-
tioned below- 
    Normalized_Distance= minDistToCommonPa-
rent / (DistFromCommonParentToRoot + min-
DistToCommonParent)                    ????..(3) 
We have translated the root of the two compo-
nents of a Bengali candidate into their English 
equivalents using a Bengali to English bilingual 
dictionary. They are passed into the WordNet 
based similarity module for measuring similarity 
between the components.  
If we take an example of a Bengali idiom hater 
panch (remaining resource) to describe our intui-
tion, we have seen that the WordNet defines two 
components of the idiom hat (hand) as ?a part of a 
limb that is farthest from the torso? and panch 
(five) as ?a number which is one more than four?. 
So from these two glosses it is quite clear that they 
are not at all semantically related in any sense.   
 
 
 
 
 
 
The synonymous sets for these two components 
extracted from the formatted dictionary are shown 
below ? 
Synset (??? ?hat?) = { ??, ??, ????, ???, ???, 
?????, ?????, ????, ????, ?????, ?????, 
?????, ???? } 
Synset (???? ?panch?) = {??, ?????, ???, ???, 
???, ????, ??, ????, ??, ????, ???, ??????, 
???????, ???? } 
It is clearly seen from the above synonymous 
sets that there is no common element and hence its 
similarity score is obviously zero. In this case, the 
vector space model cannot be drawn using zero 
dimensions. For them, a marginal weight is as-
signed to show them as completely non-
compositional phrase. To identify their non-
compositionality, we have to show that their occur-
rence is not certain only in one case; rather they 
can occur side by side in several occasions. But 
this statistical proof can be determined better using 
a large corpus. Here, for those candidate phrases, 
which show zero similarity, we have seen their 
existence more than one time in the corpus. Taking 
any decision using single occurrence may give in-
correct result because they can be unconsciously 
used by the authors in their writings. That is why, 
the more the similarity between two components in 
a bigram, the less the probability to be a MWE. 
4.5 Annotation Agreement 
Three annotators identified as A1, A2 and A3 were 
engaged to carry out the annotation. The annota-
tion agreement of 628 candidate phrases is meas-
ured using standard Cohen's kappa coefficient (?) 
(Cohen, 1960). It is a statistical measure of inter-
rater agreement for qualitative (categorical) items. 
In addition to this, we also choose the measure of 
agreements on set-valued items (MASI) (Passon-
neau, 2006) that was used for measuring agreement 
in the semantic and pragmatic annotation.  Annota-
tion results as shown in Table 2 are satisfactory.  
 
Cut-off 
Cosine-Similarity Euclidean Distance WordNet Similarity 
P R FS P R FS P R FS 
0.6 70.75 64.87 67.68 70.57 62.23 66.14 74.60 61.78 67.58 
0.5 78.56 59.45 67.74 72.97 58.79 65.12 80.90 58.75 68.06 
0.4 73.23 56.97 64.08 79.78 53.03 63.71 75.09 52.27 61.63 
 
Table 3: Precision (P), Recall (R) and F-score (FS) (in %) for various measurements 
11
The list of noun-noun collocations are extracted 
from the output of the parser for manual checking. 
It is observed that 39.39% error occurs due to 
wrong POS tagging or extracting invalid colloca-
tions by considering the bigrams in a n-gram chunk 
where n > 2. We have separated these phrases from 
the final list. 
Table 2: Inter-Annotator Agreement (in %) 
4.6 Experimental  Results 
We have used the standard IR matrices like Preci-
sion (P), Recall (R) and F-score (F) for evaluating 
the final results obtained from three modules. Hu-
man annotated list is used as the gold standard for 
the evaluation. The present system results are 
shown in Table 3.  These results are compared with 
the statistical baseline system described in (Cha-
kraborty, 2010). Our baseline system is reported 
with the precision of 39.64%. The predefined thre-
shold has been varied to catch individual results in 
each case. Increasing Recall in accordance with the 
increment of cut-off infers that the maximum 
numbers of MWEs are identified in a wide range 
of threshold. But the Precision does not increase 
considerably. It shows that the higher cut-off de-
grades the performance. The reasonable results for 
Precision and Recall have been achieved in case of 
cosine-similarity at the cut-off value of 0.5 where 
Euclidean distance and WordNet Similarity give 
maximum precision at cut-off values of 0.4 and 0.5 
respectively. In all cases, our system outperforms 
the baseline system.   
It is interesting to observe that English WordNet 
becomes a very helpful tool to identify Bengali 
MWEs. WordNet detects maximum MWEs cor-
rectly at the cut-off of 0.5. Baldwin et al, (2003) 
suggested that WordNet::Similarity measure is ef-
fective to identify empirical model of Multiword 
Expression Decomposability. This is also proved 
in this experiment as well and even for Bengali 
language. There are also candidates with very low 
value of similarity between their constituents (for 
example, ganer gajat (earth of song, affectionate 
of song), yet they are discarded from this experi-
ment because of their low frequency of occurrence 
in the corpus which could not give any judgment 
regarding collocation. Whether such an unexpec-
tedly low frequent high decomposable elements 
warrant an entry in the lexicon depends on the type 
of the lexicon being built. 
5 Conclusions 
We hypothesized that sense induction by analyzing 
synonymous sets can assist the identification of 
Multiword Expression. We have introduced an 
unsupervised approach to explore the hypothesis 
and have shown that clustering technique along 
with similarity measures can be successfully em-
ployed to perform the task. This experiment addi-
tionally contributes to the following scenarios - (i) 
Clustering of words having similar sense, (ii) Iden-
tification of MWEs for resource constraint lan-
guages and (iii) Reconstruction of Bengali 
monolingual dictionary towards the development 
of Bengali WordNet. However, in our future work, 
we will apply the present techniques for other type 
of MWEs (e.g., adjective-noun collocation, verbal 
MWEs) as well as for other languages.    
Acknowledgement 
The work reported in this paper is supported by a 
grant from the ?Indian Language to Indian Lan-
guage Machine Translation (IL-ILMT) System 
Phrase II?, funded by Department of Information 
and Technology (DIT), Govt. of India. 
References 
Agarwal, Aswini, Biswajit Ray, Monojit Choudhury, 
Sudeshna Sarkar and Anupam Basu. 2004. Automat-
ic Extraction of Multiword Expressions in Bengali: 
An Approach for Miserly Resource Scenario. In Pro-
ceedings of International Conference on Natural 
Language Processing (ICON), pp. 165-174. 
Baldwin, Timothy, Colin Bannard, Takaaki Tanaka and 
Dominic Widdows. 2003. An Empirical Model of 
Multiword Expression Decomposability. Proceed-
ings of the Association for Computational Linguis-
tics-2003, Workshop on Multiword Expressions: 
Analysis, Acquisition and Treatment, Sapporo, Japan, 
pp. 89?96. 
Ckakraborty, Tanmoy, 2010, Identification of Noun-
Noun (N-N) Collocations as Multi-Word Expressions 
in Bengali Corpus. Student Session, International 
Conference of Natural Language Processing (ICON), 
IIT Kharagpur, India 
MWEs 
[# 628] 
Agreement  between pair of annotators  
A1-A2      A2-A3    A1-A3        Avg 
KAPPA 87.23 86.14 88.78 87.38 
MASI 87.17 87.02 89.02 87.73 
12
Chakraborty, Tanmoy and Sivaji Bandyopadhyay. 2010. 
Identification of Reduplication in Bengali Corpus 
and their Semantic Analysis: A Rule Based Ap-
proach.  In proceedings of the Workshop on Multi-
word Expressions: from Theory to Applications 
(MWE 2010), 23rd International Conference on 
Computational Linguistics (COLING 2010), pp.73-
76, Beijing, China. 
Chattopadhyay Suniti K. 1992. Bhasa-Prakash Bangala 
Vyakaran, Third Edition.  
Church, Kenneth Wrad and Patrick Hans. 1990. Word 
Association Norms, Mutual Information and Lexico-
graphy.  Proceedings of 27th Association for Compu-
tational Linguistics (ACL), 16(1). pp. 22-29. 
Cohen, J. 1960. A coefficient of agreement for nominal 
scales. Educational and Psychological Measurement, 
vol. 20, pp. 37?46. 
Dunning, T. 1993. Accurate Method for the Statistic of 
Surprise and Coincidence. In Computational Linguis-
tics, pp. 61-74. 
Kilgarriff, Adam and Joseph Rosenzweig. 2000. 
Framework and results for English SENSEVAL. 
Computers and the Humanities. Senseval Special Is-
sue, 34(1-2). pp. 15-48. 
Korkontzelos,Ioannis and Suresh Manandhar. 2009. 
Detecting Compositionality in Multi-Word Expres-
sions. Proceedings of the Association for Computa-
tional Linguistics-IJCNLP, Singapore, pp. 65-68. 
Kunchukuttan F. A. and Om P. Damani. 2008. A Sys-
tem for Compound Noun Multiword Expression Ex-
traction for Hindi. Proceeding of 6th International 
Conference on Natural Language Processing 
(ICON). pp. 20-29. 
Passonneau, R.J. 2006. Measuring agreement on set-
valued items (MASI) for semantic and pragmatic an-
notation. Language Resources and Evaluation. 
Sag, Ivan A., Timothy Baldwin, Francis Bond, Ann 
Copestake and Dan Flickinger. 2002. Multiword Ex-
pressions: A Pain in the Neck for NLP. In Proceed-
ings of Conference on Intelligent Text Processing 
and Computational Linguistics (CICLING), pp. 1-15. 
Venkatapathy, Sriram and Aravind Joshi. 2005. Measur-
ing the relative compositionality of verb-noun (V-N) 
collocations by integrating features. Proceedings of 
Human Language Technology Conference and Con-
ference on Empirical Methods in Natural Language 
Processing (HLT/EMNLP), Association for Compu-
tational Linguistics. pp. 899 - 906.  
13
Proceedings of the ACL 2011 Workshop on Relational Models of Semantics (RELMS 2011), pages 19?27,
Portland, Oregon, USA, June 23, 2011. c?2011 Association for Computational Linguistics
Identifying Event ? Sentiment Association using Lexical Equivalence and 
Co-reference Approaches 
 
 
Anup Kumar Kolya1       Dipankar Das1      Asif Ekbal2      Sivaji Bandyopadhyay1 
1 Computer Science and Engineering Department, Jadavpur University, India  
2 Indian Institute of Technology, Patna (IITP), India 
anup.kolya@gmail.com, dipankar.dipnil2005@gmail.com 
asif.ekbal@gmail.com, sivaji_cse_ju@yahoo.com 
 
 
 
Abstract 
In this paper, we have identified event and sen-
timent expressions at word level from the sen-
tences of TempEval-2010 corpus and evaluated 
their association in terms of lexical equivalence 
and co-reference. A hybrid approach that con-
sists of Conditional Random Field (CRF) based 
machine learning framework in conjunction 
with several rule based strategies has been 
adopted for event identification within the 
TimeML framework. The strategies are based 
on semantic role labeling, WordNet relations 
and some handcrafted rules. The sentiment ex-
pressions are identified simply based on the 
cues that are available in the sentiment lexicons 
such as Subjectivity Wordlist, SentiWordNet 
and WordNet Affect. The identification of lexi-
cal equivalence between event and sentiment 
expressions based on the part-of-speech (POS) 
categories is straightforward. The emotional 
verbs from VerbNet have also been employed 
to improve the coverage of lexical equivalence. 
On the other hand, the association of sentiment 
and event has been analyzed using the notion of 
co-reference. The parsed dependency relations 
along with basic rhetoric knowledge help to 
identify the co-reference between event and 
sentiment expressions. Manual evaluation on 
the 171 sentences of TempEval-2010 dataset 
yields the precision, recall and F-Score values 
of 61.25%, 70.29% and 65.23% respectively.  
1 Introduction 
Event and Sentiment are two abstract entities 
closely coupled with each other from social, psy-
chological and commercial perspectives. Some 
kind of action that is going on or something that is 
being happened are addressed as events in general 
by the Natural Language (NL) researchers. The 
events are described in texts where the time, tem-
poral location and ordering of the events are speci-
fied. Event entities are represented by finite 
clauses, nonfinite clauses, nominalizations, event-
referring nouns, adjectives and even some kinds of 
adverbial clauses.  
On the other hand, text not only contains the in-
formative contents, but also some attitudinal pri-
vate information that includes sentiments. 
Nowadays, in the NLP communities, research ac-
tivities on sentiment analysis are in full swing. But, 
the identification of sentiment from texts is not an 
easy task as it is not open to any objective observa-
tion or verification (Quirk et al, 1985).  
Sometimes, similar or different types of senti-
ments are expressed on a single or multiple events. 
Sentiment of people over different events is impor-
tant as it has great influence on our society. Track-
ing users? sentiments about products or events or 
about political candidates as expressed in online 
forums, customer relationship management, stock 
market prediction, social networking etc., temporal 
question answering, document summarization, in-
formation retrieval systems are some of the impor-
tant applications of sentiment analysis.  
The identification of the association between 
event and sentiment is becoming more popular and 
interesting research challenge in the area of Natu-
ral Language Processing (NLP). Our present task is 
to identify the event and sentiment expressions 
from the text, analyze their associative relationship 
19
and investigate the insides of event-sentiment rela-
tions.  
For example, in the following sentence, the an-
notated events are, talked, sent and hijacked .But, 
it also shows the presence of underlying sentiments 
(as shown in underlined script) inscribed in the 
sentence. Here, sentiment helps to evoke the event 
property at lexical entity level (e.g. negative (-ve) 
sentiment for only the event word hijacked) as well 
as at context level (e.g. positive (+ve) sentiment 
associated with the event hijacked as the event 
word appears with the evaluative expression, re-
cover that gives the +ve polarity).  
 
?The prime minister of India told Friday that he 
has talked with top commander of Indian military 
force and sent a team to recover the host of Taj 
Hotel hijacked.?  
 
 Hence, we have organized the entire task into 
three different steps i) event identification, ii) sen-
timent expression identification and iii) identifica-
tion of event sentiment relationships at context 
level using lexical equivalence and co-reference 
approaches.  
In the first step, we propose a hybrid approach 
for event extraction from the text under the Tem-
pEval-2010 framework. Initially, we have used a 
Conditional Random Field (CRF) (Lafferty et al, 
2001) machine learning framework but we observe 
that it often makes the errors in extracting the 
events denoted by deverbial entities. This observa-
tion prompts us to employ several strategies in 
conjunction with machine learning. These strate-
gies are implemented based on semantic role labe-
ling, WordNet (Miller, 1990) and some 
handcrafted rules. We have experimented with the 
TempEval-2010 evaluation challenge setup (Kolya 
et al, 2010).  Evaluation results yield the preci-
sion, recall and F-measure values of approximate-
ly 93.00%, 96.00% and 94.47% respectively. This 
is approximately 12% higher F-measure in com-
parison to the best system (Llorens et al, 2010) of 
TempEval-2010. 
    On the other hand, the identification of the sen-
timent expressions is carried out based on the sen-
timent word. The words are searched in three 
different sentiment lexicons, the Subjectivity Word 
lists (Banea et al, 2008), SentiWordNet (Baccia-
nella et al, 2010) and WordNet Affect (Strapparava 
and Valitutti, 2004). The coarse-grained (positive 
and negative) as well as Ekman?s (1993) six fine- 
grained sentiment or emotion expressions (happy, 
sadness, anger, disgust, fear and surprise) are 
tagged in the corpus. As there is no annotation in 
the TemEval-2010 corpus for sentiment expres-
sions, the evaluation has been carried out by the 
authors and it achieves the precision, recall and F-
measure values of approximately 73.54%, 86.04% 
and 79.30% respectively 
Determining the lexical equivalence of event 
and sentiment expressions based on the POS prop-
erty at the lexical entity level is straightforward. If 
an event word also expresses the sentiment word, 
we have associated the corresponding sentiment 
type with the event word directly. In addition to the 
sentiment lexicons, the emotional verbs extracted 
from the VerbNet (Kipper-Schuler, 2005) are used 
in this phase. It improves the coverage of lexical 
equivalence by 12.76%. 
But, if the event and sentiment expressions oc-
cupy separate text spans in a sentence, we have 
adopted a co-reference approach for identifying 
their association. The parsed dependency relations 
along with some basic rhetoric components, such 
as nucleus, satellite and locus help in identifying 
the co-reference between the event and sentiment 
expressions. The text span containing sentiment 
word is hypothesized as the locus, the main effec-
tive part of the nucleus or satellite. The text span 
that reflects the primary goal of the writer is 
termed as nucleus (marked as ?{ }?) whereas the 
span that provides supplementary material is 
termed as satellite (marked as ?[ ]?). The distin-
guished identification of nucleus and satellite as 
well as their separation from each other is carried 
out based on the direct and transitive dependency 
relations, causal verbs, relaters or discourse mark-
ers. If both the locus and event are identified to-
gether in either nucleus or satellite, we term their 
association as co-referenced. If they occur sepa-
rately in nucleus and satellite and share at least one 
direct dependency relation, we consider their asso-
ciation as co-referenced.  
The evaluation of the lexical equivalence as 
well as co-reference systems has been performed 
by the authors. Primarily, the evaluation of both 
systems has been conducted on the random sam-
ples of 200 sentences of the TempEval-2010 train-
ing dataset.  Finally, the co-reference system 
achieves the precision, recall and F-Scores of 
20
61.25%, 70.29% and 65.23% respectively on 171 
sentences of the TempEval-2010 test corpus.  
The rest of the paper is organized as follows. 
Section 2 describes the related work. The event 
identification is discussed in Section 3. The identi-
fication of sentiment expressions is described in 
Section 4. Determination of lexical equivalence 
between event and sentiment expressions is speci-
fied in Section 5. The co-reference approach for 
identifying the association between event and sen-
timent is described in Section 6. Finally Section 7 
concludes the paper. 
2 Related Work 
The existing works on event extraction are based 
either on pattern-matching rules (Mani and Wilson 
2000), or on the machine learning approach (Bo-
guraev and Ando, 2005). But, still the problems 
persist with the high complexities involved in the 
proper extractions of events. The events expres-
sions were annotated in the TempEval 2007 
source in accordance with the TimeML standard 
(Pustejovsky et al, 2003). On the other hand, the 
Task B of TempEval-2010 evaluation challenge 
setup (Verhagen et al, 2010) was aimed at identi-
fying events from text. The best achieved result 
was obtained by (Llorens et al, 2010). 
The majority of subjective analysis methods 
that are related to emotion is based on textual key-
words spotting that use specific lexical resources. 
A lexicon that provides appraisal attributes for 
terms was constructed and the features were used 
for emotion classification (Whitelaw et al, 2005). 
The features along with the bag-of-words model 
give 90.2% accuracy. UPAR7 (Chaumartin, 2007), 
a rule-based system uses a combination of Word-
Net Affect and SentiWordNet. The system was 
semi-automatically enriched with the original trial 
data provided during the SemEval task (Strappara-
va and Mihalcea, 2007). SWAT (Katz et al, 2007) 
is another supervised system that uses a unigram 
model trained to annotate emotional content. 
Our motivation is that though events and senti-
ments are closely coupled with each other from 
social, psychological and commercial perspectives, 
very little attention has been given about their de-
tection and analysis. To the best of our knowledge, 
only a few tasks have been attempted (Fukuhara et 
al., 2007) (Das et al, 2010).  
Sometimes, the opinion topics are not neces-
sarily spatially coherent as there may be two opi-
nions in the same sentence on different topics, as 
well as opinions that are on the same topic sepa-
rated by opinions that do not share that topic 
(Stoyanov and Cardie 2008). The authors have es-
tablished their hypothesis by applying the co-
reference technique. Similarly, we have adopted 
the co-reference technique based on basic rhetoric 
components for identifying the association be-
tween event and sentiment expressions.  In addi-
tion to that, we have also employed the lexical 
equivalence approach for identifying their associa-
tion.  
3 Event Identification 
In this work, we propose a hybrid approach for 
event identification from the text under the Tem-
pEval-2010 framework. We use Conditional Ran-
dom Field (CRF) as the underlying machine 
learning algorithm. We observe that this machine 
learning based system often makes the errors in 
identifying the events denoted by deverbial enti-
ties. This observation prompts us to employ several 
strategies in conjunction with machine learning 
techniques. These strategies have been imple-
mented based on semantic role labeling, WordNet 
senses and some handcrafted rules.  
We have experiment with the TempEval-2010 
evaluation challenge setup (Kolya et al, 2010).  
Evaluation results yield the precision, recall and F-
measure values of approximately 93.00%, 96.00% 
and 94.47% respectively. This is approximately 
12% higher F-measure in comparison to the best 
system (Llorens et al, 2010) of TempEval-2010. 
3.1 CRF based Approach for Event Identifi-
cation 
We extract the gold-standard TimeBank features 
for events in order to train/test the CRF model. In 
the present work, we mainly use the various com-
binations of the following features:  
Part of Speech (POS) of event terms (e.g. Ad-
jective, Noun and Verb), Tense (Present, Past, Fu-
ture, Infinitive, Present part, Past part, or NONE), 
Aspect (Progressive, Perfective and Perfective 
Progressive or NONE), Class (Reporting, Percep-
tion, Aspectual, I_action, I_state, State, Occur-
rence), Stem (e.g., discount /s/).  
21
3.2 Use of Semantic Roles for Event Identifi-
cation 
We use an open source Semantic Role Labeler 
1(SRL) (Gildea et al, 2002) (Pradhan et al, 2004) 
to identify different features of the sentences. For 
each predicate in a sentence acting as event word, 
semantic roles extract all constituents, determining 
their arguments (agent, patient etc.) and adjuncts 
(locative, temporal etc.). Semantic roles can be 
used to detect the events that are the nominaliza-
tions of verbs such as agreement for agree or con-
struction for construct. Nominalizations (or, 
deverbal nouns) are commonly defined as nouns 
that are morphologically derived from verbs, 
usually by suffixation (Quirk et al, 1985). Event 
nominalizations often afford the same semantic 
roles as verbs and often replace them in written 
language (Gurevich et al, 2006).  Event nominali-
zations constitute the bulk of deverbal nouns.  The 
following example sentence shows how semantic 
roles can be used for event identification.  
 
[ARG1 All sites] were [TARGET inspected] to the satis-
faction of the inspection team and with full coope-
ration of Iraqi authorities, [ARG0 Dacey] [TARGET 
said]. 
 
   The extracted target words are treated as the 
event words. It has been observed that many of 
these target words are identified as the event ex-
pressions by the CRF model. But, there exists ma-
ny nominalised event expressions (i.e., deverbal 
nouns) that are not identified as events by the su-
pervised CRF. These nominalised expressions are 
correctly identified as events by SRL.  
3.3 Use of WordNet for Event Identification 
WordNet is mainly used to identify non-deverbal 
event nouns. We observed that the event entities 
like ?war?, ?attempt?, ?tour? are not properly identi-
fied. These words have noun (NN) POS informa-
tion as the previous approaches, i.e., CRF and SRL 
can only identify those event words that have verb 
(VB) POS information. We know from the lexical 
information of WordNet that the words like ?war? 
and ?tour? are generally used as both noun and 
verb forms in the sentence. Therefore, we have 
                                                        
1 http://cemantix.org/assert.html 
designed the following two rules based on the 
WordNet: 
 
Rule 1: The word tokens having Noun (NN) POS 
categories are looked into the WordNet. If it ap-
pears in the WordNet with noun and verb senses, 
then that word token is considered as an event.  For 
example, war has both noun and verb senses in the 
WordNet, and hence war is considered as an event.  
 
Rule 2: The stems of the noun word tokens are 
looked into the WordNet. If one of the WordNet 
senses is verb then the token is considered as verb. 
For example, the stem of proposal, i.e., propose 
has two different senses, noun and verb in the 
WordNet, and thus it is considered as an event.  
3.4    Use of Rules for Event Identification 
Here, we mainly concentrate on the identification 
of specific lexical classes like ?inspection? and 
?resignation?. These can be identified by the suf-
fixes such as (?-ci?n?), (?-tion?) or (?-ion?), i.e., the 
morphological markers of deverbal derivations. 
  Initially, we have employed the CRF based Stan-
ford Named Entity (NE) tagger2 on the TempEval-
2 test dataset. The output of the system is tagged 
with Person, Location, Organization and Other 
classes. The words starting with the capital letters 
are also considered as NEs. Thereafter, we came 
up with the following rules for event identification: 
  
Cue-1: The deverbal nouns are usually identified 
by the suffixes like ?-tion?, ?-ion?, ?-ing? and ?-ed? 
etc. The nouns that are not NEs, but end with these 
suffixes are considered as the event words. 
  
Cue 2: The verb-noun combinations are searched 
in the sentences of the test set. The non-NE noun 
word tokens are considered as the events.  
 
Cue 3: Nominals and non-deverbal event nouns 
can be identified by the complements of aspectual 
PPs headed by prepositions like during, after and 
before, and complex prepositions such as at the 
end of and at the beginning of etc.  The next word 
token(s) appearing after these clue word(s) or 
phrase(s) are considered as events.  
                                                        
2 http://nlp.stanford.edu/software/CRF-NER.shtml 
22
Cue 4: The non-NE nouns occurring after the ex-
pressions such as frequency of, occurrence of and 
period of are most probably the event nouns. 
 
Cue 5: Event nouns can also appear as objects of 
aspectual and time-related verbs, such as have be-
gun a campaign or have carried out a campaign 
etc. The non-NEs that appear after the expressions 
like ?have begun a?, ?have carried out a? etc.  are 
also denoted as the events.   
4 Sentiment Expression Identification 
Sentiment is an important cue that effectively de-
scribes the events associated with it. The binary 
classification of the sentiments (positive and nega-
tive) as well as the fine-grained categorization into 
Ekman?s (1993) six emotions is therefore em-
ployed for identifying the sentiment expressions. 
200 sentences are randomly selected from the 
training dataset of the TempEval-2010 corpus. 
These sentences have been considered as our de-
velopment set. On the other hand, 171 sentences 
were already provided as the test sentences in the 
TempEval-2010 evaluation challenge.   
The events are already annotated in the Tem-
pEval-2010 corpus. But, no sentiment or emotion 
related annotation is available in the corpus. 
Hence, we have annotated the sentiment expres-
sions at word level in a semi-supervised way. The 
word level entities are tagged by their coarse and 
fine grained sentiment tags using the available sen-
timent related lexical resources. Then the automat-
ic annotation has been evaluated manually by the 
authors. The semi-supervised sentiment annotation 
agreements were 90.23% for the development set 
and 92.45% for the test sets respectively.  
4.1 Lexicon based Approach 
The tagging of the evaluative expressions or more 
specifically the sentiment expressions on the Tem-
pEval-2010 corpus has been carried out using the 
available sentiment lexicons. We passed the sen-
tences through three sentiment lexicons, Subjectivi-
ty Wordlists (Banea et al, 2008), SentiWordNet 
(Baccianella et al, 2010) and WordNet Affect 
(Strapparava and Valitutti, 2004). Subjectivity 
Wordlist assigns words with the strong or weak 
subjectivity and prior polarities of types positive, 
negative and neutral. SentiWordNet, used in opi-
nion mining and sentiment analysis, assigns three 
sentiment scores such as positive, negative and 
objective to each synset of WordNet. WordNet Af-
fect, a small well-used lexical resource but valua-
ble for its affective annotation contains the words 
that convey emotion.  
The algorithm is that, if a word in a sentence is 
present in any of these resources; the word is 
tagged as the sentiment expression. But, if any 
word is not found in any of them, each word of the 
sentence is passed through the WordNet Morpho-
logical analyzer (Miller, 1990) to identify its root 
form and the root form is searched through the re-
sources again. If the root form is found, the corres-
ponding word is tagged as sentiment expression 
accordingly.  
The identified sentiment expressions have been 
evaluated by the authors and it achieves the preci-
sion, recall and F-Score of 73.54%, 86.04% and 
79.30%, respectively on a total of 171 test sen-
tences of the TempEval-2010 corpus.   
The identification of event words that also ex-
press sentiment is straightforward. But, the prob-
lem arises when the event and sentiment 
expressions are present separately in a sentence 
and the sentiment is either closely associated with 
the event or affects it. In case of the former, we 
have adopted the approach of lexical equivalence 
between the event and sentiment entities whereas 
the co-reference technique has been introduced for 
resolving the latter case.  
5 Lexical Equivalence between Event and 
Sentiment Expressions  
It is observed that in general the verbs, nouns and 
adjectives represent events. The sentences are 
passed through an open source Stanford Maximum 
Entropy based POS tagger (Manning and Toutano-
va, 2000). The best reported accuracy for the POS 
tagger on the Penn Treebank is 96.86% overall and 
86.91% on previously unseen words. Our objective 
was to identify the event words that also express 
sentiments. Hence, we have identified the event 
words that have also been tagged as the sentiment 
expressions. The coverage of these lexical re-
sources in identifying the event sentiment associa-
tion is shown in Table 1. 
On the other hand, not only the adjectives or 
nouns, the sentiment or emotional verbs play an 
important role in identifying the sentiment expres-
23
sions. Hence, in addition to the above mentioned 
sentiment resources, we have also incorporated 
English VerbNet (Kipper-Schuler, 2005) for the 
automatic annotation process. VerbNet associates 
the semantics of a verb with its syntactic frames 
and combines traditional lexical semantic informa-
tion such as thematic roles and semantic predi-
cates, with syntactic frames and selectional 
restrictions. Verb entries in the same VerbNet class 
share common syntactic frames and thus they are 
believed to have the same syntactic behavior. For 
example, the emotional verbs ?love? and ?enjoy? 
are members of the admire-31.2-1 class and ?en-
joy? also belongs to the class want-32.1-1.  
The XML files of VerbNet are preprocessed to 
build up a general list that contains all member 
verbs and their available syntax information re-
trieved from VerbNet. The main criterion for se-
lecting the member verbs as sentiment expressions 
is the presence of ?emotional_state? type predicate 
in their frame semantics. The frequencies of the 
event words matched against the above said four 
resources are shown in Table 1.  It has been ob-
served that the adjective events are not identified 
by the lexical resources as their frequency in the 
test corpus was very low. But, the lexical coverage 
has been improved by 12.76% by incorporating 
VerbNet. 
 
Resources Noun   Adjective  Verb 
#114    #4              #380 
Subjectivity Wordlists 
SentiWordNet 
WordNet Affect List 
VerbNet (emotional 
verbs) 
24            --             35 
32            --             59  
12            --             25 
 --            --             79 
Accuracy (in %) 59.64                    52.57 
 
Table 1: Results of Lexical Equivalence between 
Event and Sentiment based on different resources  
6 Co-reference between Event and Senti-
ment Expressions  
The opinion and/or sentiment topics are not neces-
sarily spatially coherent as there may be two opi-
nions in the same sentence on different topics. 
Sometimes, the opinions that are on the same topic 
are separated by opinions that do not share that 
topic (Stoyanov and Cardie, 2008). We observe the 
similar situation in case of associating sentiments 
with events. Hence, the hypothesis for opinion top-
ic is established for sentiment events by applying 
the co-reference technique along with the rhetori-
cal structure. We have proposed two different sys-
tems for identifying the association of sentiments 
with the events at context level. 
6.1 Baseline Co-reference System 
The baseline system has been developed based on 
the object information present in the dependency 
relations of the parsed sentences. Stanford Parser 
(Marneffe et al, 2006), a probabilistic lexicalized 
parser containing 45 different part of speech (POS) 
tags of Pen Treebank tagset  has been used to get 
the parsed sentences and dependency relations. 
The dependency relations are checked for the pre-
dicates ?dobj? so that the related components 
present in the predicate are considered as the prob-
able candidates for the events.  
If a dependency relation contains both the event 
and sentiment words, we have considered the pres-
ence of co-reference between them. But, it has 
been observed that the event and sentiment expres-
sions are also present in two different relations that 
share a common word element. Hence, if the event 
and sentiment words appear in two different rela-
tions but both of the relations contain at least one 
common element, the event and sentiment words 
are termed as co-referenced.    
Overall, the baseline co-reference system 
achieves the precision, recall and F-Scores of 
40.03%, 46.10% and 42.33% for event-sentiment 
co-reference identification. For example in the fol-
lowing sentence, the writer?s direct as well as indi-
rect emotional intentions are reflected by 
mentioning one or more topics or events (spent, 
thought) and their associated sentiments (great).  
 
?When Wong Kwan spent seventy million dol-
lars for this house, he thought it was a great deal.? 
 
The baseline co-reference system fails to asso-
ciate the sentiment expressions with their corres-
ponding event expressions. Hence, we aimed for 
the rhetoric structure based co-reference system to 
identify their association. 
6.2  Rhetoric Co-reference System 
The distribution of events and sentiment expres-
sions in different text spans of a sentence needs the 
24
analysis of sentential structure. We have incorpo-
rated the knowledge of Rhetorical Structure 
Theory (RST) (Mann and Thompson 1987) for 
identifying the events that are co-referred by their 
corresponding sentiment expressions.  
The theory maintains that consecutive discourse 
elements, termed text spans, are related by a rela-
tively small set (20?25) of rhetorical relations. 
But, instead of identifying the rhetorical relations, 
the present task acquires the basic and coarse rhe-
torical components such as locus, nucleus and sa-
tellite from a sentence.  These rhetoric clues help 
in identifying the individual event span associated 
with the span denoting the corresponding senti-
ment expression in a sentence. The text span that 
reflects the primary goal of the writer is termed as 
nucleus (marked as ?{ }?) whereas the span that 
provides supplementary material is termed as satel-
lite (marked as ?[ ]?). For example, the nucleus and 
satellite textual spans are shown in the following 
sentence as, 
 
{Traders said the market remains extremely 
nervous} because [the wild swings seen on the 
New York Stock Exchange last week]. 
 
The event or topic of an opinion or sentiment 
depends on the context in which the associated 
opinion or sentiment expression occurs (Stoyanov 
and Cardie 2008). Considering the similar hypo-
thesis in case of events instead of topics, the co-
reference between an event and a sentiment ex-
pression is identified from the nucleus and/or satel-
lite by positioning the sentiment expression as 
locus. We have also incorporated the WordNet?s 
(Miller 1990) morphological analyzer to identify 
the stemmed forms of the sentiment words.  
The preliminary separation of nucleus from sa-
tellite was carried out based on the list of frequent-
ly used causal keywords (e.g., as, because, that, 
while, whether etc) and punctuation markers (,) (!) 
(?).The discourse markers and causal verbs are 
also the useful clues if they are explicitly specified 
in the text. The identification of discourse markers 
from written text itself is a research area (Azar 
1999). Hence, our task was restricted to identify 
only the explicit discourse markers that are tagged 
by conjunctive_() or mark_() type dependency re-
lations of the parsed constituents. The dependency 
relations containing conjunctive markers (e.g., 
conj_and(), conj_or(), conj_but()) were considered 
for separating nucleus from satellite if the markers 
are present in between two successive clauses. 
Otherwise, the word token contained in the 
mark_() type dependency relation was considered 
as a discourse marker. 
The list of causal verbs is prepared by 
processing the XML files of VerbNet. If any Verb-
Net class file contains any frame with semantic 
type as Cause, we collect the member verbs of that 
XML class file and term the member verbs as 
causal verbs. We used a list that contains a total 
number of 253 causal verbs.  
If any clause tagged as S or SBAR in the parse 
tree contains any causal verb, that clause is consi-
dered as the nucleus and the rest of the clauses de-
note the satellites. Considering the basic theory of 
rhetorical structure (Mann and Thompson 1987), 
the clauses were separated into nucleus and satel-
lite to identify the event and sentiment expressions. 
The direct dependency is identified based on the 
simultaneous presence of locus and the event word 
in the same dependency relation whereas the tran-
sitive dependency is verified if the word is con-
nected to locus and event via one or more 
intermediate dependency relations.  
If the event and sentiment words are together 
present in either nucleus or satellite, the associa-
tion between the two expressions is considered as 
co-referenced. If they occur in nucleus and satellite 
separately, but the event and sentiment words are 
present in at least one direct dependency relation, 
the expressions are termed as co-referenced.  
In the previous example, the event expressions, 
?said? and ?remains? are associated with the sen-
timent expression ?nervous? as both the event ex-
pressions share the direct dependency relations 
?cop(nervous-7, remains-5)? and ?ccomp(said-2, 
nervous-7)? in the nucleus segment. Similarly, the 
event word, ?seen? and sentiment word ?wild? are 
present in the satellite part and they share a direct 
dependency relation ?partmod(swings-12, seen-
13)?. But, no direct dependency relation is present 
between the ?nervous? and ?seen? or ?said? and 
?wild? or ?remains? and ?wild?.  
6.3 Results 
Though the event annotation is specified in the 
TempEval-2010 corpus, the association between 
the event and sentiment expressions was not speci-
fied in the corpus. Hence, we have carried out the 
25
evaluation manually. The 200 random samples of 
the training set that were used in sentiment expres-
sion identification task have been considered as 
our development set. The Evaluation Vectors 
(EvalV) are prepared manually from each sentence 
of the development and test sets. The vectors 
<EvExp, SentiExp> are filled with the annotated 
events and sentiment expressions by considering 
their association. The annotation of sentiment ex-
pressions using the semi-supervised process has 
been described in Section 4. 
    The rule based baseline and rhetoric based co-
reference systems identify the event and sentiment 
expressions from each sentence and stores them in 
a Co-reference Vector (CorefV). The evaluation is 
carried out by comparing the system generated Co-
reference Vectors (CorefV) with their correspond-
ing Evaluation Vectors (EvalV). The evaluation 
results on 171 test sentences are shown in Table 2. 
 
Co-reference  
Approaches 
Prec.     Rec.    F-Score 
(in %) 
Baseline System 40.03    46.10       42.33 
Rhetoric System 61.25    70.29       65.23 
 
Table 2: Precision (Prec.), Recall (Rec.) and F-
Scores (in %) of the event-sentiment co-reference 
systems  
 
Overall, the precision, recall and F-Scores are 
61.25%, 70.29% and 65.23% for event-sentiment 
co-reference identification using rhetoric clues. 
Though the co-reference technique performs satis-
factorily for identifying the event-sentiment co-
reference, the problem arises in distinguishing the 
corresponding spans of events from an overlapped 
text span of multi-word tokens.  
7 Conclusion  
In this present work, we have identified event and 
sentiment expressions at word level from the sen-
tences of TempEval-2010 corpus and evaluated 
their association in terms of lexical equivalence 
and co-reference. It has been observed that the lex-
ical equivalence based on lexicons performs satis-
factorily but overall, the co-reference entails that 
the presence of indirect affective clues can also be 
traced with the help of rhetoric knowledge and de-
pendency relations. The association of the senti-
ments with their corresponding events can be used 
in future concerning the time based sentiment 
change over events.  
Acknowledgments 
The work is supported by a grant from the India-
Japan Cooperative Programme (DST-JST) 2009 
Research project entitled ?Sentiment Analysis 
where AI meets Psychology? funded by Depart-
ment of Science and Technology (DST), Govern-
ment of India. 
References  
Baccianella Stefano, Esuli Andrea and Sebas-tiani Fa-
brizio. 2010. SentiWordNet 3.0: An Enhanced Lexi-
cal Re-source for Sentiment Analysis and Opinion 
Mining. In Proceedings of the 7th Conference on 
Language Resources and Evaluation, pp. 2200-2204. 
Banea, Carmen, Mihalcea Rada, Wiebe Janyce. 2008.  
A Bootstrapping Method for Building Subjectivity 
Lexicons for Languages with Scarce Resources. The 
Sixth International Conference on Language Re-
sources and Evaluation. 
Boguraev, B., Ando, R. K. 2005. TimeBank-
DrivenTimeML Analysis. Annotating, Extracting and 
Reasoning about Time and Events 2005. 
Chaumartin, F. 2007. Upar7: A knowledge-based sys-
tem for headline sentiment tagging. SemEval-200,  
Czech Republic. 
Ekman Paul. 1993. An argument for basic emotions, 
Cognition and Emotion, 6(3-4):169-200. 
Fukuhara T., Nakagawa, H. and Nishida, T. 2007. Un-
derstanding Sentiment of People from News Articles: 
Temporal Sentiment Analysis of Social Events. 
ICWSM?2007, Boulder, Colorado. 
Gildea, D. and Jurafsky, D. 2002. Automatic Labeling 
of Semantic Roles. Computational Linguistics, 
28(3):245?288. 
Gurevich, O., R. Crouch, T. King, and V. de Paiva. 
2006. Deverbal Nouns in Knowledge Representation. 
Proceedings of FLAIRS, pages 670?675, Melbourne 
Beach, FL. 
Katz, P., Singleton, M. and Wicentowski, R. 2007. 
Swat-mp: the semeval-2007 systems for task 5 and 
task SemEval-2007.  
Kipper-Schuler, K. 2005.  VerbNet: A broad-coverage, 
comprehensive verb lexicon. Ph.D. thesis, Computer 
and Information Science Dept., University of Penn-
sylvania, Philadelphia, PA. 
26
Kolya, A., Ekbal, A. and Bandyopadhyay, S. 2010. 
JU_CSE_TEMP: A First Step towards Evaluating 
Events, Time Expressions and Temporal Relations. 
In Proceedings of the 5th International Workshop on 
Semantic Evaluation, ACL 2010, July 15-16, Swe-
den, pp. 345?350. 
Lafferty, J., McCallum, A.K., Pereira, F. 2001. Condi-
tional Random Fields: Probabilistic Models for Seg-
menting and Labeling Sequence Data. International 
Conference on Machine Learning. 
Llorens Hector, Estela Saquete, Borja Navarro. 2010. 
TIPSem (English and Spanish): Evaluating CRFs and 
Semantic Roles. Proceedings of the 5th International 
Workshop on Semantic Evaluation, ACL 2010, pages 
284?291, Uppsala, Sweden, 15-16 July 2010. 
Mani, I., and Wilson G. 2000. Processing of News. In 
Proceedings of the 38th Annual Meeting of the Asso-
ciation for Computational Linguistics, pp. 69-76. 
Mann, W. and S. Thompson. 1987. Rhetorical Structure 
Theory: Description and Construction of Text Struc-
ture. In G. Kempen (ed.), Natural Language Genera-
tion, Martinus Nijhoff, The Hague, pp. 85?96. 
Manning Christopher and Toutanova, Kristina. 2000. 
Enriching the Knowledge Sources Used in a Maxi-
mum Entropy Part-of-Speech Tagger. Proceedings of 
the Joint SIGDAT Conference on Empirical Methods 
in Natural Language Processing and Very Large 
Corpora (EMNLP/VLC)  
Marneffe, Marie-Catherine de, Bill MacCartney, and 
Christopher D.Manning. 2006. Generating Typed 
Dependency Parses from Phrase Structure Parses. 5th 
International Conference on Language Resources 
and Evaluation.  
Miller George A. 1990. WordNet: An on-line lexical 
database. International Journal of Lexicography, 
3(4): 235?312 
Pradhan S., Wayne W., Hacioglu, K., Martin, J.H. and 
Jurafsky, D. 2004. Shallow Semantic Parsing using 
Support Vector Machines. Proceedings of the Human 
Language Technology Conference/North American 
chapter of the Association for Computational Lin-
guistics annual meeting Boston, MA, May 2-7. 
Pustejovsky, J., Castano, J., Ingria, R., Sauri, R., Gai-
zauskas, R., Setzer, A., Katz, G. and Radev, D. 
TimeML: Robust specification of event and temporal 
expressions in text. In AAAI Spring Symposium on 
New Directions in Question-Answering, pp. 28-34, 
CA, 2003. 
Quirk, R., Greenbaum, S. Leech, G. and Svartvik, J. 
1985. A Comprehensive Grammar of the English 
Language. Longman.  
Strapparava C. and Valitutti, A. 2004. Wordnet-affect: 
an affective extension of wordnet. In 4th Internation-
al Conference on Language Resources and Evalua-
tion, pp. 1083-1086. 
Strapparava Carlo and Mihalcea Rada. 2007. SemEval-
2007 Task 14: Affective Text. 45th Aunual Meeting 
of Association for Computational linguistics. 
Stoyanov, V., and Cardie, C. 2008. Annotating topics of 
opinions. In Proceedings of LREC.  
 
27
Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, ACL-HLT 2011, pages 80?86,
24 June, 2011, Portland, Oregon, USA c?2011 Association for Computational Linguistics
Developing Japanese WordNet Affect for Analyzing Emotions 
 
 
Yoshimitsu Torii1       Dipankar Das2       Sivaji Bandyopadhyay2      Manabu Okumura1 
1Precision and Intelligence Laboratory, Tokyo Institute of Technology, Japan 
2Computer Science and Engineering Department, Jadavpur University, India 
torii@lr.pi.titech.ac.jp, dipankar.dipnil2005@gmail.com 
sivaji_cse_ju@yahoo.com, oku@pi.titech.ac.jp 
 
Abstract 
This paper reports the development of Jap-
anese WordNet Affect from the English 
WordNet Affect lists with the help of Eng-
lish SentiWordNet and Japanese WordNet. 
Expanding the available synsets of the 
English WordNet Affect using SentiWord-
Net, we have performed the translation of 
the expanded lists into Japanese based on 
the synsetIDs in the Japanese WordNet. A 
baseline system for emotion analysis of 
Japanese sentences has been developed 
based on the Japanese WordNet Affect. The 
incorporation of morphology improves the 
performance of the system. Overall, the 
system achieves average precision, recall 
and F-scores of 32.76%, 53% and 40.49% 
respectively on 89 sentences of the Japa-
nese judgment corpus and 83.52%, 49.58% 
and 62.22% on 1000 translated Japanese 
sentences of the SemEval 2007 affect sens-
ing test corpus. Different experimental out-
comes and morphological analysis suggest 
that irrespective of the google translation 
error, the performance of the system could 
be improved by enhancing the Japanese 
WordNet Affect in terms of coverage.  
1 Introduction 
Emotion analysis, a recent sub discipline at the 
crossroads of information retrieval (Sood et al, 
2009) and computational linguistics (Wiebe et al, 
2006) is becoming increasingly important from 
application view points of affective computing.  
The majority of subjective analysis methods that 
are related to emotion is based on textual keywords 
spotting that use specific lexical resources. Senti-
WordNet (Baccianella et al, 2010) is a lexical re-
source that assigns positive, negative and objective 
scores to each WordNet synset (Miller, 1995). Sub-
jectivity wordlist (Banea et al, 2008) assigns 
words with the strong or weak subjectivity and 
prior polarities of types positive, negative and neu-
tral.  Affective lexicon (Strapparava and Valitutti, 
2004), one of the most efficient resources of emo-
tion analysis, contains emotion words. To the best 
of our knowledge, these lexical resources have 
been created for English. A recent study shows that 
non-native English speakers support the growing 
use of the Internet1. Hence, there is a demand for 
automatic text analysis tools and linguistic re-
sources for languages other than English.  
In the present task, we have prepared the Japa-
nese WordNet Affect from the already available 
English WordNet Affect (Strapparava and Valitutti, 
2004). Entries in the English WordNet Affect are 
annotated using Ekman?s (1993) six emotional 
categories (joy, fear, anger, sadness, disgust, sur-
prise). The collection of the English WordNet Af-
fect 2 synsets that are used in the present work was 
provided as a resource in the ?Affective Text? 
shared task of SemEval-2007 Workshop.  
The six WordNet Affect lists that were provided 
in the shared task contain only 612 synsets in total 
with 1536 words. The words in each of the six 
emotion lists have been observed to be not more 
than 37.2% of the words present in the correspond-
ing SentiWordNet synsets. Hence, these six lists 
are expanded with the synsets retrieved from the 
                                                        
1 http://www.internetworldstats.com/stats.htm 
2 http://www.cse.unt.edu/~rada/affectivetext/ 
80
English SentiWordNet (Baccianella et al, 2010). 
We assumed that the new sentiment bearing words 
in English SentiWordNet might have some emo-
tional connotation in Japanese even keeping their 
part-of-speech (POS) information unchanged. The 
numbers of entries in the expanded word lists are 
increased by 69.77% and 74.60% at synset and 
word levels respectively. We have mapped the 
synsetID of the WordNet Affect lists with the syn-
setID of the WordNet 3.03. This mapping helps in 
expanding the WordNet Affect lists with the recent 
version of SentiWordNet 3.0 4 as well as translating 
with the Japanese WordNet (Bond et al, 2009). 
Some affect synsets (e.g., 00115193-a huffy, mad, 
sore) are not translated into Japanese as there are 
no equivalent synset in the Japanese WordNet.  
Primarily, we have developed a baseline system 
based on the Japanese WordNet Affect and carried 
out the evaluation on a Japanese judgement corpus 
of 89 sentences. The system achieves the average 
F-score of 36.39% with respect to six emotion 
classes. We have also incorporated an open source 
Japanese morphological analyser 5 . The perform-
ance of the system has been increased by 4.1% in 
average F-score with respect to six emotion classes. 
Scarcity of emotion corpus in Japanese moti-
vated us to apply an open source google translator6 
to build the Japanese emotion corpus from the 
available English SemEval-2007 affect sensing 
corpus. The baseline system based on the Japanese 
WordNet Affect achieves average precision, recall 
and F-score of 83.52%, 49.58% and 62.22% re-
spectively on 1000 translated test sentences. The 
inclusion of morphological processing improves 
the performance of the system. Different experi-
ments have been carried out by selecting different 
ranges of annotated emotion scores. Error analysis 
suggests that though the system performs satisfac-
torily in identifying the sentential emotions based 
on the available words of the Japanese WordNet 
Affect, the system suffers from the translated ver-
sion of the corpus. In addition to that, the Japanese 
WordNet Affect also needs an improvement in 
terms of coverage.  
 The rest of the paper is organized as follows. 
Different developmental phases of the Japanese 
WordNet Affect are described in Section 3. Prepa-
                                                        
3 http://wordnet.princeton.edu/wordnet/download/ 
4 http://sentiwordnet.isti.cnr.it/ 
5 http://mecab.sourceforge.net/ 
6 http://translate.google.com/# 
ration of the translated Japanese corpus, different 
experiments and evaluations based on morphology 
and the annotated emotion scores are elaborated in 
Section 4. Finally Section 5 concludes the paper. 
2 Related Works  
The extraction and annotation of subjective terms 
started with machine learning approaches (Hat-
zivassiloglou and McKeown, 1997). Some well 
known sentiment lexicons have been developed, 
such as subjective adjective list (Baroni and Veg-
naduzzo, 2004), English SentiWordNet (Esuli et. 
al., 2006), Taboada?s adjective list (Voll and 
Taboada, 2007), SubjectivityWord List (Banea et 
al., 2008) etc. Andreevskaia and Bergler (2006) 
present a method for extracting positive or negative 
sentiment bearing adjectives from WordNet using 
the Sentiment Tag Extraction Program (STEP). 
The proposed methods in (Wiebe and Riloff, 2006) 
automatically generate resources for subjectivity 
analysis for a new target language from the avail-
able resources for English. On the other hand, an 
automatically generated and scored sentiment lexi-
con, SentiFul (Neviarouskaya et al, 2009), its 
expansion, morphological modifications and dis-
tinguishing sentiment features also shows the con-
tributory results.   
But, all of the above mentioned resources are in 
English and have been used in coarse grained sen-
timent analysis (e.g., positive, negative or neutral). 
The proposed method in (Takamura et al, 2005) 
extracts semantic orientations from a small number 
of seed words with high accuracy in the experi-
ments on English as well as Japanese lexicons. 
But, it was also aimed for sentiment bearing words. 
Instead of English WordNet Affect (Strapparava 
and Valitutti, 2004), there are a few attempts in 
other languages such as, Russian and Romanian 
(Bobicev et al, 2010), Bengali (Das and Bandyop-
adhyay, 2010) etc. Our present approach is similar 
to some of these approaches but in contrast, we 
have evaluated our Japanese WordNet Affect on the 
SemEval 2007 affect sensing corpus translated into 
Japanese. In recent trends, the application of me-
chanical turk for generating emotion lexicon (Mo-
hammad and Turney, 2010) shows promising 
results. In the present task, we have incorporated 
the open source, available and accessible resources 
to achieve our goals.   
81
3 Developmental Phases  
3.1 WordNet Affect 
The English WordNet Affect, based on Ekman?s six 
emotion types is a small lexical resource compared 
to the complete WordNet but its affective annota-
tion helps in emotion analysis. Some collection of 
WordNet Affect synsets was provided as a resource 
for the shared task of Affective Text in SemEval-
2007. The whole data is provided in six files 
named by the six emotions. Each file contains a list 
of synsets and one synset per line. An example 
synset entry from WordNet Affect is as follows. 
a#00117872 angered  enraged  furious  infuri-
ated  maddened 
The first letter of each line indicates the part of 
speech (POS) and is followed by the affectID. The 
representation was simple and easy for further 
processing. We have retrieved and linked the com-
patible synsetID from the recent version of Word-
Net 3.0 with the affectID of the WordNet Affect 
synsets. We have searched each WordNet Affect 
synset in WordNet 3.0. If a matching WordNet 3.0 
synset is found, the WordNet 3.0 synsetID is 
mapped to the WordNet Affect affectID.  The link-
ing between two synsets of WordNet Affect and 
WordNet 3.0 is shown in Figure 1.  
 
WordNet Affect: 
n#05587878 anger choler ire 
a#02336957 annoyed harassed harried pestered 
vexed 
WordNet:  
07516354-n anger, ire, choler 
02455845-a annoyed harassed harried pestered 
vexed 
Linked Synset ID with Affect ID:  
   n#05587878 ?? 07516354-n anger choler ire  
  a#02336957 ?? 02455845-a annoyed harassed 
harried pestered vexed 
Figure 1: Linking between the synsets of Word-
Net Affect and WordNet 
3.2 Expansion of WordNet Affect using Sen-
tiWordNet 
It has been observed that the WordNet Affect con-
tains fewer number of emotion word entries. The 
six lists provided in the SemEval 2007 shared task 
contain only 612 synsets in total with 1536 words. 
The detail distribution of the emotion words as 
well as the synsets in the six different lists accord-
ing to their POS is shown in Table 1. Hence, we 
have expanded the lists with adequate number of 
emotion words using SentiWordNet before at-
tempting any translation of the lists into Japanese. 
SentiWordNet assigns each synset of WordNet with 
two coarse grained subjective scores such as posi-
tive and negative along with an objective score. 
SentiWordNet contains more number of coarse 
grained emotional words than WordNet Affect. We 
assumed that the translation of the coarse grained 
emotional words into Japanese might contain more 
or less fine-grained emotion words. One example 
entry of the SentiWordNet is shown below. The 
POS of the entry is followed by a synset ID, posi-
tive and negative scores and synsets containing 
sentiment words.   
SentiWordNet:  
a 121184  0.25 0.25 infuri-
ated#a#1 furious#a#2 maddened#a#1 en-
raged#a#1 angered#a#1 
Our aim is to increase the number of emotion 
words in the WordNet Affect using SentiWordNet, 
both of which are developed from the WordNet. 
Hence, each word of the WordNet Affect is re-
placed by the equivalent synsets retrieved from 
SentiWordNet if the synset contains that emotion 
word. The POS information in the WordNet Affect 
is kept unchanged during expansion. A related ex-
ample is shown in Figure 2. The distributions of 
expanded synsets and words for each of the six 
emotion classes based on four different POS types 
(noun N, verb V, adjective Adj. and adverb Adv.) 
are shown in Table 1. But, we have kept the dupli-
cate entries at synset level for identifying the emo-
tion related scores in our future attempts by 
utilizing the already associated positive and nega-
tive scores of SentiWordNet. The percentage of 
entries in the updated word lists are increased by 
69.77 and 74.60 at synset and word levels.  
3.3 Translation of Expanded WordNet Affect 
into Japanese  
We have mapped the affectID of the WordNet Af-
fect to the corresponding synsetID of the WordNet 
3.0. This mapping helps to expand the WordNet 
Affect with the recent version of SentiWordNet 3.0 
as well as translating the expanded lists into Japa-
nese using the Japanese WordNet (Bond et al, 
2009).
82
Emotion 
Classes 
WordNet Affect Synset (S) and Word (W) [After SentiWordNet updating] 
N V Adj Adv 
S W S W S W S W 
Anger 48 [198] 99 [403] 19 [103] 64 [399] 39 [89] 120 [328] 21 [23] 35 [50] 
Disgust 3 [17] 6 [21] 6 [21] 22 [62] 6  [38] 34  [230] 4  [5] 10 [19] 
Fear 23[89] 45 [224] 15  [48] 40 [243] 29  [62] 97  [261] 15 [21] 26 [49] 
Joy 73 [375] 149 [761] 40 [252] 122 [727] 84  [194] 203 [616] 30  [45] 65 [133] 
Sadness 32 [115] 64 [180] 10  [43] 33 [92] 55 [129] 169 [779] 26 [26] 43 [47] 
Surprise 5 [31]    8 [28] 7  [42] 28 [205] 12  [33] 41  [164] 4  [6] 13 [28] 
Table 1: Number of POS based Synsets and Words in six WordNet Affect lists before and after updating 
using SentiWordNet 
 
Linked Affect word:  
n#05587878 ?? 07516354-n anger choler ire  
 
SentiWordNet synsets containing  ?anger?:  
07516354-n anger, ire, choler 
14036539-n angriness, anger 
00758972-n anger, ira, ire, wrath 
01785971-v anger 
01787106-v see_red, anger 
 
SentiWordNet synsets containing  ?choler?:  
07552729-n fretfulness, fussiness, crossness, pe-
tulance, peevishness, irritability, choler 
05406958-n choler, yellow_bile 
 
Expanded Affect word:  
n#05587878?? 07516354-n anger choler ire 
14036539-n angriness anger 00758972-n anger 
ira, ire wrath 01785971-v anger  
? 05406958-n choler 
Figure 2: Expansion of WordNet Affect synset 
using SentiWordNet 
 
As the Japanese WordNet 7  is freely available 
and it is being developed based on the English 
WordNet, the synsets of the expanded lists are au-
tomatically translated into Japanese equivalent 
synsets based on the synsetIDs. The number of 
translated Japanese words and synsets for six affect 
lists are shown in Table 2 and Table 3 respectively. 
The following are some translated samples that 
contain word as well as phrase level translations. 
07510348-n surprise ? ??, ?? 
07503260-n disgust ? ????, ?? 
07532440-n unhappiness, sadness ? ????
?, ??, ???, ????, ????  
                                                        
7 http://nlpwww.nict.go.jp/wn-ja/index.en.html 
07527352-n joy, joyousness, joyfulness ? ??
?, ??, ??????, ??, ????, ??, 
??, ?, ???, ??, ?????? 
 
Emotion 
Classes 
Translated WordNet Affect list 
in Japanese (#Words) 
N V Adj Adv 
Anger 861 501 231 9 
Disgust 49 63 219 10 
Fear 375 235 334 104 
Joy 1959 1831 772 154 
Sadness 533 307 575 39 
Surprise 144 218 204 153 
Table 2: Number of POS based translated word 
entries in six Japanese WordNet Affect lists 
 
Emotion 
Classes 
Japanese WordNet Affect list 
Trans 
(#Syn) 
Non-
Trans 
(#Syn) 
Translated 
Morphemes 
(#W) (#P) 
Anger 254 159 1033 450 
Disgust 57 24 218 97 
Fear 146 74 615 315 
Joy 628 238 2940 1273 
Sadness 216 97 846 519 
Surprise 112 25 456 216 
Table 3: Number of translated (Trans) and non-
translated (Non-Trans) synsets (Syn), words (W) 
and phrases (P) in six Japanese WordNet Affects. 
3.4 Analyzing Translation Errors  
Some SentiWordNet synsets (e.g., 00115193-a huf-
fy, mad, sore) are not translated into Japanese as 
there are no equivalent synset entries in the Japa-
nese WordNet. There were a large number of word 
combinations, collocations and idioms in the Japa-
nese WordNet Affect. These parts of synsets show 
problems during translation and therefore manual 
83
translation is carried out for these types. Some of 
the English synsets (?07517292-n lividity?) were 
not translated into Japanese. But, an equivalent 
gloss of the word ?lividity? that is present in the 
Japanese WordNet is ?a state of fury so great the 
face becomes discolored?. One of the reasons of 
such translation problems may be that no equiva-
lent Japanese word sense is available for such Eng-
lish words. 
4 Evaluation and Analysis 
We have evaluated the lexical coverage of the de-
veloped Japanese WordNet Affect on a small emo-
tional judgment corpus and SemEval 2007 affect 
sensing corpus.  
4.1 Evaluation on Judgment Corpus    
The judgment corpus that is being developed by 
the Japan System Applications Co. Ltd. 8 contains 
only 100 sentences of emotional judgments. But, 
this corpus is not an open source till date. We have 
evaluated our Japanese WordNet Affect based base-
line system on these 100 sentences and the results 
for each of the six emotion classes are shown in 
Table 4. We have also incorporated an open source 
morphological analyzer9 in our baseline system.   
The algorithm is that, if a word in a sentence is 
present in any of the Japanese WordNet Affect lists; 
the sentence is tagged with the emotion label cor-
responding to that affect list. But, if any word is 
not found in any of the six lists, each word of the 
sentence is passed through the morphological 
process to identify its root form which is searched 
through the Japanese WordNet Affect lists again. If 
the root form is found in any of the six Japanese 
WordNet Affect lists, the sentence is tagged accor-
dingly. Otherwise, the sentence is tagged as non-
emotional or neutral. The average F-Score of the 
baseline system has been improved by 4.1% with 
respect to the six emotion classes. Due to the fewer 
number of sentential instances in some emotion 
classes (e.g., joy, sadness, surprise), the perfor-
mance of the system gives poor results even after 
including the morphological knowledge. One of 
the reasons may be the less number of words and 
synset entries in some WordNet Affect lists (e.g., 
fear). Hence, we have aimed to translate the Eng-
                                                        
8 http://www.jsa.co.jp/ 
9 http://mecab.sourceforge.net/ 
lish SemEval 2007 affect sensing corpus into Japa-
nese and evaluate our system on the translated cor-
pus. 
 
Emotion 
Classes  
(#Sentences) 
Judgment Corpus (in %) 
Before Morphology [After Mor-
phology] 
Precision Recall F-Score 
Anger 
 (#32) 
51.61 
[64.29] 
50.00 
[68.12] 
50.79 
[66.14] 
disgust 
 (#18) 
25.00 
[45.00] 
5.56 
[10.56] 
9.09 
[17.10] 
fear (#33) NULL 
joy  
(#3) 
3.45 
[8.08] 
66.67 
[100.00] 
6.56 
[14.95] 
Sadness  (#5) NULL 
surprise  
(#9) 
6.90 
[13.69] 
22.22 
[33.33] 
10.53 
[19.41] 
Table 4: Precision, Recall and F-Scores (in %) 
of the system per emotion class on the Judgment 
corpus by including and excluding morphology. 
4.2 Evaluation on Translated SemEval 2007 
Affect Sensing Corpus    
The English SemEval 2007 affect sensing corpus 
consists of news headlines only. Each of the news 
headlines is tagged with a valence score and scores 
for all the six Ekman?s emotions. The six emotion 
scores for each sentence are in the range of 0 to 
100. We have considered that each sentence is as-
signed a single sentential emotion tag based on the 
maximum emotion score out of six annotated emo-
tion scores. We have used the Google translator 
API 10to translate the 250 and 1000 sentences of 
the trial and test sets of the SemEval 2007 corpus 
respectively. The experiments regarding morphol-
ogy and emotion scores are conducted on the trial 
corpus. We have carried out different experiments 
on 1000 test sentences by selecting different ranges 
of emotion scores. The corresponding experimental 
results are also shown in Table 5. Incorporation of 
morphology improves the performance of the sys-
tem. On the other hand, it is observed that the per-
formance of the system decreases by increasing the 
range of Emotion Scores (ES). The reason may be 
that the numeric distribution of the sentential in-
stances in each of the emotion classes decreases as 
the range in emotion scores increases. 
                                                        
10 http://translate.google.com/# 
 
84
Emotion 
Classes 
Japanese Translated SemEval 2007 Test Corpus (in %) 
Before Morphology [After Morphology] 
Emotion Score (ES) ? 0 Emotion Score (ES) ? 10 
Precision Recall F-Score Precision Recall F-Score 
Anger 61.01[68.75] 18.83[31.16] 28.78[42.88] 44.65[52.08] 25.54[33.32] 32.49[40.35] 
disgust 79.55[85.05] 8.35[16.06] 15.12[27.01] 40.91[41.46] 9.89[18.07] 15.93[24.97] 
Fear 93.42[95.45] 10.26[16.77] 18.49[28.52] 77.63[81.82] 13.32[21.42] 22.74[34.03] 
Joy 69.07[72.68] 57.03[80.30] 62.48[76.29] 53.89[55.61] 56.50[96.22] 55.17[70.40] 
sadness 83.33[84.29] 10.58[19.54] 18.77[31.67] 67.78[69.87] 11.78[19.88] 20.07[30.86] 
surprise 94.94[94.94] 7.84[13.65] 14.48[23.99] 72.15[74.58] 8.25[15.87] 14.81[26.30] 
Emotion Score (ES) ? 30 Emotion Score (ES) ? 50 
Anger 21.38[28.12] 39.08[62.45] 27.64[38.59] 6.92[10.42] 57.89[78.02] 12.36[18.26] 
disgust 2.27[5.04] 3.70[6.72] 2.82[6.15] NIL NIL NIL 
Fear 44.74[56.82] 16.67[28.76] 24.29[38.45] 21.05[29.55] 17.98[31.26] 19.39[30.79] 
Joy 31.48[33.42] 56.86[97.08] 40.52[50.53] 12.04[24.98] 61.32[87.66] 20.12[39.10] 
sadness 37.78[69.86] 15.60[25.31] 22.08[37.22] 13.33[23.07] 12.12[22.57] 12.70[18.71] 
surprise 17.72[20.34] 8.14[18.56] 11.16[20.35] 3.80[8.50] 7.50[12.50] 5.04[10.11] 
Table 6: Precision, Recall and F-Scores (in %) of the system per emotion class on the translated Japanese 
SemEval 2007 test corpus before and after including morphology on different ranges of Emotion Scores. 
4.3 Analysis of Morphology  
Japanese affect lists include words as well as 
phrases. We deal with phrases using Japanese 
morphology tool to find affect words in a sentence 
and substitute an affect word into its original con-
jugated form. One of the main reasons of using a 
morphology tool is to analyze the conjugated form 
and to identify the phrases. For example, the Japa-
nese word for the equivalent English word ?anger? 
is "?? (o ko ru)" but there are other conjugated 
word forms such as "???(o ko tta)" that means 
?angered? and it is used in past tense. Similarly, 
other conjugated form "????? (o ko tte i ta)" 
denotes the past participle form ?have angered? of 
the original word ?anger?. The morphological form 
of its passive sense is "???? (o ko ra re ru)" 
that means ?be angered?. We identify the word 
forms from their corresponding phrases by using 
the morpheme information. For example, the 
phrase "???? (o ko ra re ru)" consists of two 
words, one is ??? (o ko ra) that is in an imper-
fective form and other word is "?? (re ru) which 
is in an original form. The original form of the im-
perfective word ?? (o ko ra) is "?? (o ko 
ru)". It has been found that some of the English 
multi-word phrases have no equivalent Japanese 
phrase available. Only the equivalent Japanese 
words are found in Japanese WordNet. For exam 
 
ple, the following synset contains a multi-word 
phrase ?see-red?. Instead of any equivalent phrases, 
only words are found in Japanese WordNet. 
01787106-v anger, see -red ? ??, ??, ?? 
5 Conclusion 
The present paper describes the preparation of Jap-
anese WordNet Affect containing six types of emo-
tion words in six separate lists. The automatic 
approach of expanding, translating and sense dis-
ambiguation tasks reduces the manual effort. The 
resource is still being updated with more number 
of emotional words to increase the coverage. The 
sense disambiguation task needs to be improved 
further in future by incorporating more number of 
translators and considering their agreement into 
account. In future we will adopt a corpus-driven 
approach for updating the resource with more 
number of emotion words and phrases for extend-
ing the emotion analysis task in Japanese. 
Acknowledgments 
The work reported in this paper is supported by a 
grant from the India-Japan Cooperative Pro-
gramme (DST-JST) 2009 Research project entitled 
?Sentiment Analysis where AI meets Psychology? 
funded by Department of Science and Technology 
(DST), Government of India. 
85
References  
Andreevskaia A. and Bergler Sabine. 2007. CLaC and 
CLaC-NB: Knowledge-based and corpus-based ap-
proaches to sentiment tagging. 4th International 
Workshop on Semantic Evaluations (SemEval-2007), 
pp. 117?120, Prague. 
Baccianella Stefano, Esuli Andrea and Sebas-tiani Fa-
brizio. 2010. SentiWordNet 3.0: An Enhanced Lexi-
cal Re-source for Sentiment Analysis and Opinion 
Mining. In Proceedings of the 7th Conference on 
Language Resources and Evaluation, pp. 2200-2204. 
Banea, Carmen, Mihalcea Rada, Wiebe Janyce. 2008.  
A Bootstrapping Method for Building Subjectivity 
Lexicons for Languages with Scarce Resources. The 
Sixth International Conference on Language Re-
sources and Evaluation (LREC 2008). 
Baroni M. and Vegnaduzzo S. 2004. Identifying subjec-
tive adjectives through web-based mutual informa-
tion. Proceedings of the German Conference on NLP. 
Bobicev Victoria, Maxim Victoria, Prodan Tatiana, 
Burciu Natalia, Anghelus Victoria. 2010. Emotions 
in words: developing a multilingual WordNet-Affect. 
CICLING 2010.  
Bond, Francis, Hitoshi Isahara, Sanae Fujita, Kiyotaka 
Uchimoto, Takayuki Kuribayashi and Kyoko Kanza-
ki. 2009. Enhancing the Japanese WordNet. 7th 
Workshop on Asian Language Resources, ACL-
IJCNLP 2009, Singapore.  
Das Dipankar and Bandyopadhyay Sivaji. 2010. Devel-
oping Bengali WordNet Affect for Analyzing Emo-
tion. 23rd International Conference on the Computer 
Processing of Oriental Languages (ICCPOL-2010), 
pp. 35-40, California, USA. 
Ekman Paul. 1992. An argument for basic emotions, 
Cognition and Emotion, 6(3-4):169-200. 
Esuli, Andrea. and Sebastiani, Fabrizio. 2006. 
SENTIWORDNET: A Publicly Available Lexical 
Resource for Opinion Mining, LREC. 
Hatzivassiloglou V. and McKeown K. R. 1997. Predict-
ing the semantic orientation of adjectives. 35th An-
nual Meeting of the ACL and the 8th Conference of 
the European Chapter of the ACL, pp. 174?181. 
Miller, A. G. 1995. WordNet: a lexical database for 
English. In Communications of the ACM, vol. 38 
(11), November, pp. 39-41. 
Mohammad, S. and Turney, P.D. 2010. Emotions 
evoked by common words and phrases: Using Me-
chanical Turk to create an emotion lexicon. Proceed-
ings of the NAACL-HLT 2010 Workshop on 
Computational Approaches to Analysis and Genera-
tion of Emotion in Text, LA, California, 26-34. 
Neviarouskaya, Alena, Prendinger Helmut, and Ishizuka 
Mitsuru. 2009. SentiFul: Generating a Reliable Lex-
icon for Sentiment Analysis. International Confe-
rence on Affective Computing and Intelligent 
Interaction (ACII'09), IEEE, pp. 363-368. 
Sood S. and Vasserman, L. 2009. ESSE: Exploring 
Mood on the Web. 3rd International AAAI Confe-
rence on Weblogs and Social Media (ICWSM) Data 
Challenge Workshop. 
Strapparava Carlo and Valitutti, A. 2004. Wordnet-
affect: an affective extension of wordnet, In 4th In-
ternational Conference on Language Resources and 
Evaluation, pp. 1083-1086. 
Strapparava Carlo and Mihalcea Rada. 2007. SemEval-
2007 Task 14: Affective Text. 45th Aunual Meeting 
of Association for Computational linguistics. 
Takamura Hiroya, Inui Takashi, Okumura Manabu. 
2005. Extracting Semantic Orientations of Words us-
ing Spin Model. 43rd Annual Meeting of the Associa-
tion for Computational Linguistics, pp.133-140.  
Voll, K. and M. Taboada. 2007. Not All Words are 
Created Equal: Extracting Semantic Orientation as a 
Function of Adjective Relevance. In Proceedings of 
the 20th Australian Joint Conference on Artificial In-
telligence. pp. 337-346, Gold Coast, Australia. 
Wiebe Janyce and Riloff Ellen. 2006. Creating Subjec-
tive and Objective Sentence Classifiers from Unan-
notated Texts. International Conference on 
Intelligent Text Processing and Computational Lin-
guistics, Mexico City, pp. 475?486. 
 
 
 
86
