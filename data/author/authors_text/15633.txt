Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1426?1436,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Relation Extraction with Relation Topics
Chang Wang James Fan Aditya Kalyanpur David Gondek
IBM T. J. Watson Research Lab
19 Skyline Drive, Hawthorne, New York 10532
{wangchan, fanj, adityakal, dgondek}@us.ibm.com
Abstract
This paper describes a novel approach to the
semantic relation detection problem. Instead
of relying only on the training instances for
a new relation, we leverage the knowledge
learned from previously trained relation detec-
tors. Specifically, we detect a new semantic
relation by projecting the new relation?s train-
ing instances onto a lower dimension topic
space constructed from existing relation de-
tectors through a three step process. First, we
construct a large relation repository of more
than 7,000 relations from Wikipedia. Second,
we construct a set of non-redundant relation
topics defined at multiple scales from the re-
lation repository to characterize the existing
relations. Similar to the topics defined over
words, each relation topic is an interpretable
multinomial distribution over the existing re-
lations. Third, we integrate the relation topics
in a kernel function, and use it together with
SVM to construct detectors for new relations.
The experimental results on Wikipedia and
ACE data have confirmed that background-
knowledge-based topics generated from the
Wikipedia relation repository can significantly
improve the performance over the state-of-the-
art relation detection approaches.
1 Introduction
Detecting semantic relations in text is very useful
in both information retrieval and question answer-
ing because it enables knowledge bases to be lever-
aged to score passages and retrieve candidate an-
swers. To extract semantic relations from text, three
types of approaches have been applied. Rule-based
methods (Miller et al, 2000) employ a number of
linguistic rules to capture relation patterns. Feature-
based methods (Kambhatla, 2004; Zhao and Grish-
man, 2005) transform relation instances into a large
amount of linguistic features like lexical, syntactic
and semantic features, and capture the similarity be-
tween these feature vectors. Recent results mainly
rely on kernel-based approaches. Many of them fo-
cus on using tree kernels to learn parse tree struc-
ture related features (Collins and Duffy, 2001; Cu-
lotta and Sorensen, 2004; Bunescu and Mooney,
2005). Other researchers study how different ap-
proaches can be combined to improve the extraction
performance. For example, by combining tree ker-
nels and convolution string kernels, (Zhang et al,
2006) achieved the state of the art performance on
ACE (ACE, 2004), which is a benchmark dataset for
relation extraction.
Although a large set of relations have been iden-
tified, adapting the knowledge extracted from these
relations for new semantic relations is still a chal-
lenging task. Most of the work on domain adapta-
tion of relation detection has focused on how to cre-
ate detectors from ground up with as little training
data as possible through techniques such as boot-
strapping (Etzioni et al, 2005). We take a differ-
ent approach, focusing on how the knowledge ex-
tracted from the existing relations can be reused to
help build detectors for new relations. We believe by
reusing knowledge one can build a more cost effec-
tive relation detector, but there are several challenges
associated with reusing knowledge.
The first challenge to address in this approach is
how to construct a relation repository that has suffi-
1426
cient coverage. In this paper, we introduce a method
that automatically extracts the knowledge charac-
terizing more than 7,000 relations from Wikipedia.
Wikipedia is comprehensive, containing a diverse
body of content with significant depth and grows
rapidly. Wikipedia?s infoboxes are particularly in-
teresting for relation extraction. They are short,
manually-created, and often have a relational sum-
mary of an article: a set of attribute/value pairs de-
scribing the article?s subject.
Another challenge is how to deal with overlap of
relations in the repository. For example, Wikipedia
authors may make up a name when a new relation
is needed without checking if a similar relation has
already been created. This leads to relation duplica-
tion. We refine the relation repository based on an
unsupervised multiscale analysis of the correlations
between existing relations. This method is parame-
ter free, and able to produce a set of non-redundant
relation topics defined at multiple scales. Similar to
the topics defined over words (Blei et al, 2003), we
define relation topics as multinomial distributions
over the existing relations. The relation topics ex-
tracted in our approach are interpretable, orthonor-
mal to each other, and can be used as basis relations
to re-represent the new relation instances.
The third challenge is how to use the relation top-
ics for a relation detector. We map relation instances
in the new domains to the relation topic space, re-
sulting in a set of new features characterizing the
relationship between the relation instances and ex-
isting relations. By doing so, background knowl-
edge from the existing relations can be introduced
into the new relations, which overcomes the limi-
tations of the existing approaches when the training
data is not sufficient. Our work fits in to a class of re-
lation extraction research based on ?distant supervi-
sion?, which studies how knowledge and resources
external to the target domain can be used to im-
prove relation extraction. (Mintz et al, 2009; Jiang,
2009; Chan and Roth, 2010). One distinction be-
tween our approach and other existing approaches is
that we represent the knowledge from distant super-
vision using automatically constructed topics. When
we test on new instances, we do not need to search
against the knowledge base. In addition, our top-
ics also model the indirect relationship between re-
lations. Such information cannot be directly found
from the knowledge base.
The contributions of this paper are three-fold.
Firstly, we extract a large amount of training
data for more than 7,000 semantic relations from
Wikipedia (Wikipedia, 2011) and DBpedia (Auer
et al, 2007). A key part of this step is how we
handle noisy data with little human effort. Sec-
ondly, we present an unsupervised way to con-
struct a set of relation topics at multiple scales.
This step is parameter free, and results in a non-
redundant, multiscale relation topic space. Thirdly,
we design a new kernel for relation detection by
integrating the relation topics into the relation de-
tector construction. The experimental results on
Wikipedia and ACE data (ACE, 2004) have con-
firmed that background-knowledge-based features
generated from the Wikipedia relation repository
can significantly improve the performance over the
state-of-the-art relation detection approaches.
2 Extracting Relations from Wikipedia
Our training data is from two parts: relation in-
stances from DBpedia (extracted from Wikipedia
infoboxes), and sentences describing the relations
from the corresponding Wikipedia pages.
2.1 Collecting the Training Data
Since our relations correspond to Wikipedia infobox
properties, we use an approach similar to that de-
scribed in (Hoffmann et al, 2010) to collect positive
training data instances. We assume that a Wikipedia
page containing a particular infobox property is
likely to express the same relation in the text of
the page. We further assume that the relation is
most likely expressed in the first sentence on the
page which mentions the arguments of the relation.
For example, the Wikipedia page for ?Albert Ein-
stein? contains an infobox property ?alma mater?
with value ?University of Zurich?, and the first sen-
tence mentioning the arguments is the following:
?Einstein was awarded a PhD by the University of
Zurich?, which expresses the relation. When look-
ing for relation arguments on the page, we go be-
yond (sub)string matching, and use link information
to match entities which may have different surface
forms. Using this technique, we are able to collect a
large amount of positive training instances of DBpe-
1427
dia relations.
To get precise type information for the argu-
ments of a DBpedia relation, we use the DBpedia
knowledge base (Auer et al, 2007) and the asso-
ciated YAGO type system (Suchanek et al, 2007).
Note that for every Wikipedia page, there is a cor-
responding DBpedia entry which has captured the
infobox-properties as RDF triples. Some of the
triples include type information, where the subject
of the triple is a Wikipedia entity, and the object
is a YAGO type for the entity. For example, the
DBpedia entry for the entity ?Albert Einstein? in-
cludes YAGO types such as Scientist, Philosopher,
Violinist etc. These YAGO types are also linked
to appropriate WordNet concepts, providing for ac-
curate sense disambiguation. Thus, for any en-
tity argument of a relation we are learning, we ob-
tain sense-disambiguated type information (includ-
ing super-types, sub-types, siblings etc.), which be-
come useful generalization features in the relation
detection model. Given a common noun, we can
also retrieve its type information by checking against
WordNet (Fellbaum, 1998).
2.2 Extracting Rules from the Training Data
We use a set of rules together with their popular-
ities (occurrence count) to characterize a relation.
A rule representing the relations between two ar-
guments has five components (ordered): argument1
type, argument2 type, noun, preposition and verb. A
rule example of ActiveYearsEndDate relation (about
the year that a person retired) is:
person100007846|year115203791|-|in|retire.
In this example, argument1 type is per-
son100007846, argument2 type is year115203791,
both of which are from YAGO type system. The
key words connecting these two arguments are in
(preposition) and retire (verb). This rule does not
have a noun, so we use a ?-? to take the position of
noun. The same relation can be represented in many
different ways. Another rule example characterizing
the same relation is
person100007846|year115203791|retirement|-|announce.
This paper only considers three types of words:
noun, verb and preposition. It is straightforward to
expand or simplify the rules by including more or
removing some word types. The keywords are ex-
tracted from the shortest path on the dependency
Figure 1: A dependency tree example.
tree between the two arguments. A dependency
tree (Figure 1) represents grammatical relations be-
tween words in a sentence. We used a slot grammar
parser (McCord, 1995) to generate the parse tree of
each sentence. Note that there could be multiple
paths between two arguments in the tree. We only
take the shortest path into consideration. The pop-
ularity value corresponding to each rule represents
how many times this rule applies to the given rela-
tion in the given data. Multiple rules can be con-
structed from one relation instance, if multiple argu-
ment types are associated with the instance, or mul-
tiple nouns, prepositions or verbs are in the depen-
dency path.
2.3 Cleaning the Training Data
To find a sentence on the Wikipedia page that is
likely to express a relation in its infobox, we con-
sider the first sentence on the page that mentions
both arguments of the relation. This heuristic ap-
proach returns reasonably good results, but brings in
about 20% noise in the form of false positives, which
is a concern when building an accurate statistical re-
lation detector. To address this issue, we have devel-
oped a two-step technique to automatically remove
some of the noisy data. In the first step, we extract
popular argument types and keywords for each DB-
pedia relation from the given data, and then use the
combinations of those types and words to create ini-
tial rules. Many of the argument types and keywords
introduced by the noisy data are often not very pop-
ular, so they can be filtered out in the first step. Not
all initial rules make sense. In the second step, we
1428
check each rule against the training data to see if that
rule really exists in the training data or not. If it does
not exist, we filter it out. If a sentence does not have
a single rule passing the above procedure, that sen-
tence will be removed. Using the above techniques,
we collect examples characterizing 7,628 DBpedia
relations.
3 Learning Multiscale Relation Topics
An extra step extracting knowledge from the raw
data is needed for two reasons: Firstly, many DB-
pedia relations are inter-related. For example, some
DBpedia relations have a subclass relationship, e.g.
?AcademyAward? and ?Award?; others overlap in
their scope and use, e.g., ?Composer? and ?Artist?;
while some are equivalent, e.g., ?DateOfBirth? and
?BirthDate?. Secondly, a fairly large amount of the
noisy labels are still in the training data.
To reveal the intrinsic structure of the current DB-
pedia relation space and filter out noise, we car-
ried out a correlation analysis of relations in the
training data, resulting in a relation topic space.
Each relation topic is a multinomial distribution
over the existing relations. We adapted diffusion
wavelets (Coifman and Maggioni, 2006) for this
task. Compared to the other well-known topic ex-
traction methods like LDA (Blei et al, 2003) and
LSI (Deerwester et al, 1990), diffusion wavelets can
efficiently extract a hierarchy of interpretable topics
without any user input parameter (Wang and Ma-
hadevan, 2009).
3.1 An Overview of Diffusion Wavelets
The diffusion wavelets algorithm constructs a com-
pressed representation of the dyadic powers of a
square matrix by representing the associated matri-
ces at each scale not in terms of the original (unit
vector) basis, but rather using a set of custom gener-
ated bases (Coifman and Maggioni, 2006). Figure
2 summarizes the procedure to generate diffusion
wavelets. Given a matrix T , the QR (a modified
QR decomposition) subroutine decomposes T into
an orthogonal matrix Q and a triangular matrix R
such that T ? QR, where |Ti,k ? (QR)i,k| < ?
for any i and k. Columns in Q are orthonormal ba-
sis functions spanning the column space of T at the
finest scale. RQ is the new representation of T with
{[?j ]?0} = DWT (T, ?, J)
//INPUT:
//T : The input matrix.
//?: Desired precision, which can be set to a small
number or simply machine precision.
//J : Number of levels (optional).
//OUTPUT:
//[?j ]?0 : extended diffusion scaling functions at
scale j.
?0 = I;
For j = 0 to J ? 1 {
([?j+1]?j , [T 2j ]?j+1?j )? QR([T 2
j ]?j?j , ?);
[?j+1]?0 = [?j+1]?j [?j ]?0 ;
[T 2j+1 ]?j+1?j+1 = ([T
2j ]?j+1?j [?j+1]?j )
2;
}
Figure 2: Diffusion Wavelets construct multiscale repre-
sentations of the input matrix at different scales. QR is a
modified QR decomposition. J is the max step number
(this is optional, since the algorithm automatically ter-
minates when it reaches a matrix of size 1 ? 1). The
notation [T ]?b?a denotes matrix T whose column space isrepresented using basis ?b at scale b, and row space is
represented using basis ?a at scale a. The notation [?b]?a
denotes basis ?b represented on the basis ?a. At an arbi-
trary scale j, we have pj basis functions, and length of
each function is lj . The number of pj is determined by
the intrinsic structure of the given dataset in QR routine.
[T ]?b?a is a pb ? la matrix, and [?b]?a is an la ? pb matrix.
respect to the space spanned by the columns of Q
(this result is based on the matrix invariant subspace
theory). At an arbitrary level j,DWT learns the ba-
sis functions from T 2j using QR. Compared to the
number of basis functions spanning T 2j ?s original
column space, we usually get fewer basis functions,
since some high frequency information (correspond-
ing to the ?noise? at that level) can be filtered out.
DWT then computes T 2j+1 using the low frequency
representation of T 2j and the procedure repeats.
3.2 Constructing Multiscale Relation Topics
Learning Relation Correlations
Assume we have M relations, and the ith of them
is characterized by mi <rule, popularity> pairs. We
use s(a, b) to represent the similarity between the
ath and bth relations. To compute s(a, b), we first
normalize the popularities for each relation, and then
1429
look for the rules that are shared by both relation a
and b. We use the product of corresponding pop-
ularity values to represent the similarity score be-
tween two relations with respect to each common
rule. s(a, b) is set to the sum of such scores over
all common rules. The relation-relation correlation
matrix S is constructed as follows:
S = [
s(1, 1) ? ? ? s(1,M)
? ? ? ? ? ? ? ? ?
s(M, 1) ? ? ? s(M,M)
]
We have more than 200, 000 argument types, tens
of thousands of distinct nouns, prepositions, and
verbs, so we potentially have trillions of distinct
rules. One rule may appear in multiple relations.
The more rules two relations share, the more related
two relations should be. The rules shared across dif-
ferent relations offer us a novel way to model the
correlations between different relations, and further
allow us to create relation topics. The rules can also
be simplified. For example, we may treat argument1,
argument2, noun, preposition and verb separately.
This results in simple rules that only involve in one
argument type or word. The correlations between
relations are then computed only based on one par-
ticular component like argument1, noun, etc.
Theoretical Analysis
Matrix S models the correlations between rela-
tions in the training data. Once S is constructed, we
adapt diffusion wavelets (Coifman and Maggioni,
2006) to automatically extract the basis functions
spanning the original column space of S at multi-
ple scales. The key strength of the approach is that
it is data-driven, largely parameter-free and can au-
tomatically determine the number of levels of the
topical hierarchy, as well as the topics at each level.
However, to apply diffusion wavelets to S, we first
need to show that S is a positive semi-definite ma-
trix. This property guarantees that all eigenvalues
of S are ? 0. Depending on the way we formal-
ize the rules, the methods to validate this property
are slightly different. When we treat argument1,
argument2, noun, preposition and verb separately, it
is straightforward to see the property holds. In The-
orem 1, we show the property also holds when we
use more complicated rules (using the 5-tuple rule
in Section 2.2 as an example in the proof).
Theorem 1. S is a Positive Semi-Denite matrix.
Proof: An arbitrary rule ri is uniquely characterized
by a five tuple: argument1 type| argument2 type|
noun| preposition| verb. Since the number of dis-
tinct argument types and words are constants, the
number of all possible rules is also a constant: R.
If we treat each rule as a feature, then the set of
rules characterizing an arbitrary relation ri can be
represented as a point [p1i , ? ? ? , pRi ] in a latent R di-
mensional rule space, where pji represents the popu-
larity of rule j in relation ri in the given data.
We can verify that the way to compute s(a, b) is
the same as s(a, b) =< [p1a ? ? ? pRa ], [p1b ? ? ? pRb ] >,
where < ?, ? > is the cosine similarity (kernel). It
follows directly from the definition of positive semi-
definite matrix (PSD) that S is PSD (Scho?lkopf and
Smola, 2002).
In our approach, we construct multiscale re-
lation topics by applying DWT to decompose
S/?max(S), where ?max(S) represents the largest
eigenvalue of S. Theorem 2 shows that this decom-
position will converge, resulting in a relation topic
hierarchy with one single topic at the top level.
Theorem 2. Let ?max(S) represent the largest
eigenvalue of matrix S, then DWT (S/?max(S), ?)
produces a set of nested subspaces of the column
space of S, and the highest level of the resulting sub-
space hierarchy is spanned by one basis function.
Proof: From Theorem 1, we know that S is a PSD
matrix. This means ?max(S) ? [0,+?) (all eigen-
values of S are non-negative). This further implies
that ?(S)/?max(S) ? [0, 1], where ?(S) represents
any eigenvalue of S.
The idea underlying diffusion wavelets is based
on decomposing the spectrum of an input matrix
into various spectral bands, spanned by basis func-
tions (Coifman and Maggioni, 2006). Let T =
S/?max(S). In Figure 2, we construct spectral
bands of eigenvalues, whose associated eigenvectors
span the corresponding subspaces. Define dyadic
spatial scales tj as
tj =
j?
t=0
2t = 2j+1 ? 1, j ? 0 .
At each spatial scale, the spectral band is defined as:
?j(T ) = {? ? ?(T ), ?tj ? ?},
1430
where ?(T ) represents any eigenvalue of T , and ? ?
(0, 1) is a pre-defined threshold in Figure 2. We can
now associate with each of the spectral bands a vec-
tor subspace spanned by the corresponding eigen-
vectors:
Vj = ?{?? : ? ? ?(T ), ?tj ? ?}?, j ? 0 .
In the limit, we obtain
lim
j??
Vj = ?{?? : ? = 1}?
That is, the highest level of the resulting subspace
hierarchy is spanned by the eigenvector associated
with the largest eigenvalue of T .
This result shows that the multiscale analysis of
the relation space will automatically terminate at the
level spanned by one basis, which is the most popu-
lar relation topic in the training data.
3.3 High Level Explanation
We first create a set of rules to characterize each in-
put relation. Since these rules may occur in multi-
ple relations, they provide a way to model the co-
occurrence relationship between different relations.
Our algorithm starts with the relation co-occurrence
matrix and then repeatedly applies QR decomposi-
tion to learn the topics at the current level while at
the same time modifying the matrix to focus more on
low-frequency indirect co-occurrences (between re-
lations) for the next level. Running DWT is equiv-
alent to running a Markov chain on the input data
forward in time, integrating the local geometry and
therefore revealing the relevant geometric structures
of the whole data set at different scales. At scale
j, the representation of T 2j+1 is compressed based
on the amount of remaining information and the de-
sired precision. This procedure is illustrated in Fig-
ure 3. In the resulting topic space, instances with
related relations will be grouped together. This ap-
proach may significantly help us detect new rela-
tions, since it potentially expands the information
brought in by new relation instances from making
use of the knowledge extracted from the existing re-
lation repository.
3.4 Benefits
As shown in Figure 3, the topic spaces at different
levels are spanned by a different number of basis
  
 	
   
Proceedings of NAACL-HLT 2013, pages 777?782,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Distant Supervision for Relation Extraction
with an Incomplete Knowledge Base
Bonan Min, Ralph Grishman, Li Wan
New York University
New York, NY 10003
{min,grishman,wanli}
@cs.nyu.edu
Chang Wang, David Gondek
IBM T. J. Watson Research Center
Yorktown Heights, NY 10598
{wangchan,dgondek}
@us.ibm.com
Abstract
Distant supervision, heuristically labeling a
corpus using a knowledge base, has emerged
as a popular choice for training relation ex-
tractors. In this paper, we show that a sig-
nificant number of ?negative? examples gen-
erated by the labeling process are false neg-
atives because the knowledge base is incom-
plete. Therefore the heuristic for generating
negative examples has a serious flaw. Building
on a state-of-the-art distantly-supervised ex-
traction algorithm, we proposed an algorithm
that learns from only positive and unlabeled
labels at the pair-of-entity level. Experimental
results demonstrate its advantage over existing
algorithms.
1 Introduction
Relation Extraction is a well-studied problem
(Miller et al, 2000; Zhou et al, 2005; Kambhatla,
2004; Min et al, 2012a). Recently, Distant Super-
vision (DS) (Craven and Kumlien, 1999; Mintz et
al., 2009) has emerged to be a popular choice for
training relation extractors without using manually
labeled data. It automatically generates training ex-
amples by labeling relation mentions1 in the source
corpus according to whether the argument pair is
listed in the target relational tables in a knowledge
base (KB). This method significantly reduces human
efforts for relation extraction.
The labeling heuristic has a serious flaw. Knowl-
edge bases are usually highly incomplete. For exam-
1An occurrence of a pair of entities with the source sentence.
ple, 93.8% of persons from Freebase2 have no place
of birth, and 78.5% have no nationality (section 3).
Previous work typically assumes that if the argument
entity pair is not listed in the KB as having a re-
lation, all the corresponding relation mentions are
considered negative examples.3 This crude assump-
tion labeled many entity pairs as negative when in
fact some of their mentions express a relation. The
number of such false negative matches even exceeds
the number of positive pairs, by 3 to 10 times, lead-
ing to a significant problem for training. Previous
approaches (Riedel et al, 2010; Hoffmann et al,
2011; Surdeanu et al, 2012) bypassed this problem
by heavily under-sampling the ?negative? class.
We instead deal with a learning scenario where we
only have entity-pair level labels that are either posi-
tive or unlabeled. We proposed an extension to Sur-
deanu et al (2012) that can train on this dataset. Our
contribution also includes an analysis on the incom-
pleteness of Freebase and the false negative match
rate in two datasets of labeled examples generated
by DS. Experimental results on a realistic and chal-
lenging dataset demonstrate the advantage of the al-
gorithm over existing solutions.
2 Related Work
Distant supervision was first proposed by Craven
and Kumlien (1999) in the biomedical domain.
2Freebase is a large collaboratively-edited KB. It is available
at http://www.freebase.com.
3There are variants of labeling heuristics. For example, Sur-
deanu et al (2011) and Sun et al (2011) use a pair < e, v >
as a negative example, when it is not listed in Freebase, but e is
listed with a different v?. These assumptions are also problem-
atic in cases where the relation is not functional.
777
Since then, it has gain popularity (Mintz et al, 2009;
Bunescu and Mooney, 2007; Wu and Weld, 2007;
Riedel et al, 2010; Hoffmann et al, 2011; Sur-
deanu et al, 2012; Nguyen and Moschitti, 2011).
To tolerate noisy labels in positive examples, Riedel
et al (2010) use Multiple Instance Learning (MIL),
which assumes only at-least-one of the relation men-
tions in each ?bag? of mentions sharing a pair of ar-
gument entities which bears a relation, indeed ex-
presses the target relation. MultiR (Hoffmann et
al., 2011) and Multi-Instance Multi-Label (MIML)
learning (Surdeanu et al, 2012) further improve it
to support multiple relations expressed by different
sentences in a bag. Takamatsu et al (2012) mod-
els the probabilities of a pattern showing relations,
estimated from the heuristically labeled dataset.
Their algorithm removes mentions that match low-
probability patterns. Sun et al (2011) and Min et
al. (2012b) also estimate the probablities of patterns
showing relations, but instead use them to relabel ex-
amples to their most likely classes. Their approach
can correct highly-confident false negative matches.
3 Problem Definition
Distant Supervision: Given a KB D (a collection
of relational tables r(e1, e2), in which r?R (R is the
set of relation labels), and < e1, e2 > is a pair of
entities that is known to have relation r) and a cor-
pus C, the key idea of distant supervision is that we
align D to C, label each bag4 of relation mentions
that share argument pair < e1, e2 > with r, other-
wise OTHER. This generates a dataset that has labels
on entity-pair (bag) level. Then a relation extractor
is trained with single-instance learning (by assum-
ing all mentions have the same label as the bag), or
Multiple-Instance Learning (by assuming at-least-
one of the mentions expresses the bag-level label),
or Multi-Instance Multi-Label learning (further as-
suming a bag can have multiple labels) algorithms.
All of these works treat the OTHER class as exam-
ples that are labeled as negative.
The incomplete KB problem: KBs are usually
incomplete because they are manually constructed,
and it is not possible to cover all human knowledge
4A bag is defined as a set of relation mentions sharing the
same entity pair as relation arguments. We will use the terms
bag and entity pair interchangeably in this paper.
nor stay current. We took frequent relations, which
involve an entity of type PERSON, from Freebase
for analysis. We define the incompleteness ?(r) of a
relation r as follows:
?(r) = |{e}|?|{e|?e?,s.t.r(e,e?)?D}||{e}|
?(r) is the percentage of all persons {e} that do
not have an attribute e? (with which r(e, e?) holds).
Table 1 shows that 93.8% of persons have no place
of birth, and 78.5% of them have no nationality.
These are must-have attributes for a person. This
shows that Freebase is highly incomplete.
Freebase relation types Incompleteness
/people/person/education 0.792
/people/person/employment history 0.923
/people/person/nationality* 0.785
/people/person/parents* 0.988
/people/person/place of birth* 0.938
/people/person/places lived* 0.966
Table 1: The incompleteness of Freebase (* are must-
have attributes for a person).
We further investigate the rate of false negative
matches, as the percentage of entity-pairs that are
not listed in Freebase but one of its mentions gen-
erated by DS does express a relation in the tar-
get set of types. We randomly picked 200 unla-
beled bags5 from each of the two datasets (Riedel
et al, 2010; Surdeanu et al, 2012) generated by DS,
and we manually annotate all relation mentions in
these bags. The result is shown in Table 2, along
with a few examples that indicate a relation holds in
the set of false negative matches (bag-level). Both
datasets have around 10% false negative matches in
the unlabeled set of bags. Taking into considera-
tion that the number of positive bags and unlabeled
bags are highly imbalanced (1:134 and 1:37 in the
Riedel and KBP dataset respectively, before under-
sampling the unlabeled class), the number of false
negative matches are 11 and 4 times the number
of positive bags in Reidel and KBP dataset, respec-
tively. Such a large ratio shows false negatives do
have a significant impact on the learning process.
4 A semi-supervised MIML algorithm
Our goal is to model the bag-level label noise,
caused by the incomplete KB problem, in addition
585% and 95.7% of the bags in the Riedel and KBP datasets
have only one relation mention.
778
Dataset
(train-
ing)
# pos-
itive
bags
# positive :
# unlabeled
% are
false
negatives
# positive
: # false
negative
has human
assessment
Examples of false negative mentions
Riedel 4,700 1:134(BD*) 8.5% 1:11.4 no
(/location/location/contains)... in Brooklyn ?s Williamsburg.
(/people/person/place lived) Cheryl Rogowski , a farmer from
Orange County ...
KBP 183,062 1:37(BD*) 11.5% 1:4 yes
(per:city of birth) Juan Martn Maldacena (born September
10, 1968) is a theoretical physicist born in Buenos Aires
(per:employee of)Dave Matthews, from the ABC News, ...
Table 2: False negative matches on the Riedel (Riedel et al, 2010) and KBP dataset (Surdeanu et al, 2012). All
numbers are on bag (pairs of entities) level. BD* are the numbers before downsampling the negative set to 10% and
5% in Riedel and KBP dataset, respectively.
to modeling the instance-level noise using a 3-layer
MIL or MIML model (e.g., Surdeanu et al (2012)).
We propose a 4-layer model as shown in Figure 1.
The input to the model is a list of n bags with a
vector of binary labels, either Positive (P), or Un-
labled (U) for each relation r. Our model can be
viewed as a semi-supervised6 framework that ex-
tends a state-of-the-art Multi-Instance Multi-Label
(MIML) model (Surdeanu et al, 2012). Since the
input to previous MIML models are bags with per-
relation binary labels of either Positive (P) or Neg-
ative (N), we add a set of latent variables ? which
models the true bag-level labels, to bridge the ob-
served bag labels y and the MIML layers. We con-
sider this as our main contribution to the model. Our
hierarchical model is shown in Figure 1.
Figure 1: Plate diagram of our model.
Let i, j be the index in the bag and mention level,
respectively. Following Surdeanu et al (2012), we
model mention-level extraction p(zrij |xij ;wz) and
multi-instance multi-label aggregation p(?ri |zi;wr?)
in the bottom 3 layers. We define:
? r is a relation label. r?R ? {OTHER}, in
which OTHER denotes no relation expressed.
? yri ?{P,U}: r holds for ith bag or the bag is
unlabeled.
6We use the term semi-supervised because the algorithm
uses unlabeled bags but existing solutions requires bags to be
labeled either positive or negative.
? ?ri ?{P,N}: a hidden variable that denotes
whether r holds for the ith bag.
? ? is an observed constant controlling the total
number of bags whose latent label is positive.
We define the following conditional probabilities:
? p(yri |?ri ) =
?
?
?
?
?
?
?
1/2 if yri = P ? ?ri = P ;
1/2 if yri = U ? ?ri = P ;
1 if yri = U ? ?ri = N ;
0 otherwise ;
It encodes the constraints between true bag-
level labels and the entity pair labels in the KB.
? p(?|?) ? N (
?n
i=1
?
r?R ?(?ri ,P )
n , 1k ) where
?(x, y) = 1 if x = y, 0 otherwise. k is a large
number. ? is the fraction of the bags that are
positive. It is an observed parameter that de-
pends on both the source corpus and the KB
used.
Similar to Surdeanu et al (2012), we also define
the following parameters and conditional probabili-
ties (details are in Surdeanu et al (2012)):
? zij?R ? {OTHER}: a latent variable that de-
notes the relation type of the jth mention in the
ith bag.
? xij is the feature representation of the jth rela-
tion mention in the ith bag. We use the set of
features in Surdeanu et al (2012).
? wz is the weight vector for the multi-class rela-
tion mention-level classifier.
? wr? is the weight vector for the rth binary top-
level aggregation classifier (from mention la-
bels to bag-level prediction). We usew? to rep-
resent w1? ,w2? , ...w
|R|
? .
? p(?ri |zi;wr?) ? Bern(f?(wr? , zi)) where f? is
probability produced by the rth top-level clas-
sifier, from the mention-label level to the bag-
label level.
? p(zrij |xij ;wz) ? Multi(fz(wz,xij)) where fz
779
is probability produced by the mention-level
classifier, from the mentions to the mention-
label level.7
4.1 Training
We use hard Expectation-Maximization (EM) algo-
rithm for training the model. Our objective function
is to maximize log-likelihood:
L(wz,w?) = logp(y, ?|x;wz,w?)
= log
?
?
p(y, ?, ?|x;wz,w?)
Since solving it exactly involves exploring an expo-
nential assignment space for ?, we approximate and
iteratively set ?? = arg? max p(?|y, ?,x;wz,w?)
p(?|y, ?,x;wz,w?) ? p(y, ?, ?|x;wz,w?)
= p(y, ?|?,x)p(?|x;wz,w?)
= p(y|?)p(?|?)p(?|x;wz,w?)
Rewriting in log form:
logp(?|y, ?,x;wz,w?)
= logp(y|?) + logp(?|?) + logp(?|x;wz,w?)
=
n
?
i=1
?
r?R
logp(yri |?ri )? k(
n
?
i=1
?
r?R
?(?ri , P )
n ? ?)
2
+
n
?
i=1
?
r?R
logp(?ri |xi;wz,w?) + const
Algorithm 1 Training (E-step:2-11; M-step:12-15)
1: for i = 1, 2 to T do
2: ?ri ? N for all yri = U and r?R
3: ?ri ? P for all yri = P and r?R
4: I = {< i, r > |?ri = N}; I ? = {< i, r > |?ri = P}
5: for k = 0, 1 to ?n? |I ?| do
6: < i?, r? >= argmax<i,r>?I p(?ri |xi;wz,w?)
7: ?r?i? ? P ; I = I\{< i?, r? >}
8: end for
9: for i = 1, 2 to n do
10: z?i = argmaxzi p(zi|?i,xi;wz,w?)
11: end for
12: w?z = argmaxwz
?n
i=1
?|xi|
j=1 logp(zij |xij ,wz)
13: for all r?R do
14: w
r(?)
? = argmaxwr?
?n
i=1 p(?ri |zi,wr?)
15: end for
16: end for
17: return wz,w?
7All classifiers are implemented with L2-regularized logistic
regression with Stanford CoreNLP package.
In the E-step, we do a greedy search (steps 5-8
in algorithm 1) in all p(?ri |xi;wz,w?) and update ?ri
until the second term is maximized. wz , w? are the
model weights learned from the previous iteration.
After fixed ?, we seek to maximize:
logp(?|xi;wz,w?) =
n
?
i=1
logp(?i|xi;wz,w?)
=
n
?
i=1
log
?
zi
p(?i, zi|xi;wz,w?)
which can be solved with an approxi-
mate solution in Surdeanu et al (2012)
(step 9-11): update zi independently with:
z?i = argmaxzi p(zi|?i,xi;wz,w?). More details
can be found in Surdeanu et al (2012).
In the M-step, we retrain both of the mention-
level and the aggregation level classifiers.
The full EM algorithm is shown in algorithm 1.
4.2 Inference
Inference on a bag xi is trivial. For each mention:
z?ij = argzij?R?{OTHER} max p(zij |xij ,wz)
Followed by the aggregation (directly with w?):
yr(?)i = argyri ?{P,N} max p(y
r
i |zi;wr?)
4.3 Implementation details
We implement our model on top of the
MIML(Surdeanu et al, 2012) code base.8 We
use the same mention-level and aggregate-level
feature sets as Surdeanu et al (2012). We adopt
the same idea of using cross validation for the E
and M steps to avoid overfitting. We initialize our
algorithm by sampling 5% unlabeled examples as
negative, in essence using 1 epoch of MIML to
initialize. Empirically it performs well.
5 Experiments
Data set: We use the KBP (Ji et al, 2011)
dataset9 prepared and publicly released by Surdeanu
et al (2012) for our experiment since it is 1) large
and realistic, 2) publicly available, 3) most im-
portantly, it is the only dataset that has associated
human-labeled ground truth. Any KB held-out eval-
uation without manual assessment will be signif-
icantly affected by KB incompleteness. In KBP
8Available at http://nlp.stanford.edu/software/mimlre.shtml
9Available from Linguistic Data Consortium (LDC).
http://projects.ldc.upenn.edu/kbp/data/
780
Figure 2: Performance on the KBP dataset. The figures on the left, middle and right show MIML, Hoffmann, and
Mintz++ compared to the same MIML-Semi curve, respectively. MIML-Semi is shown in red curves (lighter curves in
black and white) while other algorithms are shown in black curves (darker curves in black and white).
dataset, the training bags are generated by mapping
Wikipedia (http://en.wikipedia.org) infoboxes (after
merging similar types following the KBP 2011 task
definition) into a large unlabeled corpus (consisting
of 1.5M documents from the KBP source corpus and
a complete snapshot of Wikipedia). The KBP shared
task provided 200 query named entities with their as-
sociated slot values (in total several thousand pairs).
We use 40 queries as development dataset (dev), and
the rest (160 queries) as evaluation dataset. We set
? = 0.25 by tuning on the dev set and use it in the
experiments. For a fair comparison, we follow Sur-
deanu et al (2012) and begin by downsampling the
?negative? class to 5%. We also set T=8 and use
the following noisy-or (for ith bag) of mention-level
probability to rank predicted types (r) of pairs and
plot the precision-recall curves for all experiments.
Probi(r) = 1?
?
j
(1? p(zij = r|xij ;wz))
Evaluation: We compare our algorithm (MIML-
semi) to three algorithms: 1) MIML (Surdeanu et
al., 2012), the Multiple-Instance Multiple Label al-
gorithm which labels the bags directly with the KB
(y = ?). 2) MultiR (denoted as Hoffmann) (Hoff-
mann et al, 2011), a Multiple-Instance algorithm
that supports overlapping relations. It also imposes
y = ?. 3) Mintz++ (Surdeanu et al, 2012), a vari-
ant of the single-instance learning algorithm (section
3). The first two are stat-of-the-art Multi-Instance
Multi-Label algorithms. Mintz++ is a strong base-
line (Surdeanu et al, 2012) and an improved ver-
sion of Mintz et al (2009). Figure 2 shows that
our algorithm consistently outperforms all three al-
gorithms at almost all recall levels (with the excep-
tion of a very small region in the PR-curve). This
demonstrates that by treating unla-beled data set dif-
ferently and leveraging the missing positive bags,
MIML-semi is able to learn a more accurate model
for extraction. Although the proposed solution is a
specific algorithm, we believe the idea of treating
unlabeled data differently can be incorporated into
any of these algorithms that only use unlabeled data
as negative examples.
6 Conclusion
We show that the distant-supervision labeling pro-
cess generates a significant number of false nega-
tives because the knowledge base is incomplete. We
proposed an algorithm that learns from only positive
and unlabeled bags. Experimental results demon-
strate its advantage over existing algorithms.
Acknowledgments
Supported in part by the Intelligence Advanced Re-
search Projects Activity (IARPA) via Department of
Interior National Business Center contract number
D11PC20154. The U.S. Government is authorized
to reproduce and distribute reprints for Governmen-
tal purposes notwithstanding any copyright annota-
tion thereon. The views and conclusions contained
herein are those of the authors and should not be
interpreted as necessarily representing the official
policies or endorsements, either expressed or im-
plied, of IARPA, DoI/NBC, or the U.S. Government.
781
References
Razvan Bunescu and Raymond Mooney. 2007. Learning
to extract relations from the web using minimal super-
vision. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics.
Mark Craven and Johan Kumlien. 1999. Constructing bi-
ological knowledge bases by extracting information
from text sources. In Proceedings of the Seventh Inter-
national Conference on Intelligent Systems for Molec-
ular Biology.
Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke
Zettlemoyer, and Daniel S. Weld. 2011. Knowledge-
based weak supervision for information extraction of
overlapping relations. In Proceedings of the Annual
Meeting of the Association for Computational Linguis-
tics.
Heng Ji, Ralph Grishman, and Hoa T. Dang. 2011.
Overview of the TAC 2011 knowledge base popula-
tion track. In Proceedings of the Text Analytics Con-
ference.
Jing Jiang and ChengXiang Zhai. 2007. A systematic ex-
ploration of the feature space for relation extraction. In
Proceedings of HLT-NAACL-2007.
Nanda Kambhatla. 2004. Combining lexical, syntactic,
and semantic features with maximum entropy mod-
els for information extraction. In Proceedings of ACL-
2004.
Scott Miller, Heidi Fox, Lance Ramshaw, and Ralph
Weischedel. 2000. A novel use of statistical parsing
to extract information from text. In Proceedings of
NAACL-2000.
Bonan Min, Shuming Shi, Ralph Grishman and Chin-
Yew Lin. 2012a. Ensemble Semantics for Large-scale
Unsupervised Relation Extraction. In Proceedings of
EMNLP-CoNLL 2012.
Bonan Min, Xiang Li, Ralph Grishman and Ang Sun.
2012b. New York University 2012 System for KBP
Slot Filling. In Proceedings of the Text Analysis Con-
ference (TAC) 2012.
Mike Mintz, Steven Bills, Rion Snow, and Daniel Juraf-
sky. 2009. Distant supervision for relation extraction
without labeled data. In Proceedings of the 47th An-
nual Meeting of the Association for Computational
Linguistics.
Truc Vien T. Nguyen and Alessandro Moschitti. 2011.
End-to-end relation extraction using distant supervi-
sion from external semantic repositories. In Proceed-
ings of the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language Tech-
nologies.
Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions without
labeled text. In Proceedings of the European Confer-
ence on Machine Learning and Knowledge Discovery
in Databases (ECML PKDD 10).
Ang Sun, Ralph Grishman, Wei Xu, and Bonan Min.
2011. New York University 2011 system for KBP slot
filling. In Proceedings of the Text Analytics Confer-
ence.
Mihai Surdeanu, Sonal Gupta, John Bauer, David Mc-
Closky, Angel X. Chang, Valentin I. Spitkovsky, and
Christopher D. Manning. 2011. Stanfords distantly-
supervised slot-filling system. In Proceedings of the
Text Analytics Conference.
Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati,
Christopher D. Manning. 2012. Multi-instance Multi-
label Learning for Relation Extraction. In Proceed-
ings of the 2012 Conference on Empirical Methods in
Natural Language Processing and Natural Language
Learning.
TAC KBP 2011 task definition. 2011. http://nlp
.cs.qc.cuny.edu/kbp/2011/KBP2011 TaskDefinition.pdf
Shingo Takamatsu, Issei Sato, Hiroshi Nakagawa. 2012.
ReducingWrong Labels in Distant Supervision for Re-
lation Extraction. In Proceedings of 50th Annual Meet-
ing of the Association for Computational Linguistics.
Fei Wu and Daniel S. Weld. 2007. Autonomously seman-
tifying wikipedia. In Proceedings of the International
Conference on Information and Knowledge Manage-
ment (CIKM-2007).
Guodong Zhou, Jian Su, Jie Zhang and Min Zhang. 2005.
Exploring various knowledge in relation extraction. In
Proceedings of ACL-2005.
782
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 828?838,
Baltimore, Maryland, USA, June 23-25 2014. c?2014 Association for Computational Linguistics
Medical Relation Extraction with Manifold Models
Chang Wang
IBM T. J. Watson Research Center
Yorktown Heights, New York, 10598
changwangnk@gmail.com
James Fan
IBM T. J. Watson Research Center
Yorktown Heights, New York, 10598
fanj@us.ibm.com
Abstract
In this paper, we present a manifold model
for medical relation extraction. Our model
is built upon a medical corpus containing
80M sentences (11 gigabyte text) and de-
signed to accurately and efficiently detect
the key medical relations that can facilitate
clinical decision making. Our approach
integrates domain specific parsing and typ-
ing systems, and can utilize labeled as well
as unlabeled examples. To provide users
with more flexibility, we also take label
weight into consideration. Effectiveness
of our model is demonstrated both theo-
retically with a proof to show that the so-
lution is a closed-form solution and exper-
imentally with positive results in experi-
ments.
1 Introduction
There exists a vast amount of knowledge sources
and ontologies in the medical domain. Such in-
formation is also growing and changing extremely
quickly, making the information difficult for peo-
ple to read, process and remember. The combi-
nation of recent developments in information ex-
traction and the availability of unparalleled medi-
cal resources thus offers us the unique opportunity
to develop new techniques to help healthcare pro-
fessionals overcome the cognitive challenges they
face in clinical decision making.
Relation extraction plays a key role in informa-
tion extraction. Using question answering as an
example (Wang et al, 2012): in question analy-
sis, the semantic relations between the question
focus and each term in the clue can be used to
identify the weight of each term so that better
search queries can be generated. In candidate an-
swer generation, relations enable the background
knowledge base to be used for potential candidate
answer generation. In candidate answer scoring,
relation-based matching algorithms can go beyond
explicit lexical and syntactic information to detect
implicit semantic relations shared across the ques-
tion and passages.
To construct a medical relation extraction sys-
tem, several challenges have to be addressed:
? The first challenge is how to identify a set of
relations that has sufficient coverage in the
medical domain. To address this issue, we
study a real-world diagnosis related question
set and identify a set of relations that has a
good coverage of the clinical questions.
? The second challenge is how to efficiently de-
tect relations in a large amount of medical
text. The medical corpus underlying our re-
lation extraction system contains 80M sen-
tences (11 gigabytes pure text). To extract
relations from a dataset at this scale, the re-
lation detectors have to be fast. In this paper,
we speed up relation detectors through pars-
ing adaptation and replacing non-linear clas-
sifiers with linear classifiers.
? The third challenge is that the labeled rela-
tion examples are often insufficient due to the
high labeling cost. When we build a na??ve
model to detect relations, the model tends to
overfit for the labeled data. To address this
issue, we develop a manifold model (Belkin
et al, 2006) that encourages examples (in-
cluding both labeled and unlabeled exam-
ples) with similar contents to be assigned
with similar scores. Our model goes beyond
regular regression models in that it applies
constraints to those coefficients, such that the
topology of the given data manifold will be
respected. Computing the optimal weights
in a regression model and preserving mani-
fold topology are conflicting objectives, we
828
present a closed-form solution to ideally bal-
ance these two goals.
The contributions of this paper on medical rela-
tion extraction are three-fold:
? The problem setup is new. There is a
?fundamental? difference between our prob-
lem setup and the conventional setups, like
i2b2 (Uzuner et al, 2011). In i2b2 rela-
tion extraction task, entity mentions are man-
ually labeled, and each mention has 1 of 3
concepts: ?treatment?, ?problem?, and ?test?.
To resemble real-world medical relation ex-
traction challenges where perfect entity men-
tions do not exist, our new setup requires
the entity mentions to be automatically de-
tected. The most well-known tool to detect
medical entity mentions is MetaMap (Aron-
son, 2001), which considers all terms as en-
tities and automatically associates each term
with a number of concepts from UMLS CUI
dictionary (Lindberg et al, 1993) with more
than 2.7 million distinct concepts (compared
to 3 in i2b2). The huge amount of entity
mentions, concepts and noisy concept assign-
ments provide a tough situation that people
have to face in real-world applications.
? From the perspective of relation extraction
applications, we identify ?super relations?-
the key relations that can facilitate clinical
decision making (Table 1). We also present
approaches to collect training data for these
relations with a small amount of labeling ef-
fort.
? From the perspective of relation extraction
methodologies, we present a manifold model
for relation extraction utilizing both labeled
and unlabeled data. Our approach can also
take the label weight into consideration.
The experimental results show that our relation
detectors are fast and outperform the state-of-the-
art approaches on medical relation extraction by a
large margin. We also apply our model to build a
new medical relation knowledge base as a comple-
ment to the existing knowledge bases.
2 Background
2.1 Medical Ontologies and Sources
Medical domain has a huge amount of natural lan-
guage content found in textbooks, encyclopedias,
guidelines, electronic medical records, and many
other sources. It is also growing at an extremely
high speed. Substantial understanding of the med-
ical domain has already been included in the Uni-
fied Medical Language System (UMLS) (Lind-
berg et al, 1993), which includes medical con-
cepts, relations, definitions, etc. The 2012 version
of the UMLS contains information about more
than 2.7 million concepts from over 160 source
vocabularies. Softwares for using this knowledge
also exist: MetaMap (Aronson, 2001) is able to
identify concepts in text. SEMREP (Rindflesch
and Fiszman, 2003) can detect some relations us-
ing hand-crafted rules.
2.2 Relation Extraction
To extract semantic relations from text, three types
of approaches have been applied. Rule-based
methods (Miller et al, 2000) employ a number
of linguistic rules to capture relation patterns.
Feature-based methods (Kambhatla, 2004; Zhao
and Grishman, 2005) transform relation instances
into a large amount of linguistic features like lex-
ical, syntactic and semantic features, and capture
the similarity between these feature vectors. Re-
cent results mainly rely on kernel-based meth-
ods. Many of them focus on using tree kernels to
learn parse tree structure related features (Collins
and Duffy, 2001; Culotta and Sorensen, 2004;
Bunescu and Mooney, 2005).
Other researchers study how different ap-
proaches can be combined to improve the extrac-
tion performance. For example, by combining tree
kernels and convolution string kernels, (Zhang et
al., 2006) achieved the state of the art performance
on ACE data (ACE, 2004). Recently, ?distant su-
pervision? has emerged to be a popular choice for
training relation extractors without using manually
labeled data (Mintz et al, 2009; Jiang, 2009; Chan
and Roth, 2010; Wang et al, 2011; Riedel et al,
2010; Ji et al, 2011; Hoffmann et al, 2011; Sur-
deanu et al, 2012; Takamatsu et al, 2012; Min et
al., 2013).
Various relation extraction approaches have
been adapted to the medical domain, most of
which focus on designing heuristic rules targeted
for diagnosis and integrating the medical ontology
in the existing extraction approaches. Results of
some of these approaches are reported on the i2b2
data (Uzuner et al, 2011).
829
3 Identifying Key Medical Relations
3.1 Super Relations in Medical Domain
The first step in building a relation extraction sys-
tem for medical domain is to identify the relations
that are important for clinical decision making.
Four main clinical tasks that physicians engage
in are discussed in (Demner-Fushman and Lin,
2007). They are Therapy- select treatments to of-
fer a patient, taking consideration of effectiveness,
risk, cost and other factors (prevention is under the
general category of Therapy), Diagnosis (includ-
ing differential diagnosis based on findings and di-
agnostic test), Etiology- identify the factors that
cause the disease and Prognosis- estimate the pa-
tient?s likely course over time. These activities can
be translated into ?search tasks?. For example, the
search for therapy is usually the therapy selection
given a disease.
We did an independent study regarding what
clinical questions usually ask for on a set of 5,000
Doctor Dilemma (DD) questions from the Ameri-
can College of Physicians (ACP). This set includes
questions about diseases, treatments, lab tests, and
general facts1. Our analysis shows that about 15%
of these questions ask for treatments, preventions
or contraindicated drugs for a disease or another
way around, 4% are about diagnosis tests, 6% are
about the causes of a disease, 1% are about the lo-
cations of a disease, 25% are about the symptoms
of a disease, 8% are asking for definitions, 7% are
about guidelines and the remaining 34% questions
either express no relations or some relations that
are not very popular.
Based on the analysis in (Demner-Fushman and
Lin, 2007) and our own results, we decided to fo-
cus on seven key relations in the medical domain,
which are described in Table 1. We call these re-
lations ?super relations?, since they cover most
questions in the DD question set and align well
with the analysis result in (Demner-Fushman and
Lin, 2007).
3.2 Collect Training Data
This section presents how we collect training data
for each relation. The overall procedure is illus-
trated in Figure 1.
1Here?s an example of these questions and its answer:
Question: The syndrome characterized by joint pain, abdom-
inal pain, palpable purpura, and a nephritic sediment. An-
swer: Henoch-Schonlein purpura.
Large Amount of
Noisy Relation
Data
Medical Text
Relation Knowledge in
Medical Domain
Training Data for
Each Relation
For each relation, choose a
small amount of the most
representative examples
Annotation
Unlabeled Data
Labeled Data
Figure 1: Collect Training Data
Our medical corpus has incorporated a set
of medical books/journals2 and MEDLINE ab-
stracts. We also complemented these sources with
Wikipedia articles. In total, the corpus contains
80M sentences (11 gigabyte pure text).
The UMLS 2012 Release contains more than
600 relations and 50M relation instances under
around 15 categories. The RO category (RO
stands for ?has Relationship Other than synony-
mous, narrower, or broader?) is the most inter-
esting one, and covers relations like ?may treat?,
?has finding site?, etc. Each relation has a
certain number of Concept Unique Identifier
(CUI) pairs that are known to bear that rela-
tion. In UMLS, some relation information is
redundant. Firstly, half of these relations are
simply inverse of each other (e.g. the relation
?may treat? and ?may be treated by?). Secondly,
there is a significant amount of redundancy even
among non-inverse relations (e.g. the relation
?has manifestation? and ?disease has finding?).
From UMLS relations, we manually chose a
subset of them that are directly related to the su-
per relations discussed in Section 3.1. The cor-
respondences between them are given in Table 1.
One thing to note is that super relations are more
general than the UMLS relations, and one super
relation might integrate multiple UMLS relations.
Using the CUI pairs in the UMLS relation knowl-
2This is a full list of the books and journals used in
our corpus: ACP-Medical Knowledge Self-Assessment Pro-
gram, EBSCO-Dynamed, EBSCO-Quick Lessons, EBSCO-
EBCS, EBSCO-Clinical Review, Wiley-Essential Evidence
Plus: EBMG Guidelines, Wiley-Essential Evidence Topics,
Wiley-Essential Evidence Plus: EBMG Summaries, Wiley-
POEMs, Wiley-The Breast Journal, New England Journal
of Medicine, Journal Watch, NCCN-CME, NCCN-GUS,
NCCN-Compendium, NCCN-Templates, NCCN-Guidelines
for Patients, NCCN-Physician Guidelines, Merck Manual of
Diagnosis and Therapy, and UpToDate.
830
Table 1: Super relations & their arguments, UMLS sources and noise% in the annotation data
Super Relations Argument 1 Argument 2 UMLS Sources Noise% in Annotation Data
treats disease treatments may treat, treats 16%
prevents disease treatments may prevent 49%
contraindicates disease treatments contraindicated drug 97%
diagnoses disease tests may diagnose 63%
causes disease causes cause of, causative agent of 66%
location of disease locations has finding site 41%
disease has primary anatomic site
symptom of disease symptoms disease has finding 66%
disease may have finding
has manifestation
has definitional manifestation
edge base, we associate each super relation with a
set of CUI pairs.
To collect the training data for each super re-
lation, we need to collect sentences that express
the relation. To achieve this, we parsed all 80M
sentences in our medical corpus, looking for the
sentences containing the terms that are associated
with the CUI pairs in the knowledge base. This
(distant supervision) approach resulted in a huge
amount of sentences that contain the desired rela-
tions, but also brought in a lot of noise in the form
of false positives. For example, we know from
the knowledge base that ?antibiotic drug? may
treat ?Lyme disease?. However the sentence ?This
paper studies the relationship between antibiotic
drug and Lyme disease? contains both terms but
does not express the ?treats? relation.
The most reliable way to clean the training data
is to ask annotators to go through the sentences
and assign the sentences with positive/negative la-
bels. However, it will not work well when we have
millions of sentences to vet. To minimize the hu-
man labeling effort, we ran a K-medoids clustering
on the sentences associated with each super rela-
tion and kept the cluster centers as the most rep-
resentative sentences for annotation. Depending
on the number of the sentences we collected for
each relation, the #clusters was chosen from 3,000
- 6,000. The similarity of two sentences is defined
as the bag-of-words similarity of the dependency
paths connecting arguments. Part of the resulting
data was manually vetted by our annotators, and
the remaining was held as unlabeled data for fur-
ther experiments.
Our relation annotation task is quite straightfor-
ward, since both arguments are given and the de-
cision is a Yes-or-No decision. The noise rate of
each relation (#sentences expressing the relation
/ #sentences) is reported in Table 1 based on the
annotation results. The noise rates differ signifi-
cantly from one relation to another. For ?treats?
relation, only 16% of the sentences are false posi-
tives. For ?contraindicates? relation, the noise rate
is 97%.
To grow the size of the negative training set for
each super relation, we also added a small amount
of the most representative examples (also coming
from K-medoids clustering) from each unrelated
UMLS relation to the training set as negative ex-
amples. This resulted in more than 10,000 extra
negative examples for each relation.
3.3 Parsing and Typing
The most well-known tool to detect medical en-
tity mentions is MetaMap (Aronson, 2001), which
considers all terms as entities and automatically
associates each term with a number of concepts
from UMLS CUI dictionary (Lindberg et al,
1993) with 2.7 million distinct concepts.
The parser used in our system is Medi-
calESG, an adaptation of ESG (English Slot
Grammar) (McCord et al, 2012) to the medical
domain with extensions of medical lexicons inte-
grated in the UMLS 2012 Release. Compared to
MetaMap, MedicalESG is based on the same med-
ical lexicons, 10 times faster and produces very
similar parsing results.
We use the semantic types defined in
UMLS (Lindberg et al, 1993) to categorize
argument types. The UMLS consists of a set
of 133 subject categories, or semantic types,
that provide a consistent categorization of more
than 2M concepts represented in the UMLS
Metathesaurus. Our system assigns each relation
argument with one or more UMLS semantic types
through a two step process. Firstly, we use Med-
icalESG to process the input sentence, identify
segments of text that correspond to concepts in
831
Figure 2: A Parse Tree Example
the UMLS Metathesaurus and associate each of
them with one or more UMLS CUIs (Concept
Unique Identifier). Then we do a CUI lookup in
UMLS to find the corresponding semantic types
for each CUI.
Most relation arguments are associated with
multiple semantic types. For example, the term
?tetracycline hydrochloride? has two types: ?Or-
ganic Chemical? and ?Antibiotic?. Sometimes,
the semantic types are noisy due to ambiguity of
terms. For example, the term ?Hepatitis b? is asso-
ciated with both ?Pharmacologic Substance? and
?Disease or Syndrome? based on UMLS. The rea-
son for this is that people use ?Hepatitis b? to rep-
resent both ?the disease of Hepatitis b? and ?Hep-
atitis b vaccine?, so UMLS assigns both types to it.
This is a concern for relation extraction, since two
types bear opposite meanings. Our current strat-
egy is to integrate all associated types, and rely on
the relation detector trained with the labeled data
to decide how to weight different types based upon
the context.
Here is an illustrative example. Consider the
sentence: ?Antibiotics are the standard therapy
for Lyme disease?: MedicalESG first generates
a dependency parse tree (Figure 2) to represent
grammatical relations between the words in the
sentence, and then associates the words with CUIs.
For example, ?Antibiotics? is associated with CUI
?C0003232? and ?Lyme disease? is associated
with two CUIs: ?C0024198? and ?C0717360?.
CUI lookup will assign ?Antibiotics? with a se-
mantic type ?Antibiotic?, and ?Lyme disease? with
three semantic types: ?Disease or Syndrome?,
?Pharmacologic Substance? and ?Immunologic
Factor?. This sentence expresses a ?treats? rela-
tion between ?Antibiotics? and ?Lyme disease?.
4 Relation Extraction with Manifold
Models
4.1 Motivations
Given a few labeled examples and many unlabeled
examples for a relation, we want to build a re-
lation detector leveraging both labeled and unla-
beled data. Following the manifold regularization
idea (Belkin et al, 2006), our strategy is to learn
a function that assigns a score to each example.
Scores are fit so that examples (both labeled and
unlabeled) with similar content get similar scores,
and scores of labeled examples are close to their
labels. Integration of the unlabeled data can help
solve overfitting problems when the labeled data
is not sufficient.
4.2 Features
We use 8 groups of features to represent each rela-
tion example. These features are commonly used
for relation extraction.
? (1) Semantic types of argument 1, such as
?Antibiotic?.
? (2) Semantic types of argument 2.
? (3) Syntactic features representing the depen-
dency path between two arguments, such as
?subj?, ?pred?, ?mod nprep? and ?objprep?
(between arguments ?antibiotic? and ?lyme
disease?) in Figure 2.
? (4) Features modeling the incoming and out-
going links of both arguments. These fea-
tures are useful to determine if a relation goes
from argument 1 to argument 2 or vice versa.
? (5) Topic features modeling the words in
the dependency path. In the example given
in Figure 2, the dependency path contains
the following words: ?be?, ?standard ther-
apy? and ?for?. These features as well as
the features in (6) are achieved by projecting
the words onto a 100 dimensional LSI topic
space (Deerwester et al, 1990) constructed
from our medical corpus.
? (6) Topic features modeling the words in the
whole sentence.
? (7) Bag-of-words features modeling the de-
pendency path. In (7) and (8), we only con-
sider the words that have occurred in the pos-
itive training data.
832
Notations:
The input dataset X = {x
1
, ? ? ? , x
m
} is repre-
sented as a feature-instance matrix.
The desired label vector Y = {y
1
, ? ? ? , y
l
} repre-
sents the labels of {x
1
, ? ? ? , x
l
}, where l ? m.
W is a weight matrix, where W
i,j
= e
??x
i
?x
j
?
2
models the similarity of x
i
and x
j
.
?x
i
? x
j
? stands for the Euclidean distance be-
tween x
i
and x
j
in the vector space.
D is a diagonal matrix: D
i,i
=
?
j
W
i,j
.
L = D
?0.5
(D ?W )D
?0.5 is called normalized
graph Laplacian matrix.
? is a user defined l ? l diagonal matrix, where
?
i
represents the weight of label y
i
.
A =
(
? 0
0 0
)
is an m?m matrix.
V = [y
1
, ? ? ? y
l
, 0, ? ? ? , 0] is a 1?m matrix.
? is a weight scalar.
()
+ represents pseudo inverse.
Algorithm:
1. Represent each example using features:
X = {x
1
, ? ? ? , x
m
}, where x
i
is the ith ex-
ample.
2. Construct graph Laplacian matrix L
modeling the data manifold.
3. Construct vector V = [y
1
, ? ? ? y
l
, 0, ? ? ? , 0].
4. Compute projection function f for each
relation: f = (X(A+ ?L)XT )+XAV T .
Figure 3: Notations and the Algorithm to Train a
Manifold Model for Relation Extraction
? (8) Bag-of-words features modeling the
whole sentence.
In relation extraction, many recent approaches
use non-linear kernels to get the similarity of two
relation examples. To classify a relation exam-
ple, a lot of dot product computations are required.
This is very time consuming and becomes a bottle-
neck in using relation extraction to facilitate clin-
ical decision making. To speed up the classifier
during the apply time, we decided to use a linear
classifier instead of non-linear classifiers.
We represent all features in a single feature
space. For example, we use a vector of 133 en-
tries (UMLS contains 133 semantic types) to rep-
resent the types of argument 1. If argument 1 is
associated with two types: ?Organic Chemical?
and ?Antibiotic?, we set the two corresponding en-
tries to 1 and all the other entries to 0. Similar ap-
proaches are used to represent the other features.
4.3 The Main Algorithm
The problem we want to solve is formalized as fol-
lows: given a relation dataset X = {x
1
, ? ? ? , x
m
},
and the desired label Y = {y
1
, ? ? ? , y
l
} for
{x
1
, ? ? ? , x
l
}, where l ? m, we want to construct
a mapping function f to project any example x
i
to
a new space, where fTx
i
matches x
i
?s desired la-
bel y
i
. In addition, we also want f to preserve the
manifold topology of the dataset, such that similar
examples (both labeled and unlabeled) get simi-
lar scores. Here, the label is ?+1? for positive ex-
amples, and ?-1? for negative examples. Notations
and the main algorithm to construct f for each re-
lation are given in Figure 3.
4.4 Justification
The solution to the problem defined in Section 4.3
is given by the mapping function f to minimize
the following cost function:
C(f) =
?
i?l
?
i
(f
T
x
i
? y
i
)
2
+ ?
?
i,j
W
i,j
(f
T
x
i
? f
T
x
j
)
2
.
The first term of C(f) is based on labeled ex-
amples, and penalizes the difference between the
mapping result of x
i
and its desired label y
i
. ?
i
is
a user specified parameter, representing the weight
of label y
i
. The second term of C(f) does not take
label information into account. It encourages the
neighborhood relationship (geometry of the man-
ifold) within X to be preserved in the mapping.
When x
i
and x
j
are similar, the corresponding
W
i,j
is big. If f maps x
i
and x
j
to different posi-
tions, f will be penalized. The second term is use-
ful to bound the mapping function f and prevents
overfitting from happening. Here ? is the weight
of the second term. When ? = 0, the model dis-
regards the unlabeled data, and the data manifold
topology is not respected.
Compared to manifold regularization (Belkin
et al, 2006), we do not include the RKHS norm
term. Instead, we associate each labeled example
with an extra weight for label confidence. This
weight is particularly useful when the training
data comes from ?Crowdsourcing?, where we ask
833
multiple workers to complete the same task to
correct errors. In that scenario, weights can be as-
signed to labels based upon annotator agreement.
Theorem 1: f = (X(A + ?L)XT )+XAV T
minimizes the cost function C(f).
Proof:
Given the input X , we want to find the optimal
mapping function f such that C(f) is minimized:
f = argmin
f
C(f).
It can be verified that
?
i?l
?
i
(f
T
x
i
? y
i
)
2
= f
T
XAX
T
f ? 2f
T
XAV
T
+ VAV
T
.
We can also verify that
?
?
i,j
(f
T
x
i
? f
T
x
j
)
2
W
i,j
= ?f
T
XLX
T
f.
So C(f) can be written as
f
T
XAX
T
f ? 2f
T
XAV
T
+ VAV
T
+ ?f
T
XLX
T
f.
Using the Lagrange multiplier trick to differentiate
C(f) with respect to f , we have
2XAX
T
f + 2?XLX
T
f = 2XAV
T
.
This implies that
X(A+ ?L)X
T
f = XAV
T
.
So
f = (X(A+ ?L)X
T
)
+
XAV
T
,
where ?+? represents pseudo inverse.
4.5 Advantages
Our algorithm offers the following advantages:
? The algorithm exploits unlabeled data, which
helps prevent ?overfitting? from happening.
? The algorithm provides users with the flex-
ibility to assign different labels with differ-
ent weights. This feature is useful when the
training data comes from ?crowdsourcing? or
?distant supervision?.
? Different from many approaches in this area,
our algorithm provides a closed-form solu-
tion of the result. The solution is global opti-
mal regarding the cost function C(f).
? The algorithm is computationally efficient at
the apply time (as fast as linear regressions).
5 Experiments
5.1 Cross-Validation Test
We use a cross-validation test3 with the relation
data generated in Section 3.2 to compare our ap-
proaches against the state-of-the-art approaches.
The task is to classify the examples into positive
or negative for each relation. We applied a 5-fold
cross-validation. In each round of validation, we
used 20% of the data for training and 80% for test-
ing. The F
1
scores reported here are the average
of all 5 rounds. We used MedicalESG to process
the input text for all approaches.
5.1.1 Data and Parameters
This dataset includes 7 relations. We do not con-
sider the relation of ?contraindicates? in this test,
since it has too few positive examples. On average,
each relation contains about 800 positive examples
and more than 13,000 negative examples. To elim-
inate the examples that are trivial to classify, we
removed the negative examples that do not bear
the valid argument types. This removed the exam-
ples that can be easily classified by a type filter,
resulting in 3,000 negatives on average per rela-
tion. For each relation, we also collected 5,000
unlabeled examples and put them into two sets:
unlabeled set 1 and 2 (2,500 examples in each set).
No parameter tuning was taken and no relation
specific heuristic rules were applied in all tests. In
all manifold models, ? = 1. In SVM implemen-
tations, the trade-off parameter between training
error and margin was set to 1 for all experiments.
5.1.2 Baseline Approaches
We compare our approaches to three state-of-the-
art approaches including SVM with convolution
tree kernels (Collins and Duffy, 2001), linear re-
gression and SVM with linear kernels (Scho?lkopf
and Smola, 2002). To adapt the tree kernel to med-
ical domain, we followed the approach in (Nguyen
et al, 2009) to take the syntactic structures into
consideration. We also added the argument types
as features to the tree kernel. In the tree kernel im-
plementation, we assigned the tree structure and
the vector corresponding to the argument types
3If we take the perfect entity mentions and the associated
concepts provided by i2b2 (Uzuner et al, 2011) as the input,
our system can directly apply to i2b2 relation extraction data.
However, the i2b2 data has a tough data use agreement. Our
legal team held several rounds of negotiations with the i2b2
data owner and then decided we should not use it due to the
high legal risks. We are not aware of other available medical
relation extraction datasets that fit for our evaluations.
834
Table 2: F
1
Scores from a Five-Fold Cross Validation Experiment
SVM SVM Linear Manifold Manifold Manifold Manifold
Tree Linear Regression Unlabeled Predicted Labels Predicted Labels Unlabeled+Predicted
Kernel Kernel with Weights without Weights Labels with Weights
treats 0.7648 0.7850 0.7267 0.8025 0.8041 0.7884 0.8085
prevents 0.2859 0.3887 0.3922 0.5502 0.5696 0.6349 0.6332
causes 0.3885 0.5024 0.5219 0.5779 0.5088 0.3978 0.5081
location of 0.6113 0.6009 0.4968 0.7275 0.7363 0.6964 0.7454
diagnoses 0.5520 0.4934 0.3202 0.6468 0.6485 0.5720 0.6954
symptom of 0.4398 0.5611 0.5984 0.6347 0.5314 0.4515 0.5968
average 0.5071 0.5553 0.5094 0.6566 0.6331 0.5902 0.6646
with equal weights. The SVM with linear kernels
and the linear regression model used the same fea-
tures as the manifold models.
5.1.3 Settings for the Manifold Models
We tested our manifold model for each relation un-
der three different settings:
(1) Manifold Unlabeled: We combined the la-
beled data and unlabeled set 1 in training. We set
?
i
= 1 for i ? [1, l].
(2) Manifold Predicted Labels: We combined
labeled data and unlabeled set 2 in training. ?
i
=
1 for i ? [1, l]. Different from the previous set-
ting, we gave a label estimation to all the exam-
ples in the unlabeled set 2 based on the noise rate
(Noise%) from Table 1. The label of all unla-
beled examples was set to ?+1? when 100% ? 2 ?
Noise% > 0, or ?-1? otherwise. Two weighting
strategies were applied:
? With Weights: We let label weight ?
i
=
|100%? 2 ?Noise%| for all x
i
coming from
the unlabeled set 2. This setting represents an
empirical rule to estimate the label and con-
fidence of each unlabeled example based on
the sampling result.
? Without Weights: ?
i
is always set to 1.
(3) Manifold UnLabeled+Predicted Labels: a
combination of setting (1) and (2). In this setting,
the data from unlabeled set 1 was used as unla-
beled data and the data from unlabeled set 2 was
used as labeled data (With Weights).
5.1.4 Results
The results are summarized in Table 2.
The tree kernel-based approach and linear re-
gression achieved similar F
1
scores, while linear
SVM made a 5% improvement over them. One
thing to note is that the results from these ap-
proaches vary significantly. The reason for this is
that the labeled training data is not sufficient. So
the approaches that completely depend on the la-
beled data are likely to run into overfitting. Linear
SVM performed better than the other two, since
the large-margin constraint together with the lin-
ear model constraint can alleviate overfitting.
By integrating unlabeled data, the manifold
model under setting (1) made a 15% improvement
over linear regression model on F
1
score, where
the improvement was significant across all rela-
tions.
Under setting (2), the With Weights strategy
achieved a slightly worse F
1
score than the previ-
ous setting but much better result than the baseline
approaches. This tells us that estimating the label
of unlabeled examples based upon the sampling
result is one way to utilize unlabeled data and may
help improve the relation extraction results. The
results also show that the label weight is important
for this setting, since the Without Weights strategy
did not perform very well.
Compared to setting (1) and (2), setting (3)
made use of 2,500 more unlabeled examples,
and achieved the best performance among all ap-
proaches. On one hand, this result shows that
using more unlabeled data can further improve
the result. On the other hand, the insignificant
improvement over (1) and (2) strongly indicates
that how to utilize more unlabeled data to achieve
a significant improvement is non-trivial and de-
serves more attention. To what extensions the un-
labeled data can help the learning process is an
open problem. Generally speaking, when the ex-
isting data is sufficient to characterize the dataset
geometry, adding more unlabeled data will not
help (Singh et al, 2008).
We tested the tree kernel-based approach with-
out integrating the medical types as well. That re-
sulted in very poor performance: the average F
1
score was below 30%. We also applied the rules
used in SEMREP (Rindflesch and Fiszman, 2003)
to this dataset. Since the relations detected by
835
SEMREP rules cannot be perfectly aligned with
super relations, we cannot directly compare the re-
sults. Overall speaking, SEMREP rules are very
conservative and detect very few relations from the
same text.
5.2 Knowledge Base (KB) Construction
The UMLS Metathesaurus (Lindberg et al, 1993)
contains a large amount of manually extracted re-
lation knowledge. Such knowledge is invaluable
for people to collect training data to build new
relation detectors. One downside of using this
KB is its incompleteness. For example, it only
contains the treatments for about 8,000 diseases,
which are far from sufficient. Further, the medical
knowledge is changing extremely quickly, making
people hard to understand it, and update it in the
knowledge base in a timely manner.
To address these challenges, we constructed our
own relation KB as a complement to the UMLS
relation KB. We directly ran our relation detec-
tors (trained with all labeled and unlabeled exam-
ples) on our medical corpus to extract relations.
Then we combined the results and put them in a
new KB. The new KB covers all super relations
and stores the knowledge in the format of (rela-
tion name, argument 1, argument 2, confidence),
where the confidence is computed based on the re-
lation detector confidence score and relation pop-
ularity in the corpus. The most recent version of
our relation KB contains 3.4 million such entries.
We compared this new KB against UMLS KB
using an answer generation task on a set of 742
Doctor Dilemma questions. We first ran our rela-
tion detectors to detect the relation(s) in the ques-
tion clue involving question focus (what the ques-
tion asks for). Then we searched against both KBs
using the relation name and the non-focus argu-
ment for the missing argument. The search re-
sults were then generated as potential answers. We
used the same relations to do KB lookup, so the
results are directly comparable. Since most ques-
tions only have one correct answer, the precision
number is not very important in this experiment.
If we detect multiple relations in the question,
and the same answer is generated from more than
one relations, we sum up all those confidence
scores to make such answers more preferable.
Sometimes, we may generate too many answers
from KBs. For example, if the detected relation
is ?location of? and the non-focus argument is
?skin?, then thousands of answers can be gener-
ated. In this scenario, we sort the answers based
upon the confidence scores and only consider up
to p answers for each question. In our test, we
considered three numbers for p: 20, 50 and 3,000.
From Table 3, we can see that the new KB out-
performs the most popularly-used UMLS KB at
all recall levels by a large margin. This result in-
dicates that the new KB has a much better knowl-
edge coverage. The UMLS KB is manually cre-
ated and thus more precise. In our experiment, the
UMLS KB generated fewer answers than the new
KB. For example, when up to 20 answers were
generated for each question, the UMLS KB gen-
erated around 4,700 answers for the whole ques-
tion set, while the new KB generated about 7,600
answers.
Construction of the new KB cost 16 machines
(using 4?2.8G cores per machine) 8 hours. The
reported computation time is for the whole corpus
with 11G pure text.
Table 3: Knowledge Base Comparison
Recall@20 Recall@50 Recall@3000
Our KB 135/742 182/742 301/742
UMLS KB 42/742 52/742 73/742
6 Conclusions
In this paper, we identify a list of key relations that
can facilitate clinical decision making. We also
present a new manifold model to efficiently extract
these relations from text. Our model is developed
to utilize both labeled and unlabeled examples. It
further provides users with the flexibility to take
label weight into consideration. Effectiveness of
the new model is demonstrated both theoretically
and experimentally. We apply the new model to
construct a relation knowledge base (KB), and use
it as a complement to the existing manually cre-
ated KBs.
Acknowledgments
We thank Siddharth Patwardhan for help on tree
kernels, Sugato Bagchi and Dr. Herbert Chase?s
team for categorizing the Doctor Dilemma ques-
tions. We also thank Anthony Levas, Karen In-
graffea, Mark Mergen, Katherine Modzelewski,
Jonathan Hodax, Matthew Schoenfeld and Adarsh
Thaker for vetting the training data.
836
References
ACE. 2004. The automatic content extraction projects,
http://projects.ldc.upenn.edu/ace/.
A. Aronson. 2001. Effective mapping of biomedical
text to the UMLS metathesaurus: the MetaMap pro-
gram. In Proceedings of the 2001 Annual Sympo-
sium of the American Medical Informatics Associa-
tion.
M. Belkin, P. Niyogi, and V. Sindhwani. 2006.
Manifold regularization: a geometric framework
for learning from labeled and unlabeled exam-
ples. Journal of Machine Learning Research, pages
2399?2434.
R. Bunescu and R. Mooney. 2005. A shortest path de-
pendency kernel for relation extraction. In Proceed-
ings of the Conference on Human Language Tech-
nology and Empirical Methods in Natural Language
Processing.
Y. Chan and D. Roth. 2010. Exploiting background
knowledge for relation extraction. In Proceedings
of the 23rd International Conference on Computa-
tional Linguistics, pages 152?160.
M. Collins and N. Duffy. 2001. Convolution ker-
nels for natural language. In Proceedings of the
Advances in Neural Information Processing Systems
(NIPS), pages 625?632.
A. Culotta and J. Sorensen. 2004. Dependency tree
kernels for relation extraction. In Proceedings of the
42nd Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 423?429.
S. Deerwester, S. T. Dumais, G. W. Furnas, T. K. Lan-
dauer, and R. Harshman. 1990. Indexing by latent
semantic analysis. Journal of the American Society
for Information Science, 41(6):391?407.
D. Demner-Fushman and J. Lin. 2007. Answering
clinical questions with knowledge-based and statis-
tical techniques. Journal of Computational Linguis-
tics, 56:63?103.
R. Hoffmann, C. Zhang, X. Ling, L. Zettlemoyer, and
D. S. Weld. 2011. Knowledge-based weak supervi-
sion for information extraction of overlapping rela-
tions. In Proceedings of the Annual Meeting of the
Association for Computational Linguistics (ACL).
H. Ji, R. Grishman, and H. T. Dang. 2011. Overview
of the TAC 2011 knowledge base population track.
In Proceedings of the Text Analytics Conference.
J. Jiang. 2009. Multi-task transfer learning for weakly-
supervised relation extraction. In Proceedings of
the Joint Conference of the 47th Annual Meeting
of the Association for Computational Linguistics
(ACL) and the 4th International Joint Conference
on Natural Language Processing (IJCNLP), pages
1012?1020.
N. Kambhatla. 2004. Combining lexical, syntactic,
and semantic features with maximum entropy mod-
els for extracting relations. In Proceedings of the
ACL 2004 on Interactive poster and demonstration
sessions.
D. Lindberg, B. Humphreys, and A. McCray. 1993.
The Unified Medical Language System. Methods of
Information in Medicine, 32:281?291.
M. McCord, J. W. Murdock, and B. K. Boguraev.
2012. Deep parsing in Watson. IBM Journal of Re-
search and Development, 56.
S. Miller, H. Fox, L. Ramshaw, and R. Weischedel.
2000. A novel use of statistical parsing to extract in-
formation from text. In Proceedings of the 1st North
American Chapter of the Association for Computa-
tional Linguistics Conference.
B. Min, R. Grishman, L. Wan, C. Wang, and
D. Gondek. 2013. Distant supervision for relation
extraction with an incomplete knowledge base. In
The 2013 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies (NAACL-HLT).
M. Mintz, S. Bills, R. Snow, and D. Jurafsky. 2009.
Distant supervision for relation extraction without
labeled data. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the Association
for Computational Linguistics (ACL) and the 4th In-
ternational Joint Conference on Natural Language
Processing (IJCNLP), pages 1003?1011.
T. Nguyen, A. Moschitti, and G. Riccardi. 2009. Con-
volution kernels on constituent, dependency and se-
quential structures for relation extraction. In Pro-
ceedings of the 2009 Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP).
S. Riedel, L. Yao, and A. McCallum. 2010. Mod-
eling relations and their mentions without labeled
text. In Proceedings of the European Conference
on Machine Learning and Knowledge Discovery in
Databases (ECML PKDD).
T. C. Rindflesch and M. Fiszman. 2003. The inter-
action of domain knowledge and linguistic structure
in natural language processing: interpreting hyper-
nymic propositions in biomedical text. Journal of
Biomedical Informatics, 36:462?477.
B. Scho?lkopf and A. J. Smola. 2002. Learning with
Kernels: Support Vector Machines, Regularization,
Optimization, and Beyond. MIT Press.
A. Singh, R. D. Nowak, and X. Zhu. 2008. Unlabeled
data: now it helps, now it doesnot. In Proceedings
of the Advances in Neural Information Processing
Systems (NIPS).
M. Surdeanu, J. Tibshirani, R. Nallapati, and C. D.
Manning. 2012. Multi-instance multilabel learning
for relation extraction. In Proceedings of the 2012
837
Conference on Empirical Methods in Natural Lan-
guage Processing and Natural Language Learning
(EMNLP).
S. Takamatsu, I. Sato, and H. Nakagawa. 2012. Re-
ducing wrong labels in distant supervision for rela-
tion extraction. In Proceedings of the Annual Meet-
ing of the Association for Computational Linguistics
(ACL).
O?. Uzuner, B. R. South, S. Shen, and S. L. DuVall.
2011. 2010 i2b2/VA challenge on concepts, asser-
tions, and relations in clinical text. Journal of Amer-
ican Medical Informatics Association, 18:552?556.
C. Wang, J. Fan, A. Kalyanpur, and D. Gondek. 2011.
Relation extraction with relation topics. In Proceed-
ings of the 2011 Conference on Empirical Methods
in Natural Language Processing (EMNLP).
C. Wang, A. Kalyanpur, J. Fan, B. Boguraev, and
D. Gondek. 2012. Relation extraction and scoring
in DeepQA. IBM Journal of Research and Develop-
ment, 56.
M. Zhang, J. Zhang, J. Su, and G. Zhou. 2006. A com-
posite kernel to extract relations between entities
with both flat and structured features. In Proceed-
ings of the 21st International Conference on Compu-
tational Linguistics and 44th Annual Meeting of the
Association for Computational Linguistics (ACL).
S. Zhao and R. Grishman. 2005. Extracting relations
with integrated information using kernel methods.
In Proceedings of the 43rd Annual Meeting of the
Association for Computational Linguistics (ACL),
pages 419?426.
838
