Proceedings of NAACL HLT 2009: Short Papers, pages 89?92,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Automatic Agenda Graph Construction from Human-Human Dialogs 
using Clustering Method 
 
Cheongjae Lee, Sangkeun Jung, Kyungduk Kim, Gary Geunbae Lee 
Department of Computer Science and Engineering 
Pohang University of Science and Technology 
Pohang, South Korea 
{lcj80,hugman,getta,gblee}@postech.ac.kr 
 
  
 
Abstract 
Various knowledge sources are used for spo-
ken dialog systems such as task model, do-
main model, and agenda. An agenda graph is 
one of the knowledge sources for a dialog 
management to reflect a discourse structure. 
This paper proposes a clustering and linking 
method to automatically construct an agenda 
graph from human-human dialogs. Prelimi-
nary evaluation shows our approach would be 
helpful to reduce human efforts in designing 
prior knowledge. 
1 Introduction 
Data-driven approaches have been long applied for spo-
ken language technologies. Although a data-driven ap-
proach requires time-consuming data annotation, the 
training is done automatically and requires little human 
supervision. These advantages have motivated the de-
velopment of data-driven dialog modelings (Williams 
and Young, 2007, Lee et al, 2009). In general, the data-
driven approaches are more robust and portable than 
traditional knowledge-based approaches. However, var-
ious knowledge sources are still used in many spoken 
dialog systems that have been developed recently. These 
knowledge sources contain task model, domain model, 
and agenda which are powerful representation to reflect 
the hierarchy of natural dialog control. In the spoken 
dialog systems, these are manually designed for various 
purposes including dialog modeling (Bohus and Rud-
nicky, 2003, Lee et al, 2008), search space reduction 
(Young et al, 2007), domain knowledge (Roy and Sub-
ramaniam, 2006), and user simulation (Schatzmann et 
al., 2007). 
We have proposed an example-based dialog modeling 
(EBDM) framework using an agenda graph as prior 
knowledge (Lee et al, 2008). This is one of the data-
driven dialog modeling techniques and the next system 
action is determined by selecting the most similar dialog 
examples in dialog example database. In the EBDM 
framework for task-oriented dialogs, agenda graph is 
manually designed to address two aspects of a dialog 
management: (1) Keeping track of the dialog state with 
a view to ensuring steady progress towards task comple-
tion, and (2) Supporting n-best recognition hypotheses 
to improve the robustness of dialog manager. However, 
manually building such graphs for various applications 
may be labor intensive and time consuming. Thus, we 
have tried to investigate how to build this graph auto-
matically. Consequently, we sought to solve the prob-
lem by automatically building the agenda graph using 
clustering method from an annotated dialog corpus. 
2 Related Work  
Clustering techniques have been widely used to build 
prior knowledge for spoken dialog systems. One of 
them is automatic construction of domain model (or 
topic structure) which is one of the important resources 
to handle user?s queries in call centers. Traditional ap-
proach to building domain models is that the analysts 
manually generate a domain model through inspection 
of the call records. However, it has recently been pro-
posed to use an unsupervised technique to generate do-
main models automatically from call transcriptions (Roy 
and Subramaniam, 2006). In addition, there has been 
research on how to automatically learn models of task-
oriented discourse structure using dialog act and task 
information (Bangalore et al, 2006). Discourse struc-
ture is necessary for dialog state-specific speech recog-
nition and language understanding to improve the 
performance by predicting the next possible dialog 
states. In addition, the discourse structure is essential to 
determine whether the current utterance in the dialog is 
part of the current subtask or starts a new task. 
89
More recently, it has been proposed stochastic dialog 
management such as the framework of a partially ob-
servable Markov decision process (POMDP). This 
framework is statistically data-driven and theoretically 
principled dialog modeling. However, detailed dialog 
states in the master space should be clustered into gen-
eral dialog states in summary space to scale up 
POMDP-based dialog management for practical appli-
cations (Williams and Young, 2007). To address this 
problem, an unsupervised automatic clustering of dialog 
states has been introduced and investigated in POMDP-
based dialog manager (Lefevre and Mori, 2007).  
In this paper, we are also interested in exploring me-
thods that would automatically construct the agenda 
graph as prior knowledge for the EBDM framework. 
3 Agenda Graph 
In this section, we begin with a brief overview of 
EBDM framework and agenda graph. The basic idea of 
the EBDM is that the next system action is predicted by 
finding semantically similar user utterance in the dialog 
state space. The agenda graph was adapted to take into 
account the robustness problem for practical applica-
tions. Agenda graph G is a simply a way of encoding 
the domain-specific dialog control to complete the task. 
G is represented by a directed acyclic graph (DAG) 
(Figure 1). An agenda is one of the subtask flows, which 
is a possible path from root node to terminal node. G is 
composed of nodes (v) which correspond to possible 
intermediate steps in the process of completing the spe-
cified task, and edges (e) which connect nodes. In other 
words, v corresponds to dialog state to achieve domain-
specific subtask in its expected agenda. Each node in-
cludes three different components: (1) A precondition 
that must be true before the subtask is executed; (2) A 
description of the node that includes its label and iden-
tifier; and (3) Links to nodes that will be executed at the 
subsequent turn. In this system, this graph is used to 
rescore n-best ASR hypotheses and to interpret the dis-
course state such as new task, next task, and new sub-
task based on topological position on the graph. In the 
agenda graph G, each node holds a set of relevant dialog 
examples which may appear in the corresponding dialog 
states when a precondition of the node is true. To de-
termine the next system action, the dialog manager first 
generates possible candidate nodes with n-best hypo-
theses by using a discourse interpretation algorithm 
based on the agenda graph, and then selects the focus 
node which is the most likely dialog state given the pre-
vious dialog state. Finally the best example in the focus 
node is selected to determine appropriate system action. 
Human efforts are required to manually design the 
agenda graph to integrate it into the EBDM framework. 
However, it is difficult to define all possible precondi-
tion rules and to assign the transition probabilities to 
each link based only on the discretion of the system 
developer. To solve these problems, we tried to con-
struct the agenda graph from the annotated dialog cor-
pus using clustering technique. 
4 Clustering and Linking 
4.1 Node Clustering 
Each precondition has been manually defined to map 
relevant dialog examples into each node. To avoid this, 
the dialog examples are automatically grouped into the 
closest cluster (or node) by a node clustering. In this 
section, we explain a feature extraction and clustering 
method for constructing the agenda graph. 
4.1.1 Feature Extraction 
Each dialog example should be converted into a feature 
vector for a node clustering. To represent the feature 
vectors, we first extract all n-grams which occur more 
frequently than a threshold and do not contain any stop 
word as word-level features. We also extract utterance-
level and discourse-level features from the annotated 
dialog corpus to reflect semantic and contextual infor-
mation because a dialog state can be characterized using 
semantic and contextual information derivable from the 
annotations. The utterance is thus characterized by the 
set of various features as shown in Table 1. 
 
Figure 1: Example of an agenda graph for building 
guidance domain 
Feature Types Features #Size 
Word-level  
features 
unigram 175 
bigram 573 
trigram 1034 
Utterance-level  
features 
dialog act (DA) 9 
main goal (MG) 16 
slot filling status 8 
system act (SA) 26 
Discourse-level  
features 
previous DA 10 
previous MG 17 
previous SA 27 
Table 1: List of feature sets 
90
For a set of N dialog examples X={xi|i=1,..,N}, the 
binary feature vectors are represented by using a set of 
features from the dialog corpus. To calculate the dis-
tance of two feature vectors, we used a cosine measure 
as a binary vector distance measure: 
ji
ji
ji xx
xxxxd ?
??? )(1),(
 
where xi and xj denoted two feature vectors. However, 
each feature vector contains small number of non-zero 
terms (<20 features) compared to the feature space 
(>2000 features). Therefore, most pairs of utterances 
share no common feature, and their distance is close to 
1.0. To address this sparseness problem, the distance 
between two utterances can be computed by checking 
only the non-zero terms of corresponding feature vec-
tors (Liu, 2005). 
4.1.2 Clustering 
After extracting feature vectors from the dialog corpus, 
we used K-means clustering algorithm which is the sim-
plest and most commonly used algorithm employing a 
squared error criterion. At the initialization step, one 
cluster mean is randomly selected in the data set and k-1 
means are iteratively assigned by selecting the farthest 
point from pre-selected centers as the following equa-
tion:  
? ???
???
?? ??
??
1
1
,maxarg k
i
iXxk
uxdu
 
where each cluster ck is represented as a mean vector uk. 
At the assignment step, each example is assigned to the 
nearest cluster 
tc? by minimizing the distance of cluster 
mean uk and dialog example xt. 
? ?? ?tkKkt xudc ,minarg? 1 ???
 
The responsibilities rkt of each cluster ck are calcu-
lated for each example xt as the following rule: 
? ?? ?? ?? ?? ?? ??? l tl tkkt xud
xudr ,exp
,exp
?
?
 
where ? is the stiffness and usually assigned to 1. 
During the update step, the means are recomputed us-
ing the current cluster membership by reflecting their 
responsibilities: 
?
??
t kt
t tkt
k r
xru
 
4.2 Node Linking 
From the node clustering step, node vk for cluster ck is 
obtained from the dialog corpus and each node contains 
similar dialog examples by the node clustering algo-
rithm. Next, at the node linking step, each node should 
be connected with an appropriate transition probability 
to build the agenda graph which is a DAG (Figure 2). 
This linking information can come from the dialog cor-
pus because the task-oriented dialogs consist of sequen-
tial utterances to complete the tasks. Using sequences of 
dialog examples obtained with the dialog corpus, rela-
tive frequencies of all outgoing edges are calculated to 
weight directed edges: 
)(
)(),(
i
ji
ji vxn
vvxnvvf ?
???
 
where ? ?ivxn ?  represents the number of dialog exam-
ples in vi and ? ?ji vvxn ??  denotes the number of di-
alog examples having directed edge from vi to vj. Next 
some edges are pruned when the weight falls below a 
pre-defined threshold ?, and the cycle paths are removed 
by deleting minimal edge in cycle paths through a 
depth-first traversal. Finally the transition probability 
can be estimated by normalizing relative frequencies 
with the remained edges. 
?? l li jiij vvf
vvfvvp ),(
),()|(
 
5 Experiment & Result 
 A spoken dialog system for intelligent robot was devel-
oped to provide information about building (e.g., room 
number, room name, room type) and people (e.g., name, 
phone number, e-mail address).  If the user selects a 
specific room to visit, then the robot takes the user to 
the desired room. For this system, we collect a human-
human dialog corpus of about 880 user utterances from 
214 dialogs which were based on a set of pre-defined 10 
subjects relating to building guidance task. Then, we 
designed an agenda graph and integrated it into the 
EBDM framework. In addition, a simulated environ-
ment with a user simulator and an ASR channel (Jung et 
 
Figure 2: Node Linking Algorithm 
91
al., 2008) was developed to evaluate our approach by 
simulating a realistic scenario. 
First we measured the clustering performance to veri-
fy our approach for constructing the agenda graph.  We 
used the manually clustered examples by a set of pre-
condition rules as the reference clusters. Table 2 shows 
error rates when different feature sets are used for K-
means clustering in which K is equal to 10 because a 
hand-crafted graph included 10 nodes. The error rate 
was significantly reduced when using all feature sets. 
 
We also evaluated the dialog system performance 
with the agenda graphs which are manually (HC-AG) or 
automatically designed (AC-AG). We also used 10-best 
recognition hypotheses with 20% word error rate 
(WER) for a dialog management and 1000 simulated 
dialogs for an automatic evaluation. In this result, al-
though the system with HC-AG slightly outperforms the 
system with AC-AG, we believe that AC-AG can be 
helpful to manage task-oriented dialogs with less human 
costs for designing the hand-crafted agenda graph. 
 
6 Conclusion & Discussion  
In this paper, we address the problem of automatic 
knowledge acquisition of agenda graph to structure 
task-oriented dialogs. We view this problem as a first 
step in clustering the dialog states, and then in linking 
between each cluster based on the dialog corpus. The 
experiment results show that our approach can be appli-
cable to easily build the agenda graph for prior know-
ledge. 
There are several possible subjects for further re-
search on our approach. We can improve the clustering 
performance by using a distance metric learning algo-
rithm to consider the correlation between features. We 
can also discover hidden links in the graph by exploring 
new dialog flows with random walks. 
Acknowledgement 
This research was supported by the MKE (Ministry of 
Knowledge Economy), Korea, under the ITRC (Infor-
mation Technology Research Center) support program 
supervised by the IITA (Institute for Information Tech-
nology Advancement) (IITA-2009-C1090-0902-0045). 
References  
Bangalore, S., Fabbrizio, G.D. and Stent, A. 2006. Learning 
the structure of task-driven human-human dialogs. Proc. of 
the Association for Computational Linguistics, 201-208. 
Bohus, B. and Rudnicky, A. 2003. RavenClaw: Dialog Man-
agement Using Hierarchical Task Decomposition and an 
Expectation Agenda. Proc. of the European Conference on 
Speech, Communication and Technology, 597-600. 
Jung, S., Lee, C., Kim, K. and Lee, G.G. 2008. An Integrated 
Dialog Simulation Technique for Evaluating Spoken Dialog 
Systems. Proc. of Workshop on Speech Processing for Safe-
ty Critical Translation and Pervasive Applications, Interna-
tional Conference on Computational Linguistics, 9-16. 
Lee, C., Jung, S. and Lee, G.G. 2008. Robust Dialog Man-
agement with N-best Hypotheses using Dialog Examples 
and Agenda. Proc. of the Association for Computational 
Linguistics, 630-637. 
Lee, C., Jung, S., Kim, S. and Lee, G.G. 2009. Example-based 
Dialog Modeling for Practical Multi-domain Dialog System. 
Speech Communication, 51(5):466-484. 
Lefevre, F. and Mori, R.D. 2007. Unsupervised State Cluster-
ing for Stochastic Dialog Management. Proc. of the IEEE 
Workshop on Automatic Speech Recognition and Under-
standing, 550-555. 
Liu, Z. 2005. An Efficient Algorithm for Clustering Short 
Spoken Utterances. Proc. of the IEEE International Confe-
rence on Acoustics, Speech and Signal Processing, 593-596. 
Roy, S. and Subramaniam, L.V. 2006. Automatic generation 
of domain models for call centers from noisy transcriptions. 
Proc. of the Association for Computational Linguistics, 737-
744. 
Schatzmann, J., Thomson, B., Weilhammer, K., Ye, H. and 
Young, S. 2007. Agenda-based User Simulation for Boot-
strapping a POMDP Dialogue System. Proc. of the Human 
Language Technology/North American Chapter of the Asso-
ciation for Computational Linguistics, 149-152. 
Williams, J.D. and Young, S. 2007. Partially observable Mar-
kov decision processes for spoken dialog systems. Comput-
er Speech and Language, 21:393-422. 
Young, S., Schatzmann, J., Weilhammer, K. and Ye, H. 2007. 
The Hidden Information State Approach to Dialog Man-
agement. Proc. of the IEEE International Conference on 
Acoustics, Speech and Signal Processing, 149-152. 
 
System TCR (%) AvgUserTurn 
Using HC-AG 92.96 4.41 
Using AC-AG 89.95 4.39 
Table 3: Task completion rate (TCR) and average 
user turn (AvgUserTurn) (WER=20%) 
Feature sets Error rate (%) 
Word-level features 46.51 
+Utterance-level features 34.63 
+Discourse-level features 31.20 
Table 2: Error rates for node clustering (K=10) 
 
92
Proceedings of ACL-08: HLT, pages 630?637,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Robust Dialog Management with N-best Hypotheses Using Dialog Examples
and Agenda
Cheongjae Lee, Sangkeun Jung and Gary Geunbae Lee
Pohang University of Science and Technology
Department of Computer Science and Engineering
Pohang, Republic of Korea
{lcj80,hugman,gblee}@postech.ac.kr
Abstract
This work presents an agenda-based approach
to improve the robustness of the dialog man-
ager by using dialog examples and n-best
recognition hypotheses. This approach sup-
ports n-best hypotheses in the dialog man-
ager and keeps track of the dialog state us-
ing a discourse interpretation algorithm with
the agenda graph and focus stack. Given
the agenda graph and n-best hypotheses, the
system can predict the next system actions
to maximize multi-level score functions. To
evaluate the proposed method, a spoken dia-
log system for a building guidance robot was
developed. Preliminary evaluation shows this
approach would be effective to improve the ro-
bustness of example-based dialog modeling.
1 Introduction
Development of spoken dialog systems involves hu-
man language technologies which must cooperate
to answer user queries. Since the performance in
human language technologies such as Automatic
Speech Recognition (ASR) and Natural Language
Understanding (NLU)1 have been improved, this ad-
vance has made it possible to develop spoken dialog
systems for many different application domains.
Nevertheless, there are major problems for practi-
cal spoken dialog systems. One of them which must
be considered by the Dialog Manager (DM) is the
error propagation from ASR and NLU modules. In
1Through this paper, we will use the term natural language
to include both spoken language and written language
general, errors in spoken dialog systems are preva-
lent due to errors in speech recognition or language
understanding. These errors can cause the dialog
system to misunderstand a user and in turn lead to
an inappropriate response. To avoid these errors, a
basic solution is to improve the accuracy and robust-
ness of the recognition and understanding processes.
However, it has been impossible to develop perfect
ASR and NLU modules because of noisy environ-
ments and unexpected input. Therefore, the devel-
opment of robust dialog management has also been
one of the most important goals in research on prac-
tical spoken dialog systems.
In the dialog manager, a popular method to deal
with these errors is to adopt dialog mechanisms for
detecting and repairing potential errors at the con-
versational level (McTear et al, 2005; Torres et al,
2005; Lee et al, 2007). In human-computer com-
munication, the goal of error recovery strategy is
to maximize the user?s satisfaction of using the sys-
tem by guiding for the repair of the wrong informa-
tion by human-computer interaction. On the other
hand, there are different approaches to improve the
robustness of dialog management using n-best hy-
potheses. Rather than Markov Decision Processes
(MDPs), partially observable MDPs (POMDPs) po-
tentially provide a much more powerful framework
for robust dialog modeling since they consider n-
best hypotheses to estimate the distribution of the
belief state (Williams and Young, 2007).
In recent, we proposed another data-driven ap-
proach for the dialog modeling called Example-
based Dialog Modeling (EBDM) (Lee et al, 2006a).
However, difficulties occur when attempting to de-
630
ploy EBDM in practical spoken dialog systems in
which ASR and NLU errors are frequent. Thus,
this paper proposes a new method to improve the ro-
bustness of the EBDM framework using an agenda-
based approach and n-best recognition hypotheses.
We consider a domain-specific agenda to estimate
the best dialog state and example because, in task-
oriented systems, a current dialog state is highly cor-
related to the previous dialog state. We have also
used the example-based error recovery approach to
handle exceptional cases due to noisy input or unex-
pected focus shift.
This paper is organized as follows. Previous re-
lated work is described in Section 2, followed by the
methodology and problems of the example-based di-
alog modeling in Section 3. An agenda-based ap-
proach for heuristics is presented in Section 4. Fol-
lowing that, we explain greedy selection with n-best
hypotheses in Section 5. Section 6 describes the
error recovery strategy to handle unexpected cases.
Then, Section 7 provides the experimental results of
a real user evaluation to verify our approach. Finally,
we draw conclusions and make suggestions for fu-
ture work in Section 8.
2 Related Work
In many spoken dialog systems that have been devel-
oped recently, various knowledge sources are used.
One of the knowledge sources, which are usually
application-dependent, is an agenda or task model.
These are powerful representations for segmenting
large tasks into more reasonable subtasks (Rich and
Sidner, 1998; Bohus and Rudnicky, 2003; Young et
al., 2007). These are manually designed for various
purposes including dialog modeling, search space
reduction, domain knowledge, and user simulation.
In Collagen (Rich and Sidner, 1998), a plan tree,
which is an approximate representation of a partial
SharedPlan, is composed of alternating act and plan
recipe nodes for internal discourse state representa-
tion and discourse interpretation.
In addition, Bohus and Rudnicky (2003) have pre-
sented a RavenClaw dialog management which is
an agenda-based architecture using hierarchical task
decomposition and an expectation agenda. For mod-
eling dialog, the domain-specific dialog control is
represented in the Dialog Task Specification layer
using a tree of dialog agents, with each agent han-
dling a certain subtask of the dialog task.
Recently, the problem of a large state space in
POMDP framework has been solved by grouping
states into partitions using user goal trees and on-
tology rules as heuristics (Young et al, 2007).
In this paper, we are interested in exploring algo-
rithms that would integrate this knowledge source
for users to achieve domain-specific goals. We used
an agenda graph whose hierarchy reflects the natu-
ral order of dialog control. This graph is used to both
keep track of the dialog state and to select the best
example using multiple recognition hypotheses for
augmenting previous EBDM framework.
3 Example-based Dialog Modeling
Our approach is implemented based on Example-
Based Dialog Modeling (EBDM) which is one of
generic dialog modelings. We begin with a brief
overview of the EBDM framework in this sec-
tion. EBDM was inspired by Example-Based Ma-
chine Translation (EBMT) (Nagao, 1984), a trans-
lation system in which the source sentence can be
translated using similar example fragments within a
large parallel corpus, without knowledge of the lan-
guage?s structure. The idea of EBMT can be ex-
tended to determine the next system actions by find-
ing similar dialog examples within the dialog cor-
pus. The system action can be predicted by finding
semantically similar user utterances with the dialog
state. The dialog state is defined as the set of relevant
internal variables that affect the next system action.
EBDM needs to automatically construct an example
database from the dialog corpus. Dialog Example
DataBase (DEDB) is semantically indexed to gen-
eralize the data in which the indexing keys can be
determined according to state variables chosen by
a system designer for domain-specific applications
(Figure 1). Each turn pair (user turn, system turn) in
the dialog corpus is mapped to semantic instances in
the DEDB. The index constraints represent the state
variables which are domain-independent attributes.
To determine the next system action, there are three
processes in the EBDM framework as follows:
? Query Generation: The dialog manager
makes Structured Query Language (SQL)
631
Figure 1: Indexing scheme for dialog example database on building guidance domain
statement using discourse history and NLU re-
sults.
? Example Search: The dialog manager
searches for semantically similar dialog exam-
ples in the DEDB given the current dialog state.
If no example is retrieved, some state variables
can be ignored by relaxing particular variables
according to the level of importance given the
dialog?s genre and domain.
? Example Selection: The dialog manager se-
lects the best example to maximize the ut-
terance similarity measure based on lexico-
semantic similarity and discourse history simi-
larity.
Figure 2 illustrates the overall strategy of EBDM
framework for spoken dialog systems. The EBDM
framework is a simple and powerful approach
to rapidly develop natural language interfaces for
multi-domain dialog processing (Lee et al, 2006b).
However, in the context of spoken dialog system for
domain-specific tasks, this framework must solve
two problems: (1) Keeping track of the dialog state
with a view to ensuring steady progress towards task
completion, (2) Supporting n-best recognition hy-
potheses to improve the robustness of dialog man-
ager. Consequently, we sought to solve these prob-
Figure 2: Strategy of the Example-Based Dialog
Modeling (EBDM) framework.
lems by integrating the agenda graph as a heuristic
which reflects the natural hierarchy and order of sub-
tasks needed to complete the task.
4 Agenda Graph
In this paper, agenda graph G is simply a way of
encoding the domain-specific dialog control to com-
plete the task. An agenda is one of the subtask flows,
which are possible paths from root node to terminal
node. G is composed of nodes (v) which correspond
to possible intermediate steps in the process of com-
pleting the specified task, and edges (e) which con-
632
Figure 3: Example of an agenda graph for a building
guidance.
nect nodes. In other words, v corresponds to user
goal state to achieve domain-specific subtask in its
expected agenda. Each node includes three different
components: (1) A precondition that must be true
before the subtask is executed; (2) A description of
the node that includes its label and identifier; and
(3) Links to nodes that will be executed at the subse-
quent turn. For every edge eij = (vi, vj), we defined
a transition probability based on prior knowledge of
dialog flows. This probability can be assigned based
on empirical analysis of human-computer conversa-
tions, assuming that the users behave in consistent,
goal-directed ways. Alternatively, it can be assigned
manually at the discretion of the system developer
to control the dialog flow. This heuristic has ad-
vantages for practical spoken dialog system because
a key condition for successful task-oriented dialog
system is that the user and system know which task
or subtask is currently being executed. To exem-
plify, Figure 3 illustrates part of the agenda graph for
PHOPE, a building guidance robot using the spoken
dialog system. In Figure 3, G is represented by a
Directed Acyclic Graph (DAG), where each link in
the graph reflects a transition between one user goal
state and the next. The set of paths in G represent
an agenda designed by the system developer. We
adapted DAG representation because it is more in-
tuitive and flexible than hierarchical tree represen-
tation. The syntax for graph representation in our
system is described by an XML schema (Figure 4).
4.1 Mapping Examples to Nodes
In the agenda graph G, each node v should hold
relevant dialog examples corresponding to user goal
states. Therefore, the dialog examples in DEDB are
Figure 4: XML description for the agenda graph
mapped to a user goal state when a precondition of
the node is true. Initially, the root node of the DAG is
the starting state, where there is no dialog example.
Then, the attributes of each dialog example are ex-
amined via the preconditions of each user goal node
by breadth-first traversal. If the precondition is true,
the node holds relevant that may appear in the user?s
goal state. The method of selecting the best of these
examples will be described in 5.
4.2 Discourse Interpretation
Inspired by Collagen (Rich and Sidner, 1998; Lesh
et al, 2001), we investigated a discourse interpre-
tation algorithm to consider how the current user?s
goal can contribute to the current agenda in a focus
stack according to Lochbaum?s discourse interpreta-
tion algorithm (Lochbaum, 1998). The focus stack
takes into account the discourse structure by keeping
track of discourse states. In our system, the focus
stack is a set of user goal nodes which lead to com-
pletion of the subtask. The top on the focus stack is
the previous node in this set. The focus stack is up-
dated after every utterance. To interpret the type of
the discourse state, this breaks down into five main
cases of possible current node for an observed user?s
goal:
? NEW TASK: Starting a new task to complete a
new agenda (Child of the root).
? NEW SUB TASK: Starting a new subtask to
partially shift focus (A different child of the
parent).
633
? NEXT TASK: Working on the next subtask con-
tributing to current agenda (Its child node).
? CURRENT TASK: Repeating or modifying the
observed goal on the current subtask (Current
node).
? PARENT TASK: Modifying the observation on
the previous subtask (Parent node).
Nodes in parentheses denote the topological position
of the current node relative to the top node on the
focus stack. If NEXT TASK is selected, the current
node is pushed to the focus stack. NEXT TASK cov-
ers totally focused behavior, i.e., when there are no
unexpected focus shifts. This occurs when the cur-
rent user utterance is highly correlated to the pre-
vious system utterance. The remaining four cases
cover various types of discourse state. For example,
NEW SUB TASK involves starting a new subtask to
partially shift focus, thereby popping the previous
goal off the focus stack and pushing a new user goal
for the new subtask. NEW TASK, which is placed
on the node linked to root node, involves starting a
new task to complete a new agenda. Therefore, a di-
alog is re-started and the current node is pushed onto
the focus stack with the current user goal as its first
element.
If none of the above cases holds, the discourse in-
terpretation concludes that the current input should
be rejected because we expect user utterances to be
correlated to the previous turn in a task-oriented do-
main. Therefore, this interpretation does not con-
tribute to the current agenda on the focus stack due
to ASR and NLU errors that are due to noisy envi-
ronments and unexpected input. These cases can be
handled by using an error recovery strategy in Sec-
tion 6.
Figure 5 shows some examples of pseudo-codes
used in the discourse interpretation algorithm to
select the best node among possible next nodes.
S,H ,and G denote the focus stack, hypothesis, and
agenda graph, respectively. The INTERPRET al-
gorithm is initially called to interpret the current dis-
course state. Furthermore, the essence of a discourse
interpretation algorithm is to find candidate nodes of
possible next subtask for an observed user goal, ex-
pressed in the definition of GENERATE. The SE-
LECT algorithm selects the best node to maximize
Figure 5: Pseudo-codes for the discourse interpreta-
tion algorithm
the score function based on current input and dis-
course structure given the focus stack. The details
of how the score of candidate nodes are calculated
are explained in Section 5.
5 Greedy Selection with n-best Hypotheses
Many speech recognizers can generate a list of plau-
sible hypotheses (n-best list) but output only the
most probable one. Examination of the n-best list
reveals that the best hypothesis, the one with the
lowest word error rate, is not always in top-1 posi-
tion but sometimes in the lower rank of the n-best
list. Therefore, we need to select the hypothesis
that maximizes the scoring function among a set of
n-best hypotheses of each utterance. The role of
agenda graph is for a heuristic to score the discourse
state to successfully complete the task given the fo-
cus stack.
The current system depends on a greedy policy
which is based on immediate transitions rather than
full transitions from the initial state. The greedy
selection with n-best hypotheses is implemented as
follows. Firstly, every hypothesis hi is scanned and
all possible nodes are generated using the discourse
interpretation. Secondly, the multi-level score func-
tions are computed for each candidate node ci given
a hypothesis hi. Using the greedy algorithm, the
node with the highest score is selected as the user
goal state. Finally, the system actions are predicted
by the dialog example to maximize the example
score in the best node.
The generation of candidate nodes is based
on multiple hypotheses from the previous EBDM
634
framework. This previous EBDM framework chose
a dialog example to maximize the utterance similar-
ity measure. However, our system generates a set of
multiple dialog examples with each utterance sim-
ilarity over a threshold given a specific hypothesis.
Then, the candidate nodes are generated by match-
ing to each dialog example bound to the node. If the
number of matching nodes is exactly one, that node
is selected. Otherwise, the best node which would
be pushed onto the focus stack must be selected us-
ing multi-level score functions.
5.1 Node Selection
The node selection is determined by calculating
some score functions. We defined multi-level score
functions that combine the scores of ASR, SLU, and
DM modules, which range from 0.00 to 1.00. The
best node is selected by greedy search with multiple
hypotheses H and candidate nodes C as follows:
c? = argmax
hi?H,ci?C
?SH(hi) + (1? ?)SD(ci|S)
where H is a list of n-best hypotheses and C is a
set of nodes to be generated by the discourse in-
terpretation. For the node selection, we divided the
score function into two functions SH(hi), hypothe-
sis score, and SD(ci|S), discourse score, where ci is
the focus node to be generated by single hypothesis
hi.
We defined the hypothesis score at the utterance
level as
SH(hi) = ?Srec(hi) + ?Scont(hi)
where Srec(hi) denotes the recognition score which
is a generalized confidence score over the confi-
dence score of the top-rank hypothesis. Scont(hi)
is the content score in the view of content manage-
ment to access domain-specific contents. For exam-
ple, in the building guidance domain, theses contents
would be a building knowledge database including
room name, room number, and room type. The score
is defined as:
Scont(hi) =
?
?
?
N(Chi )
N(Cprev) if Chi ? Cprev
N(Chi )
N(Ctotal) if Chi * Cprev
where Cprev is a set of contents at the previous turn
and Ctotal is a set of total contents in the content
database. Chi denotes a set of focused contents by
hypothesis hi at the current turn. N(C) represents
the number of contents C. This score reflects the
degree of content coherence because the number of
contents of interest has been gradually reduced with-
out any unexpected focus shift. In the hypothesis
score, ? and ? denote weights which depend on the
accuracy of speech recognition and language under-
standing, respectively.
In addition to the hypothesis score, we defined the
discourse score SD at the discourse level to consider
the discourse structure between the previous node
and current node given the focus stack S. This score
is the degree to which candidate node ci is in focus
with respect to the previous user goal and system ut-
terance. In the agenda graph G, each transition has
its own probability as prior knowledge. Therefore,
when ci is NEXT TASK, the discourse score is com-
puted as
SD(ci|S) = P (ci|c = top(S))
where P (ci|c = top(S)) is a transition probabil-
ity from the top node c on the focus stack S to the
candidate node ci. However, there is a problem for
cases other than NEXT TASK because the graph has
no backward probability. To solve this problem, we
assume that the transition probability may be lower
than that of the NEXT TASK case because a user
utterance is likely to be influenced by the previous
turn. Actually, when using the task-oriented dialog
system, typical users stay focused most of the time
during imperfect communication (Lesh et al, 2001).
To assign the backward transition probability, we
obtain the minimum transition probability Pmin(S)
among from the top node on the focus stack S to
its children. Then, the discourse score SD can be
formalized when the candidate node ci does not cor-
respond to NEXT TASK as follows:
SD(ci|S) = max{Pmin(S)? ?Dist(ci, c), 0}
where ? is a penalty of distance between candi-
date node and previous node, Dist(ci, c), according
to type of candidate node such as NEW TASK and
NEW SUB TASK. The simplest case is to uniformly
assign ? to a specific value.
To select the best node using the node score, we
use ? (0 ? ? ? 1) as an interpolation weight
635
between the hypothesis score Sh and the discourse
score SD. This weight is empirically assigned ac-
cording to the characteristics of the dialog genre and
task. For example, ? can set lower to manage the
transactional dialog in which the user utterance is
highly correlated to the previous system utterance,
i.e., a travel reservation task, because this task usu-
ally has preference orders to fill slots.
5.2 Example Selection
After selecting the best node, we use the example
score to select the best dialog example mapped into
this node.
e? = argmax
ej?E(c?)
?Sutter(h?, ej)+(1??)Ssem(h?, ej)
where h? is the best hypothesis to maximize the
node score and ej is a dialog example in the best
node c?. Sutter(h, ej) denotes the value of the utter-
ance similarity of the user?s utterances between the
hypothesis h and dialog example ej in the best node
c? (Lee et al, 2006a).
To augment the utterance similarity used in the
EBDM framework, we also defined the semantic
score for example selection, Ssem(h, ej):
Ssem(h, ej) = # of matching index keys# of total index keys
The semantic score is the ratio of matching index
keys to the number of total index keys between hy-
pothesis h and example record ej . This score re-
flects that a dialog example is semantically closer to
the current utterance if the example is selected with
more index keys. After processing of the node and
example selection, the best example is used to pre-
dict the system actions. Therefore, the dialog man-
ager can predict the next actions with the agenda
graph and n-best recognition hypotheses.
6 Error Recovery Strategy
As noted in Section 4.2, the discourse interpretation
sometimes fails to generate candidate nodes. In ad-
dition, the dialog manager should confirm the cur-
rent information when the score falls below some
threshold. For these cases, we adapt an example-
based error recovery strategy (Lee et al, 2007). In
this approach, the system detects that something is
wrong in the user?s utterance and takes immediate
steps to address the problem using some help mes-
sages such as UtterHelp, InfoHelp, and UsageHelp
in the example-based error recovery strategies. We
also added a new help message, AgendaHelp, that
uses the agenda graph and the label of each node to
tell the user which subtask to perform next such as
?SYSTEM: Next, you can do the subtask 1)Search
Location with Room Name or 2)Search Location
with Room Type?.
7 Experiment & Result
First we developed the spoken dialog system for
PHOPE in which an intelligent robot can provide in-
formation about buildings (i.e., room number, room
location, room name, room type) and people (i.e.,
name, phone number, e-mail address, cellular phone
number). If the user selects a specific room to visit,
then the robot takes the user to the desired room.
For this system, ten people used the WOZ method to
collect a dialog corpus of about 500 utterances from
100 dialogs which were based on a set of pre-defined
10 subjects relating to domain-specific tasks. Then,
we designed an agenda graph and integrated it into
the EBDM framework.
In an attempt to quantify the impact of our ap-
proach, five Korean users participated in a prelimi-
nary evaluation. We provided them with pre-defined
scenarios and asked them to collect test data from
50 dialogs, including about 150 utterances. After
processing each dialog, the participants completed
a questionnaire to assess their satisfaction with as-
pects of the performance evaluation. The speech
recognition hypotheses are obtained by using the
Hidden Markov model Toolkit (HTK) speech rec-
ognizer adapted to our application domain in which
the word error rate (WER) is 21.03%. The results of
the Task Completion Rate (TCR) are shown in Table
1. We explored the effects of our agenda-based ap-
proach with n-best hypotheses compared to the pre-
vious EBDM framework which has no agenda graph
and supports only 1-best hypothesis.
Note that using 10-best hypotheses and the
agenda graph increases the TCR from 84.0% to
90.0%, that is, 45 out of 50 dialogs were com-
pleted successfully. The average number of turns
(#AvgTurn) to completion was also shorter, which
636
shows 4.35 turns per a dialog using the agenda graph
and 10-best hypotheses. From these results, we con-
clude that the the use of the n-best hypotheses with
the agenda graph is helpful to improve the robust-
ness of the EBDM framework against noisy inputs.
System #AvgTurn TCR (%)
1-best(-AG) 4.65 84.0
10-best(+AG) 4.35 90.0
Table 1: Task completion rate according to using the
AG (Agenda Graph) and n-best hypotheses for n=1
and n=10.
8 Conclusion & Discussion
This paper has proposed a new agenda-based ap-
proach with n-best recognition hypotheses to im-
prove the robustness of the Example-based Dialog
Modeling (EBDM) framework. The agenda graph
can be thought of as a hidden cost of applying our
methodology. However, an explicit agenda is nec-
essary to successfully achieve the purpose of using
spoken dialog system. Our preliminary results indi-
cate this fact that the use of agenda graph as heuris-
tics can increase the TCR. In addition, our approach
is robust to recognition errors because it maintains
multiple hypotheses for each user utterance.
There are several possible subjects for further re-
search on our approach. First, the optimal interpo-
lation weights should be determined. This task will
require larger dialog corpora by using user simula-
tion. Second, the cost of designing the agenda graph
should be reduced. We have focused on developing a
system to construct this graph semi-automatically by
applying dialog state clustering and utterance clus-
tering to achieve hierarchical clustering of dialog ex-
amples. Finally, future work will include expanding
our system to other applications, such as navigation
systems for automobiles.
Acknowledgement
This work was supported by grant No. RTI04-02-06
from the Regional Technology Innovation Program
and by the Intelligent Robotics Development Pro-
gram, one of the 21st Century Frontier R&D Pro-
grams funded by the Ministry of Commerce, Indus-
try and Energy (MOICE) of Korea.
References
Bohus, B. and Rudnicky A. 2003. RavenClaw: Dia-
log Management Using Hierarchical Task Decompo-
sition and an Expectation Agenda. Proceedings of the
European Conference on Speech, Communication and
Technology, 597?600.
Grosz, B.J. and Kraus, S. 1996. Collaborative Plans
for Complex Group Action. Artificial Intelligence,
86(2):269?357.
Lee, C., Jung, S., Eun, J., Jeong, M., and Lee, G.G.
2006. A Situation-based Dialogue Management using
Dialogue Examples. Proceedings of the IEEE Inter-
national Conference on Acoustics, Speech and Signal
Processing, 69?72.
Lee, C., Jung, S., Jeong, M., and Lee, G.G. 2006.
Chat and Goal-oriented Dialog Together: A Unified
Example-based Architecture for Multi-domain Dialog
Management. Proceedings of the IEEE Spoken Lan-
guage Technology Workshop, 194-197.
Lee, C., Jung, S., and Lee, G.G. 2007. Example-based
Error Reocvery Strategy For Spoken Dialog System.
Proceedings of the IEEE Automatic Speech Recogni-
tion and Understanding Workshop, 538?543.
Lesh, N., Rich, C., and Sidner, C. 2001. Collaborat-
ing with focused and unfocused users under imper-
fect communication. Proceedings of the International
Conference on User Modeling, 63?74.
Lochbaum, K.E. 1998. A Collaborative Planning Model
of Intentional Structure. Computational Linguistics,
24(4):525?572.
McTear, M., O?Neil, I., Hanna, P., and Liu, X.
2005. Handling errors and determining confirmation
strategies-An object-based approach. Speech Commu-
nication, 45(3):249?269.
Nagao, M. 1984. A Frame Work of a Mechnical Trans-
latino between Japanese and English by Analogy Prin-
ciple. Proceedings of the international NATO sympo-
sium on artificial and human intelligence, 173?180.
Rich, C. and Sidner, C.. 1998. Collagen: A Collab-
oration Agent for Software Interface Agents. Jour-
nal of User Modeling and User-Adapted Interaction,
8(3):315?350.
Torres, F., Hurtado, L.F., Garcia, F., Sanchis, E., and
Segarra, E. 2005. Error Handling in a Stochastic
Dialog System through Confidence Measure. Speech
Communication, 45(3):211?229.
Williams, J.D. and Young, S. 2007. Partially Observable
Markov Decision Processes for Spoken Dialog Sys-
tems. Computer Speech Language, 21(2):393-422.
Young, S., Schatzmann, J., Weilhammer, K., and Ye, H..
2007. The Hidden Information State Approach to Di-
alog Management. Proceedings of the IEEE Inter-
national Conference on Acoustics, Speech and Signal
Processing, 149?152.
637
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 17?20,
Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLP
Hybrid Approach to User Intention Modeling for Dialog Simulation 
 
 
Sangkeun Jung, Cheongjae Lee, Kyungduk Kim, Gary Geunbae Lee 
Department of Computer Science and Engineering 
Pohang University of Science and Technology(POSTECH) 
{hugman, lcj80, getta, gblee}@postech.ac.kr 
  
Abstract 
This paper proposes a novel user intention si-
mulation method which is a data-driven ap-
proach but able to integrate diverse user dis-
course knowledge together to simulate various 
type of users. In Markov logic framework, lo-
gistic regression based data-driven user inten-
tion modeling is introduced, and human dialog 
knowledge are designed into two layers such 
as domain and discourse knowledge, then it is 
integrated with the data-driven model in gen-
eration time. Cooperative, corrective and self-
directing discourse knowledge are designed 
and integrated to mimic such type of users. 
Experiments were carried out to investigate 
the patterns of simulated users, and it turned 
out that our approach was successful to gener-
ate user intention patterns which are not only 
unseen in the training corpus and but also per-
sonalized in the designed direction.  
1 Introduction 
User simulation techniques are widely used for learn-
ing optimal dialog strategies in a statistical dialog 
management framework and for automated evaluation 
of spoken dialog systems. User simulation can be 
layered into the user intention level and user surface 
(utterance) level. This paper proposes a novel inten-
tion level user simulation technique.  
In recent years, a data-driven user intention model-
ing is widely used since it is domain- and language 
independent. However, the problem of data-driven 
user intention simulation is the limitation of user pat-
terns. Usually, the response patterns from data-driven 
simulated user tend to be limited to the training data. 
Therefore, it is not easy to simulate unseen user inten-
tion patterns, which is quite important to evaluate or 
learn optimal dialog policies. Another problem is poor 
user type controllability in a data-driven method. 
Sometimes, developers need to switch testers between 
various type of users such as cooperative, uncoopera-
tive or novice user and so on to expose their dialog 
system to various users. 
For this, we introduce a novel data-driven user in-
tention simulation method which is powered by hu-
man dialog knowledge in Markov logic formulation 
(Richardson and Domingos, 2006) to add diversity 
and controllability to data-driven intention simulation. 
2 Related work 
Data-driven intention modeling approach uses statis-
tical methods to generate the user intention given dis-
course information (history). The advantage of this 
approach lies in its simplicity and in that it is domain- 
and language independency. N-gram based approach-
es (Eckert et al, 1997, Levin et al, 2000) and other 
approaches (Scheffler and Young, 2001, Pietquin and 
Dutoit, 2006, Schatzmann et al, 2007) are  introduced. 
There has been some work on combining rules with 
statistical models especially for system side dialog 
management (Heeman, 2007, Henderson et al, 2008). 
However, little prior research has tried to use both 
knowledge and data-driven methods together in a sin-
gle framework especially for user intention simulation.  
In this research, we introduce a novel data-driven 
user intention modeling technique which can be di-
versified or personalized by integrating human dis-
course knowledge which is represented in first-order 
logic in a single framework. In the framework, di-
verse type of user knowledge can be easily designed 
and selectively integrated into data-driven user inten-
tion simulation. 
3 Overall architecture  
The overall architecture of our user simulator is 
shown in Fig. 1. The user intention simulator accepts 
the discourse circumstances with system intention as 
input and generates the next user intention. The user 
utterance simulator constructs a corresponding user 
sentence to express the given user intention. The si-
mulated user sentence is fed to the automatic speech 
recognition (ASR) channel simulator, which then adds 
noises to the utterance. The noisy utterance is passed 
to a dialog system which consists of spoken language 
understanding (SLU) and dialog management (DM) 
modules. In this research, the user utterance simulator 
and ASR channel simulator are developed using the 
method of  (Jung et al, 2009). 
17
 
4 Markov logic 
Markov logic is a probabilistic extension of finite 
first-order logic (Richardson and Domingos, 2006). A 
Markov Logic Network (MLN) combines first-order 
logic and probabilistic graphical models in a single 
representation.  
An MLN can be viewed as a template for construct-
ing Markov networks. From the above definition, the 
probability distribution over possible worlds x speci-
fied by the ground Markov network is given by  
 
 
 
where F is the number  of formulas in the MLN and 
ni(x) is the number of true groundings of Fi in x. As 
formula weights increase, an MLN increasingly re-
sembles a purely logical KB, becoming equivalent to 
one in the limit of all infinite weights. General algo-
rithms for inference and learning in Markov logic are 
discussed in (Richardson and Domingos, 2006). 
Since Markov logic is a first-order knowledge base 
with a weight attached to each formula, it provides a 
theoretically fine framework integrating a statistically 
learned model with logically designed and inducted 
human knowledge. So the framework can be used for 
building up a hybrid user modeling with the advan-
tages of knowledge-based and data-driven models.  
5 User intention modeling in Markov 
logic 
The task of user intention simulation is to generate 
subsequent user intentions given current discourse 
circumstances. Therefore, user intention simulation 
can be formulated in the probabilistic form 
P(userIntention | context).  
In this research, we define the user intention state 
userIntention = [dialog_act, main_goal, compo-
nent_slot], where dialog_act is a domain-independent 
label of an utterance at the level of illocutionary force 
(e.g. statement, request, wh_question) and main_goal 
is the domain-specific user goal of an utterance (e.g. 
give_something, tell_purpose). Component slots 
represent domain-specific named-entities in the utter-
ance. For example, in the user intention state for the 
utterance ?I want to go to city hall? (Fig. 2), the com-
bination of each slot of semantic frame represents the 
user intention symbol. In this example, the state sym-
bol is ?request+search_loc+[loc_name]?. Dialogs on 
car navigation deal with support for the information 
and selection of the desired destination. 
The first-order language-based predicates which 
are related with discourse context information and 
with generating the next user intention are as follows: 
 
For example, after the following fragment of dialog 
for the car navigation domain,  
 
the discourse context which is passed to the user si-
mulator is illustrated in Fig. 3. 
Notice that the context information is composed of 
semantic frame (SF), discourse history (DH) and pre-
vious system intention (SI). ?isFilledComponent? 
predicate indicates which component slots are filled 
during the discourse.  ?updatedEntity? predicate is 
true if the corresponding named entity is newly up-
dated. ?hasSystemAct? and ?hasSystemActAttr? 
predicate represent previous system intention and 
mentioned attributes.  
 
 
SF 
hasIntention(?ct_01?, ?request+search_loc+loc_name?) 
hasDialogAct(?ct_01?,?wh_question?) 
hasMainGoal(?ct_01?, ?search_loc?) 
hasEntity(?ct_01?, ?loc_keyword?) 
DH 
isFilledComponent(?ct_01?, ?loc_keyword) 
!isFilledComponent(?ct_01?, ?loc_address) 
!isFilledComponent(?ct_01?, ?loc_name?) 
!isFilledComponent(?ct_01?, ?route_type?) 
updatedEntity(?ct_01?, ?loc_keyword?) 
SI 
hasNumDBResult(?ct_01?, ?many?) 
hasSystemAct(?ct_01?, ?inform?) 
hasSystemActAttr(?ct_01?, ?address,name?) 
Fig. 3 Example of discourse context in car navigation domain. 
SF=Semantic Frame, DH=Discourse History, SI=System Inten-
tion. 
raw user utterance I want to go to city hall. 
dialog_act request 
main_goal search_loc 
component.[loc_name] cityhall 
Fig. 2 Semantic frame for user intention simulation on 
car navigation domain. 
 
Fig. 1 Overall architecture of dialog simulation  
User(01) : Where are Chinese restaurants? 
// dialog_act=wh_question 
// main_goal=search_loc 
// named_entity[loc_keyword]=Chinese_restaurant 
Sys(01) : There are Buchunsung and Idongbanjum in 
Daeidong. 
// system_act=inform 
// target_action_attribute=name,address 
? User intention simulation related  predicates 
 GenerateUserIntention(context,userIntention) 
? Discourse context related predicates 
 hasIntention(context, userIntention) 
 hasDialogAct(context, dialogAct) 
 hasMainGoal(context, mainGoal) 
 hasEntity(context, entity) 
 isFilledComponent(context,entity) 
 updatedEntity(contetx, entity) 
 hasNumDBResult(context, numDBResult) 
 hasSystemAct(context, systemAct) 
 hasSystemActAttr(context, sytemActAttr) 
  isSubTask(context, subTask) 
 
1
1( ) exp( ( ))F i iiP X x w n xZ ?? ? ?
 
18
5.1 Data-driven user intention modeling in 
Markov logic 
The formulas are defined between the predicates 
which are related with discourse context information 
and corresponding user intention. The formulas for 
user intention modeling based on logistic regression 
are as follows: 
?ct, pui, ui hasIntention(ct, pui)1   
=>  GenerateUserIntention(ct, ui) 
?ct, da, ui hasDialogAct(ct, da) => GenerateUserIntention(ct,ui) 
?ct, mg, ui hasMainGoal(ct, mg) => GenerateUserIntention(ct,ui) 
?ct, en, ui hasEntity(ct, en) =>GenerateUserIntention(ct,ui) 
?ct, en, ui isFilledComponent(ct,en) 
=> GenerateUserIntention(ct,ui)  
?ct, en, ui updatedEntity(ct, en) => GenerateUserIntention(ct,ui) 
?ct, dbr, ui hasNumDBResult(ct, dbr)  
=> GenerateUserIntention(ct, ui) 
?ct, sa, ui hasSystemAct(ct, sa) =>GenerateUserIntention(ct, ui) 
?ct, attr, ui hasSystemActAttr(ct, attr) 
       =>  GenerateUserIntention(ct, ui) 
The weights of each formula are estimated from 
the data which contains the evidence (context) and 
corresponding user intention of next turn (userInten-
tion). 
5.2 User knowledge 
In this research, the user knowledge, which is used for 
deciding user intention given discourse context, is 
layered into two levels: domain knowledge and dis-
course knowledge. Domain- specific and ?dependent 
knowledge is described in domain knowledge. Dis-
course knowledge is more general and abstracted 
knowledge. It uses the domain knowledge as base 
knowledge. The subtask which is one of domain 
knowledge are defined as follows 
 
?isSubTask? implies which subtask corresponds 
to the current context. ?subTaskHasIntention? 
describes which subtask has which user intention. 
?moveTo? predicate implies the connection from sub-
task to subtask node. 
Cooperative, corrective and self-directing discourse 
knowledge is represented in Markov logic to mimic 
following users.  
? Cooperative User: A user who is cooperative with a 
system by answering what the system asked.  
? Corrective User: A user who try to correct the mis-
behavior of system by jumping to or repeating spe-
cific subtask. 
? Self-directing User: A user who tries to say what 
he/she want to without considering system?s sugges-
tion.  
Examples of discourse knowledge description for 
three types of user are shown in Fig. 4. 
                                                 
1 ct: context, ui: user intention, pui: previous user intention, da: 
dialog act, mg: main goal, en: entity, dbr:DB result, sa: system 
action, attr: target attribute of system action 
Both the formulas from data-driven model and 
formulas from discourse knowledge are used for con-
structing MLN in generation time. 
In inference, the discourse context related predi-
cates are given to MLN as true, then probabilities of 
predicate ?GenerateUserIntention? over candi-
date user intention are calculated. One of example 
evidence predicates was shown in Fig. 3. All of the 
predicates of Fig. 3 are given to MLN as true. From 
the network, the probability of P(userIntention | con-
text) is calculated. 
 
 
6 Experiments 
137 dialog examples from a real user and a dialog 
system in the car navigation domain were used to 
train the data-driven user intention simulator. The 
SLU and DM are built in the same way of (Jung et al, 
2009). After the training, simulations collected 1000 
dialog samples at each word error rate (WER) setting 
(WER=0 to 40%). The simulator model can be varied 
according to the combination of knowledge. We can 
generate eight different simulated users from A to H 
as Fig. 5. 
The overall trend of simulated dialogs are ex-
amined by defining an average score function similar 
to the reward score commonly used in reinforcement 
learning-based dialog systems for measuring both a 
cost and task success. We give 20 points for the suc-
cessful dialog state and penalize 1 point for each ac-
tion performed by the user to penalize longer dialogs.  
 A B C D E F G H 
Statistical model (S) O O O O O O O O 
Cooperative(CPR)  O   O O  O 
Corrective(COR)   O  O  O O 
Self-directing(SFD)    O  O O O 
Fig. 5 Eight different users (A to H) according to the 
combination of knowledge.  
? Subtask related predicates 
 subTaskHasIntention(subTask,userIntetion) 
 moveTo(subtask, subTask) 
 isCompletedSubTask (context, subTask) 
 isSubtask(context,subTask) 
 
Cooperative Knoweldge 
 // If system asks to specify an address explicitly, coop-
erative users would specify the address by jumping to 
the address setting subtask. 
? ct, st  isSubTask(ct, st) ^  
hasSytemAct(ct, ?specify?) ^ 
          hasSystemActAttr(ct, ?address?) 
           => moveTo(st, ?AddressSetting?) 
Corrective Knowledge 
 // If the current subtask fails, corrective users would 
repeat current subtask. 
? ct, st isSubTask(ct, st)^  
? isCompletedSubTask(ct, st) ^  
subTaskHasIntention(st, ui)  
=> GenerateUserIntention(ct,ui) 
Self-directing Knowledge 
 // Self-directing users do not make an utterance which 
is not relevant with the next subtask in their knowledge. 
? ct, st  isSubTask(ct, st) ^  
? moveTo(st, nt) ^ 
           subTaskHasIntention(nt, ui) 
 => ? GenerateUserIntention(ct, ui) 
Fig. 4 Example of cooperative, corrective and self-
directing discourse knowledge.  
19
Fig. 6 shows that simulated user C which has cor-
rective knowledge with statistical model show signifi-
cantly different trend over the most of word error rate 
settings. For the cooperative user (B), the difference is 
not as large and not statistically significant. It can be 
analyzed that the cooperative user behaviors are rela-
tively common patterns in human-machine dialog 
corpus. So, these behaviors can be already learned in 
statistical model (A).  
Using more than two type of knowledge together 
shows interesting result. Using cooperative know-
ledge with corrective knowledge together (E) shows 
much different result than using each knowledge 
alone (B and C). In the case of using self-directing 
knowledge with cooperative knowledge (F), the aver-
age scores are partially increased against base line 
scores. However, using corrective knowledge with 
self-directing knowledge does not show different re-
sult.  It can be thought that the corrective knowledge 
and self-directing knowledge are working as contra-
dictory policy in deciding user intention. Three dis-
course knowledge combined user shows very interest-
ing result. H shows much higher improvement over 
all simulated users, and the differences are significant 
results at p ? 0.001.  
To verify the proposed user simulation method can 
simulate the unseen events, the unseen rates of units 
were calculated. Fig. 7 shows the unseen unit rates of 
intention sequence. The unseen rate of n-gram varies 
according to the simulated user. Notice that simulated 
user C, E and H generates higher unseen n-gram pat-
terns over all word error settings. These users com-
monly have corrective knowledge, and the patterns 
seem to not be present in the corpus. But the unseen 
patterns do not mean poor intention simulation. High-
er task completion rate of C, E and H imply that these 
users actually generate corrective user response to 
make a successful conversation. 
7 Conclusion 
This paper presented a novel user intention simulation 
method which is a data-driven approach but able to 
integrate diverse user discourse knowledge together to 
simulate various type of user.  A logistic regression 
model is used for the statistical user intention model 
in Markov logic. Human dialog knowledge is sepa-
rated into domain and discourse knowledge, and co-
operative, corrective and self-directing discourse 
knowledge are designed to mimic such type user. The 
experiment results show that the proposed user inten-
tion simulation framework actually generates natural 
and diverse user intention patterns what the developer 
intended.  
Acknowledgments 
This research was supported by the MKE (Ministry of 
Knowledge Economy), Korea, under the 
ITRC(Information Technology Research Center) sup-
port program supervised by the IITA(Institute for In-
formation Technology Advancement) (IITA-2009-
C1090-0902-0045). 
 
 
References  
Eckert, W., Levin, E. and Pieraccini, R. 1997. User model-
ing for spoken dialogue system evaluation. Automatic 
Speech Recognition and Understanding:80-87. 
Heeman, P. 2007. Combining reinforcement learning with 
information-state update rules. NAACL. 
Henderson, J., Lemon, O. and Georgila, K. 2008. Hybrid 
reinforcement/supervised learning of dialogue policies 
from fixed data sets. Comput. Linguist., 34(4):487-511. 
Jung, S., Lee, C., Kim, K. and Lee, G.G. 2009. Data-driven 
user simulation for automated evaluation of spoken dialog 
systems. Computer Speech & Lan-
guage.doi:10.1016/j.csl.2009.03.002. 
Levin, E., Pieraccini, R. and Eckert, W. 2000. A stochastic 
model of human-machine interaction for learning dialog-
strategies. IEEE Transactions on Speech and Audio 
Processing, 8(1):11-23. 
Pietquin, O. and Dutoit, T. 2006. A Probabilistic Frame-
work for Dialog Simulation and Optimal Strategy Learn-
ing. IEEE Transactions on Audio, Speech and Language 
Processing, 14(2):589-599. 
Richardson, M. and Domingos, P. 2006. Markov logic net-
works. Machine Learning, 62(1):107-136. 
Schatzmann, J., Thomson, B. and Young, S. 2007. Statistic-
al User Simulation with a Hidden Agenda. SIGDial. 
Scheffler, K. and Young, S. 2001. Corpus-based dialogue 
simulation for automatic strategy learning and evaluation. 
NAACL Workshop on Adaptation in Dialogue Sys-
tems:64-70. 
 
Fig. 7 Unseen user intention sequence rate and task com-
pletion rate over simulated users at word error rate of 10. 
WER(%) 
model  
0 10 20 30 40 
A:S (base line) 
14.22 
(0.00) 
9.13 
(0.00) 
5.55 
(0.00) 
1.33 
(0.00) 
-1.16 
(0.00) 
B:S+CPR 
14.39 
(0.17) 
9.78 
(0.65) 
5.38 
(-0.17) 
2.32? 
(0.99) 
-1.00 
(0.16) 
C:S+COR 
14.61? 
(0.40) 
10.91
?
 
(1.78) 
7.28
?
 
(1.74) 
2.62? 
(1.30) 
-0.81 
(0.35) 
D:S+SFD 
15.70
?
 
(1.48) 
10.10? 
(0.97) 
5.51 
(-0.04) 
1.89 
(0.56) 
-0.96
?
 
(0.20) 
E:S+CPR+COR 
14.75? 
(0.53) 
10.93
?
 
(1.79) 
6.88? 
(1.33) 
2.94
?
 
(1.61) 
-1.06? 
(0.11) 
F:S+CPR+SFD 
15.75
?
 
(1.54) 
10.16? 
(1.02) 
5.80 
(0.26) 
1.88 
(0.56) 
-0.03? 
(1.13) 
G:S+COR+SFD 
14.39 
(0.17) 
9.18 
(0.05) 
5.04 
(-0.50) 
1.63 
(0.31) 
-1.52 
(-0.36) 
H:S+CPR+COR+SFD 15.70
?
 
(1.48) 
12.19
?
 
(3.05) 
9.20
?
 
(3.65) 
5.12
?
 
(3.80) 
1.32
?
 
(2.48) 
Fig. 6 Average scores of user intention models over used discourse 
knowledge. The relative improvements against statistical models 
are described between parentheses. Bold cells indicate the im-
provements are higher than 1.0.  
? : significantly different from the base line, p = 0.05,  
? : significantly different from the base line, p = 0.01,  
?
 : significantly different from the base line, p ? 0.001 
20
Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 120?127,
Columbus, June 2008. c?2008 Association for Computational Linguistics
A Frame-Based Probabilistic Framework for Spoken Dialog Manage-
ment Using Dialog Examples 
 
 
Kyungduk Kim, Cheongjae Lee, Sangkeun Jung and Gary Geunbae Lee 
Department of Computer Science and Engineering 
Pohang University of Science & Technology (POSTECH) 
San 31, Hyoja-Dong, Pohang, 790-784, Republic of Korea 
{getta, lcj80, hugman, gblee}@postech.ac.kr 
 
 
 
 
 
 
Abstract 
This paper proposes a probabilistic framework 
for spoken dialog management using dialog 
examples. To overcome the complexity prob-
lems of the classic partially observable Mar-
kov decision processes (POMDPs) based 
dialog manager, we use a frame-based belief 
state representation that reduces the complexi-
ty of belief update. We also used dialog ex-
amples to maintain a reasonable number of 
system actions to reduce the complexity of the 
optimizing policy. We developed weather in-
formation and car navigation dialog system 
that employed a frame-based probabilistic 
framework. This framework enables people to 
develop a spoken dialog system using a prob-
abilistic approach without complexity prob-
lem of POMDP. 
1 Introduction 
A robust dialog manager is an essential part of 
spoken dialog systems, because many such sys-
tems have failed in practice due to errors in speech 
recognition. Speech recognition errors can be 
propagated to spoken language understanding 
(SLU), so the speech input must be considered er-
ror-prone from a standpoint of dialog management. 
Therefore robust dialog managers are necessary to 
develop practical spoken dialog systems. 
One approach to dialog management uses the 
partially observable Markov decision process 
(POMDP) as a statistical framework, because this 
approach can model the uncertainty inherent in 
human-machine dialog (Doshi and Roy, 2007). 
The dialog manager uses a probabilistic, rather 
than deterministic, approach to manage dialog. As 
more information becomes available, the dialog 
manager updates its belief states. A POMDP-based 
dialog manager can learn the optimized policy that 
maximizes expected rewards by reinforcement 
learning. 
But applying classic POMDP to a practical di-
alog system incurs a scalability problem. The com-
putational complexity of updating belief states and 
optimizing the policy increases rapidly with the 
size of the state space in a slot-filling dialog task. 
To solve this scalability problem, the method of 
compressing states or mapping the original state 
space to summarized space can be used (Williams 
and Young, 2006; Roy et al,2005), but these algo-
rithms tend to approximate the state space exces-
sively. The complexity problem of POMDP comes 
from updating beliefs that are out of the user?s in-
tention, and from calculating the reward of system 
actions that do not satisfy user?s objective. 
In this paper, we propose a new probabilistic 
framework for spoken dialog management using 
dialog examples. We adopted a frame-based belief 
state representation to reduce the complexity of 
belief update. Furthermore, we used an example-
based approach to generate only a reasonable 
number of system action hypotheses in a new 
framework. We developed a dialog system by us-
ing our new framework in weather information 
service and car navigation service. 
120
2 Overview 
We try to address two problems of applying 
POMDP to slot-filling dialog management. 1) 
Computational complexity of belief update: it is 
difficult to maintain and update all belief states at 
every turn of dialog since there are too many di-
alog states in slot-filling dialog tasks. 2) Computa-
tional complexity of policy optimizing: optimizing 
complexity depends on both the space size of di-
alog states, and the number of available machine 
actions. In slot-filling dialog tasks, a system action 
can have various slot values so that the system 
needs to choose an action among a large number of 
action hypotheses. 
In our new probabilistic framework (Figure 1), 
we try to solve these problems. Our approach uses 
1) the frame-based belief state representation to 
solve the computational complexity problem of 
belief update and 2) the dialog examples to gener-
ate action hypotheses to solve the computational 
complexity of policy optimizing by reducing the 
number of system action hypotheses. First, the sys-
tem groups belief states dynamically using frame-
based belief state representation according to us-
er?s utterance and its SLU result. Then the system 
uses an example-based approach to generate only 
system action hypotheses that are suitable for cur-
rent belief states. If there are too many hypotheses 
for calculating expected utility, the system prunes 
them away until only a reasonable number of hy-
potheses remains. The following describes the de-
tails of each system?s component and the dialog 
managing process. 
User?s Utterance
SLU Result
Frame-bas d Belief 
St te Re r sentation
Dialog 
Example DB
Calculating
Utilities
System action
User?s Intention,
Semantic Frame,
Dialog History
Pruning 
Hypotheses
Lexico-semantic 
Similarity
Generating
Hypotheses
 
Figure 1. Overview of the system operation. Bold ar-
rows indicate the control flow. Thin arrows indicate the 
data flow.  
3 Frame-based Belief State Representation 
We assumed that the machine?s internal represen-
tation of the dialog state sm consists of three com-
ponents: user?s goal su, user?s last action au and 
dialog history sd. This section briefly describes the 
basic introduction of POMDP framework and ex-
plains each component of machine?s internal state 
in the standpoint of our frame-based probabilistic 
framework. 
3.1 POMDP for spoken dialog management 
A POMDP is defined as a tuple that consists of six 
substates: (S, A, P, R, ?, O) where S is a set of 
state, A is a set of action, P is a transition proba-
bility P(s?|s,a), R is a reward function R(s,a,s?), ? 
is a set of observation and O is an observation 
model P(o|s,a). The current state is not determinis-
tic in a POMDP framework while it is determined 
as a specific state in a Markov decision process 
(MDP) framework. In a POMDP, the probability 
distribution over all states s?S, which is referred 
as a belief state b(s), is maintained instead of de-
terministic state. At each time instant t, the system 
chooses an action a?A, and this causes the system 
to move from current state s to next state s? with 
the transition probability P(s? |s,a). Then, the sys-
tem is granted a reward R(s,a) while the system 
receives an observation o with probability of 
P(o|s?,a). The system computes the belief state in 
the next time instance b?(s?) as a following: 
 
? ????? s sbassPasoPksb )(),|(),|()(  
 
where k is a normalizing factor. This process is 
referred as belief update. 
Optimizing a POMDP policy is a process of 
finding a mapping function from belief states to 
actions that maximizes the expected reward. The 
system should compute a value function over be-
lief spaces to find optimized actions. However, 
unlike as in a MDP, each value in a POMDP is a 
function of an entire probability distribution and 
belief spaces are very complex, so that a POMDP 
has a scale problem of computing the exact value 
function. 
A POMDP for spoken dialog system is well 
formulated in (Williams and Young, 2007). First, a 
state s can be factored to three substates: (su, au, sd) 
121
where su is a user goal state, au is a user action, and 
sd is a dialog history. A system action am and user 
action au can be cast as action a and observation o 
respectively. With some independence assumption 
between variables, the belief update equation can 
be rewritten as following: 
 
,),,(           
),,|(),|(           
),|()|
~
(         
),,()(
?
? ?
???
?????
?????
???????
u
u d
a
duu
s s
mdudmuu
muuuu
duu
sasb
asasPassP
asaPaaPk
sasbb
 
where 
ua~?  is an automatic speech recognizer (ASR) 
and SLU recognition result of user action. In our 
framework, belief update is done based on this eq-
uation. But applying this directly to a spoken di-
alog system can have a problem because the 
probabilities used in the equation are hard to esti-
mate from the corpus due to the data sparseness. 
Therefore, we adopted Young?s (2007) belief up-
date formula that is simplified from the original 
equation. 
3.2 User goal state 
In a slot-filling dialog system, the user?s goal can 
be represented as a fully-filled frame in which all 
slots of the frame contain values specified by the 
user?s intention. Therefore, if a dialog system has 
W slots and each slot can have a value among V 
candidates, then VW user goals can be represented 
as frames. This means that the number of user 
goals is related exponentially to the number of 
slots. This number of user goals is intractable in 
practical dialog systems. 
Therefore, a method is needed to reduce the size 
of the state space rather than maintaining all belief 
states. To do this, we developed a frame-based be-
lief state representation in which the system dy-
namically groups set of equivalent states to a high-
level frame state. Frame state, which is a similar 
concept to the partition in the hidden information 
state (HIS) approach (Young et al 2007) 
represents the indistinguishable classes of user?s 
goals. The biggest difference between frame-based 
representation and partition-based representation is 
that the former uses only user input to split the 
frame state, whereas the latter uses the user input 
and external ontology rules such as a prior proba-
bility for belief of split partition. Therefore, the 
frame-based representation has relatively high do-
main portability because it does not need that kind 
of external domain dependent information. 
In the frame-based belief state representation, a 
partially-filled frame state represents the current 
user?s goal state for which the unfilled slot can be 
filled in the future, while a fully-filled frame state 
represents  co plete user?s goal state. Figure 2 
describes an example of the ubsumption relation-
ship between partially filled frames and fully filled 
frames.  
 
Figure 2. Subsumption relationship between partially 
filled frame and fully filled frame. The left frame is par-
tially filled and three frames in the right side are fully 
filled. 
 
At the start of a dialog, all states belong to the 
root frame state f0. As the dialog progresses, this 
root frame state is split into smaller frame states 
whenever the value of a slot is filled by the user?s 
input (Figure 3). First, if the user?s input [A=a] 
fills the slot of the root frame state f0, then it splits 
into two frame states: f1, which includes all user 
goal states with the slot A having ?a? as a value; 
and {f0-f1}, which is the relative complement of f1. 
Next, if the user?s input [B=b] is entered to the 
system, each frame f1 and {f0-f1} is split into small-
er frame states. The system updates not all belief 
states but only the beliefs of the frame states, so 
that the computational complexity remains rela-
tively small.  
If each user?s goal has uniform distribution, the 
belief of frame state b(f) can be calculated as fol-
lows:  
# of user goals contained in frame ( ) # of all user goals
fb f ?
 
This can be computed as follows:  
 
122
t
0
t
1
t
2
.
.
.
Root
Frame State
f
0
0( ) 1b f ?
{f
0
 - f
1
}
0 1
49
({ })
50
b f f? ?
f
1
(A = a)
1
1
( )b f
 f
3
(A = a)
(B = b)
3 2
1
( )
50
b f ?
{f
1 
? f
3
}
(A = a)
1 3 2
49
({ })
50
b f f? ?
 f
2
(B = b)
2 2
49
( )
50
b f ?
{{f
0
 - f
1
} - f
2
}
2
0 1 2 2
49
({{ } })
50
b f f f? ? ?
A = a
A != a
f
1 { f0 - f1}
f
0
A = a
B = b
A = a
B != b
A != a
B = b
A != a
B != b
f
3
{f
1 
? f
3
}
 f
2
{{f
0
 - f
1
} - f
2
}
User Input
A = a
User Input
B = b
User Input
C = c
Frame Splitting State Space
 
Figure 3. Splitting frame states and their beliefs with three user?s inputs. f0, f1, f2, ? denote frame states and b(f) 
means the belief of frame state f. A, B, C are the slot labels and a, b, c are the respective values of these slots. 
 
 
where Sfilled means the set of slots that are filled by 
the user?s input in frame state f, and SnotFilled means 
the set of empty slots. Vs denotes the set of availa-
ble values for slot s, and Vs? stands for the set of 
values for slot s that were specified by the user in 
other frame states. 
3.3 User action 
The SLU result of current user's utterance is used 
for the user action. The result frame of SLU con-
sists of a speech act, a main goal, and several 
named-entity component slots for each user's utter-
ance. The speech act stands for the surface-level 
speech act per single utterance and the main goal 
slot is assigned from one of the predefined classes 
which classify the main application actions in a 
specific domain such as ?search the weather 
(SEARCH_WEATHER)? or ?search the tempera-
ture (SEARCH_TEMPERATURE)? in the weather 
information service domain. The tasks for filling 
the named-entity component slots, such as, name 
of the city, name of the state, are viewed as a se-
quence labeling task. The Figure 4 shows some 
examples of predefined classes for SLU semantic 
frame in weather information service dialog system 
Our SLU module was developed based on the 
concept spotting approach, which aims to extract 
only the essential information for predefined mean-
ing representation slots, and was implemented by 
applying a conditional random field model (Lee et 
al., 2007).  
 
 
Figure 4 Example predefined classes for semantic frame 
of SLU in weather information service dialog system. 
 
3.4 Dialog history 
Similar to the traditional frame-based dialog 
management approach, a frame can represent the 
history of the dialog. The difference between the 
traditional frame-based dialog manager and our 
framework is that traditional frame-based dialog 
123
manager maintains only one frame while our 
framework can maintain multiple dialog hypothes-
es. Moreover, each hypothesis in our framework 
can have a probability as in the belief state of the 
classic POMDP.  
4 Example-based System Action Genera-
tion 
4.1 Example-based system action hypothesis 
generation 
It is impossible to consider all of the system ac-
tions as hypotheses because the number of possible 
actions is so large. We used an example-based ap-
proach to generate a reasonable number of system 
action hypotheses as hinted in (Lee et al, 2006). In 
this approach, the system retrieves the best dialog 
example from dialog example database (DEDB) 
which is semantically indexed from a dialog cor-
pus. To query a semantically close example for the 
current situation, the system uses the user?s inten-
tion (speech act and main goal), semantic frame 
(component slots) and discourse history as search 
key constraints (Lee et al, 2006). These search 
keys can be collected with SLU output (e.g., user 
intention and semantic frame) and discourse histo-
ry in a dialog manager. Figure 5 describes an ex-
ample of search key for DEDB on a weather 
information service system.  
 
User?s utterance  What will the temperature be tomorrow?  
                     Weather_Type  Time_Date  
Search key 
constraints  
Speech Act = wh_question  
Main Goal = search_temperature  
WEATHER_TYPE = 1 (filled) 
TIME_DATE = 1 (filled) 
LOC_CITY = 0 (unfilled) 
LOC_STATE = 0 (unfilled) 
Lexico-semantic  
Input 
What will the [WEATHER_TYPE] be 
[TIME_DATE]? 
Figure 5. Example search key constraints for dialog 
example database.  
 
For each frame state f1, ?, fn, the system gene-
rates one or more system action hypotheses by 
querying the DEDB respectively. Queried actions 
may inconsistent with the current frame state be-
cause the situation of indexed dialog examples 
may different from current dialog situation. There-
fore, the system maps the contents of dialog exam-
ple to information of current frame state. Slot 
values of frame state and information from content 
database (e.g., weather information database) are 
used for making the action consistent. If the system 
retrieves more than a threshold number of system 
action hypotheses using the search key constrains, 
then the system should prune away dialog exam-
ples to maintain only a reasonable number of hypo-
theses. We used lexico-semantic similarity 
between the user utterance and the retrieved exam-
ples to limit the number of hypotheses. To measure 
the lexico-semantic similarity, we first replace the 
slot values in the user utterance by its slot names to 
generate lexico-semantic input, and calculate the 
normalized edit distance between that input and 
retrieved examples (Figure 5). In the normalized 
edit distance, we defined following cost function 
C(i,j) to give a weight to the term which is re-
placed by its slot name. 
 
1, 2,
1, 2, 1, 2, _
1, 2, 1, 2, _
0  if                                       
( , ) 1  if  and ,  
1.5  if  and ,
i j
i j i j slot name
i j i j slot name
w w
C i j w w w w S
w w w w S
? ?
?? ? ??
? ? ??
 
 
where w1,i is ith word of user?s utterance, w2,j is jth 
word of dialog example?s utterance, and Sslot_name is 
the set of slot names. According to the lexico-
semantic similarity, the system appends the top Nh-
ranked hypotheses to the final action hypotheses 
(where Nh is the rank threshold). 
Many existing systems used heuristics or rule-
based approaches to reduce the number of system 
action hypotheses (Young et al, 2007). But these 
methods are not flexible enough to handle all di-
alog flows because a system developer should de-
sign new heuristics or rules whenever the system 
needs to support a new kind of dialog flow. The 
example-based approach, on the contrary, can in-
stantly refine the control of dialog flows by adding 
new dialog examples. This is a great advantage 
when a system developer wants to change or refine 
a dialog control flow. 
4.2 Calculating Expected Utilities 
We adopted the principle of maximum expected 
utility to determine the optimized system actions 
among the hypotheses (Paek and Horvitz, 2004). 
124
* argmax ( | )
argmax ( | ) ( , )
argmax ( ) ( , )
m
a
a h
h
a EU a
P H h u a h
b h u a h
?
?
?
? ?
?
?
?
?
 
 
where ? denotes all information about the envi-
ronment, u(a,h) means the utility of taking an ac-
tion when the internal state of the machine is h, 
which consists of three substates, (f, au, sd) : f is a 
frame state, au is a user?s last action, and sd is a 
dialog history. The utility function u(a,h) can be 
specific to each application. We defined a 
handcrafted utility function to calculate the ex-
pected utility. 
5 Experiments 
We performed two evaluations. 1) Real user evalu-
ation: we measured the user satisfaction with vari-
ous factors by human. 2) Simulated user 
evaluation: we implemented user simulator to 
measure the system performance with a large 
number of dialogs. We built dialog corpora in two 
domains: weather information service and car na-
vigation.  
5.1 Real user evaluation 
We built a dialog corpus in weather information 
service to measure the performance of the dialog 
system using our approach by real user evaluation. 
This corpus consists of 99 dialogs with 503 user 
utterances (turns). User?s utterances were anno-
tated with the semantic frame including speech 
acts, main goal and component slots for training 
the SLU module and indexing the DEDB. 
To evaluate the preliminary performance, four 
test volunteers among computer science people 
evaluated our dialog system with five different 
weather information-seeking tasks. The volunteers 
typed their utterances with a keyboard rather than 
using a real ASR because it is hard to control the 
WER. We employed a simulated ASR error chan-
nel by generating random errors to evaluate the 
performance of dialog management under various 
levels of WER. We will explain the details of our 
ASR channel simulator in Section 5.2. The WER is 
controlled by this ASR channel simulator while the 
volunteers were interacting with computer. To 
measure the user perception of task completion 
rate (TCR), the volunteers evaluated the system?s 
response in each dialog to measure the success turn 
rate (STR) and decided whether the entire dialog 
was successful or not. We evaluated the perfor-
mance of our dialog system based on criteria out-
lined in (Litman and Pan, 2004) by measuring user 
satisfaction, which is defined with a linear combi-
nation of three measures: TCR, Mean Recognition 
Accuracy (MRA), and STR. 
 
User Satisfaction = ?TCR +?STR + ?MRA 
 
In our evaluation, we set ?, ? and ? to 1/3, so 
that the maximum value of the user satisfaction is 
one.  
 
 
Figure 6 Dialog system performance with various word 
error rates in weather information seeking tasks. Dotted 
line is TCR; dashed line is STR; solid line is user satis-
faction. 
 
TCR, STR and user satisfaction decreased with 
WER. User satisfaction has relatively high value 
when the WER is smaller than 20% (Figure 6). If 
the WER is equal or over 20%, user satisfaction 
has small value because the TCR decreases rapidly 
in this range. 
Generally, TCR has a higher value than STR, 
because although a dialog turn may fail, users still 
have a chance to use other expressions which can 
be well recognized by the system. As a result of 
this, even when some dialog turns fail, the task can 
be completed successfully. 
TCR decreases rapidly when WER ?20%. 
When WER is high, the probability of losing the 
125
information in a user utterance is also large. Espe-
cially, if words contain important meaning, i.e., 
values of component slots in SLU, it is difficult for 
the system to generate a proper response. 
STR is 0.83 when WER is zero, i.e., although all 
user inputs are correctly recognized, the system 
sometimes didn?t generate proper outputs. This 
failure can be caused by SLU errors or malfunction 
of the dialog manager. SLU errors can be propa-
gated to the dialog manager, and this leads the sys-
tem to generate a wrong response because SLU 
results are inputs of dialog manger. 
If the WER is 20%, user satisfaction is relatively 
small because TCR decreases rapidly in this range. 
This means that our approach is useful in a system 
devoted to providing weather information, and is 
relatively robust to speech errors if the WER is less 
than 20%. 
5.2 Simulated user evaluation 
We built another dialog corpus in car navigation 
service to measure the performance of the dialog 
system by simulated user evaluation. This corpus 
consists of 123 dialogs with 510 user utterances 
(turns). The SLU result frame of this corpus has 7 
types of speech acts, 8 types of main goals, and 5 
different component slots. 
The user simulator and ASR channel simulator has 
been used for evaluating the proposed dialog man-
agement framework. The user simulator has two 
components: an Intention Simulator and a Surface 
Simulator. The Intention Simulator generates the 
next user intention given current discourse context, 
and the Surface Simulator generates user sentence 
to express the generated intention.  
ASR channel simulator simulates the speech 
recognition errors including substitution, deletion, 
and insertions errors. It uses the phoneme confu-
sion matrix to estimate the probability distribution 
for error simulation. ASR channel simulator dis-
torts the generated user utterance from Surface Si-
mulator. By simulating user intentions, surface 
form of user sentence and ASR channel, we can 
test the robustness of the proposed dialog system in 
both speech recognition and speech understanding 
errors. 
We defined a final state of dialog to automati-
cally measure TCR of a simulated dialog. If a di-
alog flow reaches the final state, the evaluator 
regards that the dialog was successfully completed. 
TCRs and average dialog lengths were measured 
under various WER conditions that were generated 
by ASR channel simulator. Until the SLU result is 
an actual input of the dialog manager, we also 
measured the SLU accuracy. If a SLU result is 
same as a user?s intention of the Intention Simula-
tor, then the evaluator considers that the result is 
correct. Unlike in the real user evaluation, the di-
alog system could be evaluated with relatively 
large amount of simulated dialogs in the simulated 
user evaluation. 5000 simulated dialogs were gen-
erated for each WER condition. 
 
 
Figure 7 TCR, SLU accuracy, and average dialog length 
of the dialog system under various WER conditions. 
 
We found that the SLU accuracy and TCR li-
nearly decreased with the WER. Similar in the 
human evaluation, TCR is about 0.9 when WER is 
zero, and it becomes below 0.7 when WER is 
higher than 20%. Average dialog length, on con-
trary, increased with WER, and it has similar val-
ues when WER is less than 10% although it 
increased relatively rapidly when WER is higher 
than 15%. 
 
6 Conclusions 
This paper proposed a new probabilistic method to 
manage the human-machine dialog by using the 
frame-state belief state representation and the ex-
ample-based system action hypothesis generation. 
The frame-based state representation reduces the 
computational complexity of belief update by 
grouping the indistinguishable user goal states. 
And the system generates the system action hypo-
126
theses with the example-based approach in order to 
refine the dialog flows easily. In addition, this ap-
proach employed the POMDP formalism to main-
tain belief distribution over dialog states so that the 
system can be robust to speech recognition errors 
by considering the uncertainty of user?s input. 
A prototype system using our approach has been 
implemented and evaluated by real and simulated 
user. According to the preliminary evaluation, our 
framework can be a useful approach to manage a 
spoken dialog system. 
We plan to progress the research on adopting a 
formalized online search to determine the optimal 
system action (Ross and Chaib-draa, 2007). With 
the online searching, system doesn?t need to be-
have the useless computation because this ap-
proach searches only possible path. We expect that 
this property of the online searching show the syn-
ergetic effect on dialog management if it combines 
with example-based approach. 
Similar to example-based approach, the case-
based reasoning approach (Eliasson, 2006) can be 
helpful for our future research. Some properties 
such as using previous cases to process current 
case can be shared with our approach. We think 
that some other properties including the concept of 
online learning can be useful for making our ap-
proach concrete 
Acknowledgments 
This research was supported by the MKE (Min-
istry of Knowledge Economy), Korea, under the 
ITRC (Information Technology Research Center) 
support program supervised by the IITA (Institute 
for Information Technology Advancement) (IITA-
2008-C1090-0801-0045) 
References  
Changki Lee, Jihyun Eun, Minwoo Jeong, and Gary 
Geunbae Lee, Y. Hwang, M. Jang, ?A multi-strategic 
concept-spotting approach for robust understanding 
of spoken Korean,? ETRI Journal, vol. 29, No.2, pp. 
179-188, 2007. 
 
Cheongjae Lee, Sangkeun Jung, Jihyun Eun, Minwoo 
Jeong and Gary Geunbae Lee, ?A situation-based di-
alogue management using dialogue examples,? in 
Proceedings of International conference on Acoustics, 
Speech, and Signal Processing, Toulouse, 2006. 
 
Diane J. Litman and Shimei Pan, ?Empirically evaluat-
ing an adaptable spoken dialogue system,? in Pro-
ceedings of the 8th International Conference on 
Spoken Language Processing, pp. 2145-2148, 2004. 
 
Finale Doshi and Nicholas Roy, ?Efficient Model 
Learning for Dialog Management,? in Proceeding of 
the ACM/IEEE international conference on Human-
robot interaction, Washington DC, 2007. 
 
Jason D. Williams and Steve Young, "Scaling POMDPs 
for dialog management with composite summary 
point-based value iteration (CSPBVI)," in Proceed-
ings of AAAI Workshop on Statistical and Empirical 
Approaches for Spoken Dialogue Systems, Boston, 
2006. 
 
Jason D. Williams and Steve Young, " Partially Observ-
able Markov Decision Processes for Spoken Dialog 
Systems." Computer Speech and Language 21(2): 
231-422, 2007 
 
Karolina Eliasson, ?The Use of Case-Based Reasoning 
in a Human-Robot Dialog System?, Licentiate of 
Engineering Thesis of Link?ping Institute of Tech-
nology at Link?ping University, 2006 
 
Nicholas Roy, Geoffrey Gordon, and Sebastian Thrun, 
?Finding approximate pomdp solutions through be-
lief compression,? Journal of Artificial Intelligence 
Research, vol. 23, pp.1?40, 2005. 
 
Spt?phane Ross, Brahim Chaib-draa, ?AEMS: An Any-
time Online Search Algorithm for Approximate Poli-
cy Refinement in Large POMDPs?, in Proceedings 
of the 20th International Joint Conference on Artifi-
cial Intelligence, 2007 
 
Steve Young, Jost Schatzmann, Karl Weilhammer and 
Hui Ye, "The hidden information state approach to 
dialog management," in Proceedings of International 
Conference on Acoustics, Speech, and Signal 
Processing, Honolulu, 2007. 
 
Tim Paek and Eric Horvitz, ?Optimizing automated call 
routing by integrating spoken dialog models with 
queuing models,? in Proceedings of HLT-NAACL, pp. 
41-48, Boston, 2004. 
 
 
127
Coling 2008: Proceedings of the workshop on Speech Processing for Safety Critical Translation and Pervasive Applications, pages 9?16
Manchester, August 2008
An Integrated Dialog Simulation Technique for Evaluating Spoken Dialog
Systems
Sangkeun Jung, Cheongjae Lee, Kyungduk Kim, Gary Geunbae Lee
Department of Computer Science and Engineering
Pohang University of Computer Science and Technology(POSTECH)
San 31, Hyoja-Dong, Pohang, 790-784, Korea
{hugman, lcj80, getta, gblee}@postech.ac.kr
Abstract
This paper proposes a novel integrated dialog
simulation technique for evaluating spoken di-
alog systems. Many techniques for simulat-
ing users and errors have been proposed for
use in improving and evaluating spoken dia-
log systems, but most of them are not easily
applied to various dialog systems or domains
because some are limited to specific domains
or others require heuristic rules. In this pa-
per, we propose a highly-portable technique for
simulating user intention, utterance and Au-
tomatic Speech Recognition (ASR) channels.
This technique can be used to rapidly build a
dialog simulation system for evaluating spo-
ken dialog systems. We propose a novel user
intention modeling and generating method that
uses a linear-chain conditional random field, a
data-driven domain specific user utterance sim-
ulation method, and a novel ASR channel sim-
ulation method with adjustable error recogni-
tion rates. Experiments using these techniques
were carried out to evaluate the performance
and behavior of previously developed dialog
systems designed for navigation dialogs, and
it turned out that our approach is easy to set up
and shows the similar tendencies of real users.
1 Introduction
Evaluation of spoken dialog systems is essential for de-
veloping and improving the systems and for assessing
their performance. Normally, humans are used to eval-
uate the systems, but training and employing human
evaluators is expensive. Furthermore, qualified human
users are not always immediately available. These in-
evitable difficulties of working with human users can
cause huge delay in development and assessment of
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
spoken dialog systems. To avoid the problems that re-
sult from using humans to evaluate spoken dialog sys-
tems, developers have widely used dialog simulation,
in which a simulated user interacts with a spoken dia-
log system.
Many techniques for user intention, utterance and er-
ror simulation have been proposed. However, previ-
ously proposed simulation techniques cannot be eas-
ily applied to evaluate various dialog systems, because
some of these techniques are specially designed to work
with their own dialog systems, some require heuristic
rules or flowcharts, and others try to build user side
dialog management systems using specialized dialog
managing methods. These problems motivated us to
develop dialog simulation techniques which allow de-
velopers to build dialog simulation systems rapidly for
use in evaluating various dialog systems.
To be successful, a simulation approach should not
depend on specific domains or rules. Also it should not
be coupled to a specific dialog management method.
Furthermore, successful dialog simulation should fully
support both user simulation and environment simula-
tion. In user simulation, it must be capable of simu-
lating both user intentions and user utterances, because
user utterances are essential for testing the language un-
derstanding component of the dialog system. In addi-
tion to user simulation, environment simulation such as
ASR channel simulation is desirable because it allows
developers to test the dialog system in various acoustic
environments.
In this paper, we propose novel dialog simulation
techniques which satisfy these requirements. We in-
troduce a new user intention simulation method based
on the sequential graphical model, and a user utterance
simulator which can generate diverse natural user utter-
ances. The user intention and utterance simulators are
both fully data-driven approaches; therefore they have
high domain- and language portability. We also propose
a novel Automatic Speech Recognizer (ASR) channel
simulator which allows the developers to set the de-
sired speech recognition performance level. Through
a case study, we showed that our approach is feasible in
successful dialog simulation to evaluate spoken dialog
9
systems.
This paper is structured as follows. We first provide a
brief introduction of other dialog simulation techniques
and their differences from our approach in Section 2.
We then introduce the overall architecture and the de-
tailed methods of intention, utterance and ASR channel
simulation in Section 3. Experiments to test the simula-
tion techniques, and a case study are described in Sec-
tion 4. We conclude with a brief summary and suggest
directions for future work in Section 5.
2 Related Works
Dialog simulation techniques can be classified accord-
ing to the purpose of the simulation. One of the pur-
poses is to support the refinement of dialog strategies.
Some techniques use large amounts of simulated data
for a systematic exploration of the dialog state space
in the framework of reinforcement learning (Schatz-
mann et al, 2005; Schatzmann et al, 2007a). Other
techniques use simulation techniques to investigate and
improve the target dialog strategies by examining the
results heuristically or automatically (Chung, 2004;
Rieser and Lemon, 2006; Torres et al, 2008). A sec-
ond purpose of dialog simulation techniques is to eval-
uate the dialog system itself qualitatively. Eckert et al,
(1997) and Lo?pez-Co?zar et., (2003; 2006) used a dialog
simulation to evaluate whole dialog systems.
Dialog simulation techniques can also be classified
according to the layers of the simulation. Typically, di-
alog simulation can be divided into three layers: user
intention, user surface (utterance) and error simulation.
Some studies have focused only on the intention level
simulation (Rieser and Lemon, 2006; Schatzmann et
al., 2007b; Cuayahuitl et al, 2005). The main purpose
of those approaches was to collect and examine inten-
tion level dialog behavior for automatically learning di-
alog strategies. In this case, surface and error simula-
tions were neglected or simply accessed normally.
Another approach is to simulate both user intention
and surface. In this approach, user utterance generation
is designed to express a given intention. Chung (2004)
tried to use the natural language generation module
of (Seneff, 2002) to generate this surface. He used a
speech synthesizer to generate user utterances. Lo?pez-
Co?zar et., (2003; 2006) collected real human utter-
ances, and selected and played the voice to provide in-
put for the spoken dialog system. Both Chung (2004)
and Lo?pez-Co?zar et., (2003; 2006) used rule based in-
tention simulation. They used real ASR to recognize
the synthesized or played voice; hence, ASR channel
simulation is not needed in their techniques. Scheffler
and Young (2000; 2001) used the lattices which are de-
rived from the grammars used by the recognition en-
gine, but generated user utterances by associating the
lattice edges with intentions. During utterance gener-
ation, they simulated errors in recognition and under-
standing by probabilistic substitution on the selection of
the edge. Schatzmann et al, (2007a; 2007b) proposed a
statistical model for user utterance generation and error
simulation using agenda based intention simulation.
The existing rule-based techniques for simulating in-
tentions or surfaces are not appropriate in the sense of
portability criteria. In addition, specific dialog manag-
ing techniques based user simulators (e.g., (Torres et
al., 2008)) are not desirable because it is not easy to
implement these techniques for other developers. An-
other important criterion for evaluating dialog simula-
tion techniques for use in evaluating spoken dialog sys-
tems is the range of simulation layers. Simulations that
are restricted to only the intention level are not suffi-
cient to evaluate the whole dialog system. Domain and
language independent techniques for simulating both
intentions and utterances are needed, and ASR channel
simulation is desirable for evaluating the spoken dia-
log systems accurately because human-machine dialog
is heavily influenced by speech recognition errors.
3 Dialog Simulation Architecture for
Dialog System Evaluation
3.1 Overall Architecture
Typical spoken dialog systems deal with the dialog be-
tween a human user and a machine. Human users ut-
ter spoken language to express their intention, which is
recognized, understood and managed by ASR, Spoken
Language Understanding (SLU) and Dialog Manager
(DM). Conventionally, ASR has been considered to be
a component of dialog systems. However, in this re-
search, we do not include a real ASR module in the di-
alog system component because a real ASR takes only
fixed level of speech as an input. To use real voices,
we must either collect real human speech or generate
voices using a speech synthesizer. However, both ap-
proaches have limitations. When recording and play-
ing real human voices, the cost of data collection is
high and the simulator can simulate only the behav-
ior of the humans who were recorded. When using a
speech synthesizer, the synthesizer can usually generate
the speech of one person, on a limited variety of speech
behaviors; this means that the dialog system cannot be
evaluated under various conditions. Also, in both ap-
proaches, freely adjusting the speech recognition per-
formance level is difficult. In this research, instead of
using real speech we simulate the ASR channel and add
noises to a clean utterance from the user simulator to
mimic the speech recognition result.
The overall architecture of our dialog simulation sep-
arates the user simulator into two levels: user intention
simulator and utterance simulator (Fig. 1). The user
intention simulator accepts the discourse circumstances
with system intention as input and generates the next
user intention. The user utterance simulator constructs
a corresponding user sentence to express the given user
intention. The simulated user sentence is fed to the
ASR channel simulator, which then adds noises to the
utterance. This noisy utterance is passed to a dialog sys-
10
Dialog System
User Simulator
ASRChannelSimulator SLU
Dialog ManagerUser Intention Simulator
User Utterance Simulator
System IntentionDialog Logs
Evaluator EvaluationResults
= Simulated User = = System =
Figure 1: Overall architecture of dialog simulation
tem which consists of a SLU and a DM. The dialog sys-
tem understands the user utterance, manages dialog and
passes the system intention to the user simulator. User
simulator, ASR channel simulator and dialog system re-
peat the conversation until the user simulator generates
an end to the dialog.
After finishing simulating one dialog successfully,
this dialog is stored in Dialog Logs. If the dialog logs
contain enough dialogs, the evaluator uses the logs to
evaluate the performance of the dialog system.
3.2 User Intention Simulation
The task of user intention simulation is to generate sub-
sequent user intentions given current discourse circum-
stances. The intention is usually represented as ab-
stracted user?s goals and information on user?s utter-
ance (surface). In other words, generating the user?s
next semantic frame from the current discourse status
constitutes the user intention simulation.
Dialog is basically sequential behavior in which par-
ticipants use language to interact with each other. This
means that intentions of the user or the system are natu-
rally embedded in a sequential structure. Therefore, in
intention modeling we must consider how to model this
sequential property. Also, we must understand that the
user?s intention depends not only on previous n-gram
user and system intentions, but also on diverse dis-
course circumstances, including dialog goal, the num-
ber of items, and the number of filled component slots.
Sophisticated user intention modeling should be able to
reflect the discourse information.
To satisfy the sequential property and use rich
information for user intention modeling, we used
linear-chain Conditional Random Field (CRF) model
(Lafferty et al, 2001) for user intention modeling.
Let Y,X be random vectors, ? = {?
k
} ? R
K be a
parameter vector, and {f
k
(y, y
?
,x
t
)}
K
k=1
be a set of
real-valued feature functions. Then a linear-chain CRF
is a distribution of p(y|x) that takes the form
UI1
DI1
UI2
DI2
UIt
DIt
UIt+1
DIt+1
?
Figure 2: Conditional Random Fields for user intention
modeling. UI
t
: User Intention ; DI
t
: Discourse Infor-
mation for the tth user turn
p(y|x) =
1
Z(x)
exp
{
K
?
k=1
?
k
f
k
(y
t
, y
t?1
,x
t
)
} (1)
where Z(x) is an instance-specific normalization func-
tion.
Z(x) =
?
y
exp
{
K
?
k=1
?
k
f
k
(y
t
, y
t?1
,x
t
)
}
CRF is an undirected graphical model that defines a
single log-linear distribution over the joint probability
of an entire label sequence given a particular observa-
tion sequence. This single distribution removes the per-
state normalization requirement and allows entire state
sequences to be accounted for at once. This property is
well suited to model the entire sequence of intentions in
a dialog. Also, CRF is a conditional model, and not a
joint model (such as the Hidden Markov Model). Arbi-
trary facts can be captured to describe the observation
in the form of indicator functions. This means that CRF
allows us to use rich discourse information to model in-
tentions.
CRF has states and observations in each time line.
We represent the user intention as state and discourse
information as observations in CRF (Fig. 2). We rep-
resent the state as a semantic frame. For example in
the semantic frame representing the user intention for
the utterance ?I want to go to city hall? (Fig. 3), dia-
log act is a domain-independent label of an utterance at
the level of illocutionary force (e.g. statement, request,
wh question) and main goal is the domain-specific user
goal of an utterance (e.g. give something, tell purpose).
Component slots represent named entities in the utter-
ance. We use the cartesian product of each slot of se-
mantic frame to represent the state of the utterance in
our CRF model. In this example, the state symbol is
?request?search loc?loc name?.
For the observation, we can use various discourse
events because CRF allows using rich information by
interpreting each event as an indicator function. Be-
cause we pursue the portable dialog simulation tech-
nique, we separated the features of the discourse in-
formation into those that are domain independent and
those that are domain dependent. Domain independent
11
I want to go to city hall.requestsearch_loccityhall
I/PRP want/VB to/TO go/VB to/TO [loc_name]/[loc_name]PRP, VB, TO, VB, TO, [loc_name]
I, want, to, go, to, [loc_name]
Structure PRP ? VB ? TO ? VB ? TO ? [loc_name]I ? want ? to ? go ? to ? [loc_name]
Semantic Frame for User Inention Simulation
Preprocessing Information for User Utterance Simulation
Structure Tags
Word Vocabulary
processed  utterance
Generation Target for User Utterance Simulation
Word Sequence
raw user utterance dialog_act main_goal component.[loc_name] 
Figure 3: Example of semantic frame for user inten-
tion, and preprocessing and generation target for user
utterance simulation.
features include discourse information which is not rel-
evant to the specific dialog domain and system. For ex-
ample, previous system acts in Fig. 4 are not dependent
on specific dialog domain. The actual values of pre-
vious system acts could be dependent on each dialog
domain and system, but the label itself is independent
because every dialog system has system parts and corre-
sponding system acts. In contrast, domain specific dis-
course information exists for each dialog system. For
example, in the navigation domain (Fig. 4), the cur-
rent position of the user or the user?s favorite restau-
rant could be very important for generating the user?s
intention. This information is dependent on the spe-
cific domain and system. We handle these features as
?OTHER INFO?.
We trained the user intention model using dialog ex-
amples of human-machine. One training example con-
sists of a sequence of user intentions and discourse in-
formation features in a given dialog. We collected train-
ing examples and trained the intention model using a
typical CRF training method, a limited-memory quasi-
Newton code for unconstrained optimization (L-BFGS)
of (Liu and Nocedal, 1989).
To generate user intentions given specific discourse
circumstances, we calculate the probability of a se-
quence of user intentions from the beginning of the
dialog to the corresponding turn. For example, sup-
pose that we need to generate user intention at the
third turn (UI
3
) (Fig. 2). We have previously sim-
ulated user intentions UI
1
and UI
2
using DI
1
and
DI
2
. In this case, we calculate the probability of
UI
1
? UI
2
? UI
3
given DI
1
, DI
2
and DI
3
. No-
tice that DI
3
contains discourse information at the third
turn: it includes previous system intention, attributes
and other useful information. Using the algorithm (Fig.
5) we generate the user intention at turn t. The proba-
bility of P (UI
1
, UI
2
, . . . , UI
t
|DI
1
, DI
2
, . . . , DI
t
) is
calculated using the equation (1). In the genera-
tion of user intention at t turn, we do not select the
UI
t
which has higher probability. Instead, we se-
lect UI
t
randomly based on the probability distribution
PREV_1_SYS_ACT previous system action.
Ex) PREV_1_SYS_ACT=confirm
PREV_1_SYS_ACT_ATTRIBUTES previous system mentioned attributes. 
Ex) PREV_1_SYS_ACT_attributes=city_name
PREV_2_SYS_ACT previous system action. 
Ex) PREV_2_SYS_ACT=confirm
PREV_2_SYS_ACT_ATTRIBUTES previous system mentioned attributes.
Ex) PREV_2_SYS_ACT_attributes=city_name
SYSTEM_HOLDING_COMP_SLOT system recognized component slot. 
Ex) SYSTEM_HOLDING_COMP_SLOT=loc_name
OTHER_INFO other useful domain dependent information
Ex) OTHER_INFO(user_fav_rest)=gajokjung
Domain Independent Features
Domain Dependent Features
Figure 4: Example feature design for navigation do-
main
UI t   ? user intention at t turn
S  ? user intentions set (UI t  ? S )
UI 1 , UI 2 , ? , UI t-1  ? already simulated user intention sequence
DI 1 , DI 2 , ? , DI t  ? discourse information from 1 to t  turn
For each UI t  in S
       Calculate P( UI 1 , UI 2 , ?, UI t |DI 1 , DI 2 , ?, DI t )
UI t  ? random user intention from P( UI 1 , UI 2 , ?, UI t |DI 1 , DI 2 , ?, DI t )
Figure 5: User intention generation algorithm
P (UI
1
, UI
2
, . . . , UI
t
|DI
1
, DI
2
, . . . , DI
t
) because we
want to generate diverse user intention sequence given
the same discourse context. If we select UI
t
which has
highest probability, user intention simulator always re-
turns the same user intention sequence.
3.3 User Utterance Simulation
Utterance simulation generates surface level utterances
which express a given user intention. For example, if
users want to go somewhere and provide place name
information, we need to generate corresponding utter-
ances (e.g. ?I want to go to [place name] or ?Let?s go to
[place name]?). We approach the task of user utterance
simulation by assuming that the types of structures and
the vocabulary are limited when we make utterances to
express certain context and intention in a specific do-
main, and that humans express their intentions by re-
combining and re-aligning these structures and vocabu-
laries.
To model this process, we need to collect the types of
structures and vocabularies. For this, we need to define
the context space. We define the structure and vocabu-
lary space as a production of dialog act and main goal.
In an example of semantic frame for the utterance ?I
want to go to city hall? (Fig. 3), the structure and vocab-
ulary (SV) space ID is ?request # search loc?, which is
produced by the dialog act and the main goal. We col-
lect structure tags, which consist of a part of speech
tag, a component slot tag, and a vocabulary that cor-
responds to SV space. For example (Fig. 3), structure
12
1. Repeat generate  S t  based on PSV(S t+1 |S t ),    until S T  = <setence_end> , where S t  ? S  ,   t=1,2,3,?.T .2. Generate W t  based on PSV (W t |S t ), where    t =1,2,3,..,T  , W t  ? V3. The generation word sequence W ={W1,W2,..,WT} is inserted    into the set of generated utterance U4. Repeat 1  to 3  for Max_Generation_Number    times, Max_Generation_Number is given by developers
1.Rescore the utterance U k  in the set of U  by the measure2.Select top n-best
First Phase ? Generating Structures and Words given SV space 
Second Phase ? Selection by measure 
Figure 6: Algorithm of user utterance simulation
tags include PRP, VB, TO, VB as a part of speech tag
and [loc name] as a component slot tag. The vocab-
ulary includes I, want, to, go, and [loc name]. In the
vocabulary, every named-entity word is replaced with
its category name.
In this way, we can collect the structure tags and vo-
cabulary for each SV space from the dialog logs. For
the given SV space, we estimate probability distribu-
tions for statistical user utterance simulation using a
training process. For each space, we estimate tag tran-
sition probability P
SV
(S
t+1
|S
t
) and collect structure
tags set S
SV
and vocabularies V
SV
.
We devised a two-phase user utterance generation al-
gorithm (Fig. 6). Symbols are as follows. The detail
explanation of Fig. 6 will be followed in the next sub-
sections.
? S
SV
: structure tag set for given SV
? V
SV
: vocabularies for given SV
? S
i
: structure tag, i = 0, ..., T, S
i
? S
SV
? W
i
: word, i = 0, ..., T, W
i
? V
SV
? W
seq
: generated word sequence. W
seq
=
(W
1
,W
2
, ...,W
T
)
? U
k
: k-th sampled utterance,
k = 1, ..., Max Sampling Number, U
k
? U
3.3.1 First Phase - Generating Structure and
Word Sequence
We generate the structure tag S
1
based on the prob-
ability of P
SV
(S
1
| < sentence start >) and then
S
1
influences the generating of S
2
after P
SV
(S
2
|S
1
).
In this way, a structure tag chain is generated sequen-
tially based on the structure tag transition probability
P
SV
(S
t+1
|S
t
) until the last generated structure tag S
T
is < sentence end >. We assume that the current
structure tag has a first order Markov property, which
means that the structure tag is only influenced by the
previous structure tag. After the structure tags are
generated, the emission probability P
SV
(W
t
|S
t
)(w =
1, . . . , T ) is used to generate the word sequence given
the tag sequence. We iterate the process of generating
structures and word sequences sufficient times to gen-
erate many different structure tags and word sequences
which may occur in real human expressions. Select-
ing natural utterances from the generated utterances re-
quires an automatic evaluation metric.
3.3.2 Second Phase - Selection by the BLEU
measure
To measure the naturalness of the generated utter-
ances, we use the BLEU (Bilingual Evaluation Under-
study) score (Papineni et al, 2001) which is widely
used for automatic evaluation in Statistical Machine
Translation (SMT). In SMT, translated candidate sen-
tences are evaluated by comparing semantically equiv-
alent reference sentences which have been translated
by a human. Evaluation of the user utterance gener-
ation shares the same task of evaluation in SMT. We
can evaluate the naturalness of generated utterances by
comparing semantically equivalent reference utterances
collected by humans. Therefore, the BLEU score can
be adopted successfully to measure the naturalness of
the utterances.
The BLEU score is the geometric mean of the n-gram
precisions with a brevity penalty. The original BLEU
metric is used to evaluate translated sentences by com-
paring them to several reference sentences. We mod-
ified the BLEU metric to compare one generated ut-
terance with several reference utterances. To rescore
the generated utterances, we used the Structure and
Word interpolated BLEU score (SWB). After the first
phase, we obtain generated utterances which have both
structure and word sequence. To measure the natu-
ralness of a generated utterance, we check both struc-
tural and lexical naturalness. We calculated Struc-
ture Sequence BLEU score using the generated struc-
ture tags sequences instead of words sequences with the
reference structure tag sequences of the SV space in the
BLEU calculation process. The Word Sequence BLEU
is calculated by measuring BLEU score using the gener-
ated words sequence with the reference word sequences
of the SV space. SWB is calculated as:
SWB = ? ? Structure Sequence BLEU
+(1? ?) ? Word Sequence BLEU
In this study, we set ? = 0.5. Using SWB, we select
the top 20-best generated utterances and return a corre-
sponding generated utterance by selecting one of them
randomly.
3.4 ASR channel Simulation
ASR channel simulation generates speech recognition
errors which might occur in the real speech recognition
process. In this study, we simulate the ASR channel and
modify the generated clean utterance to a speech rec-
ognized erroneous utterance. Successful ASR channel
simulation techniques should have the following prop-
erties: the developer should be able to set the simu-
lated word error rate (WER) between 0% ? 100%; the
simulated errors should be generated based on realistic
13
phone-level and word-level confusions; and the tech-
nique should be easily adapted to new tasks, at low cost.
Our ASR channel simulation approach is designed
to satisfy these properties. The proposed ASR channel
simulation method involved four steps: 1) Determining
error position 2) Generating error types on error marked
words. 3) Generating ASR errors such as substitution,
deletion and insertion errors, and 4) Rescoring and se-
lecting simulated erroneous utterances (Fig. 7 for Ko-
rean language example).
In the first step, we used the WER to determine the
positions of erroneous words. For each word, we ran-
domly generate a number between 0 and 1. If this num-
ber is between 0 and WER, we mark the word Error
Word (1); otherwise we mark the word Clean Word (0).
In the second step, we generate ASR error types for the
error marked words based on the error type distribution.
In the third step, we generate various types of ASR er-
ror. In the case of deletion error, we simply delete the
error marked word from the utterance. In the case of
insertion error, we select one word from the pronunci-
ation dictionary randomly, and insert it before the error
marked word. In the case of substitution error, we use a
more complex process to select a substitutable word.
To select a substitutable word, we compare the
marked error word with the words from pronunciation
dictionary which are similar in syllable sequence and
phoneme sequence. First, we convert the final word
sequence from the user simulator into a phoneme se-
quence using a Grapheme-to-Phoneme (G2P) module
(Lee et al, 2006). Then, we extract a part of the
phoneme sequence which is similar to the error marked
word from the entire phoneme sequence of the ut-
terance. The reason for extracting a target phoneme
sequence corresponding to one word from the entire
phoneme sequence is that the G2P results vary between
the boundaries of words. Then, we separate the marked
word into syllables and compare their syllable-level
similarity to other words in the pronunciation dictio-
nary. We calculate a similarity score which interpolates
syllable and phoneme level similarity using following
equations.
Similarity = ? ? Syllable Alignment Score
+(1? ?) ? Phone Alignment Score
We used the dynamic global alignment algorithm of
(Needleman and Wunsch, 1970) for both syllable and
phoneme sequence alignment. This alignment algo-
rithm requires a weight matrix. As a weight matrix,
we used a vowel confusion matrix which is based on
the manner of articulation. We consider the position
(back/front, high/mid/low) of the tongue and the shape
(round/flat) of the lips. We select candidate words
which have higher similarity than an arbitrary thresh-
old ? and replace the error marked word with a random
word from this set. We repeat steps 1 to 3 many times
(usually 100) to collect error added utterances.
In the fourth step, we rescore the error added utter-
si-chung e ga go sip eo (I want to go to city hall)si-cheong e ga go sip eo0 1 1 0 1 0- del sub - sub -Generating Error Types and Positions
Generating Candidate Lists of Noisy Utterance si-cheong - geo-gi go si eosi-cheong - ga-ja go seo eosi-cheong - gat go seu eosi-cheong - geot go sil eo
Selecting Noisy Utterance si-cheong gat go seo eo
Error Generation
Ranking with LM score
1-Step2-Step
3-Step
4-Step
Figure 7: Example of ASR channel simulation
ances using the language model (LM) score. This LM
is trained using a domain corpus which is usually used
in ASR. We select top n-best erroneous utterances (we
set n=10) and choose one of them randomly. This utter-
ance is the final result of ASR channel simulator, and is
fed into the dialog system.
4 Experiments
We proposed a method that user intention, utterance and
ASR channel simulation to rapidly assemble a simula-
tion system to evaluate dialog systems. We conducted
a case study for the navigation domain Korean spoken
dialog system to test our simulation method and exam-
ine the dialog behaviors using the simulator. We used
100 dialog examples from real user and dialog system
to train user intention and utterance simulator. We used
the SLU method of (Jeong and Lee, 2006), and dia-
log management method of (Kim et al, 2008) to build
the dialog system. After trained user simulator, we per-
form simulation to collect 5000 dialog samples for each
WER settings (WER = 0 ? 40 %).
To verify the user intention and utterance simula-
tion quality, we let two human judges to evaluate 200
randomly chosen dialogs and 1031 utterances from the
simulated dialog examples (WER=0%). At first, they
evaluate a dialog with three scale (1: Unnatural, 2: Pos-
sible, 3: Natural), then evaluate the utterances of a dia-
log with three scale (1: Unclear, 2: Understandable, 3:
Natural).
The inter evaluator agreement (kappa) is 0.45 and
0.58 for dialog and utterance evaluation respectively,
which show the moderate agreement (Fig. 8). Both
judges show the positive reactions for the quality of user
intention and utterance, the simulated dialogs can be
possibly occurred, and the quality of utterance is close
to natural human utterance.
We also did regression analysis with the results of
human evaluation and the SWB score to find out the
relationship between SWB and human judgment. Fig.
9 shows the result of polynomial regression (order 3)
result. It shows that ?Unclear? utterance might have 0.5
14
Human 1 Human 2 Average Kappa
Dialog 2.38 2.22 2.30 0.45
Utterance 2.74 2.67 2.71 0.58
Figure 8: Human evaluation results on dialog and utter-
ance
0.34
0.44
0.54
0.64
0.74
0.84
0.94
1 1.5 2 2.5 3
SW
B S
co
re
Average human evaluation for user utterances
Figure 9: Relationship between SWB score and human
judgment
? 0.7 SWB score, ?Possible? and ?Natural? simulated
utterance might have over 0.75. It means that we can
simulate good user utterance if we constrain the user
simulator with the threshold around 0.75 SWB score.
To assess the ASR channel simulation quality, we
compared how SLU of utterances was affected by
WER. SLU was quantified according to sentence er-
ror rate (SER) and concept error rate (CER). Compared
to WER set by the developer, measured WER was the
same, SER increased more rapidly, and CER increased
more slowly (Fig. 10). This means that our simula-
tion framework models SLU errors effective as well as
speech recognition errors.
Fig. 11 shows the overall dialog system behaviors us-
ing the user simulator and ASR channel simulator. As
the WER rate increased, dialog system performance de-
creased and dialog length increased. This result is sim-
ilar as observed to the dialog behaviors in real human-
0 10 20 30 40 50
WER measured 0 10 19.71 29 39.06 49.21
SER 0 33.28 56.6 70.91 81.29 88
CER 1.9087 10.1069 18.3183 26.1619 34.4322 41.755
0
10
20
30
40
50
60
70
80
90
100
err
 rat
e(%
)
Figure 10: Relationship between given WER and mea-
sured other error rates. X-axis = WER fixed by ASR
channel(%)
7.7
8.1
8.5
8.9
9.3
0.50
0.60
0.70
0.80
0.90
1.00
0 5 10 15 20 25 30 35 40
Avg
. Di
alog
 Len
gth
 (tu
rns
)
SLU
 acc
ura
cy 
and
 TC
R
Word Error Rate (%)
SLU
TCR
Length
Figure 11: Dialog simulation result on navigation do-
main
machine dialog.
5 Conclusion
This paper presented novel and easy to build dialog sim-
ulation methods for use in evaluation of spoken dia-
log systems. We proposed methods of simulating utter-
ances and user intentions to replace real human users,
and introduced an ASR channel simulation method that
acts as a real speech recognizer. We introduce a method
of simulating user intentions which is based on the CRF
sequential graphical model, and an utterance simulator
that generates user utterances. Both user intention and
utterance simulators use a fully data-driven approach;
therefore, they have high domain- and language porta-
bility. We also proposed a novel ASR channel sim-
ulator which allows the developers to set the speech
recognition performance level. We applied our meth-
ods to evaluate a navigation domain dialog system; ex-
perimental results show that the simulators successfully
evaluated the dialog system, and that simulated inten-
tion, utterance and errors closely match to those ob-
served in real human-computer dialogs. We will apply
our approach to other dialog systems and bootstrap new
dialog system strategy for the future works.
6 Acknowledgement
This research was supported by the Intelligent Robotics
Development Program, one of the 21st Century Frontier
R&D Programs funded by the Ministry of Knowledge
Economy of Korea.
References
Chung, G. 2004. Developing a flexible spoken dialog
system using simulation. Proc. ACL, pages 63?70.
Cuayahuitl, H., S. Renals, O. Lemon, and H. Shi-
modaira. 2005. Human-Computer Dialogue Sim-
ulation Using Hidden Markov Models. Automatic
15
Speech Recognition and Understanding, 2005 IEEE
Workshop on, pages 100?105.
Eckert, W., E. Levin, and R. Pieraccini. 1997. User
modeling for spoken dialogue system evaluation.
Automatic Speech Recognition and Understanding,
1997. Proceedings., 1997 IEEE Workshop on, pages
80?87.
Jeong, M. and G. Lee. 2006. Jointly Predicting
Dialog Act and Named Entity for Statistical Spo-
ken Language Understanding. Proceedings of the
IEEE/ACL 2006 workshop on spoken language tech-
nology (SLT).
Kim, K., C. Lee, S Jung, and G. Lee. 2008. A
frame-based probabilistic framework for spoken di-
alog management using dialog examples. In the 9th
sigdial workshop on discourse and dialog (sigdial
2008), To appear.
Lafferty, J.D., A. McCallum, and F.C.N. Pereira. 2001.
Conditional Random Fields: Probabilistic Models
for Segmenting and Labeling Sequence Data. Pro-
ceedings of the Eighteenth International Conference
on Machine Learning table of contents, pages 282?
289.
Lee, J., S. Kim, and G.G. Lee. 2006. Grapheme-to-
Phoneme Conversion Using Automatically Extracted
Associative Rules for Korean TTS System. In Ninth
International Conference on Spoken Language Pro-
cessing. ISCA.
Liu, D.C. and J. Nocedal. 1989. On the limited
memory BFGS method for large scale optimization.
Mathematical Programming, 45(1):503?528.
Lo?pez-Co?zar, R., A. De la Torre, JC Segura, and AJ Ru-
bio. 2003. Assessment of dialogue systems by
means of a new simulation technique. Speech Com-
munication, 40(3):387?407.
Lo?pez-Co?zar, Ramo?n, Zoraida Callejas, and Michael
Mctear. 2006. Testing the performance of spoken di-
alogue systems by means of an artificially simulated
user. Artif. Intell. Rev., 26(4):291?323.
Needleman, SB and CD Wunsch. 1970. A general
method applicable to the search for similarities in the
amino acid sequence of two proteins. J Mol Biol,
48(3):443?53.
Papineni, K., S. Roukos, T. Ward, and WJ Zhu.
2001. BLEU: a method for automatic evaluation of
MT. Research Report, Computer Science RC22176
(W0109-022), IBM Research Division, TJ Watson
Research Center, 17.
Rieser, V. and O. Lemon. 2006. Cluster-Based User
Simulations for Learning Dialogue Strategies. In
Ninth International Conference on Spoken Language
Processing. ISCA.
Schatzmann, J., K. Georgila, and S. Young. 2005.
Quantitative Evaluation of User Simulation Tech-
niques for Spoken Dialogue Systems. In 6th SIGdial
Workshop on Discourse and Dialogue. ISCA.
Schatzmann, J., B. Thomson, and S. Young. 2007a.
Error simulation for training statistical dialogue sys-
tems. Automatic Speech Recognition & Understand-
ing, 2007. ASRU. IEEE Workshop on, pages 526?
531.
Schatzmann, J., B. Thomson, and S. Young. 2007b.
Statistical User Simulation with a Hidden Agenda.
Proc. SIGDial, Antwerp, Belgium.
Scheffler, K. and S. Young. 2000. Probabilistic simula-
tion of human-machine dialogues. Proc. of ICASSP,
2:1217?1220.
Scheffler, K. and S. Young. 2001. Corpus-based dia-
logue simulation for automatic strategy learning and
evaluation. Proc. NAACL Workshop on Adaptation
in Dialogue Systems, pages 64?70.
Seneff, S. 2002. Response planning and generation
in the Mercury flight reservation system. Computer
Speech and Language, 16(3):283?312.
Torres, Francisco, Emilio Sanchis, and Encarna
Segarra. 2008. User simulation in a stochastic di-
alog system. Comput. Speech Lang., 22(3):230?255.
16
