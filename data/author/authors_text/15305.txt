The University of Alicante systems at SENSEVAL-3?
Sonia Va?zquez, Rafael Romero
Armando Sua?rez and Andre?s Montoyo
Dpt. of Software and Computing Systems
Universidad de Alicante, Spain
{svazquez,romero}@dlsi.ua.es
{armando,montoyo}@dlsi.ua.es
Iulia Nica and Antonia Mart?? ?
Dpt. of General Linguistics
Universidad de Barcelona, Spain
iulia@clic.fil.ub.es
amarti@ub.edu
Abstract
The DLSI-UA team is currently working on sev-
eral word sense disambiguation approaches, both
supervised and unsupervised. These approaches are
based on different ways to use both annotated and
unannotated data, and several resources generated
from or exploiting WordNet (Miller et al, 1993),
WordNet Domains, EuroWordNet (EWN) and addi-
tional corpora. This paper presents a view of differ-
ent system results for Word Sense Disambiguation
in different tasks of SENSEVAL-3.
1 Introduction
Word Sense Disambiguation (WSD) is an open re-
search field in Natural Language Processing (NLP).
The task of WSD consists in assigning the correct
sense to words in a particular context using an elec-
tronic dictionary as the source of words definitions.
This is a difficult problem that is receiving a great
deal of attention from the research community.
At the Second International Workshop on
Evaluating Word Sense Disambiguation Systems,
SENSEVAL-2, several supervised and unsupervised
systems took part. The more successful systems re-
lay on corpus-based and supervised learning meth-
ods. At SENSEVAL-2 the average level of accu-
racy achieved rounded 70%, which is insufficient
for such other NLP tasks as information retrieval,
machine translation, or question answering.
The DLSI-UA systems were applied to three
SENSEVAL-3 tasks: English all-words, English lex-
ical sample and Spanish Lexical Sample. Our sys-
tems use both corpus-based and knowledge-based
approaches: Maximum Entropy(ME) (Lau et al,
1993; Berger et al, 1996; Ratnaparkhi, 1998) is
a corpus-based and supervised method based on
linguistic features; ME is the core of a bootstrap-
ping algorithm that we call re-training inspired
? This paper has been partially supported by the Spanish
Government (CICyT) under project number TIC-2003-7180
and the Valencia Government (OCyT) under project number
CTIDIB-2002-151
by co-training (Blum and Mitchell, 1998); Rele-
vant Domains (RD) (Montoyo et al, 2003) is a
resource built from WordNet Domains (Magnini
and Cavaglia, 2000) that is used in an unsuper-
vised method that assigns domain and sense la-
bels; Specification Marks(SP) (Montoyo and Palo-
mar, 2000) exploits the relations between synsets
stored in WordNet (Miller et al, 1993) and does not
need any training corpora; Commutative Test (CT)
(Nica et al, 2003), based on the Sense Discrimi-
nators device derived from EWN (Vossen, 1998),
disambiguates nouns inside their syntactic patterns,
with the help of information extracted from raw cor-
pus.
A resume of which methods and how were used
in which SENSEVAL-3 tasks is shown in Table 1.
DLSI-UA Method Combined
Systems Results
ALL-NOSU RD No
LS-ENG-SU Re-t No
LS-ENG-
NOSU
RD No
LS-SPA-SU ME+Re-t No
LS-SPA-NOSU SM + ME Nouns: SM
Verbs and adj.: ME
LS-SPA- Pattern-Nica Nouns: SM
PATTERN + ME Verbs and adj.: ME
Table 1: DLSI-UA Systems at SENSEVAL-3
Most of these methods are relatively new and our
goal when participating at SENSEVAL-3 is to evalu-
ate for the first time such approaches. At the mo-
ment of writing this paper we can conclude that
these are promising contributions in order to im-
prove current WSD systems.
In the following section each method is described
briefly. Then, details of how the SENSEVAL-3 train
and testing data were processed are shown. Next,
the scores obtained by each system are explained.
Finally, some conclusions and future work are pre-
sented.
                                             Association for Computational Linguistics
                        for the Semantic Analysis of Text, Barcelona, Spain, July 2004
                 SENSEVAL-3: Third International Workshop on the Evaluation of Systems
2 Methods and Algorithms
In this section we describe the set of methods and
techniques that we used to build the four systems
that had participated in SENSEVAL-3.
2.1 Re-training and Maximum Entropy
In this section, we describe our bootstrapping
method, which we call re-training. Our method
is derived from the co-training method. Our re-
training system is based on two different views of
the data (as is also the case for co-training), de-
fined using several groups of features from those de-
scribed in Figure 1, with several filters that ensure a
high confidence sense labelling.
? the target word itself
? lemmas of content-words at positions ?1, ?2, ?3
? words at positions ?1, ?2,
? words at positions ?1, ?2, ?3
? content-words at positions ?1, ?2, ?3
? POS-tags of words at positions ?1, ?2, ?3
? lemmas of collocations at positions (?2,?1),
(?1,+1), (+1,+2)
? collocations at positions (?2,?1), (?1,+1),
(+1,+2)
? lemmas of nouns at any position in context, occur-
ring at least m% times with a sense
? grammatical relation of the target word
? the word that the target word depends on
? the verb that the target word depends on
? the target word belongs to a multi-word, as identi-
fied by the parser
? ANPA codes (Spanish only)
? IPTC codes (Spanish only)
Figure 1: Features Used for the Supervised Learn-
ing
These two views consist of two weak ME learn-
ers, based on different sets of linguistic features,
for every possible sense of a target word. We de-
cided to use ME as the core of our bootstrapping
method because it has shown to be competitive in
WSD when compared to other machine learning ap-
proaches (Sua?rez and Palomar, 2002; Ma`rquez et
al., 2003).
The main difference with respect co-training is
that the two views are used in parallel in order to
get a consensus of what label to assign to a particu-
lar context. Additional filters will ultimately deter-
mine which contexts will then be added to the next
training cycle.
Re-training performs several binary partial train-
ings with positive and negative examples for each
sense. These classifications must be merged in a
unique label for such contexts with enough evidence
of being successfully classified. This ?evidence? re-
lies on values of probability assigned by the ME
module to positive and negative labels, and the fact
that the unlabeled example is classified as positive
for a unique sense only. The set of new labeled ex-
amples feeds the training corpora of the next itera-
tion with positive and negative examples. The stop-
ping criteria is a certain number of iterations or the
failure to obtain new examples from the unlabeled
corpus.
2.2 Specification Marks
Specification Marks is an unsupervised WSD
method over nouns. Its context is the group of words
that co-occur with the word to be disambiguated in
the sentence and their relationship to the noun to
be disambiguated. The disambiguation is resolved
with the use of the WordNet lexical knowledge base.
The underlying hypothesis of the method we
present here is that the higher the similarity between
two words, the larger the amount of information
shared by two concepts. In this case, the informa-
tion commonly shared by two concepts is indicated
by the most specific concept that subsumes them
both in the taxonomy.
The input for the WSD module is a group of
nouns W = {w1, w2, ..., wn} in a context. Each
word wi is sought in WordNet, each having an asso-
ciated set of possible senses Si ={Si1, Si2, ..., Sin},
and each sense having a set of concepts in the IS-A
taxonomy (hypernymy/hyponymy relations). First,
the common concept to all the senses of the words
that form the context is gathered. This concept is
marked by the initial specification mark (ISM). If
this initial specification mark does not resolve the
ambiguity of the word, we then descend through
the WordNet hierarchy, from one level to another,
assigning new specification marks. The number of
concepts contained within the subhierarchy is then
counted for each specification mark. The sense that
corresponds to the specification mark with the high-
est number of words is the one chosen as the sense
disambiguated within the given context
We define six heuristics for our system: Heuris-
tic of Hypernym, Heuristic of Definition, Heuristic
of Common Specification Mark, Heuristic of Gloss
Hypernym, Heuristic of Hyponym and Heuristic of
Gloss Hyponym.
2.3 Relevant Domains
This is an unsupervised WSD method based on the
WordNet Domains lexical resource (Magnini and
Cavaglia, 2000). The underlying working hypoth-
esis is that domain labels, such as ARCHITEC-
TURE, SPORT and MEDICINE provide a natural
way to establish semantic relations between word
senses, that can be used during the disambiguation
process. This resource has already been used on
Word Sense Disambiguation (Magnini and Strappa-
rava, 2000), but it has not made use of glosses infor-
mation. So our approach make use of a new lexical
resource obtained from glosses information named
Relevant Domains.
First step is to obtain the Relevant Domains re-
source from WordNet glosses. For this task is nec-
essary a previous part-of-speech tagging of Word-
Net glosses (each gloss has associated a domain la-
bel). So we extract all nouns, verbs, adjectives and
adverbs from glosses and assign them their associ-
ated domain label. With this information and using
the Association Ratio formula (w=word,D=domain
label), in (1), we obtain the Relevant Domains re-
source.
AR(w,D) = Pr(w|D)log2Pr(w|D)Pr(w) (1)
The final result is for each word, a set of domain
labels sorted by Association Ratio, for example,
for word plant? its Relevant Domains are: genetics
0.177515, ecology 0.050065, botany 0.038544 . . . .
Once obtained Relevant Domains the disam-
biguation process is carried out. We obtain from
the text source the context words that co-occur with
the word to be disambiguated (context could be
a sentence or a window of words). We obtain a
context vector from Relevant Domains and context
words (in case of repeated domain labels, they are
weighted). Furthermore we need a sense vector ob-
tained in the same way as context vector from words
of glosses of each word sense. We select the cor-
rect sense using the cosine measure between con-
text vector and sense vectors. So the selected sense
is that for which the cosine with the context vector
is closer to one.
2.4 Pattern-Nica
This is an unsupervised method only for Spanish
nouns exploiting both EuroWordNet and corpus.
In this method we adopt a different approach to
WSD: the occurrence to be disambiguated is con-
sidered not separately, but integrated into a syn-
tactic pattern, and its disambiguation is carried
out in relation to this pattern. A syntactic pat-
tern is a triplet X-R-Y, formed by two lexical con-
tent units X and Y and an eventual relational el-
ement R, which corresponds to a syntactic rela-
tion between X and Y. Examples: [X=canal-noun
R=de-preposition Y=televisio?n-noun], [X=pasaje-
noun R=? Y=ae?reo-adjective]. The strategy is
based on the hypothesis that syntactic patterns in
which an ambiguous occurrence participates have
decisive influence on its meaning. We also assume
that inside a syntactic pattern a word will tend to
have the same sense: the ?quasi one sense per syn-
tactic pattern? hypothesis. The method works as fol-
lows:
Step 1, the identification of the syntactic patterns
of the ambiguous occurrence;
Step 2, the extraction of information related to it:
from corpus and from the sentential context;
Step 3, the application of the WSD algorithm on
the different information previously obtained;
Step 4, the final sense assignment by combining
the partial sense proposals from step 3.
For step 1, we POS-tag the test sentence and ex-
tract the sequences that correspond to previously de-
fined combinations of POS tags. We only kept the
patterns with frequency 5 or superior.
In step 2, we use a search corpus previously POS-
tagged. For every syntactic pattern of the ambigu-
ous occurrence X, we obtain from corpus two sets of
words: the substitutes of X into the pattern (S1) and
the nouns that co-occur with the pattern in any sen-
tence from the corpus (S2), In both cases, we keep
only the element with frequency 5 or superior.
We perform step 3 by means of the heuristics de-
fined by the Commutative Test (CT) algorithm ap-
plied on each set from 2. The algorithm is related
to the Sense Discriminators (SD) lexical device, an
adaptation of the Spanish WordNet, consisting in a
set of sense discriminators for every sense of a given
noun in WordNet. The Commutative Test algorithm
lays on the hypothesis that if an ambiguous occur-
rence can be substituted in a syntactic pattern by a
sense discriminator, then it can have the sense cor-
responding to that sense discriminator.
For step 4, we first obtain a sense assignment in
relation with each syntactic pattern, by intersecting
the sense proposals from the two heuristics corre-
sponding to a pattern; then we choose the most fre-
quent sense between those proposed by the differ-
ent syntactic patterns; finally, if there are more final
proposed senses, we choose the most frequent sense
on the base of sense numbers in WordNet.
The method we propose for nouns requires only a
large corpus, a minimal preprocessing phase (POS-
tagging) and very little grammatical knowledge, so
it can easily be adapted to other languages. Sense
assignment is performed exploiting information ex-
tracted from corpus, thus we make an intensive use
of sense untagged corpora for the disambiguation
process.
3 Tasks Processing
At this point we explain for each task the systems
processing. The results of each system are shown in
Table2:
DLSI-UA Systems Precision Recall
LS-SPA-SU 84% 84%
LS-ENG-SU 82% 32%
ALL-NOSU 34% 28%
LS-ENG-NOSU 32% 20%
LS-SPA-NOSU 62% 62%
LS-SPA-PATTERN 84% 47%
Table 2: Results at SENSEVAL-3
3.1 DLSI-UA-LS-SPA-SU
Our system, based on re-training and maximum en-
tropy methods, processed both sense labelled and
unlabelled Spanish Lexical Sample data in three
consecutive steps:
Step 1, analyzing the train corpus: words which
most frequent sense is under 70% were selected.
For each one of these words, each feature was used
in a 3-fold cross-validation in order to determine the
best set of features for re-training.
Step 2, feeding training corpora: for these se-
lected words, based on the results of the previous
step, each training corpus was enriched with new
examples from the unlabelled data using re-training.
Step 3, classifying the test data: for the selected
words, re-training was used again to obtain a first set
of answers with, a priori, a label with a high level of
confidence; the remaining contexts that re-training
could not classify were processed with the ME sys-
tem using a unique set of features for all words.
The lemmatization and POS information supplied
into the SENSEVAL-3 Spanish data were the infor-
mation used for defining the features of the system.
0ur system obtained an accuracy of 0.84 for the
Spanish lexical sample task. Unfortunately, a shal-
low analysis of the answers revealed that the UA.5
system performed slightly worse than if only the ba-
sic ME system were used1. This fact means that the
new examples extracted from the unlabelled data in-
troduced too much noise into the classifiers. Be-
cause this anomalous behavior was present only on
some words, a complete study of such new exam-
ples must be done. Probably, the number of itera-
tions done by re-training over unlabelled data were
too low and the enrichment of the training corpora
not large enough.
1The ME system, without using re-training, has not com-
peted at SENSEVAL-3: our own scoring of these set of answers
reported an accuracy of 0.856
3.2 DLSI-UA-LS-ENG-SU
In the English Lexical Sample task our system goal
was to prove that the re-training method ensures a
high level of precision.
By means of a 3-fold cross-validation of the train
data, the features were ordered from higher to lower
precision. Based on this information, four execu-
tions of re-training over the test data were done with
different selections of features for the two views of
the method. Each execution feed the learning cor-
pora of the next one with new examples, those that
re-training considered as the most probably correct.
For this system Minipar parser (Lin, 1998)was
used to properly add syntactic information to the
training and testing data.
Almost 40% of the test contexts were la-
belled by our system, obtaining these scores (for
?fine-grained? and ?coarse-grained?, respectively):
0.782/0.828 precision and 0.310/0.329 recall. In our
opinion, such results must be interpreted as very
positive because the re-training method is able to
satisfy a high level of precision if the parameters of
the system are correctly set.
3.3 DLSI-UA-ALL-NOSU and
DLSI-UA-LS-ENG-NOSU
In the English All Words and English Lexical Sam-
ple tasks RD system was performed with informa-
tion obtained from Relevant Domains resource us-
ing for the disambiguation process all the 165 do-
main labels.
For All Words task we used as input information
all nouns, verbs, adjectives and adverbs present in
a 100 words window around the word to be disam-
biguated. So our system obtained a 34% of preci-
sion and a reduced recall around 28%.
For Lexical Sample task we used all nouns, verbs,
adjectives and adverbs present in the context of each
instance obtaining around 32% precision.
We obtained a reduced precision due to we use all
the domains label hierarchy. In some experiments
realized on SENSEVAL-2 data, our system obtained
a more high precision when grouping domains into
the first three levels. Therefore we expect with re-
ducing the number of domains labels, an improve-
ment on precision.
3.4 DLSI-UA-LS-SPA-NOSU
We used a combined system for Spanish Lexical
Sample task, using the SM method for disambiguat-
ing nouns and the ME method for disambiguating
verbs and adjectives. We obtained around 62% pre-
cision and a 62% recall.
3.5 DLSI-UA-LS-SPA-PATTERN
Our goal when participating in this task was to
demonstrate that the applying of syntactic patterns
to WSD maintains high levels of precision.
In this task we used also a combined system for
Spanish Lexical Sample task, using Pattern-Nica
method for disambiguating nouns and ME method
for disambiguating verbs and adjectives. We ob-
tained around 84% precision and a 47% recall.
4 Conclusions
The supervised systems for the English and Span-
ish lexical sample tasks are very competitive. Al-
though the processing of the train and test data was
different for each task, both systems rely on re-
training, a bootstrapping method, that uses a max-
imum entropy-based WSD module.
The results for the English task prove that re-
training is capable of maintaining a high level of
precision. Nevertheless, for the Spanish task, al-
though the scores achieved were excellent, the sys-
tem must be redesigned in order to improve the clas-
sifiers.
The re-training method is a proposal that we are
trying to incorporate into text retrieval and ques-
tion answering systems that could take advantage of
sense disambiguation of a subset of words.
The unsupervised systems presented here does
not appear to be sufficient for a stand-alone WSD
solution. Wether these methods can be combined
with other supervised methods to improve their re-
sults requires further investigation.
References
Adam L. Berger, Stephen A. Della Pietra, and Vin-
cent J. Della Pietra. 1996. A maximum entropy
approach to natural language processing. Com-
putational Linguistics, 22(1):39?71.
Avrim Blum and Tom Mitchell. 1998. Combining
labeled and unlabeled data with co-training. In
Proceedings of the 11th Annual Conference on
Computational Learning Theory, pages 92?100,
Madison, Wisconsin, July. ACM Press.
R. Lau, R. Rosenfeld, and S. Roukos. 1993.
Adaptative statistical language modeling using
the maximum entropy principle. In Proceedings
of the Human Language Technology Workshop,
ARPA.
Dekang Lin. 1998. Dependency-based evaluation
of minipar. In Proceedings of the Workshop on
the Evaluation of Parsing Systems, First Inter-
national Conference on Language Resources and
Evaluation, Granada, Spain.
Bernardo Magnini and Gabriela Cavaglia. 2000.
Integrating Subject Field Codes into WordNet. In
M. Gavrilidou, G. Crayannis, S. Markantonatu,
S. Piperidis, and G. Stainhaouer, editors, Pro-
ceedings of LREC-2000, Second International
Conference on Language Resources and Evalu-
ation, pages 1413?1418, Athens, Greece.
Bernardo Magnini and C. Strapparava. 2000. Ex-
periments in Word Domain Disambiguation for
Parallel Texts. In Proceedings of the ACL Work-
shop on Word Senses and Multilinguality, Hong
Kong, China.
Llu??s Ma`rquez, Fco. Javier Raya, John Car-
roll, Diana McCarthy, Eneko Agirre, David
Mart??nez, Carlo Strapparava, and Alfio
Gliozzo. 2003. Experiment A: several all-words
WSD systems for English. Technical Report
WP6.2, MEANING project (IST-2001-34460),
http://www.lsi.upc.es/?nlp/meaning/meaning.html.
George A. Miller, Richard Beckwith, Christiane
Fellbaum, Derek Gross, and Katherine J. Miller.
1993. Five Papers on WordNet. Special Issue of
the International journal of lexicography, 3(4).
Andre?s Montoyo and Manuel Palomar. 2000. Word
Sense Disambiguation with Specification Marks
in Unrestricted Texts. In Proceedings of 11th In-
ternational Workshop on Database and Expert
Systems Applications (DEXA 2000), pages 103?
107, Greenwich, London, UK, September. IEEE
Computer Society.
Andre?s Montoyo, Sonia Va?zquez, and German
Rigau. 2003. Me?todo de desambiguacio?n le?xica
basada en el recurso le?xico Dominios Rele-
vantes. Procesamiento del Lenguaje Natural, 30,
september.
Iulia Nica, Antonia Mart??, and Andre?s Mon-
toyo. 2003. Colaboracio?n entre informacio?n
paradigma?tica y sintagma?tica en la desam-
biguacio?n sema?ntica automa?tica. XIX Congreso
de la SEPLN 2003.
Adwait Ratnaparkhi. 1998. Maximum Entropy
Models for Natural Language Ambiguity Resolu-
tion. Ph.D. thesis, University of Pennsylvania.
Armando Sua?rez and Manuel Palomar. 2002.
A maximum entropy-based word sense disam-
biguation system. In Hsin-Hsi Chen and Chin-
Yew Lin, editors, Proceedings of the 19th In-
ternational Conference on Computational Lin-
guistics, pages 960?966, Taipei, Taiwan, August.
COLING 2002.
Piek Vossen. 1998. EuroWordNet: Building a Mul-
tilingual Database with WordNets for European
Languages. The ELRA Newsletter, 3(1).
The R2D2 Team at SENSEVAL-3?
Sonia Va?zquez, Rafael Romero
Armando Sua?rez and Andre?s Montoyo
Dpto. de Lenguajes y Sistemas. Informa?ticos
Universidad de Alicante, Spain
{svazquez,romero}@dlsi.ua.es
{armando,montoyo}@dlsi.ua.es
Manuel Garc??a, M. Teresa Mart??n ?
M. ?Angel Garc??a and L. Alfonso Uren?a
Dpto. de Informa?tica
Universidad de Jae?n, Spain
{mgarcia,maite}@ujaen.es
{magc,laurena}@ujaen.es
Davide Buscaldi, Paolo Rosso ?
Antonio Molina, Ferra?n Pla? and Encarna Segarra
Dpto. de Sistemas Informa?ticos y Computacio?n
Univ. Polit. de Valencia, Spain
{dbuscaldi,prosso}@dsic.upv.es
{amolina,fpla,esegarra}@dsic.upv.es
Abstract
The R2D2 systems for the English All-Words and
Lexical Sample tasks at SENSEVAL-3 are based on
several supervised and unsupervised methods com-
bined by means of a voting procedure. Main goal
was to take advantage of training data when avail-
able, and getting maximum coverage with the help
of methods that not need such learning examples.
The results reported in this paper show that super-
vised and unsupervised methods working in par-
allel, and a simple sequence of preferences when
comparing the answers of such methods, is a feasi-
ble method. . .
The whole system is, in fact, a cascade of deci-
sions of what label to assign to a concrete instance
based on the agreement of pairs of systems, when
it is possible, or selecting the available answer from
one of them. In this way, supervised are preferred to
unsupervised methods, but these last ones are able
to tag such words that not have available training
data.
1 Introduction
Designing a system for Natural Language Process-
ing (NLP) requires a large knowledge on language
structure, morphology, syntax, semantics and prag-
matic nuances. All of these different linguistic
knowledge forms, however, have a common asso-
ciated problem, their many ambiguities, which are
difficult to resolve.
In this paper we concentrate on the resolution
of the lexical ambiguity that appears when a given
word in a context has several different meanings.
? This paper has been partially supported by the Spanish
Government (CICyT) under project number TIC-2003-7180
and the Valencia Government (OCyT) under project number
CTIDIB-2002-151
This specific task is commonly referred as Word
Sense Disambiguation (WSD). This is a difficult
problem that is receiving a great deal of attention
from the research community because its resolu-
tion can help other NLP applications as Machine
Translation (MT), Information Retrieval (IR), Text
Processing, Grammatical Analysis, Information Ex-
traction (IE), hypertext navigation and so on.
The R2D2 Team has participated in two tasks:
English all-words and lexical sample. We use sev-
eral different systems both supervised and unsuper-
vised. The supervised methods are based on Max-
imum Entropy (ME) (Lau et al, 1993; Berger et
al., 1996; Ratnaparkhi, 1998), neural network using
the Learning Vector Quantization algorithm (Koho-
nen, 1995) and Specialized Hidden Markov Mod-
els (Pla, 2000). The unsupervised methods are Rel-
evant Domains (RD) (Montoyo et al, 2003) and
the CIAOSENSO WSD system which is based on
Conceptual Density (Agirre and Rigau, 1995), fre-
quency of WordNet (Miller et al, 1993a) senses and
WordNet Domains (Magnini and Cavaglia, 2000).
In the following section we will show a more
complete description of the systems. Next, how
such methods were combined in two voting sys-
tems, and the results obtained in SENSEVAL-3. Fi-
nally, some conclusions will be presented.
2 Systems description
In this section the systems that have participated at
SENSEVAL-3 will be described.
2.1 Maximum Entropy
ME modeling provides a framework for integrating
information for classification from many heteroge-
neous information sources (Manning and Schu?tze,
1999). ME probability models have been success-
                                             Association for Computational Linguistics
                        for the Semantic Analysis of Text, Barcelona, Spain, July 2004
                 SENSEVAL-3: Third International Workshop on the Evaluation of Systems
fully applied to some NLP tasks, such as POS tag-
ging or sentence boundary detection (Ratnaparkhi,
1998). ME have been also applied to WSD (van
Halteren et al, 2001; Montoyo and Sua?rez, 2001;
Sua?rez and Palomar, 2002), and as meta-learner in
(Ilhan et al, 2001).
Our ME-based system has been shown competi-
tive (Ma`rquez et al, 2003) when compared to other
supervised systems such as Decision Lists, Support
Vector Machines, and AdaBoost. The features that
were defined to train the system are those described
in Figure 1.
? the target word itself
? lemmas of content-words at positions ?1, ?2, ?3
? words at positions ?1, ?2,
? words at positions ?1, ?2, ?3
? content-words at positions ?1, ?2, ?3
? POS-tags of words at positions ?1, ?2, ?3
? lemmas of collocations at positions (?2,?1),
(?1,+1), (+1,+2)
? collocations at positions (?2,?1), (?1,+1),
(+1,+2)
? lemmas of nouns at any position in context, occur-
ring at least m% times with a sense
? grammatical relation of the target word
? the word that the target word depends on
? the verb that the target word depends on
? the target word belongs to a multi-word, as identi-
fied by the parser
Figure 1: Features Used for the Supervised Learn-
ing of the ME system
Because the ME system needs annotated data
for the training, Semcor (Miller et al, 1993b) was
used for the English All-Words task, the system
was trained using Semcor (Miller et al, 1993b), and
parsed by Minipar (Lin, 1998). Only those words
that have 10 examples or more in Semcor were pro-
cessed in order to obtain a ME classifier.
For the Spanish Lexical Sample task, the train-
ing data from SENSEVAL-3 was the source of la-
beled examples. We did not use any parser, just the
lemmatization and POS-tagging information sup-
plied into the training data itself.
2.2 UPV-SHMM-AW
The upv-shmm-aw WSD system is a supervised ap-
proach based on Specialized Hidden Markov Mod-
els (SHMM).
Basically, a SHMM consists of changing the
topology of a Hidden Markov Model in order to get
a more accurate model which includes more infor-
mation. This is done by means of an initial step
previous to the learning process. It consists of the
redefinition of the input vocabulary and the output
tags. This redefinition is done by means of two pro-
cesses which transform the training set: the selec-
tion process chooses which input features (words,
lemmas, part-of-speech tags, ...) are relevant to the
task, and the specialization process redefines the
output tags by adding information from the input.
This specialization produces some changes in the
model topology, in order to allow the model to bet-
ter capture some contextual restrictions and to get a
more accurate model.
We used as training data the part of the Sem-
Cor corpus that is semantically annotated and su-
pervised for nouns, verbs, adjectives and adverbs,
and the test data set provided by SENSEVAL-2.
We used 10% of the training corpus as a develop-
ment data set in order to determine the best selection
and specialization criteria.
In the experiments, we used WordNet1.6 (Miller
et al, 1993a) as a dictionary that supplies all the
possible semantic senses for a given word. Our sys-
tem disambiguated all the polysemic lemmas, that
is, the coverage of our system was 100%. For un-
known words (words that did not appear in the train-
ing data set), we assigned the first sense in WordNet.
2.3 Relevant Domains
This is an unsupervised WSD method based on the
WordNet Domains lexical resource (Magnini and
Cavaglia, 2000). The underlaying working hypoth-
esis is that domain labels, such as ARCHITEC-
TURE, SPORT and MEDICINE provide a natural
way to establish semantic relations between word
senses, that can be used during the disambiguation
process. This resource has already been used on
Word Sense Disambiguation (Magnini and Strappa-
rava, 2000), but it has not made use of glosses infor-
mation. So our approach make use of a new lexical
resource obtained from glosses information named
Relevant Domains.
First step is to obtain the Relevant Domains re-
source from WordNet glosses. For this task is nec-
essary a previous part-of-speech tagging of Word-
Net glosses (each gloss has associated a domain la-
bel). So we extract all nouns, verbs, adjectives and
adverbs from glosses and assign them their associ-
ated domain label. With this information and using
the Association Ratio formula(w=word,D=domain
label), in (1), we obtain the Relevant Domains re-
source.
AR(w,D) = Pr(w|D)log2Pr(w|D)Pr(w) (1)
The final result is for each word, a set of domain
labels sorted by Association Ratio, for example,
for word plant? its Relevant Domains are: genetics
0.177515, ecology 0.050065, botany 0.038544 . . . .
Once obtained Relevant Domains the disam-
biguation process is carried out. We obtain from
the text source the context words that co-occur with
the word to be disambiguated (context could be
a sentence or a window of words). We obtain a
context vector from Relevant Domains and context
words (in case of repeated domain labels, they are
weighted). Furthermore we need a sense vector ob-
tained in the same way as context vector from words
of glosses of each word sense. We select the cor-
rect sense using the cosine measure between con-
text vector and sense vectors. So the selected sense
is that for which the cosine with the context vector
is closer to one.
2.4 LVQ-JA ?EN-ELS
The LVQ-JA ?EN-ELS system (Garc??a-Vega et al,
2003) is based on a supervised learning algorithm
for WSD. The method trains a neural network using
the Learning Vector Quantization (LVQ) algorithm
(Kohonen, 1995), integrating Semcor and several
semantic relations of WordNet.
The Vector Space Model (VSM) is used as an in-
formation representation model. Each sense of a
word is represented as a vector in an n-dimensional
space where n is the number of words in all its con-
texts.
We use the LVQ algorithm to adjust the word
weights. The input vector weights are calculated
as shown by (Salton and McGill, 1983) with the
standard (tf ? idf). They are presented to the LVQ
network and, after training, the output vectors are
obtained, containing the adjusted weights for all
senses of each word.
Any word to disambiguate is represented with a
vector in the same way. This representation must be
compared with all the trained sense vectors of the
word by applying the cosine similarity rule:
sim(wk, xi) = wk ? xi| wk | ? | xi | (2)
The sense corresponding to the vector of highest
similarity is selected as the disambiguated sense.
To train the neural network we have inte-
grated semantic information from two linguistic re-
sources: SemCor1.6 corpus and WordNet1.7.1 lex-
ical database. From Semcor1.6 we used the para-
graph as a contextual semantic unit and each con-
text was included in the training vector set. From
WordNet1.7.1 some semantic relations were consid-
ered, specifically, synonymy, antonymy, hyponymy,
homonymy, hyperonymy, meronymy, and coordi-
nate terms. This information was introduced to the
training set through the creation of artificial para-
graphs with the words of each relation. So, for a
word with 7 senses, 7 artificial paragraphs with the
synonyms of the 7 senses were added, 7 more with
all its hyponyms, and so on.
The learning algorithm is very simple. First, the
learning rate and the codebook vectors are initial-
ized. Then, the following procedure is repeated for
all the training input vectors until a stopping crite-
rion is satisfied:
- Select a training input pattern, x, with class d,
and present it to the network
- Calculate the Euclidean distance between the in-
put vector and each codebook vector || x? wk ||
- Select the codebook vector, wc, that is closest to
the input vector, x, like the winner sense.
- The winner neuron updates its weights accord-
ing the learning equation:
wc(t+ 1) = wc(t) + s ? ?(t) ? [x(t)? wc(t)] (3)
where s = 0, if k 6= c; s = 1, if x(t) and wc(t)
belong to the same class (c = d); and s = ?1, if
they do not (c 6= d). ?(t) is the learning rate, and
0 < ?(t) < 1 is a monotically decreasing func-
tion of time. It is recommended that ?(t) should
already initially be rather small, say, smaller than
0.1 (Kohonen, 1995) and ?(t) continues decreasing
to a given threshold, u, very close to 0.
2.5 CIAOSENSO
The CIAOSENSO WSD system is an unsupervised
system based on Conceptual Density, the frequency
of WordNet sense, and WordNet Domains. Concep-
tual Density is a measure of the correlation among
the sense of a given word and its context. The
noun sense disambiguation is performed by means
of a formula combining the Conceptual Density
with WordNet sense frequency (Rosso et al, 2003).
The context window used in both the English all-
words and lexical sample tasks is of 4 nouns. Ad-
ditional weights are assigned to those senses hav-
ing the same domain as the context nouns? senses.
Each weight is proportional to the frequency of such
senses, and is calculated as MDW (f, i) = 1/f ?1/i
where f is an integer representing the frequency
of the sense of the word to be disambiguated and
i gives the same information for the context word.
Example: If the word to be disambiguated is doc-
tor, the domains for senses 1 and 4 are, respec-
tively, Medicine and School. Therefore, if one of
the context words is university, the resulting weight
for doctor(4) and university(3) is 1/4 ? 1/3.
The sense disambiguation of an adjective is per-
formed only on the basis of the above weights.
Given one of its senses, we extract the synsets ob-
tained by the similar to, pertainym and attribute
relationships. For each of them, we calculate the
MDW with respect to the senses of the context
noun. The weight assigned to the adjective sense
is the average between these MDWs. The se-
lected sense is the one having the maximum average
weight.
The sense disambiguation of a verb is done nearly
in the same way, but taking into consideration only
the MDWs with the context words. In the all-words
task the context words are the noun before and af-
ter the verb, whereas in the lexical sample task the
context words are four (two before and two after the
verb), without regard to their morphological cate-
gory. This has been done in order to improve the
recall in the latter task, for which the test corpus is
made up mostly by verbs.
The sense disambiguation of adverbs (in both
tasks) is carried out in the same way of the disam-
biguation of verbs for the lexical sample task.
3 Tasks Processing
We have selected several combinations of such sys-
tems described before for two voting systems, one
for the Lexical-Sample task and the other for the
All-Words task.
3.1 English Lexical Sample Task
At the English Lexical Sample task we combined
the answers of four systems: Relevant Domains,
CIAOSENSO, LVQ-JA ?EN-ELS and Maximum En-
tropy.
The four methods worked in parallel and their
sets of answers were the input of a majority voting
procedure. This procedure selected those answers
with more systems agreements. In case of tie we
gave priority to supervised systems.
With this voting system we obtained around a
63% precision and a 52% recall.
3.2 English All Words Task
For this task we used a voting system combining
the results of Relevant Domains, Maximum En-
tropy, CIAOSENSO and UPV-SHMM-AW. So we
obtained the final results after 10 steps.
Step 1, we selected those answers with agree-
ment between ME and UPV-SHMM-AW (super-
vised systems).
Step 2, from no agreement in step 1 we selected
those answers with agreement between ME and Rel-
evant Domains.
Step 3, from no agreement in step 2 we selected
those answers with agreement between ME and
CIAOSENSO.
Step 4, from no agreement in step 3 we se-
lected those answers with agreement between
CIAOSENSO and UPV-SHMM-AW.
Step 5, from no agreement in step 4 we se-
lected those answers with agreement between UPV-
SHMM-AW and Relevant Domains.
Step 6, from no agreement in step 5 we selected
those answers with agreement between Relevant
Domains and CIAOSENSO.
Step 7, from no agreement in step 6 we selected
Maximum Entropy answers.
Step 8, from the remaining unlabeled instances
we selected UPV-SHMM-AW answers.
Step 9, from the remaining unlabeled instances
we selected Relevant Domains answers.
Step 10, from the remaining unlabeled instances
we selected CIAOSENSO answers.
Last step was labeling with the most frequent
sense in WordNet those instances that had been not
tagged by any system, but in view of the final results
only two instances had not answer and we didn?t
find them in WordNet.
With this voting system preference was given to
supervised systems over unsupervised systems.
We obtained around a 63% precision and a 63%
recall.
4 Conclusions
This paper presents the main characteristics of
the Maximum Entropy, LVQ-JAEN-ELS, UPV-
SHMM-AW, Relevant Domains and CIAOSENSO
systems within the framework of SENSEVAL-3 En-
glish Lexical Sample and All Words tasks. These
systems are combined with a voting technique ob-
taining a promising results for English All Words
and English Lexical Sample tasks.
References
Eneko Agirre and German Rigau. 1995. A pro-
posal for word sense disambiguation using Con-
ceptual Distance. In Proceedings of the Interna-
tional Conference ?Recent Advances in Natural
Language Processing? (RANLP95).
Adam L. Berger, Stephen A. Della Pietra, and Vin-
cent J. Della Pietra. 1996. A maximum entropy
approach to natural language processing. Com-
putational Linguistics, 22(1):39?71.
Manuel Garc??a-Vega, Mar??a Teresa Mart??n-
Valdivia, and Luis Alfonso Uren?a. 2003.
Aprendizaje competitivo lvq para la desam-
biguacio?n le?xica. Revista de la Sociedad
Espaola para el Procesamiento del Lenguaje
Natural, 31:125?132.
H. Tolga Ilhan, Sepandar D. Kamvar, Dan Klein,
Christopher D. Manning, and Kristina Toutanova.
2001. Combining Heterogeneous Classifiers for
Word-Sense Disambiguation. In Judita Preiss
and David Yarowsky, editors, Proceedings of the
2nd International Workshop on Evaluating Word
Sense Disambiguation Systems (SENSEVAL-2),
pages 87?90, Toulouse, France, July. ACL-
SIGLEX.
T. Kohonen. 1995. Self-organization and associa-
tive memory. 2nd Ed. Springer Verlag, Berlin.
R. Lau, R. Rosenfeld, and S. Roukos. 1993.
Adaptative statistical language modeling using
the maximum entropy principle. In Proceedings
of the Human Language Technology Workshop,
ARPA.
Dekang Lin. 1998. Dependency-based evaluation
of minipar. In Proceedings of the Workshop on
the Evaluation of Parsing Systems, First Inter-
national Conference on Language Resources and
Evaluation, Granada, Spain.
Bernardo Magnini and Gabriela Cavaglia. 2000.
Integrating Subject Field Codes into WordNet. In
M. Gavrilidou, G. Crayannis, S. Markantonatu,
S. Piperidis, and G. Stainhaouer, editors, Pro-
ceedings of LREC-2000, Second International
Conference on Language Resources and Evalu-
ation, pages 1413?1418, Athens, Greece.
Bernardo Magnini and C. Strapparava. 2000. Ex-
periments in Word Domain Disambiguation for
Parallel Texts. In Proceedings of the ACL Work-
shop on Word Senses and Multilinguality, Hong
Kong, China.
Christopher D. Manning and Hinrich Schu?tze.
1999. Foundations of Statistical Natural Lan-
guage Processing. The MIT Press, Cambridge,
Massachusetts.
Llu??s Ma`rquez, Fco. Javier Raya, John Car-
roll, Diana McCarthy, Eneko Agirre, David
Mart??nez, Carlo Strapparava, and Alfio
Gliozzo. 2003. Experiment A: several all-words
WSD systems for English. Technical Report
WP6.2, MEANING project (IST-2001-34460),
http://www.lsi.upc.es/?nlp/meaning/meaning.html.
George A. Miller, Richard Beckwith, Christiane
Fellbaum, Derek Gross, and Katherine J. Miller.
1993a. Five Papers on WordNet. Special Issue of
the International journal of lexicography, 3(4).
George A. Miller, C. Leacock, R. Tengi, and
T. Bunker. 1993b. A Semantic Concordance. In
Proceedings of ARPA Workshop on Human Lan-
guage Technology, pages 303?308, Plainsboro,
New Jersey.
Andre?s Montoyo and Armando Sua?rez. 2001.
The University of Alicante word sense disam-
biguation system. In Judita Preiss and David
Yarowsky, editors, Proceedings of the 2nd In-
ternational Workshop on Evaluating Word Sense
Disambiguation Systems (SENSEVAL-2), pages
131?134, Toulouse, France, July. ACL-SIGLEX.
Andre?s Montoyo, Sonia Va?zquez, and German
Rigau. 2003. Me?todo de desambiguacio?n le?xica
basada en el recurso le?xico Dominios Rele-
vantes. Procesamiento del Lenguaje Natural, 30,
september.
F. Pla. 2000. Etiquetado Le?xico y Ana?lisis
Sinta?ctico Superficial basado en Modelos Es-
tad??sticos. Tesis doctoral, Departamento de Sis-
temas Informa?ticos y Computacio?n. Universidad
de Polite?cnica de Valencia, Septiembre.
Adwait Ratnaparkhi. 1998. Maximum Entropy
Models for Natural Language Ambiguity Resolu-
tion. Ph.D. thesis, University of Pennsylvania.
P. Rosso, F. Masulli, D. Buscaldi, F. Pla, and
A. Molina. 2003. Automatic noun disambigua-
tion. LNCS, Springer Verlag, 2588:273?276.
G. Salton and M.J. McGill. 1983. Introduction
to modern information retrieval. McGraw-Hill,
New York.
Armando Sua?rez and Manuel Palomar. 2002.
A maximum entropy-based word sense disam-
biguation system. In Hsin-Hsi Chen and Chin-
Yew Lin, editors, Proceedings of the 19th In-
ternational Conference on Computational Lin-
guistics, pages 960?966, Taipei, Taiwan, August.
COLING 2002.
H. van Halteren, J. Zavrel, and W. Daelemans.
2001. Improving accuracy in wordclass tag-
ging through combination of machine learning
systems. Computational Linguistics, 27(2):199?
230.
Balto-Slavonic Natural Language Processing 2007, June 29, 2007, pages 19?26,
Prague, June 2007. c?2007 Association for Computational Linguistics
A Language Independent Approach for Name Categorization and
Discrimination
Zornitsa Kozareva
Departamento de Lenguajes
y Sistemas Informa?ticos
Universidad de Alicante
Alicante, Spain
zkozareva@dlsi.ua.es
Sonia Va?zquez
Departamento de Lenguajes
y Sistemas Informa?ticos
Universidad de Alicante
Alicante, Spain
svazquez@dlsi.ua.es
Andre?s Montoyo
Departamento de Lenguajes
y Sistemas Informa?ticos
Universidad de Alicante
Alicante, Spain
montoyo@dlsi.ua.es
Abstract
We present a language independent ap-
proach for fine-grained categorization and
discrimination of names on the basis of text
semantic similarity information. The exper-
iments are conducted for languages from the
Romance (Spanish) and Slavonic (Bulgar-
ian) language groups. Despite the fact that
these languages have specific characteristics
as word-order and grammar, the obtained
results are encouraging and show that our
name entity method is scalable not only to
different categories, but also to different lan-
guages. In an exhaustive experimental eval-
uation, we have demonstrated that our ap-
proach yields better results compared to a
baseline system.
1 Introduction
1.1 Background
Named Entity (NE) recognition concerns the detec-
tion and classification of names into a set of cate-
gories. Presently, most of the successful NE ap-
proaches employ machine learning techniques and
handle simply the person, organization, location and
miscellaneous categories. However, the need of
the current Natural Language Applications impedes
specialized NE extractors which can help for in-
stance an information retrieval system to determine
that a query about ?Jim Henriques guitars? is related
to the person ?Jim Henriques? with the semantic cat-
egory musician, and not ?Jim Henriques? the com-
poser. Such classification can aid the system to rank
or return relevant answers in a more accurate and
appropriate way.
So far, the state-of-art NE recognizers identify
that ?Jim Henriques? is a person, but do not sub-
categorize it. There are numerous drawbacks re-
lated to the fine-grained NE issue. First, the sys-
tems need hand annotated data which are not avail-
able for multiple categories, because their creation is
time-consuming, requires supervision by experts, a
predefined fine-grained hierarchical structure or on-
tology. Second, there is a significant lack of freely
available or developed resources for languages other
than English, and especially for the Eastern Euro-
pean ones.
The World Wide Web is a vast, multilingual
source of unstructured information which we con-
sult daily in our native language to understand what
the weather in our city is or how our favourite soccer
team performed. Therefore, the need of multilingual
and specialized NE extractors remains and we have
to focus on the development of language indepen-
dent approaches.
Together with the specialized NE categorization,
we face the problem of name ambiguity which is
related to queries for different people, locations or
companies that share the same name. For instance,
Cambridge is a city in the United Kingdom, but
also in the United States of America. ACL refers
to ?The Association of Computational Linguistics?,
?The Association of Christian Librarians? or to the
?Automotive Components Limited?. Googling the
name ?Boyan Bonev? returns thousands of docu-
ments where some are related to a member of a robot
vision group in Alicante, a teacher at the School
19
of Biomedical Science, a Bulgarian schoolboy that
participated in computer science competition among
others. So far, we have to open the documents one
by one, skim the text and decide to which ?Boyan
Bonev? the documents are related to. However, if
we resolve the name disambiguation issue, this can
lead to an automatic clustering of web pages talking
about the same individual, location or ogranization.
1.2 Related Work
Previously, (Pedersen et al, 2005) tackled the name
discrimination task by developing a language inde-
pendent approach based on the context in which the
ambiguous name occurred. They construct second
order co-occurrence features according to which the
entities are clustered and associated to different un-
derlying names. The performance of this method
ranges from 51% to 73% depending on the pair of
named entities that have to be disambiguated. Simi-
lar approach was developed by (Bagga and Baldwin,
1998), who created first order context vectors that
represent the instance in which the ambiguous name
occurs. Their approach is evaluated on 35 different
mentions of John Smith, and the f-score is 84%.
For fine-grained person NE categorization, (Fleis-
chman and Hovy, 2002) carried out a supervised
learning for which they deduced features from the
local context in which the entity resides, as well as
semantic information derived from the topic signa-
tures and WordNet. According to their results, to
improve the 70% coverage for person name catego-
rization, more sophisticated features are needed, to-
gether with a more solid data generation procedure.
(Tanev and Magnini, 2006) classified geographic
location and person names into several subclasses.
They use syntactic information and observed how
often a syntactic pattern co-occurs with certain
member of a given class. Their method reaches 65%
accuracy. (Pasca, 2004) presented a lightly super-
vised lexico-syntactic method for named entity cat-
egorization which reaches 76% when evaluated with
unstructured text of Web documents.
(Mann, 2002) populated a fine-grained proper
noun ontology using common noun patterns and fol-
lowing the hierarchy of WordNet. They studied the
influence of the newly generated person ontology in
a Question Answering system. According to the ob-
tained results, the precision of the ontology is high,
but still suffers in coverage. A similar approach for
the population of the CyC Knowledge Base (KB)
was presented in (Shah et al, 2006). They used
information from the Web and other electronically
available text corpora to gather facts about particu-
lar named entities, to validate and finally to add them
to the CyC KB.
In this paper, we present a new text semantic simi-
larity approach for fine-grained person name catego-
rization and discrimination which is similar to those
of (Pedersen et al, 2005) and (Bagga and Baldwin,
1998), but instead of simple word co-occurrences,
we consider the whole text segment and relate the
deduced semantic information of Latent Seman-
tic Analysis (LSA) to trace the text cohesion be-
tween thousands of sentences containing named en-
tities which belong to different fine-grained cate-
gories or individuals. Our method is based on the
word sense discrimination hypothesis of Miller and
Charles (1991) according to which words with sim-
ilar meaning are used in similar context, hence in
our approach we assume that the same person or
the same fine-grained person category appears in the
similar context.
2 NE categorization and discrimination
with Latent Semantic Analysis
LSA has been applied successfully in many areas
of Natural Language Processing such as Informa-
tion Retrieval (Deerwester et al, 1990), Informa-
tion Filtering (Dumais, 1995) , Word Sense Disam-
biguation (Shu?tze, 1998) among others. This is pos-
sible because LSA is a fully automatic mathemati-
cal/statistical technique for extracting and inferring
relations of expected contextual usage of words in
discourse. It uses no humanly constructed dictionar-
ies or knowledge bases, semantic networks, syntac-
tic or morphological analyzers, because it takes only
as input raw text which is parsed into words and is
separated into meaningful passages. On the basis of
this information, LSA extracts a list of semantically
related word pairs or rank documents related to the
same topic.
LSA represents explicitly terms and documents
in a rich, highly dimensional space, allowing the
underlying ?latent?, semantic relationships between
terms and documents to be exploited. LSA relies
20
on the constituent terms of a document to suggest
the document?s semantic content. However, the LSA
model views the terms in a document as somewhat
unreliable indicators of the concepts contained in the
document. It assumes that the variability of word
choice partially obscures the semantic structure of
the document. By reducing the original dimen-
sionality of the term-document space with Singular
Value Decomposition to a matrix of 300 columns,
the underlying, semantic relationships between doc-
uments are revealed, and much of the ?noise? (dif-
ferences in word usage, terms that do not help distin-
guish documents, etc.) is eliminated. LSA statisti-
cally analyzes the patterns of word usage across the
entire document collection, placing documents with
similar word usage patterns near to each other in the
term-document space, and allowing semantically-
related documents to be closer even though they may
not share terms.
Taking into consideration these properties of
LSA, we thought that instead of constructing the
traditional term-document matrix, we can construct
a term-sentence matrix with which we can find a
set of sentences that are semantically related and
talk about the same person. The rows of the term-
sentence matrix correspond to the words of the sen-
tence where the NE has to be categorized or discrim-
inated (we call this sentence target sentence), while
the columns correspond to the rest of the sentences
with NEs. The cells of the matrix show the num-
ber of times a given word from the target sentence
co-occurs in the rest of the sentences. When two
columns of the term-sentence matrix are similar, this
means that the two sentences contain similar words
and are therefore likely to be semantically related.
When two rows are similar, then the corresponding
words occur in most of the same sentences and are
likely to be semantically related.
In this way, we can obtain semantic evidence
about the words which characterize a given person.
For instance, a football player is related to words
as ball, match, soccer, goal, and is seen in phrases
such as ?X scores a goal?, ?Y is penalized?. Mean-
while, a surgeon is related to words as hospital, pa-
tient, operation, surgery and is seen in phrases such
as ?X operates Y?, ?X transplants?. Evidently, the
category football player can be distinguished easily
from that of the surgeon, because both person names
occur and relate semantically to different words.
Another advantage of LSA is its property of lan-
guage independence, and the ability to link sev-
eral flexions or declanations of the same term.
This is especially useful for the balto-slavonic lan-
guages which have rich morphology. Once the term-
sentence approach is developed, practically there is
no restrain for LSA to be applied and extended to
other languages. As our research focuses not only
on the resolution of the NE categorization and dis-
crimination problems as a whole, but also on the lan-
guage independence issue, we considered the LSA?s
usage are very appropriate.
3 Development Data Set
For the development of our name discrimination and
classification approach, we used the Spanish lan-
guage. The corpora we worked with is the EFE94-95
Spanish news corpora, which were previously used
in the CLEF competitions1. In order to identify the
named entities in the corpora, we used a machine
learning based named entity recognizer (Kozareva et
al., 2007).
For the NE categorization and discrimination ex-
periments, we used six different named entities, for
which we assumed a-priory to belong to one of the
two fine-grained NE categories PERSON SINGER
and PERSON PRESIDENT. The president names
are Bill Clinton, George Bush and Fidel Castro, and
the singer names are Madonna, Julio Iglesias and
Enrique Iglesias. We have selected these names for
our experiment, because of their high frequency in
the corpora and low level of ambiguity.
Once we have selected the names, we have col-
lected a context of 10, 25, 50 and 100 words from
the left and from the right of the NEs. This is done
in order to study the influence of the context for the
NE discrimination and categorization tasks, and es-
pecially how the context window affects LSA?s per-
formance. We should note that the context for the
NEs is obtained from the text situated between the
text tags. During the creation of the context win-
dow, we used only the words that belong to the docu-
ment in which the NE is detected. This restriction is
imposed, because if we use words from previous or
following documents, this can influence and change
1http://www.clef-campaign.org/
21
the domain and the topic in which the NE is seen.
Therefore, NE examples for which the number of
context words does not correspond to 10, 25, 50 or
100 are directly discarded.
From the compiled data, we have randomly se-
lected different NE examples and we have created
two data sets: one with 100 and another with 200
examples per NE. In the fine-grained classification,
we have substituted the occurrence of the presi-
dent and singer names with the obfuscated form
President Singer. While for the NE discrim-
ination task, we have replaced the names with the
M EI JI BC GB FC label. The first label indicates
that a given sentence can belong to the president or
to the singer category, while the second label indi-
cates that behind it can stand one of the six named
entities. The NE categorization and discrimination
experiments are carried out in a completely unsuper-
vised way, meaning that we did not use the correct
name and name category until evaluation.
4 Experimental Evaluation
4.1 Experimental Settings
As mentioned in Section 2, to establish the semantic
similarity relation between a sentence with an ob-
fuscated name and the rest of the sentences, we use
LSA2. The output of LSA is a list of sentences that
best matches the target sentence (e.g. the sentence
with the name that has to be classified or discrim-
inated) ordered by their semantic similarity score.
Strongly similar sentences have values close to 1,
and dissimilar sentences have values close to 0.
In order to group the most semantically similar
sentences which we expect to refer to the same per-
son or the same fine-grained category, we apply the
graph-based clustering algorithm PoBOC (Cleuziou
et al, 2004). We construct a new quadratic sentence-
sentence similarity matrix where the rows stand for
the sentence we want to classify, the columns stand
for the sentences in the whole corpus and the values
of the cells represent the semantic similarity scores
derived from LSA.
On the basis of this information, PoBOC forms
two clusters whose performance is evaluated in
terms of precision, recall, f-score and accuracy
which can be derived from Table 1.
2http://infomap-nlp.sourceforge.net/
number of Correct PRESIDENT Correct SINGER
Assigned PRESIDENT a b
Assigned SINGER c d
Table 1: Contingency table
We have used the same experimental setting for
the name categorization and discrimination prob-
lems.
4.2 Spanish name categorization
In Table 2, we show the results for the Spanish fine-
grained categorization. The detailed results are for
the context window of 50 words with 100 and 200
examples. All runs, outperform a simple baseline
system which returns for half of the examples the
fine-grained category PRESIDENT and for the rest
SINGER. This 50% baseline performance is due to
the balanced corpus we have created. In the column
diff., we show the difference between the 50% base-
line and the f-score of the category. As can be seen
the f-scores reaches 90%, which is with 40% more
than the baseline. According to the z? statistics with
confidence level of 0.975, the improvement over the
baseline is statistically significant.
SPANISH
cont/ex Category P. R. A. F. diff.
50/100
PRESIDENT 90.38 87.67 88.83 89.00
SINGER 87.94 90.00 88.33 88.96 +39.00
50/200
PRESIDENT 90.10 94.33 91.92 92.18
SINGER 94.04 89.50 91.91 91.71 +42.00
Table 2: Spanish NE categorization
During the error analysis, we found out that the
PERSON PRESIDENT and PERSON SINGER cat-
egories are distinguishable and separable because
of the well-established semantic similarity relation
among the words with which the NE occurs.
A pair of president sentences has lots of strongly
related words such as president:meeting, presi-
dent:government, which indicates high text cohe-
sion, while the majority of words in a president?
singer pair are weakly related, for instance presi-
dent:famous, president:concert. But still we found
out ambiguous pairs such as president:company,
where the president relates to a president of a coun-
try, while the company refers to a musical enter-
22
name c10 c25 c50 c100
Madonna 63.63 61.61 63.16 79.45
Julio Iglesias 58.96 56.68 66.00 79.19
Enrique Iglesias 77.27 80.17 84.36 90.54
Bill Clinton 52.72 48.81 74.74 73.91
George Bush 49.45 41.38 60.20 67.90
Fidel Castro 61.20 62.44 77.08 82.41
Table 3: Spanish NE discrimination
prize. Such information confuses LSA?s categoriza-
tion process and decreases the NE categorization
performance.
4.3 Spanish name discrimination
In a continuation, we present in Table 3 the f-scores
for the Spanish NE discrimination task with the 10,
25, 50 and 100 context windows. The results show
that the semantic similarity method we employ is
very reliable and suitable not only for the NE cat-
egorization, but also for the NE discrimination. A
baseline which always returns one and the same per-
son name during the NE discrimination task is 17%.
From the table can be seen that all names outperform
this baseline. The f-score performance per individ-
ual name ranges from 42% to 90%. The results are
very good, as the conflated names (three presidents
and three singers) can be easily obfuscated, because
they share the same domain and occur with the same
semantically related words.
The three best discriminated names are Enrique
Iglesias, Fidel Castro and Madonna. The name Fidel
Castro is easily discriminated due to its characteriz-
ing words Cuba, CIA, Cuban president, revolution,
tyrant. All sentences having these words or syn-
onyms related to them are associated to Fidel Cas-
tro.
Bill Clinton occurred many times with the words
democracy, Boris Yeltsin, Halifax, Chelsea (the
daughter of Bill Clinton), White House, while
George Bush appeared with republican, Ronald
Reigan, Pentagon, war in Vietnam, Barbara Bush
(the wife of George Bush).
During the data compilation process, the exam-
ples for Enrique Iglesias are considered to belong to
the Spanish singer. However, in reality some exam-
ples of Enrique Iglesias talked about the president of
a financial company in Uruguay or political issues.
Therefore, this name was confused with Bill Clin-
ton, because they shared semantically related words
such as bank, general secretary, meeting, decision,
appointment.
The discrimination process for the singer names is
good, though Madonna and Julio Iglesias appeared
in the context of concerts, famous, artist, maga-
zine, scene, backstage. The characterizing words for
Julio Iglesias are Chabeli (the daughter of Julio Igle-
sias), Spanish, Madrid, Iberoamerican. The name
Madonna occurred with words related to a picture
of Madonna, a statue in a church of Madonna, the
movie Evita.
Looking at the effect of the context window for
the NE discrimination task, it can be seen that the
best performances of 90% for Enrique Iglesias, 82%
for Fidel Castro and 79% for Madonna are achieved
with 100 words from the left and from the right of
the NE. This shows that the larger context has better
discrimination power.
4.4 Discussion
After the error analysis, we saw that the performance
of our approach depends on the quality of the data
source we worked with. Although, we have selected
names with low degree of ambiguity, during the data
compilation process for which we assumed that they
refer 100% to the SINGER or PRESIDENT cate-
gories, during the experiments we found out that one
and the same name can refer to three different in-
dividuals. This was the case of Madonna and En-
rique Iglesias. From one side this impeded the fine-
grained categorization and discrimination processes,
but opened a new line for research.
In conclusion, the conducted experiments re-
vealed a series of important observations. The first
one is that the LSA?s term-sentence approach per-
forms better with a higher number of examples, be-
cause they provide more semantic information. In
addition to the number of examples, the experiments
show that the influence of the context window for the
name discrimination is significant. The discrimina-
tion power is better for larger context windows and
this is also related to the expressiveness of the lan-
guage.
Second, our name categorization and discrimina-
tion approach outperforms the baseline with 30%.
Finally, LSA is a very appropriate approximation
for the resolution of the NE categorization and dis-
23
crimination tasks. LSA also gives logical explana-
tion about the classification decision of the person
names, providing a set of words characterizing the
category or simply a list of words describing the in-
dividual we want to classify.
5 Adaptation to Bulgarian
5.1 Motivation
So far, we have discussed and described the develop-
ment and the performance of our approach with the
Spanish language. The obtained results and observa-
tions, serve as a base for the context extraction and
the experimental setup for the rest of the languages
which we want to study. However, to verify the mul-
tilingual performance of the approach, we decided
to carry out an experiment with a language which is
very different from the Romance family.
For this reason, we choose the Bulgarian lan-
guage, which is the earliest written Slavic language.
It dates back from the creation of the old Bulgarian
alphabet Glagolista, which was later replaced by the
Cyrillic alphabet. The most typical characteristics of
the Bulgarian language are the elimination of noun
declension, suffixed definite article, lack of a verb
infinitive and complicated verb system.
The Bulgarian name discrimination data is ex-
tracted from the news corpus Sega2002. This corpus
is originally prepared and used in the CLEF compe-
titions. The corpus consists of news articles orga-
nized in different XML files depending on the year,
month, and day of the publication of the news. We
merged all files into a single one, and considered
only the text between the text tags. In order to ease
the text processing and to avoid encoding problems,
we transliterated the Cyrillic characters into Latin
ones.
The discrimination data in this experiment con-
sists of the city, country, party, river and mountain
categories. We were interested in studying not only
the multilingual issue of our approach, but also how
scalable it is with other categories. The majority
of the categories are locations and only one corre-
sponds to organization. In Table 4, we shows the
number of names which we extracted for each one
of the categories.
5.2 Bulgarian data
The cities include the capital of Bulgaria ? Sofia, the
second and third biggest Bulgarian cities ? Plovdiv
and Varna, a city from the southern parts of Bulgaria
? Haskovo, the capital of England ? London and
the capital of Russia ? Moskva. The occurrences of
these examples are conflated in the ambiguous name
CITY.
For countries we choose Russia (Rusiya)3, Ger-
many (Germaniya), France (Franciya), Turkey (Tur-
ciya) and England (Angliya). The five names are
conflated into COUNTRY.
The organizations we worked with are the two
leading Bulgarian political parties. BSP (Balgar-
ska Socialisticeska Partija, or Bulgarian Socialist
Party) is the left leaning party and the successor to
the Bulgarian Communist Party. SDS (Sayuz na
demokratichnite sili, or The Union of Democratic
Forces) is the right leaning political party. The two
organizations are conflated into PARTY.
For the RIVER category we choose Danube
(Dunav) which is the second longest river in Eu-
rope and passes by Bulgaria, Maritsa which is the
longest river that runs solely in the interior of the
Balkans, Struma and Mesta which run in Bulgaria
and Greece.
The final category consists of the oldest Bulgarian
mountain situated in the southern part of Bulgaria ?
Rhodope (Rodopi), Rila which is the highest moun-
tain in Bulgaria and on the whole Balkan Penin-
sula, and Pirin which is the second highest Bulgarian
mountain after Rila. The three mountain names are
conflated and substituted with the label MOUNTAIN.
5.3 Bulgarian name discrimination
The experimental settings coincide with those pre-
sented in Section 4 and the obtained results are
shown in Table 4. The performance of our approach
ranges from 32 to 81%. For the five categories, the
best performance is achieved for those names that
have the majority number of examples.
For instance, for the CITY category, the best per-
formance of 79% is reached with Sofia. TAs we
have previously mentioned, this is due to the fact that
LSA has more evidence about the context in which
Sofia appears. It is interesting to note that the city
3this is the Bulgarian transliteration for Russia
24
Category Instance Total P R F
City
Plovdiv 1822 44.42 83.87 58.08
Sofiya 5633 71.39 89.79 79.54
Varna 1042 32.02 82.64 46.17
Haskovo 140 21.09 69.29 32.33
London 751 31.32 84.82 45.74
Moskva 1087 39.47 88.22 54.53
Country
Rusiya 2043 55.83 86.19 67.77
Germaniya 1588 40.72 77.96 53.50
Francia 1352 37.27 77.81 50.39
Turciya 1162 43.23 84.08 57.10
Angliya 655 29.67 72.67 42.14
Party
BSP 2323 42.54 99.35 59.57
SDS 3916 64.86 98.85 78.32
River
Dunav 403 85.39 76.92 80.94
Marica 203 77.88 83.25 80.47
Mesta 81 63.64 95.06 76.24
Struma 37 56.67 91.89 70.10
Mountain
Rila 101 70.22 91.09 79.31
Pirin 294 75.11 57.48 65.12
Rodopi 135 71.04 96.29 81.76
Table 4: Bulgarian NE discrimination
Varna forms part of weak named entities such as the
University of Varna, the Major house of Varna. Al-
though, this strong entity is embedded into the weak
ones, practically Varna changes its semantic cate-
gory from a city into university, major house. This
creates additional ambiguity in our already conflated
and ambiguous names. In order to improve the per-
formance, we need a better data generation process
where the mixture of weak and strong entities will
be avoided.
The same effect of best classification for major-
ity sense is observed with the COUNTRY category.
The best performance of 67% is obtained for Rus-
sia. The other country which is distinguished sig-
nificantly well is Turkey. The 57% performance is
from 5 to 10% higher compared to the performances
of Germany, England and France. This is due to the
context in which the names occur. Turkey is related
to trading with Bulgaria and emigration, meanwhile
the other countries appear in the context of the Eu-
ropean Union, the visit of the Bulgarian president in
these countries.
During the error analysis, we noticed that in the
context of the political parties, SDS appeared many
times in with the names of the political leader or the
representatives of the BSP party and vice versa. This
impeded LSA?s classification, because of the similar
context.
Among all categories, RIVER and MOUNTAIN
obtained the best performances. The rivers Dunav
and Maritsa reached 80%, while the mountains
Rodopi achieved 81.76% f-score. Looking at the
discrimination results for the other names in these
categories, it can be seen that their performances are
much higher compared to the names of the CITY,
COUNTY and PARTY categories. This experiment
shows that the discrimination power is related to the
type of the NE category we want to resolve.
6 Conclusions
In this paper, we have presented a language indepen-
dent approach for person name categorization and
discrimination. This approach is based on the sen-
tence semantic similarity information derived from
LSA. The approach is evaluated with different NE
examples for the Spanish and Bulgarian languages.
We have observed the discrimination performance of
LSA not only with the SINGER and PRESIDENT
companies, but also with the CITY, COUNTRY,
MOUNTAIN, RIVER and PARTY. This is the first
approach which focuses on the resolution of these
categories for the Bulgarian language.
The obtained results both for Spanish and Bulgar-
ian are very promising. The baselines are outper-
formed with 25%. The person fine-grained catego-
rization reaches 90% while the name discrimination
varies from 42% to 90%. This variability is related
to the degree of the name ambiguity among the con-
flated names and similar behaviour is observed in the
co-occurence approach of (Pedersen et al, 2005).
During the experimental evaluation, we found out
that the 100% name purity (e.g. that one name be-
longs only to one and the same semantic category)
which we accept during the data creation in real-
ity contains 9% noise. These observations are con-
firmed in the additional experimental study we have
conducted with the Bulgarian language. According
to the obtained results, our text semantic similarity
approach performs very well and practically there
is no restrain to be adapted to other languages, data
sets or even new categories.
7 Future Work
In the future, we want to relate the name discrimi-
nation and categorization processes, by first encoun-
tering the different underlying meanings of a name
25
and then grouping together the sentences that belong
to the same semantic category. This process will in-
crease the performance of the NE fine-grained cat-
egorization, and will reduce the errors we encoun-
tered during the classification of the singers Enrique
Iglesias and Madonna. In addition to this experi-
ment, we want to cluster web pages on the basis of
name ambiguity. For instance, we want to process
the result for the Google?s query George Miller, and
form three separate clusters obtained on the basis of
a fine-grained and name discrimination. Thus we
can form the clusters for GeorgeMiller the congress-
man, the movie director and the father of WordNet.
This study will include also techniques for automatic
cluster stopping.
Moreover, LSA?s ability of language indepen-
dence can be exploited to resolve cross-language
NE categorization and discrimination from which
we can extract cross-language pairs of semantically
related words characterizing a person e.g. George
Bush is seen with White House in English, la Casa
Blanca in Spanish, a Casa Branka in Portuguese and
Beliat Dom in Bulgarian.
With LSA, we can also observe the time consis-
tency property of a person which changes its se-
mantic category across time. For instance, a stu-
dent turns into a PhD student, teaching assistant and
then university professor, or as in the case of Arnold
Schwarzenegger from actor to governor.
Acknowledgements
We would like to thank the three anonymous re-
viewers for their useful comments and suggestions.
This work was partially funded by the European
Union under the project QALLME number FP6 IST-
033860 and by the Spanish Ministry of Science and
Technology under the project TEX-MESS number
TIN2006-15265-C06-01.
References
A. Bagga and B. Baldwin. 1998. Entity-based cross-
document coreferencing using the vector space model.
In Proceedings of the Thirty-Sixth Annual Meeting of
the ACL and Seventeenth International Conference on
Computational Linguistics, pages 79?85.
G. Cleuziou, L. Martin, and C. Vrain. 2004. Poboc: An
overlapping clustering algorithm, application to rule-
based classification and textual data. In ECAI, pages
440?444.
S. Deerwester, S. Dumais, G. Furnas, T. Landauer, and
R. Harshman. 1990. Indexing by latent semantic anal-
ysis. In Journal of the American Society for Informa-
tion Science, volume 41, pages 391?407.
S. Dumais. 1995. Using lsi for information filtering:
Trec-3 experiments. In The Third Text Retrieval Con-
ference (TREC-3), pages 219?230.
M. Fleischman and E. Hovy. 2002. Fine grained classifi-
cation of named entities. In Proceedings of the 19th in-
ternational conference on Computational linguistics,
pages 1?7.
Z. Kozareva, O. Ferra?ndeza, A. Montoyo, R. Mun?oz,
A. Sua?rez, and J. Go?mez. 2007. Combining data-
driven systems for improving named entity recogni-
tion. Data and Knowledge Engineering, 61(3):449?
466, June.
G. Mann. 2002. Fine-grained proper noun ontologies for
question answering. In COLING-02 on SEMANET,
pages 1?7.
G. Miller and W. Charles. 1991. Contextual correlates of
semantic similarity. In Language and Cognitive Pro-
cesses, pages 1?28.
M. Pasca. 2004. Acquisition of categorized named enti-
ties for web search. In CIKM ?04: Proceedings of the
thirteenth ACM international conference on Informa-
tion and knowledge management, pages 137?145.
T. Pedersen, A. Purandare, and A. Kulkarni. 2005. Name
discrimination by clustering similar contexts. In CI-
CLing, pages 226?237.
P. Shah, D. Schneider, C. Matuszek, R.C. Kahlert,
B. Aldag, D. Baxter, J. Cabral, M. Witbrock, and
J. Curtis. 2006. Automated population of cyc: Ex-
tracting information about named-entities from the
web. In Proceedings of the Nineteenth International
FLAIRS Conference, pages 153?158.
H. Shu?tze. 1998. Automatic word sense discrimination.
In Journal of computational linguistics, volume 24.
H. Tanev and B. Magnini. 2006. Weakly supervised ap-
proaches for ontology population. In Proceeding of
11th Conference of the European Chapter of the Asso-
ciation for Computational Linguistics, pages 17?24.
26
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 334?337,
Prague, June 2007. c?2007 Association for Computational Linguistics
UA-ZBSA: A Headline Emotion Classification through Web Information
Zornitsa Kozareva, Borja Navarro, Sonia Va?zquez, Andre?s Montoyo
DLSI, University of Alicante
Carretera de San Vicente S/N
Alicante, Spain
03080
zkozareva,borja,svazquez,montoyo@dlsi.ua.es
Abstract
This paper presents a headline emotion clas-
sification approach based on frequency and
co-occurrence information collected from
the World Wide Web. The content words of
a headline (nouns, verbs, adverbs and adjec-
tives) are extracted in order to form different
bag of word pairs with the joy, disgust, fear,
anger, sadness and surprise emotions. For
each pair, we compute the Mutual Informa-
tion Score which is obtained from the web
occurrences of an emotion and the content
words. Our approach is based on the hypoth-
esis that group of words which co-occur to-
gether across many documents with a given
emotion are highly probable to express the
same emotion.
1 Introduction
The subjective analysis of a text is becoming impor-
tant for many Natural Language Processing (NLP)
applications such as Question Answering, Informa-
tion Extraction, Text Categorization among others
(Shanahan et al, 2006). The resolution of this prob-
lem can lead to a complete, realistic and coher-
ent analysis of the natural language, therefore ma-
jor attention is drawn to the opinion, sentiment and
emotion analysis, and to the identification of be-
liefs, thoughts, feelings and judgments (Quirk et al,
1985), (Wilson and Wiebe, 2005).
The aim of the Affective Text task is to clas-
sify a set of news headlines into six types of emo-
tions: ?anger?, ?disgust?, ?fear?, ?joy?, ?sadness?
and ?surprise?. In order to be able to conduct
such multi-category analysis, we believe that first
we need a comprehensive theory of what a human
emotion is, and then we need to understand how the
emotion is expressed and transmitted within the nat-
ural language. These aspects rise the need of syn-
tactic, semantic, textual and pragmatic analysis of
a text (Polanyi and Zaenen, 2006). However, some
of the major drawbacks in this field are related to
the manual or automatic acquisition of subjective ex-
pressions, as well as to the lack of resources in terms
of coverage.
For this reason, our current emotion classification
approach is based on frequency and co-occurrence
bag of word counts collected from the World Wide
Web. Our hypothesis is that words which tend to co-
occur across many documents with a given emotion
are highly probable to express this emotion.
The rest of the paper is organized as follows. In
Section 2 we review some of the related work, in
Section 3 we describe our web-based emotion classi-
fication approach for which we show a walk-through
example in Section 4. A discussion of the obtained
results can be found in Section 5 and finally we con-
clude in Section 6.
2 Related work
Our approach for emotion classification is based on
the idea of (Hatzivassiloglou and McKeown, 1997)
and is similar to those of (Turney, 2002) and (Tur-
ney and Littman, 2003). According to Hatzivas-
siloglou and McKeown (1997), adjectives with the
same polarity tended to appear together. For exam-
ple the negative adjectives ?corrupt and brutal? co-
334
occur very often.
The idea of tracing polarity through adjective co-
occurrence is adopted by Turney (2002) for the bi-
nary (positive and negative) classification of text re-
views. They take two adjectives, for instance ?ex-
cellent? and ?poor? in a way that the first adjective
expresses positive meaning, meanwhile the second
one expresses negative. Then, they extract all ad-
jectives from the review text and combine them with
?excellent? and ?poor?. The co-occurrences of these
words are searched on the web, and then the Mutual
Information score for the two groups of adjectives
is measured. When the adjective of the review ap-
pear more often with ?excellent?, then the review is
classified as positive, and when the adjectives appear
more often with ?poor?, then the review is classified
as negative.
Following Hatzivassiloglou and McKeown (1997)
and Turney (2002), we decided to observe how often
the words from the headline co-occur with each one
of the six emotions. This study helped us deduce
information according to which ?birthday? appears
more often with ?joy?, while ?war? appears more
often with ?fear?.
Some of the differences between our approach
and those of Turney (2002) are mentioned below:
? objectives: Turney (2002) aims at binary text
classification, while our objective is six class
classification of one-liner headlines. Moreover,
we have to provide a score between 0 and 100
indicating the presence of an emotion, and not
simply to identify what the emotion in the text
is. Apart from the difficulty introduced by the
multi-category classification, we have to deal
with a small number of content words while
Turney works with large list of adjectives.
? word class: Turney (2002) measures polarity
using only adjectives, however in our approach
we consider the noun, the verb, the adverb and
the adjective content words. The motivation
of our study comes from (Polanyi and Zaenen,
2006), according to which each content word
can express sentiment and emotion. In addition
to this issue we saw that most of the headlines
contain only nouns and verbs, because they ex-
press objectivity.
? search engines: Turney (2002) uses the Al-
tavista web browser, while we consider and
combine the frequency information acquired
from three web search engines.
? word proximity: For the web searches, Tur-
ney (2002) uses the NEAR operator and con-
siders only those documents that contain the
adjectives within a specific proximity. In our
approach, as far as the majority of the query
words appear in the documents, the frequency
count is considered.
? queries: The queries of Turney (2002) are made
up of a pair of adjectives, and in our approach
the query contains the content words of the
headline and an emotion.
There are other emotion classification approaches
that use the web as a source of information. For
instance, (Taboada et al, 2006) extracted from the
web co-occurrences of adverbs, adjectives, nouns
and verbs. Gamon and Aue (2005) were looking
for adjectives that did not co-occur at sentence level.
(Baroni and Vegnaduzzo, 2004) and (Grefenstette
et al, 2004) gathered subjective adjectives from the
web calculating the Mutual Information score.
Other important works on sentiment analysis are
those of (Wilson et al, 2005) and (Wiebe et al,
2005; Wilson and Wiebe, 2005), who used linguistic
information such as syntax and negations to deter-
mine polarity. Kim and Hovy (2006) integrated verb
information from FrameNet and incorporated it into
semantic role labeling.
3 Web co-occurrences
In order to determine the emotions of a
headline, we measure the Pointwise Mu-
tual Information (MI) of ei and cwj as
MI(ei, cwj) = log2 hits(ei,cwj)hits(ei)hits(cwj) , where ei ?
{anger, disgust, fear, joy, sadness, surprise}
and cwj are the content words of the headline j.
For each headline, we have six MI scores which
indicate the presence of the emotion. MI is used
in our experiments because it provides information
about the independence of an emotion and a bag of
words.
To collect the frequency and co-occurrence counts
of the headline words, we need large and massive
335
data repositories. To surmount the data sparsity
problem, we used as corpus the World Wide Web
which is constantly growing and daily updated.
Our statistical information is collected from three
web search engines: MyWay1, AlltheWeb2 and Ya-
hoo3. It is interesting to note that the emotion dis-
tribution provided by each one of the search engines
for the same headline has different scores. For this
reason, we decided to compute an intermediate MI
score as aMI =
?n
s=1 MI(ei,cwj)
s .
In the trail data, besides the MI score of an emo-
tion and all headline content words, we have calcu-
lated the MI for an emotion and each one of the con-
tent words. This allowed us to determine the most
sentiment oriented word in the headline and then we
use this predominant emotion to weight the associ-
ation sentiment score for the whole text. Unfortu-
nately, we could not provide results for the test data
set, due to the high number of emotion-content word
pairs and the increment in processing time and re-
turned responses of the search engines.
4 Example for Emotion Classification
As a walk through example, we use the Mortar as-
sault leaves at least 18 dead headline which is taken
from the trial data. The first step in our emotion clas-
sification approach consists in the determination of
the part-of-speech tags for the one-liner. The non-
content words are stripped away, and the rest of the
words are taken for web queries. To calculate the MI
score of a headline, we query the three search en-
gines combining ?mortar, assault, leave, dead? with
the anger, joy, disgust, fear, sadness and surprise
emotions. The obtained results are normalized in a
range from 0 to 100 and are shown in Table 1.
MyWay AllWeb Yahoo Av. G.Stand.
anger 19 22 24 22 22
disgust 5 6 7 6 2
fear 44 50 53 49 60
joy 15 19 20 18 0
sadness 28 36 36 33 64
surprise 4 5 6 5 0
Table 1: Performance of the web-based emotion
classification for a trail data headline
1www.myway.com
2www.alltheweb.com
3www.yahoo.com
As can be seen from the table, the three search
engines provide different sentiment distribution for
the same headline, therefore in our final experiment
we decided to calculate intermediate MI. Comparing
our results to those of the gold standard, we can say
that our approach detects significantly well the fear,
sadness and angry emotions.
5 Results and Discussion
Table 2 shows the obtained results for the affective
test data. The low performance of our approach
is explainable by the minimal knowledge we have
used. An interesting conclusion deduced from the
trail and test emotion data is that the system detects
better the negative feelings such as anger, disgust,
fear and sadness, in comparison to the positive emo-
tions such as joy and surprise. This makes us believe
that according to the web most of the word-emotion
combinations we queried are related to the expres-
sion of negative emotions.
UA-ZBSA Fine-grained Coarse-grained
Pearson Acc. P. R.
Anger 23.20 86.40 12.74 21.66
Disgust 16.21 97.30 0.00 0.00
Fear 23.15 75.30 16.23 26.27
Joy 2.35 81.80 40.00 2.22
Sadness 12.28 88.90 25.00 0.91
Surprise 7.75 84.60 13.70 16.56
Table 2: Performance of the web-based emotion
classification for the whole test data set
In the test run, we could not apply the emotion-
word weighting, however we believe that it has
a significant impact over the final performance.
Presently, we were looking for the distribution of all
content words and the emotions, but in the future we
would like to transform all words into adjectives and
then conduct web queries.
Furthermore, we would like to combine the re-
sults from the web emotion classification with the
polarity information given by SentiWordNet4. A-
priory we want to disambiguate the headline content
words and to determine the polarities of the words
and their corresponding senses. For instance, the ad-
jective ?new? has eleven senses, where new#a#3 and
new#a#5 express negativism, new#a#4 and new#a#9
positivism and the rest of the senses are objective.
4http://sentiwordnet.isti.cnr.it/
336
So far we did not consider the impact of valence
shifter (Polanyi and Zaenen, 2006) and we were un-
able to detect that a negative adverb or adjective
transforms the emotion from positive into negative
and vice versa. We are also interested in studying
how to conduct queries not as a bag of words but
bind by syntactic relations (Wilson et al, 2005).
6 Conclusion
Emotion classification is a challenging and difficult
task in Natural Language Processing. For our first
attempt to detect the amount of angry, fear, sadness,
surprise, disgust and joy emotions, we have pre-
sented a simple web co-occurrence approach. We
have combined the frequency count information of
three search engines and we have measured the Mu-
tual Information score between a bag of content
words and emotion.
According to the yielded results, the presented ap-
proach can determine whether one sentiment is pre-
dominant or not, and most of the correct sentiment
assignments correspond to the negative emotions.
However, we need to improve the approach in many
aspects and to incorporate more knowledge-rich re-
sources, as well as to tune the 0-100 emotion scale.
Acknowledgements
This research has been funded by QALLME number
FP6 IST-033860 and TEX-MESS number TIN2006-
15265-C06-01.
References
Marco Baroni and Stefano Vegnaduzzo. 2004. Identi-
fying subjective adjectives through web-based mutual
information. In Ernst Buchberger, editor, Proceedings
of KONVENS 2004, pages 17?24.
Michael Gamon and Anthony Aue. 2005. Automatic
identification of sentiment vocabulary: exploiting low
association with known sentiment terms. In Proceed-
ings of the Workshop on Feature Engineering for Ma-
chine Learning in Natural Language Processing (ACL
2005), pages 57?64.
Gregory Grefenstette, Yan Qu, James G. Shanahana, and
David A. Evans. 2004. Coupling niche browsers and
affect analysis for an opinion mining application. In
Proceeding of RIAO-04.
Vasileios Hatzivassiloglou and Kathleen R. McKeown.
1997. Predicting the semantic orientation of adjec-
tives. In Proceedings of the eighth conference on Eu-
ropean chapter of the Association for Computational
Linguistics (EACL).
Soo-Min Kim and Eduard Hovy. 2006. Extracting opin-
ions, opinion holders, and topics expressed in online
news media text. In Proceedings of the Workshop on
Sentiment and Subjectivity in Text, pages 1?8.
Livia Polanyi and Annie Zaenen. 2006. Contextual va-
lence shifter. In James G. Shanahan, Yan Qu, and
Janyce Wiebe, editors, Computing Attitude and Affect
in Text: Theory and Applications, chapter 1, pages 1?
10. Springer.
R. Quirk, S. Greenbaum, G. Leech, and J. Svartvik. 1985.
A Comprehensive Grammar of the English Language.
Longman.
James G. Shanahan, Yan Qu, and Janyce Wiebe. 2006.
Computing Attitude and Affect in Text: Theory and Ap-
plications. Springer.
Maite Taboada, Caroline Anthony, and Kimberly Voll.
2006. Methods for creating semantic orientation
databases. In Proceeding of LREC-06, the 5th Interna-
tional Conference on Language Resources and Evalu-
ation, pages 427?432.
Peter D. Turney and Michael L. Littman. 2003. Measur-
ing praise and criticism: Inference of semantic orien-
tation from association. ACM Transactions on Infor-
mation Systems, 21(4):315?346.
Peter D. Turney. 2002. Thumbs up or thumbs down?
semantic orientation applied to unsupervised classifi-
cation of reviews. In Proceedings of the 40th Annual
Meeting of the Association for Computational Linguis-
tics (ACL), pages 417?424.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.
Annotating expressions of opinions and emotions in
language. Language Resources and Evaluation (for-
merly Computers and the Humanities), 39(2-3):165?
210.
Theresa Wilson and Janyce Wiebe. 2005. Annotating
attributions and private states. In Ann Arbor, editor,
Proceedings of the Workshop on Frontiers in Corpus
Annotation II: Pie in the Sky, pages 53?60.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In Proceedings of the conference
on Human Language Technology and Empirical Meth-
ods in Natural Language Processing, pages 347?354.
337
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 338?341,
Prague, June 2007. c?2007 Association for Computational Linguistics
UA-ZSA: Web Page Clustering on the basis of Name Disambiguation
Zornitsa Kozareva, Sonia Vazquez, Andres Montoyo
DLSI, University of Alicante
Carretera de San Vicente S/N
Alicante, Spain
03080
zkozareva,svazquez,montoyo@dlsi.ua.es
Abstract
This paper presents an approach for web
page clustering. The different underlying
meanings of a name are discovered on the
basis of the title of the web page, the body
content, the common named entities across
the documents and the sub-links. This in-
formation is feeded into a K-Means cluster-
ing algorithm which groups together the web
pages that refer to the same individual.
1 Introduction
Ambiguity is the task of building up multiple alter-
native linguistic structures for a single input. Most
of the approaches focus on word sense disambigua-
tion (WSD), where the sense of a word has to be
determined depending on the context in which it is
used.
The same problem arises for named entities
shared by different people or for grandsons named
after their grandparents. For instance, querying the
name ?Michael Hammond? in the World Wide Web
where there are huge quantities of massive and un-
structured data, a search engine retrieves thousands
of documents related to this name. However, there
are several individuals sharing the name ?Michael
Hammon?. One is a biology professor at the Univer-
sity of Arizona, another is at the University of War-
wick, there is a mathematician from Toronto among
others. The question is which one of these refer-
ents we are actually looking for and interested in.
Presently, to be able to answer to this question, we
have to skim the content of the documents and re-
trieve the correct answers on our own.
To automate this process, the named entities
can be disambiguated and the different underlying
meanings of the name can be found. On the basis
of this information, the web pages can be clustered
together and organized in a hierarchical structure
which can ease the documents? browsing. This is
also the objective of the Web People Search (WePS)
task (Artiles et al, 2007). What makes the WePS
task even more challenging is the fact that in con-
trast to WSD where the number of senses of a word
are predefined, in WePS we do not know the exact
number of different individuals.
For the resolution of the WePS task, we have de-
veloped a web page clustering approach using the
title and the body content of the web pages. In ad-
dition, we group together the documents that share
many location, person and organization names, as
well as those that point out to the same sub-links.
The rest of the paper is organized as follows. In
Section 2 we describe various approaches for name
disambiguation and discrimination. Our approach
is shown in Section 3, the obtained results and a dis-
cussion are provided in Section 4 and finally we con-
clude in Section 5.
2 Related Work
Early work in the field of name disambiguation
is that of (Bagga and Baldwin, 1998) who pro-
posed cross-document coreference resolution algo-
rithm which uses vector space model to resolve the
ambiguities between people sharing the same name.
The approach is evaluated on 35 different mentions
of John Smith and reaches 85% f-score.
Mann and Yarowski (2003) developed an unsu-
338

HTML/XML cleaning 
Search 
Web 
Retrieved Documents 
Preprocessing 
Title 
Context information 
Body 
Text Proper names 
Links 
Clusters 
K-means Cluster analysis WEKA 
LSA matrix transformation 
Clustering 
On the basis of name disambiguation 
Matrix from context 
Figure 1: Architecture of the WePS System
pervised approach to name discrimination where bi-
ographical features (age, date of birth), familiar re-
lationships (wife, son, daughter) and associations
(country, company, organization) are considered.
Therefore, in our approach we use person, organiza-
tion and location names in order to construct a social
similarity network between two documents.
Another unsupervised clustering technique for
name discrimination of web pages is that of Peder-
sen and Kulkarni (2007). They used contextual vec-
tors derived from bigrams, and measured the impact
of several association measures. During the evalu-
ation, some names were easily discriminable com-
pared to others categories for which was even diffi-
cult to find and obtain discriminative feature. We
worked with their unigram model (Purandare and
Pedersen, 2004) to cluster the web pages using the
text content between the title tags.
3 Web Person Disambiguation
Our web people clustering approach is presented in
Figure 1 and consists of the following steps:
? HTML cleaning: all html tags are stripped
away, the javascript code is eliminated, the non
closed WePS tags are repaired, the missing be-
gin/end body tags are included and then the
content between the title, the body and the an-
chor tags is extracted.
? name matching: the location, person and orga-
nization names in the body texts are identified
with the GATE1 system (Cunningham, 2005).
Each named entity of a document is matched
with its corresponding named entity category
from the rest of the web pages. This infor-
mation is used to calculate the social semantic
similarity of the person, the location and the or-
ganization names. Our hypothesis is that doc-
uments with similar names tend to refer to the
same individual. The output of this module is
a matrix with binary values, where 1 stands for
the documents which share more than the half
of their proper names, and 0 otherwise.
? links: for each document, we extract the links
situated between the anchor tags. Since the
links are too specific, we wrote an url function
which transform a given web page d1 with URL
http://www.cs.ualberta.ca/?lindek/index.htm
into www.cs.ualberta.ca/?lindek,
and the web page d2 with URL
http://www.cs.ualberta.ca/?lindek/demos.htm
into www.cs.ualberta.ca/?lindek. According
to our approach, the two web pages d1 and d2
are linked to each other if their link structures
(LS) intersect, that is LS(d1)?LS(d2) 6= 0.
The output of this module is a matrix with
binary values, where 1 stands for two web
pages having more than 3 links in common and
0 otherwise.
? titles: for each document, we extract the text
between the title tags. We create a unigram
matrix which is feed into SenseClusters2. We
use automatic cluster stopping criteria with the
gap statistics which groups the web pages into
several clusters according to the context of the
titles. From the obtained clusters, we generate
a new matrix with binary values, where 1 corre-
sponds to the documents which were put in the
1http://sourceforge.net/projects/gate
2http://marimba.d.umn.edu/cgi-bin/SC-cgi/index.cgi
339
same cluster according to SenseClusters and 0
otherwise.
? bodies: the text between the body tags is ex-
tracted, tokenized and the part-of-speech (POS)
tags 3 are determined. The original text is trans-
formed by encoding the POS tag information as
follows: ?water#v the#det flowers#n and#conj
pass#v me#pron the#det glass#n of#prep wa-
ter#n?. This corpus transformation is done, be-
cause we want the Latent Semantic Analysis
(LSA) module to consider the syntactic cate-
gories of the words and to construct a more
reliable semantic space. For instance, in the
example above, there are two different repre-
sentations of water: the noun and the verb,
while without the corpus transformation LSA
sees only the string water.
? LSA4: the semantic similarity score for the
web-pages is calculated with Latent Semantic
Analysis (LSA). From the encoded body texts,
we build up a matrix, where the rows repre-
sent the words of the web-page collection, the
columns stand for the web-pages we want to
cluster and the cells show the number of times a
word of the corpus occurs in a web page. In or-
der to reduce the noise and the data sparsity, we
apply the Singular Value Decomposition algo-
rithm by reducing the original vector space into
300 dimensions. The output of the LSA mod-
ule is a matrix, which represents the semantic
similarity among the web pages.
? knowledge combination: the outputs of the
name matching, link, title and body modules
are combined into a new matrix 100 ? 400 di-
mensional matrix. The rows correspond to the
number of web pages and the columns repre-
sent the obtained values of the link, title, body
and name modules. This matrix is fed into
the K-means clustering algorithm which deter-
mines the final web page clustering.
? K-means5: the clustering of N web pages
into K disjoint subsets Sj containing Nj data
3http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/
4infomap-nlp.sourceforge.net/
5http://www.cs.waikato.ac.nz/ml/weka/
points is done by the minimization of the sum-
of-squares criterion J = ?Kj=1
?
n?Sj |xn ?
muj |2, where xn is a vector representing the
nth data point and muj is the geometric cen-
troid of the data points in Sj . The informa-
tion matrix from which the web page cluster-
ing is performed includes the similarity infor-
mation for the title, link, proper name and body.
The current implementation of K-means (Wit-
ten and Frank, 2005) does not have an au-
tomatic cluster stopping criteria, therefore the
number of clusters is set up manually.
4 Results and Discussion
Table 1 shows the obtained results for the test data
set. The average performance of our system is 56%
and we ranked on 10-th position from 16 participat-
ing teams. Although, we have used different sources
of information and various approximations, in the
future we have to surmount a number of obstacles.
One of the limitations comes from the usage of the
text snippets situated between the body tags. There
are a number of web pages which do not contain any
text. The semantic space for these documents cannot
be built with LSA and their similarity score is zero.
Despite the fact that we have eliminated the stop
words from the documents and we have transformed
the web pages by encoding the syntactic categories,
the classification power of LSA was different for the
ambiguous names and for the web pages. To some
extend this is due to the varying number of words
in the web pages. In the future, we want to con-
duct experiments with a fixed context windows for
all documents.
In this task, the number of senses (e.g. number
of different individuals that share the same name)
is unknown, and one of the major drawbacks in our
approach is related to the setting up of the number
of clusters. The K-Means clustering algorithm we
used, did not include an automatic cluster stopping
criteria, and we had to set up the number of clus-
ters manually. To be able to do that, we have ob-
served the average number of clusters per name in
the trial data. We have evaluated the performance
of our approach with several different numbers of
clusters. According to the obtained results, the best
clusters are 25 and 50. We used the same number
340
Name Purity Inverse
Purity
F
?=0.5
F
?=0.2
Mark Johnson 0,55 0,74 0,63 0,69
Sharon Goldwater 0,96 0,23 0,37 0,27
Robert Moore 0,36 0,67 0,47 0,57
Leon Barrett 0,62 0,51 0,56 0,52
Dekang Lin 0,99 0,43 0,60 0,49
Stephen Clark 0,52 0,75 0,62 0,69
Frank Keller 0,38 0,67 0,48 0,58
Jerry Hobbs 0,54 0,63 0,58 0,61
James Curran 0,53 0,61 0,57 0,59
Chris Brockett 0,73 0,40 0,51 0,44
Thomas Fraser 0,66 0,57 0,61 0,58
John Nelson 0,68 0,76 0,72 0,74
James Hamilton 0,56 0,60 0,58 0,59
William Dickson 0,59 0,78 0,67 0,73
James Morehead 0,36 0,64 0,46 0,56
Patrick Killen 0,56 0,69 0,62 0,66
George Foster 0,46 0,70 0,56 0,64
James Davidson 0,58 0,71 0,64 0,68
Arthur Morgan 0,77 0,47 0,59 0,51
Thomas Kirk 0,26 0,90 0,41 0,60
Patrick Killen 0,56 0,69 0,62 0,66
Harry Hughes 0,66 0,54 0,59 0,56
Jude Brown 0,64 0,63 0,64 0,63
Stephan Johnson 0,56 0,80 0,66 0,73
Marcy Jackson 0,40 0,73 0,52 0,63
Karen Peterson 0,56 0,72 0,63 0,68
Neil Clark 0,68 0,36 0,47 0,40
Jonathan Brooks 0,53 0,76 0,63 0,70
Violet Howard 0,58 0,75 0,65 0,71
Global average 0,58 0,64 0,58 0,60
Table 1: Evaluation results
of clusters for the test data, however this is a rough
parameter estimation.
5 Conclusion
Person name disambiguation is a very important task
whose resolution can improve the performance of
the search engine by grouping together web pages
which refer to different individuals that share the
same name.
For our participation in the WePS task, we pre-
sented a name disambiguation approach which uses
only the information extracted from the web pages.
We conducted an experimental study with the trail
data set, according to which the combination of
the title, the body, the proper names and sub-links
reaches the best performance. Our current approach
can be improved with the incorporation of automatic
cluster stopping criteria.
So far we did not take advantage of the document
ranking and the returned snippets, but we want to in-
corporate this information by measuring the snippet
similarity on the basis of relevant domain informa-
tion (Kozareva et al, 2007).
Acknowledgements
Many thanks to Ted Pedersen for useful comments
and suggestions. This work was partially funded
by the European Union under the project QALLME
number FP6 IST-033860 and by the Spanish Min-
istry of Science and Technology under the project
TEX-MESS number TIN2006-15265-C06-01.
References
J. Artiles, J. Gonzalo, and S. Sekine. 2007. The semeval-
2007 weps evaluation: Establishing a benchmark for
the web people search task. In Proceedings of Semeval
2007, Association for Computational Linguistics.
A. Bagga and B. Baldwin. 1998. Entity-based cross-
document coreferencing using the vector space model.
In Proceedings of ACL, pages 79?85.
H. Cunningham. 2005. Information Extraction, Auto-
matic. Encyclopedia of Language and Linguistics, 2nd
Edition.
Z. Kozareva, S. Vazquez, and A. Montoyo. 2007. The
usefulness of conceptual representation for the iden-
tification of semantic variability expressions. In Pro-
ceedings of the Eighth International Conference on In-
telligent Text Processing and Computational Linguis-
tics, (CICLing-2007).
G. Mann and D. Yarowsky. 2003. Unsupervised per-
sonal name disambiguation. In Proceedings of the sev-
enth conference on Natural language learning at HLT-
NAACL 2003, pages 33?40.
T. Pedersen and A. Kulkarni. 2007. Discovering identi-
ties in web contexts with unsupervised clustering. In
Proceedings of the IJCAI-2007 Workshop on Analytics
for Noisy Unstructured Text Data.
A. Purandare and T. Pedersen. 2004. Senseclusters -
finding clusters that represent word senses. In AAAI,
pages 1030?1031.
I. Witten and E. Frank. 2005. Data Mining: Practi-
cal machine learning tools and techniques, volume 2.
Morgan Kaufmann.
341
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 427?432,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
UMCC-DLSI: Integrative resource for disambiguation task
Yoan Gutie?rrez and Antonio Ferna?ndez
DI, University of Matanzas
Autopista a Varadero km 31/2
Matanzas, Cuba
yoan.gutierrez,antonio.fernandez@umcc.cu
Andre?s Montoyo and Sonia Va?zquez
DLSI, University of Alicante
Carretera de San Vicente S/N
Alicante, Spain
montoyo,svazquez@dlsi.ua.es
Abstract
This paper describes the UMCC-DLSI
system in SemEval-2010 task number 17
(All-words Word Sense Disambiguation
on Specific Domain). The main purpose
of this work is to evaluate and compare
our computational resource of WordNet?s
mappings using 3 different methods:
Relevant Semantic Tree, Relevant
Semantic Tree 2 and an Adaptation of
k-clique?s Technique. Our proposal is
a non-supervised and knowledge-based
system that uses Domains Ontology and
SUMO.
1 Introduction
Ambiguity is the task of building up multiple
alternative linguistic structures for a single
input (Kozareva et al, 2007). Word Sense
Disambiguation (WSD) is a key enabling-
technology that automatically chooses the
intended sense of a word in context. In this task,
one of the most used lexical data base is WordNet
(WN) (Fellbaum, 1998). WN is an online lexical
reference system whose design is inspired by
current psycholinguistic theories of human lexical
memory. Due to the great popularity of WN
in Natural Language Processing (NLP), several
authors (Magnini and Cavaglia, 2000), (Niles
and Pease, 2001), (Niles and Pease, 2003),
(Valitutti, 2004) have proposed to incorporate to
the semantic net of WN, some taxonomies that
characterize, in one or several concepts, the senses
of each word. In spite of the fact that there have
been developed a lot of WordNet?s mappings,
there isn?t one unique resource to integrate all
of them in a single system approach. To solve
this need we have developed a resource that joins
WN1, the SUMO Ontology2, WordNet Domains3
and WordNet Affect4. Our purpose is to test the
advantages of having all the resources together for
the resolution of the WSD task.
The rest of the paper is organized as follows.
In Section 2 we describe the architecture of the
integrative resource. Our approach is shown in
Section 3. Next section presents the obtained
results and a discussion. And finally the
conclusions in Section 5.
2 Background and techniques
2.1 Architecture of the integrative resource
Our integrative model takes WN 1.6 as nucleus
and links to it the SUMO resource. Moreover,
WordNet Domains 2.0 (WND) and WordNet
Affect 1.1 (WNAffects) are also integrated but
mapped instead to WN 2.0. From the model
showed in Figure 1, a computational resource has
been built in order to integrate the mappings above
mentioned.
The model integrator?s proposal provides
a software that incorporates bookstores of
programming classes, capable to navigate inside
the semantic graph and to apply any type of
possible algorithm to a net. The software
architecture allows to update WN?s version.
In order to maintain the compatibility with other
resources mapped to WN, we have decided to use
WN 1.6 version. However, the results can be
offered in anyone of WN?s versions.
1http://www.cogsci.princeton.edu/ wn/
2http://suo.ieee.org
3http://wndomains.fbk.eu/
4http://wndomains.fbk.eu/wnaffect.html
427
 Figure 1: WordNet integrative model
2.2 The k-clique?s Technique
Formally, a clique is the maximum number of
actors who have all possible ties presented among
themselves. A ?Maximal complete sub-graph? is
such a grouping, expanded to include as many
actors as possible.
?A k-clique is a subset of vertices C such that,
for every i, j ? C, the distance d(i, j)
k
. The 1-
clique is identical to a clique, because the distance
between the vertices is one edge. The 2-clique
is the maximal complete sub-graph with a path
length of one or two edges?. (Cavique et al, 2009)
3 The Proposal
Our proposal consists in accomplishing three runs
with different algorithms. Both first utilize the
domain?s vectors; the third method utilizes k-
cliques? techniques.
This work is divided in several stages:
1. Pre-processing of the corpus (lemmatization
with Freeling) (Atserias et al, 2006).
2. Context selection (For the first (3.1), and
the third (3.3) run the context window was
constituted by the sentence that contains
the word to disambiguate; in the second
run the context window was constituted
by the sentence that contains the word to
disambiguate, the previous sentence and the
next one).
3. Obtaining the domain vector, this vector is
used in first and the second runs (when
the lemma of the words in the analyzed
sentence is obtained, the integrative resource
of WordNet?s Mappings is used to get the
respective senses from each lemma).
4. Obtaining the all resource vector: SUMO,
Affects, and Domain resource. This is only
for the third run (3.3).
5. Relevant Semantic Tree construction
(Addition of concepts parents to the vectors.
For the first (3.1) and second (3.2) runs only
Domain resource is used; for the third (3.3)
run all the resources are used).
6. Selection of the correct senses (the first and
the second runs use the same way to do the
selection; the third run is different. We make
an exception: For the verb ?be? we select the
sense with the higher frequency according to
Freeling.
3.1 Relevant Semantic Tree
With this proposal we measure how much a
concept is correlated to the sentence, similar to
Reuters Vector (Magnini et al, 2002), but with
a different equation. This proposal has a partial
similarity with the Conceptual Density (Agirre
and Rigau, 1996) and DRelevant (Va?zquez et al,
2004) to get the concepts from a hierarchy that
they associate with the sentence.
In order to determine the Association Ratio
(RA) of a domain in relation to the sentence, the
Equation 1 is used.
RA(D, f) =
n
?
i=1
RA(D, f
i
) (1)
where:
RA(D,w) = P (D,w) ? log
2
P (D,w)
P (D)
(2)
f : is a set of words w.
f
i
: is a i-th word of the phrase f .
P (D,w): is joint probability distribution.
P (D): is marginal probability.
From now, vectors are created using the
Senseval-2?s corpus. Next, we show an example:
For the phrase: ?But it is unfair to dump
on teachers as distinct from the educational
establishment?.
By means of the process Pres-processing
analyzed in previous stage 1 we get the lemma and
the following vector.
428
Phrase [unfair; dump; teacher, distinct,
educational; establishment]
Each lemma is looked for in WordNet?s
integrative resource of mappings and it is
correlated with concepts of WND.
Vector
RA Domains
0.9 Pedagogy
0.9 Administration
0.36 Buildings
0.36 Politics
0.36 Environment
0.36 Commerce
0.36 Quality
0.36 Psychoanalysis
0.36 Economy
Table 1: Initial Domain Vector
After obtaining the Initial Domain Vector we
apply the Equation 3 in order to build the Relevant
Semantic Tree related to the phrase.
DN(CI,Df) = RA CI ?
MP (CI,Df)
TD
(3)
Where DN : is a normalized distance
CI: is the Initial Concept which you want to
add the ancestors.
Df : is Parent Domain.
RA CI: is a Association Ratio of the child
Concept.
TD: is Depth of the hierarchic tree of the
resource to use.
MP : is Minimal Path.
Applying the Equation 3 the algorithm to decide
which parent domain will be added to the vector is
shown here:
if (DN(CI,Df) > 0)
{
if ( Df not exist)
Df is added to the vector with DN value;
else
Df value = Df value + DN ;
}
As a result the Table 2 is obtained.
This vector represents the Domain tree
associated to the phrase.
After the Relevant Semantic Tree is obtained,
the Domain Factotum is eliminated from the tree.
Due to the large amount of WordNet synsets,
Vector
RA Domains
1.63 Social Science
0.9 Administration
0.9 Pedagogy
0.8 RootDomain
0.36 Psychoanalysis
0.36 Economy
0.36 Quality
0.36 Politics
0.36 Buildings
0.36 Commerce
0.36 Environment
0.11 Factotum
0.11 Psychology
0.11 Architecture
0.11 Pure Science
Table 2: Final Domain Vector
 
Figure 2: Relevant semantic tree
that do not belong to a specific domain, but
rather they can appear in almost all of them, the
Factotum domain has been created. It basically
includes two types of synsets: Generic synsets,
which are hard to classify in a particular domain;
and Stop Senses synsets which appear frequently
in different contexts, such as numbers, week
days, colors, etc. (Magnini and Cavaglia, 2000),
(Magnini et al, 2002). Words that contain this
synsets are frequently in the phrases, therefore the
senses associated to this domain are not selected.
After processing the patterns that characterize
the sentence, the following stage is to determine
the correct senses, so that the next steps ensue:
1. Senses that do not coincide with the
grammatical category of Freeling are
removed.
429
2. For each word to disambiguate all candidate
senses are obtained. Of each sense
the relevant vector are obtained using the
Equation 4, and according to the previous
Equation 3 parent concepts are added.
RA(D, s) = P (D, s) ? log
2
P (D, s)
P (D)
(4)
where s: is a sense of word.
P (D, s): is joint probability distribution
between Domain concept D and the sense s.
P (D): is marginal probability of the Domain
concept.
3. The one that accumulates the bigger value of
relevance is assigned as correct sense. The
following process is applied:
For each coincidence of the elements in the
senses? domain vector with the domain vector
of the sentence, the RA value of the analyzed
elements is accumulated. The process is
described in the Equation 5.
AC(s, V RA) =
?
k
V RA[V s
k
]
?
i=1
V RA
i
(5)
where AC: The RA value accumulated for
the analyzed elements.
V RA: Vector of relevant domains of the
sentence with the format: V RA [domain ?
value RA].
V s: Vector of relevant domain of the sense
with the format: V s [domain].
V s
k
: Is a k-th domain of the vector V s.
V RA[V s
k
]: Represents the value of RA
assigned to the domain V sk for the value
V RA.
The
?
i=1
V RA
i
term normalizes the result.
3.2 Relevant Semantic Tree 2
This run is the same as the first one with a
little difference, the context window is constituted
by the sentence that contains the word to
disambiguate, the previous sentence and the next
one.
3.3 Adaptation of k-clique?s technique to the
WSD
They are applied, of the section 3, the steps from
the 1 to the 5, where the semantic trees of concepts
are obtained.
Then they are already obtained for all the
words of the context, all the senses discriminated
according to Freeling (Atserias et al, 2006).
Then a sentence?s net of knowledge is built
by means of minimal paths among each sense
and each concept at trees. Next the k-clique?s
technique is applied to the net of knowledge to
obtain cohesive subsets of nodes.
To obtain the correct sense of each word it is
looked, as proposed sense, the sense belonging to
the subset containing more quantities of nodes and
if it has more than a sense for the same word,
the more frequent sense is chosen according to
Freeling.
4 Results and Discussion
The conducted experiments measure the
influence of the aforementioned resources in
the disambiguation task. We have evaluated them
individually and as a whole. In the Table 3 it
is represented each one of the inclusions and
combinations experimented with the Relevant
Semantic Tree method.
Resources Precision Recall Attempted
WNAffect 0.242 0.237 97.78%
SUMO 0.267 0.261 98.5%
WND 0.328 0.322 98.14%
WND &
SUMO
0.308 0.301 97.78%
WND &
SUMO &
WNAffect
0.308 0.301 97.78%
Table 3: Evaluation of integrated resources
As it can be observed, in the evaluation for
specific domain corpus the best results are reached
when only domain resource is used. But this
is not a conclusion about the resources inclusion
because the use of this method for global domain,
for example with the task English All words from
Senseval-2 (Agirre et al, 2010), the experiment
adding all the resources showed good results. This
is due to the fact that the global domain includes
information of different contexts, exactly what
is representing in the mentioned resources. For
430
this reason, in the experiment with global domain
and the inclusion of all the resource obtained
better results than using this method with specific
domain, 42% of recall and 45% of precision
(Gutie?rrez, 2010).
For example, with the k-clique?s technique,
utilizing the English All word task from Senseval-
2?s corpus, the results for the test with global
dominion were: with single domain inclusion 40
% of precision and recall; but with the three
resources 41.7 % for both measures.
Table 4 shows the obtained results for the test
data set. The average performance of our system
is 32% and we ranked on 27-th position from
27 participating systems. Although, we have
used different sources of information and various
approximations, in the future we have to surmount
a number of obstacles.
One of the limitations comes from the usage
of the POS-tagger Freeling which introduces
some errors in the grammatical discrimination.
Representing a loss of 3.7% in the precision of our
system.
The base of knowledge utilized in the task was
WordNet 1.6; but the competition demanded the
results with WordNet 3.0. In order to achieve
this we utilized mappings among versions where
119 of 1398 resulting senses emitted by Semeval-
2 were did not found. This represents an 8.5%.
In our proposal, the sense belonging to the
Factotum Domain was eliminated, what disabled
that the senses linked to this domain went
candidates to be recovered. 777 senses of 1398
annotated like correct for Semeval-2 belong to
domain Factotum, what represents that the 66%
were not recovered by our system. Considering
the senses that are not correlated to Factotum,
that is, that correlate to another domains, we are
speaking about 621 senses to define; The system
would emit results of a 72,4%. Senses selected
correctly were 450, representing a 32%. However,
189 kept on like second candidates to be elected.
This represents a 13.5%. If a technique of more
precise decision takes effect, the results of the
system could be increased largely.
5 Conclusion and future works
For our participation in the Semeval-2 task
17 (All-words Word Sense Disambiguation on
Specific Domain), we presented three methods
for disambiguation approach which uses an
Methods Precision Recall Attempted
Relevant
Domains
Tree
0.328 0.322 98.14%
Relevant
Semantic
Tree 2
0.321 0.315 98.14%
Relevant
Cliques
0.312 0.303 97.35%
Table 4: Evaluation results
integrative resource of WordNet mappings. We
conducted an experimental study with the trail
data set, according to which the Relevant Semantic
Tree reaches the best performance. Our current
approach can be improved with the incorporation
of more granularities in the hierarchy of WordNet
Domains. Because it was demonstrated that
to define correct senses associated to specific
domains an improvement of 72.4% is obtained.
At this moment, only domain information is used
in our first and second method. Besides was
demonstrated for specific domains, the inclusion
of several resources worsened the results with the
first and second proposal method, the third one has
been not experimented yet. Despite the fact that
we have knowledge of SUMO, WordNet-Affect
and WordNet Domain in our third method we still
not obtain a relevant result.
It would be convenient to enrich our resource
with other resources like Frame-Net, Concept-Net
or others with the objective of characterizing even
more the senses of the words.
Acknowledgments
This paper has been supported partially by
Ministerio de Ciencia e Innovacio?n - Spanish
Government (grant no. TIN2009-13391-C04-
01), and Conselleria d?Educacio? - Generalitat
Valenciana (grant no. PROMETEO/2009/119 and
ACOMP/2010/288).
References
Eneko Agirre and German Rigau. 1996. Word
sense disambiguation using conceptual density. In
Proceedings of the 16th International Conference
on Computational Linguistic (COLING?96),
Copenhagen, Denmark.
Eneko Agirre, Oier Lopez de Lacalle, Christiane
Fellbaum, Shu-kai Hsieh, Maurizio Tesconi, Monica
431
Monachini, Piek Vossen, and Roxanne Segers.
2010. Semeval-2010 task 17: All-words word
sense disambiguation on a specific domain. In
Proceedings of the 5th International Workshop on
Semantic Evaluations (SemEval-2010), Association
for Computational Linguistics.
Jordi Atserias, Bernardino Casas, Elisabet Comelles,
Meritxell Gonza?lez, Llu??s Padro?, and Muntsa Padro?.
2006. Freeling 1.3: Syntactic and semantic services
in an open-source nlp library. In Proceedings
of the fifth international conference on Language
Resources and Evaluation (LREC 2006), ELRA.
Lu??s Cavique, Armando B. Mendes, and Jorge M.
Santos. 2009. An algorithm to discover the k-
clique cover in networks. In EPIA ?09: Proceedings
of the 14th Portuguese Conference on Artificial
Intelligence, pages 363?373, Berlin, Heidelberg.
Springer-Verlag.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. The MIT Press.
Yoan Gutie?rrez. 2010. Resolucio?n de ambiguedad
sema?ntica mediante el uso de vectores de conceptos
relevantes.
Zornitsa Kozareva, Sonia Va?zquez, and Andre?s
Montoyo. 2007. Ua-zsa: Web page clustering on
the basis of name disambiguation. In Semeval I. 4th
International Wordshop on Semantic Evaluations.
Bernardo Magnini and Gabriela Cavaglia. 2000.
Integrating subject field codes into wordnet. In
Proceedings of Third International Conference on
Language Resources and Evaluation (LREC-2000).
Bernardo Magnini, Carlo Strapparava, Giovanni
Pezzulo, and Alfio Gliozzo. 2002. Comparing
ontology-based and corpus-based domain
annotations in wordnet. In Proceedings of the
First International WordNet Conference, pages
146?154.
Ian Niles and Adam Pease. 2001. Towards a standard
upper ontology. In FOIS, pages 2?9.
Ian Niles and Adam Pease. 2003. Linking lexicons
and ontologies: Mapping wordnet to the suggested
upper merged ontology. In IKE, pages 412?416.
Ro Valitutti. 2004. Wordnet-affect: an affective
extension of wordnet. In Proceedings of the 4th
International Conference on Language Resources
and Evaluation, pages 1083?1086.
Sonia Va?zquez, Andre?s Montoyo, and German Rigau.
2004. Using relevant domains resource for word
sense disambiguation. In IC-AI, pages 784?789.
432
Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, ACL-HLT 2011, pages 139?145,
24 June, 2011, Portland, Oregon, USA c?2011 Association for Computational Linguistics
Sentiment Classification Using Semantic Features Extracted from 
WordNet-based Resources 
Yoan Guti?rrez 
Department of Informatics 
University of Matanzas, Cuba. 
{yoan.gutierrez}@umcc.cu 
Sonia V?zquez and Andr?s Montoyo 
Department of Software and Computing 
Systems 
 University of Alicante, Spain. 
{svazquez, montoyo}@dlsi.ua.es 
 
Abstract 
In this paper, we concentrate on the 3 of 
the tracks proposed in the NTCIR 8 
MOAT, concerning the classification of 
sentences according to their 
opinionatedness, relevance and polarity. 
We propose a method for the detection of 
opinions, relevance, and polarity 
classification, based on ISR-WN (a 
resource for the multidimensional analysis 
with Relevant Semantic Trees of sentences 
using different WordNet-based information 
sources). Based on the results obtained, we 
can conclude that the resource and methods 
we propose are appropriate for the task, 
reaching the level of state-of-the-art 
approaches. 
1 Introduction 
In recent years, textual information has become 
one of the most important sources of knowledge to 
extract useful and heterogeneous data. Texts can 
provide from factual information such as 
descriptions, lists of characteristics or instructions 
to opinionated information such as reviews, 
emotions or feelings. This heterogeneity has 
motivated that dealing with the identification and 
extraction of opinions and sentiments in texts 
require special attention. In fact, the development 
of different tools to help government information 
analysts, companies, political parties, economists, 
etc to automatically get feelings from news and 
forums is a challenging task (Wiebe et al, 2005). 
Many researchers such as Balahur et al, (2010), 
Hatzivassiloglou et al(2000), Kim and Hovy 
(2006), Wiebe et al (2005) and many others have 
been working in this way and  related areas. 
Moreover, in the course of years we find a long 
tradition on developing Question Answering (QA) 
systems. However, in recent years, researchers 
have concentrated on the development of Opinion 
Questions Answering (OQA) systems (Balahur et 
al., 2010). This new task has to deal with different 
problems such as Sentiment Analysis where 
documents must be classified according to 
sentiments and subjectivity features. Therefore, a 
new kind of evaluation that takes into account this 
new issue is needed.  
One of the competitions that establishes the 
benchmark for opinion question answering 
systems, in a monolingual and cross-lingual 
setting, is the NTCIR Multilingual Opinion 
Analysis Task (MOAT) 1 . In this competition,  
researchers work hard to achieve better results on 
Opinion Analysis, introducing different 
techniques.  
In this paper, we only concentrate on three 
tracks proposed in the NTCIR 8 MOAT, 
concerning to the classification of sentences 
according to their opinionatedness, relevance and 
polarity. We propose a method for the detection of 
opinions, relevance and polarity classification, 
based on ISR-WN which is a resource for the 
multidimensional analysis with Relevant Semantic 
Trees of sentences using different WordNet-based 
information sources. 
2 Related works 
Related to Opinion Analysis task we can find 
many points of view. Some researchers say that 
adjectives combined with semantic characteristics 
provide vital information to the performance of 
Opinion Analysis (Hatzivassiloglou et al, 2000). 
Others like Zubaryeva and Savoy (2010) assume 
                                                 
1http://research.nii.ac.jp/ntcir/ntcir-ws8/meeting/ 
139
that the extraction of relevant terms on the 
documents could define their polarity, designing a 
method capable of selecting terms that clearly 
belong to one type of polarity. Another research 
based on features extraction was conducted by Lai 
et al (2010), they developed a trained system on 
Japanese Opinionated Sentence Identification. And 
Balahur and Montoyo (2009) proposed a method to 
extract, classify and summarize opinions on 
products from web reviews. It was based on the 
prior building of product characteristics taxonomy 
and on the semantic relatedness given by the 
Normalized Google Distance (Cilibrasi and 
Vit?nyi, 2007) and SVM learning. As we can see, 
the usage of features extraction is a suitable mode 
to work on Opinion Analysis task. Apart from that 
other authors have used semantic resources, for 
example, Kim and Hovy (2006, 2005) used 
semantic resources to get an approach on Holder 
Detection and Opinion Extraction tasks. 
In general, using semantic resources is one of 
the most applied procedures over different tasks 
such as Document Indexing, Document 
Classification, Word Sense Disambiguation, etc. In 
Natural Language Processing (NLP), one of the 
most used resources for WSD and other tasks is 
WordNet (WN) (Fellbaum, 1998). WN is a lexical 
dictionary with word senses and descriptions. In 
order to enrich the WN resource, it has been linked 
with different lexical resources such as WordNet 
Domains (WND) (Magnini and Cavaglia, 2000) a 
lexical resource containing the  domains of the 
synsets in WordNet, SUMO (Niles, 2001) an 
ontology relating the concepts in WordNet, 
WordNet Affect (WNA) an extension of WN 
where different synsets are annotated with one of 
the six basic emotions proposed by Ekman (1999), 
SentiWordNet (Esuli and Sebastiani, 2006) a 
lexical resource  where each synset is annotated 
with polarity, Semantic Classes (SC) (Izquierdo et 
al., 2007) a set of Base Level Concepts (BLC) 
based on WN, etc. The usage of these resources 
allows the tackling of NLP tasks from different 
points of view, depending on the resource used.  
Our approach proposes using different semantic 
dimensions according to different resources. In 
order to achieve this, we use the Integration of 
Semantic Resources based on WordNet, which we 
explain in the next section and the Semantic 
Classes (SC). 
2.1 Integration of Semantic Resources based on 
WordNet (ISR-WN) 
ISR-WN (Guti?rrez et al, 2010b) is a new 
resource that allows the integration of several 
semantic resources mapped to WN. In ISR-WN, 
WordNet 1.6 or 2.0 is used as a core to link several 
resources: SUMO, WND and WNA. As Guti?rrez 
et al (2010a) describe, the integrated resource 
allows navigate inside the semantic network. 
2.2 Semantic Classes (SC) 
The Semantic Classes resource (Izquierdo et al, 
2007) consists of a set of Base Level Concepts 
(BLC) from WN obtained before applying a 
bottom-up process using the chain of hypernym 
relations. For each synset in WN, the process 
selects as its Base Level Concept the first local 
maximum, according to the relative number of 
relations. As a result, a resource with a set of BLCs 
linked semantically to several synsets is obtained. 
In order to apply the multidimensionality that 
ISR-WN and SC provide, we have analyzed 
related approaches like (Magnini et al, 2002; 
2008) ,(V?zquez et al, 2004), (Villarejo et al, 
2005), (Zouaq et al, 2009) and others that take 
into account semantic dimensionality. Then, we 
have decided to use Relevant Semantic Trees 
(Guti?rrez et al, 2010a) because it is an approach 
capable of being applied over several dimensions 
(resources) at once. 
2.3 Relevant Semantic Trees (RST) 
RST (Guti?rrez et al, 2010a) is a method able to 
disambiguate the senses of the words contained in 
a sentence by obtaining the Relevant Semantic 
Trees from different resources. In order to measure 
the association between concepts in each sentence 
according to a multidimensional perspective, RST 
uses the Association Ratio (AR) measure (V?zquez 
et al, 2004). Our purpose is to include the 
Multidimensional Semantic Analysis into the 
Opinion Analysis using RSTs. 
In order to evaluate our approach the rules and 
corpus that concern the English monolingual 
subtasks from MOAT were used. 
2.4 English monolingual subtasks 
In these tasks the participants were provided with 
twenty topics. For each one of the topics, a 
question was given with a short and concise query, 
140
the expected polarity of the answer and the period 
of time. For each of the topics, a set of documents 
were assigned and they had to be splitted into 
sentences for the opinionated and relevance 
judgements and into opinion units for the polarity, 
opinion target and source tasks. In this work, we 
describe twelve runs for the opinionated, relevance 
and polarity judgement tasks. 
3 WSD method 
We propose an unsupervised knowledge-based 
method that uses the RST technique combined 
with SentiWordNet 3.0 (Esuli and Sebastiani, 
2006) to tackle 3 of the monolingual English tasks 
proposed in the NTCIR 8 MOAT. In this approach 
WN 2.0 version is used.  
The aim of this method is to obtain a RST of 
each sentence and then associate the RST with 
polarity values. The process involves the following 
resources: WND, WNA, the WN taxonomy, 
SUMO and Semantic Classes (SC). Because of SC 
does not have a tree structure we simply obtain the 
Relevant Semantic Classes. Subsequently, we 
determine the polarities collected for each label of 
each RST obtained according to the analyzed 
sentence. Our proposal involves four steps 
presented on sections 3.1, 3.2, 3.3 and 3.4. 
3.1 Obtaining the Relevant Semantic Trees  
In this section, we use a fragment of the original 
RST method with the aim of obtaining Relevant 
Semantic Trees of the sentences. Notice that this 
step must be applied for each resource. 
Once each sentence is analyzed, the AR value is 
obtained and related to each concept in the trees. 
Equation 1 is used to measure and to obtain the 
values of Relevant Concepts:  
                 
 
   
  (1) 
Where: 
                   
      
    
  (2) 
In both equations C is a concept; f is a sentence 
or set of words (w); fi is the i-th word of the 
sentence f; P (C, w) is the joint probability 
distribution; P (C) is the marginal probability. 
In order to illustrate the processing steps, we 
will consider the following example: ?But it is 
unfair to dump on teachers as distinct from the 
educational establishment?. Using the WND 
resource, we show the manner in which we obtain 
the RST. 
The first stage involves the lemmatization of the 
words in the sentence. For the example considered, 
the obtained lemmas are:  
Lemmas [unfair; dump; teacher, distinct, 
educational; establishment] 
Next, each lemma is looked up in ISR-WN and 
it is correlated with the WND concepts. Table 1 
shows the results after applying Equation 1 over 
the example. 
Vector 
AR Domain AR Domain 
0.90 Pedagogy 0.36 Commerce 
0.90 Administration 0.36 Quality 
0.36 Buildings 0.36 Psychoanalysis 
0.36 Politics 0.36 Economy 
0.36 Environment   
Table 1. Initial Concept Vector of Domains 
After obtaining the Initial Concept Vector of 
Domains we apply Equation 3 in order to obtain 
the Relevant Semantic Tree related to the sentence.  
                              ;(3) 
Where:  
           
         
  
 ;(4) 
Here AR(PC, f) represents the AR value of PC 
related to the sentence f;           is the AR 
value calculated with equation 1 in case of ChC 
was included in the Initial Vector, otherwise is 
calculated with the equation 3; ChC is the Child 
Concept of PC; ND is a Normalized Distance; IC 
is the Initial Concept from we have to add the 
ancestors; PC is Parent Concept; TD is Depth of 
the hierarchic tree of the resource to use; and MP 
is Minimal Path. 
Applying the Equation 3, the algorithm to 
decide which parent concept will be added to the 
vector is shown here: 
if (         value > 0 ){ 
 if ( PC had not been added to vector) 
       PC is added to the vector with AR(PC, f) value;  
else PC value = PC value + AR(PC, f) value; } 
The result after processing is shown in Table 2. 
This vector represents the Domain tree associated 
to the sentence.  After the Relevant Semantic Tree 
is obtained, the Factotum Domain is eliminated 
141
from the tree. Due to the fact that Factotum is a 
generic Domain associated to words that appear in 
general contexts it does not provide useful 
information and experimentally we confirmed that 
it introduced errors; so we eliminate it (Magnini 
and Cavaglia, 2000). 
Vector 
AR Domain AR Domain 
1.63 Social_Science  0.36 Buildings  
0.90 Administration  0.36 Commerce  
0.90 Pedagogy  0.36 Environment  
0.80 Root_Domain  0.11 Factotum 
0.36 Psychoanalysis 0.11 Psychology  
0.36 Economy  0.11 Architecture  
0.36 Quality 0.11 Pure_Science  
0.36 Politics 
  
Table 2. Final Domain Vector 
3.2 Obtaining the Positive Semantic Trees  
In order to obtain the Positive Semantic Trees 
(PST) of the sentence, we will follow the same 
process described in section 3.1. In this case, the 
AR values will be replaced by the polarity value 
pertaining to the analyzed sense. The polarity is 
obtained from the SentiWordNet 3.0 resource, 
where each given sense from ISR-WN for 
WordNet version 2.0 is mapped to WordNet 
version 3.0. Hence, we can find each given sense 
from ISR-WN in SentiWordNet 3.0 and obtain the 
respective polarities. This new value will be called 
Positive Association (PosA). The PosA value is 
calculated using Equation 4 . 
                     
 
   
  (4) 
Where: 
                      
 
   
  (5) 
Where C is a concept; f is a sentence or set of 
words (w); fi is a i-th word of the sentence f; PosA 
(C, wi) is the positive value of the sense (wi) 
related to C. 
The PosA is used to measure the positive value 
associated to the leaves of the Semantic Trees 
where Concepts are placed. Subsequently, using 
the same structure of RST we create new Semantic 
Trees without AR values. Instead, the leaves with 
Concepts of this new Semantic Trees will be 
annotated with the PosA value.  
Later, to assign some Positive value to the 
parent Concepts, each parent Concept will 
accumulate the positive values from child 
Concepts. Equation 6 shows the bottom-up 
process. 
                     
 
   
  (6) 
Where PC is the Parent Concept; ChC is the 
Child Concept of PC; and PosA(ChC) represents 
the positive value of the ChC. 
3.3 Obtaining the Negative Semantic Trees 
(NST)  
In this phase, we repeat the step described in 
Section 3.2, but for negative values. Table 3 shows 
the PST and NST obtained from the example. 
Vectors Pos-Neg 
PosA NegA Domain PosA NegA Domain 
0.00 1.00 Social_Science  0.00 0.00 Buildings  
0. 00 0.00 Administration  0.00 0.50 Commerce  
0.00 0.00 Pedagogy  0.00 0.00 Environment  
0.00 0.00 Root_Domain  0.375 0.375 Factotum 
0.00 0.00 Psychoanalysis 0.00 0.00 Psychology  
0.00 0.50 Economy  0.00 0.00 Architecture  
0.375 0.375 Quality 0.00 0.00 Pure_Science  
0.00 0.00 Politics 
   
Table 3. Final Domain Vectors Pos-Neg 
As we can see, the analyzed sentence is more 
linked to the Social_Science domain and it 
accumulates a negative value of 1 and a positive 
value of 0. This indicates that the sentence is more 
negative than positive. 
3.4 Obtaining polarities of the sentences 
In this step, we concentrate on detecting which 
polarity is more representative according to the 
Semantic Trees obtained for each resource 
(dimension). For that, we combine the RST with 
PST and RST with NST. Depending on the obtained 
results we classify the sentence as Positive, 
Negative or Neutral. Before performing this step, 
we have to normalize the three types of Semantic 
Trees (RST, PST and NST) for each dimension to 
work with values between 0 and1.  
Our main goal is to assign more weight to the 
polarities related to the most relevant Concepts in 
each Relevant Semantic Tree. Equation 7 shows 
the steps followed in order to obtain the positive 
semantic value. 
142
                          
   
  (7) 
Where ACPosA is the Positive Semantic Value 
of the analyzed sentence obtained for one 
Dimension, RST is the Relevant Semantic Tree 
sorted with the format: RST [Concept| AR]; PST is 
the Positive Semantic Tree sorted according RST 
structure with format: PST [Concept|PosA]; RSTi 
     is the i-th AR value of Concept i;      PSTi 
is the i-th PosA value of the concept i. 
In order to measure the negative semantic value 
(ACNegA), we employ a similar equation replacing 
PST with NST. After obtaining the semantic 
opinion requirements, we evaluate our approach 
over three of the tasks proposed in the NTCIR 8 
MOAT, for the monolingual English setting. 
3.5 Judging sentence opinionatedness 
The ?opinionated? subtask requires systems to 
assign the values YES or NO to each of the 
sentences in the document collection provided. 
This value is given depending on whether the 
sentence contains an opinion (Y) or it does not (N). 
In order to tackle this task, we analyze the PST and 
NST of all dimensions (WN, WSD, WNA, SUMO 
and SC). After reviewing the PSTs and NSTs if at 
least one Concept has assigned a value distinct 
from zero the result will be ?YES? in other cases 
will be ?NO?.  
3.6 Determining sentence relevance 
In the sentence relevance judgement task, the 
systems have to decide whether a sentence is 
relevant to the given question or not (Y|N). We 
assume that the given question is related to each 
sentence per topic if it has a RST 50% similar (the 
similarity is obtained by quantity of Concept labels 
that match). The analyzed sentence is relevant only 
if the PST and the NST values of all dimensions 
that are taken into account contain at least a 
positive or a negative value. 
3.7 Polarity and topic-polarity classification  
The polarity judgment task requires the systems to 
assign a value of ?POS?, ?NEG? or ?NEU? 
(positive, negative or neutral) to each of the 
sentences in the documents provided. 
Our proposal consists of accumulating the 
ACPos values and ACNeg values of all Dimensions 
and comparing them. These accumulated values 
will be named ACPosD and ACNegD respectively. 
In case ACPosD > ACNegD the assigned value is 
POS, if ACPosD < ACNegD the assigned value is 
NEG, otherwise, the assigned value is NEU. 
4 Evaluation and analysis  
In this section we concentrated on measuring the 
influence of each Dimension (resource) taken 
separately and jointly in our proposal. Also, we 
have compared our results with the best results 
obtained by the participant systems in the NTCIR 
8 MOAT competition. 
4.1 Influence of each dimension 
In this section, we present the results of the three 
tasks described above using the combination of all 
dimensions and using each of the resources 
separately. Moreover, we describe the experiments 
we have performed. Exp1: Combining all 
Dimensions (WND, WNA, WN taxonomy, SUMO 
and SC). Exp2: Using WNA. Exp3: Using WND. 
Exp4: Using SC. Exp5: Using SUMO. Exp6: 
Using WN taxonomy. The results are presented in 
Table 4. 
Exp 
Opinion Relevance Polarity 
P R F P R F P R F 
1 20.6 87.8 33.3 78.8 86.8 82.6 39.4 34.5 36.8 
2 23.8 57.2 33.6 77.9 55.8 65.1 39.7 22.2 28.5 
3 22.6 69.5 34.1 79.4 69.2 74.0 40.3 27.5 32.7 
4 20.1 88.5 33.3 78.8 87.3 82.3 39.7 34.9 37.2 
5 21.3 86.5 34.2 79.0 85.8 82.3 40.6 33.7 36.8 
6 21.1 87.6 34.1 78.8 86.6 82.5 40.5 34.2 37.1 
Table 4. Results on each task. Precision (P), Recall (R) 
and F-Measure (F). 
As we can see, the best results are obtained in 
Experiment 4 and 6, which use the WN taxonomy 
and SC to obtain the RST, PST and NST. However, 
the other experiments results are similar in 
performance level. This indicates that our proposal 
can be successfully applied to opinion mining 
tasks. 
4.2 Influence of the semantic dimensions 
without normalizing the vector 
In order to prove that the value normalization 
introduces noise, we performed the same 
experiments without normalizing vectors. In Table 
5, we show in bold font the F-Measure obtained 
143
that constitutes an improvement to previous 
results. It is important to remark that not 
normalizing the vectors helps the Polarity 
Classification task. All the experiments presented 
in Table 5 improved the previous results and the 
SC obtained one of the best results for the Polarity 
and the Relevance task. 
 
Exp Opinion Relevance Polarity 
P R F P R F P R F 
7 20.1 88.5 33.3 78.8 87.3 82.8 39.7 34.9 37.2 
8 23.3 61.1 33.7 78.4 60.0 68.0 42.3 25.5 31.8 
9 21.9 77.9 34.2 79.2 77.3 78.2 39.4 30.5 34.4 
10 20.6 87.7 33.4 78.9 86.7 82.6 44.6 38.9 41.6 
11 20.6 85.0 33.2 78.5 83.6 81.0 44.6 37.7 40.9 
12 20.5 85.5 33.1 78.7 84.4 81.5 43.7 37.0 40.1 
Table 5. Results without normalized vectors. Precision 
(P), Recall (R) and F-Measure (F). 
4.3 Comparison with other proposals 
In this section, we present a comparison between 
our proposal and the best participating systems in 
NTCIR 8 MOAT. In the sentence opinionatedness 
judgement task , the only systems that obtained 
better results compared to our proposal are UNINE 
(Zubaryeva and Savoy, 2010) and NECLC 
systems. These systems obtained F-measure values 
of 40.1% and 36.52% respectively. These results 
are not so far from our results, with the simple 
difference of 5.9% and 2.32% respectively.  
In comparison to our proposal, UNINE is based 
on selecting terms that clearly belong to one type 
of polarity compared to the others and the value 
types of polarities are defined summing the count 
number of terms that tend to be overused in 
positive, negative and neutral opinionated 
sentences possibilities (Zubaryeva and Savoy, 
2010). The opinionated score is the sum of Positive 
Scores and Negative Scores for each selected term. 
The score of non-opinionated sentences is 
computed as a sum of Objectivity Score for each 
selected term, divided by the number of words in 
the sentence. Our proposal neither takes into 
account the detection of relevant terms, nor the 
objective scores. UNINE also obtained better 
results than us in the Polarity task; we think that 
the combination of this proposal with ours could 
obtain better results. Taking into account that both 
proposals use Features Extraction we could 
combine not only Lexical Features but also 
Semantic Features. 
In the Polarity task we could obtain similar 
results to the first run of UNINE system around 
37% of F-measure but with results some distance 
of the best system that obtained a 51.03% of F-
measure. For the relevance task, our proposal 
obtained a difference of 3.22% as far as F-measure 
is concerned from the best result of all runs 
submitted by the National Taiwan University 
(NTU). So, our proposal could be located around 
the first places among the three tasks mentioned.  
5 Conclusion and further works 
In this paper our research was focused on solving a 
recent problem stemmed from the availability of 
large volumes of heterogeneous data which 
provides different kind of information. We have 
conducted an analysis of how the scientific 
community confronts the tasks related to Opinion 
Analysis. One of the most used approaches is to 
apply Features Extraction and based on this idea, 
our proposal is to apply Semantic Features 
Extraction based on Relevant Semantic Trees. 
With our proposal we are able to associate the 
polarities presented on the sentences with Concept 
Semantic Trees. Thus, the Semantic Trees allow 
the classification of sentences according to their 
opinionatedness, relevance and polarity, according 
to MOAT competition. The obtained results were 
compared with the best results obtained on this 
competition achieving values very close to the best 
systems. Several experiments were conducted 
applying vector normalization and without 
normalization to know which semantic dimension 
performed better. 
After a comparative analysis with the systems 
which results were not improved, we propose as 
further work to include the lexical features 
extraction in our proposal. We have planned to use 
Latent Semantic Analysis and other techniques to 
do this work. 
Acknowledgements 
This paper has been supported partially by 
Ministerio de Ciencia e Innovaci?n - Spanish 
Government (grant no. TIN2009-13391-C04-01), 
and Conselleria d'Educaci?n - Generalitat 
Valenciana (grant no. PROMETEO/2009/119, 
ACOMP/2010/288 and ACOMP/2011/001). 
144
 References 
Alexandra Balahur, Ester Boldrini, Andr?s Montoyo 
and Patricio Mart?nez-Barco. 2010. The OpAL 
System at NTCIR 8 MOAT. In Proceedings of 
NTCIR-8 Workshop Meeting: 241-245. Tokyo, 
Japan. 
Alexandra Balahur and Andr?s Montoyo. 2009. A 
Semantic Relatedness Approach to Classifying 
Opinion from Web Reviews. Procesamiento del 
Lenguaje Natural, 42:47-54. 
Andrea Esuli and Fabrizio Sebastiani. 2006. 
SentiWordNet: A Publicly Available Lexical 
Resource for Opinion Mining. In Fifth international 
conference on Languaje Resources and Evaluation 
417-422.  
Amal Zouaq, Michel Gagnon and Benoit Ozell. 2009. A 
SUMO-based Semantic Analysis for Knowledge 
Extraction. In Proceedings of the 4th Language & 
Technology Conference. Pozna?, Poland. 
Bernardo Magnini and Gabriela Cavaglia. 2000. 
Integrating Subject Field Codes into WordNet. In 
Proceedings of Third International Conference on 
Language Resources and Evaluation (LREC-2000): 
1413--1418.  
Bernardo Magnini, Carlo Strapparava, Giovanni 
Pezzulo and Alfio Gliozzo. 2002. Comparing 
Ontology-Based and Corpus-Based Domain 
Annotations in WordNet. In Proceedings of the First 
International WordNet Conference: 21-25 Mysore, 
India. 
Bernardo Magnini, Carlo Strapparava, Giovanni 
Pezzulo and Alfio Gliozzo. 2008. Using Domain 
Information for Word Sense Disambiguation. In 
Proceedings of the First International Conference 
on Emerging Trends in Engineering and Technology 
(icetet 2008): 1187-1191. Nagpur, India. 
Christiane Fellbaum. 1998. WordNet. An Electronic 
Lexical Database. The MIT Press.  
Guo-Hau Lai, Jyun-Wei Huang, Chia-Pei Gao and 
Richard Tzong-Han Tsai. 2010. Enhance Japanese 
Opinionated Sentence Identification using Linguistic 
Features: Experiences of the IISR Group at NTCIR-
8 MOAT Task. In Proceedings of NTCIR-8 
Workshop Meeting: 272-275. Tokyo, Japan. 
Hatzivassiloglou, Vasileios and Janyce Wiebe. 2000. 
Effects of Adjective Orientation and Gradability on 
Sentence Subjectivity. In International Conference 
on Computational Linguistics (COLING-2000).  
Ian Niles. 2001. Mapping WordNet to the SUMO 
Ontology. Teknowledge Corporation. 
Janyce Wiebe, Theresa Wilson and Claire Cardie. 2005. 
Annotating Expressions of Opinions and Emotions 
in Language. In Kluwer Academic Publishers: 
Netherlands. 
Luis Villarejo, Llu?s M?rquez and German Rigau. 2005. 
Exploring the construction of semantic class 
classiers for WSD. In Sociedad Espa?ola para el 
Procesamiento del Lenguaje Natural, 35: 195-202.  
Olena Zubaryeva and Jacques Savoy. 2010. Opinion 
Detection by Combining Machine Learning & 
Linguistic Tools In Proceedings of NTCIR-8 
Workshop Meeting: 221-227. Tokyo, Japan. 
Paul Ekman. 1999. Handbook of Cognition and 
Emotion. Handbook of Cognition and Emotion: John 
Wiley & Sons, Ltd. 
Rub?n Izquierdo, Armando Su?rez and German Rigau. 
2007. A Proposal of Automatic Selection of Coarse-
grained Semantic Classes for WSD. Procesamiento 
del Lenguaje Natural, 39:189-196. 
Rudi L. Cilibrasi and Paul M.B. Vit?nyi. 2007. The 
Google Similarity Distance. IEEE Transactions On 
Knowledge And Data Engineering, 19(3). 
Soo-Min Kim and Eduard Hovy. 2006. Extracting 
Opinions, Opinion Holders, and Topics Expressed in 
Online News Media Text. In In Proceedings of 
workshop on sentiment and subjectivity in text at 
proceedings of the 21st international conference on 
computational linguistics/the 44th annual meeting of 
the association for computational linguistics 
(COLING/ACL 2006): 1-8. Sydney, Australia. 
Soo-Min Kim and Eduard Hovy. 2005. Identifying 
Opinion Holders for Question Answering in Opinion 
Texts. In Proceedings of AAAI-05 Workshop on 
Question Answering in Restricted Domains. 
Sonia V?zquez, Andr?s Montoyo and German Rigau. 
2004. Using Relevant Domains Resource for Word 
Sense Disambiguation. In IC-AI?04. Proceedings of 
the International Conference on Artificial 
Intelligence: Ed: CSREA Press. Las Vegas, 
E.E.U.U. 
Yoan Guti?rrez, Antonio Fern?ndez, Andr?s Montoyo 
and Sonia V?zquez. 2010a. UMCC-DLSI: 
Integrative resource for disambiguation task. In 
Proceedings of the 5th International Workshop on 
Semantic Evaluation: 427-432. Uppsala, Sweden. 
Yoan Guti?rrez, Antonio Fern?ndez, Andr?s Montoyo 
and Sonia V?zquez. 2010b. Integration of semantic 
resources based on WordNet. In XXVI Congreso de 
la Sociedad Espa?ola para el Procesamiento del 
Lenguaje Natural, 45: 161-168. Universidad 
Polit?cnica de Valencia, Valencia, Spain. 
145
