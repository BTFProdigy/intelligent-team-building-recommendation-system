Proceedings of NAACL HLT 2009: Short Papers, pages 93?96,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
A Simple Sentence-Level Extraction Algorithm for Comparable Data
Christoph Tillmann and Jian-ming Xu
IBM T.J. Watson Research Center
Yorktown Heights, N.Y. 10598
{ctill,jianxu}@us.ibm.com
Abstract
The paper presents a novel sentence pair ex-
traction algorithm for comparable data, where
a large set of candidate sentence pairs is scored
directly at the sentence-level. The sentence-
level extraction relies on a very efficient im-
plementation of a simple symmetric scoring
function: a computation speed-up by a fac-
tor of 30 is reported. On Spanish-English
data, the extraction algorithm finds the highest
scoring sentence pairs from close to 1 trillion
candidate pairs without search errors. Sig-
nificant improvements in BLEU are reported
by including the extracted sentence pairs into
the training of a phrase-based SMT (Statistical
Machine Translation) system.
1 Introduction
The paper presents a simple sentence-level trans-
lation pair extraction algorithm from comparable
monolingual news data. It differs from similar
algorithms that select translation correspondences
explicitly at the document level (Fung and Che-
ung, 2004; Resnik and Smith, 2003; Snover et
al., 2008; Munteanu and Marcu, 2005; Quirk et
al., 2007; Utiyama and Isahara, 2003). In these
publications, the authors use Information-Retrieval
(IR) techniques to match document pairs that are
likely translations of each other. More complex
sentence-level models are then used to extract par-
allel sentence pairs (or fragments). From a com-
putational perspective, the document-level filtering
steps are needed to reduce the number of candidate
sentence pairs. While IR techniques might be use-
ful to improve the selection accuracy, the current pa-
per demonstrates that they are not necessary to ob-
tain parallel sentence pairs. For some data, e.g. the
Portuguese-English Reuters data used in the experi-
ments in Section 3, document-level information may
not even be available.
In this paper, sentence pairs are extracted by a sim-
ple model that is based on the so-called IBM Model-
1 (Brown et al, 1993). The Model-1 is trained
on some parallel data available for a language pair,
i.e. the data used to train the baseline systems in
Section 3. The scoring function used in this pa-
per is inspired by phrase-based SMT. Typically, a
phrase-based SMT system includes a feature that
scores phrase pairs using lexical weights (Koehn et
al., 2003) which are computed for two directions:
source to target and target to source. Here, a sen-
tence pair is scored as a phrase pair that covers all
the source and target words. The scoring function
?(S, T ) is defined as follows:
?(S, T ) = (1)
=
J?
j=1
1
J ? log(
p(sj |T )? ?? ?
1
I ?
I?
i=1
p(sj|ti) )
? ?? ?
?(sj ,T )
+
I?
i=1
1
I ? log(
p(ti|S)? ?? ?
1
J ?
J?
j=1
p(ti|sj) )
? ?? ?
?(ti,S)
93
Here, S = sJ1 is the source sentence of length J and
T = tI1 is the target sentence of length I . p(s|T )
is the Model-1 probability assigned to the source
word s given the target sentence T , p(t|S) is defined
accordingly. p(s|t) and p(t|s) are word translation
probabilities obtained by two parallel Model-1 train-
ing steps on the same data, but swapping the role
of source and target language. They are smoothed
to avoid 0.0 entries; there is no special NULL-word
model and stop words are kept. The log(?) is applied
to turn the sentence-level probabilities into scores.
These log-probabilities are normalized with respect
to the source and target sentence length: this way
the score ?(S, T ) can be used across all sentence
pairs considered, and a single manually set thresh-
old ? is used to select all those sentence pairs whose
score is above it. For computational reasons, the
sum ?(S, T ) is computed over the following terms:
?(ti, S) where 1 ? i ? I and ?(sj, T ), where
1? j? J . The ? ?s and ??s represent partial score
contributions for a given source or target position.
Note that ?(S, T ) ? 0 since the terms ?(?, S) ? 0
and ?(?, T ) ? 0.
Section 2 presents an efficient implementation of
the scoring function in Eq. 1. Its effectiveness is
demonstrated in Section 3. Finally, Section 4 dis-
cusses future work and extensions of the current al-
gorithm.
2 Sentence-Level Processing
We process the comparable data at the sentence-
level: for each language and all the documents in
the comparable data, we distribute sentences over a
list of files : one file for each news feed f (for the
Spanish Gigaword data, there are 3 news feeds) and
publication date d . The Gigaword data comes anno-
tated with sentence-level boundaries, and all docu-
ment boundaries are discarded. This way, the Span-
ish data consists of about 24 thousand files and the
English data consists of about 53 thousand files (for
details, see Table 2). For a given source sentence S,
the search algorithm computes the highest scoring
sentence pair ?(S, T ) over a set of candidate trans-
lations T ? ?, where |?| can be in the hundreds
of thousands of sentences . ? consists of all target
sentences that have been published from the same
news feed f within a 7 day window from the pub-
lication date of the current source sentence S. The
extraction algorithm is guaranteed to find the highest
scoring sentence pairs (S, T ) among all T ? ?. In
order to make this processing pipeline feasible, the
scoring function in Eq. 1 needs to be computed very
efficiently. That efficiency is based on the decompo-
sition of the scoring functions into I + J terms ( ? ?s
and ??s) where source and target terms are treated
differently. While the scoring function computation
is symmetric, the processing is organized according
the source language files: all the source sentences
are processed one-by-one with respect to their indi-
vidual candidate sets ?:
? Caching for target term ?(t, S): For each tar-
get word t that occurs in a candidate translation
T , the Model-1 based probability p(t|S) can be
cached: its value is independent of the other
words in T . The same word t in different tar-
get sentences is processed with respect to the
same source sentence S and p(t|S) has to be
computed only once.
? Array access for source terms ?(s, T ): For a
given source sentence S, we compute the scor-
ing function ?(S, T ) over a set of target sen-
tences T ? ?. The computation of the source
term ?(s, T ) is based on translation probabil-
ities p(s|t) . For each source word s, we can
retrieve all target words t for which p(s|t) > 0
just once. We store those words t along with
their probabilities in an array the size of the tar-
get vocabulary. Words t that do not have an
entry in the lexicon have a 0 entry in that ar-
ray. We keep a separate array for each source
position. This way, we reduce the probability
access to a simple array look-up. Generating
the full array presentation requires less than 50
milliseconds per source sentence on average.
? Early-Stopping: Two loops compute the scor-
ing function ?(S, T ) exhaustively for each sen-
tence pair (S, T ): 1) a loop over all the target
position terms ?(ti, S), and 2) a loop over all
source position terms ?(sj , T ) . Once the cur-
rent partial sum is lower than the best score
?(S, Tbest) computed so far, the computation
can be safely discarded as ?(ti, S), ?(sj , T ) ?
94
Table 1: Effect of the implementation techniques on a
full search that computes ?(S, T ) exhaustively for all sen-
tence pairs (S, T ) for a given S.
Implementation Technique Speed
[secs/sent]
Baseline 33.95
+ Array access source terms 19.66
+ Cache for target terms 3.83
+ Early stopping 1.53
+ Frequency sorting 1.23
0 and adding additional terms can only lower
that partial sum further.
? Frequency-Sorting: Here, we aim at making
the early pruning step more efficient. Source
and target words are sorted according to the
source and target vocabulary frequency: less
frequent words occur at the beginning of a sen-
tence. These words are likely to contribute
terms with high partial scores. As a result, the
early-stopping step fires earlier and becomes
more effective.
? Sentence-level filter: The word-overlap filter
in (Munteanu and Marcu, 2005) has been im-
plemented: for a sentence pair (S, T ) to be con-
sidered parallel the ratio of the lengths of the
two sentences has to be smaller than two. Ad-
ditionally, at least half of the words in each sen-
tence have to have a translation in the other sen-
tence based on the word-based lexicon. Here,
the implementation of the coverage restriction
is tightly integrated into the above implemen-
tation: the decision whether a target word is
covered can be cached. Likewise, source word
coverage can be decided by a simple array
look-up.
3 Experiments
The parallel sentence extraction algorithm presented
in this paper is tested in detail on the large-
scale Spanish-English Gigaword data (Graff, 2006;
Graff, 2007). The Spanish data comes from 3
news feeds: Agence France-Presse (AFP), Associ-
ated Press Worldstream (APW), and Xinhua News
Table 2: Corpus statistics for comparable data. Any
document-level information is ignored.
Spanish English
Date-Feed Files 24, 005 53, 373
Sentences 19.4 million 47.9 million
Words 601.5 million 1.36 billion
Portuguese English
Date-Feed Files 351 355
Sentences 366.0 thousand 5.3 million
Words 11.6 million 171.1 million
Agency (XIN). We do not use the additional news
feed present in the English data. Table 1 demon-
strates the effectiveness of the implementation tech-
niques in Section 2. Here, the average extraction
time per source sentence is reported for one of the
24, 000 source language files. This file contains 913
sentences. Here, the size of the target candidate set
? is 61 736 sentences. All the techniques presented
result in some improvement. The baseline uses only
the length-based filtering and the coverage filtering
without caching the coverage decisions (Munteanu
and Marcu, 2005). Caching the target word proba-
bilities results in the biggest reduction. The results
are representative: finding the highest scoring target
sentence T for a given source sentence S takes about
1 second on average. Since 20 million source sen-
tences are processed, and the workload is distributed
over roughly 120 processors, overall processing time
sums to less than 3 days. Here, the total number of
translation pairs considered is close to 1 trillion.
The effect of including additional sentence pairs
along with selection statistics is presented in Ta-
ble 3. Translation results are presented for a standard
phrase-based SMT system. Here, both languages
use a test set with a single reference. Including about
1.4 million sentence pairs extracted from the Giga-
word data, we obtain a statistically significant im-
provement from 42.3 to 45.6 in BLEU (Papineni et
al., 2002). The baseline system has been trained
on about 1.8 million sentence pairs from Europarl
and FBIS parallel data. We also present results for
a Portuguese-English system: the baseline has been
trained on Europarl and JRC data. Parallel sentence
pairs are extracted from comparable Reuters news
data published in 2006. The corpus statistics for
95
Table 3: Spanish-English and Portuguese-English extrac-
tion results.
Data Source # candidates #train pairs Bleu
Spanish-English: ? = ?4.1
Baseline - 1, 825, 709 42.3
+ Gigaword 955.5 ? 109 1, 372, 124 45.6
Portuguese-English: ? = ?5.0
Baseline - 2, 221, 891 45.3
+ Reuters 06 32.8 ? 109 48, 500 48.5
the Portuguese-English data are given in Table 2.
The selection threshold ? is determined with the
help of bilingual annotators (it typically takes a few
hours). Sentence pairs are selected with a conserva-
tive threshold ?? first. Then, all the sentence pairs are
sorted by descending score. The annotator descends
this list to determine a score threshold cut-off. Here,
translation pairs are considered to be parallel if 75
% of source and target words have a corresponding
translation in the other sentence. Using a threshold
? = ?4.1 for the Spanish-English data, results in a
selection precision of around 80 % (most of the mis-
qualified pairs are partial translations with less than
75 % coverage or short sequences of high frequency
words). This simple selection criterion proved suf-
ficient to obtain the results presented in this paper.
As can be seen from Table 3, the optimal threshold
is language specific.
4 Future Work and Discussion
In this paper, we have presented a novel sentence-
level pair extraction algorithm for comparable data.
We use a simple symmetrized scoring function
based on the Model-1 translation probability. With
the help of an efficient implementation, it avoids
any translation candidate selection at the docu-
ment level (Resnik and Smith, 2003; Smith, 2002;
Snover et al, 2008; Utiyama and Isahara, 2003;
Munteanu and Marcu, 2005; Fung and Cheung,
2004). In particular, the extraction algorithm works
when no document-level information is available.
Its usefulness for extracting parallel sentences is
demonstrated on news data for two language pairs.
Currently, we are working on a feature-rich ap-
proach (Munteanu and Marcu, 2005) to improve
the sentence-pair selection accuracy. Feature func-
tions will be ?light-weight? such that they can be
computed efficiently in an incremental way at the
sentence-level. This way, we will be able to main-
tain our search-driven extraction approach. We are
also re-implementing IR-based techniques to pre-
select translation pairs at the document-level, to
gauge the effect of this additional filtering step. We
hope that a purely sentence-level processing might
result in a more productive pair extraction in future.
References
Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della
Pietra, and Robert L. Mercer. 1993. The Mathematics
of Statistical Machine Translation: Parameter Estima-
tion. CL, 19(2):263?311.
Pascale Fung and Percy Cheung. 2004. Mining Very-
Non-Parallel Corpora: Parallel Sentence and Lexicon
Extraction via Bootstrapping and EM. In Proc, of
EMNLP 2004, pages 57?63, Barcelona, Spain, July.
Dave Graff. 2006. LDC2006T12: Spanish Gigaword
Corpus First Edition. LDC.
Dave Graff. 2007. LDC2007T07: English Gigaword
Corpus Third Edition. LDC.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical Phrase-Based Translation. In Proc. of
HLT-NAACL?03, pages 127?133, Edmonton, Alberta,
Canada, May 27 - June 1.
Dragos S. Munteanu and Daniel Marcu. 2005. Improv-
ing Machine Translation Performance by Exploiting
Non-Parallel Corpora. CL, 31(4):477?504.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A Method for Automatic
Evaluation of Machine Translation. In In Proc. of
ACL?02, pages 311?318, Philadelphia, PA, July.
Chris Quirk, Raghavendra Udupa, and Arul Menezes.
2007. Generative Models of Noisy Translations with
Applications to Parallel Fragment Extraction. In
Proc. of the MT Summit XI, pages 321?327, Copen-
hagen,Demark, September.
Philip Resnik and Noah Smith. 2003. The Web as Paral-
lel Corpus. CL, 29(3):349?380.
Noah A. Smith. 2002. From Words to Corpora: Rec-
ognizing Translation. In Proc. of EMNLP02, pages
95?102, Philadelphia, July.
Matthew Snover, Bonnie Dorr, and Richard Schwartz.
2008. Language and Translation Model Adaptation
using Comparable Corpora. In Proc. of EMNLP08,
pages 856?865, Honolulu, Hawaii, October.
Masao Utiyama and Hitoshi Isahara. 2003. Reliable
Measures for Aligning Japanese-English News Arti-
cles and Sentences. In Proc. of ACL03, pages 72?79,
Sapporo, Japan, July.
96
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 889?898,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
A Correction Model for Word Alignments
J. Scott McCarley, Abraham Ittycheriah, Salim Roukos, Bing Xiang, Jian-ming Xu
IBM T.J. Watson Research Center
1101 Kitchawan Road, Rt. 134
Yorktown Heights, NY 10598
{jsmc,abei,roukos,bxiang,jianxu}@us.ibm.com
Abstract
Models of word alignment built as sequences
of links have limited expressive power, but are
easy to decode. Word aligners that model the
alignment matrix can express arbitrary align-
ments, but are difficult to decode. We pro-
pose an alignment matrix model as a cor-
rection algorithm to an underlying sequence-
based aligner. Then a greedy decoding al-
gorithm enables the full expressive power of
the alignment matrix formulation. Improved
alignment performance is shown for all nine
language pairs tested. The improved align-
ments also improved translation quality from
Chinese to English and English to Italian.
1 Introduction
Word-level alignments of parallel text are crucial for
enabling machine learning algorithms to fully uti-
lize parallel corpora as training data. Word align-
ments appear as hidden variables in IBM Models 1-
5 (Brown et al, 1993) in order to bridge a gap be-
tween the sentence-level granularity that is explicit
in the training data, and the implicit word-level cor-
respondence that is needed to statistically model lex-
ical ambiguity and word order rearrangements that
are inherent in the translation process. Other no-
table applications of word alignments include cross-
language projection of linguistic analyzers (such as
POS taggers and named entity detectors,) a subject
which continues to be of interest. (Yarowsky et al,
2001), (Benajiba and Zitouni, 2010)
The structure of the alignment model is tightly
linked to the task of finding the optimal alignment.
Many alignment models are factorized in order to
use dynamic programming and beam search for ef-
ficient marginalization and search. Such a factoriza-
tion encourages - but does not require - a sequential
(often left-to-right) decoding order. If left-to-right
decoding is adopted (and exact dynamic program-
ming is intractable) important right context may ex-
ist beyond the search window. For example, the link-
age of an English determiner may be considered be-
fore the linkage of a distant head noun.
An alignment model that jointly models all of the
links in the entire sentence does not motivate a par-
ticular decoding order. It simply assigns comparable
scores to the alignment of the entire sentence, and
may be used to rescore the top-N hypotheses of an-
other aligner, or to decide whether heuristic pertur-
bations to the output of an existing aligner constitute
an improvement. Both the training and decoding of
full-sentence models have presented difficulties in
the past, and approximations are necessary.
In this paper, we will show that by using an ex-
isting alignment as a starting point, we can make a
significant improvement to the alignment by propos-
ing a series of heuristic perturbations. In effect, we
train a model to fix the errors of the existing aligner.
From any initial alignment configuration, these per-
turbations define a multitude of paths to the refer-
ence (gold) alignment. Our model learns alignment
moves that modify an initial alignment into the ref-
erence alignment. Furthermore, the resulting model
assigns a score to the alignment and thus could be
used in numerous rescoring algorithms, such as top-
N rescorers.
In particular, we use the maximum entropy frame-
889
work to choose alignment moves. The model is sym-
metric: source and target languages are interchange-
able. The alignment moves are sufficiently rich to
reach arbitrary phrase to phrase alignments. Since
most of the features in the model are not language-
specific, we are able to test the correction model
easily on nine language pairs; our corrections im-
proved the alignment quality compared to the input
alignments in all nine. We also tested the impact on
translation and found a 0.48 BLEU improvement on
Chinese to English and a 1.26 BLEU improvement
on English to Italian translation.
2 Alignment sequence models
Sequence models are the traditional workhorse for
word alignment, appearing, for instance, in IBM
Models 1-5. This type of alignment model is not
symmetric; interchanging source and target lan-
guages results in a different aligner. This parameter-
ization does not allow a target word to be linked to
more than one source word, so some phrasal align-
ments are simply not considered. Often the choice of
directionality is motivated by this restriction, and the
choice of tokenization style may be designed (Lee,
2004) to reduce this problem. Nevertheless, aligners
that use this parameterization internally often incor-
porate various heuristics in order to augment their
output with the disallowed alignments - for example,
swapping source and target languages to obtain a
second alignment (Koehn et al, 2007) with different
limitations. Training both directions jointly (Liang
et al, 2006) and using posterior probabilities dur-
ing alignment prediction even allows the model to
see limited right context. Another alignment combi-
nation strategy (Deng and Zhou, 2009) directly op-
timizes the size of the phrase table of a target MT
system.
Generative models (such as Models 1-5, and the
HMM model (Vogel et al, 1996)) motivate a narra-
tive where alignments are selected left-to-right and
target words are then generated conditioned upon
the alignment and the source words. Generative
models are typically trained unsupervised, from par-
allel corpora without manually annotated word-level
alignments.
Discriminative models of alignment incorporate
source and target words, as well as more linguisti-
cally motivated features into the prediction of align-
ment. These models are trained from annotated
word alignments. Examples include the maximum
entropy model of (Ittycheriah and Roukos, 2005) or
the conditional random field jointly normalized over
the entire sequence of alignments of (Blunsom and
Cohn, 2006).
3 Joint Models
An alternate parameterization of alignment is the
alignment matrix (Niehues and Vogel, 2008). For a
source sentence F consisting of words f1...fm, and
a target sentence E = e1...el, the alignment matrix
A = {?ij} is an l ? m matrix of binary variables.
If ?ij = 1, then ei is said to be linked to fj . If ei
is unlinked then ?ij = 0 for all j. There is no con-
straint limiting the number of source tokens to which
a target word is linked either; thus the binary ma-
trix allows some alignments that cannot be modeled
by the sequence parameterization. All 2lm binary
matrices are potentially allowed in alignment matrix
models. For typical l and m, 2lm  (m + 1)l, the
number of alignments described by a comparable se-
quence model. This parameterization is symmetric -
if source and target are interchanged, then the align-
ment matrix is transposed.
A straightforward approach to the alignment ma-
trix is to build a log linear model (Liu et al, 2005)
for the probability of the alignment A. (We continue
to refer to ?source? and ?target? words only for con-
sistency of notation - alignment models such as this
are indifferent to the actual direction of translation.)
The log linear model for the alignment (Liu et al,
2005) is
p(A|E,F ) = exp (
?
i ?i?i(A,E, F ))
Z(E,F ) (1)
where the partition function (normalization) is given
by
Z(E,F ) =
?
A
exp
(?
i
?i?i(A,E, F )
)
. (2)
Here the ?i(A,E, F ) are feature functions. The
model is parameterized by a set of weights ?i, one
for each feature function. Feature functions are often
binary, but are not required to be. Feature functions
890
may depend upon any number of components ?ij of
the alignment matrix A.
The sum over all alignments of a sentence pair
(2lm terms) in the partition function is computa-
tionally impractical except for very short sentences,
and is rarely amenable to dynamic programming.
Thus the partition function is replaced by an ap-
proximation. For example, the sum over all align-
ments may be restricted to a sum over the n-best
list from other aligners (Liu et al, 2005). This ap-
proximation was found to be inconsistent for small
n unless the merged results of several aligners were
used. Alternately, loopy belief propagation tech-
niques were used in (Niehues and Vogel, 2008).
Loopy belief propagation is not guaranteed to con-
verge, and feature design is influenced by consider-
ation of the loops created by the features. Outside
of the maximum entropy framework, similar models
have been trained using maximum weighted bipar-
tite graph matching (Taskar et al, 2005), averaged
perceptron (Moore, 2005), (Moore et al, 2006), and
transformation-based learning (Ayan et al, 2005).
4 Alignment Correction Model
In this section we describe a novel approach to word
alignment, in which we train a log linear (maximum
entropy) model of alignment by viewing it as correc-
tion model that fixes the errors of an existing aligner.
We assume a priori that the aligner will start from
an existing alignment of reasonable quality, and will
attempt to apply a series of small changes to that
alignment in order to correct it. The aligner naturally
consists of a move generator and a move selector.
The move generator perturbs an existing align-
ment A in order to create a set of candidate align-
mentsMt(A), all of which are nearby to A in the
space of alignments. We index the set of moves by
the decoding step t to indicate that we generate en-
tirely different (even non-overlapping) sets of moves
at different steps t of the alignment prediction. Typ-
ically the moves affect linkages local to a particular
word, e.g. the t?th source word.
The move selector then chooses one of the align-
ments At+1 ? Mt(At), and proceeds iteratively:
At+2 ? Mt+1(At+1), etc. until suitable termina-
tion criteria are reached. Pseudocode is depicted in
Fig. (1.) In practice, one move for each source and
Input: sentence pair E1 .. El, F1 .. Fm
Input: alignment A
Output: improved alignment Afinal
for t = 1? l do
generate moves:Mt(At)
select move:
At+1 ? argmaxA?Mt(At)p(A|At, E, F )
Afinal ? Al+1
{repeat for source words}
Figure 1: pseudocode for alignment correction
target word is sufficient.
4.1 Move generation
Many different types of alignment perturbations are
possible. Here we restrict ourselves to a very sim-
ple move generator that changes the linkage of ex-
actly one source word at a time, or exactly one target
word at a time. Many of our corrections are simi-
lar to those of (Setiawan et al, 2010), although our
motivation is perhaps closer to (Brown et al, 1993),
who used similar perturbations to approximate in-
tractable sums that arise when estimating the param-
eters of the generative models Models 3-5, and ap-
proach refined in (Och and Ney, 2003). We note that
our corrections are designed to improve even a high-
quality starting alignment; in contrast the model of
(Fossum et al, 2008) considers deletion of links
from an initial alignment (union of aligners) that is
likely to overproduce links.
From the point of view of the alignment ma-
trix, we consider changes to one row or one col-
umn (generically, one slice) of the alignment matrix.
At each step t, the move setMt(At) is formed by
choosing a slice of the current alignment matrix At,
and generating all possible alignments from a few
families of moves. Then the move generator picks
another slice and repeats. The m + l slices are cy-
cled in a fixed order: the first m slices correspond to
source words (ordered according to a heuristic top-
down traversal of the dependency parse tree if avail-
able), and the remaining l slices correspond to target
words, similarly parse-ordered. For each slice we
consider the following families of moves, illustrated
by rows.
? add link to row i - for one j such that ?ij = 0,
891
make ?ij = 1 (shown here for row i = 1.)
? ? ?
a ? ? ?
b ? ? ?
c ? ? ?
=?
? ? ?
a ? ? ?
b ? ? ?
c ? ? ?
? remove one or more links from row i - for some
j such that ?ij = 1, make ?ij = 0 (shown here
for i = 3.)
? ? ?
a ? ? ?
b ? ? ?
c ? ? ?
=?
? ? ?
a ? ? ?
b ? ? ?
c ? ? ?
? move a link in row i - for one j and one j? such
that ?ij = 1 and ?ij? = 0, make ?ij = 0 and
?ij? = 1 (shown here for i = 1.)
? ? ?
a ? ? ?
b ? ? ?
c ? ? ?
=?
? ? ?
a ? ? ?
b ? ? ?
c ? ? ?
? leave row i unchanged
Similar families of moves apply to column slices
(source words.) In practice, perturbations are re-
stricted by a window (typically ?5 from existing
links.) If the given source word is unlinked, we
consider adding a link to each target word in a win-
dow (?5 from nearby links.) The window size re-
strictions mean that some reference alignments are
not reachable from the starting point. However, this
is unlikely to limit performance - an oracle aligner
achieves 97.6%F -measure on the Arabic-English
training set.
4.2 Move selection
A log linear model for the selection of the candidate
alignment at t+1 from the set of alignmentsMt(At)
generated by the move generator at step t takes the
form:
p(At+1|E,F,Mt(At)) =
e
P
i ?i?i(At+1,E,F )
Z(E,F,Mt(At))
(3)
where the partition function is now given by
Z(E,F,M) =
?
A?M
e
P
i ?i?i(A,E,F ) (4)
and At+1 ? Mt(At) is required for correct normal-
ization. This equation is notationally very similar
to equation (1), except that the predictions of the
model are restricted to a small set of nearby align-
ments. For the move generator considered in this pa-
per, the summation in Eq.(4) is similarly restricted,
and hence training the model is tractable. The set
of candidate alignmentsMt(At) typically does not
contain the reference (gold) alignment; we model
the best alignment among a finite set of alternatives,
rather than the correct alignment from among all
possible alignments. This is a key difference be-
tween our model and (Liu et al, 2005).
Note that if we extended our definition of pertur-
bation to the limiting case that the alignment set in-
cluded all possible alignments then we would clearly
recover the standard log linear model of alignment.
4.3 Training
Since the model is designed to predict perturbation
to an alignment, it is trained from a collection of
errorful alignments and corresponding reference se-
quences of aligner moves that reach the reference
(gold) alignment. We construct a training set from
a collection of sentence pairs and reference align-
ments for training (A?n, En, Fn)Nn=1, as well as col-
lections of corresponding ?first pass? alignments An1
produced by another aligner. For each n, we form a
number of candidate alignment sets Mt(Ant ), one
for each source and target word. For training pur-
poses, the true alignment from the set is taken to be
the one identical withA?n in the slice targeted by the
move generator at the current step. (A small number
of move sets do not have an exact match and are dis-
carded.) Then we form an objective function from
the log likelihood of reference alignment, smoothed
with a gaussian prior
L =
?
n
Ln +
?
i
(?i/?)2 (5)
892
where the likelihood of each training sample is
Ln =
?
?
log p1(A0n|E,Fn;M(f?, A0n, E, Fn))
+
?
?
log p1(A0n|E,Fn;M(e?, A0n, E, Fn)) (6)
The likelihood has a term for each sentence pair
and for each decoder step. The model is trained
by gradient ascent using the l-BFGS method (Liu
and Nocedal, 1989), which has been successfully
used for training log linear models (Blunsom and
Cohn, 2006) in many natural language tasks, includ-
ing alignment.
5 Features
A wide variety of features were used in the model.
We group the features in three broad categories:
link-based, geometrical, and parse-based.
Link-based features are those which decompose
into a (linear) sum of alignment matrix elements ?ij .
An example link-based feature is one that fires if a
source language noun is linked to a target language
determiner. Note that this feature may fire more than
once in a given sentence pair: as with most fea-
tures in our model, it is an integer-valued feature
that counts the number of times a structure appears
in a sentence pair. These features do not capture any
correlation between different ?ij . Among the link-
based features are those based on Model 1 transla-
tion matrix parameters ?(ei|fj) and ?(fj |ei). We
bin the model 1 parameters, and form integer-valued
features for each bin that count the number of links
with ?0 < ?(ei|fj) < ?1.
Geometrical features are those which capture cor-
relation between different ?ij based on adjacency or
nearness. They capture the idea that nearby words
in one language link to nearby words in the other
language - the motivation of HMM-based models
of alignment. An example is a feature that counts
the number of times that the next word in the source
language is linked to the next word in the target lan-
guage:
?(A,E, F ) =
?
ij
?ij?i+1,j+1 (7)
Parse-based features are those which capture cor-
relation between different ?ij , but use parsing to de-
termine links which are correlated - for example, if a
determiner links to the same word as its head noun.
As an example, if ei is the headword of ei? , and fj is
the headword of fj? , then
?(A,E, F ) =
?
ij
?ij?i?j? (8)
counts the number of times that a dependency rela-
tion in one language is preserved by alignment in the
other language. This feature can also be decorated,
either lexically, or with part-of-speech tags (as many
features in all three categories are.)
5.1 Unsupervised Adaptation
We constructed a heuristic phrase dictionary for un-
supervised adapatation. After aligning a large unan-
notated parallel corpus with our aligner, we enumer-
ate fully lexicalized geometrical features that can be
extracted from the resulting alignments - these are
entries in a phrase dictionary. These features are
tied, and treated as a single real-valued feature that
fires during training and decoding phases if a set of
hypothesized links matches the geometrical feature
extracted from the unannotated data. The value of
this real-valued feature is the log of the number of
occurrences of the identical (lexicalized) geometri-
cal feature in the aligned unannotated corpus.
6 Results
We design our experiments to validate that a cor-
rection model using simple features, mostly non-
language-specific, can improve the alignment accu-
racy of a variety of existing aligners for a variety of
language pairs; we do not attempt to exactly match
features between comparison aligners - this is un-
likely to lead to a robust correction model.
6.1 Arabic-English alignment results
We trained the Arabic-English alignment system
on 5125 sentences from Arabic-English treebanks
(LDC2008E61, LDC2008E22) that had been an-
notated for word alignment. Reference parses
were used during the training. Results are mea-
sured on a 500 sentence test set, sampled from
a wide variety of parallel corpora, including vari-
ous genres. During alignment, only automatically-
generated parses (based on the parser of (Rat-
naparkhi, 1999)) were available. Alignments on
893
initial align correction model R (%) P (%) F (%) ?F
GIZA++ 76 76 76
corr(GIZA++) 86 94 90 14?
corr(ME-seq) 88 92 90 14?
HMM 73 73 73
corr(HMM) 87 92 89 16?
corr(ME-seq) 87 93 90 17?
ME-seq 82 84 83
corr(HMM) 88 92 90 7?
corr(GIZA++) 87 94 91 8?
corr(ME-seq) 89 94 91 8?
Table 1: Alignment accuracy for Arabic-English systems in percentage recall (R), precision(P), and F -measure. ?
denotes statistical significance (see text.)
lang method R (%) P(%) F (%) ?F
ZH?EN GIZA++ 55 67 61
ME-seq 66 72 69
corr(ME-seq) 74 76 75 6?
Table 2: Alignment accuracy for Chinese(ZH)-English(EN) systems. ? denotes statistical significance
lang aligner R(%) P(%) F (%) ?F
IT? EN ME-seq 74 87 80
corr(ME-seq) 84 92 88 8?
EN?IT ME-seq 75 86 80
corr(ME-seq) 84 92 88 8?
PT?EN ME-seq 77 83 80
corr(ME-seq) 87 91 89 9?
EN?PT ME-seq 79 87 83
corr(ME-seq) 88 90 89 6?
JA?EN ME-seq 72 78 75
corr(ME-seq) 77 83 80 5?
RU?EN ME-seq 81 85 83
corr(ME-seq) 82 92 87 4?
DE?EN ME-seq 77 82 79
corr(ME-seq) 78 87 82 3?
ES?EN ME-seq 93 86 90
corr(ME-seq) 92 88 90 0.6
FR?EN ME-seq 89 91 90
corr(ME-seq) 88 92 90 0.1
Table 3: Alignment accuracy for additional languages. ? denotes statistical significance; ? statistical significance not
available. IT=Italian, PT=Portuguese, JA=Japanese, RU=Russian, DE=German, ES=Spanish, FR=French
894
the training and test sets were decoded with three
other aligners, so that the robustness of the cor-
rection model to different input alignments could
be validated. The three aligners were GIZA++
(Och and Ney, 2003) (with the MOSES (Koehn
et al, 2007) postprocessing option -alignment
grow-diag-final-and) the posterior HMM
aligner of (Ge, 2004), a maximum entropy sequen-
tial model (ME-seq) (Ittycheriah and Roukos, 2005).
ME-seq is our primary point of comparison: it is
discriminatively trained (on the same training data,)
uses a rich set of features, and provides the best
alignments of the three. Three correction models
were trained: corr(GIZA++) is trained to correct
the alignments produced by GIZA++, corr(HMM)
is trained to correct the alignments produced by the
HMM aligner, and corr(ME-seq) is trained to correct
the alignments produced by the ME-seq model.
In Table (1) we show results for our system cor-
recting each of the aligners as measured in the usual
recall, precision, and F -measure.1 The resulting
improvements in F -measure of the alignments pro-
duced by our models over their corresponding base-
lines is statistically significant (p < 10?4, indicated
by a ?.) Statistical significance is tested by a Monte
Carlo bootstrap (Efron and Tibshirani, 1986) - sam-
pling with replacement the difference in F -measure
of the two system?s alignments of the same sentence
pair. Both recall and precision are improved, but the
improvement in precision is somewhat larger. We
also show cross-condition results in which a correc-
tion model trained to correct HMM alignments is ap-
plied to correct ME-seq alignments. These results
show that our correction model is robust to different
starting aligners.
6.2 Chinese-English alignment results
Table (2) presents results for Chinese-English word
alignments. The training set for the corr(ME-
seq) model consisted of approximately 8000 hand-
aligned sentences sampled from LDC2006E93 and
LDC2008E57. The model was trained to correct
the output of the ME-seq aligner, and tested on
the same condition. For this language pair, refer-
ence parses were not available in our training set, so
1We do not distinguish sure and possible links in our anno-
tations - under this circumstance, alignment error rate(Och and
Ney, 2003) is 1? F .
automatically-generated parses were used for both
training and test sets. Results are measured on a 512
sentence test set, sampled from a wide variety of par-
allel corpora of various genres. We compare perfor-
mance with GIZA++, and with the ME-seq aligner.
Again the resulting improvement over the ME-seq
aligner is statistically significant. However, here the
improvement in recall is somewhat larger than the
improvement in precision.
6.3 Additional language pairs
Table (3) presents alignment results for seven other
language pairs. Separate alignment corrector mod-
els were trained for both directions of Italian ?
English and Portuguese ? English. The training
and test data vary by language, and are sampled
uniformly from a diverse set of corpora of various
genres, including newswire, and technical manuals.
Manual alignments for training and test data were
annotated. We compare performance with the ME-
seq aligner trained on the same training data. As
with the Chinese results above, customization and
feature development for the language pairs was min-
imal. In general, machine parses were always avail-
able for the English half of the pair. Machine parses
were also available for French and Spanish. Ma-
chine part of speech tags were available for all lan-
guage (although character-based heuristic was sub-
stituted for Japanese.) Large amounts (up to 10 mil-
lion sentence pairs) of unaligned parallel text was
available for model 1 type features. Our model ob-
tained improved alignment F -measure in all lan-
guage pairs, although the improvements were small
for ES?EN and FR?EN, the language pairs for
which the baseline accuracy was the highest.
6.4 Analysis
Some of the improvement can be attributed to ?look-
ahead? during the decoding. For example, the
English word ?the?, which (during Arabic-English
alignment) should often be aligned to the same Ara-
bic words to which its headword is linked. The num-
ber of errors associated with ?the? dropped from 383
(186 false alarms, 197 misses) in the ME-seq model
to 137 (60 false alarms and 77 misses) in the current
model.
In table 5, we show contributions to performance
resulting from various classes of features. The
895
Zh-En Ar-En
method correct miss fa correct miss fa
hmm 147 256 300
GIZA++ 139 677 396 132 271 370
ME-seq 71 745 133 127 276 191
corr(ME-seq) 358 458 231 264 139 114
Table 4: Analysis of 2?1 alignments errors (misses and false alarms) for Zh-En and Ar-En aligners
largest contribution is noted by removing features
based on the Model 1 translation matrices. These
features contain a wealth of lexical information
learned from approximately 7 ? 106 parallel sen-
tences - information that cannot be learned from
a relatively small amount of word-aligned train-
ing data. Geometrical features contribute more
than parse-based features, but the contribution from
parse-based features is important, and these are
more difficult to incorporate into sequential mod-
els. We note that all of the comparison aligners had
equivalent lexical information.
We show a small improvement from the unsuper-
vised adaptation - learning phrases from the parallel
corpus that are not captured by the lexical features
based on model 1. The final row in the table shows
the result of running the correction model on its own
output. The improvement is not statistically signif-
icant, but it is important to note the performance is
stable - a further indication that the model is robust
to a wide variety of input alignments, and that our
decoding scheme is a reasonable approach to find-
ing the best alignment.
In table 4, we characterize the errors based on the
fertility of the source and target words. We focus
on the case that exactly one target word is linked to
exactly two source words. These are the links that
feature R(%) P(%) F (%) Nexact
base 89 94 91 136
base-M1 82 88 85 89
base-geometric 83 90 86 92
base-parse 87 93 90 116
base+un.adapt 89 94 92 141
+iter2 90 94 92 141
Table 5: Importance of feature classes - ablation experi-
ments
corpus-level p90
alignment TER BLEU TER BLEU
ME-seq 56.06 32.65 64.20 21.31
corr(Me-seq) 56.25 33.10 63.47 22.02
both 56.07 33.13 63.41 22.14
Table 6: Translation results, Zh to En. BLEU=BLEUr4n4
alignment TER BLEUr1n4
ME-seq 35.02 69.94
corr(Me-seq ) 33.10 71.20
Table 7: Translation results, En to It
are poorly suited for the HMM and ME-seq mod-
els used in this comparison because of the chosen
directionality: the source (Arabic, Chinese) words
are the states and the target (English) words are the
observation. The HMM is able to produce these
links only by the use of posterior probabilities, rather
than viterbi decoding. The ME-seq model only pro-
duces these links because of language-specific post-
processing. GIZA++ has an underlying sequential
model, but uses both directionalities. The correc-
tion model improved performance across all three of
these links structures. The single exception is that
the number of 2?1 false alarms increased (Zh-En
alignments) but in this case, the first pass ME-seq
alignment produced few false alarms because it sim-
ply proposed few links of this form. It is also notable
that 1?2 links are more numerous than 2?1 links,
in both language pairs. This is consequence of the
choice of directionality and tokenization style.
6.5 Translation Impact
We tested the impact of improved alignments on
the performance of a phrase-based translation sys-
tem (Ittycheriah and Roukos, 2007) for three lan-
896
guage pairs. Our alignment did not improve the
performance of a mature Arabic to English trans-
lation system, but two notable successes were ob-
tained: Chinese to English, and English to Italian.
It is well known that improved alignment perfor-
mance does not always improve translation perfor-
mance (Fraser and Marcu, 2007). A mature machine
translation system may incorporate alignments ob-
tained from multiple aligners, or from both direc-
tions of an asymmetric aligner. Furthermore, with
large amounts of training data (the Gale Phase 4
Arabic English corpus consisting of 8 ? 106 sen-
tences,) a machine translation system is subject to
a saturation effect: correcting an alignment may
not yield a significant improvement because the the
phrases learned from the correct alignment have al-
ready been acquired in other contexts.
For the Chinese to English translation system (ta-
ble 6) the training corpus consisted of 11? 106 sen-
tence pairs, subsampled to 106. The test set was
NIST MT08 Newswire, consisting of 691 sentences
and 4 reference translations. Corpus-level perfor-
mance (columns 2 and 3) improved when measured
by BLEU, but not by TER. Performance on the
most difficult sentences (near the 90th percentile,
columns 4 and 5) improved on both BLEU and TER
(Snover et al, 2006), and the improvement in BLEU
was larger for the more difficult sentences than it
was overall. Translation performance further im-
proved, by a smaller amount, using bothME-seq and
corr(ME-seq) alignments during the training.
The improved alignments impacted the transla-
tion performance of the English to Italian transla-
tion system (table 7) even more strongly. Here the
training corpus consisted of 9.4?106 sentence pairs,
subsampled to 387000 pairs. The test set consisted
of 7899 sentences. Overall performance improved
as measured by both TER and BLEU (1.26 points.)
7 Conclusions
A log linear model for the alignment matrix is used
to guide systematic improvements to an existing
aligner. Our system models arbitrary alignment ma-
trices and allows features that incorporate such in-
formation as correlations based on parse trees in
both languages. We train models to correct the er-
rors of several existing aligners; we find the resulting
models are robust to using different aligners as start-
ing points. Improvements in alignment F -measure,
often significant improvements, show that our model
successfully corrects input alignments from existing
models in all nine language pairs tested. The result-
ing Chinese-English and English-Italian word align-
ments also improved translation performance, espe-
cially on the English-Italian test, and notably on the
particularly difficult subset of the Chinese sentences.
Future work will assess its impact on translation for
the other language pairs, as well as its impact on
other tasks, such as named entity projection.
8 Acknowledgements
We would like to acknowledge the support of
DARPA under Grant HR0011-08-C-0110 for fund-
ing part of this work. The views, opinions, and/or
findings contained in this article/presentation are
those of the author/presenter and should not be in-
terpreted as representing the official views or poli-
cies, either expressed or implied, of the Defense Ad-
vanced Research Projects Agency or the Department
of Defense.
References
Necip Fazil Ayan, Bonnie J. Dorr, and Christof Monz.
2005. Alignment link projection using transformation-
based learning. In Proceedings of the conference on
Human Language Technology and Empirical Methods
in Natural Language Processing, HLT ?05, pages 185?
192, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Yassine Benajiba and Imed Zitouni. 2010. Enhanc-
ing mention detection using projection via aligned
corpora. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Processing,
EMNLP ?10, pages 993?1001. Association for Com-
putational Linguistics.
Phil Blunsom and Trevor Cohn. 2006. Discriminative
word alignment with conditional random fields. In In
Proc. of ACL-2006, pages 65?72.
Peter F. Brown, Stephen Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathematic
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19(2):263?311.
Yonggang Deng and Bowen Zhou. 2009. Optimizing
word alignment combination for phrase table training.
In Proceedings of the ACL-IJCNLP 2009 Conference
897
Short Papers, ACLShort ?09, pages 229?232, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
B. Efron and R. Tibshirani. 1986. Bootstrap meth-
ods for standard errors, confidence intervals, and other
measures of statistical accuracy. Statistical Science,
1(1):pp. 54?75.
Victoria Fossum, Kevin Knight, and Steven Abney. 2008.
Using syntax to improve word alignment precision for
syntax-based machine translation. In Proceedings of
the Third Workshop on Statistical Machine Transla-
tion, StatMT ?08, pages 44?52. Association for Com-
putational Linguistics.
Alexander Fraser and Daniel Marcu. 2007. Measuring
word alignment quality for statistical machine transla-
tion. Comput. Linguist., 33(3):293?303.
Niyu Ge. 2004. Improvement in word alignments. In
DARPA/TIDES MT workshop.
Abraham Ittycheriah and Salim Roukos. 2005. A maxi-
mum entropy word aligner for arabic-english machine
translation. In HLT-EMNLP, pages 89?96.
Abraham Ittycheriah and Salim Roukos. 2007. Direct
translation model 2. In Human Language Technolo-
gies 2007: The Conference of the NA-ACL, pages 57?
64, Rochester, New York, April. Association for Com-
putational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In ACL.
Young-Suk Lee. 2004. Morphological analysis for sta-
tistical machine translation. In Proceedings of HLT-
NAACL 2004: Short Papers on XX, HLT-NAACL ?04,
pages 57?60. Association for Computational Linguis-
tics.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of the main con-
ference on Human Language Technology Conference
of the North American Chapter of the Association of
Computational Linguistics, pages 104?111. Associa-
tion for Computational Linguistics.
Dong C. Liu and Jorge Nocedal. 1989. On the lim-
ited memory bfgs method for large scale optimization.
Mathematical Programming, 45:503?528.
Yang Liu, Qun Liu, and Shouxun Lin. 2005. Log-linear
models for word alignment. In ACL ?05: Proceedings
of the 43rd Annual Meeting on Association for Com-
putational Linguistics, pages 459?466. Association for
Computational Linguistics.
Robert C. Moore, Wen-tau Yih, and Andreas Bode. 2006.
Improved discriminative bilingual word alignment. In
ACL-44: Proceedings of the 21st International Con-
ference on Computational Linguistics and the 44th
annual meeting of the Association for Computational
Linguistics, pages 513?520. Association for Computa-
tional Linguistics.
Robert C. Moore. 2005. A discriminative framework for
bilingual word alignment. In In Proceedings of HLT-
EMNLP, pages 81?88.
Jan Niehues and Stephan Vogel. 2008. Discrimina-
tive word alignment via alignment matrix modeling.
In Proceedings of the Third Workshop on Statistical
Machine Translation, pages 18?25, Columbus, Ohio,
June. Association for Computational Linguistics.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Adwait Ratnaparkhi. 1999. Learning to parse natu-
ral language with maximum entropy models. Mach.
Learn., 34:151?175, February.
Hendra Setiawan, Chris Dyer, and Philip Resnik. 2010.
Discriminative word alignment with a function word
reordering model. In Proceedings of the 2010 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, EMNLP ?10, pages 534?544. Association for
Computational Linguistics.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Linea
Micciulla, and John Makhoul. 2006. A study of trans-
lation edit rate with targeted human annotation. In
Proceedings of Association for Machine Translation in
the Americas.
Ben Taskar, Simon Lacoste-julien, and Dan Klein. 2005.
A discriminative matching approach to word align-
ment. In In Proceedings of HLT-EMNLP, pages 73?
80.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical trans-
lation. In Proceedings of the 16th conference on Com-
putational linguistics, pages 836?841.
David Yarowsky, Grace Ngai, and Richard Wicentowski.
2001. Inducing multilingual text analysis tools via ro-
bust projection across aligned corpora. In Proceedings
of the first international conference on Human lan-
guage technology research, HLT ?01, pages 1?8. As-
sociation for Computational Linguistics.
898
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 861?870,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Adaptive HTER Estimation for Document-Specific MT Post-Editing
Fei Huang
?
Facebook Inc.
Menlo Park, CA
feihuang@fb.com
Jian-Ming Xu Abraham Ittycheriah
IBM T.J. Watson Research Center
Yorktown Heights, NY
{jianxu, abei, roukos}@us.ibm.com
Salim Roukos
Abstract
We present an adaptive translation qual-
ity estimation (QE) method to predict
the human-targeted translation error rate
(HTER) for a document-specific machine
translation model. We first introduce fea-
tures derived internal to the translation de-
coding process as well as externally from
the source sentence analysis. We show
the effectiveness of such features in both
classification and regression of MT qual-
ity. By dynamically training the QE model
for the document-specific MT model, we
are able to achieve consistency and pre-
diction quality across multiple documents,
demonstrated by the higher correlation co-
efficient and F-scores in finding Good sen-
tences. Additionally, the proposed method
is applied to IBM English-to-Japanese MT
post editing field study and we observe
strong correlation with human preference,
with a 10% increase in human translators?
productivity.
1 Introduction
Machine translation (MT) systems suffer from an
inconsistent and unstable translation quality. De-
pending on the difficulty of the input sentences
(sentence length, OOV words, complex sentence
structures and the coverage of the MT system?s
training data), some translation outputs can be per-
fect, while others are ungrammatical, missing im-
portant words or even totally garbled. As a result,
users do not know whether they can trust the trans-
lation output unless they spend time to analyze
?
This work was done when the author was with IBM Re-
search.
the MT output. This shortcoming is one of the
main obstacles for the adoption of MT systems,
especially in machine assisted human translation:
MT post-editing, where human translators have
an option to edit MT proposals or translate from
scratch. It has been observed that human trans-
lators often discard MT proposals even if some
are very accurate. If MT proposals are used prop-
erly, post-editing can increase translators produc-
tivity and lead to significant cost savings. There-
fore, it is beneficial to provide MT confidence es-
timation, to help the translators to decide whether
to accept MT proposals, making minor modifica-
tions on MT proposals when the quality is high
or translating from scratching when the quality is
low. This will save the time of reading and parsing
low quality MT and improve user experience.
In this paper we propose an adaptive qual-
ity estimation that predicts sentence-level human-
targeted translation error rate (HTER) (Snover et
al., 2006) for a document-specific MT post-editing
system. HTER is an ideal quality measurement
for MT post editing since the reference is ob-
tained from human correction of the MT output.
Document-specific MT model is an MT model that
is specifically built for the given input document.
It is demonstrated in (Roukos et al, 2012) that
document-specific MT models significantly im-
prove the translation quality. However, this raises
two issues for quality estimation. First, existing
approaches to MT quality estimation rely on lex-
ical and syntactical features defined over parallel
sentence pairs, which includes source sentences,
MT outputs and references, and translation models
(Blatz et al, 2004; Ueffing and Ney, 2007; Spe-
cia et al, 2009a; Xiong et al, 2010; Soricut and
Echihabi, 2010a; Bach et al, 2011). Therefore,
when the MT quality estimation model is trained,
861
it can not be adapted to provide accurate estimates
on the outputs of document-specific MT models.
Second, the MT quality estimation might be in-
consistent across different document-specific MT
models, thus the confidence score is unreliable and
not very helpful to users.
In contrast to traditional static MT quality es-
timation methods, our approach not only trains
the MT quality estimator dynamically for each
document-specific MT model to obtain higher pre-
diction accuracy, but also achieves consistency
over different document-specific MT models. The
experiments show that our MT quality estima-
tion is highly correlated with human judgment
and helps translators to increase the MT proposal
adoption rate in post-editing.
We will review related work on MT quality es-
timation in section 2. In section 3 we will intro-
duce the document-specific MT system built for
post-editing. We describe the static quality estima-
tion method in section 4, and propose the adaptive
quality estimation method in section 5. In section
6 we demonstrate the improvement of MT quality
estimation with our method, followed by discus-
sion and conclusion in section 7.
2 Related Work
There has been a long history of study in con-
fidence estimation of machine translation. The
work of (Blatz et al, 2004) is among the best
known study of sentence and word level features
for translation error prediction. Along this line of
research, improvements can be obtained by incor-
porating more features as shown in (Quirk, 2004;
Sanchis et al, 2007; Raybaud et al, 2009; Specia
et al, 2009b). Soricut and Echihabi (2010b) pro-
posed various regression models to predict the ex-
pected BLEU score of a given sentence translation
hypothesis. Ueffing and Hey (2007) introduced
word posterior probabilities (WPP) features and
applied them in the n-best list reranking. Target
part-of-speech and null dependency link are ex-
ploited in a MaxEnt classifier to improve the MT
quality estimation (Xiong et al, 2010).
Quality estimation focusing on MT post-editing
has been an active research topic, especially after
the WMT 2012 (Callison-Burch et al, 2012) and
WMT2013 (Bojar et al, 2013) workshops with
the ?Quality Estimation? shared task. Bic?ici et
al. (2013) proposes a number of features mea-
suring the similarity of the source sentence to the
source side of the MT training corpus, which,
combined with features from translation output,
achieved significantly superior performance in the
MT QE evaluation. Felice and Specia (2012) in-
vestigates the impact of a large set of linguisti-
cally inspired features on quality estimation accu-
racy, which are not able to outperform the shal-
lower features based on word statistics. Gonz?alez-
Rubio et al (2013) proposed a principled method
for performing regression for quality estimation
using dimensionality reduction techniques based
on partial least squares regression. Given the fea-
ture redundancy in MT QE, their approach is able
to improve prediction accuracy while significantly
reducing the size of the feature sets.
3 Document-specific MT System
In our MT post-editing setup, we are given docu-
ments in the domain of software manuals, techni-
cal outlook or customer support materials. Each
translation request comes as a document with sev-
eral thousand sentences, focusing on a specific
topic, such as the user manual of some software.
The input documents are automatically seg-
mented into sentences, which are also called seg-
ments. Thus in the rest of the paper we will use
sentences and segments interchangeably. Our par-
allel corpora includes tens of millions of sentence
pairs covering a wide range of topics. Building
a general MT system using all the parallel data
not only produces a huge translation model (unless
with very aggressive pruning), the performance on
the given input document is suboptimal due to the
unwanted dominance of out-of-domain data. Past
research suggests using weighted sentences or cor-
pora for domain adaptation (Lu et al, 2007; Mat-
soukas et al, 2009; Foster et al, 2010). Here
we adopt the same strategy, building a document-
specific translation model for each input docu-
ment.
The document-specific system is built based on
sub-sampling: from the parallel corpora we se-
lect sentence pairs that are the most similar to
the sentences from the input document, then build
the MT system with the sub-sampled sentence
pairs. The similarity is defined as the number of
n-grams that appear in both source sentences, di-
vided by the input sentence?s length, with higher
weights assigned to longer n-grams. From the
extracted sentence pairs, we utilize the standard
pipeline in SMT system building: word align-
862
Figure 1: Adaptive QE for document-specific MT system.
ment (HMM (Vogel et al, 1996) and MaxEnt (It-
tycheriah and Roukos, 2005) alignment models,
phrase pair extraction, MT model training (Itty-
cheriah and Roukos, 2007) and LM model train-
ing. The top region within the dashed line in Fig-
ure 1 shows the overall system built pipeline.
3.1 MT Decoder
The MT decoder (Ittycheriah and Roukos, 2007)
employed in our study extracts various features
(source words, morphemes and POS tags, target
words and POS tags, etc.) with their weights
trained in a maximum entropy framework. These
features are combined with other features used in
a typical phrase-based translation system. Alto-
gether the decoder incorporates 17 features with
weights estimated by PRO (Hopkins and May,
2011) in the decoding process, and achieves
state-of-the-art translation performance in vari-
ous Arabic-English translation evaluations (NIST
MT2008, GALE and BOLT projects).
4 Static MT Quality Estimation
MT quality estimation is typically formulated as
a prediction problem: estimating the confidence
score or translation error rate of the translated sen-
tences or documents based on a set of features. In
this work, we adopt HTER in (Snover et al, 2006)
as our prediction output. HTER measures the per-
centage of insertions, deletions, substitutions and
shifts needed to correct the MT outputs. In the
rest of the paper, we use TER and HTER inter-
changably.
In this section we will first introduce the set of
features, and then discuss MT QE problem from
classification and regression point of views.
4.1 Features for MT QE
The features for quality estimation should reflect
the complexity of the source sentence and the de-
coding process. Therefore we conduct syntactic
analysis on the source sentences, extract features
from the decoding process and select the follow-
ing 26 features:
? 17 decoding features, including phrase
translation probabilities (source-to-target and
target-to-source), word translation probabil-
ities (also in both directions), maxent prob-
abilities
1
, word count, phrase count, distor-
1
The maxent probability is the translation probability
863
tion probabilities, as well as a set of language
model scores.
? Sentence length, i.e., the number of words in
the source sentence.
? Source sentence syntactic features, including
the number of noun phrases, verb phrases,
adjective phrases, adverb phrases, as in-
spired by (Green et al, 2013).
? The length of verb phrases, because verbs are
typically the roots in dependency structure
and they have more varieties during transla-
tion.
? The maximum length of source phrases in
the final translation, since longer matching
source phrase indicates better coverage of the
input sentence with possibly better transla-
tions.
? The number of phrase pairs with high fuzzy
match (FM) score. The high FM phrases are
selected from sentence pairs which are clos-
est in terms of n-gram overlap to the input
sentence. These sentences are often found in
previous translations of the software manual,
and thus are very helpful for translating the
current sentence.
? The average translation probability of the
phrase translation pairs in the final transla-
tion, which provides the overall translation
quality on the phrase level.
The first 17 features come from the decod-
ing process, which are called ?decoding features?.
The remaining 9 features not related to the de-
coder are called ?external features?. To evaluate
the effectiveness of the proposed features, we train
various classifiers with different feature configura-
tions to predict whether a translation output is use-
ful (with lower TER) as described in the following
section.
4.2 MT QE as Classification
Predicting TER with various input features can
be treated as a regression problem. However for
the post-editing task, we argue that it could also
be cast as a classification problem: MT system
derived from a Maximum Entropy translation model (Itty-
cheriah and Roukos, 2005).
Configuration Training set Test set
Baseline (All negative) 80% 77%
17 decoding features only 89% 79%
9 external features only 85% 81%
total 26 features 92% 83%
Table 1: QE classification accuracy with different
feature configurations
users (including the translators) are often inter-
ested to know whether a given translation is rea-
sonably good or not. If useful, they can quickly
look through the translation and make minor mod-
ifications. On the other hand, they will just skip
reading and parsing the bad translation, and prefer
to translate by themselves from scratch. Therefore
we also develop algorithms that classify the trans-
lation at different levels, depending on whether the
TER is less than a given threshold. In our experi-
ments, we set TER=0.1 as the threshold.
We randomly select one input document with
2067 sentences for the experiment. We build
a document-specific MT system to translate this
document, then ask human translator to correct
the translation output. We compute TER for each
sentence using the human correction as the refer-
ence. The TER of the whole document is 0.31,
which means about 30% errors should be cor-
rected. In the classification task, our goal is to pre-
dict whether a sentence is a Good translation (with
TER ? 0.1), and label them for human correction.
We adopt a decision tree-based classifier, experi-
menting with different feature configurations. We
select the top 1867 sentences for training and the
bottom 200 sentences for test. In the test set, there
are 46 sentences with TER ? 0.1. Table 1 shows
the classification accuracy.
First we can see that as the overall TER is
around 0.3, predicting all the sentences being neg-
ative already has a strong baseline: 77%. How-
ever this is not helpful for the human translators,
because that means they have to translate every
sentence from scratch, and consequently there is
no productivity gain from MT post-editing. If we
only use the 17 decoding features, it improves the
classification accuracy by 9% on the training set,
but only 2% on the test set. This is probably due to
the overfitting when training the decision tree clas-
sifier. While using the 7 external features, the gain
on training set is less but the gain on the test set
864
is greater (4% improvement), because the trans-
lation output is generated based on the log-linear
combination of these decoding features, which are
biased towards the final translations. The exter-
nal features capture the syntactic structure of the
source sentence, as well as the coverage of the
training data with regard to the input sentence,
which are good indicators of the translation qual-
ity. Combining both the decoding features and the
external features, we observed the best accuracy
on both the training and test set. We will use the
combined 26 features in the following work.
4.3 MT QE as Regression
For the QE regression task, we predict the TER for
each sentence translation using the above 26 fea-
tures. We experiment with several classifiers: lin-
ear regression model, decision tree based regres-
sion model and SVM model. With the same train-
ing and test data set up, we predict the TER for
each sentence in the test set, and compute the cor-
relation coefficient (r) and root mean square error
(RMSE). Our experiments show that the decision
tree-based regression model obtains the highest
correlation coefficients (0.53) and lowest RMSE
(0.23) in both the training and test sets. We will
use this model for the adaptive MT QE in the fol-
lowing work.
5 Adaptive MT Quality Estimation
The above QE regression model is trained on a
portion of the sentences from the input document,
and evaluated on the remaining sentences from the
same document. One would like to know whether
the trained model can achieve consistent TER pre-
diction accuracy on other documents. When we
use the cross-document models for prediction, the
correlation is significantly worse (the details are
discussed in section 6.1). Therefore it is neces-
sary to build a QE regression model that?s robust
to different document-specific translation models.
To deal with this problem, we propose this adap-
tive MT QE method described below.
Our proposed method is as follows: we select a
fixed set of sentence pairs (S
q
, R
q
) to train the QE
model. The source side of the QE training data
S
q
is combined with the input document S
d
for
MT system training data subsampling. Once the
document-specific MT system is trained, we use it
to translate both the input document and the source
QE training data, obtaining the translation T
d
and
Figure 2: Correlation coefficient r between pre-
dicted TER (x-axis) and true TER (y-axis) for QE
models trained from the same document (top fig-
ure) or different document (bottom figure).
T
q
. We compute the TER of T
q
using R
q
as the
reference, and train a QE regression model with
the 26 features proposed in section 4.1. Then we
use this document-specific QE model to predict the
TER of the document translation T
d
. As the QE
model is adaptively re-trained for each document-
specific MT system, its prediction is more accurate
and consistent. Figure 1 shows the flow of our MT
system with the adaptive QE training integrated as
part of the built.
6 Experiments
In this section, we first discuss experiments that
compare adaptive QE method and static QE
method on a few documents, and then present
results we obtained after deploying the adaptive
QE method in an English-to-Japanese MT Post-
Editing project. As mentioned before, the main
motivation for us to develop MT QE classification
scheme is that translators often discard good MT
proposals and translate the segments from scratch.
We would like to provide translators with some
guidance on reasonably good MT proposals?the
sentences with low TERs?to help them increase
the leverage on MT proposals to achieve improved
productivity.
865
6.1 Evaluation on Test Set
Our experiment and evaluation is conducted over
three documents, each with about 2000 segments.
We first build document-specific MT model for
each document, then ask human translators to cor-
rect the MT outputs and obtain the reference trans-
lation. In a typical MT QE scenario, the QE model
is pre-trained and applied to various MT outputs,
even though the QE training data and MT out-
puts are generated from different translation mod-
els. To evaluate whether such model mismatch
matters, we compare the cross-model QE with the
same-model QE, where the QE training data and
the MT outputs are generated from the same MT
model.
We select one document LZA with 2067 sen-
tences. We use the first 1867 sentences to train the
static QE model and the remaining 200 sentences
are used as test set for TER prediction. We com-
pute the correlation coefficient (r) between each
predicted TER and true TER, as shown in Figure
2. We find that the TER predictions are reason-
ably correct when the training and test sentences
are from the same MT model (the top figure), with
correlation coefficients around 0.5. For the cross-
model QE, we train a static QE model with 1867
sentences from another document RTW, and use it
to predict the TER of the same 200 sentences from
document LZA (the bottom figure). We observe
significant degradation of correlation coefficient,
dropping from 0.5 to 0.1. This degradation and
unstable nature is the prime motivation to develop
a more robust MT quality estimation model.
We select 1700 sentences from multiple pre-
viously translated documents as the QE training
data, which are independent of the test documents.
We train the static QE model with this training set,
including the source sentences, references and MT
outputs (from multiple translation models). To
train the adaptive QE model for each test docu-
ment, we build a translation model whose subsam-
pling data includes source sentences from both the
test document and the QE training data. We trans-
late the QE source sentences with this newly built
MT model, and the translation output is used to
train the QE model specific to each test document.
We compare these two QE models on three doc-
uments, LZA, RTW and WC7, measuring r and
RMSE for each QE model. The result is shown
in Table 2. We find that the adaptive QE model
demonstrates higher r and lower RMSE than the
static QE model for all the test documents.
Besides the general correlation with human
judgment, we particularly focus on those reason-
ably good translations, i.e., the sentences with low
TERs which can help improve the translator?s pro-
ductivity most. Here we report the precision, re-
call and F-score of finding such ?Good? sentences
(with TER ? 0.1) on the three documents in Ta-
ble 3. Again, the adaptive QE model produces
higher recall, mostly higher precision, and signif-
icantly improved F-score. The overall F-score of
the adaptive QE model is 0.28
2
. Compared with
the static QE model?s 0.17 F-score, this is rela-
tively 64% improvement.
In the adaptive QE model, the source side QE
training data is included in the subsampling pro-
cess to build the document-specific MT model. It
would be interesting to know whether this process
will negatively affect the MT quality. We evaluate
the TER of MT outputs with and without the adap-
tive QE training on the same three documents. As
seen in Table 4, we do not notice translation qual-
ity degradation. Instead, we observe slightly im-
provement on two document, with TERs reduction
by 0.1-0.4 pt. As our MT model training data in-
clude proprietary data, the MT performance is sig-
nificantly better than publicly available MT soft-
ware.
6.2 Impact on Human Translators
We apply the proposed adaptive QE model to
large scale English-to-Japanese MT Post-Editing
project on 36 documents with 562K words. Each
English sentence can be categorized into 3 classes:
? Exact Match (EM): the source sentence is
completely covered in the bilingual training
corpora thus the corresponding target sen-
tence is returned as the translation;
? Fuzzy Match (FM): the source sentence is
similar to some sentence in the training data
(similarity measured by string editing dis-
tance), the corresponding fuzzy match target
sentence (FM proposal) as well as the MT
translation output (MT proposal) are returned
for human translators to select and correct;
? No Proposal (NP): there is no close match
source sentences in the training data (the FM
2
The adaptive QE model obtains much higher F-score
(80%) on the rest of the sentences (with TER > 0.1).
866
Document LZA RTW WC7
Num. of Sents 2067 2003 2405
r ? RMSE ? r ? RMSE ? r ? RMSE ?
Static QE 0.10 0.38 0.40 0.32 0.13 0.36
Adaptive QE 0.58 0.23 0.61 0.22 0.47 0.20
Table 2: QE regression with static and adaptive models
Document LZA RTW WC7
Num. of Sents 2067 2003 2405
P/R/F-score P/R/F-score P/R/F-score
Static QE 0.73/0.08/0.14 0.69/ 0.11/ 0.19 0.74/ 0.10/ 0.18
Adaptive QE 0.69/0.14/0.24 0.84/ 0.16/ 0.26 0.80/ 0.23/ 0.35
Table 3: Performance on predicting Good sentences with static and adaptive models
similarity score of 70% is used as the thresh-
old), therefore only the MT output is re-
turned.
EM sentences are excluded from the study be-
cause in general they do not require editing. We
focus on the FM and NP sentences
3
. In Table 5
we present the precision, recall and F-score of the
?Good? sentences in the FM and NP categories,
similar to those shown in Table 3. We consistently
observe higher performance on the FM sentences,
in terms of precision, recall and F-score. This is
expected because these sentences are well covered
in the training data. The overall F-score is in line
with the test set results shown in Table 3.
We are also interested to know whether the pro-
posed adaptive QE method is helpful to human
translators in the MT post-editing task. Based on
the TERs predicted by the adaptive QE model, we
assign each MT proposal with a confidence label:
High (0 ? TER ? 0.2), Medium (0.2 < TER ?
0.3), or Low (TER > 0.3). We present the MT pro-
posals with confidence labels to human translators,
then measure the percentage of sentences whose
MT proposals are used. From Table 6 and 7,
we can see that sentences with High and Medium
confidence labels are more frequently used by the
translators than those with Low labels, for both the
FM and NP categories. The MT usage for the FM
category is less than that for the NP category be-
cause translators can choose FM proposals instead
of the MT proposals for correction.
We also measure the translator?s productivity
gain for MT proposals with different confidence
3
The word count distribution of EM, FM and NP is 21%,
38% and 41%, respectively.
Document LZA RTW WC7
TER-Baseline 30.81 30.74 29.96
TER-with Adaptive QE 30.69 30.78 29.56
Table 4: MT Quality with and without Adaptive
QE measured by TER
labels. The productivity of a translator is defined
as the number of source words translated per unit
time. The post editing tool, IBM TranslationMan-
ager, records the time that a translator spends on
a segment and computes the number of characters
that a translator types on the segment so that we
can compute how many words the translator has
finished in a given time.
We choose the overall productivity of NP0 as
the base unit 1, where there is no proposal presents
and the translator has to translate the segments
from scratch. Measured with this unit, for exam-
ple, the overall productivity of FM0 being 1.14
implies a relative gain of 14% over that of NP0,
which demonstrates the effectiveness of FM pro-
posals.
Table 6 and 7 also show the productivity gain
on sentences with High, Medium and Low labels
from FM and NP categories. Again, the produc-
tivity gain is consistent with the confidence labels
from the adaptive QE model?s prediction. The
overall productivity gain with confidence-labeled
MT proposals is about 10% (comparing FM1 vs.
FM0 and NP1 vs. NP0). These results clearly
demonstrate the effectiveness of the adaptive QE
model in aiding the translators to make use of MT
proposals and improve productivity.
867
Category Class FM usage MT usage Productivity
High 33% 34% 1.35
FM1 Medium 47% 18% 1.21
Low 60% 8% 1.20
Overall 45% 21% 1.26
High 53% - 1.12
FM0 Medium 64% - 1.14
Low 67% - 1.16
Overall 59% - 1.14
Table 6: MT proposal usage and productivity gain in FM category.
In FM1, both Fuzzy Match and MT proposals present. In control class FM0, only Fuzzy Match proposals
present, and therefore, MT usage is not available for FM0. Strong correlation is observed between
predicted ?High? , ?Medium? and ?Low? sentences with MT usage and post editing productivity.
Category Class MT usage Productivity
High 50% 1.25
NP1 Medium 42% 1.08
Low 27% 1.00
Overall 38% 1.09
High - 1.08
NP0 Medium - 1.00
Low - 0.96
Overall - 1.00
Table 7: MT proposal usage and productivity gain in NP category.
In NP1, MT is the only proposal available, while in control NP0, there presents no proposal at all and
the translator has to translate from scratch. Strong correlation is observed between predicted ?High? ,
?Medium? and ?Low? sentences with MT usage and post editing productivity
868
Type Precision Recall F-score
FM 0.71 0.23 0.35
NP 0.67 0.18 0.29
Overall 0.69 0.21 0.32
Table 5: Performance on predicting Good sen-
tences (TER ? 0.1) by adaptive QE model
7 Discussion and Conclusion
In this paper we proposed a method to adaptively
train a quality estimation model for document-
specific MT post editing. With the 26 pro-
posed features derived from decoding process and
source sentence syntactic analysis, the proposed
QE model achieved better TER prediction, higher
correlation with human correction of MT output
and higher F-score in finding good translations.
The proposed adaptive QE model is deployed to
a large scale English-to-Japanese MT post edit-
ing project, showing strong correlation with hu-
man preference and leading to about 10% gain in
human translator productivity.
The training data for QE model can be selected
independent of the input document. With such
fixed QE training data, it is possible to measure the
consistency of the trained QE models, and to al-
low the sanity check of the document-specific MT
models. However, adding such data in the sub-
sampling process extracts more bilingual data for
building the MT models, which slightly increase
the model building time but increased the transla-
tion quality. Another option is to select the sen-
tence pairs from the MT system subsampled train-
ing data, which is more similar to the input docu-
ment thus the trained QE model could be a better
match to the input document. However, the QE
model training data is no longer constant. The
model consistency is no longer guaranteed, and
the QE training data must be removed from the
MT system training data to avoid data contamina-
tion.
References
Nguyen Bach, Fei Huang, and Yaser Al-Onaizan.
2011. Goodness: A method for measuring machine
translation confidence. In ACL, pages 211?219.
Ergun Bic?ici, Declan Groves, and Josef van Genabith.
2013. Predicting sentence translation quality using
extrinsic and language independent features. Ma-
chine Translation.
John Blatz, Erin Fitzgerald, George Foster, Simona
Gandrabur, Cyril Goutte, Alex Kulesza, Alberto
Sanchis, and Nicola Ueffing. 2004. Confidence es-
timation for machine translation. In Proceedings of
the 20th international conference on Computational
Linguistics, COLING ?04, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Ondrej Bojar, Christian Buck, Chris Callison-Burch,
Christian Federmann, Barry Haddow, Philipp
Koehn, Christof Monz, Matt Post, Radu Soricut,
and Lucia Specia. 2013. Findings of the 2013
Workshop on Statistical Machine Translation. In
Eighth Workshop on Statistical Machine Transla-
tion, WMT-2013, pages 1?44, Sofia, Bulgaria.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 workshop on statistical ma-
chine translation. In Seventh Workshop on Statis-
tical Machine Translation, pages 10?51, Montr?eal,
Canada.
Mariano Felice and Lucia Specia. 2012. Linguistic
features for quality estimation. In Seventh Workshop
on Statistical Machine Translation, pages 96?103,
Montr?eal, Canada.
George Foster, Cyril Goutte, and Roland Kuhn. 2010.
Discriminative instance weighting for domain adap-
tation in statistical machine translation. In Proceed-
ings of the 2010 Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP ?10,
pages 451?459, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Jes?us Gonz?alez-Rubio, Jose Ram?on Navarro-Cerd?an,
and Francisco Casacuberta. 2013. Dimensionality
reduction methods for machine translation quality
estimation. Machine Translation, 27(3-4):281?301.
Spence Green, Jeffrey Heer, and Christopher D. Man-
ning. 2013. The efficacy of human post-editing for
language translation. In Proceedings of the SIGCHI
Conference on Human Factors in Computing Sys-
tems, CHI ?13, pages 439?448, New York, NY,
USA. ACM.
Mark Hopkins and Jonathan May. 2011. Tuning as
ranking. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1352?1362, Edinburgh, Scotland, UK.,
July. Association for Computational Linguistics.
Abraham Ittycheriah and Salim Roukos. 2005. A
maximum entropy word aligner for arabic-english
machine translation. In In Proceedings of HLT-
EMNLP, pages 89?96.
Abraham Ittycheriah and Salim Roukos. 2007. Direct
translation model 2. In In HLT-NAACL 2007: Main
Conference, pages 57?64.
Yajuan Lu, Jin Huang, and Qun Liu. 2007. Improv-
ing statistical machine translation performance by
869
training data selection and optimization. In Pro-
ceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 343?350, Prague, Czech Republic,
June. Association for Computational Linguistics.
Spyros Matsoukas, Antti-Veikko I. Rosti, and Bing
Zhang. 2009. Discriminative corpus weight es-
timation for machine translation. In Proceedings
of the 2009 Conference on Empirical Methods in
Natural Language Processing: Volume 2 - Volume
2, EMNLP ?09, pages 708?717, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Christopher B. Quirk. 2004. Training a sentence-level
machine translation confidence measure. In In Pro-
ceedings of LREC.
Sylvain Raybaud, Caroline Lavecchia, David Langlois,
and Kamel Sma??li. 2009. New confidence mea-
sures for statistical machine translation. CoRR,
abs/0902.1033.
Salim Roukos, Abraham Ittycheriah, and Jian-Ming
Xu. 2012. Document-specific statistical machine
translation for improving human translation produc-
tivity. In Proceedings of the 13th international con-
ference on Computational Linguistics and Intelli-
gent Text Processing - Volume Part II, CICLing?12,
pages 25?39, Berlin, Heidelberg. Springer-Verlag.
Alberto Sanchis, Alfons Juan, Enrique Vidal, and De-
partament De Sistemes Informtics. 2007. Estima-
tion of confidence measures for machine translation.
In In Procedings of Machine Translation Summit XI.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study
of translation edit rate with targeted human annota-
tion. In In Proceedings of Association for Machine
Translation in the Americas, pages 223?231.
Radu Soricut and Abdessamad Echihabi. 2010a.
Trustrank: inducing trust in automatic translations
via ranking. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics, ACL ?10, pages 612?621, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Radu Soricut and Abdessamad Echihabi. 2010b.
Trustrank: Inducing trust in automatic translations
via ranking. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics, pages 612?621. Association for Computa-
tional Linguistics.
Lucia Specia, Craig Saunders, Marco Turchi, Zhuoran
Wang, and John Shawe-taylor. 2009a. Improving
the confidence of machine translation quality esti-
mates. In In Proceedings of MT Summit XII.
Lucia Specia, Marco Turchi, Zhuoran Wang, John
Shawe-Taylor, and Craig Saunders. 2009b. Improv-
ing the confidence of machine translation quality es-
timates.
Nicola Ueffing and Hermann Ney. 2007. Word-
level confidence estimation for machine translation.
Computational Linguistics, 33(1):9?40.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. Hmm-based word alignment in statistical
translation. In Proceedings of the 16th Conference
on Computational Linguistics - Volume 2, COLING
?96, pages 836?841, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Deyi Xiong, Min Zhang, and Haizhou Li. 2010. Er-
ror detection for statistical machine translation using
linguistic features. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics, ACL ?10, pages 604?611, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
870
