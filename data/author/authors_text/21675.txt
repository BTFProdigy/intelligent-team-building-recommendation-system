Proceedings of the 2010 Workshop on Graph-based Methods for Natural Language Processing, ACL 2010, pages 60?68,
Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational Linguistics
Contextually?Mediated Semantic Similarity Graphs for 
Topic Segmentation 
 
 
Geetu Ambwani 
StreamSage/Comcast 
Washington, DC, USA 
ambwani@streamsage.com 
Anthony R. Davis 
StreamSage/Comcast 
Washington, DC, USA 
davis@streamsage.com 
 
  
 
Abstract 
 
We present a representation of documents 
as directed, weighted graphs, modeling the 
range of influence of terms within the 
document as well as contextually deter-
mined semantic relatedness among terms. 
We then show the usefulness of this kind 
of representation in topic segmentation. 
Our boundary detection algorithm uses 
this graph to determine topical coherence 
and potential topic shifts, and does not re-
quire labeled data or training of parame-
ters. We show that this method yields im-
proved results on both concatenated pseu-
do-documents and on closed-captions for 
television programs. 
1 Introduction 
We present in this paper a graph-based represen-
tation of documents that models both the long-
range scope  "influence" of terms and the seman-
tic relatedness of terms in a local context. In 
these graphs, each term is represented by a series 
of nodes. Each node in the series corresponds to 
a sentence within the span of that term?s influ-
ence, and the weights of the edges are propor-
tional to the semantic relatedness among terms in 
the context. Semantic relatedness between terms 
is reinforced by the presence of nearby, closely 
related terms, reflected in increased connection 
strength between their nodes. 
We demonstrate the usefulness of our repre-
sentation by applying it to partitioning of docu-
ments into topically coherent segments. Our 
segmentation method finds locations in the graph 
of a document where one group of strongly con-
nected nodes ends and another begins, signaling 
a shift in topicality. We test this method both on 
concatenated news articles, and on a more realis-
tic segmentation task, closed-captions from 
commercial television programs, in which topic 
transitions are more subjective and less distinct. 
Our methods are unsupervised and require no 
training; thus they do not require any labeled in-
stances of segment boundaries. Our method at-
tains results significantly superior to that of Choi 
(2000), and approaches human performance on 
segmentation of television closed-captions, 
where inter-annotator disagreement is high. 
2 Graphs of lexical influence 
2.1 Summary of the approach 
Successful topic segmentation requires some re-
presentation of semantic and discourse cohesion, 
and the ability to detect where such cohesion is 
weakest. The underlying assumption of segmen-
tation algorithms based on lexical chains or other 
term similarity measures between portions of a 
document is that continuity in vocabulary reflects 
topic continuity. Two short examples illustrating 
topic shifts in television news programs, with 
accompanying shift in vocabulary, appear in 
Figure 1. 
We model this continuity by first modeling 
what the extent of a term's influence is. This dif-
fers from a lexical chain approach in that we do 
not model text cohesion through recurrence of 
terms. Rather, we determine, for each occurrence 
of a term in the document (excluding terms gen-
erally treated as stopwords), what interval of sen-
tences surrounding that occurrence is the best 
estimate of the extent of its relevance. This idea 
stems from work in Davis, et al (2004), who 
describe the use of relevance intervals in multi-
media information retrieval. We summarize their 
procedure for constructing relevance intervals in 
60
section 2.2. Next, we calculate the relatedness of 
these terms to one another. We use pointwise 
mutual information (PMI) as a similarity meas-
ure between terms, but other measures, such as 
WordNet-based similarity or Wikipedia Miner 
similarity (Milne and Witten, 2009), could aug-
ment or replace it. 
 
 
S_44 Gatorade has discontinued a drink with his 
image but that was planned before the company has 
said and they have issued a statement in support of 
tiger woods. 
S_45 And at t says that while it supports tiger 
woods personally, it is evaluating their ongoing busi-
ness relationship. 
S_46 I'm sure, alex, in the near future we're going 
to see more of this as companies weigh the short term 
difficulties of staying with tiger woods versus the 
long term gains of supporting him fully. 
S_47 Okay. 
S_48 Mark potter, miami. 
S_49 Thanks for the wrapup of that. 
S_50 We'll go now to deep freeze that's blanketing 
the great lakes all the way to right here on the east 
coast. 
 
 
S_190 We've got to get this addressed and hold 
down health care costs. 
S_191 Senator ron wyden the optimist from oregon, 
we appreciate your time tonight. 
S_192 Thank you. 
S_193 Coming up, the final day of free health clinic 
in kansas city, missouri. 
 
 
The next step is to construct a graphical repre-
sentation of the influence of terms throughout a 
document. When constructing topically coherent 
segments, we wish to assess coherence from one 
sentence to the next. We model similarity be-
tween successive sentences as a graph, in which 
each node represents both a term and a sentence 
that lies within its influence (that is, a sentence 
belonging to a relevance interval for that term). 
For example, if the term ?drink? occurs in sen-
tence 44, and its relevance interval extends to 
sentence 47, four nodes will be created for 
?drink?, each corresponding to one sentence in 
that interval. The edges in the graph connect 
nodes in successive sentences. The weight of an 
edge between two terms t and t' consists not only 
of their relatedness, but is reinforced by the pres-
ence of other nodes in each sentence associated 
with terms related to t and t'. 
The resulting graph thus consists of cohorts of 
nodes, one cohort associated with each sentence, 
and edges connecting nodes in one cohort to 
those in the next. Edges with a low weight are 
pruned from the graph. The algorithm for deter-
mining topic segment boundaries then seeks lo-
cations in which a relatively large number of re-
levance intervals for terms with relatively high 
relatedness end or begin. 
In sum, we introduce two innovations here in 
computing topical coherence.  One is that we use 
the extent of each term's relevance intervals to 
model the influence of that term, which thus ex-
tends beyond the sentences it occurs in.  Second, 
we amplify the semantic relatedness of a term t 
to a term t' when there are other nearby terms 
related to t and t'. Related terms thereby rein-
force one another in establishing coherence from 
one sentence to the next. 
2.2 Constructing relevance intervals 
As noted, the scope of a term's influence is cap-
tured through relevance intervals (RIs). We de-
scribe here how RIs are created. A corpus?in 
this case, seven years of New York Times text 
totaling approximately 325 million words?is 
run through a part-of-speech tagger.  The point-
wise mutual information between each pair of 
terms is computed using a 50-word window.1 
PMI values provide a mechanism to measure 
relatedness between a term and terms occurring 
in nearby sentences of a document. When 
processing a document for segmentation, we first 
calculate RIs for all the terms in that document. 
An RI for a term t is built sentence-by-sentence, 
beginning with a sentence where t occurs. A sen-
tence immediately succeeding or preceding the 
sentences already in the RI is added to that RI if 
it contains terms with sufficiently high PMI val-
ues with t. An adjacent sentence is also added to 
an RI if there is a pronominal believed to refer to 
t; the algorithm for determining pronominal ref-
erence is closely based on Kennedy and Bogu-
raev (1996). Expansion of an RI is terminated if 
there are no motivations for expanding it further. 
Additional termination conditions can be in-
cluded as well. For example, if large local voca-
                                                 
1 PMI values are constructed for all words other than those 
in a list of stopwords.  They are also constructed for a li-
mited set of about 100,000 frequent multi-word expressions. 
In our segmentation system, we use only the RIs for nouns 
and for multiword expressions. 
Figure 1. Two short closed-caption excerpts from 
television news programs, each containing a top-
ic shift  
61
bulary shifts or discourse cues signaling the start 
of end of a section are detected, RIs can be 
forced to end at those points. In one version of 
our system, we set these ?hard? boundaries using 
an algorithm based on Choi (2000). In this paper 
we report segmentation results with and without 
this limited use of Choi?s algorithm. Lastly, if 
two RIs for t are sufficiently close (i.e., the end 
of one lies within 150 words of the start of 
another), then the two RIs are merged. 
The aim of constructing RIs is to determine 
which portions of a document are relevant to a 
particular term. While this is related to the goal 
of finding topically coherent segments, it is of 
course distinct, as a topic typically is determined 
by the influence of multiple terms. However, RIs 
do provide a rough indication of how far a term's 
influence extends or, put another way, of "smear-
ing out" the occurrence of a term over an ex-
tended region. 
2.3 From relevance intervals to graphs 
Consider a sentence Si, and its immediate succes-
sor Si+1. Each of these sentences is contained in 
various relevance intervals; let Wi denote the set 
of terms with RIs containing Si, and Wi+1 denote 
the set containing Si+1. 
For each pair of terms a in Wi and b in Wi+1, 
we compute a connection strength c(a,b), a non-
negative real number that reflects how the two 
terms are related in the context of Si and Si+1. To 
include the context, we take into account that 
some terms in Si may be closely related, and 
should support one another in their connections 
to terms in Si+1, and vice versa, as suggested 
above. Here, we use PMI values between terms 
as the basis for connection strength, normalized 
to a similarity score that ranges between 0 and 1, 
as follows: 
 
 
The similarity between two terms is set to 0 if 
this quantity is negative. Also, we assign the 
maximum value of 1 for self-similarity. We then 
define connection strength in the following way: 
 
 
 
That is, the similarity of another term in Wi or 
Wi+1 to b or a respectively, will add to the con-
nection strength between a and b, weighted by 
the similarity of that term to a or b respectively. 
Note that this formula also includes in the sum-
mation the similarity s(a,b) between a and b 
themselves, when either x or y is set to either a or 
b.2 Figure 2 illustrates this procedure. We nor-
malize the connection strength by the total num-
ber of pairs in equation (2). 
We note in passing that many possible modifi-
cations of this formula are easily imagined. One 
obvious alternative to using the product of two 
similarity scores is to use the minimum of the 
two scores. This gives more weight to pair values 
that are both moderately high, with respect to 
pairs where one is high and the other low. Apart 
from this, we could incorporate terms from RIs 
in sentences beyond these two adjoining sen-
tences, we could weight individual terms in Wi or 
Wi+1 according to some independent measure of 
topical salience, and so on. 
 
 
Figure 2. Calculation of connection strength be-
tween two nodes 
 
What emerges from this procedure is a 
weighted graph of connections across slices of a 
document (sentences, in our experiments). Each 
node in the graph is labeled with a term and a 
sentence number, and represents a relevance in-
terval for that term that includes the indicated 
sentence.  The edges of the graph connect nodes 
associated with adjacent sentences, and are 
weighted by the connection strength. Because 
many weak connections are present in this graph, 
we remove edges that are unlikely to contribute 
to establishing topical coherence. There are vari-
ous options for pruning: removing edges with 
connection strengths below a threshold, retaining 
only the top n edges, cutting the graph between 
two sentences where the total connection 
strength of edges connecting the sentences is 
small, and using an edge betweenness algorithm 
(e.g., Girvan and Newman, 2002) to remove 
edges that have high betweenness (and hence are 
indicative of a "thin" connection).  
                                                 
2 In fact, the similarity s(ai,bj) will be counted twice, once 
in each summation in the formula above; we retain this 
additional weighting of s(ai,bj). 
62
  
Figure 3. A portion of the graph generated from the first excerpt in Figure 1. Each node is labeled 
S_i__term_pos, where i indicates the sentence index  
 
We have primarily investigated the first me-
thod, removing edges with a connection strength 
less than 0.5. Two samples of the graphs we pro-
duce, corresponding to the excerpts in figure 1, 
appear in figures 3 and 4. 
2.4 Finding segment boundaries in graphs 
Segment boundaries in these graphs are hypothe-
sized where there are relatively few, relatively 
weak connections from a cohort of nodes asso-
ciated with one sentence to the cohort of nodes 
associated with the following sentence. If a term 
has a node in one cohort and in the succeeding 
cohort (that is, its RI continues across the two 
corresponding sentences) it counts against a 
segment boundary at that location, whereas terms 
with nodes on only one side of the boundary 
count in favor of a segment. For example, in fig-
ure 3, a new set of RIs start in sentence 48, 
where we see nodes for ?Buffalo?, ?Michigan?, 
?Worth?, Marquette?, and ?Miami?, and RIs in 
preceding sentences for ?Tiger Woods?, ?Gato-
rade?, etc. end.  Note that the corresponding ex-
cerpt in figure 1 shows a clear topic shift be-
tween a story on Tiger Woods ending at sentence 
46, and a story about Great Lakes weather be-
ginning at sentence 48. 
Similarly, in figure 4, RIs for ?Missouri?, 
?city? and ?health clinic? include sentences 190. 
191, and 192; thus these are evidence against a 
segment boundary at this location. On the other 
hand, several other terms, such as ?Oregon?, 
?Ron?, ?Senator?, and ?bill?, have RIs that end 
at sentence 191, which argues in favor of a 
boundary there.  We present further details of our 
boundary heuristics in section 4.1. 
3 Related Work 
The literature on topic segmentation has mostly 
focused on detecting a set of segments, typically 
non-hierarchical and non-overlapping, exhaus-
tively composing a document. Evaluation is then 
relatively simple, employing pseudo-documents 
constructed by concatenating a set of documents. 
This is a suitable technique for detecting coarse-
grained topic shifts. As Ferret (2007) points out, 
approaches to the problem vary both in the kinds 
of knowledge they depend on, and on the kinds 
of features they employ. 
Research on topic segmentation has exploited 
information internal to the corpus of documents 
to be segmented and information derived from 
external resources. If a corpus of documents per-
tinent to a domain is available, statistical topic 
models such as those developed by Beeferman et 
al. (1999) or Blei and Moreno (2001) can be tai-
lored to documents of that type. Lexical cohesion 
techniques include similarity measures between 
adjacent blocks of text, as in TextTiling (Hearst, 
1994, 1997) and lexical chains based on recur-
rences of a term or related terms, as in Morris 
and Hirst (1991), Kozima (1993), and Galley, et 
al. (2003). In Kan, et al (1998) recurrences of 
the same term within a certain number of sen-
tences are used for chains (the number varies 
with the type of term), and chains are based on 
entity reference as well as lexical identity. Our 
method is related to lexical chain techniques, in 
that the graphs we construct contain chains of 
nodes that extend the influence of a term beyond 
the site where it occurs.  But we differ in that we 
do not require a term (or a semantically related 
term) to recur, in order to build such chains. 
63
  
 
Figure 4. A portion of the graph generated from the second excerpt in Figure 1. Each node is labeled 
S_i__term_pos, where i indicates the sentence index  
 
In this respect, our approach also resembles 
that of Matveeva and Levow (2007), who build 
semantic similarity among terms into their lexi-
cal cohesion model through latent semantic anal-
ysis. Our techniques differ in that we incorporate 
semantic relatedness between terms directly into 
a graph, rather than computing similarities be-
tween blocks of text. 
In our experiments, we compare our method to 
C99 (Choi, 2000), an algorithm widely treated as 
a baseline.  Choi?s algorithm is based on a meas-
ure of local coherence; vocabulary similarity be-
tween each pair of sentences in a document is 
computed and the similarity scores of nearby 
sentences are ranked, with boundaries hypothe-
sized where similarity across sentences is low. 
4 Experiments, results, and evaluation  
4.1 Systems compared 
As noted above, we tested our system against the 
C99 segmentation algorithm (Choi, 2000). The 
implementation of C99 we use comes from the 
MorphAdorner website (Burns, 2006). We also 
compared our system to two simpler baseline 
systems without RIs. One uses graphs that do not 
represent a term?s zone of influence, but contain 
just a single node for each occurrence of a term. 
The second represents a term?s zone of influence 
in an extremely simple fashion, as a fixed num-
ber of sentences starting from each occurrence of 
that term.  We tried several values ranging from 
5 to 20 sentences for this extension. In addition, 
we varied two parameters to find the best-
performing combination of settings: the thre-
shold for pruning low-weight edges, and the 
threshold for positing a segment boundary.  In 
both the single-node and fixed-extension sys-
tems, the connection strength between nodes is 
calculated in the same way as for our full system. 
These comparisons aim to demonstrate two 
things. First, segmentation is greatly improved 
when we extend the influence of terms beyond 
the sentences they occur in. Second, the RIs 
prove more effective than fixed-length exten-
sions in modeling that influence accurately. 
Lastly, to establish how much we can gain 
from using Choi?s algorithm to determine termi-
nation points for RIs, we also compared two ver-
sions of our system: one in which RIs are calcu-
lated without information from Choi?s algorithm 
and a second with these boundaries included. 
Table 1 lists the systems we compare in the 
experiments described below. 
 
 
C99 Implementation of Choi (2000) 
SS+C 
Our full Segmentation System, incor-
porating ?hard? boundaries determined 
by modified Choi algorithm 
SS 
Our system, using RIs without ?hard? 
boundaries determined by modified 
Choi algorithm 
FE 
Our system, using fixed extension of a 
term from its occurrence 
SN 
Our system, using a single node for 
each term occurrence (no extension) 
 
Table 1. Systems compared in our experiments 
 
4.2 Data and parameter settings 
We tested our method on two sets of data.  One 
set consists of concatenated news stories, follow-
ing the approach of Choi (2000) and others since; 
the other consists of closed captions for twelve 
U.S. commercial television programs. Because 
the notion of a topic is inherently subjective, we 
follow many researchers who have reported re-
sults on "pseudo-documents"?documents formed 
by concatenating several randomly selected doc-
uments?so that the boundaries of segments are 
known, sharp, and not dependent on annotator 
variability (Choi, 2000). However, we also are 
64
interested in our system?s performance on more 
realistic segmentation tasks, as noted in the in-
troduction. 
In testing our algorithm, we first generated 
graphs from the documents in each dataset, as 
described in section 2. We pruned edges in the 
graphs with connection strength of less than 0.5. 
To find segment boundaries, we seek locations 
where the number of common terms associated 
with successive sentences is at a minimum.  This 
quantity needs to be normalized by some meas-
ure of how many nodes are present on either side 
of a potential boundary. We tested three normali-
zation factors: the total number of nodes on both 
sides of the potential segment boundary, the 
maximum of the numbers of nodes on each side 
of the boundary, and the minimum of the num-
bers of nodes on each side of the boundary. The 
results for all three of these were very similar, so 
we report only those for the maximum. This 
measure provides a ranking of all possible boun-
daries in a document (that is, between each pair 
of consecutive sentences), with a value of 0 be-
ing most indicative of a boundary. After experi-
menting with a few threshold values, we selected 
a threshold of 0.6, and posit a boundary at each 
point where the measure falls below this thre-
shold. 
4.3 Evaluation metrics 
We compute precision, recall, and F-measure 
based on exact boundary matches between the 
system and the reference segmentation.  As nu-
merous researchers have pointed out, this alone 
is not a perspicacious way to evaluate a segmen-
tation algorithm, as a system that misses a gold-
standard boundary by one sentence would be 
treated just like one that misses it by ten.  We 
therefore computed two additional, widely used 
measures, Pk (Beeferman, et al, 1997) and Win-
dowDiff (Pevzner and Hearst, 2002).  Pk assesses 
a penalty against a system for each position of a 
sliding window across a document in which the 
system and the gold standard differ on the pres-
ence or absence of (at least one) segment boun-
dary. WindowDiff is similar, but where the sys-
tem differs from the gold standard, the penalty is 
equal to the difference in the number of bounda-
ries between the two. This penalizes missed 
boundaries and ?near-misses? less than Pk (but 
see Lamprier, et al, (2007) for further analysis 
and some criticism of WindowDiff). For both Pk 
and WindowDiff, we used a window size of half 
the average reference segment length, as sug-
gested by Beeferman, et al (1997). Pk and Win-
dowDiff values range between 0 and 1, with 
lower values indicating better performance in 
detecting segment boundaries. Note that both Pk 
and WindowDiff are asymmetrical measures; 
different values will result if the system?s and the 
gold-standard?s boundaries are switched. 
4.4 Concatenated New York Times articles 
The concatenated pseudo-documents consist of 
New York Times articles selected at random from 
the New York Times Annotated Corpus.3  Each 
pseudo-document contains twenty articles, with 
an average of 623.6 sentences.  Our test set con-
sists of 185 of these pseudo-documents.4 
 
N = 185 
 Prec. Rec. F Pk WD 
C99 
? 0.404 0.569 0.467 0.338 0.360 
s.d 0.106 0.121 0.105 0.109 0.135 
SS 
? 0.566 0.383 0.448 0.292 0.317 
s.d. 0.176 0.135 0.140 0.070 0.084 
SS 
+C 
? 0.578 0.535 0.537 0.262 0.283 
s.d. 0.148 0.197 0.150 0.081 0.098 
FE 
? 0.265 0.140 0.176 0.478 0.536 
s.d. 0.123 0.042 0.055 0.055 0.076 
SN 
? 0.096 0.112 0.099 0.570 0.702 
s.d. 0.040 0.024 0.027 0.072 0.164 
 
Table 2. Performance of C99 and SS on segmen-
tation of concatenated New York Times articles, 
without specifying a number of boundaries 
 
Tables 2 and 3 provide summary results on the 
concatenated news articles. We ran the five sys-
tems listed in table 1 on the full dataset without 
any additional restrictions on the number of ar-
ticle boundaries to be detected. Means and stan-
dard deviations for each method on the five me-
trics are displayed in table 2. C99 typically finds 
many more boundaries than the 20 that are 
present (30.65 on average). Our SS system finds 
fewer than the true number of boundaries (14.52 
on average), while the combined system SS+C 
finds almost precisely the correct number (19.98 
on average). We used one-tailed paired t-tests of 
equal means to determine statistical significance 
at the 0.01 level. Although only SS+C?s perfor-
mance is significantly better in terms of F-
                                                 
3
www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?catalogId=L
DC2008T19 
4 Only article text is used, though occasionally some ob-
vious heading material, such as book title, author and pub-
lisher at the beginning of a book review, is present also. 
65
measure, both versions of our system outperform 
C99 according to Pk and WindowDiff.  
Using the baseline single node system (SN) 
yields very poor performance. These results (ta-
ble 2, row SN) are obtained with the edge-
pruning threshold set to a connection strength of 
0.9, and the boundary threshold set to 0.2, at 
which the average number of boundaries found is 
26.86. Modeling the influence of terms beyond 
the sentences they occur in is obviously valuable. 
The baseline fixed-length extensions system 
(FE) does better than SN but significantly worse 
than RIs. We found that, among the parameter 
settings yielding between 10 and 30 boundaries 
per document on average, the best results occur 
with the extension set to 6 sentences, the edge-
pruning threshold set to a connection strength of 
0.5, and the boundary threshold set to 0.7. The 
results for this setting are reported in table 2, row 
FE (the average number of segments per docu-
ment is 12.5).  Varying these parameters has only 
minor effects on performance, although the 
number of boundaries found can of course be 
tuned. RIs clearly provide a benefit over this type 
of system, by modeling a term?s influence dy-
namically rather than as a fixed interval.  
From here on, we report results only for the 
two systems: C99 and our best-performing sys-
tem, SS+C. 
For 86 of the documents, in which both C99 
and SS+C found more than 20 boundaries, we 
also calculate the performance on the best-
scoring 20 boundaries found by each system. 
These results are displayed in table 3. Note that 
when the number of boundaries to be found by 
each system is fixed at the actual number of 
boundaries, the values of precision and recall are 
necessarily identical. Here too our system out-
performs C99, and the differences are statistical-
ly significant, according to a one-tailed paired t-
test of equal means at the 0.01 level. 
 
 
N = 86 
 Prec.=Rec.=F Pk WD 
C99 
? 0.530 0.222 0.231 
s.d 0.105 0.070 0.074 
SS + C 
? 0.643 0.192 0.201 
s.d. 0.130 0.076 0.085 
 
Table 3.  Performance of C99 and SS on segmen-
tation of concatenated New York Times articles, 
selecting the 20 best-scoring boundaries 
4.5 Human-annotated television program 
closed-captions 
We selected twelve television programs for 
which we have closed-captions; they are a mix of 
headline news (3 shows), news commentary (4 
shows), documentary/lifestyle (3 shows), one 
comedy/drama episode, and one talk show. Only 
the closed captions are used, no speaker intona-
tion, video analysis, or metadata is employed.  
The closed captions are of variable quality, with 
numerous spelling errors. 
Five annotators were instructed to indicate 
topic boundaries in the closed-caption text files.  
Their instructions were open-ended in the sense 
that they were not given any definition of what a 
topic or a topic shift should be, beyond two short 
examples, were not told to find a specific number 
of boundaries, but were allowed to indicate how 
important a topic was on a five-point scale, en-
couraging them to indicate minor segments or 
subtopics within major topics if they chose to do 
so. For some television programs, particularly 
the news shows, major boundaries between sto-
ries on disparate topics are likely be broadly 
agreed on, whereas in much of the remaining 
material the shifts may be more fine-grained and 
judgments varied. In addition, the scripted nature 
of television speech results in many carefully 
staged transitions and teasers for upcoming ma-
terial, making boundaries more diffuse or con-
founded than in some other genres. 
We combined the five annotators? segmenta-
tions, to produce a single set of boundaries as a 
reference. We used a three-sentence sliding win-
dow, and if three or more of the five annotators 
place a boundary in that window, we assign a 
boundary where the majority of them place it (in 
case of a tie, we choose one location arbitrarily). 
Although the annotators are rather inconsistent in 
their use of this rating system, a given annotator 
tends to be consistent in the granularity of seg-
mentation employed across all documents. This 
observation is consistent with the remarks of Ma-
lioutov and Barzilay (2006) regarding varying 
topic granularity across human annotators on 
spoken material. We thus computed two versions 
of the combined boundaries, one in which all 
boundaries are used, and another in which we 
ignore minor boundaries?those the annotator 
assigned a score of 1 or 2.  We ran our experi-
ments with both versions of the combined boun-
daries as the reference segmentation. 
We use Pk to assess inter-annotator agreement 
among our five annotators. Table 4 presents two 
66
Pk values for each pair of annotators; one set of 
values is for all boundaries, while the other is for 
?major? boundaries, assigned an importance of 3 
or greater on the five-point scale. The Pk value 
for each annotator with respect to the two refer-
ence segmentations is also provided. 
 
 A B C D E Ref 
A 
 0.36 
0.48 
0.30 
0.45 
0.27 
0.44 
0.42 
0.67 
0.20 
0.38 
B 
0.29 
0.40 
 0.29 
0.32 
0.27 
0.33 
0.33 
0.55 
0.20 
0.25 
C 
0.57 
0.48 
0.60 
0.44 
 0.41 
0.20 
0.67 
0.61 
0.40 
0.18 
D 
0.36 
0.46 
0.41 
0.46 
0.27 
0.20 
 0.53 
0.63 
0.22 
0.26 
E 
0.33 
0.35 
0.31 
0.34 
0.33 
0.30 
0.32 
0.31 
 0.25 
0.27 
Ref 
0.25 
0.39 
0.32 
0.35 
0.24 
0.17 
0.21 
0.22 
0.42 
0.58 
 
 
Table 4. Pk values for the segmentations pro-
duced by each pair of annotators (A-E) and for 
the combined annotation described in section 
4.5; upper values are for all boundaries and low-
er values are for boundaries of segments scored 3 
or higher 
 
These numbers are rather high, but compara-
ble to those obtained by Malioutov and Barzilay 
(2006) in a somewhat similar task of segmenting 
video recordings of physics lectures. The Pk val-
ues are lower for the reference boundary set, 
which we therefore feel some confidence in us-
ing as a reference segmentation. 
 
 Prec. Rec. F Pk WD 
All topic boundaries 
C99 
? 0.197 0.186 0.184 0.476 0.507 
s.d 0.070 0.072 0.059 0.078 0.102 
SS 
+C 
? 0.315 0.208 0.240 0.421 0.462 
s.d. 0.089 0.073 0.064 0.072 0.084 
Major topic boundaries only 
C99 
? 0.170 0.296 0.201 0.637 0.812 
s.d. 0.063 0.134 0.060 0.180 0.405 
SS 
+C 
? 0.271 0.316 0.271 0.463 0.621 
s.d. 0.102 0.138 0.077 0.162 0.445 
 
Table 5. Performance of C99 and SS+C on seg-
mentation of closed-captions for twelve televi-
sion programs, with the two reference segmenta-
tions using ?all topic boundaries? and ?major 
topic boundaries only? 
 
As the television closed-captions are noisy 
with respect to data quality and inter-annotator 
disagreement, the performance of both systems is 
worse than on the concatenated news articles, as 
expected. We present the summary performance 
of C99 and SS+C in table 5, again using two ver-
sions of the reference. Because of the small test 
set size, we cannot claim statistical significance 
for any of these results, but we note that on aver-
age SS+C outperforms C99 on all measures.  
5 Conclusions and future work 
We have presented an approach to text segmen-
tation that relies on a novel graph based repre-
sentation of document structure and semantics. It 
successfully models topical coherence using 
long-range influence of terms and a contextually 
determined measure of semantic relatedness. Re-
levance intervals, calculated using PMI and other 
criteria, furnish an effective model of a term?s 
extent of influence for this purpose. Our measure 
of semantic relatedness reinforces global co-
occurrence statistics with local contextual infor-
mation, leading to an improved representation of 
topical coherence. We have demonstrated signif-
icantly improved segmentation resulting from 
this combination, not only on artificially con-
structed pseudo-documents, but also on noisy 
data with more diffuse boundaries, where inter-
annotator agreement is fairly low.   
Although the system we have described here is 
not trained in any way, it provides an extensive 
set of parameters that could be tuned to improve 
its performance. These include various tech-
niques for calculating the similarity between 
terms and combining those similarities in con-
nection strengths, heuristics for scoring potential 
boundaries, and thresholds for selecting those 
boundaries. Moreover, the graph representation 
lends itself to techniques for finding community 
structure and centrality, which may also prove 
useful in modeling topics and topic shifts. 
We have also begun to explore segment labe-
ling, identifying the most ?central? terms in a 
graph according to their connection strengths. 
Those terms whose nodes are strongly connected 
to others within a segment appear to be good 
candidates for segment labels. 
Finally, although we have so far applied this 
method only to linear segmentation, we plan to 
explore its application to hierarchical or overlap-
ping topical structures. We surmise that strongly 
connected subgraphs may correspond to these 
more fine-grained aspects of discourse structure.  
67
Acknowledgements 
We thank our colleagues David Houghton, 
Olivier Jojic, and Robert Rubinoff, as well as the 
anonymous referees, for their comments and 
suggestions. 
References  
Doug Beeferman, Adam Berger, and John Laf-
ferty. 1997.  Text Segmentation Using Expo-
nential Models. Proceedings of the Second 
Conference on Empirical Methods in Natural 
Language Processing, 35-46. 
Doug Beeferman, Adam Berger, and John Laf-
ferty. 1999. Statistical models for text segmen-
tation. Machine Learning, 34(1):177?210.  
David M. Blei and Pedro J. Moreno. 2001. Topic 
segmentation with an aspect hidden Markov 
model. Proceedings of the 24th Annual Meet-
ing of ACM SIGIR, 343?348. 
Burns, Philip R. 2006. MorphAdorner: Morpho-
logical Adorner for English Text. 
http://morphadorner.northwestern.edu/morpha
dorner/textsegmenter/. 
Freddy Y.Y. Choi. 2000. Advances in domain 
independent linear text segmentation. Pro-
ceedings of NAACL 2000, 109-117. 
Anthony Davis, Phil Rennert, Robert Rubinoff, 
Tim Sibley, and Evelyne Tzoukermann. 2004. 
Retrieving what's relevant in audio and video: 
statistics and linguistics in combination. Pro-
ceedings of RIAO 2004, 860-873. 
Olivier Ferret. 2007. Finding document topics for 
improving topic segmentation. Proceedings of 
the 45th Annual Meeting of the ACL, 480?487. 
Michel Galley, Kathleen McKeown, Eric Fosler-
Lussier, and Hongyan Jing. 2003. Discourse 
Segmentation of Multi-Party Conversation. 
Proceedings of the 41st Annual Meeting of the 
ACL, 562-569. 
Michelle Girvan and M.E.J. Newman. 2002. 
Community structure in social and biological 
networks. Proceedings of the National Acad-
emy of Sciences, 99:12, 7821-7826. 
Marti A. Hearst. 1994. Multi-paragraph segmen-
tation of expository text. Proceedings of the 
32nd Annual Meeting of the ACL, 9-16. 
Marti A. Hearst. 1997. TextTiling: Segmenting 
Text into Multi-Paragraph Subtopic Passages. 
Computational Linguistics, 23:1, 33-64. 
Min-Yen Kan, Judith L. Klavans, and Kathleen 
R. McKeown. 1998.  Linear Segmentation and 
Segment Significance. Proceedings of the 6th 
International Workshop on Very Large Cor-
pora, 197-205. 
Christopher Kennedy and Branimir Boguraev. 
1996. Anaphora for Everyone: Pronominal 
Anaphora Resolution without a Parser. Pro-
ceedings of the 16th International Conference 
on Computational Linguistics, 113-118. 
Hideki Kozima. 1993. Text segmentation based 
on similarity between words. Proceedings of 
the 31st Annual Meeting of the ACL (Student 
Session), 286-288. 
Sylvain Lamprier, Tassadit Amghar, Bernard 
Levrat and Frederic Saubion. 2007. On Evalu-
ation Methodologies for Text Segmentation 
Algorithms. Proceedings of the 19th IEEE In-
ternational Conference on Tools with Artifi-
cial Intelligence, 19-26. 
Igor Malioutov and Regina Barzilay. 2006. Min-
imum Cut Model for Spoken Lecture Segmen-
tation. Proceedings of the 21st International 
Conference on Computational Linguistics and 
44th Annual Meeting of the ACL, 25?32. 
Irina Matveeva and Gina-Anne Levow. 2007. 
Topic Segmentation with Hybrid Document 
Indexing. Proceedings of the 2007 Joint Con-
ference on Empirical Methods in Natural 
Language Processing and Computational 
Natural Language Learning, 351?359, 
David Milne and Ian H. Witten. 2009. An Open-
Source Toolkit for Mining Wikipedia. 
http://www.cs.waikato.ac.nz/~dnk2/publicatio
ns/AnOpenSourceToolkitForMiningWikipedia
.pdf. 
Jane Morris and Graeme Hirst. 1991. Lexical 
cohesion computed by thesaural relations. as 
an indicator of the structure of text. Computa-
tional Linguistics, 17:1, 21-48. 
Lev Pevzner and Marti A. Hearst. 2002. A criti-
que and improvement of an evaluation metric 
for text segmentation. Computational Linguis-
tics, 28:1, 19?36. 
 
68
Proceedings of the 2014 Workshop on Biomedical Natural Language Processing (BioNLP 2014), pages 59?67,
Baltimore, Maryland USA, June 26-27 2014.
c?2014 Association for Computational Linguistics
A System for Predicting ICD-10-PCS Codes
from Electronic Health Records
Michael Subotin
3M Health Information Systems
Silver Spring, MD
msubotin@mmm.com
Anthony R. Davis
3M Health Information Systems
Silver Spring, MD
adavis4@mmm.com
Abstract
Medical coding is a process of classify-
ing health records according to standard
code sets representing procedures and di-
agnoses. It is an integral part of health
care in the U.S., and the high costs it
incurs have prompted adoption of natu-
ral language processing techniques for au-
tomatic generation of these codes from
the clinical narrative contained in elec-
tronic health records. The need for effec-
tive auto-coding methods becomes even
greater with the impending adoption of
ICD-10, a code inventory of greater com-
plexity than the currently used code sets.
This paper presents a system that predicts
ICD-10 procedure codes from the clinical
narrative using several levels of abstrac-
tion. First, partial hierarchical classifica-
tion is used to identify potentially rele-
vant concepts and codes. Then, for each
of these concepts we estimate the confi-
dence that it appears in a procedure code
for that document. Finally, confidence val-
ues for the candidate codes are estimated
using features derived from concept confi-
dence scores. The concept models can be
trained on data with ICD-9 codes to sup-
plement sparse ICD-10 training resources.
Evaluation on held-out data shows promis-
ing results.
1 Introduction
In many countries reimbursement rules for health
care services stipulate that the patient encounter
must be assigned codes representing diagnoses
that were made for and procedures that were per-
formed on the patient. These codes may be as-
signed by general health care personnel or by spe-
cially trained medical coders. The billing codes
used in the U.S. include International Statisti-
cal Classification of Diseases and Related Health
Problems (ICD) codes, whose version 9 is cur-
rently in use and whose version 10 was scheduled
for adoption in October 2014
1
, as well as Current
Procedural Terminology (CPT) codes. The same
codes are also used for research, internal book-
keeping, and other purposes.
Assigning codes to clinical documentation of-
ten requires extensive technical training and in-
volves substantial labor costs. This, together with
increasing prominence of electronic health records
(EHRs), has prompted development and adoption
of NLP algorithms that support the coding work-
flow by automatically inferring appropriate codes
from the clinical narrative and other information
contained in the EHR (Chute et al., 1994; Heinze
et al., 2001; Resnik et al., 2006; Pakhomov et al.,
2006; Benson, 2006). The need for effective auto-
coding methods becomes especially acute with the
introduction of ICD-10 and the associated increase
of training and labor costs for manual coding.
The novelty and complexity of ICD-10 presents
unprecedented challenges for developers of rule-
based auto-coding software. Thus, while ICD-9
contains 3882 codes for procedures, the number
of codes defined by the ICD-10 Procedure Cod-
ing System (PCS) is greater than 70,000. Further-
more, the organization of ICD-10-PCS is funda-
mentally different from ICD-9, which means that
the investment of time and money that had gone
into writing auto-coding rules for ICD-9 proce-
dure codes cannot be easily leveraged in the tran-
sition to ICD-10.
In turn, statistical auto-coding methods are con-
strained by the scarcity of available training data
with manually assigned ICD-10 codes. While this
problem will be attenuated over the years as ICD-
10-coded data are accumulated, the health care
1
The deadline was delayed by at least a year while this
paper was in review.
59
industry needs effective technology for ICD-10
computer-assisted coding in advance of the imple-
mentation deadline. Thus, for developers of statis-
tical auto-coding algorithms two desiderata come
to the fore: these algorithms should take advantage
of all available training data, including documents
supplied only with ICD-9 codes, and they should
possess high capacity for statistical generalization
in order to maximize the benefits of training mate-
rial with ICD-10 codes.
The auto-coding system described here seeks
to meet both these requirements. Rather than
predicting codes directly from the clinical narra-
tive, a set of classifiers is first applied to identify
coding-related concepts that appear in the EHR.
We use General Equivalence Mappings (GEMs)
between ICD-9 and ICD-10 codes (CMS, 2014)
to train these models not only on data with human-
assigned ICD-10 codes, but also on ICD-9-coded
data. We then use the predicted concepts to de-
rive features for a model that estimates probabil-
ity of ICD-10 codes. Besides the intermediate ab-
straction to concepts, the code confidence model
itself is also designed so as to counteract sparsity
of the training data. Rather than train a separate
classifier for each code, we use a single model
whose features can generalize beyond individual
codes. Partial hierarchical classification is used for
greater run-time efficiency. To our knowledge, this
is the first research publication describing an auto-
coding system for ICD-10-PCS. It is currently de-
ployed, in tandem with other auto-coding mod-
ules, to support computer-assisted coding in the
3M
TM
360 Encompass
TM
System.
The rest of the paper is organized as follows.
Section 2 reviews the overall organization of ICD-
10-PCS. Section 4.1 outlines the run-time process-
ing flow of the system to show how its components
fit together. Section 4.2 describes the concept con-
fidence models, including the hierarchical classi-
fication components. Section 4.3 discusses how
data with manually assigned ICD-9 codes is used
to train some of the concept confidence models.
Section 4.4 describes the code confidence model.
Finally, Section 5 reports experimental results.
2 ICD-10 Procedure Coding System
ICD-10-PCS is a set of codes for medical proce-
dures, developed by 3M Health Information Sys-
tems under contract to the Center for Medicare and
Medicaid Services of the U.S. government. ICD-
10-PCS has been designed systematically; each
code consists of seven characters, and the charac-
ter in each of these positions signifies one partic-
ular aspect of the code. The first character des-
ignates the ?section? of ICD-10-PCS: 0 for Med-
ical and Surgical, 1 for Obstetrics, 2 for Place-
ment, and so on. Within each section, the seven
components, or axes of classification, are intended
to have a consistent meaning; for example in the
Medical and Surgical section, the second charac-
ter designates the body system involved, the third
the root operation, and so on (see Table 1 for a
list). All procedures in this section are thus clas-
sified along these axes. For instance, in a code
such as 0DBJ3ZZ, the D in the second position in-
dicates that the body system involved is the gas-
trointestinal system, B in the third position always
indicates that the root operation is an excision of a
body part, the J in the fourth position indicates that
the appendix is the body part involved, and the 3 in
the fifth position indicates that the approach is per-
cutaneous. The value Z in the last two axes means
than neither a device nor a qualifier are specified.
Character Meaning
1st Section
2nd Body System
3rd Root Operation
4th Body Part
5th Approach
6th Device
7th Qualifier
Table 1: Character Specification of the Medical
and Surgical Section of ICD-10-PCS
Several consequences of the compositional
structure of ICD-10-PCS are especially relevant
for statistical auto-coding methods.
On the one hand, it defines over 70,000 codes,
many of which are logically possible, but very rare
in practice. Thus, attempts to predict the codes as
unitary entities are bound to suffer from data spar-
sity problems even with a large training corpus.
Furthermore, some of the axis values are formu-
lated in ways that are different from how the cor-
responding concepts would normally be expressed
in a clinical narrative. For example, ICD-10-PCS
uses multiple axes (root opreration, body part, and,
in a sense, the first two axes as well) to encode
what many traditional procedure terms (such as
those ending in -tomy and -plasty) express by a
60
single word, while the device axis uses generic
categories where a clinical narrative would refer
only to specific brand names. This drastically lim-
its how much can be accomplished by matching
code descriptions or indexes derived from them
against the text of EHRs.
On the other hand, the systematic conceptual
structure of PCS codes and of the codeset as a
whole can be exploited to compensate for data
sparsity and idiosyncracies of axis definitions by
introducing abstraction into the model.
3 Related work
There exists a large literature on automatic clas-
sification of clinical text (Stanfill et al., 2010). A
sizeable portion of it is devoted to detecting cate-
gories corresponding to billing codes, but most of
these studies are limited to one or a handful of cat-
egories. This is in part because the use of patient
records is subject to strict regulation. Thus, the
corpus used for most auto-coding research up to
date consists of about two thousand documents an-
notated with 45 ICD-9 codes (Pestian et al., 2007).
It was used in a shared task at the 2007 BioNLP
workshop and gave rise to papers studying a va-
riety of rule-based and statistical methods, which
are too numerous to list here.
We limit our attention to a smaller set of re-
search publications describing identification of an
entire set of billing codes, or a significant por-
tion thereof, which better reflects the role of auto-
coding in real-life applications. Mayo Clinic was
among the earliest adopters of auto-coding (Chute
et al., 1994), where it was deployed to assign
codes from a customized and greatly expanded
version of ICD-8, consisting of almost 30K diag-
nostic codes. A recently reported version of their
system (Pakhomov et al., 2006) leverages a com-
bination of example-based techniques and Na??ve
Bayes classification over a database of over 20M
EHRs. The phrases representing the diagnoses
have to be itemized as a list beforehand. In an-
other pioneering study, Larkey & Croft (1995) in-
vestigated k-Nearest Neighbor, Na??ve Bayes, and
relevance feedback on a set of 12K discharge
summaries, predicting ICD-9 codes. Heinze et
al (2000) and Ribeiro-Neto et al (2001) describe
systems centered on symbolic computation. Jiang
et al (2006) discuss confidence assessment for
ICD-9 and CPT codes, performed separately from
code generation. Medori & Fairon (2010) com-
bine information extraction with a Na??ve Bayes
classifier, working with a corpus of about 20K dis-
charge summaries in French. In a recent paper,
Perotte et al (2014) study standard and hierarchi-
cal classification using support vector machines on
a corpus of about 20K EHRs with ICD-9 codes.
We are not aware of any previous publications
on auto-coding for ICD-10-PCS, and the results
of these studies cannot be directly compared with
those reported below due to the unique nature of
this code set. Our original contributions also in-
clude explicit modeling of concepts and the ca-
pability to assign previously unobserved codes
within a machine learning framework.
4 Methods
4.1 Run-time processing flow
We first describe the basic run-time processing
flow of the system, shown in Figure 1.
Figure 1: Run-time processing flow
In a na??ve approach, one could generate all
codes from the ICD-10-PCS inventory for each
EHR
2
and estimate their probability in turn, but
this would be too computationally expensive. In-
stead, the hypothesis space is restricted by two-
2
We use the term EHR generically in this paper. The sys-
tem can be applied at the level of individual clinical docu-
ments or entire patient encounters, whichever is appropriate
for the given application.
61
level hierarchical classification with beam search.
First, a set of classifiers estimates the confidence
of all PCS sections (one-character prefixes of the
codes), one per section. The sections whose con-
fidence exceeds a threshold are used to generate
candidate body systems (two-character code pre-
fixes), whose confidence is estimated by another
set of classifiers. Then, body systems whose con-
fidence exceeds a threshold are used to generate
a set of candidate codes and the set of concepts
expressed by these codes. The probability of ob-
serving each of the candidate concepts in the EHR
is estimated by a separate classifier. Finally, these
concept confidence scores are used to derive fea-
tures for a model that estimates the probability of
observing each of the candidate codes, and the
highest-scoring codes are chosen according to a
thresholding decision rule.
The choice of two hierarchical layers is partially
determined by the amount of training data with
ICD-10 codes available for this study, since many
three-character code prefixes are too infrequent to
train reliable classifiers. Given more training data,
additional hierarchical classification layers could
be used, which would trade a higher risk of recall
errors against greater processing speed. The same
trade-off can be negotiated by adjusting the beam
search threshold.
4.2 Concept confidence models
Estimation of concept confidence ? including the
confidence of code prefixes in the two hierarchi-
cal classification layers ? is performed by a set of
classifiers, one per concept, which are trained on
EHRs supplied with ICD-10 and ICD-9 procedure
codes.
The basis for training the concept models is
provided by a mapping between codes and con-
cepts expressed by the codes. For example, the
code 0GB24ZZ (Excision of Left Adrenal Gland,
Percutaneous Endoscopic Approach) expresses,
among other concepts, the concept adrenal gland
and the more specific concept left adrenal gland.
It also expresses the concept of adrenalectomy
(surgical removal of one or both of the adrenal
glands), which corresponds to the regular expres-
sion 0G[BT][234]..Z over ICD-10-PCS codes.
We used the code-to-concept mapping described
in Mills (2013), supplemented by some additional
categories that do not correspond to traditional
clinical concepts. For example, our set of concepts
included entries for the categories of no device
and no qualifer, which are widely used in ICD-10-
PCS. We also added entries that specified the de-
vice axis or the qualifier axis together with the first
three axes, where they were absent in the original
concept map, reasoning that the language used to
express the choice of the device or qualifier can be
specific to particular procedures and body parts.
For data with ICD-10-PCS codes, the logic used
to generate training instances is straightforward.
Whenever a manually assigned code expresses a
given concept, a positive training instance for the
corresponding classifier is generated. Negative
training instances are sub-sampled from the con-
cepts generated by hierarchical classification lay-
ers for that EHR. As can be seen from this logic,
the precise question that the concept models seek
to answer is as follows: given that this particular
concept has been generated by the upstream hier-
archical layers, how likely is it that it will be ex-
pressed by one of the ICD-10 procedure codes as-
signed to that EHR?
In estimating concept confidence we do not at-
tempt to localize where in the clinical narrative
the given concept is expressed. Our baseline
feature set is simply a bag of tokens. We also
experimented with other feature types, including
frequency-based weighting schemes for token fea-
ture values and features based on string matches of
Unified Medical Language System (UMLS) con-
cept dictionaries. For the concepts of left and right
we define an additional feature type, indicating
whether the token left or right appears more fre-
quently in the EHR. While still rudimentary, this
feature type is more apt to infer laterality than a
bag of tokens.
A number of statistical methods can be used
to estimate concept confidence. We use the
Mallet (McCallum, 2002) implementation of `
1
-
regularized logistic regression, which has shown
good performance for NLP tasks in terms of ac-
curacy as well as scalability at training and run-
time (Gao et al., 2007).
4.3 Training on ICD-9 data
In training concept confidence models on data
with ICD-9 codes we make use of the General
Equivalence Mappings (GEMs), a publicly avail-
able resource establishing relationships between
ICD-9 and ICD-10 codes (CMS, 2014). Most cor-
respondences between ICD-9 and ICD-10 proce-
62
dure codes are one-to-many, although other map-
ping patterns are also found. Furthermore, a code
in one set can correspond to a combination of
codes from the other set. For example, the ICD-
9 code for combined heart-lung transplantation
maps to a set of pairs of ICD-10 codes, the first
code in the pair representing one of three possible
types of heart transplantation, and the other rep-
resenting one of three possible types of bilateral
lung transplantation.
A complete description of the rules underlying
GEMs and our logic for processing them is beyond
the scope of this paper, and we limit our discussion
to the principles underlying our approach. We first
distribute a unit probability mass over the ICD-
10 codes or code combinations mapped to each
ICD-9 code, using logic that reflects the struc-
ture of GEMs and distributing probability mass
uniformly among comparable alternatives. From
these probabilities we compute a cumulative prob-
ability mass for each concept appearing in the
ICD-10 codes. For example, if an ICD-9 code
maps to four ICD-10 codes over which we dis-
tribute a uniform probability distibution, and a
given concept appears in two of them, we assign
the probability of 0.5 to that concept. For a given
EHR, we assign to each concept the highest prob-
ability it receives from any of the codes observed
for the EHR. Finally, we use the resulting concept
probabilities to weight positive training instances.
Negative instances still have unit weights, since
they correspond to concepts that can be unequivo-
cably ruled out based on the GEMs.
4.4 Code confidence model
The code confidence model produces a confidence
score for candidate codes generated by the hierar-
chical classification layers, using features derived
from the output of the code confidence models
described above. The code confidence model is
trained on data with ICD-10 codes. Whenever a
candidate code matches a code assigned by hu-
man annotators, a positive training instance is gen-
erated. Otherwise, a negative instance is gener-
ated, with sub-sampling. We report experiments
using logistic regression with `
1
and `
2
regulariza-
tion (Gao et al., 2007).
The definition of features used in the model re-
quires careful attention, because it is in the form of
the feature space that the proposed model differs
from a standard one-vs-all approach. To elucidate
the contrast we may start with a form of the feature
space that would correspond to one-vs-all classi-
fication. This can be achieved by specifying the
identity of a particular code in all feature names.
Then, the objective function for logistic regression
would decompose into independent learning sub-
problems, one for each code, producing a collec-
tion of one-vs-all classifiers. There are clear draw-
backs to this approach. If all parameters are re-
stricted to a specific code, the training data would
be fragmented along the same lines. Thus, even
if features derived from concepts may seem to en-
able generalization, in reality they would in each
case be estimated only from training instances cor-
responding to a single code, causing unnecessary
data sparsity.
This shortcoming can be overcome in logistic
regression simply by introducing generalized fea-
tures, without changing the rest of the model (Sub-
otin, 2011). Thus, in deriving features from scores
of concept confidence models we include only
those concepts which are expressed by the given
code, but we do not specify the identity of the code
in the feature names. In this way the weights for
these features are estimated at once from training
instances for all codes in which these concepts ap-
pear. We combine these generalized features with
the code-bound features described earlier. The lat-
ter should help us learn more specific predictors
for particular procedures, when such predictors
exist in the feature space.
While the scores of concept confidence mod-
els provide the basis for the feature space of the
code confidence model, there are multiple ways in
which features can be derived from these scores.
The simplest way is to take concept identity (op-
tionally specified by code identity) as the fea-
ture name and the confidence score as the feature
value. We supplement these features with features
based on score quantization. That is, we thresh-
old each concept confidence score at several points
and define binary features indicating whether the
score exceeds each of the thresholds. For both
these feature types, we generate separate features
for predictions of concept models trained on ICD-
9 data and concept models trained on ICD-10 data
in order to allow the code confidence model to
learn how useful predictions of concept confidence
models are, depending on the type of their training
data.
Both the concept confidence models and the
63
code confidence model can be trained on data with
ICD-10 codes. We are thus faced with the ques-
tion of how best to use this limited resource. The
simplest approach would be to train both types of
models on all available training data, but there is a
concern that predictions of the concept models on
their own training data would not reflect their out-
of-sample performance, and this would mislead
the code confidence model into relying on them
too much. An alternative approach, often called
stacked generalization (Wolpert, 1992), would be
to generate training data for the code confidence
model by running concept confidence models on
out-of-sample data. We compare the performance
of these approaches below.
5 Evaluation
5.1 Methodology
We evaluated the proposed model using a cor-
pus of 28,536 EHRs (individual clinical records),
compiled to represent a wide variety of clinical
contexts and supplied with ICD-10-PCS codes by
trained medical coders. The corpus was annotated
under the auspices of 3M Health Information Sys-
tems for the express purpose of developing auto-
coding technology for ICD-10. There was a total
of 51,082 PCS codes and 5,650 unique PCS codes
in the corpus, only 76 of which appeared in more
than 100 EHRs, and 2,609 of which appeared just
once. Multiple coders worked on some of the doc-
uments, but they were allowed to collaborate, pro-
ducing what was effectively a single set of codes
for each EHR. We held out about a thousand EHRs
for development testing and evaluation, each, us-
ing the rest for training. The same corpus, as well
as 175,798 outpatient surgery EHRs with ICD-9
procedure codes submitted for billing by a health
provider were also used to train hierarchical and
concept confidence models.
We evaluated auto-coding performance by a
modified version of mean reciprocal rank (MRR).
MRR is a common evaluation metric for systems
with ranked outputs. For a set of Q correct out-
puts with ranks rank
i
among all outputs, standard
MRR is computed as:
MRR =
1
Q
Q
?
i=1
1
rank
i
For example, a MRR value of 0.25 means that
that the correct answer has rank 4 on average. This
metric is designed for tasks where only one of the
outputs can be correct. When applied directly to
tasks where more than one output can be correct,
MRR unfairly penalizes cases with multiple cor-
rect outputs, increasing the rank of some correct
outputs on account of other, higher-ranked outputs
that are also correct. We modify MRR for our task
by ignoring correct outputs in the rank computa-
tions. In other words, the rank of a correct output
is computed as the number of higher-ranked incor-
rect outputs, plus one. This metric has the advan-
tage of summarizing the accuracy of an auto-coder
without reference to a particular choice of thresh-
old, which may be determined by business rules or
research considerations, as would be the case for
precision and recall.
One advantage of regularized logistic regres-
sion is that the value of 1 is often a near-optimal
setting for the regularization trade-off parameter.
This can save considerable computation time that
would be required for tuning this parameter for
each experimental condition. We have previously
observed that the value of 1 consistently produced
near-optimal results for the `
1
regularizer in con-
cept confidence models and for the `
2
regularizer
in the code confidence models, and we have used
this setting for all the experiments reported here.
For the code confidence model with `
1
-regularized
logistic regression we saw a slight improvement
with weaker regularization, and we report the best
result we obtained for this model below.
5.2 Results
The results are shown in Table 2. The top MMR
score of 0.572 corresponds to a micro-averaged F-
score of 0.485 (0.490 precision, 0.480 recall) when
the threshold is chosen to obtain approximately
equal values for recall and precision
3
. The best
result was obtained when:
? the concept models used bag-of-tokens fea-
tures (with the additional laterality features
described in Section 4.2);
? both concept models trained on ICD-9 data
and those trained on ICD-10 data were used;
? the code confidence model was trained on
data with predictions of concept models
trained on all of ICD-10 data (i.e., no
3
To put these numbers into perspective, note that the aver-
age accuracy of trained medical coders for ICD-10 has been
estimated to be 63% (HIMSS/WEDI, 2013).
64
data splitting for stacked generalization was
used);
? the code confidence model used all of the fea-
ture types described in Section 4.4;
? the code confidence model used logistic re-
gression with `
2
regularization.
We examine the impact of all these choices on
system performance in turn.
Model MRR
All data, all features, `
2
reg. 0.572
Concept model training:
Trained on ICD-10 only 0.558
Trained on ICD-9 only 0.341
Code model features:
One-vs-all 0.519
No code-bound features 0.553
No quantization features 0.560
Stacked generalization:
half & half data split 0.501
5-fold cross-validation 0.539
Code model algorithm:
`
1
regularization 0.528
Table 2: Evaluation results. Each row after the
first correponds to varying one aspect of the model
shown in the first row. See Section 5.3 for details
of the experimental conditions.
5.3 Discussion
Despite its apparent primitive nature, the bag-of-
token feature space for the concept confidence
models has turned out to provide a remarkably
strong baseline. Our experiments with frequency-
based weighting schemes for the feature values
and with features derived from text matches from
the UMLS concept dictionaries did not yield sub-
stantial improvements in the results. Thus, the use
of UMLS-based features, obtained using Apache
ConceptMapper, yielded a relative improvement
of 0.6% (i.e., 0.003 in absolute terms), but at the
cost of nearly doubling run-time processing time.
Nonetheless, we remain optimistic that more so-
phisticated features can benefit performance of the
concept models while maintaining their scalabil-
ity.
As can be seen from the table, both concept
models trained on ICD-9 data and those trained on
ICD-10 data contributed to the overall effective-
ness of the system. However, the contribution of
the latter is markedly stronger. This suggests that
further research is needed in finding the best ways
of exploiting ICD-9-coded data for ICD-10 auto-
coding. Given that data with ICD-9 codes is likely
to be more readily available than ICD-10 training
data in the foreseeable future, this line of investi-
gation holds potential for significant gains in auto-
coding performance.
For the choice of features used in the code con-
fidence model, the most prominent contribution is
made by the feature that generalize beyond spe-
cific codes, as discussed in Section 4.4. Adding
these features yields a 10% relative improvement
over the set of features equivalent to a one-vs-
all model. In fact, using the generalized features
alone (see the row marked ?no code-bound fea-
tures? in Table 2) gives a score only 0.02 lower
than the best result. As would be expected, gener-
alized features are particularly important for codes
with limited training data. Thus, if we restrict
our attention to codes with fewer than 25 training
instances (which account for 95% of the unique
codes in our ICD-10 training data), we find that
generalized features yielded a 25% relative im-
provement over the one-vs-all model (0.247 to
0.309). In contrast, for codes with over 100 train-
ing instances (which account for 1% of the unique
codes, but 36% of the total code volume in our
corpus) the relative improvement from generalized
features is less than 4% (0.843 to 0.876). These
numbers afford two further observations. First,
the model can be improved dramatically by adding
a few dozen EHRs per code to the training cor-
pus. Secondly, there is still much room for re-
search in mitigating the effects of data sparsity
and improving prediction accuracy for less com-
mon codes. Elsewhere in Table 2 we see that
quantization-based features contribute a modest
predictive value.
Perhaps the most surprising result of the series
came from investigating the options for using the
available ICD-10 training data, which act as train-
ing material both for concept confidence models
and the code confidence model. The danger of
training both type of models on the same corpus
is intuitively apparent. If the training instances
for the code model are generated by concept mod-
els whose training data included the same EHRs,
the accuracy of these concept predictions may not
65
reflect out-of-sample performance of the concept
models, causing the code model to rely on them
excessively.
The simplest implementation of Wolpert?s
stacked generalization proposal, which is intended
to guard against this risk, is to use one part of the
corpus to train one predictive layer and use its pre-
dictions on the another part of the corpus to train
the other layer. The result in Table 2 (see the
row marked ?half & half data split?) shows that
the resulting increase in sparsity of the training
data for both models leads to a major degradation
of the system?s performance, even though at run-
time concept models trained on all available data
are used. We also investigated a cross-validation
version of stacked generalization designed to mit-
igate against this fragmentation of training data.
We trained a separate set of concept models on the
training portion of each cross-validation fold, and
ran them on the held-out portion. The training set
for the code confidence model was then obtained
by combining these held-out portions. At run-
time, concept models trained on all of the avail-
able data were used. However, as intuitively com-
pelling as the arguments motivating this procedure
may be, the results were not competitive with the
baseline approach of using all available training
data for all the models.
Finally, we found that an `
2
regularizer per-
formed clearly better than an `
1
regularizer for the
code confidence model, even though we set the `
2
trade-off constant to 1 and tuned the `
1
trade-off
constant on the development test set. This is in
contrast to concept confidence models, where we
observed slightly better results with `
1
regulariza-
tion than with `
2
regularization.
6 Conclusion
We have described a system for predicting ICD-
10-PCS codes from the clinical narrative con-
tained in EHRs. The proposed approach seeks to
mitigate the sparsity of training data with manu-
ally assigned ICD-10-PCS codes in three ways:
through an intermediate abstraction to clinical
concepts, through the use of data with ICD-9
codes to train concept confidence models, and
through the use of a code confidence model
whose parameters can generalize beyond individ-
ual codes. Our experiments show promising re-
sults and point out directions for further research.
Acknowledgments
We would like to thank Ron Mills for provid-
ing the crosswalk between ICD-10-PCS codes and
clinical concepts; Guoli Wang, Michael Nossal,
Kavita Ganesan, Joel Bradley, Edward Johnson,
Lyle Schofield, Michael Connor, Jean Stoner and
Roxana Safari for helpful discussions relating to
this work; and the anonymous reviewers for their
constructive criticism.
References
Sean Benson. 2006. Computer-assisted Coding Soft-
ware Improves Documentation, Coding, Compli-
ance, and Revenue. Perspectives in Health Infor-
mation Management, CAC Proceedings, Fall 2006.
Centers for Medicare & Medicaid Services. 2014.
General Equivalence Mappings. Documentation
for Technical Users. Electronically published at
cms.gov.
Chute CG, Yang Y, Buntrock J. 1994. An evalua-
tion of computer assisted clinical classification algo-
rithms. Proc Annu Symp Comput Appl Med Care.,
1994:162?6.
Jianfeng Gao, Galen Andrew, Mark Johnson, Kristina
Toutanova. 2007. A Comparative Study of Param-
eter Estimation Methods for Statistical Natural Lan-
guage Processing. ACL 2007.
Daniel T. Heinze, Mark L. Morsch, Ronald E. Shef-
fer, Jr., Michelle A. Jimmink, Mark A. Jennings,
William C. Morris, and Amy E. W. Morsch. 2000.
LifeCode
TM
? A Natural Language Processing Sys-
tem for Medical Coding and Data Mining. AAAI
Proceedings.
Daniel T. Heinze, Mark Morsch, Ronald Sheffer,
Michelle Jimmink, Mark Jennings, William Mor-
ris, and Amy Morsch. 2001. LifeCode: A De-
ployed Application for Automated Medical Coding.
AI Magazine, Vol 22, No 2.
HIMSS/WEDI. 2013. ICD-10 National Pilot Pro-
gram Outcomes Report. Electronically published at
himss.org.
Yuankai Jiang, Michael Nossal, and Philip Resnik.
2006. How Does the System Know It?s Right?
Automated Confidence Assessment for Compliant
Coding. Perspectives in Health Information Man-
agement, Computer Assisted Coding Conference
Proceedings, Fall 2006.
Leah Larkey and W. Bruce Croft. 1995. Automatic As-
signment of ICD9 Codes To Discharge Summaries.
Technical report, Center for Intelligent Information
Retrieval at University of Massachusetts.
66
Andrew Kachites McCallum. 2002. MAL-
LET: A Machine Learning for Language Toolkit.
http://mallet.cs.umass.edu
Medori, Julia and Fairon, C?edrick. 2010. Machine
Learning and Features Selection for Semi-automatic
ICD-9-CM Encoding. Proceedings of the NAACL
HLT 2010 Second Louhi Workshop on Text and Data
Mining of Health Documents, 2010: 84?89.
Ronald E. Mills. 2013. Methods using multi-
dimensional representations of medical codes. US
Patent Application US20130006653.
S.V. Pakhomov, J.D. Buntrock, and C.G. Chute. 2006.
Automating the assignment of diagnosis codes to
patient encounters using example-based and ma-
chine learning techniques. J Am Med Inform Assoc,
13(5):516?25.
Adler Perotte, Rimma Pivovarov, Karthik Natarajan,
Nicole Weiskopf, Frank Wood, No?emie Elhadad .
2014. Diagnosis code assignment: models and eval-
uation metrics. J Am Med Inform Assoc, 21(2):231?
7.
Pestian, JP, Brew C, Matykiewicz P, Hovermale DJ,
Johnson N, Bretonnel Cohen K, and Duch W. 2007.
A shared task involving multi-label classification
of clinical free text. Proceedings ACL: BioNLP,
2007:97?104.
Philip Resnik, Michael Niv, Michael Nossal, Gregory
Schnitzer, Jean Stoner, Andrew Kapit, and Richard
Toren. 2006. Using intrinsic and extrinsic metrics
to evaluate accuracy and facilitation in computer-
assisted coding.. Perspectives in Health Informa-
tion Management, Computer Assisted Coding Con-
ference Proceedings, Fall 2006.
Berthier Ribeiro-Neto, Alberto H.F. Laender and Lu-
ciano R.S. de Lima. 2001. An experimental study
in automatically categorizing medical documents.
Journal of the American Society for Information Sci-
ence and Technology, 52(5): 391?401.
Mary H. Stanfill, Margaret Williams, Susan H. Fenton,
Robert A. Jenders, and William R. Hersh. 2010.
A systematic literature review of automated clinical
coding and classification systems. J Am Med Inform
Assoc., 17(6): 646?651.
Michael Subotin. 2011. An exponential translation
model for target language morphology. ACL 2011.
David H. Wolpert. 1992. Stacked Generalization.
Neural Networks, 5:241?259.
67
