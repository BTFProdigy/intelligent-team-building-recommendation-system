49
50
51
52
Proceedings of the Second Workshop on Statistical Machine Translation, pages 181?184,
Prague, June 2007. c?2007 Association for Computational Linguistics
Getting to know Moses:  
Initial experiments on German?English factored translation  
Maria Holmqvist, Sara Stymne, and Lars Ahrenberg 
Department of Computer and Information Science 
Link?pings universitet, Sweden 
{marho,sarst,lah}@ida.liu.se 
 
 
 
Abstract 
We present results and experiences from 
our experiments with phrase-based statisti-
cal machine translation using Moses. The 
paper is based on the idea of using an off-
the-shelf parser to supply linguistic infor-
mation to a factored translation model and 
compare the results of German?English 
translation to the shared task baseline sys-
tem based on word form. We report partial 
results for this model and results for two 
simplified setups. Our best setup takes ad-
vantage of the parser?s lemmatization and 
decompounding. A qualitative analysis of 
compound translation shows that decom-
pounding improves translation quality. 
1 Introduction  
One of the stated goals for the shared task of this 
workshop is ?to offer newcomers a smooth start 
with hands-on experience in state-of-the-art statis-
tical machine translation methods?. As our previ-
ous research in machine translation has been 
mainly concerned with rule-based methods, we 
jumped at this offer. 
We chose to work on German-to-English trans-
lation for two reasons. Our primary practical inter-
est lies with translation between Swedish and Eng-
lish, and of the languages offered for the shared 
task, German is the one closest in structure to 
Swedish. While there are differences in word order 
and morphology between Swedish and German, 
there are also similarities, e.g., that both languages 
represent nominal compounds as single ortho-
graphic words. We chose the direction from Ger-
man to English because our knowledge of English 
is better than our knowledge of German, making it 
easier to judge the quality of translation output. 
Experiments were performed on the Europarl data. 
With factored statistical machine translation, 
different levels of linguistic information can be 
taken into account during training of a statistical 
translation system and decoding. In our experi-
ments we combined syntactic and morphological 
factors from an off-the-shelf parser with the fac-
tored translation framework in Moses (Moses, 
2007). We wanted to test the following hypotheses: 
? Translation models based on lemmas will im-
prove translation quality (Popovi? and Ney, 
2004) 
? Decompounding German nominal compounds 
will improve translation quality (Koehn and 
Knight, 2003) 
? Re-ordering models based on word forms and 
parts-of-speech will improve translation qual-
ity (Zens and Ney, 2006). 
2 The parser 
The parser, Machinese Syntax, is a commercially 
available dependency parser from Connexor Oy 1. 
It provides each word with lemma, part-of-speech, 
morphological features and dependency relations 
(see Figure 1). In addition, the lemmas of com-
pounds are marked by a ?#? separating the two 
parts of the compound. For the shared task we only 
used shallow linguistic information: lemma, part-
of-speech and morphology. The compound bound-
ary identification was used to split noun com-
                                                 
1 Connexor Oy, http://www.connexor.com. 
181
pounds to make the German input more similar to 
English text. 
 
1 Mit   mit   pm>2    @PREMARK PREP 
2 Blick blick advl>10 @NH N MSC SG DAT 
3 auf   auf   pm>5    @PREMARK PREP 
 
Figure 1. Example of parser output 
 
We used the parser?s tokenization as given. Some 
common multiword units, such as ?at all? and ?von 
heute?, are treated as single words by the parser 
(cf. Niessen and Ney, 2004). The German parser 
also splits contracted prepositions and determiners 
like ?zum? ? ?zu dem? (?to the?). 
3 System description 
For our experiments with Moses we basically fol-
lowed the shared task baseline system setup to 
train our factored translation models. After training 
a statistical model, minimum error-rate tuning was 
performed to tune the model parameters. All ex-
periments were performed on an AMD 64 Athlon 
4000+ processor with 4 Gb of RAM and 32 bit 
Linux (Ubuntu).  
Since time as well as computer resources were 
limited we designed a model that we hoped would 
make the best use of all available factors. This 
model turned out to be too complex for our ma-
chine and in later experiments we abandoned it for 
a simpler model.  
3.1 Pre-processing 
In the pre-processing step we used the standard 
pre-processing of the shared task baseline system, 
parsed the German and English texts and processed 
the output to obtain four factors: word form, 
lemma, part-of-speech and morphology. Missing 
values for lemma, part-of-speech and morphology 
were replaced with default values. 
Noun compounds are very frequent in German, 
2.9% of all tokens in the tuning corpus were identi-
fied by the parser as noun compounds. Compounds 
tend to lead to sparse data problems and splitting 
them has been shown to improve German-English 
translation (Koehn and Knight, 2003). Thus we 
decided to decompund German noun compounds 
identified as such by our parser.  
We used a simple strategy to remove fillers and 
to correct some obvious mistakes. We removed the 
filler ?-s? that appear before a marked split unless it 
was one of ?-ss?, ?-urs?, ?-eis? or ?-us?. This applied 
to 35% of the noun compounds in the tuning cor-
pus. The fillers were removed both in the word 
form and the lemma (see Figure 2). 
There were some mistakes made by the parser, 
for instance on compounds containing the word 
?nahmen? which was incorrectly split as ?stel-
lungn#ahmen? instead of ?stellung#nahmen? 
(?statement?). These splits were corrected by mov-
ing the ?n? to the right side of the split. 
We then split noun-lemmas on hyphens unless 
there were numbers on either side of it and on the 
places marked by ?#?. Word forms were split in the 
corresponding places as the lemmas. 
The part-of-speech and morphology of the last 
word in the compound is the same as for the whole 
compound. For the other parts we hypothesized 
that part-of-speech is Noun and the morphology is 
unknown, marked by the tag UNK. 
 
Parser output: 
unionsl?nder unions#land N NEU PL ACC 
 
Factored output: 
union|union|N|UNK 
l?nder|land|N|NEU_PL_ACC 
 
Figure 2. Compound splitting for ?unionsl?nder? 
(?countries in the union?) 
 
These strategies are quite crude and could be fur-
ther refined by studying the parser output thor-
oughly to pinpoint more problems.  
3.2 Training translation models with linguis-
tic factors 
After pre-processing, the German?English Eu-
roparl training data contains four factors: 0: word 
form, 1: lemma, 2: part-of-speech, 3: morphology. 
As a first step in training our translation models we 
performed word alignment on lemmas as this could 
potentially improve word alignment. 
3.2.1 First setup 
Factored translation requires a number of decoding 
steps, which are either mapping steps mapping a 
source factor to a target factor or generation steps 
generating a target factor from other target factors. 
Our first setup contained three mapping steps, T0?
T2, and one generation step, G0.  
 
 
182
T0: 0-0 (word ? word) 
T1: 1-1 (lemma ? lemma) 
T2: 2,3-2,3  (pos+morph ? pos+morph) 
G0:  1,2,3-0  (lemma+pos+morph ? word)  
 
With the generation step, word forms that did not 
appear in the training data may still get translated 
if the lemma, part-of-speech and morphology can 
be translated separately and the target word form 
can be generated from these factors. 
Word order varies a great deal between German 
and English. This is especially true for the place-
ment of verbs. To model word order changes we 
included part-of-speech information and created 
two reordering models, one based on word form 
(0), the other on part-of-speech (2): 
 
0-0.msd-bidirectional-fe 
2-2.msd-bidirectional-fe 
 
The decoding times for this setup turned out to be 
unmanageable. In the first iteration of parameter 
tuning, decoding times were approx. 6 
min/sentence. In the second iteration decoding 
time increased to approx. 30 min/sentence.  Re-
moving one of the reordering models did not result 
in a significant change in decoding time. Just trans-
lating the 2000 sentences of test data with untuned 
parameters would take several days. We inter-
rupted the tuning and abandoned this setup. 
3.2.2 Second setup 
Because of the excessive decoding times of the 
first factored setup we resorted to a simpler system 
that only used the word form factor for the transla-
tion and reordering models. This setup differs from 
the shared task baseline in the following ways: 
First, it uses the tokenization provided by the 
parser. Second, alignment was performed on the 
lemma factor. Third, German compounds were 
split using the method described above. To speed 
up tuning and decoding, we only used the first 200 
sentences of development data (dev2006) for tun-
ing and reduced stack size to 50.  
 
T0: 0-0 (word ? word) 
R:  0-0.msd-bidirectional-fe 
3.2.3 Third setup 
To test our hypothesis that word reordering would 
benefit from part-of-speech information we created 
another simpler model. This setup has two map-
ping steps, T0 and T1, and a reordering model 
based on part-of-speech.  
 
T0: 0-0 (word ? word) 
T1: 2,3-2,3 (pos+morph ? pos+morph) 
R: 2-2.msd-bidirectional-fe 
4 Results  
We compared our systems to a baseline system 
with the same setup as the WMT2007 shared task 
baseline system but tuned with our system?s sim-
plified tuning settings (200 instead of 2000 tuning 
sentences, stack size 50). Table 1 shows the Bleu 
improvement on the 200 sentences development 
data from the first and last iteration of tuning. 
 
Dev2006 (200) System 
1st iteration Last iteration 
Baseline 19.56 27.07 
First 21.68 - 
Second 20.43 27.16 
Third 20.72 24.72 
 Table 1. Bleu scores on 200 sentences of tuning 
data before and after tuning 
 
The final test of our systems was performed on the 
development test corpus (devtest2006) using stack 
size 50. The results are shown in Table 2. The low 
Bleu score for the third setup implies that reorder-
ing on part-of-speech is not enough on its own. 
The second setup performed best with a slightly 
higher Bleu score than the baseline. We used the 
second setup to translate test data for our submis-
sion to the shared task.  
 
System Devtest2006 (NIST/Bleu) 
Baseline 6.7415 / 25.94  
First - 
Second  6.8036 / 26.04 
Third 6.5504 / 24.57 
Table 2. NIST and Bleu scores on development 
test data 
4.1 Decompounding 
We have evaluated the decompounding strategy by 
analyzing how the first 75 identified noun com-
pounds of the devtest corpus were translated by our 
second setup compared to the baseline. The sample 
183
excluded doubles and compounds that had no clear 
translation in the reference corpus.  
Out of these 75 compounds 74 were nouns that 
were correctly split and 1 was an adjective that was 
split incorrectly: ?allumfass#ende?. Despite that it 
was incorrectly identified and split it was trans-
lated satisfyingly to ?comprehensive?. 
The translations were grouped into the catego-
ries shown in Table 3. The 75 compounds were 
classified into these categories for our second sys-
tem and the baseline system, as shown in Table 4. 
As can be seen the compounds were handled better 
by our system, which had 62 acceptable transla-
tions (C or V) compared to 48 for the baseline and 
did not leave any noun compounds untranslated.  
 
Table 3. Classification scheme with examples for 
compound translations 
 
Table 4. Classification of 75 compounds from our 
second system and the baseline system 
Decompounding of nouns reduced the number 
of untranslated words, but there were still some 
left. Among these were cases that can be handled 
such as separable prefix verbs like ?aufzeigten? 
(?pointed out?) (Niessen and Ney, 2000) or adjec-
tive compounds such as ?multidimensionale? 
(?multi dimensional?). There were also some noun 
compounds left which indicates that we might need 
a better decompounding strategy than the one used 
by the parser (see e.g. Koehn and Knight, 2003). 
4.2 Experiences and future plans  
With the computer equipment at our disposal, 
training of the models and tuning of the parameters 
turned out to be a very time-consuming task. For 
this reason, the number of system setups we could 
test was small, and much fewer than we had hoped 
for. Thus it is too early to draw any conclusions as 
regards our hypotheses, but we plan to perform 
more tests in the future, also on Swedish?English 
data. The parser's ability to identify compounds 
that can be split before training seems to give a 
definite improvement, however, and is a feature 
that can likely be exploited also for Swedish-to-
English translation with Moses. 
References 
Koehn, Philipp and Kevin Knight, 2003. Empirical 
methods for compound splitting. In Proceedings of 
EACL 2003, 187-194. Budapest, Hungary. 
Moses ? a factored phrase-based beam-search decoder 
for machine translation. 13 April 2007,  URL:  
http://www.statmt.org/moses/ . 
Niessen, Sonja and Hermann Ney, 2004. Statistical ma-
chine translation with scarce resources using mor-
pho-syntactic information. Computational Linguis-
tics, 181-204 . 
Niessen, Sonja and Hermann Ney, 2000. Improving 
SMT Quality with Morpho-syntactic Analysis. In 
Proceedings of Coling 2000. 1081-1085. Saar-
br?cken, Germany.  
Popovi?, Maja and Hermann Ney, 2004.  Improving 
Word Alignment Quality using Morpho-Syntactic In-
formation. In Proceedings of Coling 2004, 310-314, 
Geneva, Switzerland. 
Zens, Richard and Hermann Ney, 2006. Discriminative 
Reordering Models for Statistical Machine Transla-
tion. In HLT-NAACL: Proceedings of the Workshop 
on Statistical Machine Translation, 55-63, New York 
City, NY.  
Category Example 
C-correct Regelungsentwurf 
Draft regulation  
Ref: Draft regulation 
V-variant Schlachth?fen 
Abattoirs  
Ref: Slaughter houses  
P-partly correct Anpassungsdruck 
Pressure 
Ref: Pressure for adaption 
F-wrong form L?nderberichte 
Country report  
Ref: Country reports 
W-wrong Erbonkel 
Uncle dna  
Ref: Sugar daddy 
U-untranslated Schlussentwurf 
Schlussentwurf  
Ref: Final draft  
Baseline system 
 C V P W U F Tot 
C 36 1 3  3 1 44 
V 1 9 2 1 5  18 
P   3  2  5 
W    1 2  3 
U       0 
F 1     4 5 S
ec
on
d 
sy
st
em
 
Tot 38 10 8 2 12 5 75 
184
Proceedings of the Third Workshop on Statistical Machine Translation, pages 135?138,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Effects of Morphological Analysis in Translation between German and
English
Sara Stymne, Maria Holmqvist and Lars Ahrenberg
Department of Computer and Information Science
Linko?ping University, Sweden
{sarst,marho,lah}@ida.liu.se
Abstract
We describe the LIU systems for German-
English and English-German translation sub-
mitted to the Shared Task of the Third Work-
shop of Statistical Machine Translation. The
main features of the systems, as compared
with the baseline, is the use of morphologi-
cal pre- and post-processing, and a sequence
model for German using morphologically rich
parts-of-speech. It is shown that these addi-
tions lead to improved translations.
1 Introduction
Research in statistical machine translation (SMT)
increasingly makes use of linguistic analysis in order
to improve performance. By including abstract cat-
egories, such as lemmas and parts-of-speech (POS),
in the models, it is argued that systems can become
better at handling sentences for which training data
at the word level is sparse. Such categories can be
integrated in the statistical framework using factored
models (Koehn et al, 2007). Furthermore, by pars-
ing input sentences and restructuring based on the
result to narrow the structural difference between
source and target language, the current phrase-based
models can be used more effectively (Collins et al,
2005).
German differs structurally from English in sev-
eral respects (see e.g. Collins et al, 2005). In this
work we wanted to look at one particular aspect
of restructuring, namely splitting of German com-
pounds, and evaluate its effect in both translation di-
rections, thus extending the initial experiments re-
ported in Holmqvist et al (2007). In addition, since
German is much richer in morphology than English,
we wanted to test the effects of using a sequence
model for German based on morphologically sub-
categorized parts-of-speech. All systems have been
specified as extensions of the Moses system pro-
vided for the Shared Task.
2 Part-of-speech and Morphology
For both English and German we used the part-of-
speech tagger TreeTagger (Schmid, 1994) to obtain
POS-tags.
The German POS-tags from TreeTagger were re-
fined by adding morphological information from
a commercial dependency parser, including case,
number, gender, definiteness, and person for nouns,
pronouns, verbs, adjectives and determiners in the
cases where both tools agreed on the POS-tag. If
they did not agree, the POS-tag from TreeTagger
was chosen. This tag set seemed more suitable for
SMT, with tags for proper names and foreign words
which the commercial parser does not have.
3 Compound Analysis
Compounding is common in many languages, in-
cluding German. Since compounding is highly pro-
ductive it increases vocabulary size and leads to
sparse data problems.
Compounds in German are formed by joining
words, and in addition filler letters can be inserted
or letters can be removed from the end of all but the
last word of the compound (Langer, 1998). We have
chosen to allow simple additions of letter(s) (-s, -n,
-en, -nen, -es, -er, -ien) and simple truncations (-e,
135
-en, -n). Example of compounds with additions and
truncations can be seen in (1).
(1) a. Staatsfeind (Staat + Feind)
public enemy
b. Kirchhof (Kirche + Hof)
graveyard
3.1 Splitting compounds
Noun and adjective compounds are split by a mod-
ified version of the corpus-based method presented
by Koehn and Knight (2003). First the German lan-
guage model data is POS-tagged and used to calcu-
late frequencies of all nouns, verbs, adjectives, ad-
verbs and the negative particle. Then, for each noun
and adjective all splits into these known words from
the corpus, allowing filler additions and truncations,
are considered, choosing the splitting option with
the highest arithmetic mean1 of the frequencies of
its parts.
A length limit of each part was set to 4 charac-
ters. For adjectives we restrict the number of parts
to maximum two, since they do not tend to have
multiple parts as often as nouns. In addition we
added a stop list with 14 parts, often mistagged, that
gave rise to wrong adjective splits, such as arische
(?Aryan?) in konsularische (?consular?).
As Koehn and Knight (2003) points out, parts of
compounds do not always have the same meaning
as when they stand alone, e.g. Grundrechte (?basic
rights?), where the first part, Grund, usually trans-
lates as foundation, which is wrong in this com-
pound. To overcome this we marked all compound
parts but the last, with the symbol ?#?. Thus they are
handled as separate words. Parts of split words also
receive a special POS-tag, based on the POS of the
last word of the compound, and the last part receives
the same POS as the full word.
We also split words containing hyphens based on
the same algorithm. Their parts receive a different
POS-tag, and the hyphens are left at the end of all
but the last part.
1We choose the arithmetic mean over the geometric mean
used by Koehn and Knight (2003) in order to increase the num-
ber of splits.
3.2 Merging compounds
For translation into German, the translation output
contains split compounds, which need to be merged.
An algorithm for merging has been proposed by
Popovic? et al (2006) using lists of compounds and
their parts. This method cannot merge unseen com-
pounds, however, so instead we base merging on
POS. If a word has a compound-POS, and the fol-
lowing word has a matching POS, they are merged.
If the next POS does not match, a hyphen is added
to the word, allowing for coordinated compounds as
in (2).
(2) Wasser- und Bodenqualita?t
water and soil quality
4 System Descriptions
The main difference of our system in relation to the
baseline system of the Shared Task2 is the pre- and
post-processing described above, the use of a POS
factor, and an additional sequence model on POS.
We also modified the tuning to include compound
merging, and used a smaller corpus, 600 sentences
picked evenly from the dev2006 corpus, for tuning.
We use the Moses decoder (Koehn et al, 2007) and
SRILM language models (Stolcke, 2002).
4.1 German ? English
We used POS as an output factor, as can be seen in
Figure 1. Using additional factors only on the tar-
get side means that only the training data need to be
POS-tagged, not the tuning data or translation input.
However, POS-tagging is still performed for Ger-
man as input to the pre-processing step. As Figure 1
shows we have two sequence models. A 5-gram lan-
guage model based on surface form using Kneser-
Ney smoothing and in addition a 7-gram sequence
model based on POS using Witten-Bell3 smoothing.
The training corpus was filtered to sentences with
2?40 words, resulting in a total of 1054688 sen-
tences. Training was done purely on Europarl data,
but results were submitted both on Europarl and
2http://www.statmt.org/wmt08/baseline.
html
3Kneser-Ney smoothing can not be used for the POS se-
quence model, since there were counts-of-counts of zero. How-
ever, Witten-Bell smoothing gives good results when the vocab-
ulary is small.
136
7?gram
POSPOS
wordword
Source Target
5?gram
word
Factors Sequence
models
Figure 1: Architecture of the factored system
News data. The news data were submitted to see
how well a pure out-of-domain system could per-
form.
In the pre-processing step compounds were split.
This was done for training, tuning and translation.
In addition German contracted prepositions and de-
terminers, such as zum from zu dem (?to the?), when
identified as such by the tagger, were split.
4.2 English ? German
All features of the German to English system were
used, and in addition more fine-grained German
POS-tags that were sub-categorized for morpholog-
ical features. This was done for training, tuning
and sequence models. At translation time no pre-
processing was needed for the English input, but a
post-processing step for the German output is re-
quired, including the merging of compounds and
contracted prepositions and determiners. The latter
was done in connection with uppercasing, by train-
ing an instance of Moses on a lower cased corpus
with split contractions and an upper-cased corpus
with untouched contractions. The tuning step was
modified so that merging of compounds were done
as part of the tuning.
4.3 Baseline
For comparison, we constructed a baseline accord-
ing to the shared-task description, but with smaller
tuning corpus, and the same sentence filtering for the
translation model as in the submitted system, using
only sentences of length 2-40.
In addition we constructed a factored baseline
system, with POS as an output factor and a se-
quence model for POS. Here we only used the orig-
inal POS-tags from TreeTagger, no additional mor-
phology was added for German.
De-En En-De
Baseline 26.95 20.16
Factored baseline 27.43 20.27
Submitted system 27.63 20.46
Table 1: Bleu scores for Europarl (test2007)
De-En En-De
Baseline 19.54 14.31
Factored baseline 20.16 14.37
Submitted system 20.61 14.77
Table 2: Bleu scores for News Commentary (nc-test2007)
5 Results
Case-sensitive Bleu scores4 (Papineni et al, 2002)
for the Europarl devtest set (test2007) are shown in
table 1. We can see that the submitted system per-
forms best, and that the factored baseline is better
than the pure baseline, especially for translation into
English.
Bleu scores for News Commentary5 (nc-test2007)
are shown in Table 2. Here we can also see that the
submitted system is the best. As expected, Bleu is
much lower on out-of-domain news text than on the
Europarl development test set.
5.1 Compounds
The quality of compound translations were analysed
manually. The first 100 compounds that could be
found by the splitting algorithm were extracted from
the Europarl reference text, test2007, together with
their English translations6 .
System translations were compared to the an-
notated compounds and classified into seven cate-
gories: correct, alternative good translation, correct
but different form, part of the compound translated,
no direct equivalent, wrong and untranslated. Out
of these the first three categories can be considered
good translations.
We performed the error analysis for the submitted
and the baseline system. The result can be seen in
4The %Bleu notation is used in this report
5No development test set for News test were provided, so we
present result for the News commentary, which can be expected
to give similar results.
6The English translations need not be compounds. Com-
pounds without a clear English translation were skipped.
137
De ? En En ? De
Subm Base Subm Base
Correct 50 46 40 39
Alternative 36 26 32 29
Form 5 7 6 8
Part 2 5 10 15
No equivalent 6 2 8 5
Wrong 1 7 1 1
Untranslated ? 7 3 3
Table 3: Results of the error analysis of compound trans-
lations
Table 3. For translation into English the submitted
system handles compound translations considerably
better than the baseline with 91% good translations
compared to 79%. In the submitted system all com-
pounds have a translation, compared to the baseline
system which has 7% of the compounds untrans-
lated. In the other translation direction the difference
is smaller, the biggest difference is that the submit-
ted system has fewer cases of partial translation.
5.2 Agreement in German NPs
To study the effects of using fine-grained POS-tags
in the German sequence model, a similar close study
of German NPs was performed. 100 English NPs
having at least two dependents of the head noun
were selected from a randomly chosen subsection
of the development test set. Their translations in
the baseline and submitted system were then identi-
fied. Translations that were not NPs were discarded.
In about two thirds (62 out of 99) of the cases, the
translations were identical. For the remainder, 12
translations were of equal quality, the submitted sys-
tem had a better translation in 17 cases (46%), and a
worse one in 8 cases (22%). In the majority of cases
where the baseline was better, this was due to word
selection, not agreement.
6 Conclusions
Adding morphological processing improved trans-
lation results in both directions for both text types.
Splitting compounds gave a bigger effect for trans-
lation from German. Marking of compound parts
worked well, with no untranslated parts left in the
sample used for evaluation. The mini-evaluation
of German NPs in English-German translation in-
dicates that the morphologically rich POS-based se-
quence model for German also had a positive effect.
Acknowledgement
We would like to thank Joe Steinhauer for help with
the evaluation of German output.
References
M. Collins, P. Koehn, and I. Kuc?erova?. 2005. Clause re-
structuring for statistical machine translation. In Pro-
ceedings of the 43rd Annual Meeting of the ACL, pages
531?540, Ann Arbor, Michigan.
M. Holmqvist, S. Stymne, and L. Ahrenberg. 2007. Get-
ting to know Moses: Initial experiments on German-
English factored translation. In Proceedings of the
Second Workshop on Statistical Machine Translation,
pages 181?184, Prague, Czech Republic. Association
for Computational Linguistics.
P. Koehn and K. Knight. 2003. Empirical methods for
compound splitting. In Proceedings of the tenth con-
ference of EACL, pages 187?193, Budapest, Hungary.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit for
statistical machine translation. In Proceedings of the
45th Annual Meeting of the ACL, demonstration ses-
sion, Prague, Czech Republic.
S. Langer. 1998. Zur Morphologie und Semantik von
Nominalkomposita. In Tagungsband der 4. Konferenz
zur Verarbeitung natu?rlicher Sprache (KONVENS),
pages 83?97.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. In Proceedings of the 40th Annual Meet-
ing of the ACL, pages 311?318, Philadelphia, Pennsyl-
vania.
M. Popovic?, D. Stein, and H. Ney. 2006. Statistical ma-
chine translation of German compound words. In Pro-
ceedings of FinTAL - 5th International Conference on
Natural Language Processing, pages 616?624, Turku,
Finland.
H. Schmid. 1994. Probabilistic part-of-speech tagging
using decision trees. In Preoceedings of the Interna-
tional Conference on New Methods in Language Pro-
cessing, Manchester, UK.
A. Stolcke. 2002. SRILM - an extensible language
modeling toolkit. In Proceedings of the International
Conference on Spoken Language Processing (ICSLP),
pages 901?904, Denver, Colorado.
138
Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 120?124,
Athens, Greece, 30 March ? 31 March 2009. c?2009 Association for Computational Linguistics
Improving alignment for SMT by reordering
and augmenting the training corpus
Maria Holmqvist, Sara Stymne, Jody Foo and Lars Ahrenberg
Department of Computer and Information Science
Link?ping University, Sweden
{marho,sarst,jodfo,lah}@ida.liu.se
Abstract
We describe the LIU systems for English-
German and German-English translation
in the WMT09 shared task. We focus on
two methods to improve the word align-
ment: (i) by applying Giza++ in a sec-
ond phase to a reordered training cor-
pus, where reordering is based on the
alignments from the first phase, and (ii)
by adding lexical data obtained as high-
precision alignments from a different word
aligner. These methods were studied in
the context of a system that uses com-
pound processing, a morphological se-
quence model for German, and a part-
of-speech sequence model for English.
Both methods gave some improvements to
translation quality as measured by Bleu
and Meteor scores, though not consis-
tently. All systems used both out-of-
domain and in-domain data as the mixed
corpus had better scores in the baseline
configuration.
1 Introduction
It is an open question whether improved word
alignment actually improves statistical MT. Fraser
and Marcu (2007) found that improved alignments
as measured by AER will not necessarily improve
translation quality, whereas Ganchev et al (2008)
did improve translation quality on several lan-
guage pairs by extending the alignment algorithm.
For this year?s shared task we therefore stud-
ied the effects of improving word alignment in the
context of our system for the WMT09 shared task.
Two methods were tried: (i) applying Giza++ in
a second phase to a reordered training corpus,
where reordering is based on the alignments from
the first phase, and (ii) adding lexical data ob-
tained as high-precision alignments from a differ-
ent word aligner. The submitted system includes
the first method in addition to the processing of
compounds and additional sequence models used
by Stymne et al (2008). Heuristics were used
to generate true-cased versions of the translations
that were submitted, as reported in section 6.
In this paper we report case-insensitive Bleu
scores (Papineni et al, 2002), unless otherwise
stated, calculated with the NIST tool, and case-
insensitive Meteor-ranking scores, without Word-
Net (Agarwal and Lavie, 2008).
2 Baseline system
Our baseline system uses compound split-
ting, compound merging and part-of-
speech/morphological sequence models (Stymne
et al, 2008). Except for these additions it is
similar to the baseline system of the workshop1.
The translation system is a factored phrase-
based translation system that uses the Moses
toolkit (Koehn et al, 2007) for decoding and train-
ing, GIZA++ for word alignment (Och and Ney,
2003), and SRILM (Stolcke, 2002) for language
models. Minimum error rate training was used to
tune the model feature weights (Och, 2003).
Tuning was performed on the news-dev2009a
set with 1025 sentences. All development test-
ing was performed on the news-dev2009b set with
1026 sentences.
2.1 Sequence model based on part-of-speech
and morphology
The translation models were factored with one ad-
ditional output factor. For English we used part-
of-speech tags obtained with TreeTagger (Schmid,
1994). For German we enriched the tags from
TreeTagger with morphological information, such
as case or tense, that we get from a commercial
1http://www.statmt.org/wmt09/baseline.
html
120
dependency parser2.
We used the extra factor in an additional se-
quence model which can improve agreement be-
tween words, and word order. For German this
factor was also used for compound merging.
2.2 Compound processing
Prior to training and translation, compound pro-
cessing was performed using an empirical method
based on (Koehn and Knight, 2003; Stymne,
2008). Words were split if they could be split
into parts that occur in a monolingual corpus. We
chose the split with the highest arithmetic mean
of the corpus frequencies of compound parts. We
split nouns, adjectives and verbs into parts that
were content words or particles. A part had to
be at least 3 characters in length and a stop list
was used to avoid parts that often lead to errors,
such as arische (Aryan) in konsularische (con-
sular). Compound parts sometimes have special
compound suffixes, which could be additions or
truncations of letters, or combinations of these.
We used the top 10 suffixes from a corpus study
of Langer (1998), and we also treated hyphens as
suffixes of compound parts. Compound parts were
given a special part-of-speech tag that matched the
head word.
For translation into German, compound parts
were merged to form compounds, both during test
and tuning. The merging is based on the spe-
cial part-of-speech tag used for compound parts
(Stymne, 2009). A token with this POS-tag is
merged with the next token, either if the POS-tags
match, or if it results in a known word.
3 Domain adaptation
This year three training corpora were available, a
small bilingual news commentary corpus, a rea-
sonably large Europarl corpus, and a very large
monolingual news corpus, see Table 1 for details.
The bilingual data was filtered to remove sen-
tences longer than 60 words. Because the German
news training corpus contained a number of En-
glish sentences, this corpus was cleaned by remov-
ing sentences containing a number of common En-
glish words.
Based on Koehn and Schroeder (2007) we
adapted our system from last year, which was fo-
cused on Europarl, to perform well on test data
2Machinese syntax, from Connexor Oy http://www.
connexor.eu
Corpus German English
news-commentary09 81,141
Europarl 1,331,262
news-train08 9,619,406 21,215,311
Table 1: Number of sentences in the corpora (after
filtering)
Corpus En?De De?En
Bleu Meteor Bleu Meteor
News com. 12.13 47.01 17.21 36.08
Europarl 12.92 47.27 18.53 37.65
Mixed 12.91 47.96 18.76 37.69
Mixed+ 14.62 49.48 19.92 38.18
Table 2: Results of domain adaptation
from the news domain. We used the possibility
to include several translation models in the Moses
decoder by using multiple alternative decoding
paths. We first trained systems on either bilingual
news data or Europarl. Then we trained a mixed
system, with two translation models one from each
corpus, a language model from the bilingual news
data, and a Europarl reordering model. The mixed
system was slightly better than the Europarl only
system. All sequence models used 5-grams for
surface form and 7-grams for part-of-speech. All
scores are shown in Table 2.
We wanted to train sequence models on the
large monolingual corpora, but due to limited
computer resources, we had to use a lower order
for this, than on the small corpus. Thus our se-
quence models on this data has lower order than
those trained on bilingual news or Europarl, with
4-grams for surface form and 6-grams for part-
of-speech. We also used the entropy-based prun-
ing included in the SRILM toolkit, with 10?8 as
a threshold. Using these sequence models in the
mixed model, called mixed+, improved the results
drastically, as shown in Table 2.
The other experiments reported in this paper are
based on the mixed+ system.
4 Improved alignment by reordering
Word alignment with Giza++ has been shown to
improve from making the source and target lan-
guage more similar, e.g., in terms of segmentation
(Ma et al, 2007) or word order.
We used the following simple procedure to im-
prove alignment of the training corpus by reorder-
ing the words in one of the texts according to the
121
Corpus En?De De?En
Bleu Meteor Bleu Meteor
Mixed+ 14.62 49.48 19.92 38.18
Re-Src 14.63 49.80 20.54 38.86
Re-Trg 14.51 48.62 20.48 38.73
Table 3: Results of reordering experiments
word order in the other language:
1. Word align the corpus with Giza++.
2. Reorder the German words according to the
order of the English words they are aligned
to. (This is a common step in approaches that
extract reordering rules for translation. How-
ever, this is not what we use it for here.)
3. Word align the reordered German and origi-
nal English corpus with Giza++.
4. Put the reordered German words back into
their original position and adjust the align-
ments so that the improved alignment is pre-
served.
After this step we will have a possibly improved
alignment compared to the original Giza++ align-
ment. A phrase table was extracted from the align-
ment and training was performed as usual. The re-
ordering procedure was carried out on both source
(Re-Src) and target data (Re-Trg) and the results
of translating devtest data using these alignments
are shown in Table 3.
Compared with our baseline (mixed+), Bleu
and Meteor increased for the translation direction
German?English. Both source reordering and tar-
get reordering resulted in a 0.6 increase in Bleu.
For translation into German, source reordering
resulted in a somewhat higher Meteor score, but
overall did not seem to improve translation. Tar-
get reordering in this direction resulted in lower
scores.
It is not clear why reordering improved trans-
lation for German?English and not for English?
German. In all experiments, the heuristic sym-
metrization of directed Giza++ alignments was
performed in the intended translation direction 3.
3Our experiments show that symmetrization in the wrong
translation direction will result in lower translation quality
scores.
5 Augmenting the corpus with an
extracted dictionary
Previous research (Callison-Burch et al, 2004;
Fraser and Marcu, 2006) has shown that includ-
ing word aligned data during training can improve
translation results. In our case we included a dic-
tionary extracted from the news-commentary cor-
pus during the word alignment.
Using a method originally developed for term
extraction (Merkel and Foo, 2007), the news-
commentary09 corpus was grammatically anno-
tated and aligned using a heuristic word aligner.
Candidate dictionary entries were extracted from
the alignments. In order to optimize the qual-
ity of the dictionary, dictionary entry candidates
were ranked according to their Q-value, a metric
specifically designed for aligned data (Merkel and
Foo, 2007). The Q-value is based on the following
statistics:
? Type Pair Frequencies (TPF), i.e. the number
of times where the source and target types are
aligned.
? Target types per Source type (TpS), i.e. the
number of target types a specific source type
has been aligned to.
? Source types per Target type (SpT), i.e. the
number of source types a specific target type
has been aligned to.
The Q-value is calculated as
Q?value= TPFTpS+SpT . A high Q-value indi-
cates a dictionary candidate pair with a relatively
low number of translation variations. The candi-
dates were filtered using a Q-value threshold of
0.333, resulting in a dictionary containing 67287
entries.
For the experiments, the extracted dictionary
was inserted 200 times into the corpus used dur-
ing word alignment. The added dictionary entries
were removed before phrase extraction. Experi-
ments using the extracted dictionary as an addi-
tional phrase table were also run, but did not result
in any improvement of translation quality.
The results can be seen in Table 4. There was
no evident pattern how the inclusion of the dictio-
nary during alignment (DictAl) affected the trans-
lation quality. The inclusion of the dictionary pro-
duced both higher and lower Bleu scores than the
122
Corpus En?De De?En
Bleu Meteor Bleu Meteor
Mixed+ 14.62 49.48 19.92 38.18
DictAl 14.73 49.39 18.93 37.71
Table 4: Results of domain adaptation
Corpus En?De De?En
Mixed+ 13.31 17.47
with OOV 13.74 17.96
Table 5: Case-sensitive Bleu scores
baseline system depending on the translation di-
rection. Meteor scores were however consistently
lower than the baseline system.
6 Post processing of out-of-vocabulary
words
In the standard systems all out-of-vocabulary
words are transferred as is from the translation in-
put to the translation output. Many of these words
are proper names, which do not get capitalized
properly, or numbers, which have different for-
matting in German and English. We used post-
processing to improve this.
For all unknown words we capitalized either the
first letter, or all letters, if they occur in that form
in the translation input. For unknown numbers
we switched between the German decimal comma
and the English decimal point for decimal num-
bers. For large numbers, English has a comma
to separate thousands, and German has a period.
These were also switched. This improved case-
sensitive Bleu scores in both translation directions,
see Table 5.
7 Submitted system
For both translation directions De-En and En-De
we submitted a system with two translation mod-
els trained on bilingual news and Europarl. The
alignment was improved by using the reordering
techniques described in section 4. The systems
also use all features described in this paper except
for the lexical augmentation (section 5) which did
not result in significant improvement. The results
of the submitted systems on devtest data are bold-
faced in Table 3.
Corpus En?De De?En
All 14.63 20.54
En-De orig. 19.93 26.82
Other set 11.66 16.17
Table 6: Bleu scores for the reordered systems on
two sections of development set news-dev2009b.
NIST scores show the same distribution.
8 Results on two sections of devtest data
Comparisons of translation output with reference
translations on devtest data showed some surpris-
ing differences, which could be attributed to cor-
responding differences between source and refer-
ence data. The differences were not evenly dis-
tributed but especially frequent in those sections
where the original language was something other
than English or German. To check the homogene-
ity of the devtest data we divided it into two sec-
tions, one for documents of English or German
origin, and the other for the remainder. It turned
out that scores were dramatically different for the
two sections, as shown in Table 6.
The reason for the difference is likely to be that
only the En-De set contains source texts and trans-
lations, while the other section contains parallel
translations from the same source. This suggests
that it would be interesting to study the effects of
splitting the training corpus in the same way be-
fore training.
9 Conclusion
The results of augmenting the training corpus with
an extracted lexicon were inconclusive. How-
ever, the alignment reordering improved transla-
tion quality, especially in the De?En direction.
The result of these reordering experiments indi-
cates that better word alignment quality will im-
prove SMT. The reordering method described in
this paper also has the advantage of only requir-
ing two runs of Giza++, no additional resources or
training is necessary to get an improved alignment.
References
Abhaya Agarwal and Alon Lavie. 2008. Meteor, M-
BLEU and M-TER: Evaluation metrics for high-
correlation with human rankings of machine trans-
lation output. In Proceedings of the Third Workshop
on Statistical Machine Translation, pages 115?118,
Columbus, Ohio.
123
Chris Callison-Burch, David Talbot, and Miles Os-
borne. 2004. Statistical machine translation with
word- and sentence-aligned parallel corpora. In Pro-
ceedings of the 42nd Annual Meeting of ACL, pages
175?182, Barcelona, Spain.
Alexander Fraser and Daniel Marcu. 2006. Semi-
supervised training for statistical word alignment. In
Proceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meeting
of ACL, pages 769?776, Sydney, Australia.
Alexander Fraser and Daniel Marcu. 2007. Measuring
word alignment quality for statistical machine trans-
lation. Computational Linguistics, 33(3):293?303.
Kuzman Ganchev, Jo?o de Almeida Varelas Gra?a, and
Ben Taskar. 2008. Better alignments = better trans-
lations? In Proceedings of the 46th Annual Meeting
of ACL, pages 986?993, Columbus, Ohio.
Philipp Koehn and Kevin Knight. 2003. Empirical
methods for compound splitting. In Proceedings of
the tenth conference of EACL, pages 187?193, Bu-
dapest, Hungary.
Philipp Koehn and Josh Schroeder. 2007. Experi-
ments in domain adaptation for statistical machine
translation. In Proceedings of the Second Workshop
on Statistical Machine Translation, pages 224?227,
Prague, Czech Republic.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In Proceedings of the 45th Annual Meeting of
ACL, demonstration session, Prague, Czech Repub-
lic.
Stefan Langer. 1998. Zur Morphologie und Seman-
tik von Nominalkomposita. In Tagungsband der
4. Konferenz zur Verarbeitung nat?rlicher Sprache
(KONVENS), pages 83?97.
Yanjun Ma, Nicolas Stroppa, and Andy Way. 2007.
Boostrapping word alignment via word packing. In
Proceedings of the 45th Annual Meeting of ACL,
pages 304?311, Prague, Czech Republic.
Magnus Merkel and Jody Foo. 2007. Terminology
extraction and term ranking for standardizing term
banks. In Proceedings of the 16th Nordic Con-
ference of Computational Linguistics (NODALIDA-
2007), pages 349?354, Tartu, Estonia.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting of ACL, pages 160?167, Sap-
poro, Japan.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of the 40th Annual Meeting of ACL, pages 311?318,
Philadelphia, Pennsylvania.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In International Con-
ference on New Methods in Language Processing,
pages 44?49, Manchester, UK.
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In Proceedings of the Sev-
enth International Conference on Spoken Language
Processing (ICSLP), pages 901?904, Denver, Col-
orado.
Sara Stymne, Maria Holmqvist, and Lars Ahrenberg.
2008. Effects of morphological analysis in transla-
tion between German and English. In Proceedings
of the Third Workshop on Statistical Machine Trans-
lation, pages 135?138, Columbus, Ohio.
Sara Stymne. 2008. German compounds in factored
statistical machine translation. In Aarne Ranta and
Bengt Nordstr?m, editors, Proceedings of GoTAL,
6th International Conference on Natural Language
Processing, LNCS/LNAI Volume 5221, pages 464?
475.
Sara Stymne. 2009. A comparison of merging strate-
gies for translation of German compounds. In Pro-
ceedings of the EACL09 Student Research Work-
shop, Athens, Greece.
124
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 183?188,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Vs and OOVs: Two Problems for Translation between German and
English
Sara Stymne, Maria Holmqvist, Lars Ahrenberg
Linko?ping University
Sweden
{sarst,marho,lah}@ida.liu.se
Abstract
In this paper we report on experiments
with three preprocessing strategies for im-
proving translation output in a statistical
MT system. In training, two reordering
strategies were studied: (i) reorder on the
basis of the alignments from Giza++, and
(ii) reorder by moving all verbs to the
end of segments. In translation, out-of-
vocabulary words were preprocessed in a
knowledge-lite fashion to identify a likely
equivalent. All three strategies were im-
plemented for our English?German sys-
tem submitted to the WMT10 shared task.
Combining them lead to improvements in
both language directions.
1 Introduction
We present the Liu translation system for the con-
strained condition of the WMT10 shared transla-
tion task, between German and English in both di-
rections. The system is based on the 2009 Liu sub-
mission (Holmqvist et al, 2009), that used com-
pound processing, morphological sequence mod-
els, and improved alignment by reordering.
This year we have focused on two issues: trans-
lation of verbs, which is problematic for transla-
tion between English and German since the verb
placement is different with German verbs often be-
ing placed at the end of sentences; and OOVs, out-
of-vocabulary words, which are problematic for
machine translation in general. Verb translation
is targeted by trying to improve alignment, which
we believe is a crucial step for verb translation
since verbs that are far apart are often not aligned
at all. We do this mainly by moving verbs to the
end of sentences previous to alignment, which we
also combine with other alignments. We trans-
form OOVs into known words in a post-processing
step, based on casing, stemming, and splitting of
hyphenated compounds. In addition, we perform
general compound splitting for German both be-
fore training and translation, which also reduces
the OOV rate.
All results in this article are for the develop-
ment test set newstest2009, on truecased output.
We report Bleu scores (Papineni et al, 2002) and
Meteor ranking (without WordNet) scores (Agar-
wal and Lavie, 2008), using percent notation. We
also used other metrics, but as they gave similar
results they are not reported. For significance test-
ing we used approximate randomization (Riezler
and Maxwell, 2005), with p < 0.05.
2 Baseline System
The 2010 Liu system is based on the PBSMT base-
line system for the WMT shared translation task1.
We use the Moses toolkit (Koehn et al, 2007) for
decoding and to train translation models, Giza++
(Och and Ney, 2003) for word alignment, and the
SRILM toolkit (Stolcke, 2002) to train language
models. The main difference to the WMT base-
line is that the Liu system is trained on truecased
data, as in Koehn et al (2008), instead of lower-
cased data. This means that there is no need for a
full recasing step after translation, instead we only
need to uppercase the first word in each sentence.
2.1 Corpus
We participated in the constrained task, where we
only trained the Liu system on the news and Eu-
roparl corpora provided for the workshop. The
translation and reordering models were trained us-
ing the bilingual Europarl and news commentary
corpora, which we concatenated.
We used two sets of language models, one
where we first trained two models on Europarl
and news commentary, which we then interpolated
1http://www.statmt.org/wmt10/baseline.
html
183
with more weight given to the news commentary,
using weights from Koehn and Schroeder (2007).
The second set of language models were trained
on monolingual news data. For tuning we used
every second sentence, in total 1025 sentences, of
news-test2008.
2.2 Training with Limited Computational
Resources
One challenge for us was to train the transla-
tion sytem with limited computational resources.
We trained all systems on one Intel Core 2 CPU,
3.0Ghz, 16 Gb of RAM, 64 bit Linux (RedHat)
machine. This constrained the possibilities of us-
ing the data provided by the workshop to the full.
The main problem was training the language mod-
els, since the monolingual data was very large
compared to the bilingual data.
In order to train language models that were both
fast at runtime, and possible to train with the avail-
able memory, we chose to use the SRILM toolkit
(Stolcke, 2002), with entropy-based pruning, with
10?8 as a threshold. To reduce the model size we
also used lower order models for the large corpus;
4-grams instead of 5-grams for words and 6-grams
instead of 7-grams for the morphological models.
It was still impossible to train on the monolingual
English news corpus, with nearly 50 million sen-
tences, so we split that corpus into three equal size
parts, and trained three models, that were interpo-
lated with equal weights.
3 Morphological Processing
We added morphological processing to the base-
line system, by training additional sequence mod-
els on morphologically enriched part-of-speech
tags, and by compound processing for German.
We utilized the factored translation framework
in Moses, to enrich the baseline system with an
additional target sequence model. For English
we used part-of-speech tags obtained using Tree-
Tagger (Schmid, 1994), enriched with more fine-
grained tags for the number of determiners, in or-
der to target more agreement issues, since nouns
already have number in the tagset. For German
we used morphologically rich tags from RFTag-
ger (Schmid and Laws, 2008), that contains mor-
phological information such as case, number, and
gender for nouns and tense for verbs. We used
the extra factor in an additional sequence model
on the target side, which can improve word order
System Bleu Meteor
Baseline 13.42 48.83
+ morph 13.85 49.69
+ comp 14.24 49.41
Table 1: Results for morphological processing,
English?German
System Bleu Meteor
Baseline 18.34 38.13
+ morph 18.39 37.86
+ comp 18.50 38.47
Table 2: Results for morphological processing,
German?English
and agreement between words. For German the
factor was also used for compound merging.
Prior to training and translation, compound pro-
cessing was performed, using an empirical method
(Koehn and Knight, 2003; Stymne, 2008) that
splits words if they can be split into parts that oc-
cur in a monolingual corpus, choosing the split-
ting option with the highest arithmetic mean of its
part frequencies in the corpus. We split nouns,
adjectives and verbs, into parts that are content
words or particles. We imposed a length limit on
parts of 3 characters for translation from German
and of 6 characters for translation from English,
and we had a stop list of parts that often led to
errors, such as arische (Aryan) in konsularische
(consular). We allowed 10 common letter changes
(Langer, 1998) and hyphens at split points. Com-
pound parts were given a special part-of-speech
tag that matches the head word.
For translation into German, compound parts
were merged into full compounds using a method
described in Stymne and Holmqvist (2008), which
is based on matching of the special part-of-speech
tag for compound parts. A word with a compound
POS-tag were merged with the next word, if their
POS-tags were matching.
Tables 1 and 2 show the results of the addi-
tional morphological processing. Adding the se-
quence models on morphologically enriched part-
of-speech tags gave a significant improvement for
translation into German, but similar or worse re-
sults as the baseline for translation into English.
This is not surprising, since German morphology
is more complex than English morphology. The
addition of compound processing significantly im-
proved the results on Meteor for translation into
184
English, and it also reduced the number of OOVs
in the translation output by 20.8%. For translation
into German, compound processing gave a signif-
icant improvement on both metrics compared to
the baseline, and on Bleu compared to the system
with morphological sequence models. Overall, we
believe that both compound splitting and morphol-
ogy are useful; thus all experiments reported in the
sequel are based on the baseline system with mor-
phology models and compound splitting, which
we will call base.
4 Improved Alignment by Reordering
Previous work has shown that translation quality
can be improved by making the source language
more similar to the target language, for instance
in terms of word order (Wang et al, 2007; Xia
and McCord, 2004). In order to harmonize the
word order of the source and target sentence, they
applied hand-crafted or automatically induced re-
ordering rules to the source sentences of the train-
ing corpus. At decoding time, reordering rules
were again applied to input sentences before trans-
lation. The positive effects of such methods seem
to come from a combination of improved align-
ment and improved reordering during translation.
In contrast, we focus on improving the word
alignment by reordering the training corpus. The
training corpus is reordered prior to word align-
ment with Giza++ (Och and Ney, 2003) and then
the word links are re-adjusted back to the original
word positions. From the re-adjusted corpus, we
create phrase tables that allow translation of non-
reordered input text. Consequently, our reordering
only affects the word alignment and the phrase ta-
bles extracted from it.
We investigated two ways of reordering. The
first method is based on word alignments and the
other method is based on moving verbs to sim-
ilar positions in the source and target sentences.
We also investigated different combinations of re-
orderings and alignments. All results for the sys-
tems with improved reordering are shown in Ta-
bles 3 and 4.
4.1 Reordering Based on Alignments
The first reordering method does not require any
syntactic information or rules for reordering. We
simply used symmetrized Giza++ word align-
ments to reorder the words in the source sentences
to reflect the target word order and applied Giza++
System Bleu Meteor
base 14.24 49.41
reorder 14.32 49.58
verb 13.93 49.22
base+verb 14.38 49.72
base+verb+reorder 14.39 49.39
Table 3: Results for improved alignment,
English?German
System Bleu Meteor
base 18.50 38.47
reorder 18.77 38.53
verb 18.61 38.53
base+verb 18.66 38.61
base+verb+reorder 18.73 38.59
Table 4: Results for improved alignment,
German?English
again to the reordered training corpus. The follow-
ing steps were performed to produce the final word
alignment:
1. Word align the training corpus with Giza++.
2. Reorder the source words according to the or-
der of the target words they are aligned to
(store the original source word positions for
later).
3. Word align the reordered source and original
target corpus with Giza++.
4. Re-adjust the new word alignments so that
they align source and target words in the orig-
inal corpus.
The system built on this word alignment (re-
order) had a significant improvement in Bleu score
over the unreordered baseline (base) for transla-
tion into English, and small improvements other-
wise.
4.2 Verb movement
The positions of finite verbs are often very differ-
ent in English and German, where they are often
placed at the end of sentences. In several cases we
noted that finite verbs were misaligned by Giza++.
To improve the alignment of verbs, we moved all
verbs in both English and German to the end of the
sentences prior to word alignment. The reordered
sentences were word aligned with Giza++ and the
185
resulting word links were then re-adjusted to align
words in the original corpus.
The system created from this alignment (verb)
resulted in significantly lower scores than base for
translation into German, and similar scores as base
for translation into English.
4.3 Combination Systems
The alignment based on reordered verbs did not
produce a better alignment in terms of Bleu scores
of the resulting translations, which led us to the
conclusion that the alignment was noisy. How-
ever, it is possible that we did correctly align some
words that were misaligned in the baseline align-
ment. To investigate this issue we concatenated
first the baseline and verb alignments, and then all
three alignments, and extracted phrase tables from
the concatenated training sets.
All scores for both combined systems signifi-
cantly outperformed the unfactored baseline, and
were slightly better than base. For translation into
German it was best to use the combination of only
verb and base, which was significantly better than
base on Meteor. This shows that even though the
verb alignments were not good when used in a sin-
gle system, they still could contribute in a combi-
nation system.
5 Preprocessing of OOVs
Out-of-vocabulary words, words that have not
been seen in the training data, are a problem in
statistical machine translation, since no transla-
tions have been observed for them. The standard
strategy is to transfer them as is to the translation
output, which, naive as it sounds, actually works
well in some cases, since many OOVs are numbers
or proper names (Stymne and Holmqvist, 2008).
However, it still results in incomprehensible words
in the output in many cases. We have investi-
gated several ways of changing unknown words
into similar words that have been seen in the train-
ing data, in a preprocessing step.
We also considered another OOV problem,
number formatting, since it differs between En-
glish and German. To address this, we swapped
decimal points/commas, and other delimeters for
unknown numbers in a post-processing step.
In the preprocessing step, we applied a num-
ber of transformations to each OOV word, accept-
ing the first applicable transformation that led to a
known word:
Type German English
total OOVs 1833 1489
casing 124 26
stemming 270 72
hyphenated words 230 124
end hyphens 24 ?
Table 5: Number of affected words by OOV-
preprocessing
1. Change the word into a known cased ver-
sion (since we trained a truecased system,
this handles cased variations of words)
2. Stem the word, and if we know the stem,
choose the most common realisation of that
stem (using a Porter stemmer)
3. For hyphenated words, split at the hyphen (if
any of the resulting parts are OOVs, they are
recursively treated as well)
4. Remove hyphens at the end of German words
(that could result from compound splitting)
The first two steps were based on frequency lists
of truecased and stemmed words that we compiled
from the monolingual training corpora.
Inspection of the initial results showed that
proper names were often changed into other words
in English, so we excluded them from the prepro-
cessing by not applying it to words with an initial
capital letter. This happened to a lesser extent for
German, but here it was impossible to use the same
simple heuristic for proper names, since German
nouns also have an initial capital letter.
The number of affected words for the baseline
using the final transformations are shown in Table
5. Even though we managed to transform some
words, we still lack a transformation for the ma-
jority of OOVs. Despite this, there is a tendency of
small improvements on both metrics in the major-
ity of cases in both translation directions, as shown
in Tables 6 and 7.
Figure 1 shows an example of how OOV pro-
cessing affects one sentence for translation from
German to English. In this case splitting a hy-
phenated compound gives a better translation,
even though the word opening is chosen rather
than jack. There is also a stemming change,
where the adjective ausgereiftesten (the most well-
engineered), is changed form superlative to posi-
tive. This results in a more understandable trans-
186
DE original Die besten und technisch ausgereiftesten Telefone mit einer 3,5-mm-O?ffnung
fu?r normale Kopfho?rer kosten bis zu fu?nfzehntausend Kronen.
DE preprocessed die besten und technisch ausgereifte Telefone mit einer 3,5 mm O?ffnung fu?r
normale Kopf Ho?rer kosten bis zu fu?nfzehntausend Kronen .
base+verb+reorder The best and technically ausgereiftesten phones with a 3,5-mm-O?ffnung for
normal earphones cost up to fifteen thousand kronor.
base+verb+reorder
+OOV
The best and technologically advanced phones with a 3.5 mm opening for nor-
mal earphones cost up to fifteen thousand kronor.
EN reference The best and most technically well-equipped telephones, with a 3.5 mm jack
for ordinary headphones, cost up to fifteen thousand crowns.
Figure 1: Example of the effects of OOV processing for German?English
System Bleu Meteor
base 14.24 49.41
+ OOV 14.26 49.43
base+verb 14.38 49.72
+ OOV 14.42 49.75
+ MBR 14.41 49.77
Table 6: Results for OOV-processing and MBR,
English?German.
System Bleu Meteor
base 18.50 38.47
+ OOV 18.48 38.59
base+verb+reorder 18.73 38.59
+ OOV 18.81 38.70
+ MBR 18.84 38.75
Table 7: Results for OOV-processing and MBR,
German?English.
lation, which, however, is harmful to automatic
scores, since the preceding word, technically,
which is identical to the reference, is changed into
technologically.
This work is related to work by Arora et al
(2008), who transformed Hindi OOVs by us-
ing morphological analysers, before translation to
Japanese. Our work has the advantage that it is
more knowledge-lite, as it only needs a Porter
stemmer and a monolingual corpus. Mirkin et al
(2009) used WordNet to replace OOVs by syn-
onyms or hypernyms, and chose the best overall
translation partly based on scoring of the source
transformations. Our OOV handling could po-
tentially be used in combination with both these
strategies.
6 Final Submission
For the final Liu shared task submission we
used the base+verb+reorder+OOV system for
German?English and the base+verb+OOV sys-
tem for English?German, which had the best
overall scores considering all metrics. To these
systems we added minimum Bayes risk (MBR)
decoding (Kumar and Byrne, 2004). In standard
decoding, the top suggestion of the translation sys-
tem is chosen as the system output. In MBR de-
coding the risk is spread by choosing the trans-
lation that is most similar to the N highest scor-
ing translation suggestions from the system, with
N = 100, as suggested in Koehn et al (2008).
MBR decoding gave hardly any changes in auto-
matic scores, as shown in Tables 6 and 7. The final
system was significantly better than the baseline in
all cases, and significantly better than base on Me-
teor in both translation directions, and on Bleu for
translation into English.
7 Conclusions
As in Holmqvist et al (2009) reordering by us-
ing Giza++ in two phases had a small, but consis-
tent positive effect. Aligning verbs by co-locating
them at the end of sentences had a largely negative
effect. However, when output from this method
was concatenated with the baseline alignment be-
fore extracting the phrase table, there were con-
sistent improvements. Combining all three align-
ments, however, had mixed effects. Combining re-
ordering in training with a knowledge-lite method
for handling out-of-vocabulary words led to sig-
nificant improvements on Meteor scores for trans-
lation between German and English in both direc-
tions.
187
References
Abhaya Agarwal and Alon Lavie. 2008. METEOR,
M-BLEU and M-TER: Evaluation metrics for high-
correlation with human rankings of machine transla-
tion output. In Proceedings of the Third Workshop
on Statistical Machine Translation, pages 115?118,
Columbus, Ohio, USA.
Karunesh Arora, Michael Paul, and Eiichiro Sumita.
2008. Translation of unknown words in phrase-
based statistical machine translation for languages
of rich morphology. In Proceedings of the 1st Inter-
national Workshop on Spoken Languages Technolo-
gies for Under-Resourced Languages, pages 70?75,
Hanoi, Vietnam.
Maria Holmqvist, Sara Stymne, Jody Foo, and Lars
Ahrenberg. 2009. Improving alignment for SMT
by reordering and augmenting the training corpus.
In Proceedings of the Fourth Workshop on Statis-
tical Machine Translation, pages 120?124, Athens,
Greece.
Philipp Koehn and Kevin Knight. 2003. Empirical
methods for compound splitting. In Proceedings of
the 10th Conference of the EACL, pages 187?193,
Budapest, Hungary.
Philipp Koehn and Josh Schroeder. 2007. Experi-
ments in domain adaptation for statistical machine
translation. In Proceedings of the Second Workshop
on Statistical Machine Translation, pages 224?227,
Prague, Czech Republic.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In Proceedings of the 45th Annual Meeting
of the ACL, demonstration session, pages 177?180,
Prague, Czech Republic.
Philipp Koehn, Abhishek Arun, and Hieu Hoang.
2008. Towards better machine translation quality for
the German-English language pairs. In Proceedings
of the Third Workshop on Statistical Machine Trans-
lation, pages 139?142, Columbus, Ohio, USA.
Shankar Kumar and William Byrne. 2004. Minimum
Bayes-risk decoding for statistical machine transla-
tion. In Proceedings of the 2004 Human Language
Technology Conference of the NAACL, pages 169?
176, Boston, Massachusetts, USA.
Stefan Langer. 1998. Zur Morphologie und Seman-
tik von Nominalkomposita. In Tagungsband der
4. Konferenz zur Verarbeitung natu?rlicher Sprache
(KONVENS), pages 83?97, Bonn, Germany.
Shachar Mirkin, Lucia Specia, Nicola Cancedda, Ido
Dagan, Marc Dymetman, and Idan Szpektor. 2009.
Source-language entailment modeling for translat-
ing unknown terms. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Natu-
ral Language Processing of the AFNLP, pages 791?
799, Suntec, Singapore.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evaluation of machine translation. In Proceedings
of the 40th Annual Meeting of the ACL, pages 311?
318, Philadelphia, Pennsylvania, USA.
Stefan Riezler and John T. Maxwell. 2005. On some
pitfalls in automatic evaluation and significance test-
ing for MT. In Proceedings of the Workshop on In-
trinsic and Extrinsic Evaluation Measures for MT
and/or Summarization at the 43th Annual Meeting of
the ACL, pages 57?64, Ann Arbor, Michigan, USA.
Helmut Schmid and Florian Laws. 2008. Estimation of
conditional probabilities with decision trees and an
application to fine-grained pos tagging. In Proceed-
ings of the 22th International Conference on Com-
putational Linguistics, pages 777?784, Manchester,
UK.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of the
International Conference on New Methods in Lan-
guage Processing, pages 44?49, Manchester, UK.
Andreas Stolcke. 2002. SRILM ? an extensible
language modeling toolkit. In Proceedings of the
Seventh International Conference on Spoken Lan-
guage Processing, pages 901?904, Denver, Col-
orado, USA.
Sara Stymne and Maria Holmqvist. 2008. Process-
ing of Swedish compounds for phrase-based statis-
tical machine translation. In Proceedings of the
12th Annual Conference of the European Associa-
tion for Machine Translation, pages 180?189, Ham-
burg, Germany.
Sara Stymne. 2008. German compounds in factored
statistical machine translation. In Proceedings of
GoTAL ? 6th International Conference on Natural
Language Processing, pages 464?475, Gothenburg,
Sweden.
Chao Wang, Michael Collins, and Philipp Koehn.
2007. Chinese syntactic reordering for statistical
machine translation. In Proc. of the 2007 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, pages 737?745, Prague, Czech Republic.
Fei Xia and Michael McCord. 2004. Improving
a statistical MT system with automatically learned
rewrite patterns. In Proceedings of the 20th Inter-
national Conference on Computational Linguistics,
pages 508?514, Geneva, Switzerland.
188
Proceedings of the 2010 Workshop on Graph-based Methods for Natural Language Processing, ACL 2010, pages 103?107,
Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational Linguistics
Computing Word Senses by Semantic Mirroring and Spectral Graph
Partitioning
Martin Fagerlund
Linko?ping University
Linko?ping, Sweden
marfa229@student.liu.se
Lars Elde?n
Linko?ping University
Linko?ping, Sweden
lars.elden@liu.se
Magnus Merkel
Linko?ping University
Linko?ping, Sweden
magnus.merkel@liu.se
Lars Ahrenberg
Linko?ping University
Linko?ping, Sweden
lars.ahrenberg@liu.se
Abstract
Using the technique of ?semantic mirror-
ing? a graph is obtained that represents
words and their translations from a paral-
lel corpus or a bilingual lexicon. The con-
nectedness of the graph holds information
about the different meanings of words that
occur in the translations. Spectral graph
theory is used to partition the graph, which
leads to a grouping of the words according
to different senses. We also report results
from an evaluation using a small sample of
seed words from a lexicon of Swedish and
English adjectives.
1 Introduction
A great deal of linguistic knowledge is encoded
implicitly in bilingual resources such as par-
allel texts and bilingual dictionaries. Dyvik
(1998, 2005) has provided a knowledge discov-
ery method based on the semantic relationship be-
tween words in a source language and words in
a target language, as manifested in parallel texts.
His method is called Semantic mirroring and the
approach utilizes the way that different languages
encode lexical meaning by mirroring source words
and target words back and forth, in order to es-
tablish semantic relations like synonymy and hy-
ponymy. Work in this area is strongly related to
work within Word Sense Disambiguation (WSD)
and the observation that translations are a good
source for detecting such distinctions (Resnik &
Yarowsky 1999, Ide 2000, Diab & Resnik 2002).
A word that has multiple meanings in one lan-
guage is likely to have different translations in
other languages. This means that translations
serve as sense indicators for a particular source
word, and make it possible to divide a given word
into different senses.
In this paper we propose a new graph-based ap-
proach to the analysis of semantic mirrors. The
objective is to find a viable way to discover syn-
onyms and group them into different senses. The
method has been applied to a bilingual dictionary
of English and Swedish adjectives.
2 Preparations
2.1 The Translation Matrix
In these experiments we have worked with a
English-Swedish lexicon consisting of 14850 En-
glish adjectives, and their corresponding Swedish
translations. Out of the lexicon was created a
translation matrix B, and two lists with all the
words, one for English and one for Swedish. B
is defined as
B(i, j) =
{
1, if i ? j,
0, otherwise.
The relation i ? j means that word i translates
to word j.
2.2 Translation
Translation is performed as follows. From the
word i to be translated, we create a vector e?i, with
a one in position i, and zeros everywhere else.
Then perform the matrix multiplication Be?i if it
is a Swedish word to be translated, or BT e?i if it is
an English word to be translated. e?i has the same
length as the list in which the word i can be found.
3 Semantic Mirroring
We start with an English word, called eng11. We
look up its Swedish translations. Then we look up
1Short for english1. We will use swe for Swedish words.
103
the English translations of each of those Swedish
words. We have now performed one ?mirror-
operation?. In mathematical notation:
f = BBT e?eng1.
The non-zero elements in the vector f represent
English words that are semantically related to
eng1. Dyvik (1998) calls the set of words that we
get after two translations the inverse t-image. But
there is one problem. The original word should not
be here. Therefore, in the last translation, we mod-
ify the matrix B, by replacing the row in B corre-
sponding to eng1, with an all-zero row. Call this
new modified matrixBmod1. So instead of the ma-
trix multiplication performed above, we start over
with the following one:
Bmod1BT e?eng1. (1)
To make it clearer from a linguistic perspective,
consider the following figure2.
eng2
swe1
33
f
f
f
f
f
f
f
f
f
f eng3
eng1
++
X
X
X
X
X
X
X
X
X
X
//
33
f
f
f
f
f
f
f
f
f
f swe2
33
f
f
f
f
f
f
f
f
f
f
++
X
X
X
X
X
X
X
X
X
X
eng1
swe3 //
++
X
X
X
X
X
X
X
X
X
X
eng4
eng5
The words to the right in the picture above
(eng2,...,eng5) are the words we want to divide
into senses. To do this, we need some kind of
relation between the words. Therefore we con-
tinue to translate, and perform a second ?mirror
operation?. To keep track of what each word in
the inverse t-image translates to, we must first
make a small modification. We have so far done
the operation (1), which gave us a vector, call it
e ? R14850?1. The vector e consists of nonzero in-
tegers in the positions corresponding to the words
in the invers t-image, and zeros everywhere else.
We make a new matrix E, with the same number
of rows as e, and the same number of columns as
there are nonzeros in e. Now go through every el-
ement in e, and when finding a nonzero element
in row i, and if it is the j:th nonzero element, then
put a one in position (i, j) in E. The procedure is
illustrated in (2).
2The arrows indicate translation.
?
?
?
?
?
?
?
?
1
0
2
1
0
3
?
?
?
?
?
?
?
?
??
?
?
?
?
?
?
?
?
1 0 0 0
0 0 0 0
0 1 0 0
0 0 1 0
0 0 0 0
0 0 0 1
?
?
?
?
?
?
?
?
(2)
When doing our second ?mirror operation?, we do
not want to translate through the Swedish words
swe1,...,swe3. We once again modify the matrix
B, this time replacing the columns of B corre-
sponding to the Swedish words swe1,...,swe3, with
zeros. Call this second modified matrix Bmod2.
With the matrix E from (2), we now get:
Bmod2BTmod2E (3)
We illustrate the operation (3):
swe4 //
))
S
S
S
S
eng6
eng2
55
k
k
k
k
// swe5 //
))
S
S
S
S
eng2
swe1
55
k
k
k
k
eng3
55
k
k
k
k
swe1 eng3
eng1
))
S
S
S
S
//
55
k
k
k
k
swe2
55
k
k
k
k
))
S
S
S
S
eng1 swe2 eng1
swe3 //
))
S
S
S
S
eng4
))
S
S
S
S
swe3 eng4
eng5 //
))
S
S
S
S
swe6
55
k
k
k
k
//
))
S
S
S
S
eng5
swe7
55
k
k
k
k
// eng7
Now we have got the desired relation between
eng2,...eng5. In (3) we keep only the rows corre-
sponding to eng2,...eng5, and get a symmetric ma-
trix A, which can be considered as the adjacency
matrix of a graph. The adjacency matrix and the
graph of our example are illustrated below.
A =
?
?
?
?
2 1 0 0
1 1 0 0
0 0 1 1
0 0 1 2
?
?
?
?
(4)
eng2
eng3
eng4
eng5
Figure 1: The graph to the matrix in (4).
The adjacency matrix should be interpreted in the
following way. The rows and the columns corre-
spond to the words in the inverse t-image. Follow-
ing our example, eng2 corresponds to row 1 and
104
column 1, eng3 corresponds to row 2 and column
2, and so on. The elements on position (i, i) in
A are the vertex weights. The vertex weight as-
sociated with a word, describes how many transla-
tions that word has in the other language, e.g. eng2
translates to swe4 and swe5 that is translated back
to eng2. So the vertex weight for eng2 is 2, as also
can be seen in position (1, 1) in (4). A high vertex
weight tells us that the word has a high number of
translations, and therefore probably a wide mean-
ing.
The elements in the adjacency matrix on posi-
tion (i, j), i ?= j are the edge weights. These
weights are associated with two words, and de-
scribe how many words in the other language that
both word i and j are translated to. E.g. eng5
and eng4 are both translated to swe6, and it fol-
lows that the weight, w(eng4,eng5) = 1. If we in-
stead would take eng5 and eng7, we see that they
both translate to swe6 and swe7, so the weight be-
tween those words, w(eng5,eng7) = 2. (But this is
not shown in the adjacency matrix, since eng7 is
not a word in the inverse t-image). A high edge
weight between two words tells us that they share
a high number of translations, and therefore prob-
ably have the same meanings.
4 Graph Partitioning
The example illustrated in Figure 1 gave as a re-
sult two graphs that are not connected. Dyvik ar-
gues that in such a case the graphs represent two
groups of words of different senses. In a larger
and more realistic example one is likely to obtain
a graph that is connected, but which can be parti-
tioned into two subgraphs without breaking more
than a small number of edges. Then it is reason-
able to ask whether such a partitioning has a sim-
ilar effect in that it represents a partitioning of the
words into different senses.
We describe the mathematical procedure of par-
titioning a graph into subgraphs, using spectral
graph theory (Chung, 1997). First, define the de-
gree d(i) of a vertex i to be
d(i) =
?
j
A(i, j).
Let D be the diagonal matrix defined by
D(i, j) =
{
d(i), if i = j,
0, otherwise.
The Laplacian L is defined as
L = D ?A.
We define the normalised Laplacian L to be
L = D?
1
2LD?
1
2 .
Now calculate the eigenvalues ?0, . . . , ?n?1, and
the eigenvectors of L. The smallest eigen-
value, ?0, is always equal to zero, as shown by
Chung (1997). The multiplicity of zero among
the eigenvalues is equal to the number of con-
nected components in the graph, as shown by
Spielman (2009). We will look at the eigenvector
belonging to the second smallest eigenvalue, ?1.
This eigenpair is often referred to as the Fiedler
value and the Fiedler vector. The entries in the
Fiedler vector corresponds to the vertices in the
graph. (We will assume that there is only one
component in the graph. If not, chose the com-
ponent with the largest number of vertices). Sort
the Fiedler vector, and thus sorting the vertices in
the graph. Then make n? 1 cuts along the Fiedler
vector, dividing the elements of the vector into two
sets, and for each cut compute the conductance,
?(S), defined as
?(S) = d(V ) |?(S, S?) |
d(S)d(S?)
, (5)
where d(S) =
?
i?S d(i). | ?(S, S?) | is the total
weight of the edges with one end in S and one end
in S?, and V = S + S? is the set of all vertices in
the graph. Another measure used is the sparsity,
sp(S), defined as
sp(S) = |?(S, S?) |
min(d(S), d(S?))
(6)
For details, see (Spielman, 2009). Choose the cut
with the smallest conductance, and in the graph,
delete the edges with one end in S and the other
end in S?. The procedure is then carried out until
the conductance, ?(S), reaches a tolerance. The
tolerance is decided by human evaluators, per-
forming experiments on test data.
5 Example
We start with the word slithery, and after the mir-
roring operation (3) we get three groups of words
in the inverse t-image, shown in Table 1. After
two partitionings of the graph to slithery, using the
method described in section 4, we get five sense
groups, shown in Table 2.
105
smooth slimy saponaceous
slick smooth-faced
lubricious oleaginous
slippery oily slippy
glib greasy
sleek
Table 1: The three groups of words after the mir-
roring operation.
slimy glib oleaginous
smooth-faced slippery oily
smooth lubricious greasy
sleek slick
saponaceous slippy
Table 2: The five sense groups of slithery after two
partitionings.
6 Evaluation
A small evaluation was performed using a ran-
dom sample of 10 Swedish adjectives. We gen-
erated sets under four different conditions. For the
first, using conductance (5). For the second, using
sparsity (6). For the third and fourth, we set the
diagonal entries in the adjacency matrix to zero.
These entries tell us very little of how the words
are connected to each other, but they may effect
how the partitioning is made. So for the third, we
used conductance and no vertex weights, and for
the fourth we used sparsity and no vertex weights.
There were only small differences in results due to
the conditions, so we report results only for one of
them, the one using vertex weights and sparsity.
Generated sets, with singletons removed, were
evaluated from two perspectives: consistency and
synonymy with the seed word. For consistency a
three-valued scheme was used: (i) the set forms a
single synset, (ii) at least two thirds of the words
form a single synset, and (iii) none of these. Syn-
onymy with the seed word was judged as either
yes or no.
Two evaluators first judged all sets indepen-
dently and then coordinated their judgements. The
criterion for consistency was that at least one do-
main, such as personality, taste, manner, can be
found where all adjectives in the set are inter-
changeable. Results are shown in Table 3.
Depending on howwe count partially consistent
groups this gives a precision in the range 0.57 to
0.78. We have made no attempt to measure recall.
Count Average Percentage
All groups 58 5.8 100
Consistent
groups
33 3.3 57
2/3 consistency 12 1.2 21
Synonymy
with seed word
14 1.4 24
Table 3: Classified output with frequencies from
one type of partition
It may be noted that group size varies. There are
often several small groups with just 2 or 3 words,
but sometimes as many as 10-15 words make up a
group. For large groups, even though they are not
fully consistent, the words tend to be drawn from
two or three synsets.
7 Conclusion
So far we have performed a relatively limited num-
ber of tests of the method. Those tests indi-
cate that semantic mirroring coupled with spectral
graph partitioning is a useful method for comput-
ing word senses, which can be developed further
using refined graph theoretic and linguistic tech-
niques in conjunction.
8 Future work
There is room for many more investigations of the
approach outlined in this paper. We would like
to explore the possibility to have a vertex (word)
belong to multiple synsets, instead of having dis-
crete cuts between synsets. In the present solu-
tion a vertex belongs to only one partition of a
graph, making it impossible to having the same
word belong to several synsets. We would also
like to investigate the properties of graphs to see
whether it is possible to automatically measure
how close a seed word is to a particular synset.
Furthermore, more thorough evaluations of larger
data sets would give us more information on how
to combine similar synsets which were generated
from distinct seed words and explore more com-
plex semantic fields. In our future research we will
test the method also on other lexica, and perform
experiments with the different tolerances involved.
We will also perform extensive tests assessing the
results using a panel of human evaluators.
106
References
Daniel A. Spielman. 2009. Spectral Graph theory.
Lecture notes.
Daniel A. Spielman, S. -H. Teng. 2006. Spectral par-
titioning works: Planar graphs and finite element
meshes. Elsevier Inc.
Diab, M. Resnik, P. 2002. An Unsupervised Method
for Word Sense Tagging using Parallel Corpora.
Proceedings of the 40th Annual Meeting of the As-
sociation for Computational Linguistics. 255-262.
Fan R. K. Chung. 1997. Spectral Graph Theory.
American Mathematical Society, Providence, Rhode
Island.
H. Dyvik. 1998. A Translational Basis for Semantics.
In: Stig Johansson and Signe Oksefjell (eds.): Cor-
pora and Crosslinguistic Research: Theory, Method
and Case Studies, pp. 51-86. Rodopi.
H. Dyvik. 2005. Translations as a Semantic Knowl-
edge Source. Proceedings of the Second Baltic Con-
ference on Human Language Technologies, Tallinn.
Nancy Ide. 2000. Cross-lingual sense determination:
Can it work? Computers and the Humanities: Spe-
cial issue on SENSEVAL, 34:223?234.
Philip Resnik , David Yarowsky. Distinguishing sys-
tems and distinguishing senses: new evaluation
methods for Word Sense Disambiguation Natu-
ral Language Engineering, v.5 n.2, p.113-133, June
1999
107
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 393?398,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
Experiments with word alignment, normalization and clause reordering for
SMT between English and German
Maria Holmqvist, Sara Stymne and Lars Ahrenberg
Department of Computer and Information Science
Linko?ping University, Sweden
firstname.lastname@liu.se
Abstract
This paper presents the LIU system for the
WMT 2011 shared task for translation be-
tween German and English. For English?
German we attempted to improve the trans-
lation tables with a combination of standard
statistical word alignments and phrase-based
word alignments. For German?English trans-
lation we tried to make the German text more
similar to the English text by normalizing Ger-
man morphology and performing rule-based
clause reordering of the German text. This re-
sulted in small improvements for both transla-
tion directions.
1 Introduction
In this paper we present the LIU system for the
WMT11 shared task, for translation between En-
glish and German in both directions. We added a
number of features that address problems for trans-
lation between German and English such as word or-
der differences, incorrect alignment of certain words
such as verbs, and the morphological complexity
of German compared to English, as well as dealing
with previously unseen words.
In both translation directions our systems in-
clude compound processing, morphological se-
quence models, and a hierarchical reordering model.
For German?English translation we also added mor-
phological normalization, source side reordering,
and processing of out-of-vocabulary words (OOVs).
For English?German translation, we extracted word
alignments with a supervised method and combined
these alignments with Giza++ alignments in various
ways to improve the phrase table. We experimented
with different ways of combining the two alignments
such as using heuristic symmetrization and interpo-
lating phrase tables.
Results are reported on three metrics, BLEU (Pa-
pineni et al, 2002), NIST (Doddington, 2002) and
Meteor ranking scores (Agarwal and Lavie, 2008)
based on truecased output.
2 Baseline System
This years improvements were added to the LIU
baseline system (Stymne et al, 2010). Our base-
line is a factored phrase based SMT system that uses
the Moses toolkit (Koehn et al, 2007) for transla-
tion model training and decoding, GIZA++ (Och
and Ney, 2003) for word alignment, SRILM (Stol-
cke, 2002) an KenLM (Heafield, 2011) for language
modelling and minimum error rate training (Och,
2003) to tune model feature weights. In addition,
the LIU baseline contains:
? Compound processing, including compound
splitting and for translation into German also
compound merging
? Part-of-speech and morphological sequence
models
All models were trained on truecased data. Trans-
lation and reordering models were trained using the
bilingual Europarl and News Commentary corpora
that were concatenated before training. We created
two language models. The first model is a 5-gram
model that we created by interpolating two language
393
models from bilingual News Commentary and Eu-
roparl with more weight on the News Commentary
model. The second model is a 4-gram model trained
on monolingual News only. All models were cre-
ated using entropy-based pruning with 10?8 as the
threshold.
Due to time constraints, all tuning and evaluation
were performed on half of the provided shared task
data. Systems were tuned on 1262 sentences from
newstest2009 and all results reported in Tables 1 and
2 are based on a devtest set of 1244 sentences from
newstest2010.
2.1 Sequence models with part-of-speech and
morphology
To improve target word order and agreement in the
translation output, we added an extra output factor in
our translation models consisting of tags with POS
and morphological features. For English we used
tags that were obtained by enriching POS tags from
TreeTagger (Schmid, 1994) with additional morpho-
logical features such as number for determiners. For
German, the POS and morphological tags were ob-
tained from RFTagger (Schmid and Laws, 2008)
which provides morphological information such as
case, number and gender for nouns and tense for
verbs. We trained two sequence models for each
system over this output factor and added them as
features in our baseline system. The first sequence
model is a 7-gram model interpolated from models
of bilingual Europarl and News Commentary. The
second model is a 6-gram model trained on mono-
lingual News only.
2.2 Compound processing
In both translation directions we split compounds,
using a modified version of the corpus-based split-
ting method of Koehn and Knight (2003). We split
nouns, verb, and adjective compounds into known
parts that were content words or cardinal numbers,
based on the arithmetic mean of the frequency of
the parts in the training corpus. We allowed 10 com-
mon letter changes (Langer, 1998) and hyphens at
split points. Compound parts were kept in their sur-
face form and compound modifiers received a part-
of-speech tag based on that of the tag of the full com-
pound.
For translation into German, compounds were
merged using the POS-merging strategy of Stymne
(2009). A compound part in the translation output,
identified by the special part-of-speech tags, was
merged with the next word if that word had a match-
ing part-of-speech tag. If the compound part was
followed by the conjunction und (and), we added a
hyphen to the part, to account for coordinated com-
pounds.
2.3 Hierarchical reordering
In our baseline system we experimented with two
lexicalized reordering models. The standard model
in Moses (Koehn et al, 2005), and the hierarchi-
cal model of Galley and Manning (2008). In both
models the placement of a phrase is compared to
that of the previous and/or next phrase. In the stan-
dard model up to three reorderings are distinguished,
monotone, swap, and discontinuous. In the hier-
archical model the discontinuous class can be fur-
ther subdivided into two classes, left and right dis-
continuous. The hierarchical model further differs
from the standard model in that it compares the or-
der of the phrase with the next or previous block of
phrases, not only with the next or previous single
phrase.
We investigated one configuration of each
model. For the standard model we used the msd-
bidirectional-fe setting, which uses three orienta-
tions, is conditioned on both the source and target
language, and considers both the previous and next
phrase. For the hierarchical model we used all four
orientations, and again it is conditioned on both the
source and target language, and considers both the
previous and next phrase.
The result of replacing the standard reordering
model with an hierarchical model is shown in Table
1 and 2. For translation into German adding the hi-
erarchical model led to small improvements as mea-
sured by NIST and Meteor. For translation in the
other direction, the differences on automatic metrics
were very small. Still, we decided to use the hierar-
chical model in all our systems.
3 German?English
For translation from German into English we fo-
cused on making the German source text more sim-
ilar to English by removing redundant morphology
394
and changing word order before training translation
models.
3.1 Normalization
We performed normalization of German words to re-
move distinctions that do not exist in English, such
as case distinctions on nouns. This strategy is sim-
ilar to that of El-Kahlout and Yvon (2010), but we
used a slightly different set of transformations, that
we thought better mirrored the English structure.
For morphological tags we used RFTagger and for
lemmas we used TreeTagger. The morphological
transformations we performed were the following:
? Nouns:
? Replace with lemma+s if plural number
? Replace with lemma otherwise
? Verbs:
? Replace with lemma if present tense, not
third person singular
? Replace with lemma+p if past tense
? Adjectives:
? Replace with lemma+c if comparative
? Replace with lemma+sup if superlative
? Replace with lemma otherwise
? Articles:
? Definite articles:
? Replace with des if genitive
? Replace with der otherwise
? Indefinite articles:
? Replace with eines if genitive
? Replace with ein otherwise
? Pronouns:
? Replace with RELPRO if relative
? Replace with lemma if indefinite, interrog-
ative, or possessive pronouns
? Add +g to all pronouns which are geni-
tive, unless they are possessive
For all word types that are not mentioned in the
list, surface forms were kept.
BLEU NIST Meteor
Baseline 21.01 6.2742 41.32
+hier reo 20.94 6.2800 41.24
+normalization 20.85 6.2370 41.04
+source reordering 21.06 6.3082 41.40
+ OOV proc. 21.22 6.3692 41.51
Table 1: German?English translation results. Results are
cumulative.
We also performed those tokenization and
spelling normalizations suggested by El-Kahlout
and Yvon (2010), that we judged could safely be
done for translation from German without collect-
ing corpus statistics. We split words with numbers
and letters, such as 40-ja?hrigen or 40ja?hrigen (40
year-old), unless the suffix indicates that it is a ordi-
nal, such as 70sten (70th). We also did some spelling
normalization by exchanging ? with ss and replacing
tripled consonants with doubled consonants. These
changes would have been harmful for translation
into German, since they change the language into a
normalized variant, but for translation from German
we considered them safe.
3.2 Source side reordering
To make the word order of German input sen-
tences more English-like a version of the rules of
(Collins et al, 2005) were partially implemented us-
ing tagged output from the RFTagger. Basically,
beginnings of subordinate clauses, their subjects (if
present) and final verb clusters were identified based
on tag sequences, and the clusters were moved to
the beginning of the clause, and reordered so that
the finite verb ended up in the second clause posi-
tion. Also, some common adverbs were moved with
the verb cluster and placed between finite and non-
finite verbs. After testing, we decided to apply these
rules only to subordinate clauses at the end of sen-
tences, since these were the only ones that could be
identified with good precision. Still, some 750,000
clauses were reordered.
3.3 OOV Processing
We also added limited processing of OOVs. In a pre-
processing step we replaced unknown words with
known cased variants if available, removed markup
from normalized words if that resulted in an un-
395
known token, and split hyphened words. We also
split suspected names in cases where we had a pat-
tern with a single upper-case letter in the middle of a
word, such as ConocoPhillips into Conoco Phillips.
In a post-processing step we changed the number
formatting of unknown numbers by changing dec-
imal points and thousand separators, to agree with
English orthography. This processing only affects
a small number of words, and cannot be expected
to make a large impact on the final results. Out
of 884 OOVs in the devtest, 39 had known cased
options, 126 hyphened words were split, 147 cases
had markup from the normalization removed, and 13
suspected names were split.
3.4 Results
The results of these experiments can be seen in Table
1 where each new addition is added to the previous
system. When we compare the new additions with
the baseline with hierarchical reordering, we see that
while the normalization did not seem to have a posi-
tive effect on any metric, both source reordering and
OOV processing led to small increases on all scores.
4 English?German
For translation from English into German we at-
tempted to improve the quality of the phrase table by
adding new word alignments to the standard Giza++
alignments.
4.1 Phrase-based word alignment
We experimented with different ways of com-
bining word alignments from Giza++ with align-
ments created using phrase-based word alignment
(PAL) which previously has been shown to improve
alignment quality for English?Swedish (Holmqvist,
2010). The idea of phrase-based word alignment is
to use word and part-of-speech sequence patterns
from manual word alignments to align new texts.
First, parallel phrases containing a source segment,
a target segment and links between source and target
words are extracted from word aligned texts (Figure
1). In the second step, these phrases are matched
against new parallel text and if a matching phrase
is found, word links from the phrase are added to
the corresponding words in the new text. In order
to increase the number of matching phrases and im-
prove word alignment recall, words in the parallel
En: a typical example
De: ein typisches Beispiel
Links: 0-0 1-1 2-2
En: a JJ example
De: ein ADJA Beispiel
Links: 0-0 1-1 2-2
En: DT JJ NN
De: ART ADJA N
Links: 0-0 1-1 2-2
Figure 1: Examples of parallel phrases used in word
alignment.
BLEU NIST Meteor
Baseline 16.16 6.2742 50.89
+hier reo 16.06 6.2800 51.25
+pal-gdfa 16.14 5.6527 51.10
+pal-dual 15.71 5.5735 50.43
+pal-inter 15.92 5.6230 50.73
Table 2: English?German translation results, results
are cumulative except for the three alternative PAL-
configurations.
segments were replaced by POS/morphological tags
from RFTagger.
Alignment patterns were extracted from 1000 sen-
tences in the manually word aligned sample of
English?German Europarl texts from Pado and Lap-
ata (2006). All parallel phrases were extracted from
the word aligned texts, as when extracting a trans-
lation model. Parallel phrases that contain at least
3 words were generalized with POS tags to form
word/POS patterns for alignment. A subset of these
patterns, with high alignment precision (> 0.80) on
the 1000 sentences, were used to align the entire
training corpus.
We combined the new word alignments with
the Giza++ alignments in two ways. In the first
method, we used a symmetrization heuristic similar
to grow-diag-final-and to combine three word align-
ments into one, the phrase-based alignment and two
Giza++ alignments in different directions. In the
second method we extracted a separate phrase ta-
ble from the sparser phrase-based alignment using
a constrained method of phrase extraction that lim-
ited the number of unaligned words in each phrase
pair. The reason for constraining the phrase table
396
extraction was that the standard extraction method
does not work well for the sparse word alignments
that PAL produces, but we think it could still be
useful for extracting highly reliable phrases. After
some experimentation we decided to allow an unlim-
ited number of internal unaligned words, that is un-
aligned words that are surrounded by aligned words,
but limit the number of external unaligned words,
i.e., unaligned words at the beginning or end of the
phrase, to either one each in the source and target
phrase, or to zero.
We used two ways to include the sparse phrase-
table into the translation process:
? Have two separate phrase-tables, the sparse ta-
ble, and the standard GIZA++ based phrase-
table, and use Moses? dual decoding paths.
? Interpolate the sparse phrase-table with the
standard phrase-table, using the mixture model
formulation of Ueffing et al (2007), with equal
weights, in order to boost the probabilities of
highly reliable phrases.
4.2 Results
We evaluated our systems on devtest data and found
that the added phrase-based alignments did not pro-
duce large differences in translation quality com-
pared to the baseline system with hierarchical re-
ordering as shown in Table 2. The system created
with a heuristic combination of PAL and Giza++
(pal-gdfa) had a small increase in BLEU, but no im-
provement on the other metrics. Systems using a
phrase table extracted from the sparse alignments
did not produce better results than baseline. The sys-
tem using dual decoding paths (pal-dual) produced
worse results than the system using an interpolated
phrase table (pal-inter).
5 Submitted systems
The LIU system participated in German?English
and English?German translation in the WMT 2011
shared task. The new additions were a combina-
tion of unsupervised and supervised word align-
ments, spelling normalization, clause reordering and
OOV processing. Our submitted systems contain
all additions described in this paper. For English-
German we used the best performing method of
BLEU
System Devtest Test
en-de
baseline +hier 16.1 14.5
submitted 16.1 14.8
de-en
baseline +hier 20.9 19.3
submitted 21.2 19.9
Table 3: Summary of devtest results and shared task test
results for submitted systems and LIU baseline with hier-
archical reordering.
word alignment combination which was the method
that uses heuristic combination similar to grow-diag-
final-and.
The results of our submitted systems are shown
in Table 3 where we compare them to the LIU base-
line system with hierarchical reordering models. We
report modest improvements on the devtest set for
both translation directions. We also found small im-
provements of our submitted systems in the official
shared task evaluation on the test set newstest2011.
References
Abhaya Agarwal and Alon Lavie. 2008. Meteor,
M-BLEU and M-TER: Evaluation metrics for high-
correlation with human rankings of machine transla-
tion output. In Proceedings of the Third Workshop
on Statistical Machine Translation, pages 115?118,
Columbus, Ohio.
Michael Collins, Philipp Koehn, and Ivona Kucerova?.
2005. Clause restructuring for statistical machine
translation. In Proceedings of the 43rd Annual Meet-
ing of the ACL, pages 531?540, Ann Arbor, Michigan.
George Doddington. 2002. Automatic evaluation of ma-
chine translation quality using n-gram co-occurence
statistics. In Proceedings of the Second International
Conference on Human Language Technology, pages
228?231, San Diego, California.
I?lknur Durgar El-Kahlout and Franc?ois Yvon. 2010. The
pay-offs of preprocessing for German-English statisti-
cal machine translation. In Proceedings of the Inter-
national Workshop on Spoken Language Translation,
pages 251?258, Paris, France.
Michel Galley and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. In Proceedings of the Conference on Empir-
ical Methods in Natural Language Processing, pages
848?856, Honolulu, Hawaii.
Kenneth Heafield. 2011. KenLM: Faster and Smaller
Language Model Queries. In Proceedings of the Sixth
397
Workshop on Statistical Machine Translation, Edin-
burgh, UK.
Maria Holmqvist. 2010. Heuristic word alignment
with parallel phrases. In Proceedings of the Seventh
Conference on International Language Resources and
Evaluation, pages 744-748, Valletta, Malta.
Philipp Koehn and Kevin Knight. 2003. Empirical meth-
ods for compound splitting. In Proceedings of the
Tenth Conference of EACL, pages 187?193, Budapest,
Hungary.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh system description
for the 2005 IWSLT speech translation evaluation. In
Proceedings of the International Workshop on Spoken
Language Translation, Pittsburgh, Pennsylvania.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the ACL, Demon-
stration Session, 177?180, Prague, Czech Republic.
Stefan Langer. 1998. Zur Morphologie und Semantik
von Nominalkomposita. In Tagungsband der 4. Kon-
ferenz zur Verarbeitung natu?rlicher Sprache, pages
83?97, Bonn, Germany.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting of the ACL, pages 160?167, Sap-
poro, Japan.
Sebastian Pado and Mirella Lapata. 2006. Optimal con-
stituent alignment with edge covers for semantic pro-
jection. In Proceedings of the 21st International Con-
ference on Computational Linguistics and 44th Annual
Meeting of the ACL, pages 1161?1168, Sydney, Aus-
tralia.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of the 40th Annual Meeting of ACL, pages 311?318,
Philadelphia, Pennsylvania.
Helmut Schmid and Florian Laws. 2008. Estimation
of conditional probabilities with decision trees and an
application to fine-grained POS tagging. In Proceed-
ings of the 20th International Conference on Compu-
tational Linguistics, pages 777?784, Manchester, UK.
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In International Conference
on New Methods in Language Processing, pages 44?
49, Manchester, UK.
Andreas Stolcke. 2002. SRILM - an extensible language
modeling toolkit. In Proceedings of the Seventh Inter-
national Conference on Spoken Language Processing,
pages 901?904, Denver, Colorado.
Sara Stymne, Maria Holmqvist, and Lars Ahrenberg.
2010. Vs and OOVs: Two problems for translation
between German and English. In Proceedings of the
Joint Fifth Workshop on Statistical Machine Trans-
lation and MetricsMATR, pages 189?194, Uppsala,
Sweden.
Sara Stymne. 2009. A comparison of merging strategies
for translation of German compounds. In Proceedings
of the EACL Student Research Workshop, pages 61?
69, Athens, Greece.
Nicola Ueffing, Gholamreza Haffari, and Anoop Sarkar.
2007. Semi-supervised model adaptation for statistical
machine translation. Machine Translation, 21(2):77?
94.
398
Proceedings of the Ninth Workshop on Innovative Use of NLP for Building Educational Applications , pages 34?42,
Baltimore, Maryland USA, June 26, 2014. c?2014 Association for Computational Linguistics
Translation Class Instruction as Collaboration 
in the Act of Translation 
 
Lars Ahrenberg 
Department of Computer and  
Information Science,  
Link?ping University 
lars.ahrenberg@liu.se 
 
Ljuba Tarvi 
University of Helsinki 
Helsinki, Finland 
ljuba.tarvi@welho.com 
 
  
 
Abstract 
The paper offers an effective way of 
teacher-student computer-based collabo-
ration in translation class. We show how 
a quantitative-qualitative method of 
analysis supported by word alignment 
technology can be applied to student 
translations for use in the classroom. The 
combined use of natural-language pro-
cessing and manual techniques enables 
students to ?co-emerge? during highly 
motivated collaborative sessions. Within 
the advocated approach, students are pro-
active seekers for a better translation 
(grade) in a teacher-centered computer-
based peer-assisted translation class.  
1 Introduction 
Tools for computer-assisted translation (CAT), 
including translation memories, term banks, and 
more, are nowadays standard tools for transla-
tors. The proper use of such tools and resources 
are also increasingly becoming obligatory parts 
of translator training. Yet we believe that transla-
tion technology has more to offer translator 
training, in particular as a support for classroom 
interaction. Our proposal includes a quantitative 
analysis of translations, supported by word 
alignment technology, to enable joint presenta-
tion, discussion, and assessment of individual 
student translation in class. For comparisons 
with related work, see section 4. 
From the pedagogical point of view, the sug-
gested procedure embraces at least four types of 
evaluation: students? implied self-evaluation, a 
preliminary computer evaluation, teacher?s eval-
uation after manually correcting the imperfect 
computer alignment and assessment, and peer 
evaluation during the collaborative team work in 
class, when the versions produced by the stu-
dents are simultaneously displayed, discussed 
and corrected if necessary.  
Theoretically, translations are viewed here as 
mappings between two languages through emer-
gent conceptual spaces based on an intermediate 
level of representation (e.g., Honkela et. al., 
2010). In terms of praxis, the basic approach is 
rooted in the idea (Vinay & Darbelnet, 1958) of 
consecutive numbering of the tokens (words) in 
the original text. This simple technique enables -
finding and labeling, in accordance with a cho-
sen set of rules, certain isomorphic correspond-
ences between the source and target tokens. 
Finding such correspondences is what current 
machine translation approaches attempt to 
achieve by statistical means in the training 
phase. 
The quantitative-qualitative technique we use 
here is the Token Equivalence Method (TEM) 
(Tarvi 2004). The use of the TEM in translation 
teaching originated as an argument (involving 
the second author) in a teacher-student debate 
over the relevance of a grade. The considerable 
time spent on the manual preparation of texts for 
translation using the TEM proved to be fairly 
well compensated for by the evident objectivity 
of the grades - the argument that, say, only 65% 
of the original text has been retained in a transla-
tion is difficult to brush aside. Later, the method 
was applied in research. Tarvi (2004) compared 
the classical Russian novel in verse by A. Push-
kin Eugene Onegin (1837) with its nineteen Eng-
lish translations. Figures calculated manually on 
10% of the text of the novel showed an excellent 
fit with the results on the same material obtained 
elsewhere by conventional comparative meth-
ods. Thus, we believe that characterizations of 
relations between source and target texts in ob-
34
jective terms is a good thing for translation eval-
uation. 
1.1 The TEM: Basics and Example 
Methodologically, the TEM focuses not on 
translation ?shifts? but on what has been kept in 
translation. The basic frame for analysis in the 
TEM is the Token Frame (2.2.1), which accounts 
for the number of the original tokens retained in 
translations. The other four frames (2.2.2-3, 
2.3.1-2), although useful in gauging the compar-
ative merits of the translations and the individual 
strategies, are optional.  
To concisely illustrate the method, one sen-
tence will be used ? the famous 13-token open-
ing sentence of Leo Tolstoy?s Anna Karenina:  
Vse  schastlivye  semyi  pohozhi drug na druga, 
kazhdaya neschastlivaya semya neschastliva  po 
svoemu. (All happy families resemble one an-
other, every unhappy  family is unhappy in its 
own way.) 
Eight English translations of this sentence 
(2.1) will be used for analysis.  
The source text and all its translations are to-
kenized and analyzed linguistically in different 
ways. NLP tools  such as lemmatizers, part-of-
speech taggers and parsers can be applied. Most 
importantly, however, to support the computa-
tion of the Token Frames (2.2.1), they must be 
word-aligned with the source text (2.6). The 
teacher or the students are expected to review the 
alignments and correct them if they are not ac-
ceptable.  Given the corrected alignments, the 
aligned data can be used by the teacher and the 
students in the classroom. 
After this introduction of the basics of the 
theoretical approach and relevant automatic 
methods for their implementation, the paper is 
built around the basic structural foci of any in-
struction unit: before class (2), in class (3), and 
outside class (4).  
2 Before class 
This section describes the techniques of pro-
cessing Source Texts (ST) and Target Texts (TT) 
by teachers and students.  
2.1 Token Numbering and Labeling 
The procedure starts with consecutive number-
ing of the original tokens:  
 
(1)Vse (2)schastlivye (3)semyi (4)pohozhi (5)drug (6)na 
(7)druga, (8)kazhdaya (9)neschastlivaya (10)semya 
(11)neschastliva (12)po (13)svoemu.  
 
The second step is establishing, via the proce-
dure of (corrected) alignment, the correspond-
ences between the Source tokens (St) and Target 
tokens (Tt). As a result, every corresponding TT 
token (Tt), if found, is designated with the num-
ber of its source counterpart (St). Besides, since 
no Tt may remain unlabeled, two types of Tts 
which have no counterparts in the ST are labeled 
as Extra tokens (2.3.1) and Formal tokens 
(2.3.2). Here are the eight translations of the ex-
ample sentence: 
 
Leo Wiener: (1899): 
(1)All (2)happy (3)families (4)resemble (5-6-7)one an-
other; (8)every (9)unhappy (10)family (Ft)is 
(11)unhappy (12)in (Ft)its (13)own way.  
 
Constance Garnett (1901):  
(2)Happy (3)families (Ft)are (1)all (4)alike; (8)every 
(9)unhappy (10)family (Ft)is (11)unhappy (12)in (Ft)its 
(13)own way. 
 
Rochelle S. Townsend (1912):  
(1)All (2)happy (3)families (Ft)are (Et)more (Et)or 
(Et)less (4)like (5-6-7)one another; (8)every (9)unhappy 
(10)family (Ft)is (11)unhappy (12)in (Ft)its (13)own 
(Et)particular way.  
 
Aylmer & Louise Maude (1918):   
(1)All (2)happy (3)families (4)resemble (5-6-7)one an-
other, (Et)but (8)each (9)unhappy (10)family (Ft)is 
(11)unhappy (12)in (Ft)its (13)own way. 
 
Rosemary Edmonds (1954):   
(1)All (2)happy (3)families (Ft)are (4)alike; (Et)but 
(Ft)an (9)unhappy (10)family (Ft)is (11)unhappy 
(12)after (Ft)its (13)own fashion. 
 
Joel Carmichael (1960):  
(2)Happy (3)families (Ft)are (1)all (4)alike; (8)every 
(9)unhappy (10)family (Ft)is (11)unhappy (12)in (Ft)its 
(13)own way. 
 
David Magarschack (1961):  
(1)All (2)happy (3)families (Ft)are (4)like (5-6-7)one 
another; (8)each (9)unhappy (10)family (Ft)is 
(11)unhappy (12)in (Ft)its (13)own way.  
 
Richard Pevear & Larisa Volokhonsky (2000):  
(1)All (2)happy (3)families (Ft)are (4)alike; (8)each 
(9)unhappy (10)family (Ft)is (11)unhappy (12)in (Ft)its 
(13)own way.  
 
As is seen, two of the versions are clones 
(Carmichael, Pevear-Volokhonsky), one transla-
tion (Garnett) differs from the original only by 
the choice of the adjective (St 8), while the re-
maining five versions are more diverse. Note the 
mode of denoting Tts suggested here: only the 
35
meaningful denotative tokens get labeled, e.g., 
are (4)alike, or is (11)unhappy; if not one Tt but 
a group of tokens is used as an isomorph to a 
single St, the whole group is underlined, e.g., 
(13)own way, or (13)own fashion.  
Although St 4 has been rendered as are alike 
(Edmonds, Pevear-Volokhonsky, Garnett, Car-
michael), are like (Townsend, Magarschack), 
and resemble (Wiener, the Maudes), all these 
rendering are viewed as retaining the denotative 
meaning of the original token. Or, for instance, 
St 12, whether rendered as after (Edmonds) or in 
(all the rest), is also viewed as retained in trans-
lation. The connotative shades of meaning most 
suitable for the outlined goals can be discussed 
in class (3.2).  
This mode of displaying the isomorphisms 
can be converted to the style of representation 
used in word alignment systems such as Giza++ 
(Och and Ney, 2003) as follows: Extra tokens 
and Formal tokens give rise to null links. Groups 
of tokens that correspond yield groups of links. 
Thus, the analysis for Wiener?s translation wo-
uld come out as below: 
1-1 2-2 3-3 4-4 5-5 5-6 6-5 6-6 7-5 7-6 8-7 9-8 
10-9 0-10 11-11 12-12 0-13 13-14 13-15. 
In gauging the content, two types of basic and 
optional analytical frames, content and formal, 
are used. Based on the way of calculating the 
results, the analytical frames will be considered 
here in two sections, percentage frames (2.2) and 
count frames (2.3).  
 
2.2  The TEM: Percentage Frames 
 
The results in these frames are calculated as per-
centages of the ST information retained in trans-
lations.  
2.2.1 Basic Content Frame (Token Frame) 
After finding the isomorphic counterparts, the 
percentages of the retained tokens are presented 
in Table 1 (column I). As one can see, Wiener, 
the Maudes, Magarschack and Townsend trans-
lated all thirteen tokens and, hence, scored 100% 
each; Garnett, Carmichael and Pevear-
Volokhonsky omitted Sts 5-6-7 and thus scored 
76%, while Edmonds left out four tokens, Sts 5-
6-7-8, thus retaining 69% of the original.  
2.2.2 Optional Formal Frame 1 (Morphology 
Frame) 
In this frame, if a token is rendered with the 
same part of speech as in the original, the Tt in 
question gets a count. As can be seen in Table 1 
(column II), only two translators, Wiener and the 
Maudes, kept the same type of predicate 1 (St 4) 
as in the original ? resemble ? while in the re-
maining six translations the type of predicate 1 
has been changed into a compound one: are 
alike (Edmonds, Pevear-Volokhonsky, Garnett, 
Carmichael), and are like (Townsend, 
Magarschack). Therefore, in this frame, Wiener 
and the Maudes get a 100% each; Edmonds, 
with her two changed parts of speech, gets 84%, 
while the remaining five translators, who 
changed one part of speech each, score 92%.  
2.2.3 Optional Formal Frame 2 (Syntax) 
Another possible way of gauging the ?presence? 
of the original in its translation is monitoring the 
syntactic changes. If at least two tokens are ren-
dered in the same sequence as in the original and 
preserve the same syntactic functions, they are 
considered syntactically kept. Non-translated Sts 
are viewed as non-kept syntactic positions. Table 
1 (column III) shows that Edmonds, who lost 
four syntactic positions, scores 76%, Garnett, 
Magarschack and Townsend get 92% each, the 
rest translators score a 100%.  
 
2.2.4 The Translation Quotient (TQ) 
As a result of either manual or computer-assisted 
translation processing, the teacher gets a tabulat-
ed picture (Table 1) of the three analyzed frames 
(columns I, II, III).  
In an attempt to combine the obtained figures 
in a meaningful whole, the Translation Quotient 
parameter (TQ, column IV) is used: it is the 
arithmetic mean of the percentages in the moni-
tored frames. If one adds up the percentage re-
sults in all three frames and divides the obtained 
figure by the number of frames, one gets a TQ, 
measured in percentage points (pp), which re-
flects a general quantitative picture of the con-
tent-form rendering of the original. This cumula-
tive parameter has shown a perfect fit with the 
results obtained by other methods of compara-
tive assessment (Tarvi 2004). Table 1 shows four 
groups of TQ results, from 100% (2 versions) 
through 97% (2) through 86% (3) to 74% (1). 
 
2.3 The TEM: Count Frames 
To further differentiate the translations in their 
closeness to the original, pure counts of some 
quantitative parameters can be added to the pic-
ture in Table 1: column V (extra tokens, Ets) and 
VI (formal Tokens, Fts).  
36
2.3.1 Optional Content Frame 1 
This frame is a useful tool of assessment, as it 
shows what has been added to the translation, 
i.e., the Tts that have no counterparts in the orig-
inal, labeled as extra Tokens (Et). Table 1 (col-
umn V) shows that Wiener, Magarschack, Gar-
nett, Carmachael, and Pevear-Volokhonsky add-
ed no extra Tokens (Ets), the Maudes and Ed-
monds added by one Et each, while Townsend ? 
four.  
2.3.2 Optional Formal Frame 3 
In this frame, the center of attention is formal 
Tokens (Fts) ? articles, tense markers, etc. Table 
1 (column VI) shows that Fts are employed in 
different quantities: Wiener and the Maudes used 
two Fts each, Edmonds used four, the rest trans-
lators ? three Fts each.  
2.4 TEM Results: the 13-Token Sentence  
The table below gives a cumulative picture of 
the results in each of the five frames considered:  
 
Table 1. Cumulative Overall Table (13 tokens): Rank Order 
I II III IV V VI 
TF MF SF TQ Et Ft 
(2.2.1) (2.2.2) (2.2.3) (2.2.4) (2.2.5) (2.2.6) 
(%) (%) (%) (pp) (count) (count) 
Leo Wiener (1899)   
100  100  100  100 0 2 
Aylmer & Louise Maude (1918)  
100  100  100  100 1 2 
David Magarschack (1961)   
100  92  100  97 0 3 
Rochelle S. Townsend (1912)  
100  92  100  97 4 3 
Constance Garnett (1901)   
76  92  92  86 0 3 
Joel Carmichael (1960)   
76  92  92  86 0 3 
Pevear & Volokhonsky (2000)  
76  92  92  86 0 3 
Rosemary Edmonds (1954)  
69  84  69  74 1 4 
 
As is seen, there are four groups of the TQ re-
sults. In the 100% group, Wiener has a slight 
advantage (in terms of isomorphism) over the 
Maudes, since he introduced no extra tokens. In 
the 97% group, Townsends?s translation inferior-
ity (in terms of closeness) is expressed in four 
extra tokens as compared to no extra tokens in 
Magarschack?s version. In the 86% block, no 
distinctions can be made because they are word-
for-word clones, except for Pevear-
Volokhonsky?s use of ?each? instead of ?every? 
(St 8). Edmonds? version (TQ = 74%) has a rec-
ord (for this sample) number of formal tokens, 
four. It does not imply that the translation is bad 
? this kind of judgment can arise only after a 
discussion in classroom (3.3).  
The one-sentence example, used here peda-
gogically to explain the TEM techniques, cannot 
be considered to be fairly representative of the 
quantitative parameters and their qualitative im-
plications of translated texts. Therefore, we offer 
the results obtained for a much bigger sample 
from Anna Karenina.  
2.4.1 TEM Results: the 405-Token Excerpt  
Sheldon (1997) performs a detailed conventional 
comparative analysis of the four ?focal points? of 
the novel: the opening sentence considered 
above (13 tokens), the ball scene (73 tokens), the 
seduction scene (103 tokens) and the suicide 
scene (216 tokens). He analyzed the seven trans-
lations considered here, except for the version by 
Pevear and Volokhonsky, which was published 
three years later. Sheldon concluded that it was 
Carmichael who showed the best fit with the 
original.  
Here are the quantitative results obtained with 
the TEM applied to the same excerpts.  
 
Table 2. Cumulative Overall Table (405 tokes): Rank Order 
Lost Kept TQ Ft Et 
tokens tokens  used  used 
(count) (count) (%) (count) (count) 
David Magarshack (1961)   
9 396 97,7 96 14 
Joe Carmichael (1960)   
18 387 95,5 95 15 
Constance Garnett (1901)   
20 385 95,0 90 8 
Aylmer & Louise Maude (1918)   
30 375 92,5 91 17 
Rosemary Edmonds (1954)   
34 371 91,6 87 14 
Leo Wiener (1899)    
57 348 85,9 74 20 
Rochelle S. Townsend (1912)   
69 336 82,9 79 42 
 
As is seen, the TQs range from 97,7% to 
82,9%. Since the TEM does not cover all aspects 
of Sheldon?s analysis, it favors Magarshack?s 
version, with Carmichael?s translation lauded by 
Sheldon following it closely.  
2.5 Language pair independence 
In our example with translation from Russian to 
English, there is an asymmetry in that formal 
tokens are largely to be seen only on the target 
side. However, the TEM frames can equally be 
applied in the reverse direction or to any lan-
guage pair. Whether or not we choose to exclude 
some formal tokens from the counting, the 
37
frames are applied in the same way to all transla-
tions and their relative differences will be re-
vealed. 
2.6 Computational analysis  
It has been suggested before that virtual learning 
environments are useful for translation teaching 
(e.g., Fictumova (2007)). Our point here is that 
fine-grained quantitative methods, such as the 
TEM, can be put to use given support from com-
putational linguistic tools. The proposed envi-
ronment consists of a central server and a num-
ber of client systems for the students. Communi-
cation between them is handled as in any e-
learning environment, where exercises, grades 
and other course materials can be stored and ac-
cessed. The server includes several modules for 
monolingual text analysis, such as sentence 
segmentation, tokenization, lemmatization and 
PoS-tagging. A parser may also be included to 
support the computation of the syntax frame. 
More importantly, there are modules for sen-
tence and word alignments, since this is what is 
required to support the TEM analysis. In addi-
tion, there are modules for reviewing and cor-
recting outputs from all analyzers. 
 
2.6.1 Tokenization 
In principle, tokenization, numbering and label-
ing of tokens (2.1), are processes that computers 
can handle with ease. It is important, though, 
that the tokenization is done in a way that sup-
ports the purpose to which it will be used. In this 
case, a tokenization module that only looks at 
spaces and separators will not be optimal, as the 
primary unit of TEM is semantic, and may span 
several text words. Moreover, punctuation marks 
are not treated as separate tokens in the TEM. 
This problem could be overcome by tokenizing 
in two steps. In the first step punctuation marks 
are removed, lexical tokens are identified using 
word lists and then formatted as character strings 
that have no internal spaces. In the second stage 
spaces are used to identify and number the to-
kens. Formal tokens can to a large extent be 
identified as part of this process, using word 
lists, but extra tokens cannot be identified until 
after the word alignment. 
 
2.6.2 Sentence alignment 
In some cases the translation task may require 
students not to change sentence boundaries and a 
one-to-one correspondence between source sen-
tences and sentences of the translations can be 
assumed to hold when translations are delivered. 
If not, a sentence alignment tool such as 
hunalign (Varga et al., 2005) can be used. 
 
2.6.3 Word alignment 
The accuracy of word alignment systems are 
quite far from 100%. The best performing sys-
tems are either statistical, such as Giza++ (Och 
& Ney, 2003), or hybrid (Moore et al., 2006) and 
require vast amounts of text to perform well. In 
the translation class context, the source text will 
be fairly short, perhaps a few thousand words as 
a maximum. Even with, say, 20 student transla-
tions, the total bitext, consisting of the source 
text repeated once for each student translation 
and sentence-aligned with it, will be too short for 
a statistical aligner to work well. For this reason, 
a hybrid system that relies on a combination of 
bilingual resources and statistics for the word 
alignment seems to be the best choice (cf. 
Ahrenberg & Tarvi, 2013). 
An advantage of having a short source text is 
that the teacher can develop a dictionary for it in 
advance to be used by the word aligner. While a 
teacher cannot predict all possible translations 
that a student may come up with, this is a re-
source that can be re-used and extended over 
several semesters and student groups. 
 
Table 3. Alignment performance on an excerpt from 
Anna Karenina using different combinations of statisti-
cal alignment and lexical resources. 
 Prec  Recall F-score 
Giza++ 0.499 0.497 0.498 
Wordlist based 0.881 0.366 0.517 
Combination 0.657 0.610 0.633 
Comb + filters 0.820 0.508 0.628 
 
Table 3 shows some results for the Russian-
English 405-token excerpt discussed above with 
different combinations of Giza++-output and 
lexicon-based alignments. Standard tokenization 
was used except that punctuation marks were 
deleted. The source then consists of eight itera-
tions of the excerpt, altogether 3304 tokens1 and 
the target text consisting of eight different trans-
lations has 4205 tokens. The files were lemma-
tized before alignment. 
The bilingual resources used are a word list of 
English function words such as articles and pos-
sessives that are likely to have no formal coun-
terpart in the source and a bilingual word list 
created by looking up content words in Google 
                                               
1
 Standard tokenization does not recognize multitoken 
units.  
38
Translate. Not all translations suggested by 
Google have been included. The mean number 
of translations per Russian lemma is 1.5. In the 
combinations priority has been given to the 
alignments proposed by the word lists as they are 
deemed to have a higher precision.2 So, the third 
row means that Giza++ alignments have been 
replaced by null links and lexical links induced 
by the word lists in all cases where there was a 
contradiction. The fourth row is the result of ap-
plying a set of filters based on word frequencies 
in the corpus and alignment topology to the pre-
vious combination. 
Obviously, if a complete alignment is called 
for it is clear that the output of the system must 
be reviewed and hand-aligned afterwards. There 
are several interactive word-alignment tools that 
can be used for this purpose (Tiedemann, 2011), 
but it will still be time-consuming. However, the 
burden can be shared between teacher and stu-
dents, and efforts may be focused on a part of 
the text only. 
2.7 Workflow 
After selection of a ST to be used for a transla-
tion exercise, the system will have it segmented 
into sentences, tokenized, and numbered. Then 
the teacher checks the outcome and corrects it if 
necessary. The files are then sent to the students. 
Within the suggested approach, the students are 
asked to use the client version of the system for 
translation and then upload their translations to 
their teacher by a set date before class, or to 
bring them to class on memory sticks.  
When a student translation is in place in the 
server system, it can be aligned and graded au-
tomatically. Of course, the significance of the 
grades depends on the accuracy of the alignment, 
but both the student and the teacher can contrib-
ute to the reviewing. For instance, the teacher 
might have marked some words and phrases as 
especially significant and the student can review 
the alignments for them in the system for his or 
her particular translation.  
3 In Class 
When translations and their alignments are in 
place in the server system, they can be used as 
                                               
2
 The fact that precision is not 100% for wordlist ba-
sed alignment has two major causes. First, some con-
tent words appear two or three times in a sentence and 
the system does not manage to pick the right occur-
rence. Also, some common English prepositions get 
aligned when they shouldn?t. 
input to various visualization tools. This we see 
as a further advantage of our approach which 
will stimulate discussion and reflections among 
the students. Students? translations can be dis-
played individually or collectively, on a sentence 
basis or a phrase basis. Using again the opening 
sentence of Anna Karenina as our example, the 
outcome for one sentence can look as in Figure 
1, where also some of the token frames de-
scribed above are automatically computed from 
the alignment.3 Within this format, the teacher is 
acting as a post-editing human agent who can 
combine both manners of assessment ? comput-
er-assisted and manual. 
Since the method combines human and com-
puter resources, it might raise the effectiveness 
of translation class instruction manifold 
(Lengyel 2006: 286). The TEM also depersonal-
izes the problem of grading.  
Figure 1. Alignment screenshot for Segment 1 of 
Translation 1 (Joel Carmichael, 1960) with metrics. 
 
3.1 From Translation Quotients to 
Grades 
As has been demonstrated, the TEM allows one 
to get a certain ?cline of fidelity? from the most 
faithful translation to the freest version. Based 
on these relative assessments, one can convert 
the cumulative figures obtained on a number of 
quantitative parameters to grades. It should be 
remembered that although the analytical ad-
                                               
3
 Alignments of the excerpts from Anna Karenina can 
be accessed at http://www.ida.liu.se/~lah/AnnaK/ 
39
vantage of the frames is that they are minimally 
subjective, the step from TQ to grades is neither 
context- nor value-free but depends heavily on 
the translation task.  
 
Table 4. From TQs to Grades 
 TQ  Rank Grade 
Magarshack 97,7 1 Excellent 
Carmichael 95,5 2 Good  
Garnett 95,0 3 Good 
The Maudes 92,5 4 Good -  
Edmonds 91,6 5 Good - 
Wiener 85,9 6 Satisfactory 
Townsend 82,9 7 Satisfactory - 
 
3.2 Gauging Quality  
The highlight of the approach is class team 
work, in the course of which students are ex-
pected to have a chance to insert meaningful cor-
rections into their translations and thus improve 
their ?home? grades by the end of class. Because 
the tokens are numbered, the teacher can easily 
bring any St, or a group of Sts, on the screen to-
gether with all the versions of its or their transla-
tions. 
 It is at this stage that the qualitative side of 
the TEM comes into play with the aim of im-
proving the final quantitative grade. Let us, for 
instance, consider the way a group of two tokens 
from the sentence-example has been rendered. 
As can be seen here in the manual (Table 5) and 
computer (Figure 2) versions, this pair of source 
tokens has been rendered in three different ways. 
In a computer-equipped class, the required 
changes can be immediately introduced into the 
translated texts under discussion. 
 
3.3 Final Grading 
As was mentioned in Section 1, the suggested 
procedure embraces the four basic types of trans-
lation evaluation. The method generates absolute 
score (overall estimates) based on relative scores 
in separate frames (Table 1).  
The first monitoring gives a quantitative esti-
mate of students? homework. After class discus-
sion, which is supposed, like any post-editing, to 
change the home translations for the better, one 
more monitoring is carried out, using the same 
frames. If the system is made incremental, the 
final grade, which is an arithmetic mean of the 
home and class grades, can be registered auto-
matically. If, at the end of class, the final grades 
are exhibited on screen in their ranking order, it 
might be the best possible motivation for stu-
dents to work diligently both at home and in 
class.  
 
Table 5. Renderings of Source tokens 12-13 
 
LW:  (12)in (Ft)its (13)own way. 
CG: (12)in (Ft)its (13)own way. 
ALM:  (12)in (Ft)its (13)own way. 
JC:  (12)in (Ft)its (13)own way. 
DM:  (12)in (Ft)its (13)own way.  
RPLV:  (12)in (Ft)its (13)own way.  
RE:  (12)after (Ft)its (13)own fashion.  
RT: (12)in (Ft)its (13)own (Et)particular way.  
 
Figure 2. Renderings of Source tokens 12-13 
(computed alignments) 
 
 
 
4 Outside Class  
Within machine translation research, work has 
been going on for several years, and is still very 
active, for the search of metrics that assess the 
similarity of a system translation with human 
reference translations. Metrics, such as BLEU 
(Papineni et al.. 2002), TER (Snover et al.. 
2006), and Meteor (Lavie and  Denkowski: 
2009), could also be included in the proposed 
environment. Published translations or transla-
tions that the teacher recognizes as particularly 
good can be used as reference translations. How-
ever, the scores of these metrics do not give as 
much qualitative information as the TEM 
frames.  
The role of corpora in translation and transla-
tion training is a topic of some interest (e.g. 
Zanettin et al.: 2003). In translator training, the 
corpora are mostly seen as resources for the stu-
dent to use when practicing translation (Lopez-
40
Rodriguez and Tercedor-Sanchez: 2008). This is 
orthogonal to what we are proposing here, i.e., 
enabling immediate comparisons and assess-
ments of students? translations as a class-based 
activity. A system with a similar purpose is re-
ported in Shei and Pain (2002: 323) who de-
scribe it as an ?intelligent tutoring system de-
signed to help student translators learn to appre-
ciate the distinction between literal and liberal 
translation?. Their system allows students to 
compare their own translations with reference 
translations and have them classified in terms of 
categories such as literal, semantic, and commu-
nicative. The comparisons are made one sen-
tence at a time, using the Dice coefficient, i.e., 
by treating the sentences as bags of words. Our 
proposal, in contrast, uses more advanced com-
putational linguistics tools and provides text lev-
el assessment based on word alignment. 
Michaud and McCoy (2013) describe a sys-
tem and a study where the goal, as in our pro-
posal, is to develop automatic support for trans-
lator training. They focus on the inverted TERp 
metric (Snover et al., 2009) for evaluation of 
student translations. TERp requires a reference 
translation but can represent the difference be-
tween a given translation and the reference in 
terms of editing operations such as insertion, 
deletion, change of word order and matches of 
different kinds. A weak positive correlation with 
instructor-based grades (using Pearson?s r) could 
be demonstrated in the study and the authors ar-
gue that TERp is sufficiently reliable to provide 
feed-back to students in a tutoring environment.  
The main difference between their proposal 
and ours is that we start with a metric that has 
been developed for the task of grading human 
translations, while TERp is originally an MT 
metric. Thus, TEM does not require reference 
translations, but on the other hand its computa-
tion has not been automated and so, that is where 
our current efforts are focused.  It should be em-
phasized that the teacher?s load within this ap-
proach remains quite heavy but the reviewing 
work may be shared between teachers and stu-
dents.  
Both the TEM and TERp provide quantitative 
measurements that can lay the foundation for 
qualitative discussions and feedback to students 
but as the TEM does not require a reference it 
gives the students more freedom in improving 
their work.  
As a more or less objective way of measuring 
the quantity with allowances made for quality, 
the method can also be used by teachers at ex-
ams, by editors for choosing a translation, by 
managers recruiting new in-house translators, by 
translators for self-monitoring, etc. The comput-
er-generated figures are obtained right on the 
spot ? they may not be exactly accurate but they 
give a rough general picture at the level of con-
tent-form ?presence? of the original in its transla-
tions. 
Acknowledgement 
We are grateful to the anonymous reviewers who 
provided useful comments and additional refer-
ences. 
 
References 
Ahrenberg, Lars and Tarvi, Ljuba. 2013. Natural lan-
guage processing for the translation class. Proceed-
ings of the second workshop on NLP for comput-
er-assisted language learning at NODALIDA 2013 
(May 22, Oslo). NEALT Proceedings Series 17 / 
Link?ping Electronic Conference Proceedings 86: 
1?10.  
Carmichael, Joel. 1960. Anna Karenina, by Lev Tol-
stoy. New York: Bantam Books, 1980.  
Edmonds, Rosemary. 1954. Anna Karenina, by Lev 
Tolstoy. London: The Folio Society, 1975.  
Garnett, Constance. 1901. Anna Karenina, by Leo 
Tolstoy, with revisions by Leonard J. Kent and Ni-
na Berberova. New York: Modern Library, 1993.  
Honkela, Timo et al. 2010. GIGA: Grounded 
Intersubjective Concept Analysis: A Method for 
Enhancing Mutual Understanding and Participa-
tion, Espoo: Aalto University School of Science 
and Technology.  
Lavie, Alon and Denkowski, Michael J. 2009. ?The 
Meteor metric for automatic evaluation of machine 
translation,? Machine Translation, Vol 23 (2-3) 
105-115. 
Lengyel, Istv?n. 2006. ?Book reviews. Ljuba Tarvi: 
Comparative Translation Assessment: Quantifying 
Quality,? Across Languages and Cultures 7 (2) 
2006, 284-286.  
Liang, Percy, Taskar, Ben, and Klein, Dan. 2006. 
?Alignment by Agreement.? In Proceedings of the 
Human Language Technology Conference of the 
North American Chapter of the Association for 
Computational Linguistics, 2006, 104-111. 
L?pez-Rodr?guez, Clara In?s and Tercedor-S?nchez, 
Mar?a Isabel. 2008. ?Corpora and Students' Auton-
omy in Scientific and Technical Translation train-
ing,? Journal of Specialized Translation 
(JoSTrans), Issue 09 (2008), 2-19. 
41
Magarschack, David. 1961. Anna Karenina, by Lev 
Tolstoy. New York: The New American Library.  
Maude, Louise and Maude, Aylmer. 1918. Anna 
Karenina, by Lev Tolstoy, a Norton Critical Edi-
tion, ed. George Gabian, with the Maude transla-
tion as revised by George Gibian. 2d edition. New 
York: Norton and Co, 1995.  
Michaud, Lisa N. and McCoy, Patricia Ann. 2013. 
Applying Machine Translation Metrics to Student-
Written Translations. Proceedings of the Eighth 
Workshop on Innovative Use of NLP for Building 
Educational Applications (BEA8), 2013, 306-311. 
Moore, Robert C, Yih, Wen-tau, and Bode, Anders. 
2006. ?Improved Discriminative Bilingual Word 
Alignment.? In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics 
and 44th Annual Meeting of the ACL, 2006, 513-
520. 
Papineni, Kishore, Roukos, Salim, Ward, Todd, and 
Zhu, Wei-Jing. 2002. ?BLEU: a method for auto-
matic evaluation of machine translation.? In Pro-
ceedings of the 40th Annual Meeting on Associa-
tion for Computational Linguistics, 311-318. 
Pevear, Richard & Volokhonsky, Larissa. 2000. Leo 
Tolstoy. Anna Karenina. Penguin Books.  
Shei, Chi-Chiang and Pain, Helen. 2002. ?Computer-
Assisted Teaching of Translation Methods.? Liter-
ary & Linguistic Computing, Vol, 17, No 3 (2002), 
323-343. 
Sheldon, Richard. 1997. ?Problems in the English 
Translation of Anna Karenina.? Essays in the Art 
and Theory of Translation, Lewiston-Queenston-
Lampeter: The Edwin Mellen Press, 1997 
Snover, Matthew, Dorr, Bonnie, Schwartz, Richard, 
Micciulla, Linnea and Makhoul, John. 2006. ?A 
Study of Translation Edit Rate with Targeted Hu-
man Annotation.? Proceedings of the 7th Confer-
ence of the Association for Machine Translation in 
the Americas, 223-231. 
Snover, Matthew, Madnani, Nitin, Dorr, Bonnie J., 
and Schwartz, Richard. 2009. Fluency, adequacy, 
or HTER? Exploring different judgments with a 
tunable MT metric. Proceedings of the EACL 
Fourth Workshop on Statistical Machine Transla-
tion, Athens, Greece, March 30-31, 2009: 259-268. 
Tarvi, Ljuba. 2004. Comparative Translation As-
sessment: Quantifying Quality, Helsinki: Helsinki 
University Press.  
Tiedemann. 2011. Bitext Alignment. Morgan & Clay-
pool Publishers. 
Townsend, Rochelle S. 1912. Anna Karenina, by 
Count Leo Tolstoi. London & Toronto: J.M. Dent 
& Sons; New york: E.P. Dutton and Co, 1928.  
Varga, Daniel, N?meth, Laszlo, Hal?csy, Peter, 
Kornai, Andras, Tr?n, Viktor, and Viktor Nagy, 
2005. Parallel corpora for medium density lan-
guages. Proceedings of RANLP 2005. 
Vinay, Jean-Paul & Darbelnet, Jean. 1995 [1958]. 
Comparative Stylistics of French and English. A 
Methodology for Translation, Amsterdam: John 
Benjamins.  
Wiener, Leo. 1899. Anna Karenina, by Lyof N. 
Tolstoi, vols II-IV: The Novels and Other Works of 
Lyof N. Tolstoi. New York: Charles Scribner?s 
Sons, 1904.  
Zanettin, Federico, Bernardini, Silvia, and Stewart, 
Dominic (eds.). 2003. Corpora in Translator Edu-
cation, Manchester. 
 
42
