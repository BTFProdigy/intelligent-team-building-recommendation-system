Proceedings of the 7th SIGdial Workshop on Discourse and Dialogue, pages 76?79,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Semantic tagging for resolution of indirect anaphora
R. Vieira1, E. Bick2, J. Coelho1, V. Muller1, S. Collovini1, J. Souza1, L. Rino3
UNISINOS1, University of Denmark2, UFSCAR3
renatav@unisinos.br, eckhard.bick@mail.dk, lucia@dc.ufscar.br
Abstract
This paper presents an evaluation of indi-
rect anaphor resolution which considers as
lexical resource the semantic tagging pro-
vided by the PALAVRAS parser. We de-
scribe the semantic tagging process and a
corpus experiment.
1 Introduction
Bridging anaphora represents a special part of the
general problem of anaphor resolution. As a spe-
cial case of anaphora, it has been studied and dis-
cussed by different authors and for various lan-
guages. There are many problems in develop-
ing such studies. First, bridging is not a regu-
lar class, it seldom contains cases of associative
and indirect anaphora (defined in the sequence);
lexical resources such as Wordnet are not avail-
able for every language, and even when available
such resources have proven to be insufficient for
the problem. In fact, different sources of lexi-
cal knowledge have been evaluated for anaphora
resolution (Poesio et al, 2002; Markert and Nis-
sim, 2005; Bunescu, 2003). At last, corpus stud-
ies of bridging anaphora usually report results
on a reduced number of examples, because this
kind of data is scarce. Usually bridging anaphora
considers two types: Associative anaphors are
NPs that have an antecedent that is necessary
to their interpretation (the relation between the
anaphor and its antecedent is different from iden-
tity); and Indirect anaphor are those that have
an identity relation with their antecedents but the
anaphor and its antecedent have different head-
nouns. In both associative and indirect anaphora,
the semantic relation holding between the anaphor
and its antecedent play an essential role for res-
olution. However, here we present an evalu-
ation of the semantic tagging provided by the
Portuguese parser PALAVRAS (Bick, 2000)
(http://visl.sdu.dk/visl/pt/parsing/automatic) as a
lexical resource for indirect anaphora resolution.
We focus on indirect anaphors for two reasons,
they are greater in number and they present better
agreement features concerning human annotation.
2 Semantic Annotation with Prototype
Tags
As a Constraint Grammar system, PALAVRAS
encodes all annotational information as word
based tags. A distinction is made between mor-
phological, syntactic, valency and semantic tags,
and for a given rule module (or level of analysis),
one tag type will be regarded as primary (= flagged
for disambiguation), while tags from lower lev-
els provide unambiguous context, and tags from
higher levels ambiguous lexical potentialities.
Thus, semantic tags are regarded as secondary
help tags at the syntactic level, but will have un-
dergone some disambiguation at the anaphora res-
olution level. The semantic noun classes were
conceived as distinctors rather than semantic de-
finitions, the goal being on the one hand to cap-
ture semantically motivated regularities and rela-
tions in syntax, on the other hand to allow to dis-
tinguish between different senses, or to chose dif-
ferent translation equivalents in MT applications.
A limited set of semantic prototype classes was
deamed ideal for both purposes, since it allows at
the same time similarity-based lumping of words
(useful in structural analysis, IR, anaphora reso-
lution) and context based polysemy resolution for
an individual word (useful in MT, lexicography,
alignment). Though we define class hypernyms
as prototypes in the Roschian sense (Rosch, 1978)
76
as an (idealized) best instance of a given class of
entities, we avoided low level prototypes, using
<Azo> for four-legged land-animals rather than
<dog> and <cat> for dog and cat races etc.).
Where possible, systematic sub-classes were es-
tablished. Semiotic artifacts <sem>, for instance
are sub-divided into ?readables? <sem-r> (book-
prototype: book, paper, magazine), ?watchables?
<sem-w> (film, show, spectacle), ?listenables?
etc. The final category inventory, though devel-
oped independently, resembles the ontology used
in the multilingual European SIMPLE project
(http://www.ub.es/ gilcub/SIMPLE/simple.html).
For the sake of rule based inheritance reasoning,
semantic prototype classes were bundled using a
matrix of 16 atomic semantic features. Thus,
the atomic feature +MOVE is shared by the dif-
ferent human and animal prototypes as well as
the vehicle prototype, but the vehicle prototype
lacks the +ANIM feature, and only the bun-
dle on human prototypes (<Hprof>, <Hfam>,
<Hideo>,...) shares the +HUM feature (human
professional, human family, human follower of a
theory/belief/conviction/ideology). In the parser,
a rule selecting the +MOVE feature (e.g. for sub-
jects of movement verbs) will help discard com-
peting senses from lemmas with the above proto-
types, since they will all inherit choices based on
the shared atomic feature. Furthermore, atomic
features can themselves be subjected to inheri-
tance rules, e.g. +HUM ?> +ANIM ?> +CON-
CRETE, or +MOVE?> +MOVABLE. In Table 1,
which contains examples of polysemic institution
nouns, positive features are marked with capital
letters, negative features with small letters1. The
words in the Table 1 are ambiguous with regard
to the feature H, and since it is only the <inst>
prototype that contributes the +HUM feature po-
tential, it can be singled out by a rule selecting
?H? or by discarding ?h?. The parser?s about 140
prototypes have been manually implemented for a
lexicon of about 35.000 nouns. In addition, the
?HUM category was also introduced as a selec-
tion restriction for 2.000 verb senses (subject re-
striction) and 1.300 adjective senses (head restric-
tion).
While the semantic annotation of common
nouns is carried out by disambiguating a given
lemma?s lexicon-listed prototype potential, this
strategy is not sufficient for proper nouns, due
1furn=furniture, con=container, inst=institution
Ee = entities (?CONCRETE)
Jj = ?MOVABLE
Hh = ?HUMAN ENTITY
Mm = ?MAS
Ll = ?LOCATION
polysemy spectrum
Ee j Hh m Ll faculdade
E H L <inst> univ. faculty
e h l <f-c> property
Ee j Hh m Ll fundo
e h L <Labs> bottom
E H L <inst> foundation
e h l <ac> <smP> funds
Ee j Hh Mm Ll indu?stria
E H m L <inst> industry
e h M l <am> diligence
E Jj Hh m L rede
J h <con> net
j H <inst> <+n> network
J h <furn> hammock
Table 1: Feature bundles in prototype based poly-
semy
to the productive nature of this word class. In
two recent NER projects, the parser was aug-
mented with a pattern recognition module and a
rule-based module for identifying and classify-
ing names. In the first project (Bick, 2003),
6 main classes with about 15 subclasses were
used in a lexeme-based approach, while the
second adopted the 41 largely functional cate-
gories of Linguateca?s joint HAREM evaluation
in 2005 (http://www.linguateca.com). A lexicon-
registered name like Berlin would have a stable
tag (<civ> = civitas) in the first version, while
it would be tagged as either <hum>, <top> or
<org> in the second, dependent on context. At
the time of writing, we have not yet tagged our
anaphora corpus with name type tags, and it is
unclear which approach, lexematic or functional,
will work best for the resolution of indirect and
associative anaphora.
3 Indirect Anaphora Resolution
Our work was based on a corpus formed by 31
newspaper articles, from Folha de Sa?o Paulo, writ-
ten in Brazilian Portuguese. The corpus was au-
tomatically parsed using the parser PALAVRAS,
and manually annotated for anaphoricity using
the MMAX tool(http://mmax.eml-research.de/) .
Four subjects annotated the corpus. All annota-
tors agreed on the antecedent in 73% of the cases,
in other 22% of the cases there was agreement be-
tween three annotators and in 5% of the cases only
two annotators agreed. There were 133 cases of
77
definite Indirect anaphors (NPs starting with def-
inite articles) from the total of 1454 definite de-
scriptions (near to 10%) and 2267 NPs.
The parser gives to each noun of the text (or to
most of them) a semantic tag. For instance, the
noun japone?s [japanese] has the following seman-
tic tags ling and Hnat, representing the features:
human nationality and language respectively.
<word id="word_28">
<n can="japone?s" gender="M" number="S">
<secondary_n tag="Hnat"/>
<secondary_n tag="ling"/>
</n>
</word>
The approach consists in finding relationships
with previous nouns through the semantic tags.
The chosen antecedent will be the nearest expres-
sion with the largest number of equal semantic
tags. For instance, in the example below, the
anaphor is resolved by applying this resolution
principle, to japone?s - a l??ngua.
O Eurocenter oferece cursos de japone?s em Kanazawa.
Apo?s um me?s, o aluno falara? modestamente a l??ngua.
The Eurocenter offers Japanese courses in Kanazawa. Af-
ter one month, a student can modestly speak the language.
As both expressions (japanese and language)
hold the semantic tag ?ling? the anaphor is re-
solved. For the experiments, we considered as cor-
rect the cases where the antecedent found automat-
ically was the same as in the manual annotation
(same), and also the cases in which the antecedent
of the manual annotation was found further up in
the chain identified automatically (in-chain). We
also counted those cases in which the antecedent
of the manual annotation was among the group of
candidates sharing the same tags (in-candidates),
but was not the chosen one (the chosen being the
nearest with greater number of equal tags).
Indirect anaphora
Results # % of Total
Same 25 19%
In-chain 15 11%
Total Correct 40 30%
In-candidates 9 7%
Unsolved 40 30%
Error 44 33%
Total 133 100%
Table 2: Indirect anaphor resolution
Table 2 shows the results of the indirect anaphor
resolution. In 19% of the cases, the system found
the same antecedent as marked in the manual an-
notation. Considering the chain identified by the
system the correct cases go up to 30%. The great
number of unsolved cases were related to the fact
that proper names were not tagged. Considering
mainly the tagged nouns (about 93 cases), the cor-
rect cases amount to 43%). This gives us an idea
of the quality of the tags for the task. We further
tested if increasing the weight of more specific
features in opposition to the more general ones
would help in the antecedent decision process. A
semantic tag that is more specific receives a higher
weight The semantic tag set has three levels, level
1, which is more general receives weight 1, level 2
receives 5, and level 3 receives 10. See the exam-
ple below.
<A> 1 Animal, umbrella tag
<AA> 5 Group of animals
<Adom> 10 Domestic animal
In this experiment the chosen candidate is the
nearest one whose sum of equal tag values has
higher weight. Table 3 shows just a small im-
provement in the correct cases. If we do not
consider unsolved cases, mostly related to proper
names, indirect anaphors were correctly identified
in 46% of the cases (43/96).
Indirect anaphora
Results # % of Total
Same 24 18%
In-chain 19 14%
Total Correct 43 32%
In-candidates 6 5%
Unsolved 40 30%
Error 44 33%
Total 133 100%
Table 3: Indirect anaphor - weighting schema
Since there is no semantic tagging for proper
names as yet, the relationship between pairs such
as Sa?o Carlos - a cidade [Sa?o Carlos - the city]
could not be found. Regarding wrong antecedents,
we have seen that some semantic relationships are
weaker, having no semantic tags in common, for
instance: a proposta - o aumento [the proposal -
the rise]. In some cases the antecedent is not a
previous noun phrase but a whole sentence, para-
graph or disjoint parts of the text. As we con-
sider only relations holding between noun phrases,
these cases could not be resolved. Finally, there
are cases of plain heuristic failure. For instance,
establishing a relationship between os professores
78
[the teachers], with the semantic tags H and Hprof,
and os politicos [the politicians], with the seman-
tic tags H and Hprof, when the correct antecedent
was os docentes [the docents], with the semantic
tags HH (group of humans) and Hprof.
4 Final Remarks
Previous work on nominal anaphor resolution has
used lexical knowledge in different ways. (Poe-
sio et al, 1997) presented results concerning the
resolution of bridging definitions, using the Word-
Net (Fellbaum, 1998), where bridging DDs en-
close our Indirect and Associative anaphora. Poe-
sio et al reported 35% of recall for synonymy,
56% for hypernymy and 38% for meronymy.
(Schulte im Walde, 1997) evaluated the bridg-
ing cases presented in (Poesio et al, 1997), on
the basis of lexical acquisition from the British
National Corpus. She reported a recall of 33%
for synonymy, 15% for hypernymy and 18% for
meronymy. (Poesio et al, 2002) considering syn-
tactic patterns for lexical knowledge acquisition,
obtained better results for resolving meronymy
(66% of recall). (Gasperin and Vieira, 2004)
tested the use of word similarity lists on resolv-
ing indirect anaphora, reporting 33% of recall.
(Markert and Nissim, 2005) presented two ways
(WordNet and Web) of obtaining lexical knowl-
edge for antecedent selection in coreferent DDs
(Direct and Indirect anaphora). Markert and
Nissim achieved 71% of recall using Web-based
method and 65% of recall using WordNet-based
method. We can say that our results are very sat-
isfactory, considering the related work. Note that
usually evaluation of bridging anaphora is made
on the basis of a limited number of cases, because
the data is sparse. Our study was based on 133
examples, which is not much but surpasses some
of the previous related work. Mainly, our results
indicate that the semantic tagging provided by the
parser is a good resource for dealing with the prob-
lem, if compared to other lexical resources such as
WordNet and acquired similarity lists. We believe
that the results will improve significantly once se-
mantic tags for proper names are provided by the
parser. This evaluation is planned as future work.
Acknowledgments
This work was partially funded by CNPq.
References
Eckhard Bick. 2000. The Parsing System PALAVRAS:
Automatic Grammatical Analysis of Protuguese in
a Constraint Grammar Framework. Ph.D. thesis,
Arhus University, Arhus.
Eckhard Bick. 2003. Multi-level ner for portuguese in
a cg framework. In Nuno J. et al Mamede, editor,
Computational Processing of the Portuguese Lan-
guage (Procedings of the 6th International Work-
shop, PROPOR 2003), number 2721 in Lecture
Notes in Computer Science, pages 118?125, Faro,
Portugal. Springer.
Razvan Bunescu. 2003. Associative anaphora reso-
lution: A web-based approach. In Proceedings of
the Workshop on The Computational Treatment of
Anaphora - EACL 2003, Budapest.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge,
MA.
Caroline Gasperin and Renata Vieira. 2004. Us-
ing word similarity lists for resolving indirect
anaphora. In Proceedings of ACL Workshop on Ref-
erence Resolution and its Applications, pages 40?
46, Barcelona.
Katja Markert and Malvina Nissim. 2005. Comparing
knowledge sources for nominal anaphora resolution.
Computational Linguistics, 31(3):367?401.
Massimo Poesio, Renata Vieira, and Simone Teufel.
1997. Resolving bridging descriptions in un-
restricted texts. In Proceedings of the Work-
shop on Operational Factors In Practical, Robust,
Anaphora Resolution for Unrestricted Texts, pages
1?6, Madrid.
Masimo Poesio, Ishikawa Tomonori, Sabine Shulte im
Walde, and Renata Vieira. 2002. Acquiring lexical
knowledge for anaphora resolution. In Proceedings
of 3rd Language resources and evaluation confer-
ence LREC 2002, Las Palmas.
Eleanor Rosch. 1978. Principles of categorization.
In E. Rosch and B. Lloyd, editors, Cognition and
Categorization, pages 27?48. Hillsdale, New Jersey:
Lawrence Erlbaum Associate.
Sabine Schulte im Walde. 1997. Resolving Bridging
Descriptions in High-Dimensional Space Resolving
Bridging Descriptions in High-Dimensional Space.
Ph.D. thesis, Institut fu?r Maschinelle Sprachverar-
beitung, Universita?t Stuttgart, and Center for Cogni-
tive Science, University of Edinburgh, Edinburgh.
79
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 79?84,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
QuEst - A translation quality estimation framework
Lucia Specia?, Kashif Shah?, Jose G. C. de Souza? and Trevor Cohn?
?Department of Computer Science
University of Sheffield, UK
{l.specia,kashif.shah,t.cohn}@sheffield.ac.uk
?Fondazione Bruno Kessler
University of Trento, Italy
desouza@fbk.eu
Abstract
We describe QUEST, an open source
framework for machine translation quality
estimation. The framework allows the ex-
traction of several quality indicators from
source segments, their translations, exter-
nal resources (corpora, language models,
topic models, etc.), as well as language
tools (parsers, part-of-speech tags, etc.). It
also provides machine learning algorithms
to build quality estimation models. We
benchmark the framework on a number of
datasets and discuss the efficacy of fea-
tures and algorithms.
1 Introduction
As Machine Translation (MT) systems become
widely adopted both for gisting purposes and to
produce professional quality translations, auto-
matic methods are needed for predicting the qual-
ity of a translated segment. This is referred to as
Quality Estimation (QE). Different from standard
MT evaluation metrics, QE metrics do not have
access to reference (human) translations; they are
aimed at MT systems in use. QE has a number of
applications, including:
? Deciding which segments need revision by a
translator (quality assurance);
? Deciding whether a reader gets a reliable gist
of the text;
? Estimating how much effort it will be needed
to post-edit a segment;
? Selecting among alternative translations pro-
duced by different MT systems;
? Deciding whether the translation can be used
for self-training of MT systems.
Work in QE for MT started in the early 2000?s,
inspired by the confidence scores used in Speech
Recognition: mostly the estimation of word pos-
terior probabilities. Back then it was called confi-
dence estimation, which we believe is a narrower
term. A 6-week workshop on the topic at John
Hopkins University in 2003 (Blatz et al, 2004)
had as goal to estimate automatic metrics such as
BLEU (Papineni et al, 2002) and WER. These
metrics are difficult to interpret, particularly at the
sentence-level, and results of their very many trials
proved unsuccessful. The overall quality of MT
was considerably lower at the time, and therefore
pinpointing the very few good quality segments
was a hard problem. No software nor datasets
were made available after the workshop.
A new surge of interest in the field started re-
cently, motivated by the widespread used of MT
systems in the translation industry, as a conse-
quence of better translation quality, more user-
friendly tools, and higher demand for translation.
In order to make MT maximally useful in this
scenario, a quantification of the quality of trans-
lated segments similar to ?fuzzy match scores?
from translation memory systems is needed. QE
work addresses this problem by using more com-
plex metrics that go beyond matching the source
segment with previously translated data. QE can
also be useful for end-users reading translations
for gisting, particularly those who cannot read the
source language.
QE nowadays focuses on estimating more inter-
pretable metrics. ?Quality? is defined according to
the application: post-editing, gisting, etc. A num-
ber of positive results have been reported. Exam-
ples include improving post-editing efficiency by
filtering out low quality segments which would re-
quire more effort or time to correct than translating
from scratch (Specia et al, 2009; Specia, 2011),
selecting high quality segments to be published as
they are, without post-editing (Soricut and Echi-
habi, 2010), selecting a translation from either
an MT system or a translation memory for post-
editing (He et al, 2010), selecting the best trans-
lation from multiple MT systems (Specia et al,
79
2010), and highlighting sub-segments that need re-
vision (Bach et al, 2011).
QE is generally addressed as a supervised ma-
chine learning task using a variety of algorithms to
induce models from examples of translations de-
scribed through a number of features and anno-
tated for quality. For an overview of various al-
gorithms and features we refer the reader to the
WMT12 shared task on QE (Callison-Burch et
al., 2012). Most of the research work lies on
deciding which aspects of quality are more rel-
evant for a given task and designing feature ex-
tractors for them. While simple features such as
counts of tokens and language model scores can be
easily extracted, feature engineering for more ad-
vanced and useful information can be quite labour-
intensive. Different language pairs or optimisation
against specific quality scores (e.g., post-editing
time vs translation adequacy) can benefit from
very different feature sets.
QUEST, our framework for quality estimation,
provides a wide range of feature extractors from
source and translation texts and external resources
and tools (Section 2). These go from simple,
language-independent features, to advanced, lin-
guistically motivated features. They include fea-
tures that rely on information from the MT sys-
tem that generated the translations, and features
that are oblivious to the way translations were
produced (Section 2.1). In addition, by inte-
grating a well-known machine learning toolkit,
scikit-learn,1 and algorithms that are known
to perform well on this task, QUEST provides a
simple and effective way of experimenting with
techniques for feature selection and model build-
ing, as well as parameter optimisation through grid
search (Section 2.2). In Section 3 we present
experiments using the framework with nine QE
datasets.
In addition to providing a practical platform
for quality estimation, by freeing researchers from
feature engineering, QUEST will facilitate work
on the learning aspect of the problem. Quality
estimation poses several machine learning chal-
lenges, such as the fact that it can exploit a large,
diverse, but often noisy set of information sources,
with a relatively small number of annotated data
points, and it relies on human annotations that are
often inconsistent due to the subjectivity of the
task (quality judgements). Moreover, QE is highly
1http://scikit-learn.org/
non-linear: unlike many other problems in lan-
guage processing, considerable improvements can
be achieved using non-linear kernel techniques.
Also, different applications for the quality predic-
tions may benefit from different machine learn-
ing techniques, an aspect that has been mostly ne-
glected so far. Finally, the framework will also
facilitate research on ways of using quality predic-
tions in novel extrinsic tasks, such as self-training
of statistical machine translation systems, and for
estimating quality in other text output applications
such as text summarisation.
2 The QUEST framework
QUEST consists of two main modules: a feature
extraction module and a machine learning mod-
ule. The first module provides a number of feature
extractors, including the most commonly used fea-
tures in the literature and by systems submitted to
the WMT12 shared task on QE (Callison-Burch et
al., 2012). More than 15 researchers from 10 in-
stitutions contributed to it as part of the QUEST
project.2 It is implemented in Java and provides
abstract classes for features, resources and pre-
processing steps so that extractors for new features
can be easily added.
The basic functioning of the feature extraction
module requires raw text files with the source and
translation texts, and a few resources (where avail-
able) such as the source MT training corpus and
language models of source and target. Configura-
tion files are used to indicate the resources avail-
able and a list of features that should be extracted.
The machine learning module provides
scripts connecting the feature files with the
scikit-learn toolkit. It also uses GPy, a
Python toolkit for Gaussian Processes regression,
which outperformed algorithms commonly used
for the task such as SVM regressors.
2.1 Feature sets
In Figure 1 we show the types of features that
can be extracted in QUEST. Although the text
unit for which features are extracted can be of any
length, most features are more suitable for sen-
tences. Therefore, a ?segment? here denotes a sen-
tence.
From the source segments QUEST can extract
features that attempt to quantify the complexity
2http://www.dcs.shef.ac.uk/?lucia/
projects/quest.html
80
Confidence indicatorsComplexity indicators Fluency indicators
Adequacyindicators
Source text TranslationMT system
Figure 1: Families of features in QUEST.
of translating those segments, or how unexpected
they are given what is known to the MT system.
Examples of features include:
? number of tokens in the source segment;
? language model (LM) probability of source
segment using the source side of the parallel
corpus used to train the MT system as LM;
? percentage of source 1?3-grams observed in
different frequency quartiles of the source
side of the MT training corpus;
? average number of translations per source
word in the segment as given by IBM 1
model with probabilities thresholded in dif-
ferent ways.
From the translated segments QUEST can ex-
tract features that attempt to measure the fluency
of such translations. Examples of features include:
? number of tokens in the target segment;
? average number of occurrences of the target
word within the target segment;
? LM probability of target segment using a
large corpus of the target language to build
the LM.
From the comparison between the source and
target segments, QUEST can extract adequacy
features, which attempt to measure whether the
structure and meaning of the source are pre-
served in the translation. Some of these are based
on word-alignment information as provided by
GIZA++. Features include:
? ratio of number of tokens in source and target
segments;
? ratio of brackets and punctuation symbols in
source and target segments;
? ratio of percentages of numbers, content- /
non-content words in the source & target seg-
ments;
? ratio of percentage of nouns/verbs/etc in the
source and target segments;
? proportion of dependency relations between
(aligned) constituents in source and target
segments;
? difference between the depth of the syntactic
trees of the source and target segments;
? difference between the number of
PP/NP/VP/ADJP/ADVP/CONJP phrases in
the source and target;
? difference between the number of per-
son/location/organization entities in source
and target sentences;
? proportion of person/location/organization
entities in source aligned to the same type of
entities in target segment;
? percentage of direct object personal or pos-
sessive pronouns incorrectly translated.
When available, information from the MT sys-
tem used to produce the translations can be very
useful, particularly for statistical machine transla-
tion (SMT). These features can provide an indi-
cation of the confidence of the MT system in the
translations. They are called ?glass-box? features,
to distinguish them from MT system-independent,
?black-box? features. To extract these features,
QUEST assumes the output of Moses-like SMT
systems, taking into account word- and phrase-
alignment information, a dump of the decoder?s
standard output (search graph information), global
model score and feature values, n-best lists, etc.
For other SMT systems, it can also take an XML
file with relevant information. Examples of glass-
box features include:
? features and global score of the SMT system;
? number of distinct hypotheses in the n-best
list;
? 1?3-gram LM probabilities using translations
in the n-best to train the LM;
? average size of the target phrases;
? proportion of pruned search graph nodes;
? proportion of recombined graph nodes.
We note that some of these features are
language-independent by definition (such as the
confidence features), while others can be depen-
dent on linguistic resources (such as POS taggers),
or very language-specific, such as the incorrect
translation of pronouns, which was designed for
Arabic-English QE.
Some word-level features have also been im-
plemented: they include standard word posterior
probabilities and n-gram probabilities for each tar-
81
get word. These can also be averaged across the
whole sentence to provide sentence-level value.
The complete list of features available is given
as part of QUEST?s documentation. At the current
stage, the number of BB features varies from 80
to 123 depending on the language pair, while GB
features go from 39 to 48 depending on the SMT
system used (see Section 3).
2.2 Machine learning
QUEST provides a command-line interface mod-
ule for the scikit-learn library implemented
in Python. This module is completely indepen-
dent from the feature extraction code and it uses
the extracted feature sets to build QE models.
The dependencies are the scikit-learn li-
brary and all its dependencies (such as NumPy3
and SciPy4). The module can be configured to
run different regression and classification algo-
rithms, feature selection methods and grid search
for hyper-parameter optimisation.
The pipeline with feature selection and hyper-
parameter optimisation can be set using a con-
figuration file. Currently, the module has an
interface for Support Vector Regression (SVR),
Support Vector Classification, and Lasso learn-
ing algorithms. They can be used in conjunction
with the feature selection algorithms (Randomised
Lasso and Randomised decision trees) and the grid
search implementation of scikit-learn to fit
an optimal model of a given dataset.
Additionally, QUEST includes Gaussian Pro-
cess (GP) regression (Rasmussen and Williams,
2006) using the GPy toolkit.5 GPs are an ad-
vanced machine learning framework incorporating
Bayesian non-parametrics and kernel machines,
and are widely regarded as state of the art for
regression. Empirically we found the perfor-
mance to be similar to SVR on most datasets,
with slightly worse MAE and better RMSE.6 In
contrast to SVR, inference in GP regression can
be expressed analytically and the model hyper-
parameters optimised directly using gradient as-
cent, thus avoiding the need for costly grid search.
This also makes the method very suitable for fea-
ture selection.
3http://www.numpy.org/
4http://www.scipy.org/
5https://github.com/SheffieldML/GPy
6This follows from the optimisation objective: GPs use a
quadratic loss (the log-likelihood of a Gaussian) compared to
SVR which penalises absolute margin violations.
Data Training Test
WMT12 (en-es) 1,832 422
EAMT11 (en-es) 900 64
EAMT11 (fr-en) 2,300 225
EAMT09-s1-s4 (en-es) 3,095 906
GALE11-s1-s2 (ar-en) 2,198 387
Table 1: Number of sentences used for training
and testing in our datasets.
3 Benchmarking
In this section we benchmark QUEST on nine ex-
isting datasets using feature selection and learning
algorithms known to perform well in the task.
3.1 Datasets
The statistics of the datasets used in the experi-
ments are shown in Table 1.7
WMT12 English-Spanish sentence translations
produced by an SMT system and judged for
post-editing effort in 1-5 (worst-best), taking a
weighted average of three annotators.
EAMT11 English-Spanish (EAMT11-en-es)
and French-English (EAMT11-fr-en) sentence
translations judged for post-editing effort in 1-4.
EAMT09 English sentences translated by four
SMT systems into Spanish and scored for post-
editing effort in 1-4. Systems are denoted by s1-s4.
GALE11 Arabic sentences translated by two
SMT systems into English and scored for ade-
quacy in 1-4. Systems are denoted by s1-s2.
3.2 Settings
Amongst the various learning algorithms available
in QUEST, to make our results comparable we se-
lected SVR with radial basis function (RBF) ker-
nel, which has been shown to perform very well
in this task (Callison-Burch et al, 2012). The op-
timisation of parameters is done with grid search
using the following ranges of values:
? penalty parameter C: [1, 10, 10]
? ?: [0.0001, 0.1, 10]
? : [0.1, 0.2, 10]
where elements in list denote beginning, end and
number of samples to generate, respectively.
For feature selection, we have experimented
with two techniques: Randomised Lasso and
7The datasets can be downloaded from http://www.
dcs.shef.ac.uk/?lucia/resources.html
82
Gaussian Processes. Randomised Lasso (Mein-
shausen and Bu?hlmann, 2010) repeatedly resam-
ples the training data and fits a Lasso regression
model on each sample. A feature is said to be se-
lected if it was selected (i.e., assigned a non-zero
weight) in at least 25% of the samples (we do this
1000 times). This strategy improves the robust-
ness of Lasso in the presence of high dimensional
and correlated inputs.
Feature selection with Gaussian Processes is
done by fitting per-feature RBF widths (also
known as the automatic relevance determination
kernel). The RBF width denotes the importance
of a feature, the narrower the RBF the more impor-
tant a change in the feature value is to the model
prediction. To make the results comparable with
our baseline systems we select the 17 top ranked
features and then train a SVR on these features.8
As feature sets, we select all features available
in QUEST for each of our datasets. We differen-
tiate between black-box (BB) and glass-box (GB)
features, as only BB are available for all datasets
(we did not have access to the MT systems that
produced the other datasets). For the WMT12 and
GALE11 datasets, we experimented with both BB
and GB features. For each dataset we build four
systems:
? BL: 17 baseline features that performed well
across languages in previous work and were
used as baseline in the WMT12 QE task.
? AF: All features available for dataset.
? FS: Feature selection for automatic ranking
and selection of top features with:
? RL: Randomised Lasso.
? GP: Gaussian Process.
Mean Absolute Error (MAE) and Root Mean
Squared Error (RMSE) are used to evaluate the
models.
3.3 Results
The error scores for all datasets with BB features
are reported in Table 2, while Table 3 shows the re-
sults with GB features, and Table 4 the results with
BB and GB features together. For each table and
dataset, bold-faced figures are significantly better
than all others (paired t-test with p ? 0.05).
It can be seen from the results that adding more
BB features (systems AF) improves the results in
most cases as compared to the baseline systems
8More features resulted in further performance gains on
most tasks, with 25?35 features giving the best results.
Dataset System #feats. MAE RMSE
WMT12
BL 17 0.6802 0.8192
AF 80 0.6703 0.8373
FS(RL) 69 0.6628 0.8107
FS(GP) 17 0.6537 0.8014
EAMT11(en-es)
BL 17 0.4867 0.6288
AF 80 0.4696 0.5438
FS(RL) 29 0.4657 0.5424
FS(GP) 17 0.4640 0.5420
EAMT11(fr-en)
BL 17 0.4387 0.6357
AF 80 0.4275 0.6211
FS(RL) 65 0.4266 0.6196
FS(GP) 17 0.4240 0.6189
EAMT09-s1
BL 17 0.5294 0.6643
AF 80 0.5235 0.6558
FS(RL) 73 0.5190 0.6516
FS(GP) 17 0.5195 0.6511
EAMT09-s2
BL 17 0.4604 0.5856
AF 80 0.4734 0.5973
FS(RL) 59 0.4601 0.5837
FS(GP) 17 0.4610 0.5825
EAMT09-s3
BL 17 0.5321 0.6643
AF 80 0.5437 0.6827
FS(RL) 67 0.5338 0.6627
FS(GP) 17 0.5320 0.6630
EAMT09-s4
BL 17 0.3583 0.4953
AF 80 0.3569 0.5000
FS(RL) 40 0.3554 0.4995
FS(GP) 17 0.3560 0.4949
GALE11-s1
BL 17 0.5456 0.6905
AF 123 0.5359 0.6665
FS(RL) 56 0.5358 0.6649
FS(GP) 17 0.5410 0.6721
GALE11-s2
BL 17 0.5532 0.7177
AF 123 0.5381 0.6933
FS(RL) 54 0.5369 0.6955
FS(GP) 17 0.5424 0.6999
Table 2: Results with BB features.
Dataset System #feats. MAE RMSE
WMT12 AF 47 0.7036 0.8476FS(RL) 26 0.6821 0.8388
FS(GP) 17 0.6771 0.8308
GALE11-s1 AF 39 0.5720 0.7392FS(RL) 46 0.5691 0.7388
FS(GP) 17 0.5711 0.7378
GALE11-s2
AF 48 0.5510 0.6977
FS(RL) 46 0.5512 0.6970
FS(GP) 17 0.5501 0.6978
Table 3: Results with GB features.
Dataset System #feats. MAE RMSE
WMT12 AF 127 0.7165 0.8476FS(RL) 26 0.6601 0.8098
FS(GP) 17 0.6501 0.7989
GALE11-s1 AF 162 0.5437 0.6741FS(RL) 69 0.5310 0.6681
FS(GP) 17 0.5370 0.6701
GALE11-s2
AF 171 0.5222 0.6499
FS(RL) 82 0.5152 0.6421
FS(GP) 17 0.5121 0.6384
Table 4: Results with BB and GB features.
83
BL, however, in some cases the improvements are
not significant. This behaviour is to be expected
as adding more features may bring more relevant
information, but at the same time it makes the rep-
resentation more sparse and the learning prone to
overfitting. In most cases, feature selection with
both or either RL and GP improves over all fea-
tures (AF). It should be noted that RL automati-
cally selects the number of features used for train-
ing while FS(GP) was limited to selecting the top
17 features in order to make the results compara-
ble with our baseline feature set. It is interesting
to note that system FS(GP) outperformed the other
systems in spite of using fewer features. This tech-
nique is promising as it reduces the time require-
ments and overall computational complexity for
training the model, while achieving similar results
compared to systems with many more features.
Another interesting question is whether these
feature selection techniques identify a common
subset of features from the various datasets. The
overall top ranked features are:
? LM perplexities and log probabilities for
source and target;
? size of source and target sentences;
? average number of possible translations of
source words (IBM 1 with thresholds);
? ratio of target by source lengths in words;
? percentage of numbers in the target sentence;
? percentage of distinct unigrams seen in the
MT source training corpus.
Interestingly, not all top ranked features are
among the baseline 17 features which are report-
edly best in literature.
GB features on their own perform worse than
BB features, but in all three datasets, the combi-
nation of GB and BB followed by feature selec-
tion resulted in significantly lower errors than us-
ing only BB features with feature selection, show-
ing that the two features sets are complementary.
4 Remarks
The source code for the framework, the datasets
and extra resources can be downloaded from
http://www.quest.dcs.shef.ac.uk/.
The project is also set to receive contribution from
interested researchers using a GitHub repository:
https://github.com/lspecia/quest.
The license for the Java code, Python and shell
scripts is BSD, a permissive license with no re-
strictions on the use or extensions of the software
for any purposes, including commercial. For pre-
existing code and resources, e.g., scikit-learn, GPy
and Berkeley parser, their licenses apply, but fea-
tures relying on these resources can be easily dis-
carded if necessary.
Acknowledgments
This work was supported by the QuEst (EU
FP7 PASCAL2 NoE, Harvest program) and QT-
LaunchPad (EU FP7 CSA No. 296347) projects.
References
N. Bach, F. Huang, and Y. Al-Onaizan. 2011. Good-
ness: a method for measuring machine translation
confidence. In ACL11, pages 211?219, Portland.
J. Blatz, E. Fitzgerald, G. Foster, S. Gandrabur,
C. Goutte, A. Kulesza, A. Sanchis, and N. Ueffing.
2004. Confidence Estimation for Machine Transla-
tion. In Coling04, pages 315?321, Geneva.
C. Callison-Burch, P. Koehn, C. Monz, M. Post,
R. Soricut, and L. Specia. 2012. Findings of the
2012 workshop on statistical machine translation. In
WMT12, pages 10?51, Montre?al.
Y. He, Y. Ma, J. van Genabith, and A. Way. 2010.
Bridging SMT and TM with Translation Recom-
mendation. In ACL10, pages 622?630, Uppsala.
N. Meinshausen and P. Bu?hlmann. 2010. Stability se-
lection. Journal of the Royal Statistical Society: Se-
ries B (Statistical Methodology), 72:417?473.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
BLEU: a method for automatic evaluation of ma-
chine translation. In ACL02, pages 311?318,
Philadelphia.
C.E. Rasmussen and C.K.I. Williams. 2006. Gaus-
sian processes for machine learning, volume 1. MIT
Press, Cambridge.
R. Soricut and A. Echihabi. 2010. Trustrank: Induc-
ing trust in automatic translations via ranking. In
ACL11, pages 612?621, Uppsala.
L. Specia, M. Turchi, N. Cancedda, M. Dymetman,
and N. Cristianini. 2009. Estimating the Sentence-
Level Quality of Machine Translation Systems. In
EAMT09, pages 28?37, Barcelona.
L. Specia, D. Raj, and M. Turchi. 2010. Ma-
chine translation evaluation versus quality estima-
tion. Machine Translation, 24(1):39?50.
L. Specia. 2011. Exploiting objective annotations
for measuring translation post-editing effort. In
EAMT11, pages 73?80, Leuven.
84
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 701?705,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
FBK: Cross-Lingual Textual Entailment Without Translation
Yashar Mehdad
FBK-irst
Trento , Italy
mehdad@fbk.eu
Matteo Negri
FBK-irst
Trento , Italy
negri@fbk.eu
Jose? Guilherme C. de Souza
FBK-irst & University of Trento
Trento, Italy
desouza@fbk.eu
Abstract
This paper overviews FBK?s participation
in the Cross-Lingual Textual Entailment
for Content Synchronization task organized
within SemEval-2012. Our participation is
characterized by using cross-lingual matching
features extracted from lexical and semantic
phrase tables and dependency relations. The
features are used for multi-class and binary
classification using SVMs. Using a combi-
nation of lexical, syntactic, and semantic fea-
tures to create a cross-lingual textual entail-
ment system, we report on experiments over
the provided dataset. Our best run achieved
an accuracy of 50.4% on the Spanish-English
dataset (with the average score and the me-
dian system respectively achieving 40.7% and
34.6%), demonstrating the effectiveness of a
?pure? cross-lingual approach that avoids in-
termediate translations.
1 Introduction
So far, cross-lingual textual entailment (CLTE)
(Mehdad et al, 2010) has been applied to: i)
available TE datasets (?YES?/?NO? uni-directional
relations between monolingual pairs) transformed
into their cross-lingual counterpart by translating
the hypotheses into other languages (Negri and
Mehdad, 2010), and ii) machine translation evalu-
ation datasets (Mehdad et al, 2012b). The content
synchronization task represents a challenging appli-
cation scenario to test the capabilities of CLTE sys-
tems, by proposing a richer inventory of phenomena
(i.e. ?Bidirectional?/?Forward?/?Backward?/?No
entailment? multi-directional entailment relations).
Multi-directional CLTE recognition can be seen
as the identification of semantic equivalence and in-
formation disparity between two topically related
sentences, at the cross-lingual level. This is a core
aspect of the multilingual content synchronization
task, which represents a challenging application sce-
nario for a variety of NLP technologies, and a shared
research framework for the integration of semantics
and MT technology.
The CLTE methods proposed so far adopt either
a ?pivoting approach? (translation of the two in-
put texts into the same language, as in (Mehdad et
al., 2010)), or an ?integrated solution? that exploits
bilingual phrase tables to capture lexical relations
and contextual information (Mehdad et al, 2011).
The promising results achieved with the integrated
approach still rely on phrasal matching techniques
that disregard relevant semantic aspects of the prob-
lem. By filling this gap integrating linguistically
motivated features, in our participation, we propose
an approach that combines lexical, syntactic and se-
mantic features within a machine learning frame-
work (Mehdad et al, 2012a).
Our submitted runs have been produced by train-
ing and optimizing multiclass and binary SVM clas-
sifiers, over the Spanish-English (Spa-Eng) devel-
opment set. In both cases, our results were posi-
tive, showing significant improvements over the me-
dian systems and average scores obtained by partic-
ipants. The overall results confirm the difficulty of
the task, and the potential of our approach in com-
bining linguistically motivated features in a ?pure?
cross-lingual approach that avoids the recourse to
external MT components.
701
2 Experiments
In our experiment we used the Spa-Eng portion of
the dataset described in (Negri et al, 2012; Negri
et al, 2011), consisting of 500 multi-directional en-
tailment pairs which was provided to train the sys-
tems and 500 pairs for the submission. Each pair in
the dataset is annotated with ?Bidirectional?, ?For-
ward?, ?Backward? or ?No entailment? judgements.
2.1 Approach
Our system builds on the integration of lexical,
syntactic and semantic features in a supervised
learning framework. Our model builds on three
main feature sets, respectively derived from: i)
phrase tables, ii) dependency relations, and iii)
semantic phrase tables.
1. Phrase Table (PT) matching: through
these features, a semantic judgement about entail-
ment is made exclusively on the basis of lexical
evidence. The matching features are calculated
with a phrase-to-phrase matching process. A phrase
in our approach is an n-gram composed of one
or more (up to 5) consecutive words, excluding
punctuation. Entailment decisions are assigned
combining phrasal matching scores calculated for
each level of n-grams (i.e. considering the number
of 1-grams, 2-grams,..., 5-grams extracted from H
that match with n-grams in T). Phrasal matches,
performed either at the level of tokens, lemmas, or
stems, can be of two types:
1. Exact: in the case that two phrases are identical
at one of the three levels (token, lemma, stem).
2. Lexical: in the case that two different phrases
can be mapped through entries of the resources
used to bridge T and H (i.e. phrase tables).
For each phrase in H, we first search for exact
matches at the level of token with phrases in T. If
no match is found at a token level, the other levels
(lemma and stem) are attempted. Then, in case of
failure with exact matching, lexical matching is per-
formed at the same three levels. To reduce redun-
dant matches, the lexical matches between pairs of
phrases which have already been identified as exact
matches are not considered.
Once the matching phase for each n-gram
level has been concluded, the number of matches
Matchn and the number of phrases in the hypoth-
esis H(n) is used to estimate the portion of phrases
in H that are matched at each level n (Equation 1).1
Since languages can express the same meaning with
different amounts of words, a phrase with length n
in H can match a phrase with any length in T.
Matchn =
Matchn
|H(n)|
(1)
In order to build English-Spanish phrase tables
for our experiments, we used the freely available
Europarl V.4, News Commentary and United
Nations Spanish-English parallel corpora released
for the WMT10 Shared Translation Task.2 We
run the TreeTagger (Schmid, 1995) and Snowball
stemmer (Porter, 2001) for preprocessing, and used
the Giza++ (Och and Ney, 2000) toolkit to align the
tokenized corpora at the word level. Subsequently,
we extracted the bi-lingual phrase table from the
aligned corpora using the Moses toolkit (Koehn et
al., 2007).
2. Dependency Relation (DR) matching tar-
gets the increase of CLTE precision. By adding
syntactic constraints to the matching process,
DR features aim to reduce wrong matches often
occurring at the lexical level. For instance, the con-
tradiction between ?Yahoo acquired Overture? and
?Overture compro? Yahoo? is evident when syntax
(in this case subject-object inversion) is taken into
account, but can not be caught by bag-of-words
methods.
We define a dependency relation as a triple that
connects pairs of words through a grammatical rela-
tion. For example, ?nsubj (loves, John)? is a depen-
dency relation with head loves and dependent John
connected by the relation nsubj, which means that
?John? is the subject of ?loves?. DR matching cap-
tures similarities between dependency relations, by
combining the syntactic and lexical level. In a valid
match, while the relation has to be the same (?exact?
1When checking for entailment from H to T, the normaliza-
tion is carried out dividing the number of n-grams in H by the
number of n-grams in T. The same holds for dependency rela-
tion and semantic phrase table matching.
2http://www.statmt.org/wmt10/
702
match), the connected words must be either the same
or semantically equivalent in the two languages. For
example, ?nsubj (loves, John)? can match ?nsubj
(ama, John)? and ?nsubj (quiere, John)? but not
?dobj (quiere, John)?.
Given the dependency tree representations of T
and H, for each grammatical relation (r) we calcu-
late a DR matching score (Matchr, see Equation 2)
as the number of matching occurrences of r in T and
H (respectively DRr(T ) and DRr(H)), divided by
the number of occurrences of r in H.
matchr =
|match(DRr(T ), DRr(H))|
|DRr(H)|
(2)
In our experiments, in order to extract de-
pendency relation (DR) matching features, the
dependency tree representations of English and
Spanish texts have been produced with DepPattern
(Otero and Lopez, 2011). We then mapped the
sets of dependency relation labels for the English-
Spanish parser output into: Adjunct, Determiner,
Object, Subject and Preposition. The dictionary,
containing about 9M bilingual word pairs, created
during the alignment of the English-Spanish parallel
corpora provided the lexical knowledge to perform
matches when the connected words are different.
3. Semantic Phrase Table (SPT) matching:
represents a novel way to leverage the integration
of semantics and MT-derived techniques. To this
aim, SPT improves CLTE methods relying on pure
lexical match, by means of ?generalized? phrase
tables annotated with shallow semantic labels.
Semantically enhanced phrase tables, with entries in
the form ?[LABEL] word1...wordn [LABEL]? (e.g.
?[ORG] acquired [ORG]?), are used as a recall-
oriented complement to the lexical phrase tables
used in machine translation (token-based entries like
?Yahoo acquired Overture?). The main motivation
for this augmentation is that word replacement with
semantic tags allows to match T-H tokens that do
not occur in the original bilingual parallel corpora
used for phrase table extraction. Our hypothesis
is that the increase in recall obtained from relaxed
matches through semantic tags in place of ?out of
vocabulary? terms (e.g. unseen person, location, or
organization names) is an effective way to improve
CLTE performance, even at the cost of some loss in
precision. Semantic phrase tables, however, have
two additional advantages. The first is related to
their smaller size and, in turn, its positive impact
on system?s efficiency, due to the considerable
search space reduction. Semantic tags allow to
merge different sequences of tokens into a single tag
and, consequently, different phrase entries can be
unified to one semantic phrase entry. As a result, for
instance, the SPT used in our experiments is more
than 30% smaller than the original token-based one.
The second advantage relates to their potential im-
pact on the confidence of CLTE judgements. Since
a semantic tag might cover more than one token
in the original entry phrase, SPT entries are often
short generalizations of longer original phrases.
Consequently, the matching process can benefit
from the increased probability of mapping higher
order n-grams (i.e. those providing more contextual
information) from H into T and vice-versa.
Like lexical phrase tables, SPTs are extracted
from parallel corpora. As a first step, we annotate
the corpora with named-entity taggers (FreeLing in
our case (Carreras et al, 2004)) for the source and
target languages, replacing named entities with gen-
eral semantic labels chosen from a coarse-grained
taxonomy including the categories: person, location,
organization, date and numeric expression. Then,
we combine the sequences of unique labels into one
single token of the same label, and we run Giza++
(Och and Ney, 2000) to align the resulting seman-
tically augmented corpora. Finally, we extract the
semantic phrase table from the augmented aligned
corpora using the Moses toolkit (Koehn et al, 2007).
For the matching phase, we first annotate T and
H in the same way we labeled our parallel corpora.
Then, for each n-gram order (n=1 to 5, excluding
punctuation), we use the SPT to calculate a matching
score (SPT matchn, see Equation 3), as the num-
ber of n-grams in H that match with phrases in T
divided by the number of n-grams in H. The match-
ing algorithm is same as the phrase table matching
one.
SPT matchn =
|SPTn(H) ? SPT (T )|
|SPTn(H)|
(3)
703
Run Features Classification Parameter selection Result
1 PT+SPT+DR Multiclass Entire training set 0.502
2 PT+SPT+DR Multiclass 2-fold cross validation 0.490
3 PT+SPT+DR Binary Entire training set 0.504
4 PT+SPT+DR Binary 2-fold cross validation 0.500
Table 1: Summary of the submitted runs and results for Spa-Eng dataset.
Forward Backward No entailment Bidirectional
P R F1 P R F1 P R F1 P R F1
0.515 0.704 0.595 0.546 0.568 0.557 0.447 0.304 0.362 0.482 0.440 0.460
Table 2: Best run?s Precision/Recall/F1 scores.
In our supervised learning framework, the com-
puted PT, SPT and DR scores are used as sepa-
rate features, giving to an SVM classifier, LIBSVM
(Chang and Lin, 2011), the possibility to learn opti-
mal feature weights from training data.
2.2 Submitted runs
In order to test our models under different condi-
tions, we set the CLTE problem both as two-way and
multiclass classification tasks.
Two-way classification casts multidirectional en-
tailment as a unidirectional problem, where each
pair is analyzed checking for entailment both from
left to right and from right to left. In this condi-
tion, each original test example is correctly clas-
sified if both pairs originated from it are correctly
judged (?YES-YES? for bidirectional, ?YES-NO?
for forward, ?NO-YES? for backward entailment,
and ?NO-NO? for no entailment). Two-way clas-
sification represents an intuitive solution to capture
multidirectional entailment relations but, at the same
time, a suboptimal approach in terms of efficiency
since two checks are performed for each pair.
Multiclass classification is more efficient, but at
the same time more challenging due to the higher
difficulty of multiclass learning, especially with
small datasets. We also tried to use the parameter se-
lection tool for C-SVM classification using the RBF
(radial basis function) kernel, available in LIBSVM
package. Our submitted runs and results have been
obtained with the settings summarized in table 1.
As can be seen from the table, our best result has
been achieved by Run 3 (50.4% accuracy), which
is significantly higher than the average and median
score over the best runs obtained by participants
(44.0% and 40.7% respectively). The detailed re-
sults achieved by the best run are reported in Table
2. We can observe that our system is performing
well for recognizing the unidirectional entailment
(i.e. forward and backward), while the performance
drops over no entailment pairs. The low results for
bidirectional cases also reflect the difficulty of dis-
criminating the no entailment pairs from the bidi-
rectional ones. Looking at the detailed results, we
can observe a high recall in the forward and back-
ward entailment cases, which could be explained by
the effectiveness of the semantic phrase table match-
ing features aiming at coverage increase over lexi-
cal methods. Adding more linguistically motivated
features and weighting the non-matched phrases can
be a starting point to improve the overall results for
other cases (bidirectional and no entailment).
3 Conclusion
In this paper we described our participation to the
cross-lingual textual entailment for content synchro-
nization task at SemEval-2012. We approached this
task by combining lexical, syntactic and semantic
features, at the cross-lingual level without recourse
to intermediate translation steps. In spite of the
difficulty and novelty of the task, our results on
the Spanish-English dataset (0.504) prove the effec-
tiveness of the approach with significant improve-
ments over the reported average and median accu-
racy scores for the 29 submitted runs (respectively
40.7% and 34.6%).
Acknowledgments
This work has been partially supported by the EU-
funded project CoSyne (FP7-ICT-4-248531).
704
References
X. Carreras, I. Chao, L. Padro?, and M. Padro?. 2004.
FreeLing: An Open-Source Suite of Language An-
alyzers. In Proceedings of the 4th International
Conference on Language Resources and Evaluation
(LREC?04).
C.C. Chang and C.J. Lin. 2011. LIBSVM: A Library
for Support Vector Machines. ACM Transactions on
Intelligent Systems and Technology (TIST), 2(3).
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open Source Toolkit for
Statistical Machine Translation. In Proceedings of the
45th Annual Meeting of the ACL on Interactive Poster
and Demonstration Sessions (ACL 2007).
Y. Mehdad, M. Negri, and M. Federico. 2010. Towards
Cross-Lingual Textual Entailment. In Proceedings of
the 11th Annual Conference of the North American
Chapter of the Association for Computational Linguis-
tics (NAACL HLT 2010).
Y. Mehdad, M. Negri, and M. Federico. 2011. Using
Bilingual Parallel Corpora for Cross-Lingual Textual
Entailment. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies (ACL HLT 2011).
Y. Mehdad, M. Negri, and M. Federico. 2012a. Detect-
ing Semantic Equivalence and Information Disparity
in Cross-lingual Documents. In Proceedings of the
ACL?12.
Y. Mehdad, M. Negri, and M. Federico. 2012b. Match
without a Referee: Evaluating MT Adequacy without
Reference Translations. In Proceedings of the Ma-
chine Translation Workshop (WMT2012).
M. Negri and Y. Mehdad. 2010. Creating a Bi-lingual
Entailment Corpus through Translations with Mechan-
ical Turk: $100 for a 10-day rush. In Proceedings of
the NAACL HLT 2010 Workshop on Creating Speech
and Language Data with Amazon?s Mechanical Turk,
pages 212?216. Association for Computational Lin-
guistics.
M. Negri, L. Bentivogli, Y. Mehdad, D. Giampiccolo, and
A. Marchetti. 2011. Divide and conquer: Crowd-
sourcing the creation of cross-lingual textual entail-
ment corpora. In Proceedings of EMNLP 2011.
M. Negri, A. Marchetti, Y. Mehdad, L. Bentivogli, and
D. Giampiccolo. 2012. Semeval-2012 Task 8: Cross-
lingual Textual Entailment for Content Synchroniza-
tion. In Proceedings of the 6th International Workshop
on Semantic Evaluation (SemEval 2012).
F.J. Och and H. Ney. 2000. Improved Statistical Align-
ment Models. In Proceedings of the 38th Annual
Meeting of the Association for Computational Linguis-
tics (ACL 2000).
P.G. Otero and I.G. Lopez. 2011. A Grammatical For-
malism Based on Patterns of Part-of-Speech Tags. In-
ternational journal of corpus linguistics, 16(1).
M. Porter. 2001. Snowball: A language for stemming
algorithms.
H. Schmid. 1995. Treetaggera language indepen-
dent part-of-speech tagger. Institut fu?r Maschinelle
Sprachverarbeitung, Universita?t Stuttgart.
705
