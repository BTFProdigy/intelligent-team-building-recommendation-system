MiTAP for SARS Detection 
 
 
Laurie E. Damianos, Samuel Bayer, 
Michael A. Chisholm, John Henderson,  
Lynette Hirschman, William Morgan, 
Marc Ubaldino, Guido Zarrella 
The MITRE Corporation 
202 Burlington Road 
Bedford, MA 01730 
{laurie, sam, chisholm, 
jhndrsn, lynette, wmorgan, 
ubaldino,jzarrella}@mitre.org 
James M. Wilson, V, MD and  
Marat G. Polyak 
Division of Integrated Biodefense 
ISIS Center, Georgetown University 
2115 Wisconsin Avenue Suite 603 
Washington, DC 20007 
{wilson, mgp5} 
@isis.imac.georgetown.edu 
 
Abstract 
The MiTAP prototype for SARS detection 
uses human language technology for detect-
ing, monitoring, and analyzing potential indi-
cators of infectious disease outbreaks and 
reasoning for issuing warnings and alerts. Mi-
TAP focuses on providing timely, multi-
lingual information access to analysts, domain 
experts, and decision-makers worldwide. Data 
sources are captured, filtered, translated, 
summarized, and categorized by content. 
Critical information is automatically extracted 
and tagged to facilitate browsing, searching, 
and scanning, and to provide key terms at a 
glance. The processed articles are made avail-
able through an easy-to-use news server and 
cross-language information retrieval system 
for access and analysis anywhere, any time. 
Specialized newsgroups and customizable fil-
ters or searches on incoming stories allow us-
ers to create their own view into the data 
while a variety of tools summarize, indicate 
trends, and provide alerts to potentially rele-
vant spikes of activity. 
1 
2 
Background 
Potentially catastrophic biological events that threaten 
US national security are steadily increasing in fre-
quency. These events pose immediate danger to ani-
mals, plants, and humans. Current disease surveillance 
systems are inadequate for detecting indicators early 
enough to ensure the rapid response needed to combat 
these biological events and corresponding public reac-
tion. Recent examples of outbreaks include both the 
HIV/AIDS and foot and mouth pandemics, the spread of 
West Nile virus to and across the US, the escape of Rift 
Valley Fever from Africa, SARS, and the translocation 
of both mad cow disease (BSE) and monkey pox to the 
United States.  
Biological surveillance systems in the United States 
rely most heavily on human medical data for signs of 
epidemic activity. These systems span multiple organi-
zations and agencies, are often not integrated, and have 
no alerting capability. As a result, responders have an 
insufficient amount of lead time to prepare for biologi-
cal events or catastrophes. 
Indications and Warnings (I&Ws) provide the poten-
tial for early alert of impending biological events, per-
haps weeks to months in advance. Sources of I&Ws 
include transportation data, telecommunication traffic, 
economic indices, Internet news, RSS feeds (RSS) in-
cluding weblogs, commerce, agricultural surveillance, 
weather, and other environmental data. Retrospective 
analyses of major infectious disease outbreaks (e.g., 
West Nile Virus and SARS) show that I&Ws were pre-
sent weeks to months in advance, but these indicators 
were missed because data sources were difficult to ob-
tain and hard to integrate. As a result, the available in-
formation was not utilized for appropriate national and 
international response. This illuminates a critical need in 
biodefense for an integrated system linking I&Ws for 
biological events from multiple and disparate sources 
with the response community. 
Introduction 
MiTAP (Damianos et al 2002) was originally devel-
oped by the MITRE Corporation under the Defense 
Advanced Research Projects Agency (DARPA) 
Translingual Information Detection Extraction and 
Summarization (TIDES) program. TIDES aims to revo-
lutionize the way that information is obtained from hu-
man language by enabling people to find and interpret 
relevant information quickly and effectively, regardless 
of language or medium. MiTAP was initially created for 
tracking and monitoring infectious disease outbreaks 
and other biological threats as part of a DARPA Inte-
grated Feasibility Experiment in biosecurity to explore 
the integration of synergistic TIDES language process-
ing technologies applied to a real world domain. The 
system has since been expanded to other domains such 
as weapons of mass destruction, satellite monitoring, 
and suspect terrorist activity. In addition, researchers 
and analysts are examining hundreds of MiTAP data 
sources for differing perspectives on conflict and hu-
manitarian relief efforts. 
Our newest MiTAP prototype explores the integra-
tion of outputs from operational data mining (anomaly 
detection), human language technology (information 
extraction, temporal tagging, machine translation, cross-
language information retrieval), and visualization tools 
to detect SARS-specific I&Ws in Asia, with relevance 
to pathogen translocation to the United States. Using 
feeds from English and Chinese language newswire, 
weblogs, and other Internet data, the system translates 
Chinese text data and tracks keyword combinations 
thought to represent I&Ws specific to SARS outbreaks 
in China. Analysts can use cross-language information 
retrieval for retrospective analysis and improving the 
I&W model, save searches to use as filters on incoming 
data, view trends, and visualize the data along a time-
line. Figure 1 shows an overview of the prototype. 
Warnings generated by this MiTAP prototype are in-
tended to complement traditional biosurveillance and 
communications already in use by the international pub-
lic health community. This system represents an expan-
sion of current US surveillance capabilities to detect 
biological agents of catastrophic potential.
 
 
Figure 1 Overview of the MiTAP prototype for SARS detection. 
3 Component Technologies 
The MiTAP prototype relies extensively on human 
language technology and expert system reasoning. 
Below, MiTAP capabilities are described briefly 
along with their contributing component 
technologies. 
3.1 
3.2 
3.3 
3.4 
3.5 
Information Processing 
After Internet news sources are captured and 
normalized, they are passed through a zoner using 
human-generated rules to identify source, date, and 
other information such as headline, or title, and 
content. The Alembic natural language analyzer (Ab-
erdeen et al 1995; Vilain and Day 1996) processes 
the zoned messages to identify paragraph, sentence, 
and word boundaries as well as part-of-speech tags. 
The messages then pass through the Alembic named 
entity recognizer for identification and tagging of 
person, organization, location, and disease names. 
Finally, the article is processed by the TempEx 
normalizing time expression tagger (Mani and Wil-
son 2000). 
For Chinese and other non-English sources, the 
CyberTrans machine translation system (Miller et al 
2001) is used to translate articles automatically into 
English. CyberTrans wraps commercial and research 
translation engines to produce a common set of 
interfaces; the current prototype makes use of the 
SYSTRAN Chinese-English system.  
RSS feeds can provide a high volume textual ge-
stalt.  Weblogs, in particular, are a good source of 
timely text, some of which is topical and all of which 
is based on personal observations and experiences. 
Aggregate measurements on these feeds can provide 
indications of public health-related phenom-
ena.  Consider the relative rates of words and phrases 
such as "stay home from" or "pneumonia.?  Geotem-
poral location of non-seasonal spikes in relative rank 
of these strings can establish suspicion for further 
investigation by I&W experts. 
Browsing 
English language data and pairs of foreign language 
documents and their translated versions are made 
available on a news server (INN 2001) for browsing. 
The system categorizes and bins articles into 
newsgroups based on their content. To do this, the 
system relies on a combination of the information 
extraction results as well as human-generated rules 
for pattern matching. Newsgroups are created to 
provide multiple perspectives on the data; analysts 
can subscribe to specific disease tracking 
newsgroups, regional newsgroups, specific data 
source newsgroups, or to customized topic tracking 
newsgroups that may be based on several related 
subjects. 
Tagged entities in each article are color-coded to 
enable rapid scanning of information and easy identi-
fication of key names. The five most frequently men-
tioned locations in each article as well as the top five 
people are presented as a list for quick reference. 
Information Retrieval 
To supplement access to the articles on the news 
server and to allow for retrospective analysis, articles 
are indexed using the Lucene information retrieval 
system (The Jakarta Project 2001) for English 
language documents and using PSE (Darwish 2002) 
for foreign language documents. Web links are 
maintained between foreign language documents and 
their translated versions to allow for more accurate 
human translations of selected documents. 
Analysts can perform full text, source-specific 
queries over the entire set of archived documents and 
view the retrieved results as a relevance-ranked list or 
as a plot across a timeline. A cross-language informa-
tion retrieval interface allows users to search in Eng-
lish across the Chinese language sources. 
Users can also save specific search constraints to 
be used as filters on incoming data. These saved 
searches provide a simple analytic capability as well 
as an alerting feature. (See below.) 
Analysis 
To assist analysts in identifying relevant and related 
articles, we have integrated multi-document summa-
rization and watch lists. Columbia University?s 
Newsblaster (McKeown et al 2002) automatically 
detects daily topics, clusters MiTAP articles around 
those topics, and generates multi-document summari-
zations which are made available on the news server. 
Multiple technologies (e.g., coreference, information 
extraction) from Alias I, Inc. (Baldwin et al 2002) 
produces comprehensive views on specific named 
entities (i.e., people or disease) across MiTAP docu-
ments. These views are summarized through ranked 
lists, highlighting important topics of the day and 
activities which might indicate disease outbreak.  
Finely-tuned searches can be saved and applied as 
filters or topic tracking mechanisms. These saved 
searches are automatically updated at specific inter-
vals and can be aggregated and displayed visually as 
bar graphs to reveal spikes of activity that otherwise 
might go undetected. 
Alerting 
The MiTAP prototype has two separate alerting ca-
pabilities: saved searches and an integrated expert 
system. The saved search functionality allows ana-
lysts to set thresholds for alerting purposes. For ex-
ample, MiTAP can send email when any new article 
arrives, when a specified maximum number of arti-
cles arrives, or when the daily number of new articles 
increases by some percentage of the total or moving 
average. 
The Human Language Indication Detector 
(HLID) performs data fusion on a number of dispa-
rate sources, compressing a large volume of informa-
tion into a smaller but more significant set of alerts. 
HLID monitors a variety of sources including MiTAP 
articles, information events in RSS feeds, and other 
dynamically updated information on the World Wide 
Web. HLID analyzes events from these sources in 
real time and generates an estimate of significance 
for each, complete with an audit trail of supporting 
and negating evidence. This allows an analyst to di-
rect a search for indicators towards interesting data 
while reducing the time spent investigating false 
alarms and insignificant events.  
HLID is composed of four major components. 
The first is an event collector, which monitors a data 
source and triggers action when an event is observed. 
These events are sent to the rule based reasoning en-
gine, an expert system shell (JESS 2004) with hand 
authored rules. The engine performs vetting and ini-
tial investigation of each event by identifying corre-
lated events, corroborating or invalidating evidence, 
and references to supporting information. The engine 
can also supplement its knowledge base by perform-
ing a directed search via the query management sys-
tem, which allows retrieval of information from a 
wide variety of sources including databases and web 
pages. Lastly, the alerting mechanism disseminates 
the conclusions reached by the system and provides 
an interface that allows an analyst to launch a deeper 
search for indicators and warnings. 
4 
5 
Acknowledgments 
This work has been funded, in part, by the Defense 
Advanced Research Projects Agency Translingual 
Information Detection Extraction and Summarization 
program under contract numbers DAAB07-01-C-
C201 and W15P7T-04-C-D001, the Office of the 
Secretary of Defense in support of the Coalition Pro-
visional Authority in Baghdad, and a MITRE Special 
Initiative for Rapid Integration of Novel Indications 
and Warnings for SARS. 
References 
Aberdeen, J., Burger, J., Day, D., Hirschman, L., 
Robinson, P., and Vilain, M. 1995. MITRE: De-
scription of the Alembic System as Used for 
MUC-6. In Proceedings of the Sixth Message Un-
derstanding Conference (MUC-6). 
Baldwin, B,, Moore, M., Ross, A., Shah, D.  2002. 
Trinity Information Access System. Proceedings of 
Human Lanuage Technology Conference, San 
Diego, CA. 
Damianos, L., Ponte, J., Wohlever, S., Reeder, F., 
Day, D., Wilson, G., Hirschman, L. 2002. MiTAP, 
Text and Audio Processing for Bio-Security: A 
Case Study In Proceedings of IAAI-2002: The 
Fourteenth Innovative Applications of Artificial 
Intelligence Conference, Edmonton, Alberta, Can-
ada. 
Darwish, K. PSE: A Small Search Engine written in 
Perl 2002 
http://tides.umiacs.umd.edu/software.html 
INN: InterNetNews, Internet Software Consortium 
2001, http://www.isc.org/products/INN.  
The Jakarta Project, 2001 
http://jakarta.apache.org/lucene/docs/index.html. 
JESS: the Rule Engine for the Java? Platform 2004 
http://herzberg.ca.sandia.gov/jess/  
Mani, I. and Wilson, G. 2000. Robust Temporal 
Processing of News. In Proceedings of the 38th 
Annual Meeting of the Association for Computa-
tional Linguistics (ACL'2000), 69-76. 
McKeown, K., Barzilay, R., Evan, D., Hatzivassi-
loglou, V., Klavans, J., Sable, C., Schiffman, B., 
Sigelman, S. 2002. Tracking and Summarizing 
News on a Daily Basis with Columbia's Newsblas-
ter. In Proceedings of HLT 2002: Human Lan-
guage Technology Conference. 
Miller, K., Reeder, F., Hirschman, L., Palmer, D. 
2001. Multilingual Processing for Operational 
Users, NATO Workshop on Multilingual Process-
ing at EUROSPEECH. 
RSS RDF Site Summary http://purl.org/rss/1.0/spec 
Vilain, M. and Day, D. 1996. Finite-state phrase 
parsing by rule sequences. In Proceedings of the 
1996 International Conference on Computational 
Linguistics (COLING-96), Copenhagen, Denmark. 
Direct Maximization of Average Precision by Hill-Climbing, with a
Comparison to a Maximum Entropy Approach
William Morgan and Warren Greiff and John Henderson
The MITRE Corporation
202 Burlington Road MS K325
Bedford, MA 01730
{wmorgan, greiff, jhndrsn}@mitre.org
Abstract
We describe an algorithm for choosing term
weights to maximize average precision. The
algorithm performs successive exhaustive
searches through single directions in weight
space. It makes use of a novel technique for
considering all possible values of average pre-
cision that arise in searching for a maximum in
a given direction. We apply the algorithm and
compare this algorithm to a maximum entropy
approach.
1 Introduction
This paper presents an algorithm for searching term
weight space by directly hill-climbing on average pre-
cision. Given a query and a topic?that is, given a set
of terms, and a set of documents, some of which are
marked ?relevant??the algorithm chooses weights that
maximize the average precision of the document set when
sorted by the sum of the weighted terms. We show that
this algorithm, when used in the larger context of finding
?optimal? queries, performs similar to a maximum en-
tropy approach, which does not climb directly on average
precision.
This work is part of a larger research program on the
study of optimal queries. Optimal queries, for our pur-
poses, are queries that best distinguish relevant from non-
relevant documents for a corpus drawn from some larger
(theoretical) population of documents. Although both
performance on the training data and generalization abil-
ity are components of optimal queries, in this paper we
focus only on the former.
2 Motivation
Our initial approach to the study of optimal queries em-
ployed a conditional maximum entropy model. This
model exhibited some problematic behavior, which mo-
tivated the development of the weight search algorithm
described here.
The maximum entropy model is used as follows. It is
given a set of relevant and non-relevant documents and a
vector of terms (the query). For any document, the model
predicts the probability of relevance for that document
based on the Okapi term frequency (tf ) scores (Robertson
and Walker, 1994) for the query terms within it. Queries
are developed by starting with the best possible one-term
query and adding individual terms from a candidate set
chosen according to a mutual information criterion. As
each term is added, the model coefficients are set to max-
imize the probability of the empirical data (the document
set plus relevance judgments), as described in Section 4.
Treating the model coefficients as term weights yields
a weighted query. This query produces a retrieval status
value (RSV) for each document that is a monotonically
increasing function of the probability of relevance, in ac-
cord with the probability ranking principle (Robertson,
1977). We can then calculate the average precision of the
document set as ordered by these RSVs.
As each additional query term represents another de-
gree of freedom, one would expect model performance to
improve at each step. However, we noted that the addition
of a new term would occasionally result in a decrease in
average precision?despite the fact that the model could
have chosen a zero weight for the newly added term.
Figure 1 shows an example of this phenomenon for one
TREC topic.
This is the result of what might be called ?metric di-
vergence?. While we use average precision to evaluate
the queries, the maximum entropy model maximizes the
likelihood of the training data. These two metrics occa-
sionally disagree in their evaluation of particular weight
vectors. In particular, maximum entropy modeling may
favor increasing the estimation of documents lower in the
ranking at the expense of accuracy in the prediction of
highly ranked documents. This can increase training data
likelihood yet have a detrimental effect on average preci-
sion.
The metric divergence problem led us to consider an al-
ternative approach for setting term weights which would
hill-climb on average precision directly. In particular, we
were interested in evaluating the results produced by the
maximum entropy approach?how much was the maxi-
mization of likelihood affecting the ultimate performance
as measured by average precision? The algorithm de-
scribed in the following section was developed to this
end.
3 The Weight Search Algorithm
The general behavior of the weight search algorithm is
similar to the maximum entropy modeling described in
Section 2?given a document corpus and a term vector,
it seeks to maximize average precision by choosing a
weight vector that orders the documents optimally. Un-
like the maximum entropy approach, the weight search
algorithm hill-climbs directly on average precision.
The core of the algorithm is an exhaustive search of a
single direction in weight space. Although each direction
is continuous and unbounded, we show that the search
can be performed with a finite amount of computation.
This technique arises from a natural geometric interpreta-
tion of changes in document ordering and how they affect
average precision.
At the top level, the algorithm operates by cycling
through different directions in weight space, performing
an exhaustive search for a maximum in each direction,
until convergence is reached. Although a global maxi-
mum is found in each direction, the algorithm relies on a
greedy assumption of unimodality and, as with the max-
imum entropy model, is not guaranteed to find a global
maximum in the multi-dimensional space.
3.1 Framework
This section formalizes the notion of weight space and
what it means to search for maximum average precision
within it.
Queries in information retrieval can be treated as vec-
tors of terms t1, t2, ? ? ? , tN . Each term is, as the name
suggests, an individual word or phrase that might oc-
cur in the document corpus. Every term t i has a weight
?i determining its ?importance? relative to the other
terms of the query. These weights form a weight vec-
tor ? = ??1 ?2 ? ? ? ?N ?. Further, given a document
corpus ?, for each document dj ? ? we have a ?value
vector? ?j = ??j1 ?j2 ? ? ? ?jN ?, where each ?value?
?ji ? < gives some measure of term ti within document
dj?typically the frequency of occurrence or a function
thereof. In the case of the standard tf-idf formula, ? ji
is the term frequency and ?i the inverse document fre-
quency.
If the document corpus and set of terms is held fixed,
the average precision calculation can be considered a
function f : <N ? [0, 1] mapping ? to a single aver-
age precision value. Finding the weight vectors in this
5 10 15 20
0.
2
0.
4
0.
6
0.
8
1.
0
0.
2
0.
4
0.
6
0.
8
1.
0
number of terms
a
ve
ra
ge
 p
re
cis
io
n
Figure 1: Average precision by query size as generated
by the maximum entropy model for TREC topic 307.
context is then the familiar problem of finding maxima in
an N -dimensional landscape.
3.2 Powell?s algorithm
One general approach to this problem of searching a
multi-dimensional space is to decompose the problem
into a series of iterated searches along single directions
within the space. Perhaps the most basic technique, cred-
ited to Powell, is simply a round-robin-style iteration
along a set of unchanging direction vectors, until conver-
gence is reached (Press et al, 1992, pp. 412-420). This
is the approach used in this study.
Formally, the procedure is as follows. You are given
a set of direction vectors ?1, ?2, ? ? ? , ?N and a starting
point pi0. First move pi0 to the maximum along ?1 and
call this pi1, i.e. pi1 = pi0 + ?1?1 for some scalar ?1.
Next move pi1 to the maximum along ?2 and call this
pi2, and so on, until the final point piN . Finally, replace
pi0 with piN and repeat the entire process, starting again
with ?1. Do this until some convergence criterion is met.
This procedure has no guaranteed rate of convergence,
although more sophisticated versions of Powell?s algo-
rithm do. In practice this has not been a problem.
3.3 Exhaustively searching a single direction
Powell?s algorithm can make use of any one-dimensional
search technique. Rather than applying a completely gen-
eral hill-climbing search, however, in the case where doc-
ument scores are calculated by a linear equation on the
terms, i.e.
?j =
N
?
i=1
?i?ji = ? ? ?j
as they are in the tf-idf formula, we can exhaustively
search in a single direction of the weight space in an effi-
cient manner. This potentially yields better solutions and
potentially converges more quickly than a general hill-
climbing heuristic.
scale
do
cu
m
en
t s
co
re
a
a
b
b
e
c
c
?1
?2
f
f
d d
Figure 2: Sample plot of ? versus ? for a given direction.
The insight behind the algorithm is as follows. Given
a direction ? in weight space and a starting point pi, the
score of each document is a linear function of the scale ?
along ? from pi:
?j = ? ? ?j
= (pi + ??) ? ?j
= pi ? ?j + ? (? ? ?j) .
i.e. document di?s score, plotted against ?, is a line with
slope ? ? ?i and y-intercept pi ? ?j .
Consider the graph of lines for all documents, such as
the example in Figure 2. Each vertical slice of the graph,
at some point ? on the x axis, represents the order of the
documents when ? = ?; specifically, the order of the
documents is given by the order of the intersections of
the lines with the vertical line at x = ?.
Now consider the set of intersections of the document
lines. Given two documents dr and ds, their intersection,
if it exists, lies at point ?rs = (?xrs, ?yrs) where
?xrs =
pi ? (?s ? ?r)
? ? (?r ? ?s)
, and
?yrs = pi ? ?r + ?xrs (? ? ?r)
(Note that this is undefined if ? ? ?r = ? ? ?s, i.e., if the
document lines are parallel.)
Let ? be the set of all such document intersection
points for a given direction, document set and term vec-
tor. Note that more than two lines may intersect at the
same point, and that two intersections may share the same
x component while having different y components.
Now consider the set ?x, defined as the projection of
? onto the x axis, i.e. ?x = {? | ? ? ? ? s.t. ?x = ?}.
The points in ?x represent precisely those values of ?
where two or more documents are tied in score. There-
fore, the document ordering changes at and only at these
points of intersection; in other words, the points in ?x
partition the range of ? into at most M(M ?1)/2+1 re-
gions, where M is the total number of documents. Within
a given region, document ordering is invariant and hence
average precision is constant. As we can calculate the
boundaries of, and the document ordering and average
precision within, each region, we now have a way of find-
ing the maximum across the entire space by evaluating
a finite number of regions. Each of the O(M 2) regions
requires an O(M log M) sort, yielding a total computa-
tional bound of O(M 3 log M).
In fact, we can further reduce the computation by ex-
ploiting the fact that the change in document ordering be-
tween any two regions is known and is typically small.
The weight search algorithm functions in this manner. It
sorts the documents completely to determine the order-
ing in the left-most region. Then, it traverses the regions
from left to right and updates the document ordering in
each, which does not require a sort. Average precision
can be incrementally updated based on the document or-
dering changes. This reduces the computational bound to
O(M2 log M), the requirement for the initial sort of the
O(M2) intersection points.
4 Experiment Setup
In order to compare the results of the weight search al-
gorithm to those of the maximum entropy model, we em-
ployed the same experiment setup. We ran on 15 topics,
which were manually selected from the TREC 6, 7, and
8 collections (Voorhees and Harman, 2000), with the ob-
jective of creating a representative subset. The document
sets were divided into randomly selected training, valida-
tion and test ?splits?, comprising 25%, 25%, and 50%,
respectively, of the complete set.
For each query, a set of candidate terms was selected
based on mutual information between (binary) term oc-
currence and document relevance. From this set, terms
were chosen individually to be included in the query,
and coefficients for all terms were calculated using L-
BFGS, a quasi-Newton unconstrained optimization algo-
rithm (Zhu et al, 1994).
For experimenting with the weight search algorithm,
we investigated queries of length 1 through 20 for each
topic, so each topic involved 20 experiments. The first
term weight was fixed at 1.0. The single-term queries
did not require a weight search, as the weight of a single
term does not affect the average precision score. For the
remaining 19 experiments for each topic, the direction
vectors ? were chosen such that the algorithm searched
a single term weight at a time. For example, a query with
5 10 15 20
0.
2
0.
4
0.
6
0.
8
1.
0
0.
2
0.
4
0.
6
0.
8
1.
0
number of terms
a
ve
ra
ge
 p
re
cis
io
n
0.
2
0.
4
0.
6
0.
8
1.
0
a
ve
ra
ge
 p
re
cis
io
n
0.
2
0.
4
0.
6
0.
8
1.
0
a
ve
ra
ge
 p
re
cis
io
n
0.
2
0.
4
0.
6
0.
8
1.
0
a
ve
ra
ge
 p
re
cis
io
n
0.
2
0.
4
0.
6
0.
8
1.
0
a
ve
ra
ge
 p
re
cis
io
n
0.
2
0.
4
0.
6
0.
8
1.
0
a
ve
ra
ge
 p
re
cis
io
n
0.
2
0.
4
0.
6
0.
8
1.
0
a
ve
ra
ge
 p
re
cis
io
n
0.
2
0.
4
0.
6
0.
8
1.
0
a
ve
ra
ge
 p
re
cis
io
n
0.
2
0.
4
0.
6
0.
8
1.
0
a
ve
ra
ge
 p
re
cis
io
n
0.
2
0.
4
0.
6
0.
8
1.
0
a
ve
ra
ge
 p
re
cis
io
n
0.
2
0.
4
0.
6
0.
8
1.
0
a
ve
ra
ge
 p
re
cis
io
n
0.
2
0.
4
0.
6
0.
8
1.
0
a
ve
ra
ge
 p
re
cis
io
n
0.
2
0.
4
0.
6
0.
8
1.
0
a
ve
ra
ge
 p
re
cis
io
n
0.
2
0.
4
0.
6
0.
8
1.
0
a
ve
ra
ge
 p
re
cis
io
n
0.
2
0.
4
0.
6
0.
8
1.
0
a
ve
ra
ge
 p
re
cis
io
n
301
302
307
330
332
347
352
375
383
384
388
391
407
425
439
Figure 3: Average precision versus query size for the
weight search algorithm. Each line represents a topic.
i terms used the i ? 1 directions
?i,1 = ?0 1 0 0 ? ? ? 0?,
?i,2 = ?0 0 1 0 ? ? ? 0?,...
?i,i?1 = ?0 0 0 0 ? ? ? 1?.
The two-term query for a topic started the search from the
point pi2,0 = ?1 0?, and each successive experiment for
that topic was initialized with the starting point pi0 equal
to the final point in the previous iteration, concatenated
with a 0. The ?value vectors? ?j used in all experiments
were Okapi tf scores.
5 Results
The average precision scores obtained by the maximum
entropy and weight search algorithm experiments are
listed in Table 1. The ?Best AP? and ?No. Terms?
columns describe the query size at which average preci-
sion was best and the score at that point. These columns
show that the maximum entropy approach performs just
as well as the average precision hill-climber, and in some
cases actually performs slightly better. This suggests that
the metric divergence as seen in Figure 1 did not prohibit
the maximum entropy approach from maximizing aver-
age precision in the course of maximizing likelihood.
The ?5 term AP? column compares the performance
of the algorithms on smaller queries. The weight search
algorithm shows a slight advantage over the maximum
entropy model on 10 of the 15 topics and equal perfor-
mance on the others, but definitive conclusions are diffi-
cult at this stage.
Figure 3 shows the average precision achieved by the
weight search algorithm, for all 20 query sizes and for
all 15 topics. Unlike the maximum entropy results,
the algorithm is guaranteed to yield monotonically non-
decreasing scores.
Topic 5 term AP Best AP No. Terms
WS ME WS ME WS ME
301 0.68 0.67 0.90 0.90 >20 >20
302 0.88 0.86 1.00 1.00 10 10
307 0.57 0.56 0.98 0.89 >20 >20
330 0.65 0.61 1.00 1.00 10 10
332 0.74 0.72 0.99 1.00 >20 18
347 0.78 0.78 1.00 1.00 17 14
352 0.55 0.55 0.94 0.93 >20 >20
375 0.92 0.92 1.00 1.00 9 9
383 0.89 0.89 1.00 1.00 9 9
384 0.77 0.73 1.00 1.00 8 8
388 0.82 0.80 1.00 1.00 7 7
391 0.64 0.63 0.99 0.98 >20 >20
407 0.83 0.83 1.00 1.00 9 9
425 0.75 0.73 1.00 1.00 12 12
439 0.53 0.51 1.00 1.00 17 16
Table 1: Average precision achieved for weight search
(WS) and maximum entropy (ME) algorithms.
6 Conclusions
We developed an algorithm for exhaustively searching a
continuous and unbounded direction in term weight space
in O(M2 log M) time. Initial results suggest that the
maximum entropy approach performs as well as this al-
gorithm, which hill-climbs directly on average precision,
allaying our concerns that the metric divergence exhib-
ited by the maximum entropy approach is a problem for
studying optimal queries.
References
William H. Press, Brian P. Flannery, Saul A. Teukolsky,
and William T. Vetterling. 1992. Numerical Recipes
in C: The Art of Scientic Computing. Cambridge Uni-
versity Press, second edition.
S. E. Robertson and S. Walker. 1994. Some simple effec-
tive approximations to the 2-Poisson model for proba-
bilistic weighted retrieval. In W. Bruce Croft and C. J.
van Rijsbergen, editors, Proc. 17th SIGIR Conference
on Information Retrieval.
S. E. Robertson. 1977. The probability ranking principle
in IR. Journal of Documentation, 33:294?304.
E. M. Voorhees and D. K. Harman. 2000. Overview of
the eighth Text REtrieval Conference (TREC-8). In
E. M. Voorhees and D. K. Harman, editors, The Eighth
Text REtreival Conference (TREC-8). NIST Special
Publication 500-246.
C. Zhu, R. Byrd, P. Lu, and J. Nocedal. 1994. LBFGS-B:
Fortran subroutines for large-scale bound constrained
optimization. Technical Report NAM-11, EECS De-
partment, Northwestern University.
               	 
        	       	   
        
     Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 175?182,
Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005
Gaming Fluency: Evaluating the Bounds and Expectations of
Segment-based Translation Memory
John Henderson and William Morgan
The MITRE Corporation
202 Burlington Road
Bedford, MA 01730
{jhndrsn,wmorgan}@mitre.org
Abstract
Translation memories provide assis-
tance to human translators in produc-
tion settings, and are sometimes used
as first-pass machine translation in as-
similation settings because they pro-
duce highly fluent output very rapidly.
In this paper, we describe and eval-
uate a simple whole-segment transla-
tion memory, placing it as a new lower
bound in the well-populated space of
machine translation systems. The re-
sult is a new way to gauge how far ma-
chine translation has progressed com-
pared to an easily understood baseline
system.
The evaluation also sheds light on the
evaluation metric and gives evidence
showing that gaming translation with
perfect fluency does not fool bleu the
way it fools people.
1 Introduction and background
Translation Memory (TM) systems provide
roughly concordanced results from an archive of
previously translated materials. They are typ-
ically used by translators who want computer
assistance for searching large archives for tricky
translations, and also to help ensure a group
of translators rapidly arrive at similar terminol-
ogy (Macklovitch et al, 2000). Several compa-
nies provide commercial TMs and systems for
using and sharing them. TMs can add value to
computer assisted translation services (Drugan,
2004).
Machine Translation (MT) developers make
use of similar historical archives (parallel texts,
bitexts), to produce systems that perform a task
very similar to TMs. But while TM systems
and MT systems can appear strikingly simi-
lar, (Marcu, 2001) key differences exist in how
they are used.
TMs often need to be fast because they are
typically used interactively. They aim to pro-
duce highly readable, fluent output, usable in
document production settings. In this setting,
errors of omission are more easily forgiven than
errors of commission so, just like MT, TM out-
put must look good to users who have no access
to the information in source texts.
MT, on the other hand, is often used in as-
similation settings, where a batch job can of-
ten be run on multiple processors. This permits
variable rate output and allows slower systems
that produce better translations to play a part.
Batch MT serving a single user only needs to run
at roughly the same rate the reader can consume
its output.
Simple TMs operate on an entire translation
segment, roughly the size of a sentence or two,
while more sophisticated TMs operate on units
of varying size: a word, a phrase, or an entire
segment (Callison-Burch et al, 2004). Mod-
ern approaches to MT, especially statistical MT,
typically operate on more fine-grained units,
words and phrases (Och and Ney, 2004). The re-
lationship between whole segment TM and MT
can be viewed as a continuum of translation
granularity:
175
-ff
segments
simple TM
words
MThybrid TM
Simple TM systems, focusing on segment-level
granularity, lie at one extreme, and word-
for-word, IBM-model MT systems on the
other. Example-Based MT (EBMT), phrase-
based, and commercial TM systems likely lie
somewhere in between.
This classification motivates our work here.
MT systems have well-studied and popular eval-
uation techniques such as bleu (Papineni et al,
2001). In this paper we lay out a methodology
for evaluating TMs along the lines of MT evalu-
ation. This allows us to measure the raw relative
value of TM and MT as translation tools, and to
develop expectations for how TM performance
increases as the size of the memory increases.
There are many ways to perform TM segmen-
tation and phrase extraction. In this study, we
use the most obvious and simple condition?a
full segment TM. This gives a lower bound on
real TM performance, but a lower bound which
is not trivial.
Section 2 details the architecture of our simple
TM. Section 3 describes experiments involving
different strategies for IR, oracle upper bounds
on TM performance as the memory grows, and
techniques for rescoring the retrievals. Section 4
discusses the results of the experiments.
2 A Simple Chinese-English
Translation Memory
For our experiments below, we constructed a
simple translation memory from a sentence-
aligned parallel corpus. The system consists of
three stages. A source-language input string is
rewritten to form an information retrieval (IR)
query. The IR engine is called to return a list
of candidate translation pairs. Finally a single
target-language translation as output is chosen.
2.1 Query rewriting
To retrieve a list of translation candidates from
the IR engine, we first create a query which is
a concatenation of all possible ngrams of the
source sentence, for all ngram sizes from 1 to
a fixed n.
We rely on the fact that the Chinese data
in the translation memory is tokenized and in-
dexed at the unigram level. Each Chinese char-
acter in the source sentence is tokenized indi-
vidually, and we make use of the IR engine?s
phrase query feature, which matches documents
in which all terms in the phrase appear in con-
secutive order, to create the ngrams. For exam-
ple, to produce a trigram + bigram + unigram
query for a Chinese sentence of 10 characters, we
would create a query consisting of eight three-
character phrases, nine two-character phrases,
and 10 single-character ?phrases?. All phrases
are weighted equally in the query.
This approach allows us to perform lookups
for arbitrary ngram sizes. Depending on the
specifics of how idf is calculated, this may yield
different results from indexing ngrams directly,
but it is advantageous in terms of space con-
sumed and scalability to different ngram sizes
without reindexing.
This is a slight generalization of the success-
ful approach to Chinese information retrieval us-
ing bigrams (Kwok, 1997). Unlike that work,
we perform no second stage IR after query ex-
pansion. Using a segmentation-independent en-
gineering approach to Chinese IR allows us to
sidestep the lack of a strong segmentation stan-
dard for our heterogeneous parallel corpus and
prepares us to rapidly move to other languages
with segmentation or lemmatization challenges.
2.2 The IR engine
Simply for performance reasons, an IR engine,
or some other sort of index, is needed to imple-
ment a TM (Brown, 2004). We use the open-
source Lucene v1.4.3, (Apa, 2004) as our IR en-
gine. Lucene scores candidate segments from
the parallel text using a modified tf-idf formula
that includes normalizations for the input seg-
ment length and the candidate segment length.
We did not modify any Lucene defaults for these
experiments.
To form our translation memory, we indexed
all sentence pairs in the translation memory cor-
pora, each pair as a separate document. We
176
Source
TM output
However , everything depended on the missions to be decided by the Security Council .
The presentations focused on the main lessons learned from their activities in the field .
It is wrong to commit suicide or to use ones own body as a weapon of destruction .
There was practically full employment in all sectors .
One reference translation (of four)
Doug Collins said, ?He may appear any time. It really depends on how he feels.?
At present, his training is defense oriented but he also practices shots.
He is elevating the intensity to test whether his body can adapt to it.
So far as his knee is concerned, he thinks it heals a hundred percent after the surgery.?
Table 1: Typical TM output. Excerpt from a story about athlete Michael Jordan.
indexed in such a way that IR searches can be
restricted to just the source language side or just
the target language side.
2.3 Rescoring
The IR engine returns a list of candidate trans-
lation pairs based on the query string, and the
final stage of the TM process is the selection of
a single target-language output sentence from
that set.
We consider a variety of selection metrics in
the experiments below. For each metric, the
source-language side of each pair in the candi-
date list is evaluated against the original source
language input string. The target language seg-
ment of the pair with the highest score is then
output as the translation.
In the case of automated MT evaluation met-
rics, which are not necessarily symmetric, the
source-language input string is treated as the
reference and the source-language side of each
pair returned by the IR engine as the hypothe-
sis.
All tie-breaking is done via tf-idf , i.e. if multi-
ple entries share the same score, the one ranked
higher by the search engine will be output.
Table 1 gives a typical example of how the TM
performs. Four contiguous source segments are
presented, followed by TM output and finally
one of the reference translations for those source
segments. The only indicator of the translation
quality available to monolingual English speak-
ers is the awkwardness of the segments as a
group. By design, the TM performs with perfect
fluency at the segment level.
3 Experiments
We performed several experiments in the course
of optimizing this TM, all using the same set
of parallel texts for the TM database and
multiple-reference translation corpus for eval-
utation. The parallel texts for the TM come
from several Chinese-English parallel corpora,
all available from the Linguistic Data Consor-
tium (LDC). These corpora are described in Ta-
ble 2. We discarded any sentence pairs that
seemed trivially incomplete, corrupt, or other-
wise invalid. In the case of LDC2002E18, in
which sentences were aligned automatically and
confidence scores produced for each alignment,
we dropped all pairs with scores above 9, indi-
cating poor alignment. No duplication checks
were performed. Our final corpus contained ap-
proximately 7 million sentence pairs and con-
tained 3.2 GB of UTF-8 data.
Our evaluation corpus and reference corpus
177
come from the data used in the NIST 2002 MT
competition. (NIST, 2002). The evaluation cor-
pus is 878 segments of Chinese source text. The
reference corpus consists of four independent
human-generated reference English translations
of the evaluation corpus.
All performance measurements were made us-
ing a fast reimplementation of NIST?s bleu.
bleu exhibits a high correlation with human
judgments of translation quality when measur-
ing on large sections of text (Papineni et al,
2001). Furthermore, using bleu allowed us to
compare our performance to that of other sys-
tems that have been tested with the same eval-
uation data.
3.1 An upper bound on whole-segment
translation memory
Our first experiment was to determine an upper
bound for the entire translation memory corpus.
In other words, given an oracle that picks the
best possible translation from the translation
memory corpus for each segment in the evalu-
ation corpus, what is the bleu score for the re-
sulting document? This score is unlikely to ap-
proach the maximum, bleu =100 because this
oracle is constrained to selecting a translation
from the target language side of the parallel cor-
pus. All of the calculations for this experiment
are performed on the target language side of the
parallel text.
We were able to take advantage of a trait
particular to bleu for this experiment, avoid-
ing many of bleu score calculations required
to assess all of the 878 ? 7.5 million combina-
tions. bleu produces a score of 0 for any hy-
pothesis string that doesn?t share at least one
4-gram with one reference string. Thus, for
each set of four references, we created a Lucene
query that returned all translation pairs which
matched at least one 4-gram with one of the ref-
erences. We picked the top segment by calcu-
lating bleu scores against the references, and
created a hypothesis document from these seg-
ments.
Note that, for document scores, bleu?s
brevity penalty (BP) is applied globally to an
entire document and not to individual segments.
Thus, the document score does not necessarily
increase monotonically with increases in scores
of individual segments. As more than 99% of
the segment pairs we evaluated yielded scores of
zero, we felt this would not have a significant
effect on our experiments. Also, the TM does
not have much liberty to alter the length of the
returned segments. Individual segments were
chosen to optimize bleu score, and the result-
ing documents exhibited appropriately increas-
ing scores. While there is no efficient strategy
for whole-document bleu maximization, an it-
erative rescoring of the entire document while
optimizing the choice of only one candidate seg-
ment at a time could potentially yield higher
scores than those we report here.
3.2 TM performance with varied
Ngram length
The second experiment was to determine the ef-
fect that different ngram sizes in the Chinese IR
query have on the IR engine?s ability to retrieve
good English translations.
We considered cumulative ngram sizes from 1
to 7, i.e. unigram, unigram + bigram, unigram
+ bigram + trigram, and so on. For each set
of ngram sizes, we created a Lucene query for
every segment of the (Chinese) evaluation cor-
pus. We then produced a hypothesis document
by combining the English sides of the top re-
sults returned by Lucene for each query. The
hypothesis document was evaluated against the
reference corpora by calculating a bleu score.
While it was observed that IR perfor-
mance is maximized by performing bigram
queries (Kwok, 1997), we had reason to believe
the TM would not be similar. TMs must at-
tempt to match short sequences of stop words
that indicate grammar as well as more tradi-
tional content words. Note that our system
performed neither stemming nor stop word (or
ngram) removal on the input Chinese strings.
3.3 An upper bound on TM N-best list
rescoring
The next experiment was to determine an upper
bound on the performance of tf-idf for differ-
ent result set sizes, i.e. for different (maximum)
178
LDC Id Description Pairs
LDC2002E18 Xinhua Chinese-English Parallel News Text v. 1.0 beta 2 64,371
LDC2002E58 Sinorama Chinese-English Parallel Text 103,216
LDC2003E25 Hong Kong News Parallel Text 641,308
LDC2004E09 Hong Kong Hansard Parallel Text 1,247,294
LDC2004E12 UN Chinese-English Parallel Text v. 2 4,979,798
LDC2000T47 Hong Kong Laws Parallel Text 302,945
Total 7,338,932
Table 2: Sentence-aligned parallel corpora used for the creation of the translation memory. The
?pairs? column gives the number of translation pairs available after trivial pruning.
numbers of translation pairs returned by the IR
engine. This experiment describes the trade-off
between more time spent in the IR engine cre-
ating a longer list of returns and the potential
increase in translation score.
To determine how much IR was ?enough? IR,
we performed an oracle experiment on different
IR query sizes. For each segment of the evalua-
tion corpus, we performed a cumulative 4-gram
query as described in Section 4.2. We produced
the n-best list oracle?s hypothesis document by
selecting the English translation from this result
set with the highest bleu score when evaluated
against the corresponding segment from the ref-
erence corpus. We then evaluated the hypoth-
esis documents against the reference corpus by
computing bleu scores.
3.4 N-best list rescoring with several
MT evaluation metrics
The fourth experiment was to determine
whether we could improve upon tf-idf by apply-
ing automated MT metrics to pick the best sen-
tence from the top n translation pairs returned
by the IR engine. We compared a variety of
metrics from MT evaluation literatures. All of
these were run on the tokens in the source lan-
guage side of the IR result, comparing against
the single pseudo-reference, the original source
language segment. While many of these metrics
aren?t designed to perform well with one refer-
ence, they stand in as good approximate string
matching algorithms.
The score that the IR engine associates with
each segment is retained and marked as tf-idf
in this experiment. Naturally, bleu (Papineni
et al, 2001) was the first choice metric, as it
was well-matched to the target language evalu-
ation function. rouge was a reimplementation
of ROUGE-L from (Lin and Och, 2004). It com-
putes an F-measure from precision and recall
that are both based on the longest common sub-
sequence of the hypothesis and reference strings.
wer-g is a variation on traditional word error
rate that was found to correlate very well with
human judgments (Foster et al, 2003), and per
is the traditional position-independent error rate
that was also shown to correlate well with hu-
man judgments (Leusch et al, 2003). Finally,
a random metric was added to show the bleu
value one could achieve by selecting from the top
n strictly by chance.
After the individual metrics are calculated
for these segments, a uniform-weight log-linear
combination of the metrics is calculated and
used to produce a new rank ordering under the
belief that the different metrics will make pre-
dictions that are constructive in aggregate.
4 Results
4.1 An upper bound for whole-sentence
TM
Figure 1 shows the maximum possible bleu
score that can an oracle can achieve by selecting
the best English-side segment from the parallel
text. The upper bound achieved here is a bleu
score of 17.7, and this number is higher than
the best performing system in the correspond-
ing NIST evaluation.
Note the log-linear growth in the resulting
179
 7
 8
 9
 10
 11
 12
 13
 14
 15
 16
 17
 18
 10000  100000  1e+06  1e+07
O
ra
cle
 B
LE
U 
sc
or
e
Corpus size (segments)
Size bleu
73389 7.88
366947 10.82
733893 12.58
3669466 16.27
7338932 17.69
Figure 1: Oracle bounds on TM performance as
corpus size increases.
 2.5
 3
 3.5
 4
 4.5
 5
 5.5
 6
 1  2  3  4  5  6  7
BL
EU
 s
co
re
 o
f T
M
Max n-gram length
Ngrams in query bleu
1 2.72
1,2 4.73
1,2,3 5.68
1,2,3,4 5.87
1,2,3,4,5 5.80
1,2,3,4,5,6 5.52
1,2,3,4,5,6,7 5.48
Figure 2: bleu scores for different cumulative
ngram sizes, when retrieving only the first trans-
lation pair.
bleu score of the TM with increasing database
size. As the database is increased by a factor
of ten, the TM gains approximately 5 points of
bleu. While this trend has a natural limit at
20 orders of magnitude, it is unlikely that this
amount of text, let alne parallel text, will be a
indexed in the foreseeable future. This rate is
more useful in interpolation, giving an idea of
how much could be gained from adding to cor-
pora that are smaller than 7.5 million segments.
4.2 The effect of ngram size on Chinese
tf-idf retrieval
Figure 2 shows that our best performance is
realized when IR queries are composed of cu-
mulative 4-grams (i.e. unigrams + bigrams +
trigrams + 4-grams). As hypothesized, while
longer sequences are not important in document
retrieval in Chinese IR, they convey information
that is useful in segment retrieval in the trans-
lation memory. For the remainder of the ex-
periments, we restrict ourselves to cumulative
4-gram queries.
Note that the 4-gram result here (bleu of
5.87) provides the baseline system performance
measure as well as the value when the segments
are reranked according to tf-idf .
4.3 Upper bounds for tf-idf
Figure 3 gives the n-best list rescoring bounds.
The upper bound continues to increase up to
the top 1000 results. The plateau achieved af-
ter 1000 IR results suggests that is little to be
gained from further IR engine retrieval.
Note the log-linear growth in the bleu score
the oracle achieves as the n-best list extends on
the left side of the figure. As the list length
is increased by a factor of ten, the oracle up-
per bound on performance increases by roughly
3 points of bleu. Of course, for a system to
perform as well as the oracle does becomes pro-
gressively harder as the n-best list size increases.
Comparing this result with the experiment
in section 4.1 indicates that making the oracle
choose among Chinese source language IR re-
sults and limiting its view to the 1000 results
given by the IR engine incurs only a minor re-
duction of the oracle?s bleu score, from 17.7 to
180
16.3. This is one way to measure the impact
of crossing this particular language barrier and
using IR rather than exhaustive search.
 4
 6
 8
 10
 12
 14
 16
 18
 0.1  1  10  100  1000  10000  100000  1e+06  1e+07
O
ra
cle
 B
LE
U 
sc
or
e
N-best list size
Size bleu score
1 5.87
5 8.47
10 9.51
50 12.09
100 13.18
500 15.36
1000 16.29
7338932 17.69
Figure 3: bleu scores for different (maximum)
numbers of translation pairs returned by IR en-
gine, where the optimal segment is chosen from
the results by an oracle.
4.4 Using automated MT metrics to
pick the best TM sentence
Each metric was run on the top 1000 results
from the IR engine, on cumulative 4-gram
queries. Each metric was given the (Chinese)
evaluation corpus segment as the single refer-
ence, and scored the Chinese side of each of the
1000 resulting translation pairs against that ref-
erence. The hypothesis document for each met-
ric consisted of the English side of the transla-
tion pair with the best score for each segment.
These documents were scored with bleu against
the reference corpus. Ties (e.g. cases where a
metric gave all 1000 pairs the same score) were
broken with tf-idf .
Results of the rescoring experiment run on
Metric bleu
bleu 6.20
wer-g 5.90
rouge 5.88
tf-idf 5.87
per 5.72
random 3.32
log(tf-idf )
+log(bleu)
+log(rouge)
-log(wer-g)
-log(per) 6.56
Table 3: bleu scores for different metrics when
picking the best translation from 100 translation
pairs returned by the IR engine.
an n-best list of size 100 are given in Table 3.
Choosing from 1000 pairs did not give better
results. Choosing from only 10 gave worse re-
sults. The random baseline given in the table
represents the expected score from choosing ran-
domly among the top 100 IR returns. While the
scores of the individual metrics aside from per
and bleu reveal no differences, bleu and the
combination metric performed better than the
individual metrics.
Surprisingly, tf-idf was outperformed only by
bleu and the combination metric. While we
hoped to gain much more from n-best list rescor-
ing on this task, reaching toward the limits dis-
covered in section 4.3, the combination metric
was less than 0.5 bleu points below the lower
range of systems that were entered in the NIST
2002 evals. The bleu scores of research systems
in that competition roughly ranged between 7
and 15. Of course, each of the segments pro-
duced by the TM exhibit perfect fluency.
5 Discussion
The maximum bleu score attained by a TM we
describe (6.56) would place it in last place in the
NIST 2002 evals, but by less than 0.5 bleu. Suc-
cessive NIST competitions have exhibited im-
pressive system progress, but each year there
have been newcomers who score near (or in some
cases lower than) our simple TM baseline.
181
We have presented several experiments that
quantitatively describe how well a simple TM
performs when measured with a standard MT
evaluation measure, bleu. We showed that the
translation performance of a TM grows as a log-
linear function of corpus size below 7.5 million
segments. We showed, somewhat surprisingly,
only 1000 IR returns need be evaluated by a
rescorer to get within 1 bleu point of the max-
imum possible score attainable by the TM.
In future work, we expect to validate these
results with other language pairs. One question
is: how well does this simple IR query expansion
address segmented languages and languages that
allow more liberal word order? Supervised train-
ing of n-best reranking schemes would also de-
termine how far the oracle bound can be pushed.
The computationally more expensive reranking
procedure that attempts to optimize bleu on
the entire document should be investigated to
determine how much can be gained by better
global management of the brevity penalty.
Finally, we believe it?s worth noting the degree
to which high fluency of the TM output could
potentially mislead target-language-only readers
in their estimation of the system?s performance.
Table 1 is representative of system output, and
is a good example of why translations should not
be judged solely on the fluency of a few segments
of target language output.
References
Apache Software Foundation, 2004. Lucene 1.4.3
API. http://lucene.apache.org/java/docs/api/.
Ralf D. Brown. 2004. A modified burrows-
wheeler transform for highly-scalable example-
based translation. In Machine Translation: From
Real Users to Research, Proceedings of the 6th
Conference of the Association for Machine Trans-
lation (AMTA-2004), Washington, D.C., USA.
Chris Callison-Burch, Colin Bannard, and Josh
Schroeder. 2004. Searchable translation memo-
ries. In Proceedings of ASLIB Translation and the
Computer 26.
Joanna Drugan. 2004. Multilingual document man-
agement and workflow in the european institu-
tions. In Proceedings of ASLIB Translation and
the Computer 26.
George Foster, Simona Gandrabur, Cyril Goutte,
Erin Fitzgerald, Alberto Sanchis, Nicola Ueffing,
John Blatz, and Alex Kulesza. 2003. Confidence
estimation for machine translation. Technical re-
port, JHU Center for Language and Speech Pro-
cessing.
K. L. Kwok. 1997. Comparing representations in
chinese information retrieval. In SIGIR ?97: Pro-
ceedings of the 20th annual international ACM SI-
GIR conference on Research and development in
information retrieval, pages 34?41, New York, NY,
USA. ACM Press.
G. Leusch, N. Ueffing, and H. Ney. 2003. A
novel string-to-string distance measure with ap-
plications to machine translation evaluation. In
Proc. of the Ninth MT Summit, pages 240?247.
Chin-Yew Lin and Franz Josef Och. 2004. Or-
ange: a method for evaluating automatic evalu-
ation metrics for machine translation. In Proceed-
ings of the 20th International Conference on Com-
putational Linguistics (COLING 2004), Geneva,
Switzerland, August.
E. Macklovitch, M. Simard, and Ph. Langlais. 2000.
Transsearch: A free translation memory on the
world wide web. In Second International Con-
ference On Language Resources and Evaluation
(LREC), volume 3, pages 1201?1208, Athens
Greece, jun.
Daniel Marcu. 2001. Towards a unified approach
to memory- and statistical-based machine trans-
lation. In ACL, pages 378?385.
NIST. 2002. The NIST 2002 machine trans-
lation evaluation plan (MT-02). NIST
web site. http://www.nist.gov/speech/
tests/mt/doc/2002-MT-EvalPlan-v1.3.pdf.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine
translation. Computational Linguistics, 30(4).
K. Papineni, S. Roukos, T. Ward, and W-J. Zhu.
2001. BLEU: a method for automatic evalua-
tion of machine translation. Technical Report
RC22176 (W0109-022), IBM Research Division,
Thomas J. Watson Research Center.
182
Proceedings of the 7th SIGdial Workshop on Discourse and Dialogue, pages 96?103,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Automatically Detecting Action Items in Audio Meeting Recordings
William Morgan Pi-Chuan Chang Surabhi Gupta
Department of Computer Science
Stanford University
353 Serra Mall
Stanford, CA 94305-9205
ruby@cs.stanford.edu
pcchang@cs.stanford.edu
surabhi@cs.stanford.edu
Jason M. Brenier
Department of Linguistics
Center for Spoken Language Research
Institute of Cognitive Science
University of Colorado at Boulder
594 UCB
Boulder, Colorado 80309-0594
jbrenier@colorado.edu
Abstract
Identification of action items in meeting
recordings can provide immediate access
to salient information in a medium noto-
riously difficult to search and summarize.
To this end, we use a maximum entropy
model to automatically detect action item-
related utterances from multi-party audio
meeting recordings. We compare the ef-
fect of lexical, temporal, syntactic, seman-
tic, and prosodic features on system per-
formance. We show that on a corpus of ac-
tion item annotations on the ICSI meeting
recordings, characterized by high imbal-
ance and low inter-annotator agreement,
the system performs at an F measure of
31.92%. While this is low compared to
better-studied tasks on more mature cor-
pora, the relative usefulness of the features
towards this task is indicative of their use-
fulness on more consistent annotations, as
well as to related tasks.
1 Introduction
Meetings are a ubiquitous feature of workplace
environments, and recordings of meetings pro-
vide obvious benefit in that they can be replayed
or searched through at a later date. As record-
ing technology becomes more easily available and
storage space becomes less costly, the feasibil-
ity of producing and storing these recordings in-
creases. This is particularly true for audio record-
ings, which are cheaper to produce and store than
full audio-video recordings.
However, audio recordings are notoriously diffi-
cult to search or to summarize. This is doubly true
of multi-party recordings, which, in addition to the
difficulties presented by single-party recordings,
typically contain backchannels, elaborations, and
side topics, all of which further confound search
and summarization processes. Making efficient
use of large meeting corpora thus requires intel-
ligent summary and review techniques.
One possible user goal given a corpus of meet-
ing recordings is to discover the action items de-
cided within the meetings. Action items are deci-
sions made within the meeting that require post-
meeting attention or labor. Rapid identification
of action items can provide immediate access to
salient portions of the meetings. A review of ac-
tion items can also function as (part of) a summary
of the meeting content.
To this end, we explore the task of applying
maximum entropy classifiers to the task of auto-
matically detecting action item utterances in au-
dio recordings of multi-party meetings. Although
available corpora for action items are not ideal, it
is hoped that the feature analysis presented here
will be of use to later work on other corpora.
2 Related work
Multi-party meetings have attracted a significant
amount of recent research attention. The creation
of the ICSI corpus (Janin et al, 2003), comprised
of 72 hours of meeting recordings with an average
of 6 speakers per meeting, with associated tran-
scripts, has spurred further annotations for var-
ious types of information, including dialog acts
(Shriberg et al, 2004), topic hierarchies and action
items (Gruenstein et al, 2005), and ?hot spots?
(Wrede and Shriberg, 2003).
The classification of individual utterances based
on their role in the dialog, i.e. as opposed to their
semantic payload, has a long history, especially
in the context of dialog act (DA) classification.
96
Research on DA classification initially focused
on two-party conversational speech (Mast et al,
1996; Stolcke et al, 1998; Shriberg et al, 1998)
and, more recently, has extended to multi-party
audio recordings like the ICSI corpus (Shriberg
et al, 2004). Machine learning techniques such
as graphical models (Ji and Bilmes, 2005), maxi-
mum entropy models (Ang et al, 2005), and hid-
den Markov models (Zimmermann et al, 2005)
have been used to classify utterances from multi-
party conversations.
It is only more recently that work focused
specifically on action items themselves has been
developed. SVMs have been successfully applied
to the task of extracting action items from email
messages (Bennett and Carbonell, 2005; Corston-
Oliver et al, 2004). Bennett and Carbonell, in par-
ticular, distinguish the task of action item detec-
tion in email from the more well-studied task of
text classification, noting the finer granularity of
the action item task and the difference of seman-
tics vs. intent. (Although recent work has begun to
blur this latter division, e.g. Cohen et al (2004).)
In the audio domain, annotations for action item
utterances on several recorded meeting corpora,
including the ICSI corpus, have recently become
available (Gruenstein et al, 2005), enabling work
on this topic.
3 Data
We use action item annotations produced by Gru-
enstein et al (2005). This corpus provides topic
hierarchy and action item annotations for the ICSI
meeting corpus as well as other corpora of meet-
ings; due to the ready availability of other types of
annotations for the ICSI corpus, we focus solely
on the annotations for these meetings. Figure 1
gives an example of the annotations.
The corpus covers 54 ICSI meetings annotated
by two human annotators, and several other meet-
ings annotated by one annotator. Of the 54 meet-
ings with dual annotations, 6 contain no action
items. For this study we consider only those meet-
ings which contain action items and which are an-
notated by both annotators.
As the annotations were produced by a small
number of untrained annotators, an immediate
question is the degree of consistency and reliabil-
ity. Inter-annotator agreement is typically mea-
sured by the kappa statistic (Carletta, 1996), de-
kappa
fre
qu
en
cy
0.0 0.2 0.4 0.6 0.8 1.0
0
2
4
6
8
Figure 2: Distribution of ? (inter-annotator agree-
ment) across the 54 ICSI meetings tagged by two
annotators. Of the two meetings with ? = 1.0, one
has only two action items and the other only four.
fined as:
? = P (O) ? P (E)1 ? P (E)
where P (O) is the probability of the observed
agreement, and P (E) the probability of the ?ex-
pected agreement? (i.e., under the assumption the
two sets of annotations are independent). The
kappa statistic ranges from ?1 to 1, indicating per-
fect disagreement and perfect agreement, respec-
tively.
Overall inter-annotator agreement as measured
by ? on the action item corpus is poor, as noted in
Purver et al (2006), with an overall ? of 0.364 and
values for individual meetings ranging from 1.0 to
less than zero. Figure 2 shows the distribution of
? across all 54 annotated ICSI meetings.
To reduce the effect of poor inter-annotator
agreement, we focus on the top 15 meetings as
ranked by ?; the minimum ? in this set is 0.435.
Although this reduces the total amount of data
available, our intention is that this subset of the
most consistent annotations will form a higher-
quality corpus.
While the corpus classifies related action item
utterances into action item ?groups,? in this study
we wish to treat the annotations as simply binary
attributes. Visual analysis of annotations for sev-
eral meetings outside the set of chosen 15 suggests
that the union of the two sets of annotations yields
the most consistent resulting annotation; thus, for
this study, we consider an utterance to be an action
item if at least one of the annotators marked it as
such.
The 15-meeting subset contains 24,250 utter-
97
A1 A2
X X So that will be sort of the assignment for next week, is to?
X X to?for slides and whatever net you picked and what it can do and?and how far
you?ve gotten. Pppt!
X - Well, I?d like to also,
X X though, uh, ha- have a first cut at what the
X X belief-net looks like.
- X Even if it?s really crude.
- - OK? So, you know,
- - here a- here are?
- X So we?re supposed to @@ about features and whatnot, and?
Figure 1: Example transcript and action item annotations (marked ?X?) from annotators A1 and A2.
?@@? signifies an unintelligible word. This transcript is from an ICSI meeting recording and has ? =
0.373, ranking it 16th out of 54 meetings in annotator agreement.
0 500 1000 1500 2000 2500
Figure 3: Number of total and action item utter-
ances across the 15 selected meetings. There are
24,250 utterances total, 590 of which (2.4%) are
action item utterances.
ances total; under the union strategy above, 590 of
these are action item utterances. Figure 3 shows
the number of action item utterances and the num-
ber of total utterances in the 15 selected meetings.
One noteworthy feature of the ICSI corpus un-
derlying the action item annotations is the ?digit
reading task,? in which the participants of meet-
ings take turns reading aloud strings of digits.
This task was designed to provide a constrained-
vocabulary training set of speech recognition de-
velopers interested in multi-party speech. In this
study we did not remove these sections; the net
effect is that some portions of the data consist of
these fairly atypical utterances.
4 Experimental methodology
We formulate the action item detection task as one
of binary classification of utterances. We apply a
maximum entropy (maxent) model (Berger et al,
1996) to this task.
Maxent models seek to maximize the condi-
tional probability of a class c given the observa-
tions X using the exponential form
P (c|X) = 1Z(X) exp
[
?
i
?i,c fi,c(X)
]
where fi,c(X) is the ith feature of the data X
in class c, ?i,c is the corresponding weight, and
Z(X) is a normalization term. Maxent models
choose the weights ?i,c so as to maximize the en-
tropy of the induced distribution while remaining
consistent with the data and labels; the intuition is
that such a distribution makes the fewest assump-
tions about the underlying data.
Our maxent model is regularized by a quadratic
prior and uses quasi-Newton parameter optimiza-
tion. Due to the limited amount of training data
(see Section 3) and to avoid overfitting, we em-
ploy 10-fold cross validation in each experiment.
To evaluate system performance, we calculate
the F measure (F ) of precision (P ) and recall (R),
defined as:
P = |A ? C||A|
R = |A ? C||C|
F = 2PRP + R
where A is the set of utterances marked as action
items by the system, and C is the set of (all) cor-
rect action item utterances.
98
The use of precision and recall is motivated by
the fact that the large imbalance between posi-
tive and negative examples in the corpus (Sec-
tion 3) means that simpler metrics like accuracy
are insufficient?a system that simply classifies
every utterance as negative will achieve an accu-
racy of 97.5%, which clearly is not a good reflec-
tion of desired behavior. Recall and F measure for
such a system, however, will be zero.
Likewise, a system that flips a coin weighted in
proportion to the number of positive examples in
the entire corpus will have an accuracy of 95.25%,
but will only achieve P = R = F = 2.4%.
5 Features
As noted in Section 3, we treat the task of produc-
ing action item annotations as a binary classifica-
tion task. To this end, we consider the following
sets of features. (Note that all real-valued features
were range-normalized so as to lie in [0, 1] and that
no binning was employed.)
5.1 Immediate lexical features
We extract word unigram and bigram features
from the transcript for each utterance. We nor-
malize for case and for certain contractions; for
example, ?I?ll? is transformed into ?I will?.
Note that these are oracle features, as the tran-
scripts are human-produced and not the product
of automatic speech recognizer (ASR) system out-
put.
5.2 Contextual lexical features
We extract word unigram and bigram features
from the transcript for the previous and next ut-
terances across all speakers in the meeting.
5.3 Syntactic features
Under the hypothesis that action item utterances
will exhibit particular syntactic patterns, we use
a conditional Markov model part-of-speech (POS)
tagger (Toutanova and Manning, 2000) trained on
the Switchboard corpus (Godfrey et al, 1992) to
tag utterance words for part of speech. We use the
following binary POS features:
? Presence of UH tag, denoting the presence of
an ?interjection? (including filled pauses, un-
filled pauses, and discourse markers).
? Presence of MD tag, denoting presence of a
modal verb.
? Number of NN* tags, denoting the number of
nouns.
? Number of VB* tags, denoting the number of
verbs.
? Presence of VBD tag, denoting the presence
of a past-tense verb.
5.4 Prosodic features
Under the hypothesis that action item utterances
will exhibit particular prosodic behavior?for ex-
ample, that they are emphasized, or are pitched a
certain way?we performed pitch extraction using
an auto-correlation method within the sound anal-
ysis package Praat (Boersma and Weenink, 2005).
From the meeting audio files we extract the fol-
lowing prosodic features, on a per-utterance basis:
(pitch measures are in Hz; intensity in energy; nor-
malization in all cases is z-normalization)
? Pitch and intensity range, minimum, and
maximum.
? Pitch and intensity mean.
? Pitch and intensity median (0.5 quantile).
? Pitch and intensity standard deviation.
? Pitch slope, processed to eliminate halv-
ing/doubling.
? Number of voiced frames.
? Duration-normalized pitch and intensity
ranges and voiced frame count.
? Speaker-normalized pitch and intensity
means.
5.5 Temporal features
Under the hypothesis that the length of an utter-
ance or its location within the meeting as a whole
will determine its likelihood of being an action
item?for example, shorter statements near the
end of the meeting might be more likely to be ac-
tion items?we extract the duration of each utter-
ance and the time from its occurrence until the end
of the meeting. (Note that the use of this feature
precludes operating in an online setting, where the
end of the meeting may not be known in advance.)
5.6 General semantic features
Under the hypothesis that action item utterances
will frequently involve temporal expressions?e.g.
?Let?s have the paper written by next Tuesday??
we use Identifinder (Bikel et al, 1997) to mark
temporal expressions (?TIMEX? tags) in utterance
transcripts, and create a binary feature denoting
99
the existence of a temporal expression in each ut-
terance.
Note that as Identifinder was trained on broad-
cast news corpora, applying it to the very different
domain of multi-party meeting transcripts may not
result in optimal behavior.
5.7 Dialog-specific semantic features
Under the hypothesis that action item utterances
may be closely correlated with specific dialog
act tags, we use the dialog act annotations from
the ICSI Meeting Recorder Dialog Act Corpus.
(Shriberg et al, 2004) As these DA annotations
do not correspond one-to-one with utterances in
the ICSI corpus, we align them in the most liberal
way possible, i.e., if at least one word in an utter-
ance is annotated for a particular DA, we mark the
entirety of that utterance as exhibiting that DA.
We consider both fine-grained and coarse-
grained dialog acts.1 The former yields 56 fea-
tures, indicating occurrence of DA tags such
as ?appreciation,? ?rhetorical question,? and
?task management?; the latter consists of only
7 classes??disruption,? ?backchannel,? ?filler,?
?statement,? ?question,? ?unlabeled,? and ?un-
known.?
6 Results
The final performance for the maxent model
across different feature sets is given in Table 1.
F measures scores range from 13.81 to 31.92.
Figure 4 shows the interpolated precision-recall
curves for several of these feature sets; these
graphs display the level of precision that can be
achieved if one is willing to sacrifice some recall,
and vice versa.
Although ideally, all combinations of features
should be evaluated separately, the large number
of features in this precludes this strategy. The
combination of features explored here was cho-
sen so as to start from simpler features and suc-
cessively add more complex ones. We start with
transcript features that are immediate and context-
independent (?unigram?, ?bigram?, ?TIMEX?);
then add transcript features that require context
(?temporal?, ?context?), then non-transcript (i.e.
audio signal) features (?prosodic?), and finally add
features that require both the transcript and the au-
dio signal (?DA?).
1We use the map 01 grouping defined in the MRDA cor-
pus to collapse the tags.
0.0 0.2 0.4 0.6 0.8 1.0
0.
0
0.
2
0.
4
0.
6
0.
8
1.
0
recall
pr
ec
isi
on
0.
0
0.
2
0.
4
0.
6
0.
8
1.
0
pr
ec
isi
on
0.
0
0.
2
0.
4
0.
6
0.
8
1.
0
pr
ec
isi
on
0.
0
0.
2
0.
4
0.
6
0.
8
1.
0
pr
ec
isi
on
0.
0
0.
2
0.
4
0.
6
0.
8
1.
0
pr
ec
isi
on
0.
0
0.
2
0.
4
0.
6
0.
8
1.
0
pr
ec
isi
on
unigram
bigram
temporal
context+prosodic
fine?grained DAs
Figure 4: Interpolated precision-recall curve for
several (cumulative) feature sets. This graph sug-
gests the level of precision that can be achieved
if one is willing to sacrifice some recall, and vice
versa.
In total, nine combinations of features were
considered. In every case except that of syn-
tactic and coarse-grained dialog act features, the
additional features improved system performance
and these features were used in succeeding exper-
iments. Syntactic and coarse-grained DA features
resulted in a drop in performance and were dis-
carded from succeeding systems.
7 Analysis
The unigram and bigram features provide signif-
icant discriminative power. Tables 2 and 3 give
the top features, as determined by weight, for the
models trained only on these features. It is clear
from Table 3 that the detailed end-of-utterance
punctuation in the human-generated transcripts
provide valuable discriminative power.
The performance gain from adding TIMEX tag-
ging features is small and likely not statistically
significant. Post-hoc analysis of the TIMEX tag-
ging (Section 5.6) suggests that Identifinder tag-
ging accuracy is quite plausible in general, but ex-
hibits an unfortunate tendency to mark the digit-
reading (see Section 3) portion of the meetings as
temporal expressions. It is plausible that remov-
ing these utterances from the meetings would al-
low this feature a higher accuracy.
Based on the low feature weight assigned, utter-
ance length appears to provide no significant value
to the model. However, the time until the meet-
ing is over ranks as the highest-weighted feature
in the unigram+bigram+TIMEX+temporal feature
set. This feature is thus responsible for the 39.25%
100
features number F % imp.
unigram 6844 13.81
unigram+bigram 61281 16.72 21.07
unigram+bigram+TIMEX 61284 16.84 0.72
unigram+bigram+TIMEX+temporal 61286 23.45 39.25
unigram+bigram+TIMEX+temporal+syntactic 61291 21.94 -6.44
unigram+bigram+TIMEX+temporal+context 183833 25.62 9.25
unigram+bigram+TIMEX+temporal+context+prosodic 183871 27.44 7.10
unigram+bigram+TIMEX+temporal+context+prosodic+coarse DAs 183878 26.47 -3.53
unigram+bigram+TIMEX+temporal+context+prosodic+fine DAs 183927 31.92 16.33
Table 1: Performance of the maxent classifier as measured by F measure, the relative improvement from
the preceding feature set, and the number of features, across all feature sets tried. Italicized lines denote
the addition of features which do not improve performance; these are omitted from succeeding systems.
feature +/- ?
?pull? + 2.2100
?email? + 1.7883
?needs? + 1.7212
?added? + 1.6613
?mm-hmm? - 1.5937
?present? + 1.5740
?nine? - 1.5019
?!? - 1.5001
?five? - 1.4944
?together? + 1.4882
Table 2: Features, evidence type (positive denotes
action item), and weight for the top ten features
in the unigram-only model. ?Nine? and ?five? are
common words in the digit-reading task (see Sec-
tion 3).
feature +/- ?
?- $? - 1.4308
?i will? + 1.4128
?, $? - 1.3115
?uh $? - 1.2752
?w- $? - 1.2419
?. $? - 1.2247
?email? + 1.2062
?six $? - 1.1874
?* in? - 1.1833
?so $? - 1.1819
Table 3: Features, evidence type and weight for
the top ten features in the unigram+bigram model.
The symbol * denotes the beginning of an utter-
ance and $ the end. All of the top ten features are
bigrams except for the unigrams ?email?.
feature +/- ?
mean intensity (norm.) - 1.4288
mean pitch (norm.) - 1.0661
intensity range + 1.0510
?i will? + 0.8657
?email? + 0.8113
reformulate/summarize (DA) + 0.7946
?just go? (next) + 0.7190
?i will? (prev.) + 0.7074
?the paper? + 0.6788
understanding check (DA) + 0.6547
Table 4: Features, evidence type and weight for
the top ten features on the best-performing model.
Bigrams labeled ?prev.? and ?next? correspond to
the lexemes from previous and next utterances, re-
spectively. Prosodic features labeled as ?norm.?
have been normalized on a per-speaker basis.
boost in F measure in row 3 of Table 1.
The addition of part-of-speech tags actually de-
creases system performance. It is unclear why this
is the case. It may be that the unigram and bi-
gram features already adequately capture any dis-
tinctions these features make, or simply that these
features are generally not useful for distinguishing
action items.
Contextual features, on the other hand, im-
prove system performance significantly. A post-
hoc analysis of the action item annotations makes
clear why: action items are often split across mul-
tiple utterances (e.g. as in Figure 1), only a portion
of which contain lexical cues sufficient to distin-
guish them as such. Contextual features thus allow
utterances immediately surrounding these ?obvi-
ous? action items to be tagged as well.
101
Prosodic features yield a 7.10% increase in
F measure, and analysis shows that speaker-
normalized intensity and pitch, and the range in
intensity of an utterance, are valuable discrimina-
tive features. The subsequent addition of coarse-
grained dialog act tags does not further improve
system performance. It is likely this is due to rea-
sons similar to those for POS tags?either the cat-
egories are insufficient to distinguish action item
utterances, or whatever usefulness they provide is
subsumed by other features.
Table 4 shows the feature weights for the top-
ranked features on the best-scoring system. The
addition of the fine-grained DA tags results in a
significant increase in performance.The F measure
of this best feature set is 31.92%.
8 Conclusions
We have shown that several classes of features are
useful for the task of action item annotation from
multi-party meeting corpora. Simple lexical fea-
tures, their contextual versions, the time until the
end of the meeting, prosodic features, and fine-
grained dialog acts each contribute significant in-
creases in system performance.
While the raw system performance numbers of
Table 1 are low relative to other, better-studied
tasks on other, more mature corpora, we believe
the relative usefulness of the features towards this
task is indicative of their usefulness on more con-
sistent annotations, as well as to related tasks.
The Gruenstein et al (2005) corpus provides
a valuable and necessary resource for research in
this area, but several factors raise the question of
annotation quality. The low ? scores in Section 3
are indicative of annotation problems. Post-hoc
error analysis yields many examples of utterances
which are somewhat difficult to imagine as pos-
sible, never mind desirable, to tag. The fact that
the extremely useful oracular information present
in the fine-grained DA annotation does not raise
performance to the high levels that one might ex-
pect further suggests that the annotations are not
ideal?or, at the least, that they are inconsistent
with the DA annotations.2
This analysis is consistent with the findings of
Purver et al (2006), who achieve an F measure of
2Which is not to say they are devoid of significant value?
training and testing our best system on the corpus with the
590 positive classifications randomly shuffled across all ut-
terances yields an F measure of only 4.82.
less than 25% when applying SVMs to the classi-
fication task to the same corpus, and motivate the
development of a new corpus of action item anno-
tations.
9 Future work
In Section 6 we showed that contextual lexical
features are useful for the task of action item de-
tection, at least in the fairly limited manner em-
ployed in our implementation, which simply looks
at immediate previous and immediate next utter-
ances. It seems likely that applying a sequence
model such as an HMM or conditional random
field (CRFs) will act as a generalization of this fea-
ture and may further improve performance.
Addition of features such as speaker change and
?hot spots? (Wrede and Shriberg, 2003) may also
aid classification. Conversely, it is possible that
feature selection techniques may improve perfor-
mance by helping to eliminate poor-quality fea-
tures. In this work we have followed an ?ev-
erything but the kitchen sink? approach, in part
because we were curious about which features
would prove useful. The effect of adding POS and
coarse-grained DA features illustrates that this is
not necessarily the ideal strategy in terms of ulti-
mate system performance.
In general, the features evaluated in this
work are an indiscriminate mix of human- and
automatically-generated features; of the human-
generated features, some are plausible to generate
automatically, at some loss of quality (e.g. tran-
scripts) while others are unlikely to be automati-
cally generated in the foreseeable future (e.g. fine-
grained dialog acts). Future work may focus on
the effects that automatic generation of the former
has on overall system performance (although this
may require higher-quality annotations to be use-
ful.) For example, the detailed end-of-utterance
punctuation present in the human transcripts pro-
vides valuable discriminative power (Table 3), but
current ASR systems are not likely to be able to
provide this level of detail. Switching to ASR out-
put will have a negative effect on performance.
One final issue is that of utterance segmenta-
tion. The scheme used in the ICSI meeting corpus
does not necessarily correspond to the ideal seg-
mentation for other tasks. The action item annota-
tions were performed on these segmentations, and
in this study we did not attempt resegmentation,
but in the future it may prove valuable to collapse,
102
for example, successive un-interrupted utterances
from the same speaker into a single utterance.
In conclusion, while overall system perfor-
mance does not approach levels typical of better-
studied classification tasks such as named-entity
recognition, we believe that this is a largely a prod-
uct of the current action item annotation quality.
We believe that the feature analysis presented here
is useful, for this task and for other related tasks,
and that, provided with a set of more consistent
action item annotations, the current system can be
used as is to achieve better performance.
Acknowledgments
The authors wish to thank Dan Jurafsky, Chris
Manning, Stanley Peters, Matthew Purver, and
several anonymous reviewers for valuable advice
and comments.
References
Jeremy Ang, Yang Liu, and Elizabeth Shriberg. 2005.
Automatic dialog act segmentation and classifica-
tion in multiparty meetings. In Proceedings of the
ICASSP.
Paul N. Bennett and Jaime Carbonell. 2005. Detecting
action-items in e-mail. In Proceedings of SIGIR.
Adam Berger, Stephen Della Pietra, and Vincent Della
Pietra. 1996. A maximum entropy approach to nat-
ural language processing. Computational Linguis-
tics, 22(1):39?71.
D. Bikel, S. Miller, R. Schwartz, and R. Weischedel.
1997. Nymble: a high-performance learning name-
finder. In Proceedings of the Conference on Applied
NLP.
Paul Boersma and David Weenink. 2005. Praat: doing
phonetics by computer v4.4.12 (computer program).
J. Carletta. 1996. Assessing agreement on classifica-
tion tasks: The kappa statistic. Computational Lin-
guistics, 22(2):249?254.
William W. Cohen, Vitor R. Carvalho, and Tom M.
Mitchell. 2004. Learning to classify email into
?speech acts?. In Proceedings of EMNLP.
Simon Corston-Oliver, Eric Ringger, Michael Ga-
mon, and Richard Campbell. 2004. Task-focused
summarization of email. In Text Summarization
Branches Out: Proceedings of the ACL Workshop.
J. Godfrey, E. Holliman, and J.McDaniel. 1992.
SWITCHBOARD: Telephone speech corpus for
research and development. In Proceedings of
ICAASP.
Alexander Gruenstein, John Niekrasz, and Matthew
Purver. 2005. Meeting structure annotation: Data
and tools. In Proceedings of the 6th SIGDIAL Work-
shop on Discourse and Dialogue.
Adam Janin, Don Baron, Jane Edwards, Dan Ellis,
David Gelbart, Nelson Morgan, Barbara Peskin,
Thilo Pfau, Elizabeth Shriberg, Andreas Stolcke,
and Chuck Wooters. 2003. The ICSI meeting cor-
pus. In Proceedings of the ICASSP.
Gang Ji and Jeff Bilmes. 2005. Dialog act tag-
ging using graphical models. In Proceedings of the
ICASSP.
Marion Mast, R. Kompe, S. Harbeck, A. Kie?ling,
H. Niemann, E. No?th, E.G. Schukat-Talamazzini,
and V. Warnke. 1996. Dialog act classification with
the help of prosody. In Proceedings of the ICSLP.
Matthew Purver, Patrick Ehlen, and John Niekrasz.
2006. Detecting action items in multi-party meet-
ings: Annotation and initial experiments. In Pro-
ceedings of the 3rd Joint Workshop on MLMI.
Elizabeth Shriberg, Rebecca Bates, Andreas Stolcke,
Paul Taylor, Daniel Jurafsky, Klaus Ries, Noah Coc-
caro, Rachel Martin, Marie Meteer, and Carol Van
EssDykema. 1998. Can prosody aid the auto-
matic classification of dialog acts in conversational
speech? Language and Speech, 41(3?4):439?487.
Elizabeth Shriberg, Raj Dhillon, Sonali Bhagat, Jeremy
Ang, and Hannah Carvey. 2004. The ICSI meeting
recorder dialog act (MRDA) corpus. In Proceedings
of the 5th SIGDIAL Workshop on Discourse and Di-
alogue.
Andreas Stolcke, Elizabeth Shriberg, Rebecca Bates,
Noah Coccaro, Daniel Jurafsky, Rachel Mar-
tin, Marie Meteer, Klaus Ries, Paul Taylor, and
Carol Van EssDykema. 1998. Dialog act model-
ing for conversational speech. In Proceedings of
the AAAI Spring Symposium on Applying Machine
Learning to Discourse Processing.
Kristina Toutanova and Christopher D. Manning.
2000. Enriching the knowledge sources used in a
maximum entropy part-of-speech tagger. In Pro-
ceedings of EMNLP.
Britta Wrede and Elizabeth Shriberg. 2003. Spot-
ting ?hot spots? in meetings: Human judgments and
prosodic cues. In Proceedings of the European Con-
ference on Speech Communication and Technology.
Matthias Zimmermann, Yang Liu, Elizabeth Shriberg,
and Andreas Stolcke. 2005. Toward joint segmen-
tation and classification of dialog acts in multiparty
meetings. In Proceedings of the 2nd Joint Workshop
on MLMI.
103
