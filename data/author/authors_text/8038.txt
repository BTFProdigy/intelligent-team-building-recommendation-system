Entering Text with A FourButton Device
Kumiko TanakaIshii and Yusuke Inutsuka and Masato Takeichi
The University of Tokyo
 Bunkyoku Hongo Japan
fkumiko inu takeichigipltutokyoacjp
Abstract
This paper presents the design of a textentry device
that requires only four buttons Such a device is ap
plicable as the text interface of portable machines and
as an interface for disabled people The textentry
system is predictive the basis for this is an adaptive
language model Our evaluation showed that the sys
tem is at least as ecient for the entry of free text as
the textentry systems of currentgeneration mobile
phones The system requires fewer keystrokes than a
full keyboard After adaptation one user reached a
maximum speed of  wpm
 Introduction
Electronic machinery is becoming smaller recent de
velopments in palmtop and mobilephone technologies
oer dramatic examples of this process Since smaller
machines are more portable their users have freer ac
cess to information Here however the user interface
is a major issue
If a machine is being used as a medium for person
toperson communications a natural interface might
be speechbased For other tasks however like brows
ing through Internet pages or looking up databases
the most natural tool for control and data entry is
still the keyboard
Mobile machines oer little surface space so only
a few buttons are available for the entry of text The
most representative form is the use of 	
 keys for text
entry on mobile phones However even smaller ma
chines continue to appear such as watchsized com
puters It might not be possible to equip such ma
chines with more than four or ve buttons Ques
tions then arise Is it possible to enter text with a
small number of buttons What about four buttons
How ecient can we make this
Other potential applications for text entry with
four buttons include the control panels of oce ma
chines and household machines Although these ma
chines increasingly contain functions that allow ac
cess to the Internet sucient surface space for a full
keyboard is often not available Another potential
application is in textentry interfaces for elderly and
disabled people A report of Advanced Design of
Integrated Information Society 


 indicates that
keyboard operation is the highest hurdle to the use of
computers by the aged The situation is even worse
for people with handrelated disabilities A textentry
device with four large buttons might facilitate human
machine communications by such people
The idea of decreasing the number of keys on
the keyboard in itself is not new The oldest re
alization of this idea is the stenotype keyboards
With the recent popularity of mobile machines re
searchers have become increasingly interested in one
handed keyboardsMathias et al 	 Most of the
work to date in this eld has been related to mo
bile phones Text entry on currentgeneration devices
remains cumbersome so innovative companiesTegic
 


ZICorp 


Slangsoft 


 have pro
posed predictive methods for the more ecient entry
of text by implementing a method that had rst been
proposed some years earlierRau and Skiena 	
The results of several studies have veried its e
ciencyJames and Reischel 

	TanakaIshii et al



 so the technology looks promising in the con
text of mobile phones
Our study goes further in decreasing the number of
buttons than the abovecited studies In our study
we tried various textentry methods and found the
predictive method to be the best As far as we know
no other study that includes the application of a lan
guage model has yet been carried out in this context
neither has the eciency of this approach been ex
amined Additionally the major contribution is our
study of the potential power of a language model by
examining its actual eciency on a device with few
buttons
In the next section we rst show how text is en
tered via our TouchMeKey keypad
 An Example
Figure 		 shows the GUI for the TouchMeKey key
pad Nine buttons are visible with four on either side
of the central boxes plus a quit button on the right
hand side In this paper we only count those buttons
that are only used for the entry of characters that
is the four on the righthand side We also impose
the constraint that the buttons may only be pressed
one at a time because the inclusion of keychords in
creases the actual number of buttons by including the
combinations of keys
Six or seven letters of the alphabet are assigned to
each of the buttons The no 	 key has abcdef the
no  key has ghijkl the no  key has mnopqrs
and the no  key has tuvwxyz The small letters
Figure 	 Entering the word technology with the
TouchMeKey keypad
are assigned to the same keys as the corresponding
capital letters All other ASCII characters other than
the alphanumeric characters are assigned to the no
 key
Suppose that we have just entered the string hu
man language The text appears in the upper box
in the middle of the window the upper textbox in
Figure 		 We now wish to enter the word tech
nology Words are entered through a singletap
percharacter form of predictive entry a key is only
pressed once to enter a character For example the
no  button is pressed once to enter the t of tech
nology To enter the subsequent e the no 	 button
is pressed once
After the no 	 button has been pressed the Touch
MeKey window is as shown in Figure 	

 Here we
see two dierences from Figure 		 The rst is that
	 appears in the box in the middle of the window
This indicates the string that the user has just en
tered The second change is that some words have ap
peared in the lower box in the middle of the window
a listbox that we call the candidatebox These
words are the candidate words that correspond to the
users input 	
Each press of a button by the user makes the Touch
MeKey system automatically search the dictionary
for candidates The candidates include longer words
as well as if such words exist words of the same
length as the entered sequence of digits The can
didates are thus all words that begin with one letter
from tuvwxyz followed by one letter from abcdef
For example text was and vendors are candi
dates as is the twocharacter candidate we
The numerous candidates are sorted into an order
before they are placed in the candidate box and shown
to the user The order is according to word probabil
ity as determined on the basis of PPM prediction
by partial match which has been proposed in the
informationtheory domain A detailed description is
given in x but we summarize the methods essence
here as part of our explanation of Figure 	 The rele
vance of each candidate is measured by statistics from
two sources
Base dictionary the unigram statistics collected
from a huge corpus of newspaper data and
User corpus the ngram statistics obtained from a
small personal document supplied by the user
In this example the Base dictionary is constructed
from one year of issues of the Wall Street Journal
WSJ that contains  


 dierent words and the
User Corpus is a computer magazine that contains 	




 words The particular User corpus is the reason
for the appearance of the relatively uncommon word
vendors among the top ve candidates Figure 	
Our target technology appears as the second
ranked candidate In selecting this word the user
highlights it by using the down button on the left

Note that the most recently pressed button is framed by a
thick line
hand side of the windowFigure 	 and then presses
the enter button Figure 	 We see that the se
lected candidate now appears in the upper textbox


In describing our realization of the TouchMeKey
system outlined above the following four questions
are discussed in the remainder of this paper
Interface Is some method other than that described
above suitable for text entry with a fourbutton
device
Candidate Estimation How can the system esti
mate the relevance of each candidate
Key Assignment How should characters be as
signed to the individual buttons
Number Of Keys What is the minimum number of
keys required Is the entry of free text with only
two buttons reasonably ecient
 Interface
Various methods for the entry of text via a four
button device are conceivable The biggest choice is
whether or not to adopt a predictive method
 NonPredictive Entry Methods
Lets start by considering the case where we dont
adopt prediction This means that we need to enable
the exact entry of the individual characters via the
four buttons One method of this type involves as
signing an order to the characters on each key a key
is then pressed i times to obtain the ith character
we call this the multitap method This method is
commonly applied on mobile phones
However there are two problems with this method
Firstly the user often needs to press a key numerous
times to obtain a single target character Secondly
there is an ambiguity in the user action when two
characters assigned to the same button are to be en
tered one after another aa requires the entry of 		
that can also be b This situation requires the use
of an escape
A second possible method is to press a rst button
to select it and then enter the number i to select the
ith character which is assigned to the rst button
For example on many mobile phones o is obtained
by pressing the no  key and then the no  key
since o is the third letter on the no  key However
if the number of letters on each key is greater than
the number of keys entry of the higher i values is
implausibly dicult With the TouchMeKey system
for example a system for the easy entry of fth and
sixth characters etc is not possible
In short the free entry of text turns out to be too
dicult with a fourbutton device unless we adopt

As with any system where a predictive method is applied
the weak point of TouchMeKey is the processing of unknown
words which do not appear in the dictionary Therefore it
is important that the Base dictionary contains a rich vocabu
lary When however an unknown word occurs it may still be
entered character by character by using the methods described
inx or the system may be connected with a larger dictionary
via a network
Table 	 Data used in this work
name WSJ ZIFF JA
usage base user user
dictionary corpus corpus
domain newspaper computer scientic
magazine paper
Total no wrds  	 

million wrds
No di wrds 
 

 
thousand wrds
Wrds in common   
with Base
Dictionary 
Wrds Avr len   
L
avr

Test document  
 	
no wrds
No di wrds   
in test doc
prediction This is so even for the case of English the
written form of which has relatively few characters
and is even more so for languages with large numbers
of characters such as Chinese Japanese or Thai 
characters We are thus obliged to use prediction
 Predictive Text entry
Generally there are two ways to predict candidates
The rst is the singletap method The earliest
appearance of this idea was at the beginning of the

s in Japan in discussions of processing systems
for Japanese textCoLtd 	 more recent work
has been concerned with mobile phones James and
Reischel 

	TanakaIshii et al 



The second way is prediction by prefix Given a
user input the system searches for words with the
corresponding prefix
This method of collecting candidates to be oered
to the user has been particularly successful in the en
try of Chinese text The method has also been applied
to certain textentry systems in the manmachine in
terface domain too Masui 	
As the description of x indicates the combination
of the two methods is adopted in our TouchMeKey
system It thus needs to process many candidates
for a single user entry The mechanism of estimating
levels of relevance for the words is explained in the
next section
 Applying an Adaptive Language
Model in Candidate Estimation
As was summarized in x the PPM prediction by
partial match framework is used by TouchMeKey
to estimate the relevance of candidates Its charac
teristic is that the word distribution is adapted to the
style of the users corpus
PPM was originally proposed as an adaptive lan
guage model for use in improving the compression
rates of arithmetic coding The estimation of prob
abilities by PPM thus guarantees a lowering of the
entropy of the language model PPM has successfully
been adapted to the userinterface domain in certain
previous worksTanakaIshii et al 

	Ward et al




Broadly PPM interpolates the ngram counts in
the user corpus and the statistics in the base dictio
nary The following formula is used to estimate a
probability for the ith word w
i
 P w
i

P w
i
 
kmax
X
k
u
k
P
k
w
i
 	
Here k the order indicates the number of words be
fore w
i
that are used in the calculation of P
k
w
i

For example P

w
i
 is estimated on the basis of the
occurrence of w
i
and w
i
 P
k
w
i
 is calculated as
P
k
w
i
 
c
k
w
i

C
k

where C
k
is the frequency of the order k as a context
and c
k
w
i
 is the frequency with which w
i
occurs in
that context P
k
w
i
 when k  	 describes a base
probability that is obtained from the base dictionary
For other k P
k
w
i
 is calculated from statistics ob
tained from User corpus Finally u
k
is a weighting
probability that is multiplied to P
k
w
i
 to obtain the
nal P w
i
 Of the many studies of u
k
Teahan 



we have chosen PPMABell et al 	
 the sim
plest because our preliminary experiments showed no
signicant dierence in performance among the meth
ods we tried
We decided to utilize this PPM framework because
the context is the most suitable item of information
for the elimination of irrelevant candidates Small
machines are in a personal context and oce and
household machines are used in particular contexts
With this method the language model is adaptable
on the Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 871?879,
Singapore, 6-7 August 2009.
c?2009 ACL and AFNLP
Multilingual Spectral Clustering
Using Document Similarity Propagation
Dani Yogatama and Kumiko Tanaka-Ishii
Graduate School of Information Science and Technology, University of Tokyo
13F Akihabara Daibiru, 1-18-13 Kanda Chiyoda-ku, Tokyo, Japan
yogatama@cl.ci.i.u-tokyo.ac.jp kumiko@i.u-tokyo.ac.jp
Abstract
We present a novel approach for multilin-
gual document clustering using only com-
parable corpora to achieve cross-lingual
semantic interoperability. The method
models document collections as weighted
graph, and supervisory information is
given as sets of must-linked constraints for
documents in different languages. Recur-
sive k-nearest neighbor similarity propa-
gation is used to exploit the prior knowl-
edge and merge two language spaces.
Spectral method is applied to find the best
cuts of the graph. Experimental results
show that using limited supervisory in-
formation, our method achieves promis-
ing clustering results. Furthermore, since
the method does not need any language
dependent information in the process, our
algorithm can be applied to languages in
various alphabetical systems.
1 Introduction
Document clustering is unsupervised classifica-
tion of text collections into distinct groups of sim-
ilar documents. It has been used in many in-
formation retrieval tasks, including data organiza-
tion (Siersdorfer and Sizov, 2004), language mod-
eling (Liu and Croft, 2004), and improving per-
formances of text categorization system (Aggar-
wal et al, 1999). Advance in internet technology
has made the task of managing multilingual docu-
ments an intriguing research area. The growth of
internet leads to the necessity of organizing docu-
ments in various languages. There exist thousands
of languages, not to mention countless minor ones.
Creating document clustering model for each lan-
guage is simply unfeasible. We need methods to
deal with text collections in diverse languages si-
multaneously.
Multilingual document clustering (MLDC) in-
volves partitioning documents, written in more
than one languages, into sets of clusters. Simi-
lar documents, even if they are written in differ-
ent languages, should be grouped together into
one cluster. The major challenge of MLDC is
achieving cross-lingual semantic interoperability.
Most monolingual techniques will not work since
documents in different languages are mapped into
different spaces. Spectral method such as Latent
Semantic Analysis has been commonly applied
for MLDC task. However, current techniques
strongly rely on the presence of common words
between different languages. This method would
only work if the languages are highly related, i.e.,
languages that share the same root. Therefore, we
need another method to improve the robustness of
MLDC model.
In this paper, we focus on the problem of bridg-
ing multilingual space for document clustering.
We are given text documents in different lan-
guages and asked to group them into clusters such
that documents that belong to the same topic are
grouped together. Traditional monolingual ap-
proach is impracticable since it is unable to pre-
dict how similar two multilingual documents are.
They have two different spaces which make con-
ventional cosine similarity irrelevant. We try to
solve this problem utilizing prior knowledge in
the form of must-linked constraints, gathered from
comparable corpora. Propagation method is used
to guide the language-space merging process. Ex-
perimental results show that the approach gives
encouraging clustering results.
This paper is organized as follows. In section 2,
we review related work. In section 3, we propose
our algorithm for multilingual document cluster-
ing. The experimental results are shown in section
4. Section 5 concludes with a summary.
871
2 Related Work
Chen and Lin (2000) proposed methods to clus-
ter multilingual documents using translation tech-
nology, relying on cross-lingual dictionary and
machine-translation system. Multilingual ontol-
ogy, such as Eurovoc, is also popular for MLDC
(Pouliquen et al, 2004). However, such resources
are scarce and expensive to build. Several other
drawbacks of using this technique include dictio-
nary limitation and word ambiguity.
More recently, parallel texts have been used to
connect document collections from different lan-
guages (Wei et al, 2008). This is done by collaps-
ing columns in a term by document matrix that are
translations of each other. Nevertheless, building
parallel texts is also expensive and requires a lot of
works, hence shifting the paradigm of multilingual
works to comparable corpora.
Comparable corpora are collections of texts in
different languages regarding similar topics pro-
duced at the same time. The key difference be-
tween comparable corpora and parallel texts is that
documents in comparable corpora are not neces-
sarily translations of each other. They are easier
to be acquired, and do not need exhaustive works
to be prepared. News agencies often give informa-
tion in many different languages and can be good
sources for comparable corpora. Terms in com-
parable corpora, being about the same topic, up
to some point explain the same concepts in differ-
ent languages. Pairing comparable corpora with
spectral method such as Latent Semantic Analysis
has become prevalent, e.g. (Gliozzo and Strappar-
ava, 2005). They rely on the presence of common
words and proper nouns among various languages
to build a language-independent space. The per-
formance of such method is highly dependent on
the languages being used. Here, we present an-
other approach to exploit knowledge in compa-
rable corpora; using propagation method to aid
spreading similarity between collections of docu-
ments in different languages.
Spectral clustering is the task of finding good
clusters by using information contained in the
eigenvectors of a matrix derived from the data.
It has been successfully applied in many applica-
tions including information retrieval (Deerwester
et al, 2003) and computer vision (Meila and Shi,
2000). An in-depth analysis of spectral algo-
rithm for clustering problems is given in (Ng et
al., 2002). Zhang and Mao (2008) used a related
technique called Modularity Eigenmap to extract
community structure features from the document
network to solve hypertext classification problem.
Semi-supervised clustering enhances clustering
task by incorporating prior knowledge to aid clus-
tering process. It allows user to guide the cluster-
ing process by giving some feedback to the model.
In traditional clustering algorithm, only unlabeled
data is used to find assignments of data points
to clusters. In semi-supervised clustering, prior
knowledge is given to improve performance of the
system. The supervision is usually given as pair
of must-linked constraints and cannot link con-
straints, first introduced in (Wagstaff and Cardie,
2000). Kamvar et al (2003) proposed spectral
learning algorithm that can take supervisory infor-
mation in the form of pairwise constraints or la-
beled data. Their algorithm is intended to be used
in monolingual context, while our algorithm is de-
signed to work in multilingual context.
3 Multilingual Spectral Clustering
There have been several works on multilingual
document clustering as mention previously in Sec-
tion 2. Our key contribution here is the propaga-
tion method to make spectral clustering algorithm
works for multilingual problems. The clustering
model exploits the supervisory information by de-
tecting k nearest neighbors of the newly-linked
documents, and propagates document similarity to
these neighbors. The model can be applied to any
multilingual text collections regardless of the lan-
guages. Overall algorithm is given in Section 3.1
and the method to merge multilingual spaces by
similarity propagation is given in Section 3.2.
3.1 Spectral Clustering Algorithm
Spectral clustering tries to find good clusters by
using top eigenvectors of normalized data affin-
ity matrix. The document set is being modeled as
undirected graph G(V,E,W ), where V , E, and
W denote the graph vertex set, edge set, and tran-
sition probability matrix, respectively. In graph
G, v ? V represents a document, and weight
w
ij
?W represents transition probability between
document v
i
to v
j
. The transition probabilities
can be interpreted as edge flows in Markov ran-
dom walk over graph vertices (documents in col-
lections).
Algorithm to perform spectral clustering is
given in Algorithm 1. Let A be affinity matrix
872
where element A
ij
is cosine similarity between
document v
i
and v
j
(Algorithm 1, line 1). It is
straightforward that documents belonging to dif-
ferent languages will have similarity zero. Rare
exception occurs when they have common words
because the languages are related one another.
As a consequence, the similarity matrix will have
many zeros. Our model amplifies prior knowledge
in the form of comparable corpora by perform-
ing document similarity propagation, presented in
Section 3.2 (Algorithm 1, line 4; Algorithm 2, ex-
plained in Section 3.2). After propagation, the
affinity matrix is post-processed (Algorithm 1, line
6, explained in Section 3.2) before being trans-
formed into transition probability matrix.
The transformation can be done using any nor-
malization for spectral methods. Define N =
D
?1
A, as in (Meila and Shi, 2001), where D is the
diagonal matrix whose elements D
ij
=
?
j
A
ij
(Algorithm 1, line 7). Alternatively, we can define
N = D
?1/2
AD
?1/2
(Ng et al, 2002), or N =
(A + d
max
I ? D)/d
max
(Fiedler, 1975), where
d
max
is the maximum rowsum of A. For our ex-
periment, we use the first normalization method,
though other methods can be applied as well.
Meila and Shi (2001) show that probability tran-
sition matrix N with t strong clusters will have t
piecewise constant eigenvectors. They also sug-
gest using these t eigenvectors in clustering pro-
cess. We use the information contains in t largest
eigenvectors of N (Algorithm 1, line 8-11) and
perform K-means clustering algorithm to find the
data clusters (Algorithm 1, line 12).
3.2 Propagating Prior Knowledge
We use information obtained from comparable
corpora to merge multilingual language spaces.
Suppose we have text collections in L different
languages. We combine this collections with com-
parable corpora, also in L languages, that act as
our supervisory information. Comparable corpora
are used to gather prior knowledge by making
must-linked constraints for documents in different
languages that belong to the same topic in the cor-
pora, propagating similarity to other documents
while doing so.
Initially, our affinity matrix A represents cosine
similarity between all pairs of documents. A
ij
is
set to zero if j is not the top k nearest neighbors
of i and likewise. Next, set A
ij
and A
ji
to 1 if
document i and document j are different in lan-
Algorithm 1 Multilingual Spectral Clustering
Input: Term by document matrix M , pairwise
constraints
Output: Document clusters
1: Create graph affinity matrix A ? R
n?n
where
each element A
ij
represents the similarity be-
tween document v
i
and v
j
.
2: for all pairwise constraints in comparable cor-
pora do
3: A
ij
? 1, A
ji
? 1.
4: Recursive Propagation (A,S, ?, k, v
i
, v
j
).
5: end for
6: Post-process matrix A so that every value in
A is greater than ? and less than 1.
7: Form a diagonal matrix D, where D
ii
=
?
j
A
ij
. Normalize N = D
?1
A.
8: Find x
1
, x
2
? ? ? , x
t
, the t largest eigenvectors
of N.
9: Form matrix X = [x
1
, x
2
, ? ? ? , x
t
] ? R
n?t
.
10: Normalize row X to be unit length.
11: Project each document into eigen-space
spanned by the above t eigenvectors (by treat-
ing each row of X as a point in R
t
, row i rep-
resents document v
i
).
12: ApplyK-means algorithm in this space to find
document clusters.
guage and belong to the same topic in our com-
parable corpora. This will incorporate the must-
linked constraint to our model. We can also give
supervisory information for pairs of document in
the same language, but this is optional. We also do
not use cannot-linked constraints since the main
goal is to merge multilingual spaces. In our exper-
iment we show that using only must-linked con-
straints with propagation is enough to achieve en-
couraging clustering results.
The supervisory information acquired from
comparable corpora only connects two nodes in
our graph. Therefore, the number of edges be-
tween documents in different languages is about
as many as the number of must-linked constraints
given. We argue that we need more edges between
pairs of documents in different languages to get
better results.
We try to build more edges by propagating sim-
ilarity to other documents that are most similar to
the newly-linked documents. Figure 1 gives an il-
lustration of edge-creation process when two mul-
tilingual documents (nodes) are connected. Sup-
873
yx1
v
i
y
x2
z
x1
v
j
z
x2
(a) Connect two nodes
y
x1
v
i
y
x2
z
x1
v
j
z
x2
(b) Effect on neighbor nodes
Figure 1: Pairing two multilingual documents af-
fect their neighbors. v
i
and v
j
are documents in
two different languages. y
x
and z
x
are neighbors
of v
i
and v
j
respectively.
pose that we have six documents in two differ-
ent languages. Initially, documents are only con-
nected with other documents that belong to the
same language. The supervisory information tells
us that two multilingual documents v
i
and v
j
should be connected (Figure 1(a)). We then build
an edge between these two documents. Further-
more, we also use this information to build edges
between v
i
and neighbors of v
j
and likewise (Fig-
ure 1(b)).
This follows from the hypothesis that bringing
together two documents should also bring other
documents that are similar to those two closer in
our clustering space. Klein et al (2002) stated
that a good clustering algorithm, besides satisfy-
ing known constraints, should also be able to sat-
isfy the implications of those constraints. Here,
we allow not only instance-level inductive impli-
cations, but utilize it to get higher-level inductive
implications. In other words, we alter similarity
space so that it can detect other clusters by chang-
ing the topology of the original space.
The process is analogous to shortening the dis-
tance between sets of documents in Euclidean
space. In vector space model, two documents that
are close to each other have high similarity, and
thus will belong to the same cluster. Pairing two
documents can be seen as setting the distance in
this space to 0, thus raising their similarity to 1.
While doing so, each document would also draw
sets of documents connected to it closer to the cen-
tre of the merge, which is equivalent to increasing
their similarities.
Suppose we have document v
i
and v
j
, and y and
z are sets of their respective k nearest neighbors,
where |y| = |z| = k. The propagation method
is a recursive algorithm with base S, the num-
ber of desired level of propagation. Recursive k-
nearest neighbor makes decision to give high sim-
ilarity between multilingual documents not only
determined by their similarity to the newly-linked
documents, but also their similarity to the k near-
est neighbors of the respective document. Several
documents are affected by a single supervisory in-
formation. This will prove useful when only lim-
ited amount of supervisory information given. It
uses document similarity matrix A, as defined in
the previous section.
1. For y
x
? y we propagate ?A
v
i
y
x
to A
v
j
y
x
.
Set A
y
x
v
j
= A
v
j
y
x
(Algorithm 2, line 5-6).
In other words, we propagate the similarity
between document v
i
and y nearest neighbors
of v
i
to document v
j
.
2. Similarly, for z
x
? z we propagate ?A
v
j
z
x
to A
v
i
z
x
. Set A
z
x
v
i
= A
v
i
z
x
(Algorithm 2,
line 10-11). In other words, we propagate the
similarity between document v
j
and z nearest
neighbors of v
j
to document v
i
.
3. Propagate higher order similarity to k nearest
neighbors of y and z, discounting the similar-
ity quadratically, until required level of prop-
agation S is reached (Algorithm 2, line 7 and
12).
The coefficient ? represents the degree of en-
forcement that the documents similar to a docu-
ment in one language, will also have high simi-
larity with other document in other language that
is paired up with its ancestor. On the other hand,
k represents the number of documents that are af-
fected by pairing up two multilingual documents.
After propagation, similarity of documents that
falls below some threshold ? is set to zero (Al-
gorithm 1, line 6). This post-processing step is
performed to nullify insignificant similarity values
propagated to a document. Additionally, if there
exists similarity of documents that is higher than
one, it is set to one.
874
Algorithm 2 Recursive Propagation
Input: Affinity matrix A, level of propagation S,
?, number of nearest neighbors k, document v
i
and v
j
Output: Propagated affinity matrix
1: if S = 0 then
2: return
3: else
4: for all y
x
? k-NN document v
i
do
5: A
v
j
y
x
? A
v
j
y
x
+ ?A
v
i
y
x
6: A
y
x
v
j
? A
v
j
y
x
7: Recursive Propagation (A,S ? 1,
?
2
, k, y
x
, v
j
)
8: end for
9: for all z
x
? k-NN document v
j
do
10: Set A
v
i
z
x
? A
v
i
z
x
+ ?A
v
j
z
x
11: Set A
z
x
v
i
? A
v
i
z
x
12: Recursive Propagation (A,S ? 1,
?
2
, k, v
i
, z
x
)
13: end for
14: end if
4 Performance Evaluation
The goals of empirical evaluation include (1) test-
ing whether the propagation method can merge
multilingual space and produce acceptable clus-
tering results; (2) comparing the performance to
spectral clustering method without propagation.
4.1 Data Description
We tested our model using Reuters Corpus Vol-
ume 2 (RCV2), a multilingual corpus contain-
ing news in thirteen different languages. For our
experiment, three different languages: English,
French, and Spanish; in six different topics: sci-
ence, sports, disasters accidents, religion, health,
and economy are used. We discarded documents
with multiple category labels.
We do not apply any language specific pre-
processing method to the raw text data. Mono-
lingual TFIDF is used for feature weighting. All
document vectors are then converted into unit vec-
tor by dividing by its length. Table 1 shows the
average length of documents in our corpus.
4.2 Evaluation Metric
For our experiment, we used Rand Index (RI)
which is a common evaluation technique for clus-
tering task where the true class of unlabeled data
English French Spanish Total
Science 290.10 165.10 213.45 222.88
Sports 182.55 156.83 189.75 176.37
Disasters 154.29 175.89 165.31 165.16
Religion 317.77 177.91 242.67 246.11
Health 251.19 233.70 227.25 237.38
Economy 266.89 192.55 306.11 255.08
Total 243.79 183.61 224.09 217.16
Table 1: Average number of words of documents
in the corpus. Each language consists of 600 doc-
uments, and each topic consists of 100 documents
(per language).
is known. Rand Index measures the percentage of
decisions that are correct, or simply the accuracy
of the model. Rand Index is defined as:
RI =
TP + TN
TP + FP + TN + FN
Rand Index penalizes false positive and false neg-
ative decisions during clustering. It takes into ac-
count decision that assign two similar documents
to one cluster (TP), two dissimilar documents to
different clusters (TN), two similar documents to
different clusters (FN), and two dissimilar docu-
ments to one cluster (FP). We do not include links
created by supervisory information when calculat-
ing true positive decisions and only consider the
number of free decisions made.
We also used F
?
-measure, the weighted har-
monic mean of precision (P) and recall (R). F
?
-
measure is defined as:
F
?
=
(?
2
+ 1)PR
?
2
P +R
P =
TP
TP + FP
R =
TP
TP + FN
Last, we used purity to evaluate the accuracy of
assignments. Purity is defined as:
Purity =
1
N
?
t
max
j
|?
t
? c
j
|
whereN is the number of documents, t is the num-
ber of clusters, j is the number of classes, ?
t
and
c
j
are sets of documents in cluster t and class j
respectively.
875
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.2  0.4  0.6  0.8  1
R
an
d 
In
de
x
Proportion of supervisory information
With propagation
Without propagation
LSA
(a) Rand Index for 6 topics
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.2  0.4  0.6  0.8  1
R
an
d 
In
de
x
Proportion of supervisory information
With propagation
Without propagation
LSA
(b) Rand Index for 4 topics
Figure 2: Rand Index on the RCV2 task with (a) 1800 documents, 6 topics; and (b) 1200 documents, 4
topics as the proportion of supervisory information increases. k = 30, ? = 0.03, ? = 0.5, t = number of
topics, and S = 2.
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.2  0.4  0.6  0.8  1
P
ur
ity
Proportion of supervisory information
With propagation
Without propagation
LSA
(a) Purity for 6 topics
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.2  0.4  0.6  0.8  1
P
ur
ity
Proportion of supervisory information
With propagation
Without propagation
LSA
(b) Purity for 4 topics
Figure 3: Purity on the RCV2 task with (a) 1800 documents, 6 topics; and (b) 1200 documents, 4 topics
as the proportion of supervisory information increases. k = 30, ? = 0.03, ? = 0.5, t = number of topics,
and S = 2.
4.3 Experimental Results
To prove the effectiveness of our clustering algo-
rithm, we performed the following experiments on
our data set. We first tested our algorithm on four
topics, science, sports, religion, and economy. We
then tested our algorithm using all six topics to
get an understanding of the performance of our
model in larger collections with more topics. We
used subset of our data as supervisory informa-
tion and built must-linked constraints from it. The
proportion of supervisory information provided to
the system is given in x-axis (Figure 2 - Figure
4.3). 0.2 here means 20% of documents in each
language are taken to be used as prior knowledge.
Since the number of documents in each language
for our experiment is the same, we have the same
numbers of documents in subset of English col-
lection, subset of French collection, and subset of
Spanish collection. We also ensure there are same
numbers of documents for a particular topic in all
three languages. We can build must-linked con-
straints as follows. For each document in the sub-
set of English collection, we create must-linked
constraints with one randomly selected document
from the subset of French collection and one ran-
domly selected document from the subset of Span-
ish collection that belong to the same topic with it.
We then create must-linked constraint between the
respective French and Spanish documents. The
constraints given to the algorithm are chosen so
that there are several links that connect every topic
in every language. Note that the class label in-
876
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.2  0.4  0.6  0.8  1
F
2-
m
e
a
s
u
re
Proportion of supervisory information
With propagation
Without propagation
LSA
(a) F
2
-measure for 6 topics
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.2  0.4  0.6  0.8  1
F
2-
m
e
a
s
u
re
Proportion of supervisory information
With propagation
Without propagation
LSA
(b) F
2
-measure for 4 topics
Figure 4: F
2
-measure on the RCV2 task with (a) 1800 documents, 6 topics; and (b) 1200 documents, 4
topics as the proportion of supervisory information increases. k = 30, ? = 0.03, ? = 0.5, t = number of
topics, and S = 2.
formation is only used to build must-linked con-
straints between documents, and we do not assign
the documents to a particular cluster.
Figure 2 shows the Rand Index as proportion
of supervisory information increases. Figure 3
and Figure 4.3 give purity and F
2
-measure for
the algorithm respectively. To show the impor-
tance of the propagation in multilingual space, we
give comparison with spectral clustering model
without propagation. Three lines in Figure 2 to
Figure 4.3 indicate: (1) results with propagation
(solid line); (2) results without propagation (long-
dashed line); and (3) results using Latent Se-
mantic Analysis(LSA)-based method by exploit-
ing common words between languages (short-
dashed line). For each figure, 6 plots are taken
starting from 0 in 0.2-point-increments. We con-
ducted the experiments three times for each pro-
portion of supervisory information and use the av-
erage values. As we can see from Figure 2, Fig-
ure 3, and Figure 4.3, the propagation method can
significantly improve the performance of spectral
clustering algorithm. For 1800 documents in 6
topics, we manage to achieve RI = 0.91, purity
= 0.84, and F
2
-measure = 0.76 with only 20% of
documents (360 documents) used as supervisory
information. Spectral clustering algorithm with-
out propagation can only achieve 0.69, 0.30, 0.28
for RI, purity, and F
2
-measure respectively. The
propagation method is highly effective when only
small amount of supervisory information given to
the algorithm. Obviously, the more supervisory in-
formation given, the better the performance is. As
the number of supervisory information increases,
the difference of the model performance with and
without propagation becomes smaller. This is
because there are already enough links between
multilingual documents, so we do not necessar-
ily build more links through similarity propagation
anymore. However, even when there are already
many links, our model with propagation still out-
performs the model without propagation.
We compare the performance of our algorithm
to LSA-based multilingual document clustering
model. We performed LSA to the multilingual
term by document matrix. We do not use paral-
lel texts and only rely on common words across
languages as well as must-linked constraints to
build multilingual space. The results show that ex-
ploiting common words between languages alone
is not enough to build a good multilingual se-
mantic space, justifying the usage of supervisory
information in multilingual document clustering
task. When supervisory information is introduced,
our method achieves better results than LSA-based
method. In general, the LSA-based method per-
forms better than the model without propagation.
We assess the sensitivity of our algorithm to
parameter ?, the penalty for similarity propaga-
tion. We assess the sensitivity of our algorithm
to parameter ?, the penalty for similarity prop-
agation. We tested our algorithm using various
?, starting from 0 to 1 in 0.2-point-increments,
while other parameters being held constant. Fig-
ure 5(a) shows that changing ? to some extent af-
fects the performance of the algorithm. However,
after some value of reasonable ? is found, increas-
ing ? does not have significant impact on the per-
877
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.2  0.4  0.6  0.8  1
R
an
d 
In
de
x
?
(a) Changing ?, k = 30, t = 6
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  20  40  60  80  100
R
an
d 
In
de
x
k
(b) Changing k, ? = 0.5, t = 6
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  5  10  15  20
R
an
d 
In
de
x
t
(c) Changing t, ? = 0.5, k = 30
Figure 5: Rand Index on the RCV2 task with 1800 documents and 6 topics as (a) ? increases; (b)
k increases; and (c) t increases. ? = 0.03, S = 2, and 20% of documents are used as supervisory
information.
formance of the algorithm. We also tested our al-
gorithm using various k, starting from 0 to 100
in 20-point-increments. Figure 5(b) reveals that
the performances of the model with different k are
comparable, as long as k is not too small. How-
ever, using too large k will slightly decrease the
performance of the model. Too many propaga-
tions make several dissimilar documents receive
high similarity value that cannot be nullified by
the post-processing step. Last, we experimented
using various t ranging from 2 to 20. Figure 5(c)
shows that the method performs best when t = 10,
and for reasonable value of t the method achieves
comparable performance.
5 Conclusion
We present here a multilingual spectral cluster-
ing model that is able to work irrespective of the
languages being used. The key component of
our model is the propagation algorithm to merge
multilingual spaces. We tested our algorithm
on Reuters RCV2 Corpus and compared the per-
formance with spectral clustering model without
propagation. Experimental results reveal that us-
ing limited supervisory information, the algorithm
achieves encouraging clustering results.
References
Charu C. Aggarwal, Stephen C. Gates and Philip S.
Yu. 1999. On The Merits of Building Catego-
rization Systems by Supervised Clustering. In Pro-
ceedings of Conference on Knowledge Discovery in
Databases:352-356.
Hsin-Hsi Chen and Chuan-Jie Lin. 2000. A Mul-
tilingual News Summarizer. In Proceedings of
18th International Conference on Computational
Linguistics:159-165.
Scott C. Deerwester, Susan T. Dumais, Thomas K. Lan-
dauer, George W. Furnas, and Richard A. Harsh-
man. 1990. Indexing by Latent Semantic Analy-
sis. Journal of the American Society of Information
Science:41(6):391-407.
Miroslav Fiedler. 1975. A Property of Eigenvectors of
Nonnegative Symmetric Matrices and its Applica-
tions to Graph Theory. Czechoslovak Mathematical
Journal, 25:619-672.
878
Alfio Gliozzo and Carlo Strapparava. 2005. Cross lan-
guage Text Categorization by acquiring Multilingual
Domain Models from Comparable Corpora. In Pro-
ceedings of the ACL Workshop on Building and Us-
ing Parallel Texts:9-16.
Sepandar D. Kamvar, Dan Klein, and Christopher D.
Manning. 2003. Spectral Learning. In Proceed-
ings of the International Joint Conference on Artifi-
cial Intelligence (IJCAI).
Dan Klein, Sepandar D. Kamvar, and Christopher D.
Manning. 2002. From instance-level constraints to
space-level constraints: Making the most of prior
knowledge in data clustering. In The Nineteenth In-
ternational Conference on Machine Learning.
Xiaoyong Liu and W. Bruce Croft. 2004. Cluster-
based Retrieval using Language Models. In Pro-
ceedings of the 27th annual international ACM SI-
GIR conference on Research and development in in-
formation retrieval:186-193.
Marinla Meil?a and Jianbo Shi. 2000. Learning seg-
mentation by random walks. In Advances in Neural
Information Processing Systems:873-879.
Marinla Meil?a and Jianbo Shi. 2001. A Random Walks
View of Spectral Segmentation. In AI and Statistics
(AISTATS).
Andrew Y. Ng, Michael I. Jordan, and Yair Weiss.
2002. On Spectral Clustering: Analysis and an al-
gorithm. In Proceedings of Advances in Neural In-
formation Processing Systems (NIPS 14).
Bruno Pouliquen, Ralf Steinberger, Camelia Ignat,
Emilia K?asper, and Irina Temnikova. 2004. Mul-
tilingual and Cross-lingual News Topic Tracking. In
Proceedings of the 20th International Conference on
Computational Linguistics.
Stefan Siersdorfer and Sergej Sizov. 2004. Restrictive
Clustering and Metaclustering for Self-Organizing
Document. In Proceedings of the 27th annual in-
ternational ACM SIGIR conference on Research and
development in information retrieval.
Kiri Wagstaff and Claire Cardie 2000. Clustering
with Instance-level Constraints. In Proceedings
of the 17th International Conference on Machine
Learning:1103-1110.
Chih-Ping Wei, Christopher C. Yang, and Chia-Min
Lin. 2008. A Latent Semantic Indexing Based Ap-
proach to Multilingual Document Clustering. In De-
cision Support Systems, 45(3):606-620
Dell Zhang and Robert Mao. 2008. Extracting Com-
munity Structure Features for Hypertext Classifi-
cation. In Proceedings of the 3rd IEEE Interna-
tional Conference on Digital Information Manage-
ment (ICDIM).
879








Entropy as an Indicator of Context Boundaries
?An Experiment Using a Web Search Engine?
Kumiko Tanaka-Ishii
Graduate School of Information Science and Technology,
University of Tokyo
kumiko@i.u-tokyo.ac.jp
Abstract. Previous works have suggested that the uncertainty of tokens
coming after a sequence helps determine whether a given position is
at a context boundary. This feature of language has been applied to
unsupervised text segmentation and term extraction. In this paper, we
fundamentally verify this feature. An experiment was performed using a
web search engine, in order to clarify the extent to which this assumption
holds. The verification was applied to Chinese and Japanese.
1 Introduction
The theme of this paper is the following assumption:
The uncertainty of tokens coming after a sequence helps determine whether
a given position is at a context boundary. (A)
Intuitively, the variety of successive tokens at each character inside a word mono-
tonically decreases according to the offset length, because the longer the preced-
ing character n-gram, the longer the preceding context and the more it restricts
the appearance of possible next tokens. On the other hand, the uncertainty at
the position of a word border becomes greater and the complexity increases, as
the position is out of context. This suggests that a word border can be detected
by focusing on the differentials of the uncertainty of branching. This assumption
is illustrated in Figure 1. In this paper, we measure this uncertainty of successive
tokens by utilizing the entropy of branching (which we mathematically define in
the next section).
This assumption dates back to the fundamental work done by Harris [6] in
1955, where he says that when the number of different tokens coming after every
prefix of a word marks the maximum value, then the location corresponds to the
morpheme boundary. Recently, with the increasing availability of corpora, this
characteristic of language data has been applied for unsupervised text segmenta-
tion into words and morphemes. Kempe [8] reports an experiment to detect word
borders in German and English texts by monitoring the entropy of successive
characters for 4-grams. Many works in unsupervised segmentation utilise the
fact that the branching stays low inside words but increases at a word or mor-
pheme border. Some works apply this fact in terms of frequency [10] [2], while
others utilise more sophisticated statistical measures: Sun et al [12] use mutual
information; Creutz [4] use MDL to decompose Finnish texts into morphemes.
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 93?105, 2005.
c
? Springer-Verlag Berlin Heidelberg 2005
94 K. Tanaka-Ishii
This assumption seems to hold not only at the character level but also at
the word level. For example, the uncertainty of words coming after the word
sequence, ?The United States of?, is small (because the word America is very
likely to occur), whereas the uncertainty is greater for the sequence ?computa-
tional linguistics?, suggesting that there is a context boundary just after this
term. This observation at the word level has been applied to term extraction
by utilising the number of different words coming after a word sequence as an
indicator of collocation boundaries [5] [9].
Fig. 1. Intuitive illustration of a variety of successive tokens and a word boundary
As can be seen in these previous works, the above assumption (A) seems
to govern language structure both microscopically at the morpheme level and
macroscopically at the phrase level. Assumption (A) is interesting not only from
an engineering viewpoint but also from a language and cognitive science view-
point. For example, some recent studies report that the statistical, innate struc-
ture of language plays an important role in children?s language acquisition [11].
Therefore, it is important to understand the innate structure of language, in
order to shed light on how people actually acquire it.
Consequently, this paper verifies assumption (A) in a fundamental manner.
We address the questions of why and to what extent (A) holds. Unlike recent,
previous works based on limited numbers of corpora, we use a web search engine
to obtain statistics, in order to avoid the sparseness problem as much as pos-
sible. Our discussion focuses on correlating the entropy of branching and word
boundaries, because the definition of a word boundary is clearer than that of a
morpheme or phrase unit. In terms of detecting word boundaries, our experi-
ments were performed in character sequence, so we chose two languages in which
segmentation is a crucial problem: Chinese which contains only ideograms, and
Japanese, which contains both ideograms and phonograms. Before describing
the experiments, we discuss assumption (A) in more detail.
2 The Assumption
Given a set of elements ? and a set of n-gram sequences ?n formed of ?, the
conditional entropy of an element occurring after an n-gram sequence Xn is
defined as
Entropy as an Indicator of Context Boundaries 95
Fig. 2. Decrease in H(X|Xn) for characters when n is increased
H(X |Xn) = ?
?
xn??n
P (Xn = xn)
?
x??
P (X = x|Xn = xn) log P (X = x|Xn = xn)
where P (X = x) indicates the probability of occurrence of x.
A well-known observation on language data states that H(X |Xn) decreases
as n increases [3]. For example, Figure 2 shows the entropy values as n increases
from 1 to 9 for a character sequence. The two lines correspond to Japanese and
English data, from corpora consisting of the Mainichi newspaper (30 MB) and
the WSJ (30 MB), respectively. This phenomenon indicates that X will become
easier to estimate as the context of Xn gets longer. This can be intuitively
understood: it is easy to guess that ?e? will follow after ?Hello! How ar?, but it
is difficult to guess what comes after the short string ?He?.
The last term ? log P (X = x|Xn = xn) in formula above indicates the
information of a token of x coming after xn, and thus the branching after xn.
The latter half of the formula, the local entropy value for a given xn
H(X |Xn = xn) = ?
?
x??
P (X = x|Xn = xn) log P (X = x|Xn = xn), (1)
indicates the average information of branching for a specific n-gram sequence xn.
As our interest in this paper is this local entropy, we denote simply H(X |Xn =
xn) as h(xn) in the rest of this paper.
The decrease in H(X |Xn) globally indicates that given an n-length sequence
xn and another (n + 1)-length sequence yn+1, the following inequality holds on
average:
h(xn) > h(yn+1). (2)
One reason why inequality (2) holds for language data is that there is context in
language, and yn+1 carries a longer context as compared with xn. Therefore, if
we suppose that xn is the prefix of xn+1, then it is very likely that
h(xn) > h(xn+1) (3)
holds, because the longer the preceding n-gram, the longer the same context. For
example, it is easier to guess what comes after x6=?natura? than what comes
after x5 = ?natur?. Therefore, the decrease in H(X |Xn) can be expressed as the
96 K. Tanaka-Ishii
Fig. 3. Our model for boundary detection based on the entropy of branching
concept that if the context is longer, the uncertainty of the branching decreases
on average. Then, taking the logical contraposition, if the uncertainty does not
decrease, the context is not longer, which can be interpreted as the following:
If the complexity of successive tokens increases, the location is at the
context border. (B)
For example, in the case of x7 = ?natural?, the entropy h(?natural?) should
be larger than h(?natura?), because it is uncertain what character will allow x7
to succeed. In the next section, we utilise assumption (B) to detect the context
boundary.
3 Boundary Detection Using the Entropy of Branching
Assumption (B) gives a hint on how to utilise the branching entropy as an
indicator of the context boundary. When two semantic units, both longer than
1, are put together, the entropy would appear as in the first figure of Figure 3.
The first semantic unit is from offsets 0 to 4, and the second is from 4 to 8,
with each unit formed by elements of ?. In the figure, one possible transition of
branching degree is shown, where the plot at k on the horizontal axis denotes
the entropy for h(x0,k) and xn,m denotes the substring between offsets n and m.
Ideally, the entropy would take a maximum at 4, because it will decrease as
k is increased in the ranges of k < 4 and 4 < k < 8, and at k = 4, it will rise.
Therefore, the position at k = 4 is detected as the ?local maximum value? when
monitoring h(x0,k) over k. The boundary condition after such observation can
be redefined as the following:
Bmax Boundaries are locations where the entropy is locally maximised.
A similar method is proposed by Harris [6], where morpheme borders can be
detected by using the local maximum of the number of different tokens coming
after a prefix.
This only holds, however, for semantic units longer than 1. Units often have
a length of 1: at the character level, in Japanese and Chinese, there are many
one-character words, and at the word level, there are many single words that do
not form collocations. If a unit has length 1, then the situation will look like the
second graph in Figure 3, where three semantic units, x0,4, x4,5 x5,8, are present,
with the middle unit having length 1. First, at k = 4, the value of h increases.
Entropy as an Indicator of Context Boundaries 97
At k = 5, the value may increase or decrease, because the longer context results
in an uncertainty decrease, though an uncertainty decrease does not necessarily
mean a longer context. When h increases at k = 5, the situation would look like
the second graph. In this case, the condition Bmaxwill not suffice, and we need
a second boundary condition:
Bincrease Boundaries are locations where the entropy is increased.
On the other hand, when h decreases at k = 5, then even Bincreasecannot be
applied to detect k = 5 as a boundary. We have other chances to detect k = 5,
however, by considering h(xi,k) where 0 < i < k. According to inequality (2),
then, a similar trend should be present for plots of h(xi,k), assuming h(x0,n) >
h(x0,n+1); then, we have
h(xi,n) > h(xi,n+1), for 0 < i < n. (4)
The value h(xi,k) would hopefully rise for some i if the boundary at k = 5 is
important, although h(xi,k) can increase or decrease at k = 5, just as in the case
for h(x0,n).
Therefore, when the target language consists of many one element units,
Bincreaseis crucial for collecting all boundaries. Note that boundaries detected
by Bmaxare included in those detected by the condition Bincrease.
Fig. 4. Kempe?s model for boundary detection
Kempe?s detection model is based solely on the assumption that the un-
certainty of branching takes a local maximum at a context boundary. Without
any grounding on this assumption, Kempe [8] simply calculates the entropy of
branching for a fixed length of 4-grams. Therefore, the length of n is set to 3,
h(xi?3,i) is calculated for all i, and the maximum values are claimed to indicate
the word boundary. This model is illustrated in Figure 4, where the plot at each
k indicates the value of h(xk?3,k). Note that at k = 4, the h value will be highest.
It is not possible, however, to judge whether h(xi?3,i) is larger than h(xi?2,i+1)
in general: Kempe?s experiments show that the h value simply oscillates at a low
value in such cases.
In contrast, our model is based on the monotonic decrease in H(X |Xn). It
explains the increase in h at the context boundary by considering the entropy
decrease with a longer context.
98 K. Tanaka-Ishii
Summarising what we have examined, in order to verify assumption (A),
which is replaced by assumption (B), the following questions must be answered
experimentally:
Q1 Does the condition described by inequality (3) hold?
Q2 Does the condition described by inequality (4) hold?
Q3 To what extent are boundaries extracted by Bmaxor Bincrease?
In the rest of this paper, we demonstrate our experimental verification of these
questions.
So far, we have considered only regular order processing: the branching degree
is calculated for successive elements of xn. We can also consider the reverse order,
which involves calculating h for the previous element of xn. In the case of the
previous element, the question is whether the head of xn forms the beginning of
a context boundary. We use the subscripts suc and prev to indicate the regular
and reverse orders, respectively. Thus, the regular order is denoted as hsuc(xn),
while the reverse order is denoted by hprev(xn).
In the next section, we explain how we measure the statistics of xn, before
proceeding to analyze our results.
4 Measuring Statistics by Using the Web
In the experiments described in this paper, the frequency counts were obtained
using a search engine. This was done because the web represents the largest pos-
sible database, enabling us to avoid the data sparseness problem to the greatest
extent possible.
Given a sequence xn, h(xn) is measured by the following procedure.
1. xn is sent to a search engine.
2. One thousand snippets, at maximum, are downloaded and xn is searched
for through these snippets. If the number of occurrences is smaller than N ,
then the system reports that xn is unmeasurable.
3. The elements occurring before and after xn are counted, and hsuc(xn) and
hprev(xn) are calculated.
N is a parameter in the experiments described in the following section, and
a higher N will give higher precision and lower recall. Another aspect of the
experiment is that the data sparseness problem quickly becomes significant for
longer strings. To address these issues, we chose N=30.
The value of h is influenced by the indexing strategy used by a given search
engine. Defining f(x) as the frequency count for string x as reported by the
search engine,
f(xn) > f(xn+1) (5)
should usually hold if xn is a prefix of xn+1, because all occurrences of xn contain
occurrences of xn+1. In practice, this does not hold for many search engines,
namely, those in which xn+1 is indexed separately from xn and an occurrence of
xn+1 is not included in one of xn. For example, the frequency count of ?mode?
does not include that of ?model?, because it is indexed separately. In particular,
Entropy as an Indicator of Context Boundaries 99
Fig. 5. Entropy changes for a Japanese character sequence (left:regular; right:reverse)
search engines use this indexing strategy at the string level for languages in
which words are separated by spaces, and in our case, we need a search engine
in which the count of xn includes that of xn+1. Although we are interested in
the distribution of tokens coming after the string xn and not directly in the
frequency, a larger value of f(xn) can lead to a larger branching entropy.
Among the many available search engines, we decided to use AltaVista, be-
cause its indexing strategy seems to follow inequality (5) better than do the
strategies of other search engines. AltaVista used to utilise string-based index-
ing, especially for non-segmented languages. Indexing strategies are currently
trade secrets, however, so companies rarely make them available to the pub-
lic. We could only guess at AltaVistafs strategy by experimenting with some
concrete examples based on inequality (5).
5 Analysis for Small Examples
We will first examine the validity of the previous discussion by analysing some
small examples. Here, we utilise Japanese examples, because this language con-
tains both phonograms and ideograms, and it can thus demonstrate the features
of our method for both cases.
The two graphs in Figure 5 show the actual transition of h for a Japanese
sentence formed of 11 characters: x0,11 = (We think of
the future of (natural) language processing (studies)). The vertical axis represents
the entropy value, and the horizontal axis indicates the offset of the string. In
the left graph, each line starting at an offset of m+1 indicates the entropy values
of hsuc(xm,m+n) for n > 0, with plotted points appearing at k = m + n. For
example, the leftmost solid line starting at offset k = 1 plots the h values of x0,n
for n > 0, with m=0 (refer to the labels on some plots):
x0,1 =
x0,2 =
. . .
x0,5 =
with each value of h for the above sequence x0,n appearing at the location of n.
???????????
?
??
?????
100 K. Tanaka-Ishii
Concerning this line, we may observe that the value increases slightly at po-
sition k = 2, which is the boundary of the word (language). This location
will become a boundary for both conditions, Bmaxand Bincrease. Then, at posi-
tion k = 3, the value drastically decreases, because the character coming after
(language proce) is limited (as an analogy in English, ssing is the major
candidate that comes after language proce). The value rises again at x0,4, be-
cause the sequence leaves the context of (language processing). This
location will also become a boundary whether Bmaxor Bincreaseis chosen. The
line stops at n = 5, because the statistics of the strings x0,n for n > 5 were
unmeasurable.
The second leftmost line starting from k = 2 shows the transition of the
entropy values of hsuc(x1,1+n) for n > 0; that is, for the strings starting from
the second character , and so forth. We can observe a trend similar to
that of the first line, except that the value also increases at 5, suggesting that
k = 5 is the boundary, given the condition Bincrease.
The left graph thus contains 10 lines. Most of the lines are locally maximized
or become unmeasurable at the offset of k = 5, which is the end of a large portion
of the sentence. Also, some lines increase at k = 2, 4, 7, and 8, indicating the
ends of words, which is correct. Some lines increase at low values at 10: this
is due to the verb (think), whose conjugation stem is detected as a
border.
Similarly, the right-hand graph shows the results for the reverse order, where
each line ending at m ? 1 indicates the plots of the value of hprev(xm?n,m) for
n > 0, with the plotted points appearing at position k = m ? n. For example,
the rightmost line plots h for strings ending with (from m = 11 and n = 10
down to 5):
x10,11 =
x9,11 =
. . .
x6,11 =
x5,11 =
where x4,11 became unmeasurable. The lines should be analysed from back to
front, where the increase or maximum indicates the beginning of a word. Overall,
the lines ending at 4 or 5 were unmeasurable, and the values rise or take a
maximum at k = 2, 4 or 7.
Note that the results obtained from the processing in each direction differ.
The forward pass detects 2,4,5,7,8, whereas the backward pass detects 2,4,7.
The forward pass tends to detect the end of a context, while the backward pass
typically detects the beginning of a context. Also, it must be noted that this
analysis not only shows the segmenting position but also the structure of the
sentence. For example, a rupture of the lines and a large increase in h are seen
at k = 5, indicating the large semantic segmentation position of the sentence. In
the right-hand graph, too, we can see two large local maxima at 4 and 7. These
segment the sentence into three different semantic parts.
??
???
????
?
???
?
??
?????
??????
?
Entropy as an Indicator of Context Boundaries 101
Fig. 6. Other segmentation examples
On these two graphs, questions Q1 through Q3 from ?3 can be addressed as
follows. First, as for Q1, the condition indicated by inequality (3) holds in most
cases where all lines decrease at k = 3, 6, 9, which correspond to inside words.
There is one counter-example, however, caused by conjugation. In Japanese con-
jugation, a verb has a prefix as the stem, and the suffix varies. Therefore, with
our method, the endpoint of the stem will be regarded as the boundary. As con-
jugation is common in languages based on phonograms, we may guess that this
phenomenon will decrease the performance of boundary detection.
As for Q2, we can say that the condition indicated by inequality (4) holds,
as the upward and downward trends at the same offset k look similar. Here
too, there is a counter-example, in the case of a one element word, as indicated
in ?3. There are two one-word words x4,5= and x7,8= , where the
gradients of the lines differ according to the context length. In the case of one
of these words, h can rise or fall between two successive boundaries indicating
a beginning and end. Still, we can see that this is complemented by examining
lines starting from other offsets. For example, at k = 5, some lines end with an
increase.
As for Q3, if we pick boundary condition Bmax, by regarding any unmeasur-
able case as h = ??, and any maximum of any line as denoting the boundary,
then the entry string will be segmented into the following:
This segmentation result is equivalent to that obtained by many other Japanese
segmentation tools. Taking Bincreaseas the boundary condition, another bound-
ary is detected in the middle of the last verb (think, segmented at
(language)j (processing)j (of)j (future)j (of)j (think).
? ?
?? ?? ?? ? ???
?? ?
?
102 K. Tanaka-Ishii
the stem of the verb)?. If we consider detecting the word boundary, then this
segmentation is incorrect; therefore, to increase the precision, it would be better
to apply a threshold to filter out cases like this. If we consider the morpheme
level, however, then this detection is not irrelevant.
These results show that the entropy of branching works as a measure of
context boundaries, not only indicating word boundaries, but also showing the
sentence structure of multiple layers, at the morpheme, word, and phrase levels.
Some other successful segmentation examples in Chinese and Japanese are
shown in Figure 6. These cases were segmented by using Bmax. Examples 1
through 4 are from Chinese, and 5 through 12 are from Japanese, where ?|? indi-
cates the border. As this method requires only a search engine, it can segment
texts that are normally difficult to process by using language tools, such as insti-
tution names (5, 6), colloquial expressions (7 to 10), and even some expressions
taken from Buddhist scripture (11, 12).
6 Performance on a Larger Scale
6.1 Settings
In this section, we show the results of larger-scale segmentation experiments on
Chinese and Japanese. The reason for the choice of languages lies in the fact that
the process utilised here is based on the key assumption regarding the semantic
aspects of language data. As an ideogram already forms a semantic unit as itself,
we intended to observe the performance of the procedure with respect to both
ideograms and phonograms. As Chinese contains ideograms only, while Japanese
contains both ideograms and phonograms, we chose these two languages.
Because we need correct boundaries with which to compare our results, we
utilised manually segmented corpora: the People?s Daily corpus from Beijing
University [7] for Chinese, and the Kyoto University Corpus [1] for Japanese.
In the previous section, we calculated h for almost all substrings of a given
string. This requires O(n2) searches of strings, with n being the length of the
given string. Additionally, the process requires a heavy access load to the web
search engine. As our interest is in verifying assumption (B), we conducted our
experiment using the following algorithm for a given string x.
1. Set m = 0, n=1.
2. Calculate h for xm,n
3. If the entropy is unmeasurable, set m = m + 1,n = m + 2, and go to step 2.
4. Compare the result with that for xm,n?1.
5. If the value of h fulfils the boundary conditions, then output n as the bound-
ary. Set m = m + 1, n = m + 2, and go to 2.
6. Otherwise, set n = n + 1 and go to 2.
The point of the algorithm is to ensure that the string length is not increased once
the boundary is found, or if the entropy becomes unmeasurable. This algorithm
becomes O(n2) in the worst case where no boundary is found and all substrings
are measurable, although this is very unlikely to be the case. Note that this
Entropy as an Indicator of Context Boundaries 103
Fig. 7. Precision and recall of word segmentation using the branching entropy in Chi-
nese and Japanese
algorithm defines the regular order case, but we also conducted experiments in
reverse order, too.
As for the boundary condition, we utilized Bincrease, as it includes Bmax. A
threshold val could be set to the margin of difference:
h(xn+1) ? h(xn) > val. (6)
The larger val is, the higher the precision, and the lower the recall. We varied
val in the experiment in order to obtain the precision and recall curve.
As the process is slow and heavy, the experiment could not be run through
millions of words. Therefore, we took out portions of the corpora used for each
language, which consisted of around 2000 words (Chinese 2039, Japanese 2254).
These corpora were first segmented into phrases at commas, and each phrase
was fed into the procedure described above. The suggested boundaries were
then compared with the original, correct boundaries.
6.2 Results
The results are shown in Figure 7. The horizontal axis and vertical axes represent
the precision and recall, respectively. The figure contains two lines, corresponding
to the results for Japanese or Chinese. Each line is plotted by varying val from
0.0 to 3.0 with a margin of 0.5, where the leftmost points of the lines are the
results obtained for val=0.0.
The precision was more than 90% for Chinese with val > 2.5. In the case
of Japanese, the precision deteriorated by about 10%. Even without a threshold
(val = 0.0), however, the method maintained good precision in both languages.
The locations indicated incorrectly were inside phonogram sequences consist-
ing of long foreign terms, and in inflections in the endings of verbs and adjectives.
In fact, among the incorrect points, many could be detected as correct segmenta-
tions. For example, in Chinese, surnames were separated from first names by our
104 K. Tanaka-Ishii
procedure, whereas in the original corpus, complete names are regarded as single
words. As another example in Chinese, the character is used to indicate
?-ist? in English, as in (revolutionist) and our process suggested that
there is a border in between However, in the original corpus,
these words are not segmented before but are instead treated as one word.
Unlike the precision, the recall ranged significantly according to the thresh-
old. When val was high, the recall became small, and the texts were segmented
into larger phrasal portions. Some successful examples in Japanese for val=3.0
are shown in the following.
The segments show the global structure of the phrases, and thus, this result
demonstrates the potential validity of assumption (B). In fact, such sentence
segmentation into phrases would be better performed in a word-based manner,
rather than a character-based manner, because our character-based experiment
mixes the word-level and character-level aspects at the same time. Some previous
works on collocation extraction have tried boundary detection using branching
[5]. Boundary detection by branching outputs tightly coupled words that can be
quite different from traditional grammatical phrases. Verification of such aspects
remains as part of our future work.
Overall, in these experiments, we could obtain a glimpse of language structure
based on assumption (B) where semantic units of different levels (morpheme,
word, phrase) overlaid one another, as if to form a fractal of the context. The
entropy of branching is interesting in that it has the potential to detect all
boundaries of different layers within the same framework.
7 Conclusion
We conducted a fundamental analysis to verify that the uncertainty of tokens
coming after a sequence can serve to determine whether a position is at a con-
text boundary. By inferring this feature of language from the well-known fact
that the entropy of successive tokens decreases when a longer context is taken,
we examined how boundaries could be detected by monitoring the entropy of
successive tokens. Then, we conducted two experiments, a small one in Japanese,
and a larger-scale experiment in both Chinese and Japanese, to actually segment
words by using only the entropy value. Statistical measures were obtained using
a web search engine in order to overcome data sparseness.
Through analysis of Japanese examples, we found that the method worked
better for sequences of ideograms, rather than for phonograms. Also, we ob-
served that semantic layers of different levels (morpheme, word, phrase) could
potentially be detected by monitoring the entropy of branching. In our larger-
scale experiment, points of increasing entropy correlated well with word borders
and
{ are jbig j problems j suchas power
decentralizaion.)
{ (We think that j it is not the time
for
breakup).
?
???
?? ?
?
?????? ??? ?????
??????????? ??????
(There
Entropy as an Indicator of Context Boundaries 105
References
1. Kyoto University Text Corpus Version 3.0, 2003. http://www.kc.t.u-tokyo.ac.jp/nl-
resource/corpus.html.
2. R.K. Ando and L. Lee. Mostly-unsupervised statistical segmentation of japanese:
Applications to kanji. In ANLP-NAACL, 2000.
3. T.C. Bell, J.G. Cleary, and I. H. Witten. Text Compression. Prentice Hall, 1990.
4. M. Creutz and Lagus K. Unsupervised discovery of morphemes. In Workshop of
the ACL Special Interest Group in Computational Phonology, pages 21?30, 2002.
5. T.K. Frantzi and S. Ananiadou. Extracting nested collocations. 16th COLING,
pages 41?46, 1996.
6. S.Z. Harris. From phoneme to morpheme. Language, pages 190?222, 1955.
7. ICL. People daily corpus, beijing university, 1999. Institute of Computational
Linguistics, Beijing University http://162.105.203.93/Introduction/ corpustag-
ging.htm.
8. A. Kempe. Experiments in unsupervised entropy-based corpus segmentation. In
Workshop of EACL in Computational Natural Language Learning, pages 7?13,
1999.
9. H. Nakagawa and T. Mori. A simple but powerful automatic termextraction
method. In Computerm2: 2nd International Workshop on Computational Termi-
nology, pages 29?35, 2002.
10. S. Nobesawa, J. Tsutsumi, D.S. Jang, T. Sano, K. Sato, and M Nakanishi. Seg-
menting sentences into linky strings using d-bigram statistics. In COLING, pages
586?591, 1998.
11. J.R. Saffran. Words in a sea of sounds: The output of statistical learning. Cognition,
81:149?169, 2001.
12. M. Sun, Dayang S., and B. K. Tsou. Chinese word segmentation without using
lexicon and hand-crafted training data. In COLING-ACL, 1998.
especially in the case of Chinese. These results reveal an interesting aspect of
the statistical structure of language.
Multilingual Text Entry using Automatic Language Detection
Yo Ehara and Kumiko Tanaka-Ishii
Graduate School of Information Science and Technology, University of Tokyo
13F Akihabara Daibiru, 1-18-13 SotoKanda Chiyoda-ku, Tokyo, Japan
ehara@r.dl.itc.u-tokyo.ac.jp kumiko@i.u-tokyo.ac.jp
Abstract
Computer users increasingly need to pro-
duce text written in multiple languages.
However, typical computer interfaces re-
quire the user to change the text entry soft-
ware each time a different language is used.
This is cumbersome, especially when lan-
guage changes are frequent.
To solve this problem, we propose TypeAny,
a novel front-end interface that detects the
language of the user?s key entry and au-
tomatically dispatches the input to the ap-
propriate text entry system. Unlike previ-
ously reported methods, TypeAny can han-
dle more than two languages, and can easily
support any new language even if the avail-
able corpus is small.
When evaluating this method, we obtained
language detection accuracy of 96.7% when
an appropriate language had to be chosen
from among three languages. The number of
control actions needed to switch languages
was decreased over 93% when using Ty-
peAny rather than a conventional method.
1 Introduction
Globalization has increased the need to produce
multilingual text ? i.e., text written in more than
one language ? for many users. When producing
a text in a language other than English, a user has
to use text entry software corresponding to the other
language which will transform the user?s key stroke
sequences into text of the desired language. Such
software is usually called an input method engine
(IME) and is available for each widely used lan-
guage. When producing a multilingual text on a
typical computer interface, though, the user has to
switch IMEs every time the language changes in a
multilingual text. The control actions to choose an
appropriate IME are cumbersome, especially when
the language changes frequently within the text.
To solve this problem, we propose a front-end in-
terface called TypeAny. This interface detects the
language that the user is using to enter text and dy-
namically switches IMEs. Our system is situated
between the user key entry and various IMEs. Ty-
peAny largely frees the user from the need to exe-
cute control actions when switching languages.
The production of multilingual text involves three
kinds of key entry action:
? actions to enter text
? actions to control an IME1
? actions to switch IMEs
Regarding the first and second types, substantial
work has been done in the UI and NLP domain,
as summarized in (MacKenzie and Tanaka-Ishii,
2007). There has especially been much work re-
garding Chinese and Japanese because in these lan-
guages the number of actions of the second type is
closely related to the accuracy of conversion from
Romanized transcription to characters in each of
these languages, and this directly reflects the capa-
bility of the language model used.
1When using predictive methods such as completion, or
kana-kanji conversion in Japanese, the user has to indicate to
the IME when it should predict and which proposed candidate
to choose.
441
In contrast, this paper addresses the question of
how to decrease the need for the third type of action.
From the text entry viewpoint, this question has re-
ceived much less attention than the need to reduce
the number of actions of the second type. As far as
we know, this issue has only been directly addressed
by Chen et al (2000), who proposed integrating En-
glish entry into a Chinese input system rather than
implementing multilingual input.
Reports on ways to detect a change in the lan-
guage used are more abundant. (Murthy and Ku-
mar, 2006) studied the language identification prob-
lem based on small samples in several Indian lan-
guages when machine learning techniques are used.
Although they report a high accuracy for the method
they developed, their system handles switches be-
tween two Indian languages only. In contrast, Ty-
peAny can handle any number of languages mixed
within a text.
(Alex, 2005) addresses a related task, called for-
eign inclusion detection (FID). The task is to find
foreign (i.e., English) inclusions, such as foreign
noun compounds, within monolingual (i.e., Ger-
man) texts. Alex reported that the use of FID to
build a polyglot TTS synthesizer was also consid-
ered (Pfister and Romsdorfer, 2003), (Marcadet et
al., 2005). Recently, Alex used FID to improve pars-
ing accuracy (Alex et al, 2007). While FID relies
on large corpora and lexicons, our model requires
only small corpora since it incorporates the transi-
tion probabilities of language switching. Also, while
FID is specific to alphabetic languages, we made our
method language-independent by taking into consid-
eration the inclusion problem at the key entry level.
In the following, we introduce the design of Ty-
peAny, explain its underlying model, and report on
our evaluation of its effectiveness.
2 Design of TypeAny
Figure 1 shows an example of text written in En-
glish, Japanese and Russian. The strings shown be-
tween the lines indicate the Roman transcription of
Japanese and Russian words.
With a conventional computer interface, entering
the text shown in Figure 1 would require at least six
control actions since there are six switches between
languages: from English to Japanese and back, and
Figure 1: Example of Multilingual Text in English,
Japanese and Russian
Figure 2: System Structure
twice from English to Russian and back. Note that
such IME switches are also required even when the
text consists only of European languages. Each Eu-
ropean language has its own font and diacritic sys-
tem, which are realized by using IMEs.
TypeAny solves the problem of changing IME. It
is situated between the user?s key entry and various
IMEs as shown in the system architecture diagram
of Figure 2. The user?s key entry sequence is input
to our client software. The client sends the sequence
to the server which has the language identifier mod-
ule. This module detects the language of the key
sequence and then sends the key sequence to the ap-
propriate IME2. The selected IME then converts the
key entries into text of the detected language.
In our study, IMEs for European languages are
built using simple transliteration: e.g., ?[? typed in
an English keyboard is transliterated into ?u?? of Ger-
man. In contrast, the IMEs for Japanese and Chinese
2Precisely speaking, TypeAny detects keyboard layouts (i.e.,
Qwerty, Dvorak, Azerty, etc.) as well as the languages used.
442
Figure 3: Entry Flow
require a more complicated system because in these
languages there are several candidate transcriptions
for a key sequence. Fortunately, several existing
software resources can be used for this. We use An-
thy3 as the IME for Japanese. As for the IME for
Chinese, we used a simple word-based pinyin-hanzi
conversion system.
TypeAny restarts the language detection every
time a certain delimiter appears in the user?s key se-
quence. By using delimiters, the system can avoid
resorting to a combinatorial search to find the bor-
der between languages. Such delimiters naturally
occur in natural language texts. For example, in
the case of European languages, blank spaces are
used to delimit words and it is unlikely that two lan-
guages will be mixed within one word. In languages
such as Chinese and Japanese, blank spaces are typ-
ically used to indicate that the entry software should
perform conversion, thus guaranteeing that the se-
quence between two delimiters will consist of only
one language4. Therefore, assuming that a text frag-
ment between two delimiters is written in just one
language is natural for users. A text fragment be-
tween two delimiters is called a token in TypeAny.
An example of the TypeAny procedure to enter
3http://anthy.sourceforge.jp/
4Note that a token can consist of a sequence longer than a
word, since many types of conversion software allow the con-
version of multiple words at one time.
the text from Figure 15 is shown in Figure 3. In
each step in Figure 3, the text is entered in the first
line, where the token that the user is entering is high-
lighted. The language estimated from the token is
shown in the locale window shown below (called the
Status Window). Each step proceeds as follows.
(a) The initial state.
(b) The user first wants to type a token ?Some? in
English. When ?Som? is typed, the system
identifies that the entry is in English. The user
confirms this language by looking at the locale
window.
(c) The user finishes entering the token ?Some? and
when the user enters a blank space, the token
?Some? is confirmed as English text. TypeAny
restarts detection for the language of the next
token. The tokens up to and including ?offer?
are entered similarly to ?Some?.
(d) The user types in a token ?ikura? in Japanese.
The moment ?iku? is typed, ?iku? is identified
as Japanese, as is confirmed by the user through
the locale window.
(e) When the user finishes entering the token
?ikura? and types in a blank space, the se-
quence is sent to a Japanese IME to be con-
verted into ?ikura?, so that a Japanese text frag-
ment is obtained.
(f) Through conventional kana-kanji conversion,
5This case assumes use of the Qwerty keyboard.
443
Figure 4: When Detection Fails
the user can select the appropriate conversion
of ?ikura? among from candidates shown in the
Lookup Window and the token is confirmed.
TypeAny begins detecting the language of the
next token. The tokens between ?or? and ?Rus-
sian? are successfully identified as English in a
way similar to procedures (b) and (c).
(g) The key entry ?brhf? is the key sequence for the
Russian token whose English transliteration is
?ikra?.
(h) Since ?brhf? is identified as Russian, ?brhf? is
converted into Russian characters.
(i) The following word ?Caviar? is detected as En-
glish, as in (b) and (c).
As seen in this example, the user does not need to
take any action to switch IMEs to enter tokens of
different languages.
Two types of detection failure occur in TypeAny:
Failure A: the language should switch, but the new
language is incorrectly selected.
Failure B: the language should not switch, but Ty-
peAny misjudges that it should.
While conventional methods require a control ac-
tion every time the language switches, TypeAny re-
quires a control action only to correct such a failure.
Therefore, Failure A never increases the number
of control actions compared to that of conventional
methods. On the other hand, Failure B errors are
a concern as such failures might increase the num-
ber of control actions to beyond the number required
by a conventional method. Thus, the effectiveness
of introducing TypeAny depends on a trade-off be-
tween fewer control actions at language switching
points and potentially more control actions due to
Failure B errors. Our evaluation in ?4.2 shows that
the increase in the number of actions due to Failure
B errors is insignificant.
In the event of failures, the user can see that there
is a problem by watching the locale window and
then easily correct the language by pressing the TAB
key. For example, while ?in? was correctly judged
for our example, suppose it is incorrectly detected as
Japanese as shown in Figure 4(a). In this case, the
user can manually correct the locale by pressing the
TAB key once. The locale is then changed from Fig-
ure 4(a) (where ?in? is identified as Japanese), to (b)
where ?in? is identified as English.
Note that the language of some tokens will be am-
biguous. For example, the word ?sushi? can be both
English and Japanese because ?sushi? has almost be-
come an English word: many loan words share this
ambiguity. Another case is when diacritic marks are
considered: for example, the word ?fur? is usually
an English word, but some German users may wish
to use this word as ?fu?r? without diacritic marks.
Such a habit is widely seen among users of Euro-
pean languages. Some of this sort of ambiguity is
disambiguated by considering the context and by on-
line learning, which is incorporated in the detection
model as explained next.
3 Language Detection
3.1 Language Detection Model
We modeled the language detection as a hidden
Markov model (HMM) process whose states corre-
spond to languages and whose outputs correspond to
tokens from a language.
Here, the goal is to estimate the languages l?m1 by
maximizing P(lm1 , tm1 ), where l ? L denotes a lan-
guage in L, a set of languages, and t denotes a to-
ken6. By applying a hidden Markov model, the max-
imization of P(lm1 , tm1 ) is done as shown in Equa-
tion (1).
l?m1 = argmaxlm1 ?L
P(lm1 , tm1 )
= argmax
lm1 ?L
P(tm1 |lm1 )P(lm1 )
? argmax
lm1 ?L
( m?
i=1
P (ti|li)
)( m?
i=1
P (li|li?1i?k)
)
(1)
In the last transformation of Equation (1), it
is assumed that P(tm1 |lm1 ) ?
?m
i=1 P (ti|li) and
P(li|li?11 ) ? P (li|li?1i?k) for the first and the second
terms, respectively. In Equation (1), the first term
6Let tvu = (tu, tu+1, . . . , tv) be an ordered list consisting of
v ? u+ 1 elements for v ? u.
444
corresponds to the output probabilities and the sec-
ond term corresponds to the transition probabilities.
In a usual HMM process, a system finds the lan-
guage sequence (i.e., state sequence) lm1 that maxi-
mizes Equation (1) by typically using a Viterbi algo-
rithm. In our case, too, the system can estimate the
language sequence for a sequence of tokens. How-
ever, as discussed earlier, since it is unlikely that a
user enters a token consisting of multiple languages,
our system is designed only to estimate the language
of the latest token lm, supposing that the languages
of the previous lm?11 are correct.
In the following two sections, the estimation of
each term is explained.
3.2 Output Probabilities
The output probabilities P (ti|li) indicate the proba-
bilities of tokens in a monolingual corpus, and their
modeling has been substantially investigated in NLP.
Note that the estimation of P (ti|li) requires
monolingual corpora. If the corpora are large,
P (ti|li) is estimated from the token frequencies.
However, because large corpora are not always
available, especially for minor languages, P (ti|li)
is estimated using key entry sequence probabilities
based on n-grams (with maximum n being nmax) as
follows:
P (ti|li) = P (c|ti|1 |li) =
|ti|?
r=1
P (cr|cr?1r?nmax+1, li)(2)
In Equation (2), ti = c|ti|1 and |ti| is the length of ti
with respect to the key entry sequence. For exam-
ple, in the case of ti=?ikura?, |ti| = 5 and c1=?i?,
c2=?k?, c3=?u? and |ti|=5. Here, each probability
P (cr|cr?1r?nmax+1, li) needs to be smoothed.
Values of P (ti|li) are estimated from monolin-
gual corpora. If the corpora are large, P (ti|li) is
estimated from the token frequencies. However, be-
cause large corpora are not always available, espe-
cially for minor languages, P (ti|li) is estimated us-
ing smoothed character-based n-grams. Prediction
by Partial Matching, or PPM is adopted for this task,
since it naturally incorporates online learning and it
is effective in various NLP tasks as reported in (Tea-
han et al, 2000) and (Tanaka-Ishii, 2006). PPM
uses cr?nmax1 as a corpus for training. PPM is de-
signed to predict the next cr by estimating the nmax-
gram probability P (cr|cr?1r?nmax+1) using backing-
off techniques with regard to the current context.
Precisely, the probability is estimated as a weighted
sum of different (n + 1)-gram probabilities up to a
fixed nmax-gram as follows:
P (cr|cr?1r?nmax+1) =
nmax?1?
n=?1
wnpn(cr) (3)
The weights wn are determined through escape
probabilities. Depending on how the escape prob-
abilities are calculated, there are several PPM vari-
ants, which are named PPMA, PPMB, PPMC, and
so on. PPMC, the one that we have used, is also
known as Witten-Bell smoothing in the NLP field
(Manning and Schuetze, 1999). The escape proba-
bilities are defined as follows.
wn = (1? en)
ncont?
n?=n+1
en? (?1 ? n < ncont)(4)
wncont = (1? en)
Here, ncont is defined as the maximum n that satis-
fies Xn 6= 0. Let Xn be the number of cr?1r?n, xn be
the number of crr?n and qn be the number of differ-
ent keycodes followed by cr?1r?n found in cr?n?11 .
Using these notations, pn(cr) is defined as
pn(cr) = xnXn (5)
In PPMC, the escape probabilities are calculated as
en = qnXn + qn (6)
For further details, see (Bell et al, 1990).
3.3 Language Transition Probabilities
Only a small corpus is typically available to esti-
mate P (lm|lm?1m?kmax+1), where kmax is the longestk-gram in the language sequence to be considered.
Thus, the transition probability is estimated on-line,
making use of language that will be corrected inter-
actively by the user. For this on-line learning, we
adopted PPM as well as the output probabilities.
Note that a large kmax may reduce accuracy,
which is intuitively explained as follows. While
there is typically a high probability that the subse-
quent language will be the same as the current lan-
guage, it is unlikely that any language sequence will
have long regular patterns. Therefore, kmax should
be fixed according to this consideration.
445
 0
 20
 40
 60
 80
 100
 2  3  4  5  6  7  8
A
c
c
u
ra
c
y
 (%
)
Number of languages
PPM
ML
baseline
Figure 5: Detection Accuracy Test1
 75
 80
 85
 90
 95
 100
 2  3  4  5  6  7  8
A
c
c
u
ra
c
y
 (%
)
Number of languages
PPM
ML
baseline
Figure 6: Detection Accuracy Test2
4 Evaluation
We evaluated TypeAny with respect to two mea-
sures: language detection capability when using arti-
ficially generated multilingual corpora, and the num-
ber of required control actions when using actual
multilingual corpora.
4.1 Language Detection Accuracy
The ideal experiment would be to use actual multi-
lingual corpora for many language sets. However, it
is still difficult to collect a large amount of multilin-
gual corpora with adequate quality for the test data
of languages.
Therefore, we measured the language detection
accuracies using artificially generated multilingual
corpora by mixing monolingual corpora for every
combination varying from two to eight languages.
First, the following monolingual corpora were
collected: editions of the Mainichi newspaper in
2004 for Japanese, the Peking University corpus
for Chinese, and the Leipzig corpora (Biemann et
al., 2007) for English, French, German, Estonian,
Finnish and Turkish. The text of each of these cor-
pora was transformed into a sequence of key entries.
Two test sets, Test1 and Test2, were generated by
using different mixture rates. In Test1, languages
were mixed uniformly and randomly, whereas in
Test2 a major language accounted for 90% of the
text and the remaining 10% included different lan-
guages chosen uniformly and randomly. Test2 is
more realistic since a document is usually composed
in one major language.
The output and language transition probabilities
were estimated and smoothed using PPMC as de-
scribed in ?3. Since part of the target of the exper-
iment was to clarify the relation between learning
size and accuracy, the output probabilities and tran-
sition probabilities were not trained on-line while
the text was entered using PPMC, thus accuracy was
measured by fixing the language model at this initial
state. We used nmax = 5 for the output probability
and kmax = 1 for the transition probability since the
distribution of languages in the corpus was uniform
here as we generated it uniformly. (See formula (4)
in ?3.2).
A 10-fold cross validation was applied to the gen-
erated corpora. Each generated corpus was 111
Kbytes in size, consisting of a disjoint 100-Kbyte
training part and an 11-Kbyte testing part. The out-
put probabilities were trained using the 100-Kbyte
training part. The language transition probabilities
were trained using about 2000 tokens.
The results for Test1 and Test2 are shown in Fig-
ure 5 and Figure 6, respectively. The horizontal
axis shows the number of languages and the ver-
tical axis shows the detection accuracy. There are
three lines: PPM indicates that the transition proba-
bilities were trained by PPM; ML indicates that no
transition probability was used and the language was
detected using only output probabilities (maximum
likelihood only); Baseline is the accuracy when the
most frequent language is always selected.
446
As shown in Figure 5 (Test1), when the mix-
ture was uniform, the PPM performance was slightly
lower but very close to that of ML. This was because
PPM would be theoretically equivalent to ML with
infinite learning of language transition probabilities,
since languages were uniformly distributed in Test1.
These results show that our PPM for transition prob-
abilities learns this uniformity in Test1.
As shown in Figure 6, PPM clearly outperformed
ML in Test2. This was because ML has no way
to learn the transition probability, which was biased
with the major language being used 90% of the time.
This shows that the introduction of language transi-
tion probabilities accounts for higher performance.
Interestingly, ML falls below the baseline case when
more than three languages were used in Test2, a sit-
uation that has rarely been considered in previous
studies. This suggests that language detection using
only ML requires large corpora for learning to select
one appropriate language, and that this requirement
can be alleviated by using PPM.
Another finding is that the detection accuracy de-
pends on the language set. For example, the accu-
racy for language sets consisting of both French and
English tended to be lower than for other language
sets due to the spelling closeness between these two
languages. For example, the accuracy for test data
consisting of 90% English, 5% French and 5% Ger-
man was 94.4%. This is not surprising since the de-
tection was made only within a token (which cor-
responds to a word in European languages): natu-
rally there were many words whose language was
ambiguous within the test set. In contrast, high ac-
curacies were obtained for test sets consisting of lan-
guages more different in their nature. We obtained
97.5% accuracy for test data consisting of 90% En-
glish, 5% Finnish and 5% Turkish; this accuracy was
higher than the average for all test sets.
4.2 Number of Control Actions
The second evaluation was done to compare the
number of control actions needed to switch lan-
guages with TypeAny and with a conventional
method. As mentioned in ?1, three types of key-
board actions are used when entering text. Our work
7E.: English, J.: Japanese and C.: Chinese.
8http://en.wikitravel.org/
9http://en.wikipedia.org/
Table 1: Articles Used in the Decrease Test
article Article 1 Article 2
Foreign tokens 286 55
Total tokens 1725 5100
Inclusion ratio 16.6% 1.1%
languages E., J. E., J., C.7
content Introduction
of Japanese
phrases for
traveling
About tofu
(bean curd)
Source Wikitravel 8 Wikipedia 9
Table 2: Required Number of Control Actions
Article 1 Article 2
Conventional 572 110
Number of switches (100%) (100%)
Ours Failure A 2.8% 3.6%
Failure B 1.6% 2.7%
Total Failures 4.4% 6.3%
Decrease 95.6% 93.6%
only concerns the control action to switch language,
though, and the comparison in this section focuses
on this type of action.
This evaluation was done using two samples of
actual multilingual text collected from the Web. The
features of these samples are shown in the top block
of Table 1. In both cases, the major language was
English.
For each of these articles, the number of con-
trol actions required with the conventional method
and with TypeAny was measured. The conventional
method requires a control action every time the lan-
guage switches. For TypeAny, control actions are re-
quired only to correct language detection failures. In
both cases, the action required to switch languages
or correct the language was counted as one action.
For the language model, the output probabilities
were first trained using the 100-Kbyte monolingual
corpora collected for the previous evaluation. The
transition probabilities were not trained beforehand;
i.e., the system initially regarded the languages to be
uniformly distributed. Since this experiment was in-
tended to simulate a realistic case, both output and
447
transition probabilities were trained on-line using
PPMC while the text was entered. Here, both nmax
and kmax were set at 5.
The results are shown in Table 2. First, some de-
tection errors occurred for Article 2 because ?tofu?
was detected as Japanese at the beginning of entry,
even though it was used as an English word in the
original text. As noted at the end of ?2, such loan
words can cause errors. However, since our system
uses PPM and learns on-line, our system learned that
?tofu? had to be English, and such detection errors
occurred only at the beginning of the text.
Consequently, there was a substantial decrease in
the number of necessary control actions with Ty-
peAny, over 93%, for both articles. An especially
large decrease was observed for Article 2, even
though the text was almost all in English (98.9%).
There was only a small increase in the incidence rate
of Failure B for Article 2, so the total decrease in the
number of required actions was still large, putting
to rest the concern discussed in ?2. These results
demonstrate the effectiveness of our approach.
5 Conclusion
TypeAny is a novel multilingual text input interface
in which the languages used for entries are detected
automatically. We modeled the language detection
as an HMM process whose transition probabilities
are estimated by on-line learning through the PPM
method.
This system achieved language detection accu-
racy of 96.7% in an evaluation where it had to
choose the appropriate language from among three
languages with the major language accounting for
90% of the sample. In addition, the number of con-
trol actions required to switch IMEs was decreased
by over 93%. These results show the promise of our
system and suggest that it will work well under real-
istic circumstances.
An interesting objection might be raised to the
conclusions of this study: some users might find
it difficult to watch the locale window all the time
and prefer the conventional method despite having
to work with a large number of key types. We plan to
examine and clarify the cognitive load of such users
in our future work.
References
B. Alex, A. Dubey, and F. Keller. 2007. Using foreign in-
clusion detection to improve parsing performance. In
Proceedings of EMNLP-CoNLL, Prague, Czech, June.
B. Alex. 2005. An unsupervised system for identifying
English inclusions in German text. In Proceedings of
the ACL Student Research Workshop, pages 133?138,
Ann Arbor, Michigan, June. Association for Compu-
tational Linguistics.
T. C. Bell, J. G. Clear, and I. H. Witten. 1990. Text Com-
pression. Prentice-Hall, New Jersey.
C. Biemann, G. Heyer, U. Quasthoff, and M. Richter.
2007. The Leipzig corpora collection - monolingual
corpora of standard size. In Proceedings of Corpus
Linguistics, Birmingham, United Kingdom, July.
Z. Chen and K. Lee. 2000. A new statistical approach
to Chinese input. In The 38th Annual Meeting of the
Association for Computer Linguistics, pages 241?247,
Hong Kong, October.
I. S. MacKenzie and K. Tanaka-Ishii. 2007. Text Entry
Systems ?Mobility, Accessibility, Universality?. Mor-
gan Kaufmann.
C. D. Manning and H. Schuetze. 1999. Foundations of
Statistical Natural Language Processing. MIT Press.
J.-C. Marcadet, V. Fischer, and C. Waast-Richard. 2005.
A transformation-based learning approach to language
identification for mixed-lingual text-to-speech synthe-
sis. In Interspeech 2005 - ICSLP, pages 2249?2252,
Lisbon, Portugal.
K. N. Murthy and G. B. Kumar. 2006. Language identi-
fication from small text samples. Journal of Quantita-
tive Linguistics, 13:57?80.
B. Pfister and H. Romsdorfer. 2003. Mixed-lingual anal-
ysis for polyglot TTS synthesis. In Eurospeech, pages
2037?2040, Geneva, Switzerland.
K. Tanaka-Ishii. 2006. Word-based text entry techniques
using adaptive language models. Journal of Natural
Language Engineering, 13(1):51?74.
W. J. Teahan, Y. Wen, R. MacNab, and I. H. Witten.
2000. A compression-based algorithm for Chinese
word segmentation. In Computational Linguistics,
volume 26, pages 375?393.
448
Multi-Agent Explanation Strategies in Real-Time Domains
Kumiko Tanaka-Ishii
University of Tokyo,
7-3-1 Hongo Bunkyo-ku
Tokyo 113-8656
Japan
kumiko@ipl.t.u-tokyo.ac.jp
Ian Frank
Electrotechnical Laboratory
1-1-4 Umezono, Tsukuba
Ibaraki 305-0085
Japan
ianf@etl.go.jp
Abstract
We examine the benets of using multi-
ple agents to produce explanations. In
particular, we identify the ability to con-
struct prior plans as a key issue con-
straining the eectiveness of a single-
agent approach. We describe an imple-
mented system that uses multiple agents
to tackle a problem for which prior plan-
ning is particularly impractical: real-
time soccer commentary. Our commen-
tary system demonstrates a number of
the advantages of decomposing an expla-
nation task among several agents. Most
notably, it shows how individual agents
can benet from following dierent dis-
course strategies. Further, it illustrates
that discourse issues such as controlling
interruption, abbreviation, and main-
taining consistency can also be decom-
posed: rather than considering them at
the single level of one linear explana-
tion they can also be tackled separately
within each individual agent. We evalu-
ate our system's output, and show that
it closely compares to the speaking pat-
terns of a human commentary team.
1 Introduction
This paper deals with the issue of high-level vs
low-level explanation strategies. How should an
explanation nd a balance between describing the
overall, high-level properties of the discourse sub-
ject, and the low-level, procedural details? In par-
ticular, we look at the di?culties presented by do-
mains that change in real-time. For such domains,
the balance between reacting to the domain events
as they occur and maintaining the overall, high-
level consistency is critical.
We argue that it is benecial to decompose the
overall explanation task so that it is carried out by
more than one agent. This allows a single agent
to deal with the tracking of the low-level develop-
ments in the domain, leaving the others to con-
centrate on the high-level picture. The task of
each individual agent is simplied, since they only
have to maintain consistency for a single discourse
strategy. Further, discourse issues such as control-
ling interruption, abbreviation, and maintaining
consistency can also be decomposed: rather than
considering them at the single level of one linear
explanation they can be tackled separately within
each individual agent and then also at the level of
inter-agent cooperation.
We look at real-world examples of explanation
tasks that are carried out by multiple agents, and
also give a more detailed protocol analysis of one
of these examples: World Cup soccer commen-
tary by TV announcers. We then describe an
actual implementation of an explanation system
that produces multi-agent commentary in real-
time for a game of simulated soccer. In this sys-
tem, each of the agents selects their discourse con-
tent on the basis of importance scores attached to
events in the domain. The interaction between
the agents is controlled to maximise the impor-
tance score of the uttered comments.
Although our work focuses on real-time do-
mains such as soccer, our discussion in x2 puts
our contribution in a wider context and iden-
ties a number of the general benets of using
multiple agents for explanation tasks. We chose
the game of soccer for our research primarily be-
cause it is a multi-agent game in which various
events happen simultaneously on the eld. Thus,
it is an excellent domain to study real-time con-
tent selection among many heterogeneous facts.
A second reason for choosing soccer is that de-
tailed, high-quality logs of simulated soccer games
are available on a real-time basis from Soccer
Server, the o?cial soccer simulation system for
the `RoboCup' Robotic Soccer World Cup initia-
tive (Kitano et al, 1997).
Ease of making prior plans 
difficult possible easy
  sports
commentary
 mind
games
commentary
   car
navigation
systems
panel
discussion
live
lecture
lecture(TV)  business
presentation
Figure 1: Common explanation tasks categorised according to the ease of planning them in advance
2 Explanation Strategies
In this paper, we use the term explanation in its
broadest possible sense, covering the entire spec-
trum from planned lectures to commentating on
sports events. Any such explanation task is af-
fected by many considerations, including the level
of knowledge assumed of the listeners and the
available explanation time. However, the issue we
mainly concentrate on here has not previously re-
ceived signicant attention: the benets of split-
ting an explanation task between multiple agents.
2.1 Explanations and Multi-Agency
The general task of producing explanations with
multiple agents has not been studied in depth
in the literature. Even for the `naturally' multi-
agent task of soccer commentary, the systems de-
scribed in the recent AI Magazine special issue on
RoboCup (Andre et al, 2000) are all single-agent.
However, one general issue that has been studied
at the level of single agents is the trade-o be-
tween low-level and high-level explanations. For
example, in tutoring systems (Cawsey, 1991) has
described a system that handles real-time interac-
tions with a user by separating the control of the
content planning and dialogue planning.
We believe that the key issue constraining the
use of high-level and low-level explanations in a
discourse is the ability to construct prior plans .
For example, researchers in the eld of discourse
analysis, (e.g., (Sinclair and Coulthard, 1975))
have found that relatively formal types of di-
alogues follow a regular hierarchical structure.
When it is possible to nd these kinds of a priori
plans for a discourse to follow, approaches such as
those cited above for tutoring are very eective.
However, if prior plans are hard to specify, a sin-
gle agent may simply nd it becomes overloaded.
Typically there will be two conicting goals: deal
with and explain each individual (unplanned) do-
main event as it occurs, or build up and explain
a more abstract picture that conveys the overall
nature of the explanation topic.
Thus, for any changing domain in which it is
hard to plan the overall discourse, it can be bene-
cial to divide the explanation task between mul-
tiple agents. Especially for real-time domains, the
primary benet of decomposing the explanation
task in this way is that it allows each agent to
use a dierent discourse strategy to explain dif-
ferent aspects of the domain (typically, high-level
or low-level). However, we can see from Figure 1
that even some activities that are highly planned
are sometimes carried out by multiple agents. For
example, business presentations are often carried
out by a team of people, each of which is an expert
in some particular area. Clearly, there are other
benets that come from decomposing the explana-
tion task between more than one agent. We can
give a partial list of these here:
 Agents may start with dierent abilities. For
example, in a panel session, one panellist may
be an expert on Etruscan vases, while another
may be an expert on Byzantian art.
 It can take time to observe high-level pat-
terns in a domain, and to explain them co-
herently. Having a dedicated agent for com-
menting on the low-level changes increases
the chance that higher-level agents have a
chance to carry out analysis.
 A team of agents can converse together.
In particular, they can make explanations
to each other instead of directly explaining
things to the listeners. This can be a more
comfortable psychological position for the lis-
tener to accept new information.
 The simple label of \expert" adds weight to
the words of a speaker, as shown convincingly
by the research of (Reeves and Nass, 1996).
The use of multiple agents actually gives a
chance to describe individual agents as \ex-
perts" on specic topics.
 Even a single agent speaking in isolation
could describe itself as an expert on various
topics. However, (Reeves and Nass, 1996)
also show that self-praise has far less impact
than the same praise from another source.
Rather than describing themselves as experts,
a multi-agent framework allows agents to de-
scribe the other agents as experts.
To illustrate the dierent roles that can be
taken by multiple agents in an explanation task,
we carried out a simple protocol analysis of an ex-
ample from the far left of our scale of Figure 1:
soccer commentary.
2.2 Soccer Protocol Analysis
We analysed the video of the NHK coverage of the
rst half 1998 World Cup nal. This commentary
was carried out by a team of two people who we
call the `announcer' and the `expert'. The gures
in Table 1 demonstrate that there are clear dier-
ences between the roles assumed by this commen-
tary team. Although both use some background
knowledge to ll out their portions of the commen-
tary, the announcer mostly comments on low-level
events, whilst the expert mostly gives higher-level,
state-based information. Further, we can see that
the announcer asked questions of the expert with
a high frequency.
Overall, there is a clear indication that one
agent follows the low-level events and that the
other follows the high-level nature of the game.
Accordingly, their discourse strategies are also dif-
ferent: the announcer tends to speak in shorter
phrases, whereas the expert produces longer anal-
yses of any given subject. The commentary team
collaborates so that the consistency between high-
level, low-level, and background comments is bal-
anced within the content spoken by each individ-
ual, and also within the overall commentary.
2.3 A First Implementation
As a rst step towards a multi-agent explanation
system based on the above observations, the fol-
lowing sections describe how we implemented a
commentary system for a game of simulated soc-
cer. Our experience with this system reected
the discussion above in that we found it was very
di?cult to consistently manage all the possible
discourse topics within a single-agent framework.
When changing to a multi-agent system, however,
we found that a small number of simple rules for
inter-agent interaction produced a far more man-
ageable system. We also found that the system
was behaviourally very similar to the protocol of
Table 1.
3 An Architecture For Multi-
Agent Soccer Commentary
Figure 2 shows the basic architecture of our soc-
cer commentator system. As we mentioned in the
Introduction, this system is designed to produce
live commentary for games played on RoboCup's
Soccer Server. Since the Soccer Server was orig-
inally designed as a testbed for multi-agent sys-
tems (Noda et al, 1998), we call our commenta-
tor Mike (\Multi-agent Interactions Knowledge-
ably Explained"). Typically, Mike is used to add
atmosphere to games played in the RoboCup tour-
naments, so we assume that the people listening
to Mike can also see the game being described.
AnalyserAnnouncer
Soccer Server
Voronoi
Statistics
Basic
TTS
shared 
memory
commentary
Communicator
Figure 2: Mike | a multi-agent commentator
The Soccer Server provides a real-time game log
of a very high quality, sending information on the
positions of the players and the ball to a moni-
toring program every 100msec. Specically, this
information consists of 1) player location and ori-
entation, 2) ball location, and 3) game score and
play modes (throw ins, goal kicks, etc).
This information is placed in Mike's shared
memory, where it is processed by a number of
`Soccer Analyser' modules that analyse higher-
level features of a game. These features include
statistics on player positions, and also `bigrams' of
ball play chains represented as rst order Markov
chains. The Voronoi analyser uses Voronoi dia-
grams to assess game features such as defensive ar-
eas. Note that we do not consider the Soccer Anal-
ysers to be `agents'; they are simply processes that
manipulate the information in the shared mem-
ory. The only true `agents' in the system are the
Announcer and the Analyser, which communicate
both with each other and with the audience.
All information in Mike's shared memory is
represented in the form of commentary fragments
that we call propositions. Each proposition con-
sists of a tag and some attributes. For example,
a pass from player No.5 to No.11 is represented
as (Pass 5 11), where Pass is the tag, and the
Commentary Feature Announcer Expert Note
Background comment
(e.g., on stadium, or team backgrounds)
7% 20% (predened plan)
Event-based comment 82% 3% (low-level)
State-based comment 11% 77% (high-level)
Average length of comment 1.3sec 3.8sec (consistency)
Asks a question to the other 30 0 (new explanation mode)
Interrupts the other 5 0 (priority of roles)
Announcer describes expert as expert 0 n/a (adds weight to expert)
Table 1: Protocol analysis of announcer and expert utterances in professional TV coverage of soccer
Local Global
E Kick ChangeForm
v Pass
e Dribble SideChange
n ShootPredict
t
S Mark TeamPassSuccessRate
t PlayerPassSuccessRate AveragePassDistance
a ProblematicPlayer Score
t PlayerActive Time
e
Table 2: Examples of Mike's proposition tags
numbers 5 and 11 are the attributes. Mike uses
around 80 dierent tags categorised in two ways:
as being local or global and as being state-based
or event-based. Table 2 shows some examples of
categorised proposition tags.
The operation of the Announcer and the Anal-
yser agents is described in detail in the following
section. Basically, they select propositions from
the shared memory (based on their `importance
scores') and process them with inference rules to
produce higher-level chains of explanations. The
discourse control techniques of interruption, rep-
etition, abbreviation, and silence are used to con-
trol both the dialogue strategies of each individual
agent and also the interaction between them.
To produce variety in the commentary, each
possible proposition is associated with several pos-
sible commentary templates (output can be in En-
glish or Japanese). Figure 3 shows the overall
repertoire of Mike's comments. The actual spo-
ken commentary is realised with o-the-shelf text-
to-speech software (Fujitsu's Japanese Synthesiser
for Japanese, and DecTalk for English).
4 Multi-Agent NL Generation
In this section, we describe how Mike uses impor-
tance scores, real-time inferencing, and discourse
control strategies to implement | and control the
interaction between | agents with diering expla-
 Explanation of complex events. Forma-
tion and position changes, advanced plays.
 Evaluation of team plays. Average forma-
tions, formations at a certain moment, play-
ers' locations, indication of active or prob-
lematic players, winning passwork patterns,
wasteful movements.
 Suggestions for improving play. Loose
defence areas, better locations for inactive
players.
 Predictions. Passes, game results, shots.
 Set pieces. Goal kicks, throw ins, kick os,
corner kicks, free kicks.
 Passwork. Tracking of basic passing play.
Figure 3: Mike's repertoire of statements
nation strategies.
To form a single coherent commentary with
multiple agents we extended the single-agent
framework of (Tanaka-Ishii et al, 1998). The ba-
sic principle of this framework is that given a set
of scores that capture the information transmit-
ted by making any utterance, the most eective
dialogue is the one that maximises the total score
of all the propositions that are verbalised. We
therefore created two agents with dierent strate-
gies for content scheduling. One agent acts as an
announcer, following the low-level events on the
eld. This agent's strategy is biased to allow fre-
quent topic change and although it uses inference
rules to look for connections between propositions
in the shared memory, it only uses short chains of
inference. On the other hand, the second agent
acts as an `expert analyst', and is predominantly
state based. The expert's strategy is biased to
have more consistency, and to apply longer chains
of inference rules than the announcer.
4.1 Importance Scores
In Mike, importance scores are designed to cap-
ture the amount of information that any given
proposition will transmit to an audience. They are
not xed values, but are computed from scratch at
every game step (100msec). The importance score
of each proposition depends on three factors: 1)
the elapsed time since the proposition was gener-
ated, 2) for event-based propositions, a compar-
ison of the place associated with the proposition
and the current location of the ball, and 3) the
frequency that the proposition has already been
stated. To keep the number of comments in the
shared memory to a manageable number they are
simply limited in number, with the oldest entries
being removed as new propositions are added.
4.2 Real Time Inference
Mike's commentary propositions are the results
of large amounts of real-time data processing,
but are typically low-level. A commentary based
solely on these propositions would be rather de-
tailed and disconnected. Thus, to analyse the
play more deeply, Mike gives the commentary
agents access to a set of forward-chaining rules
that describe the possible relationships between
the propositions. In total, there are 145 of these
rules, divided into the two classes of logical con-
sequences and second order relations. We give a
representative example from each class here:
 Logical consequence:
(PassSuccessRate player percentage)
(PassPattern player Goal) !
(active player)
 Second order relation:
(PassSuccessRate player percentage)
(PlayerOnVoronoiLine player) !
(Reason @1 @2)
The basic premise of the announcer's dialogue
strategy is to follow the play by repeatedly choos-
ing the proposition with the highest importance
score. Before stating this proposition, however,
the announcer checks any applicable inference
rules in a top down manner, in an attempt to
produce higher-level commentary fragments and
background related information. In contrast to
this, the expert agent has a library of themes (e.g.,
pass statistics, formation, stamina) between which
it chooses based on the propositions selected by
the announcer so far. It then uses inference rules
to try to construct a series of high-level inferences
related to the theme. The expert applies rules
until it succeeds in constructing a single coherent
piece of structured commentary. When it is the
agent's turn to speak it can then send this com-
mentary to the TTS software.
4.3 Discourse Control Strategies
Consider a passage of commentary where the an-
nouncer is speaking and a proposition with a
much larger importance score than the one be-
ing uttered appears in the shared memory. If this
occurs, the total importance score may become
larger if the announcer immediately interrupts the
current utterance and switches to the new one.
As an example, the left of Figure 4 shows (solid
line) the change of the importance score with time
when an interruption takes place (the dotted line
represents the importance score without interrup-
tion). The left part of the solid line is lower than
the dotted, because we assume that the rst utter-
ance conveys less of its importance score when it
is not completely uttered. However, the right part
of the solid line is higher than the dotted line, be-
cause the importance of the second utterance will
be lower by the time it is uttered without inter-
rupting the commentary. Note that after selecting
a proposition to be uttered, its importance score
is assumed to decrease with time (as indicated in
the gure, the decrease is computed dynamically
and will be dierent for each proposition, and of-
ten not even linear). The decision of whether or
not to interrupt is based on a comparison of the
area between the solid or dotted lines and the hor-
izontal axis.
Similarly, it may happen that when the two
most important propositions in shared memory
Importance Score/Time
Ends the
first utterrance
without interruption
An Important
event occurs
with interruption
without interruption
time
<
>
?
Importance Score/Time
But can utter other
important content
with abbreviation
without abbreviation
time
<
>
?
Becomes less comprehensive
      because of abbreviation
Figure 4: Change of importance score on inter-
ruption and abbreviation
Importance Score/Time
Another utterrance
with repetition
without repetition
time
<
>
?
Repeated utterrance have higher 
score by emphasis
Figure 5: Increase in importance scores caused by
emphasis of repeating a proposition
are of similar importance, the amount of com-
municated information can best be maximised by
quickly uttering the most important proposition
and then moving on to the second before it loses
importance due to some development of the game
situation. This is illustrated in the second graph
of Figure 4. Here, the left hand side of the solid
line is lower than that of the dotted because an
abbreviated utterance (which might not be gram-
matically correct, or whose context might not be
fully given) transmits less information than a more
complete utterance. But since the second propo-
sition can be uttered before losing its importance
score, the right hand part of the solid line is higher
than that of the dotted. As before, the benets or
otherwise of this modication should be decided
by comparing the two areas made by the solid and
the dotted line with the horizontal axis.
We originally designed these techniques just to
improve the ability of the announcer agent to
follow the play. With two commentary agents,
however, both interruption and abbreviation can
be adapted to control inter-agent switching. In
Mike, the default operation is for the announcer
to keep talking while the ball is in the nal third
of the eld, or while there are important proposi-
tions to utter. When the announcer has nothing
to say, the expert agent can speak or both agents
can remain silent. If the expert agent chooses to
speak, it may happen that an important event
on the eld makes the announcer wants to speak
again. We model both interruption and abbre-
viation as multi-agent versions of the graphs of
Figure 4: the agent speaking the rst utterance is
the expert and the agent speaking the second is
the announcer.
We use two further discourse control techniques
in Mike: repetition and silence. Repetition is
depicted in Figure 5. Sometimes it can happen
that the remaining un-uttered propositions in the
shared memory have much smaller scores than
any of those that have already been selected. In
this case, we allow individual agents to repeat
things that they have previously said. Also, we
allow them to repeat things that the other agent
has said, to increase the eectiveness of the dia-
logue between them. Finally, we also model si-
lence by adding bonuses to the importance scores
of the propositions uttered by the commentators.
Specically, we add a bonus to the scores of propo-
sitions uttered directly before a period where both
commentators are silent (the longer that a com-
mentary continues uninterrupted, the higher the
silence bonus). This models the benet of giving
listeners time to actually digest the commentary.
Also, a period of silence contributes a bonus to
the importance scores of the immediately follow-
ing propositions. This models the increased em-
phasis of pausing before an important statement.
4.3.1 Communication Templates
To improve the smoothness of the transfer of the
commentary between the two agents we devised a
small number of simple communication templates.
The phrases contained in these templates are spo-
ken by the agents wheneverMike realises that the
commentary is switching between them. For the
purposes of keeping the two agents distinct, the
expert agent is referred to by the announcer as E-
Mike (\Expert Mike"). To pass the commentary
to the Expert, the Announcer can use a number
of phrases such as \E-Mike, over to you", \Any
impressions, E-Mike?", or just \E-Mike?". The
announcer can also pass over control by simply
stopping speaking. If the commentary switches
from Announcer to Expert with a question, the
Expert will start with \Yes..." or \Well...".
The communication templates for passing the
commentary in the other direction (Expert to An-
nouncer) are shown in Table 3. To help listeners
distinguish the dialogue between the Announcer
and Expert better, we also use a female voice for
one agent and a male voice for the other.
5 Evaluation
Mike is robust enough for us to have used it
to produce live commentary at RoboCup events,
and to be distributed on the Internet (it has been
downloaded by groups in Australia and Hungary
and used for public demonstrations). A short ex-
ample of Mike's output is shown in Figure 6.
To evaluate Mike more rigorously we carried
out two questionnaire-based evaluations, and also
a log comparison with the data produced from the
real-world soccer commentary in x2. For the rst
of the questionnaire evaluations, we used as sub-
Question Scale Results
Is the game better with or without commentary? (5=with, 1=without) 4.97
Was the commentary easy to understand? (5=easy, 1=hard) 3.44
Were the commentary contents accurate? (5=correct, 1=incorrect) 3.25
Was the commentary informative? (5=yes, 1=no) 3.53
Did you get tired of the commentary? (5=no, 1=quickly) 3.97
Table 4: Average responses of 20 subjects to rst questionnaire evaluation of (two-agent) Mike
Question Scale 1-agent 2-agent Di
Is the game better with or without...? (5=with, 1=without) 4.45 4.45 0%
Was the commentary easy to understand? (5=easy, 1=hard) 2.95 3.25 +10%
Were the commentary contents accurate? (5=correct, 1=incorrect) 2.65 2.95 +11%
Was the commentary informative? (5=yes, 1=no) 3.15 3.35 +6%
Did you get tired of the commentary? (5=no, 1=quickly) 2.35 3.35 +43%
Table 5: Dierence in response with ten subjects when viewing 1-agent and 2-agent versions of Mike
Announcer interrupts expert
Sorry, E-MIKE.
Have to stop you there E-MIKE.
Oh!...
But look at this!
Announcer speaks when expert stops
Thanks.
That's very true.
Thanks E-MIKE.
Maybe that will change as the game goes on.
OK...
Table 3: Phrases used by announcer when inter-
rupting the expert, or when speaking after the ex-
pert agent has simply stopped (no interruption)
jects twenty of the attendees of a recent RoboCup
Spring camp. All these subjects were familiar with
the RoboCup domain and the Soccer Server en-
vironment. We showed them an entire half of a
RoboCup game commentated by Mike and col-
lated their responses to the questions shown in
Table 4. These results largely show that the lis-
teners found the commentary to be useful and to
contain enough information to maintain their at-
tention. We also included some open-ended ques-
tions on the questionnaire to elicit suggestions
for features that should be strengthened or incor-
porated in future versions of Mike. The most
frequent responses here were requests for more
background information on previous games played
by the teams (possible in RoboCup, but to date
we have only done this thoroughly for individ-
Announcer: yellow 9,in the middle of the
eld,yellow team (a set play happened here). Any
impressions, E-Mike?
Analyser: Well, here are statistics concerning
possessions, left team has slightly smaller value of
possession, it is 43 percent versus 56. right team
has rather higher value of territorial advantage,
Overall, right team is ahead there. (Score is cur-
rently 0-0. E-Mikejudges that red team is doing
better).
Announcer: Really. dribble , yellow 3, on the
left, great long pass made to yellow 1, for red 6,
red 2's pass success rate is 100 percent. E-Mike?
Analyser: Looking at the dribbles and steals, red
team was a little less successful in dribbling, red
team has a lower value of dribble average length,
left is 21 meters whereas right is 11, right team
has a few less players making zero passes, yellow
team has made slightly less stealing,
Announcer: wow (interruption because red 11
made a shot), red 11, goal, red 11, Goal ! It was
red 10, And a pass for red 11 ! The score is 0 1!
Figure 6: Example of Mike's commentary from
RoboCup'98 nal
ual games), more conversation between the agents
(we plan to improve this with more communica-
tion templates), and more emotion in the voices of
the commentators (we have not yet tackled such
surface-level NLG issues). We also asked what the
ideal number of commentators for a game would
be; almost all subjects replied 2, with just two
replying 3 and one replying 1.
The above results are encouraging for Mike,
but to show that the use of multiple agents was
actually one of the reasons for the favourable audi-
ence impression, we carried out a further test. We
Commentary feature Announcer Expert Note
Background comment 16% 22% (predened plan)
Event-based comment 64% 0% (low-level)
State-based comment 20% 78% (high-level)
Average length of comment 1.1sec 2.9sec (consistency)
Asks a question to the other 12.2 0 (new explanation mode)
Interrupts the other 8.6 0 (priority of roles)
Announcer describes expert as expert 0 n/a (adds weight to expert)
Table 6: Breakdown of Mike's agent utterances over ten randomly selected RoboCup half-games
created a single-agent version of Mike by switch-
ing o the male/female voices in the TTS soft-
ware and disabling the communication templates.
This single-agent commentator comments on al-
most exactly the same game content as the multi-
agent version, but with a single voice. We re-
cruited ten volunteers with no prior knowledge of
RoboCup and showed them both the single-agent
and multi-agent versions of Mike commentating
the same game as used in the previous experiment.
We split the subjects into two groups so that one
group watched the multi-agent version rst, and
the other watched the single-agent version rst.
Table 5 shows that the average questionnaire re-
sponses over the two groups were lower than with
the subjects who were familiar with RoboCup, but
that the multi-agent version was more highly eval-
uated than the single-agent version. Thus, even
the supercially small modication of removing
the agent dialogue has a measurable eect on the
commentary.
Finally, we analysed Mike's commentary us-
ing the same criteria as our protocol analysis of
human soccer commentary in x2.2. We selected
ten half-games at random from the 1998 RoboCup
and compiled statistics on Mike's output with an
automatic script. The results of this analysis (Ta-
ble 6) show a marked similarity to those of the
human commentators. This initial result is a very
encouraging sign for further work in this area.
6 Conclusions
We have argued for superiority of producing
explanations with multiple, rather than single,
agents. In particular, we identied the di?culty of
producing prior plans as the key issue constrain-
ing the ability of a single agent to switch between
high-level and low-level discourse strategies.
As a rst step towards a multi-agent explana-
tion system with solid theoretical underpinnings,
we described the explanation strategies used by
our live soccer commentary system, Mike. We
showed how a set of importance scores and infer-
ence rules can be used as the basis for agents with
dierent discourse strategies, and how the dis-
course control techniques of interruption, abbrevi-
ation, repetition and silence can be used not just
to moderate the discourse of an individual agent,
but also the interaction between agents. We eval-
uated Mike's output through listener surveys,
showing that it represents an advance over exist-
ing commentary programs, which are all single-
agent. We also found that the discourse strategies
of Mike's agents closely resembled those revealed
by the protocol analysis of a team of real-life soc-
cer commentators.
References
E. Andre, K. Binsted, K. Tanaka-Ishii, S. Luke,
G. Herzog, and T. Rist. 2000. Three RoboCup
simulation league commentator systems. AI
Magazine, 21(1):57{66, Spring.
A.J. Cawsey. 1991. Generating intreractive expla-
nations. In Proceedings of the Ninth National
Conference on Articial Intelligence (AAAI-
91), pages 86{91.
H. Kitano, M. Asada, Y. Kuniyoshi, I. Noda,
E. Osawa, and H. Matsubara. 1997. RoboCup:
A challenge problem for AI. AI Magazine,
pages 73{85, Spring.
I. Noda, H. Matsubara, K. Hiraki, and I. Frank.
1998. Soccer Server: a tool for research on
multi-agent systems. Applied Articial Intelli-
gence, 12(2{3):233{251.
B. Reeves and C. Nass. 1996. The Media Equa-
tion. CSLI Publications.
J. Sinclair and R. Coulthard. 1975. Towards
an Analysis of Discourse: The English Used by
Teachers and Pupils. Oxford University Press.
K. Tanaka-Ishii, K. Hasida, and I. Noda. 1998.
Reactive content selection in the generation of
real-time soccer commentary. In Proceedings of
COLING-ACL'98, pages 1282{1288, Montreal.
 
	 	 
 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 428?435,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Unsupervised Segmentation of Chinese Text
by Use of Branching Entropy
Zhihui Jin and Kumiko Tanaka-Ishii
Graduate School of Information Science and Technology
University of Tokyo
Abstract
We propose an unsupervised segmen-
tation method based on an assumption
about language data: that the increas-
ing point of entropy of successive char-
acters is the location of a word bound-
ary. A large-scale experiment was con-
ducted by using 200 MB of unseg-
mented training data and 1 MB of test
data, and precision of 90% was attained
with recall being around 80%. More-
over, we found that the precision was
stable at around 90% independently of
the learning data size.
1 Introduction
The theme of this paper is the following as-
sumption:
The uncertainty of tokens coming
after a sequence helps determine
whether a given position is at a
boundary. (A)
Intuitively, as illustrated in Figure 1, the vari-
ety of successive tokens at each character in-
side a word monotonically decreases according
to the oset length, because the longer the pre-
ceding character n-gram, the longer the pre-
ceding context and the more it restricts the
appearance of possible next tokens. For ex-
ample, it is easier to guess which character
comes after \natura" than after \na". On the
other hand, the uncertainty at the position of
a word border becomes greater, and the com-
plexity increases, as the position is out of con-
text. With the same example, it is dicult to
guess which character comes after \natural ".
This suggests that a word border can be de-
tected by focusing on the dierentials of the
uncertainty of branching.
In this paper, we report our study on ap-
plying this assumption to Chinese word seg-
Figure 1: Intuitive illustration of a variety of
successive tokens and a word boundary
mentation by formalizing the uncertainty of
successive tokens via the branching entropy
(which we mathematically dene in the next
section). Our intention in this paper is above
all to study the fundamental and scientic sta-
tistical property underlying language data, so
that it can be applied to language engineering.
The above assumption (A) dates back to
the fundamental work done by Harris (Harris,
1955), where he says that when the number
of dierent tokens coming after every prex of
a word marks the maximum value, then the
location corresponds to the morpheme bound-
ary. Recently, with the increasing availabil-
ity of corpora, this property underlying lan-
guage has been tested through segmentation
into words and morphemes. Kempe (Kempe,
1999) reports a preliminary experiment to de-
tect word borders in German and English texts
by monitoring the entropy of successive char-
acters for 4-grams. Also, the second author
of this paper (Tanaka-Ishii, 2005) have shown
how Japanese and Chinese can be segmented
into words by formalizing the uncertainty with
the branching entropy. Even though the test
data was limited to a small amount in this
work, the report suggested how assumption
428
(A) holds better when each of the sequence el-
ements forms a semantic unit. This motivated
our work to conduct a further, larger-scale test
in the Chinese language, which is the only hu-
man language consisting entirely of ideograms
(i.e., semantic units). In this sense, the choice
of Chinese as the language in our work is es-
sential.
If the assumption holds well, the most im-
portant and direct application is unsuper-
vised text segmentation into words. Many
works in unsupervised segmentation so far
could be interpreted as formulating assump-
tion (A) in a similar sense where branch-
ing stays low inside words but increases
at a word or morpheme border. None of
these works, however, is directly based on
(A), and they introduce other factors within
their overall methodologies. Some works are
based on in-word branching frequencies for-
mulated in an original evaluation function,
as in (Ando and Lee, 2000) (boundary pre-
cision=84.5%,recall=78.0%, tested on 12500
Japanese ideogram words). Sun et al (Sun
et al, 1998) uses mutual information (bound-
ary p=91.8%, no report for recall, 1588 Chi-
nese characters), and Feng(Feng et al, 2004)
incorporates branching counts in the evalua-
tion function to be optimized for obtaining
boundaries (word precision=76%, recall=78%,
2000 sentences). From the performance results
listed here, we can see that unsupervised seg-
mentation is more dicult, by far, than super-
vised segmentation; therefore, the algorithms
are complex, and previous studies have tended
to be limited in terms of both the test corpus
size and the target.
In contrast, as assumption (A) is simple, we
keep this simplicity in our formalization and
directly test the assumption on a large-scale
test corpus consisting of 1001 KB manually
segmented data with the training corpus con-
sisting of 200 MB of Chinese text.
Chinese is such an important language that
supervised segmentation methods are already
very mature. The current state-of-the-art seg-
mentation software developed by (Low et al,
2005), which ranks as the best in the SIGHAN
bakeo (Emerson, 2005), attains word preci-
sion and recall of 96.9% and 96.8%, respec-
tively, on the PKU track. There is also free
 0.5
 1
 1.5
 2
 2.5
 3
 3.5
 4
 4.5
 5
 1  2  3  4  5  6  7  8
e
n
tro
py
offset
Figure 2: Decrease in H(X jX
n
) for Chinese
characters when n is increased
software such as (Zhang et al, 2003) whose
performance is also high. Even then, as most
supervised methods learn on manually seg-
mented newspaper data, when the input text
is not from newspapers, the performance can
be insucient. Given that the construction of
learning data is costly, we believe the perfor-
mance can be raised by combining the super-
vised and unsupervised methods.
Consequently, this paper veries assump-
tion (A) in a fundamental manner for Chinese
text and addresses the questions of why and to
what extent (A) holds, when applying it to the
Chinese word segmentation problem. We rst
formalize assumption (A) in a general manner.
2 The Assumption
Given a set of elements  and a set of n-gram
sequences 
n
formed of , the conditional en-
tropy of an element occurring after an n-gram
sequence X
n
is dened as
H(X jX
n
) =
 
X
x
n
2
n
P (x
n
)
X
x2
P (xjx
n
) logP (xjx
n
);
(1)
where P (x) = P (X = x), P (xjx
n
) = P (X =
xjX
n
= x
n
), and P (X = x) indicates the prob-
ability of occurrence of x.
A well-known observation on language data
states that H(X jX
n
) decreases as n increases
(Bell et al, 1990). For example, Figure 2
shows how H(X jX
n
) shifts when n increases
from 1 to 8 characters, where n is the length of
a word prex. This is calculated for all words
existing in the test corpus, with the entropy
being measured in the learning data (the learn-
ing and test data are dened in x4).
This phenomenon indicates that X will be-
come easier to estimate as the context of X
n
429
gets longer. This can be intuitively under-
stood: it is easy to guess that \e" will follow
after \Hello! How ar", but it is dicult to
guess what comes after the short string \He".
The last term   log P (xjx
n
) in the above for-
mula indicates the information of a token of x
coming after x
n
, and thus the branching after
x
n
. The latter half of the formula, the local
entropy value for a given x
n
,
H(X jX
n
= x
n
) =  
X
x2
P (xjx
n
) logP (xjx
n
);
(2)
indicates the average information of branching
for a specic n-gram sequence x
n
. As our in-
terest in this paper is this local entropy, we
denote H(X jX
n
= x
n
) simply as h(x
n
) in the
rest of this paper.
The decrease in H(X jX
n
) globally indicates
that given an n-length sequence x
n
and an-
other (n+1)-length sequence y
n+1
, the follow-
ing inequality holds on average:
h(x
n
) > h(y
n+1
): (3)
One reason why inequality (3) holds for lan-
guage data is that there is context in language,
and y
n+1
carries a longer context as compared
with x
n
. Therefore, if we suppose that x
n
is
the prex of x
n+1
, then it is very likely that
h(x
n
) > h(x
n+1
) (4)
holds, because the longer the preceding n-
gram, the longer the same context. For ex-
ample, it is easier to guess what comes af-
ter x
6
=\natura" than what comes after x
5
=
\natur". Therefore, the decrease in H(X jX
n
)
can be expressed as the concept that if the con-
text is longer, the uncertainty of the branching
decreases on average. Then, taking the logical
contraposition, if the uncertainty does not de-
crease, the context is not longer, which can be
interpreted as the following:
If the entropy of successive tokens in-
creases, the location is at a context
border. (B)
For example, in the case of x
7
= \natu-
ral", the entropy h(\natural") should be larger
than h(\natura"), because it is uncertain what
character will allow x
7
to succeed. In the next
section, we utilize assumption (B) to detect
context boundaries.
Figure 3: Our model for boundary detection
based on the entropy of branching
3 Boundary Detection Using the
Entropy of Branching
Assumption (B) gives a hint on how to utilize
the branching entropy as an indicator of the
context boundary. When two semantic units,
both longer than 1, are put together, the en-
tropy would appear as in the rst gure of Fig-
ure 3. The rst semantic unit is from osets
0 to 4, and the second is from 4 to 8, with
each unit formed by elements of . In the g-
ure, one possible transition of the branching
degree is shown, where the plot at k on the
horizontal axis denotes the entropy for h(x
0;k
)
and x
n;m
denotes the substring between osets
n and m.
Ideally, the entropy would take a maximum
at 4, because it will decrease as k is increased
in the ranges of k < 4 and 4 < k < 8, and
at k = 4, it will rise. Therefore, the position
at k = 4 is detected as the \local maximum
value" when monitoring h(x
0;k
) over k. The
boundary condition after such observation can
be redened as the following:
B
max
Boundaries are locations where the en-
tropy is locally maximized.
A similar method is proposed by Harris (Har-
ris, 1955), where morpheme borders can be
detected by using the local maximum of the
number of dierent tokens coming after a pre-
x.
This only holds, however, for semantic units
longer than 1. Units often have a length of
430
1, especially in our case with Chinese charac-
ters as elements, so that there are many one-
character words. If a unit has length 1, then
the situation will look like the second graph
in Figure 3, where three semantic units, x
0;4
,
x
4;5
, and x
5;8
, are present, with the middle
unit having length 1. First, at k = 4, the
value of h increases. At k = 5, the value may
increase or decrease, because the longer con-
text results in an uncertainty decrease, though
an uncertainty decrease does not necessarily
mean a longer context. When h increases at
k = 5, the situation will look like the second
graph. In this case, the condition B
max
will
not suce, and we need a second boundary
condition:
B
increase
Boundaries are locations where the
entropy is increased.
On the other hand, when h decreases at k = 5,
then even B
increase
cannot be applied to detect
k = 5 as a boundary. We have other chances to
detect k = 5, however, by considering h(x
i;k
),
where 0 < i < k. According to inequality
(3), then, a similar trend should be present
for plots of h(x
i;k
), assuming that h(x
0;n
) >
h(x
0;n+1
); then, we have
h(x
i;n
) > h(x
i;n+1
); for 0 < i < n: (5)
The value h(x
i;k
) would hopefully rise for some
i if the boundary at k = 5 is important,
although h(x
i;k
) can increase or decrease at
k = 5, just as in the case for h(x
0;n
).
Therefore, when the target language con-
sists of many one-element units, B
increase
is
crucial for collecting all boundaries. Note that
the boundaries detected by B
max
are included
in those detected by the condition B
increase
,
and also that B
increase
is a boundary condition
representing the assumption (B) more directly.
So far, we have considered only regular-
order processing: the branching degree is cal-
culated for successive elements of x
n
. We can
also consider the reverse order, which involves
calculating h for the previous element of x
n
. In
the case of the previous element, the question
is whether the head of x
n
forms the beginning
of a context boundary.
Next, we move on to explain how we ac-
tually applied the above formalization to the
problem of Chinese segmentation.
4 Data
The whole data for training amounted to 200
MB, from the Contemporary Chinese Cor-
pus of the Center of Chinese Linguistics at
Peking University (Center for Chinese Linguis-
tics, 2006). It consists of several years of Peo-
ples' Daily newspapers, contemporary Chinese
literature, and some popular Chinese maga-
zines. Note that as our method is unsuper-
vised, this learning corpus is just text without
any segmentation.
The test data were constructed by selecting
sentences from the manually segmented Peo-
ple's Daily corpus of Peking University. In to-
tal, the test data amounts to 1001 KB, consist-
ing 147026 Chinese words. The word bound-
aries indicated in the corpus were used as our
golden standard.
As punctuation is clear from text bound-
aries in Chinese text, we pre-processed the test
data by segmenting sentences at punctuation
locations to form text fragments. Then, from
all fragments, n-grams of less than 6 charac-
ters were obtained. The branching entropies
for all these n-grams existing within the test
data were obtained from the 200 MB of data.
We used 6 as the maximum n-gram length
because Chinese words with a length of more
than 5 characters are rare. Therefore, scan-
ning the n-grams up to a length of 6 was su-
cient. Another reason is that we actually con-
ducted the experiment up to 8-grams, but the
performance did not improve from when we
used 6-grams.
Using this list of words ranging from un-
igrams to 6-grams and their branching en-
tropies, the test data were processed so as to
obtain the word boundaries.
5 Analysis for Small Examples
Figure 4 shows an actual graph of
the entropy shift for the input phrase
(wei lai fa zhan
de mu biao he zhi dao fang zhen, the aim and
guideline of future development). The upper
gure shows the entropy shift for the forward
case, and the lower gure shows the entropy
shift for the backward case. Note that for the
backward case, the branching entropy was
calculated for characters before the x
n
.
In the upper gure, there are two lines, one
431
Figure 4: Entropy shift for a small example
(forward and backward)
for the branching entropy after the substrings
starting from . The leftmost line plots
h( ), h( ) : : : h( ). There
are two increasing points, indicating that the
phrase was segmented between and ,
and between and . The second line
plots h( ) : : : h( ). The increas-
ing locations are between and , be-
tween and , and after .
The lower gure is the same. There are two
lines, one for the branching entropy before the
substring ending with sux . The rightmost
line plots h( ), h( ) . . .h( )
running from back to front. We can see in-
creasing points (as seen from back to front) be-
tween and , and between and .
As for the last line, it also starts from and
runs from back to front, indicating boundaries
between and , between and ,
and just before .
If we consider all the increasing points in all
four lines and take the set union of them, we
obtain the correct segmentation as follows:
j j j j j j ,
which is the 100 % correct segmentation in
terms of both recall and precision.
In fact, as there are 12 characters in this
input, there should be 12 lines starting from
each character for all substrings. For read-
ability, however, we only show two lines each
for the forward and backward cases. Also, the
maximum length of a line is 6, because we only
took 6-grams out of the learning data. If we
consider all the increasing points in all 12 lines
and take the set union, then we again obtain
100 % precision and recall. It is amazing how
all 12 lines indicate only correct word bound-
aries.
Also, note how the correct full segmenta-
tion is obtained only with partial information
from 4 lines taken from the 12 lines. Based
on this observation, we next explain the algo-
rithm that we used for a larger-scale experi-
ment.
6 Algorithm for Segmentation
Having determined the entropy for all n-grams
in the learning data, we could scan through
each chunk of test data in both the forward
order and the backward order to determine the
locations of segmentation.
As our intention in this paper is above all to
study the innate linguistic structure described
by assumption (B), we do not want to add any
artifacts other than this assumption. For such
exact verication, we have to scan through all
possible substrings of an input, which amounts
to O(n
2
) computational complexity, where n
indicates the input length of characters.
Usually, however, h(x
m;n
) becomes impos-
sible to measure when n   m becomes large.
Also, as noted in the previous section, words
longer than 6 characters are very rare in Chi-
nese text. Therefore, given a string x, all n-
grams of no more than 6 grams are scanned,
and the points where the boundary condition
holds are output as boundaries.
As for the boundary conditions, we have
B
max
and B
increase
, and we also utilize
B
ordinary
, where location n is considered as a
boundary when the branching entropy h(x
n
)
is simply above a given threshold. Precisely,
there are three boundary conditions:
B
max
h(x
n
) > valmax,
where h(x
n
) takes a local maximum,
B
increase
h(x
n+1
)  h(x
n
) > valdelta,
B
ordinary
h(x
n
) > val,
where valmax, valdelta, and val are arbitrary
thresholds.
432
7 Large-Scale Experiments
7.1 Denition of Precision and Recall
Usually, when precision and recall are ad-
dressed in the Chinese word segmentation do-
main, they are calculated based on the number
of words. For example, consider a correctly
segmented sequence \aaajbbbjcccjddd", with
a,b,c,d being characters and \j" indicating a
word boundary. Suppose that the machine's
result is \aaabbbjcccjddd"; then the correct
words are only \ccc" and \ddd", giving a value
of 2. Therefore, the precision is 2 divided
by the number of words in the results (i.e., 3
for the words \aaabbb", \ccc", \ddd"), giving
67%, and the recall is 2 divided by the total
number of words in the golden standard (i.e., 4
for the words \aaa",\bbb", \ccc", \ddd") giv-
ing 50%. We call these values the word pre-
cision and recall, respectively, throughout this
paper.
In our case, we use slightly dierent mea-
sures for the boundary precision and recall,
which are based on the correct number of
boundaries. These scores are also utilized espe-
cially in previous works on unsupervised seg-
mentation (Ando and Lee, 2000) (Sun et al,
1998). Precisely,
Precision =
N
correct
N
test
(6)
Recall =
N
correct
N
true
; where (7)
N
correct
is the number of correct boundaries in
the result,
N
test
is the number of boundaries in the test
result, and,
N
true
is the number of boundaries in the
golden standard.
For example, in the case of the machine result
being \aaabbbjcccjddd", the precision is 100%
and the recall is 75%. Thus, we consider there
to be no imprecise result as a boundary in the
output of \aaabbbjcccjddd".
The crucial reason for using the boundary
precision and recall is that boundary detec-
tion and word extraction are not exactly the
same task. In this sense, assumption (A) or
(B) is a general assumption about a bound-
ary (of a sentence, phrase, word, morpheme).
Therefore, the boundary precision and recall
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0.55  0.6  0.65  0.7  0.75  0.8  0.85  0.9  0.95  1
re
ca
ll
precision
Bincrease
Bordinary
Bmax
Figure 5: Precision and recall
measure serves for directly measuring bound-
aries.
Note that all precision and recall scores from
now on in this paper are boundary precision
and recall. Even in comparing the super-
vised methods with our unsupervised method
later, the precision and recall values are all re-
calculated as boundary precision and recall.
7.2 Precision and Recall
The precision and recall graph is shown in Fig-
ure 5. The horizontal axis is the precision
and the vertical axis is the recall. The three
lines from right to left (top to bottom) cor-
respond to B
increase
(0:0  valdelta  2:4),
B
max
(4:0  valmax  6:2), and B
ordinary
(4:0  val  6:2). All are plotted with an
interval of 0.1. For every condition, the larger
the threshold, the higher the precision and the
lower the recall.
We can see how B
increase
and B
max
keep
high precision as compared with B
ordinary
. We
also can see that the boundary can be more
easily detected if it is judged as comprising
the proximity value of h(x
n
).
For B
increase
, in particular, when valdelta =
0:0, the precision and recall are still at 0.88 and
0.79, respectively. Upon increasing the thresh-
old to valdelta = 2:4, the precision is higher
than 0.96 at the cost of a low recall of 0.29. As
for B
max
, we also observe a similar tendency
but with low recall due to the smaller number
of local maximum points as compared with the
number of increasing points. Thus, we see how
B
increase
attains a better performance among
the three conditions. This shows the correct-
ness of assumption (B).
From now on, we consider only B
increase
and
proceed through our other experiments.
433
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 10  100  1000  10000  100000  1e+06
size(KB)
recall
precision
Figure 6: Precision and recall depending on
training data size
Next, we investigated how the training data
size aects the precision and recall. This time,
the horizontal axis is the amount of learning
data, varying from 10 KB up to 200 MB, on
a log scale. The vertical axis shows the pre-
cision and recall. The boundary condition is
B
increase
with valdelta = 0:1.
We can see how the precision always re-
mains high, whereas the recall depends on the
amount of data. The precision is stable at an
amazingly high value, even when the branch-
ing entropy is obtained from a very small cor-
pus of 10 KB. Also, the linear increase in the
recall suggests that if we had more than 200
MB of data, we would expect to have an even
higher recall. As the horizontal axis is in a log
scale, however, we would have to have giga-
bytes of data to achieve the last several per-
cent of recall.
7.3 Error Analysis
According to our manual error analysis, the
top-most three errors were the following:
 Numbers: dates, years, quantities (ex-
ample: 1998, written in Chinese number
characters)
 One-character words (example: (at)
(again) (toward) (and))
 Compound Chinese words (example:
(open mind) being segmented
into (open) and (mind))
The reason for the bad results with numbers
is probably because the branching entropy for
digits is less biased than for usual ideograms.
Also, for one-character words, our method is
limited, as we explained in x3. Both of these
two problems, however, can be solved by ap-
plying special preprocessing for numbers and
one-character words, given that many of the
one-character words are functional characters,
which are limited in number. Such improve-
ments remain for our future work.
The third error type, in fact, is one that
could be judged as correct segmentation. In
the case of \open mind", it was not segmented
into two words in the golden standard; there-
fore, our result was judged as incorrect. This
could, however, be judged as correct.
The structures of Chinese words and phrases
are very similar, and there are no clear crite-
ria for distinguishing between a word and a
phrase. The unsupervised method determines
the structure and segments words and phrases
into smaller pieces. Manual recalculation of
the accuracy comprising such cases also re-
mains for our future work.
8 Conclusion
We have reported an unsupervised Chinese
segmentation method based on the branching
entropy. This method is based on an assump-
tion that \if the entropy of successive tokens
increases, the location is at the context bor-
der." The entropies of n-grams were learned
from an unsegmented 200-MB corpus, and the
actual segmentation was conducted directly
according to the above assumption, on 1 MB
of test data. We found that the precision was
as high as 90% with recall being around 80%.
We also found an amazing tendency for the
precision to always remain high, regardless of
the size of the learning data.
There are two important considerations for
our future work. The rst is to gure out how
to combine the supervised and unsupervised
methods. In particular, as the performance of
the supervised methods could be insucient
for data that are not from newspapers, there
is the possibility of combining the supervised
and unsupervised methods to achieve a higher
accuracy for general data. The second future
work is to verify our basic assumption in other
languages. In particular, we should undertake
experimental studies in languages written with
phonogram characters.
434
References
R.K. Ando and L. Lee. 2000. Mostly-unsupervised
statistical segmentation of Japanese: Applica-
tions to kanji. In ANLP-NAACL.
T.C. Bell, J.G. Cleary, andWitten. I.H. 1990. Text
Compression. Prentice Hall.
Center for Chinese Linguistics. 2006. Chi-
nese corpus. visited 2006, searchable from
http://ccl.pku.edu.cn/YuLiao Contents.Asp,
part of it freely available from
http://www.icl.pku.edu.cn.
T. Emerson. 2005. The second international chi-
nese word segmentation bakeo. In SIGHAN.
H.D. Feng, K. Chen, C.Y. Kit, and Deng. X.T.
2004. Unsupervised segmentation of chinese cor-
pus using accessor variety. In IJCNLP, pages
255{261.
S.Z. Harris. 1955. From phoneme to morpheme.
Language, pages 190{222.
A. Kempe. 1999. Experiments in unsupervised
entropy-based corpus segmentation. In Work-
shop of EACL in Computational Natural Lan-
guage Learning, pages 7{13.
J.K. Low, H.T Ng, and W. Guo. 2005. A maxi-
mum entropy approach to chinese word segmen-
tation. In SIGHAN.
M. Sun, D. Shen, and B. K. Tsou. 1998. Chi-
nese word segmentation without using lexicon
and hand-crafted training data. In COLING-
ACL.
K. Tanaka-Ishii. 2005. Entropy as an indicator of
context boundaries |an experiment using a web
search engine |. In IJCNLP, pages 93{105.
H.P. Zhang, Yu H.Y., Xiong D.Y., and Q Liu.
2003. Hhmm-based chinese lexical analyzer ict-
clas. In SIGHAN. visited 2006, available from
http://www.nlp.org.cn.
435
Sorting Texts by Readability
Kumiko Tanaka-Ishii?
Satoshi Tezuka
Hiroshi Terada
Graduate School of Information Science
and Technology, University of Tokyo
This article presents a novel approach for readability assessment through sorting. A comparator
that judges the relative readability between two texts is generated through machine learning, and
a given set of texts is sorted by this comparator. Our proposal is advantageous because it solves
the problem of a lack of training data, because the construction of the comparator only requires
training data annotated with two reading levels. The proposed method is compared with regres-
sion methods and a state-of-the art classification method. Moreover, we present our application,
called Terrace, which retrieves texts with readability similar to that of a given input text.
1. Introduction
Readability assessment is an important NLP issue with much application in the domain
of language education. The capability to automatically judge the readability of a text
would greatly help language teachers and learners, who currently spend a great deal of
time skimming through texts looking for a text at an appropriate reading level.
Substantial previous work has been done over the past decades (Klare 1963, DuBay
2004a, 2004b). Early work generated measures based on simple text statistics by as-
suming that these reflect the text reading level. For example, Kincaid, Fishburne, and
Rodgers (1975) assumed that the lengths of words and sentences represent their re-
spective difficulty. Chall and Dale (1995) used a manually constructed list of words
assumed to capture the difficulty of vocabulary. These measures are easy to use but
difficult to apply to languages other than English, because some features, such as word
length, are specific to alphabetic writing. Such methods, however, do not compete with
recent methods based on more sophisticated handling of language statistics. Collins-
Thompson and Callan (2004) proposed a classification model by constructing different
language models for different school grades (Si and Callan 2001), and Schwarm and
Ostendorf (2005) applied a support vector machine (SVM). Both of these methods
outperform classical methods and are less language-dependent.
These new methods, however, have a serious problem when implementation is at-
tempted for multiple languages: the lack of training corpora. Large amounts of training
data annotated with 12 school grades have not been at all easy to obtain on a reasonable
scale. Another possibility might have been to manually construct such training data,
? University of Tokyo Cross Field, 13F Akihabara Daibiru, 1-18-13 Kanda Chiyoda-ku, Tokyo, Japan.
E-mail: kumiko@kumish.net.
Submission received: 7 October 2008; revised submission received: 17 September 2009; accepted for
publication: 19 December 2009.
? 2010 Association for Computational Linguistics
Computational Linguistics Volume 36, Number 2
but humans are generally unable to precisely judge the level of a given text among 12
arbitrary levels. The corpora therefore have to be constructed from academic texts used
in schools. The amount of such data, however, is limited, and its use is usually strictly
limited by copyrights. Thus, it is crucial to devise a new method or approach that allows
readability assessment by using only generally available corpora, such as newspapers.
Given a single text, it is hard to attribute an absolute readability level from among
12 levels, but given two texts, there should be a better chance of judging which of them
is more difficult. This intuition led to the new model presented in this article. Our idea
is based on sorting, which is implemented in two stages:
 A comparator is generated by an SVM. This comparator judges the relative
readability of two given texts.
 Given a set of texts, the texts are sorted by the comparator with a sorting
algorithm. In our case, we used a robust binary insertion sort, as explained
in further detail later in this article.
The first step requires a training corpus, but because the comparator only judges which
of two texts is more difficult, the texts of a corpus need only be labeled according to
two different levels. Two sets of texts?one difficult, the other easy?are far easier to
obtain than a training corpus annotated for 12 different levels. Overall, our model of
readability thus differs from previous regression or classification models.
Applying this new method, we also present an application, called Terrace, which is a
system that retrieves a text with readability similar to that of a given input text. Terrace
was originally motivated by a faculty request made by teachers of multiple foreign
languages. The system currently works for English and Japanese, and the languages
will be extended to include Chinese and French.
Note that we do not claim that our model and method is better than existing meth-
ods. Although our method does compete well with previous methods, the classification
approach used in any given scenario should remain the most natural, relevant method.
The intention of this article is simply to propose an alternative way of handling read-
ability assessment, especially when adequate training corpora annotated with multiple
levels are not available.
2. Related Work
Readability, in general, describes the ease with which a text document can be read and
understood. Readability is studied in at least two different domains, those of coherence
(Barzilay and Lapata 2008) and language learning. Readability in this article signifies
the latter, for both a mother tongue and a second language.
Even within this domain, substantial previous work has been done (Klare 1963;
DuBay 2004a, 2004b). DuBay (2004a) writes that:
By the 1980s, there were 200 formulas and over a thousand studies published on the
readability formulas attesting to their strong theoretical and statistical validity (p. 2).
Every method of readability assessment extracts some features from a text and maps
the feature space to some readability norm. There are the two viewpoints regarding
features and the mapping of feature values to readability, and correspondingly there are
two kinds of work in this domain.
204
Tanaka-Ishii, Tezuka, and Terada Sorting Texts by Readability
Regarding the first type, many researchers have reported how various features
affect the readability of text in terms of vocabulary, syntax, and discourse relations.
Recently, Pitler and Nenkova (2008) presented an impressive verification of the effects of
each kind of feature and found that vocabulary and discourse relations are prominent,
although other features are not negligible.
The focus of the current work, however, is not on what feature set to consider, so we
use the same features throughout the article, as explained further in Section 3.1. Rather,
the focus of this article is on mapping the extracted feature values to a readability norm.
So far, two models have been used for this: regression and classification.
In regression, readability is given by a score based on a linearly weighted sum
of feature values. Early methods, from the Wannetka formula (Washburne and Vogel
1928), to the recent methods of Flesch?Kincaid (Kincaid, Fishburne, and Rodgers 1975)
and Dale?Chall (Chall and Dale 1995), are of this kind. Elaboration of such regression
methods in a more modern context could proceed through a generalized linear model
based on estimation of the weights by machine learning, although we have not found
such an approach within the literature of readability assessment for language learning.
Our proposal is compared with such an enhanced version of regression in Section 8.
In classification, readability is segmented by academic grades, and the assessment
is conducted as a classification task. The first is implemented by means of statistical
classification modeling, as reported in Collins-Thompson and Callan (2004) and Si
and Callan (2001). The authors used a language model (unigrams) and a naive Bayes
classifier by presuming different language models for each reading level. A language
model Mi is constructed for each level of readability i by using different corpora for
each level. The readability of a given text T is assessed using the formula L(Mi|T) =
?w?TC(w) log Pr(w|Mi), where w denotes a word in text T, C(w) denotes the frequency
of w, and Pr(w|Mi) denotes the probability of w under Mi. The second is based on an
SVM (Schwarm and Ostendorf 2005) and the authors also studied the effect of statistical
features, such as n-grams and syntactic features.
In these papers, the readability norms are represented by means of scores and
classes of readability. That is, given a single text, the system assigns a value correspond-
ing to a school grade. The result is easy to understand, and various applications have
been constructed with this type of scoring. This solution only works, however, when
a sufficient amount of training data with annotations regarding multiple levels is pro-
vided. Usually, the availability of training data in readability assessment is limited, even
for school grading. This is due to the inherent difficulty of classifying the readability of a
text into 12 grades, making it difficult to uniformly construct a large set of training data.
Moreover, the copyright issue is more serious for academic texts.1 Given this situation,
when readability assessment is modeled by regression or classification, a research team
wanting to apply these previous methods faces the problem of assembling training data,
as we did for over a year.
In this article, the readability norm is designed in a completely different way: Given
two texts, a comparator judges which is more difficult. By applying this comparator, a
set of texts is sorted. The readability of a text is assessed by searching for its position
1 We asked the authors of previous studies based on the classification approach to share their training
data, but they could not because of the copyright issue. This is a serious issue in Japanese, as well:
These copyrights are more tightly protected than those of normal texts, and publishing companies
refused to provide us with electronic files. We thus had to scan texts and use OCR to obtain the test
data utilized in this work. This experience demonstrates how difficult it is to obtain large-scale
training data with multiple levels.
205
Computational Linguistics Volume 36, Number 2
within the sorted texts. The norm is thus considered as the location of a text among an
ordered set of texts. Our approach linguistically enhances assessment of the readability of
a text as the relative ease compared to other texts, not as the absolute difficulty of the text.
The root of this idea has been presented in two articles of which we are aware. In
Inui and Yamamoto (2001), the readability of sentences for deaf people is judged by a
comparator generated by an SVM. In addition, Pitler and Nenkova (2008) presented a
comparison of texts in terms of difficulty by using an SVM. Similarly to what we present
in Section 3.1, those authors propose constructing a comparator by using an SVM to
compare two sentences or texts with multiple features. However, neither further applied
this approach to obtain readability assessment based on sorting. Our contribution in
this study is therefore that we show how a machine learning method can be used as a
comparator and applied to sort texts.
Our method can be situated more generally among machine learning methods for
ranking, where the methods learn so that they rank a set of elements given a set of
ordered training data. Various methods have been proposed so far. In one of the earliest
attempts, Cohen, Schapire, and Singer (1998) obtain a function that scores the probabil-
ity that an element is ranked higher than another, and rank all elements by maximizing
the sum of the pairwise probabilities. In another, Joachims (2002) applies an SVM to
rank elements, by devising the input vector by subtraction of feature values. In more
recent studies, such as Xia et al (2008), an attempt is made to directly obtain the ranking
function for the whole ordered training data, not as a composition of pairwise function
application between elements. Among these methods, our proposal is unique in two
ways. First, none of the previous methods, as far as we know, proposed discretized
ranking based on sorting. Second and most importantly, all previous methods assume
the existence of fully ordered training data. In contrast, as emphasized by our problem
described in this section, such training data are difficult to acquire in the readability
domain, and we have to devise a method which works even when very limited training
data are all that is available. Our contribution lies in our study of the possibility of
using a learning-to-rank method even when learning data are only partially available.
Such an approach can be further considered for learning-to-rank methods in general in
future work.
3. The Method
In our method, a comparator of the level of difficulty of two texts is generated by using
machine learning and then the comparator is applied to sort a set of texts. The method
has two parts:
 construction of the readability relation <, and
 sorting and searching texts by using the relation.
These tasks are explained in the following sections.
3.1 Readability Comparator
The readability comparator is constructed by applying machine learning. Given texts
a, b ? S, where S is a set of texts, feature vectors Va and Vb are constructed. By applying
an operator ?, Vab is constructed as Va ? Vb. When Vab is entered into the comparator, the
comparator outputs 1 when a > b (i.e., a is more difficult) and ?1 when a < b. Because
206
Tanaka-Ishii, Tezuka, and Terada Sorting Texts by Readability
Figure 1
Construction of a comparator using an SVM.
the output is binary, we use an SVM to construct the comparator (see Figure 1). Note
that Vab and Vba are not the same. Reversibility of a feature vector is thus not obvious in
our work, but ideally, when the judgment for Vab is 1, that for Vba will be ?1.
In terms of constructing a feature vector Va for a text a, substantial features have
been proposed (Klare 1963; DuBay 2004a, 2004b; Schwarm and Ostendorf 2005; Pitler
and Nenkova 2008). In this work, we only utilize the most basic features of vocabulary
in terms of word frequencies for three reasons. First, as stated in Section 2, because
the focus of this article is not to study the set of features, it is best to set the feature
issue aside and use only the most fundamental features. Even then, there are many
viewpoints to be verified, as will be seen in Sections 7?9. Furthermore, we have to take
into account the pros and cons of various features, because naive features only capture
coarse, default trends and could degrade performance. For example, the text length
in our data tends to be longer for more difficult texts, thus having a bad influence on
short, higher-grade texts and long, lower-grade texts. Therefore, in this article, we only
consider simple features regarding vocabulary. Second, previous work has argued for
the fundamental nature of vocabulary as a factor in readability (Alderson 1984; Laufer
1991; Pitler and Nenkova 2008). Third, some features other than word frequencies are
language-dependent in terms of the writing system, corpus availability, and perfor-
mance of NLP analysis systems. For example, average word length, used in the Flesch?
Kincaid approach, cannot be applied to Japanese. Thus, we have focused on features
that are available for any language, so that the performance becomes comparable across
languages.
The features we use are as follows. There are two factors within vocabulary: the local
and global factors. The local factor is what words are used and how frequently they ap-
pear within a text, whereas the global factor indicates the degree of readability of these
words among the overall vocabulary. Both are considered within this article, as follows:
Local: the frequency of each word divided by the frequency of the number of words in
the text (this is called relative frequency here).
Global: the log frequency of words obtained from a large corpus.
Local frequency is the most basic statistic used in machine learning methods. Relative
frequency is used to avoid the influence of text length, as mentioned above.
Regarding global frequency, psychological studies have shown that every word has
a level of familiarity that is fairly commonly understood among people. For exam-
ple, the verb meet is more familiar than the verb encounter. Such levels of familiarity
207
Computational Linguistics Volume 36, Number 2
attributed to all words should affect the difficulty levels of texts. In Chall and Dale
(1995), the authors counted the number of words not appearing in a list of the most
basic 3,000 words, in order to judge vocabulary difficulty, but the plausibility of such a
list is difficult to evaluate and such lists might not exist for languages other than English.
An alternative is to use word lists with familiarity scores, such as the MRC list in English
(Database 2006) or the Amano list in Japanese (Amano and Kondo 2000), but such lists
are costly to generate and the word coverage is limited. Instead, we decided to use the
log frequency obtained from several terabytes of corpora, because the psychological
familiarity score is known to correlate strongly with the log frequency obtained from
corpora, as reported in Tanaka-Ishii and Terada (2009) and Amano and Kondo (1995).
Tanaka-Ishii and Terada show that log frequency correlates better for larger corpora.
Thus, as will be explained in Section 6, Web corpora consisting of 6 terabytes for
English and 2 terabytes for Japanese?possibly the largest corpora available for the two
languages?were used to obtain the log frequencies.
Local and global features are extracted for all words of the two texts a and b to
be compared. The final vector is generated by applying an operator ?, such that Vab =
Va ? Vb. Two simple possibilities for the operation ? are the following:
Concatenation: concatenate Vb?s elements after the elements of Va.
Conjunction: produce a new value for the ith element of Vab from the ith elements of Va
and Vb by some function. Typical possibilities for such a function are subtraction
and division.
The two possible vectors for Vab are explained by the illustration in Figure 2. The upper
half of the figure shows Vab generated by concatenation. Because the comparator is
Figure 2
Composition of the vectors fed into the SVM.
208
Tanaka-Ishii, Tezuka, and Terada Sorting Texts by Readability
constructed by machine learning, the dimension of the vectors is four times the number
of word tokens that appear in the training data. Each kind of word has a location in
a vector, which appears four times within Vab: twice in each Va and Vb for local and
global features. The first half denotes Va, and the second half denotes Vb. In the first half
of Va, the vector values are the relative frequencies for words appearing in text a, or zero
otherwise. For the second half of Va, the values are log frequencies measured in a large
corpus for the words appearing in a, or zero otherwise. Vb is constructed in a similar
manner, and the two vectors are concatenated.
In contrast, in the case of conjunction, the dimension of the vector remains twice
the number of word tokens appearing in the training data, as illustrated in the lower
half of Figure 2. The ith dimension of Vab is calculated as a function of the ith values of
Va and Vb. Pitler and Nenkova (2008) are likely to have used subtraction within their
experiment.2 For this reason, among various other possible functions for the conjunction
of two vector values, we also used subtraction.
Construction of a comparator requires training data. Because the comparator judges
the difficulty between two texts, the training data contains two sets of texts, a set Ld of
a relatively difficult level and a set Le of a relatively easy level. The SVM is trained
on a combination of texts taken from Le and Ld. The training data can combine up to
2 ? |Le| ? |Ld|. For example, if |Le| = |Ld| = 600, the number of data points for training
could amount to 720,000. Whether learning can fully exploit such combined training
data is an interesting issue, which is examined in Section 8.
After training, for two given texts a? and b?, the relative difficulty is judged by
inputting Va?b? into the SVM. In this phase, if a new word which was not in the training
corpora appears in either a? or b?, the word is ignored for the purposes of this work.
3.2 Sorting and Searching Texts
With a comparator thus constructed, the texts of S are sorted. Further, readability
assessment is performed by searching for the position of a text within the sorted texts.
The sorting and searching algorithms can be chosen from among the basic ones
already established within computer science. There are further requirements, however,
as follows:
 Searching must be as fast as possible for the actual application.
 Sorting and searching should be implemented with the same procedure,
given the nature of the application, as will be explained in Section 5.
Moreover, sorting should be done incrementally to facilitate the addition
of texts.
 Sorting must be made robust to overcome judgment errors made by the
comparator.
Given the first two requirements, we chose binary insertion sort and binary sorting from
among various sorting and searching algorithms, because it provides one of the fastest
search methods.
2 This is no more than our guess. The precise calculation method unfortunately cannot be understood from
their paper, because the description is very brief.
209
Computational Linguistics Volume 36, Number 2
To meet the last requirement, the algorithm is made robust by performing multiple
comparisons at one time, as explained here. In binary insertion sort, given a new text,
the insertion position is searched for among the previously sorted texts (from the easiest
text to the most difficult text), and the text is inserted between two successive texts i
and i + 1, where the text is more difficult than the ith easiest text and it is easier than
the i + 1th text. Through repetition of this operation, all texts are sorted. Because our
comparator is generated using an SVM, one sole comparison could lead to an erroneous
result. Moreover, one comparison error could lead to disastrous sorting results with a
binary sort, because binary search changes the insertion position significantly, especially
at the beginning of the search procedure.
To avoid such comparison mistakes, the comparison judgment is made through
multiple comparisons with texts located at the proximity of each position. The number
of multiple comparisons is called the width and is denoted as M = 2K + 1, with K
being a positive natural number, as illustrated in Figure 3. M is dynamically changed
during the search procedure by narrowing it down to indicate the exact position. The
procedure starts with a given set of texts S and an empty set SS. At each judgment, a
text to be inserted is removed from S and inserted into SS. The procedure proceeds as
follows:
1. Obtain a text x from S. If SS is empty, the text x is put into SS and another
text x is obtained from S.
2. The texts of SS are already sorted and are indexed by i = 1 . . . n, where
i = 1 is the easiest and i = n is the most difficult. Searching is done using
three variables, which are initialized as follows: bottom ? 1; top ? n;
middle ? (top + bottom)/2. The width M is set as follows: M = 2 ? K + 1
(K ? N ).
3. If M > (top ? bottom)/2, then K ? (top ? bottom)/2. A change in K also
changes the value of M, which is always set to 2 ? K + 1.
4. Compare x with texts at i = middle ? K...middle + K. If more than K texts
are judged as difficult, then top ? middle ? 1. Otherwise, bottom ?
middle + 1.
5. If top < bottom, then insert x before the bottom. Otherwise, go to step 3.
Figure 3
Robust comparison.
210
Tanaka-Ishii, Tezuka, and Terada Sorting Texts by Readability
6. Insert x before bottom.
7. If |S| > 1, then go to step 1. Otherwise, the procedure is complete.
The complexity for sorting a set |S| amounts to O(C|S|2), where C is the maximum value
of M. Because the time complexity required for insertion is log n, it might seem that
the total time complexity is O(C|S| log |S|), but this is not the case: Random access for
binary insertion requires the whole data structure to be implemented by an array, which
requires copying of the whole SS at every insertion. Note that the sorting speed at this
point does not affect the usability of our application, since the text collection is sorted
offline (as explained in Section 5).
On the other hand, the searching speed does affect the usability. For sorted texts,
the readability of a text is assessed by searching for its insertion position. The searching
is done by binary search, through the same procedure in steps 2 to 6. The computational
complexity is O(C log |SS|), where C is the maximum value of M.
4. Pros and Cons of the Proposed Method
Now that we have explained our method, we compare it with the previous models of
regression and classification mentioned in Section 2. The comparison is summarized in
Table 1.
In previous work that we mentioned, the readability of a text is represented by a
score or class, which typically has been indicated in terms of school grades (third row).
In contrast, readability in our method is presented as a position among texts, indicating
the ranking of a text situated globally among the other texts of SS. Considering the
nature of the output of assessment, the regression method is continuous, in that feature
values are mapped to scores within a continuous range, whereas classification and our
method are both discrete, in that the former gives a class and the latter gives a rank
(fourth row).
This fundamental difference gives rise to pros and cons. Above all, the advantage
of our method is that it facilitates the construction of training data (fifth row), because it
requires only two sets of typically difficult and easy texts. Here, what kinds of corpora
the regression method requires in the modern machine learning context has not been
clarified because of the lack of previous work in machine learning regression. However,
our empirical results, shown in Section 8, suggest that the two sets of difficult and
easy training data will not be sufficient, and machine learning regression requires texts
labeled with scores for different levels.
Table 1
Qualitative comparison of our method with previous methods.
previous methods our method
model regression classification ordering
readability score class ranking
output continuous discrete discrete
required levels of training data multiple multiple two
comprehensiveness high high questionable
speed of assessment fast fast slow
211
Computational Linguistics Volume 36, Number 2
At the same time, there are disadvantages to our model. First, because the method
only outputs a relative ranking, applications must be re-designed differently from those
in the previous work (sixth row). It could be said that when a > b for documents a and
b, then a contains more unfamiliar words as tokens. Even if two texts are next to each
other, however, their readability could be very different. For example, texts of grades 1
and 11 could be next to each other in a collection if it lacks texts from grade 2 to 10. An
application system generated using our model must cope with this new problem caused
by this lack of absoluteness for the readability norm. In the next section, we show how
this problem is dealt with in Terrace through the use of graphical representation.
Second, the scores and classes of regression and classification are used to hash the
position of texts within the readability norm. On the other hand, because our method
lacks scores, the location of a text must always be searched for. Thus, the previous
methods require only O(1) time for searching, whereas our method requires a certain
amount of time before a response is obtained. Therefore, it must be verified that an ap-
plication works within a reasonable response time when handling a large text collection.
In Section 9, we show that the response time is indeed within a reasonable time.
5. Terrace: An Application
As mentioned in Section 1, creation of this system was motivated by a request from
faculty members who teach multiple foreign languages. These teachers must look for
up-to-date reading materials with appropriate reading levels every day. This requires
scanning through newspapers and Web resources. The teachers? request was that we
construct a system that would facilitate this text search task. More precisely, given a
sample text used within the classroom, the system should return texts of similar levels
of readability from among newspaper articles and other on-line archives.
At the beginning of this project, we tried to apply Schwarm and Ostendorf (2005)
and Collins-Thompson and Callan (2004). Because the request covered multiple lan-
guages, though, we faced the corpus construction problem in different languages. This
problem was serious, to the extent that training corpora used in published work in
English were unavailable. We were forced to look for another path towards building
the requested system without annotated corpora having academic grades.
The problem of returning texts of similar readability does not necessarily require
intermediating a score/class, because both the input and output are texts. Thus, chang-
ing our way of thinking, we looked for another readability norm, which led to the
framework presented so far.
Terrace is a Web-based system in which the user uploads a text, and the system
returns texts of similar readability from among a collection of texts usable as teaching
materials, which are crawled for and obtained from the Web. Currently, Terrace works
for English and Japanese. The number of languages is currently being increased.
Figure 4 shows an example of giving an input text to Terrace. After the user uploads
the text and clicks the Terrace search button, a page like that shown in Figure 5 is
displayed.
After showing the Terrace banner, the system presents the ranking of the text within
the text collection, which is crawled from the Web site. The number of texts in the
collection is 14,877 and each is taken from CNN (CNN 2008). The ranking in this
example is 8,174th from the easiest text. A horizontal bar is shown below, indicating
the location among the 14,877 texts, with triangular indicators showing the locations
of texts with annotation in terms of grades. These indicators are meant to help users
understand the location of the input text. They are generated by sorting texts with
212
Tanaka-Ishii, Tezuka, and Terada Sorting Texts by Readability
Figure 4
Screen shot of Terrace input.
Figure 5
Screen shot of Terrace result for the given input.
annotations together with the text collection. For example, the leftmost indicator G1
shows where a text with an annotation of grade one is located within the bar. Note that
such indicators are easily generated, because the number of annotated texts does not
need to be large.
213
Computational Linguistics Volume 36, Number 2
Figure 6
Terrace system.
The texts with the closest readability are shown below, and then easier texts are
listed. Further down, more difficult texts are listed. By clicking on any of these texts, the
user can obtain texts with the desired readability.
This functionality of Terrace is implemented via two modules, one each for search-
ing and for crawling, as shown in Figure 6. The crawler collects texts from news sites and
other related archive sites every day, and the module incrementally sorts the collected
documents. These texts are searched upon a user request.
6. Data for Evaluation
In the rest of this article, the proposed method and the Terrace system are evaluated.
The key question to be considered through the evaluation is whether the comparator
can discern slight differences in the readability levels of test data from only two sets of
training data that are roughly different.
The proposed method was tested for English and Japanese. The data is summarized
in Table 2, where the upper block corresponds to English and the lower to Japanese. For
Table 2
Training and test data.
English Data
label corpus levels # of Texts # of Words
Training & TD1-E Time normal 600 623,203
children 600 259,163
TD2-M-E AtoZ 27 levels (5 grades) 674 1,060,557
TD2-F-E English textbook 5 levels (linear) 153 114,054
Japanese Data
label corpus levels # of Texts # of Words
Training & TD1-J Asahi newspaper normal 600 841,289
children 600 533,568
TD2-M-J Japanese Textbook 6 levels (linear) 58 121,610
TD2-F-J Japanese Proficiency Test 4 levels 44 87,846
214
Tanaka-Ishii, Tezuka, and Terada Sorting Texts by Readability
both languages, there were training data and test data. The test data consisted of two
sorts:
TD1: A collection of texts taken from the same kind of data as the training data.
TD2: A collection of texts unrelated to the training data and originally assigned levels
or a linear ordering. These levels and ordering were used as the correct ordering
in our work.
TD2 further consisted of two kinds of data in each language: data for learners of their
mother tongue (i.e., children and students) and data for learners of a foreign language.
These are labeled as TD2-M and TD2-F, respectively.
For English, the training data were taken from Time (Time 2008) and Time For Kids
(TimeForKids 2008). We downloaded 600 articles (that is, |Ld| = |Le| = 600), of which 100
were used as TD1-E. The total number of different words in TD1-E is 22,736, which is
the dimension of the feature vector of a text when TD2 is used as the text data.3 When
using subtraction as ?, the dimension is doubled (for local and global), and when using
concatenation, the dimension is four times this value. TD2-M-E consisted of the data set
called AtoZ, which can be purchased (ReadingA-Z.com 2008). Each of the texts in this
data set is labeled by 27 levels and graded by 5 levels. TD2-F-E consisted of the English
textbooks used in Japanese junior high and high schools (Morizumi 2007; Yoneyama
2007). These texts are classified into five grades and also linearly ordered; that is, the
texts become more difficult in their order of appearance in the textbooks. These levels
and orders originally attached to the data were used as the gold standard in this study.
For the global frequency, we used the log frequency of each word as measured from
almost 6 terabytes of Web data in English, scanned in the autumn of 2006 (Tanaka-Ishii
and Terada 2009).
For Japanese, the training data and TD1-J were taken from Asahi newspapers
(AsahiNewspaper 2008; KodomoAsahi 2008). Six hundred (600) articles were acquired,
and 100 of these were used as TD1-J. The total number of different words in this training
data is 48,762.4 TD2-M-J consisted of Japanese junior high and high school textbooks
with six grades, which also appeared in a linear order (Miyaji 2008). TD2-F-J consisted
of the texts used in the Japanese language proficiency test (JEES 2008). The texts in this
data set are classified into four levels and not linearly ordered. For the global frequency,
we used the log frequency of each word as measured from almost 2 terabytes of Web
data in Japanese, scanned in the autumn of 2006.
The other evaluation settings were as follows. As the SVM (Joachims 1998, 1999), we
used LIBSVM (Chang and Lin 2001). The SVM training was done using the parameters
of cost = 0.1 and gamma = 0.00001 with a Gaussian kernel. The value of K used in robust
sorting (Section 3.2) was K = 2.
7. Evaluation of the Comparator
The basic performance was first tested using TD1. Because |Ld| = |Le| = 600, 500 texts
were chosen and paired randomly, and 2 ? 500 = 1,000 pairs were used for training. The
factor of 2 is necessary, because a pair can be used twice by exchanging the comparison
order Ved and Vde. The remaining 100 texts were randomly paired and tested. The results
3 Words are transformed into their standard forms using the Lancaster algorithm (Paice and Husk 2008).
Moreover, we do not use feature selection, because it has been reported that the distribution of function
words also counts with respect to readability (Collins-Thompson and Callan 2004).
4 Words are transformed into their standard forms using MeCab (Kudo 2009).
215
Computational Linguistics Volume 36, Number 2
Table 3
Accuracy of the comparator for different operators and features tested on TD1.
English
Concatenation 91.67%
Subtraction 94.27%
Japanese
Concatenation 95.47%
Subtraction 95.47%
presented here were produced through six-fold cross-validation, and each fold was
further repeated five times by changing the pairing of training and testing (i.e., total
execution was done 30 times to obtain one performance value).
Table 3 shows the accuracy for English (upper block) and Japanese (lower block).
The rows of a block represent the different operators explained in Section 3.1. Overall,
the scores were above 90%.
The comparator performance for TD2 was also investigated (Table 4). Here, the
training data amount was set to 2 ? 600. The accuracy was measured for all pairs of
texts with different levels. For TD2-M-E, all five levels were considered, whereas for
the linearly ordered TD2-F-E and TD2-M-J, the levels were considered by grade (that
is, five levels for TD2-F-E and six levels for TD2-M-J). The accuracy reported here is the
average of execution done five times by changing the random pairing for the training
data. The performance was, in general, lower than that for TD1, but still stayed close
to 90%. For the operator ?, whether concatenation or subtraction was used made no
difference. Therefore, from here on, the operator ? is set to concatenation.
To evaluate the classification performance in more detail, the accuracy for every
two-class combination was determined for TD2-M-E in English, as shown in Table 5.
Because TD2-M-E has five grades, the columns indicate the 1st to 4th grades, whereas
the rows indicate the 2nd to 5th grades. The evaluation thus forms a 4 ? 4 table, where
each cell indicates the accuracy of distinguishing the two?class pairs for the row and
column.
Table 4
Accuracy of the comparator (TD2).
English
Concatenation Subtraction
TD2-M-E 97.23 % 97.14 %
TD2-F-J 90.06 % 90.06 %
Japanese
Concatenation Subtraction
TD2-M-J 88.18 % 88.18 %
TD2-F-J 94.49 % 94.49 %
216
Tanaka-Ishii, Tezuka, and Terada Sorting Texts by Readability
Table 5
Classification results between two classes, tested for TD2-M-E in English.
Grade 4 Grade 3 Grade 2 Grade 1
Grade 5 84.28% 98.18% 100.00% 100.00%
Grade 4 ? 94.16% 99.97% 100.00%
Grade 3 ? ? 96.79% 100.00%
Grade 2 ? ? ? 99.01%
The closer to the diagonal, the more difficult the classification task was, because
the levels to be distinguished became closer to each other. The results reflect this ten-
dency, with lower values for cells closer to the diagonal. In particular, the performance
for discerning grades between pairs of the 4th and 5th grades was poor. Distinction
between the 1st/2nd and 2nd/3rd grades was more successful than that between the
3rd/4th and 4th/5th grades, since the lower the grades the easier it is to discern two
given successive school levels.
Before going on to actually sort text using the comparator, we verified how abnor-
mal our generated comparator was. Ideally, we want a complete ordering of the set,
and for this the comparator must obey certain laws in the sense of mathematical sets. A
comparator is considered abnormal if it does not obey two laws:
Reversibility: Texts a, b are defined as reversible if b < a and a > b both hold. This
corresponds to the law that when Vab?s value is +1, then Vba?s value must be ?1
and vice versa.
Transitivity: If a < b and b < c, then a < c.
Especially for transitivity, in an ordered set this law is the primary requirement that
must be fulfilled among ordered elements. If transitivity does not hold in many triples,
we have to introduce partial ordering instead of the total ordering considered thus far.
The anomalies were measured by using the four TD2 data sets. For all pairs and
triples of TD2, we tested the reversibility and transitivity for all possible pairs and triples
by changing the random pairing of test data (TD1) five times. For reversibility,
 10 pairs out of 226,801 for TD2-M-E were non-reversible once,
 1 pair out of 11,628 for TD2-F-E was non-reversible once, and
 all pairs for all other data and random pairings of training data were
reversible.
Such strong results were obtained because the training was done by reversing the
order of the pairs (thus, the SVM learned both Ved as +1 and Vde as ?1.) Similarly, for
transitivity,
 one triple out of 50,803,424 of TD2-M-E was non-transitive once, and
 all triples for all other data and random pairings of training data were
transitive.
Such results show how rarely these anomalies occur in our method. Therefore, our
choice of total ordering seems relevant.
217
Computational Linguistics Volume 36, Number 2
8. Evaluation of Sorting
Since the basic results have been clarified thus far, we will now report the results for
concatenation for a more global evaluation of sorting and searching.
The TD2 data were sorted by the method explained in Section 3.2, and the correla-
tion with the correct order was investigated. Three methods were used for comparison:
 Flesch?Kincaid
 Dale?Chall
 Support vector regression (SVR [Drucker et al 1996])
In these three methods, the readability level is obtained as a value, whereas our method
presents an order. Therefore, the results of the three methods were sorted according to
the values. The resulting orders for the three methods and for ours were then compared
with the correct order in the test data. We used the finest annotation for correct ordering;
for example, 27 levels for TD2-M-E, linear ordering for TD2-F-E, linear ordering for
TD2-M-J, and 4 levels for TD2-F-J.
Here, SVR was trained by labeling the Time/Asahi newspaper texts as +1.0 and
the texts of Time/Asahi for children as ?1.0, and then using the 600 texts for training.
The LIBSVM package was used with the same kernel and parameter settings given in
Section 6.
We used Spearman?s correlation to evaluate the ordering. Spearman?s basic correla-
tion formula is
rs = 1 ?
6
?n
i=1 d
2
i
n3 ? n
where n is the number of texts, and di is the difference in ranking between the correct
and obtained results for text i. This formula has an extended version to cope with
multiple elements having the same ranking. Given x and y as ordered sequences with
the same ranking, the correlation is given as follows:
rs =
Tx + Ty ?
?n
i=1 d
2
i
2
?
TxTy
where
Tx = (n3 ? n) ?
nx
?
i=1
(t3i ? ti), Ty = (n3 ? n) ?
ny
?
j=1
(t3j ? tj)
Here, nx and ny are the numbers of the rankings for equivalently ranked elements in
x and y, respectively, and ti, tj denote the number of elements with the same ranking
as elements which are indexed as i, j, respectively. For example, given an order x =
[1,2,3,3,4], nx = 1, because only 3 had the same ranking, ti = 2 for i = 1, because there
are two 3s.
The results are shown in Figure 7. The horizontal axis represents the four data sets
of TD2, and the vertical axis represents the correlation value. Note that for the Japanese
218
Tanaka-Ishii, Tezuka, and Terada Sorting Texts by Readability






Co
rre
la
tio
n
Test Data
TD2-M-E TD2-F-E TD2-M-J TD2-F-J
Flesch-Kincaid
Dale-Chall
SVR
Our Method
Figure 7
Correlation with the test data.
data, there are only two bars, because Flesch?Kincaid and Dale?Chall are inapplicable.
Moreover, note that the vertical heights are comparable within a data set but not among
data sets, because the number of levels for each data set is different.
Comparing within each block, our method performed better than any other, having
a correlation of more than 0.8 for all cases. Flesch?Kincaid performed quite well, having
a high correlation of more than 0.6 for the TD2-M-E data. The use of SVR was less
effective, with the correlation being lower than that with our method. This shows that
the performance of the regression method is limited, even with a machine learning
method, when training data for two levels only are available. In contrast, our method
shows that even with rough two-level training data, high correlation is achievable. Such
performance is enabled by comparison among texts even in the middle range between
difficult and easy texts.
How this performance compares to that of previous classification methods is diffi-
cult to say. Above all, our method cannot be fairly compared with previous classification
methods from the viewpoint of classification, because in order to transform our sorted
results into classes, we would have to give the number of texts in each class. Because
this information is not provided to the classification methods, the comparison would be
unfair, thus favoring our method. Therefore, the comparison must be made by means of
correlation. Another problem is that because we do not possess the training data used
in previous work, we could only test with what we have listed in Table 2.
Therefore, we compared the performance using TD2-E-M as training data. Because
the amount of training data was small, the number of classes considered here was five.
Slightly less than half of the data (67 texts) was taken from each of five different levels,
and the remaining texts (which differed in number at each level) were used for testing:
a sufficient amount of test data is needed, too, since our evaluation is done through
219
Computational Linguistics Volume 36, Number 2
correlation measured on sorted results. By exchanging the halves, the result reported
here is the average of two-fold cross-validation.
We compared three methods:
1. A classification method using part of TD2-E-M as training data.
2. The sorting method using the same part of TD2-E-M as training data.
3. The sorting method with TD1-E as training data.
The first classification method followed that of Schwarm and Ostendorf (2005). The
amount of training data used to build a classifier for each class was 67 for +1 and 4 ? (67)
for ?1. The parameters used for the SVM were the same as those used for our method.
For the second method, the training data consisted of 1,340 (2 ? 5C2 [pairs among 5
classes] ? 67 texts) random pairs of two successive levels. Each fold of two-fold cross-
validation was done five times by changing the pairs randomly. For the third method,
TD1-E was used as in the previous evaluation, but this time the verification was done
with five levels (whereas the first block in Figure 7 was evaluated with 27 levels). The
amount of training data was the same as TD1 (i.e., 1,200) and the experiment was done
five times by changing the pairs randomly.
The results are shown in Figure 8. Three bars are shown from first to third, cor-
responding to each of the three methods. For classification, the correlation was 0.925,
whereas the correlations of the second and third bars were 0.946, 0.941, respectively. Our
methods thus slightly outperformed the classification method. Moreover, the difference
between the second and third methods showed that two-level training data could
perform similarly to multiple-level training data. This shows the strength of our method











Co
rre
la
tio
n
Methods
SVM Our Method
    (AtoZ)
Our Method
    (Time)
Figure 8
Comparison of classification methods and our model on TD2-M-E.
220
Tanaka-Ishii, Tezuka, and Terada Sorting Texts by Readability
in that it can complement a limited amount of training data through relative comparison
within the set.
It is not our intention, though, to assert that our method is better than the classifica-
tion method based only on this experiment. Classification has a much better chance of
achieving better performance with large-scale training data, especially when features
are studied further. The point here is to show the potential of the sorting model,
especially when sufficient amounts of corpus data annotated in multiple classes are
unavailable.
Finally, we investigated the effect of the amount of training data as mentioned in
Section 3.1. Given a set of relatively difficult texts Ld and a set of easy texts Le, the
maximum number of training pairs will amount to 2 ? |Ld| ? |Le|, with |L| indicating
the number of elements in set L. Two effects related to the amount of data should
be considered. First, there is the effect of the absolute amount of data used; that is,
the amounts of |Ld| and |Le|. We let N = |Ld| = |Le| and measured the correlation shift
by changing N from 100 to 600. For each N, the number of training pairs was 2N,
constructed by randomly sampling N documents from Ld and Le and forming N pairs,
and then constructing Vde and Ved for each pair.
Second, the effect of combination should be considered; that is, the number of pairs
whose maximum number is 2 ? |Ld| ? |Le| for a given N. When N = 600, this amounts
to 720,000 training pairs, which is too large in terms of the time required for training.
Therefore, by fixing N = 100, we tested the learning effect for numbers of training pairs
up to 2 ? 100 ? {1, 5, 10, 50, 100}.
The results are shown in Figure 9, where the first graph shows the effect of the
amount of data and the second graph shows the effect of combination. The horizontal
axes represent the amount of training data (namely, 2N for the first graph and 2 ? 100 ?
{1, 5, 10, 50, 100} for the second graph), and the vertical axes represent the correlation.
In the second graph, the horizontal axis is in log scale. Each graph has four lines, each
corresponding to a subset of data from TD2. Every plot was obtained by averaging five
repetitions of the random pairing of learning data.
In the first graph, the increase in the amount of local data led to only a slight
increase in performance, which is almost invisible. In the second graph, the increase
in combination did not lead to higher performance, but rather to drastically decreased
performance for the English data. This decrease was especially prominent with TD2-F-
E, the English textbook for Japanese students. This must have been due to the different
natures of the training and test data. On the other hand, the texts of the Asahi newspaper
articles and TD2-{M,F}-J are controlled under a similar standard (in terms of vocabu-
lary, syntax, and so forth), which would account for the difference from the case in
English.
These results suggest that even if our method has the possibility of obtaining large
amounts of training data by combination, this would not lead to higher performance.
Moreover, the graphs suggest the importance of obtaining a sufficient amount of train-
ing data from two levels which match the target domain.
9. Evaluation of the Terrace System
9.1 Evaluation of Searching
The search performance using the algorithm presented in Section 3.2 was evaluated via
the average positional error when a text is searched. The search performance of our
221
Computational Linguistics Volume 36, Number 2






     
TD2-M-E
TD2-F-E
TD2-M-J
TD2-F-J
Co
rre
la
tio
n
Data Amount






     
Co
rre
la
tio
n
Data Amount
TD2-M-E
TD2-F-E
TD2-M-J
TD2-F-J
Figure 9
Effect of increasing the amount of data (left: corpus data amount; right: training data amount
increased by combination).
method was compared with that of the three methods presented at the beginning of
Section 8. Because the sorting performance already differed among the methods, we
evaluated the performance on correctly sorted texts. One text was removed from the
correctly sorted texts, its position was searched for with each method, and the difference
222
Tanaka-Ishii, Tezuka, and Terada Sorting Texts by Readability
between the resulting and correct positions was measured. This procedure was repeated
for all texts of TD2, and the average difference was obtained. The finest levels for TD2
were used as the correct annotation. The execution was again performed five times by
changing the random pairing within the training data.
The results are shown in Figure 10. The horizontal axis represents blocks of TD2
data, and the vertical axis represents the average locational error of searching divided
by the total number of levels of each data. Unlike the correlation figures seen so far, the
smaller the value of each result, the better the precision of the search results. As was the
case in Figure 7, the heights of bars are comparable within a data block, whereas bars
across blocks are not comparable because of the different numbers of levels.
Naturally, the results are reversed from those of Figure 7. Methods with higher
performance in sorting had smaller errors. Overall, our method had the smallest errors
among all methods.
Because Terrace is different from previous systems based on the absolute scores
of Flesch?Kincaid or Dale?Chall, it was presented to several language teachers who
originated the Terrace project (as mentioned in Section 5). They reacted positively to
Terrace, even though the system does not show any absolute readability scores. There
are two main reasons for the favorable response. First, as mentioned, because they
need texts rather than scores, teachers liked the fact that Terrace returns texts directly
without outputting the values. With previous systems, teachers themselves had to input
one candidate text after another to find one of an appropriate level. Second, for for-
eign language teachers, especially of languages other than English, scores are not well
standardized, and teachers are often puzzled by some readability scores. The teaching
levels do not necessarily correspond to standard school grades for natives. Therefore,
they prefer that a system outputs texts directly as in Terrace, because this means they






Av
er
ag
e 
Er
ro
r o
f S
ea
rc
h
TD2-M-E TD2-F-E TD2-M-J TD2-F-J
Test Data
Flesch-Kincaid
Dale-Chall
SVR
Our Method
Figure 10
Search comparison.
223
Computational Linguistics Volume 36, Number 2
do not have to interpret some score which is difficult to assess. Nevertheless, they
found that the graphical indicators are helpful for locating themselves among numerous
collections.
9.2 Response Time
In Section 4, qualitative analysis showed that a disadvantage of our model is the
greater computational complexity in readability assessment. This section reports on the
response time when using Terrace.
The response time of Terrace is determined by the addition of three factors:
 extraction of features from a text
 searching for the text?s position within a text collection
 system overhead time (server access, Web page generation, etc.)
The time needed for feature vector construction is linear with respect to the text length.
This process is conducted once before a search. The text?s position within the text
collection is then searched for. Because comparison is performed multiple times in a
search, this factor dominates the response time. As mentioned in Section 3.2, this time
is logarithmic with respect to the number of texts in a collection. The last factor is
constant.
We evaluated the response time for TD2-M-E, since we currently have a larger text
collection in English and TD2-M-E is a larger data set than TD-F-E. As noted in Section 5,
this collection amounts to 14,877 texts taken from CNN (CNN 2008). The input texts
were all texts of TD2-M-E. Because K = 2, which makes M = 2 ? K + 1 = 5 (e.g., the
width of comparison was 5), the total number of comparisons was about 70 for one
search. The computational environment was as follows: CPU: Intel(R) Core(TM)2 Quad
CPU Q9550 (2.83 GHz); main memory: 3.5 GB; OS: Debian GNU/Linux 4.0.
The results are plotted in Figure 11. Because the key factor for response time is
the length of a text, the horizontal axis represents the text length and the vertical axis
represents the response time. Each plot corresponds to one text in TD2-M-E. The longer
the text, the slower the response time will be. The plotted points are separated into
two clusters, with one at the upper right and the other just below. This disjunction
corresponds to texts which were judged as more difficult and those judged as easier,
respectively, at the first comparison within the binary search. Because the Terrace system
stores its feature vectors in a MySQL database, when the number of non-zero features
is large, access becomes slower, even when all vector dimensions are the same. For
texts judged as more difficult at the first comparison, the vector length in the string
(which is how it is implemented in MySQL) becomes longer, and the overall procedure
takes a longer time. In any case, for all texts the response time was less than a second.
Therefore, although our method does have a complexity disadvantage, the results of
this evaluation suggest this is not a serious problem.
10. Conclusion
We have described a new model of readability assessment that uses sorting. Our
approach makes it possible to assess the readability of texts in terms of relative ease
224
Tanaka-Ishii, Tezuka, and Terada Sorting Texts by Readability





Proceedings of the ACL 2010 Conference Short Papers, pages 189?193,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Tree-Based Deterministic Dependency Parsing
? An Application to Nivre?s Method ?
Kotaro Kitagawa Kumiko Tanaka-Ishii
Graduate School of Information Science and Technology,
The University of Tokyo
kitagawa@cl.ci.i.u-tokyo.ac.jp kumiko@i.u-tokyo.ac.jp
Abstract
Nivre?s method was improved by en-
hancing deterministic dependency parsing
through application of a tree-based model.
The model considers all words necessary
for selection of parsing actions by includ-
ing words in the form of trees. It chooses
the most probable head candidate from
among the trees and uses this candidate to
select a parsing action.
In an evaluation experiment using the
Penn Treebank (WSJ section), the pro-
posed model achieved higher accuracy
than did previous deterministic models.
Although the proposed model?s worst-case
time complexity is O(n2), the experimen-
tal results demonstrated an average pars-
ing time not much slower than O(n).
1 Introduction
Deterministic parsing methods achieve both effec-
tive time complexity and accuracy not far from
those of the most accurate methods. One such
deterministic method is Nivre?s method, an incre-
mental parsing method whose time complexity is
linear in the number of words (Nivre, 2003). Still,
deterministic methods can be improved. As a spe-
cific example, Nivre?s model greedily decides the
parsing action only from two words and their lo-
cally relational words, which can lead to errors.
In the field of Japanese dependency parsing,
Iwatate et al (2008) proposed a tournament model
that takes all head candidates into account in judg-
ing dependency relations. This method assumes
backward parsing because the Japanese depen-
dency structure has a head-final constraint, so that
any word?s head is located to its right.
Here, we propose a tree-based model, applica-
ble to any projective language, which can be con-
sidered as a kind of generalization of Iwatate?s
idea. Instead of selecting a parsing action for
two words, as in Nivre?s model, our tree-based
model first chooses the most probable head can-
didate from among the trees through a tournament
and then decides the parsing action between two
trees.
Global-optimization parsing methods are an-
other common approach (Eisner, 1996; McDon-
ald et al, 2005). Koo et al (2008) studied
semi-supervised learning with this approach. Hy-
brid systems have improved parsing by integrat-
ing outputs obtained from different parsing mod-
els (Zhang and Clark, 2008).
Our proposal can be situated among global-
optimization parsing methods as follows. The pro-
posed tree-based model is deterministic but takes a
step towards global optimization by widening the
search space to include all necessary words con-
nected by previously judged head-dependent rela-
tions, thus achieving a higher accuracy yet largely
retaining the speed of deterministic parsing.
2 Deterministic Dependency Parsing
2.1 Dependency Parsing
A dependency parser receives an input sentence
x = w1, w2, . . . , wn and computes a dependency
graph G = (W,A). The set of nodes W =
{w0, w1, . . . , wn} corresponds to the words of a
sentence, and the node w0 is the root of G. A is
the set of arcs (wi, wj), each of which represents a
dependency relation where wi is the head and wj
is the dependent.
In this paper, we assume that the resulting de-
pendency graph for a sentence is well-formed and
projective (Nivre, 2008). G is well-formed if and
only if it satisfies the following three conditions of
being single-headed, acyclic, and rooted.
2.2 Nivre?s Method
An incremental dependency parsing algorithm
was first proposed by (Covington, 2001). After
189
Table 1: Transitions for Nivre?s method and the proposed method.
Transition Precondition
Nivre?s
Method
Left-Arc (?|wi, wj |?,A) ? (?,wj |?,A ? {(wj , wi)}) i ?= 0 ? ??wk (wk, wi) ? A
Right-Arc (?|wi, wj |?,A) ? (?|wi|wj , ?, A ? {(wi, wj)})
Reduce (?|wi, ?, A) ? (?, ?,A) ?wk (wk, wi) ? A
Shift (?,wj |?,A) ? (?|wj , ?, A)
Proposed
Method
Left-Arc (?|ti, tj |?,A) ? (?, tj |?,A ? {(wj , wi)}) i ?= 0
Right-Arc (?|ti, tj |?,A) ? (?|ti, ?, A ? {(mphc(ti, tj), wj)})
Shift (?, tj |?,A) ? (?|tj , ?, A)
studies taking data-driven approaches, by (Kudo
and Matsumoto, 2002), (Yamada and Matsumoto,
2003), and (Nivre, 2003), the deterministic incre-
mental parser was generalized to a state transition
system in (Nivre, 2008).
Nivre?s method applying an arc-eager algorithm
works by using a stack of words denoted as ?, for
a buffer ? initially containing the sentence x. Pars-
ing is formulated as a quadruple (S, Ts, sinit, St),
where each component is defined as follows:
? S is a set of states, each of which is denoted
as (?, ?,A) ? S.
? Ts is a set of transitions, and each element of
Ts is a function ts : S ? S.
? sinit = ([w0], [w1, . . . , wn], ?) is the initial
state.
? St is a set of terminal states.
Syntactic analysis generates a sequence of optimal
transitions ts provided by an oracle o : S ? Ts,
applied to a target consisting of the stack?s top ele-
ment wi and the first element wj in the buffer. The
oracle is constructed as a classifier trained on tree-
bank data. Each transition is defined in the upper
block of Table 1 and explained as follows:
Left-Arc Make wj the head of wi and pop wi,
where wi is located at the stack top (denoted
as ?|wi), when the buffer head is wj (denoted
as wj |?).
Right-Arc Make wi the head of wj , and push wj .
Reduce Pop wi, located at the stack top.
Shift Push the word wj , located at the buffer head,
onto the stack top.
The method explained thus far has the following
drawbacks.
Locality of Parsing Action Selection
The dependency relations are greedily determined,
so when the transition Right-Arc adds a depen-
dency arc (wi, wj), a more probable head of wj
located in the stack is disregarded as a candidate.
Features Used for Selecting Reduce
The features used in (Nivre and Scholz, 2004) to
define a state transition are basically obtained from
the two target words wi and wj , and their related
words. These words are not sufficient to select Re-
duce, because this action means that wj has no de-
pendency relation with any word in the stack.
Preconditions
When the classifier selects a transition, the result-
ing graph satisfies well-formedness and projectiv-
ity only under the preconditions listed in Table 1.
Even though the parsing seems to be formulated as
a four-class classifier problem, it is in fact formed
of two types of three-class classifiers.
Solving these problems and selecting a more
suitable dependency relation requires a parser that
considers more global dependency relations.
3 Tree-Based Parsing Applied to Nivre?s
Method
3.1 Overall Procedure
Tree-based parsing uses trees as the procedural el-
ements instead of words. This allows enhance-
ment of previously proposed deterministic mod-
els such as (Covington, 2001; Yamada and Mat-
sumoto, 2003). In this paper, we show the applica-
tion of tree-based parsing to Nivre?s method. The
parser is formulated as a state transition system
(S, Ts, sinit, St), similarly to Nivre?s parser, but ?
and ? for a state s = (?, ?,A) ? S denote a stack
of trees and a buffer of trees, respectively. A tree
ti ? T is defined as the tree rooted by the word wi,
and the initial state is sinit = ([t0], [t1, . . . , tn], ?),
which is formed from the input sentence x.
The state transitions Ts are decided through the
following two steps.
1. Select the most probable head candidate
(MPHC): For the tree ti located at the stack
top, search for and select the MPHC for wj ,
which is the root word of tj located at the
buffer head. This procedure is denoted as a
190


	


	

 	 
 



	

 w j

Figure 1: Example of a tournament.
function mphc(ti, tj), and its details are ex-
plained in ?3.2.
2. Select a transition: Choose a transition,
by using an oracle, from among the follow-
ing three possibilities (explained in detail in
?3.3):
Left-Arc Make wj the head of wi and pop
ti, where ti is at the stack top (denoted
as ?|ti, with the tail being ?), when the
buffer head is tj (denoted as tj |?).
Right-Arc Make the MPHC the head of wj ,
and pop the MPHC.
Shift Push the tree tj located at the buffer
head onto the stack top.
These transitions correspond to three possibilities
for the relation between ti and tj : (1) a word of ti
is a dependent of a word of tj ; (2) a word of tj is a
dependent of a word of ti; or (3) the two trees are
not related.
The formulations of these transitions in the
lower block of Table 1 correspond to Nivre?s tran-
sitions of the same name, except that here a tran-
sition is applied to a tree. This enhancement from
words to trees allows removal of both the Reduce
transition and certain preconditions.
3.2 Selection of Most Probable Head
Candidate
By using mphc(ti, tj), a word located far from wj
(the head of tj) can be selected as the head can-
didate in ti. This selection process decreases the
number of errors resulting from greedy decision
considering only a few candidates.
Various procedures can be considered for im-
plementing mphc(ti, tj). One way is to apply the
tournament procedure to the words in ti. The tour-
nament procedure was originally introduced for
parsing methods in Japanese by (Iwatate et al,
 
	




		 


 	
ti t j
mphc ),( ji tt
	
 
	




		 


 	
ti t j
Figure 2: Example of the transition Right.
2008). Since the Japanese language has the head-
final property, the tournament model itself consti-
tutes parsing, whereas for parsing a general pro-
jective language, the tournament model can only
be used as part of a parsing algorithm.
Figure 1 shows a tournament for the example
of ?with,? where the word ?watched? finally wins.
Although only the words on the left-hand side of
tree tj are searched, this does not mean that the
tree-based method considers only one side of a de-
pendency relation. For example, when we apply
the tree-based parsing to Yamada?s method, the
search problems on both sides are solved.
To implement mphc(ti, tj), a binary classifier
is built to judge which of two given words is more
appropriate as the head for another input word.
This classifier concerns three words, namely, the
two words l (left) and r (right) in ti, whose ap-
propriateness as the head is compared for the de-
pendent wj . All word pairs of l and r in ti are
compared repeatedly in a ?tournament,? and the
survivor is regarded as the MPHC of wj .
The classifier is generated through learning of
training examples for all ti and wj pairs, each
of which generates examples comparing the true
head and other (inappropriate) heads in ti. Ta-
ble 2 lists the features used in the classifier. Here,
lex(X) and pos(X) mean the surface form and part
of speech of X , respectively. X left means the
dependents of X located on the left-hand side of
X , while Xright means those on the right. Also,
Xhead means the head of X . The feature design
concerns three additional words occurring after
wj , as well, denoted as wj+1, wj+2, wj+3.
3.3 Transition Selection
A transition is selected by a three-class classifier
after deciding the MPHC, as explained in ?3.1.
Table 1 lists the three transitions and one precon-
191
Table 2: Features used for a tournament.
pos(l), lex(l)
pos(lhead), pos(lleft), pos(lright)
pos(r), lex(r)
pos(rhead), pos(rleft), pos(rright)
pos(wj), lex(wj), pos(wleftj )
pos(wj+1), lex(wj+1), pos(wj+2), lex(wj+2)
pos(wj+3), lex(wj+3)
Table 3: Features used for a state transition.
pos(wi), lex(wi)
pos(wlefti ), pos(wrighti ), lex(wlefti ), lex(wrighti )
pos(MPHC), lex(MPHC)
pos(MPHChead), pos(MPHCleft), pos(MPHCright)
lex(MPHChead), lex(MPHCleft), lex(MPHCright)
pos(wj), lex(wj), pos(wleftj ), lex(wleftj )
pos(wj+1), lex(wj+1), pos(wj+2), lex(wj+2), pos(wj+3), lex(wj+3)
dition. The transition Shift indicates that the tar-
get trees ti and tj have no dependency relations.
The transition Right-Arc indicates generation of
the dependent-head relation between wj and the
result of mphc(ti, tj), i.e., the MPHC for wj . Fig-
ure 2 shows an example of this transition. The
transition Left-Arc indicates generation of the de-
pendency relation in which wj is the head of wi.
While Right-Arc requires searching for the MPHC
in ti, this is not the case for Left-Arc1.
The key to obtaining an accurate tree-based
parsing model is to extend the search space while
at the same time providing ways to narrow down
the space and find important information, such as
the MPHC, for proper judgment of transitions.
The three-class classifier is constructed as fol-
lows. The dependency relation between the target
trees is represented by the three words wi, MPHC,
and wj . Therefore, the features are designed to in-
corporate these words, their relational words, and
the three words next to wj . Table 3 lists the exact
set of features used in this work. Since this transi-
tion selection procedure presumes selection of the
MPHC, the result of mphc(ti, tj) is also incorpo-
rated among the features.
4 Evaluation
4.1 Data and Experimental Setting
In our experimental evaluation, we used Yamada?s
head rule to extract unlabeled dependencies from
the Wall Street Journal section of a Penn Treebank.
Sections 2-21 were used as the training data, and
section 23 was used as the test data. This test data
1The head word of wi can only be wj without searching
within tj , because the relations between the other words in tj
and wi have already been inferred from the decisions made
within previous transitions. If tj has a child wk that could
become the head of wi under projectivity, this wk must be
located between wi and wj . The fact that wk?s head is wj
means that there were two phases before ti and tj (i.e., wi
and wj) became the target:
? ti and tk became the target, and Shift was selected.
? tk and tj became the target, and Left-Arc was selected.
The first phase precisely indicates that wi and wk are unre-
lated.
was used in several other previous works, enabling
mutual comparison with the methods reported in
those works.
The SVMlight package2 was used to build the
support vector machine classifiers. The binary
classifier for MPHC selection and the three-class
classifier for transition selection were built using a
cubic polynomial kernel. The parsing speed was
evaluated on a Core2Duo (2.53 GHz) machine.
4.2 Parsing Accuracy
We measured the ratio of words assigned correct
heads to all words (accuracy), and the ratio of sen-
tences with completely correct dependency graphs
to all sentences (complete match). In the evalua-
tion, we consistently excluded punctuation marks.
Table 4 compares our results for the proposed
method with those reported in some previous
works using equivalent training and test data.
The first column lists the four previous methods
and our method, while the second through fourth
columns list the accuracy, complete match accu-
racy, and time complexity, respectively, for each
method. Here, we obtained the scores for the pre-
vious works from the corresponding articles listed
in the first column. Note that every method used
different features, which depend on the method.
The proposed method achieved higher accuracy
than did the previous deterministic models. Al-
though the accuracy of our method did not reach
that of (McDonald and Pereira, 2006), the scores
were competitive even though our method is de-
terministic. These results show the capability of
the tree-based approach in effectively extending
the search space.
4.3 Parsing Time
Such extension of the search space also concerns
the speed of the method. Here, we compare its
computational time with that of Nivre?s method.
We re-implemented Nivre?s method to use SVMs
with cubic polynomial kernel, similarly to our
2http://svmlight.joachims.org/
192
Table 4: Dependency parsing performance.
Accuracy Complete Time Global vs. Learning
match complexity deterministic method
McDonald & Pereira (2006) 91.5 42.1 O(n3) global MIRA
McDonald et al (2005) 90.9 37.5 O(n3) global MIRA
Yamada & Matsumoto (2003) 90.4 38.4 O(n2) deterministic support vector machine
Goldberg & Elhadad (2010) 89.7 37.5 O(n log n) deterministic structured perceptron
Nivre (2004) 87.1 30.4 O(n) deterministic memory based learning
Proposed method 91.3 41.7 O(n2) deterministic support vector machine
10 20 30 40 500
10
20
30
40
50
60
Nivre?s Method
length of input sentence
parsin
g time
  [sec]
10 20 30 40 500
10
20
30
40
50
60
Proposed Method
length of input sentence
parsin
g time
  [sec]
Figure 3: Parsing time for sentences.
method. Figure 3 shows plots of the parsing times
for all sentences in the test data. The average pars-
ing time for our method was 8.9 sec, whereas that
for Nivre?s method was 7.9 sec.
Although the worst-case time complexity for
Nivre?s method is O(n) and that for our method is
O(n2), worst-case situations (e.g., all words hav-
ing heads on their left) did not appear frequently.
This can be seen from the sparse appearance of the
upper bound in the second figure.
5 Conclusion
We have proposed a tree-based model that decides
head-dependency relations between trees instead
of between words. This extends the search space
to obtain the best head for a word within a deter-
ministic model. The tree-based idea is potentially
applicable to various previous parsing methods; in
this paper, we have applied it to enhance Nivre?s
method.
Our tree-based model outperformed various de-
terministic parsing methods reported previously.
Although the worst-case time complexity of our
method is O(n2), the average parsing time is not
much slower than O(n).
References
Xavier Carreras. 2007. Experiments with a higher-order
projective dependency parse. Proceedings of the CoNLL
Shared Task Session of EMNLP-CoNLL, pp. 957-961.
Michael A. Covington. 2001. A fundamental algorithm for
dependency parsing. Proceedings of ACM, pp. 95-102.
Jason M. Eisner. 1996. Three new probabilistic models
for dependency parsing: An exploration. Proceedings of
COLING, pp. 340-345.
Yoav Goldberg and Michael Elhadad. 2010. An Efficient Al-
gorithm for Easy-First Non-Directional Dependency Pars-
ing. Proceedings of NAACL.
Masakazu Iwatate, Masayuki Asahara, and Yuji Matsumoto.
2008. Japanese dependency parsing using a tournament
model. Proceedings of COLING, pp. 361?368.
Terry Koo, Xavier Carreras, and Michael Collins. 2008.
Simple semi-supervised dependency parsing. Proceed-
ings of ACL, pp. 595?603.
Taku Kudo and Yuji Matsumoto. 2002. Japanese depen-
dency analysis using cascaded chunking Proceedings of
CoNLL, pp. 63?69.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. Proceedings of ACL, pp. 91?98.
Ryan McDonald and Fernando Pereira. 2006. Online learn-
ing of approximate dependency parsing algorithms. Pro-
ceedings of the EACL, pp. 81?88.
Joakim Nivre. 2003. An efficient algorithm for projective
dependency parsing. Proceedings of IWPT, pp. 149?160.
Joakim Nivre. 2008. Algorithms for deterministic incremen-
tal dependency parsing. Computational Linguistics, vol.
34, num. 4, pp. 513?553.
Joakim Nivre and Mario Scholz. 2004. Deterministic depen-
dency parsing of English text. Proceedings of COLING,
pp. 64?70.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical
dependency analysis with support vector machines. Pro-
ceedings of IWPT, pp. 195?206.
Yue Zhang and Stephen Clark. 2008. A tale of two parsers:
investigating and combining graph-based and transition-
based dependency parsing using beamsearch. Proceed-
ings of EMNLP, pp. 562?571.
193
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 969?978,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Text Segmentation by Language Using Minimum Description Length
Hiroshi Yamaguchi
Graduate School of
Information Science and Technology,
University of Tokyo
yamaguchi.hiroshi@ci.i.u-tokyo.ac.jp
Kumiko Tanaka-Ishii
Faculty and Graduate School of Information
Science and Electrical Engineering,
Kyushu University
kumiko@ait.kyushu-u.ac.jp
Abstract
The problem addressed in this paper is to seg-
ment a given multilingual document into seg-
ments for each language and then identify the
language of each segment. The problem was
motivated by an attempt to collect a large
amount of linguistic data for non-major lan-
guages from the web. The problem is formu-
lated in terms of obtaining the minimum de-
scription length of a text, and the proposed so-
lution finds the segments and their languages
through dynamic programming. Empirical re-
sults demonstrating the potential of this ap-
proach are presented for experiments using
texts taken from the Universal Declaration of
Human Rights and Wikipedia, covering more
than 200 languages.
1 Introduction
For the purposes of this paper, a multilingual text
means one containing text segments, limited to those
longer than a clause, written in different languages.
We can often find such texts in linguistic resources
collected from the World Wide Web for many non-
major languages, which tend to also contain portions
of text in a major language. In automatic process-
ing of such multilingual texts, they must first be seg-
mented by language, and the language of each seg-
ment must be identified, since many state-of-the-art
NLP applications are built by learning a gold stan-
dard for one specific language. Moreover, segmen-
tation is useful for other objectives such as collecting
linguistic resources for non-major languages and au-
tomatically removing portions written in major lan-
guages, as noted above. The study reported here was
motivated by this objective. The problem addressed
in this article is thus to segment a multilingual text
by language and identify the language of each seg-
ment. In addition, for our objective, the set of target
languages consists of not only major languages but
also many non-major languages: more than 200 lan-
guages in total.
Previous work that directly concerns the problem
addressed in this paper is rare. The most similar
previous work that we know of comes from two
sources and can be summarized as follows. First,
(Teahan, 2000) attempted to segment multilingual
texts by using text segmentation methods used for
non-segmented languages. For this purpose, he used
a gold standard of multilingual texts annotated by
borders and languages. This segmentation approach
is similar to that of word segmentation for non-
segmented texts, and he tested it on six different
European languages. Although the problem set-
ting is similar to ours, the formulation and solution
are different, particularly in that our method uses
only a monolingual gold standard, not a multilin-
gual one as in Teahan?s study. Second, (Alex, 2005)
(Alex et al, 2007) solved the problem of detecting
words and phrases in languages other than the prin-
cipal language of a given text. They used statisti-
cal language modeling and heuristics to detect for-
eign words and tested the case of English embed-
ded in German texts. They also reported that such
processing would raise the performance of German
parsers. Here again, the problem setting is similar to
ours but not exactly the same, since the embedded
text portions were assumed to be words. Moreover,
the authors only tested for the specific language pair
of English embedded in German texts. In contrast,
our work considers more than 200 languages, and
the portions of embedded text are larger: up to the
paragraph level to accommodate the reality of mul-
tilingual texts. The extension of our work to address
the foreign word detection problem would be an in-
teresting future work.
From a broader view, the problem addressed in
this paper is further related to two genres of previ-
ous work. The first genre is text segmentation. Our
problem can be situated as a sub-problem from the
viewpoint of language change. A more common set-
ting in the NLP context is segmentation into seman-
tically coherent text portions, of which a represen-
tative method is text tiling as reported by (Hearst,
1997). There could be other possible bases for text
969
segmentation, and our study, in a way, could lead
to generalizing the problem. The second genre is
classification, and the specific problem of text clas-
sification by language has drawn substantial atten-
tion (Grefenstette, 1995) (Kruengkrai et al, 2005)
(Kikui, 1996). Current state-of-the-art solutions use
machine learning methods for languages with abun-
dant supervision, and the performance is usually
high enough for practical use. This article con-
cerns that problem together with segmentation but
has another particularity in aiming at classification
into a substantial number of categories, i.e., more
than 200 languages. This means that the amount of
training data has to remain small, so the methods
to be adopted must take this point into considera-
tion. Among works on text classification into lan-
guages, our proposal is based on previous studies us-
ing cross-entropy such as (Teahan, 2000) and (Juola,
1997). We explain these works in further detail in
?3.
This article presents one way to formulate the seg-
mentation and identification problem as a combina-
torial optimization problem; specifically, to find the
set of segments and their languages that minimizes
the description length of a given multilingual text. In
the following, we describe the problem formulation
and a solution to the problem, and then discuss the
performance of our method.
2 Problem Formulation
In our setting, we assume that a small amount (up
to kilobytes) of monolingual plain text sample data
is available for every language, e.g., the Universal
Declaration of Human Rights, which serves to gen-
erate the language model used for language identifi-
cation. This entails two sub-assumptions.
First, we assume that for all multilingual text,
every text portion is written in one of the given
languages; there is no input text of an unknown
language without learning data. In other words,
we use supervised learning. In line with recent
trends in unsupervised segmentation, the problem
of finding segments without supervision could be
solved through approaches such as Bayesian meth-
ods; however, we report our result for the supervised
setting since we believe that every segment must be
labeled by language to undergo further processing.
Second, we cannot assume a large amount of
learning data, since our objective requires us to con-
sider segmentation by both major and non-major
languages. For most non-major languages, only a
limited amount of corpus data is available.1
This constraint suggests the difficulty of applying
certain state-of the art machine learning methods re-
quiring a large learning corpus. Hence, our formu-
lation is based on the minimum description length
(MDL), which works with relatively small amounts
of learning data.
In this article, we use the following terms and
notations. A multilingual text to be segmented is
denoted as X = x1, . . . , x|X|, where xi denotes
the i-th character of X and |X| denotes the text?s
length. Text segmentation by language refers here
to the process of segmenting X by a set of borders
B = [B1, . . . , B|B|], where |B| denotes the num-
ber of borders, and each Bi indicates the location
of a language border as an offset number of charac-
ters from the beginning. Note that a pair of square
brackets indicates a list. Segmentation in this paper
is character-based, i.e., a Bi may refer to a position
inside a word. The list of segments obtained from
B is denoted as X = [X0, . . . , X|B|], where the con-
catenation of the segments equals X . The language
of each segment Xi is denoted as Li, where Li ? L,
the set of languages. Finally, L = [L0, . . . , L|B|]
denotes the sequence of languages corresponding to
each segmentXi. The elements in each adjacent pair
in L must be different.
We formulate the problem of segmenting a multi-
lingual text by language as follows. Given a multi-
lingual text X , the segments X for a list of borders
B are obtained with the corresponding languages L.
Then, the total description length is obtained by cal-
culating each description length of a segment Xi for
the language Li:
(X?, L?) = argmin
X,L
|B|
?
i=0
dlLi(Xi). (1)
The function dlLi(Xi) calculates the description
length of a text segment Xi through the use of a
language model for Li. Note that the actual total
description length must also include an additional
term, log2 |X|, giving information on the number
of segments (with the maximum to be segmented
1In fact, our first motivation was to collect a certain amount
of corpus data for non-major languages from Wikipedia.
970
by each character). Since this term is a common
constant for all possible segmentations and the min-
imization of formula (1) is not affected by this term,
we will ignore it.
The model defined by (1) is additive for Xi, so
the following formula can be applied to search for
language Li given a segment Xi :
L?i = argmin
Li?L
dlLi(Xi), (2)
under the constraint that Li 6= Li?1 for i ?
{1, . . . |B|}. The function dl can be further decom-
posed as follows to give the description length in an
information-theoretic manner:
dlLi(Xi) =? log2 PLi(Xi)
+ log2 |X|+ log2 |L|+ ?.
(3)
Here, the first term corresponds to the code length
of the text chunk Xi given a language model for
Li, which in fact corresponds to the cross-entropy
of Xi for Li multiplied by |Xi|. The remaining
terms give the code lengths of the parameters used
to describe the length of the first term: the second
term corresponds to the segment location; the third
term, to the identified language; and the fourth term,
to the language model of language Li. This fourth
term will differ according to the language model
type; moreover, its value can be further minimized
through formula (2). Nevertheless, since we use a
uniform amount of training data for every language,
and since varying ? would prevent us from improv-
ing the efficiency of dynamic programming, as ex-
plained in ?4, in this article we set ? to a constant
obtained empirically.
Under this formulation, therefore, when detect-
ing the language of a segment as in formula (2), the
terms of formula (3) other than the first term will be
constant: what counts is only the first term, simi-
larly to much of the previous work explained in the
following section. We thus perform language de-
tection itself by minimizing the cross-entropy rather
than the MDL. For segmentation, however, the con-
stant terms function as overhead and also serve to
prohibit excessive decomposition.
Next, after briefly introducing methods to calcu-
late the first term of formula (3), we explain the so-
lution to optimize the combinatorial problem of for-
mula (1).
3 Calculation of Cross-Entropy
The first term of (3), ? log2 PLi(Xi), is the cross-
entropy of Xi for Li multiplied by |Xi|. Vari-
ous methods for computing cross-entropy have been
proposed, and these can be roughly classified into
two types based on different methods of univer-
sal coding and the language model. For example,
(Benedetto et al, 2002) and (Cilibrasi and Vita?nyi,
2005) used the universal coding approach, whereas
(Teahan and Harper, 2001) and (Sibun and Reynar,
1996) were based on language modeling using PPM
and Kullback-Leibler divergence, respectively.
In this section, we briefly introduce two meth-
ods previously studied by (Juola, 1997) and (Teahan,
2000) as representative of the two types, and we fur-
ther explain a modification that we integrate into the
final optimization problem. We tested several other
coding methods, but they did not perform as well as
these two methods.
3.1 Mean of Matching Statistics
(Farach et al, 1994) proposed a method to esti-
mate the entropy, through a simplified version of the
LZ algorithm (Ziv and Lempel, 1977), as follows.
Given a text X = x1x2 . . . xixi+1 . . ., Leni is de-
fined as the longest match length for two substrings
x1x2 . . . xi and xi+1xi+2 . . .. In this article, we de-
fine the longest match for two strings A and B as the
shortest prefix of string B that is not a substring of
A. Letting the average of Leni be E [Len], Farach
proved that |E [Len]? log2 iH(X) | probabilistically con-
verges to zero as i ??, where H(X) indicates the
entropy of X . Then, H(X) is estimated as
H?(X) = log2 iE [Len] .
(Juola, 1997) applied this method to estimate the
cross-entropy of two given texts. For two strings
Y = y1y2 . . . y|Y | and X = x1x2 . . . x|X|, let
Leni(Y ) be the match length starting from xi of X
for Y 2. Based on this formulation, the cross-entropy
is approximately estimated as
J?Y (X) =
log2 |Y |
E [Leni(Y )]
.
2This is called a matching statistics value, which explains
the subsection title.
971
Since formula (1) of ?2 is based on adding the
description length, it is important that the whole
value be additive to enable efficient optimization (as
will be explained in ?4). We thus modified Juola?s
method as follows to make the length additive:
J? ?Y (X) = E
[
log2 |Y |
Leni(Y )
]
.
Although there is no mathematical guarantee that
J?Y (X) or J? ?Y (X) actually converges to the cross-
entropy, our empirical tests showed a good estimate
for both cases3. In this article, we use J? ?Y (X) as
a function to obtain the cross-entropy and for multi-
plication by |X| in formula (3).
3.2 PPM
As a representative method for calculating the
cross-entropy through statistical language model-
ing, we adopt prediction by partial matching (PPM),
a language-based encoding method devised by
(Cleary and Witten, 1984). It has the particular char-
acteristic of using a variable n-gram length, unlike
ordinary n-gram models4. It models the probability
of a text X with a learning corpus Y as follows:
PY (X) = PY (x1 . . . x|X|)
=
|X|
?
t=1
PY (xt|xt?1 . . . xmax(1,t?n)),
where n is a parameter of PPM, denoting the max-
imum length of the n-grams considered in the
model5. The probability PY (X) is estimated by es-
cape probabilities favoring the longer sequences ap-
pearing in the learning corpus (Bell et al, 1990).
The total code length of X is then estimated as
? logPY (X). Since this value is additive and gives
the total code length of X for language Y , we adopt
this value in our approach.
4 Segmentation by Dynamic Programming
By applying the above methods, we propose a solu-
tion to formula (1) through dynamic programming.
3This modification means that the original J?Y (X) is ob-
tained through the harmonic mean, with Len obtained
through the arithmetic mean, whereas J? ?Y (X) is obtained
through the arithmetic mean with Len as the harmonic
mean.
4In the context of NLP, this is known as Witten-Bell smooth-
ing.
5In the experiments reported here, n is set to 5 throughout.
Considering the additive characteristic of the de-
scription length formulated previously as formula
(1), we denote the minimized description length for
a given text X simply as DP(X), which can be de-
composed recursively as follows6:
DP(X) = min
t?{0,...,|X|},L?L
{DP(x0 . . . xt?1)
+ dlL(xt . . . x|X|)},
(4)
In other words, the computation of DP(X) is de-
composed into obtaining the addition of two terms
by searching through t ? {0, . . . , |X|} and L ? L.
The first term gives theMDL for the first t characters
of textX , while the second term, dlL(xt+1 . . . x|X|),
gives the description length of the remaining charac-
ters under the language model for L.
We can straightforwardly implement this recur-
sive computation through dynamic programming, by
managing a table of size |X| ? |L|. To fill a cell of
this table, formula (4) suggests referring to t ? |L|
cells and calculating the description length of the
rest of the text forO(|X|?t) cells for each language.
Since t ranges up to |X|, the brute-force computa-
tional complexity is O(|X|3 ? |L|2).
The complexity can be greatly reduced, however,
when the function dl is additive. First, the de-
scription length can be calculated from the previ-
ous result, decreasing O(|X| ? t) to O(1) (to ob-
tain the code length of an additional character). Sec-
ond, the referred number of cells t ? |L| is in fact
U ? |L|, with U  |X|: for MMS, U can be
proven to be O(log |Y |), where |Y | is the maximum
length among the learning corpora; and for PPM, U
corresponds to the maximum length of an n-gram.
Third, this factor U ? |L| can be further decreased
to U ? 2, since it suffices to possess the results for
the two7 best languages in computing the first term
of (4). Consequently, the complexity decreases to
O(U ? |X| ? |L|).
6This formula can be used directly to generate a set L in
which all adjacent elements differ. The formula can also be
used to generate segments for which some adjacent lan-
guages coincide and then further to generate L through
post-processing by concatenating segments of the same
language.
7This number means the two best scores for different lan-
guages, which is required to obtain L directly: in addition
to the best score, if the language of the best coincides with
L in formula (4), then the second best is also needed. If
segments are subjected to post-processing, this value can
be one.
972
Table 1: Number of languages for each writing system
character kinds UDHR Wiki
Latin 260 158
Cyrillic 12 20
Devanagari 0 8
Arabic 1 6
Other 4 30
5 Experimental Setting
5.1 Monolingual Texts (Training / Test Data)
In this work, monolingual texts were used both for
training the cross-entropy computation and as test
data for cross-validation: the training data does not
contain any test data at all. Monolingual texts were
also used to build multilingual texts, as explained in
the following subsection.
Texts were collected from the World Wide Web
and consisted of two sets. The first data set con-
sisted of texts from the Universal Declaration of
Human Rights (UDHR)8. We consider UDHR the
most suitable text source for our purpose, since the
content of every monolingual text in the declaration
is unique. Moreover, each text has the tendency
to maximally use its own language and avoid vo-
cabulary from other languages. Therefore, UDHR-
derived results can be considered to provide an em-
pirical upper bound on our formulation. The set L
consists of 277 languages , and the texts consist of
around 10,000 characters on average.
The second data set was Wikipedia data from
Wikipedia Downloads9, denoted as ?Wiki? in the
following discussion. We automatically assembled
the data through the following steps. First, tags in
the Wikipedia texts were removed. Second, short
lines were removed since they typically are not sen-
tences. Third, the amount of data was set to 10,000
characters for every language, in correspondence
with the size of the UDHR texts. Note that there
is a limit to the complete cleansing of data. After
these steps, the set L contained 222 languages with
sufficient data for the experiments.
Many languages adopt writing systems other than
the Latin alphabet. The numbers of languages for
various representative writing systems are listed in
Table 1 for both UDHR and Wiki, while the Ap-
8
http://www.ohchr.org/EN/UDHR/Pages/Introduction.aspx
9
http://download.wikimedia.org/
pendix at the end of the article lists the actual lan-
guages. Note that in this article, a character means
a Unicode character throughout, which differs from
a character rendered in block form for some writing
systems.
To evaluate language identification for monolin-
gual texts, as will be reported in ?6.1, we conducted
five-times cross-validation separately for both data
sets. We present the results in terms of the average
accuracy AL, the ratio of the number of texts with a
correctly identified language to |L|.
5.2 Multilingual Texts (Test Data)
Multilingual texts were needed only to test the per-
formance of the proposed method. In other words,
we trained the model only through monolingual
data, as mentioned above. This differs from the
most similar previous study (Teahan, 2000), which
required multilingual learning data.
The multilingual texts were generated artificially,
since multilingual texts taken directly from the web
have other issues besides segmentation. First, proper
nouns in multilingual texts complicate the final judg-
ment of language and segment borders. In prac-
tical application, therefore, texts for segmentation
must be preprocessed by named entity recognition,
which is beyond the scope of this work. Second, the
sizes of text portions in multilingual web texts dif-
fer greatly, which would make it difficult to evaluate
the overall performance of the proposed method in a
uniform manner.
Consequently, we artificially generated two kinds
of test sets from a monolingual corpus. The first is
a set of multilingual texts, denoted as Test1, such
that each text is the conjunction of two portions in
different languages. Here, the experiment is focused
on segment border detection, which must segment
the text into two parts, provided that there are two
languages. Test1 includes test data for all language
pairs, obtained by five-times cross-validation, giving
25?|L|? (|L|?1) multilingual texts. Each portion
of text for a single language consists of 100 char-
acters taken from a random location within the test
data.
The second kind of test set is a set of multilingual
texts, denoted as Test2, each consisting of k seg-
ments in different languages. For the experiment, k
is not given to the procedure, and the task is to ob-
tain k as well as B and L through recursion. Test2
973
was generated through the following steps:
1. Choose k from among 1,. . . ,5.
2. Choose k languages randomly from L, where
some of the k languages can overlap.
3. Perform five-times cross-validation on the texts
of all languages. Choose a text length ran-
domly from {40,80,120,160}, and randomly
select this many characters from the test data.
4. Shuffle the k languages and concatenate the
text portions in the resultant order.
For this Test2 data set, every plot in the graphs
shown in ?6.2 was obtained by randomly averaging
1,000 tests.
By default, the possibility of segmentation is con-
sidered at every character offset in a text, which
provides a lower bound for the proposed method.
Although language change within the middle of a
word does occur in real multilingual documents,
it might seem more realistic to consider language
change at word borders. Therefore, in addition to
choosing B from {1, . . . , |X|}, we also tested our
approach under the constraint of choosing borders
from bordering locations, which are the locations of
spaces. In this case, B is chosen from this subset of
{1, . . . , |X|}, and, in step 3 above, text portions are
generated so as to end at these bordering locations.
Given a multilingual text, we evaluate the outputs
B and L through the following scores:
PB/RB: Precision/recall of the borders detected
(i.e., the correct borders detected, divided by
the detected/correct border).
PL/RL: Precision/recall of the languages detected
(i.e., the correct languages detected, divided by
the detected/correct language).
P s and Rs are obtained by changing the param-
eter ? given in formula (3), which ranges over
1,2,4,. . . ,256 bits. In addition, we verify the speed,
i.e., the average time required for processing a text.
Although there are web pages consisting of texts
in more than 2 languages, we rarely see a web page
containing 5 languages at the same time. There-
fore, Test1 reflects the most important case of 2 lan-
guages only, whereas Test2 reflects the case of mul-
tiple languages to demonstrate the general potential
of the proposed approach.
The experiment reported here might seem like a
case of over-specification, since all languages are
considered equally likely to appear. Since our mo-
tivation has been to eliminate a portion in a major
 0.7
 0.75
 0.8
 0.85
 0.9
 0.95
 1
 0  20  40  60  80  100  120  140  160  180  200
ac
cu
ra
cy
input length (characters)
PPM (UDHR)
MMS (UDHR)
PPM (Wiki)
MMS (Wiki)
Figure 1: Accuracy of language identification for mono-
lingual texts
language from the text, there could be a formula-
tion specific to the problem. We consider it trivial,
however, to specify such a narrow problem within
our formulation, and it will lead to higher perfor-
mance than that of the reported results, in any case.
Therefore, we believe that our general formulation
and experiment show the broadest potential of our
approach to solving this problem.
6 Experimental Results
6.1 Language Identification Performance
We first show the performance of language identifi-
cation using formula (2), which is used as the com-
ponent of the text segmentation by language. Fig-
ure 1 shows the results for language identification
of monolingual texts with the UDHR and Wiki test
data. The horizontal axis indicates the size of the in-
put text in characters, the vertical axis indicates the
accuracy AL, and the graph contains four plots10 for
MMS and PPM for each set of data.
Overall, all plots rise quickly despite the se-
vere conditions of a large number of languages
(over 200), a small amount of input data, and a
small amount of learning data. The results show
that language identification through cross-entropy is
promising.
Two further global tendencies can be seen. First,
the performance was higher for UDHR than for
Wiki. This is natural, since the content of Wikipedia
is far broader than that of UDHR. In the case of
UDHR, when the test data had a length of 40 char-
acters, the accuracy was over 95% for both the PPM
and the MMS methods. Second, PPM achieved
10The results for PPM and MMS for UDHR are almost the
same, so the graph appears to contain only three plots.
974
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
-5 -4 -3 -2 -1  0  1  2  3  4  5
cu
m
m
ula
tiv
e 
pr
op
or
tio
n
relative position (characters)
PPM (UDHR)
MMS (UDHR)
PPM (Wiki)
MMS (Wiki)
Figure 2: Cumulative distribution of segment borders
slightly better performance than did MMS. When
the test data amounted to 100 characters, PPM
achieved language identification with accuracy of
about 91.4%. For MMS, the identification accu-
racy was a little less significant and was about 90.9%
even with 100 characters of test data.
The amount of learning data seemed sufficient for
both cases, with around 8,000 characters. In fact,
we conducted tests with larger amounts of learning
data and found a faster rise with respect to the input
length, but the maximum possible accuracy did not
show any significant increase.
Errors resulted from either noise or mistakes due
to the language family. The Wikipedia test data was
noisy, as mentioned in ?5.1. As for language fam-
ily errors, the test data includes many similar lan-
guages that are difficult even for humans to correctly
judge. For example, Indonesian and Malay, Picard
and Walloon, and Norwegian Bokma?l and Nynorsk
are all pairs representative of such confusion.
Overall, the language identification performance
seems sufficient to justify its application to our main
problem of text segmentation by language.
6.2 Text Segmentation by Language
First, we report the results obtained using the Test1
data set. Figure 2 shows the cumulative distribution
obtained for segment border detection. The horizon-
tal axis indicates the relative location by character
with respect to the correct border at zero, and the
vertical axis indicates the cumulative proportion of
texts whose border is detected at that relative point.
The figure shows four plots for all combinations of
the two data sets and the two methods. Note that
segment borders are judged by characters and not
by bordering locations, as explained in ?5.2.
 0.8
 0.85
 0.9
 0.95
 1
 0.8  0.85  0.9  0.95  1
pr
ec
isi
on
recall
0.98
0.97
0.88
0.87
PPM (UDHR)
MMS (UDHR)
PPM (Wiki)
MMS (Wiki)
 0.6
 0.65
 0.7
 0.75
 0.8
 0.6  0.65  0.7  0.75  0.8
pr
ec
isi
on
recall
0.77
0.76
0.70
0.68
PPM (UDHR)
MMS (UDHR)
PPM (Wiki)
MMS (Wiki)
Figure 3: PL/RL (language, upper graph) and PB/RB
(border, lower graph) results, where borders were taken
from any character offset
Since the plots rise sharply at the middle of the
horizontal axis, the borders were detected at or very
near the correct place in many cases.
Next, we examine the results for Test2. Fig-
ure 3 shows the two precision/recall graphs for lan-
guage identification (upper graph) and segment bor-
der detection (lower graph), where borders were
taken from any character offset. In each graph,
the horizontal axis indicates precision and the ver-
tical axis indicates recall. The numbers appearing
in each figure are the maximum F-score values for
each method and data set combination. As can be
seen from these numbers, the language identifica-
tion performance was high. Since the text portion
size was chosen from among the values 40, 80, 120,
or 160, the performance is comprehensible from the
results shown in ?6.1. Note also that PPM performed
slightly better than did MMS.
For segment border performance (lower graph),
however, the results were limited. The main reason
for this is that both MMS and PPM tend to detect
a border one character earlier than the correct loca-
tion, as was seen in Figure 2. At the same time,
much of the test data contains unrealistic borders
975
 0.7
 0.75
 0.8
 0.85
 0.9
 0.95
 1
 0.7  0.75  0.8  0.85  0.9  0.95  1
pr
ec
isi
on
recall
0.94
0.91
0.84
0.81
PPM (UDHR)
MMS (UDHR)
PPM (Wiki)
MMS (Wiki)
Figure 4: PB/RB, where borders were limited to spaces
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  200  400  600  800  1000
tim
e (s
)
input length (characters)
PPM (UDHR)
MMS (UDHR)
PPM (Wiki)
MMS (Wiki)
Figure 5: Average processing speed for a text
within a word, since the data was generated by con-
catenating two text portions with random borders.
Therefore, we repeated the experiment with Test2
under the constraint that a segment border could oc-
cur only at a bordering location, as explained in ?5.2.
The results with this constraint were significantly
better, as shown in Figure 4. The best result was for
UDHR with PPM at 0.9411. We could also observe
how PPM performed better at detecting borders in
this case. In actual application, it would be possible
to improve performance by relaxing the procedural
conditions, such as by decreasing the number of lan-
guage possibilities.
In this experiment for Test2, k ranged from 1 to
5, but the performance was not affected by the size
of k. When the F-score was examined with respect
to k, it remained almost equal to k in all cases. This
shows how each recursion of formula (4) works al-
most independently, having segmentation and lan-
guage identification functions that are both robust.
Lastly, we examine the speed of our method.
Since |L| is constant throughout the comparison,
11The language identification accuracy slightly increased as
well, by 0.002.
the time should increase linearly with respect to the
input length |X|, with increasing k having no ef-
fect. Figure 5 shows the speed for Test2 processing,
with the horizontal axis indicating the input length
and the vertical axis indicating the processing time.
Here, all character offsets were taken into consid-
eration, and the processing was done on a machine
with a Xeon5650 2.66-GHz CPU. The results con-
firm that the complexity increased linearly with re-
spect to the input length. When the text size became
as large as several thousand characters, the process-
ing time became as long as a second. This time
could be significantly decreased by introducing con-
straints on the bordering locations and languages.
7 Conclusion
This article has presented a method for segmenting
a multilingual text into segments, each in a differ-
ent language. This task could serve for preprocess-
ing of multilingual texts before applying language-
specific analysis to each text. Moreover, the pro-
posed method could be used to generate corpora in a
variety of languages, since many texts in minor lan-
guages tend to contain chunks in a major language.
The segmentation task was modeled as an opti-
mization problem of finding the best segment and
language sequences to minimize the description
length of a given text. An actual procedure for ob-
taining an optimal result through dynamic program-
ming was proposed. Furthermore, we showed a way
to decrease the computational complexity substan-
tially, with each of our two methods having linear
complexity in the input length.
Various empirical results were shown for lan-
guage identification and segmentation. Overall,
when segmenting a text with up to five random por-
tions of different languages, where each portion con-
sisted of 40 to 120 characters, the best F-scores for
language identification and segmentation were 0.98
and 0.94, respectively.
For our future work, details of the methods must
be worked out. In general, the proposed approach
could be further applied to the actual needs of pre-
processing and to generating corpora of minor lan-
guages.
976
References
Beatrice Alex, Amit Dubey, and Frank Keller. 2007.
Using foreign inclusion detection to improve parsing
performance. In Proceedings of the Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
pages 151?160.
Beatrice Alex. 2005. An unsupervised system for iden-
tifying english inclusions in german text. In Pro-
ceedings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics, Student Research
Workshop, pages 133?138.
T.C. Bell, J.G. Cleary, and I. H. Witten. 1990. Text Com-
pression. Prentice Hall.
Dario Benedetto, Emanuele Caglioti, and Vittorio Loreto.
2002. Language trees and zipping. Physical Review
Letters, 88(4).
Rudi Cilibrasi and Paul Vita?nyi. 2005. Clustering by
compression. IEEE Transactions on Information The-
ory, 51(4):1523?1545.
John G. Cleary and Ian H. Witten. 1984. Data compres-
sion using adaptive coding and partial string matching.
IEEE Transactions on Communications, 32:396?402.
Martin Farach, Michiel Noordewier, Serap Savari, Larry
Shepp, Abraham J. Wyner, and Jacob Ziv. 1994. On
the entropy of dna: Algorithms and measurements
based on memory and rapid convergence. In Proceed-
ings of the Sixth Annual ACM-SIAM Symposium on
Discrete Algorithms, pages 48?57.
Gregory Grefenstette. 1995. Comparing two language
identification schemes. In Proceedings of 3rd Inter-
national Conference on Statistical Analysis of Textual
Data, pages 263?268.
Marti A. Hearst. 1997. Texttiling: Segmenting text into
multi-paragraph subtopic passages. Computational
Linguistics, 23(1):33?64.
Patrick Juola. 1997. What can we do with small cor-
pora? document categorization via cross-entropy. In
Proceedings of an Interdisciplinary Workshop on Sim-
ilarity and Categorization.
Gen-itiro Kikui. 1996. Identifying the coding system and
language of on-line documents on the internet. In Pro-
ceedings of 16th International Conference on Compu-
tational Linguistics, pages 652?657.
Casanai Kruengkrai, Prapass Srichaivattana, Virach
Sornlertlamvanich, and Hitoshi Isahara. 2005. Lan-
guage identification based on string kernels. In
Proceedings of the 5th International Symposium on
Communications and Information Technologies, pages
926?929.
Penelope Sibun and Jeffrey C. Reynar. 1996. Language
identification: Examining the issues. In Proceedings
of 5th Symposium on Document Analysis and Infor-
mation Retrieval, pages 125?135.
William J. Teahan and David J. Harper. 2001. Using
compression-based language models for text catego-
rization. In Proceedings of the Workshop on Language
Modeling and Information Retrieval, pages 83?88.
William John Teahan. 2000. Text classification and seg-
mentation using minimum cross-entropy. In RIAO,
pages 943?961.
Jacob Ziv and Abraham Lempel. 1977. A universal al-
gorithm for sequential data compression. IEEE Trans-
actions on Information Theory, 23(3):337?343.
Appendix
This Appendix lists all the languages contained in our data sets,
as summarized in Table 1.
For UDHR
Latin
Achinese, Achuar-Shiwiar, Adangme, Afrikaans, Aguaruna,
Aja, Akuapem Akan, Akurio, Amahuaca, Amarakaeri, Ambo-
Pasco Quechua, Arabela, Arequipa-La Unio?n Quechua, Arpi-
tan, Asante Akan, Asha?ninka, Ashe?ninka Pajonal, Asturian,
Auvergnat Occitan, Ayacucho Quechua, Aymara, Baatonum,
Balinese, Bambara, Baoule?, Basque, Bemba, Beti, Bikol, Bini,
Bislama, Bokma?l Norwegian, Bora, Bosnian, Breton, Buginese,
Cajamarca Quechua, Caldero?n Highland Quichua, Candoshi-
Shapra, Caquinte, Cashibo-Cacataibo, Cashinahua, Catalan,
Cebuano, Central Kanuri, Central Mazahua, Central Nahuatl,
Chamorro, Chamula Tzotzil, Chayahuita, Chickasaw, Chiga,
Chokwe, Chuanqiandian Cluster Miao, Chuukese, Corsican,
Cusco Quechua, Czech, Dagbani, Danish, Dendi, Ditammari,
Dutch, Eastern Maninkakan, Emiliano-Romagnolo, English,
Esperanto, Estonian, Ewe, Falam Chin, Fanti, Faroese, Fi-
jian, Filipino, Finnish, Fon, French, Friulian, Ga, Gagauz,
Galician, Ganda, Garifuna, Gen, German, Gheg Albanian,
Gonja, Guarani, Gu?ila? Zapotec, Haitian Creole, Haitian Cre-
ole (popular), Haka Chin, Hani, Hausa, Hawaiian, Hiligaynon,
Huamal??es-Dos de Mayo Hua?nuco Quechua, Huautla Maza-
tec, Huaylas Ancash Quechua, Hungarian, Ibibio, Icelandic,
Ido, Igbo, Iloko, Indonesian, Interlingua, Irish, Italian, Ja-
vanese, Jola-Fonyi, K?iche?, Kabiye`, Kabuverdianu, Kalaal-
lisut, Kaonde, Kaqchikel, Kasem, Kekch??, Kimbundu, Kin-
yarwanda, Kituba, Konzo, Kpelle, Krio, Kurdish, Lamnso?,
Languedocien Occitan, Latin, Latvian, Lingala, Lithuanian,
Lozi, Luba-Lulua, Lunda, Luvale, Luxembourgish, Madurese,
Makhuwa, Makonde, Malagasy, Maltese, Mam, Maori,
Mapudungun, Margos-Yarowilca-Lauricocha Quechua, Mar-
shallese, Mba, Mende, Metlato?noc Mixtec, Mezquital Otomi,
Mi?kmaq, Miahuatla?n Zapotec, Minangkabau, Mossi, Mozara-
bic, Murui Huitoto, M??skito, Ndonga, Nigerian Pidgin, Nomat-
siguenga, North Jun??n Quechua, Northeastern Dinka, Northern
Conchucos Ancash Quechua, Northern Qiandong Miao, North-
ern Sami, Northern Kurdish, Nyamwezi, Nyanja, Nyemba,
Nynorsk Norwegian, Nzima, Ojitla?an Chinantec, Oromo,
Palauan, Pampanga, Papantla Totonac, Pedi, Picard, Pichis
Ashe?ninka, Pijin, Pipil, Pohnpeian, Polish, Portuguese, Pu-
laar, Purepecha, Pa?ez, Quechua, Rarotongan, Romanian, Ro-
mansh, Romany, Rundi, Salinan, Samoan, San Lu??s Potos??
Huastec, Sango, Sardinian, Scots, Scottish Gaelic, Serbian,
977
Serer, Seselwa Creole French, Sharanahua, Shipibo-Conibo,
Shona, Slovak, Somali, Soninke, South Ndebele, Southern
Dagaare, Southern Qiandong Miao, Southern Sotho, Spanish,
Standard Malay, Sukuma, Sundanese, Susu, Swahili, Swati,
Swedish, Sa?otomense, Tahitian, Tedim Chin, Tetum, Tidikelt
Tamazight, Timne, Tiv, Toba, Tojolabal, Tok Pisin, Tonga
(Tonga Islands), Tonga (Zambia), Tsonga, Tswana, Turkish,
Tzeltal, Umbundu, Upper Sorbian, Urarina, Uzbek, Veracruz
Huastec, Vili, Vlax Romani, Walloon, Waray, Wayuu, Welsh,
Western Frisian, Wolof, Xhosa, Yagua, Yanesha?, Yao, Yapese,
Yoruba, Yucateco, Zhuang, Zulu
Cyrillic
Abkhazian, Belarusian, Bosnian, Bulgarian, Kazakh, Mace-
donian, Ossetian, Russian, Serbian, Tuvinian, Ukrainian, Yakut
Arabic
Standard Arabic
Other
Japanese, Korean, Mandarin Chinese, Modern Greek
For Wiki
Latin
Afrikaans, Albanian, Aragonese, Aromanian, Arpitan, As-
turian, Aymara, Azerbaijani, Bambara, Banyumasan, Basque,
Bavarian, Bislama, Bosnian, Breton, Catala`, Cebuano, Central
Bikol, Chavacano, Cornish, Corsican, Crimean Tatar, Croatian,
Czech, Danish, Dimli, Dutch, Dutch Low Saxon, Emiliano-
Romagnolo, English, Esperanto, Estonian, Ewe, Extremaduran,
Faroese, Fiji Hindi, Finnish, French, Friulian, Galician, Ger-
man, Gilaki, Gothic, Guarani, Hai//om, Haitian, Hakka Chi-
nese, Hawaiian, Hungarian, Icelandic, Ido, Igbo, Iloko, Indone-
sian, Interlingua, Interlingue, Irish, Italian, Javanese, Kabyle,
Kalaallisut, Kara-Kalpak, Kashmiri, Kashubian, Kongo, Ko-
rean, Kurdish, Ladino, Latin, Latvian, Ligurian, Limburgan,
Lingala, Lithuanian, Lojban, Lombard, Low German, Lower
Sorbian, Luxembourgish, Malagasy, Malay, Maltese, Manx,
Maori, Mazanderani, Min Dong Chinese, Min Nan Chinese,
Nahuatl, Narom, Navajo, Neapolitan, Northern Sami, Norwe-
gian, Norwegian Nynorsk, Novial, Occitan, Old English, Pam-
panga, Pangasinan, Panjabi, Papiamento, Pennsylvania Ger-
man, Piemontese, Pitcairn-Norfolk, Polish, Portuguese, Pushto,
Quechua, Romanian, Romansh, Samoan, Samogitian Lithua-
nian, Sardinian, Saterfriesisch, Scots, Scottish Gaelic, Serbo-
Croatian, Sicilian, Silesian, Slovak, Slovenian, Somali, Span-
ish, Sranan Tongo, Sundanese, Swahili, Swati, Swedish, Taga-
log, Tahitian, Tarantino Sicilian, Tatar, Tetum, Tok Pisin, Tonga
(Tonga Islands), Tosk Albanian, Tsonga, Tswana, Turkish,
Turkmen, Uighur, Upper Sorbian, Uzbek, Venda, Venetian,
Vietnamese, Vlaams, Vlax Romani, Volapu?k, Vo?ro, Walloon,
Waray, Welsh, Western Frisian, Wolof, Yoruba, Zeeuws, Zulu
Cyrillic
Abkhazian, Bashkir, Belarusian, Bulgarian, Chuvash, Erzya,
Kazakh, Kirghiz, Macedonian, Moksha, Moldovan, Mongo-
lian, Old Belarusian, Ossetian, Russian, Serbian, Tajik, Udmurt,
Ukrainian, Yakut
Arabic
Arabic, Egyptian Arabic, Gilaki, Mazanderani, Persian,
Pushto, Uighur, Urdu
Devanagari
Bihari, Hindi, Marathi, Nepali, Newari, Sanskrit
Other
Amharic, Armenian, Assamese, Bengali, Bishnupriya,
Burmese, Central Khmer, Chinese, Classical Chinese, Dhivehi,
Gan Chinese, Georgian, Gothic, Gujarati, Hebrew, Japanese,
Kannada, Lao, Malayalam, Modern Greek, Official Aramaic,
Panjabi, Sinhala, Tamil, Telugu, Thai, Tibetan, Wu Chinese,
Yiddish, Yue Chinese
978
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 273?276,
The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics
YouBot: A Simple Framework for Building Virtual Networking Agents
Seiji Takegata, Kumiko Tanaka-Ishii
Graduate School of Information Science and Technology, University of Tokyo
13F Akihabara Daibiru, 1-18-13 SotoKanda Chiyoda-ku, Tokyo, Japan
takegata@cl.ci.i.u-tokyo.ac.jp, kumiko@i.u-tokyo.ac.jp
Abstract
This paper proposes a simple framework
for building ?virtual networking agents?;
programs that can communicate with users
and collect information through the inter-
net. These virtual agents can also commu-
nicate with each other to share information
that one agent does not have. The frame-
work - ?YouBot? - provides basic functions
such as protocol handling, authentication,
and data storage. The behavior of the vir-
tual agents is defined by a task proces-
sor (?TP?) which can be written in a light-
weight language such as JavaScript. It is
very easy to add new functions to a virtual
agent. The last part of this paper discusses
the micro-blog system ?twitter? and other
web services as information sources that a
virtual agent can utilise to make its behav-
ior more suited to the user.
1 Introduction
Recently, communicating in short sentences, such
as via Instant Messenger or SMS, has become
more common; the use of ?Twitter?, especially,
is spreading very quickly and widely. These net-
working tools are not only for chatting, but also for
gathering information on and discussing a world
of topics. Short sentences are suitable for Nat-
ural Language Interface processes like question-
answering, recommendation, or reservation sys-
tems; thus, Natural Language Interfaces are be-
coming increasingly important in this area of com-
munications.
There are many dialogue systems that process
natural language as a user-input, like ?UC? (Wilen-
sky 1987), ?tour guide? (Prodanov et.al. 2002), but
most of them are designed for a specific individ-
ual purpose, so, have to locate different systems
for different purposes. This problem has been one
of the main barriers preventing dialogue systems
from being adopted more widely.
Our framework -?YouBots?- can accept the
user?s messages as input, and respond in natural
language. The behavior of these agents is defined
by task processors (?TPs?) which can be written
in a light-weight language, eg. JavaScript. It
is very easy to add new TPs to a virtual agent.
Web-browsers like Firefox have a similar add-on
mechanism and, through open-source collabora-
tion, now have thousands of types of extension.
We hope that, in the same way, developers will
be encouraged to write new TPs for our YouBot
framework.
Personal Digital Assistant is an example of this
kind of application. Its schedule manager, contact
manager and to-do list are easily implemented on
this framework. Q&A system is another example;
it would be realized by cooperating with webser-
vice or other external system.
The framework also has a unique networking
feature to help the bots communicate with each
other: It is called ?Inter-bot communication?, a
feature which expands the ways in which the vir-
tual agent can get preferred information for the
user.
2 Outline of the Networking Bot
Most existing dialogue systems only use their in-
ternal data. So their application is often limited
to a specific purpose, as in domain-specific expert
systems. Using a network feature enabling bots
to communicate with each other, our system can
obtain many types of information from other, ex-
ternal systems. Figure 1 shows users communi-
cating with their own bots, and bots communicat-
ing with each other to collect information for their
users. Each connection in the figure is conducted
by XMPP protocol1.
1http://www.xmpp.org/
273
Information in each bot can be linked in the
same manner as web pages, and combine to form
semantic structures in the way of the Semantic
Web (Berners-Lee 2001), this can improve the
bots behaviour.
Figure 1: Network of Users and Bots.
If TPs are designed to share information
through the network, a user need not know which
system contains the information he or she needs.
They only need to talk to their own personal bot,
then the bot will find the information for them.
Each user has their own bot, and can share infor-
mation through these bots. The modes of inter-
action with other users and modes of information
gathering depend on how the TPs are written.
3 Task Managing
Within our framework, a ?task manager? invokes a
?task processor? as shown in Figure 2:-
Figure 2: Task-managing
There are existing systems that process tasks
with modular components - TPs; among these, we
find two approaches, one is centralized and the
other distributed. In the centralized approach, a
user-message is analyzed by a central component
of the system, often called the ?dialogue manager?.
Then the dialogue manager decides which TP to
invoke. The ?Smart Personal Assistant? (Nguyen
et al 2005) uses ?BDI Theory? (Bratman 1987) to
determine the user?s intention in the dialogue man-
ager. Then, a TP which satisfies the user?s demand
can be selected. In this approach, interpretation
can be carried out efficiently, but the task manager
needs to be revised every time a new TP is added.
This is not an easy operation unless the task man-
ager is configured to recognize the functionality of
a new TP automatically. This may be viewed as a
serious weakness of the centralized approach.
On the other hand, there is ?RIME? (Nakano et
al. 2008) which adopts a distributed approach -
where the user-message is sent to each of the TPs,
which interpret it and return a ?score? indicating
how well they can handle the message. Conse-
quently, the TP returning the highest score will
process the user?s message. This approach suf-
fers from the inefficiency of having to interpret the
user?s message many times in each TP. On the pos-
itive side, there is no need to revise or redesign
central components when a new TP is added.
We have decided to adopt the distributed ap-
proach because we think expandability is more im-
portant than speed. Our framework uses ?Script-
ing Engine? in which JavaScript codes can run.
JavaScript is very easy to write, owing to which,
many people write extensions for Firefox in which
JavaScript codes can also run. How simply a TP
can be written is a very important factor in the at-
traction of developers.
4 How to Write Task Processors
There are three types of designated TP in the
YouBot system: a ?user task processor?, a ?bot
task processor?, and a ?twitter task processor?. The
?user-TP? is for processing messages from the user
- explained in the ?Basic Task Processor? subsec-
tion (see below); the ?bot-TP? is for processing in-
quiries from other bots - explained in the ?Inter-bot
communication? subsection (see below); and the
?twitter-TP? reads the user?s tweets at the Twitter
site - explained in the ?Cooperation with External
Services? subsection (also see below). Each TP is
saved to an individual JavaScript file in the ?task?
folder with a .js extension. The YouBot Frame-
work reads these files when the program starts and
when a ?reload? command is issued.
274
4.1 Basic Task Processors
The JavaScript code for a basic TP needs at least
one variable and two functions. The variable
?type? indicates the type of task - which can ei-
ther be a user-task, bot-task, or twitter-task. The
mandatory functions are ?estimate? and ?process?,
an approach introduced in the ?Blackboard? multi-
agent system (Corkill 1991). The ?estimate? func-
tion receives a user-message from the task man-
ager and returns its score, which shows how likely
it is that this TP will be the best among the other
TPs to process the message. For example, when
a TP uses pattern-matching for message interpre-
tion, the score may be higher if the matched pat-
tern is more complicated, or may be zero if no
pattern matched the user message. The ?estimate?
function can use not only pattern-matching, but
also various data calculated or stored in different
ways; such as the dialogue history or information
from external systems. The YouBot Framework
gathers and compares the scores returned from the
TPs, then selects the processor which returned the
highest score to process the message. The ?pro-
cess? function of the TP handles the user-message
and makes a response to the user. During the pro-
cessing, this function can access the internal data
store or an external system to get or save various
information.
4.2 Pattern Matching
Our framework provides a handy way to do
pattern-matching, using four types of placeholder:
An OR conditional placeholder is defined by
?{abc |def }? format.
I {will go|went }to school.
matches both ?I will go to school.? and ?I went
to school.? Optional selection can be defined with
this ?(abc|def)? format.
Yes (I do |it is).
matches ?Yes I do?, ?Yes it is? and just ?Yes?
Using ?[abc]? format, the content of the place-
holder can be retrieved. For example, the pattern:
I went to [place].
matches the sentence ?I went to school.? or ?I
went to see a doctor.? If the pattern matches the
user?s message, an object holding the contents of
the placeholder will be returned. You can get the
contents with the ?get? function, specifying the
placeholder - in this case ?[place]?
To define a placeholder which matches only one
specified pattern, ?<abc>? format is used. For ex-
ample, the placeholder ?<date>? can be defined
so that it matches a date expression such as ?yes-
terday? or ?on Sunday?. Then the pattern:
I went to [place] <date>.
matches ?I went to school on Sunday?, but does
not match ?I went to school with my brother?.
The content of <date> placeholder can also be re-
trieved with ?get? function. Retrieved data can be
kept in the data store and used in interaction with
the user later.
4.3 The Data store
Many chatter bots don?t remember what they have
said before. ?A.L.I.C.E? (Wallace 2008) has a
short memory - just one single interaction. Un-
usually, YouBot has a long-term data store for its
memory. It holds key=value style properties which
can be defined by the TP. To save schedule data, as
in:-
type="schedule"
date="2010/05/14"
item="Submission dead-line"
- we create a new data object, set its properties,
and use the ?save? function. To retrieve specific
data from the data store, a ?data selector? object is
provided. If the following condition is set up in
the data selector:-
type="schedule"
date="2010/05/09"
- then a list of matching data is retrieved from
the data store. The Youbot framework also pro-
vides a facility for responding to inquiries from
other bots, and this raises security issues. In this
framework, a default security filter is installed in
the data selector to send information only to priv-
ileged bots. Data objects saved in the data store
have security attributes for which the default is
?secret?, and only the owner of the bot can ac-
cess this information. This attribute can be set to
?private? or ?official? - then, the information will
only be accessible to the bots which have ?private?
or ?official? privilege. Developers do not have to
worry about this data security setting during inter-
bot communication.
4.4 Inter-bot communication
A user-TP can send an inquiry to another bot -
about, for example, the user?s schedule or knowl-
edge and expertise. The TP generates an ?Inquiry
Sender? object , sets the inquiry and the target
bot?s address, then uses the ?send? function. This
275
inquiry is formatted as an inter-bot message so
that the receiving bot can distinguish it from user-
messages. The receiving bot generates an ?Inquiry
Responder? object for each of the incoming inter-
bot messages; then the Task Manager sends the
messages to the bot-TP. Next, the bot-TPs estimate
the likelihood of processing the message and re-
turn scores - with the bot-TP which returns the
highest score being selected to respond. A re-
sponding message is sent back to the inquiring bot
in the inter-bot message format. then a function
named ?convey? - within the inquiring TP - is in-
voked to make a response to the user. A function
named ?timeout? is invoked when no response has
been returned.
Figure 3: Inter-Bot Communication
To respond to an inquiry from another bot, a
bot-TP for that inquiry has to be defined. Besides
which, remote bots have to be given privilege to
collect information which has a security attribute
restricting access. If a TP developer fails to spec-
ify a security attribute for the data, no access will
be allowed without the right privilege, because the
default setting is secret.
4.5 Cooperation with External Systems
A bot can read the user?s tweets at the twitter site
at specified intervals. The User?s tweets are sent
to twitter-TPs, then estimated and processed in
a same manner as user-TPs and bot-TPs. A bot
can get information about a user?s status, interests,
and favorites; these data are useful for generating
preferable responses for the user.
The Youbot framework also provides a utility
function which takes URI and retrieves HTML
code. This function can be used to access search
engines or news sites. Services such as online
shopping or recommendation engines represent
the type of business model that would be suited
to the application of the Youbot framework.
5 Interaction Example
The following are examples of interactions which
YouBot might handle:
USER: I will meet John at 9 tomorrow.
SYSTEM: Is that A.M or P.M?
USER: pm
SYSTEM: There?s a meeting with Mr. Smith at 8pm.
USER: It?s been canceled.
SYSTEM: I see.
6 Conclusion
We proposed a simple framework for virtual
agents. Its functionality can be easily extended
by adding task processing modules written in
JavaScript. The Youbot framework provides util-
ity objects which make task processing even eas-
ier. Networking ability is also provided to expand
the networked information?s reach, while data se-
curity is maintained. Future work will include
normalizing the estimation score. Another chal-
lenge is how best to share contextual information
among TPs so they can interact to generate better
responses for the user.
References
Robert Wilensky. Ther Berkley UNIX Consultant
Project. Informatik-Fachberichte, volume 155,
pages 286?296, Springer, 1987.
P. J. Prodanov, A. Drygajlo, G. Ramel, M. Meisser, and
R. Siegwart. Voice enabled interface for interactive
tour-guided robots. In Proceedings IEEE/RSJ Inter-
national Conference on Intelligent Robots and Sys-
tems, pages 1332?1337, 2002.
T. Berners-Lee and J. Hendler and O. Lassila. The Se-
mantic Web. In Scientific American, pages 34?43,
May 2001.
R.S. Wallace. The Anatomy of A.L.I.C.E. Parsing the
Turing Test, pages 181?210, Springer Netherlands,
2008.
A. Nguyen. An agent-based approach to dialogue man-
agement in personal assistants. In Proceedings of
IUI-2005, pages 137?144. ACM Press, 2005.
M. Bratman. Intentions, Plans, and Practical Reason.
Harvard University Press, 1987.
M. Nakano, K. Funakoshi, Y. Hasegawa, and H. Tsu-
jino. A Framework for Building Conversational
Agents Based on a Multi-Expert Model. In Proceed-
ings of the 9th SIGdial Workshop on Discourse and
Dialogue, pages 88?91. ACL, 2008.
Daniel D. Corkill. Blackboard systems. AI Expert, vol-
ume 6, pages 40?47, 2008.
276
