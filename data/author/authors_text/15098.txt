Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1103?1113, Dublin, Ireland, August 23-29 2014.
A Dependency Edge-based Transfer Model for Statistical Machine
Translation
Hongshen Chen
?
? Jun Xie
?
Fandong Meng
?
? Wenbin Jiang
?
Qun Liu
??
?Key Laboratory of Intelligent Information Processing
Institute of Computing Technology, Chinese Academy of Sciences
?University of Chinese Academy of Sciences
{chenhongshen,xiejun,mengfandong,jiangwenbin}@ict.ac.cn
?CNGL, School of Computing, Dublin City University
qliu@computing.dcu.ie
Abstract
Previous models in syntax-based statistical machine translation usually resort to some kinds
of synchronous procedures, few of these works are based on the analysis-transfer-generation
methodology. In this paper, we present a statistical implementation of the analysis-transfer-
generation methodology in rule-based translation. The procedures of syntax analysis, syntax
transfer and language generation are modeled independently in order to break the synchronous
constraint, resorting to dependency structures with dependency edges as atomic manipulating
units. Large-scale experiments on Chinese to English translation show that our model exhibits
state-of-the-art performance by significantly outperforming the phrase-based model. The statis-
tical transfer-generation method results in significantly better performance with much smaller
models.
1 Introduction
Researches in statistical machine translation have been flourishing in recent years. Statistical translation
methods can be divided into word-based (Brown et al., 1993), phrase-based (Marcu and Wong, 2002;
Koehn et al., 2003) and syntax-based models (Yamada and Knight, 2001; Graehl and Knight, 2004;
Chiang, 2005; Liu et al., 2006; Mi et al., 2008; Huang et al., 2006; Lin, 2004; Ding and Palmer, 2004;
Quirk et al., 2005; Shen et al., 2008; Xie et al., 2011; Meng et al., 2013). Compared with word-based and
phrase-based methods, syntax-based models perform better in long distance reordering and enjoy higher
generalization capability by leveraging the hierarchical structures in natural languages, and achieve the
state-of-the-art performance in these years.
Most syntax-based models (except for Lin (2004) ) utilize some kinds of synchronous generation
procedures which directly model the structural correspondence between two languages. In contrast,
the analysis-transfer-generation methodology in rule-based translation solves the machine translation
problem in a more divided scheme, where the processing procedures of analysis, structural transfer and
language generation are modeled separately. The analysis-transfer-generation strategy can tolerate higher
non-isomorphism between languages if with a more general transformation unit and it can facilitate
elaborating engineering of each processing procedure, however, there isn?t a statistical transfer model
that shows the comparable performance with the current state-of-the-art SMT model so far.
In this paper, we propose a novel statistical analysis-transfer-generation model for machine transla-
tion, to integrate the advantages of the transfer-generation scheme and the statistical modeling. The
procedures of transfer and generation are modeled on dependency structures with dependency edges
as atomic manipulating units. First, the source sentence is parsed by a dependency parser. Then, the
source dependency structure is transferred into a target structure by translation rules, which composed
of the source and target edges. Last, the target sentence is finally generated from the target edges which
are used as intermediate syntactic structures. By directly modeling the edge, the most basic unit in the
dependency tree, which definitely describe the modifying relationship and positional relation between
words, our model alleviates the non-isomorphic problem and shows the flexibility of reordering.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1103
E
?ob?m?j?nti?nji?n?f?b? ?nqu?nzh?nlu? sh?n?m?n?
f?b?
?ob?m? ji?n?
*:sh?n?m?n?
nsubj advmod
dobj
f?b?
f?b? sh?n?m?n?
?nqu?n
nn
obama 
issue
will
issue
issue
*
sh?n?m?n?
zh?nlu?
nn
a statement of 
security
a statement of 
strategy
f?b?/VV
??
ji?n?/AD
?
sh?n?m?n?/NN
??
?nqu?n/NN
??
zh?nlu?/NN
??
nsubj
advmod
dobj
nn nn
?ob?m?/NN
???
obama today will issue a statement of security strategy
D ? ? ?
? ?
j?nti?n/NT
??
tmod
j?nti?n
tomod
f?b?
today
issue
?
??????????????
Figure 1: (a)An example of labeled Chinese dependency tree aligned with the corresponding English
sentence. (b) Examples of the transfer rules extracted from the tree. ?*? denotes a variable. All the inner
nodes are treated as variables. The label on the target side of a rule denotes whether the head and the
dependent are adjacent or not.
The rest of the paper is organized as follows, we first describe the dependency edge-based transfer
model (Section 2). Then, we present our rule acquisition algorithm (Section 3), the decoding and target
sentence generation process (Section 4). Finally, large-scale experiments (Section 5) on Chinese-to-
English translation show that our edge-based transfer model gains state-of-the-art performance by sig-
nificantly outperforming the phrase-based model (Koehn et al., 2003) by averaged +1.34 BLEU points on
three test sets. To the best of our knowledge, this is the first transfer-generation-based statistical machine
translation model that achieves the state-of-the-art performance.
2 Dependency Edge-based Transfer Model
2.1 Edges in Dependency Trees
Given a sentence, its dependency tree is a directed acyclic graph with words in the sentence as nodes.
An example dependency tree is shown in Figure 1 (a). An edge in the tree represents a dependency
relationship between a pair of words, a head and a dependent. When a nominal dependent acts as a
subject and modifies a verbal head, they usually have a fixed relative position. In Figure 1 (a), ?`aob?am?a?
modifies ?f?ab`u?. The grammatical relation label nsubj (Chang et al., 2009) between them denotes that a
noun phrase acts as the subject of a clause. ?`aob?am?a? is on the left of ?f?ab`u?.
Based on the above observations, we take the edge as the elementary structure of a dependency tree
and regard a dependency tree to be a set of edges.
Definition 1. An source side edge is a 4-tuple e = ?H,D,P,R?, where H is the head, D is the depen-
dent, P denotes the relative position between H and D, left or right, R is the grammatical relation label
.
In Figure 1 (b), the upper sides of transfer rules are source side edges extracted from the dependency
tree.
1104
f?b?/VV
ji?n?/AD sh?n?m?n?/NN
nsubj
advmod
dobj
?ob?m?/NN j?nti?n/NT
tmod
Transfer
select the left
adjacent edge
? ? ? ?
extend to the left
will
ad
jac
en
t
obama
issue
non
-ad
jace
nt
today
no
n-a
dja
cen
t
? ? ?
will
ad
jac
en
t
obama
issue
non
-ad
jace
nt
today
no
n-a
dja
cen
t
?? ?
H1:obama today will issue
H2:today obama will issue
extend to the right
*
adjacent
? ? ? ?
a statement of security strategy
ad
jac
en
t
issue
?
will
will issue
will
ad
jac
en
t
obama
issue
non-
adjac
ent
today
non
-ad
jac
ent
? ? ?
will
ad
jac
en
t
obama
issue
non-
adja
cent
today
non
-ad
jac
ent
?? ?
H1:obama today will issue a statement of security strategy
H2:today obama will issue a statement of security strategy
?
adjacent
?
*
Generation
Analysis
?ob?m? j?nti?n ji?n? f?b? ?nqu?n zh?nlu? sh?n?m?n?
f?b?
?ob?m? ji?n?
nsubj advmod
f?b?
obama
issue
will
non
-ad
jace
nt
issue
adj
ace
nt
j?nti?n
tomod
f?b?
today
issue
non
-ad
jace
nt
*:sh?n?m?n?
dobj
f?b?
issue
*
adjacent
Figure 2: An example partial generation of translation. The same set of rules generate two target hy-
potheses with the same words and different word order. Assume the sub-tree rooted at ?sh?engm??ng? has
been translated to the corresponding target sentence fragment.
2.2 Transfer Rules
A transfer rule of our model represents the reordering and relative positions of edges between language
pairs. For example, in Figure 1 (b), the first rule shows that when a nominal subject modifies a verb, the
target side keeps the same position relations. ?obama? is also on the left of ?issue?, the same with the
source side relative position. The 5-th and 6-th rules show the inversion relations between the source and
the target. Formally, a transfer rule can be defined as a triple ?e, f,??, where e is an edge extracted from
the source dependency tree, f is a target edge. ? denotes one-to-one correspondence between variables
in e and f .
Figure 1 (b) are part of transfer rules extracted from the word aligned sentence in Figure 1 (a). The
target edge denotes whether the target dependent is on the left or the right side of the target head, the
1105
label on the edge indicates whether the target head and the target dependent are adjacent or not. If
the dependent is an internal node(contrast with the leaf nodes in the dependency tree), then it will be
regarded as a substitution node. The dependent in the 4-th transfer rule is an internal node and the its
corresponding target side is a substitution variable.
Figure 2 shows a partial transfer-generation of our model which involves three phases. First, analysis.
Given a source language sentence, we obtain its dependency tree using a dependency parser. We assume
that the sub-tree of the substitution node has been translated. Second, transfer. For each internal node,
we transfer the source side edges between the head and all its dependents into the target sides. In the
second block of Figure 2, we transfer four edges into the target sides. Third, generation, corresponding
to the third block of Figure 2. We generate the target sentence with the target side edges starting from the
target head, ?issue?. We first try to concatenate the edges to the left. First, we select a target side edge
that is on the left side of ?issue? and adjacent to it to form a consecutive phrase. Edge 3 is selected and ?to
issue? is generated. Then, we enumerate all possible left concatenations of the other edges that are not
adjacent to ?issue?. The two sequences(1,2,3 and 2,1,3) of the edges are generated, corresponding to the
two hypotheses. After that, we extend the two hypotheses to the right. The internal node ?sh?engm??ng? is
a substitution node, so the candidate translation of the sub-tree rooted at ?sh?engm??ng? is concatenated to
the two hypotheses. Finally, we generate the two candidate translations of the input sentence.
3 Acquisition of Transfer Rules
Transfer rules can be extracted automatically from a word-aligned corpus, which is a set of triples
?T, S,A?, where T is a source dependency tree, S is a target side sentence and A is an alignment relation
between T and S. Following the dependency-to-string model (Xie et al., 2011), we extract transfer rules
from each triple ?T, S,A? by three steps:
1. Tree Annotation: Label each node in the dependency tree with the alignment information
2. Edges Identification: Identify acceptable edges from the annotated dependency tree
3. Rule induction: Induce a set of lexicalized and un-lexicalized transfer rules from the acceptable
edges.
3.1 Tree Annotation
Given a triple ?T, S,A? as Figure 3 shows, we define two attributes for every node in T: node span and
sub-tree span:
Definition 2. Given a node n, its node span nsp(n) is a set of consecutive indexes of the target words
aligned with the node n.
For example, nsp(?anqu?an)={7-8}, which corresponds to the target word ?of? and ?security?.
Definition 3. A node span nsp(n) is consistent if for any other node n
?
in the dependency tree, nsp(n)
and nsp(n
?
) are not overlapping.
For example, nsp(zh`anlu`e) is consistent, while nsp(?anqu?an) is not consistent for it corresponds to the
same word ?of? with nsp(sh?engm??ng).
Definition 4. Given a sub-tree T
?
rooted at n, the sub-tree span tsp(n) of n is a consecutive target word
indexes from the lower bound of the nsp of all the nodes in T
?
to the upper bound of those spans.
For example, tsp(sh?engm??ng)={5-9},which corresponds to the target phrase ?a statement of security
strategy?.
Definition 5. A sub-tree span tsp(n) is consistent if for any other node n
?
that is not in the sub-tree
rooted at n in the dependency tree, tsp(n) and nsp(n
?
) are not overlapping.
For example, tsp(sh?engm??ng) is consistent, even though nsp(sh?engm??ng) is not consistent, while
tsp(?anqu?an) is not consistent for ?sh?engm??ng? is not a node in sub-tree rooted at ??anqu?an? and ??anqu?an?
corresponds to the same word ?of ? with nsp(sh?engm??ng) .
1106
f?b?/VV
ji?n?/AD sh?n?m?n?/NN
?nqu?n/NN zh?nlu?/NN
nsubj advmod dobj
nn nn
?ob?m?/NN
obama today will issue a statement of security strategy
j?nti?n/NT
tmod
1 2 3 4 5 6 7 8 9
{1-1}/{1-1} {2-2}/{2-2} {3-3}/{3-3}
{4-4}/{1-9}
{5-7}/{5-9}
{7-8}/{7-8} {9-9}/{9-9}
Figure 3: An example of annotated dependency tree. Each node is annotated with two spans, the former
is node span and the latter is sub-tree span. The gray edge is not acceptable. It is different from Figure
1, because ??anqu?an? aligned with two words in Figure 3. ?of? in the target side is aligned with both
??anqu?an? and ?sh?engm??ng? which makes the gray edge un-acceptable.
3.2 Acceptable Edges Identification
We identify the edges from the annotated dependency tree that are acceptable for rule induction.
For an acceptable edge, its node span of the head nsp(head) and the sub-tree span of the dependent
tsp(dependent) satisfy the following properties:
1. nsp(head) and tsp(dependent) are consistent.
2. nsp(head) and tsp(dependent) are non-overlapping.
For example, tsp(?anqu?an) and nsp(sh?engm??ng) are neither consistent nor non-overlapping. So the
gray edge between head ?sh?engm??ng? and dependent ??anqu?an? is not an acceptable edge. nsp(f?ab`u)
and tsp(sh?engm??ng) are consistent and the two spans are non-overlapping. Thus, the edge between head
?f?ab`u? and dependent ?sh?engm??ng? is an acceptable edge.
3.3 Transfer Rule Induction
From each acceptable source side edge, we induce a set of lexicalized and un-lexicalized transfer rules.
We induce a lexicalized transfer rule from an acceptable edge by the following procedures:
1. extract the source side edge and mark the internal nodes as substitution sites. This form the input of
a transfer rule.
2. extract the position information according to nsp(head) and tsp(dependent), whether they are adja-
cent or not and whether tsp(dependent) is on the left side or the right side of nsp(head).
In Figure 4, the first transfer rule is lexicalized rule, it is induced from the edge between ?f?ab`u? and
?`aob?am?a?.
In addition to the lexicalized rules described above, we also generalized the rules by replacing the
word in an source side edge with a wild card and the part of speech of the word. For example, the rule
in Figure 4 can be generalized in two ways. The generalized versions of the rule apply to ?`aob?am?a?
modifying any verb and ?f?ab`u? modifying any noun, respectively. The generalized rules are also called
1107
Generalize head
?ob?m?
nsubj
f?b?
obama
issue
non
-adj
acen
t
?ob?m?
nsubj
*:VV
obama
*
non-
adja
cent
Generalize
dependent
*:NN
nsubj
f?b?
*
issue
non-a
djace
nt
?
?
?
Figure 4: Generalization of transfer rule.
un-lexicalized rules for the loss of word information. The single node translations of the generalized
words are also extracted.
The unaligned words of the target side is handled by extending nsp(head) and tsp(dependent) on both
left and right directions. We do this process similar with the method of Och and Ney (2004). We might
obtain m(m ? 1) extended rules from an acceptable edge. The frequency of each rule is divided by m.
We take the extracted rule set as observed data and make use of relative frequency estimator to obtain
the translation probabilities P (t|s) and P (s|t).
4 Decoding and Generation
We follow Och and Ney (2002), using a general log-linear model to score the sentence generated by each
concatenation of the target edges. Let c be concatenations that concatenate the target edges to generate
the target sentence e. The probability of e is defined as?
P (c) ?
?
i
?
i
(c)
?
i
(1)
where ?
i
(c) are features defined on concatenations and ?
i
are feature weights. In our experiments of
this paper, thirteen features are used as follows:
? Transfer rules translation probabilities P (t|s) and P (s|t), and lexical translation probabilities
P
lex
(t|s) and P
lex
(s|t);
? Bilingual phrases probabilities P
bp
(t|s) and P
bp
(s|t), and bilingual phrases lexical translation prob-
abilities P
bplex
(t|s) and P
bplex
(s|t);
? Transfer rule penalty exp(?1);
? Bilingual phrase penalty exp(?1);
? Pseudo translation rule penalty exp(?1);
? Target word penalty exp(|e|);
? Language model P
lm
(e).
Our decoder is based on a bottom-up chart-based beam-search algorithm. We regard the decoding
process as the composition of the target side edges. For a given source language sentence, we obtain its
1108
f?b?
j?nti?n?ob?m?
obama today to issue
f?b?
sh?n?m?n?
?nqu?n zh?nlu?
issue a statement of security strategy
ji?n?
Figure 5: Two examples of the phrases incorporated in our model.
dependency tree T with an external dependency parser. Each node in T is traversed in post-order. For
each internal node and root node n, we do the transfer-generation translation as the following procedures:
1. Extract all the source side edges including the lexicalized and generalized edges between n and all
its dependents using the same way we extract the source side edges of the transfer rules.
2. Transfer the source side edges into target side edges. For a generalized rule, we restore it to a lex-
icalized rule by combining it with the single word translation. For no matched edges, we construct
the pseudo translation rule according to the word order of the source head-dependent relation.
3. Generate the target sentence by bi-directional extension from an adjacent target edge. We first
group all the target edges by their heads. For each group, we generate translation hypotheses with
the following procedures:
(a) Select an adjacent target edge as the starting position;
(b) Extend to the left side and enumerate all possible permutations of the target edges directing
left;
(c) Extend to the right side and enumerate all possible permutations of the target edges directing
right.
Considering that in dependency trees, a head may relate to more than 4 edges which results in
massive search space. We reduce the time complexity by using the maximum distortion limit. The
distortion is defined as (a
i
? b
i?1
? 1), where a
i
denotes the start position of the source side edge
that is translated into the ith target side edge and b
i?1
denotes the end position of the source side
edge translated into the (i ? 1)th target side edge.
When we reach the root node, the candidate translations of the input sentence are generated.
In our model, only the adjacent target edge of a transfer rule can be regarded as a consecutive phrase
and its corresponding source side length is only 2. As we start extending the target sentence from
the target head, it is quite natural to incorporate the bilingual phrases to make the target sentences be
extended from the phrases as well as the single target head word. Due to the flexibility of our model,
we can incorporate not only the syntactic phrases which are phrases covering a whole sub-tree, but also
the non-syntactic phrases as the fixed dependency structures in Shen et al. (2008) which are consecutive
phrases covering the head. Figure 5 shows two examples of the phrases incorporated in our model.
We prune the search space in several ways. First, beam threshold ?, items with a score worse than ?
times of the best score in the same span will be discarded; second, beam size b, items with a score worse
than the bth best item will be discarded. For our experiments, we set ? = 10
?3
and b = 300; Third,
we also prune rules for the same edge with a fixed rule limit (r = 200), which denotes the maximum
number of rules we keep.
5 Experiments
In this section, the performance of our model is evaluated by comparing with phrase-based model (Koehn
et al., 2003), on the NIST Chinese-to-English translation tasks. We also present the influence of the
1109
mt02-tune mt03 mt04 mt05
0 30.81 30.03 32.44 30.09
1 33.4 32.07 34.55 31.77
2 34.39 32.7 35.4 32.59
3 34.04 32.69 35.46 32.54
4 33.59 31.75 35.15 32.17
30
31
32
33
34
35
36
0 1 2 3 4
maximum distortion limit
BL
EU
(%
)
mt02-tune
mt03
mt04
mt05
Figure 6: Effect of different maximum distortion limits on development
set (mt02) and three tests(mt03,04,05). The performance of all the sets
are consistent.
maximum distortion limit to our model. We take open source phrase-based system Moses (with default
configuration)
1
as our baseline system.
5.1 Experimental Setting
Our training corpus consists of 1.25M sentence pairs from LDC data, including LDC2002E18, LD-
C2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06.
To obtain the dependency trees of the source side, we parse the source sentences with Stanford Parser
(Klein and Manning, 2003) into projective dependency structures with nodes annotated by POS tags and
edges by dependency labels.
To obtain the word alignments, we run GIZA++ (Och and Ney, 2003) on the corpus in both directions
and apply ?grow-diag-and? refinement (Koehn et al., 2003). We extract the phrases covering no more
than 10 nodes of the fixed structures.
We use SRILM (Stolcke, 2002) to train a 4-gram language model with modified Kneser-Ney smooth-
ing on the Xinhua portion of the Gigaword corpus.
We use NISTMTEvaluation test set 2002 as our development set, 2003-2005 NIST datasets as testsets.
The quality of translations is evaluated by the case insensitive NIST BLEU-4 metric
2
.
We make use of the minimum error rate training algorithm (Och, 2003) in order to maximize the
BLEU score of the development set.
The statistical significance test is performed by sign-test (Collins et al., 2005).
5.2 Influence of Maximum Distortion Limit
Figure 6 gives the performance of our system with different maximum distortion limits in terms of
uncased BLEU of three NIST test sets. The performance of different distortion limit are consistent on
both development set and three test sets. Maximum distortion limit 2 gets the best performances. A low
distortion limit may cause the target sentence been translated more close to the sequence of the source,
especially when the distortion limit equals to 0, none of the reordering is allowed, while a high distortion
limit may lead the good translations be flooded by too many ambiguities when enumerating the possible
sequences of the target non-adjacent dependents. We choose 2 as the maximum distortion limit in the
next experiments.
1
http://www.statmt.org/moses/
2
ftp://jaguar.ncsl.nist.glv/mt/resources/mteval-v11b.pl.
1110
System Rule # MT03 MT04 MT05 Average
Moses 44.49M 32.03 32.83 31.81 32.22
DEBT 30.7M 32.7* 35.4* 32.59* 33.56
Table 1: Statistics of the extracted rules on training data and the BLEU scores (%) on the test sets.
?DEBT? denotes our edge-based transfer model. The ?*? denotes that the results are significantly better
than the baseline system (p<0.01).
5.3 Performance of Our Model
Tabel 1 illustrates the translation results of our experiments. We (DEBT) surpass the baseline over +1.34
BLEU points on average. Our model significant outperforms the baseline phrase-based model, with
p < 0.01 on statistical significance test sign-test (Collins et al., 2005).
We also list the statistical number of rules extracted from the training corpus. The number of our
transfer rules is only 69.0% of the rules extracted by Moses, thus, the total rules in our model is 31%
smaller than Moses.
6 Related Work
Transfer-based MT systems usually take a parse tree in the source language and translate it into a parse
tree in the target language with transfer rules. Both our model and some of those previous works ac-
quired transfer rules automatically from word-aligned corpus (Richardson et al., 2001; Carbonell et al.,
2002; Lavoie et al., 2002; Lin, 2004). Gimpel and Smith (2009) and Gimpel and Smith (2014) used
quasi-synchronous dependency grammar for MT and they are similar to our idea of doing transfer of
dependency syntax in a non-synchronous setting. They do the translation as monolingual lattice parsing.
As dependency-based system, Lin (2004) used path as the transfer unit and regarded the translation
problem with minimal path covering. Quirk et al. (2005) and Xiong et al. (2007) used treelets to model
the source dependency tree using synchronous grammars. Quirk et al. (2005) projected the source depen-
dency structure into target side by word alignment and faced the problem of non-isomorphism between
languages. Xiong et al. (2007) directly modeled the treelet to the corresponding target string to alleviate
the problem. Xie et al. (2011) directly specified the ordering information in head-dependents rules that
represent the source side as head-dependents relations and the target side as string.
Differently, our model uses a much simpler elementary structure, edge, which consist of only a head
and a dependent. As a transfer-generation model, we transfer an edge in the source dependency tree into
target side and incorporate the position information on the target edge , which alleviate non-isomorphism
problem and incorporate ordering among different target edges simultaneously. Moreover, our decoding
method is quite different from previous dependency tree-based works. After parsing a given source
language sentence, we transfer and generate the target sentence fragments recursively on each internal
node of the dependency tree bottom-up.
7 Conclusions and Future Work
In this paper, we present a novel dependency edge-based transfer model using dependency trees on the
source side for machine translation. We directly transfer the edges in source dependency tree into the
target sides and then generate the target sentences by beam-search. With the concise transfer rules,
our model is compatible with both the syntactic and non-syntactic phrases. Although the generation
process of our model seems relatively simple, it still exhibits a good performance and outperforms the
phrase-based model on large scale experiments. For the first time, a statistical transfer model shows a
comparable performance with the state-of-the-art translation models.
Since the translation procedure is divided into three phases and each phase can be modeled indepen-
dently, we would like to take further steps focusing on modeling the target language generation process
specifically to ensure a better grammatical translation with the help of natural language generation meth-
ods.
1111
Acknowledgments
The authors were supported by National Key Technology R&D Program (No. 2012BAH39B03), CAS
Action Plan for the Development of Western China (No. KGZD-EW-501), and Sino-Thai Scientific and
Technical Cooperation (No. 60-625J). Sincere thanks to the anonymous reviewers for their thorough
reviewing and valuable suggestions.
References
Peter F Brown, Vincent J Della Pietra, Stephen A Della Pietra, and Robert L Mercer. 1993. The mathematics of
statistical machine translation: Parameter estimation. Computational linguistics, 19(2):263?311.
Jaime Carbonell, Katharina Probst, Erik Peterson, Christian Monson, Alon Lavie, Ralf Brown, and Lori Levin.
2002. Automatic rule learning for resource-limited mt. In Machine Translation: From Research to Real Users,
pages 1?10. Springer.
Pi-Chuan Chang, Huihsin Tseng, Dan Jurafsky, and Christopher D Manning. 2009. Discriminative reordering
with chinese grammatical relations features. In Proceedings of the Third Workshop on Syntax and Structure in
Statistical Translation, pages 51?59. Association for Computational Linguistics.
David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proceedings
of the 43rd Annual Meeting on Association for Computational Linguistics, pages 263?270. Association for
Computational Linguistics.
Michael Collins, Philipp Koehn, and Ivona Ku?cerov?a. 2005. Clause restructuring for statistical machine transla-
tion. In Proceedings of the 43rd annual meeting on association for computational linguistics, pages 531?540.
Association for Computational Linguistics.
Yuan Ding and Martha Palmer. 2004. Synchronous dependency insertion grammars: A grammar formalism for
syntax based statistical mt. In Workshop on Recent Advances in Dependency Grammars (COLING), pages
90?97.
Kevin Gimpel and Noah A Smith. 2009. Feature-rich translation by quasi-synchronous lattice parsing. In Pro-
ceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 1-Volume 1,
pages 219?228. Association for Computational Linguistics.
Kevin Gimpel and Noah A Smith. 2014. Phrase dependency machine translation with quasi-synchronous tree-to-
tree features. Computational Linguistics.
Jonathan Graehl and Kevin Knight. 2004. Training tree transducers. In Daniel Marcu Susan Dumais and Salim
Roukos, editors, HLT-NAACL 2004: Main Proceedings, pages 105?112, Boston, Massachusetts, USA, May 2 -
May 7. Association for Computational Linguistics.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006. Statistical syntax-directed translation with extended domain
of locality. In Proceedings of AMTA, pages 66?73.
Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings
of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on
Human Language Technology - Volume 1, NAACL ?03, pages 48?54, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Benoit Lavoie, Michael White, and Tanya Korelsky. 2002. Learning domain-specific transfer rules: an experiment
with korean to english translation. In Proceedings of the 2002 COLING workshop on Machine translation in
Asia-Volume 16, pages 1?7. Association for Computational Linguistics.
Dekang Lin. 2004. A path-based transfer model for machine translation. In Proceedings of Coling 2004, pages
625?630, Geneva, Switzerland, Aug 23?Aug 27. COLING.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-string alignment template for statistical machine translation.
In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting
of the Association for Computational Linguistics, pages 609?616. Association for Computational Linguistics.
Daniel Marcu and William Wong. 2002. A phrase-based, joint probability model for statistical machine transla-
tion. In Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume
10, pages 133?139. Association for Computational Linguistics.
1112
Fandong Meng, Jun Xie, Linfeng Song, Yajuan L?u, and Qun Liu. 2013. Translation with source constituency
and dependency trees. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language
Processing, pages 1066?1076, Seattle, Washington, USA, October. Association for Computational Linguistics.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-based translation. In Proceedings of ACL-08: HLT, pages
192?199, Columbus, Ohio, June. Association for Computational Linguistics.
Franz Josef Och and Hermann Ney. 2002. Discriminative training and maximum entropy models for statistical
machine translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics,
ACL ?02, pages 295?302, Stroudsburg, PA, USA. Association for Computational Linguistics.
Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models.
Computational linguistics, 29(1):19?51.
Franz Josef Och and Hermann Ney. 2004. The alignment template approach to statistical machine translation.
Computational linguistics, 30(4):417?449.
Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the
41st Annual Meeting on Association for Computational Linguistics-Volume 1, pages 160?167. Association for
Computational Linguistics.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. Dependency treelet translation: Syntactically informed
phrasal SMT. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics
(ACL?05), pages 271?279, Ann Arbor, Michigan, June. Association for Computational Linguistics.
Stephen Richardson, William Dolan, Arul Menezes, and Jessie Pinkham. 2001. Achieving commercial-quality
translation with example-based methods. In Proceedings of MT Summit VIII, pages 293?298. Santiago De
Compostela, Spain.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A new string-to-dependency machine translation algorithm
with a target dependency language model. In Proceedings of ACL-08: HLT, pages 577?585, Columbus, Ohio,
June. Association for Computational Linguistics.
Andreas Stolcke. 2002. Srilm-an extensible language modeling toolkit. In Proceedings of ICSLP, volume 30,
pages 901?904.
Jun Xie, Haitao Mi, and Qun Liu. 2011. A novel dependency-to-string model for statistical machine translation.
In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ?11, pages
216?226, Stroudsburg, PA, USA. Association for Computational Linguistics.
Deyi Xiong, Qun Liu, and Shouxun Lin. 2007. A dependency treelet string correspondence model for statistical
machine translation. In Proceedings of the Second Workshop on Statistical Machine Translation, StatMT ?07,
pages 40?47, Stroudsburg, PA, USA. Association for Computational Linguistics.
Kenji Yamada and Kevin Knight. 2001. A syntax-based statistical translation model. In Proceedings of the 39th
Annual Meeting on Association for Computational Linguistics, pages 523?530. Association for Computational
Linguistics.
1113
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 412?420, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Iterative Annotation Transformation with Predict-Self Reestimation
for Chinese Word Segmentation
Wenbin Jiang and Fandong Meng and Qun Liu and Yajuan Lu?
Key Laboratory of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences
{jiangwenbin, mengfandong, liuqun, lvyajuan}@ict.ac.cn
Abstract
In this paper we first describe the technol-
ogy of automatic annotation transformation,
which is based on the annotation adaptation
algorithm (Jiang et al2009). It can auto-
matically transform a human-annotated cor-
pus from one annotation guideline to another.
We then propose two optimization strategies,
iterative training and predict-self reestimation,
to further improve the accuracy of annota-
tion guideline transformation. Experiments on
Chinese word segmentation show that, the it-
erative training strategy together with predict-
self reestimation brings significant improve-
ment over the simple annotation transforma-
tion baseline, and leads to classifiers with sig-
nificantly higher accuracy and several times
faster processing than annotation adaptation
does. On the Penn Chinese Treebank 5.0,
it achieves an F-measure of 98.43%, signif-
icantly outperforms previous works although
using a single classifier with only local fea-
tures.
1 Introduction
Annotation guideline adaptation depicts a general
pipeline to integrate the knowledge of corpora with
different underling annotation guidelines (Jiang et
al., 2009). In annotation adaptation two classifiers
are cascaded together, where the classification re-
sults of the lower classifier are used as guiding fea-
tures of the upper classifier, in order to achieve more
accurate classification. This method can automat-
ically adapt the divergence between different an-
notation guidelines and bring improvement to Chi-
nese word segmentation. However, the need of cas-
caded classification decisions makes it less practical
for tasks of high computational complexity such as
parsing, and less efficient to incorporate more than
two annotated corpora.
In this paper, we first describe the algorithm of
automatic annotation transformation. It is based on
the annotation adaptation algorithm, and it focuses
on the automatic transformation (rather than adapta-
tion) of a human-annotated corpus from one annota-
tion guideline to another. First, a classifier is trained
on the corpus with an annotation guideline not de-
sired, it is used to classify the corpus with the an-
notation guideline we want, so as to obtain a corpus
with parallel annotation guidelines. Then a second
classifier is trained on the parallelly annotated cor-
pus to learn the statistical regularity of annotation
transformation, and it is used to process the previous
corpus to transform its annotation guideline to that
of the target corpus. Instead of the online knowl-
edge integration methodology of annotation adapta-
tion, annotation transformation can lead to improved
classification accuracy in an offline manner by using
the transformed corpora as additional training data
for the classifier. This method leads to an enhanced
classifier with much faster processing than the cas-
caded classifiers in annotation adaptation.
We then propose two optimization strategies, iter-
ative training and predict-self reestimation, to fur-
ther improve the accuracy of annotation transfor-
mation. Although the transformation classifiers
can only be trained on corpora with autogenerated
(rather than gold) parallel annotations, an iterative
training procedure can gradually improve the trans-
412
formation accuracy by iteratively optimizing the par-
allelly annotated corpora. Both source-to-target and
target-to-source annotation transformations are per-
formed in each training iteration, and the trans-
formed corpora are used to provide better annota-
tions for the parallelly annotated corpora of the next
iteration; then the better parallelly annotated corpora
will result in more accurate transformation classi-
fiers, which will generate better transformed corpora
in the new iteration. The predict-self reestimation
is based on the following hypothesis, a better trans-
formation result should be easier to be transformed
back to the original form. The predict-self heuristic
is also validated by Daume? III (2009) in unsuper-
vised dependency parsing.
Experiments in Chinese word segmentation show
that, the iterative training strategy together with
predict-self reestimation brings significant improve-
ment over the simple annotation transformation
baseline. We perform optimized annotation trans-
formation from the People?s Daily (Yu et al2001)
to the Penn Chinese Treebank 5.0 (CTB) (Xue et
al., 2005), in order to improve the word segmenter
with CTB annotation guideline. Compared to anno-
tation adaptation, the optimized annotation transfor-
mation strategy leads to classifiers with significantly
higher accuracy and several times faster processing
on the same data sets. On CTB 5.0, it achieves an F-
measure of 98.43%, significantly outperforms pre-
vious works although using a single classifier with
only local features.
The rest of the paper is organized as follows.
Section 2 describes the classification-based Chinese
word segmentation method. Section 3 details the
simple annotation transformation algorithm and the
two optimization methods. After the introduction of
related works in section 4, we give the experimental
results on Chinese word segmentation in section 5.
2 Classification-Based Chinese Word
Segmentation
Chinese word segmentation can be formalized as
the problem of sequence labeling (Xue and Shen,
2003), where each character in the sentence is given
a boundary tag denoting its position in a word. Fol-
lowing Ng and Low (2004), joint word segmenta-
tion and part-of-speech (POS) tagging can also be
Algorithm 1 Perceptron training algorithm.
1: Input: Training examples (xi, yi)
2: ~?? 0
3: for t? 1 .. T do
4: for i? 1 .. N do
5: zi ? argmaxz?GEN(xi) ?(xi, z) ? ~?
6: if zi 6= yi then
7: ~?? ~? + ?(xi, yi)??(xi, zi)
8: Output: Parameters ~?
solved in a character classification approach by ex-
tending the boundary tags to include POS informa-
tion. For word segmentation we adopt the 4 bound-
ary tags of Ng and Low (2004), b, m, e and s, where
b, m and e mean the beginning, the middle and the
end of a word, and s indicates a single-character
word. The word segmentation result can be gen-
erated by splitting the labeled character sequence
into subsequences of pattern s or bm?e, indicating
single-character words or multi-character words, re-
spectively.
We choose the perceptron algorithm (Collins,
2002) to train the character classifier. It is an online
training algorithm and has been successfully used in
many NLP tasks, including POS tagging (Collins,
2002), parsing (Collins and Roark, 2004) and word
segmentation (Zhang and Clark, 2007; Jiang et al
2008; Zhang and Clark, 2010).
The training procedure learns a discriminative
model mapping from the inputs x ? X to the outputs
y ? Y , where X is the set of sentences in the train-
ing corpus and Y is the set of corresponding labeled
results. We use the function GEN(x) to enumerate
the candidate results of an input x, and the function
? to map a training example (x, y) ? X ? Y to a
feature vector ?(x, y) ? Rd. Given the character
sequence x, the decoder finds the output F (x) that
maximizes the score function:
F (x) = argmax
y?GEN(x)
S(y|~?,?, x)
= argmax
y?GEN(x)
?(x, y) ? ~?
(1)
Where ~? ? Rd is the parameter vector (that is, the
discriminative model) and ?(x, y) ? ~? is the inner
product of ?(x, y) and ~?.
Algorithm 1 shows the perceptron algorithm for
tuning the parameter ~?. The ?averaged parameters?
413
Type Feature Templates
Unigram C?2 C?1 C0
C1 C2
Bigram C?2C?1 C?1C0 C0C1
C1C2 C?1C1
Property Pu(C0)
T (C?2)T (C?1)T (C0)T (C1)T (C2)
Table 1: Feature templates for classification-based Chi-
nese segmentation model.
technology (Collins, 2002) is used for better per-
formance. The feature templates for the classifier
is shown in Table 1. C0 denotes the current char-
acter, while C?i/Ci denote the ith character to the
left/right of C0. The function Pu(?) returns true
for a punctuation character and false for others, the
function T (?) classifies a character into four types:
number, date, English letter and others.
3 Iterative and Predict-Self Annotation
Transformation
This section first describes the technology of au-
tomatic annotation transformation, then introduces
the two optimization strategies, iterative training and
predict-self reestimation. Iterative training takes
a global view, it conducts several rounds of bidi-
rectional annotation transformations, and improve
the transformation performance round by round.
Predict-self reestimation takes a local view instead,
it considers each training sentence, and improves the
transformation performance by taking into account
the predication result of the reverse transformation.
The two strategies can be adopted jointly to obtain
better transformation performance.
3.1 Automatic Annotation Transformation
Annotation adaptation can integrate the knowledge
from two corpora with different underling annota-
tion guidelines. First, a classifier (source classi-
fier) is trained on the corpus (source corpus) with
an annotation standard (source annotation) not de-
sired, it is then used to classify the corpus (target
corpus) with the annotation standard (target annota-
tion) we want. Then a second classifier (transforma-
tion classifier 1) is trained on the target corpus with
1It is called target classifier in (Jiang et al2009). We
think that transformation classifier better reflects its role, the
Type Feature Templates
Baseline C?2 C?1 C0
C1 C2
C?2C?1 C?1C0 C0C1
C1C2 C?1C1
Pu(C0)
T (C?2)T (C?1)T (C0)T (C1)T (C2)
Guiding ?
C?2 ? ? C?1 ? ? C0 ? ?
C1 ? ? C2 ? ?
C?2C?1 ? ? C?1C0 ? ? C0C1 ? ?
C1C2 ? ? C?1C1 ? ?
Pu(C0) ? ?
T (C?2)T (C?1)T (C0)T (C1)T (C2) ? ?
Table 2: Feature templates for annotation transformation,
where ? is short for ?(C0), representing the source an-
notation of C0.
the source classifier?s classification result as guid-
ing features. In decoding, a raw sentence is first de-
coded by the source classifier, and then inputted into
the transformation classifier together with the anno-
tations given by the source classifier, so as to obtain
an improved classification result.
However, annotation adaptation has a drawback,
it has to cascade two classifiers in decoding to inte-
grate the knowledge in two corpora, thus seriously
degrades the processing speed. This paper describes
a variant of annotation adaptation, name annotation
transformation, aiming at automatic transformation
(rather than adaptation) between annotation stan-
dards of human-annotated corpora. In annotation
transformation, a source classifier and a transforma-
tion classifier are trained in the same way as in an-
notation adaptation. The transformation classifier is
used to process the source corpus, with the classi-
fication label derived from the segmented sentences
as the guiding features, so as to relabel the source
corpus with the target annotation guideline. By inte-
grating the target corpus and the transformed source
corpus for the training of the character classifier, im-
proved classification accuracy can be achieved.
Both the source classifier and the transforma-
tion classifier are trained with the perceptron algo-
rithm. The feature templates used for the source
classifier are the same with those for the baseline
renaming also avoids name confusion in the optimized annota-
tion transformation.
414
Algorithm 2 Baseline annotation transformation.
1: function ANNOTRANS(Cs, Ct)
2: Ms ? TRAIN(Cs)
3: Cst ? ANNOTATE(Ms, Ct)
4: Ms?t ? TRANSTRAIN(Cst , Ct)
5: Cts ? TRANSANNOTATE(Ms?t, Cs)
6: Ct? ? Cts ? Ct
7: return Ct?
8: function DECODE(M, ?, x)
9: return argmaxy?GEN(x) S(y|M,?, x)
character classifier. The feature templates for the
transformation classifier are the same with those in
annotation adaptation, as listed in Table 2. Al-
gorithm 2 shows the overall training algorithm
for annotation transformation. Cs and Ct denote
the source corpus and the target corpus; Ms and
Ms?t denote the source classifier and the trans-
formation classifier; Cqp denotes the p corpus re-
labeled in q annotation guideline, for example Cts
is the source corpus transformed to target annota-
tion guideline; Functions TRAIN and TRANSTRAIN
both invoke the perceptron algorithm, yet with
different feature sets; Functions ANNOTATE and
TRANSANNOTATE call the function DECODE with
different models (source/transformation classifiers),
feature functions (without/with guiding features),
and inputs (raw/source-annotated sentences).
The best training iterations for the functions
TRAIN and TRANSTRAIN are determined on the de-
veloping sets of the source corpus and the target
corpus, respectively. In the algorithm the param-
eters corresponding to developing sets are omitted
for simplicity. Compared to the online knowledge
integration methodology of annotation adaptation,
annotation transformation leads to improved perfor-
mance in an offline manner by integrating corpora
before the training procedure. This manner could
achieve processing several times as fast as the cas-
caded classifiers in annotation adaptation. In the fol-
lowing we will describe the two optimization strate-
gies in details.
3.2 Iterative Training for Annotation
Transformation
The training of annotation transformation is based
on an auto-generated (rather than gold) parallelly an-
notated corpus, where the source annotation is pro-
Algorithm 3 Iterative annotation transformation.
1: function ITERANNOTRANS(Cs, Ct)
2: Ms ? TRAIN(Cs)
3: Cst ? ANNOTATE(Ms, Ct)
4: Mt ? TRAIN(Ct)
5: Cts ? ANNOTATE(Mt, Cs)
6: repeat
7: Ms?t ? TRANSTRAIN(Cst , Ct)
8: Mt?s ? TRANSTRAIN(Cts, Cs)
9: Cts ? TRANSANNOTATE(Ms?t, Cs)
10: Cst ? TRANSANNOTATE(Mt?s, Ct)
11: Ct? ? Cts ? Ct
12: M? ? TRAIN(Ct?)
13: until EVAL(M?) converges
14: return Ct?
15: function DECODE(M, ?, x)
16: return argmaxy?GEN(x) S(y|M,?, x)
vided by the source classifier. Therefore, the perfor-
mance of transformation training is correspondingly
determined by the accuracy of the source classifier.
We propose an iterative training procedure to
gradually improve the transformation accuracy by
iteratively optimizing the parallelly annotated cor-
pora. In each training iteration, both source-to-target
and target-to-source annotation transformations are
performed, and the transformed corpora are used to
provide better annotations for the parallelly anno-
tated corpora of the next iteration. Then in the new
iteration, the better parallelly annotated corpora will
result in more accurate transformation classifiers, so
as to generate better transformed corpora.
Algorithm 3 shows the overall procedure of the
iterative training method. The loop of lines 6-13
iteratively performs source-to-target and target-to-
source annotation transformations. The source an-
notations of the parallelly annotated corpora, Cst and
Cts, are initialized by applying the source and tar-
get classifiers respectively on the target and source
corpora (lines 2-5). In each training iteration, the
transformation classifiers are trained on the current
parallelly annotated corpora (lines 7-8), they are
used to produce the transformed corpora (lines 9-10)
which provide better annotations for the parallelly
annotated corpora of the next iteration. The itera-
tive training terminates when the performance of the
classifier trained on the merged corpus Cts ? Ct con-
verges.
415
The discriminative training of TRANSTRAIN pre-
dicts the target annotations with the guidance of
source annotations. In the first iteration, the trans-
formed corpora generated by the transformation
classifiers are better than the initialized ones gener-
ated by the source and target classifiers, due to the
assistance of the guiding features. In the follow-
ing iterations, the transformed corpora provide bet-
ter annotations for the parallelly annotated corpora
of the subsequent iteration, the transformation ac-
curacy will improve gradually along with optimiza-
tion of the parallelly annotated corpora until conver-
gence.
3.3 Predict-Self Reestimation for Annotation
Transformation
The predict-self hypothesis is implicit in many unsu-
pervised learning approaches, such as Markov ran-
dom field. This methodology has also been success-
fully used by Daume? III (2009) in unsupervised de-
pendency parsing. The basic idea of predict-self is
that, if a prediction is a better candidate for an input,
it can be easier converted back to the original input
by a reverse procedure. If applied to the task of an-
notation transformation, predict-self indicates that a
better transformation candidate following the target
annotation guideline can be easier transformed back
to the original form following the source annotation
guideline.
The most intuitionistic strategy to introduce the
predict-self methodology into annotation transfor-
mation is using a reversed annotation transforma-
tion procedure to filter out unreliable predictions of
the previous transformation. In detail, a source-to-
target annotation transformation is performed on the
source annotated sentence to obtain a prediction that
follows the target annotation guideline, then a sec-
ond, target-to-source transformation is performed
on this prediction result to check whether it can
be transformed back to the previous source annota-
tion. Transformation results failing in this reversal
verification are discarded, so this strategy is named
predict-self filtration.
A more precious strategy can be called predict-
self reestimation. Instead of using the reversed
transformation procedure for filtration, the rees-
timation strategy integrates the scores given by
the source-to-target and target-to-source annotation
transformation models when evaluating the transfor-
mation candidates. By properly tuning the relative
weights of the two transformation directions, bet-
ter transformation performance would be achieved.
The scores of the two transformation models are
weighted integrated in a log-linear manner:
S+(y|Ms?t,Mt?s,?, x)
= (1? ?)? S(y|Ms?t,?, x)
+ ?? S(x|Mt?s,?, y)
(2)
The weight parameter ? is tuned on the develop-
ing set. To integrating the predict-self reestima-
tion into the iterative transformation training, a re-
versed transformation model is introduced and the
enhanced scoring function above is used when the
function TRANSANNOTATE invokes the function
DECODE.
4 Related Works
Researches focused on the automatic adaptation
between different corpora can be roughly clas-
sified into two kinds, adaptation between differ-
ent domains (with different statistical distribution)
(Blitzer et al2006; Daume? III, 2007), and adapta-
tion between different annotation guidelines (Jiang
et al2009; Zhu et al2011). There are also
some efforts that totally or partially resort to man-
ual transformation rules, to conduct treebank con-
version (Cahill and Mccarthy, 2002; Hockenmaier
and Steedman, 2007; Clark and Curran, 2009), and
word segmentation guideline transformation (Gao
et al2004; Mi et al2008). This work focuses
on the automatic transformation between annotation
guidelines, and proposes better annotation transfor-
mation technologies to improve the transformation
accuracy and the utilization rate of human-annotated
knowledge.
The iterative training procedure proposed in this
work shares some similarity with the co-training al-
gorithm in parsing (Sarkar, 2001), where the train-
ing procedure lets two different models learn from
each other during parsing the raw text. The key
idea of co-training is utilize the complementarity of
different parsing models to mine additional training
data from raw text, while iterative training for an-
notation transformation emphasizes the iterative op-
timization of the parellelly annotated corpora used
416
Partition Sections # of word
CTB
Training 1? 270 0.47M
400? 931
1001? 1151
Developing 301? 325 6.66K
Test 271? 300 7.82K
PD
Training 02? 06 5.86M
Test 01 1.07M
Table 3: Data partitioning for CTB and PD.
to train the transformation models. The predict-
self methodology is implicit in many unsupervised
learning approaches, it has been successfully used
by (Daume? III, 2009) in unsupervised dependency
parsing. We adapt this idea to the scenario of anno-
tation transformation to improve transformation ac-
curacy.
In recent years many works have been devoted to
the word segmentation task. For example, the in-
troduction of global training or complicated features
(Zhang and Clark, 2007; Zhang and Clark, 2010);
the investigation of word structures (Li, 2011);
the strategies of hybrid, joint or stacked modeling
(Nakagawa and Uchimoto, 2007; Kruengkrai et al
2009; Wang et al2010; Sun, 2011), and the semi-
supervised and unsupervised technologies utilizing
raw text (Zhao and Kit, 2008; Johnson and Gold-
water, 2009; Mochihashi et al2009; Hewlett and
Cohen, 2011). We estimate that the annotation trans-
formation technologies can be adopted jointly with
complicated features, system combination and semi-
supervised/unsupervised technologies to further im-
prove segmentation performance.
5 Experiments and Analysis
We perform annotation transformation from Peo-
ple?s Daily (PD) (Yu et al2001) to Penn Chi-
nese Treebank 5.0 (CTB) (Xue et al2005), follow-
ing the same experimental setting as the annotation
adaptation work (Jiang et al2009) for convenience
of comparison. The two corpora are segmented fol-
lowing different segmentation guidelines and differ
largely in quantity of data. CTB is smaller in size
with about 0.5M words, while PD is much larger,
containing nearly 6M words.
Test on (F1%)
Train on CTB SPD
CTB 97.35 86.65(? 10.70)
SPD 91.23(? 3.02) 94.25
Table 4: Performance of the perceptron classifiers for
Chinese word segmentation.
Model Time (s) Accuracy (F1%)
Merging 1.33 93.79
Anno. Adapt. 4.39 97.67
Anno. Trans. 1.33 97.69
Baseline 1.21 97.35
Table 5: Comparison of the baseline annotation transfor-
mation, annotation adaptation and a simple corpus merg-
ing strategy.
To approximate more general scenarios of anno-
tation adaptation problems, we extract from PD a
subset which is comparable to CTB in size. We ran-
domly select 20, 000 sentences (0.45M words) from
the PD training data as the new training set, and
1000/1000 sentences from the PD test data as the
new test/developing set. 2 We name the smaller ver-
sion of PD as SPD. The balanced source corpus and
target corpus also facilitate the investigation of an-
notation transformation.
5.1 Baseline Classifiers for Word Segmentation
We train the baseline perceptron classifiers de-
scribed in section 2 on the training sets of SPD
and CTB, using the developing sets to determine the
best training iterations. The performance measure-
ment indicators for word segmentation is balanced
F-measure, F = 2PR/(P + R), a function of Pre-
cision P and Recall R. where P is the percentage
of words in segmentation result that are segmented
correctly, and R is the percentage of correctly seg-
mented words in the gold standard words.
Accuracies of the baseline classifiers are listed in
Table 4. We also report the performance of the clas-
sifiers on the test sets of the opposite corpora. Ex-
perimental results are in line with our expectations.
A classifier performs better in its corresponding test
set, and performs significantly worse on a test set
following a different annotation guideline.
2There are many extremely long sentences in original PD
corpus, we split them into normal sentences according to period
punctuations.
417
 95.4
 95.6
 95.8
 96
 96.2
 96.4
 0  1  2  3  4  5  6  7  8  9  10
Ac
cu
ra
cy
 (F
%)
Training iterations
Iterative training
Baseline annotation transformation
Figure 1: Learning curve of iterative training for annota-
tion transformation.
5.2 Annotation Transformation vs. Annotation
Adaptation
Experiments of annotation transformation are con-
ducted on the direction of SPD-to-CTB. The trans-
formed corpus can be merged into the regular cor-
pus, so as to train an enhanced classifier. As com-
parison, the cascaded model of annotation adapta-
tion (Jiang et al2009) is faithfully implemented
(yet using our feature representation) and tested on
the same adaptation direction.
Table 5 shows the performances of the classi-
fiers resulted by the baseline annotation transforma-
tion and annotation adaptation, as well as the clas-
sifier trained on the directly merged corpus. The
time costs for decoding are also listed to facilitate
the comparison of practicality. We find that the sim-
ple corpus merging strategy leads to dramatic de-
crease in accuracy, due to the different and incom-
patible annotation guidelines. The baseline annota-
tion transformation method leads to a classifier with
accuracy increment comparable to that of the anno-
tation adaptation strategy, while consuming only one
third of the decoding time.
5.3 Iterative Training with Predict-Self
Reestimation
We adopt the iterative training strategy to the base-
line annotation transformation model. The CTB de-
veloping set is used to determine the best training
iteration for annotation transformation from SPD to
CTB. After each iteration, we test the performance
of the classifier trained on the merged corpus. Fig-
ure 1 shows the performance curve, with iterations
 95.4
 95.6
 95.8
 96
 96.2
 96.4
 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
Ac
cu
ra
cy
 (F
%)
Predict-self ratio
Predict-self reestimation
Predict-self filtration
Baseline annotation transformation
Figure 2: Performance of predict-self filtration and
predict-self reestimation.
 95.4
 95.6
 95.8
 96
 96.2
 96.4
 0  1  2  3  4  5  6  7  8  9  10
Ac
cu
ra
cy
 (F
%)
Training iterations
Iterative training with predict-self reestimation
Iterative training
Figure 3: Learning curve of iterative training with
predict-self reestimation for annotation transformation.
ranging from 1 to 10. The performance of the base-
line annotation transformation model is naturally in-
cluded in the curve (located at iteration 1). The
curve shows that the performance of the classifier
trained on the merged corpus consistently improves
from iteration 2 to iteration 5.
Experimental results of predict-self filtration and
predict-self reestimation are shown in Figure 2.
The curve shows the performance of the predict-self
reestimation according to a series of weight param-
eters, ranging from 0 to 1 with step 0.05. The point
at ? = 0 shows the performance of the baseline
annotation transformation strategy. The upper hor-
izontal line shows the performance of predict-self
filtration. We find that predict-self filtration brings
slight improvement over the baseline, and predict-
self reestimation outperforms the filtration strategy
when ? falls in a proper range. An initial analysis
on the experimental results of predict-self filtration
418
Model Time (s) Accuracy (F1%)
SPD? CTB
Anno. Adapt. 4.39 97.67
Opt. Trans. 1.33 97.97
PD? CTB
Anno. Adapt. 4.76 98.15
Opt. Trans. 1.37 98.43
Previous Works
(Jiang et al2008) 97.85
(Kruengkrai et al2009) 97.87
(Zhang and Clark, 2010) 97.79
(Sun, 2011) 98.17
Table 6: The performance of the iterative annotation
transformation with predict-self reestimation compared
with annotation adaptation.
shows that, the filtration discards 5% of the train-
ing sentences and these discarded sentences contain
nearly 10% of training words. It can be confirmed
that the sentences discarded by predict-self filtra-
tion are much longer and more complicated. With a
properly tuned weight, predict-self reestimation can
make better use of the training data. The best F-
measure improvement achieved over the annotation
transformation baseline is 0.3 points, a little worse
than that brought by iterative training.
Figure 3 shows the performance curve of iterative
annotation transformation with predict-self reesti-
mation. We find that the predict-self reestimation
brings improvement to the iterative training at each
iteration. The maximum performance is achieved
at iteration 4. The corresponding model is evalu-
ated on the test set of CTB, table 6 shows the ex-
perimental results. Compared to annotation adapta-
tion, the optimized annotation transformation strat-
egy leads to a classifier with significantly higher ac-
curacy and several times faster processing. When
using the whole PD as the source corpus, the final
classifier 3 achieves an F-measure of 98.43%, sig-
nificantly outperforms previous works although us-
ing a single classifier with only local features. Of
course, the comparison between our system and pre-
vious works without using additional training data
is unfair. This work aim to find another way to im-
prove Chinese word segmentation, which focuses on
the collection of more training data instead of mak-
3The predict-self reestimation ratio ? is fixed after the first
training iteration for efficiency.
ing full use of a certain corpus. We believe that the
performance can be further improved by adopting
the advanced technologies of previous works, such
as complicated features and model combination.
Considering the fact that today some corpora for
word segmentation are really large (usually tens
of thousands of sentences), it is necessary to ob-
tain the latest CTB and investigate whether and
how much does annotation transformation bring im-
provement to a much higher baseline. On the other
hand, it is valuable to conduct experiments with
more source-annotated training data, such as the
SIGHAN dataset, to investigate the trend of im-
provement along with the increment of the addi-
tional annotated sentences. It is also valuable to
evaluate the improved word segmenter on the out-
of-domain datasets. However, currently most cor-
pora for Chinese word segmentation do not explic-
itly distinguish the domains of their data sections, it
makes such evaluations difficult to conduct.
6 Conclusion and Future Works
In this paper, we first describe an annotation trans-
formation algorithm to automatically transform a
human-annotated corpus from one annotation guide-
line to another. Then we propose two optimization
strategies, iterative training and predict-self reesti-
mation, to further improve the accuracy of anno-
tation guideline transformation. On Chinese word
segmentation, the optimized annotation transforma-
tion strategy leads to classifiers with obviously bet-
ter performance and several times faster processing
on the same datasets, compared to annotation adap-
tation. When adopting the whole PD as the source
corpus, the final classifier significantly outperforms
previous works on CTB 5.0, although using a single
classifier with only local features.
As future works, we will investigate the accel-
eration of the iterative training and the weight pa-
rameter tuning, and extend the optimized annotation
transformation strategy to joint Chinese word seg-
mentation and POS tagging, parsing and other NLP
tasks.
Acknowledgments
The authors were supported by National Natural
Science Foundation of China, Contracts 90920004
419
and 61100082, and 863 State Key Project No.
2011AA01A207. We are grateful to the anonymous
reviewers for their thorough reviewing and valuable
suggestions.
References
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In Proceedings of EMNLP.
Aoife Cahill and Mairead Mccarthy. 2002. Automatic
annotation of the penn treebank with lfg f-structure in-
formation. In in Proceedings of the LREC Workshop.
Stephen Clark and James R. Curran. 2009. Comparing
the accuracy of ccg and penn treebank parsers. In Pro-
ceedings of ACL-IJCNLP.
Michael Collins and Brian Roark. 2004. Incremental
parsing with the perceptron algorithm. In Proceedings
of ACL 2004.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings of
EMNLP, pages 1?8, Philadelphia, USA.
Hal Daume? III. 2007. Frustratingly easy domain adapta-
tion. In Proceedings of ACL.
Hal Daume? III. 2009. Unsupervised search-based struc-
tured prediction. In Proceedings of ICML.
Jianfeng Gao, Andi Wu, Mu Li, Chang-Ning Huang,
Hongqiao Li, Xinsong Xia, and Haowei Qin. 2004.
Adaptive chinese word segmentation. In Proceedings
of ACL.
Daniel Hewlett and Paul Cohen. 2011. Fully unsuper-
vised word segmentation with bve and mdl. In Pro-
ceedings of ACL.
Julia Hockenmaier and Mark Steedman. 2007. Ccgbank:
a corpus of ccg derivations and dependency structures
extracted from the penn treebank. In Computational
Linguistics, volume 33(3), pages 355?396.
Wenbin Jiang, Liang Huang, Yajuan Lv, and Qun Liu.
2008. A cascaded linear model for joint chinese word
segmentation and part-of-speech tagging. In Proceed-
ings of ACL.
Wenbin Jiang, Liang Huang, and Qun Liu. 2009. Au-
tomatic adaptation of annotation standards: Chinese
word segmentation and pos tagging?a case study. In
Proceedings of the 47th ACL.
Mark Johnson and Sharon Goldwater. 2009. Improving
nonparameteric bayesian inference: experiments on
unsupervised word segmentation with adaptor gram-
mars. In Proceedings of NAACL.
Canasai Kruengkrai, Kiyotaka Uchimoto, Junichi
Kazama, Yiou Wang, Kentaro Torisawa, and Hitoshi
Isahara. 2009. An error-driven word-character hybrid
model for joint chinese word segmentation and pos
tagging. In Proceedings of ACL-IJCNLP.
Zhongguo Li. 2011. Parsing the internal structure of
words: A new paradigm for chineseword segmenta-
tion. In Proceedings of ACL.
Haitao Mi, Deyi Xiong, and Qun Liu. 2008. Research
on strategy of integrating chinese lexical analysis and
parser. In Journal of Chinese Information Processing.
Daichi Mochihashi, Takeshi Yamada, and Naonori Ueda.
2009. Bayesian unsupervised word segmentation with
nested pitman-yor language modeling. In Proceedings
of ACL-IJCNLP.
Tetsuji Nakagawa and Kiyotaka Uchimoto. 2007. A hy-
brid approach to word segmentation and pos tagging.
In Proceedings of ACL.
Hwee Tou Ng and Jin Kiat Low. 2004. Chinese part-of-
speech tagging: One-at-a-time or all-at-once? word-
based or character-based? In Proceedings of EMNLP.
Anoop Sarkar. 2001. Applying co-training methods to
statistical parsing. In Proceedings of NAACL.
Weiwei Sun. 2011. A stacked sub-word model for
joint chinese word segmentation and part-of-speech
tagging. In Proceedings of ACL.
Kun Wang, Chengqing Zong, and Keh-Yih Su. 2010. A
character-based joint model for chinese word segmen-
tation. In Proceedings of COLING.
Nianwen Xue and Libin Shen. 2003. Chinese word seg-
mentation as lmr tagging. In Proceedings of SIGHAN
Workshop.
Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha
Palmer. 2005. The penn chinese treebank: Phrase
structure annotation of a large corpus. In Natural Lan-
guage Engineering.
Shiwen Yu, Jianming Lu, Xuefeng Zhu, Huiming Duan,
Shiyong Kang, Honglin Sun, Hui Wang, Qiang Zhao,
and Weidong Zhan. 2001. Processing norms of mod-
ern chinese corpus. Technical report.
Yue Zhang and Stephen Clark. 2007. Chinese segmenta-
tion with a word-based perceptron algorithm. In Pro-
ceedings of ACL 2007.
Yue Zhang and Stephen Clark. 2010. A fast decoder for
joint word segmentation and pos-tagging using a sin-
gle discriminative model. In Proceedings of EMNLP.
Hai Zhao and Chunyu Kit. 2008. Unsupervised segmen-
tation helps supervised learning of character tagging
for word segmentation and named entity recognition.
In Proceedings of SIGHAN Workshop.
Muhua Zhu, Jingbo Zhu, and Minghan Hu. 2011. Better
automatic treebank conversion using a feature-based
approach. In Proceedings of ACL.
420
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1066?1076,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Translation with Source Constituency and Dependency Trees
Fandong Meng?? Jun Xie? Linfeng Song?? Yajuan Lu?? Qun Liu??
?Key Laboratory of Intelligent Information Processing
Institute of Computing Technology, Chinese Academy of Sciences
?University of Chinese Academy of Sciences
{mengfandong,xiejun,songlinfeng,lvyajuan}@ict.ac.cn
?Centre for Next Generation Localisation
Faculty of Engineering and Computing, Dublin City University
qliu@computing.dcu.ie
Abstract
We present a novel translation model, which
simultaneously exploits the constituency and
dependency trees on the source side, to com-
bine the advantages of two types of trees. We
take head-dependents relations of dependency
trees as backbone and incorporate phrasal n-
odes of constituency trees as the source side
of our translation rules, and the target side as
strings. Our rules hold the property of long
distance reorderings and the compatibility
with phrases. Large-scale experimental result-
s show that our model achieves significantly
improvements over the constituency-to-string
(+2.45 BLEU on average) and dependency-
to-string (+0.91 BLEU on average) model-
s, which only employ single type of trees,
and significantly outperforms the state-of-the-
art hierarchical phrase-based model (+1.12
BLEU on average), on three Chinese-English
NIST test sets.
1 Introduction
In recent years, syntax-based models have become a
hot topic in statistical machine translation. Accord-
ing to the linguistic structures, these models can be
broadly divided into two categories: constituency-
based models (Yamada and Knight, 2001; Graehl
and Knight, 2004; Liu et al, 2006; Huang et al,
2006), and dependency-based models (Lin, 2004;
Ding and Palmer, 2005; Quirk et al, 2005; Xiong
et al, 2007; Shen et al, 2008; Xie et al, 2011).
These two kinds of models have their own advan-
tages, as they capture different linguistic phenome-
na. Constituency trees describe how words and se-
quences of words combine to form constituents, and
constituency-based models show better compatibil-
ity with phrases. However, dependency trees de-
scribe the grammatical relation between words of
the sentence, and represent long distance dependen-
cies in a concise manner. Dependency-based mod-
els, such as dependency-to-string model (Xie et al,
2011), exhibit better capability of long distance re-
orderings.
In this paper, we propose to combine the advan-
tages of source side constituency and dependency
trees. Since the dependency tree is structurally sim-
pler and directly represents long distance depen-
dencies, we take dependency trees as the backbone
and incorporate constituents to them. Our mod-
el employs rules that represent the source side as
head-dependents relations which are incorporated
with constituency phrasal nodes, and the target side
as strings. A head-dependents relation (Xie et al,
2011) is composed of a head and all its dependents in
dependency trees, and it encodes phrase pattern and
sentence pattern (typically long distance reordering
relations). With the advantages of head-dependents
relations, the translation rules of our model hold the
property of long distance reorderings and the com-
patibility with phrases.
Our new model (Section 2) extracts rules from
word-aligned pairs of source trees (constituency
and dependency) and target strings (Section 3), and
translate source trees into target strings by employ-
ing a bottom-up chart-based algorithm (Section 4).
Compared with the constituency-to-string (Liu et al,
2006) and dependency-to-string (Xie et al, 2011)
models that only employ a single type of trees, our
1066
??/VV
???/NR ?/AD ???/NN
??/NR ?/M ??/JJ
??/OD
NP1
VP2
VP3
????? ? ????? ? ????
NR AD VV NR OD M JJ NN
NP1
CLP
QP
NP
NP
VP2
ADVP
VP3
NP
IP
(a)
(c)
Intel         will   launch  Asia     first              super     laptop
Chinese: ??? ? ?? ?? ?? ? ?? ???
English:  Intel will launch the first Ultrabook in Asia
ADVP NP
(b)
Figure 1: Illustration of phrases that can not be captured by a dependency tree (b) while captured by a constituency tree
(a), where the bold phrasal nodes NP1,VP2,VP3 indicate the phrases which can not be captured by dependency syn-
tactic phrases. (c) is the corresponding bilingual sentences. The subscripts of phrasal nodes are used for distinguishing
the nodes with same phrasal categories.
approach yields encouraging results by exploiting t-
wo types of trees. Large-scale experiments (Sec-
tion 5) on Chinese-English translation show that
our model significantly outperforms the state-of-
the-art single constituency-to-string model by av-
eraged +2.45 BLEU points, dependency-to-string
model by averaged +0.91 BLEU points, and hierar-
chical phrase-based model (Chiang, 2005) by aver-
aged +1.12 BLEU points, on three Chinese-English
NIST test sets.
2 Grammar
We take head-dependents relations of dependency
trees as backbone and incorporate phrasal nodes of
constituency trees as the source side of our transla-
tion rules, and the target side as strings. A head-
dependents relation consists of a head and all its de-
pendents in dependency trees, and it can represent
long distance dependencies. Incorporating phrasal
nodes of constituency trees into head-dependents
relations further enhances the compatibility with
phrases of our rules. Figure 1 shows an example of
phrases which can not be captured by a dependen-
cy tree while captured by a constituency tree, such
as the bold phrasal nodes NP1,VP2 and VP3. The
phrasal node NP1 in the constituency tree indicates
that ??? )P? is a noun phrase and it should
be translated as a basic unit, while in the depen-
dency tree it is a non-syntactic phrase. The head-
dependents relation in the top level of the dependen-
cy tree presents long distance dependencies of the
words ?=A?, ???, ????, and ?)P? in a
concise manner, which is useful for long distance re-
ordering. We adopt this kind of rule representation
to hold the property of long distance reorderings and
the compatibility with phrases.
Figure 2 shows two examples of our translation
rules corresponding to the top level of Figure 1-(b).
We can see that r1 captures a head-dependents rela-
tion, while r2 extends r1 by incorporating a phrasal
node VP2 to replace the two nodes ???/VV? and
?)P/NN?. As shown in Figure 1-(b), VP2 con-
sists of two parts, a head node ???/VV? and a
subtree rooted at the dependent node ?)P/NN?.
Therefore, we use VP2 and the POS tags of the t-
wo nodes VV and NN to denote the part covered
by VP2 in r2, to indicate that the source sequence
covered by VP2 can be translated by a bilingual
phrase. Since VP2 covers a head node ???/VV?,
we represent r2 by constructing a new head node
1067
1
??
??? ? 1
1
2
1 2
??? ? 1
Figure 2: Two examples of our translation rules corre-
sponding to the top level of Figure 1-(b). r1 captures a
head-dependents relation, and r2 extends r1 by incorpo-
rating a phrasal node VP2. ?x1:NN? indicates a substitu-
tion site which can be replaced by a subtree whose root
has POS tag ?NN?. ?x1:VP2|||VV NN? indicates a sub-
stitution site which can be replaced by a source phrase
covered by a phrasal node VP (the phrasal node consist-
s of two dependency nodes with POS tag VV and NN,
respectively). The underline denotes a leaf node.
VP2|||VV NN. For simplicity, we use a shorten for-
m CHDR to represent the head-dependents relations
with/without constituency phrasal nodes.
Formally, our grammar G is defined as a 5-tuple
G = ??, Nc, Nd,?, R?, where ? is a set of source
language terminals, Nc is a set of constituency
phrasal categories, Nd is a set of categories (POS
tags) for the terminals in ?, ? is a set of target lan-
guage terminals, and R is a set of translation rules
that include bilingual phrases for translating source
language terminals and CHDR rules for translation
and reordering. A CHDR rule is represented as a
triple ?t, s,??, where:
? t is CHDR with each node labeled by a ter-
minal from ? or a variable from a set X =
{x1, x2, ? ? ? } constrained by a terminal from ?
or a category from Nd or a joint category (con-
structed by the categories from Nc and Nd);
? s ? (X ??) denotes the target side string;
? ? denotes one-to-one links between nontermi-
nals in t and variables in s.
We use the lexicon dependency grammar (Hellwig,
2006) which adopts a bracket representation to ex-
press the head-dependents relation and CHDR. For
example, the left-hand sides of r1 and r2 in Figure 2
can be respectively represented as follows:
(=A) (?)?? (x1:NN)
(=A) (?) x1:VP2|||VV NN
??/VV
???/NR ?/AD ???/NN
??/NR?/M ??/JJ
??/OD
NP1
VP2
VP3
??? ? ?? ?? ?? ? ?? ???
Parseing      Labelling
???/NR ?/AD launch
Intel will launch ????? in Asia
Intel will launch in Asia
(a)
(b)
(c)
(d)
(e)
NP1
?/M
??/OD
Intel will launch in Asiathe    first(f)
Ultrabook
Ultrabook
???/NN
??/NR?/M ??/JJ
??/OD
NP1
r3
r4 r5
r6
r7
?/M
??/OD
r8
(x1:NR) (x2:AD) ?? (x3:???) x1 x2 launch x3
Intel???
? will
(??)(x1:M)x2:NP1|||JJ_NN x1 x2 in Aisa 
????? Ultrabook
?? (?) the first
Translation Rules
r3
r4
r5
r6
r7
r8
(g)
Figure 3: An example derivation of translation. (g) lists
all the translation rules. r3, r6 and r8 are CHDR rules,
while r4, r5 and r7 are bilingual phrases, which are used
for translating source terminals. The dash lines indicate
the reordering when employing a translation rule.
The formalized presentation of r2 in Figure 2-(b):
t = (=A) (?) x1:VP2|||VV NN
s = Intel will x1
?= x1:VP2|||VV NN ? x1
where the underline indicates a leaf node.
Figure 3 gives an example of the translation
derivation in our model, with the translation rules
1068
listed in (g). r3, r6 and r8 are CHDR rules, while
r4, r5 and r7 are bilingual phrases, which are used
for translating source language terminals. Given a
sentence to translate in (a), we first parse it into a
constituency tree and a dependency tree, then label
the phrasal nodes from the constituency tree to the
dependency tree, and yield (b). Then, we translate
it into a target string by the following steps. At the
root node, we apply rule r3 to translate the top level
head-dependents relation and results in four unfin-
ished substructures and target strings in (c). From
(c) to (d), there are three steps (one rule for one step).
We use r4 to translate ?=A? to ?Intel?, r5 to
translate ??? to ?will?, and r6 to translate the right-
most unfinished part. Then, we apply r7 to translate
the phrase ???)P? to ?Ultrabook?, and yield
(e). Finally, we apply r8 to translate the last frag-
ment to ?the first?, and get the final result (f).
3 Rule Extraction
In this section, we describe how to extract rules from
a set of 4-tuples ?C, T, S,A?, where C is a source
constituency tree, T is a source dependency tree, S
is a target side sentence, and A is an word alignmen-
t relation between T /C and S. We extract CHDR
rules from each 4-tuple ?C, T, S,A? based on GHK-
M algorithm (Galley et al, 2004) with three steps:
1. Label the dependency tree with phrasal nodes
from the constituency tree, and annotate align-
ment information to the phrasal nodes labeled
dependency tree (Section 3.1).
2. Identify acceptable CHDR fragments from the
annotated dependency tree for rule induction
(Section 3.2).
3. Induce a set of lexicalized and generalized
CHDR rules from the acceptable fragments
(Section 3.3).
3.1 Annotation
Given a 4-tuple ?C, T, S,A?, we first label phrasal
nodes from the constituency tree C to the depen-
dency tree T , which can be easily accomplished by
phrases mapping according to the common covered
source sequences. As dependency trees can capture
some phrasal information by dependency syntactic
??/VV
{3-3}{1-8}
???/NR
{1-1}{1-1}
?/AD
{2-2}{2-2}
???/NN
{6-6}{4-8}
??/NR
{7-8}{7-8}
?/M
{null}{4-5}
??/JJ
{6-6}{6-6}
??/OD
{4-5}{4-5}
NP1
<6-6>
VP2
<3-8>
VP3
<2-8>
Figure 4: An annotated dependency tree. Each node is
annotated with two spans, the former is node span and
the latter subtree span. The fragments covered by phrasal
nodes are annotated with phrasal spans. The nodes de-
noted by the solid line box are not nsp consistent.
phrases, in order to complement the information that
dependency trees can not capture, we only label the
phrasal nodes that cover dependency non-syntactic
phrases.
Then, we annotate alignment information to the
phrasal nodes labeled dependency tree T , as shown
in Figure 4. For description convenience, we make
use of the notion of spans (Fox, 2002; Lin, 2004).
Given a node n in the source phrasal nodes labeled
T with word alignment information, the spans of n
induced by the word alignment are consecutive se-
quences of words in the target sentence. As shown
in Figure 4, we annotate each node n of phrasal n-
odes labeled T with two attributes: node span and
subtree span; besides, we annotate phrasal span to
the parts covered by phrasal nodes in each subtree
rooted at n. The three types of spans are defined as
follows:
Definition 1 Given a node n, its node span nsp(n)
is the consecutive target word sequence aligned with
the node n.
Take the node ???/NR? in Figure 4 for example,
nsp(??/NR)={7-8}, which corresponds to the tar-
get words ?in? and ?Asia?.
Definition 2 Given a subtree T ? rooted at n, the
subtree span tsp(n) of n is the consecutive target
word sequence from the lower bound of the nsp of
1069
all nodes in T ? to the upper bound of the same set of
spans.
For instance, tsp()P/NN)={4-8}, which corre-
sponds to the target words ?the first Ultrabook in A-
sia?, whose indexes are from 4 to 8.
Definition 3 Given a fragment f covered by a
phrasal node, the phrasal span psp(f) of f is
the consecutive target word sequence aligned with
source string covered by f .
For example, psp(VP2)=?3-8?, which corresponds
to the target word sequence ?launch the first Ultra-
book in Asia?.
We say nsp, tsp and psp are consistent according
to the notion in the phrase-based model (Koehn et
al., 2003). For example, nsp(??/NR), tsp()P
/NN) and psp(NP1) are consistent while nsp(?
?/JJ) and nsp()P/NN) are not consistent.
The annotation can be achieved by a single pos-
torder transversal of the phrasal nodes labeled de-
pendency tree. For simplicity, we call the annotat-
ed phrasal nodes labeled dependency tree annotated
dependency tree. The extraction of bilingual phrases
(including the translation of head node, dependen-
cy syntactic phrases and the fragment covered by a
phrasal node) can be readily achieved by the algo-
rithm described in Koehn et al, (2003). In the fol-
lowing, we focus on CHDR rules extraction.
3.2 Acceptable Fragments Identification
Before present the method of acceptable fragments
identification, we give a brief description of CHDR
fragments. A CHDR fragment is an annotated frag-
ment that consists of a source head-dependents rela-
tion with/without constituency phrasal nodes, a tar-
get string and the word alignment information be-
tween the source and target side. We identify the ac-
ceptable CHDR fragments that are suitable for rule
induction from the annotated dependency tree. We
divide the acceptable CHDR fragments into two cat-
egories depending on whether the fragments con-
tain phrasal nodes. If an acceptable CHDR frag-
ment does not contain phrasal nodes, we call it
CHDR-normal fragment, otherwise CHDR-phrasal
fragment. Given a CHDR fragment F rooted at n,
we say F is acceptable if it satisfies any one of the
following properties:
CHDR-phrasal Rules
r9: (???)(?)x1:VP2|||VV_NN Intel will x1
r10: (x1:NR)(x2:AD)x3:VP2|||VV_NN x1 x2 x3
r11: (???)x1:VP3|||AD_VV_NN Intel x1
r12: (x1:NR)x2:VP3|||AD_VV_NN x1 x2
CHDR-normal Rules
r4: (x1:NR) (x2:AD) ?? (x3:NN) x1 x2 launch x3
Intel will launch x1r3: (???) (?)?? (x1:NN)
r2: (x1:NR) (x2:AD) ?? (x3:???) x1 x2 launch x3
r1: (???) (?)?? (x1:???) Intel will launch x1
r5: (???) (?) x1:VV (x2:???) Intel will x1 x2
r8: (x1:NR) (x2:AD) x3:VV (x4:NN) x1 x2 x3 x4
r6: (x1:NR) (x2:AD) x3:VV (x4:???) x1 x2 x3 x4
Intel will x1 x2r7: (???) (?) x1:VV (x2:NN)
(d)
??/VV
???/NR ?/AD ???/NN
Intel
1
will
2
launch
3
the first Ultrabook in Asia
4-8
(a)
Intel
1
will
2
launch the first Ultrabook in Asia
3-8
VP2
??/VV
???/NR ?/AD ???/NN(b)
(c)
Intel
1
will launch the first Ultrabook in Asia
2-8
VP3
??/VV
???/NR ?/AD ???/NN
VP2|||VV_NN
VP3|||AD_VV_NN
Figure 5: Examples of a CHDR-normal fragment (a), two
CHDR-phrasal fragments (b) and (c) that are identified
from the top level of the annotated dependency tree in
Figure 4, and the corresponding CHDR rules (d) induced
from (a), (b) and (c). The underline denotes a leaf node.
1. Without phrasal nodes, the node span of the
root n is consistent and the subtree spans of
n?s all dependents are consistent. For example,
Figure 5-(a) shows a CHDR-normal fragmen-
t that identified from the top level of the an-
notated dependency tree in Figure 4, since the
nsp(??/VV), tsp(=A/NR), tsp(?/AD)
and tsp()P/NN) are consistent.
1070
2. With phrasal nodes, the phrasal spans of
phrasal nodes are consistent; and for the other
nodes, the node span of head (if it is not cov-
ered by any phrasal node) is consistent, and the
subtree spans of dependents are consistent. For
instance, Figure 5-(b) and (c) show two CHDR-
phrasal fragments identified from the top level
of Figure 4. In Figure 5-(b), psp(VP2), tsp(=
A/NR) and tsp(?/AD) are consistent. In
Figure 5-(c), psp(VP3) and tsp(=A/NR)
are consistent.
The identification of acceptable fragments can be
achieved by a single postorder transversal of the an-
notated dependency tree. Typically, each acceptable
fragment contains at most three types of nodes: head
node, head of the related CHDR; internal nodes, in-
ternal nodes of the related CHDR except head node;
leaf nodes, leaf nodes of the related CHDR.
3.3 Rule Induction
From each acceptable CHDR fragment, we induce
a set of lexicalized and generalized CHDR rules.
We induce CHDR-normal rules and CHDR-phrasal
rules from CHDR-normal fragments and CHDR-
phrasal fragments, respectively.
We first induce a lexicalized form of CHDR rule
from an acceptable CHDR fragment:
1. For a CHDR-normal fragment, we first mark
the internal nodes as substitution sites. This
forms the input of a CHDR-normal rule. Then
we generate the target string according to the
node span of the head and the subtree spans of
the dependents, and turn the word sequences
covered by the internal nodes into variables.
This forms the output of a lexicalized CHDR-
normal rule.
2. For a CHDR-phrasal fragment, we first mark
the internal nodes and the phrasal nodes as sub-
stitution sites. This forms the input of a CHDR-
phrasal rule. Then we construct the output of
the CHDR-phrasal rule in almost the same way
with constructing CHDR-normal rules, except
that we replace the target sequences covered by
the internal nodes and the phrasal nodes with
variables.
For example, rule r1 in Figure 5-(d) is a lexicalized
CHDR-normal rule induced from the CHDR-normal
fragment in Figure 5-(a). r9 and r11 are CHDR-
phrasal rules induced from the CHDR-phrasal frag-
ment in Figure 5-(b) and Figure 5-(c) respectively.
As we can see, these CHDR-phrasal rules are par-
tially unlexicalized.
To alleviate the sparseness problem, we gener-
alize the lexicalized CHDR-normal rules and par-
tially unlexicalized CHDR-phrasal rules with un-
lexicalized nodes by the method proposed in Xie
et al, (2011). As the modification relations be-
tween head and dependents are determined by the
edges, we can replace the lexical word of each n-
ode with its category (POS tag) and obtain new
head-dependents relations with unlexicalized nodes
keeping the same modification relations. We gen-
eralize the rule by simultaneously turn the nodes of
the same type (head, internal, leaf) into their cate-
gories. For example, CHDR-normal rules r2 ? r7
are generalized from r1 in Figure 5-(d). Besides, r10
and r12 are the corresponding generalized CHDR-
phrasal rules. Actually, our CHDR rules are the su-
perset of head-dependents relation rules in Xie et
al., (2011). CHDR-normal rules are equivalent with
the head-dependents relation rules and the CHDR-
phrasal rules are the extension of these rules. For
convenience of description, we use the subscript to
distinguish the phrasal nodes with the same catego-
ry, such as VP2 and VP3. In actual operation, we use
VP instead of VP2 and VP3.
We handle the unaligned words of the target side
by extending the node spans of the lexicalized head
and leaf nodes, and the subtree spans of the lexical-
ized dependents, on both left and right directions.
This procedure is similar with the method of Och
and Ney, (2004). During this process, we might ob-
tain m(m ? 1) CHDR rules from an acceptable
fragment. Each of these rules is assigned with a frac-
tional count 1/m. We take the extracted rule set as
observed data and make use of relative frequency es-
timator to obtain the translation probabilities P (t|s)
and P (s|t).
4 Decoding and the Model
Following Och and Ney, (2002), we adopt a general
loglinear model. Let d be a derivation that convert a
1071
source phrasal nodes labeled dependency tree into a
target string e. The probability of d is defined as:
P (d) ?
?
i
?i(d)?i (1)
where ?i are features defined on derivations and ?i
are feature weights. In our experiments of this paper,
the features are used as follows:
? CHDR rules translation probabilities P (t|s)
and P (s|t), and CHDR rules lexical translation
probabilities Plex(t|s) and Plex(s|t);
? bilingual phrases translation probabilities
Pbp(t|s) and Pbp(s|t), and bilingual phrases
lexical translation probabilities Pbplex(t|s) and
Pbplex(s|t);
? rule penalty exp(?1);
? pseudo translation rule penalty exp(?1);
? target word penalty exp(|e|);
? language model Plm(e).
We have twelve features in our model. The values of
the first four features are accumulated on the CHDR
rules and the next four features are accumulated on
the bilingual phrases. We also use a pseudo transla-
tion rule (constructed according to the word order of
head-dependents relation) as a feature to guarantee
the complete translation when no matched rules can
be found during decoding.
Our decoder is based on bottom-up chart-based
algorithm. It finds the best derivation that convert
the input phrasal nodes labeled dependency tree into
a target string among all possible derivations. Giv-
en the source constituency tree and dependency tree,
we first generate phrasal nodes labeled dependency
tree T as described in Section 3.1, then the decoder
transverses each node in T by postorder. For each
node n, it enumerates all instances of CHDR rooted
at n, and checks the rule set for matched translation
rules. A larger translation is generated by substitut-
ing the variables in the target side of a translation
rule with the translations of the corresponding de-
pendents. Cube pruning (Chiang, 2007; Huang and
Chiang, 2007) is used to find the k-best items with
integrated language model for each node.
To balance the performance and speed of the de-
coder, we limit the search space by reducing the
number of translation rules used for each node.
There are two ways to limit the rule table size: by
a fixed limit (rule-limit) of how many rules are re-
trieved for each input node, and by a threshold (rule-
threshold) to specify that the rule with a score low-
er than ? times of the best score should be discard-
ed. On the other hand, instead of keeping the full
list of candidates for a given node, we keep a top-
scoring subset of the candidates. This can also be
done by a fixed limit (stack-limit) and a threshold
(stack-threshold).
5 Experiments
We evaluated the performance of our model by com-
paring with hierarchical phrase-based model (Chi-
ang, 2007), constituency-to-string model (Liu et al,
2006) and dependency-to-string model (Xie et al,
2011) on Chinese-English translation. First, we de-
scribe data preparation (Section 5.1) and systems
(Section 5.2). Then, we validate that our model sig-
nificantly outperforms all the other baseline models
(Section 5.3). Finally, we give detail analysis (Sec-
tion 5.4).
5.1 Data Preparation
Our training data consists of 1.25M sentence pairs
extracted from LDC 1 data. We choose NIST MT
Evaluation test set 2002 as our development set,
NIST MT Evaluation test sets 2003 (MT03), 2004
(MT04) and 2005 (MT05) as our test sets. The qual-
ity of translations is evaluated by the case insensitive
NIST BLEU-4 metric 2.
We parse the source sentences to constituency
trees (without binarization) and projective depen-
dency trees with Stanford Parser (Klein and Man-
ning, 2002). The word alignments are obtained by
running GIZA++ (Och and Ney, 2003) on the corpus
in both directions and using the ?grow-diag-final-
and? balance strategy (Koehn et al, 2003). We get
bilingual phrases from word-aligned data with algo-
rithm described in Koehn et al (2003) by running
Moses Toolkit 3. We apply SRI Language Modeling
Toolkit (Stolcke and others, 2002) to train a 4-gram
1Including LDC2002E18, LDC2003E07, LDC2003E14,
Hansards portion of LDC2004T07, LDC2004T08 and LD-
C2005T06.
2ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v11b.pl
3http://www.statmt.org/moses/
1072
System Rule # MT03 MT04 MT05 Average
Moses-chart 116.4M 34.65 36.47 34.39 35.17
cons2str 25.4M+32.5M 33.14 35.12 33.27 33.84
dep2str 19.6M+32.5M 34.85 36.57 34.72 35.38
consdep2str 23.3M+32.5M 35.57* 37.68* 35.62* 36.29
Table 1: Statistics of the extracted rules on training data and the BLEU scores (%) on the test sets of different systems.
The ?+? denotes that the rules are composed of syntactic translation rules and bilingual phrases (32.5M). The ?*?
denotes that the results are significantly better than all the other systems (p<0.01).
language model with modified Kneser-Ney smooth-
ing on the Xinhua portion of the English Gigaword
corpus. We make use of the standard MERT (Och,
2003) to tune the feature weights in order to maxi-
mize the system?s BLEU score on the development
set. The statistical significance test is performed by
sign-test (Collins et al, 2005).
5.2 Systems
We take the open source hierarchical phrase-based
system Moses-chart (with default configuration),
our in-house constituency-to-string system cons2str
and dependency-to-string system dep2str as our
baseline systems.
For cons2str, we follow Liu et al, (Liu et al,
2006) to strict that the height of a rule tree is no
greater than 3 and phrase length is no greater than
7. To keep consistent with our proposed model,
we implement the dependency-to-string model (X-
ie et al, 2011) with GHKM (Galley et al, 2004)
rule extraction algorithm and utilize bilingual phras-
es to translate source head node and dependency
syntactic phrases. Our dep2str shows comparable
performance with Xie et al, (2011), which can be
seen by comparing with the results of hierarchical
phrase-based model in our experiments. For dep2str
and our proposed model consdep2str, we set rule-
threshold and stack-threshold to 10?3, rule-limit to
100, stack-limit to 300, and phrase length limit to 7.
5.3 Experimental Results
Table 1 illustrates the translation results of our ex-
periments. As we can see, our consdep2str sys-
tem has gained the best results on all test sets, with
+1.12 BLEU points higher than Moses-chart, +2.45
BLEU points higher than cons2str, and +0.91 BLEU
points higher than dep2str, averagely on MT03,
MT04 and MT05. Our model significantly outper-
forms all the other baseline models, with p<0.01
on statistical significance test sign-test (Collins et
al., 2005). By exploiting two types of trees on
source side, our model gains significant improve-
ments over constituency-to-string and dependency-
to-string models, which employ single type of trees.
Table 1 also lists the statistical results of rules ex-
tracted from training data by different systems. Ac-
cording to our statistics, the number of rules extract-
ed by our consdep2str system is about 18.88% larger
than dep2str, without regard to the 32.5M bilingual
phrases. The extra rules are CHDR-phrasal rules,
which can bring in BLEU improvements by enhanc-
ing the compatibility with phrases. We will conduct
a deep analysis in the next sub-section.
5.4 Analysis
In this section, we first illustrate the influence of
CHDR-phrasal rules in our consdep2str model. We
calculate the proportion of 1-best translations in test
sets that employ CHDR-phrasal rules, and we cal-
l this proportion ?CHDR-phrasal Sent.?. Besides,
the proportion of CHDR-phrasal rules in all CHDR
rules is calculated in these translations, and we cal-
l this proportion ?CHDR-phrasal Rule?. Table 2
lists the using of CHDR-phrasal rules on test sets,
showing that CHDR-phrasal Sent. on all test sets
are higher than 50%, and CHDR-phrasal Rule on al-
l three test sets are higher than 10%. These results
indicate that CHDR-phrasal rules do play a role in
decoding.
Furthermore, we compare some actual transla-
tions of our test sets generated by cons2str, de-
p2str and consdep2str systems, as shown in Fig-
ure 6. In the first example, the Chinese input hold-
s long distance dependencies ???I ?? ?
... \u ... L? '??, which correspond
to the sentence pattern ?noun+adverb+prepositional
1073
System MT03 MT04 MT05
CHDR-phrasal Sent. 50.71 61.80 56.19
CHDR-phrasal Rule 10.53 13.55 10.83
Table 2: The proportion (%) of 1-best translations that
employs CHDR-phrasal rules (CHDR-phrasal Sent.) and
the proportion (%) of CHDR-phrasal rules in all CHDR
rules in these translations (CHDR-phrasal Rule).
phrase+verb+noun?. Cons2str gives a bad result
with wrong global reordering, while our consdep2str
system gains an almost correct result since we cap-
ture this pattern by CHDR-normal rules. In the sec-
ond example, we can see that the Chinese phrase
?2g?y? is a non-syntactic phrase in the depen-
dency tree, and this phrase can not be captured by
head-dependents relation rules in Xie et al, (2011),
thus can not be translated as one unit. Since we en-
code constituency phrasal nodes to the dependency
tree, ?2g?y? is labeled by a phrasal node ?VP?
(means verb phrase), which can be captured by our
CHDR-phrasal rules and translated into the correct
result ?reemergence? with bilingual phrases.
By combining the merits of constituency and
dependency trees, our consdep2str model learns
CHDR-normal rules to acquire the property of long
distance reorderings and CHDR-phrasal rules to ob-
tain good compatibility with phrases.
6 Related Work
In recent years, syntax-based models have witnessed
promising improvements. Some researchers make
efforts on constituency-based models (Graehl and
Knight, 2004; Liu et al, 2006; Huang et al, 2006;
Zhang et al, 2007; Mi et al, 2008; Liu et al, 2009;
Liu et al, 2011; Zhai et al, 2012). Some works pay
attention to dependency-based models (Lin, 2004;
Ding and Palmer, 2005; Quirk et al, 2005; Xiong et
al., 2007; Shen et al, 2008; Xie et al, 2011). These
models are based on single type of trees.
There are also some approaches combining mer-
its of different structures. Marton and Resnik (2008)
took the source constituency tree into account and
added soft constraints to the hierarchical phrase-
based model (Chiang, 2005). Cherry (2008) u-
tilized dependency tree to add syntactic cohesion
to the phrased-based model. Mi and Liu, (2010)
proposed a constituency-to-dependency translation
model, which utilizes constituency forests on the
source side to direct the translation, and depen-
dency trees on the target side to ensure grammati-
cality. Feng et al (2012) presented a hierarchical
chunk-to-string translation model, which is a com-
promise between the hierarchical phrase-based mod-
el and the constituency-to-string model. Most work-
s make effort to introduce linguistic knowledge in-
to the phrase-based model and hierarchical phrase-
based model with constituency trees. Only the work
proposed by Mi and Liu, (2010) utilized constituen-
cy and dependency trees, while their work applied
two types of trees on two sides.
Instead, our model simultaneously utilizes con-
stituency and dependency trees on the source side to
direct the translation, which is concerned with com-
bining the advantages of two types of trees in trans-
lation rules to advance the state-of-the-art machine
translation.
7 Conclusion
In this paper, we present a novel model that si-
multaneously utilizes constituency and dependency
trees on the source side to direct the translation. To
combine the merits of constituency and dependen-
cy trees, our model employs head-dependents rela-
tions incorporating with constituency phrasal nodes.
Experimental results show that our model exhibits
good performance and significantly outperforms the
state-of-the-art constituency-to-string, dependency-
to-string and hierarchical phrase-based models. For
the first time, source side constituency and depen-
dency trees are simultaneously utilized to direct the
translation, and the model surpasses the state-of-the-
art translation models.
Since constituency tree binarization can lead
to more constituency-to-string rules and syntactic
phrases in rule extraction and decoding, which im-
prove the performance of constituency-to-string sys-
tems, for future work, we would like to do research
on encoding binarized constituency trees to depen-
dency trees to improve translation performance.
Acknowledgments
The authors were supported by National Natural Sci-
ence Foundation of China (Contracts 61202216),
1074
MT05 ---- segment 448
??? ?? ? ?? ?? ??? ?? ?? ? ?? ?? ???
cons2srt: united nations with the indonesian government have expressed concern over the time limit for foreign troops .
consdep2srt: the united nations has expressed concern over the deadline of the indonesian government on foreign troops .
reference: The United Nations has expressed concern over the deadline the Indonesian government imposed on foreign troops.
??? ?? ?? ?? ??? ?? ?? ? ??? ?? ?? ?
dobjpobj
prep
advmod
nsubj
pnuct
the united nations has the deadline of the indonesian government on foreign troopsexpressed concern over .
?? ?? ?? ? ?? ?? ??? ??? 6$56?? ??
dep2srt: ?? again severe acute respiratory syndrome ( SARS ) case ??
consdep2srt: ?? reemergence of a severe acute respiratory syndrome ( SARS ) case??
reference: ?? the reemergence of a severe acute respiratory syndrome (SARS) case ??
MT04 ---- segment 194
dep cons & dep
??/VV
??/AD ?/DEG
VP
reemergence
???/NN
??/JJ ??/JJ??/VV
??/AD ?/DEG
again
???/NN
??/JJ ??/JJ
Figure 6: Actual examples translated by the cons2str, dep2str and consdep2str systems.
863 State Key Project (No. 2011AA01A207),
and National Key Technology R&D Program (No.
2012BAH39B03), Key Project of Knowledge Inno-
vation Program of Chinese Academy of Sciences
(No. KGZD-EW-501). Qun Liu.s work was
partially supported by Science Foundation Ireland
(Grant No. 07/CE/I1142) as part of the CNGL
at Dublin City University. Sincere thanks to the
anonymous reviewers for their thorough reviewing
and valuable suggestions. We appreciate Haitao Mi,
Zhaopeng Tu and Anbang Zhao for insightful ad-
vices in writing.
References
Colin Cherry. 2008. Cohesive phrase-based decoding for
statistical machine translation. In ACL, pages 72?80.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
the 43rd Annual Meeting on Association for Computa-
tional Linguistics, pages 263?270.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Michael Collins, Philipp Koehn, and Ivona Kuc?erova?.
2005. Clause restructuring for statistical machine
translation. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics,
pages 531?540.
Yuan Ding and Martha Palmer. 2005. Machine trans-
lation using probabilistic synchronous dependency in-
sertion grammars. In Proceedings of the 43rd Annual
Meeting on Association for Computational Linguistic-
s, pages 541?548.
Yang Feng, Dongdong Zhang, Mu Li, Ming Zhou, and
Qun Liu. 2012. Hierarchical chunk-to-string transla-
tion. In Proceedings of the 50th Annual Meeting of
the Association for Computational Linguistics: Long
Papers-Volume 1, pages 950?958.
Heidi J Fox. 2002. Phrasal cohesion and statistical
machine translation. In Proceedings of the ACL-02
conference on Empirical methods in natural language
processing-Volume 10, pages 304?3111.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule. In Pro-
1075
ceedings of HLT/NAACL, volume 4, pages 273?280.
Boston.
Jonathan Graehl and Kevin Knight. 2004. Training tree
transducers. In Proc. HLT-NAACL, pages 105?112.
Peter Hellwig. 2006. Parsing with dependency gram-
mars. An International Handbook of Contemporary
Research, 2:1081?1109.
Liang Huang and David Chiang. 2007. Forest rescoring:
Faster decoding with integrated language models. In
Annual Meeting-Association For Computational Lin-
guistics, volume 45, pages 144?151.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006. S-
tatistical syntax-directed translation with extended do-
main of locality. In Proceedings of AMTA, pages 66?
73.
Dan Klein and Christopher D Manning. 2002. Fast exact
inference with a factored model for natural language
parsing. In Advances in neural information processing
systems, volume 15, pages 3?10.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of the 2003 Conference of the North American
Chapter of the Association for Computational Linguis-
tics on Human Language Technology-Volume 1, pages
48?54.
Dekang Lin. 2004. A path-based transfer model for ma-
chine translation. In Proceedings of the 20th interna-
tional conference on Computational Linguistics, pages
625?630.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine trans-
lation. In Proceedings of the 21st International Con-
ference on Computational Linguistics and the 44th
annual meeting of the Association for Computational
Linguistics, pages 609?616.
Yang Liu, Yajuan Lu?, and Qun Liu. 2009. Improving
tree-to-tree translation with packed forests. In Pro-
ceedings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International Join-
t Conference on Natural Language Processing of the
AFNLP: Volume 2-Volume 2, pages 558?566.
Yang Liu, Qun Liu, and Yajuan Lu?. 2011. Adjoining
tree-to-string translation. In Proceedings of the 49th
Annual Meeting of the Association for Computational
Linguistics: Human Language Technologies-Volume
1, pages 1278?1287.
Yuval Marton and Philip Resnik. 2008. Soft syntactic
constraints for hierarchical phrased-based translation.
In Proceedings of ACL-08: HLT, pages 1003?1011.
Haitao Mi and Qun Liu. 2010. Constituency to depen-
dency translation with forests. In Proceedings of the
48th Annual Meeting of the Association for Computa-
tional Linguistics, pages 1433?1442.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proceedings of ACL-08: HLT,
pages 192?199.
Franz Josef Och and Hermann Ney. 2002. Discrimi-
native training and maximum entropy models for s-
tatistical machine translation. In Proceedings of the
40th Annual Meeting on Association for Computation-
al Linguistics, pages 295?302.
Franz Josef Och and Hermann Ney. 2003. A systemat-
ic comparison of various statistical alignment models.
Computational linguistics, 29(1):19?51.
Franz Josef Och and Hermann Ney. 2004. The alignmen-
t template approach to statistical machine translation.
Computational linguistics, 30(4):417?449.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting on Association for Computation-
al Linguistics-Volume 1, pages 160?167.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. De-
pendency treelet translation: Syntactically informed
phrasal smt. In Proceedings of the 43rd Annual
Meeting on Association for Computational Linguistic-
s, pages 271?279.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proceedings of ACL-08: HLT, pages 577?585.
Andreas Stolcke et al 2002. Srilm-an extensible lan-
guage modeling toolkit. In Proceedings of the inter-
national conference on spoken language processing,
volume 2, pages 901?904.
Jun Xie, Haitao Mi, and Qun Liu. 2011. A nov-
el dependency-to-string model for statistical machine
translation. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing,
pages 216?226.
Deyi Xiong, Qun Liu, and Shouxun Lin. 2007. A de-
pendency treelet string correspondence model for s-
tatistical machine translation. In Proceedings of the
Second Workshop on Statistical Machine Translation,
pages 40?47.
Kenji Yamada and Kevin Knight. 2001. A syntax-based
statistical translation model. In Proceedings of the
39th Annual Meeting on Association for Computation-
al Linguistics, pages 523?530.
Feifei Zhai, Jiajun Zhang, Yu Zhou, and Chengqing
Zong. 2012. Tree-based translation without using
parse trees. In Proceedings of COLING 2012, pages
3037?3054.
Min Zhang, Hongfei Jiang, AiTi Aw, Jun Sun, Sheng Li,
and Chew Lim Tan. 2007. A tree-to-tree alignment-
based model for statistical machine translation. MT-
Summit-07, pages 535?542.
1076
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 546?556,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Modeling Term Translation for Document-informed Machine Translation
Fandong Meng
1, 2
Deyi Xiong
3
Wenbin Jiang
1, 2
Qun Liu
4, 1
1
Key Laboratory of Intelligent Information Processing
Institute of Computing Technology, Chinese Academy of Sciences
2
University of Chinese Academy of Sciences
{mengfandong,jiangwenbin,liuqun}@ict.ac.cn
3
School of Computer Science and Technology, Soochow University
dyxiong@suda.edu.cn
4
Centre for Next Generation Localisation, Dublin City University
Abstract
Term translation is of great importance for
statistical machine translation (SMT), es-
pecially document-informed SMT. In this
paper, we investigate three issues of term
translation in the context of document-
informed SMT and propose three cor-
responding models: (a) a term trans-
lation disambiguation model which se-
lects desirable translations for terms in the
source language with domain information,
(b) a term translation consistency model
that encourages consistent translations for
terms with a high strength of translation
consistency throughout a document, and
(c) a term bracketing model that rewards
translation hypotheses where bracketable
source terms are translated as a whole
unit. We integrate the three models into
hierarchical phrase-based SMT and eval-
uate their effectiveness on NIST Chinese-
English translation tasks with large-scale
training data. Experiment results show
that all three models can achieve sig-
nificant improvements over the baseline.
Additionally, we can obtain a further
improvement when combining the three
models.
1 Introduction
A term is a linguistic expression that is used as
the designation of a defined concept in a language
(ISO 1087). As terms convey concepts of a text,
term translation becomes crucial when the text is
translated from its original language to another
language. The translations of terms are often af-
fected by the domain in which terms are used and
the context that surrounds terms (Vasconcellos et
al., 2001). In this paper, we study domain-specific
and context-sensitive term translation for SMT.
In order to achieve this goal, we focus on three
issues of term translation: 1) translation ambigu-
ity, 2) translation consistency and 3) bracketing.
First, term translation ambiguity is related to trans-
lations of the same term in different domains. A
source language term may have different transla-
tions when it occurs in different domains. Second,
translation consistency is about consistent trans-
lations for terms that occur in the same document.
Usually, it is undesirable to translate the same term
in different ways as it occurs in different parts of
a document. Finally, bracketing concerns whether
a multi-word term is bracketable during transla-
tion. Normally, a multi-word term is translated as
a whole unit into a contiguous target string.
We study these three issues in the context
of document-informed SMT. We use document-
informed information to disambiguate term trans-
lations in different documents and maintain con-
sistent translations for terms that occur in the same
document. We propose three different models for
term translation that attempt to address the three
issues mentioned above. In particular,
? Term Translation Disambiguation Model: In
this model, we condition the translations of
terms in different documents on correspond-
ing per-document topic distributions. In do-
ing so, we enable the decoder to favor trans-
lation hypotheses with domain-specific term
translations.
? Term Translation Consistency Model: This
model encourages the same terms with a high
strength of translation consistency that occur
in different parts of a document to be trans-
lated in a consistent fashion. We calculate
the translation consistency strength of a term
based on the topic distribution of the docu-
ments where the term occurs in this model.
? Term Bracketing Model: We use the brack-
eting model to reward translation hypothe-
546
ses where bracketable multi-word terms are
translated as a whole unit.
We integrate the three models into hierarchical
phrase-based SMT (Chiang, 2007). Large-scale
experiment results show that they are all able to
achieve significant improvements of up to 0.89
BLEU points over the baseline. When simulta-
neously integrating the three models into SMT,
we can gain a further improvement, which outper-
forms the baseline by up to 1.16 BLEU points.
In the remainder of this paper, we begin with
a brief overview of related work in Section 2,
and bilingual term extraction in Section 3. We
then elaborate the proposed three models for term
translation in Section 4. Next, we conduct experi-
ments to validate the effectiveness of the proposed
models in Section 5. Finally, we conclude and pro-
vide directions for future work in Section 6.
2 Related Work
In this section, we briefly introduce related work
and highlight the differences between our work
and previous studies.
As we approach term translation disambigua-
tion and consistency via topic modeling, our mod-
els are related to previous work that explores the
topic model (Blei et al., 2003) for machine trans-
lation (Zhao and Xing, 2006; Su et al., 2012;
Xiao et al., 2012; Eidelman et al., 2012). Zhao
and Xing (2006) employ three models that enable
word alignment process to leverage topical con-
tents of document-pairs with topic model. Su et al.
(2012) establish the relationship between out-of-
domain bilingual corpus and in-domain monolin-
gual corpora via topic mapping and phrase-topic
distribution probability estimation for translation
model adaptation. Xiao et al. (2012) propose a
topic similarity model for rule selection. Eidel-
man et al. (2012) use topic models to adapt lexical
weighting probabilities dynamically during trans-
lation. In these studies, the topic model is not used
to address the issues of term translation mentioned
in Section 1.
Our work is also related to document-level
SMT in that we use document-informed informa-
tion for term translation. Tiedemann (2010) pro-
pose cache-based language and translation mod-
els, which are built on recently translated sen-
tences. Gong et al. (2011) extend this by further
introducing two additional caches. They employ
a static cache to store bilingual phrases extracted
from documents in training data that are similar to
the document being translated and a topic cache
with target language topic words. Recently we
have also witnessed efforts that model lexical co-
hesion (Hardmeier et al., 2012; Wong and Kit,
2012; Xiong et al., 2013a; Xiong et al., 2013b)
as well as coherence (Xiong and Zhang, 2013)
for document-level SMT. Hasler et al. (2014a)
use topic models to learn document-level transla-
tion probabilities. Hasler et al. (2014b) use topic-
adapted model to improve lexical selection. The
significant difference between our work and these
studies is that term translation has not been inves-
tigated in these document-level SMT models.
Itagaki and Aikawa (2008) employ bilingual
term bank as a dictionary for machine-aided trans-
lation. Ren et al. (2009) propose a binary feature
to indicate whether a bilingual phrase contains a
term pair. Pinis and Skadins (2012) investigate that
bilingual terms are important for domain adapta-
tion of machine translation. These studies do not
focus on the three issues of term translation as
discussed in Section 1. Furthermore, domain and
document-informed information is not used to as-
sist term translation.
Itagaki et al. (2007) propose a statistical method
to calculate translation consistency for terms with
explicit domain information. Partially inspired
by their study, we introduce a term translation
consistency metric with document-informed infor-
mation. Furthermore, we integrate the proposed
term translation consistency model into an actual
SMT system, which has not been done by Itagaki
et al. (2007). Ture et al. (2012) use IR-inspired
tf-idf scores to encourage consistent translation
choice. Guillou (2013) investigates what kind of
words should be translated consistently. Term
translation consistency has not been investigated
in these studies.
Our term bracketing model is also related
to Xiong et al. (2009)?s syntax-driven bracket-
ing model for phrase-based translation, which pre-
dicts whether a phrase is bracketable or not using
rich syntactic constraints. The difference is that
we construct the model with automatically created
bilingual term bank and do not depend on any syn-
tactic knowledge.
3 Bilingual Term Extraction
Bilingual term extraction is to extract terms from
two languages with the purpose of creating or ex-
547
tending a bilingual term bank, which in turn can
be used to improve other tasks such as information
retrieval and machine translation. In this paper, we
want to automatically build a bilingual term bank
so that we can model term translation to improve
translation quality of SMT. Our interest is to ex-
tract multi-word terms.
Currently, there are mainly two strategies to
conduct bilingual term extraction from parallel
corpora. One of them is to extract term candi-
dates separately for each language according to
monolingual term metrics, such as C-value/NC-
value (Frantzi et al., 1998; Vu et al., 2008), or
other common cooccurrence measures such as
Log-Likelihood Ratio, Dice coefficient and Point-
wise Mutual Information (Daille, 1996; Piao et
al., 2006). The extracted monolingual terms are
then paired together (Hjelm, 2007; Fan et al.,
2009; Ren et al., 2009). The other strategy is to
align words and word sequences that are transla-
tion equivalents in parallel corpora and then clas-
sify them into terms and non-terms (Merkel and
Foo, 2007; Lefever et al., 2009; Bouamor et al.,
2012). In this paper, we adopt the first strategy.
In particular, for each sentence pair, we collect all
source phrases which are terms and find aligned
target phrases for them via word alignments. If
the target side is also a term, we store the source
and target term as a term pair.
We conduct monolingual term extraction using
the C-value/NC-value metric and Log-Likelihood
Ratio (LLR) measure respectively. We then com-
bine terms extracted according to the two metrics
mentioned above. For the C-value/NC-value met-
ric based term extraction, we implement it in the
same way as described in Frantzi et al. (1998).
This extraction method recognizes linguistic pat-
terns (mainly noun phrases) listed as follows.
((Adj|Noun)
+
|((Adj|Noun)
?
(NounPrep)
?
)(Adj|Noun)
?
)Noun
It captures the linguistic structures of terms. For
the LLR metric based term extraction, we imple-
ment it according to Daille (1996), who estimate
the propensity of two words to appear together as a
multi-word expression. We then adopt LLR-based
hierarchical reducing algorithm proposed by Ren
et al. (2009) to extract terms with arbitrary lengths.
Since the C-value/NC-value metric based extrac-
tion method can obtain terms in strict linguistic
patterns while the LLR measure based method ex-
tracts more flexible terms, these two methods are
complementary to each other. Therefore, we use
these two methods to extract monolingual multi-
word terms and then combine the extracted terms.
4 Models
This section presents the three models of term
translation. They are the term translation dis-
ambiguation model, term translation consistency
model and term bracketing model respectively.
4.1 Term Translation Disambiguation Model
The most straightforward way to disambiguate
term translations in different domains is to cal-
culate the conditional translation probability of
a term given domain information. We use the
topic distribution of a document obtained by a
topic model to represent the domain information
of the document. Since Latent Dirichlet Alloca-
tion (LDA) (Blei et al., 2003) is the most widely-
used topic model, we exploit it for inferring topic
distributions of documents. Xiao et al. (2012)
proposed a topic similarity model for rule selec-
tion. Different from their work, we take an eas-
ier strategy that estimates topic-conditioned term
translation probabilities rather than rule-topic dis-
tributions. This makes our model easily scalable
on large training data.
With the bilingual term bank created from the
training data, we calculate the source-to-target
term translation probability for each term pair con-
ditioned on the topic distribution of the source
document where the source term occurs. We main-
tain a K-dimension (K is the number of topics)
vector for each term pair. The k-th component
p(t
e
|t
f
, z = k) measures the conditional transla-
tion probability from source term t
f
to target term
t
e
given the topic k.
We calculate p(t
e
|t
f
, z = k) via maximum
likelihood estimation with counts from training
data. When the source part of a bilingual term
pair occurs in a document D with topic distribu-
tion p(z|D) estimated via LDA tool, we collect
an instance (t
f
, t
e
, p(z|D), c), where c is the frac-
tion count of the instance as described in Chiang
(2007). After collection, we get a set of instances
I = {(t
f
, t
e
, p(z|D), c)}with different document-
topic distributions for each bilingual term pair. Us-
ing these instances, we calculate the probability
548
p(t
e
|t
f
, z = k) as follows:
p(t
e
|t
f
, z = k)
=
?
i?I,i.t
f
=t
f
,i.t
e
=t
e
i.c ? p(z = k|D)
?
i?I,i.t
f
=t
f
i.c ? p(z = k|D)
(1)
We associate each extracted term pair in our
bilingual term bank with its corresponding topic-
conditioned translation probabilities estimated in
the Eq. (1). When translating sentences of docu-
ment D
?
, we first get the topic distribution of D
?
using LDA tool. Given a sentence which contains
T terms {t
f
i
}
T
1
in D
?
, our term translation disam-
biguation model TermDis can be denoted as
TermDis =
T
?
i=1
P
d
(t
e
i
|t
f
i
, D
?
) (2)
where the conditional source-to-target term trans-
lation probability P
d
(t
e
i
|t
f
i
, D
?
) given the docu-
ment D
?
is formulated as follows:
P
d
(t
e
i
|t
f
i
, D
?
)
=
K
?
k=1
p(t
e
i
|t
f
i
, z = k) ? p(z = k|D
?
) (3)
Whenever a source term t
f
i
is translated into t
e
i
,
we check whether the pair of t
f
i
and its translation
t
e
i
can be found in our bilingual term bank. If it
can be found, we calculate the conditional transla-
tion probability from t
f
i
to t
e
i
given the document
D
?
according to Eq. (3).
The term translation disambiguation model is
integrated into the log-linear model of SMT as a
feature. Its weight is tuned via minimum error rate
training (MERT) (Och, 2003). Through the fea-
ture, we can enable the decoder to favor translation
hypotheses that contain target term translations ap-
propriate for the domain represented by the topic
distribution of the corresponding document.
4.2 Term Translation Consistency Model
The term translation disambiguation model helps
the decoder select appropriate translations for
terms that are in accord with their domains. Yet
another translation issue related to the domain-
specific term translation is to what extent a term
should be translated consistently given the domain
where it occurs. Term translation consistency in-
dicates the translation stability that a source term
is translated into the same target term (Itagaki et
al., 2007). When translating a source term, if the
translation consistency strength of the source term
is high, we should take the corresponding target
term as the translation for it. Otherwise, we may
need to create a new translation for it according to
its context. In particular, we want to enable the
decoder to choose between: 1) translating a given
source term into the extracted corresponding tar-
get term or 2) translating it in another way accord-
ing to the strength of its translation consistency.
In doing so, we can encourage consistent transla-
tions for terms with a high translation consistency
strength throughout a document.
Our term translation consistency model can ex-
actly measure the strength of term translation con-
sistency in a document. Since the essential com-
ponent of our term translation consistency model
is the translation consistency strength of the source
term estimated under the topic distribution, we de-
scribe how to calculate it before introducing the
whole model.
With the bilingual term bank created from
training data, we first group each source term
and all its corresponding target terms into a 2-
tuple G?t
f
, Set(t
e
)?, where t
f
is the source term
and Set(t
e
) is the set of t
f
?s corresponding tar-
get terms. We maintain a K-dimension (K is
the number of topics) vector for each 2-tuple
G?t
f
, Set(t
e
)?. The k-th component measures the
translation consistency strength cons(t
f
, k) of the
source term t
f
given the topic k.
We calculate cons(t
f
, k) for each
G?t
f
, Set(t
e
)? with counts from training data as
follows:
cons(t
f
, k) =
M
?
m=1
N
m
?
n=1
(
q
mn
? p(k|m)
Q
k
)
2
(4)
Q
k
=
M
?
m=1
N
m
?
n=1
q
mn
? p(k|m) (5)
where M is the number of documents in which
the source term t
f
occurs, N
m
is the number of
unique corresponding term translations of t
f
in the
mth document, q
mn
is the frequency of the nth
translation of t
f
in the mth document, p(k|m) is
the conditional probability of the mth document
over topic k, and Q
k
is the normalization factor.
All translations of t
f
are from Set(t
e
). We adapt
Itagaki et al. (2007)?s translation consistency met-
ric for terms to our topic-based translation consis-
tency measure in the Eq. (4). This equation cal-
culates the translation consistency strength of the
source term t
f
given the topic k according to the
distribution of t
f
?s translations in each document
549
where they occur. According to Eq. (4), the trans-
lation consistency strength is a score between 0
and 1. If a source term only occurs in a document
and all its translations are the same, the translation
consistency strength of this term is 1.
We reorganize our bilingual term bank into a
list of 2-tuples G?t
f
, Set(t
e
)?s, each of which is
associated with a K-dimension vector storing the
topic-conditioned translation consistency strength
calculated in the Eq. (4). When translating sen-
tences of document D, we first get the topic dis-
tribution of D via LDA tool. Given a sentence
which contains T terms {t
f
i
}
T
1
in D, our term
translation consistency model TermCons can be
denoted as
TermCons =
T
?
i=1
exp(S
c
(t
f
i
|D)) (6)
where the strength of translation consistency for
t
f
i
given the document D is formulated as fol-
lows:
S
c
(t
f
i
|D) = log(
K
?
k=1
cons(t
f
i
, k) ? p(k|D)) (7)
During decoding, whenever a hypothesis just
translates a source term t
f
i
into t
e
, we check
whether the translation t
e
can be found in Set(t
e
)
of t
f
i
from the reorganized bilingual term bank. If
it can be found, we calculate the strength of trans-
lation consistency for t
f
i
given the document D
according to Eq. (7) and take it as a soft con-
straint. If the S
c
(t
f
i
|D) of t
f
i
is high, the decoder
should translate t
f
i
into the extracted correspond-
ing target terms. Otherwise, the decoder will se-
lect translations from outside of Set(t
e
) for t
f
i
. In
doing so, we encourage terms to be translated in
a topic-dependent consistency pattern in the test
data similar to that in the training data so that we
can control the translation consistency of terms in
the test data.
The term translation consistency model is also
integrated into the log-linear model of SMT as a
feature. Through the feature, we can enable the
decoder to translate terms with a high translation
consistency in a document into corresponding tar-
get terms from our bilingual term bank rather than
other translations in a consistent fashion.
4.3 Term Bracketing Model
The term translation disambiguation model and
consistency model concern the term translation ac-
curacy with domain information. We further pro-
pose a term bracketing model to guarantee the in-
tegrality of term translation. Xiong et al. (2009)
proposed a syntax-driven bracketing model for
phrase-based translation, which predicts whether
a phrase is bracketable or not using rich syntac-
tic constraints. If a source phrase remains con-
tiguous after translation, they refer to this type of
phrase as bracketable phrase, otherwise unbrack-
etable phrase. For multi-word terms, it is also
desirable to be bracketable since a source term
should be translated as a whole unit and its trans-
lation should be contiguous.
In this paper, we adapt Xiong et al. (2009)?s
bracketing approach to term translation and build
a classifier to measure the probability that a source
term should be translated in a bracketable man-
ner. For all source parts of the extracted bilingual
term bank, we find their target counterparts in the
word-aligned training data. If the corresponding
target counterpart remains contiguous, we take the
source term as a bracketable instance, otherwise
an unbracketable instance. With these bracketable
and unbracketable instances, we train a maximum
entropy binary classifier to predict bracketable (b)
probability of a given source term t
f
within par-
ticular contexts c(t
f
). The binary classifier is for-
mulated as follows:
P
b
(b|c(t
f
)) =
exp(
?
j
?
j
h
j
(b, c(t
f
)))
?
b
?
exp(
?
j
?
j
h
j
(b
?
, c(t
f
)))
(8)
where h
j
? {0, 1} is a binary feature function and
?
j
is the weight of h
j
. We use the following fea-
tures: 1) the word sequence of the source term, 2)
the first word of the source term, 3) the last word
of the source term, 4) the preceding word of the
first word of the source term, 5) the succeeding
word of the last word of the source term, and 6)
the number of words in the source term.
Given a source sentence which contains T terms
{t
f
i
}
T
1
, our term bracketing model TermBrack
can be denoted as
TermBrack =
T
?
i=1
P
b
(b|c(t
f
i
)) (9)
Whenever a hypothesis just covers a source term
t
f
i
, we calculate the bracketable probability of t
f
i
according to Eq. (8).
The term bracketing model is integrated into the
log-linear model of SMT as a feature. Through the
feature, we want the decoder to translate source
terms with a high bracketable probability as a
whole unit.
550
Source Target D M
F?angy`u X`?t?ong defence mechanisms
F?angy`u X`?t?ong defence systems
F?angy`u X`?t?ong defense programmes 470 56
F?angy`u X`?t?ong prevention systems
... ...
Zh`anlu`e D?aod`an F?angy`u X`?t?ong strategic missile defense system 7 0
Table 1: Examples of bilingual terms extracted from the training data. ?D? means the total number of
documents in which the corresponding source term occurs and ?M? denotes the number of documents in
which the corresponding source term is translated into different target terms. The source side is Chinese
Pinyin. To save space, we do not list all the 23 different translations of the source term ?F?angy`u X`?t?ong?.
5 Experiments
In this section, we conducted experiments to an-
swer the following three questions.
1. Are our term translation disambiguation,
consistency and bracketing models able to
improve translation quality in BLEU?
2. Does the combination of the three models
provide further improvements?
3. To what extent do the proposed models affect
the translations of test sets?
5.1 Setup
Our training data consist of 4.28M sentence pairs
extracted from LDC
1
data with document bound-
aries explicitly provided. The bilingual training
data contain 67,752 documents, 124.8M Chinese
words and 140.3M English words. We chose
NIST MT05 as the MERT (Och, 2003) tuning set,
NIST MT06 as the development test set, and NIST
MT08 as the final test set. The numbers of docu-
ments/sentences in NIST MT05, MT06 and MT08
are 100/1082, 79/1664 and 109/1357 respectively.
The word alignments were obtained by running
GIZA++ (Och and Ney, 2003) on the corpora in
both directions and using the ?grow-diag-final-
and? balance strategy (Koehn et al., 2003). We
adopted SRI Language Modeling Toolkit (Stol-
cke and others, 2002) to train a 4-gram language
model with modified Kneser-Ney smoothing on
the Xinhua portion of the English Gigaword cor-
pus. For the topic model, we used the open source
1
The corpora include LDC2003E07, LDC2003E14,
LDC2004T07, LDC2004E12, LDC2005E83, LDC2005T06,
LDC2005T10, LDC2006E24, LDC2006E34, LDC2006E85,
LDC2006E92, LDC2007E87, LDC2007E101,
LDC2008E40, LDC2008E56, LDC2009E16 and
LDC2009E95.
LDA tool GibbsLDA++
2
with the default setting
for training and inference. We performed 100 it-
erations of the L-BFGS algorithm implemented in
the MaxEnt toolkit
3
with both Gaussian prior and
event cutoff set to 1 to train the term bracketing
prediction model (Section 4.3).
We performed part-of-speech tagging for mono-
lingual term extraction (C-value/NC-vaule method
in Section 3) of the source and target languages
with the Stanford NLP toolkit
4
. The bilingual term
bank was extracted based on the following param-
eter settings of term extraction methods. Empiri-
cally, we set the maximum length of a term to 6
words
5
. For both the C-value/NC-value and LLR-
based extraction methods, we set the context win-
dow size to 5 words, which is a widely-used set-
ting in previous work. And we set C-value/NC-
value score threshold to 0 and LLR score threshold
to 10 according to the training corpora.
We used the case-insensitive 4-gram BLEU
6
as
our evaluation metric. In order to alleviate the im-
pact of the instability of MERT (Och, 2003), we
ran it three times for all our experiments and pre-
sented the average BLEU scores on the three runs
following the suggestion by Clark et al. (2011).
We used an in-house hierarchical phrase-based
decoder to verify our proposed models. Although
the decoder translates a document in a sentence-
by-sentence fashion, it incorporates document-
informed information for sentence translation via
the proposed term translation models trained on
documents.
2
http://sourceforge.net/projects/gibbslda/
3
http://homepages.inf.ed.ac.uk/lzhang10/maxent toolkit.html
4
http://nlp.stanford.edu/software/tagger.shtml
5
We determine the maximum length of a term by testing
{5, 6, 7, 8} in our preliminary experiments. We find that
length 6 produces a slightly better performance than other
values.
6
ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v11b.pl
551
Zh?y?u W?iyu?nhu? Ch?ngyu?n C?i  K? C?nji? W?iyu?nhu? Sh?ny? ?
Only members of the commission shall take part  in the commission deliberations .
?
He these proposals
T? Ji?ng Zh?xi? Ji?ny? Ji?o Y?u Y? G? B?zh?ngj? W?iyu?nhu? Sh?ny?
submit for approval to a committee of ministers .
(a)
(b)
Figure 1: An example of unbracketable source term in the training data. In (a), ?W?eiyu?anhu`? Sh?eny`?? is
bracketable while in (b) it is unbracketable. The solid lines connect bilingual phrases. The source side is
Chinese Pinyin.
5.2 Bilingual Term Bank
Before reporting the results of the proposed mod-
els, we provide some statistics of the bilingual
term bank extracted from the training data.
According to our statistics, about 1.29M bilin-
gual terms are extracted from the training data.
65.07% of the sentence pairs contain bilingual
terms in the training data. And on average, a
source term has about 1.70 different translations.
These statistics indicate that terms are frequently
used in real-world data and that a source term can
be translated into different target terms.
We also present some examples of bilingual
terms extracted from the training data in Table 1.
Accordingly, we show the total number of doc-
uments in which the corresponding source term
occurs and the number of documents in which
the corresponding source term is translated into
different target terms. The source term ?F?angy`u
X`?t?ong? has 23 different translations in total. They
are distributed in 470 documents in the training
data. In 414 documents, ?F?angy`u X`?t?ong? has
only one single translation. However, in the other
56 documents it has different translations. This
indicates that ?F?angy`u X`?t?ong? is not consistently
translated in these 56 documents. Different from
this, the source term ?Zh`anlu`e D?aod`an F?angy`u
X`?t?ong? only has one translation. And it is trans-
lated consistently in all 7 documents where it oc-
curs. In fact, according to our statistics, there are
about 5.19% source terms whose translations are
not consistent even in the same document.
These examples and statistics suggest 1) that
source terms have domain-specific translations
and 2) that source terms are not necessarily trans-
lated in a consistent manner even in the same doc-
ument. These are exactly the reasons why we pro-
pose the term translation disambiguation and con-
sistency model based on domain information rep-
resented by topic distributions.
Actually, 36.13% of the source terms are not
necessarily translated into target strings as a whole
unit. We show an example of such terms in Fig-
ure 1. In Figure 1-(a), ?W?eiyu?anhu`? Sh?eny`?? is a
term, and is translated into ?commission deliber-
ations? as a whole unit. Therefore ?W?eiyu?anhu`?
Sh?eny`?? is bracketable in this sentence. How-
ever, in Figure 1-(b), ?W?eiyu?anhu`?? and ?Sh?eny`??
are translated separately. Therefore ?W?eiyu?anhu`?
Sh?eny`?? is an unbracketable term in this sentence.
This is the reason why we propose a bracketing
model to predict whether a source term is brack-
etable or not.
5.3 Effect of the Proposed Models
In this section, we validate the effectiveness of the
proposed term translation disambiguation model,
consistency model and bracketing model respec-
tively. In addition to the traditional hiero (Chi-
ang, 2007) system, we also compare against the
?CountFeat? method in Ren et al. (2009) who use
a binary feature to indicate whether a bilingual
phrase contains a term pair. Although Ren et al.
(2009)?s experiments are conducted in a phrase-
based system, the idea can be easily applied to a
hierarchical phrase-based system.
We carried out experiments to investigate the ef-
fect of the term translation disambiguation model
(Dis-Model) and report the results in Table 2. In
order to find the topic number setting with which
our model has the best performance, we ran exper-
iments using the MT06 as the development test set.
From Table 2, we observe that the Dis-Model ob-
tains steady improvements over the baseline and
?CountFeat? method with the topic number K
552
Models MT06 MT08 Avg
Baseline 32.43 24.14 28.29
CountFeat 32.77 24.29 28.53
Dis-Model
K = 50 32.94* 24.53 28.74
K = 100 33.10* 24.57 28.84
K = 150 33.16* 24.67* 28.92
K = 200 33.08* 24.55 28.81
Cons-Model
K = 50 33.09* 24.59 28.84
K = 100 33.13* 24.74* 28.94
K = 150 33.32*+ 24.84*+ 29.08
K = 200 33.02* 24.73* 28.88
Brack-Model 33.09* 24.66* 28.88
Combined-Model 33.59*+ 24.99*+ 29.29
Table 2: BLEU-4 scores (%) of the term translation disambiguation model (Dis-Model), the term transla-
tion consistency model (Cons-Model), the term bracketing model (Brack-Model), and the combination of
the three models, on the development test set MT06 and the final test set MT08. K ? {50, 100, 150, 200}
which is the number of topics for the Dis-Model and the Cons-Model. ?Combined-Model? is the combi-
nation of the three single modes with topic number 150 for the Dis-Model and the Cons-Model. ?Base-
line? is the traditional hierarchical phrase-based system. ?CountFeat? is the method that adds a counting
feature to reward translation hypotheses containing bilingual term pairs. The ?*? and ?+? denote that the
results are significantly (Clark et al., 2011) better than those of the baseline system and the CountFeat
method respectively (p<0.01).
ranging from 50 to 150. However, when we set K
to 200, the performance drops. The highest BLEU
scores 33.16 and 24.67 are obtained at the topic
setting K = 150. In fact, our Dis-Model gains
higher performance in BLEU than both the tradi-
tional hiero baseline and the ?CountFeat? method
with all topic settings. The ?CountFeat? method
rewards translation hypotheses containing bilin-
gual term pairs. However it does not explore any
domain information. Our Dis-Model incorporates
domain information to conduct translation disam-
biguation and achieves higher performance. When
the topic number is set to 150, we gain the high-
est BLEU score, which is higher than that of the
baseline by 0.73 and 0.53 BLEU points on MT06
and MT08, respectively. The final gain over the
baseline is on average 0.63 BLEU points.
We conducted the second group of experiments
to study whether the term translation consistency
model (Cons-Model) is able to improve the per-
formance in BLEU, as well as to investigate the
impact of different topic numbers on the Cons-
Model. Results are shown in Table 2, from which
we observe the similar phenomena to what we
have found in the Dis-Model. Our Cons-Model
gains higher BLEU scores than the baseline sys-
tem and the ?CountFeat? method with all topic
settings. Setting topic number to 150 achieves the
highest BLEU score, which is higher than base-
line by 0.89 BLEU points and 0.70 BLEU points
on MT06 and MT08 respectively, and on average
0.79 BLEU points.
We also conducted experiments to verify the ef-
fectiveness of the term bracketing model (Brack-
Model), which conducts bracketing prediction for
source terms. Results in Table 2 show that
our Brack-Model gains higher BLEU scores than
those of the baseline system and the ?CountFeat?
method. The final gain of Brack-Model over the
baseline is 0.66 BLEU points and 0.52 points on
MT06 and MT08 respectively, and on average
0.59 BLEU points.
5.4 Combination of the Three Models
As shown in the previous subsection, the term
translation disambiguation model, consistency
model and bracketing model substantially outper-
form the baseline. Now, we investigate whether
using these three models simultaneously can lead
to further improvements. The last row in Table 2
shows that the combination of the three models
(Combined-Model) achieves higher BLEU score
than all single models, when we set the topic num-
ber to 150 for the term translation disambigua-
tion model and consistency model. The final gain
553
Models MT06 MT08
Best-Dis-Model 30.89 30.14
Best-Cons-Model 38.04 36.70
Brack-Model 60.46 55.78
Combined-Model 54.39 50.85
Table 3: Percentage (%) of 1-best translations
which are generated by the Combined-Model and
the three single models with best settings on the
development test set MT06 and the final test set
MT08. The topic number is 150 for Best-Dis-
Model and Best-Cons-Model.
of the Combined-Model over the baseline is 1.16
BLEU points and 0.85 points on MT06 and MT08
respectively, and on average 1.00 BLEU points.
5.5 Analysis
In this section, we investigate to what extent the
proposed models affect the translations of test sets.
In Table 3, we show the percentage of 1-best trans-
lations affected by the Combined-Model and the
three single models with best settings on test sets
MT06 and MT08. For single models, if the corre-
sponding feature (disambiguation, consistency or
bracketing) is activated in the 1-best derivation,
the corresponding model has impact on the 1-best
translation. For the Combined-Model, if any of
the corresponding features is activated in the 1-
best derivation, the Combined-Model affects the
1-best translation.
From Table 3, we can see that 1-best transla-
tions of source sentences affected by any of the
proposed models account for a high proportion
(30%?60%) on both MT06 and MT08. This in-
dicates that all proposed models play an important
role in the translation of both test sets. Among
the three proposed models, the Brack-Model is the
one that affects the largest number of 1-best trans-
lations in both test sets. And the percentage is
60.46% and 55.78% on MT06 and MT08 respec-
tively. The Brack-Model only considers source
terms during decoding, while the Dis-Model and
Cons-Model need to match both source and target
terms. The Brack-Model is more likely to be acti-
vated. Hence the percentage of 1-best translations
affected by this model is higher than those of the
other two models. Since we only investigate the
1-best translations generated by the Combined-
Model and single models, the translations gener-
ated by some single models (e.g., Brack-Model)
may not be generated by the Combined-Model.
Therefore it is hard to say that the numbers of 1-
best translations affected by the Combined-Model
must be greater than those of single models.
6 Conclusion and Future Work
We have studied the three issues of term trans-
lation and proposed three different term trans-
lation models for document-informed SMT. The
term translation disambiguation model enables
the decoder to favor the most suitable domain-
specific translations with domain information for
source terms. The term translation consistency
model encourages the decoder to translate source
terms with a high domain translation consistency
strength into target terms rather than other new
strings. Finally, the term bracketing model re-
wards hypotheses that translate bracketable terms
into continuous target strings as a whole unit.
We integrate the three models into a hierarchical
phrase-based SMT system
7
and evaluate their ef-
fectiveness on the NIST Chinese-English transla-
tion task with large-scale training data. Experi-
ment results show that all three models achieve
significant improvements over the baseline. Ad-
ditionally, combining the three models achieves a
further improvement. For future work, we would
like to evaluate our models on term translation
across a range of different domains.
Acknowledgments
This work was supported by National Key Tech-
nology R&D Program (No. 2012BAH39B03) and
CAS Action Plan for the Development of Western
China (No. KGZD-EW-501). Deyi Xiong?s work
was supported by Natural Science Foundation of
Jiangsu Province (Grant No. BK20140355). Qun
Liu?s work was partially supported by Science
Foundation Ireland (Grant No. 07/CE/I1142) as
part of the CNGL at Dublin City University. Sin-
cere thanks to the anonymous reviewers for their
thorough reviewing and valuable suggestions. The
corresponding author of this paper, according to
the meaning given to this role by University of
Chinese Academy of Sciences and Soochow Uni-
versity, is Deyi Xiong.
7
Our models are not limited to hierarchical phrase-based
SMT. They can be easily applied to other SMT formalisms,
such as phrase- and syntax-based SMT.
554
References
David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent dirichlet allocation. the Journal of ma-
chine Learning research, 3:993?1022.
Houda Bouamor, Aur?elien Max, and Anne Vilnat.
2012. Validation of sub-sentential paraphrases ac-
quired from parallel monolingual corpora. In Pro-
ceedings of the 13th Conference of the European
Chapter of the Association for Computational Lin-
guistics, pages 716?725. Association for Computa-
tional Linguistics.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Jonathan H Clark, Chris Dyer, Alon Lavie, and Noah A
Smith. 2011. Better hypothesis testing for statistical
machine translation: Controlling for optimizer insta-
bility. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies: short papers-Volume
2, pages 176?181.
B?eatrice Daille. 1996. Study and implementation of
combined techniques for automatic extraction of ter-
minology. Journal of The balancing act: Combin-
ing symbolic and statistical approaches to language,
1:49?66.
Vladimir Eidelman, Jordan Boyd-Graber, and Philip
Resnik. 2012. Topic models for dynamic translation
model adaptation. In Proceedings of the 50th An-
nual Meeting of the Association for Computational
Linguistics: Short Papers-Volume 2, pages 115?119.
Association for Computational Linguistics.
Xiaorong Fan, Nobuyuki Shimizu, and Hiroshi Nak-
agawa. 2009. Automatic extraction of bilin-
gual terms from a chinese-japanese parallel corpus.
In Proceedings of the 3rd International Universal
Communication Symposium, pages 41?45. ACM.
Katerina T Frantzi, Sophia Ananiadou, and Junichi
Tsujii. 1998. The c-value/nc-value method of au-
tomatic recognition for multi-word terms. In Re-
search and Advanced Technology for Digital Li-
braries, pages 585?604. Springer.
Zhengxian Gong, Min Zhang, and Guodong Zhou.
2011. Cache-based document-level statistical ma-
chine translation. In Proceedings of the Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 909?919.
Liane Guillou. 2013. Analysing lexical consistency
in translation. In Proceedings of the Workshop on
Discourse in Machine Translation, pages 10?18.
Christian Hardmeier, Joakim Nivre, and J?org Tiede-
mann. 2012. Document-wide decoding for phrase-
based statistical machine translation. In Proceed-
ings of the 2012 Joint Conference on Empiri-
cal Methods in Natural Language Processing and
Computational Natural Language Learning, pages
1179?1190.
Eva Hasler, Phil Blunsom, Philipp Koehn, and Barry
Haddow. 2014a. Dynamic topic adaptation for
phrase-based mt. In Proceedings of the 14th Confer-
ence of the European Chapter of the Association for
Computational Linguistics, Gothenburg, Sweden.
Eva Hasler, Barry Haddow, and Philipp Koehn. 2014b.
Dynamic topic adaptation for smt using distribu-
tional profiles. In Proceedings of the Ninth Work-
shop on Statistical Machine Translation, pages 445?
456, Baltimore, Maryland, USA, June. Association
for Computational Linguistics.
Hans Hjelm. 2007. Identifying cross language term
equivalents using statistical machine translation and
distributional association measures. In Proceedings
of 16th Nordic Conference of Computational Lin-
guistics Nodalida, pages 97?104.
Masaki Itagaki and Takako Aikawa. 2008. Post-mt
term swapper: Supplementing a statistical machine
translation system with a user dictionary. In LREC.
Masaki Itagaki, Takako Aikawa, and Xiaodong He.
2007. Automatic validation of terminology trans-
lation consistency with statistical method. Proceed-
ings of MT summit XI, pages 269?274.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology-
Volume 1, pages 48?54.
Els Lefever, Lieve Macken, and Veronique Hoste.
2009. Language-independent bilingual terminology
extraction from a multilingual parallel corpus. In
Proceedings of the 12th Conference of the European
Chapter of the Association for Computational Lin-
guistics, pages 496?504.
Magnus Merkel and Jody Foo. 2007. Terminology
extraction and term ranking for standardizing term
banks. In Proceedings of 16th Nordic Conference
of Computational Linguistics Nodalida, pages 349?
354.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Annual Meeting on Association for Compu-
tational Linguistics-Volume 1, pages 160?167.
Scott SL Piao, Guangfan Sun, Paul Rayson, and
Qi Yuan. 2006. Automatic extraction of chi-
nese multiword expressions with a statistical tool.
In Workshop on Multi-word-expressions in a Mul-
tilingual Context held in conjunction with the 11th
EACL, Trento, Italy, pages 17?24.
555
Pinis and Skadins. 2012. Mt adaptation for under-
resourced domains?what works and what not. In
Human Language Technologies?The Baltic Perspec-
tive: Proceedings of the Fifth International Confer-
ence Baltic HLT 2012, volume 247, page 176. IOS
Press.
Zhixiang Ren, Yajuan L?u, Jie Cao, Qun Liu, and Yun
Huang. 2009. Improving statistical machine trans-
lation using domain bilingual multiword expres-
sions. In Proceedings of the Workshop on Multiword
Expressions: Identification, Interpretation, Disam-
biguation and Applications, pages 47?54.
Andreas Stolcke et al. 2002. Srilm-an extensible lan-
guage modeling toolkit. In Proceedings of the inter-
national conference on spoken language processing,
volume 2, pages 901?904.
Jinsong Su, Hua Wu, Haifeng Wang, Yidong Chen,
Xiaodong Shi, Huailin Dong, and Qun Liu. 2012.
Translation model adaptation for statistical machine
translation with monolingual topic information. In
Proceedings of the 50th Annual Meeting of the
Association for Computational Linguistics: Long
Papers-Volume 1, pages 459?468.
J?org Tiedemann. 2010. Context adaptation in statisti-
cal machine translation using models with exponen-
tially decaying cache. In Proceedings of the 2010
Workshop on Domain Adaptation for Natural Lan-
guage Processing, pages 8?15.
Ferhan Ture, Douglas W Oard, and Philip Resnik.
2012. Encouraging consistent translation choices.
In Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 417?426. Association for Computational Lin-
guistics.
Muriel Vasconcellos, Brian Avey, Claudia Gdaniec,
Laurie Gerber, Marjorie Le?on, and Teruko Mita-
mura. 2001. Terminology and machine translation.
Handbook of Terminology Management, 2:697?723.
Thuy Vu, Ai Ti Aw, and Min Zhang. 2008. Term ex-
traction through unithood and termhood unification.
In Proceedings of the third international joint con-
ference on natural language processing.
Billy Wong and Chunyu Kit. 2012. Extending ma-
chine translation evaluation metrics with lexical co-
hesion to document level. In Proceedings of the
2012 Joint Conference on Empirical Methods in
Natural Language Processing and Computational
Natural Language Learning, pages 1060?1068.
Xinyan Xiao, Deyi Xiong, Min Zhang, Qun Liu, and
Shouxun Lin. 2012. A topic similarity model for hi-
erarchical phrase-based translation. In Proceedings
of the 50th Annual Meeting of the Association for
Computational Linguistics: Long Papers-Volume 1,
pages 750?758.
Deyi Xiong and Min Zhang. 2013. A topic-based
coherence model for statistical machine translation.
In Proceedings of the Twenty-Seventh AAAI Confer-
ence on Artificial Intelligence (AAAI-13), Bellevue,
Washington, USA, July.
Deyi Xiong, Min Zhang, Aiti Aw, and Haizhou Li.
2009. A syntax-driven bracketing model for phrase-
based translation. In Proceedings of the Joint Con-
ference of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Natural
Language Processing of the AFNLP, pages 315?
323.
Deyi Xiong, Guosheng Ben, Min Zhang, Yajuan L?u,
and Qun Liu. 2013a. Modeling lexical cohesion for
document-level machine translation. In Proceedings
of the Twenty-Third international joint conference
on Artificial Intelligence, pages 2183?2189. AAAI
Press.
Deyi Xiong, Yang Ding, Min Zhang, and Chew Lim
Tan. 2013b. Lexical chain based cohesion mod-
els for document-level statistical machine transla-
tion. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, pages
1563??1573.
Bing Zhao and Eric P Xing. 2006. Bitam: Bilingual
topic admixture models for word alignment. In Pro-
ceedings of the COLING/ACL on Main conference
poster sessions, pages 969?976.
556
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 715?720,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
ICT: A Translation based Method for Cross-lingual Textual Entailment 
 
 
Fandong Meng, Hao Xiong and Qun Liu 
Key Lab. of Intelligent Information Processing 
Institute of Computing Technology 
Chinese Academy of Sciences 
P.O. Box 2704, Beijing 100190, China 
{mengfandong,xionghao,liuqun}@ict.ac.cn 
 
 
 
 
 
 
Abstract 
In this paper, we present our system descrip-
tion in task of Cross-lingual Textual Entail-
ment. The goal of this task is to detect 
entailment relations between two sentences 
written in different languages. To accomplish 
this goal, we first translate sentences written 
in foreign languages into English. Then, we 
use EDITS1, an open source package, to rec-
ognize entailment relations. Since EDITS only 
draws monodirectional relations while the task 
requires bidirectional prediction, thus we ex-
change the hypothesis and test to detect en-
tailment in another direction. Experimental 
results show that our method achieves promis-
ing results but not perfect results compared to 
other participants. 
1 Introduction 
In Cross-Lingual Textual Entailment task (CLTE) 
of 2012, the organizers hold a task for Cross-
Lingual Textual Entailment. The Cross-Lingual 
Textual Entailment task addresses textual entail-
ment (TE) recognition under a new dimension 
(cross-linguality), and within a new challenging 
application scenario (content synchronization) 
Readers can refer to M. Negri et al 2012.s., for 
more detailed introduction. 1 
Textual entailment, on the other hand, recog-
nize, generate, or extract pairs of natural language 
expressions, and infer that if one element is true, 
whether the other element is also true. Several 
methods are proposed by previous researchers. 
There have been some workshops on textual en-
tailment in recent years. The recognizing textual 
entailment challenges (Bar-Haim et al 2006; 
Giampiccolo, Magnini, Dagan, & Dolan, 2007; 
Giampiccolo, Dang, Magnini, Dagan, & Dolan, 
2008), currently in the 7th year, provide additional 
significant thrust. Consequently, there are a large 
number of published articles, proposed methods, 
and resources related to textual entailment. A spe-
cial issue on textual entailment was also recently 
published, and its editorial provides a brief over-
view of textual entailment methods (Dagan, Dolan, 
Magnini, & Roth, 2009).  
Textual entailment recognizers judge whether 
or not two given language expressions constitute a 
correct textual entailment pair. Different methods 
may operate at different levels of representation of 
the input expressions. For example, they may treat 
the input expressions simply as surface strings, 
they may operate on syntactic or semantic repre-
sentations of the input expressions, or on represen-
tations combining information from different 
                                                          
1http://edits.fbk.eu/ 
715
levels. Logic-based approach is to map the lan-
guage expressions to logical meaning representa-
tions, and then rely on logical entailment checks, 
possibly by invoking theorem provers (Rinaldi et 
al., 2003; Bos & Markert, 2005; Tatu & Moldovan, 
2005, 2007). An alternative to use logical meaning 
representations is to start by mapping each word of 
the input language expressions to a vector that 
shows how strongly the word co-occurs with par-
ticular other words in corpora (Lin, 1998b), possi-
bly also taking into account syntactic information, 
for example requiring that the co-occurring words 
participate in particular syntactic dependencies 
(Pad?o & Lapata, 2007). Several textual entailment 
recognizing methods operate directly on the input 
surface strings. For example, they compute the 
string edit distance (Levenshtein, 1966) of the two 
input strings, the number of their common words, 
or combinations of several string similarity 
measures (Malakasiotis & Androutsopoulos, 2007). 
Dependency grammar parsers (Melcuk, 1987; Ku-
bler, McDonald, & Nivre, 2009) are popular in 
textual entailment research. However, cross-lingual 
textual entailment brings some problems on past 
algorithms. On the other hand, many methods can?t 
be applied to it directly.  
In this paper, we propose a translation based 
method for cross-lingual textual entailment, which 
has been described in Mehdad et al 2010. First, we 
translate one part of the text, which termed as ?t1? 
and written in one language, into English, which 
termed as ?t2?. Then, we use EDITS, an open 
source package, to recognize entailment relations 
between two parts. Large-scale experiments are 
conducted on four language pairs, French-English, 
Spanish-English, Italian-English and German-
English. Although our method achieves promising 
results reported by organizers, it is still far from 
perfect compared to other participants. 
The remainder of this paper is organized as 
follows. We describe our system framework in 
section 2. We report experimental results in section 
3 and draw our conclusions in the last section. 
2 System Description 
Figure 1 illustrates the overall framework of our 
system, where a machine translation model is em-
ployed to translate foreign language into English, 
since original EDITS could only deal with the text 
in the same language pairs.  
   In the following of this section, we will de-
scribe the translation module and configuration of 
EDITS in details. 
 
Figure 1:  The framework of our system. 
 
2.1 Machine Translation 
Recently, machine translation has attracted inten-
sive attention and has been well studied in natural 
language community. Effective models, such as 
Phrase-Based model (Koehn et al, 2003), Hierar-
chical Phrase-Based model (HPB) (Chiang, 2005), 
and Syntax-Based (Liu et al, 2006) model have 
been proposed to improve the translation quality. 
However, since current translation models require 
parallel corpus to extract translation rules, while 
parallel corpus on some language pairs such as 
Italian-English and Spanish-English are hard to 
obtain, therefore, we could use Google Translation 
Toolkit (GTT) to generate translation. 
Specifically, WMT 2 released some bilingual 
corpus for training, thus we use some portion to 
train a French-English translation engine using 
hierarchical phrase-based model. We also exploit 
system combination technique (A Rosti et al, 2007) 
to improve translation quality via blending the 
translation of our models and GTT?s. It is worth 
noting that GTT only gives 1-best translation, thus 
we duplicate 50 times to generate 50-best for sys-
tem combination.  
                                                          
2  http://www.statmt.org/wmt12/ 
716
2.2 Textual Entailment 
Many methods have been proposed to recognize 
textual entailment relations between two expres-
sions written in the same language. Since edit dis-
tance algorithms are effective on this task, we 
choose this method. And we use popular toolkit, 
EDITS, to accomplish the textual entailment task. 
EDITS is an open source software, which is 
used for recognizing entailment relations between 
two parts of text, termed as ?T? and ?H?. The sys-
tem is based on the edit distance algorithms, and 
computes the ?T?-?H? distance as the cost of the 
edit operations (i.e. insertion, deletion and substitu-
tion) that are necessary to transform ?T? into ?H?. 
EDITS requires that three modules are defined: an 
edit distance algorithm, a cost scheme for the three 
edit operations, and a set of rules expressing either 
entailment or contradiction. Each module can be 
easily configured by the user as well as the system 
parameters. EDITS can work at different levels of 
complexity, depending on the linguistic analysis 
carried on over ?T? and ?H?. Both linguistic pro-
cessors and semantic resources that are available to 
the user can be integrated within EDITS, resulting 
in a flexible, modular and extensible approach to 
textual entailment. 
 
 
Figure 2: An Example of two expressions 
EDITS can recognize.  
 
Figure 2 shows an example of two expressions 
that EDITS can recognize. EDITS will give an an-
swer that whether expression ?H? is true given that 
expression ?T? is true. The result is a Boolean val-
ue. If ?H? is true given ?T? is true, then the result 
is ?YES?, otherwise ?NO?. 
EDITS implements a distance-based frame-
work which assumes that the probability of an en-
tailment relation between a given ?T?-?H? pair is 
inversely proportional to the distance between ?T? 
and ?H? (i.e. the higher the distance, the lower is 
the probability of entailment). Within this frame-
work the system implements and harmonizes dif-
ferent approaches to distance computation, 
providing both edit distance algorithms, and simi-
larity algorithms. Each algorithm returns a normal-
ized distance score (a number between 0 and 1). At 
a training stage, distance scores calculated over 
annotated ?T?-?H? pairs are used to estimate a 
threshold that best separates positive from negative 
examples. The threshold, which is stored in a 
Model, is used at a test stage to assign an entail-
ment judgment and a confidence score to each test 
pair. 
 
 
Figure 3: Our configured file for training 
 
Figure 3 shows our configuration file for train-
ing models, we choose ?distance? algorithm in 
EDITS, and ?default_matcher?, and ?ignore_case? , 
and some other default but effective configured 
parameters. 
 
 
Figure 4: The overall training and decoding 
procedure in our system. 
 
Figure 4 shows our training and decoding 
procedure. As EDITS can only recognize textual 
entailment from one part to the other, we manually 
change the tag ?H? with ?T?, and generate the re-
sults again, and then compute two parts? entailment 
relations. For example, if ?T?-?H? is ?YES?, and 
?H?-?T? is ?NO?, then the entailment result be-
tween them is ?forward?; if ?T?-?H? is ?NO?, and 
?H?-?T? is ?YES?, then the entailment result be-
tween them is ?backward?; if both  ?T?-?H? and 
?H?-?T? are ?YES?, the result is ?bidirectional?; 
717
otherwise ?no_entailment?. 
3 Experiments and Results 
Since organizers of SemEval 2012 task 8 supply a 
piece of data for training, we thus exploit it to op-
timize parameters for EDITS. Table 1 shows the F-
measure score of training set analyzed by EDITS, 
where ?FE? represents French-English, ?SE? rep-
resents Spanish-English, ?IE? represents Italian-
English and ?GE? represents Italian-English.  
 
Judgment  FE SE IE GE 
forward 
backward 
no_entailment 
bidirectional 
Overall 
0.339 
0.611 
0.533 
0.515 
0.516 
0.373 
0.574 
0.535 
0.502 
0.506 
0.440 
0.493 
0.494 
0.506 
0.488 
0.327 
0.552 
0.494 
0.495 
0.482 
Table 1:  Results on training set. 
 
From Table 1, we can see that the perfor-
mance of ?forward? prediction is lower than others. 
One explanation is that the ?T? is translated from 
foreign language, which is error unavoidable. Thus 
some rules used for checking ?T?, such as stop-
word list will be disabled. Then it is possible to 
induce a ?NO? relation between ?T? and ?H? that 
results in lower recall of ?forward?. 
Since for French-English, we build a system 
combination for improving the quality of transla-
tion. Table 2 shows the results of BLEU score of 
translation quality, and F-score of entailment 
judgment.  
 
System  BLEU4 F-score 
HPB 
GTT 
COMB 
28.74 
30.08 
30.57 
0.496 
0.508 
0.516 
Table 2:  Performance of different translation 
model, where COMB represents system com-
bination. 
 
From table 2, we find that the translation qual-
ity slightly affect the correctness of entailment 
judgment. However, the difference of performance 
in entailment judgment is smaller than that in 
translation quality. We explain that the translation 
models exploit phrase-based rules to direct the 
translation, and the translation errors mainly come 
from the disorder between each phrases.  While a 
distance based entailment model generally consid-
ers the similarity of phrases between test and hy-
pothesis, thus the disorder of phrases influences the 
judgment slightly.   
Using the given training data for tuning pa-
rameters, table 3 to table 6 shows the detailed ex-
perimental results on testing data, where P 
represents precision and R indicates recall, and 
both of them are calculated by given evaluation 
script. 
  
French -- English 
Judgment P R F-measure 
forward 
backward 
no_entailment 
bidirectional 
Overall 
Best System 
0.750 
0.517 
0.385 
0.444 
0.192 
0.496 
0.656 
0.480 
0.306 
0.506 
0.485 
0.462 
0.456 
0.570 
 Table 3: Test results on French-English 
 
Spanish -- English 
Judgment  P R F-measure 
forward 
backward 
no_entailment 
bidirectional 
Overall 
Best System 
0.750 
0.440 
0.395 
0.436 
0.240 
0.472 
0.560 
0.520 
0.364 
0.456 
0.464 
0.474 
0.448 
0.632 
Table 4: Test results on Spanish-English 
  
Italian ? English 
Judgment  P R F-measure 
forward 
backward 
no_entailment 
bidirectional 
Overall 
Best System 
0.661 
0.554 
0.427 
0.383 
0.296 
0.368 
0.448 
0.704 
0.409 
0.442 
0.438 
0.496 
0.454 
0.566 
 Table 5: Test results on Italian-English 
 
German ? English 
Judgment  P R F-measure 
forward 
backward 
no_entailment 
bidirectional 
Overall 
Best System 
0.718 
0.493 
0.390 
0.439 
0.224 
0.552 
0.512 
0.552 
0.341 
0.521 
0.443 
0.489 
0.460 
0.558 
Table 6: Test results on German-English 
718
 
After given golden testing reference, we also 
investigate the effect of training set to testing set. 
We choose testing set from RTE1 and RTE2, both 
are English text, as our training set for optimiza-
tion of EDITS, and the overall results are shown in 
table 7 to table 10, where CLTE is training set giv-
en by this year?s organizers. 
 
French -- English 
Judgment  CLTE RTE1 RTE2 
forward 
backward 
no_entailment 
bidirectional 
Overall 
0.306 
0.506 
0.485 
0.462 
0.456 
0.248 
0.425 
0.481 
0.472 
0.430 
0.289 
0.440 
0.485 
0.485 
0.444 
Table 7: Test results on French-English 
given different training set. 
 
Spanish ? English 
Judgment  CLTE RTE1 RTE2 
forward 
backward 
no_entailment 
bidirectional 
Overall 
0.364 
0.456 
0.464 
0.474 
0.448 
0.293 
0.332 
0.386 
0.484 
0.400 
0.297 
0.372 
0.427 
0.503 
0.424 
Table 8: Test results on Spanish-English 
given different training set. 
 
Italian -- English 
Judgment  CLTE RTE1 RTE2 
forward 
backward 
no_entailment 
bidirectional 
Overall 
0.409 
0.442 
0.438 
0.496 
0.454 
0.333 
0.394 
0.410 
0.474 
0.420 
0.335 
0.436 
0.421 
0.480 
0.432 
Table 9: Test results on Italian-English 
given different training set. 
 
German ? English 
Judgment  CLTE RTE1 RTE2 
forward 
backward 
no_entailment 
bidirectional 
Overall 
0.341 
0.521 
0.443 
0.489 
0.460 
0.377 
0.372 
0.437 
0.487 
0.434 
0.425 
0.460 
0.457 
0.508 
0.470 
Table 10: Test results on German-English 
given different training set. 
 
Results in table 7 and table 8 shows that mod-
els trained on ?CLTE? have better performance 
than those trained on RTE1 and RTE2, except ?bi-
directional? judgment type. In Table 9, all results 
decoding by models trained on ?CLTE? are the 
best. And in Table 10, only a few results decoding 
by models trained on ?RTE1? and ?RTE2? have 
higher score. The reason may be that, the test cor-
pora are bilingual, there are some errors in the ma-
chine translation procedure when translate one part 
of the test from its language into the other. When 
training on these bilingual text and decoding these 
bilingual text, these two procedure have error con-
sistency. Some errors may be counteracted. If we 
train on RTE, a standard monolingual text, and 
decode a bilingual text, more errors may exist be-
tween the two procedures. So we believe that, if 
we use translation based strategy (machine transla-
tion and monolingual textual entailment) to gener-
ate cross-lingual textual entailment, we should use 
translation based strategy to train models, rather 
than use standard monolingual texts. 
4 Conclusion 
In this paper, we demonstrate our system frame-
work for this year?s cross-lingual textual entail-
ment task. We propose a translation based model 
to address cross-lingual entailment. We first trans-
late all foreign languages into English, and then 
employ EDITS to induce entailment relations. Ex-
periments show that our method achieves promis-
ing results but not perfect results compared to other 
participants. 
Acknowledgments 
The authors were supported by National Science 
Foundation of China, Contracts 90920004, and 
High-Technology R&D Program (863) Project No 
2011AA01A207 and 2012BAH39B03.  We thank 
organizers for their generous supplied resources 
and arduous preparation. We also thank anony-
mous reviewers for their thoughtful suggestions. 
References  
Bar-Haim, R., Dagan, I., Dolan, B., Ferro, L., Giampic-
colo, D., Magnini, B., & Szpektor, I. 2006.The 2nd 
PASCAL recognising textual entailment challenge. In 
Proc. of the 2nd PASCAL ChallengesWorkshop on 
Recognising Textual Entailment, Venice, Italy. 
719
Bos, J., & Markert, K. 2005. Recognising textual en-
tailment with logical inference. In Proc. Of the Conf. 
on HLT and EMNLP, pp. 628?635, Vancouver, BC, 
Canada. 
Dagan, I., Dolan, B., Magnini, B., & Roth, D. 2009. 
Recognizing textual entailment: Rational,evaluation 
and approaches. Nat. Lang. Engineering, 15(4), i?
xvii. Editorial of the special issue on Textual Entail-
ment. 
David Chiang. 2005. A hierarchical phrase-based model 
for statistical machine translation. In Proceedings of 
ACL 2005, pages 263?270. 
Giampiccolo, D., Dang, H., Magnini, B., Dagan, I., & 
Dolan, B. 2008. The fourth PASCAL recognizing tex-
tual entailment challenge. In Proc. of the Text Anal-
ysis Conference, pp. 1?9, Gaithersburg, MD. 
Giampiccolo, D., Magnini, B., Dagan, I., & Dolan, B. 
2007. The third PASCAL recognizing textual entail-
ment challenge. In Proc. of the ACL-Pascal Work-
shop on Textual Entailment and Paraphrasing, pp. 1?
9, Prague, Czech Republic. 
I. Dagan and O. Glickman.2004. Probabilistic Textual 
Entailment: Generic Applied Modeling of Language 
Variability. Proceedings of the PASCAL Workshop 
of Learning Methods for Text Understanding and 
Mining. 
Ion Androutsopoulos and Prodromos Malakasiotis. 
2010.A Survey of Paraphrasing and Textual Entail-
ment Methids. Journal of Artificial Intelligence Re-
search, 32, 135-187. 
Kouylekov, M. and Negri, M. 2010. An open-source 
package for recognizing textual entailment. Proceed-
ings of the ACL 2010 System Demonstrations, 42-47. 
Kubler, S., McDonald, R., & Nivre, J. 2009. Dependen-
cy Parsing. Synthesis Lectures on HLT. Morgan and 
Claypool Publishers. 
Levenshtein, V. 1966. Binary codes capable of correct-
ing deletions, insertions, and reversals. Soviet 
Physice-Doklady, 10, 707?710. 
Lin, D. 1998b. An information-theoretic definition of 
similarity. In Proc. of the 15th Int. Conf. on Machine 
Learning, pp. 296?304, Madison, WI. Morgan 
Kaufmann, San Francisco, CA. 
Malakasiotis, P., & Androutsopoulos, I. 2007. Learning 
textual entailment using SVMs and string similarity 
measures. In Proc. of the ACL-PASCAL Workshop 
on Textual Entailment and Paraphrasing, pp. 42?47, 
Prague. ACL. 
Mehdad, Y. and Negri, M. and Federico, M.2010. To-
wards Cross-Lingual Textual Entailment. Human 
Language Technologies.The 2010 Annual Confer-
ence of the NAACL. 321-324. 
Mehdad, Y. and Negri, M. and Federico, M.2011. Using 
bilingual parallel corpora for cross-lingual textual 
entailment. Proceedings of ACL-HLT 
Melcuk, I. 1987. Dependency Syntax: Theory and Prac-
tice. State University of New York Press. 
M. Negri, A. Marchetti, Y. Mehdad, L. Bentivogli, and 
D. Giampiccolo.2012. Semeval-2012 Task 8: Cross-
ligual Textual Entailment for Content Synchronizatio
n. In Proceedings of the 6th International Workshop 
on Semantic Evaluation (SemEval 2012).  
Negri, M. and Bentivogli, L. and Mehdad, Y. and 
Giampiccolo, D. and Marchetti, A.2011. Divide and 
conquer: crowdsourcing the creation of cross-lingual 
textual entailment corpora. Proceedings of the Con-
ference on Empirical Methods in Natural Language 
Processing. 
Pad?o, S., & Lapata, M. 2007. Dependency-based con-
struction of semantic space models. Comp. Ling., 
33(2), 161?199. 
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003. 
Statistical phrase-based translation. In Proceedings 
of the 2003 Human Language Technology Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, Edmonton, 
Canada, July. 
Rinaldi, F., Dowdall, J., Kaljurand, K., Hess, M., & 
Molla, D. 2003. Exploiting paraphrases in a question 
answering system. In Proc. of the 2nd Int. Workshop 
in Paraphrasing, pp. 25?32, Saporo, Japan. 
Rosti, A. and Matsoukas, S. and Schwartz, R. Improved 
word-level system combination for machine transla-
tion, ANNUAL MEETING-ASSOCIATION FOR 
COMPUTATIONAL LINGUISTICS,2007 
Tatu, M., & Moldovan, D. 2005. A semantic approach 
to recognizing textual entailment. In Proc. of the 
Conf. on HLT and EMNLP, pp. 371?378, Vancouver, 
Canada. 
Tatu, M., & Moldovan, D. 2007. COGEX at RTE 3. In 
Proc. of the ACL-PASCAL Workshop on Textual 
Entailment and Paraphrasing, pp. 22?27, Prague, 
Czech Republic. 
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree?to 
string alignment template for statistical machine 
translation. In Proceedings of ACL 2006, pages 609?
616, Sydney, Australia, July. 
720
Proceedings of the 15th Conference on Computational Natural Language Learning: Shared Task, pages 76?80,
Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational Linguistics
ETS: An Error Tolerable System for Coreference Resolution
Hao Xiong , Linfeng Song , Fandong Meng , Yang Liu , Qun Liu and Yajuan Lu?
Key Lab. of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences
P.O. Box 2704, Beijing 100190, China
{xionghao,songlinfeng,mengfandong,yliu,liuqun,lvyajuan}@ict.ac.cn
Abstract
This paper presents our error tolerable sys-
tem for coreference resolution in CoNLL-
2011(Pradhan et al, 2011) shared task (closed
track). Different from most previous reported
work, we detect mention candidates based on
packed forest instead of single parse tree, and
we use beam search algorithm based on the
Bell Tree to create entities. Experimental re-
sults show that our methods achieve promising
results on the development set.
1 Introduction
Over last decades, there has been increasing inter-
est on coreference resolution within NLP commu-
nity. The task of coreference resolution is to iden-
tify expressions in a text that refer to the same dis-
course entity. This year, CoNLL1 holds a shared
task aiming to model unrestricted coreference in
OntoNotes.2 The OntoNotes project has created a
large-scale, accurate corpus for general anaphoric
coreference that covers entities and events not lim-
ited to noun phrases or a limited set of entity types.
And Pradhan et al (2007) have ever used this corpus
for similar unrestricted coreference task.
Our approach to this year?s task could be divided
into two steps: mention identification and creation
of entities. The first stage is conducted on the anal-
ysis of parse trees produced by input data. The of-
ficial data have provided gold and automatic parse
trees for each sentences in training and development
1http://conll.bbn.com/
2http://www.bbn.com/ontonotes/
set. However, according to statistics, almost 3%
mentions have no corresponding constituents in au-
tomatic parse trees. Since only automatic parse trees
will be provided in the final test set, the effect of
parsing errors are inevitable. To alleviate this issue,
based on given automatic parse trees, we modify a
state-of-the-art parser (Charniak and Johnson, 2005)
to generate packed forest, and determine mention
candidates among all constituents from both given
parse tree and packed forest. The packed forest is a
compact representation of all parse trees for a given
sentence. Readers can refer to (Mi et al, 2008) for
detailed definitions.
Once the mentions are identified, the left step is
to group mentions referring to same object into sim-
ilar entity. This problem can be viewed as binary
classification problem of determining whether each
mention pairs corefer. We use a Maximum Entropy
classifier to predict the possibility that two mentions
refer to the similar entity. And mainly following the
work of Luo et al (2004), we use a beam search
algorithm based on Bell Tree to obtain the global
optimal classification.
As this is the first time we participate competi-
tion of coreference resolution, we mainly concen-
trate on developing fault tolerant capability of our
system while omitting feature engineering and other
helpful technologies.
2 Mention Detection
The first step of the coreference resolution tries to
recognize occurrences of mentions in documents.
Note that we recognize mention boundaries only on
development and test set while generating training
76
Figure 1: Left side is parse tree extracted from develop-
ment set, and right side is a forest. ?my daughter? is a
mention in this discourse, however it has no correspond-
ing constituent in parse tree, but it has a corresponding
constituent NP0 in forest.
instances using gold boundaries provided by official
data.
The first stage of our system consists of following
three successive steps:
? Extracting constituents annotated with NP,
NNP, PRP, PRP$ and VBD POS tags from sin-
gle parse tree.
? Extracting constituents with the same tags as
the last step from packed forest.
? Extracting Named Entity recognized by given
data.
It is worth mentioning that above three steps will
produce duplicated mentions, we hence collect all
mentions into a list and discard duplicated candi-
dates. The contribution of using packed forest is that
it extends the searching space of mention candidates.
Figure 1 presents an example to explain the advan-
tage of employing packed forest to enhance the men-
tion detection process. The left side of Figure 1 is
the automatic parse tree extracted from development
set, in which mention ?my daughter? has no corre-
sponding constituent in its parse tree. Under nor-
mal strategy, such mention will not be recognized
and be absent in the clustering stage. However, we
find that mention has its constituent NP0 in packed
forest. According to statistics, when using packed
forest, only 0.5% mentions could not be recognized
while the traditional method is 3%, that means the
theoretical upper bound of our system reaches 99%
compared to baseline?s 97%.
Since the requirement of this year?s task is
to model unrestricted coreference, intuitively, we
should not constraint in recognizing only noun
phrases but also adjective phrase, verb and so on.
However, we find that most mentions appeared in
corpus are noun phrases, and our experimental re-
sults indicate that considering constituents annotated
with above proposed POS tags achieve the best per-
formance.
3 Determining Coreference
This stage is to determine which mentions belong to
the same entity. We train a Maximum Entropy clas-
sifier (Le, 2004) to decide whether two mentions are
coreferent. We use the method proposed by Soon, et
al.?s to generate the training instances, where a posi-
tive instance is formed between current mention Mj
and its closest preceding antecedent Mi, and a neg-
ative instance is created by paring Mj with each of
the intervening mentions, Mi+1, Mi+2,...,Mj?1.
We use the following features to train our classi-
fier.
Features in Soon et al?s work (Soon et al, 2001)
Lexical features
IS PREFIX: whether the string of one mention is
prefix of the other;
IS SUFFIX: whether the string of one mention is
suffix of the other;
ACRONYM: whether one mention is the acronym
of the other;
Distance features
SENT DIST: distance between the sentences con-
taining the two mentions;
MEN DIST: number of mentions between two
mentions;
Grammatical features
IJ PRONOUN: whether both mentions are pro-
noun;
I NESTED: whether mention i is nested in an-
other mention;
J NESTED: whether mention j is nested in an-
other mention;
Syntax features
HEAD: whether the heads of two mentions have
the same string;
HEAD POS: whether the heads of two mentions
have the same POS;
HEA POS PAIRS: pairs of POS of the two men-
tions? heads;
77
Semantic features
WNDIST: distance between two mentions in
WordNet;
I ARG0: whether mention i has the semantic role
of Arg0;
J ARG0: whether mention j has the semantic role
of Arg0;
IJ ARGS: whether two mentions have the seman-
tic roles for similar predicate;
In the submitted results, we use the L-BFGS pa-
rameter estimation algorithm with gaussian prior
smoothing (Chen and Rosenfeld, 1999). We set the
gaussian prior to 2 and train the model in 100 itera-
tions.
3.1 Creation of Entities
This stage aims to create the mentions detected in
the first stage into entities, according to the predic-
tion of classifier. One simple method is to use a
greedy algorithm, by comparing each mention to its
previous mentions and refer to the one that has the
highest probability. In principle, this algorithm is
too greedy and sometimes results in unreasonable
partition (Ng, 2010). To address this problem, we
follow the literature (Luo et al, 2004) and propose
to use beam search to find global optimal partition.
Intuitively, creation of entities can be casted as
partition problem. And the number of partitions
equals the Bell Number (Bell, 1934), which has a
?closed? formula B(n) = 1e
??
k=0
kn
k! . Clearly, this
number is very huge when n is large, enumeration of
all partitions is impossible, so we instead designing
a beam search algorithm to find the best partition.
Formally, the task is to optimize the following ob-
jective,
y? = argmax
??P
?
e??
Prob(e) (1)
where P is all partitions, Prob(e) is the cost of
entity e. And we can use the following formula to
calculate the Prob(e),
Prob(e) =
?
i?e,j?e
pos(mi,mj)
+
?
i?e,j /?e
neg(mi,mj)
(2)
where pos(mi,mj) is the score predicted by clas-
sifier that the possibility two mentions mi and mj
group into one entity, and neg(mi,mj) is the score
that two mentions are not coreferent.
Theoretically, we can design a dynamic algorithm
to obtain the best partition schema. Providing there
are four mentions from A to D, and we have ob-
tained the partitions of A, B and C. To incorporate
D, we should consider assigning D to each entity of
every partition, and generate the partitions of four
mentions. For detailed explanation, the partitions
of three mentions are [A][B][C], [AB][C], [A][BC]
and [ABC], when considering the forth mention D,
we generate the following partitions:
? [A][B][C][D], [AD][B][C], [A][BD][C],
[A][B][CD]
? [AB][C][D], [ABD][C],[AB][CD]
? [A][BC][D], [AD][BC], [A][BCD]
? [ABC][D], [ABCD]
The score of partition [AD][B][C] can be
calculated by score([A][B][C]) + pos(A,D) +
neg(B,D) + neg(C,D). Since we can computer
pos and neg score between any two mentions in
advance, this problem can be efficiently solved by
dynamic algorithm. However, in practice, enumer-
ating the whole partitions is intractable, we instead
exploiting a beam with size k to store the top k parti-
tions of current mention size, according to the score
the partition obtain. Due to the scope limitation, we
omit the detailed algorithm, readers can refer to Luo
et al (2004) for detailed description, since our ap-
proach is almost similar to theirs.
4 Experiments
4.1 Data Preparation
The shared task provided data includes information
of lemma, POS, parse tree, word sense, predicate
arguments, named entity and so on. In addition to
those information, we use a modified in house parser
to generate packed forest for each sentence in devel-
opment set, and prune the packed forest with thresh-
old p=3 (Huang, 2008). Since the OntoNotes in-
volves multiple genre data, we merge all files and
78
Mention MUC BCUBED CEAFM CEAFE BLANC
baseline 58.97% 44.17% 63.24% 45.08% 37.13% 62.44%
baseline gold 59.18% 44.48% 63.46% 45.37% 37.47% 62.36%
sys forest 59.07% 44.4% 63.39% 45.29% 37.41% 62.41%
sys btree 59.44% 44.66% 63.77% 45.62% 37.82% 62.47%
sys forest btree 59.71% 44.97% 63.95% 45.91% 37.96% 62.52%
Table 1: Experimental results on development set (F score).
Mention MUC BCUBED CEAFM CEAFE BLANC
sys1 54.5% 39.15% 63.91% 45.32% 37.16% 63.18%
sys2 53.06% 35.55% 59.68% 38.24% 32.03% 50.13%
Table 2: Experimental results on development set with different training division (F score).
take it as our training corpus. We use the sup-
plied score toolkit 3 to compute MUC, BCUBED,
CEAFM, CEAFE and BLANC metrics.
4.2 Experimental Results
We first implement a baseline system (baseline)
that use single parse tree for mention detection
and greedy algorithm for creation of entities. We
also run the baseline system using gold parse tree,
namely baseline gold. To investigate the contribu-
tion of packed forest, we design a reinforced sys-
tem, namely sys forest. And another system, named
as sys btree, is used to see the contribution of beam
search with beam size k=10. Lastly, we combine
two technologies and obtain system sys forest btree.
Table 1 shows the experimental results on devel-
opment data. We find that the system using beam
search achieve promising improvement over base-
line. The reason for that has been discussed in last
section. We also find that compared to baseline,
sys forest and baseline gold both achieve improve-
ment in term of some metrics. And we are glad to
find that using forest, the performance of our sys-
tem is approaching the system based on gold parse
tree. But even using the gold parse tree, the im-
provement is slight. 4 One reason is that we used
some lexical and grammar features which are dom-
3http://conll.bbn.com/download/scorer.v4.tar.gz
4Since under task requirement, singleton mentions are fil-
tered out, it is hard to recognize the contribution of packed for-
est to mention detection, while we may incorrectly resolve some
mentions into singletons that affects the score of mention detec-
tion.
inant during prediction, and another explanation is
that packed forest enlarges the size of mentions but
brings difficulty to resolve them.
To investigate the effect of different genres to de-
velop set, we also perform following compared ex-
periments:
? sys1: all training corpus + WSJ development
corpus
? sys2: WSJ training corpus + WSJ development
corpus
Table 2 indicates that knowledge from other genres
can help coreference resolution. Perhaps the reason
is the same as last experiments, where syntax diver-
sity affects the task not very seriously.
5 Conclusion
In this paper, we describe our system for CoNLL-
2011 shared task. We propose to use packed for-
est and beam search to improve the performance of
coreference resolution. Multiple experiments prove
that such improvements do help the task.
6 Acknowledgement
The authors were supported by National Natural
Science Foundation of China, Contracts 90920004.
We would like to thank the anonymous reviewers
for suggestions, and SHUGUANG COMPUTING
PLATFORM for supporting experimental platform.
79
References
E.T. Bell. 1934. Exponential numbers. The American
Mathematical Monthly, 41(7):411?419.
E. Charniak and M. Johnson. 2005. Coarse-to-fine n-
best parsing and maxent discriminative reranking. In
Proceedings of the 43rd Annual Meeting on Associ-
ation for Computational Linguistics, pages 173?180.
Association for Computational Linguistics.
Stanley F. Chen and Ronald Rosenfeld. 1999. A gaussian
prior for smoothing maximum entropy models. Tech-
nical report, CMU-CS-99-108.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of
ACL-08: HLT, pages 586?594, Columbus, Ohio, June.
Z. Le. 2004. Maximum entropy modeling toolkit for
Python and C++.
X. Luo, A. Ittycheriah, H. Jing, N. Kambhatla, and
S. Roukos. 2004. A mention-synchronous corefer-
ence resolution algorithm based on the bell tree. In
Proceedings of the 42nd Annual Meeting on Associa-
tion for Computational Linguistics, pages 135?es. As-
sociation for Computational Linguistics.
H. Mi, L. Huang, and Q. Liu. 2008. Forestbased transla-
tion. In Proceedings of ACL-08: HLT, pages 192?199.
Citeseer.
Vincent Ng. 2010. Supervised noun phrase coreference
research: The first fifteen years. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 1396?1411, Uppsala, Swe-
den, July. Association for Computational Linguistics.
Sameer Pradhan, Lance Ramshaw, Ralph Weischedel,
Jessica MacBride, and Linnea Micciulla. 2007. Unre-
stricted Coreference: Identifying Entities and Events
in OntoNotes. In in Proceedings of the IEEE Inter-
national Conference on Semantic Computing (ICSC),
September 17-19.
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, and Nianwen Xue.
2011. Conll-2011 shared task: Modeling unrestricted
coreference in ontonotes. In Proceedings of the Fif-
teenth Conference on Computational Natural Lan-
guage Learning (CoNLL 2011), Portland, Oregon,
June.
W.M. Soon, H.T. Ng, and D.C.Y. Lim. 2001. A ma-
chine learning approach to coreference resolution of
noun phrases. Computational Linguistics, 27(4):521?
544.
80
