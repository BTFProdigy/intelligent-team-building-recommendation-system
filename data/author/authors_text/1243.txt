R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 933 ? 944, 2005. 
? Springer-Verlag Berlin Heidelberg 2005 
Towards Robust High Performance  
Word Sense Disambiguation of English Verbs  
Using Rich Linguistic Features 
Jinying Chen and Martha Palmer 
Department of Computer and Information Science, 
University of Pennsylvania, Philadelphia, PA, 19104, USA 
{jinying, mpalmer}@cis.upenn.edu  
Abstract. This paper shows that our WSD system using rich linguistic features 
achieved high accuracy in the classification of English SENSEVAL2 verbs for 
both fine-grained (64.6%) and coarse-grained (73.7%) senses. We describe 
three specific enhancements to our treatment of rich linguistic features and pre-
sent their separate and combined contributions to our system?s performance. 
Further experiments showed that our system had robust performance on test 
data without high quality rich features.  
1   Introduction 
Word sense disambiguation (WSD) has been regarded as essential or necessary in 
many high-level NLP applications that require a certain degree of semantic interpreta-
tion, such as machine translation, information retrieval (IR) and question answering, 
etc. However, previous investigations into the role of WSD in IR have shown that low 
accuracy in WSD negated any possible performance increase from ambiguity resolu-
tion [1,2]. This suggests that improving the performance of WSD systems is crucial 
for applications to attain benefits from WSD. 
Much effort has been aimed at the creation of sense tagged corpora that can be 
used to develop supervised WSD systems with high accuracy. 1  However, highly 
polysemous words with subtle sense distinctions still pose major challenges for auto-
matic systems, as evidenced in SENSEVAL2 [3]. This problem seems more serious 
for verbs, as indicated by the relatively poorer performance achieved by the best sys-
tem in the SENSEVAL2 English lexical sample task for verbs: 56.6% accuracy, in 
contrast with the 64.2% accuracy for all parts-of-speech [4,5]. On the other hand, 
disambiguating verb senses accurately is very important for lexical selection in MT. It 
is also helpful for information retrieval, especially for fact retrieval systems that take 
full-sentence queries as their input. Therefore, this paper will focus on improving the 
accuracy of our supervised WSD system for verbs. 
We are using a linguistically rich approach for verb sense disambiguation. Linguis-
tically rich approaches [5-9] utilize syntactic and/or semantic features, e.g., syntactic 
relations, selectional preferences, and semantic information of NP arguments of verbs, 
                                                          
1
 http://www.senseval.org/ 
934 J. Chen and M. Palmer 
etc. In verb sense disambiguation, Dang and Palmer's work [5] demonstrated that their 
system, which achieved 59.6% accuracy (62.5% in a recent report [10]) in disambigu-
ating the SENSEVAL2 English verbs, benefited substantially from using rich linguis-
tic features that capture information about a verb's lexical semantics.    
On the other hand, the performance of a system using rich linguistic features relies 
heavily on the quality of preprocessing, such as part-of-speech tagging, parsing, fea-
ture extraction and generation, etc. How accurate and how robust can such a system 
be? In particular, we are interested in the following three questions: How much ad-
vantage can we gain from the rich-feature approach by careful extraction and treat-
ment of the rich features? How much will a relatively poor quality of preprocessing 
negatively affect the system's performance? Which strategies can we adopt to allevi-
ate these negative effects?  
To address the first question, we enhance the feature extraction and generation of 
our original system, which was inspired by Dang?s system[10], in three ways. First, to 
increase the recall of the extraction of a verb's subject, we carefully handle relative 
clauses, nonfinite clauses, and verbs within prepositional phrases by using linguistic 
knowledge and heuristics. Second, to treat semantic features of NP arguments of 
verbs and prepositions in a more uniform way, we incorporate a rule-based pronoun 
resolver and also unify the semantic features generated by WordNet [11] and by a 
named entity tagger. Third, we treat sentential complements of verbs in a verb-
specific way. Our evaluation on the SENSEVAL2 English verbs shows that our new 
system achieves 64.6% accuracy, which is significantly better than the best system on 
English verbs in SENSEVAL2 (57.6%) and also outperforms Dang's system (62.5%). 
Further experiments indicate that the three enhancements are all beneficial. They each 
boost the system's performance by 1.0~1.2 percent and the combined gain is 2.6 per-
cent. A similar performance improvement is achieved for coarse-grained senses: 
73.7% vs. Dang's 71.7%. The data analysis of the results suggests that further im-
provements may come from disambiguating WordNet synsets and from using statisti-
cal methods for subject extraction and pronoun resolution.  
We address the last two robustness questions in two more experiments. To investi-
gate how the parser's performance affects our system, we divide the test data into an 
easy set that is similar to the parser's training material and a hard set that is not. The 
evaluation shows that although our system's accuracy is lower on the hard set, it is 
still high (62.2%). In the second experiment, our system is trained with rich features 
and tested on data with linguistically impoverished features. The results show little 
penalty from missing rich features at the test phase. The observations from this ex-
periment also suggest the following strategy for using WSD systems that utilize rich 
linguistic features. When good parsers are not available at the time of application, the 
use of topical features and any available, accurate rich features (e.g., features associ-
ated with the verb's direct object) will alleviate penalties.  
The rest of the paper is organized as follows. We introduce our system and the 
three major enhancements we made in Section 2. In Section 3, we show the evalua-
tion results on SENSEVAL2 English verbs and show how much the three enhance-
ments improve our system's performance. We then discuss the potential improve-
ments of our system in the future. In Section 4, we investigate the robustness of our 
system and propose our strategy for alleviating the negative effects of poor preproc-
essing. We conclude our discussion in Section 5. 
 Towards Robust High Performance Word Sense Disambiguation of English Verbs 935 
2   System Description 
Our original WSD system was inspired by the successful MaxEnt WSD system of 
Dang [5,10]. We used the same machine learning model, Mallet, that implements a 
smoothing maximum entropy (ME) model with a Gaussian prior [12].  An attractive 
property of ME models is that there is no assumption of feature independence [13]. 
Empirical studies have shown that a ME model with a Gaussian prior generally out-
performs ME models with other smoothing methods [14]. In addition to topical and 
collocation features, we also used similar rich syntactic and semantic features, al-
though we implemented them in different ways. Furthermore, we enhanced the treat-
ment of certain rich linguistic features, which we believed would boost the system's 
performance. Before discussing these enhancements, we first briefly describe the 
basic syntactic and semantic features used by our system: 
Syntactic features: 
1. Is the sentence passive, semi-passive2 or active?  
2. Does the target verb have a subject or object? If so, what is the head of its  
         subject or/and object? 
3. Does the target verb have a sentential complement? 
4. Does the target verb have a PP adjunct? If so, what is the preposition and what is 
        the head of the NP argument of the preposition?   
Semantic features: 
1. The Named Entity tags of proper nouns and certain types of common nouns  
2. The WordNet synsets and hypernyms of head nouns of the NP arguments of 
         verbs and prepositions 
To better explore the advantage of using rich syntactic and semantic features, we 
enhanced our original system in three primary aspects: increasing the recall of the 
extraction of a verb's subject; unifying the treatment of semantic features of pronouns, 
common nouns and proper nouns; and providing a verb-specific treatment of senten-
tial complements.  These are each described in more detail below. 
2.1   Increasing Subject Extraction Recall  
To extract a subject, our original system simply checks the left NP siblings of the 
highest VP that contains the target verb and is within the innermost clause (see Figure 
1). This method has high precision but low recall. Typical examples from 
SENSEVAL2 data that are not handled by this approach are shown in (1a-c).3 
(1) a. Relative clauses: For Republicanssbj [SBAR who beganverb this campaign with  
              such high hopes],  ... 
      b. Nonfinite clauses: Isbj didn't ever want [S to seeverb that woman again]. 
      c.Verbs within PP's: Karipo and her womensbj had succeeded [PP in drivingverb 
             a hundred invaders from the isle ...] 
                                                          
2
  Verbs that are past participles and are not preceded by be or have verbs are semi-passive. 
3
  The target verb and its subject or subject candidates are underlined and the innermost clause 
or the PP containing the verb is bracketed.  
936 J. Chen and M. Palmer 
 
Fig. 1. position for verb?s subject 
 
To increase the recall, we refined the procedure of subject extraction by adding 
rules based on linguistic knowledge and bracketing labels that can handle relative 
clauses, nonfinite clauses, and verbs within prepositional phrases (PP's). For example, 
for cases like (1a), if a clause containing the target verb has a bracketing label SBAR 
and an NP parent, and is headed by a relative pronoun such as that, which or who, 
then check its left NP siblings for the verb's subject. For cases like (1b) and (1c), if the 
parent node of a nonfinite clause S or a PP is a VP, then continue searching positions 
outside the S or PP. For the last case, we also use a heuristic, i.e., a check as to 
whether the subject candidate is a person or an organization, to filter out non-person-
and-organization candidate NPs whose parent nodes are not labeled as S or SBAR. 
Many cases like (2a-b) can be handled correctly using this heuristic. 
(2) a. A number of accounts of the events accused the ministrysbj [PP of pullingverb  
               the plug on the UAL deal ...]. 
       b. Mr. Wolfsbj faces a monumental task [PP in pullingverb the company back  
                together again]. 
The above rule-based approach does not handle difficult cases like (3a-b) very well.  
(3) a. Freddy's instinct was [S to keepverb growing by stock mergers and small  
               expenditure of cash ...] 
       b.The arrangement I had with him was [S to workverb four hours a day]. 
With this enhancement, our new system extracts about 35% more subjects than be-
fore.  
S
NP         VP
? 
VP 
target verb
position for subject
Republicans   SBAR 
who        S 
began this campaign ? 
??        NP
(1a)  ?    
ADVP       VP
ever    want         S
to see that woman again  
??       VP
(1b)  ?    
   succeeded       PP 
in driving a hundred ?  
??       VP
(1c)  ?    
 Towards Robust High Performance Word Sense Disambiguation of English Verbs 937 
2.2   Unifying Semantic Features 
In this section we describe the changes to the use of semantic features. In order to 
provide a more uniform treatment for the semantic features of the NP arguments of 
verbs and prepositions, we first merge the semantic features associated with proper 
nouns and common nouns.  We then extend our treatment to include pronouns by 
adding a pronoun resolution module. 
2.2.1   Merging Semantic Features 
Our system used an automatic named entity tagger, IdentiFinderTM [15], to tag proper 
nouns with Person, Organization and Location and common nouns with Date, 
Time, Percent and Money. Additional semantic features are all WordNet synsets and 
hypernyms4 of the head nouns of NP arguments, i.e., the system does not disambigu-
ate different WordNet senses of a head noun.  
To utilize semantic features more efficiently, we refine their treatment. Previously 
there was no overlap between semantic features generated by the named entity tagger 
and by WordNet. For example, a personal proper noun only has a Person tag that has 
no similarity to the WordNet synsets and hypernyms associated with similar common 
nouns such as specialist and doctor, etc. This is likely to be a problem for many WSD 
tasks that usually have small amounts of training data, such as SENSEVAL2. To 
overcome this problem, our new system associates a common noun (or a noun phrase) 
with each Named Entity tag (see 4) and adds the WordNet semantic features of these 
nouns (or noun phrases) to the original semantic feature set. 
(4)  Person ? someone,   Organization ? organization,   Location ? location 
       Time ? time unit,   Date ? time period,   Percent ? percent,   Money ? money 
2.2.2   Adding Pronoun Resolution  
Our original system has no special treatment for pronouns, although a rough count 
shows that about half of the training instances contain pronominal arguments. Lacking 
a high performance automatic pronoun resolution module, we adopt a hybrid approach. 
For personal pronouns, we simply treat them as personal proper nouns. For the rest of 
the pronouns including they, them, it, themselves and itself, which occur in about 13% 
of the training instances, we programmed a rather simple rule-based pronoun resolver. 
In brief, the resolver searches the parse tree for antecedent candidates similarly to 
Hobb's algorithm as exemplified in [16] and uses several syntactic and semantic con-
straints to filter out impossible candidates. The constraints include syntactic constraints 
for anaphora antecedents [16], number agreement, and whether the candidate is a per-
son. The first candidate that survives the filtering is regarded as the antecedent of the 
pronoun and its semantic features are added to the original feature set.  
2.3   Verb-Specific Sentential Complements  
The different types of sentential complements can be very useful for distinguishing 
certain verb senses. (5a-b) shows two sentences containing the verb call in the 
SENSEVAL2 training data. Call has WordNet Sense 1 (name) in (5a) and Sense 3 
                                                          
4
 A unique number defined in WordNet represents each synset or hypernym. 
938 J. Chen and M. Palmer 
(ascribe) in (5b). In both cases, call takes a small clause as its sentential complement, 
i.e., it has the subcategorization frame X call Y Z. The difference is that Z is a Named 
Entity when call is in Sense 1, and Z is usually a common NP or an adjective phrase 
(ADJP) when call is in Sense 3.  
(5)  a. The slender, handsome fellow was calledverb [S Dandy Brandon]. 
    b.The White House is purposely not callingverb [S the meeting a summit] ?  
Another example is shown in (6). The verb keep has WordNet Sense 1 (maintain) 
in (6a) and Sense 2 (continue) in (6b). In Sense 1, keep often takes a small clause and 
has the subcategorization frame X keep Y ADJP. In contrast, keep takes a sentential 
complement the head verb of which is in the present tense when it is in Sense 2. 
(6) a. He shook his head, keptverb [S his face expressionless]. 
      b. We keepverb [S wondering what Mr. Gates wanted to say]. 
Our original system uses a single feature hasSent to represent whether the target 
verb has a sentential complement or not, which cannot capture the rich information 
that is crucial to distinguishing certain verb senses but is deeply embedded in the 
sentential complements, as described above. Therefore, we treat sentential comple-
ments in a more fine-grained, verb-specific way. We resort to WordNet and PropBank 
[17] for the information about verb subcategorization frames. Another advantage of 
this verb-specific treatment is that it can filter out illegal sentential complements gen-
erated by the parser. 
3   System Evaluation 
Since the more recent SENSEVAL3 data were collected over the internet and had a 
relatively low quality of annotation, we decided to evaluate our new system on the 
SENSEVAL2 English verbs. Ratnaparkhi's MaxEnt sentence boundary detector and 
POS tagger [18], Bikel's parsing engine [19], and a named entity tagger, Identi-
FinderTM [15], were used to preprocess the training and test data automatically. 
3.1   Experimental Results  
Table 1 shows the performance of our system (MX-RF) on the 29 verbs with fine-
grained WordNet senses. Columns 2 and 3 show the number of senses and normalized 
sense perplexity5 for each verb in the test data respectively. It also gives the perform-
ance of the best system on English verbs in SENSEVAL2, KUNLP [5], and Dang's 
system [10]. As we see, our system achieves an average accuracy of 64.6%, which is 
significantly better than KUNLP  (57.6%) that only uses linguistically impoverished 
features (topical and collocation features). Our system also outperforms Dang's sys-
tem (62.5%). Recall that the types of rich linguistic features used by our system were 
originally inspired by Dang?s system, although we implemented them in different 
ways. Therefore, we attribute the more success of our new system mainly to the three 
                                                          
5
  It is calculated as the entropy of the sense distribution of a verb in the test data divided by the 
largest possible entropy, i.e., log2 (the number of senses of the verb in the test data). 
 Towards Robust High Performance Word Sense Disambiguation of English Verbs 939 
specific enhancements we made. To our best knowledge, the accuracy our system 
achieved is the best result for this task at present. 
To investigate exactly how much we gain by enhancing the system in the three 
ways discussed in Section 2, we tested our system by removing our refinements (sub-
ject extraction, pronoun coreferences, and verb-specific sentential complements) 
separately and all together. The results (columns 8-11) show that each refinement 
boosts the system's performance by 1.0~1.2 percent and that together they achieve an 
improvement of 2.6 percent. This confirms the utility of these enhancements. 
In addition to fine-grained verb senses, we also evaluated our system on coarse-
grained senses (see Table 2). Previous work [20] suggested that not all NLP applica-
tions need fine-grained sense distinctions; in some cases coarser granularities will 
suffice. Furthermore, it has been demonstrated that annotation with coarser senses is 
much faster and more accurate [21].  The SENSEVAL2 verb senses have been 
grouped by using both syntactic and semantic criteria, with a resulting inter-annotator 
agreement (ITA) of 82% (column 4).  As we expected, the accuracy of our system 
increases by about 9 percent on the coarse-grained senses to 73.7%, which again con-
sistently outperforms Dang's system (71.7%).   
3.2   Discussion 
Compared verb-by-verb, the performance of our system is better than or comparable 
to Dang's on most verbs, except that it has notably lower accuracy on develop, dress 
and serve. It is not obvious why, since although our features are similar to Dang's, the 
implementations are different. Nevertheless, an investigation of the specific features 
our system generated for these three verbs gives us a few clues. The semantic catego-
ries  of  the  direct  objects  of  the three verbs are very diverse, so there are not 
enough instances of similar categories for the model to generalize. Therefore, the 
system performance benefits little from our enhancements.  In fact, our system may 
be more susceptible to noisy data introduced by the pronoun resolver for these three 
verbs. Erroneous antecedents found by the resolver are indistinguishable from the 
actual direct objects that occur rarely in the training data, and therefore they get the 
same treatment from the machine learning algorithm. 
The experimental results and the above data analysis suggest that our system can 
be improved further by increasing the accuracy of subject extraction and pronoun 
resolution. We expect a state-of-the-art pronoun resolution module and a statistical 
subject finder to do better jobs in the future. Our current system does not distinguish 
senses of nouns when using WordNet synsets and hypernyms as semantic features, 
which introduces many irrelevant features (associated with the irrelevant senses). 
The machine learning algorithm sometimes cannot generalize well using these fea-
tures. A potential solution for this problem is to distinguish the senses of the target 
verb and its NP arguments simultaneously. Furthermore, we need to have a better 
generalization, or clustering, of WordNet synsets and hypernyms, especially when 
the subject or object of a verb has semantic versatility.  More performance improve-
ments will bring us closer to our goal of an overall level of accuracy of 80%, espe-
cially with respect to coarse-grained senses, that should finally be more beneficial to 
NLP applications. 
940 J. Chen and M. Palmer 
Table 1. Evaluation of MX-RF on the SENSEVAL2 English verbs, with fine-grained senses 
Verb #of Sen
Sen 
Per-
plex. 
ITA KUNLP Dang 2004
MX-
RF 
MX-RF 
w/o sbj 
extract.
MX-
RF 
w/o 
pron.
MX-RF 
w/o verb 
spec sent-
comp 
MX-
RF w/o 
all 
three 
begin 7 0.63 81.2 81.4 89.3 91.2 90.0 90.4 89.3 88.6 
call 17 0.86 69.3 48.5 54.5 56.8 56.8 55.3 53.8 52.3 
carry 19 0.87 60.7 45.5 39.4 44.7 45.5 40.2 43.2 42.4 
collab-
orate 2 0.47 75.0 90.0 90.0 90.0 90.0 90.0 90.0 90.0 
develop 14 0.82 67.8 42.0 58.0 49.3 49.3 50.7 49.3 49.3 
draw 21 0.95 76.7 34.1 31.7 41.5 39.0 34.1 41.5 36.6 
dress 12 0.79 86.5 71.2 72.9 64.4 64.4 69.5 67.8 64.4 
drift 9 0.89 50.0 53.1 40.6 67.2 51.6 60.9 64.1 48.4 
drive 13 0.84 58.8 54.8 59.5 60.7 60.7 58.3 60.7 58.3 
face 6 0.38 78.6 82.8 83.9 81.2 82.3 83.3 81.2 83.3 
ferret 0 0.00 1.00 100.0 100.0 100.0 100.0 100.0 100.0 100.0 
find 17 0.94 44.3 27.9 36.8 41.2 36.8 36.8 36.8 33.8 
keep 20 0.79 79.1 44.8 61.2 64.2 61.9 65.7 61.2 57.5 
leave 10 0.86 67.2 50.0 60.6 57.6 57.6 54.5 53.0 50.0 
live 9 0.70 79.7 59.7 70.1 69.4 69.4 67.9 69.4 67.9 
match 7 0.79 56.5 52.4 50.0 59.5 61.9 57.1 59.5 57.1 
play 20 0.85 N/A 37.9 53.0 62.1 59.1 62.1 62.1 62.1 
pull 25 0.89 68.1 45.0 50.0 58.3 56.6 58.3 53.3 56.7 
replace 4 0.85 65.9 55.6 60.0 61.1 60.0 55.5 61.1 57.8 
see 13 0.84 70.9 39.1 39.1 44.2 39.9 42.8 41.3 35.5 
serve 11 0.85 90.8 68.6 74.5 68.6 66.7 64.7 68.6 66.7 
strike 20 0.89 76.2 40.7 38.9 51.9 50.0 53.7 51.9 55.6 
train 8 0.87 28.8 58.7 63.5 60.3 60.3 63.5 60.3 63.5 
treat 5 0.88 96.9 56.8 50.0 50.0 50.0 52.3 50.0 56.8 
turn 26 0.93 74.2 37.3 49.3 48.5 44.0 47.8 47.0 46.3 
use 6 0.65 74.3 65.8 71.1 69.7 72.4 68.4 69.7 68.4 
wander 5 0.47 65.0 82.0 80.0 82.0 82.0 82.0 82.0 82.0 
wash 7 0.94 87.5 83.3 66.7 75.0 75.0 75.0 75.0 75.0 
work 18 0.84 N/A 45.0 45.0 53.3 51.7 50.0 53.3 43.3 
average 12 0.77 71.3 57.6 62.5 64.6 63.4 63.6 63.4 62.0 
Table 2. Evaluation of MX-RF on coarse-grained senses of the SENSEVAL2 English verbs 
 # of grp ITA grp Acc. of Dang 2004 Acc. of MX-RF 
Ave. on 29 verbs 5.9 82.0 71.7 73.7 
 Towards Robust High Performance Word Sense Disambiguation of English Verbs 941 
4   System Robustness 
A frequent criticism of systems using rich linguistic features is that they do not port 
well to domains for which accurate preprocessors are not available. In this section we 
discuss two experiments designed to address the following two questions: How much 
will a relatively poor quality of preprocessing negatively affect the system's perform-
ance? Which strategies can we adopt to alleviate these negative effects?  
4.1   Experiment I 
Since the parser is the most critical component of our preprocessing and is more 
likely to have lower performance when it is used in an unfamiliar data set, we investi-
gate how the performance of the parser on different test data sets affects our system. 
We divided the SENSEVAL2 test data into two sets: an easy set and a hard set. The 
test data from the Wall Street Journal (wsj) sections of Penn Treebank (PTB) [22] are 
put into the easy set because they are similar to the parser's training data: 02-21 wsj 
sections. The hard set contains test data from the Brown sections of PTB and BNC 
data. It is expected that the parser and therefore the system will perform better on the 
easy set. We trained our system on the whole SENSEVAL2 training data set and 
evaluated its performance on the easy and hard test sets separately. The results are 
shown in Table 3. 
Table 3. Performance on different test data sets 
Test data set Hard Easy Whole Set
Num. of test inst. 895 911 1806 
Average Acc. 62.2 66.9 64.6 
As we expected, the system's performance on the hard test set is 4.7 percent lower 
than on the easy set. On the other hand, even on the hard set, its accuracy (62.2%) is 
still high and is comparable to Dang's system. It is worth noting that the experiment is 
preliminary because the easy set and the hard set are most likely to be different not 
only on whether they are familiar to the parser but also on the subtlety and distribu-
tions of their senses. Nevertheless, it is evidence of our system's robustness.   
4.2   Experiment I I 
There will be situations where systems trained with rich linguistic features extracted 
from high quality parses will be run on applications where such rich features will not 
be available. It is most likely that systems in such situations will go back to a position 
similar to where rich features are not available in both the training and test phases. 
However, could things get even worse? A machine learning model often tends to 
favor informative features (e.g., rich linguistic features in our case) and fit the distri-
bution of these features well in its training phase. Therefore, it is expected that the 
model will be penalized more heavily when these informative features are used in its 
training phase but are not accessible in its test phase. In this subsection, we discuss a 
942 J. Chen and M. Palmer 
second experiment to test the robustness of our system in such situations and explore 
possible strategies for alleviating penalties.  
We trained our system with rich features of the SENSEVAL2 training data and 
tested its performance on the SENSEVAL2 test data with three different feature sets: 
a rich set containing topical, collocation, syntactic and semantic features 
(top+col+syn+sem), a poor set containing topical and collocation features (top+col) 
and a medium set containing topical and collocation features plus features for direct 
objects (top+col+obj). The reason we include the medium set is that a parser can 
usually find the direct object of verbs. Furthermore, we trained and tested our system 
on SENSEVAL2 data with linguistically impoverished features (top+col) and used 
this result as a control. As shown in Table 4, the system's accuracy drops to the same 
level as the control (58.0% vs. 58.1%) when it is trained with rich features but tested 
with poor features. When the features associated with the verb's direct object are 
added, the system's performance improves (59.1%).  
The experimental results here suggest that our system has not been penalized very 
much when rich linguistic features are only available in its training phase. Intuitively, 
the topical features6 our system uses alleviate the penalty. As expected, when the 
topical features of the test data were excluded, the performance of our system dropped 
to 54.8%. But this will be a common problem for all systems using topical features, 
not only for systems using rich linguistic features.7 These results suggest a strategy 
for using our system and other similar systems in a more robust way. When a state-of-
art parser is not available for the application data, topical features can be used to alle-
viate the penalty. Rich features that can be obtained more easily and reliably, e.g., 
features associated with the direct object of verbs, can also be used whenever they are 
available. 
Table 4. Performance of our system trained and tested on data sets with different features  
  top+col+syn+sem top+col 
top+col+syn+sem 64.6   
top+col 58.0 58.1 
top+col+obj 59.1   
5   Conclusion 
We have shown that our system using rich linguistic features was more successful, 
compared with the previous best systems, in classifying the fine-grained and coarse-
grained SENSEVAL2 verb senses. The three enhancements to the system's treatment 
                                                          
6
  Our system uses all the contextual nouns, verbs, adjectives and adverbs that are not in a stop 
word list as topical features. 
7
  In fact, the performance of our system trained with (top+col) features and tested with only 
collocation features also dropped to 55.8%, in contrast to the control accuracy 58.1%. 
Training set
Test set 
 Towards Robust High Performance Word Sense Disambiguation of English Verbs 943 
of rich linguistic features were beneficial. Further improvements may come from 
disambiguating WordNet synsets and improving the accuracy of subject extraction 
and pronoun resolution. Furthermore, our system was robust when it was applied to 
test data that had a relatively poor quality of rich features. Based on the experimental 
results, we proposed a strategy for using systems with rich features in a more robust 
way. Our goal is to continue to improve the performance of our current WSD system, 
with respect to both fine-grained and coarse-grained senses, so that it becomes in-
creasingly beneficial to NLP applications. 
References 
1. Mark Sanderson: Word sense disambiguation and information retrieval. In Proceedings of 
the 17th Int. ACM SIGIR, Dublin, IE (1994).  
2. Christopher Stokoe, Michael P. Oakes, John Tait: Word sense disambiguation and infor-
mation retrieval revisited. In Proceedings of the 26th annual int. ACM SIGIR conference 
on research and development in information retrieval. Toronto, Canada (2003). 
3. Philip Edmonds and Scott Cotton: SENSEVAL-2: Overview. In Proceedings of 
SENSEVAL-2: 2nd Int. Workshop on Evaluating WSD Systems. ACL-SIGLEX, Tou-
louse, France (2001). 
4. David Yarowsky, Silviu Cucerzan, Radu Florian, Charles Schafer and Richard Wicen-
towski: The Johns hopkins SENSEVAL2 system description. In Proceedings of 
SENSEVAL-2: 2nd Int.Workshop on Evaluating WSD Systems. Toulouse France (2001). 
5. Hoa T. Dang and Martha Palmer: Combining contextual features for word sense disam-
biguation. In Proceedings of the SIGLEX/SENSEVAL Workshop on WSD: Recent Suc-
cesses and Future Directions, in conjunction with ACL-02, Philadelphia (2002).  
6. Mart?nez David, Agirre Enek. and M?rquez Liuis: Syntactic Features for High Precision 
Word Sense Disambiguation. In Proceedings of the 19th International COLING. Taipei 
(2002). 
7. Dekang Lin: Using Syntactic Dependency as Local Context to Resolve Word Sense Am-
biguity In Proceedings of ACL-97, Madrid, Spain (1997). 
8. Yoong Keok Lee and Hwee Tou Ng: An empirical evaluation of knowledge sources and 
learning algorithms for word sense disambiguation. In Proceedings of the Conference on 
Empirical Methods in Natural Language Processing (EMNLP) (2002) pages 41?48. 
9. Rada Mihalcea and Ehsanul Faruque: Sense Learner: Minimally Supervised Word Sense 
Disambiguation for All Words in Open Text. In Proceedings of SENSEVAL-3: Third In-
ternational Workshop on the Evaluation of Systems for the Semantic Analysis of Text, 
Barcelona, Spain (2004). 
10. Hoa T. Dang: Investigations into the role of lexical semantics in word sense disambigua-
tion.  PhD Thesis. University of Pennsylvania (2004). 
11. Christiane Fellbaum: WordNet - an Electronic Lexical Database. The MIT Press, Cam-
bridge, Massachusetts, London, UK (1998). 
12. Andrew K. McCallum: MALLET: A Machine Learning for Language Toolkit.  
http://www.cs. umass.edu/~mccallum/mallet (2002). 
13. Adam L. Berger, Stephen A. Della Piertra, and Vincent J. Della Pietra: A maximum en-
tropy approach to natural language processing. Compuational Linguistics, (1996) 22(1): 
39-71. 
14. Stanley. F. Chen and Ronald Rosenfeld: A Gaussian prior for smoothing maximum en-
tropy models. Technical Report CMU-CS-99-108, CMU (1999). 
944 J. Chen and M. Palmer 
15. Daniel M. Bikel, Richard Schwartz and Ralph M. Weischedel: An algorithm that learns 
what's in a name. Machine Learning, (1999) 34(1-3). Special Issue on Natural Language 
Learning. 
16. Shalom Lappin and Herbert Leass: An algorithm for pronominal anaphora resolution. 
Computational Linguistics, (1994) 20(4): 535-561. 
17. Paul Kingsbury, Martha Palmer, and Mitch Marcus: Adding semantic annotation to the 
Penn Tree-Bank. In Proceedings of HLT 2002, San Diego, CA (2002). 
18. Adwait Ratnaparkhi: Maximum entropy models for natural language ambiguity resolution. 
Ph.D. thesis, University of Pennsylvania (1998). 
19. 19 Daniel M. Bikel: Design of a multi-lingual, parallel-processing statistical parsing en-
gine.In Proceedings of HLT 2002. San Diego, CA (2002). 
20. Paul Buitelaar: Reducing lexical semantic complexity with systematic polysemous classes 
and underspecification. In Poceedings of the ANLP Workshop on Syntactic and Semantic 
Complexity in NLP Systems. Seattle, WA (2000). 
21. Martha Palmer, Olga B. Malaya and Hoa T. Dang: Different sense granularities for differ-
ent appli-cations. In Proceedings of HLT/NAACL-04. Boston (2004). 
22. Mitchell Marcus, Grace Kim, Mary A. Marcinkiewicz, Robert MacIntyre, Mark Ferguson, 
Karen Katz and Britta Schasberger: The Penn Treebank: annotating predicate argument 
structure. In Proceedings of the ARPA'94 HLT Workshop (1994). 
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 120?127,
New York, June 2006. c?2006 Association for Computational Linguistics
An Empirical Study of the Behavior of Active Learning for Word Sense 
Disambiguation 
1 Jinying Chen, 1 Andrew Schein, 1 Lyle Ungar, 2 Martha Palmer 
1 Department of Computer and Information Science 
University of Pennsylvania 
Philadelphia, PA, 19104 
{jinying,ais,ungar}@cis.upenn.edu 
2 Linguistic Department  
University of Colorado 
Boulder, CO, 80309 
Martha.Palmer@colorado.edu 
Abstract 
This paper shows that two uncertainty-
based active learning methods, combined 
with a maximum entropy model, work 
well on learning English verb senses. 
Data analysis on the learning process, 
based on both instance and feature levels, 
suggests that a careful treatment of feature 
extraction is important for the active 
learning to be useful for WSD. The 
overfitting phenomena that occurred 
during the active learning process are 
identified as classic overfitting in machine 
learning based on the data analysis. 
1 Introduction 
Corpus-based methods for word sense 
disambiguation (WSD) have gained popularity in 
recent years. As evidenced by the SENSEVAL 
exercises (http://www.senseval.org), machine 
learning models supervised by sense-tagged 
training corpora tend to perform better on the 
lexical sample tasks than unsupervised methods. 
However, WSD tasks typically have very limited 
amounts of training data due to the fact that 
creating large-scale high-quality sense-tagged 
corpora is difficult and time-consuming. Therefore, 
the lack of sufficient labeled training data has 
become a major hurdle to improving the 
performance of supervised WSD.  
A promising method for solving this problem 
could be the use of active learning. Researchers 
use active learning methods to minimize the 
labeling of examples by human annotators. A 
decrease in overall labeling occurs because active 
learners (the machine learning models used in 
active learning) pick more informative examples 
for the target word (a word whose senses need to 
be learned) than those that would be picked 
randomly. Active learning requires human labeling 
of the newly selected training data to ensure high 
quality. 
We focus here on pool-based active learning 
where there is an abundant supply of unlabeled 
data, but where the labeling process is expensive.  
In NLP problems such as text classification (Lewis 
and Gale, 1994; McCallum and Nigam, 1998), 
statistical parsing (Tang et al, 2002), information 
extraction (Thompson et al, 1999), and named 
entity recognition (Shen et al, 2004), pool-based 
active learning has produced promising results.  
This paper presents our experiments in applying 
two active learning methods, a min-margin based 
method and a Shannon-entropy based one, to the 
task of the disambiguation of English verb senses. 
The contribution of our work is not only in 
demonstrating that these methods work well for the 
active learning of coarse-grained verb senses, but 
also analyzing the behavior of the active learning 
process on two levels: the instance level and the 
feature level. The analysis suggests that a careful 
treatment of feature design and feature generation 
is important for a successful application of active 
learning to WSD. We also accounted for the 
overfitting phenomena that occurred in the learning 
process based on our data analysis.  
The rest of the paper is organized as follows. In 
Section 2, we introduce two uncertainty sampling 
methods used in our active learning experiments 
and review related work in using active learning 
for WSD. We then present our active learning 
experiments on coarse-grained English verb senses 
in Section 3 and analyze the active learning 
120
process in Section 4. Section 5 presents 
conclusions of our study.        
2 Active Learning Algorithms 
The methods evaluated in this work fit into a 
common framework described by Algorithm 1 (see 
Table 1). The key difference between alternative 
active learning methods is how they assess the 
value of labeling individual examples, i.e., the 
methods they use for ranking and selecting the 
candidate examples for labeling. The framework is 
wide open to the type of ranking rule employed. 
Usually, the ranking rule incorporates the model 
trained on the currently labeled data.  This is the 
reason for the requirement of a partial training set 
when the algorithm begins. 
                                Algorithm 1 
Require: initial training set, pool of unlabeled examples 
  Repeat 
Select T random examples from pool 
      Rank T examples according to active learning rule 
     Present the top-ranked example to oracle for labeling 
     Augment the training set with the new example 
  Until Training set reaches desirable size 
Table 1. A Generalized Active Learning Loop 
 
In our experiments we look at two variants of 
the uncertainty sampling heuristic: entropy 
sampling and margin sampling. Uncertainty 
sampling is a term invented by Lewis and Gale 
(Lewis and Gale, 1994) to describe a heuristic 
where a probabilistic classifier picks examples for 
which the model?s current predictions are least 
certain. The intuitive justification for this approach 
is that regions where the model is uncertain 
indicate a decision boundary, and clarifying the 
position of decision boundaries is the goal of 
learning classifiers. Schein (2005) demonstrates 
the two methods run quickly and compete 
favorably against alternatives when combined with 
the logistic regression classifier. 
2.1 Entropy Sampling 
A key question is how to measure uncertainty.  
Different methods of measuring uncertainty will 
lead to different variants of uncertainty sampling.  
We will look at two such measures.  As a 
convenient notation we use q (a vector) to 
represent the trained model?s predictions, with cq  
equal to the predicted probability of class c .  One 
method is to pick the example whose prediction 
vector q displays the greatest Shannon entropy: 
??
c
cc qq log    (1) 
Such a rule means ranking candidate examples 
in Algorithm 1 by Equation 1.  
2.2 Margin Sampling 
An alternative method picks the example with the 
smallest margin: the difference between the largest 
two values in the vector q (Abe and Mamitsuka, 
1998). In other words, if c and 'c are the two most 
likely categories for example nx , the margin is 
measured as follows: 
)|'Pr()|Pr( nnn xcxcM ?=   (2) 
In this case Algorithm 1 would rank examples 
by increasing values of margin, with the smallest 
value at the top of the ranking. 
Using either method of uncertainty sampling, 
the computational cost of picking an example from 
T candidates is: O(TD) where D is the number of 
model parameters.   
2.3 Related Work 
To our best knowledge, there have been very few 
attempts to apply active learning to WSD in the 
literature (Fujii and Inui, 1999; Chklovski and 
Mihalcea, 2002; Dang, 2004). Fujii and Inui (1999) 
developed an example sampling method for their 
example-based WSD system in the active learning 
of verb senses in a pool-based setting. Unlike the 
uncertainty sampling methods (such as the two 
methods we used), their method did not select 
examples for which the system had the minimal 
certainty. Rather, it selected the examples such that 
after training using those examples the system 
would be most certain about its predictions on the 
rest of the unlabeled examples in the next iteration. 
This sample selection criterion was enforced by 
calculating a training utility function. The method 
performed well on the active learning of Japanese 
verb senses. However, the efficient computation of 
the training utility function relied on the nature of 
the example-based learning method, which made 
their example sampling method difficult to export 
to other types of machine learning models. 
Open Mind Word Expert (Chklovski and 
Mihalcea, 2002) was a real application of active 
learning for WSD. It collected sense-annotated 
examples from the general public through the Web 
to create the training data for the SENSEVAL-3 
lexical sample tasks. The system used the 
121
disagreement of two classifiers (which employed 
different sets of features) on sense labels to 
evaluate the difficulty of the unlabeled examples 
and ask the web users to tag the difficult examples 
it selected. There was no formal evaluation for this 
active learning system.  
Dang (2004) used an uncertainty sampling 
method to get additional training data for her WSD 
system. At each iteration the system selected a 
small set of examples for which it had the lowest 
confidence and asked the human annotators to tag 
these examples. The experimental results on 5 
English verbs with fine-grained senses (from 
WordNet 1.7) were a little surprising in that active 
learning performed no better than random 
sampling. The proposed explanation was that the 
quality of the manually sense-tagged data was 
limited by an inconsistent or unclear sense 
inventory for the fine-grained senses. 
3 Active Learning Experiments 
3.1 Experimental Setting 
We experimented with the two uncertainty 
sampling methods on 5 English verbs that had 
coarse-grained senses (see Table 2), as described 
below. By using coarse-grained senses, we limit 
the impact of noisy data due to unclear sense 
boundaries and therefore can get a clearer 
observation of the effects of the active learning 
methods themselves.  
verb # of 
sen. 
baseline 
acc. (%) 
Size of data for 
active learning 
Size of 
test data  
Add 3 91.4 400 100 
Do 7 76.9 500 200 
Feel 3 83.6 400 90 
See 7 59.7 500 200 
Work 9 68.3 400 150 
Table 2. The number of senses, the baseline 
accuracy, the number of instances used for active 
learning and for held-out evaluation for each verb 
 
The coarse-grained senses are produced by 
grouping together the original WordNet senses 
using syntactic and semantic criteria (Palmer et al, 
2006). Double-blind tagging is applied to 50 
instances of the target word. If the ITA < 90%, the 
sense entry is revised by adding examples and 
explanations of distinguishing criteria. 
Table 2 summarizes the statistics of the data. 
The baseline accuracy was computed by using the 
?most frequent sense? heuristic to assign sense 
labels to verb instances (examples). The data used 
in active learning (Column 4 in Table 2) include 
two parts: an initial labeled training set and a pool 
of unlabeled training data. We experimented with 
sizes 20, 50 and 100 for the initial training set. The 
pool of unlabeled data had actually been annotated 
in advance, as in most pool-based active learning 
experiments. Each time an example was selected 
from the pool by the active learner, its label was 
returned to the learner. This simulates the process 
of asking human annotators to tag the selected 
unlabeled example at each time. The advantage of 
using such a simulation is that we can experiment 
with different settings (different sizes of the initial 
training set and different sampling methods).  
The data sets used for active learning and for 
held-out evaluation were randomly sampled from a 
large data pool for each round of the active 
learning experiment. We ran ten rounds of the 
experiments for each verb and averaged the 
learning curves for the ten rounds. 
In the experiments, we used random sampling 
(picking up an unlabeled example randomly at 
each time) as a lower bound. Another control 
(ultimate-maxent) was the learner?s performance 
on the test set when it was trained on a set of 
labeled data that were randomly sampled from a 
large data pool and equaled the amount of data 
used in the whole active learning process (e.g., 400 
training data for the verb add).  
The machine learning model we used for active 
learning was a regularized maximum entropy 
(MaxEnt) model (McCallum, 2002). The features 
used for disambiguating the verb senses included 
topical, collocation, syntactic (e.g., the subject, 
object, and preposition phrases taken by a target 
verb), and semantic (e.g., the WordNet synsets and 
hypernyms of the head nouns of a verb?s NP 
arguments) features (Chen and Palmer, 2005). 
3.2 Experimental Results 
Due to space limits, Figure 1 only shows the 
learning curves for 4 verbs do, feel, see, and work 
(size of the initial training set = 20). The curve for 
the verb add is similar to that for feel. These curves 
clearly show that the two uncertainty sampling 
methods, the entropy-based (called entropy-maxent 
in the figure) and the margin-based (called 
min_margin-maxent), work very well for active 
learning of the senses of these verbs. 
 
122
Figure 1 Active learning for four verbs  
Both methods outperformed the random 
sampling method in that they reached the upper-
bound accuracy earlier and had smoother learning 
curves. For the four verbs add, do, feel and see, 
their learning curves reached the upper bound at 
about 200~300 iterations, which means 1/2 or 1/3 
of the annotation effort can be saved for these 
verbs by using active learning, while still achieving 
the same level of performance as supervised WSD 
without using active learning. Given the large-
scale annotation effort currently underway in the 
OntoNotes project (Hovy et al, 2006), this could 
provide considerable savings in annotation effort 
and speed up the process of providing sufficient 
data for a large vocabulary. The OntoNotes project 
has now provided coarse-grained entries for over 
350 verbs, with corresponding double?blind 
annotation and adjudication in progress.  As this 
adjudicated data becomes available, we will be 
able to train our system accordingly. Preliminary 
results for 22 of these coarse-grained verbs (with 
an average grouping polysemy of 4.5) give us an 
average accuracy of 86.3%. This will also provide 
opportunities for more experiments with active 
learning, where there are enough instances.  Active 
learning could also be beneficial in porting these 
supervised taggers to new genres with different 
sense distributions. 
We also experimented with different sizes of 
the initial training set (20, 50 and 100) and found 
no significant differences in the performance at 
different settings. That means, for these 5 verbs, 
only 20 labeled training instances will be enough 
to initiate an efficient active learning process.        
From Figure 1, we can see that the two 
uncertainty sampling methods generally perform 
equally well except that for the verb do, the min-
margin method is slightly better than the entropy 
method at the beginning of active learning. This 
may not be so surprising, considering that the two 
methods are equal for two-class classification tasks 
(see Equations 1 and 2 for their definition) and the 
verbs used in our experiments have coarse-grained 
senses and often have only 2 or 3 major senses.   
An interesting phenomenon observed from 
these learning curves is that for the two verbs add 
and feel, the active learner reached the upper 
bound very soon (at about 100 iterations) and then 
even breached the upper bound. However, when 
the training set was extended, the learner?s 
performance dropped and eventually returned to 
123
the same level of the upper bound. We discuss the 
phenomenon below.  
4 Analysis of the Learning Process 
In addition to verifying the usefulness of active 
learning for WSD, we are also interested in a 
deeper analysis of the learning process. For 
example, why does the active learner?s 
performance drop sometimes during the learning 
process? What are the characteristics of beneficial 
features that help to boost the learner?s accuracy? 
How do we account for the overfitting phenomena 
that occurred during the active learning for the 
verbs add and feel? We analyzed the effect of both 
instances and features throughout the course of 
active learning using min-margin-based sampling.  
4.1 Instance-level Analysis  
Intuitively, if the learner?s performance drops after 
a new example is added to the training set, it is 
likely that something has gone wrong with the new 
example. To find out such bad examples, we 
define a measure credit_inst for instance i as: 
??
=
+
=
?
m
r
ll
n
l
AccAcclisel
m 1
1
1
)(),(1   (3) 
where Accl and Accl+1 are the classification 
accuracies of the active learner at the lth and 
(l+1)th iterations. n is the total number of 
iterations of active learning and m is the number of 
rounds of active learning (m=10 in our case). 
),( lisel is 1 iff instance i is selected by the active 
learner at the lth iteration and is 0 if otherwise. 
An example is a bad example if and only if it 
satisfies the following conditions: 
a)  its credit_inst value is negative 
b) it increases the learner?s performance, if it 
does, less often than it decreases the 
performance in the 10 rounds. 
We ranked the bad examples by their 
credit_inst values and their frequency of 
decreasing the learner?s performance in the 10 
rounds. Table 3 shows the top five bad examples 
for feel and work. There are several reasons why 
the bad examples may hurt the learner?s 
performance. Column 3 of Table 3 proposes 
reasons for many of our bad examples. We 
categorized these reasons into three major types. 
I. The major senses of a target verb depend 
heavily on the semantic categories of its NP 
arguments but WordNet sometimes fails to provide 
the appropriate semantic categories (features) for 
the head nouns of these NP arguments. For 
example, feel in the board apparently felt no 
pressure has Sense 1 (experience). In Sense 1, feel 
typically takes an animate subject. However, 
board, the head word of the verb?s subject in the 
above sentence has no animate meanings defined 
in WordNet. Even worse, the major meaning of 
board, i.e., artifact, is typical for the subject of feel 
in Sense 2 (touch, grope). Similar semantic type 
mismatches hold for the last four bad examples of 
the verb work in Table 3.  
II. The contexts of the target verb are difficult 
for our feature exaction module to analyze. For 
example, the antecedent for the pronoun subject 
they in the first example of work in Table 3 should 
be ringers, an agent subject that is typical for 
Sense 1 (exert oneself in an activity). However, the 
feature exaction module found the wrong 
antecedent changes that is an unlikely fit for the 
intended verb sense. In the fourth example for feel, 
the feature extraction module cannot handle the 
expletive ?it? (a dummy subject) in ?it was felt 
that?, therefore, it cannot identify the typical 
syntactic pattern for Sense 3 (find, conclude), i.e., 
subject+feel+relative clause. 
III. Sometimes, deep semantic and discourse 
analyses are needed to get the correct meaning of 
the target verb. For example, in the third example 
of feel, ??, he or she feels age creeping up?, it is 
difficult to tell whether the verb has Sense 1 
(experience) or Sense 3 (find) without an 
understanding of the meaning of the relative clause 
and without looking at a broader discourse context. 
The syntactic pattern identified by our feature 
extraction module, subject+feel+relative clause, 
favors Sense 3 (find), which leads to an inaccurate 
interpretation for this case. 
Recall that the motivation behind uncertainty 
samplers is to find examples near decision 
boundaries and use them to clarify the position of 
these boundaries. Active learning often does find 
informative examples, either ones from the less 
common senses or ones close to the boundary 
between the different senses. However, active 
learning also identifies example sentences that are 
difficult to analyze. The failure of our feature 
extraction module, the lack of appropriate semantic 
categories for certain NP arguments in WordNet, 
the lack of deep analysis (semantic and discourse 
analysis) of the context of the target verb can all 
124
         Table 3 Data analysis of the top-ranked bad examples found for two verbs 
produce misleading features. Therefore, in order to 
make active learning useful for its applications, 
both identifying difficult examples and getting 
good features for these examples are equally 
important. In other words, a careful treatment of 
feature design and feature generation is necessary 
for a successful application of active learning. 
There is a positive side to identifying such 
?bad? examples; one can have human annotators 
look at the features generated from the sentences 
(as we did above), and use this to improve the data 
or the classifier. Note that this is exactly what we 
did above: the identification of bad sentences was 
automatic, and they could then be reannotated or 
removed from the training set or the feature 
extraction module needs to be refined to generate 
informative features for these sentences. 
Not all sentences have obvious interpretations; 
hence the two question marks in Table 3. An 
example can be bad for many reasons: conflicting 
features (indicative of different senses), misleading 
features (indicative of non-intended senses), or just 
containing random features that are incorrectly 
incorporated into the model. We will return to this 
point in our discussion of the overfitting 
phenomena for active learning in Section 4.3. 
4.2 Feature-level Analysis 
The purpose of our feature-level analysis is to 
identify informative features for verb senses. The 
learning curve of the active learner may provide 
some clues. The basic idea is, if the learner?s 
performance increases after adding a new example, 
it is likely that the good example contains good 
features that contribute to the clarification of sense 
boundaries. However, the feature-level analysis is 
much less straightforward than the instance-level 
analysis since we cannot simply say the features 
that are active (present) in this good example are 
all good. Rather, an example often contains both 
good and bad features, and many other features 
that are somehow neutral or uninformative. The 
interaction or balance between these features 
determines the final outcome. On the other hand, a 
statistics based analysis may help us to find 
features that tend to be good or bad. For this 
analysis, we define a measure credit_feat for 
feature i as: 
feel Proposed reasons for bad examples Senses 
Some days the coaches make you feel as though you 
are part of a large herd of animals . 
? S1: experience 
And , with no other offers on the table , the board 
apparently felt no pressure to act on it.  
subject: board, no ?animate? meaning in 
WordNet  
S1: experience 
Sometimes a burst of aggressiveness will sweep over a 
man -- or his wife -- because he or she feels age 
creeping up.  
syntactic pattern: sbj+feel+relative clause 
headed by that, a typical pattern for Sense 
3 (find) rather than Sense 1 (experience)  
S1: experience 
At this stage it was felt I was perhaps more pertinent as 
chief. executive . 
syntactic pattern: sbj+feel+relative clause, 
typical for Sense 3 (find) but has not been 
detected by the feature exaction module 
S3: find, conclude
I felt better Tuesday evening when I woke up. ? S1: experience 
Work    
When their changes are completed, and after they have 
worked up a sweat, ringers often ?? 
subject: they, the feature exaction module 
found the wrong antecedent (changes 
rather than ringers) for they 
S1: exert oneself 
in an activity 
Others grab books, records , photo albums , sofas and 
chairs , working frantically in the fear that an 
aftershock will jolt the house again . 
subject: others (means people here), no 
definition in WordNet 
S1: exert oneself 
in an activity 
Security Pacific 's factoring business works with 
companies in the apparel, textile and food industries ?
subject: business, no ?animate? meaning 
in WordNet 
S1: exert oneself 
in an activity 
? ; blacks could work there , but they had to leave at 
night . 
subject: blacks, no ?animate? meaning in 
WordNet 
S1: exert oneself 
in an activity 
? has been replaced by alginates (gelatin-like material 
) that work quickly and accurately and with least 
discomfort to a child . 
subject: alginates, unknown by WordNet S2: perform, 
function, behave 
125
??
=
+
=
?
m
r l
ll
n
l act
AccAccliactive
m 1
1
1
1)(),(1         (4) 
where ),( liactive is 1 iff feature i is active in the 
example selected by the active learner at the lth 
iteration and is 0 if otherwise. actl is the total 
number of active features in the example selected 
at the lth iteration. n and m have the same 
definition as in Equation 3.  
A feature is regarded as good if its credit_feat 
value is positive. We ranked the good features by 
their credit_feat values.  By looking at the top-
ranked good features for the verb work (due to 
space limitations, we omit the table data), we 
identify two types of typically good features.  
The first type of good feature occurs frequently 
in the data and has a frequency distribution over 
the senses similar to the data distribution over the 
senses. Such features include those denoting that 
the target verb takes a subject (subj), is not used in 
a passive mode (morph_normal), does not take a 
direct object (intransitive), occurs in present tense 
(word_work, pos_vb, word_works, pos_vbz), and 
semantic features denoting an abstract subject 
(subjsyn_16993 1) or an entity subject (subjsyn_ 
1742), etc. We call such features background 
features. They help the machine learning model 
learn the appropriate sense distribution of the data. 
In other words, a learning model only using such 
features will be equal to the ?most frequent sense? 
heuristic used in WSD.  
Another type of good feature occurs less 
frequently and has a frequency distribution over 
senses that mismatches with the sense distribution 
of the data. Such features include those denoting 
that the target verb takes an inanimate subject 
(subj_it), takes a particle out (prt_out), is followed 
directly by the word out (word+1_out), or occurs at 
the end of the sentence. Such features are 
indicative of less frequent verb senses  that still 
occur fairly frequently in the data. For example, 
taking an inanimate subject (subj_it) is a strong 
clue for Sense 2 (perform, function, behave) of the 
verb work. Occurring at the end of the sentence is 
also indicative of Sense 2 since when work is used 
in Sense 1 (exert oneself in an activity), it tends to 
take adjuncts to modify the activity as in He is 
working hard to bring up his grade. 
                                                          
1 Those features are from the WordNet. The numbers are 
WordNet ids of synsets and hypernyms. 
There are some features that don?t fall into the 
above two categories, such as the topical feature 
tp_know and the collocation feature pos-2_nn. 
There are no obvious reasons why they are good 
for the learning process, although it is possible that 
the combination of two or more such features 
could make a clear sense distinction. However, this 
hypothesis cannot be verified by our current 
statistics-based analysis. It is also worth noting that 
our current feature analysis is post-experimental 
(i.e., based on the results). In the future, we will try 
automatic feature selection methods that can be 
used in the training phase to select useful features 
and/or their combinations.  
We have similar results for the feature analysis 
of the other four verbs. 
4.3 Account for the Overfitting Phenomena 
Recall that in the instance-level analysis in Section 
4.1, we found that some examples hurt the learning 
performance during active learning but for no 
obvious reasons (the two examples marked by ? in 
Table 3). We found that these two examples 
occurred in the overfitting region for feel. By 
looking at the bad examples (using the same 
definition for bad example as in Section 4.1) that 
occurred in the overfitting region for both feel and 
add, we identified two major properties of these 
examples. First, most of them occurred only once 
as bad examples (19 out 23 for add and 40 out of 
63 for feel). Second, many of the examples had no 
obvious reasons for their badness. 
Based on the above observations, we believe 
that the overfitting phenomena that occurred for 
the two verbs during active learning is typical of 
classic overfitting, which is consistent with a 
"death by a thousand mosquito bites" of rare bad 
features, and consistent with there often being (to 
mix a metaphor) no "smoking gun" of a bad 
feature/instance that is added in, especially in the 
region far away from the starting point of active 
learning. 
5 Conclusions 
We have shown that active learning can lead to 
substantial reductions (often by half) in the number 
of observations that need to be labeled to achieve a 
given accuracy in word sense disambiguation, 
compared to labeling randomly selected instances. 
In a follow-up experiment, we also compared a 
larger number of different active learning methods. 
126
The results suggest that for tasks like word sense 
disambiguation where maximum entropy methods 
are used as the base learning models, the minimum 
margin active criterion for active learning gives 
superior results to more comprehensive 
competitors including bagging and two variants of 
query by committee (Schein, 2005). By also taking 
into account the high running efficiency of the 
min-margin method, it is a very promising active 
learning method for WSD. 
We did an analysis on the learning process on 
two levels: instance-level and feature-level. The 
analysis suggests that a careful treatment of feature 
design and feature generation is very important for 
the active learner to take advantage of the difficult 
examples it finds during the learning process. The 
feature-level analysis identifies some 
characteristics of good features. It is worth noting 
that the good features identified are not particularly 
tied to active learning, and could also be obtained 
by a more standard feature selection method rather 
than by looking at how the features provide 
benefits as they are added in.   
For a couple of the verbs examined, we found 
that active learning gives higher prediction 
accuracy midway through the training than one 
gets after training on the entire corpus.  Analysis 
suggests that this is not due to bad examples being 
added to the training set. It appears that the widely 
used maximum entropy model with Gaussian 
priors is overfitting: the model by including too 
many features and thus fitting noise as well as 
signal.  Using different strengths of the Gaussian 
prior does not solve the problem. If a very strong 
prior is used, then poorer accuracy is obtained. We 
believe that using appropriate feature selection 
would cause the phenomenon to vanish. 
Acknowledgements 
This work was supported by National Science 
Foundation Grant NSF-0415923, Word Sense 
Disambiguation, the DTO-AQUAINT NBCHC-
040036 grant under the University of Illinois 
subcontract to University of Pennsylvania 2003-
07911-01 and the GALE program of the Defense 
Advanced Research Projects Agency, Contract No. 
HR0011-06-C-0022. Any opinions, findings, and 
conclusions or recommendations expressed in this 
material are those of the authors and do not 
necessarily reflect the views of the National 
Science Foundation, the DTO, or DARPA.  
References 
Naoki Abe and Hiroshi Mamitsuka. 1998. Query 
learning strategies using boosting and bagging. In 
Proc. of ICML1998, pages 1?10. 
Jinying Chen and Martha Palmer. 2005. Towards 
Robust High Performance Word Sense 
Disambiguation of English Verbs Using Rich 
Linguistic Features, In Proc. of IJCNLP2005, Oct., 
Jeju, Republic of Korea. 
Tim Chklovski and Rada Mihalcea, Building a Sense 
Tagged Corpus with Open Mind Word Expert, in 
Proceedings of the ACL 2002 Workshop on "Word 
Sense Disambiguation: Recent Successes and Future 
Directions", Philadelphia, July 2002. 
Hoa T. Dang. 2004. Investigations into the role of 
lexical semantics in word sense disambiguation.  PhD 
Thesis. University of Pennsylvania. 
Atsushi Fujii, Takenobu Tokunaga, Kentaro Inui, 
Hozumi Tanaka. 1998. Selective sampling for 
example-based word sense disambiguation, 
Computational Linguistics, v.24 n.4, p.573-597, Dec.  
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance 
Ramshaw and Ralph Weischedel. OntoNotes: The 
90% Solution. Accepted by HLT-NAACL06. Short 
paper. 
David D. Lewis and William A. Gale. 1994. A 
sequential algorithm for training text classifiers. In W. 
Bruce Croft and Cornelis J. van Rijsbergen, editors, 
Proceedings of SIGIR-94, Dublin, IE. 
Andrew K. McCallum. 2002. MALLET: A Machine 
Learning for Language Toolkit.  http://www.cs. 
umass.edu/~mccallum/mallet. 
Andew McCallum and Kamal Nigam. 1998. Employing 
EM in pool-based active learning for text 
classification. In Proc. of ICML ?98. 
Martha Palmer, Hoa Trang Dang and Christiane 
Fellbaum. (to appear, 2006). Making fine-grained and 
coarse-grained sense distinctions, both manually and 
automatically. Natural Language Engineering. 
Andrew I. Schein. 2005. Active Learning for Logistic 
Regression. Ph.D. Thesis. Univ. of Pennsylvania. 
Dan Shen, Jie Zhang, Jian Su, Guodong Zhou and Chew 
Lim Tan. 2004 Multi-criteria-based active learning 
for named entity recognition, In Proc. of ACL04, 
Barcelona, Spain. 
Min Tang, Xiaoqiang Luo, and Salim Roukos. 2002. 
Active learning for statistical natural language 
parsing. In Proc. of ACL 2002. 
Cynthia A. Thompson, Mary Elaine Califf, and 
Raymond J. Mooney. 1999. Active learning for 
natural language parsing and information extraction. 
In Proc. of ICML-99. 
127
Chinese Verb Sense Discrimination Using an EM Clustering Model with Rich 
Linguistic Features 
Jinying Chen, Martha Palmer 
Department of Computer and Information Science 
University of Pennsylvania 
Philadelphia, PA, 19104 
{jinying,mpalmer}@linc.cis.upenn.edu 
 
Abstract 
This paper discusses the application of the 
Expectation-Maximization (EM) clustering 
algorithm to the task of Chinese verb sense 
discrimination. The model utilized rich 
linguistic features that capture predicate-
argument structure information of the target 
verbs. A semantic taxonomy for Chinese 
nouns, which was built semi-automatically 
based on two electronic Chinese semantic 
dictionaries, was used to provide semantic 
features for the model. Purity and normalized 
mutual information were used to evaluate the 
clustering performance on 12 Chinese verbs. 
The experimental results show that the EM 
clustering model can learn sense or sense 
group distinctions for most of the verbs 
successfully. We further enhanced the model 
with certain fine-grained semantic categories 
called lexical sets. Our results indicate that 
these lexical sets improve the model?s 
performance for the three most challenging 
verbs chosen from the first set of experiments. 
1 Introduction 
Highly ambiguous words may lead to irrelevant 
document retrieval and inaccurate lexical choice in 
machine translation (Palmer et al, 2000), which 
suggests that word sense disambiguation (WSD) is 
beneficial and sometimes even necessary in such 
NLP tasks. This paper addresses WSD in Chinese 
through developing an Expectation-Maximization 
(EM) clustering model to learn Chinese verb sense 
distinctions. The major goal is to do sense 
discrimination rather than sense labeling, similar to 
(Sch?tze, 1998). The basic idea is to divide 
instances of a word into several clusters that have 
no sense labels. The instances in the same cluster 
are regarded as having the same meaning. Word 
sense discrimination can be applied to document 
retrieval and similar tasks in information access, 
and to facilitating the building of large annotated 
corpora. In addition, since the clustering model can 
be trained on large unannotated corpora and 
evaluated on a relatively small sense-tagged 
corpus, it can be used to find indicative features for 
sense distinctions through exploring huge amount 
of available unannotated text data.   
The EM clustering algorithm (Hofmann and 
Puzicha, 1998) used here is an unsupervised 
machine learning algorithm that has been applied 
in many NLP tasks, such as inducing a 
semantically labeled lexicon and determining 
lexical choice in machine translation (Rooth et al, 
1998), automatic acquisition of verb semantic 
classes (Schulte im Walde, 2000) and automatic 
semantic labeling (Gildea and Jurafsky, 2002). In 
our task, we equipped the EM clustering model 
with rich linguistic features that capture the 
predicate-argument structure information of verbs 
and restricted the feature set for each verb using 
knowledge from dictionaries. We also semi-
automatically built a semantic taxonomy for 
Chinese nouns based on two Chinese electronic 
semantic dictionaries, the Hownet dictionary1 and 
the Rocling dictionary.2 The 7 top-level categories 
of this taxonomy were used as semantic features 
for the model. Since external knowledge is used to 
obtain the semantic features and guide feature 
selection, the model is not completely 
unsupervised from this perspective; however, it 
does not make use of any annotated training data. 
Two external quality measures, purity and 
normalized mutual information (NMI) (Strehl. 
2002), were used to evaluate the model?s 
performance on 12 Chinese verbs. The 
experimental results show that rich linguistic 
features and the semantic taxonomy are both very 
useful in sense discrimination. The model 
generally performs well in learning sense group 
distinctions for difficult, highly polysemous verbs 
and sense distinctions for other verbs. Enhanced by 
certain fine-grained semantic categories called 
lexical sets (Hanks, 1996), the model?s 
                                                     
1 http://www.keenage.com/. 
2 A Chinese electronic dictionary liscenced from The 
Association for Computational Linguistics and Chinese 
Language Processing (ACLCLP), Nankang, Taipei, 
Taiwan. 
performance improved in a preliminary experiment 
for the three most difficult verbs chosen from the 
first set of experiments. 
The paper is organized as follows: we briefly 
introduce the EM clustering model in Section 2 
and describe the features used by the model in 
Section 3. In Section 4, we introduce a semantic 
taxonomy for Chinese nouns, which is built semi-
automatically for our task but can also be used in 
other NLP tasks such as co-reference resolution 
and relation detection in information extraction. 
We report our experimental results in Section 5 
and conclude our discussion in Section 6. 
2 EM Clustering Model 
The basic idea of our EM clustering approach is 
similar to the probabilistic model of co-occurrence 
described in detail in (Hofmann and Puzicha 
1998). In our model, we treat a set of features { }mfff ,...,, 21 , which are extracted from the 
parsed sentences that contain a target verb, as 
observed variables. These variables are assumed to 
be independent given a hidden variable c, the sense 
of the target verb. Therefore the joint probability of 
the observed variables (features) for each verb 
instance, i.e., each parsed sentence containing the 
target verb, is defined in equation (1), 
? ?
=
=
c
m
i
im cfpcpfffp
1
21 )|()(),...,,(           (1)  
The if ?s are discrete-valued features that can 
take multiple values. A typical feature used in our 
model is shown in (2), 
 
=if
???
???
?
             (2) 
 
At the beginning of training (i.e., clustering), the 
model?s parameters )(cp  and )|( cfp i  are 
randomly initialized.3 Then, the probability of c 
conditioned on the observed features is computed 
in the expectation step (E-step), using equation (3),  
? ?
?
=
==
c
m
i
i
m
i
i
m
cfpcp
cfpcp
fffcp
1
1
21
)|()(
)|()(
),...,,|(~   (3) 
                                                     
3 In our experiments, for verbs with more than 3 
senses, syntactic and semantic restrictions derived from 
dictionary entries are used to constrain the random 
initialization. 
In the maximization step (M-step), )(cp  and 
)|( cfp i  are re-computed by maximizing the log-
likelihood of all the observed data which is 
calculated by using ),...,,|(~ 21 mfffcp  estimated 
in the E-step. The E-step and M-step are repeated 
for a fixed number of rounds, which is set to 20 in 
our experiments,4 or till the amount of change of 
)(cp  and )|( cfp i  is under the threshold 0.001.  
When doing classification, for each verb 
instance, the model calculates the same conditional 
probability as in equation (3) and assigns the 
instance to the cluster with the maximal 
),...,,|( 21 mfffcp . 
3 Features Used in the Model 
The EM clustering model uses a set of linguistic 
features to capture the predicate-argument 
structure information of the target verbs. These 
features are usually more indicative of verb sense 
distinctions than simple features such as words 
next to the target verb or their POS tags. For 
example, the Chinese verb ??| chu1? has a sense 
of produce, the distinction between this sense and 
the verb?s other senses, such as happen and go out, 
largely depends on the semantic category of the 
verb?s direct object. Typical examples are shown 
in (1), 
 
   (1 ?? ? ? ??) a.  /their  /county  /produce  /banana 
          ?Their county produces bananas.? 
 
?? ? ? ?      b. /their  /county  /happen  /big    
? ?          /event /ASP 
          ?A big event happened in their county.? 
 
?? ? ? ?      c.  /their  /county  /go out  ?/door     
? ?          /right away  /be  /mountain 
          ?In their county, you can see mountains as soon  
           as you step out of the doors.? 
 
The verb has the sense produce in (1a) and its 
object should be something producible, such as 
??? /banana?. While in (1b), with the sense 
happen, the verb typically takes an event or event-
like ?? object, such as ? /big event?, 
??? /accident? or ???/problem? etc. In (1c), 
?the verb?s object ? /door? is closely related to 
location, consistent with the sense go out. In 
contrast, simple lexical or POS tag features 
sometimes fail to capture such information, which 
can be seen clearly in (2), 
                                                     
4 In our experiments, we set 20 as the maximal 
number of rounds after trying different numbers of 
rounds (20, 40, 60, 80, 100) in a preliminary 
experiment. 
0  iff  the target verb has no sentential 
         complement 
1  iff  the target verb has a nonfinite  
         sentential complement 
2  iff  the target verb has a finite    
             sentential complement 
?? ?   (2) a. /last year  /produce  ??/banana  3000 
??         / kilogram 
         ?3000 kilograms of bananas were produced last  
          year.?  
       
? ?      b. /in order to /produce   ??/Hainan            
?? ? ??          /best  /DE   /banana 
         ?In order to produce the best bananas in  
          Hainan, ??? 
 
The verb?s object ???/banana?, which is next 
to the verb in (2a), is far away from the verb in 
(2b). For (2b), a classifier only looking at the 
adjacent positions of the target verb tends to be 
misled by the NP right after the verb, i.e., 
???/Hainan?, which is a Province in China and a 
typical object of the verb with the sense go out.    
Five types of features are used in our model: 
1. Semantic category of the subject of the target 
verb 
2. Semantic category of the object of the target 
verb 
3. Transitivity of the target verb 
4. Whether the target verb takes a sentential 
complement and which type of sentential 
complement (finite or nonfinite) it takes 
5. Whether the target verb occurs in a verb 
compound  
We obtain the values for the first two types of 
features (1) and (2) from a semantic taxonomy for 
Chinese nouns, which we will introduce in detail in 
the next section. 
In our implementation, the model uses different 
features for different verbs. The criteria for feature 
selection are from the electronic CETA dictionary 
file 5 and a hard copy English-Chinese dictionary, 
The Warmth Modern Chinese-English Dictionary.6 
For example, the verb ??|chu1? never takes 
sentential complements, thus the fourth type of 
feature is not used for it. It could be supposed that 
we can still have a uniform model, i.e., a model 
using the same set of features for all the target 
verbs, and just let the EM clustering algorithm find 
useful features for different verbs automatically. 
The problem here is that unsupervised learning 
models (i.e., models trained on unlabeled data) are 
more likely to be affected by noisy data than 
supervised ones. Since all the features used in our 
model are extracted from automatically parsed 
sentences that inevitably have preprocessing errors 
such as segmentation, POS tagging and parsing 
errors, using verb-specific sets of features can 
alleviate the problem caused by noisy data to some 
extent. For example, if the model already knows 
                                                     
5 Licensed from the Department of Defense 
6 The Warmth Modern Chinese-English Dictionary, 
Wang-Wen Books Ltd, 1997. 
that a verb like ??|chu1? can never take sentential 
complements (i.e., it does not use the fourth type of 
feature for that verb), it will not be misled by 
erroneous parsing information saying that the verb 
takes sentential complements in certain sentences. 
Since the corresponding feature is not included, the 
noisy data is filtered out. In our EM clustering 
model, all the features selected for a target verb are 
treated in the same way, as described in Section 2. 
4 A Semantic Taxonomy Built Semi-
automatically 
Examples in (1) have shown that the semantic 
category of the object of a verb sometimes is 
crucial in distinguishing certain Chinese verb 
senses. And our previous work on information 
extraction in Chinese (Chen et al, 2004) has 
shown that semantic features, which are more 
general than lexical features but still contain rich 
information about words, can be used to improve a 
model?s capability of handling unknown words, 
thus alleviating potential sparse data problems.  
We have two Chinese electronic semantic 
dictionaries: the Hownet dictionary, which assigns 
26,106 nouns to 346 semantic categories, and the 
Rocling dictionary, which assigns 4,474 nouns to 
110 semantic categories.7 A preliminary 
experimental result suggests that these semantic 
categories might be too fine-grained for the EM 
clustering model (see Section 5.2 for greater 
details). An analysis of the sense distinctions of 
several Chinese verbs also suggests that more 
general categories on top of the Hownet and 
Rocling categories could still be informative and 
most importantly, could enable the model to 
generate meaningful clusters more easily. We 
therefore built a three-level semantic taxonomy 
based on the two semantic dictionaries using both 
automatic methods and manual effort.  
The taxonomy was built in three steps. First, a 
simple mapping algorithm was used to map 
semantic categories defined in Hownet and 
Rocling into 27 top-level WordNet categories.8 
The Hownet or Rocling semantic categories have 
English glosses. For each category gloss, the 
algorithm looks through the hypernyms of its first 
sense in WordNet and chooses the first WordNet 
top-level category it finds. 
                                                     
7 Hownet assigns multiple entries (could be different 
semantic categories) to polysemous words. The Rocling 
dictionary we used only assigns one entry (i.e., one 
semantic category) to each noun.  
8 The 27 categories contain 25 unique beginners for 
noun source files in WordNet, as defined in (Fellbaum, 
1998) and two higher level categories Entity and 
Abstraction. 
The mapping obtained from step 1 needs further 
modification for two reasons. First, the glosses of 
Hownet or Rocling semantic categories usually 
have multiple senses in WordNet. Sometimes, the 
first sense in WordNet for a category gloss is not 
its intended meaning in Hownet or Rocling. In this 
case, the simple algorithm cannot get the correct 
mapping. Second, Hownet and Rocling sometimes 
use adjectives or non-words as category glosses, 
such as animate and LandVehicle etc., which have 
no WordNet nominal hypernyms at all. However, 
those adjectives or non-words usually have 
straightforward meanings and can be easily 
reassigned to an appropriate WordNet category. 
Although not accurate, the automatic mapping in 
step 1 provides a basic framework or skeleton for 
the semantic taxonomy we want to build and 
makes subsequent work easier.  
In step 2, hand correction, we found that we 
could make judgments and necessary adjustments 
on about 80% of the mappings by only looking at 
the category glosses used by Hownet or Rocling, 
such as livestock, money, building and so on. For 
the other 20%, we could make quick decisions by 
looking them up in an electronic table we created. 
For each Hownet or Rocling category, our table 
lists all the nouns assigned to it by the two 
dictionaries. We merged two WordNet categories 
into others and subdivided three categories that 
seemed more coarse-grained than others into 2~5 
subcategories. Step 2 took three days and 35 
intermediate-level categories were generated.  
In step 3, we manually clustered the 35 
intermediate-level categories into 7 top-level 
semantic categories. Figure 1 shows part of the 
taxonomy. 
The EM clustering model uses the 7 top-level 
categories to define the first two types of features 
that were introduced in Section 3. For example, the 
value of a feature kf  is 1 if and only if the object 
NP of the target verb belongs to the semantic 
category Event and is otherwise 0. 
5 Clustering Experiments 
Since we need labeled data to evaluate the 
clustering performance but have limited sense- 
tagged corpora, we applied the clustering model to 
12 Chinese verbs in our experiments. The verbs are 
chosen from 28 annotated verbs in Penn Chinese 
Treebank so that they have at least two verb 
meanings in the corpus and for each of them, the 
number of instances for a single verb sense does 
not exceed 90% of the total number of instances.  
In our task, we generally do not include senses 
for other parts of speech of the selected words, 
such as noun, preposition, conjunction and particle 
etc., since the parser we used has a very high 
accuracy in distinguishing different parts of speech 
of these words (>98% for most of them). However, 
we do include senses for conjunctional and/or 
prepositional usage of two words, ??|dao4? and 
??|wei4?, since our parser cannot distinguish the 
verb usage from the conjunctional or prepositional 
usage for the two words very well. 
Five verbs, the first five listed in Table 1, are 
both highly polysemous and difficult for a 
supervised word sense classifier (Dang et al, 
2002). 9 In our experiments, we manually grouped 
the verb senses for the five verbs. The criteria for 
the grouping are similar to Palmer et al?s (to 
appear) work on English verbs, which considers 
both sense coherence and predicate-argument 
structure distinctions. Figure 2 gives an example of  
                                                     
9 In the supervised task, their accuracies are lower 
than 85%, and four of them are even lower than the 
baselines. 
Entity 
Plant     Artifact 
             Document 
   Food    ?? 
          Money 
 drinks, edible, meals, vegetable, ?  
Location 
Location_Part            
   Location     
    Group  ??  
institution, army, corporation, ? 
 Event
Natural Phenomena 
                Happening
Activity    ?? 
              Process 
chase, cut, pass, split, cheat, ? 
  process, BecomeLess, StateChange, disappear, ?. 
Top level 
 
Intermediate level
 
Hownet/Rocling 
categories 
Figure 1.   Part of the 3-level Semantic Taxonomy for Chinese Nouns (other top-level nodes 
are Time, Human, Animal and State) 
the definition of sense groups. The manually 
defined sense groups are used to evaluate the 
model?s performance on the five verbs. 
The model was trained on an unannotated 
corpus, People?s Daily News (PDN), and tested on 
the manually sense-tagged Chinese Treebank (with 
some additional sense-tagged PDN data).10 We 
parsed the training and test data using a Maximum 
Entropy parser and extracted the features from the 
parsed data automatically. The number of clusters 
used by the model is set to the number of the 
defined senses or sense groups of each target verb. 
For each verb, we ran the EM clustering algorithm 
ten times. Table 2 shows the average performance 
and the standard deviation for each verb. Table 1 
summarizes the data used in the experiments, 
where we also give the normalized sense 
perplexity11 of each verb in the test data. 
5.1 Evaluation Methods 
We use two external quality measures, purity 
and normalized mutual information (NMI) (Strehl. 
2002) to evaluate the clustering performance. 
Assuming a verb has l senses, the clustering model 
assigns n instances of the verb into k clusters, in is 
the size of the ith cluster, jn  is the number of 
instances hand-tagged with the jth sense, and jin is 
the number of instances with the jth sense in the ith 
cluster, purity is defined in equation (4): 
 
?
=
=
k
i
j
ij
n
n
purity
1
max1             (4) 
                                                     
10 The sense-tagged PDN data we used here are the 
same as in (Dang et al, 2002). 
11 It is calculated as the entropy of the sense 
distribution of a verb in the test data divided by the 
largest possible entropy, i.e., log2 (the number of senses 
of the verb in the test data).  
It can be interpreted as classification accuracy 
when for each cluster we treat the majority of 
instances that have the same sense as correctly 
classified. The baseline purity is calculated by 
treating all instances for a target verb in a single 
cluster. The purity measure is very intuitive. In our 
case, since the number of clusters is preset to the 
number of senses, purity for verbs with two senses 
is equal to classification accuracy defined in 
supervised WSD. However, for verbs with more 
than 2 senses, purity is less informative in that a 
clustering model could achieve high purity by 
making the instances of 2 or 3 dominant senses the 
majority instances of all the clusters.  
Mutual information (MI) is more theoretically 
well-founded than purity. Treating the verb sense 
and the cluster as random variables S and C, the 
MI between them is defined in equation (5): 
??
?
= =
=
=
l
j
k
i
j
i
j
i
j
i
cs
nn
nn
n
n
cpsp
cspcspCSMI
1 1
,
log
)()(
),(log),(),(
           (5) 
MI(S,C) characterizes the reduction in  
uncertainty of one random variable S (or C) due to 
knowing the other variable C (or S).  A single 
cluster with all instances for a target verb has a 
zero MI. Random clustering also has a zero MI in 
the limit. In our experiments, we used [0,1]-
normalized mutual information (NMI) (Strehl. 
2002). A shortcoming of this measure, however, is 
that the best possible clustering (upper bound) 
evaluates to less than 1, unless classes are 
balanced. Unfortunately, unbalanced sense 
distribution is the usual case in WSD tasks, which 
makes NMI itself hard to interpret. Therefore, in 
addition to NMI, we also give its upper bound 
(upper-NMI) and the ratio of NMI and its upper 
bound (NMI-ratio) for each verb, as shown in 
columns 6 to 8 in Table 2. 
Senses for ??|dao4?                Sense groups for ??|dao4? 
 
1. to go to, leave for 
2. to come 
3. to arrive 
4. to reach a particular stage, condition, or level 
5. marker for completion of activities (after a verb) 
6. marker for direction of activities (after a verb) 
7. to reach a time point 
8. up to, until (prepositional usage) 
9. up to, until, (from ?) to ? (conjunctional usage) 
1, 2
4,7,8,9
 5 
 3 
6 
Figure 2.  Sense groups for the Chinese verb ??|dao4? 
Verb| Pinyin Sample senses of 
the verb 
# Senses in 
test data 
# Sense 
groups in 
test data 
Sense 
perplexity  
# 
Clusters 
# Training 
instances  
# Test 
instances 
?    |chu1 go out /produce 16 7 0.68 8 399 157 
?    |dao4 come /reach 9 5 0.72 6 1838 186 
?    |jian4 see /show 8 5 0.68 6 117 82 
?    |xiang3 think/suppose 6 4 0.64 6 94 228 
?    |yao4 Should/intend to 8 4 0.65 7 2781 185 
??|biao3shi4 Indicate /express 2  0.93 2 666 97 
??|fa1xian4 discover /realize 2  0.76 2 319 27 
??|fa1zhan3 develop /grow 3  0.69 3 458 130 
??|hui1fu4 resume /restore 4  0.83 4 107 125 
?    |shuo1 say /express by 
written words 
7  0.40 7 2692 307 
??|tou2ru4 to input /plunge into 2  1.00 2 136 23 
?    |wei2_4 to be /in order to 6  0.82 6 547 463 
 
Verb Sense 
perplexity  
Baseline 
Purity (%) 
Purity 
(%) 
Std. Dev. of 
purity (%) 
NMI Upper- 
NMI 
NMI- 
ratio (%) 
Std. Dev. of 
NMI ratio (%) 
? 0.68 52.87 63.31 1.59 0.2954 0.6831 43.24 1.76 
? 0.72 40.32 90.48 1.08 0.4802 0.7200 75.65 0.00 
? 0.68 58.54 72.20 1.61 0.1526 0.6806 22.41 0.66 
? 0.64 68.42 79.39 3.74 0.2366 0.6354 37.24 8.22 
? 0.65 69.19 69.62 0.34 0.0108 0.6550 1.65 0.78 
?? 0.93 64.95 98.04 1.49 0.8670 0.9345 92.77 0.00 
?? 0.76 77.78 97.04 3.87 0.7161 0.7642 93.71 13.26 
?? 0.69 53.13 90.77 0.24 0.4482 0.6918 64.79 2.26 
?? 0.83 45.97 65.32 0.00 0.1288 0.8234 15.64 0.00 
?    0.40 80.13 93.00 0.58 0.3013 0.3958 76.13 4.07 
?? 1.00 52.17 95.65 0.00 0.7827 0.9986 78.38 0.00 
? 0.82 32.61 75.12 0.43 0.4213 0.8213 51.30 2.07 
Average 0.73 58.01 82.50 1.12 0.4088 0.7336 54.41 3.31 
5.2 Experimental Results 
Table 2 summarizes the experimental results for 
the 12 Chinese verbs. As we see, the EM clustering 
model performs well on most of them, except the 
verb ??|yao4?.12 The NMI measure NMI-ratio 
turns out to be more stringent than purity. A high 
purity does not necessarily mean a high NMI-ratio. 
Although intuitively, NMI-ratio should be related 
to sense perplexity and purity, it is hard to 
formalize the relationships between them from the 
results. In fact, the NMI-ratio for a particular verb 
is eventually determined by its concrete sense 
distribution in the test data and the model?s 
clustering behavior for that verb. For example, the 
verbs ??|chu1? and ??|jian4? have the same 
sense perplexity and ??|jian4? has a higher purity 
than ??|chu1? (72.20% vs. 63.31%), but the NMI-
ratio for ??|jian4? is much lower than ??|chu1? 
(22.41% vs. 43.24%). An analysis of the 
                                                     
12 For all the verbs except ??|yao4?, the model?s 
purities outperformed the baseline purities significantly 
(p<0.05, and p<0.001 for 8 of them).  
classification results for ??|jian4? shows that the 
clustering model made the instances of the verb?s 
most dominant sense the majority instances of 
three clusters (of total 5 clusters), which is 
penalized heavily by the NMI measure.    
Rich linguistic features turn out to be very 
effective in learning Chinese verb sense 
distinctions. Except for the two verbs, 
???|fa1xian4? and ???|biao3shi4?, the sense 
distinctions of which can usually be made only by 
syntactic alternations,13 features such as semantic 
features or combinations of semantic features and 
syntactic alternations are very beneficial and 
sometimes even necessary for learning sense 
distinctions of other verbs. For example, the verb 
??|jian4? has one sense see, in which the verb 
typically takes a Human subject and a sentential 
complement, while in another sense show, the verb 
typically takes an Entity subject and a State object. 
An inspection of the classification results shows 
                                                     
13 For example, the verb ???|fa1xian4? takes an 
object in one sense discover and a sentential 
complement in the other sense realize. 
Table 1.   A summary of the training and test data used in the experiments  
Table 2.   The performance of the EM clustering model on 12 Chinese verbs measured
by purity and normalized mutual information (NMI) 
that the EM clustering model has indeed learned 
such combinatory patterns from the training data. 
The experimental results also indicate that the 
semantic taxonomy we built is beneficial for the 
task. For example, the verb ???|tou1ru4? has 
two senses, input and plunge into. It typically takes 
an Event object for the second sense but not for the 
first one. A single feature obtained from our 
semantic taxonomy, which tests whether the verb 
takes an Event object, captures this property neatly 
(achieves purity 95.65% and NMI-ratio 78.38% 
when using 2 clusters). Without the taxonomy, the 
top-level category Event is split into many fine-
grained Hownet or Rocling categories, which 
makes it very difficult for the EM clustering model 
to learn sense distinctions for this verb. In fact, in a 
preliminary experiment only using the Hownet and 
Rocling categories, the model had the same purity 
as the baseline (52.17%) and a low NMI-ratio 
(4.22%) when using 2 clusters. The purity 
improved when using more clusters (70.43% with 
4 clusters and 76.09% with 6), but it was still much 
lower than the purity achieved by using the 
semantic taxonomy and the NMI-ratio dropped 
further (1.19% and 1.20% for the two cases).  
By looking at the classification results, we 
identified three major types of errors. First, 
preprocessing errors create noisy data for the 
model. Second, certain sense distinctions depend 
heavily on global contextual information (cross-
sentence information) that is not captured by our 
model. This problem is especially serious for the 
verb ??|yao4?. For example, without global 
contextual information, the verb can have at least 
three meanings want, need or should in the same 
clause, as shown in (3).   
 
(3) ? ? ??/he    /want/need/should    /at once          
?? ?? ?      /finish reading  /this /book. 
     ?He wants to/needs to/should finish reading this   
     book at once.? 
 
Third, a target verb sometimes has specific types 
of NP arguments or co-occurs with specific types 
of verbs in verb compounds in certain senses. Such 
information is crucial for distinguishing these 
senses from others, but is not captured by the 
general semantic taxonomy used here. We did 
further experiments to investigate how much 
improvement the model could gain by capturing 
such information, as discussed in Section 5.3. 
5.3 Experiments with Lexical Sets 
As discussed by Patrick Hanks (1996), certain 
senses of a verb are often distinguished by very 
narrowly defined semantic classes (called lexical 
sets) that are specific to the meaning of that verb 
sense. For example, in our case, the verb 
???|hui1fu4? has a sense recover in which its 
direct object should be something that can be 
recovered naturally. A typical set of object NPs of 
the verb for this particular sense is partially listed 
in (4), 
(4) Lexical set for naturally recoverable things 
?? ?? ??{ /physical strength, /body, /health,   
?? ??/mental energy, /hearing ??, /feeling, 
???/memory, ??} 
Most words in this lexical set belong to the 
Hownet category attribute and the top-level 
category State in our taxonomy. However, even the 
lower-level category attribute still contains many 
other words irrelevant to the lexical set, some of 
which are even typical objects of the verb for two 
other senses, resume and regain, such as 
???/diplomatic relations? in ???/resume 
??/diplomatic relations? and ???/reputation? 
in ???/regain??/reputation?. Therefore, a 
lexical set like (4) is necessary for distinguishing 
the recover sense from other senses of the verb.  
It has been argued that the extensional definition 
of lexical sets can only be done using corpus 
evidence and it cannot be done fully automatically 
(Hanks, 1997). In our experiments, we use a 
bootstrapping approach to obtain five lexical sets 
semi-automatically for three verbs ??|chu1?, 
??|jian4? and ???|hui1fu4? that have both low 
purity and low NMI-ratio in the first set of 
experiments. 14 We first extracted candidates for 
the lexical sets from the training data. For example, 
we extracted all the direct objects of the verb 
???|hui1fu4? and all the verbs that combined 
with the verb ??|chu1? to form verb compounds 
from the automatically parsed training data. From 
the candidates, we manually selected words to 
form five initial seed sets, each of which contains 
no more than ten words. A simple algorithm was 
used to search for all the words that have the same 
detailed Hownet semantic definitions (semantic 
category plus certain supplementary information) 
as the seed words. We did not use Rocling because 
its semantic definitions are so general that a seed 
word tends to extend to a huge set of irrelevant 
words. Highly relevant words were manually 
selected from all the words found by the searching 
algorithm and added to the initial seed sets. The 
enlarged sets were used as lexical sets. 
The enhanced model first uses the lexical sets to 
obtain the semantic category of the NP arguments 
                                                     
14 We did not include ??|yao4?, since its meaning 
rarely depends on local predicate-argument structure 
information. 
of the three verbs. Only when the search fails does 
the model resort to the general semantic taxonomy. 
The model also uses the lexical sets to determine 
the types of the compound verbs that contain the 
target verb ??|chu1? and uses them as new 
features. 
Table 3 shows the model?s performance on the 
three verbs with or without using lexical sets. As 
we see, lexical sets improves the model?s 
performance on all of them, especially on the verb 
??|chu1?. Although the results are still 
preliminary, they nevertheless provide us hints of 
how much a WSD model for Chinese verbs could 
gain from lexical sets. 
 
w/o  lexical sets (%) with lexical sets (%) Verb Purity NMI-ratio Purity NMI-ratio 
? 63.61 43.24 76.50 52.81 
? 72.20 22.41 77.56 34.63 
?? 65.32 15.64 69.03 19.71 
6 Conclusion 
We have shown that an EM clustering model 
that uses rich linguistic features and a general 
semantic taxonomy for Chinese nouns generally 
performs well in learning sense distinctions for 12 
Chinese verbs. In addition, using lexical sets 
improves the model?s performance on three of the 
most challenging verbs.  
Future work is to extend our coverage and to 
apply the semantic taxonomy and the same types 
of features to supervised WSD in Chinese. Since 
the experimental results suggest that a general 
semantic taxonomy and more constrained lexical 
sets are both beneficial for WSD tasks, we will 
develop automatic methods to build large-scale 
semantic taxonomies and lexical sets for Chinese, 
which reduce human effort as much as possible but 
still ensure high quality of the obtained taxonomies 
or lexical sets. 
7 Acknowledgements 
This work has been supported by an ITIC 
supplement to a National Science Foundation 
Grant, NSF-ITR-EIA-0205448. Any opinions, 
findings, and conclusions or recommendations 
expressed in this material are those of the author(s) 
and do not necessarily reflect the views of the 
National Science Foundation. 
References  
Jinying Chen, Nianwen Xue and Martha Palmer. 
2004. Using a Smoothing Maximum Entropy 
Model for Chinese Nominal Entity Tagging. In 
Proceedings of the 1st Int. Joint Conference on 
Natural Language Processing. Hainan Island, 
China. 
Hoa Trang Dang, Ching-yi Chia, Martha Palmer, 
and Fu-Dong Chiou. 2002. Simple Features for 
Chinese Word Sense Disambiguation. In 
Proceedings of COLING-2002 Nineteenth Int. 
Conference on Computational Linguistics, 
Taipei, Aug.24?Sept.1.  
Christiane Fellbaum. 1998. WordNet ? an 
Electronic Lexical Database. The MIT Press, 
Cambridge, Massachusetts, London. 
Daniel Gildea and Daniel Jurafsky. 2002. 
Automatic Labeling of Semantic Roles. 
Computational Linguistics, 28(3): 245-288, 
2002.  
Patrick Hanks. 1996. Contextual dependencies and 
lexical sets. The Int. Journal of Corpus 
Linguistics, 1:1. 
Patrick Hanks. 1997. Lexical sets: relevance and 
probability. in B. Lewandowska-Tomaszczyk 
and M. Thelen (eds.) Translation and Meaning, 
Part 4, School of Translation and Interpreting, 
Maastricht, The Netherlands. 
Thomas Hofmann and Puzicha Jan. 1998. 
Statistical models for co-occurrence data, MIT 
Artificial Intelligence Lab., Technical Report 
AIM-1625. 
Adam Kilgarriff and Martha Palmer. 2000. 
Introduction to the sepcial issue on SENSEVAL. 
Computers and the Humanities, 34(1-2): 15-48. 
Martha Palmer, Hoa Trang Dang, and Christiane 
Fellbaum. To appear. Making fine-grained and 
coarse-grained sense distinctions, both manually 
and automatically. Natural Language 
Engineering.  
Mats Rooth, Stefan Riezler, Detlef Prescher, Glenn 
Carroll, and Franz Beil. 1998. EM-based 
clustering for NLP applications. AIMS Report 
4(3).Institut f?r Maschinelle Sprachverarbeitung.  
Sabine Schulte im Walde. 2000. Clustering verbs 
semantically according to their alternation 
behaviour. In Proceedings of the 18th Int. 
Conference on Computational Linguistics, 747-
753. 
Hinrich Sch?tze. 1998. Automatic Word Sense 
Discrimination. Computational Linguistics, 24 
(1): 97-124. 
Alexander Strehl. 2002. Relationship-based 
Clustering and Cluster Ensembles for High-
dimensional Data Mining. Dissertation. The 
University of Texas at Austin. http://www.lans. 
ece.utexas.edu/~strehl/diss/. 
Table 3.  Clustering performance with and 
without lexical sets for three Chinese verbs
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 921?928,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Aligning Features with Sense Distinction Dimensions  
1 Nianwen Xue,  2 Jinying Chen,  3 Martha Palmer 
1CSLR and 3Department of Linguistics 
University of Colorado 
Boulder, CO, 80309 
{Nianwen.Xue,Martha.Palmer}@colorado.edu 
2 Department of Computer and Information Science 
University of Pennsylvania 
Philadelphia, PA, 19104 
jinying@cis.upenn.edu 
 
Abstract 
In this paper we present word sense 
disambiguation (WSD) experiments on 
ten highly polysemous verbs in Chinese, 
where significant performance 
improvements are achieved using rich 
linguistic features. Our system performs 
significantly better, and in some cases 
substantially better, than the baseline on 
all ten verbs. Our results also 
demonstrate that features extracted from 
the output of an automatic Chinese 
semantic role labeling system in general 
benefited the WSD system, even though 
the amount of improvement was not 
consistent across the verbs. For a few 
verbs, semantic role information actually 
hurt WSD performance. The 
inconsistency of feature performance is a 
general characteristic of the WSD task, as 
has been observed by others. We argue 
that this result can be explained by the 
fact that word senses are partitioned 
along different dimensions for different 
verbs and the features therefore need to 
be tailored to particular verbs in order to 
achieve adequate accuracy on verb sense 
disambiguation. 
1 Introduction 
Word sense disambiguation, the determination of 
the correct sense of a polysemous word from a 
number of possible senses based on the context 
in which it occurs, is a continuing obstacle to 
high performance natural language processing 
applications. There are several well-documented 
factors that make accurate WSD particularly 
challenging. The first has to do with how senses 
are defined. The English data used for the 
SENSEVAL exercises, arguably the most widely 
used data to train and test WSD systems, are 
annotated based on very fine-grained distinctions 
defined in WordNet (Fellbaum, 1998), with 
human inter-annotator agreement at a little over 
seventy percent and the top-ranked systems? 
performances falling between 60%~70%  
(Palmer, et al, 2001; Mihalcea et al, 2004). The 
second source of difficulty for accurate WSD 
comes from how senses are distributed. It is 
often the case that a polysemous word has a 
dominant sense or several dominant senses that 
occur with high frequency and not enough 
instances can be found for its low frequency 
senses in the currently publicly available data. 
There are on-going efforts to address these 
issues. For example, the sense annotation 
component of the OntoNotes project (Hovy, et 
al., 2006) attempts to create a large-scale coarse-
grained sense-annotated corpus with senses 
defined based on explicit linguistic criteria. 
These problems will be alleviated when 
resources like this are available to the general 
NLP community. There have already been 
experiments that show such coarse-grained 
senses lead to substantial improvement in system 
performance (Palmer et al 2006).  
The goal of our experiments is to explore the 
implications of a related and yet separate 
problem, specifically the extent to which the 
linguistic criteria used to define senses are 
related to what features need to be used in 
machine-learning systems. There are already 
published results that show WSD for different 
syntactic categories may need different types of 
features. For example, Yarowsky and Florian 
(2002), in their experiments on SENSEVAL2 
English data, showed that sense distinctions of 
verbs relied more on linguistically motivated 
features than other parts-of-speech. In this paper, 
921
we will go one step further and show that even 
for words of the same syntactic category senses 
are often defined along different dimensions 
based on different criteria. One direct implication 
of this observation for supervised machine-
learning approaches to WSD is that the features 
have to be customized for different word 
categories, or even for different words of the 
same category. This supports previous arguments 
for word-specific feature design and parametric 
modeling for WSD tasks (Chen and Palmer, 
2005; Hoste et al 2002). We report experiments 
on ten highly polysemous Chinese verbs and 
show that features are not uniformly useful for 
all words.  
The rest of the paper is organized as follows. 
In Section 2, we describe our WSD system, 
focusing on the features we used. We also briefly 
compare the features we use for Chinese with 
those used in a similar English WSD system. In 
Section 3, we present our experimental results 
and show that although rich linguistic features 
and features derived from a Chinese Semantic 
Role Labeling improve the WSD accuracy, the 
improvement is not uniform across all verbs. We 
show that this lack of consistency is due to the 
different dimensions along which the features are 
defined. In Section 4, we discuss related work. 
Finally Section 5 concludes this paper and 
describes future directions.  
2 WSD System for Chinese Verbs 
Our WSD system uses a smoothed maximum 
entropy (MaxEnt) model with a Gaussian prior 
(McCallum, 2002) for learning Chinese verb 
senses. The primary reason is that the MaxEnt 
model provides a natural way for combining 
different features without the assumption of 
feature independence. Furthermore, smoothing 
the MaxEnt model with a Gaussian prior is better 
than other smoothing methods at alleviating the 
overfitting problem caused by low frequency 
features (Chen et al, 1999). This model has been 
applied successfully for English WSD (Dang, 
2004; Chen and Palmer, 2005). 
The features used by our Chinese WSD 
system include: 
Collocation Features 
- Previous and next word  (relative to the target 
verb), w-1 and w1 and their parts-of-speech p-1 
and p1 
 
Syntactic Features 
- Whether the target verb takes a direct object 
(i.e., in a transitive use)  
- Whether the verb takes a sentential 
complement 
- Whether the verb, if it consists of a single 
character, occurs at the last position of a 
compound verb 
 
Semantic Features 
- The semantic role information about the verbs 
- The semantic categories for the verb?s NP 
arguments from a general Chinese noun 
Taxonomy 
  
All of these features require some level of 
preprocessing of the Chinese raw text, which 
comes without word boundaries. To extract the 
collocation features the raw text needs to be 
segmented and POS-tagged; to extract the 
syntactic and semantic features, the Chinese text 
needs to be parsed. We use an integrated parser 
that does segmentation, POS-tagging and parsing 
in one step. Since part of the sense-tagged data 
comes from the Chinese Treebank that the parser 
is trained on, we divide the Chinese Treebank 
into nine equal-sized portions and parse each 
portion with a parsing model trained on the other 
eight portions so that the parser has not seen any 
of the data it parses. The data that is not from the 
Chinese Treebank is parsed with a parsing model 
trained on the entire Chinese Treebank. The 
parser produces a segmented, POS-tagged and 
parsed version of the same text to facilitate the 
extraction of the different types of features. The 
extraction of the semantic role labels as features 
requires the use of a semantic role tagger, which 
we describe in greater detail in Section 2.2. 
In addition to using the semantic role labeling 
information, we also extract another type of 
semantic features from the verb?s NP arguments. 
These features are top-level semantic categories 
from a three-level general taxonomy for Chinese 
nouns, which was created semi-automatically 
based on two Chinese semantic dictionaries 
(Chen and Palmer, 2004). 
2.1 A Comparison with  Our English WSD 
System 
Similar to our English WSD system, which 
achieved the best published results on 
SENSEVAL2 English verbs for both fine-
grained and coarse-grained senses (Chen and 
Palmer, 2005), our Chinese WSD system uses 
the same smoothed MaxEnt machine learning 
model and linguistically motivated features for 
Chinese verb sense disambiguation. However, 
the features used in the two systems differ 
922
somewhat  due to the different properties of  the 
two languages . 
For example, our English system uses the 
inflected form and the part-of-speech tag of the 
target verb as feature. For Chinese we no longer 
use such features since Chinese words, unlike 
English ones, do not contain morphology that 
marks tense. 
The collocation features used by our English 
system include bi-grams and tri-grams of the 
words that occur within two positions before or 
after the target verb and their part-of-speech tags. 
In contrast, our Chinese system extracts 
collocation features from a narrower, three-word 
window, with one word immediately before and 
after the target verb. This decision was made 
based on two observations about the Chinese 
language. First, certain single-character Chinese 
verbs, such as the verbs ??|chu?, ??|kai? and 
??|cheng? in our experiments, often form a 
compound with  a verb to its immediate left. That 
verb is often a good indicator of the sense of this 
verb. An example is given in (1):   
 
(1) ??    ?       ??   ? 
       Liaoning  already     show      completion         
      
    ???                    ??          ?? ? 
     multidimensional   development        trend  
 
 ?Liaoning Province has shown the trend of 
multidimensional development.? 
 
Being the last  word of a verb compound is a 
strong indicator for Sense 8 of the verb ??
|chu1? (used after a verb to indicate direction or 
aspect), as in ???|cheng2xian4?|chu1?. 
Second, unlike English common nouns that 
often require determiners such as the, a or an, 
Chinese common nouns can stand alone. 
Therefore, the direct object of a verb often 
occurs right after the verb in Chinese, as shown 
in (2). 
 
?2???         ??         ??      ??           ?  
    mobilize     people      tighten   waistband  collect 
 
    ?   ?  ?? (direct object)? 
    funds   build highway 
 
?Mobilize people to tighten their waistbands (i.e., 
save money) in order to collect funds to build 
highways.? 
  
Based on these observations, we use words 
surrounding  the target verb and their part-of-
speech tags  as collocation features. A further 
investigation on the different sizes of the context 
window (3,5,7,9,11) showed that  increasing the 
window size decreased our system?s accuracy.   
2.2 Features Based on Automatic Semantic 
Role Tagging 
In a recent paper on the WSD of English verbs, 
Dang and Palmer (2005) showed that semantic 
role information significantly improves the WSD 
accuracy of English verbs for both fine-grained 
and coarse-grained senses. However, this result 
assumes the human annotation of the Penn 
English Propbank (Palmer et al 2005). It seems 
worthwhile to investigate whether the semantic 
role information produced by a fully automatic 
Semantic Role tagger can improve the WSD 
accuracy on verbs, and test the hypothesis that 
the senses of a verb  have a high correlation to 
the arguments it takes. To that end, we assigned 
semantic role labels to the arguments  of the 
target verb with a fully automatic semantic role 
tagger (Xue and Palmer, 2005) trained on the 
Chinese Propbank (CPB) (Xue and Palmer, 
2003), a corpus annotated with semantic role 
labels that are similiar in style to the Penn 
English Propbank. In this annotation, core 
arguments such as agent or theme are labeled 
with numbered arguments such as Arg0 and Arg1, 
up to Arg5 while adjunct-like elements are 
assigned functional tags such as TMP (for 
temporal), MNR, prefixed by ArgM. The 
Semantic Role tagger takes as input syntactic 
parses produced by the parser described above as 
input and produces a list of arguments for each 
of  the sense-tagged target verbs and assigns 
argument labels to them. Features are extracted 
from both the core arguments and adjuncts of the 
target verb. In addition to providing the sematnic 
role labels (e.g., Arg0 and Arg1) of the extracted 
core arguments, the Semantic Role tagger also 
provides Hownet (Dong and Dong, 1991) 
semantic categories associated with these 
arguments. (3) shows the arguments for the 
target verb ???  identified by the Semantic Role 
tagger: 
 
(3)  [ArgM-MNR ??    ?    ?     ?      ?] ,    
                  through  three   year   hard   work,   
  
 [arg0 ?        ?]    [rel ?]    ?             
            whole   county   dig         finish   
 
  [Arg1 ?      ??]      ?       ? ? 
        deep    well         three  classifier  
923
 
?The whole county finished digging three deep 
wells through 3 years of hard work.? 
 
Based on the output of the Semantic Role tagger 
and the Chinese noun taxonomy (as described  in 
Section 2.1), the following features are extracted: 
 
SRL+lex               SRL+HowNet     SRL+Taxonomy 
ARG1-??  ARG1-??         ARG1_location  
ARG0-? ARG0-??         ARG0_location 
ARGM|MNR-?? ARGM|MNR-?? ARGM|MNR 
 
In this example, semantic role related features 
include: (1) the head word of the core arguments 
(ARG1-?? and ARG0-?) and the adjunct 
(ARGM|MNR-??); (2) the HowNet semantic 
category for the head word (ARG1-??, ARG0-
??, ARGM|MNR-??); (3) the semantic role 
label of the adjunct (ARGM|MNR); and (4) the 
top level semantic category from the taxonomy 
of Chinese nouns for the head word of the NP 
arguments (ARG1_location and ARG0_location).       
3 Experimental Results 
The data we used for our experiments are 
developed as part of the OntoNotes project 
(Hovy et al, 2006) and they come from a variety 
of sources. Part of the data is from the Chinese 
Treebank (Xue et al 2005), which has a 
combination of Xinhua news and Sinorama 
News Magazine. Since some verbs have an 
insufficient number of instances for any 
meaningful experiments, we also annotated 
portions of the People?s Daily corpus, developed 
by Peking University. We chose not to use the 
Chinese WSD dataset used in Senseval 3 1 
because we are mainly interested in investigating 
how the features used in WSD are related to the 
criteria used to define the senses of Chinese 
verbs. The Chinese Senseval dataset includes 
both nouns and verbs. In addition, the criteria 
used to define their senses are not made explicit 
and therefore are not clear to us.  
Table 1 summarizes the corpus statistics and 
the experimental results for the 10 highly 
polysemous Chinese verbs used in our 
experiments. The results were obtained by using 
5-fold cross validation. The top five verbs are 
verbs that were identified as difficult verbs in 
Dang et als (2002) experiments. The first three 
columns show the verbs (and their pinyin), the 
number of instances and the number of senses for 
                                                 
1 http://www.senseval.org/senseval3 
each verb in the data. The fourth column shows 
the sense entropy for each verb in its test data, as 
calculated in Equation 1. 
)(log)(
1
i
n
i
i sensePsenseP?
=
?            (1) 
Where n is the number of senses of a verb in our 
data; )( isenseP is the probability of the ith sense 
of the verb, which is estimated based on the 
frequency count of the verb?s senses in the data. 
Sense entropy generally reflects the frequency 
distribution of senses in the corpus. A verb with 
an evenly distributed sense distribution tends to 
have a high entropy value. However, a verb can 
also have a high sense entropy simply because it 
is highly polysemous (say, has 20 or more senses) 
even though the sense distribution may be 
skewed, with one or two dominant senses. To 
separate the effects of the number of senses, we 
also use a normalized sense entropy metric (the 
sixth column in Table 1), as calculated in 
Equation 2.  
)1(log1
)(log)(
1
1
n
P
n
sensePsenseP
n
i
i
n
i
i
?
?
=
=
?
?
        (2) 
Here a large sense number n corresponds to a 
high value for the normalization factor 
)1(log1
1 n
P
n
n
i
?
=
? . Therefore, normalized sense 
entropy can indicate sense frequency distribution 
more precisely than sense entropy.  
Table 1 (Columns 7 to 10) also shows the 
experimental results. As we can see, on average, 
our system achieved about 19% improvement 
(absolute gain) in accuracy compared to the most 
frequent sense baseline. Its performance is 
consistently better than the baseline for all 10 
verbs. 
3.1 Corpus Statistics and Disambiguation 
Accuracy 
The data in Table 1 shows that verbs with a high 
normalized sense entropy have the low frequency 
baselines. Furthermore, this relation is stronger 
than that between un-normalized sense entropy 
and the baseline. However, sense entropy is a 
better predictor for system performance than 
normalized sense entropy. The reason is intuitive: 
unlike the baseline, the automatic WSD system, 
trained on the training data, does not only rely on 
sense frequency information to predict senses.  
924
  
# of 
instance 
# of 
sense 
sense 
entropy
norm. 
sense 
entropy baseline all feat all-SRL 
?|chu 271 11 1.12 0.47 74.54 79.70 78.59 
??|huifu 113 3 0.93 0.84 50.44 69.91 72.57 
?|jian 167 7 1.01 0.52 72.46 82.63 82.03 
?|xiang 231 6 1.00 0.56 65.80 76.19 77.49 
?|yao 254 9 1.56 0.71 33.46 46.46 49.21 
?|cheng 161 8 1.38 0.67 43.48 73.29 72.67 
?|da 313 21 2.29 0.75 20.77 45.05 32.59 
?|kai 382 18 2.31 0.80 19.37 50.00 39.27 
??|tongguo 384 4 0.97 0.70 55.73 81.51 79.17 
??|fazhan 1141 7 0.88 0.45 74.76 79.58 77.56 
average   9.4     51.08 70.18 67.13 
total 3417             
The number of senses has a direct impact on how 
many training instances exist for each verb sense. 
As a consequence, it is more difficult for the 
system to make good generalizations from the 
limited training data that is available for highly 
polysemous verbs. Therefore, sense entropy, 
which is based on both sense frequency 
distribution and polysemy is more appropriate 
for predicting system accuracy. A related 
observation is that the system gain (compared 
with the baseline) is bigger for verbs with a high 
normalized sense entropy, such as ???|huifu?, 
??|da?, ??|kai?, and ???|tongguo?, than for 
other verbs; and the system gain is very small for 
verbs with low normalized sense entropy and a 
relatively large number of senses, such as ??
|chu? and ???|fazhan?, since they already have 
high baselines. 
3.2 The Effect of Semantic Role Features 
When Semantic Role information is used in 
features, the system?s performance on average 
improves 3.05%, from 67.13% to 70.18% 
compared with when the features derived from 
the Semantic Role information is not used.  If we 
look at the system?s performance on individual 
verbs, the results show that adding Semantic 
Role information as features improves the 
accuracy of 7 of the 10 verbs. For the remaining 
3 verbs, adding semantic role information 
actually hurts the system?s performance. We 
believe this apparent inconsistency can be 
explained by looking at how senses are defined 
for the different verbs.  The two verbs that 
present the most challenge to the system, are 
??|da? and  ??|yao? While Semantic Role 
features substantially improve the accuracy of 
??|da?, they actually hurt the accuracy of ??
|yao?. For ??|yao?, its three most frequent 
senses account for 86% of its total instances (232 
out of 270) and they are the ?intend to?, ?must, 
should? and ?need? senses: 
 
(4) Three most frequent senses of ??|yao? 
(a)  ??        ??       ?       ??? ?? ? 
    two sides  indicate  intend  further   cooperation                  
 
?The two sides indicated that they intended to step up 
their cooperation.? 
 
(b) ?     ?  ? ? ??  ?            ?? ? 
      road  very  slippery,     everybody  should  careful     
 
?The road is slippery. Everybody should be careful.? 
 
(c) ??  ?                   ?       ?       ? 
 Suzhou Steel Works  every   year    need   depend 
      
???            ??         ?? ? 
 
 the Great Canal      transport    raw material 
 
?Suzhou Steel Works needs to depend on the Great 
Canal to transport raw material.?  
 
Two of the senses, ?must? and ?need?, are 
used as auxiliary verbs. As such, they do not take 
arguments in the same way non-auxiliary verbs 
do. For example, they do not take noun phrases 
as arguments. As a result, the Semantic Role 
tagger, which assigns argument labels to head 
words of noun phrases or clauses, cannot 
produce a meaningful argument for an auxiliary 
verb. For the ?intend to? sense, even if it is not 
Table 1 Corpus Statistics and Experimental Results for the 10 Chinese Verbs 
925
an auxiliary verb, it still does not take a noun 
phrase as an object. Instead, its object is a verb 
phrase or a clause, depending on the analysis.  
The correct head word of its argument should be 
the lower verb, which apparently is not a useful 
discriminative feature either. 
In contrast, the senses of ??|da? are generally 
defined based on their arguments. The three most 
frequent senses of ? ? |da? are ?call by 
telephone?, ?play? and ?fight? and they account 
for 40% of the ??|da? instances. Some examples 
are provided in (5) 
 
(5) Top three senses of ??|da? 
 
(a) ?      ??     ??                          ?     
       you   have     queue in long line     call    
        
?? ??            ?        ?? ?           ? 
       public   phone  DE     experience    ma    
 
?Do you have the experience of queuing in a line 
and waiting to make a call with a public phone?? 
 
(b) ??      ??     ?? ??           
a few     on duty    personnel      sit      
 
? ?            ?  ??   ? 
one      circle     play    poker  
 
?A few of the personnel on duty were sitting in a 
circle and playing poker.? 
 
(c) ?? ?   ??  ??  ?? 
mobilize    whole society  power   fight 
 
       ??   ??  ? ? 
       helping the poor crucial battle 
 
??mobilize the power of the whole society and 
fight the crucial battle of helping the poor.? 
 
The senses of ??|da? are to a large extent 
determined by its PATIENT (or Arg1) argument, 
which is generally realized in the object position. 
The Arg1 argument usually forms highly 
coherent lexical classes. For example, the Arg1 
of the ?call? sense can be ? ? ?
|dianghua/phone, ???|shouji/cellphone?, 
etc. its Arg1 argument can be ? ? ?
|langqiu/basketball?, ? ?? |qiaopai/bridge?, 
???|youxi/game?, etc for the "play" sense,  
Finally , for its sense ?fight?, the Arg1 argument 
can be ???|gongjian/crucial ?|zhan/battle?, 
??? |xiangzhang/street warfare?, ?????
youjizhan/guerilla warfare?, etc.. It?s not 
surprising that recognizing the arguments of 
??|da? is crucial in determining its sense.  
The accuracy for both verbs is still very low, 
but for very different reasons. In the case of 
??|yao4?, the challenge is identifying 
discriminative features that may not be found in 
the narrow local context. These could for 
instance include discourse features. In the case of 
??|da?, one important reason why the accuracy 
is still low is because ??|da? is highly 
polysemous and has over forty senses. Given its 
large number of senses, the majority of its senses 
do not have enough instances to train a 
reasonable model. We believe that more data will 
improve its WSD accuracy.  
There are other dimensions along which verb 
senses are defined in addition to whether or not a 
verb is an auxiliary verb and what type of 
auxiliary verb it is, and what types of arguments 
it takes. One sense of ??|chu? is a verb particle 
that indicates the direction or aspect of the main 
verb that generally immediately precedes it. In 
this case the most important feature for 
identifying this sense is the collocation feature.  
 
Our experimental results seem to lend support 
to a WSD approach where features are tailored to 
each target word, or at least each class of words, 
based on a careful analysis of the dimensions 
along which senses are defined. Automatic 
feature selection (Blum and Langley, 1997) 
could also prove useful in providing this type of 
tailoring. An issue that immediately arises is the 
feasibility of this approach. At least for Chinese, 
the task is not too daunting, as the number of 
highly polysemous verbs is small. Our estimation 
based on a 250K-word chunk of the Chinese 
Treebank and a large electronic dictionary in our 
possession shows only 6% or 384 verb types 
having four or more definitions in the dictionary.  
Even for these verbs, the majority of them are 
not difficult to disambiguate, based on work by 
Dang et al (2002). Only a small number of these 
verbs truly need customized features. 
4 Related work 
There is a large body of literature on WSD and 
here we only discuss a few that are most relevant 
to our work. Dang and Palmer (2005) also use 
predicate-argument information as features in 
their work on English verbs, but their argument 
labels are not produced by an automatic SRL 
system. Rather, their semantic role labels are 
directly extracted from a human annotated 
926
corpus, the English Proposition Bank (Palmer et 
al, 2005), citing the inadequate accuracy of 
automatic semantic role labeling systems. In 
contrast, we used a fully antomated SRL system 
trained on the Chinese Propbank. Nevertheless, 
their results show, as ours do, that the use of 
semantic role labels as features improves the 
WSD accuracy of verbs.  
There are relatively few attempts to use 
linguistically motivated features for Chinese 
word sense disambiguation. Niu et al(2004) 
applied a Naive Bayesian model to Chinese 
WSD and experimented with different window 
sizes for extracting local and topical features and 
different types of local features (e.g., bigram 
templates, local words with position or parts-of-
speech information). One basic finding of their 
experiments is that simply increasing the window 
size for extracting local features or enriching the 
set of local features does not improve 
disambiguation performance. This is consistent 
with our usage of a small size window for 
extracting bigram collocation features. Li et al 
(2005) used sense-tagged true bigram 
collocations 2  as features. These features were 
obtained from a collocation extraction system 
that used lexical co-occurrence statistics to 
extract candidate collocations and then selected 
true collocations by using syntactic dependencies 
(Xu et al, 2003). In their experiments on 
Chinese nouns and verbs extracted from the 
People?s Daily News and the SENSEVAL3 data 
set,  the Naive Bayesian classifier using true 
collocation features generally performed better 
than that using simple bigram collocation 
features (i.e., bigram co-occurence features). It is 
worth noting that the true collocations overlap to 
a large degree with rich syntactic information 
used here such as the subject and direct object of 
a target verb. Therefore, their experiments show 
evidence that rich linguistic information benefits 
WSD on Chinese, consistent with our results. 
Our work is more closely related to the work 
of Dang et al(2002), who conducted 
experiments on 28 verbs and achieved an 
accuracy of 94.2%. However the high accuarcy is 
largely due to  the fact that their verbs are 
randomly chosen from the Chinese Treebank and 
some of them are not even polysemous (having a 
single sense). Extracting features from the gold 
                                                 
2 In their definition, a collocation is a recurrent and 
conventional fixed expression of words that holds 
syntactic and semantic relations. 
 
standard parses also contributed to the high 
accuracy, although not by much. For 5 of their 28 
verbs, their initial experimental results did not 
break the most frequent sense baseline. They 
annotated additional data on those five verbs and 
their system trained on this new data did 
outperfom the baseline. However, they 
concluded that the contribution of linguistic 
motivated features, such as features extracted 
from a syntactic parse, is insignificant, a finding 
they attributed to unique properties of Chinese 
given that the same syntactic features 
significantly improves the WSD accuracy. Our 
experimental results show that this conclusion is 
premature, without a detailed analysis of the 
senses for the individual verbs.  
5 Conclusion and future work 
We presented experiments with ten highly 
polysemous Chinese verbs and showed that a 
previous conclusion that rich linguistic features 
are not useful for the WSD of Chinese verbs is 
premature. We demonstrated that rich linguistic 
features, specifically features based on syntactic 
and semantic role information, are useful for the 
WSD of Chinese verbs. We believe that the 
WSD systems can benefit even more from rich 
linguistic features as the performance of other 
NLP tools such as parsers and Semantic Role 
Taggers improves. Our experimental results also 
lend support to the position that feature design 
for WSD should be linked tightly to the study of 
the criteria that sense distinctions are based on. 
This position calls for the customization of 
features for individual verbs based on 
understanding of the dimensions along which 
sense distinctions are made and a closer marriage 
between machine learning and linguistics. We 
believe this represents a rich area of exploration 
and we intend to experiment with more verbs 
with further customization of features, including 
experimenting with automatic feature selection. 
Acknowledgement 
This work was supported by National Science 
Foundation Grant NSF-0415923, Word Sense 
Disambiguation, the DTO-AQUAINT NBCHC-
040036 grant under the University of Illinois 
subcontract to University of Pennsylvania 2003-
07911-01 and the GALE program of the Defense 
Advanced Research Projects Agency, Contract 
No. HR0011-06-C-0022. Any opinions, findings, 
and conclusions or recommendations expressed 
in this material are those of the authors and do 
927
not necessarily reflect the views of the National 
Science Foundation, the DTO, or DARPA. 
References 
Avrim L. Blum and Pat Langley. 1997. Selection of 
relevant features and examples in machine learning. 
Artificial Intelligence, 97:245-271, 1997. 
Jinying Chen and Martha Palmer. 2004. Chinese Verb 
Sense Discrimination Using an EM Clustering 
Model with Rich Linguistic Features, In Proc. of 
the 42nd Annual meeting of the Assoication for 
Computational Linguistics, ACL-04. July 21-24, 
Barcelona, Spain 
Jinying Chen and Martha Palmer. 2005. Towards 
Robust High Performance Word Sense 
Disambiguation of English Verbs Using Rich 
Linguistic Features. In Proc. of the 2nd 
International Joint Conference on Natural 
Language Processing. Jeju Island, Korea, in press.    
Stanley. F. Chen and Ronald Rosenfeld. 1999. A 
Gaussian Prior for Smoothing Maximum Entropy 
Modals. Technical Report CMU-CS-99-108, CMU. 
Hoa T. Dang, Ching-yi Chia, Martha Palmer and Fu-
Dong Chiou. 2002. Simple Features for Chinese 
Word Sense Disambiguation. In Proceedings of 
COLING-2002, the Nineteenth Int. Conference on 
Computational Linguistics, Taipei, Aug.24?Sept.1. 
Hoa T. Dang. 2004. Investigations into the role of 
lexical semantics in word sense disambiguation.  
PhD Thesis. University of Pennsylvania.  
Hoa Dang and Martha Palmer. 2005. The role of 
semantic roles in  disambiguating verb senses. In 
Proceedings of ACL-05, Ann Arbor, Michigan. 
Zhendong Dong and Qiang Dong, HowNet. 1991. 
http://www.keenage.com. 
Christiane Fellbaum, ed. 1998. WordNet: An 
Electronic Lexical Database. Cambridge, MA: 
MIT Press. 
Veronique Hoste, Iris Hendrickx, Walter Daelemans, 
and Antal van den Bosch. 2002. Parameter 
optimization for machine-learning of word sense 
disambiguation. NLE, Special Issue on Word Sense 
Disambiguation Systems, 8(4):311?325.  
Eduard Hovy, Mtchchell Marcus, Martha Palmer, 
Lance Ramshaw and Ralph Weischedel. 2006. 
OntoNotes: the 90% solution. In Proceedings of the 
HLT-NAACL 2006, New York City. 
Wanyin Li, Qin Lu and Wenjie Li. 2005. Integrating 
Collocation Features in Chinese Word Sense 
Disambiguation. In Proceedings of the Fourth 
Sighan Workshop on Chinese Language Processing.  
pp: 87-94. Jeju, Korea.  
Andrew K. McCallum: MALLET: A Machine 
Learning for Language Toolkit.  http://www.cs. 
umass.edu/~mccallum/mallet (2002). 
Rada Mihalcea, Timothy Chklovski and Adam 
Kilgarriff. 2004. The Senseval-3 English lexical 
sample task. In Proceedings of Senseval-3: The 
Third International Workshop on the Evaluation of 
Systems for the Semantic Analysis of Text. 
Barcelona, Spain. July. 
Zheng-Yu Niu, Dong-Hong Ji and Chew Lim Tan, 
Optimizing Feature Set for Chinese Word Sense 
Disambiguation. 2004. In Proceedings of the 3rd 
International Workshop on the Evaluation of 
Systems for the Semantic Analysis of Text 
(SENSEVAL-3). Barcelona, Spain. 
Martha Palmer, Christiane Fellbaum, Scott Cotton, 
Lauren Delfs, and Hoa Trang Dang. 2001. English 
tasks: All-words and verb lexical sample. 
Proceedings of Senseval-2: Second International 
Workshop on Evaluating Word Sense 
Disambiguation Systems, Toulouse, France, 21-24. 
Martha Palmer, Daniel Gildea, and Paul Kingsbury. 
2005. The Proposition Bank: An Annotated Corpus 
of Semantic Roles, Computational Linguistics, 
31(1): 71?106. 
Martha Palmer, Christiane Fellbaum and Hoa Trang 
Dang. (to appear, 2006). Making fine-grained and 
coarse-grained sense distinctions, both manually 
and automatically. Natural Language Engineering. 
Ruifeng Xu , Qin Lu, and Yin Li. 2003. An automatic 
Chinese Collocation Extraction Algorithm Based 
On Lexical Statistics. In Proceedings of the 
NLPKE Workshop. Beijing, China. 
Nianwen Xue, Fei Xia, Fu-Dong Chiou and Martha 
Palmer. 2005. The Penn Chinese Treebank: Phrase 
Structure Annotation of a Large Corpus. Natural 
Language Engineering, 11(2):207-238. 
Nianwen Xue and Martha Palmer. 2003. Annotating 
Propositions in the Penn Chinese Treebank, In 
Proceedings of the 2nd SIGHAN Workshop on 
Chinese Language Processing, in conjunction with 
ACL'03. Sapporo, Japan. 
Nianwen Xue and Martha Palmer. 2005. Automatic 
Semantic Role Labeling for Chinese Verbs. In 
Proceedings of the 19th International Joint 
Conference on Artificial Intelligence. Edinburgh, 
Scotland. 
David Yarowsky and Radu Florian. 2002. Evaluating 
sense disambiguation across diverse parameter 
spaces. Journal of Natural Language Engineering,  
8(4): 293?310. 
 
928
Proceedings of the Workshop on Frontiers in Corpus Annotation II: Pie in the Sky, pages 61?67,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
A Parallel Proposition Bank II for Chinese and English?
Martha Palmer, Nianwen Xue, Olga Babko-Malaya, Jinying Chen, Benjamin Snyder
Department of Computer and Information Science
University of Pennsylvania
{mpalmer/xueniwen/malayao/Jinying/bsnyder3}@linc.cis.upenn.edu
Abstract
The Proposition Bank (PropBank) project
is aimed at creating a corpus of text an-
notated with information about seman-
tic propositions. The second phase of
the project, PropBank II adds additional
levels of semantic annotation which in-
clude eventuality variables, co-reference,
coarse-grained sense tags, and discourse
connectives. This paper presents the re-
sults of the parallel PropBank II project,
which adds these richer layers of semantic
annotation to the first 100K of the Chinese
Treebank and its English translation. Our
preliminary analysis supports the hypoth-
esis that this additional annotation recon-
ciles many of the surface differences be-
tween the two languages.
1 Introduction
There is a pressing need for a consensus on a task-
oriented level of semantic representation that can en-
able the development of powerful new semantic ana-
lyzers in the same way that the Penn Treebank (Mar-
cus et al, 1993) enabled the development of sta-
tistical syntactic parsers (Collins, 1999; Charniak,
2001). We believe that shallow semantics expressed
as a dependency structure, i.e., predicate-argument
structure, for verbs, participial modifiers, and nom-
inalizations provides a feasible level of annotation
that would be of great benefit. This annotation, cou-
pled with word senses, minimal co-reference links,
?This work is funded by the NSF via Grant EIA02-05448 .
event identifiers, and discourse and temporal rela-
tions, could provide the foundation for a major ad-
vance in our ability to automatically extract salient
relationships from text. This will in turn facilitate
breakthroughs in message understanding, machine
translation, fact retrieval, and information retrieval.
The Proposition Bank project is a major step towards
providing this type of annotation. It takes a prac-
tical approach to semantic representation, adding a
layer of predicate argument information, or seman-
tic roles, to the syntactic structures of the Penn Tree-
bank (Palmer et al, 2005). The Frame Files that
provide guidance to the annotators constitute a rich
English lexicon with explicit ties between syntac-
tic realizations and coarse-grained senses, Frame-
sets. PropBank Framesets are distinguished primar-
ily by syntactic criteria such as differences in sub-
categorization frames, and can be seen as the top-
level of an hierarchy of sense distinctions. Group-
ings of fine-grained WordNet senses, such as those
developed for Senseval2 (Palmer et al, to appear)
provide an intermediate level, where groups are dis-
tinguished by either syntactic or semantic criteria.
WordNet senses constitute the bottom level. The
PropBank Frameset distinctions, which can be made
consistently by humans and systems (over 90% ac-
curacy for both), are surprisingly compatible with
the groupings; 95% of the groups map directly onto
a single PropBank frameset sense (Palmer et al,
2004).
The semantic annotation provided by PropBank
is only a first approximation at capturing the full
richness of semantic representation. Additional an-
notation of nominalizations and other noun pred-
61
icates has already begun at NYU. This paper de-
scribes the results of PropBank II, a project to pro-
vide richer semantic annotation to structures that
have already been propbanked, specifically, eventu-
ality ID.s, coreference, coarse-grained sense tags,
and discourse connectives. Of special interest to the
machine translation community is our finding, pre-
sented in this paper, that PropBank II annotation rec-
onciles many of the surface differences of the two
languages.
2 PropBank I
PropBank (Palmer et al, 2005) is an annotation of
the Wall Street Journal portion of the Penn Treebank
II (Marcus et al, 1994) with ?predicate-argument?
structures, using sense tags for highly polysemous
words and semantic role labels for each argument.
An important goal is to provide consistent seman-
tic role labels across different syntactic realizations
of the same verb, as in the window in [ARG0 John]
broke [ARG1 the window] and [ARG1 The window]
broke. PropBank can provide frequency counts for
(statistical) analysis or generation components in
a machine translation system, but provides only a
shallow semantic analysis in that the annotation is
close to the syntactic structure and each verb is its
own predicate.
In PropBank, semantic roles are defined on a
verb-by-verb basis. An individual verb?s seman-
tic arguments are simply numbered, beginning with
0. Polysemous verbs have several framesets, cor-
responding to a relatively coarse notion of word
senses, with a separate set of numbered roles, a role-
set, defined for each Frameset. For instance, leave
has both a DEPART Frameset ([ARG0 John] left
[ARG1 the room]) and a GIVE Frameset, ([ARG0
I] left [ARG1 my pearls] [ARG2 to my daughter-in-
law] [ARGM-LOC in my will].) While most Frame-
sets have three or four numbered roles, as many
as six can appear, in particular for certain verbs of
motion. Verbs can take any of a set of general,
adjunct-like arguments (ARGMs), such as LOC (lo-
cation), TMP (time), DIS (discourse connectives),
PRP (purpose) or DIR (direction). Negations (NEG)
and modals (MOD) are also marked.
There are several other annotation projects,
FrameNet (Baker et al, 1998), Salsa (Ellsworth et
al., 2004), and the Prague Tectogrammatics (Haji-
cova and Kucerova, 2002), that share similar goals.
Berkeley.s FrameNet project, (Baker et al, 1998;
Fillmore and Atkins, 1998; Johnson et al, 2002)
is committed to producing rich semantic frames on
which the annotation is based, but it is less con-
cerned with annotating complete texts, concentrat-
ing instead on annotating a set of examples for each
predicator (including verbs, nouns and adjectives),
and attempting to describe the network of relations
among the semantic frames. For instance, the buyer
of a buy event and the seller of a sell event would
both be Arg0.s (Agents) in PropBank, while in
FrameNet one is the BUYER and the other is the
SELLER. The Salsa project (Ellsworth et al, 2004)
in Germany is producing a German lexicon based
on the FrameNet semantic frames and annotating a
large German newswire corpus. PropBank style an-
notation is being used for verbs which do not yet
have FrameNet frames defined.
The PropBank annotation philosophy has been
extended to the Penn Chinese Proposition Bank
(Xue and Palmer, 2003). The Chinese PropBank an-
notation is performed on a smaller (250k words) and
yet growing corpus annotated with syntactic struc-
tures (Xue et al, To appear). The same syntac-
tic alternations that form the basis for the English
PropBank annotation also exist in robust quantities
in Chinese, even though it may not be the case that
the same exact verbs (meaning verbs that are close
translations of one another) have the exact same
range of syntactic realization for Chinese and En-
glish. For example, in (1), ?#c/New Year???/
reception? plays the same role in (a) and (b), which
is the event or activity held, even though it occurs in
different syntactic positions. Assigning the same ar-
gument label, Arg1, to both instances, captures this
regularity. It is worth noting that the predicate /?
1/hold? does not have passive morphology in (1a),
despite what its English translation suggests. Like
the English PropBank, the adjunct-like elements re-
ceive more general labels like TMP or LOC, as also
illustrated in (1). The functional tags for Chinese
and English PropBanks are to a large extent similar
and more details can be found in (Xue and Palmer,
2003).
(1) a. [ARG1 #c/New Year ???/reception] [ARGM-
TMP 8 U/today] [ARGM-LOC 3/at M ~
62
/DiaoyutaiIU,/state guest house ?1/hold]
?The New Year reception was held in Diao-yutai
State Guest House today.?
b. [ARG0 /[^/Tang Jiaxuan] [ARGM-TMP 8
U/today] [ARGM-LOC 3/at M~/Diaoyutai I
U,/state guest house] ?1/ hold [arg1 #c/New
Year???/reception]
?Tang Jiaxuan was holding the New Year reception in
Diaoyutai State Guest House today.?
3 A Parallel PropBank II
As discussed above, PropBank II adds richer se-
mantic annotation to the PropBank I predicate ar-
gument structures, notably eventuality variables,
co-references, coarse-grained sense tags (Babko-
Malaya et al, 2004; Babko-Malaya and Palmer,
2005), and discourse connectives (Xue, To appear)
To create our parallel PropBank II, we began with
the first 100K words of the Chinese Treebank which
had already been propbanked, and which we had
had translated into English. The English transla-
tion was first treebanked and then propbanked, and
we are now in the process of adding the PropBank
II annotation to both the English and the Chinese
propbanks. We will discuss our progress on each of
the three individual components of PropBank II in
turn, bringing out translation issues along the way
that have been highlighted by the additional anno-
tation. In general we find that this level of abstrac-
tion facilitates the alignment of the source and tar-
get language descriptions: event ID.s and event
coreferences simplify the mappings between verbal
and nominal events; English coarse-grained sense
tags correspond to unique Chinese lemmas; and dis-
course connectives correspond well.
3.1 Eventuality variables
Positing eventuality1 variables provides a straight-
forward way to represent the semantics of adver-
bial modifiers of events and capture nominal and
pronominal references to events. Given that the ar-
guments and adjuncts for the verbs are already an-
notated in Propbank I, adding eventuality variables
is for the most part straightforward. The example
in (2) illustrates a Propbank I annotation, which is
identified with a unique event id in Propbank II.
1The term ?eventuality? is used here to refer to events and
states.
(2) a. Mr. Bush met him privately in the White House on
Thursday.
b. Propbank I: Rel: met, Arg0: Mr. Bush, Arg1: him,
ArgM-MNR: privately, ArgM-LOC: in the White
House, ArgM-TMP: on Thursday.
c. Propbank II: ?e meeting(e) & Arg0(e,Mr. Bush) &
Arg1(e, him) & MNR (e, privately) & LOC(e, in the
White House) & TMP (e, on Thursday).
Annotation of event variables starts by auto-
matically associating all Propbank I annotations
with potential event ids. Since not all annotations
actually denote eventualities, we manually filter
out selected classes of verbs. We further attempt
to identify all nouns and nominals which describe
eventualities as well as all sentential arguments of
the verbs which refer to events. And, finally, part
of the PropBank II annotation involves tagging of
event coreference for pronouns as well as empty
categories. All these tasks are discussed in more
detail below.
Identifying event modifiers. The actual annota-
tion starts from the presumption that all verbs are
events or states and nouns are not. All the verbs in
the corpus are automatically assigned a unique event
identifier and the manual part of the task becomes (i)
identification of verbs or verb senses that do not de-
note eventualities, (ii) identification of nouns that do
denote events. For example, in (3), begin is an as-
pectual verb that does not introduce an event vari-
able, but rather modifies the verb -take., as is
supported by the fact that it is translated as an ad-
verb ??/initially? in the corresponding Chinese sen-
tence.
(3) ?:/key u?/develop /DE ??/medicine ?/and )
?/biology E?/technology, #/new E?/technology,
#/new ?/material, O ? ?/computer 9/and A
^/application, 1/photo >/electric ?Nz/integration
/etc. ?/industry ?/already ?/initially ?/take 5
/shape.
/Key developments in industries such as medicine,
biotechnology, new materials, computer and its applica-
tions, protoelectric integration, etc. have begun to take
shape.0
Nominalizations as events Although most nouns
do not introduce eventualities, some do and these
nouns are generally nominalizations2 . This is true
2The problem of identifying nouns which denote events is
addressed as part of the sense-tagging tagging. Detailed discus-
sion can be found in (Babko-Malaya and Palmer, 2005).
63
for both English and Chinese, as is illustrated in (4).
Both /u?/develop0and /\/deepening0are
nominalized verbs that denote events. Having a par-
allel propbank annotated with event variables allows
us to see how events are lined up in the two lan-
guages and how their lexical realizations can vary.
The nominalized verbs in Chinese can be translated
into verbs or their nominalizations, as is shown in
the alternative translations of the Chinese original
in (4). What makes this particular example even
more interesting is the fact that the adjective mod-
ifier of the events, /??/continued0, can ac-
tually be realized as an aspectual verb in English.
The semantic representations of the Propbank II an-
notation, however, are preserved: both the aspec-
tual verb /continue0in English and the adjective
/??/continued0in Chinese are modifiers of the
events denoted by /u?/development0and /
\/deepening0.
(4) ? X/with ? I/China ? L/economy /DE ?
?/continuedu?/development ?/and?/to	/outside
m?/open/DE??/continued\/deepen ,
/As China.s economy continues to develop and
its practice of opening to the outside continues to
deepen,0
/With the continued development of China.s economy
and the continued deepening of its practice of opening to
the outside,0
Event Coreference Another aspect of the event
variable annotation involves identifying pronominal
expressions that corefer with events. These pronom-
inal expressions may be overt, as in the Chinese ex-
ample in (5), while others correspond to null pro-
nouns, marked as pro3. in the Treebank annotations,
as in (6):
(5) ?/additionally, ? ?/export ??/commodity (
/structure UY/continue ` z/optimize, c/last
year ? ?/industry ? ? ?/finished product ?
?/export /quota ?/account for I/entire country
? ?/export o /quantity /DE ' ?/proportion
?/reach z??l??:8/85.6 percent, ?/this ?
?/clearly L?/indicate ?I/China ??/industry 
?/product/DE?E/produce Y?/level'/compared
with L /past k/have 
/LE ?/very ?/big J
p/improvement.
/Moreover, the structure of export com-modities
continues to optimize, and last year.s export volume
of manufactured products ac-counts for 85.6 percent of
3The small *pro* and big *PRO* distinction made in the
Chinese Treebank is exploratory in nature. The idea is that it is
easier to erase this distinction if it turns out to be implausible or
infeasible than to add it if it turns out to be important.
the whole countries.export, *pro* clearly indicating
that China.s industrial product manufacturing level has
improved.0
(6) ?
/these ?J/achievement ?/among k/have ?
z n? l/138 ?/item Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 165?169,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
How Much Can We Gain from Supervised Word Alignment? 
 
 
Jinxi Xu and Jinying Chen 
Raytheon BBN Technologies 
10 Moulton Street, Cambridge, MA 02138, USA 
{jxu,jchen}@bbn.com 
  
 
 
Abstract 
Word alignment is a central problem in sta-
tistical machine translation (SMT). In re-
cent years, supervised alignment algo-
rithms, which improve alignment accuracy 
by mimicking human alignment, have at-
tracted a great deal of attention. The objec-
tive of this work is to explore the perform-
ance limit of supervised alignment under 
the current SMT paradigm. Our experi-
ments used a manually aligned Chinese-
English corpus with 280K words recently 
released by the Linguistic Data Consortium 
(LDC). We treated the human alignment as 
the oracle of supervised alignment. The re-
sult is surprising:  the gain of human 
alignment over a state of the art unsuper-
vised method (GIZA++) is less than 1 point 
in BLEU. Furthermore, we showed the 
benefit of improved alignment becomes 
smaller with more training data, implying 
the above limit also holds for large training 
conditions. 
1 Introduction 
Word alignment is a central problem in statistical 
machine translation (SMT). A recent trend in this 
area of research is to exploit supervised learning to 
improve alignment accuracy by mimicking human 
alignment. Studies in this line of work include 
Haghighi et al, 2009; DeNero and Klein, 2010; 
Setiawan et al, 2010, just to name a few. 
The objective of this work is to explore the per-
formance limit of supervised word alignment. 
More specifically, we would like to know what 
magnitude of gain in MT performance we can ex-
pect from supervised alignment over the state of 
the art unsupervised alignment if we have access to 
a large amount of parallel data. Since alignment 
errors have been assumed to be a major hindrance 
to good MT, an answer to such a question might 
help us find new directions in MT research. 
Our method is to use human alignment as the 
oracle of supervised learning and compare its per-
formance against that of GIZA++ (Och and Ney 
2003), a state of the art unsupervised aligner. Our 
study was based on a manually aligned Chinese-
English corpus (Li, 2009) with 280K word tokens. 
Such a study has been previously impossible due to 
the lack of a hand-aligned corpus of sufficient size.   
To our surprise, the gain in MT performance us-
ing human alignment is very small, less than 1 
point in BLEU. Furthermore, our diagnostic ex-
periments indicate that the result is not an artifact 
of small training size since alignment errors are 
less harmful with more data. 
We would like to stress that our result does not 
mean we should discontinue research in improving 
word alignment. Rather it shows that current trans-
lation models, of which the string-to-tree model 
(Shen et al, 2008) used in this work is an example, 
cannot fully utilize super-accurate word alignment. 
In order to significantly improve MT quality we 
need to improve both word alignment and the 
translation model. In fact, we found that some of 
the information in the LDC hand-aligned corpus 
that might be useful for resolving certain transla-
tion ambiguities (e.g. verb tense, pronoun co-
references and modifier-head relations) is even 
harmful to the system used in this work. 
165
2 Experimental Setup 
2.1 Description of MT System 
We used a state of the art hierarchical decoder in 
our experiments. The system exploits a string to 
tree translation model, as described by Shen et al 
(2008). It uses a small set of linguistic and contex-
tual features, such as word translation probabilities, 
rule translation probabilities, language model 
scores, and target side dependency scores, to rank 
translation hypotheses. In addition, it uses a large 
number of discriminatively tuned features, which 
were inspired by Chiang et al (2009) and imple-
mented in a way described in (Devlin 2009). Some 
of the features, e.g. context dependent word trans-
lation probabilities and discriminative word pairs, 
are motivated in part to discount bad translation 
rules caused by noisy word alignment. The system 
used a 3-gram language model (LM) for decoding 
and a 5-gram LM for rescoring. Both LMs were 
trained on about 9 billion words of English text. 
We tuned the system on a set of 4,171 sentences 
and tested on a set of 4,060 sentences. Both sets 
were drawn from the Chinese newswire develop-
ment data for the DARPA GALE program. On av-
erage, each sentence has around 1.7 reference 
translations for both sets. The tuning metric was 
BLEU, but we reported results in BLEU (Papineni 
et al, 2002) and TER (Snover et al, 2006). 
2.2 Hand Aligned Corpus 
The hand aligned corpus we used is LDC2010E63, 
which has around 280K words (English side). This 
corpus was annotated with alignment links be-
tween Chinese characters and English words. Since 
the MT system used in this work is word-based, we 
converted the character-based alignment to word-
based alignment. We aligned Chinese word s to 
English word t if and only if s contains a character 
c that was aligned to t in the LDC annotation. 
 A unique feature of the LDC annotation is that 
it contains information beyond simple word corre-
spondences. Some links, called special links in this 
work, provide contextual information to resolve 
ambiguities in tense, pronoun co-reference, modi-
fier-head relation and so forth. The special links 
are similar to the so-called possible links described 
in other studies (Och and Ney, 2003; Fraser and 
Marcu, 2007), but are not identical. While such 
links are useful for making high level inferences, 
they cannot be effectively exploited by the transla-
tion model used in this work. Worse, they can hurt 
its performance by hampering rule extraction. 
Since the special links were marked with special 
tags to distinguish them from regular links, we can 
selectively remove them and check the impact on 
MT performance. 
Figure 1 shows an example sentence with hu-
man alignment. Solid lines indicate regular word 
correspondences while dashed lines indicate spe-
cial links. Tags inside [] indicate additional infor-
mation about the function of the words connected 
by special links. 
 
 
 
 
 
 
 
Figure 1: An example sentence pair with human 
alignment 
 
2.3 Parallel Corpora and Alignment Schemes 
Our experiments used two parallel training corpora, 
aligned by alternative schemes, from which trans-
lation rules were extracted. 
 
The corpora are: 
? Small: the 280K word hand-aligned cor-
pus, with human alignment removed 
? Large: a 31M word corpus of Chinese-
English text, comprising a number of 
component corpora, one of which is the 
small corpus1 
 
The alignment schemes are: 
? giza-weak: Subdivide the large corpus into 
110 chunks of equal size and run GIZA++ 
separately on each chunk. One of the 
chunks is the small corpus mentioned 
above. This produced low quality unsuper-
vised alignment. 
                                                          
1
 Other data items included are LDC{2002E18,2002L27, 
2005E83,2005T06,2005T10,2005T34,2006E24,2006E34, 
2006E85,2006E92,2006G05,2007E06,2007E101,2007E46, 
2007E87,2008E40,2009E16,2008E56} 
Chinese: gei[OMN]       ni    ti gong          jie shi 
 
English:  provide  you   with[OMN]  an[DET]   explanation  
166
? giza-strong: Run GIZA++ on the large 
corpus in one large chunk. Alignment for 
the small corpus was extracted for experi-
ments involving the small corpus. This 
produced high quality unsupervised align-
ment. 
? gold-original: human alignment, including 
special links 
? gold-clean: human alignment, excluding 
special links 
Needless to say, gold alignment schemes do not 
apply to the large corpus. 
3 Results  
3.1 Results on Small Corpus 
The results are shown in Table 2. The special links 
in the human alignment hurt MT (Table 2, gold-
original vs. gold-clean). In fact, with such links, 
human alignment is worse than unsupervised 
alignment (Table 2, gold-original vs. giza-strong). 
After removing such links, human alignment is 
better than unsupervised alignment, but the gain is 
small, 0.72 point in BLEU (Table 2, gold-clean vs. 
giza-strong). As expected, having access to more 
training data increases the quality of unsupervised 
alignment (Table 1) and as a result the MT per-
formance (Table 2, giza-strong vs. giza-weak).  
 
 
Alignment Precision Recall  F 
  gold-clean 1.00 1.00 1.00 
giza-strong 0.81 0.72 0.76 
giza-weak 0.65 0.58 0.61 
 
Table 1: Precision, recall and F score of different 
alignment schemes. F score is the harmonic mean 
of precision and recall. 
 
 
 
Alignment BLEU TER 
 giza-weak 18.73 70.50 
giza-strong 21.94 66.70 
gold-original 20.81 67.50 
gold-clean 22.66 65.92 
 
Table 2: MT results (lower case) on small corpus 
It is interesting to note that from giza-weak to giza-
strong, alignment accuracy improves by 15% and 
the BLEU score improves by 3.2 points. In com-
parison, from giza-strong to gold-clean, alignment 
accuracy improves by 24% but BLEU score only 
improves by 0.72 point. This anomaly can be 
partly explained by the inherent ambiguity of word 
alignment. For example, Melamed (1998) reported 
inter annotator agreement for human alignments in 
the 80% range. The LDC corpus used in this work 
has a higher agreement, about 90% (Li et al, 
2010). That means much of the disagreement be-
tween giza-strong and gold alignments is probably 
due to arbitrariness in the gold alignment. 
3.2 Results on Large Corpus 
As discussed before, the gain using human align-
ment over GIZA++ is small on the small corpus. 
One may wonder whether the small magnitude of 
the improvement is an artifact of the small size of 
the training corpus. 
To dispel the above concern, we ran diagnostic 
experiments on the large corpus to show that with 
more training data, the benefit from improved 
alignment is less critical. The results are shown in 
Table 3. On the large corpus, the difference be-
tween good and poor unsupervised alignments is 
2.37 points in BLEU (Table 3, giza-strong vs. giza-
weak). In contrast, the difference between the two 
schemes is larger on the small corpus, 3.21 points 
in BLEU (Table 2, giza-strong vs. giza-weak). 
Since the quality of alignment of each scheme does 
not change with corpus size, the results indicate 
that alignment errors are less harmful with more 
training data. We can therefore conclude the small 
magnitude of the gain using human alignment is 
not an artifact of small training. 
Comparing giza-strong of Table 3 with giza-
strong of Table 2, we can see the difference in MT 
performance is about 8 points in BLEU (20.94 vs. 
30.21). This result is reasonable since the small 
corpus is two orders of magnitude smaller than the 
large corpus. 
 
 
Alignment BLEU TER 
 giza-weak 27.84 59.38 
giza-strong 30.21 56.62 
 
Table 3: MT results (lower case) on large corpus 
167
3.3 Discussions  
Some studies on supervised alignment (e.g.  
Haghighi et al, 2009; DeNero and Klein, 2010) 
reported improvements greater than the limit we 
established using an oracle aligner. This seemingly 
inconsistency can be explained by a number of 
factors. First, we used more data (31M) to train 
GIZA++, which improved the quality of unsuper-
vised alignment. Second, some of the features in 
the MT system used in this work, such as context 
dependent word translation probabilities and dis-
criminatively trained penalties for certain word 
pairs, are designed to discount incorrect translation 
rules caused by alignment errors. Third, the large 
language model (trained with 9 billion words) in 
our experiments further alleviated the impact of 
incorrect translation rules. Fourth, the GALE test 
set has fewer reference translations than the NIST 
test sets typically used by other researchers (1.7 
references for GALE, 4 references for NIST).  It is 
well known that BLEU is very sensitive to the 
number of references used for scoring. Had we 
used a test set with more references, the improve-
ment in BLEU score would probably be higher. An 
area for future work is to examine the impact of 
each factor on BLEU score. While these factors 
can affect the numerical value of our result, they 
do not affect our main conclusion: Improving word 
alignment alone will not produce a breakthrough in 
MT quality.  
DeNero and Klein (2010) described a technique 
to exploit possible links, which are similar to spe-
cial links in the LDC hand aligned data, to improve 
rule coverage. They extracted rules with and with-
out possible links and used the union of the ex-
tracted rules in decoding. We applied the technique 
on the LDC hand aligned data but got no gain in 
MT performance. 
Our work assumes that unsupervised aligners 
have access to a large amount of training data. For 
language pairs with limited training, unsupervised 
methods do not work well. In such cases, super-
vised methods can make a bigger difference. 
4 Related Work 
The study of the relation between alignment qual-
ity and MT performance can be traced as far as to 
Och and Ney, 2003. A more recent study in this 
area is Fraser and Marcu, 2007. Unlike our work, 
both studies did not report MT results using oracle 
alignment. 
Recent work in supervised alignment include 
Haghighi et al, 2009; DeNero and Klein, 2010; 
Setiawan et al, 2010, just to name a few. Fossum 
et al (2008) used a heuristic based method to de-
lete problematic alignment links and improve MT.   
Li (2009) described the annotation guideline of 
the hand aligned corpus (LDC2010E63) used in 
this work. This corpus is at least an order of mag-
nitude larger than similar corpora. Without it this 
work would not be possible.  
5 Conclusions  
Our experiments showed that even with human 
alignment, further improvement in MT quality will 
be small with the current SMT paradigm. Our ex-
periments also showed that certain alignment in-
formation suitable for making complex inferences 
can even hamper current SMT models. A future 
direction for SMT is to develop translation models 
that can effectively employ such information. 
Acknowledgments 
This work was supported by DARPA/IPTO Con-
tract No. HR0011-06-C-0022 under the GALE 
program2 (Approved for Public Release, Distribu-
tion Unlimited). The authors are grateful to Mi-
chael Kayser for suggestions to improve the pres-
entation of this paper.  
References  
David Chiang, Kevin Knight, and Wei Wang. 2009. 
11,001 new features for statistical machine transla-
tion. In Proceedings of Human Language Technolo-
gies: The 2009 Annual Conference of the North 
American Chapter of the ACL, pages 218?226. 
John DeNero and Dan Klein. 2010. Discriminative 
Modeling of Extraction Sets for Machine Translation. 
In Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics, pages 1453?
1463.  
                                                          
2
 The views, opinions, and/or findings contained in this arti-
cle/presentation are those of the author/presenter and should 
not be interpreted as representing the official views or policies, 
either expressed or implied, of the Defense Advanced Re-
search Projects Agency or the Department of Defense. 
168
Jacob Devlin. 2009. Lexical features for statistical ma-
chine translation. Master?s thesis, University of 
Maryland. 
Victoria Fossum, Kevin Knight and Steven Abney. 
2008. Using Syntax to Improve Word Alignment 
Precision for Syntax-Based Machine Translation, In 
Proceedings of the third Workshop on Statistical MT, 
ACL, pages 44-52.  
Alexander Fraser and Daniel Marcu. 2007. Measuring 
Word Alignment Quality for Statistical Machine 
Translation. Computational Linguistics. 33(3): 293-
303. 
Aria Haghighi, John Blitzer, John DeNero and Dan 
Klein. 2009. Better word alignments with supervised 
ITG models, In Proceedings of the Joint Conference 
of the 47th Annual Meeting of the ACL and the 4th 
International Joint Conference on Natural Language 
Processing of the AFNLP, pages 923-931.  
Xuansong Li. 2009. Guidelines for Chinese-English 
Word Alignment, Version 4.0, April 16, 2009, 
http://ww.ldc.upenn.edu/Project/GALE. 
Xuansong Li, Niyu Ge, Stephen Grimes, Stephanie M. 
Strassel and  Kazuaki Maeda. 2010. Enriching Word 
Alignment with Linguistic Tags. In Proceedings of 
the Seventh International Conference on Language 
Resources and Evaluation, Valletta, Malta. 
Dan Melamed. 1998. Manual annotation of translational 
equivalence: The blinker project. Technical Report 
98-07, Institute for Research in Cognitive Science, 
Philadelphia. 
Franz Josef Och and Hermann Ney. 2003. A systematic 
comparison of various statistical alignment models. 
Computational Linguistics, 29(1):19-51. 
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic 
evaluation of machine translation. In Proceedings of 
the 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311?318. 
Hendra Setiawan, Chris Dyer, and Philip Resnik. 2010. 
Discriminative Word Alignment with a Function 
Word Reordering Model. In Proceedings of 2010 
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 534?544. 
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A 
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In 
Proceedings of ACL-08: HLT, pages 577?585. 
Matthew Snover, Bonnie Dorr, Richard Schwartz, Linea 
Micciulla, and John Makhoul. 2006. A Study of 
Translation Edit Rate with Targeted Human Annota-
tion. In Proceedings of Association for Machine 
Translation in the Americas, pages 223-231.  
 
 
169
