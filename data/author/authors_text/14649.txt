Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1046?1056,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Multi-level Structured Models for Document-level Sentiment Classification
Ainur Yessenalina
Dept. of Computer Science
Cornell University
Ithaca, NY, USA
ainur@cs.cornell.edu
Yisong Yue
Dept. of Computer Science
Cornell University
Ithaca, NY, USA
yyue@cs.cornell.edu
Claire Cardie
Dept. of Computer Science
Cornell University
Ithaca, NY, USA
cardie@cs.cornell.edu
Abstract
In this paper, we investigate structured mod-
els for document-level sentiment classifica-
tion. When predicting the sentiment of a sub-
jective document (e.g., as positive or nega-
tive), it is well known that not all sentences
are equally discriminative or informative. But
identifying the useful sentences automatically
is itself a difficult learning problem. This pa-
per proposes a joint two-level approach for
document-level sentiment classification that
simultaneously extracts useful (i.e., subjec-
tive) sentences and predicts document-level
sentiment based on the extracted sentences.
Unlike previous joint learning methods for
the task, our approach (1) does not rely on
gold standard sentence-level subjectivity an-
notations (which may be expensive to obtain),
and (2) optimizes directly for document-level
performance. Empirical evaluations on movie
reviews and U.S. Congressional floor debates
show improved performance over previous ap-
proaches.
1 Introduction
Sentiment classification is a well-studied and active
research area (Pang and Lee, 2008). One of the main
challenges for document-level sentiment categoriza-
tion is that not every part of the document is equally
informative for inferring the sentiment of the whole
document. Objective statements interleaved with the
subjective statements can be confusing for learning
methods, and subjective statements with conflicting
sentiment further complicate the document catego-
rization task. For example, authors of movie reviews
often devote large sections to (largely objective) de-
scriptions of the plot (Pang and Lee, 2004). In ad-
dition, an overall positive review might still include
some negative opinions about an actor or the plot.
Early research on document-level sentiment clas-
sification employed conventional machine learning
techniques for text categorization (Pang et al, 2002).
These methods, however, assume that documents are
represented via a flat feature vector (e.g., a bag-of-
words). As a result, their ability to identify and ex-
ploit subjectivity (or other useful) information at the
sentence-level is limited.
And although researchers subsequently proposed
methods for incorporating sentence-level subjectiv-
ity information, existing techniques have some un-
desirable properties. First, they typically require
gold standard sentence-level annotations (McDon-
ald et al (2007), Mao and Lebanon (2006)). But
the cost of acquiring such labels can be prohibitive.
Second, some solutions for incorporating sentence-
level information lack mechanisms for controlling
how errors propagate from the subjective sentence
identification subtask to the main document classifi-
cation task (Pang and Lee, 2004). Finally, solutions
that attempt to handle the error propagation problem
have done so by explicitly optimizing for the best
combination of document- and sentence-level clas-
sification accuracy (McDonald et al, 2007). Opti-
mizing for this compromise, when the real goal is
to maximize only the document-level accuracy, can
potentially hurt document-level performance.
In this paper, we propose a joint two-level model
to address the aforementioned concerns. We formu-
late our training objective to directly optimize for
1046
document-level accuracy. Further, we do not require
gold standard sentence-level labels for training. In-
stead, our training method treats sentence-level la-
bels as hidden variables and jointly learns to predict
the document label and those (subjective) sentences
that best ?explain? it, thus controlling the propaga-
tion of incorrect sentence labels. And by directly
optimizing for document-level accuracy, our model
learns to solve the sentence extraction subtask only
to the extent required for accurately classifying doc-
ument sentiment. A software implementation of our
method is also publicly available.1
For the rest of the paper, we will discuss re-
lated work, motivate and describe our model, present
an empirical evaluation on movie reviews and U.S.
Congressional floor debates datasets and close with
discussion and conclusions.
2 Related Work
Pang and Lee (2004) first showed that sentence-
level extraction can improve document-level per-
formance. They used a cascaded approach by
first filtering out objective sentences and perform-
ing subjectivity extractions using a global min-cut
inference. Afterward, the subjective extracts were
converted into inputs for the document-level senti-
ment classifier. One advantage of their approach
is that it avoids the need for explicit subjectiv-
ity annotations. However, like other cascaded ap-
proaches (e.g., Thomas et al (2006), Mao and
Lebanon (2006)), it can be difficult to control how
errors propagate from the sentence-level subtask to
the main document classification task.
Instead of taking a cascaded approach, one can
directly modify the training of flat document clas-
sifiers using lower level information. For instance,
Zaidan et al (2007) used human annotators to mark
the ?annotator rationales?, which are text spans that
support the document?s sentiment label. These an-
notator rationales are then used to formulate addi-
tional constraints during SVM training to ensure that
the resulting document classifier is less confident in
classifying a document that does not contain the ra-
tionale versus the original document. Yessenalina et
al. (2010) extended this approach to use automati-
cally generated rationales.
1http://projects.yisongyue.com/svmsle/
A natural approach to avoid the pitfalls associ-
ated with cascaded methods is to use joint two-
level models that simultaneously solve the sentence-
level and document-level tasks (e.g., McDonald et
al. (2007), Zaidan and Eisner (2008)). Since these
models are trained jointly, the sentence-level pre-
dictions affect the document-level predictions and
vice-versa. However, such approaches typically
require sentence-level annotations during training,
which can be expensive to acquire. Furthermore,
the training objectives are usually formulated as a
compromise between sentence-level and document-
level performance. If the goal is to predict well at the
document-level, then these approaches are solving a
much harder problem that is not exactly aligned with
maximizing document-level accuracy.
Recently, researchers within both Natural Lan-
guage Processing (e.g., Petrov and Klein (2007),
Chang et al (2010), Clarke et al (2010)) and
other fields (e.g., Felzenszwalb et al (2008), Yu
and Joachims (2009)) have analyzed joint multi-
level models (i.e., models that simultaneously solve
the main prediction task along with important sub-
tasks) that are trained using limited or no explicit
lower level annotations. Similar to our approach, the
lower level labels are treated as hidden or latent vari-
ables during training. Although the training process
is non-trivial (and in particular requires a good ini-
tialization of the hidden variables), it avoids the need
for human annotations for the lower level subtasks.
Some researchers have also recently applied hidden
variable models to sentiment analysis, but they were
focused on classifying either phrase-level (Choi and
Cardie, 2008) or sentence-level polarity (Nakagawa
et al, 2010).
3 Extracting Hidden Explanations
In this paper, we take the view that each document
has a subset of sentences that best explains its sen-
timent. Consider the ?annotator rationales? gener-
ated by human judges for the movie reviews dataset
(Zaidan et al, 2007). Each rationale is a text span
that was identified to support (or explain) its parent
document?s sentiment. Thus, these rationales can be
interpreted as (something close to) a ground truth la-
beling of the explanatory segments. Using a dataset
where each document contains only its rationales,
1047
Algorithm 1 Inference Algorithm for (2)
1: Input: x
2: Output: (y, s)
3: s+ ? argmaxs?S(x) ~wT?(x,+1, s)
4: s? ? argmaxs?S(x) ~wT?(x,?1, s)
5: if ~wT?(x,+1, s+) > ~wT?(x,?1, s?) then
6: Return (+1, s+)
7: else
8: Return (?1, s?)
9: end if
cross validation experiments using an SVM classi-
fier yields 97.44% accuracy ? as opposed to 86.33%
accuracy when using the full text of the original doc-
uments. Clearly, extracting the best supporting seg-
ments can offer a tremendous performance boost.
We are interested in settings where human-
extracted explanations such as annotator rationales
might not be readily available, or are imperfect. As
such, we will formulate the set of extracted sen-
tences as latent or hidden variables in our model.
Viewing the extracted sentences as latent variables
will pose no new challenges during prediction, since
the model is expected to predict all labels at test
time. We will leverage recent advances in training
latent variable SVMs (Yu and Joachims, 2009) to ar-
rive at an effective training procedure.
4 Model
In this section, we present a two-level document
classification model. Although our model makes
predictions at both the document and sentence lev-
els, it will be trained (and evaluated) only with re-
spect to document-level performance. We begin
by presenting the feature structure and inference
method. We will then describe a supervised train-
ing algorithm based on structural SVMs, and finally
discuss some extensions and design decisions.
Let x denote a document, y = ?1 denote the sen-
timent (for us, a binary positive or negative polarity)
of a document, and s denote a subset of explanatory
sentences in x. Let ?(x, y, s) denote a joint fea-
ture map that outputs features describing the qual-
ity of predicting sentiment y using explanation s for
document x. We focus on linear models, so given a
(learned) weight vector ~w, we can write the quality
of predicting y (with explanation s) as
F (x, y, s; ~w) = ~wT?(x, y, s), (1)
and a document-level sentiment classifier as
h(x; ~w) = argmax
y=?1
max
s?S(x)
F (x, y, s; ~w), (2)
where S(x) denotes the collection of feasible expla-
nations (e.g., subsets of sentences) for x.
Let xj denote the j-th sentence of x. We propose
the following instantiation of (1),
~wT?(x, y, s) =
1
N(x)
?
j?s
y ? ~wTpol?pol(x
j) + ~wTsubj?subj(x
j), (3)
where the first term in the summation captures the
quality of predicting polarity y on sentences in s,
the second term captures the quality of predicting s
as the subjective sentences, and N(x) is a normaliz-
ing factor (which will be discussed in more detail in
Section 4.3). We represent the weight vector as
~w =
[
~wpol
~wsubj
]
, (4)
and ?pol(xj) and ?subj(xj) denote the polarity and
subjectivity features of sentence xj , respectively.
Note that ?pol and ?subj are disjoint by construc-
tion, i.e., ?Tpol?subj = 0. We will present extensions
in Section 4.5.
For example, suppose ?pol and ?subj were both
bag-of-words feature vectors. Then we might learn
a high weight for the feature corresponding to the
word ?think? in ?subj since that word is indicative
of the sentence being subjective (but not necessarily
indicating positive or negative polarity).
4.1 Making Predictions
Algorithm 1 describes our inference procedure. Re-
call from (2) that our hypothesis function predicts
the sentiment label that maximizes (3). To do this,
we compare the best set of sentences that explains
a positive polarity prediction with the best set that
explains a negative polarity prediction.
We now specify the structure of S(x). In this pa-
per, we use a cardinality constraint,
S(x) = {s ? {1, . . . , |x|} : |s| ? f(|x|)}, (5)
1048
Algorithm 2 Training Algorithm for OP 1
1: Input: {(x1, y1), . . . , (xN , yN )} //training data
2: Input: C //regularization parameter
3: Input: (s1, . . . , sN ) //initial guess
4: ~w ? SSVMSolve(C, {(xi, yi, si)}Ni=1)
5: while ~w not converged do
6: for i = 1, . . . , N do
7: si ? argmaxs?S(xi) ~w
T?(xi, yi, s)
8: end for
9: ~w ? SSVMSolve(C, {(xi, yi, si)}Ni=1)
10: end while
11: Return ~w
where f(|x|) is a function that depends only on the
number of sentences in x. For example, a simple
function is f(|x|) = |x| ? 0.3, indicating that at most
30% of the sentences in x can be subjective.
Using this definition of S(x), we can then com-
pute the best set of subjective sentences for each
possible y by computing the joint subjectivity and
polarity score of each sentence xj in isolation,
y ? ~wTpol?pol(x
j) + ~wTsubj?subj(x
j),
and selecting the top f(|x|) as s (or fewer, if there
are fewer than f(|x|) that have positive joint score).
4.2 Training
For training, we will use an approach based on latent
variable structural SVMs (Yu and Joachims, 2009).
Optimization Problem 1.
min
~w,??0
1
2
?~w?2 +
C
N
N?
i=1
?i (6)
s.t. ?i :
max
s?Si
~wT?(xi, yi, s) ?
max
s??S(xi)
~wT?(xi,?yi, s
?) + 1? ?i (7)
OP 1 optimizes the standard SVM training objec-
tive for binary classification. Each training example
has a corresponding constraint (7), which is quanti-
fied over the best possible explanation of the train-
ing polarity label. Note that we never observe the
true explanation for the training labels; they are the
hidden or latent variables. The hidden variables are
also ignored in the objective function.
As a result, one can interpret OP 1 to be directly
optimizing a trade-off between model complexity
(as measured using the 2-norm) and document-level
classification error in the training set. This has two
main advantages over related training approaches.
First, it solves the multi-level problem jointly as op-
posed to separately, which avoids introducing diffi-
cult to control propagation errors. Second, it does
not require solving the sentence-level task perfectly,
and also does not require precise sentence-level
training labels. In other words, our goal is to learn to
identify the informative (subjective) sentences that
best explain the training labels to the extent required
for good document classification performance.
OP 1 is non-convex because of the constraints (7).
To solve OP 1, we use the combination of the CCCP
algorithm (Yuille and Rangarajan, 2003) with cut-
ting plane training of structural SVMs (Joachims et
al., 2009), as proposed in Yu and Joachims (2009).
Suppose each constraint (7) is replaced by
~wT?(xi, yi, si) ? max
s??S(xi)
~wT?(xi,?yi, s
?)+1??i,
where si is some fixed explanation (e.g., an initial
guess of the best explanation). Then OP 1 reduces
to a standard structural SVM, which can be solved
efficiently (Joachims et al, 2009). Algorithm 2 de-
scribes our training procedure. Starting with an ini-
tial guess si for each training example, the training
procedure alternates between solving an instance of
the resulting structural SVM (called SSVMSolve in
Algorithm 2) using the currently best known expla-
nations si (Line 9), and making a new guess of the
best explanations (Line 7). Yu and Joachims (2009)
showed that this alternating procedure for training
latent variable structural SVMs is an instance of the
CCCP procedure (Yuille and Rangarajan, 2003), and
so is guaranteed to converge to a local optimum.
For our experiments, we do not train until conver-
gence, but instead use performance on a validation
set to choose the halting iteration. Since OP 1 is non-
convex, a good initialization is necessary. To gener-
ate the initial explanations, one can use an off-the-
shelf sentiment classifier such as OpinionFinder2
(Wilson et al, 2005). For some datasets, there ex-
ist documents with annotated sentences, which we
2http://www.cs.pitt.edu/mpqa/
opinionfinderrelease/
1049
can treat either as the ground truth or another (very
good) initial guess of the explanatory sentences.
4.3 Feature Representation
Like any machine learning approach, we must spec-
ify a useful set of features for the? vectors described
above. We will consider two types of features.
Bag-of-words. Perhaps the simplest approach is
to define ? using a bag-of-words feature representa-
tion, with one feature corresponding to each word in
the active lexicon of the corpus. Using such a feature
representation might allow us to learn which words
have high polarity (e.g., ?great?) and which are in-
dicative of subjective sentences (e.g., ?opinion?).
Sentence properties. We can incorporate many
useful features to describe sentence subjectivity. For
example, subjective sentences might densely popu-
late the end of a document, or exhibit spatial co-
herence (so features describing previous sentences
might be useful for classifying the current sentence).
Such features cannot be compactly incorporated into
flat models that ignore the document structure.
For our experiments, we normalize each ?subj
and ?pol to have unit 2-norm.
Joint Feature Normalization. Another design
decision is the choice of normalization N(x) in (3).
Two straightforward choices are N(x) = f(|x|) and
N(x) =
?
f(|x|), where f(|x|) is the size con-
straint as described in (5). In our experiments we
tried both and found the square root normalization
to work better in practice; therefore all the experi-
mental results are reported using N(x) =
?
f(|x|).
The appendix contains an analysis that sheds light
on when square root normalization can be useful.
4.4 Incorporating Proximity Information
As mentioned in Section 4.3, it is possible (and
likely) for subjective sentences to exhibit spatial co-
herence (e.g., they might tend to group together).
To exploit this structure, we will expand the feature
space of ?subj to include both the words of the cur-
rent and previous sentence as follows,
?subj(x, j) =
[
?subj(xj)
?subj(xj?1)
]
.
The corresponding weight vector can be written as
~w?subj =
[
~wsubj
~wprevSubj
]
.
By adding these features, we are essentially assum-
ing that the words of the previous sentence are pre-
dictive of the subjectivity of the current sentence.
Alternative approaches include explicitly ac-
counting for this structure by treating subjective
sentence extraction as a sequence-labeling problem,
such as in McDonald et al (2007). Such struc-
ture formulations can be naturally encoded in the
joint feature map. Note that the inference procedure
in Algorthm 1 is still tractable, since it reduces to
comparing the best sequence of subjective/objective
sentences that explains a positive sentiment versus
the best sequence that explains a negative sentiment.
For this study, we chose not to examine this more
expressive yet more complex structure.
4.5 Extensions
Though our initial model (3) is simple and intuitive,
performance can depend heavily on the quality of
latent variable initialization and the quality of the
feature structure design. Consider the case where
the initialization contains only objective sentences
that do not convey any sentiment. Then all the fea-
tures initially available during training are gener-
ated from these objective sentences and are thus use-
less for sentiment classification. In other words, too
much useful information has been suppressed for
the model to make effective decisions. To hedge
against learning poor models due to using a poor
initialization and/or a suboptimal feature structure,
we now propose extensions that incorporate infor-
mation from the entire document.
We identify the following desirable properties that
any such extended model should satisfy:
(A) The model should be linear.
(B) The model should be trained jointly.
(C) The component that models the entire docu-
ment should influence which sentences are ex-
tracted.
The first property stems from the fact that our ap-
proach relies on linear models. The second property
is desirable since joint training avoids error propaga-
tion that can be difficult to control. The third prop-
erty deals with the information suppression issue.
1050
4.5.1 Regularizing Relative to a Prior
We first consider a model that satisfies properties
(A) and (C). Using the representation in (4), we pro-
pose a training procedure that regularize ~wpol rela-
tive to a prior model. Suppose we have a weight
vector ~w0 which indicated the a priori guess of the
contribution of each corresponding feature, then we
can train our model using OP 2,
Optimization Problem 2.
min
~w,??0
1
2
?~w ? ~w0?
2 +
C
N
N?
i=1
?i
s.t. ?i :
max
s?Si
~wT?(xi, yi, s) ?
max
s??S(xi)
~wT?(xi,?yi, s
?) + 1? ?i
For our experiments, we use
~w0 =
[
~wdoc
0
]
,
where ~wdoc denotes a weight vector trained to clas-
sify the polarity of entire documents. Then one can
interpret OP 2 as enforcing that the polarity weights
~wpol not be too far from ~wdoc. Note that ~w0 must be
available before training. Therefore this approach
does not satisfy property (B).
4.5.2 Extended Feature Space
One simple way to satisfy all three aforemen-
tioned properties is to jointly model not only po-
larity and subjectivity of the extracted sentences,
but also polarity of the entire document. Let ~wdoc
denote the weight vector used to model the polar-
ity of entire document x (so the document polarity
score is then ~wTdoc?pol(x)). We can also incorporate
this weight vector into our structured model to com-
pute a smoothed polarity score of each sentence via
~wTdoc?pol(x
j). Following this intuition, we propose
the following structured model,
~wT?(x, y, s) =
y
N(x)
?
?
?
j?s
(
~wTpol?pol(x
j) + ~wTdoc?pol(x
j)
)
?
?
+
1
N(x)
?
?
?
j?s
~wTsubj?subj(x
j)
?
?+ y ? ~wTdoc?pol(x)
where the weight vector is now
~w =
?
?
~wpol
~wsubj
~wdoc
?
? .
Training this model via OP 1 achieves that ~wdoc is
(1) used to model the polarity of the entire docu-
ment, and (2) used to compute a smoothed estimate
of the polarity of the extracted sentences. This sat-
isfies all three properties (A), (B), and (C), although
other approaches are also possible.
5 Experiments
5.1 Experimental Setup
We evaluate our methods using the Movie Reviews
and U.S. Congressional Floor Debates datasets, fol-
lowing the setup used in previous work for compar-
ison purposes.3
Movie Reviews. We use the movie reviews
dataset from Zaidan et al (2007) that was originally
released by Pang and Lee (2004). This version con-
tains annotated rationales for each review, which we
use to generate an additional initialization during
training (described below). We follow exactly the
experimental setup used in Zaidan et al (2007).4
U.S. Congressional Floor Debates. We also
use the U.S. Congressional floor debates transcripts
from Thomas et al (2006). The data was extracted
from GovTrack (http://govtrack.us), which has all
available transcripts of U.S. floor debates in the
House of Representatives in 2005. As in previ-
ous work, only debates with discussions of ?con-
troversial? bills were considered (where the los-
ing side had at least 20% of the speeches). The
goal is to predict the vote (?yea? or ?nay?) for the
speaker of each speech segment. For our experi-
ments, we evaluate our methods using the speaker-
based speech-segment classification setting as de-
scribed in Thomas et al (2006).5
3Datasets in the required format for SVMsle are available at
http://www.cs.cornell.edu/
?
ainur/data.html
4Since the rationale annotations are available for nine out of
10 folds, we used the 10-th fold as the blind test set. We trained
nine different models on subsets of size eight, used the remain-
ing fold as the validation set, and then measured the average
performance on the final test set.
5In the other setting described in Thomas et al (2006)
(segment-based speech-segment classification), around 39% of
1051
Table 1: Summary of the experimental results for the Movie Reviews (top) and U.S. Congressional Floor Debates
(bottom) datasets using SVMsle, SVMsle w/ Prior and SVMslefs with and without proximity features.
INITIALIZATION SVMsle + Prox.Feat. SVM
sle
+ Prox.Feat. SVMslefs + Prox.Feat.w/ Prior
Random 30% 87.22 85.44 87.61 87.56 89.50 88.22
Last 30% 89.72 ? 88.83 90.50 ? 90.00 ? 91.06 ? 91.22 ?
OpinionFinder 91.28 ? 90.89 ? 91.72 ? 93.22 ? 92.50 ? 92.39 ?
Annot.Rationales 91.61 ? 92.00 ? 92.67 ? 92.00 ? 92.28 ? 93.22 ?
INITIALIZATION SVMsle + Prox.Feat. SVM
sle
+ Prox.Feat. SVMslefs + Prox.Feat.w/ Prior
Random 30% 78.84 73.14 78.49 76.40 77.33 73.84
Last 30% 73.26 73.95 71.51 73.60 67.79 73.37
OpinionFinder 77.33 79.53 77.09 78.60 77.67 77.09
? For Movie Reviews, the SVM baseline accuracy is 88.56%. A ? (or ?) indicates statically significantly better performance than
baseline according to the paired t-test with p < 0.001 (or p < 0.05).
? For U.S. Congressional Floor Debates, the SVM baseline accuracy is 70.00%. Statistical significance cannot be calculated because
the data comes in a single split.
Since our training procedure solves a non-convex
optimization problem, it requires an initial guess of
the explanatory sentences. We use an explanatory
set size (5) of 30% of the number of sentences in
each document, L = d0.3 ? |x|e, with a lower cap of
1. We generate initializations using OpinionFinder
(Wilson et al, 2005), which were shown to be a
reasonable substitute for human annotations in the
Movie Reviews dataset (Yessenalina et al, 2010).6
We consider two additional (baseline) methods
for initialization: using a random set of sentences,
and using the last 30% of sentence in the document.
In the Movie Reviews dataset, we also use sentences
containing human-annotator rationales as a final ini-
tialization option. No such manual annotations are
available for the Congressional Debates.
5.2 Experimental Results
We evaluate three versions of our model: the ini-
tial model (3) which we call SVMsle (SVMs for
Sentiment classification with Latent Explanations),
SVMsle regularized relative to a prior as described in
the documents in the whole dataset contain only 1-3 sentences,
making it an uninteresting setting to analyze with our model.
6We select all sentences whose majority vote of Opinion-
Finder word-level polarities matches the document?s sentiment.
If there are fewer than L sentences, we add sentences starting
from the end of the document. If there are more, we remove
sentences starting from the beginning of the document.
Section 4.5.1 which we refer to as SVMsle w/ Prior,7
and the feature smoothing model described in Sec-
tion 4.5.2 which we call SVMslefs . Due to the diffi-
culty of selecting a good prior, we expect SVMslefs to
exhibit the most robust performance.
Table 1 shows a comparison of our proposed
methods on the two datasets. We observe that
SVMslefs provides both strong and robust perfor-
mance. The performance of SVMsle is generally bet-
ter when trained using a prior than not in the Movie
Reviews dataset. Both extensions appear to hurt
performance in the U.S. Congressional Floor De-
bates dataset. Using OpinionFinder to initialize our
training procedure offers good performance across
both datasets, whereas the baseline initializations
exhibit more erratic performance behavior.8 Unsur-
prisingly, initializing using human annotations (in
the Movie Reviews dataset) can offer further im-
provement. Adding proximity features (as described
in Section 4.4) in general seems to improve perfor-
mance when using a good initialization, and hurts
performance otherwise.
7We either used the same value of C to train both standard
SVM model and SVMsle w/ Prior or used the best standard
SVM model on the validation set to train SVMsle w/ Prior. We
chose the combination that works the best on the validation set.
8Using the random initialization on the U.S. Congressional
Floor Debates dataset offers surprisingly good performance.
1052
Table 2: Comparison of SVMslefs with previous work on
the Movie Reviews dataset. We considered two settings:
when human annotations are available (Annot. Labels),
and when they are unavailable (No Annot. Labels).
METHOD ACC
Baseline SVM 88.56
Annot. Zaidan et al (2007) 92.20
Labels SVMslefs 92.28
SVMslefs + Prox.Feat. 93.22
No Annot. Yessenalina et al (2010) 91.78
Labels SVMslefs 92.50
SVMslefs +Prox.Feat. 92.39
Table 3: Comparison of SVMslefs with previous work on
the U.S. Congressional Floor Debates dataset for the
speaker-based segment classification task.
METHOD ACC
Baseline SVM 70.00
Prior work Thomas et al (2006) 71.28Bansal et al (2008) 75.00
Our work SVM
sle
fs 77.67
SVMslefs + Prox.Feat. 77.09
Tables 2 and 3 show a comparison of SVMslefs with
previous work on the Movie Reviews and U.S. Con-
gressional Floor Debates datasets, respectively. For
the Movie Reviews dataset, we considered two set-
tings: when human annotations are available, and
when they are not (in which case we initialized using
OpinionFinder). For the U.S. Congressional Floor
Debates dataset we used only the latter setting, since
there are no annotations available for this dataset. In
all cases we observe SVMslefs showing improved per-
formance compared to previous results.
Training details. We tried around 10 different
values for C parameter, and selected the final model
based on the validation set. The training proce-
dure alternates between training a standard struc-
tural SVM model and using the subsequent model
to re-label the latent variables. We selected the halt-
ing iteration of the training procedure using the val-
idation set. When initializing using human annota-
tions for the Movie Reviews dataset, the halting iter-
ation is typically the first iteration, whereas the halt-
ing iteration is typically chosen from a later iteration
Figure 1: Overlap of extracted sentences from different
SVMslefs models on the Movie Reviews training set.
Figure 2: Test accuracy on the Movie Reviews dataset for
SVMslefs while varying extraction size.
when initializing using OpinionFinder.
Figure 1 shows the per-iteration overlap of ex-
tracted sentences from SVMslefs models initialized us-
ing OpinionFinder and human annotations on the
Movie Reviews training set. We can see that train-
ing has approximately converged after about 10 it-
erations.9 We can also see that both models itera-
tively learn to extract sentences that are more similar
to each other than their respective initializations (the
overlap between the two initializations is 57%). This
is an indicator that our learning problem, despite be-
ing non-convex and having multiple local optima,
has a reasonably large ?good? region that can be ap-
proached using different initialization methods.
Varying the extraction size. Figure 2 shows how
accuracy on the test set of SVMslefs changes on the
Movie Reviews dataset as a function of varying the
extraction size f(|x|) from (5). We can see that per-
formance changes smoothly10 (and so is robust), and
that one might see further improvement from more
9The number of iterations required to converge is an upper
bound on the number of iterations from which to choose the
halting iteration (based on a validation set).
10The smoothness will depend on the initialization.
1053
Table 4: Example ?yea? speech with Latent Explanations from the U.S. Congressional Floor Debates dataset predicted
by SVMslefs with OpinionFinder initialization. Latent Explanations are preceded by solid circles with numbers denoting
their preference order (1 being most preferred by SVMslefs ). The five least subjective sentences are preceded by circles
with numbers denoting the subjectivity order (1 being least subjective according to SVMslefs ).
? Mr. Speaker, I am proud to stand
on the house floor today to speak in
favor of the Stem Cell Research En-
hancement Act, legislation which will
bring hope to millions of people suffer-
ing from disease in this nation. ? I
want to thank Congresswoman Degette
and Congressman Castle for their tire-
less work in bringing this bill to the
house floor for a vote.
? The discovery of embryonic stem
cells is a major scientific breakthrough.
? Embryonic stem cells have the po-
tential to form any cell type in the
human body. This could have pro-
found implications for diseases such as
Alzheimer?s, Parkinson?s, various forms
of brain and spinal cord disorders, dia-
betes, and many types of cancer. ? Ac-
cording to the Coalition for the Ad-
vancement of Medical Research, there
are at least 58 diseases which could po-
tentially be cured through stem cell re-
search.
That is why more than 200 major
patient groups, scientists, and medical
research groups and 80 Nobel Laure-
ates support the Stem Cell Research En-
hancement Act. ? They know that this
legislation will give us a chance to find
cures to diseases affecting 100 million
Americans.
I want to make clear that I oppose re-
productive cloning, as we all do. I have
voted against it in the past. ? However,
that is vastly different from stem cell re-
search and as an ovarian cancer sur-
vivor, I am not going to stand in the way
of science.
Permitting peer-reviewed Federal
funds to be used for this research,
combined with public oversight of these
activities, is our best assurance that
research will be of the highest quality
and performed with the greatest dignity
and moral responsibility. The policy
President Bush announced in August
2001 has limited access to stem cell
lines and has stalled scientific progress.
As a cancer survivor, I know the des-
peration these families feel as they wait
for a cure. ? This congress must not
stand in the way of that progress. ? We
have an opportunity to change the lives
of millions, and I hope we take it. ? I
urge my colleagues to support this leg-
islation.
careful tuning of the size constraint.
Examining an example prediction. Our pro-
posed methods are not designed to extract inter-
pretable explanations, but examining the extracted
explanations might still yield meaningful informa-
tion. Table 4 contains an example speech from the
U.S. Congressional Floor Debates test set, with La-
tent Explanations found by SVMslefs highlighted in
boldface. This speech was made in support of the
Stem Cell Research Enhancement Act. For com-
parison, Table 4 also shows the five least subjective
sentences according to SVMslefs . Notice that most of
these ?objective? sentences can plausibly belong to
speeches made in opposition to bills that limit stem
cell research funding. That is, they do not clearly in-
dicate the speaker?s stance towards the specific bill
in question. We can thus see that our approach can
indeed learn to infer sentences that are essential to
understanding the document-level sentiment.
6 Discussion
Making good structural assumptions simplifies the
development process. Compared to methods that
modify the training of flat document classifiers (e.g.,
Zaidan et al (2007)), our approach uses fewer pa-
rameters, leading to a more compact and faster train-
ing stage. Compared to methods that use a cascaded
approach (e.g., Pang and Lee (2004)), our approach
is more robust to errors in the lower-level subtask
due to being a joint model.
Introducing latent variables makes the training
procedure more flexible by not requiring lower-level
labels, but does require a good initialization (i.e., a
reasonable substitute for the lower-level labels). We
believe that the widespread availability of off-the-
shelf sentiment lexicons and software, despite being
developed for a different domain, makes this issue
less of a concern, and in fact creates an opportunity
for approaches like ours to have real impact.
One can incorporate many types of sentence-level
information that cannot be directly incorporated into
a flat model. Examples include scores from another
sentence-level classifier (e.g., from Nakagawa et. al
(2010)) or combining phrase-level polarity scores
(e.g., from Choi and Cardie (2008)) for each sen-
tence, or features that describe the position of the
sentence in the document.
Most prior work on the U.S. Congressional Floor
Debates dataset focused on using relationships be-
tween speakers such as agreement (Thomas et al,
2006; Bansal et al, 2008), and used a global min-
cut inference procedure. However, they require all
1054
test instances to be known in advance (i.e., their for-
mulations are transductive). Our method is not lim-
ited to the transductive setting, and instead exploits
a different and complementary structure: the latent
explanation (i.e., only some sentences in the speech
are indicative of the speaker?s vote).
In a sense, the joint feature structure used in
our model is the simplest that could be used. Our
model makes no explicit structural dependencies be-
tween sentences, so the choice of whether to extract
each sentence is essentially made independently of
other sentences in the document. More sophisticated
structures can be used if appropriate. For instance,
one can formulate the sentence extraction task as
a sequence labeling problem similar to (McDonald
et al, 2007), or use a more expressive graphical
model such as in (Pang and Lee, 2004; Thomas et
al., 2006). So long as the global inference proce-
dure is tractable or has a good approximation al-
gorithm, then the training procedure is guaranteed
to converge with rigorous generalization guarantees
(Finley and Joachims, 2008). Since any formulation
of the extraction subtask will suppress information
for the main document-level task, one must take care
to properly incorporate smoothing if necessary.
Another interesting direction is training models to
predict not only sentiment polarity, but also whether
a document is objective. For example, one can pose
a three class problem (?positive?, ?negative?, ?ob-
jective?), where objective documents might not nec-
essarily have a good set of (subjective) explanatory
sentences, similar to (Chang et al, 2010).
7 Conclusion
We have presented latent variable structured mod-
els for the document sentiment classification task.
These models do not rely on sentence-level an-
notations, and are trained jointly (over both the
document and sentence levels) to directly optimize
document-level accuracy. Experiments on two stan-
dard sentiment analysis datasets showed improved
performance over previous results.
Our approach can, in principle, be applied to any
classification task that is well modeled by jointly
solving an extraction subtask. However, as evi-
denced by our experiments, proper training does re-
quire a reasonable initial guess of the extracted ex-
planations, as well as ways to mitigate the risk of
the extraction subtask suppressing too much infor-
mation (such as via feature smoothing).
Acknowledgments
This work was supported in part by National Science
Foundation Grants BCS-0904822, BCS-0624277, IIS-
0535099; by a gift from Google; and by the Department
of Homeland Security under ONR Grant N0014-07-1-
0152. The second author was also supported in part by
a Microsoft Research Graduate Fellowship. The authors
thank Yejin Choi, Thorsten Joachims, Nikos Karampatzi-
akis, Lillian Lee, Chun-Nam Yu, and the anonymous re-
viewers for their helpful comments.
Appendix
Recall that all the ?subj and ?pol vectors have unit 2-
norm, which is assumed here to be desirable. We now
show that using N(x) =
?
f(|x|) achieves a similar
property for ?(x, y, s). We can write the squared 2-norm
of ?(x, y, s) as
|?(x, y, s)|2 =
1
N(x)2
?
?
?
j?s
y ? ?pol(x
j) + ?subj(x
j)
?
?
2
=
1
f(|x|)
?
?
?
?
?
?
j?s
?pol(x
j)
?
?
2
+
?
?
?
j?s
?subj(x
j)
?
?
2
?
?
? ,
where the last equality follows from the fact that
?pol(x
j)T?subj(x
j) = 0,
due to the two vectors using disjoint feature spaces by
construction. The summation of the ?pol(xj) terms is
written as
?
?
?
j?s
?pol(x
j)
?
?
2
=
?
j?s
?
i?s
?pol(x
j)T?pol(x
i)
?
?
j?s
?pol(x
j)T?pol(x
j) (8)
=
?
j?s
1 ? f(|x|),
where (8) follows from the sparsity assumption that
?i 6= j : ?pol(x
j)T?pol(x
i) ? 0.
A similar argument applies for the ?subj(xj) terms.
Thus, by choosing N(x) =
?
f(|x|) the joint feature
vectors ?(x, y, s) will have approximately equal magni-
tude as measured using the 2-norm.
1055
References
Mohit Bansal, Claire Cardie, and Lillian Lee. 2008. The
power of negative thinking: Exploiting label disagree-
ment in the min-cut classification framework. In In-
ternational Conference on Computational Linguistics
(COLING).
Ming-Wei Chang, Dan Goldwasser, Dan Roth, and Vivek
Srikumar. 2010. Discriminative learning over con-
strained latent representations. In Conference of the
North American Chapter of the Association for Com-
putational Linguistics (NAACL).
Yejin Choi and Claire Cardie. 2008. Learning with com-
positional semantics as structural inference for subsen-
tential sentiment analysis. In Empirical Methods in
Natural Language Processing (EMNLP).
James Clarke, Dan Goldwasser, Ming-Wei Chang, and
Dan Roth. 2010. Driving semantic parsing from the
world?s response. In ACL Conference on Natural Lan-
guage Learning (CoNLL), July.
Pedro Felzenszwalb, David McAllester, and Deva Ra-
manan. 2008. A discriminatively trained, multiscale,
deformable part model. In IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR).
Thomas Finley and Thorsten Joachims. 2008. Train-
ing structural svms when exact inference is intractable.
In International Conference on Machine Learning
(ICML).
Thorsten Joachims, Thomas Finley, and Chun-Nam Yu.
2009. Cutting plane training of structural svms. Ma-
chine Learning, 77(1):27?59.
Yi Mao and Guy Lebanon. 2006. Isotonic conditional
random fields and local sentiment flow. In Neural In-
formation Processing Systems (NIPS).
Ryan McDonald, Kerry Hannan, Tyler Neylon, Mike
Wells, and Jeff Reynar. 2007. Structured models for
fine-to-coarse sentiment analysis. In Annual Meet-
ing of the Association for Computational Linguistics
(ACL).
Tetsuji Nakagawa, Kentaro Inui, and Sadao Kurohashi.
2010. Dependency tree-based sentiment classification
using crfs with hidden variables. In Conference of the
North American Chapter of the Association for Com-
putational Linguistics (NAACL).
Bo Pang and Lillian Lee. 2004. A sentimental education:
Sentiment analysis using subjectivity summarization
based on minimum cuts. In Annual Meeting of the As-
sociation for Computational Linguistics (ACL).
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and Trends in Infor-
mation Retrieval, 2(1-2):1?135.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment classification using ma-
chine learning techniques. In Empirical Methods in
Natural Language Processing (EMNLP).
Slav Petrov and Dan Klein. 2007. Discriminative log-
linear grammars with latent variables. In Neural In-
formation Processing Systems (NIPS).
Matt Thomas, Bo Pang, and Lillian Lee. 2006. Get
out the vote: Determining support or opposition from
Congressional floor-debate transcripts. In Empirical
Methods in Natural Language Processing (EMNLP).
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In Empirical Methods in Natural
Language Processing (EMNLP).
Ainur Yessenalina, Yejin Choi, and Claire Cardie. 2010.
Automatically generating annotator rationales to im-
prove sentiment classification. In Annual Meeting of
the Association for Computational Linguistics (ACL).
Chun-Nam Yu and Thorsten Joachims. 2009. Learning
structural svms with latent variables. In International
Conference on Machine Learning (ICML).
Alan L. Yuille and Anand Rangarajan. 2003. The
concave-convex procedure. Neural Computation,
15(4):915?936, April.
Omar F. Zaidan and Jason Eisner. 2008. Modeling an-
notators: a generative approach to learning from an-
notator rationales. In Empirical Methods in Natural
Language Processing (EMNLP).
Omar F. Zaidan, Jason Eisner, and Christine Piatko.
2007. Using ?annotator rationales? to improve ma-
chine learning for text categorization. In Conference
of the North American Chapter of the Association for
Computational Linguistics (NAACL).
1056
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 172?182,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Compositional Matrix-Space Models for Sentiment Analysis
Ainur Yessenalina
Dept. of Computer Science
Cornell University
Ithaca, NY, 14853
ainur@cs.cornell.edu
Claire Cardie
Dept. of Computer Science
Cornell University
Ithaca, NY, 14853
cardie@cs.cornell.edu
Abstract
We present a general learning-based approach
for phrase-level sentiment analysis that adopts
an ordinal sentiment scale and is explicitly
compositional in nature. Thus, we can model
the compositional effects required for accu-
rate assignment of phrase-level sentiment. For
example, combining an adverb (e.g., ?very?)
with a positive polar adjective (e.g., ?good?)
produces a phrase (?very good?) with in-
creased polarity over the adjective alone. In-
spired by recent work on distributional ap-
proaches to compositionality, we model each
word as a matrix and combine words us-
ing iterated matrix multiplication, which al-
lows for the modeling of both additive and
multiplicative semantic effects. Although the
multiplication-based matrix-space framework
has been shown to be a theoretically ele-
gant way to model composition (Rudolph and
Giesbrecht, 2010), training such models has
to be done carefully: the optimization is non-
convex and requires a good initial starting
point. This paper presents the first such al-
gorithm for learning a matrix-space model for
semantic composition. In the context of the
phrase-level sentiment analysis task, our ex-
perimental results show statistically signifi-
cant improvements in performance over a bag-
of-words model.
1 Introduction
Sentiment analysis has been an active research area
in recent years. Work in the area ranges from iden-
tifying the sentiment of individual words to deter-
mining the sentiment of phrases, sentences and doc-
uments (see Pang and Lee (2008) for a survey). The
bulk of previous research, however, models just pos-
itive vs. negative sentiment, collapsing positive (or
negative) words, phrases and documents of differ-
ing intensities into just one positive (or negative)
class. For word-level sentiment, therefore, these
methods would not recognize a difference in senti-
ment between words like ?good? and ?great?, which
have the same direction of polarity (i.e., positive)
but different intensities. At the phrase level, the
methods will fail to register compositional effects in
sentiment brought about by intensifiers like ?very?,
?absolutely?, ?extremely?, etc. ?Happy? and ?very
happy?, for example, will both be considered sim-
ply ?positive? in sentiment. In real-world settings,
on the other hand, sentiment values extend across a
polarity spectrum ? from very negative, to neutral,
to very positive. Recent research has shown, in par-
ticular, that modeling intensity at the phrase level is
important for real-world natural language process-
ing tasks including question answering and textual
entailment (de Marneffe et al, 2010).
This paper describes a general approach for
phrase-level sentiment analysis that takes these real-
world requirements into account: we adopt a five-
level ordinal sentiment scale and present a learning-
based method that assigns ordinal sentiment scores
to phrases.
Importantly, our approach will also be explicitly
compositional1 in nature so that it can accurately ac-
count for critical interactions among the words in
1The Principle of Compositionality asserts that the meaning
of a complex expression is a function of the meanings of its
constituent expressions and the rules used to combine them.
172
each sentiment-bearing phrase. Consider, for exam-
ple, combining an adverb like ?very? with a polar
adjective like ?good?. ?Good? has an a priori posi-
tive sentiment, so ?very good? should be considered
more positive even though ?very?, on its own, does
not bear sentiment. Combining ?very? with a nega-
tive adjective, like ?bad?, produces a phrase (?very
bad?) that should be characterized as more negative
than the original adjective. Thus, it is convenient
to think of the effect of combining an intensifying
adverb with a polar adjective as being multiplica-
tive in nature, if we assume the adjectives (?good?
and ?bad?) to have positive and a negative sentiment
scores, respectively.
Next, let us consider adverbial negators like ?not?
combined with polar adjectives. When model-
ing only positive and negative labels for sentiment,
negators are generally treated as flipping the polar-
ity of the adjective it modifies (Choi and Cardie,
2008; Nakagawa et al, 2010). However, recent work
(Taboada et al, 2011; Liu and Seneff, 2009) sug-
gests that the effect of the negator when ordinal sen-
timent scores are employed is more akin to damp-
ening the adjective?s polarity rather than flipping it.
For example, if ?perfect? has a strong positive sen-
timent, then the phrase ?not perfect? is still positive,
though to a lesser degree. And while ?not terrible? is
still negative, it is less negative than ?terrible?. For
these cases, it is convenient to view ?not? as shift-
ing polarity to the opposite side of polarity scale by
some value.
There are, of course, more interesting examples of
compositional semantic effects on sentiment: e.g.,
prevent cancer, ease the burden. Here, the verbs
prevent and ease act as content-word negators (Choi
and Cardie, 2008) in that they modify the negative
sentiment of their direct object arguments so that the
phrase as a whole is perceived as somewhat positive.
Nonetheless, the vast majority of methods for
phrase- and sentence-level sentiment analysis do not
tackle the task compositionally: they, instead, em-
ploy a bag-of-words representation and, at best, in-
corporate additional features to account for nega-
tors, intensifiers, and for contextual valence shifters,
which can change the sentiment over neighboring
words (e.g., Polanyi and Zaenen (2004), Wilson et
al. (2005) , Kennedy and Inkpen (2006), Shaikh et
al. (2007)).
One notable exception is Moilanen and Pulman
(2007), who propose a compositional semantic ap-
proach to assign a positive or negative sentiment to
newspaper article titles. However, their knowledge-
based approach presupposes the existence of a sen-
timent lexicon and a set of symbolic compositional
rules.
But learning-based compositional approaches
for sentiment analyis also exist. Choi and
Cardie (2008), for example, propose an algo-
rithm for phrase-based sentiment analysis that learns
proper assignments of intermediate sentiment anal-
ysis decision variables given the a priori (i.e., out
of context) polarity of the words in the phrase and
the (correct) phrase-level polarity. As in Moilianen
and Pulman (2007), semantic inference is based on
(a small set of) hand-written compositional rules. In
contrast, Nakagawa et. al (2010) use a dependency
parse tree to guide the learning of compositional ef-
fects. Each of the above, however, uses a binary
rather than an ordinal sentiment scale.
In contrast, our proposed method for phrase-
level sentiment analysis is inspired by recent work
on distributional approaches to compositionality.
In particular, Baroni and Zamparelli (2010) tackle
adjective-noun compositions using a vector repre-
sentation for nouns and learning a matrix represen-
tation for each adjective. The adjective matrices are
then applied as functions over the meanings of nouns
? via matrix-vector multiplication ? to derive the
meaning of adjective-noun combinations. Rudolph
and Giesbrecht (2010) show theoretically, that mul-
tiplicative matrix-space models are a general case
of vector-space models and furthermore exhibit de-
sirable properties for semantic analysis: they take
into account word order and are algebraically, neuro-
logically and psychologically plausible. This work,
however, does not present an algorithm for learning
such models; nor does it provide empirical evidence
in favor of matrix-space models over vector-space
models.
In the sections below, we propose a learning-
based approach to assign ordinal sentiment scores to
sentiment-bearing phrases using a general composi-
tional matrix-space model of language. In contrast
to previous work, all words are modeled as matri-
ces, independent of their part-of-speech, and com-
positional inference is uniformly modeled as ma-
173
trix multiplication. To predict an ordinal scale sen-
timent value, we employ Ordered Logistic Regres-
sion, introducing a novel training algorithm to ac-
commodate our compositional matrix-space repre-
sentations (Section 2). To our knowledge, this is the
first such algorithm for learning matrix-space mod-
els for semantic composition. We evaluate the ap-
proach on a standard sentiment corpus (Wiebe et al,
2005) (Section 3), making use of its manually anno-
tated phrase-level annotations for polarity and inten-
sity, and compare our approach to the more com-
monly employed bag-of-words model. We show
(Section 4) that our matrix-space model significantly
outperforms a bag-of-words model for the ordinal
scale sentiment prediction task.
2 The Model for Ordinal Scale Sentiment
Prediction
As described above, our task is to predict an ordi-
nal scale sentiment value for a phrase. To this end,
we employ a sentiment scale with five ordinal val-
ues: VERY NEGATIVE, NEGATIVE, NEUTRAL, POS-
ITIVE and VERY POSITIVE. Given a set of phrase-
level training examples with their gold-standard or-
dinal sentiment value, we then use an Ordered Lo-
gistic Regression (OLogReg) model for prediction.
Unfortunately, our matrix-space representation pre-
cludes doing this directly.
We have chosen OLogReg, as opposed to say
PRanking (Crammer and Singer, 2001), because op-
timization of the former is more attractive: the ob-
jective (likelihood) is smooth and the gradients are
continuous. As will become clear shortly, learn-
ing our models is not trivial and it is important to
use sophisticated off-the-shelf optimizers such as L-
BFGS.
For a bag-of-words model, OLogReg learns one
weight for each word and a set of thresholds by max-
imizing the likelihood of the training data. Typically,
this is accomplished by using an optimizer like L-
BFGS whose interface needs the value and gradient
of the likelihood with respect to the parameters at
their current values. In the next subsections, we in-
stantiate OLogReg for our sentiment prediction task
using a matrix-space word model (2.1 and 2.2) and
a bag-of-words model (2.3). The learning formula-
tion of bag-of-words OLogReg is convex therefore
we will get the global optimum; in contrast, the op-
timization problem for matrix-space model is non-
convex, it is important to initialize the model well.
Initialization of the matrix-space model is discussed
in Section 2.4.
2.1 Notation
In the subsequent subsections we will use the
following notation. Let n be the number of phrases
in the training set and let d be the number of words
in the dictionary. Let xi be the i-th phrase and yi
would be the label of xi, where yi takes r different
values yi ? {0, . . . , r ? 1}. Then |xi| will denote
the length of the phrase xi, and the words in i-th
phrase are: xi = xi1, xi2, . . . , xi|xi|; xij , 1 ? j ? |xi|
is the j-th word of i-th phrase; where xij is from the
dictionary: 1 ? xij ? d.
In the case of the bag-of-words model, ?(xi) ?
Rd is the representation of the i-th phrase. ?j(xi)
counts the number of times the j-th word from the
dictionary appears in the i-th phrase. Given a w ?
Rd it assigns a score ?i to a phrase xi by
?i = wT?(xi) =
|xi|?
j=1
wxij (1)
In the case of the matrix-space model the ?(xi) ?
R|xi|?d is the representation of the i-th phrase.
?jk(xi) is 1, if xij is the k-th word in the dictionary,
and zero otherwise. Given u, v ? Rm and a set of
matrices {Wp ? Rm?m}dp=1, one for each word, it
assigns a score ?i to a phrase xi by
?i = uT
?
?
|xi|?
j=1
d?
k=1
Wk?jk(xi)
?
? v
= uT
?
?
|xi|?
j=1
Wxij
?
? v (2)
where ?|xi|j=1 Wxij = Wxi1Wxi2 ? ? ?Wxi|xi| in exactlythis order. We choose to map matrices to the real
numbers by using vectors u and v from Rm?1; so
that ? = uTMv, where M ? Rm?m, which is sen-
sitive to the order of matrices2 , i.e. uTM1M2v 6=
2Care must be taken in choosing way to map matrix to a real
174
uTM2M1v.
Modeling composition. Am?mmatrix, represent-
ing a word, can be considered as a linear function,
mapping from Rm to Rm. Composition of words is
modeled by function composition, in our case com-
position of linear functions, i.e. matrix multipli-
cation. Note, that unlike bag-of-words model, the
matrix-space model takes word order into account,
since matrix multiplication is not commutative op-
eration.
2.2 Ordered Logistic Regression
Now we will describe our objective function for
OLogReg and its derivatives. OLogReg has r ?
1 thresholds (?0, . . . ?r?2), so introducing ??1 =
?? and ?r?1 = ? leads to the unified expression
for posterior probabilities for all values of k:
P (yi = k|x) = P (?k?1 < ?i ? ?k)
= F (?k ? ?i)? F (?k?1 ? ?i)
F (x) is an inverse-logit function
F (x) = e
x
1 + ex
this is its derivative:
dF (x)
dx = F (x)(1? F (x))
Therefore the negative loglikelihood of the training
data will look like the following (Hardin and Hilbe,
2007):
L = ?
n?
i=1
r?1?
k=0
ln(F (?k ? ?i)? F (?k?1 ? ?i))I(yi = k)
where r is the number of ordinal classes, ?i is the
score of i-th phrase, I is the indicator function that
is equal to 1 ? when yi = k, and zero otherwise. We
need to minimize the objective L with respect to the
following constraints:
?k?1 ? ?k, 1 ? k ? r ? 2 (3)
number. For example, one other way to map matrices to the
real numbers is to use the determinant of a matrix; however, the
determinant is not sensitive to the word order: det(M1M2) =
det(M1)det(M2) = det(M2M1); which is not desirable for a
model that needs to account for word order.
(The constraints are similar to the ones in PRank al-
gorithm). For ease of optimization we parametrize
our model via ?0, and ?j , 1 ? j ? r ? 2:
??1 = ??,
?0,
?1 = ?0 + ?1,
?2 = ?0 +
?2
j=1 ?j ,
. . . ,
?r?2 = ?0 +
?r?2
j=1 ?j
?r?1 = ?,
where ?1, . . ., ?r?2 are non-negative values, that rep-
resent how far the corresponding thresholds are from
each other. Then the constraints (3) would be:
?j ? 0, 1 ? j ? r ? 2 (4)
To simplify the equations we can rewrite the nega-
tive loglikelihood as follows:
L = ?
n?
i=1
r?1?
k=0
ln(Aik ?Bik)I(yi = k) (5)
where
Aik =
{
F (?0 +
?k
j=1 ?j ? ?i), if k = 0, . . . , r ? 2
1, if k = r ? 1
Bik =
{
0, if k = 0
F (?0 +
?k?1
j=1 ?j ? ?i), if k = 1, . . . , r ? 1
Let?s introduce Lik = ? ln(Aik ? Bik)I(yi = k)
and then the derivative of Lik with respect to ?0 will
be:
?Lik
??0
= ?[Aik(1?Aik)?Bik(1?Bik)]Aik ?Bik
I(yi = k)
= (Aik + Bik ? 1)I(yi = k)
For j = yi:
?Lik
??j
= ?Aik(1?Aik)Aik ?Bik
I(yi = k)
For all j < yi:
?Lik
??j
= (Aik + Bik ? 1)I(yi = k)
For all j > yi: ?Lik??j = 0.The derivative with respect to the score ?i is:
?Lik
??i
= (?Aik ?Bik + 1)I(yi = k) (6)
175
2.2.1 Matrix-Space Word Model
Here we show the derivatives with respect to a
word. For the OLogReg model with matrix-space
word representations, we have:
?L
?Wxij
= ?L??i
? ??i?Wxij
The expression for ?L??i is given in (6); we will derive
??i
?Wxij
from (2). In the case of the Matrix-Space word
model each word is represented as an m?m affine
matrix W :
W =
(
A b
0 1
)
(7)
We choose the class of affine matrices since for
affine matrices matrix multiplication represents both
operations: linear transformation and translation.
Linear transformation is important for modeling
changes in sentiment - translation is also useful (we
make use of a translation vector during initialization,
see Section 2.4). In this work we consider m ? 3
since we want the matrix A from (7) to represent
rotation and scaling. Applying the affine transfor-
mation W to vector [x, 1]T is equivalent to applying
linear transformation A and translation b to x. 3
Though vectors u and v can be learned together
with word matrices Wj , we choose to fix u and v.
The main intuition behind fixing u and v is to re-
duce the degrees of freedom of the model: differ-
ent assignments of u, v and Wj-s can lead to the
same score ?, i.e. there exist u? v? and W?j-s dif-
ferent from u, v and Wj-s respectively, such that
?(u, v,W ) would be equal to ?(u?, v?, W? ). 4
3
?
A b
0 1
??
x
1
?
=
?
Ax+ b
1
?
where A is a linear transformation, b is a translation vector.
Also the product of affine matrices is an affine matrix.
4The specific choice of u and v leads to an equivalent model
for all u? and v? such that u? = MTu, v? = M?1v, where M is
any invertible transformation (i.e. u?, v? are derived from u,v by
applying linear transformations MT , M?1 respectively):
uTW1W2v = (uTM)(M?1W1M)(M?1W2M)(M?1v)
= u?T W?1W?2v?
The derivative of the phrase ?i with respect to j-th
word Wj would be (for brevity we drop the phrase
index and Wj refers to Wxij and p refers to |xi|):
??i
?Wj
=
(?uTW1W2 . . .Wpv
?Wj
)
=
[
(uTW1 . . .Wj?1)T (Wj+1 . . .Wpv)T
]
=
[
(W Tj?1 . . .W T1 )(uvT )(W Tp . . .W Tj+1)
]
(see Peterson and Pederson(2008)).
In case if a certain word appears multiple times in
the phrase, the derivative with respect to that word
would be a sum of derivatives with respect to each
appearance of a word, while all other appearances
are fixed. For example,
(?uTWW1Wv
?W
)
= u(W1Wv)T + (uTWW1)T vT
where W is a representation of a word that is re-
peated.
So given the expression (6) for ?L??i , the derivativewith respect to each word can be computed. Notice
that the update for the j-th word in a sentence de-
pends on the order words, which is in line with our
desire to account for word order.
2.2.2 Optimization
The goal of training procedure is for the i-th
phrase with p words x1x2 . . . xp to learn word ma-
trices W1, W2, . . . , Wp such that resulting ?i-s will
lead to the lowest negative loglikelihood. The goal
of training procedure is to find word matrices W1,
W2, . . . Wp and thresholds ?0, ?1, . . . ?r?2 such
that the negative loglikelihood is minimized. So,
given the negative loglikelihood and the derivatives
with respect ?0 and ?j-s and word matrices W , we
optimize objective (5) subject to ?j ? 0. We use L-
BFGS-B (Large-scale Bound-constrained Optimiza-
tion) by Byrd et al (1995) as an optimizer.
2.2.3 Regularization in Matrix-Space Model
In order to make sure that the L-BFGS-B updates
do not cause numerical issues we perform the fol-
lowing regularization to the resulting matrices. An
m by m matrix Wj that can be represented as:
Wj =
(
A11 a12
aT21 a22
)
176
where A11 ? Rm?1?m?1, a12, a21 ? Rm?1?1,
a22 ? R. First make the matrix affine by updating
the last row, then the updated matrix will look like:
W?j =
(
A11 a12
0 1
)
It can be proven that such a projection returns the
closest affine matrix in Frobenius norm.
However, we also want to regularize the model to
avoid ill-conditioned matrices. Ill-conditioned ma-
trices represent transformations whose output is very
sensitive to small changes in the input and therefore
they have a similar effect to having large weights
in a bag-of-words model. To perform such a reg-
ularization we ?shrink? the singular values of A11
towards one. More specifically, we first use the
Singular Value Decomposition (SVD) of the A11:
U?V T = A11, where U and V are orthogonal ma-
trices, ? is a matrix with singular values on the diag-
onal. Then we update singular values in the follow-
ing way to get ??: ??ii = ?hii, where h is a parameter
between 0 and 1. If h = 1 then ?ii remains the
same. In the extreme case h = 0 then ?hii = 1. For
intermediate values of h the singular values of A11
would be brought closer to one. Finally, we recom-
pute A?11: A?11 = U ??V T . So, Wj would be :
W?j =
(
A?11 a12
0 1
)
2.2.4 Learning in the Matrix-Space Model
We use Algorithm 1 to learn the matrix-space
model. What essentially happens is that we iter-
ate two steps: optimizing the W matrices using L-
BFGS-B and the projection step. L-BFGS-B returns
a solution that is not necessarily an affine matrix.
After projecting to the space of affine matrices we
start L-BFGS-B from a better initial point. In prac-
tice, the first few iterations lead to large decrease in
negative loglikelihood.
2.3 Bag-Of-Words Model
In the bag-of-words model the score of the i-th
phrase is given in (1). Therefore, the partial deriva-
tive with respect to j-th word in i-th phrase ??i?wxij
is
equal to the number cj of times xji appears in xi, so:
?L
?wxij
= ?L??i
? cj
Algorithm 1 Training Algorithm for Matrix-Space
OLogReg
1: Input: {(x1, y1), . . . , (xn, yn)} //training data
2: Input: h //projection parameter
3: Input: T //number of iterations
4: Input: W , ?0 and ?j //initial values
5: for t = 1, . . . , T do
6: (W , ?0, ?j)=minimize L using L-BFGS-B
7: for i = 1, . . . , d do
8: Wi=Project(Wi, h)
9: end for
10: end for
11: Return W , ?0, ?j
Optimization. We minimize negative loglikelihood
using L-BFGS-B subject to ?j ? 0.
Regularization. To prevent overfitting for bag-of-
words model we regularize w. The L2-regularized
negative loglikelihood will consist of the expression
in (5) and an additional term ?2 ||w||22, where || ? ||2is the L2-norm of a vector. The derivative of the
additional term with respect to w will be:
? ?2 ||w||22
?w = ?w
Hence the partial derivative with respect to wxij willhave an additional term ?wxij .
2.4 Initialization
Initialization of bag-of-words OLogReg. We ini-
tialize the weight for each word with zero and ?0
with a random number and ?j-s with non-negative
random numbers. Since the learning problem for
bag-of-words OLogReg is convex, we will get the
global optimum.
Better Initialization of Matrix-Space Model. Pre-
liminary experiments showed that the Matrix-Space
model needs a good initialization. Initializing with
different random matrices reaches different local
minima and the quality of local minima depends on
initialization. Therefore, it is important to initialize
the model with a good initial point. One way to ini-
tialize the Matrix-Space model is to use the weights
learned by the bag-of-words model. We use the
following intuition for initializing the Matrix-Space
model. As noted in Section 2.2.1 applying trans-
formation A of affine matrix W can model a linear
177
transformation, while vector b represents a transla-
tion. Since matrix-space model can encode a vector-
space model (Rudolph and Giesbrecht, 2010), we
can initialize the matrices to exactly mimic the bag-
of-words model. In order to do that we place the
weight, learned by the bag-of-words model in the
first component of b. Let?s assume that wx1 and wx2
are the weights learned for two distinct words x1 and
x2 respectively. To compute the polarity score of
a phrase x1, x2 the bag-of-words model sums the
weights of these two words: wx1 and wx2 . Now we
want to have the same effect in matrix-space model.
Here we assume m = 3.
Z =
?
?
1 0 wx1
0 1 0
0 0 1
?
?
?
?
1 0 wx2
0 1 0
0 0 1
?
?
=
?
?
1 0 wx1 + wx2
0 1 0
0 0 1
?
?
Finally, there is a step of mapping matrix Z to a
number using u and v, such that ?(Z) = wx1 +
wx2 .We also want vector u and v to be such that:
uT
?
?
1 0 wx1 + wx2
0 1 0
0 0 1
?
? v = wx1 + wx2 (8)
The last equation can help us construct u and v.
We also set u and v to be orthogonal: uT v = 0.
So, we arbitrarily choose two orthogonal vectors for
which equation (8) holds: u = [1,?2, 1]T and v =
[1,?
?
2, 1]T .5
3 Experimental Methodology
For experimental evaluation of the proposed method
we use the publicly available Multi-Perspective
Question Answering (MPQA)6 corpus (Wiebe et al,
2005) version 1.2, which contains 535 newswire
documents that are manually annotated with phrase-
level subjectivity and intensity. We use the
expression-level boundary markings in MPQA to
extract phrases. We evaluate on positive, negative
and neutral opinion expressions that have intensities
5If m > 3, u and v can be set using the same intuition.
6http://www.cs.pitt.edu/mpqa/
Polarity Intensity Ordinal
label
negative high, extreme 0
negative medium 1
neutral high, extreme, medium 2
positive medium 3
positive high, extreme 4
Table 1: Mapping of combination of polarities and inten-
sities from MPQA dataset to our ordinal sentiment scale.
?medium?, ?high? or ?extreme?.7 The schematic
mapping of phrase polarity and intensity values on
ordinal sentimental scale is shown in Table 1.
3.1 Training Details
We perform 10-fold cross-validation on phrases ex-
tracted from the MPQA corpus: eight folds for train-
ing; one as a validation set; and one as test set. In
total there were 8022 phrases. Before training, we
extract lemmas for each word. For evaluation we
use Ranking Loss: 1n
?
i |y?i ? yi|, where y?i is the
prediction.
Choice of dimensionality m. The reported ex-
periments are done by setting m = 3. Preliminary
experiments with higher values of m (5, 20, 50), did
not lead to a better performance and increased the
training time; therefore we did not use those values
in our final experiments.
3.2 Methods
PRank. For each of the folds, we run 500 iterations
of PRank and choose an early stopping iteration us-
ing a model that led to the lowest ranking loss on the
validation set; afterwards report the average perfor-
mance of on a test set.
Bag-of-words OLogReg. To prevent overfitting we
search for the best regularization parameter among
the following values of ?: 10i, from 10?4 to 104.
The lowest negative log-likelihood value on the val-
idation set is attained for8 ? = 0.1. With this value
of ? fixed, the final model is the one with the lowest
negative loglikelihood on the training set.
7We ignored low-intensity phrases similar to (Choi and
Cardie, 2008; Nakagawa et al, 2010).
8We pick single ? that gives best average validation set per-
formance, and then use it to compute the average test set perfor-
mance.
178
Method Ranking loss
PRank 0.7808
Bag-of-words OLogReg 0.6665
Matrix-space OLogReg+RandInit 0.7417
Matrix-space OLogReg+BowInit 0.6375?
Table 2: Ranking loss for vector-space Ordered Logistic
Regression and Matrix-Space Logistic Regression.
? Stands for a significant difference w.r.t. the Bag-Of-
Words OLogReg model with p-value less than 0.001
(p < 0.001)
Matrix-space OLogReg+RandInit. First, we ini-
tialized matrices with with random numbers from
normal distribution N(0, 0.1) and set u and v as in
section 2.4, T is set to 25. We run with two different
random seeds and three different values for the pa-
rameter h: [0.1, 0.5, 0.9] and report the performance
of the model that had the lowest likelihood on the
validation set. The setting of h that lead to the best
model was 0.9.
Matrix-space OLogReg+BowInit. For the matrix-
space models we initialize the model with the out-
put of the regularized Bag-of-words OLogReg as de-
scribed in Section 2.4, T is set to 25. Then we use
the training procedure of Algorithm 1. We consider
three different values for the parameter h [0.1, 0.5,
0.9] and choose as the model with the lowest valida-
tion set negative log-likelihood. The best setting of
h was 0.1.
4 Results and Discussion
We report Ranking Loss for the four models in Ta-
ble 2. The worst performance (denoted by the high-
est ranking loss value) is obtained by PRank, fol-
lowed by matrix-space OLogReg with random ini-
tialization. Bag-of-words OLogReg obtains quite
good performance, and matrix-space OLogReg, ini-
tialized using the bag-of-words model performs the
best, showing statistically significant improvements
over the bag-of-words OLogReg model according to
a paired t-test. .
To see what the bag-of-word and matrix-space
models are learning we performed inference on a
few examples. In Table 3 we show the sentiment
scores of the best performing bag-of-words OLo-
gReg model and the best performing model based
Phrase Matrix-space Bag-of-words
OLogReg+BowInit OLogReg
not -0.83 -0.42
very 0.23 0.04
good 2.81 1.51
very good 3.53 1.55
not good -0.16 1.09
not very good 0.66 1.13
bad -1.67 -1.42
very bad -2.01 -1.38
not bad -0.54 -1.85
not very bad -1.36 -1.80
Table 3: Phrase and the sentiment scores of the phrase for
2 models Matrix-space OLogReg+BowInit and Bag-of-
words OLogReg respectively. Notice that relative rank-
ing order what matters
on matrices Matrix-space OLogReg+BowInit. By
sentiment score, we mean equation (1) of Bag-of-
words OLogReg and equation (2) of Matrix-space
OLogReg+BowInit.
Here we choose two popular adjectives like
?good? and ?bad? that appeared in the training data,
and examine the effect of applying the intensifier
?very? on the sentiment score. As we can see,
the matrix-space model learns a matrix that inten-
sifies both ?bad? and ?good? in the correct sentiment
scale, i.e., ?(good) < ?(very good) and ?(bad) <
?(very bad), while the bag-of-words model gets the
sentiment of ?very bad? wrong: it is more positive
than ?bad?. We also looked at the effect of combin-
ing ?not? with these adjectives. The matrix-space
model correctly encodes the effect of the negator
for both positive and negative adjectives, such that
?(not good) < ?(good) and ?(bad) < ?(not bad).
For the interesting case of applying a negator to a
phrase with an intensifier, ?(not good) should be
less than ?(not very good) and ?(not very bad)
should be less than ?(not bad).9 As shown in Ta-
ble 3, these are predicted correctly by the matrix-
space model, which the matrix-space model gets
right, but the bag-of-words model misses in the case
of ?bad?.
Also notice that since in the matrix-space model
9See the detailed discussion in Taboada et al (2011) and Liu
and Seneff (2009).
179
each word is represented as a function, more specif-
ically a linear operator, and the function composi-
tion defined as matrix multiplication, we can think
of ?not very? being an operator itself, that is a com-
position of operator ?not? and operator ?very?.
5 Related Work
Sentiment Analysis. There has been a lot of
research in determining the sentiment of words
and constructing polarity dictionaries (Hatzivas-
siloglou and McKeown, 1997; Wiebe, 2000; Rao
and Ravichandran, 2009; Mohammad et al, 2009;
Velikovich et al, 2010). Some recent work is try-
ing to identify the degree of sentiment of adjectives
and adverbs from text using co-occurrence statistics.
Work by Taboada et. al (2011) and Liu and Sen-
eff (2009), suggest ways of computing the sentiment
of adjectives from data, and computing the effect
of combining adjective with adverb as multiplica-
tive effect and combining adjective with negation as
additive effect. However these models require the
knowledge of a part of speech of given words and
the list of negators (since the negator is an adjective
as well). In our work we propose a single unified
model for handling all words of any part of speech.
On the other hand, there has been some research
in trying to model compositional effects for senti-
ment at the phrase- and sentence-level. Choi and
Cardie (2008) hand-code compositional rules in or-
der to model compositional effects of combining dif-
ferent words in the phrase. The hand-coded rules
are based on domain knowledge and used to learn
the effects of combining words in the phrase. An-
other recent work that tries to model the compo-
sitional semantics of combining different words is
Nakagawa et. al. (2010), which proposes a model
that learns the effects of combining different words
using phrase/sentence dependency parse trees and an
initial polarity dictionary. They present a learning
method that employs hidden variables for sentiment
classification: given the polarity of a sentence and
the a priori polarities of its words, they learn how
to model the interactions between words with head-
modifier relations in the dependency tree.
Some of the previous work looked at MPQA
phrase-level classification. Wilson et al (2004) tack-
les the problem of classifying clauses according to
their subjective strength but not polarity; Wilson et
al. (2005) classifies phrases according to their po-
larity/sentiment but not strength. Our task is differ-
ent: we classify phrases according to a single ordinal
scale that combines both polarity and strength.
Task of predicting document-level star ratings was
considered in (Pang and Lee, 2005; Goldberg and
Zhu, 2006). In the current work we look at fine-
grained sentiment analysis, more specifically we
study word representations for use in true compo-
sitional semantic settings.
Distributional Semantics and Compositional-
ity. Research in the area of distributional seman-
tics in NLP and Cognitive Science has looked at
different word representations and different ways of
combining words. Mitchell and Lapata (2010) pro-
pose a framework for vector-based semantic com-
position. They define composition as an additive
or multiplicative function of two vectors and show
that compositional approaches generally outperform
non-compositional approaches that treat the phrase
as the union of single lexical items.
Work by Baroni and Zamparelli (2010) models
nouns as vectors in some semantic space and ad-
jectives as matrices. It shows that modeling adjec-
tives as linear transformations and applying those
linear transformations to nouns results in final vec-
tors for adjective-noun compositions that are close
in semantic space to other similar phrases. The
authors argue that modeling adjectives as a linear
transformation is a better idea than using additive
vector-space models. In this work, a separate ma-
trix for each adjective is learned using the Par-
tial Least Squares method in a completely unsuper-
vised way. The recent paper by Rudolph and Gies-
brecht (2010), described in the introduction, argues
for multiplicative matrix-space models. In contrast
to other work in this area, our work is concerned
with a specific dimension of word meaning ? sen-
timent. Our techniques, however, are quite general
and should be applicable to other problems in lexical
semantics.
6 Conclusions and Future work
In the current work we present a novel matrix-space
model for ordinal scale sentiment prediction and an
algorithm for learning such a model. The proposed
180
model learns a matrix for each word; the composi-
tion of words is modeled as iterated matrix multi-
plication. The matrix-space framework with iterated
matrix multiplication defines an elegant framework
for modeling composition; it is also quite general.
We use the matrix-space framework in the context
of sentiment prediction, a domain where interesting
compositional effects can be observed. The main fo-
cus of this work was to study word representations
(represent as a single weight vs. as a matrix) for use
in true compositional semantic settings. One of the
benefits of the proposed approach is that by learn-
ing matrices for words, the model can handle unseen
word compositions (e.g. unseen bigrams) when the
unigrams involved have been seen.
However, it is not trivial to learn a matrix-space
model. Since the final optimization problem is non-
convex, the initialization has to be done carefully.
Here the weights learned in bag-of-words model
come to rescue and provide good initial point for op-
timization procedure. The final model outperforms
the bag-of-words based model, which suggests that
this research direction is very promising.
Though in our model the order of composition is
the same as the word order, we believe that a linguis-
tically informed order of composition can give us
further performance gains. For example, one can use
the output of a dependency parser to guide the order
of composition, similar to Nakagawa et al (2010).
Another possibility for improvement is to use the in-
formation about the scope of negation. In the current
work we assume the scope of negation to be the ex-
pression following the negation; in reality, however,
determining the scope of negation is a complex lin-
guistic phenomenon (Moilanen and Pulman, 2007).
So the proposed model can benefit from identify-
ing the scope of negation, similar to (Councill et al,
2010).
Also we plan to consider other ways to initialize
the matrix-space model. One interesting direction to
explore might be to use non-negative matrix factor-
ization (Lee and Seung, 2001), co-clustering tech-
niques (Dhillon, 2001) to better initialize words that
share similar contexts. The other possible direction
is to use existing sentiment lexicons and employ-
ing a ?curriculum learning? strategy (Bengio et al,
2009; Kumar et al, 2010) for our learning problem.
Acknowledgments
This work was supported in part by National Science
Foundation Grants BCS-0904822, BCS-0624277,
IIS-0968450; and by a gift from Google. We thank
the anonymous reviewers, and David Bindel, Nikos
Karampatziakis, Lillian Lee and Cornell NLP group
for useful suggestions and insightful discussions.
References
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: representing
adjective-noun constructions in semantic space. In
Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, EMNLP
?10, pages 1183?1193, Morristown, NJ, USA. Asso-
ciation for Computational Linguistics.
Yoshua Bengio, Je?ro?me Louradour, Ronan Collobert, and
Jason Weston. 2009. Curriculum learning. In Pro-
ceedings of the 26th Annual International Conference
on Machine Learning, ICML ?09. ACM.
R. H. Byrd, P. Lu, and J. Nocedal. 1995. A limited
memory algorithm for bound constrained optimiza-
tion. SIAM Journal on Scientific and Statistical Com-
puting, pages 1190?1208.
Yejin Choi and Claire Cardie. 2008. Learning with com-
positional semantics as structural inference for subsen-
tential sentiment analysis. In Empirical Methods in
Natural Language Processing (EMNLP).
Isaac G. Councill, Ryan McDonald, and Leonid Ve-
likovich. 2010. What?s great and what?s not: learn-
ing to classify the scope of negation for improved sen-
timent analysis. In Proceedings of the Workshop on
Negation and Speculation in Natural Language Pro-
cessing, NeSp-NLP ?10, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Koby Crammer and Yoram Singer. 2001. Pranking with
ranking. In Advances in Neural Information Process-
ing Systems 14, pages 641?647. MIT Press.
Marie-Catherine de Marneffe, Christopher D. Manning,
and Christopher Potts. 2010. Was it good? It was
provocative. learning the meaning of scalar adjectives.
In Proceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics, Uppsala, Swe-
den, July 11?16. ACL.
I. S. Dhillon. 2001. Co-clustering documents and words
using bipartite spectral graph partitioning. In KDD.
Andrew B. Goldberg and Jerry Zhu. 2006. Seeing
stars when there aren?t many stars: Graph-based semi-
supervised learning for sentiment categorization. In
HLT-NAACL Workshop on Textgraphs: Graph-based
Algorithms for Natural Language Processing.
181
James W. Hardin and Joseph Hilbe. 2007. Generalized
Linear Models and Extensions. Stata Press.
Vasileios Hatzivassiloglou and Kathleen R. McKeown.
1997. Predicting the semantic orientation of adjec-
tives. In EACL, pages 174?181.
Alistair Kennedy and Diana Inkpen. 2006. Sentiment
classification of movie reviews using contextual va-
lence shifters. Computational Intelligence, 22(2, Spe-
cial Issue on Sentiment Analysis)):110?125.
M. Pawan Kumar, Benjamin Packer, and Daphne Koller.
2010. Self-paced learning for latent variable models.
In Advances in Neural Information Processing Sys-
tems 23. NIPS.
D. Lee and H. Seung. 2001. Algorithms for non-negative
matrix factorization. In NIPS.
Jingjing Liu and Stephanie Seneff. 2009. Review sen-
timent scoring via a parse-and-paraphrase paradigm.
In Proceedings of the 2009 Conference on Empiri-
cal Methods in Natural Language Processing, pages
161?169, Singapore, August. Association for Compu-
tational Linguistics.
Jeff Mitchell and Mirella Lapata. 2010. Composition in
distributional models of semantics. Cognitive Science,
34(8):1388?1429.
Saif Mohammad, Cody Dunne, and Bonnie Dorr. 2009.
Generating high-coverage semantic orientation lexi-
cons from overtly marked words and a thesaurus.
In Proceedings of the 2009 Conference on Empiri-
cal Methods in Natural Language Processing, pages
599?608, Singapore, August. Association for Compu-
tational Linguistics.
Karo Moilanen and Stephen Pulman. 2007. Sentiment
composition. In Proceedings of Recent Advances in
Natural Language Processing (RANLP 2007), pages
378?382, September 27-29.
Tetsuji Nakagawa, Kentaro Inui, and Sadao Kurohashi.
2010. Dependency tree-based sentiment classification
using crfs with hidden variables. In Conference of the
North American Chapter of the Association for Com-
putational Linguistics (NAACL).
Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting
class relationships for sentiment categorization with
respect to rating scales. In Proceedings of the ACL,
pages 115?124.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and Trends in Infor-
mation Retrieval, 2(1-2):1?135.
K. B. Petersen and M. S. Pedersen. ?2008?. The Matrix
Cookbook. ?Technical University of Denmark?, ?oct?.
?Version 20081110?.
Livia Polanyi and Annie Zaenen. 2004. Contextual
lexical valence shifters. In Proceedings of the AAAI
Spring Symposium on Exploring Attitude and Affect in
Text: Theories and Applications.
Delip Rao and Deepak Ravichandran. 2009. Semi-
supervised polarity lexicon induction. In Proceedings
of the 12th Conference of the European Chapter of the
ACL (EACL 2009), pages 675?682, Athens, Greece,
March. Association for Computational Linguistics.
Sebastian Rudolph and Eugenie Giesbrecht. 2010. Com-
positional matrix-space models of language. In Pro-
ceedings of the 48th Annual Meeting of the Association
for Computational Linguistics, ACL ?10, pages 907?
916, Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Mostafa Shaikh, Helmut Prendinger, and Ishizuka Mit-
suru. 2007. Assessing sentiment of text by semantic
dependency and contextual valence analysis.
Maite Taboada, Julian Brooke, Milan Tofiloskiy, and
Kimberly Vollz. 2011). Lexicon-based methods for
sentiment analysis. In Computational Linguistics.
Leonid Velikovich, Sasha Blair-Goldensohn, Kerry Han-
nan, and RyanMcDonald. 2010. The viability of web-
derived polarity lexicons. In Human Language Tech-
nologies: The 2010 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 777?785, Los Angeles, Cal-
ifornia, June. Association for Computational Linguis-
tics.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.
Annotating expressions of opinions and emotions in
language. Language Resources and Evaluation (for-
merly Computers and the Humanities), 39(2/3):164?
210.
Janyce M. Wiebe. 2000. Learning subjective adjectives
from corpora. In In AAAI, pages 735?740.
Theresa Wilson, Janyce Wiebe, and Rebecca Hwa. 2004.
Just how mad are you? In AAAI. AAAI.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In Empirical Methods in Natural
Language Processing (EMNLP).
182
Proceedings of the ACL 2010 Conference Short Papers, pages 336?341,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Automatically generating annotator rationales
to improve sentiment classification
Ainur Yessenalina Yejin Choi Claire Cardie
Department of Computer Science, Cornell University, Ithaca NY, 14853 USA
{ainur, ychoi, cardie}@cs.cornell.edu
Abstract
One of the central challenges in sentiment-
based text categorization is that not ev-
ery portion of a document is equally in-
formative for inferring the overall senti-
ment of the document. Previous research
has shown that enriching the sentiment la-
bels with human annotators? ?rationales?
can produce substantial improvements in
categorization performance (Zaidan et al,
2007). We explore methods to auto-
matically generate annotator rationales for
document-level sentiment classification.
Rather unexpectedly, we find the automat-
ically generated rationales just as helpful
as human rationales.
1 Introduction
One of the central challenges in sentiment-based
text categorization is that not every portion of
a given document is equally informative for in-
ferring its overall sentiment (e.g., Pang and Lee
(2004)). Zaidan et al (2007) address this prob-
lem by asking human annotators to mark (at least
some of) the relevant text spans that support each
document-level sentiment decision. The text spans
of these ?rationales? are then used to construct ad-
ditional training examples that can guide the learn-
ing algorithm toward better categorizationmodels.
But could we perhaps enjoy the performance
gains of rationale-enhanced learningmodels with-
out any additional human effort whatsoever (be-
yond the document-level sentiment label)? We hy-
pothesize that in the area of sentiment analysis,
where there has been a great deal of recent re-
search attentiongiven to various aspects of the task
(Pang and Lee, 2008), this might be possible: us-
ing existing resources for sentiment analysis, we
might be able to construct annotator rationales au-
tomatically.
In this paper, we explore a number of methods
to automatically generate rationales for document-
level sentiment classification. In particular, we in-
vestigate the use of off-the-shelf sentiment analy-
sis components and lexicons for this purpose. Our
approaches for generating annotator rationales can
be viewed as mostly unsupervised in that we do not
require manually annotated rationales for training.
Rather unexpectedly, our empirical results show
that automatically generated rationales (91.78%)
are just as good as human rationales (91.61%) for
document-level sentiment classification of movie
reviews. In addition, complementing the hu-
man annotator rationales with automatic rationales
boosts the performance even further for this do-
main, achieving 92.5% accuracy. We further eval-
uate our rationale-generation approaches on prod-
uct review data for which human rationales are not
available: here we find that even randomly gener-
ated rationales can improve the classification accu-
racy although rationales generated from sentiment
resources are not as effective as for movie reviews.
The rest of the paper is organized as follows.
We first briefly summarize the SVM-based learn-
ing approach of Zaidan et al (2007) that allows the
incorporation of rationales (Section 2). We next
introduce three methods for the automatic gener-
ation of rationales (Section 3). The experimental
results are presented in Section 4, followed by re-
lated work (Section 5) and conclusions (Section
6).
2 Contrastive Learning with SVMs
Zaidan et al (2007) first introduced the notion of
annotator rationales ? text spans highlighted by
human annotators as support or evidence for each
document-level sentiment decision. These ratio-
nales, of course, are only useful if the sentiment
categorization algorithm can be extended to ex-
ploit the rationales effectively. With this in mind,
Zaidan et al (2007) propose the following con-
336
trastive learning extension to the standard SVM
learning algorithm.
Let ~xi be movie review i, and let {~rij} be the
set of annotator rationales that support the posi-
tive or negative sentiment decision for ~xi. For each
such rationale~rij in the set, construct a contrastive
training example ~vij , by removing the text span
associated with the rationale ~rij from the original
review ~xi. Intuitively, the contrastive example ~vij
should not be as informative to the learning algo-
rithm as the original review ~xi, since one of the
supporting regions identified by the human anno-
tator has been deleted. That is, the correct learned
model should be less confident of its classifica-
tion of a contrastive example vs. the corresponding
original example, and the classification boundary
of the model should be modified accordingly.
Zaidan et al (2007) formulate exactly this intu-
ition as SVM constraints as follows:
(?i, j) : yi (~w~xi ? ~w~vij) ? ?(1 ? ?ij)
where yi ? {?1,+1} is the negative/positive sen-
timent label of document i, ~w is the weight vector,
? ? 0 controls the size of the margin between the
original examples and the contrastive examples,
and ?ij are the associated slack variables. After
some re-writing of the equations, the resulting ob-
jective function and constraints for the SVM are as
follows:
1
2 ||~w||
2 + C
?
i
?i + Ccontrast
?
ij
?ij (1)
subject to constraints:
(?i) : yi ~w ? ~xi ? 1 ? ?i, ?i ? 0
(?i, j) : yi ~w ? ~xij ? 1 ? ?ij ?ij ? 0
where ?i and ?ij are the slack variables for ~xi
(the original examples) and ~xij (~xij are named as
pseudo examples and defined as ~xij = ~xi?~vij? ), re-
spectively. Intuitively, the pseudo examples (~xij)
represent the difference between the original ex-
amples (~xi) and the contrastive examples (~vij),
weighted by a parameter ?. C and Ccontrast are
parameters to control the trade-offs between train-
ing errors and margins for the original examples ~xi
and pseudo examples ~xij respectively. As noted in
Zaidan et al (2007),Ccontrast values are generally
smaller than C for noisy rationales.
In the work described below, we similarly em-
ploy Zaidan et al?s (2007) contrastive learning
method to incorporate rationales for document-
level sentiment categorization.
3 Automatically Generating Rationales
Our goal in the current work, is to generate anno-
tator rationales automatically. For this, we rely on
the following two assumptions:
(1) Regions marked as annotator rationales are
more subjective than unmarked regions.
(2) The sentiment of each annotator rationale co-
incides with the document-level sentiment.
Note that assumption 1 was not observed in the
Zaidan et al (2007) work: annotators were asked
only to mark a few rationales, leaving other (also
subjective) rationale sections unmarked.
And at first glance, assumption (2) might seem
too obvious. But it is important to include as there
can be subjective regions with seemingly conflict-
ing sentiment in the same document (Pang et al,
2002). For instance, an author for a movie re-
view might express a positive sentiment toward
the movie, while also discussing a negative sen-
timent toward one of the fictional characters ap-
pearing in the movie. This implies that not all sub-
jective regions will be relevant for the document-
level sentiment classification ? rather only those
regions whose polarity matches that of the docu-
ment should be considered.
In order to extract regions that satisfy the above
assumptions, we first look for subjective regions
in each document, then filter out those regions that
exhibit a sentiment value (i.e., polarity) that con-
flicts with polarity of the document. Assumption
2 is important as there can be subjective regions
with seemingly conflicting sentiment in the same
document (Pang et al, 2002).
Because our ultimate goal is to reduce human
annotation effort as much as possible, we do not
employ supervised learning methods to directly
learn to identify good rationales from human-
annotated rationales. Instead, we opt for methods
that make use of only the document-level senti-
ment and off-the-shelf utilities that were trained
for slightly different sentiment classification tasks
using a corpus from a different domain and of a
different genre. Although such utilities might not
be optimal for our task, we hoped that these ba-
sic resources from the research community would
constitute an adequate source of sentiment infor-
mation for our purposes.
We next describe three methods for the auto-
matic acquisition of rationales.
337
3.1 Contextual Polarity Classification
The first approach employs OpinionFinder (Wil-
son et al, 2005a), an off-the-shelf opinion anal-
ysis utility.1 In particular, OpinionFinder identi-
fies phrases expressing positive or negative opin-
ions. Because OpinionFinder models the task as
a word-based classification problem rather than a
sequence tagging task, most of the identified opin-
ion phrases consist of a single word. In general,
such short text spans cannot fully incorporate the
contextual information relevant to the detection of
subjective language (Wilson et al, 2005a). There-
fore, we conjecture that good rationales should ex-
tend beyond short phrases.2 For simplicity, we
choose to extend OpinionFinder phrases to sen-
tence boundaries.
In addition, to be consistentwith our second op-
erating assumption, we keep only those sentences
whose polarity coincides with the document-level
polarity. In sentences where OpinionFindermarks
multiple opinion words with opposite polarities
we perform a simple voting ? if words with pos-
itive (or negative) polarity dominate, then we con-
sider the entire sentence as positive (or negative).
We ignore sentences with a tie. Each selected sen-
tence is considered as a separate rationale.
3.2 Polarity Lexicons
Unfortunately, domain shift as well as task mis-
match could be a problem with any opinion util-
ity based on supervised learning.3 Therefore, we
next consider an approach that does not rely on su-
pervised learning techniques but instead explores
the use of a manually constructed polarity lexicon.
In particular, we use the lexicon constructed for
Wilson et al (2005b), which contains about 8000
words. Each entry is assigned one of three polarity
values: positive, negative, neutral. We construct
rationales from the polarity lexicon for every in-
stance of positive and negative words in the lexi-
con that appear in the training corpus.
As in the OpinionFinder rationales, we extend
the words found by the PolarityLexicon approach
to sentence boundaries to incorporate potentially
1Available at www.cs.pitt.edu/mpqa/opinionfinderrelease/.
2This conjecture is indirectly confirmed by the fact that
human-annotated rationales are rarely a single word.
3It is worthwhile to note that OpinionFinder is trained on a
newswire corpus whose prevailing sentiment is known to be
negative (Wiebe et al, 2005). Furthermore, OpinionFinder
is trained for a task (word-level sentiment classification) that
is different from marking annotator rationales (sequence tag-
ging or text segmentation).
relevant contextual information. We retain as ra-
tionales only those sentences whose polarity co-
incides with the document-level polarity as deter-
mined via the voting scheme of Section 3.1.
3.3 Random Selection
Finally, we generate annotator rationales ran-
domly, selecting 25% of the sentences from each
document4 and treating each as a separate ratio-
nale.
3.4 Comparison of Automatic vs.
Human-annotated Rationales
Before evaluating the performance of the au-
tomatically generated rationales, we summarize
in Table 1 the differences between automatic
vs. human-generated rationales. All computa-
tions were performed on the same movie review
dataset of Pang and Lee (2004) used in Zaidan et
al. (2007). Note, that the Zaidan et al (2007) an-
notation guidelines did not insist that annotators
mark all rationales, only that some were marked
for each document. Nevertheless, we report pre-
cision, recall, and F-score based on overlap with
the human-annotated rationales of Zaidan et al
(2007), so as to demonstrate the degree to which
the proposed approaches align with human intu-
ition. Overlap measures were also employed by
Zaidan et al (2007).
As shown in Table 1, the annotator rationales
found by OpinionFinder (F-score 49.5%) and the
PolarityLexicon approach (F-score 52.6%) match
the human rationales much better than those found
by random selection (F-score 27.3%).
As expected, OpinionFinder?s positive ratio-
nales match the human rationales at a significantly
lower level (F-score 31.9%) than negative ratio-
nales (59.5%). This is due to the fact that Opinion-
Finder is trained on a dataset biased toward nega-
tive sentiment (see Section 3.1 - 3.2). In contrast,
all other approaches show a balanced performance
for positive and negative rationales vs. human ra-
tionales.
4 Experiments
For our contrastive learning experiments we use
SVM light (Joachims, 1999). We evaluate the use-
fulness of automatically generated rationales on
4We chose the value of 25% to match the percentage of
sentences per document, on average, that contain human-
annotated rationales in our dataset (24.7%).
338
% of sentences Precision Recall F-Score
Method selected ALL POS NEG ALL POS NEG ALL POS NEG
OPINIONFINDER 22.8% 54.9 56.1 54.6 45.1 22.3 65.3 49.5 31.9 59.5
POLARITYLEXICON 38.7% 45.2 42.7 48.5 63.0 71.8 55.0 52.6 53.5 51.6
RANDOM 25.0% 28.9 26.0 31.8 25.9 24.9 26.7 27.3 25.5 29.0
Table 1: Comparison of Automatic vs. Human-annotated Rationales.
five different datasets. The first is the movie re-
view data of Pang and Lee (2004), which was
manually annotated with rationales by Zaidan et
al. (2007)5; the remaining are four product re-
view datasets from Blitzer et al (2007).6 Only
the movie review dataset contains human annota-
tor rationales. We replicate the same feature set
and experimental set-up as in Zaidan et al (2007)
to facilitate comparison with their work.7
The contrastive learning method introduced in
Zaidan et al (2007) requires three parameters: (C,
?, Ccontrast). To set the parameters, we use a grid
search with step 0.1 for the range of values of each
parameter around the point (1,1,1). In total, we try
around 3000 different parameter triplets for each
type of rationales.
4.1 Experiments with the Movie Review Data
We follow Zaidan et al (2007) for the training/test
data splits. The top half of Table 2 shows the
performance of a system trained with no anno-
tator rationales vs. two variations of human an-
notator rationales. HUMANR treats each rationale
in the same way as Zaidan et al (2007). HU-
MANR@SENTENCE extends the human annotator
rationales to sentence boundaries, and then treats
each such sentence as a separate rationale. As
shown in Table 2, we get alost the same per-
formance from these two variations (91.33% and
91.61%).8 This result demonstrates that locking
rationales to sentence boundaries was a reasonable
5Available at http://www.cs.jhu.edu/?ozaidan/rationales/.
6http://www.cs.jhu.edu/?mdredze/datasets/sentiment/.
7We use binary unigram features corresponding to the un-
stemmed words or punctuation marks with count greater or
equal to 4 in the full 2000 documents, then we normalize the
examples to the unit length. When computing the pseudo ex-
amples ~xij = ~xi?~vij? we first compute (~xi ? ~vij) using the
binary representation. As a result, features (unigrams) that
appeared in both vectors will be zeroed out in the resulting
vector. We then normalize the resulting vector to a unit vec-
tor.
8The performance of HUMANR reported by Zaidan et al
(2007) is 92.2% which lies between the performance we get
(91.61%) and the oracle accuracy we get if we knew the best
parameters for the test set (92.67%).
Method Accuracy
NORATIONALES 88.56
HUMANR 91.61?
HUMANR@SENTENCE 91.33? ?
OPINIONFINDER 91.78? ?
POLARITYLEXICON 91.39? ?
RANDOM 90.00?
OPINIONFINDER+HUMANR@SENTENCE 92.50? 4
Table 2: Experimental results for the movie
review data.
? The numbers marked with ? (or ?) are statistically
significantly better than NORATIONALES according to a
paired t-test with p < 0.001 (or p < 0.01).
? The numbers marked with 4 are statistically significantly
better than HUMANR according to a paired t-test with
p < 0.01.
? The numbers marked with ? are not statistically signifi-
cantly worse than HUMANR according to a paired t-test with
p > 0.1.
choice.
Among the approaches that make use of only
automatic rationales (bottom half of Table 2), the
best is OPINIONFINDER, reaching 91.78% accu-
racy. This result is slightly better than results
exploiting human rationales (91.33-91.61%), al-
though the difference is not statistically signifi-
cant. This result demonstrates that automatically
generated rationales are just as good as human
rationales in improving document-level sentiment
classification. Similarly strong results are ob-
tained from the POLARITYLEXICON as well.
Rather unexpectedly, RANDOM also achieves
statistically significant improvement over NORA-
TIONALES (90.0% vs. 88.56%). However, notice
that the performance of RANDOM is statistically
significantly lower than those based on human ra-
tionales (91.33-91.61%).
In our experiments so far, we observed that
some of the automatic rationales are just as
good as human rationales in improving the
document-level sentiment classification. Could
we perhaps achieve an even better result if we
combine the automatic rationales with human
339
rationales? The answer is yes! The accuracy
of OPINIONFINDER+HUMANR@SENTENCE
reaches 92.50%, which is statistically signifi-
cantly better than HUMANR (91.61%). In other
words, not only can our automatically generated
rationales replace human rationales, but they can
also improve upon human rationales when they
are available.
4.2 Experiments with the Product Reviews
We next evaluate our approaches on datasets for
which human annotator rationales do not exist.
For this, we use some of the product review data
from Blitzer et al (2007): reviews for Books,
DVDs, Videos and Kitchen appliances. Each
dataset contains 1000 positive and 1000 negative
reviews. The reviews, however, are substantially
shorter than those in the movie review dataset:
the average number of sentences in each review
is 9.20/9.13/8.12/6.37 respectively vs. 30.86 for
the movie reviews. We perform 10-fold cross-
validation, where 8 folds are used for training, 1
fold for tuning parameters, and 1 fold for testing.
Table 3 shows the results. Rationale-based
methods perform statistically significantly bet-
ter than NORATIONALES for all but the Kitchen
dataset. An interesting trend in product re-
view datasets is that RANDOM rationales are just
as good as other more sophisticated rationales.
We suspect that this is because product reviews
are generally shorter and more focused than the
movie reviews, thereby any randomly selected
sentence is likely to be a good rationale. Quantita-
tively, subjective sentences in the product reviews
amount to 78% (McDonald et al, 2007), while
subjective sentences in the movie review dataset
are only about 25% (Mao and Lebanon, 2006).
4.3 Examples of Annotator Rationales
In this section, we examine an example to com-
pare the automatically generated rationales (using
OPINIONFINDER) with human annotator ratio-
nales for the movie review data. In the following
positive document snippet, automatic rationales
are underlined, while human-annotated ratio-
nales are in bold face.
...But a little niceness goes a long way these days, and
there?s no denying the entertainment value of that thing
you do! It?s just about impossible to hate. It?s an
inoffensive, enjoyable piece ofnostalgia that is sure to leave
audiences smiling and humming, if not singing, ?that thing
you do!? ?quite possibly for days...
Method Books DVDs Videos Kitchen
NORATIONALES 80.20 80.95 82.40 87.40
OPINIONFINDER 81.65? 82.35? 84.00? 88.40
POLARITYLEXICON 82.75? 82.85? 84.55? 87.90
RANDOM 82.05? 82.10? 84.15? 88.00
Table 3: Experimental results for subset of
Product Review data
? The numbers marked with ? (or ?) are statistically
significantly better than NORATIONALES according to a
paired t-test with p < 0.05 (or p < 0.08).
Notice that, although OPINIONFINDER misses
some human rationales, it avoids the inclusion of
?impossible to hate?, which contains only negative
terms and is likely to be confusing for the con-
trastive learner.
5 Related Work
In broad terms, constructing annotator rationales
automatically and using them to formulate con-
trastive examples can be viewed as learning with
prior knowledge (e.g., Schapire et al (2002), Wu
and Srihari (2004)). In our task, the prior knowl-
edge corresponds to our operating assumptions
given in Section 3. Those assumptions can be
loosely connected to recognizing and exploiting
discourse structure (e.g., Pang and Lee (2004),
Taboada et al (2009)). Our automatically gener-
ated rationales can be potentially combined with
other learning frameworks that can exploit anno-
tator rationales, such as Zaidan and Eisner (2008).
6 Conclusions
In this paper, we explore methods to automatically
generate annotator rationales for document-level
sentiment classification. Our study is motivated
by the desire to retain the performance gains of
rationale-enhanced learning models while elimi-
nating the need for additional human annotation
effort. By employing existing resources for sen-
timent analysis, we can create automatic annota-
tor rationales that are as good as human annotator
rationales in improving document-level sentiment
classification.
Acknowledgments
We thank anonymous reviewers for their comments. This
work was supported in part by National Science Founda-
tion Grants BCS-0904822, BCS-0624277, IIS-0535099 and
by the Department of Homeland Security under ONR Grant
N0014-07-1-0152.
340
References
John Blitzer, Mark Dredze, and Fernando Pereira. 2007. Bi-
ographies, bollywood, boom-boxes and blenders: Domain
adaptation for sentiment classification. In Proceedings of
the 45th Annual Meeting of the Association of Computa-
tional Linguistics, pages 440?447, Prague, Czech Repub-
lic, June. Association for Computational Linguistics.
Thorsten Joachims. 1999. Making large-scale support vector
machine learning practical. pages 169?184.
Yi Mao and Guy Lebanon. 2006. Sequential models for sen-
timent prediction. In Proceedings of the ICML Workshop:
Learning in Structured Output Spaces Open Problems in
Statistical Relational Learning Statistical Network Analy-
sis: Models, Issues and New Directions.
Ryan McDonald, Kerry Hannan, Tyler Neylon, Mike Wells,
and Jeff Reynar. 2007. Structured models for fine-to-
coarse sentiment analysis. In Proceedings of the 45th
Annual Meeting of the Association of Computational Lin-
guistics, pages 432?439, Prague, Czech Republic, June.
Association for Computational Linguistics.
Bo Pang and Lillian Lee. 2004. A sentimental education:
sentiment analysis using subjectivity summarization based
on minimum cuts. In ACL ?04: Proceedings of the 42nd
Annual Meeting on Association for Computational Lin-
guistics, page 271, Morristown, NJ, USA. Association for
Computational Linguistics.
Bo Pang and Lillian Lee. 2008. Opinion mining and senti-
ment analysis. Found. Trends Inf. Retr., 2(1-2):1?135.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002.
Thumbs up?: sentiment classification using machine
learning techniques. In EMNLP ?02: Proceedings of the
ACL-02 conference on Empirical methods in natural lan-
guage processing, pages 79?86, Morristown, NJ, USA.
Association for Computational Linguistics.
Robert E. Schapire, Marie Rochery, Mazin G. Rahim, and
Narendra Gupta. 2002. Incorporating prior knowledge
into boosting. In ICML ?02: Proceedings of the Nine-
teenth International Conference on Machine Learning,
pages 538?545, San Francisco, CA, USA. Morgan Kauf-
mann Publishers Inc.
Maite Taboada, Julian Brooke, and Manfred Stede. 2009.
Genre-based paragraph classification for sentiment anal-
ysis. In Proceedings of the SIGDIAL 2009 Conference,
pages 62?70, London, UK, September. Association for
Computational Linguistics.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.
Annotating expressions of opinions and emotions in lan-
guage. Language Resources and Evaluation, 1(2):0.
Theresa Wilson, Paul Hoffmann, Swapna Somasundaran, Ja-
son Kessler, Janyce Wiebe, Yejin Choi, Claire Cardie,
Ellen Riloff, and Siddharth Patwardhan. 2005a. Opinion-
finder: a system for subjectivity analysis. In Proceedings
of HLT/EMNLP on Interactive Demonstrations, pages 34?
35, Morristown, NJ, USA. Association for Computational
Linguistics.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005b.
Recognizing contextual polarity in phrase-level sentiment
analysis. In HLT-EMNLP ?05: Proceedings of the con-
ference on Human Language Technology and Empirical
Methods in Natural Language Processing, pages 347?
354, Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Xiaoyun Wu and Rohini Srihari. 2004. Incorporating
prior knowledgewith weighted margin support vector ma-
chines. In KDD ?04: Proceedings of the tenth ACM
SIGKDD international conference on Knowledge discov-
ery and data mining, pages 326?333, New York, NY,
USA. ACM.
Omar F. Zaidan and Jason Eisner. 2008. Modeling anno-
tators: a generative approach to learning from annotator
rationales. In EMNLP ?08: Proceedings of the Confer-
ence on Empirical Methods in Natural LanguageProcess-
ing, pages 31?40, Morristown, NJ, USA. Association for
Computational Linguistics.
Omar F. Zaidan, Jason Eisner, and Christine Piatko. 2007.
Using ?annotator rationales? to improve machine learning
for text categorization. In NAACLHLT 2007; Proceedings
of the Main Conference, pages 260?267, April.
341
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 601?610,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Computational Approaches to Sentence Completion
Geoffrey Zweig, John C. Platt
Christopher Meek
Christopher J.C. Burges
Microsoft Research
Redmond, WA 98052
Ainur Yessenalina
Cornell University
Computer Science Dept.
Ithaca, NY 14853
Qiang Liu
Univ. of California, Irvine
Info. & Comp. Sci.
Irvine, California 92697
Abstract
This paper studies the problem of sentence-
level semantic coherence by answering SAT-
style sentence completion questions. These
questions test the ability of algorithms to dis-
tinguish sense from nonsense based on a vari-
ety of sentence-level phenomena. We tackle
the problem with two approaches: methods
that use local lexical information, such as the
n-grams of a classical language model; and
methods that evaluate global coherence, such
as latent semantic analysis. We evaluate these
methods on a suite of practice SAT questions,
and on a recently released sentence comple-
tion task based on data taken from five Conan
Doyle novels. We find that by fusing local
and global information, we can exceed 50%
on this task (chance baseline is 20%), and we
suggest some avenues for further research.
1 Introduction
In recent years, standardized examinations have
proved a fertile source of evaluation data for lan-
guage processing tasks. They are valuable for many
reasons: they represent facets of language under-
standing recognized as important by educational ex-
perts; they are organized in various formats designed
to evaluate specific capabilities; they are yardsticks
by which society measures educational progress;
and they affect a large number of people.
Previous researchers have taken advantage of this
material to test both narrow and general language
processing capabilities. Among the narrower tasks,
the identification of synonyms and antonyms has
been studied by (Landauer and Dumais, 1997; Mo-
hammed et al, 2008; Mohammed et al, 2011; Tur-
ney et al, 2003; Turney, 2008), who used ques-
tions from the Test of English as a Foreign Lan-
guage (TOEFL), Graduate Record Exams (GRE)
and English as a Second Language (ESL) exams.
Tasks requiring broader competencies include logic
puzzles and reading comprehension. Logic puzzles
drawn from the Law School Administration Test
(LSAT) and the GRE were studied in (Lev et al,
2004), which combined an extensive array of tech-
niques to solve the problems. The DeepRead sys-
tem (Hirschman et al, 1999) initiated a long line of
research into reading comprehension based on test
prep material (Charniak et al, 2000; Riloff and The-
len, 2000; Wang et al, 2000; Ng et al, 2000).
In this paper, we study a new class of problems
intermediate in difficulty between the extremes of
synonym detection and general question answer-
ing - the sentence completion questions found on
the Scholastic Aptitude Test (SAT). These questions
present a sentence with one or two blanks that need
to be filled in. Five possible words (or short phrases)
are given as options for each blank. All possible an-
swers except one result in a nonsense sentence. Two
examples are shown in Figure 1.
The questions are highly constrained in the sense
that all the information necessary is present in the
sentence itself without any other context. Neverthe-
less, they vary widely in difficulty. The first of these
examples is relatively simple: the second half of the
sentence is a clear description of the type of behavior
characterized by the desired adjective. The second
example is more sophisticated; one must infer from
601
1. One of the characters in Milton Murayama?s
novel is considered because he deliber-
ately defies an oppressive hierarchical society.
(A) rebellious (B) impulsive (C) artistic (D)
industrious (E) tyrannical
2. Whether substances are medicines or poisons
often depends on dosage, for substances that are
in small doses can be in large.
(A) useless .. effective
(B) mild .. benign
(C) curative .. toxic
(D) harmful .. fatal
(E) beneficial .. miraculous
Figure 1: Sample sentence completion questions
(Educational-Testing-Service, 2011).
the contrast between medicine and poison that the
correct answer involves a contrast, either useless vs.
effective or curative vs. toxic. Moreover, the first, in-
correct, possibility is perfectly acceptable in the con-
text of the second clause alone; only irrelevance to
the contrast between medicine and poison eliminates
it. In general, the questions require a combination of
semantic and world knowledge as well as occasional
logical reasoning. We study the sentence comple-
tion task because we believe it is complex enough to
pose a significant challenge, yet structured enough
that progress may be possible.
As a first step, we have approached the prob-
lem from two points-of-view: first by exploiting lo-
cal sentence structure, and secondly by measuring
a novel form of global sentence coherence based
on latent semantic analysis. To investigate the use-
fulness of local information, we evaluated n-gram
language model scores, from both a conventional
model with Good-Turing smoothing, and with a re-
cently proposed maximum-entropy class-based n-
gram model (Chen, 2009a; Chen, 2009b). Also
in the language modeling vein, but with potentially
global context, we evaluate the use of a recurrent
neural network language model. In all the language
modeling approaches, a model is used to compute a
sentence probability with each of the potential com-
pletions. To measure global coherence, we propose
a novel method based on latent semantic analysis
(LSA). We find that the LSA based method performs
best, and that both local and global information can
be combined to exceed 50% accuracy. We report re-
sults on a set of questions taken from a collection
of SAT practice exams (Princeton-Review, 2010),
and further validate the methods with the recently
proposed MSR Sentence Completion Challenge set
(Zweig and Burges, 2011).
Our paper thus makes the following contributions:
First, we present the first published results on the
SAT sentence completion task. Secondly, we eval-
uate the effectiveness of both local n-gram informa-
tion, and global coherence in the form of a novel
LSA-based metric. Finally, we illustrate that the lo-
cal and global information can be effectively fused.
The remainder of this paper is organized as fol-
lows. In Section 2 we discuss related work. Section
3 describes the language modeling methods we have
evaluated. Section 4 outlines the LSA-based meth-
ods. Section 5 presents our experimental results. We
conclude with a discussion in Section 6.
2 Related Work
The past work which is most similar to ours is de-
rived from the lexical substitution track of SemEval-
2007 (McCarthy and Navigli, 2007). In this task,
the challenge is to find a replacement for a word or
phrase removed from a sentence. In contrast to our
SAT-inspired task, the original answer is indicated.
For example, one might be asked to find alternates
for match in ?After the match, replace any remain-
ing fluid deficit to prevent problems of chronic de-
hydration throughout the tournament.? Two consis-
tently high-performing systems for this task are the
KU (Yuret, 2007) and UNT (Hassan et al, 2007)
systems. These operate in two phases: first they find
a set of potential replacement words, and then they
rank them. The KU system uses just an N-gram lan-
guage model to do this ranking. The UNT system
uses a large variety of information sources, and a
language model score receives the highest weight.
N-gram statistics were also very effective in (Giu-
liano et al, 2007). That paper also explores the use
of Latent Semantic Analysis to measure the degree
of similarity between a potential replacement and its
context, but the results are poorer than others. Since
the original word provides a strong hint as to the pos-
602
sible meanings of the replacements, we hypothesize
that N-gram statistics are largely able to resolve the
remaining ambiguities. The SAT sentence comple-
tion sentences do not have this property and thus are
more challenging.
Related to, but predating the Semeval lexical sub-
stitution task are the ESL synonym questions pro-
posed by Turney (2001), and subsequently consid-
ered by numerous research groups including Terra
and Clarke (2003) and Pado and Lapata (2007).
These questions are similar to the SemEval task, but
in addition to the original word and the sentence
context, the list of options is provided. Jarmasz and
Szpakowicz (2003) used a sophisticated thesaurus-
based method and achieved state-of-the art perfor-
mance, which is 82%.
Other work on standardized tests includes the syn-
onym and antonym tasks mentioned in Section 1,
and more recent work on a SAT analogy task in-
troduced by (Turney et al, 2003) and extensively
used by other researchers (Veale, 2004; Turney and
Littman, 2005; D. et al, 2009).
3 Sentence Completion via Language
Modeling
Perhaps the most straightforward approach to solv-
ing the sentence completion task is to form the com-
plete sentence with each option in turn, and to eval-
uate its likelihood under a language model. As
discussed in Section 2, this was found be be very
effective in the ranking phase of several SemEval
systems. In this section, we describe the suite of
state-of-the-art language modeling techniques for
which we will present results. We begin with n-
gram models; first a classical n-gram backoff model
(Chen and Goodman, 1999), and then a recently pro-
posed class-based maximum-entropy n-gram model
(Chen, 2009a; Chen, 2009b). N-gram models have
the obvious disadvantage of using a very limited
context in predicting word probabilities. There-
fore we evaluate the recurrent neural net model of
(Mikolov et al, 2010; Mikolov et al, 2011b). This
model has produced record-breaking perplexity re-
sults in several tasks (Mikolov et al, 2011a), and has
the potential to encode sentence-span information in
the network hidden-layer activations. We have also
evaluated the use of parse scores, using an off-the-
shelf stochastic context free grammar parser. How-
ever, the grammatical structure of the alternatives is
often identical. With scores differing only in the fi-
nal non-terminal/terminal rewrites, this did little bet-
ter than chance. The use of other syntactically de-
rived features, for example based on a dependency
parse, are likely to be more effective, but we leave
this for future work.
3.1 Backoff N-gram Language Model
Our baseline model is a Good-Turing smoothed
model trained with the CMU language modeling
toolkit (Clarkson and Rosenfeld, 1997). For the SAT
task, we used a trigram language model trained on
1.1B words of newspaper data, described in Section
5.1. All bigrams occurring at least twice were re-
tained in the model, along with all trigrams occur-
ring at least three times. The vocabulary consisted
of all words occurring at least 100 times in the data,
along with every word in the development or test
sets. This resulted in a 124k word vocabulary and
59M n-grams. For the Conan Doyle data, which we
henceforth refer to as the Holmes data (see Section
5.1), the smaller amount of training data allowed us
to use 4-grams and a vocabulary cutoff of 3. This re-
sulted in 26M n-grams and a 126k word vocabulary.
3.2 Maximum Entropy Class-Based N-gram
Language Model
Word-class information provides a level of abstrac-
tion which is not available in a word-level lan-
guage model; therefore we evaluated a state-of-the-
art class based language model. Model M (Chen,
2009a; Chen, 2009b) is a recently proposed class
based exponential n-gram language model which
has shown improvements across a variety of tasks
(Chen, 2009b; Chen et al, 2009; Emami et al,
2010). The key ideas are the modeling of word n-
gram probabilities with a maximum entropy model,
and the use of word-class information in the defini-
tion of the features. In particular, each word w is
assigned deterministically to a class c, allowing the
n-gram probabilities to be estimated as the product
of class and word parts
P (wi|wi?n+1 . . . wi?2wi?1) =
P (ci|ci?n+1 . . . ci?2ci?1, wi?n+1 . . . wi?2wi?1)
P (wi|wi?n+1 . . . wi?2wi?1, ci).
603
Both components are themselves maximum entropy
n-gram models in which the probability of a word
or class label l given history h is determined by
1
Z exp(
?
k fk(h, l)). The features fk(h, l) used are
the presence of various patterns in the concatena-
tion of hl, for example whether a particular suffix
is present in hl.
3.3 Recurrent Neural Net Language Model
Many of the questions involve long-range depen-
dencies between words. While n-gram models have
no ability to explicitly maintain long-span context,
the recently proposed recurrent neural-net model of
(Mikolov et al, 2010) does. Related approaches
have been proposed by (Sutskever et al, 2011;
Socher et al, 2011). In this model, a set of neu-
ral net activations s(t) is maintained and updated at
each sentence position t. These activations encapsu-
late the sentence history up to the tth word in a real-
valued vector which typically has several hundred
dimensions. The word at position t is represented as
a binary vector w(t) whose length is the vocabulary
size, and with a ?1? in a position uniquely associated
with the word, and ?0? elsewhere. w(t) and s(t) are
concatenated to predict an output distribution over
words, y(t). Updating is done with two weight ma-
trices u and v and nonlinear functions f() and g()
(Mikolov et al, 2011b):
x(t) = [w(t)T s(t ? 1)T ]T
sj(t) = f(
?
i
xi(t)uji)
yk(t) = g(
?
j
sj(t)vkj)
with f() being a sigmoid and g() a softmax:
f(x) =
1
1 + exp(?z)
, g(zm) =
exp(zm)
?
k exp(zk)
The output y(t) is a probability distribution over
words, and the parameters u and v are trained with
back-propagation to minimize the Kullback-Leibler
(KL) divergence between the predicted and observed
distributions. Because of the recurrent connections,
this model is similar to a nonlinear infinite impulse
response (IIR) filter, and has the potential to model
long span dependencies. Theoretical considerations
(Bengio et al, 1994) indicate that for many prob-
lems, this may not be possible, but in practice it is
an empirical question.
4 Sentence Completion via Latent
Semantic Analysis
Latent Semantic Analysis (LSA) (Deerwester et al,
1990) is a widely used method for representing
words and documents in a low dimensional vector
space. The method is based on applying singular
value decomposition (SVD) to a matrix W repre-
senting the occurrence of words in documents. SVD
results in an approximation of W by the product
of three matrices, one in which each word is rep-
resented as a low-dimensional vector, one in which
each document is represented as a low dimensional
vector, and a diagonal scaling matrix. The simi-
larity between two words can then be quantified as
the cosine-similarity between their respective scaled
vectors, and document similarity can be measured
likewise. It has been used in numerous tasks, rang-
ing from information retrieval (Deerwester et al,
1990) to speech recognition (Bellegarda, 2000; Coc-
caro and Jurafsky, 1998).
To perform LSA, one proceeds as follows. The
input is a collection of n documents which are ex-
pressed in terms of words from a vocabulary of size
m. These documents may be actual documents such
as newspaper articles, or simply as in our case no-
tional documents such as sentences. Next, a m x n
matrix W is formed. At its simplest, the ijth entry
contains the number of times word i has occurred in
document j - its term frequency or TF value. More
conventionally, the entry is weighted by some no-
tion of the importance of word i, for example the
negative logarithm of the fraction of documents that
contain it, resulting in a TF-IDF weighting (Salton
et al, 1975). Finally, to obtain a subspace represen-
tation of dimension d, W is decomposed as
W ? USV T
where U is m x d, V T is d x n, and S is a d x d diag-
onal matrix. In applications, d << n and d << m;
for example one might have a 50, 000 word vocab-
ulary and 1, 000, 000 documents and use a 300 di-
mensional subspace representation.
An important property of SVD is that the rows
of US - which represents the words - behave sim-
ilarly to the original rows of W , in the sense that
the cosine similarity between two rows in US ap-
proximates the cosine similarity between the corre-
604
sponding rows in W . Cosine similarity is defined as
sim(x,y) = x?y?x??y? .
4.1 Total Word Similarity
Perhaps the simplest way of doing sentence comple-
tion with LSA is to compute the total similarity of a
potential answer a with the rest of the words in the
sentence S, and to choose the most related option.
We define the total similarity as:
totsim(a,S) =
?
w?S
sim(a,w)
When the completion requires two words, total sim-
ilarity is the sum of the contributions for both words.
This is our baseline method for using LSA, and one
of the best methods we have found.
4.2 Sentence Reconstruction
Recall that LSA approximates a weighted word-
document matrix W as the product of low rank
matrices U and V along with a scaling matrix S:
W ? USV T . Using singular value decomposition,
this is done so as to minimize the mean square re-
construction error
?
ij Q
2
ij whereQ = W?USV
T .
From the basic definition of LSA, each column ofW
(representing a document) is represented as
Wj = USV Tj , (1)
that is, as a linear combination of the set of basis
functions formed by the columns of US, with the
combination weights specified in V Tj . When a new
document is presented, it is also possible to repre-
sent it in terms of the same basis vectors. Moreover,
we may take the reconstruction error induced by this
representation to be a measure of how consistent the
new document is with the original set of documents
used to determine U S and V (Bellegarda, 2000).
It remains to represent a new document in terms
of the LSA bases. This is done as follows (Deer-
wester et al, 1990; Bellegarda, 2000), again with
the objective of minimizing the reconstruction error.
First, note that since U is column-orthonormal, (1)
implies that
Vj = W Tj US
?1 (2)
Thus, if we notionally index a new document by p,
we proceed by forming a new column (document)
vector Wp using the standard term-weighting, and
then find its LSA-space representation Vp using (2).
We can evaluate the reconstruction quality by insert-
ing the result in (1). The reconstruction error is then
||(UUT ? I)Wp||2
Note that if all the dimensions are retained, the re-
construction error is zero; in the case that only the
highest singular vectors are used, however, it is not.
Due to the fact that the sentences vary in length we
choose the number of retained singular vectors as a
fraction f of the sentence length. If the answer has
n words we use the top nf components. In practice,
a f of 1.2 was selected on the basis of development
set results.
4.3 A LSA N-gram Language Model
In the context of speech recognition, LSA has been
combined with classical n-gram language models
in (Coccaro and Jurafsky, 1998; Bellegarda, 2000).
The crux of this idea is to interpolate an n-gram lan-
guage model probability with one based on LSA,
with the intuition that the standard n-gram model
will do a good job predicting function words, and
the LSA model will do a good job on words pre-
dicted by their long-span context. This logic makes
sense for the sentence completion task as well, mo-
tivating us to evaluate it.
To do this, we adopt the procedure of (Coccaro
and Jurafsky, 1998), using linear interpolation be-
tween the n-gram and LSA probabilities:
p(w|history) =
?png(w|history) + (1 ? ?)plsa(w|history)
The probability of a word given its history is com-
puted by the LSA model in the following way. Let h
be the sum of all the LSA word vectors in the his-
tory. Let m be the smallest cosine similarity be-
tween h and any word in the vocabulary V : m =
minw?V sim(h,w). The probability of a word w in
the context of history h is given by
Plsa(w|h) =
sim(h,w) ? m
?
q?V (sim(h, q) ? m)
Since similarity can be negative, subtracting the
minimum (m) ensures that all the estimated prob-
abilities are between 0 and 1.
605
4.4 Improving Efficiency and Expressiveness
Given the basic framework described above, a num-
ber of enhancements are possible. In terms of ef-
ficiency, recall that it is necessary to perform SVD
on a term-document matrix. The data we used was
grouped into paragraph ?documents,? of which there
were over 27 million, with 2.6 million unique words.
While the resulting matrix is highly sparse, it is nev-
ertheless impractical to perform SVD. We overcome
this difficulty in two ways. First, we restrict the set
of documents used to those which are ?relevant? to
a given test set. This is done by requiring that a doc-
ument contain at least one of the potential answer-
words. Secondly, we restrict the vocabulary to the
set of words present in the test set. For the sentence-
reconstruction method of Section 4.2, we have found
it convenient to do data selection per-sentence.
To enhance the expressive power of LSA, the term
vocabulary can be expanded from unigrams to bi-
grams or trigrams of words, thus adding information
about word ordering. This was also used in the re-
construction technique.
5 Experimental Results
5.1 Data Resources
We present results with two datasets. The first is
taken from 11 Practice Tests for the SAT & PSAT
2011 Edition (Princeton-Review, 2010). This book
contains eleven practice tests, and we used all the
sentence completion questions in the first five tests
as a development set, and all the questions in the last
six tests as the test set. This resulted in sets with 95
and 108 questions respectively. Additionally, we re-
port results on the recently released MSR Sentence
Completion Challenge (Zweig and Burges, 2011).
This consists of a set of 1, 040 sentence completion
questions based on sentences occurring in five Co-
nan Doyle Sherlock Holmes novels, and is identical
in format to the SAT questions. Due to the source of
this data, we refer to it as the Holmes data.
To train models, we have experimented with a
variety of data sources. Since there is no publi-
cally available collection of SAT questions suitable
to training, our methods have all relied on unsu-
pervised data. Early on, we ran a set of experi-
ments to determine the relevance of different types
of data. Thinking that data from an encyclopedia
Data Dev % Correct Test % Correct
Encarta 26 33
Wikipedia 32 31
LA Times 39 42
Table 1: Effectiveness of different types of training data.
might be useful, we evaluated an electronic version
of the 2003 Encarta encyclopedia, which has ap-
proximately 29M words. Along similar lines, we
used a collection of Wikipedia articles consisting of
709M words. This data is the entire Wikipedia as of
January 2011, broken down into sentences, with fil-
tering to remove sentences consisting of URLs and
Wiki author comments. Finally, we used a com-
mercial newspaper dataset consisting of all the Los
Angeles Times data from 1985 to 2002, containing
about 1.1B words. These data sources were evalu-
ated using the baseline n-gram LM approach of Sec-
tion 3.1. Initial experiments indicated that that the
Los Angeles Times data is best suited to this task
(see Table 1), and our SAT experiments use this
source. For the MSR Sentence Completion data,
we obtained the training data specified in (Zweig
and Burges, 2011), consisting of approximately 500
19th-century novels available from Project Guten-
berg, and comprising 48M words.
5.2 Human Performance
To provide human benchmark performance, we
asked six native speaking high school students and
five graduate students to answer the questions on the
development set. The high-schoolers attained 87%
accuracy and the graduate students 95%. Zweig and
Burges (2011) cite a human performance of 91%
on the Holmes data. Statistics from a large cross-
section of the population are not available. As a fur-
ther point of comparison, we note that chance per-
formance is 20%.
5.3 Language Modeling Results
Table 2 summarizes our language modeling results
on the SAT data. With the exception of the base-
line backoff n-gram model, these techniques were
too computationally expensive to utilize the full Los
Angeles Times corpus. Instead, as with LSA, a ?rel-
evant? corpus was selected of the sentences which
contain at least one answer option from either the
606
Method Data (Dev / Test) Dev Test
3-gram GT 1.1B / 1.1B 39% 42%
Model M 193M / 236M 35 41
RNN 36M / 44M 37 42
LSA-LM 293M / 358 M 48 44
Table 2: Performance of language modeling methods on
SAT questions.
Method Dev ppl Dev Test ppl Test
3-gram GT 195 36% 190 44%
Model M 178 36 175 42
RNN 147 37 144 42
Table 3: Performance of language modeling methods us-
ing identical training data and vocabularies.
development or test set. Separate subsets were made
for development and test data. This data was further
sub-sampled to obtain the training set sizes indicated
in the second column. For the LSA-LM, an interpo-
lation weight of 0.1 was used for the LSA score, de-
termined through optimization on the development
set. We see from this table that the language models
perform similarly and achieve just above 40% on the
test set.
To make a more controlled comparison that nor-
malizes for the amount of training data, we have
trained Model M, and the Good-Turing model on
the same data subset as the RNN, and with the same
vocabulary. In Table 3, we present perplexity re-
sults on a held-out set of dev/test-relevant Los Ange-
les Times data, and performance on the actual SAT
questions. Two things are notable. First, the re-
current neural net has dramatically lower perplexity
than the other methods. This is consistent with re-
sults in (Mikolov et al, 2011a). Secondly, despite
the differences in perplexity, the methods show little
difference on SAT performance. Because Model M
was not better, only uses n-gram context, and was
used in the construction of the Holmes data (Zweig
and Burges, 2011), we do not consider it further.
5.4 LSA Results
Table 4 presents results for the methods of Sections
4.1 and 4.2. Of all the methods in isolation, the sim-
ple approach of Section 4.1 - to use the total cosine
similarity between a potential answer and the other
words in the sentence - has performed best. The ap-
Method Dev Test
Total Word Similarity 46% 46%
Reconstruction Error 53 41
Table 4: SAT performance of LSA based methods.
Method Test
3-input LSA 46%
LSA + Good-Turing LM 53
LSA + Good-Turing LM + RNN 52
Table 5: SAT test set accuracy with combined methods.
proach of using reconstruction error performed very
well on the development set, but unremarkably on
the test set.
5.5 Combination Results
A well-known trick for obtaining best results from
a machine learning system is to combine a set of
diverse methods into a single ensemble (Dietterich,
2000). We use ensembles to get the highest accuracy
on both of our data sets.
We use a simple linear combination of the out-
puts of the other models discussed in this paper. For
the LSA model, the linear combination has three in-
puts: the total word similarity, the cosine similarity
between the sum of the answer word vectors and the
sum of the rest of sentence?s word vectors, and the
number of out-of-vocabulary terms in the answer.
Each additional language model beyond LSA con-
tributes an additional input: the probability of the
sentence under that language model.
We train the parameters of the linear combination
on the SAT development set. The training minimizes
a loss function of pairs of answers: one correct and
one incorrect fill-in from the same question. We use
the RankNet loss function (Burges et al, 2005):
min
~w
f(~w ? (~x ? ~y)) + ?||~w||2
where ~x are the input features for the incorrect an-
swer, ~y are the features for the correct answer, ~w
are the weights for the combination, and f(z) =
log(1 + exp(z)). We tune the regularizer via 5-
fold cross validation, and minimize the loss using
L-BFGS (Nocedal and Wright, 2006). The results
on the SAT test set for combining various models
are shown in Table 5.
607
5.6 Holmes Data Results
To measure the robustness of our approaches, we
have applied them to the MSR Sentence Completion
set (Zweig and Burges, 2011), termed the Holmes
data. In Table 6, we present the results on this set,
along with the comparable SAT results. Note that
the latter are derived from models trained with the
Los Angeles Times data, while the Holmes results
are derived from models trained with 19th-century
novels. We see from this table that the results are
similar across the two tasks. The best performing
single model is LSA total word similarity.
For the Holmes data, combining the models out-
performs any single model. We train the linear com-
bination function via 5-fold cross-validation: the
model is trained five times, each time on 3/5 of the
data, the regularization tuned on 1/5 of the data, and
tested on 1/5. The test results are pooled across all
5 folds and are shown in Table 6. In this case, the
best combination is to blend LSA, the Good-Turing
language model, and the recurrent neural network.
6 Discussion
To verify that the differences in accuracy between
the different algorithms are not statistical flukes, we
perform a statistical significance test on the out-
puts of each algorithm. We use McNemar?s test,
which is a matched test between two classifiers (Di-
etterich, 1998). We use the False Discovery Rate
method (Benjamini and Hochberg, 1995) to control
the false positive rate caused by multiple tests. If
we allow 2% of our tests to yield incorrectly false
results, then for the SAT data, the combination of
the Good-Turing smoothed language model with an
LSA-based global similarity model (52% accuracy)
is better that the baseline alone (42% accuracy).
Secondly, for the Holmes data, we can state that
LSA total similarity beats the recurrent neural net-
work, which in turn is better than the baseline n-
gram model. The combination of all three is sig-
nificantly better than any of the individual models.
To better understand the system performance and
gain insight into ways of improving it, we have ex-
amined the system?s errors. Encouragingly, one-
third of the errors involve single-word questions
which test the dictionary definition of a word. This
is done either by stating the definition, or provid-
Method SAT Holmes
Chance 20% 20%
GT N-gram LM 42 39
RNN 42 45
LSA Total Similarity 46 49
Reconstruction Error 41 41
LSA-LM 44 42
Combination 53 52
Human 87 to 95 91
Table 6: Performance of methods on the MSR Sentence
Completion Challenge, contrasted with SAT test set.
ing a stereotypical use of the word. An example of
the first case is: ?Great artists are often prophetic
(visual): they perceive what we cannot and antici-
pate the future long before we do.? (The system?s
incorrect answer is in parentheses.) An example
of the second is: ?One cannot help but be moved
by Theresa?s heartrending (therapeutic) struggle to
overcome a devastating and debilitating accident.?
At the other end of the difficulty spectrum are
questions involving world knowledge and/or logical
implications. An example requiring both is, ?Many
fear that the ratification (withdrawal) of more le-
nient tobacco advertising could be detrimental to
public health.? About 40% of the errors require this
sort of general knowledge to resolve. Based on our
analysis, we believe that future research could prof-
itably exploit the structured information present in
a dictionary. However, the ability to identify and
manipulate logical relationships and embed world
knowledge in a manner amenable to logical manip-
ulation may be necessary for a full solution. It is
an interesting research question if this could be done
implicitly with a machine learning technique, for ex-
ample recurrent or recursive neural networks.
7 Conclusion
In this paper we have investigated methods for
answering sentence-completion questions. These
questions are intriguing because they probe the abil-
ity to distinguish semantically coherent sentences
from incoherent ones, and yet involve no more con-
text than the single sentence. We find that both local
n-gram information and an LSA-based global coher-
ence model do significantly better than chance, and
that they can be effectively combined.
608
References
J. Bellegarda. 2000. Exploiting latent semantic informa-
tion in statistical language modeling. Proceedings of
the IEEE, 88(8).
Yoshua Bengio, Patrice Simard, and Paolo Frasconi.
1994. Learning long-term dependencies with gradi-
ent descent is difficult. IEEE Transactions on Neural
Networks, 5(2):157 ?166.
Y. Benjamini and Y. Hochberg. 1995. Controlling the
fase discovery rate: a practical and powerful approach
to multiple testing. J. Royal Statistical Society B,
53(1):289?300.
C. Burges, T. Shaked., E. Renshaw, A. Lazier, M. Deeds,
N. Hamilton, and G. Hullender. 2005. Learning to
rank using gradient descent. In Proc. ICML, pages 89?
96.
Eugene Charniak, Yasemin Altun, Rodrigo de Salvo
Braz, Benjamin Garrett, Margaret Kosmala, Tomer
Moscovich, Lixin Pang, Changhee Pyo, Ye Sun,
Wei Wy, Zhongfa Yang, Shawn Zeller, and Lisa
Zorn. 2000. Reading comprehension programs in
a statistical-language-processing class. In Proceed-
ings of the 2000 ANLP/NAACL Workshop on Read-
ing comprehension tests as evaluation for computer-
based language understanding sytems - Volume 6,
ANLP/NAACL-ReadingComp ?00, pages 1?5. Asso-
ciation for Computational Linguistics.
Stanley Chen and Joshua Goodman. 1999. An empirical
study of smoothing techniques for language modeling.
Computer Speech and Language, 13(4):359?393.
S. Chen, L. Mangu, B. Ramabhadran, R. Sarikaya, and
A. Sethy. 2009. Scaling shrinkage-based language
models. In ASRU.
S. Chen. 2009a. Performance prediction for exponential
language models. In NAACL-HLT.
S. Chen. 2009b. Shrinking exponential language models.
In NAACL-HLT.
P.R. Clarkson and R. Rosenfeld. 1997. Statistical
language modeling using the CMU-Cambridge
Toolkit. In Proceedings ESCA Eurospeech,
http://www.speech.cs.cmu.edu/SLM/toolkit.html.
N. Coccaro and D. Jurafsky. 1998. Towards better in-
tegration of semantic predictors in statistical language
modeling. In Proceedings, ICSLP.
Bollegala D., Matsuo Y., and Ishizuka M. 2009. Measur-
ing the similarity between implicit semantic relations
from the web. InWorldWideWeb Conference (WWW).
S. Deerwester, S.T. Dumais, G.W. Furnas, T.K. Landauer,
and R. Harshman. 1990. Indexing by latent semantic
analysis. Journal of the American Society for Informa-
tion Science, 41(96).
T.G. Dietterich. 1998. Approximate statistical tests
for comparing supervised classification learning algo-
rithms. Neural Computation, 10:1895?1923.
T.G. Dietterich. 2000. Ensemble methods in machine
learning. In International Workshop on Multiple Clas-
sifier Systems, pages 1?15. Springer-Verlag.
Educational-Testing-Service. 2011.
https://satonlinecourse.collegeboard.com/sr/digital assets/
assessment/pdf/0833a611-0a43-10c2-0148-
cc8c0087fb06-f.pdf.
A. Emami, S. Chen, A. Ittycheriah, H. Soltau, and
B. Zhao. 2010. Decoding with shrinkage-based lan-
guage models. In Interspeech.
Claudio Giuliano, Alfio Gliozzo, and Carlo Strapparava.
2007. Fbk-irst: Lexical substitution task exploiting
domain and syntagmatic coherence. In Proceedings
of the 4th International Workshop on Semantic Evalu-
ations, SemEval ?07, pages 145?148, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Samer Hassan, Andras Csomai, Carmen Banea, Ravi
Sinha, and Rada Mihalcea. 2007. Unt: Subfinder:
Combining knowledge sources for automatic lexical
substitution. In Proceedings of the 4th International
Workshop on Semantic Evaluations, SemEval ?07,
pages 410?413, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Lynette Hirschman, Mark Light, Eric Breck, and John D.
Burger. 1999. Deep read: A reading comprehension
system. In Proceedings of the 37th Annual Meeting of
the Association for Computational Linguistics.
Thomas Landauer and Susan Dumais. 1997. A solution
to Plato?s problem: The latent semantic analysis the-
ory of the acquisition, induction, and representation of
knowledge. Psychological Review, 104(2), pages 211?
240.
Iddo Lev, Bill MacCartney, Christopher D. Manning, and
Roger Levy. 2004. Solving logic puzzles: from ro-
bust processing to precise semantics. In Proceedings
of the 2nd Workshop on Text Meaning and Interpreta-
tion, pages 9?16. Association for Computational Lin-
guistics.
Jarmasz M. and Szpakowicz S. 2003. Roget?s thesaurus
and semantic similarity. In Recent Advances in Natu-
ral Language Processing (RANLP).
Diana McCarthy and Roberto Navigli. 2007. Semeval-
2007 task 10: English lexical substitution task. In Pro-
ceedings of the 4th International Workshop on Seman-
tic Evaluations (SemEval-2007), pages 48?53.
Tomas Mikolov, Martin Karafiat, Jan Cernocky, and San-
jeev Khudanpur. 2010. Recurrent neural network
based language model. In Proceedings of Interspeech
2010.
609
Tomas Mikolov, Anoop Deoras, Stefan Kombrink, Lukas
Burget, and Jan Cernocky. 2011a. Empirical evalua-
tion and combination of advanced language modeling
techniques. In Proceedings of Interspeech 2011.
Tomas Mikolov, Stefan Kombrink, Lukas Burget, Jan
Cernocky, and Sanjeev Khudanpur. 2011b. Ex-
tensions of recurrent neural network based language
model. In Proceedings of ICASSP 2011.
Saif Mohammed, Bonnie Dorr, and Graeme Hirst. 2008.
Computing word pair antonymy. In Empirical Meth-
ods in Natural Language Processing (EMNLP).
Saif M. Mohammed, Bonnie J. Dorr, Graeme Hirst, and
Peter D. Turney. 2011. Measuring degrees of seman-
tic opposition. Technical report, National Research
Council Canada.
Hwee Tou Ng, Leong Hwee Teo, and Jennifer Lai Pheng
Kwan. 2000. A machine learning approach to answer-
ing questions for reading comprehension tests. In Pro-
ceedings of the 2000 Joint SIGDAT conference on Em-
pirical methods in natural language processing and
very large corpora: held in conjunction with the 38th
Annual Meeting of the Association for Computational
Linguistics - Volume 13, EMNLP ?00, pages 124?132.
J. Nocedal and S. Wright. 2006. Numerical Optimiza-
tion. Springer-Verlag.
Sebastian Pado and Mirella Lapata. 2007. Dependency-
based construction of semantic space models. Compu-
tational Linguistics, 33 (2), pages 161?199.
Princeton-Review. 2010. 11 Practice Tests for the SAT
& PSAT, 2011 Edition. The Princeton Review.
Ellen Riloff and Michael Thelen. 2000. A rule-based
question answering system for reading comprehension
tests. In Proceedings of the 2000 ANLP/NAACL Work-
shop on Reading comprehension tests as evaluation for
computer-based language understanding sytems - Vol-
ume 6, ANLP/NAACL-ReadingComp ?00, pages 13?
19.
G. Salton, A. Wong, and C. S. Yang. 1975. A Vector
Space Model for Automatic Indexing. Communica-
tions of the ACM, 18(11).
Richard Socher, Cliff Chiung-Yu Lin, Andrew Y. Ng,
and Christopher D. Manning. 2011. Parsing natural
scenes and natural language with recursive neural net-
works. In Proceedings of the 2011 International Con-
ference on Machine Learning (ICML-2011).
Ilya Sutskever, James Martens, and Geoffrey Hinton.
2011. Generating text with recurrent neural networks.
In Proceedings of the 2011 International Conference
on Machine Learning (ICML-2011).
E. Terra and C. Clarke. 2003. Frequency estimates for
statistical word similarity measures. In Conference
of the North American Chapter of the Association for
Computational Linguistics (NAACL).
Peter Turney and Michael Littman. 2005. Corpus-based
learning of analogies and semantic relations. Machine
Learning, 60 (1-3), pages 251?278.
Peter D. Turney, Michael L. Littman, Jeffrey Bigham,
and Victor Shnayder. 2003. Combining independent
modules to solve multiple-choice synonym and anal-
ogy problems. In Recent Advances in Natural Lan-
guage Processing (RANLP).
Peter D. Turney. 2001. Mining the web for synonyms:
PMI-IR versus LSA on TOEFL. In European Confer-
ence on Machine Learning (ECML).
Peter Turney. 2008. A uniform approach to analo-
gies, synonyms, antonyms, and associations. In In-
ternational Conference on Computational Linguistics
(COLING).
T. Veale. 2004. Wordnet sits the sat: A knowledge-based
approach to lexical analogy. In European Conference
on Artificial Intelligence (ECAI).
W. Wang, J. Auer, R. Parasuraman, I. Zubarev,
D. Brandyberry, and M. P. Harper. 2000. A ques-
tion answering system developed as a project in a
natural language processing course. In Proceed-
ings of the 2000 ANLP/NAACL Workshop on Read-
ing comprehension tests as evaluation for computer-
based language understanding sytems - Volume 6,
ANLP/NAACL-ReadingComp ?00, pages 28?35.
Deniz Yuret. 2007. Ku: word sense disambiguation
by substitution. In Proceedings of the 4th Interna-
tional Workshop on Semantic Evaluations, SemEval
?07, pages 207?213, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Geoffrey Zweig and Christopher J.C. Burges. 2011. The
Microsoft Research sentence completion challenge.
Technical Report MSR-TR-2011-129, Microsoft.
610
