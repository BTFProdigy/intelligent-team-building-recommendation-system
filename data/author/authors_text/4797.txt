Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 688?697, Prague, June 2007. c?2007 Association for Computational Linguistics
The Infinite PCFG using Hierarchical Dirichlet Processes
Percy Liang Slav Petrov Michael I. Jordan Dan Klein
Computer Science Division, EECS Department
University of California at Berkeley
Berkeley, CA 94720
{pliang, petrov, jordan, klein}@cs.berkeley.edu
Abstract
We present a nonparametric Bayesian model
of tree structures based on the hierarchical
Dirichlet process (HDP). Our HDP-PCFG
model allows the complexity of the grammar
to grow as more training data is available.
In addition to presenting a fully Bayesian
model for the PCFG, we also develop an ef-
ficient variational inference procedure. On
synthetic data, we recover the correct gram-
mar without having to specify its complex-
ity in advance. We also show that our tech-
niques can be applied to full-scale parsing
applications by demonstrating its effective-
ness in learning state-split grammars.
1 Introduction
Probabilistic context-free grammars (PCFGs) have
been a core modeling technique for many as-
pects of linguistic structure, particularly syntac-
tic phrase structure in treebank parsing (Charniak,
1996; Collins, 1999). An important question when
learning PCFGs is how many grammar symbols
to allocate to the learning algorithm based on the
amount of available data.
The question of ?how many clusters (symbols)??
has been tackled in the Bayesian nonparametrics
literature via Dirichlet process (DP) mixture mod-
els (Antoniak, 1974). DP mixture models have since
been extended to hierarchical Dirichlet processes
(HDPs) and HDP-HMMs (Teh et al, 2006; Beal et
al., 2002) and applied to many different types of
clustering/induction problems in NLP (Johnson et
al., 2006; Goldwater et al, 2006).
In this paper, we present the hierarchical Dirich-
let process PCFG (HDP-PCFG). a nonparametric
Bayesian model of syntactic tree structures based
on Dirichlet processes. Specifically, an HDP-PCFG
is defined to have an infinite number of symbols;
the Dirichlet process (DP) prior penalizes the use
of more symbols than are supported by the training
data. Note that ?nonparametric? does not mean ?no
parameters?; rather, it means that the effective num-
ber of parameters can grow adaptively as the amount
of data increases, which is a desirable property of a
learning algorithm.
As models increase in complexity, so does the un-
certainty over parameter estimates. In this regime,
point estimates are unreliable since they do not take
into account the fact that there are different amounts
of uncertainty in the various components of the pa-
rameters. The HDP-PCFG is a Bayesian model
which naturally handles this uncertainty. We present
an efficient variational inference algorithm for the
HDP-PCFG based on a structured mean-field ap-
proximation of the true posterior over parameters.
The algorithm is similar in form to EM and thus in-
herits its simplicity, modularity, and efficiency. Un-
like EM, however, the algorithm is able to take the
uncertainty of parameters into account and thus in-
corporate the DP prior.
Finally, we develop an extension of the HDP-
PCFG for grammar refinement (HDP-PCFG-GR).
Since treebanks generally consist of coarsely-
labeled context-free tree structures, the maximum-
likelihood treebank grammar is typically a poor
model as it makes overly strong independence as-
sumptions. As a result, many generative approaches
to parsing construct refinements of the treebank
grammar which are more suitable for the model-
ing task. Lexical methods split each pre-terminal
symbol into many subsymbols, one for each word,
and then focus on smoothing sparse lexical statis-
688
tics (Collins, 1999; Charniak, 2000). Unlexicalized
methods refine the grammar in a more conservative
fashion, splitting each non-terminal or pre-terminal
symbol into a much smaller number of subsymbols
(Klein and Manning, 2003; Matsuzaki et al, 2005;
Petrov et al, 2006). We apply our HDP-PCFG-GR
model to automatically learn the number of subsym-
bols for each symbol.
2 Models based on Dirichlet processes
At the heart of the HDP-PCFG is the Dirichlet pro-
cess (DP) mixture model (Antoniak, 1974), which is
the nonparametric Bayesian counterpart to the clas-
sical finite mixture model. In order to build up an
understanding of the HDP-PCFG, we first review
the Bayesian treatment of the finite mixture model
(Section 2.1). We then consider the DP mixture
model (Section 2.2) and use it as a building block
for developing nonparametric structured versions of
the HMM (Section 2.3) and PCFG (Section 2.4).
Our presentation highlights the similarities between
these models so that each step along this progression
reflects only the key differences.
2.1 Bayesian finite mixture model
We begin by describing the Bayesian finite mixture
model to establish basic notation that will carry over
the more complex models we consider later.
Bayesian finite mixture model
? ? Dirichlet(?, . . . , ?) [draw component probabilities]
For each component z ? {1, . . . ,K}:
??z ? G0 [draw component parameters]
For each data point i ? {1, . . . , n}:
?zi ? Multinomial(?) [choose component]
?xi ? F (?;?zi) [generate data point]
The model has K components whose prior dis-
tribution is specified by ? = (?1, . . . , ?K). The
Dirichlet hyperparameter ? controls how uniform
this distribution is: as ? increases, it becomes in-
creasingly likely that the components have equal
probability. For each mixture component z ?
{1, . . . ,K}, the parameters of the component ?z are
drawn from some prior G0. Given the model param-
eters (?,?), the data points are generated i.i.d. by
first choosing a component and then generating from
a data model F parameterized by that component.
In document clustering, for example, each data
point xi is a document represented by its term-
frequency vector. Each component (cluster) z
has multinomial parameters ?z which specifies a
distribution F (?;?z) over words. It is custom-
ary to use a conjugate Dirichlet prior G0 =
Dirichlet(??, . . . , ??) over the multinomial parame-
ters, which can be interpreted as adding ???1 pseu-
docounts for each word.
2.2 DP mixture model
We now consider the extension of the Bayesian finite
mixture model to a nonparametric Bayesian mixture
model based on the Dirichlet process. We focus
on the stick-breaking representation (Sethuraman,
1994) of the Dirichlet process instead of the stochas-
tic process definition (Ferguson, 1973) or the Chi-
nese restaurant process (Pitman, 2002). The stick-
breaking representation captures the DP prior most
explicitly and allows us to extend the finite mixture
model with minimal changes. Later, it will enable us
to readily define structured models in a form similar
to their classical versions. Furthermore, an efficient
variational inference algorithm can be developed in
this representation (Section 2.6).
The key difference between the Bayesian finite
mixture model and the DP mixture model is that
the latter has a countably infinite number of mixture
components while the former has a predefined K.
Note that if we have an infinite number of mixture
components, it no longer makes sense to consider
a symmetric prior over the component probabilities;
the prior over component probabilities must decay in
some way. The stick-breaking distribution achieves
this as follows. We write ? ? GEM(?) to mean
that ? = (?1, ?2, . . . ) is distributed according to the
stick-breaking distribution. Here, the concentration
parameter ? controls the number of effective com-
ponents. To draw ? ? GEM(?), we first generate
a countably infinite collection of stick-breaking pro-
portions u1, u2, . . . , where each uz ? Beta(1, ?).
The stick-breaking weights ? are then defined in
terms of the stick proportions:
?z = uz
?
z?<z
(1 ? uz?). (1)
The procedure for generating ? can be viewed as
iteratively breaking off remaining portions of a unit-
689
0 1?1 ?2 ?3 ...
Figure 1: A sample ? ? GEM(1).
length stick (Figure 1). The component probabilities
{?z} will decay exponentially in expectation, but
there is always some probability of getting a smaller
component before a larger one. The parameter ? de-
termines the decay of these probabilities: a larger ?
implies a slower decay and thus more components.
Given the component probabilities, the rest of the
DP mixture model is identical to the finite mixture
model:
DP mixture model
? ? GEM(?) [draw component probabilities]
For each component z ? {1, 2, . . . }:
??z ? G0 [draw component parameters]
For each data point i ? {1, . . . , n}:
?zi ? Multinomial(?) [choose component]
?xi ? F (?;?zi) [generate data point xn]
2.3 HDP-HMM
The next stop on the way to the HDP-PCFG is the
HDP hidden Markov model (HDP-HMM) (Beal et
al., 2002; Teh et al, 2006). An HMM consists of a
set of hidden states, where each state can be thought
of as a mixture component. The parameters of the
mixture component are the emission and transition
parameters. The main aspect that distinguishes it
from a flat finite mixture model is that the transi-
tion parameters themselves must specify a distribu-
tion over next states. Hence, we have not just one
top-level mixture model over states, but also a col-
lection of mixture models, one for each state.
In developing a nonparametric version of the
HMM in which the number of states is infinite, we
need to ensure that the transition mixture models
of each state share a common inventory of possible
next states. We can achieve this by tying these mix-
ture models together using the hierarchical Dirichlet
process (HDP) (Teh et al, 2006). The stick-breaking
representation of an HDP is defined as follows: first,
the top-level stick-breaking weights ? are drawn ac-
cording to the stick-breaking prior as before. Then,
a new set of stick-breaking weights ?? are generated
according based on ?:
?? ? DP(??,?), (2)
where the distribution of DP can be characterized
in terms of the following finite partition property:
for all partitions of the positive integers into sets
A1, . . . , Am,
(??(A1), . . . ,??(Am)) (3)
? Dirichlet
(
???(A1), . . . , ???(Am)
)
,
where ?(A) =
?
k?A ?k.
1 The resulting ?? is an-
other distribution over the positive integers whose
similarity to ? is controlled by a concentration pa-
rameter ??.
HDP-HMM
? ? GEM(?) [draw top-level state weights]
For each state z ? {1, 2, . . . }:
??Ez ? Dirichlet(?) [draw emission parameters]
??Tz ? DP(?
?, ?) [draw transition parameters]
For each time step i ? {1, . . . , n}:
?xi ? F (?;?Ezi) [emit current observation]
?zi+1 ? Multinomial(?Tzi) [choose next state]
Each state z is associated with emission param-
eters ?Ez . In addition, each z is also associated
with transition parameters ?Tz , which specify a dis-
tribution over next states. These transition parame-
ters are drawn from a DP centered on the top-level
stick-breaking weights ? according to Equations (2)
and (3). Assume that z1 is always fixed to a special
START state, so we do not need to generate it.
2.4 HDP-PCFG
We now present the HDP-PCFG, which is the focus
of this paper. For simplicity, we consider Chomsky
normal form (CNF) grammars, which has two types
of rules: emissions and binary productions. We con-
sider each grammar symbol as a mixture component
whose parameters are the rule probabilities for that
symbol. In general, we do not know the appropriate
number of grammar symbols, so our strategy is to
let the number of grammar symbols be infinite and
place a DP prior over grammar symbols.
1Note that this property is a specific instance of the general
stochastic process definition of Dirichlet processes.
690
HDP-PCFG
? ? GEM(?) [draw top-level symbol weights]
For each grammar symbol z ? {1, 2, . . . }:
??Tz ? Dirichlet(?
T ) [draw rule type parameters]
??Ez ? Dirichlet(?
E) [draw emission parameters]
??Bz ? DP(?
B ,??T ) [draw binary production parameters]
For each node i in the parse tree:
?ti ? Multinomial(?Tzi) [choose rule type]
?If ti = EMISSION:
??xi ? Multinomial(?Ezi) [emit terminal symbol]
?If ti = BINARY-PRODUCTION:
??(zL(i), zR(i)) ? Multinomial(?
B
zi) [generate children symbols]
?
?Bz
?Tz
?Ez
z ?
z1
z2
x2
z3
x3
T
Parameters Trees
Figure 2: The definition and graphical model of the HDP-PCFG. Since parse trees have unknown structure,
there is no convenient way of representing them in the visual language of traditional graphical models.
Instead, we show a simple fixed example tree. Node 1 has two children, 2 and 3, each of which has one
observed terminal child. We use L(i) and R(i) to denote the left and right children of node i.
In the HMM, the transition parameters of a state
specify a distribution over single next states; simi-
larly, the binary production parameters of a gram-
mar symbol must specify a distribution over pairs
of grammar symbols for its children. We adapt the
HDP machinery to tie these binary production distri-
butions together. The key difference is that now we
must tie distributions over pairs of grammar sym-
bols together via distributions over single grammar
symbols.
Another difference is that in the HMM, at each
time step, both a transition and a emission are made,
whereas in the PCFG either a binary production or
an emission is chosen. Therefore, each grammar
symbol must also have a distribution over the type
of rule to apply. In a CNF PCFG, there are only
two types of rules, but this can be easily generalized
to include unary productions, which we use for our
parsing experiments.
To summarize, the parameters of each grammar
symbol z consists of (1) a distribution over a finite
number of rule types ?Tz , (2) an emission distribu-
tion ?Ez over terminal symbols, and (3) a binary pro-
duction distribution ?Bz over pairs of children gram-
mar symbols. Figure 2 describes the model in detail.
Figure 3 shows the generation of the binary pro-
duction distributions ?Bz . We draw ?
B
z from a DP
centered on ??T , which is the product distribution
over pairs of symbols. The result is a doubly-infinite
matrix where most of the probability mass is con-
state
right child state
left child state
right child state
left child state
? ? GEM(?)
??T
?Bz ? DP(??
T )
Figure 3: The generation of binary production prob-
abilities given the top-level symbol probabilities ?.
First, ? is drawn from the stick-breaking prior, as
in any DP-based model (a). Next, the outer-product
??T is formed, resulting in a doubly-infinite matrix
matrix (b). We use this as the base distribution for
generating the binary production distribution from a
DP centered on ??T (c).
centrated in the upper left, just like the top-level dis-
tribution ??T .
Note that we have replaced the general
691
G0 and F (?Ezi) pair with Dirichlet(?
E) and
Multinomial(?Ezi) to specialize to natural language,
but there is no difficulty in working with parse
trees with arbitrary non-multinomial observations
or more sophisticated word models.
In many natural language applications, there is
a hard distinction between pre-terminal symbols
(those that only emit a word) and non-terminal sym-
bols (those that only rewrite as two non-terminal or
pre-terminal symbols). This can be accomplished
by letting ?T = (0, 0), which forces a draw ?Tz to
assign probability 1 to one rule type.
An alternative definition of an HDP-PCFG would
be as follows: for each symbol z, draw a distribution
over left child symbols lz ? DP(?) and an inde-
pendent distribution over right child symbols rz ?
DP(?). Then define the binary production distribu-
tion as their cross-product ?Bz = lzr
T
z . This also
yields a distribution over symbol pairs and hence de-
fines a different type of nonparametric PCFG. This
model is simpler and does not require any additional
machinery beyond the HDP-HMM. However, the
modeling assumptions imposed by this alternative
are unappealing as they assume the left child and
right child are independent given the parent, which
is certainly not the case in natural language.
2.5 HDP-PCFG for grammar refinement
An important motivation for the HDP-PCFG is that
of refining an existing treebank grammar to alle-
viate unrealistic independence assumptions and to
improve parsing accuracy. In this scenario, the set
of symbols is known, but we do not know how
many subsymbols to allocate per symbol. We in-
troduce the HDP-PCFG for grammar refinement
(HDP-PCFG-GR), an extension of the HDP-PCFG,
for this task.
The essential difference is that now we have a
collection of HDP-PCFG models for each symbol
s ? S, each one operating at the subsymbol level.
While these HDP-PCFGs are independent in the
prior, they are coupled through their interactions in
the parse trees. For completeness, we have also in-
cluded unary productions, which are essentially the
PCFG counterpart of transitions in HMMs. Finally,
since each node i in the parse tree involves a symbol-
subsymbol pair (si, zi), each subsymbol needs to
specify a distribution over both child symbols and
subsymbols. The former can be handled through
a finite Dirichlet distribution since all symbols are
known and observed, but the latter must be handled
with the Dirichlet process machinery, since the num-
ber of subsymbols is unknown.
HDP-PCFG for grammar refinement (HDP-PCFG-GR)
For each symbol s ? S:
??s ? GEM(?) [draw subsymbol weights]
?For each subsymbol z ? {1, 2, . . . }:
???Tsz ? Dirichlet(?
T ) [draw rule type parameters]
???Esz ? Dirichlet(?
E(s)) [draw emission parameters]
???usz ? Dirichlet(?
u) [unary symbol productions]
???bsz ? Dirichlet(?
b) [binary symbol productions]
??For each child symbol s? ? S:
????Uszs? ? DP(?
U ,?s?) [unary subsymbol prod.]
??For each pair of children symbols (s?, s??) ? S ? S:
????Bszs?s?? ? DP(?
B ,?s??
T
s??) [binary subsymbol]
For each node i in the parse tree:
?ti ? Multinomial(?Tsizi) [choose rule type]
?If ti = EMISSION:
??xi ? Multinomial(?Esizi) [emit terminal symbol]
?If ti = UNARY-PRODUCTION:
??sL(i) ? Multinomial(?
u
sizi) [generate child symbol]
??zL(i) ? Multinomial(?
U
sizisL(i)) [child subsymbol]
?If ti = BINARY-PRODUCTION:
??(sL(i), sR(i)) ? Mult(?sizi) [children symbols]
??(zL(i), zR(i)) ? Mult(?
B
sizisL(i)sR(i)) [subsymbols]
2.6 Variational inference
We present an inference algorithm for the HDP-
PCFG model described in Section 2.4, which can
also be adapted to the HDP-PCFG-GR model with
a bit more bookkeeping. Most previous inference
algorithms for DP-based models involve sampling
(Escobar and West, 1995; Teh et al, 2006). How-
ever, we chose to use variational inference (Blei
and Jordan, 2005), which provides a fast determin-
istic alternative to sampling, hence avoiding issues
of diagnosing convergence and aggregating samples.
Furthermore, our variational inference algorithm es-
tablishes a strong link with past work on PCFG re-
finement and induction, which has traditionally em-
ployed the EM algorithm.
In EM, the E-step involves a dynamic program
that exploits the Markov structure of the parse tree,
and the M-step involves computing ratios based on
expected counts extracted from the E-step. Our vari-
ational algorithm resembles the EM algorithm in
form, but the ratios in the M-step are replaced with
weights that reflect the uncertainty in parameter es-
692
??Bz
?Tz
?Ez
z ?
z1
z2 z3
T
Parameters Trees
Figure 4: We approximate the true posterior p over
parameters ? and latent parse trees z using a struc-
tured mean-field distribution q, in which the distri-
bution over parameters are completely factorized but
the distribution over parse trees is unconstrained.
timates. Because of this procedural similarity, our
method is able to exploit the desirable properties of
EM such as simplicity, modularity, and efficiency.
2.7 Structured mean-field approximation
We denote parameters of the HDP-PCFG as ? =
(?,?), where ? denotes the top-level symbol prob-
abilities and ? denotes the rule probabilities. The
hidden variables of the model are the training parse
trees z. We denote the observed sentences as x.
The goal of Bayesian inference is to compute the
posterior distribution p(?, z | x). The central idea
behind variational inference is to approximate this
intractable posterior with a tractable approximation.
In particular, we want to find the best distribution q?
as defined by
q?
def
= argmin
q?Q
KL(q(?, z)||p(?, z | x)), (4)
where Q is a tractable subset of distributions. We
use a structured mean-field approximation, meaning
that we only consider distributions that factorize as
follows (Figure 4):
Q
def
=
{
q(z)q(?)
K?
z=1
q(?Tz )q(?
E
z )q(?
B
z )
}
. (5)
We further restrict q(?Tz ), q(?
E
z ), q(?
B
z ) to be
Dirichlet distributions, but allow q(z) to be any
multinomial distribution. We constrain q(?) to be a
degenerate distribution truncated at K; i.e., ?z = 0
for z > K. While the posterior grammar does have
an infinite number of symbols, the exponential de-
cay of the DP prior ensures that most of the proba-
bility mass is contained in the first few symbols (Ish-
waran and James, 2001).2 While our variational ap-
proximation q is truncated, the actual PCFG model
is not. AsK increases, our approximation improves.
2.8 Coordinate-wise ascent
The optimization problem defined by Equation (4)
is intractable and nonconvex, but we can use a sim-
ple coordinate-ascent algorithm that iteratively op-
timizes each factor of q in turn while holding the
others fixed. The algorithm turns out to be similar in
form to EM for an ordinary PCFG: optimizing q(z)
is the analogue of the E-step, and optimizing q(?)
is the analogue of the M-step; however, optimizing
q(?) has no analogue in EM. We summarize each
of these updates below (see (Liang et al, 2007) for
complete derivations).
Parse trees q(z): The distribution over parse trees
q(z) can be summarized by the expected suffi-
cient statistics (rule counts), which we denote as
C(z ? zl zr) for binary productions and C(z ?
x) for emissions. We can compute these expected
counts using dynamic programming as in the E-step
of EM.
While the classical E-step uses the current rule
probabilities ?, our mean-field approximation in-
volves an entire distribution q(?). Fortunately, we
can still handle this case by replacing each rule prob-
ability with a weight that summarizes the uncer-
tainty over the rule probability as represented by q.
We define this weight in the sequel.
It is a common perception that Bayesian inference
is slow because one needs to compute integrals. Our
mean-field inference algorithm is a counterexample:
because we can represent uncertainty over rule prob-
abilities with single numbers, much of the existing
PCFG machinery based on EM can be modularly
imported into the Bayesian framework.
Rule probabilities q(?): For an ordinary PCFG,
the M-step simply involves taking ratios of expected
2In particular, the variational distance between the stick-
breaking distribution and the truncated version decreases expo-
nentially as the truncation level K increases.
693
counts:
?Bz (zl, zr) =
C(z ? zl zr)
C(z ? ??)
. (6)
For the variational HDP-PCFG, the optimal q(?) is
given by the standard posterior update for Dirichlet
distributions:3
q(?Bz ) = Dirichlet(?
B
z ;?
B??T + ~C(z)), (7)
where ~C(z) is the matrix of counts of rules with left-
hand side z. These distributions can then be summa-
rized with multinomial weights which are the only
necessary quantities for updating q(z) in the next it-
eration:
WBz (zl, zr)
def
= expEq[log?Bz (zl, zr)] (8)
=
e?(C(z?zl zr)+?
B?zl?zr )
e?(C(z???)+?B)
, (9)
where ?(?) is the digamma function. The emission
parameters can be defined similarly. Inspection of
Equations (6) and (9) reveals that the only difference
between the maximum likelihood and the mean-field
update is that the latter applies the exp(?(?)) func-
tion to the counts (Figure 5).
When the truncation K is large, ?B?zl?zr is near
0 for most right-hand sides (zl, zr), so exp(?(?)) has
the effect of downweighting counts. Since this sub-
traction affects large counts more than small counts,
there is a rich-get-richer effect: rules that have al-
ready have large counts will be preferred.
Specifically, consider a set of rules with the same
left-hand side. The weights for all these rules only
differ in the numerator (Equation (9)), so applying
exp(?(?)) creates a local preference for right-hand
sides with larger counts. Also note that the rule
weights are not normalized; they always sum to at
most one and are equal to one exactly when q(?) is
degenerate. This lack of normalization gives an ex-
tra degree of freedom not present in maximum like-
lihood estimation: it creates a global preference for
left-hand sides that have larger total counts.
Top-level symbol probabilities q(?): Recall that
we restrict q(?) = ???(?), so optimizing ? is
equivalent to finding a single best ??. Unlike q(?)
3Because we have truncated the top-level symbol weights,
the DP prior on ?Bz reduces to a finite Dirichlet distribution.
 
0
 
0.5 1
 
1.5 2  0
 
0.5
 
1
 
1.5
 
2
x
exp(?(x
)) x
Figure 5: The exp(?(?)) function, which is used in
computing the multinomial weights for mean-field
inference. It has the effect of reducing a larger frac-
tion of small counts than large counts.
and q(z), there is no closed form expression for
the optimal ??, and the objective function (Equa-
tion (4)) is not convex in ??. Nonetheless, we can
apply a standard gradient projection method (Bert-
sekas, 1999) to improve ?? to a local maxima.
The part of the objective function in Equation (4)
that depends on ?? is as follows:
L(??) = logGEM(??;?)+ (10)
K?
z=1
Eq[logDirichlet(?Bz ;?
B????T )]
See Liang et al (2007) for the derivation of the gra-
dient. In practice, this optimization has very little ef-
fect on performance. We suspect that this is because
the objective function is dominated by p(x | z) and
p(z | ?), while the contribution of p(? | ?) is mi-
nor.
3 Experiments
We now present an empirical evaluation of the HDP-
PCFG(-GR) model and variational inference tech-
niques. We first give an illustrative example of the
ability of the HDP-PCFG to recover a known gram-
mar and then present the results of experiments on
large-scale treebank parsing.
3.1 Recovering a synthetic grammar
In this section, we show that the HDP-PCFG-GR
can recover a simple grammar while a standard
694
S ? X1X1 | X2X2 | X3X3 | X4X4
X1 ? a1 | b1 | c1 | d1
X2 ? a2 | b2 | c2 | d2
X3 ? a3 | b3 | c3 | d3
X4 ? a4 | b4 | c4 | d4
S
Xi Xi
{ai, bi, ci, di} {ai, bi, ci, di}
(a) (b)
Figure 6: (a) A synthetic grammar with a uniform
distribution over rules. (b) The grammar generates
trees of the form shown on the right.
PCFG fails to do so because it has no built-in con-
trol over grammar complexity. From the grammar in
Figure 6, we generated 2000 trees. The two terminal
symbols always have the same subscript, but we col-
lapsed Xi to X in the training data. We trained the
HDP-PCFG-GR, with truncation K = 20, for both
S and X for 100 iterations. We set al hyperparame-
ters to 1.
Figure 7 shows that the HDP-PCFG-GR recovers
the original grammar, which contains only 4 sub-
symbols, leaving the other 16 subsymbols unused.
The standard PCFG allocates all the subsymbols to
fit the exact co-occurrence statistics of left and right
terminals.
Recall that a rule weight, as defined in Equa-
tion (9), is analogous to a rule probability for stan-
dard PCFGs. We say a rule is effective if its weight
is at least 10?6 and its left hand-side has posterior
is also at least 10?6. In general, rules with weight
smaller than 10?6 can be safely pruned without af-
fect parsing accuracy. The standard PCFG uses all
20 subsymbols of both S and X to explain the data,
resulting in 8320 effective rules; in contrast, the
HDP-PCFG uses only 4 subsymbols for X and 1 for
S, resulting in only 68 effective rules. If the thresh-
old is relaxed from 10?6 to 10?3, then only 20 rules
are effective, which corresponds exactly to the true
grammar.
3.2 Parsing the Penn Treebank
In this section, we show that our variational HDP-
PCFG can scale up to real-world data sets. We ran
experiments on the Wall Street Journal (WSJ) por-
tion of the Penn Treebank. We trained on sections
2?21, used section 24 for tuning hyperparameters,
and tested on section 22.
We binarize the trees in the treebank as follows:
for each non-terminal node with symbol X , we in-
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20
0.25
subsymbol
pos
ter
ior
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20
0.25
subsymbol
pos
ter
ior
standard PCFG HDP-PCFG
Figure 7: The posteriors over the subsymbols of the
standard PCFG is roughly uniform, whereas the pos-
teriors of the HDP-PCFG is concentrated on four
subsymbols, which is the true number of symbols
in the grammar.
troduce a right-branching cascade of new nodes with
symbol X . The end result is that each node has at
most two children. To cope with unknown words,
we replace any word appearing fewer than 5 times
in the training set with one of 50 unknown word to-
kens derived from 10 word-form features.
Our goal is to learn a refined grammar, where each
symbol in the training set is split into K subsym-
bols. We compare an ordinary PCFG estimated with
maximum likelihood (Matsuzaki et al, 2005) and
the HDP-PCFG estimated using the variational in-
ference algorithm described in Section 2.6.
To parse new sentences with a grammar, we com-
pute the posterior distribution over rules at each span
and extract the tree with the maximum expected cor-
rect number of rules (Petrov and Klein, 2007).
3.2.1 Hyperparameters
There are six hyperparameters in the HDP-PCFG-
GR model, which we set in the following manner:
? = 1, ?T = 1 (uniform distribution over unar-
ies versus binaries), ?E = 1 (uniform distribution
over terminal words), ?u(s) = ?b(s) = 1N(s) , where
N(s) is the number of different unary (binary) right-
hand sides of rules with left-hand side s in the tree-
bank grammar. The two most important hyperpa-
rameters are ?U and ?B , which govern the sparsity
of the right-hand side for unary and binary rules.
We set ?U = ?B although more performance could
probably be gained by tuning these individually. It
turns out that there is not a single ?B that works for
all truncation levels, as shown in Table 1.
If the top-level distribution ? is uniform, the value
of ?B corresponding to a uniform prior over pairs of
children subsymbols is K2. Interestingly, the opti-
mal ?B appears to be superlinear but subquadratic
695
truncation K 2 4 8 12 16 20
best ?B 16 12 20 28 48 80
uniform ?B 4 16 64 144 256 400
Table 1: For each truncation level, we report the ?B
that yielded the highest F1 score on the development
set.
K PCFG PCFG (smoothed) HDP-PCFG
F1 Size F1 Size F1 Size
1 60.47 2558 60.36 2597 60.5 2557
2 69.53 3788 69.38 4614 71.08 4264
4 75.98 3141 77.11 12436 77.17 9710
8 74.32 4262 79.26 120598 79.15 50629
12 70.99 7297 78.8 160403 78.94 86386
16 66.99 19616 79.2 261444 78.24 131377
20 64.44 27593 79.27 369699 77.81 202767
Table 2: Shows development F1 and grammar sizes
(the number of effective rules) as we increase the
truncation K.
in K. We used these values of ?B in the following
experiments.
3.2.2 Results
The regime in which Bayesian inference is most
important is when training data is scarce relative to
the complexity of the model. We train on just sec-
tion 2 of the Penn Treebank. Table 2 shows how
the HDP-PCFG-GR can produce compact grammars
that guard against overfitting. Without smoothing,
ordinary PCFGs trained using EM improve as K in-
creases but start to overfit around K = 4. Simple
add-1.01 smoothing prevents overfitting but at the
cost of a sharp increase in grammar sizes. The HDP-
PCFG obtains comparable performance with a much
smaller number of rules.
We also trained on sections 2?21 to demon-
strate that our methods can scale up and achieve
broadly comparable results to existing state-of-the-
art parsers. When using a truncation level of K =
16, the standard PCFG with smoothing obtains an
F1 score of 88.36 using 706157 effective rules while
the HDP-PCFG-GR obtains an F1 score of 87.08 us-
ing 428375 effective rules. We expect to see greater
benefits from the HDP-PCFG with a larger trunca-
tion level.
4 Related work
The question of how to select the appropriate gram-
mar complexity has been studied in earlier work.
It is well known that more complex models nec-
essarily have higher likelihood and thus a penalty
must be imposed for more complex grammars. Ex-
amples of such penalized likelihood procedures in-
clude Stolcke and Omohundro (1994), which used
an asymptotic Bayesian model selection criterion
and Petrov et al (2006), which used a split-merge
algorithm which procedurally determines when to
switch between grammars of various complexities.
These techniques are model selection techniques
that use heuristics to choose among competing sta-
tistical models; in contrast, the HDP-PCFG relies on
the Bayesian formalism to provide implicit control
over model complexity within the framework of a
single probabilistic model.
Johnson et al (2006) also explored nonparamet-
ric grammars, but they do not give an inference al-
gorithm for recursive grammars, e.g., grammars in-
cluding rules of the form A ? BC and B ? DA.
Recursion is a crucial aspect of PCFGs and our
inference algorithm does handle it. Finkel et al
(2007) independently developed another nonpara-
metric model of grammars. Though their model is
also based on hierarchical Dirichlet processes and is
similar to ours, they present a different inference al-
gorithm which is based on sampling. Kurihara and
Sato (2004) and Kurihara and Sato (2006) applied
variational inference to PCFGs. Their algorithm is
similar to ours, but they did not consider nonpara-
metric models.
5 Conclusion
We have presented the HDP-PCFG, a nonparametric
Bayesian model for PCFGs, along with an efficient
variational inference algorithm. While our primary
contribution is the elucidation of the model and algo-
rithm, we have also explored some important empir-
ical properties of the HDP-PCFG and also demon-
strated the potential of variational HDP-PCFGs on a
full-scale parsing task.
696
References
C. E. Antoniak. 1974. Mixtures of Dirichlet processes
with applications to Bayesian nonparametric prob-
lems. Annals of Statistics, 2:1152?1174.
M. Beal, Z. Ghahramani, and C. Rasmussen. 2002. The
infinite hidden Markov model. In Advances in Neural
Information Processing Systems (NIPS), pages 577?
584.
D. Bertsekas. 1999. Nonlinear programming.
D. Blei and M. I. Jordan. 2005. Variational inference for
Dirichlet process mixtures. Bayesian Analysis, 1:121?
144.
E. Charniak. 1996. Tree-bank grammars. In Association
for the Advancement of Artificial Intelligence (AAAI).
E. Charniak. 2000. A maximum-entropy-inspired parser.
In North American Association for Computational
Linguistics (NAACL), pages 132?139.
M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University
of Pennsylvania.
M. D. Escobar and M. West. 1995. Bayesian density
estimation and inference using mixtures. Journal of
the American Statistical Association, 90:577?588.
T. S. Ferguson. 1973. A Bayesian analysis of some non-
parametric problems. Annals of Statistics, 1:209?230.
J. R. Finkel, T. Grenager, and C. Manning. 2007. The
infinite tree. In Association for Computational Lin-
guistics (ACL).
S. Goldwater, T. Griffiths, and M. Johnson. 2006. Con-
textual dependencies in unsupervised word segmenta-
tion. In International Conference on Computational
Linguistics and Association for Computational Lin-
guistics (COLING/ACL).
H. Ishwaran and L. F. James. 2001. Gibbs sampling
methods for stick-breaking priors. Journal of the
American Statistical Association, 96:161?173.
M. Johnson, T. Griffiths, and S. Goldwater. 2006. Adap-
tor grammars: A framework for specifying composi-
tional nonparametric Bayesian models. In Advances
in Neural Information Processing Systems (NIPS).
D. Klein and C. Manning. 2003. Accurate unlexicalized
parsing. In Association for Computational Linguistics
(ACL), pages 423?430.
K. Kurihara and T. Sato. 2004. An application of the
variational Bayesian approach to probabilistic context-
free grammars. In International Joint Conference on
Natural Language Processing Workshop Beyond Shal-
low Analyses.
K. Kurihara and T. Sato. 2006. Variational Bayesian
grammar induction for natural language. In Interna-
tional Colloquium on Grammatical Inference.
P. Liang, S. Petrov, M. I. Jordan, and D. Klein.
2007. Nonparametric PCFGs using Dirichlet pro-
cesses. Technical report, Department of Statistics,
University of California at Berkeley.
T. Matsuzaki, Y. Miyao, and J. Tsujii. 2005. Probabilis-
tic CFG with latent annotations. In Association for
Computational Linguistics (ACL).
S. Petrov and D. Klein. 2007. Learning and inference
for hierarchically split PCFGs. In Human Language
Technology and North American Association for Com-
putational Linguistics (HLT/NAACL).
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning accurate, compact, and interpretable tree an-
notation. In International Conference on Computa-
tional Linguistics and Association for Computational
Linguistics (COLING/ACL).
J. Pitman. 2002. Combinatorial stochastic processes.
Technical Report 621, Department of Statistics, Uni-
versity of California at Berkeley.
J. Sethuraman. 1994. A constructive definition of Dirich-
let priors. Statistica Sinica, 4:639?650.
A. Stolcke and S. Omohundro. 1994. Inducing prob-
abilistic grammars by Bayesian model merging. In
Grammatical Inference and Applications.
Y. W. Teh, M. I. Jordan, M. Beal, and D. Blei. 2006.
Hierarchical Dirichlet processes. Journal of the Amer-
ican Statistical Association, 101:1566?1581.
697
Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 887?896, Prague, June 2007. c?2007 Association for Computational Linguistics
A Probabilistic Approach to Diachronic Phonology
Alexandre Bouchard-Co?te?? Percy Liang? Thomas L. Griffiths? Dan Klein?
?Computer Science Division ?Department of Psychology
University of California at Berkeley
Berkeley, CA 94720
Abstract
We present a probabilistic model of di-
achronic phonology in which individual
word forms undergo stochastic edits along
the branches of a phylogenetic tree. Our ap-
proach allows us to achieve three goals with
a single unified model: (1) reconstruction
of both ancient and modern word forms, (2)
discovery of general phonological changes,
and (3) selection among different phyloge-
nies. We learn our model using a Monte
Carlo EM algorithm and present quantitative
results validating the model.
1 Introduction
Modeling how languages change phonologically
over time (diachronic phonology) is a central topic
in historical linguistics (Campbell, 1998). The ques-
tions involved range from reconstruction of ancient
word forms, to the elucidation of phonological drift
processes, to the determination of phylogenetic re-
lationships between languages. However, this prob-
lem has received relatively little attention from the
computational community. What work there is has
focused on the reconstruction of phylogenies on the
basis of a Boolean matrix indicating the properties
of words in different languages (Gray and Atkinson,
2003; Evans et al, 2004; Ringe et al, 2002; Nakhleh
et al, 2005).
In this paper, we present a novel framework, along
with a concrete model and experiments, for the prob-
abilistic modeling of diachronic phonology. We fo-
cus on the case where the words are etymological
cognates across languages, e.g. French faire and
Spanish hacer from Latin facere (to do). Given
this information as input, we learn a model acting
at the level of individual phoneme sequences, which
can be used for reconstruction and prediction, Our
model is fully generative, and can be used to reason
about a variety of types of information. For exam-
ple, we can observe a word in one or more modern
languages, say French and Spanish, and query the
corresponding word form in another language, say
Italian. This kind of lexicon-filling has applications
in machine translation. Alternatively, we can also
reconstruct ancestral word forms or inspect the rules
learned along each branch of a phylogeny to identify
salient patterns. Finally, the model can be used as a
building block in a system for inferring the topology
of phylogenetic trees. We discuss all of these cases
further in Section 4.
The contributions of this paper are threefold.
First, the approach to modeling language change at
the phoneme sequence level is new, as is the spe-
cific model we present. Second, we compiled a new
corpus1 and developed a methodology for quantita-
tively evaluating such approaches. Finally, we de-
scribe an efficient inference algorithm for our model
and empirically study its performance.
1.1 Previous work
While our word-level model of phonological change
is new, there have been several computational inves-
tigations into diachronic linguistics which are rele-
vant to the present work.
The task of reconstructing phylogenetic trees
1nlp.cs.berkeley.edu/pages/historical.html
887
for languages has been studied by several authors.
These approaches descend from glottochronology
(Swadesh, 1955), which views a language as a col-
lection of shared cognates but ignores the structure
of those cognates. This information is obtained from
manually curated cognate lists such as the data of
Dyen et al (1997).
As an example of a cognate set encoding, consider
the meaning ?eat?. There would be one column for
the cognate set which appears in French as manger
and Italian as mangiare since both descend from the
Latin mandere (to chew). There would be another
column for the cognate set which appears in both
Spanish and Portuguese as comer, descending from
the Latin comedere (to consume). If this were the
only data, algorithms based on this data would tend
to conclude that French and Italian were closely re-
lated and that Spanish and Portuguese were equally
related. However, the cognate set representation has
several disadvantages: it does not capture the fact
that the cognate is closer between Spanish and Por-
tuguese than between French and Spanish, nor do
the resulting models let us conclude anything about
the regular processes which caused these languages
to diverge. Also, the existing cognate data has been
curated at a relatively high cost. In our work, we
track each word using an automatically obtained
cognate list. While our cognates may be noisier,
we compensate by modeling phonological changes
rather than boolean mutations in cognate sets.
There has been other computational work in this
broad domain. Venkataraman et al (1997) describe
an information theoretic measure of the distance be-
tween two dialects of Chinese. Like our approach,
they use a probabilistic edit model as a formaliza-
tion of the phonological process. However, they do
not consider the question of reconstruction or infer-
ence in multi-node phylogenies, nor do they present
a learning algorithm for such models.
Finally, for the specific application of cog-
nate prediction in machine translation, essentially
transliteration, there have been several approaches,
including Kondrak (2002). However, the phenom-
ena of interest, and therefore the models, are ex-
tremely different. Kondrak (2002) presents a model
for learning ?sound laws,? general phonological
changes governing two completely observed aligned
cognate lists. His model can be viewed as a special
la
es it
la
vl
ib
es pt
it
la
it pt
es
la
it es
pt
Topology 1 Topology 2 *Topology 3 *Topology 4
Figure 1: Tree topologies used in our experiments. *Topology
3 and *Topology 4 are incorrect evolutionary tree used for our
experiments on the selection of phylogenies (Section 4.4).
case of ours using a simple two-node topology.
There is also a rich literature (Huelsenbeck et al,
2001) on the related problems of evolutionary biol-
ogy. A good reference on the subject is Felsenstein
(2003). In particular, Yang and Rannala (1997), Mau
and Newton (1997) and Li et al (2000) each inde-
pendently presented a Bayesian model for comput-
ing posteriors over evolutionary trees. A key dif-
ference with our model is that independence across
evolutionary sites is assumed in their work, while
the evolution of the phonemes in our model depends
on the environment in which the change occurs.
2 A model of phonological change
Assume we have a fixed set of word types (cog-
nate sets) in our vocabulary V and a set of languages
L. Each word type i has a word form wil in each lan-
guage l ? L, which is represented as a sequence of
phonemes and might or might not be observed. The
languages are arranged according to some tree topol-
ogy T (see Figure 1 for examples). One might con-
sider models that simultaneously induce the topol-
ogy and cognate set assignments, but let us fix both
for now. We discuss one way to relax this assump-
tion and present experimental results in Section 4.4.
Our generative model (Figure 3) specifies a dis-
tribution over the word forms {wil} for each word
type i ? V and each language l ? L. The genera-
tive process starts at the root language and generates
all the word forms in each language in a top-down
manner. One appealing aspect about our model is
that, at a high-level, it reflects the actual phonolog-
ical process that languages undergo. However, im-
portant phenomena like lexical drift, borrowing, and
other non-phonological changes are not modeled.
888
Our generative model can be summarized as fol-
lows:
For each word i ? V :
?wiROOT ? LanguageModel
For each branch (k ? l) ? T :
??k?l ? Dirichlet(?) [choose edit params.]
?For each word i ? V :
??wil ? Edit(wik, ?k?l) [sample word form]
In the remainder of this section, we describe each
of the steps in the model.
2.1 Language model
For the distributionw ? LanguageModel, we used a
simple bigram phoneme model. The phonemes were
partitioned into natural classes (see Section 4 for de-
tails). A root word form consisting of n phonemes
x1 ? ? ?xn is generated with probability
plm(x1)
n?
j=2
plm(xj | NaturalClass(xj?1)),
where plm is the distribution of the language model.
2.2 Edit model
The stochastic edit model y ? Edit(x, ?) describes
how a single old word form x = x1 ? ? ?xn changes
along one branch of the phylogeny with parameters
? to produce a new word form y. This process is
parameterized by rule probabilities ?k?l, which are
specific to branch (k ? l).
The generative process is as follows: for each
phoneme xi in the old word form, walking from
left to right, choose a rule to apply. There are
three types of rules: (1) deletion of the phoneme,
(2) substitution with another phoneme (possibly the
same one), or (3) insertion of another phoneme, ei-
ther before or after the existing one. The prob-
ability of applying a rule depends on a context
(NaturalClass(xi?1),NaturalClass(xi+1)). Figure 2
illustrates the edits on an example. The context-
dependence allows us to represent phenomena such
as the fact that s is likely to be deleted only in word-
final contexts.
The edit model we have presented approximately
encodes a limited form of classic rewrite-driven seg-
mental phonology (Chomsky and Halle, 1968). One
# C V C V C #
# f o k u s #
# f w O k o #
# C V V C V #
f ? f / # Vo ? w O / C Ck ? k / V Vu ? o / C Cs ? / V #
Edits applied Rules used
Figure 2: An example of edits that were used to transform
the Latin word FOCUS (/fokus/) into the Italian word fuoco
(/fwOko/) (fire) along with the context-specific rules that were
applied.
could imagine basing our model on more modern
phonological theory, but the computational proper-
ties of the edit model are compelling, and it is ade-
quate for many kinds of phonological change.
In addition to simple edits, we can model some
classical changes that appear to be too complex to be
captured by a single left-to-right edit model of this
kind. For instance, bleeding and feeding arrange-
ments occur when one phonological change intro-
duces a new context, which triggers another phono-
logical change, but the two cannot occur simultane-
ously. For example, vowel raising e ? i / c might
be needed before palatalization t ? c / i. Instead
of capturing such an interaction directly, we can
break up a branch into two segments joined at an in-
termediate language node, conflating the concept of
historically intermediate languages with the concept
of intermediate stages in the application of sequen-
tial rules.
However, many complex processes are not well-
represented by our basic model. One problem-
atic case is chained shifts such as Grimm?s law in
Proto-Germanic or the Great Vowel Shift in English.
To model such dependent rules, we would need
to use a more complex prior distributions over the
edit parameters. Another difficult case is prosodic
changes, such as unstressed vowel neutralizations,
which would require a representation of supraseg-
mental features. While our basic model does not
account for these phenomena, extensions within the
generative framework could capture such richness.
3 Learning and inference
We use a Monte Carlo EM algorithm to fit the pa-
rameters of our model. The algorithm iterates be-
tween a stochastic E-step, which computes recon-
889
...
wiA
wiB
wiC wiD
... ...word type i = 1 . . . |V |
eiA?B?A?B
eiB?C?B?C eiB?D ?B?D
Figure 3: The graphical model representation of our model: ?
are the parameters specifying the stochastic edits e, which gov-
ern how the words w evolve. The plate notation indicates the
replication of the nodes corresponding to the evolving words.
structions based on the current edit parameters, and
an M-step, which updates the edit parameters based
on the reconstructions.
3.1 Monte Carlo E-step: sampling the edits
The E-step needs to produce expected counts of how
many times each edit (such as o ? O) was used in
each context. An exact E-step would require sum-
ming over all possible edits involving all languages
in the phylogeny (all unobserved {e}, {w} variables
in Figure 3). Unfortunately, unlike in the case of
HMMs and PCFGs, our model permits no tractable
dynamic program to compute these counts exactly.
Therefore, we resort to a Monte Carlo E-step,
where many samples of the edit variables are col-
lected, and counts are computed based on these sam-
ples. Samples are drawn using Gibbs sampling (Ge-
man and Geman, 1984): for each word form of a
particular language wil, we fix all other variables in
the model and sample wil along with its correspond-
ing edits.
In the E-step, we fix the parameters, which ren-
ders the word types conditionally independent, just
as in an HMM. Therefore, we can process each word
type in turn without approximation.
First consider the simple 4-language topology in
Figure 3. Suppose that the words in languages A,
C and D are fixed, and we wish to infer the word
at language B along with the three corresponding
sets of edits (remember the edits fully determine the
words). There are an exponential number of possi-
ble words/edits, but it turns out that we can exploit
theMarkov structure in the edit model to consider all
such words/edits using dynamic programming, in a
way broadly similar to the forward-backward algo-
rithm for HMMs.
Figure 4 shows the lattice for the dynamic pro-
gram. Each path connecting the two shaded end-
point states represents a particular word form for
language B and a corresponding set of edits. Each
node in the lattice is a state of the dynamic pro-
gram, which is a 5-tuple (iA, iC , iD, c1, c2), where
iA, iC and iD are the cursor positions (represented
by dots in Figure 4) in each of the word forms of
A,C and D, respectively; c1 is the natural class of
the phoneme in the word form for B that was last
generated; and c2 corresponds to the phoneme that
will be generated next.
Each state transition involves applying a rule
to A?s current phoneme (which produces 0?2
phonemes in B) and applying rules to B?s new 0?2
phonemes. There are three types of rules (deletion,
substitution, insertion), resulting in 30+32+34 = 91
types of state transitions. For illustration, Figure 4
shows the simpler case where B only has one child
C. Given these rules, the new state is computed by
advancing the appropriate cursors and updating the
natural classes c1 and c2. The weight of each tran-
sition w(s ? t) is a product of the language model
probability and the rule probabilities that were cho-
sen.
For each state s, the dynamic program computes
W (s), the sum of the weights of all paths leaving s,
W (s) =
?
s?t
w(s ? t)W (t).
To sample a path, we start at the leftmost state,
choose the transition with probability proportional
to its contribution in the sum for computing W (s),
and repeat until we reach the rightmost state.
We applied a few approximations to speed up the
sampling of words, which reduced the running time
by several orders of magnitude. For example, we
pruned rules with low probability and restricted the
890
An example of a dynamic programming lattice
...
...
... ... ... ... ... ... ...
...
patr ? ia
# C V C C# p a t r ? V #a #
patr ? ja
x [T1] p1ed(i ? /C V) x
x [T3] plm(j | C) p1ed(i ? j/C V) p2ed(j ? j/C V) x
x [T11] plm(j | C) plm(i | C) p1ed(i ? j i/C V) p2ed(j ? j/C V) p2ed(i ? /C V) x
. . .
patri ? a
# C V C C# p a t r ? V #a #
patr ? ja
patri ? a
# C V C C C# p a t r j ? V #a #
patrj ? a
patri ? a
# C V C C C V# p a t r j i ? V #a #
patrj ? a
. . .
Types of state transitions (x: ancient phoneme, y: intermediate, z: modern)
x
y
x
y
z
x
y
z
x
y
z z
x
y
z
y
z
x
y
z
y
z
x
y
z
y
z z
x
y
z
y
z
x
y
z
y
z
x
y
z
y
z z
x
y
z z
y
z
x
y
z z
y
z
x
y
z z
y
z z[T1] [T2] [T3] [T4] [T5] [T6] [T7] [T8] [T9] [T10] [T11] [T12] [T13]
Figure 4: The dynamic program involved in sampling an intermediate word form given one ancient and one modern word form.
One lattice node is expanded to show the dynamic program state (represented by the part not grayed out) and three of the many
possible transitions leaving the state. Each transition is labeled with the weight of the transition, which is the product of the relevant
model probabilities. At the bottom, the 13 types of state transitions are shown.
state space of the dynamic program by limiting the
deviation in cursor positions.
3.2 M-step: updating the parameters
The M-step is standard once we have computed
the expected counts of edits in the E-step. For
each branch (k ? l) ? T in the phylogeny,
we compute the maximum likelihood estimate
of the edit parameters {?k?l(x ? ? / c1 c2)}.
For example, the parameter corresponding to
x = /e/, ? = /e s/, c1 = ALVEOLAR, c2 = # is
the probability of inserting a final /s/ after an /e/
which is itself preceded by an alveolar phoneme.
The probability of each rule is estimated as follows:
?k?l(x ? ? / c1 c2) =
#(x ? ? / c1 c2) + ?(x ? ? / c1 c2)? 1?
?? #(x ? ?? / c1 c2) + ?(x ? ?? / c1 c2)? 1
,
where ? is the concentration hyperparameter of the
Dirichlet prior. The value ? ? 1 can be interpreted
as the number of pseudocounts for a rule.
4 Experiments
In this section we show the results of our experi-
ments with our model. The experimental conditions
are summarized in Table 1, with additional informa-
Experiment Topology Heldout
Latin reconstruction (4.2) 1 la:293
Italian reconstruction (4.2) 1 it:117
Sound changes (4.3) 2 None
Phylogeny selection (4.4) 2, 3, 4 None
Table 1: Conditions under which each of the experiments pre-
sented in this section were performed. The topology indices
correspond to those displayed in Figure 1. Note that by condi-
tional independence, the topology used for Spanish reconstruc-
tion reduces to a chain. The heldout column indicates howmany
words, if any, were heldout for edit distance evaluation, and
from which language.
tion on the specifics of the experiments presented in
Section 4.5. We start with a description of the corpus
we created for these experiments.
4.1 Corpus
In order to train and evaluate our system, we
compiled a corpus of Romance cognate words.
The raw data was taken from three sources: the
wiktionary.org website, a Bible parallel cor-
pus (Resnik et al, 1999) and the Europarl corpus
(Koehn, 2002). From an XML dump of the Wik-
tionary data, we extracted multilingual translations,
which provide a list of word tuples in a large num-
ber of languages, including a few ancient languages.
891
The Europarl and the biblical data were processed
and aligned in the standard way, using combined
GIZA++ alignments (Och and Ney, 2003).
We performed our experiments with four lan-
guages from the Romance family (Latin, Italian,
Spanish, and Portuguese). For each of these lan-
guages, we used a simple in-house rule-based sys-
tem to convert the words into their IPA represen-
tations.2 After augmenting our alignments with
the transitive closure3 of the Europarl, Bible and
Wiktionary data, we filtered out non-cognate words
by thresholding the ratio of edit distance to word
length.4 The preprocessing is constraining in that we
require that all the elements of a tuple to be cognates,
which leaves out a significant portion of the data be-
hind (see the row Full entries in Table 2). However,
our approach relies on this assumption, as there is no
explicit model of non-cognate words. An interest-
ing direction for future work is the joint modeling of
phonology with the determination of the cognates,
but our simpler setting lets us focus on the proper-
ties of the edit model. Moreover, the restriction to
full entries has the side advantage that the Latin bot-
tleneck prevents the introduction of too many neol-
ogisms, which are numerous in the Europarl data, to
the final corpus.
Since we used automatic tools for preparing our
corpus rather than careful linguistic analysis, our
cognate list is much noiser in terms of the pres-
ence of borrowed words and phonemeic transcrip-
tion errors compared to the ones used by previous
approaches (Swadesh, 1955; Dyen et al, 1997). The
benefit of our mechanical preprocessing is that more
cognate data can easily be made available, allowing
us to effectively train richer models. We show in the
rest of this section that our phonological model can
indeed overcome this noise and recover meaningful
patterns from the data.
2The tool and the rules we used are available at
nlp.cs.berkeley.edu/pages/historical.html.
3For example, we would infer from an la-es bible align-
ment confessionem-confesio?n (confession) and an es-it Eu-
roparl alignment confesio?n-confessione that the Latin word con-
fessionem and the Italian word confessione are related.
4To be more precise we keep a tuple (w1, w2, . . . , wp) iff
d(wi,wj)
l?(wi,wj)
? 0.7 for all i, j ? {1, 2, . . . , p}, where l? is the mean
length
|wi|+|wj |
2 and d is the Levenshtein distance.
Name Languages Tuples Word forms
Raw sources of data used to create the corpus
Wiktionary es,pt,la,it 5840 11724
Bible la,es 2391 4782
Europarl es,pt 36905 73773
it,es 39506 78982
Main stages of preprocessing of the corpus
Closure es,pt,la,it 40944 106090
Cognates es,pt,la,it 27996 69637
Full entries es,pt,la,it 586 2344
Table 2: Statistics of the dataset we compiled for the evaluation
of our model. We show the languages represented, the number
of tuples and the number of word forms found in each of the
source of data and pre-processing steps involved in the creation
of the dataset we used to test our model. By full entry, we mean
the number of tuples that are jointly considered cognate by our
preprocessing system and that have a word form known for each
of the languages of interest. These last row forms the dataset
used for our experiments.
Language Baseline Model Improvement
Latin 2.84 2.34 9%
Spanish 3.59 3.21 11%
Table 3: Results of the edit distance experiment. The language
column corresponds to the language held-out for evaluation. We
show the mean edit distance across the evaluation examples.
4.2 Reconstruction of word forms
We ran the system using Topology 1 in Figure 1 to
demonstrate the the system can propose reasonable
reconstructions of Latin word forms on the basis of
modern observations. Half of the Latin words at the
root of the tree were held out, and the (uniform cost)
Levenshtein edit distance from the predicted recon-
struction to the truth was computed. Our baseline is
to pick randomly, for each heldout node in the tree,
an observed neighboring word (i.e. copy one of the
modern forms). We stopped EM after 15 iterations,
and reported the result on a Viterbi derivation using
the parameters obtained. Our model outperformed
this baseline by a 9% relative reduction in average
edit distance. Similarly, reconstruction of modern
forms was also demonstrated, with an improvement
of 11% (see Table 3).
To give a qualitative feel for the operation of the
system (good and bad), consider the example in Fig-
ure 5, taken from this experiment. The Latin dentis
/dEntis/ (teeth) is nearly correctly reconstructed as
/dEntes/, reconciling the appearance of the /j/ in the
892
/dEntis/
/djEntes/ /dEnti/
i ? E
E? j E s ?
Figure 5: An example of a Latin reconstruction given the Span-
ish and Italian word forms.
Spanish and the disappearance of the final /s/ in the
Italian. Note that the /is/ vs. /es/ ending is difficult
to predict in this context (indeed, it was one of the
early distinctions to be eroded in vulgar Latin).
While the uniform-cost edit distance misses im-
portant aspects of phonology (all phoneme substitu-
tions are not equal, for instance), it is parameter-free
and still seems to correlate to a large extent with lin-
guistic quality of reconstruction. It is also superior
to held-out log-likelihood, which fails to penalize er-
rors in the modeling assumptions, and to measuring
the percentage of perfect reconstructions, which ig-
nores the degree of correctness of each reconstructed
word.
4.3 Inference of phonological changes
Another use of our model is to automatically recover
the phonological drift processes between known or
partially known languages. To facilitate evaluation,
we continued in the well-studied Romance evolu-
tionary tree. Again, the root is Latin, but we now add
an additional modern language, Portuguese, and two
additional hidden nodes. One of the nodes charac-
terizes the least common ancestor of modern Span-
ish and Portuguese; the other, the least common an-
cestor of all three modern languages. In Figure 1,
Topology 2, these two nodes are labelled vl (Vulgar
Latin) and ib (Proto-Ibero Romance) respectively.
Since we are omitting many other branches, these
names should not be understood as referring to ac-
tual historical proto-languages, but, at best, to col-
lapsed points representing several centuries of evo-
lution. Nonetheless, the major reconstructed rules
still correspond to well known phenomena and the
learned model generally places them on reasonable
branches.
Figure 6 shows the top four general rules for
each of the evolutionary branches in this experiment,
ranked by the number of times they were used in the
derivations during the last iteration of EM. The la,
es, pt, and it forms are fully observed while the
vl and ib forms are automatically reconstructed.
Figure 6 also shows a specific example of the evolu-
tion of the Latin VERBUM (word/verb), along with
the specific edits employed by the model.
While quantitative evaluation such as measuring
edit distance is helpful for comparing results, it is
also illuminating to consider the plausibility of the
learned parameters in a historical light, which we
do here briefly. In particular, we consider rules on
the branch between la and vl, for which we have
historical evidence. For example, documents such
as the Appendix Probi (Baehrens, 1922) provide in-
dications of orthographic confusions which resulted
from the growing gap between Classical Latin and
Vulgar Latin phonology around the 3rd and 4th cen-
turies AD. The Appendix lists common misspellings
of Latin words, from which phonological changes
can be inferred.
On the la to vl branch, rules for word-final dele-
tion of classical case markers dominate the list (rules
ranks 1 and 3 for deletion of final /s/, ranks 2 and
4 for deletion of final /m/). It is indeed likely that
these were generally eliminated in Vulgar Latin. For
the deletion of the /m/, the Appendix Probi contains
pairs such as PASSIM NON PASSI and OLIM NON
OLI. For the deletion of final /s/, this was observed
in early inscriptions, e.g. CORNELIO for CORNE-
LIOS (Allen, 1989). The frequent leveling of the
distinction between /o/ and /u/ (rules ranked 5 and 6)
can be also be found in the Appendix Probi: COLU-
BER NON COLOBER. Note that in the specific ex-
ample shown, the model lowers the orignal /u/ and
then re-raises it in the pt branch due to a latter pro-
cess along that branch.
Similarily, major canonical rules were discovered
in other branches as well, for example, /v/ to /b/
fortition in Spanish, /s/ to /z/ voicing in Italian,
palatalization along several branches, and so on. Of
course, the recovered words and rules are not per-
fect. For example, reconstructed Ibero /tRinta/ to
Spanish /tReinta/ (thirty) is generated in an odd fash-
ion using rules /e/ to /i/ and /n/ to /in/. Moreover,
even when otherwise reasonable systematic sound
changes are captured, the crudeness of our fixed-
granularity contexts can prevent the true context
893
r ? R / many environmentse ? / #i ? / #t ? d / UNROUNDED UNROUNDED
u ? o / many environmentsv ? b / initial or intervocalict ? t e / ALVEOLAR #z ? s / ROUNDED UNROUNDED
/werbum/ (la)
/verbo/ (vl)
/veRbo/ (ib)
/beRbo/ (es) /veRbu/ (pt)
/vErbo/ (it)
s ? / #m ? /u ? o / many environmentsw ? v / # UNROUNDED
u ? o / ALVEOLAR #e ? E / many environmentsi ? / many environmentsi ? e / ALVEOLAR #
a ? 5 / ALVEOLAR #n ? m / UNROUNDED ALVEOLARo ? u / ALVEOLAR #e ? 1 / BILABIAL ALVEOLAR
m ?u ? ow ? v
r ? R
v ? b o ? u
e ? E
Figure 6: The tree shows the system?s hypothesised derivation of a selected Latin word form, VERBUM (word/verb) into the modern
Spanish, Italian and Portuguese pronunciations. The Latin root and modern leaves were observed while the hidden nodes as well as
all the derivations were obtained using the parameters computed by our model after 15 iterations of EM. Nontrivial rules (i.e. rules
that are not identities) used at each stage are shown along the corresponding edge. The boxes display the top four nontrivial rules
corresponding to each of these evolutionary branches, ordered by the number of time they were applied during the last E round of
sampling. Note that since our natural classes are of fixed granularity, some rules must be redundantly discovered, which tends to
flood the top of the rule lists with duplicates of the top few rules. We summarized such redundancies in the above tables.
from being captured, resulting in either rules apply-
ing with low probability in overly coarse environ-
ments or rules being learned redundantly in overly
fine environments.
4.4 Selection of phylogenies
In this experiment, we show that our model can be
used to select between various topologies of phylo-
genies. We first presented to the algorithm the uni-
versally accepted evolutionary tree corresponding to
the evolution of Latin into Spanish, Portuguese and
Italian (Topology 2 in Figure 1). We estimated the
log-likelihood L? of the data under this topology.
Next, we estimated the log-likelihood L? under two
defective topologies (*Topology 3 and *Topology
4). We recorded the log-likelihood ratio L? ? L?
after the last iteration of EM. Note that the two like-
lihoods are comparable since the complexity of the
two models is the same.5
We obtained a ratio of L? ? L? = ?4458 ?
(?4766) = 307 for Topology 2 versus *Topology
3, and ?4877? (?5125) = 248 for Topology 2 ver-
sus *Topology 4 (the experimental setup is described
in Table 1). As one would hope, this log-likelihood
ratio is positive in both cases, indicating that the sys-
tem prefers the true topology over the wrong ones.
While it may seem, at the first glance, that this re-
sult is limited in scope, knowing the relative arrange-
5If a word was not reachable in one of the topology, it was
ignored in both models for the computation of the likelihoods.
ment of all groups of four nodes is actually sufficient
for constructing a full-fledged phylogenetic tree. In-
deed, quartet-based methods, which have been very
popular in the computational biology community,
are precisely based on this fact (Erdos et al, 1996).
There is a rich literature on this subject and approxi-
mate algorithms exist which are robust to misclassi-
fication of a subset of quartets (Wu et al, 2007).
4.5 More experimental details
This section summarizes the values of the parame-
ters we used in these experiments, their interpreta-
tion, and the effect of setting them to other values.
The Dirichlet prior on the parameters can be in-
terpreted as adding pseudocounts to the correspond-
ing edits. It is an important way of infusing par-
simony into the model by setting the prior of the
self-substitution parameters much higher than that
of the other parameters. We used 6.0 as the prior on
the self-substitution parameters, and for all environ-
ments, 1.1 was divided uniformly across the other
edits. As long as the prior on self-substitution is
kept within this rough order of magnitude, varying
them has a limited effect on our results. We also ini-
tialized the parameters with values that encourage
self-substitutions. Again, the results were robust to
perturbation of initialization as long as the value for
self-substitution dominates the other parameters.
The experiments used two natural classes for
vowels (rounded and unrounded), and six natural
894
classes for consonants, based on the place of ar-
ticulation (alveolar, bilabial, labiodental, palatal,
postalveolar, and velar). We conducted experi-
ments to evaluate the effect of using different natural
classes and found that finer ones can help if enough
data is used for training. We defer the meticulous
study of the optimal granularity to future work, as it
would be a more interesting experiment under a log-
linear model. In such a model, contexts of different
granularities can coexist, whereas such coexistence
is not recognized by the current model, giving rise
to many duplicate rules.
We estimated the bigram phoneme model on the
words in the root languages that were not heldout.
Just as in machine translation, the language model
was found to contribute significantly to reconstruc-
tion performance. We tried to increase the weight of
the language model by exponentiating it to a power,
as is often done in NLP applications, but we did
not find that it had any significant impact on per-
formance.
In the reconstruction experiments, when the data
was not reachable by the model, the word used in
the initialization was used as the prediction, and
the evolution of these words were ignored when re-
estimating the parameters. Words were initialized
by picking at random, for each unobserved node, an
observed node?s corresponding word.
5 Conclusion
We have presented a novel probabilistic model of
diachronic phonology and an associated inference
procedure. Our experiments indicate that our model
is able to both produce accurate reconstructions as
measured by edit distance and identify linguisti-
cally plausible rules that account for the phonologi-
cal changes. We believe that the probabilistic frame-
work we have introduced for diachronic phonology
is promising, and scaling it up to richer phylogenetic
may indeed reveal something insightful about lan-
guage change.
6 Acknowledgement
We would like to thank Bonnie Chantarotwong for
her help with the IPA converter and our reviewers
for their comments. This work was supported by
a FQRNT fellowship to the first author, a NDSEG
fellowship to the second author, NSF grant number
BCS-0631518 to the third author, and a Microsoft
Research New Faculty Fellowship to the fourth au-
thor.
References
W. Sidney Allen. 1989. Vox Latina: The Pronunciation
of Classical Latin. Cambridge University Press.
W.A. Baehrens. 1922. Sprachlicher Kommentar zur
vulga?rlateinischen Appendix Probi. Halle (Saale) M.
Niemeyer.
L. Campbell. 1998. Historical Linguistics. The MIT
Press.
N. Chomsky and M. Halle. 1968. The Sound Pattern of
English. Harper & Row.
I. Dyen, J.B. Kruskal, and P. Black.
1997. FILE IE-DATA1. Available at
http://www.ntu.edu.au/education/langs/ielex/IE-
DATA1.
P. L. Erdos, M. A. Steel, L. A. Szekely, and T. J. Warnow.
1996. Local quartet splits of a binary tree infer all
quartet splits via one dyadic inference rule. Technical
report, DIMACS.
S. N. Evans, D. Ringe, and T. Warnow. 2004. Inference
of divergence times as a statistical inverse problem. In
P. Forster and C. Renfrew, editors, Phylogenetic Meth-
ods and the Prehistory of Languages. McDonald Insti-
tute Monographs.
Joseph Felsenstein. 2003. Inferring Phylogenies. Sin-
auer Associates.
S. Geman and D. Geman. 1984. Stochastic relaxation,
Gibbs distributions, and the Bayesian restoration of
images. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 6:721?741.
R. D. Gray and Q. Atkinson. 2003. Language-tree di-
vergence times support the Anatolian theory of Indo-
European origins. Nature.
John P. Huelsenbeck, Fredrik Ronquist, Rasmus Nielsen,
and Jonathan P. Bollback. 2001. Bayesian inference
of phylogeny and its impact on evolutionary biology.
Science.
P. Koehn. 2002. Europarl: A Multilingual Corpus for
Evaluation of Machine Translation.
G. Kondrak. 2002. Algorithms for Language Recon-
struction. Ph.D. thesis, University of Toronto.
895
S. Li, D. K. Pearl, and H. Doss. 2000. Phylogenetic tree
construction using Markov chain Monte Carlo. Jour-
nal of the American Statistical Association.
Bob Mau and M.A. Newton. 1997. Phylogenetic in-
ference for binary data on dendrograms using markov
chain monte carlo. Journal of Computational and
Graphical Statistics.
L. Nakhleh, D. Ringe, and T. Warnow. 2005. Perfect
phylogenetic networks: A new methodology for re-
constructing the evolutionary history of natural lan-
guages. Language, 81:382?420.
F. J. Och and H. Ney. 2003. A Systematic Comparison of
Various Statistical Alignment Models. Computational
Linguistics, 29:19?51.
P. Resnik, Mari Broman Olsen, and Mona Diab. 1999.
The bible as a parallel corpus: Annotating the ?book of
2000 tongues?. Computers and the Humanities, 33(1-
2):129?153.
D. Ringe, T. Warnow, and A. Taylor. 2002. Indo-
european and computational cladistics. Transactions
of the Philological Society, 100:59?129.
M. Swadesh. 1955. Towards greater accuracy in lex-
icostatistic dating. Journal of American Linguistics,
21:121?137.
A. Venkataraman, J. Newman, and J.D. Patrick. 1997.
A complexity measure for diachronic chinese phonol-
ogy. In J. Coleman, editor, Computational Phonology.
Association for Computational Linguistics.
G. Wu, J. A. You, and G. Lin. 2007. Quartet-based
phylogeny reconstruction with answer set program-
ming. IEEE/ACM Transactions on computational bi-
ology, 4:139?152.
Ziheng Yang and Bruce Rannala. 1997. Bayesian phy-
logenetic inference using dna sequences: A markov
chain monte carlo method. Molecular Biology and
Evolution 14.
896
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 104?111,
New York, June 2006. c?2006 Association for Computational Linguistics
Alignment by Agreement
Percy Liang
UC Berkeley
Berkeley, CA 94720
pliang@cs.berkeley.edu
Ben Taskar
UC Berkeley
Berkeley, CA 94720
taskar@cs.berkeley.edu
Dan Klein
UC Berkeley
Berkeley, CA 94720
klein@cs.berkeley.edu
Abstract
We present an unsupervised approach to
symmetric word alignment in which two
simple asymmetric models are trained
jointly to maximize a combination of
data likelihood and agreement between
the models. Compared to the stan-
dard practice of intersecting predictions of
independently-trained models, joint train-
ing provides a 32% reduction in AER.
Moreover, a simple and efficient pair of
HMM aligners provides a 29% reduction
in AER over symmetrized IBM model 4
predictions.
1 Introduction
Word alignment is an important component of a
complete statistical machine translation pipeline
(Koehn et al, 2003). The classic approaches to un-
supervised word alignment are based on IBM mod-
els 1?5 (Brown et al, 1994) and the HMM model
(Ney and Vogel, 1996) (see Och and Ney (2003) for
a systematic comparison). One can classify these
six models into two groups: sequence-based models
(models 1, 2, and HMM) and fertility-based models
(models 3, 4, and 5).1 Whereas the sequence-based
models are tractable and easily implemented, the
more accurate fertility-based models are intractable
and thus require approximation methods which are
1IBM models 1 and 2 are considered sequence-based models
because they are special cases of HMMs with transitions that do
not depend on previous states.
difficult to implement. As a result, many practition-
ers use the complex GIZA++ software package (Och
and Ney, 2003) as a black box, selecting model 4 as
a good compromise between alignment quality and
efficiency.
Even though the fertility-based models are more
accurate, there are several reasons to consider av-
enues for improvement based on the simpler and
faster sequence-based models. First, even with
the highly optimized implementations in GIZA++,
models 3 and above are still very slow to train. Sec-
ond, we seem to have hit a point of diminishing re-
turns with extensions to the fertility-based models.
For example, gains from the new model 6 of Och
and Ney (2003) are modest. When models are too
complex to reimplement, the barrier to improvement
is raised even higher. Finally, the fertility-based
models are asymmetric, and symmetrization is com-
monly employed to improve alignment quality by
intersecting alignments induced in each translation
direction. It is therefore natural to explore models
which are designed from the start with symmetry in
mind.
In this paper, we introduce a new method for word
alignment that addresses the three issues above. Our
development is motivated by the observation that in-
tersecting the predictions of two directional models
outperforms each model alone. Viewing intersec-
tion as a way of finding predictions that both models
agree on, we take the agreement idea one step fur-
ther. The central idea of our approach is to not only
make the predictions of the models agree at test time,
but also encourage agreement during training. We
define an intuitive objective function which incor-
104
porates both data likelihood and a measure of agree-
ment between models. Then we derive an EM-like
algorithm to maximize this objective function. Be-
cause the E-step is intractable in our case, we use
a heuristic approximation which nonetheless works
well in practice.
By jointly training two simple HMM models, we
obtain 4.9% AER on the standard English-French
Hansards task. To our knowledge, this is the lowest
published unsupervised AER result, and it is com-
petitive with supervised approaches. Furthermore,
our approach is very practical: it is no harder to
implement than a standard HMM model, and joint
training is no slower than the standard training of
two HMM models. Finally, we show that word
alignments from our system can be used in a phrase-
based translation system to modestly improve BLEU
score.
2 Alignment models: IBM 1, 2 and HMM
We briefly review the sequence-based word align-
ment models (Brown et al, 1994; Och and Ney,
2003) and describe some of the choices in our
implementation. All three models are generative
models of the form p(f | e) = ?a p(a, f | e),
where e = (e1, . . . , eI) is the English sentence,
f = (f1, . . . , fJ) is the French sentence, and a =
(a1, . . . , aJ ) is the (asymmetric) alignment which
specifies the position of an English word aligned to
each French word. All three models factor in the
following way:
p(a, f | e) =
J
?
j=1
pd(aj | aj? , j)pt(fj | eaj ), (1)
where j? is the position of the last non-null-aligned
French word before position j.2
The translation parameters pt(fj | eaj ) are pa-
rameterized by an (unsmoothed) lookup table that
stores the appropriate local conditional probability
distributions. The distortion parameters pd(aj = i? |
aj? = i) depend on the particular model (we write
aj = 0 to denote the event that the j-th French word
2The dependence on aj? can in fact be implemented as a
first-order HMM (see Och and Ney (2003)).
is null-aligned):
pd(aj =0 | aj?= i) = p0
pd(aj = i? 6= 0 | aj?= i) ?
(1? p0) ?
?
?
?
?
?
1 (IBM 1)
c(i??b jIJ c) (IBM 2)
c(i??i) (HMM),
where p0 is the null-word probability and c(?) con-
tains the distortion parameters for each offset argu-
ment. We set the null-word probability p0 = 1I+1
depending on the length of the English sentence,
which we found to be more effective than using a
constant p0.
In model 1, the distortion pd(? | ?) specifies a uni-
form distribution over English positions. In model
2, pd(? | ?) is still independent of aj? , but it can now
depend on j and i? through c(?). In the HMM model,
there is a dependence on aj? = i, but only through
c(i? i?).
We parameterize the distortion c(?) using a multi-
nomial distribution over 11 offset buckets c(?
?5), c(?4), . . . , c(4), c(? 5).3 We use three sets of
distortion parameters, one for transitioning into the
first state, one for transitioning out of the last state,
and one for all other transitions. This works better
than using a single set of parameters or ignoring the
transitions at the two ends.
3 Training by agreement
To motivate our joint training approach, we first
consider the standard practice of intersecting align-
ments. While the English and French sentences
play a symmetric role in the word alignment task,
sequence-based models are asymmetric: they are
generative models of the form p(f | e) (E?F), or
p(e | f) (F?E) by reversing the roles of source and
target. In general, intersecting the alignment predic-
tions of two independently-trained directional mod-
els reduces AER, e.g., from 11% to 7% for HMM
models (Table 2). This suggests that two models
make different types of errors that can be eliminated
upon intersection. Figure 1 (top) shows a common
type of error that intersection can partly remedy. In
3For each sentence, the probability mass of each of the two
end buckets c(??5) or c(? 5) is uniformly divided among
those valid offsets.
105
In
de
pe
n
de
n
tt
ra
in
in
g
w
e
de
e
m
e
d it
in
a
dv
is
a
bl
e
t
o
a
t
t
e
n
d
t
he
m
e
e
t
in
g
a
n
d
s
o
in
fo
r
m
e
d
c
o
jo .
nous
ne
avons
pas
cru
bon
de
assister
a`
la
re?union
et
en
avons
informe?
le
cojo
en
conse?quence
.
w
e
de
e
m
e
d it
in
a
dv
is
a
bl
e
t
o
a
t
t
e
n
d
t
he
m
e
e
t
in
g
a
n
d
s
o
in
fo
r
m
e
d
c
o
jo .
nous
ne
avons
pas
cru
bon
de
assister
a`
la
re?union
et
en
avons
informe?
le
cojo
en
conse?quence
.
w
e
de
e
m
e
d it
in
a
dv
is
a
bl
e
t
o
a
t
t
e
n
d
t
he
m
e
e
t
in
g
a
n
d
s
o
in
fo
r
m
e
d
c
o
jo .
nous
ne
avons
pas
cru
bon
de
assister
a`
la
re?union
et
en
avons
informe?
le
cojo
en
conse?quence
.
E?F: 84.2/92.0/13.0 F?E: 86.9/91.1/11.5 Intersection: 97.0/86.9/7.6
Jo
in
tt
ra
in
in
g
w
e
de
e
m
e
d it
in
a
dv
is
a
bl
e
t
o
a
t
t
e
n
d
t
he
m
e
e
t
in
g
a
n
d
s
o
in
fo
r
m
e
d
c
o
jo .
nous
ne
avons
pas
cru
bon
de
assister
a`
la
re?union
et
en
avons
informe?
le
cojo
en
conse?quence
.
w
e
de
e
m
e
d it
in
a
dv
is
a
bl
e
t
o
a
t
t
e
n
d
t
he
m
e
e
t
in
g
a
n
d
s
o
in
fo
r
m
e
d
c
o
jo .
nous
ne
avons
pas
cru
bon
de
assister
a`
la
re?union
et
en
avons
informe?
le
cojo
en
conse?quence
.
w
e
de
e
m
e
d it
in
a
dv
is
a
bl
e
t
o
a
t
t
e
n
d
t
he
m
e
e
t
in
g
a
n
d
s
o
in
fo
r
m
e
d
c
o
jo .
nous
ne
avons
pas
cru
bon
de
assister
a`
la
re?union
et
en
avons
informe?
le
cojo
en
conse?quence
.
E?F: 89.9/93.6/8.7 F?E: 92.2/93.5/7.3 Intersection: 96.5/91.4/5.7
Figure 1: An example of the Viterbi output of a pair of independently trained HMMs (top) and a pair of
jointly trained HMMs (bottom), both trained on 1.1 million sentences. Rounded boxes denote possible
alignments, square boxes are sure alignments, and solid boxes are model predictions. For each model, the
overall Precision/Recall/AER on the development set is given. See Section 4 for details.
this example, COJO is a rare word that becomes a
garbage collector (Moore, 2004) for the models in
both directions. Intersection eliminates the spurious
alignments, but at the expense of recall.
Intersection after training produces alignments
that both models agree on. The joint training pro-
cedure we describe below builds on this idea by en-
couraging the models to agree during training. Con-
sider the output of the jointly trained HMMs in Fig-
ure 1 (bottom). The garbage-collecting rare word is
no longer a problem. Not only are the individual
E?F and F?E jointly-trained models better than
their independently-trained counterparts, the jointly-
trained intersected model also provides a signifi-
cant overall gain over the independently-trained in-
tersected model. We maintain both high precision
and recall.
Before we introduce the objective function for
joint training, we will write the two directional mod-
els in a symmetric way so that they share the same
106
alignment spaces. We first replace the asymmetric
alignments a with a set of indicator variables for
each potential alignment edge (i, j): z = {zij ?
{0, 1} : 1 ? i ? I, 1 ? j ? J}. Each z can be
thought of as an element in the set of generalized
alignments, where any subset of word pairs may be
aligned (Och and Ney, 2003). Sequence-based mod-
els p(a | e, f) induce a distribution over p(z | e, f)
by letting p(z | e, f) = 0 for any z that does not
correspond to any a (i.e., if z contains many-to-one
alignments).
We also introduce the more compact notation
x = (e, f) to denote an input sentence pair. We
put arbitrary distributions p(e) and p(f) to remove
the conditioning, noting that this has no effect on
the optimization problem in the next section. We
can now think of the two directional sequence-based
models as each inducing a distribution over the
same space of sentence pairs and alignments (x, z):
p1(x, z; ?1) = p(e)p(a, f | e; ?1)
p2(x, z; ?2) = p(f)p(a, e | f ; ?2).
3.1 A joint objective
In the next two sections, we describe how to jointly
train the two models using an EM-like algorithm.
We emphasize that this technique is quite general
and can be applied in many different situations
where we want to couple two tractable models over
input x and output z.
To train two models p1(x, z; ?1) and p2(x, z; ?2)
independently, we maximize the data likelihood
?
x pk(x; ?k) =
?
x
?
z pk(x, z; ?k) of each model
separately, k ? {1, 2}:
max
?1,?2
?
x
[log p1(x; ?1) + log p2(x; ?2)] . (2)
Above, the summation over x enumerates the sen-
tence pairs in the training data.
There are many possible ways to quantify agree-
ment between two models. We chose a particularly
simple and mathematically convenient measure ?
the probability that the alignments produced by the
two models agree on an example x:
?
z
p1(z | x; ?1)p2(z | x; ?2).
We add the (log) probability of agreement to the
standard log-likelihood objective to couple the two
models:
max
?1,?2
?
x
[log p1(x; ?1) + log p2(x; ?2) +
log
?
z
p1(z | x; ?1)p2(z | x; ?2)]. (3)
3.2 Optimization via EM
We first review the EM algorithm for optimizing a
single model, which consists of iterating the follow-
ing two steps:
E : q(z;x) := p(z | x; ?),
M : ?? := argmax
?
?
x,z
q(z;x) log p(x, z; ?).
In the E-step, we compute the posterior distribution
of the alignments q(z;x) given the sentence pair x
and current parameters ?. In the M-step, we use ex-
pected counts with respect to q(z;x) in the maxi-
mum likelihood update ? := ??.
To optimize the objective in Equation 3, we can
derive a similar and simple procedure. See the ap-
pendix for the derivation.
E: q(z;x) := 1Zxp1(z | x; ?1)p2(z | x; ?2),
M: ?? = argmax
?
?
x,z
q(z;x) log p1(x, z; ?1)
+
?
x,z
q(z;x) log p2(x, z; ?2),
where Zx is a normalization constant. The M-step
decouples neatly into two independent optimization
problems, which lead to single model updates using
the expected counts from q(z;x). To compute Zx in
the E-step, we must sum the product of two model
posteriors over the set of possible zs with nonzero
probability under both models. In general, if both
posterior distributions over the latent variables z
decompose in the same tractable manner, as in
the context-free grammar induction work of Klein
and Manning (2004), the summation could be
carried out efficiently, for example using dynamic
programming. In our case, we would have to sum
over the set of alignments where each word in
English is aligned to at most one word in French
and each word in French is aligned to at most one
107
word in English. Unfortunately, for even very
simple models such as IBM 1 or 2, computing the
normalization constant over this set of alignments
is a #P -complete problem, by a reduction from
counting matchings in a bipartite graph (Valiant,
1979). We could perhaps attempt to compute q us-
ing a variety of approximate probabilistic inference
techniques, for example, sampling or variational
methods. With efficiency as our main concern, we
opted instead for a simple heuristic procedure by
letting q be a product of marginals:
q(z;x) :=
?
i,j
p1(zij | x; ?1)p2(zij | x; ?2),
where each pk(zij | x; ?k) is the posterior marginal
probability of the (i, j) edge being present (or ab-
sent) in the alignment according to each model,
which can be computed separately and efficiently.
Now the new E-step only requires simple
marginal computations under each of the mod-
els. This procedure is very intuitive: edges on
which the models disagree are discounted in the E-
step because the product of the marginals p1(zij |
x; ?1)p2(zij | x; ?2) is small. Note that in general,
this new procedure is not guaranteed to increase our
joint objective. Nonetheless, our experimental re-
sults show that it provides an effective method of
achieving model agreement and leads to significant
accuracy gains over independent training.
3.3 Prediction
Once we have trained two models, either jointly
or independently, we must decide how to combine
those two models to predict alignments for new sen-
tences.
First, let us step back to the case of one model.
Typically, the Viterbi alignment argmaxz p(z | x)
is used. An alternative is to use posterior decoding,
where we keep an edge (i, j) if the marginal edge
posterior p(zij | x) exceeds some threshold 0 < ? <
1. In symbols, z = {zij = 1 : p(zij = 1 | x) ? ?}.4
Posterior decoding has several attractive advan-
tages over Viterbi decoding. Varying the threshold
? gives a natural way to tradeoff precision and re-
call. In fact, these posteriors could be used more di-
4See Matusov et al (2004) for an alternative use of these
marginals.
rectly in extracting phrases for phrase-based trans-
lation. Also, when we want to combine two mod-
els for prediction, finding the Viterbi alignment
argmaxz p1(z | x)p2(z | x) is intractable for
HMM models (by a reduction from quadratic as-
signment), and a hard intersection argmaxz1 p1(z1 |
x) ? argmaxz2 p2(z2 | x) might be too sparse.
On the other hand, we can threshold the product of
two edge posteriors quite easily: z = {zij = 1 :
p1(zij = 1 | x)p2(zij = 1 | x) ? ?}.
We noticed a 5.8% relative reduction in AER (for
our best model) by using posterior decoding with a
validation-set optimized threshold ? instead of using
hard intersection of Viterbi alignments.
4 Experiments
We tested our approach on the English-French
Hansards data from the NAACL 2003 Shared Task,
which includes a training set of 1.1 million sen-
tences, a validation set of 37 sentences, and a test set
of 447 sentences. The validation and test sentences
have been hand-aligned (see Och and Ney (2003))
and are marked with both sure and possible align-
ments. Using these alignments, alignment error rate
(AER) is calculated as:
(
1? |A ? S|+ |A ? P ||A|+ |S|
)
? 100%,
where A is a set of proposed edges, S is the sure
gold edges, and P is the possible gold edges.
As a preprocessing step, we lowercased all words.
Then we used the validation set and the first 100 sen-
tences of the test set as our development set to tune
our models. Lastly, we ran our models on the last
347 sentences of the test set to get final AER results.
4.1 Basic results
We trained models 1, 2, and HMM on the Hansards
data. Following past work, we initialized the trans-
lation probabilities of model 1 uniformly over word
pairs that occur together in some sentence pair.
Models 2 and HMM were initialized with uni-
form distortion probabilities and model 1 translation
probabilities. Each model was trained for 5 itera-
tions, using the same training regimen as in Och and
Ney (2003).
108
Model Indep. Joint Reduction
10K sentences
Model 1 27.4 23.6 13.8
Model 2 18.2 14.9 18.5
HMM 12.1 8.4 30.6
100K sentences
Model 1 21.5 19.2 10.9
Model 2 13.1 10.2 21.7
HMM 8.0 5.3 33.1
1.1M sentences
Model 1 20.0 16.5 17.5
Model 2 11.4 9.2 18.8
HMM 6.6 5.2 21.5
Table 1: Comparison of AER between independent
and joint training across different size training sets
and different models, evaluated on the development
set. The last column shows the relative reduction in
AER.
Table 1 shows a summary of the performance of
independently and jointly trained models under var-
ious training conditions. Quite remarkably, for all
training data sizes and all of the models, we see
an appreciable reduction in AER, especially on the
HMM models. We speculate that since the HMM
model provides a richer family of distributions over
alignments than either models 1 or 2, we can learn
to synchronize the predictions of the two models,
whereas models 1 and 2 have a much more limited
capacity to synchronize.
Table 2 shows the HMM models compared to
model 4 alignments produced by GIZA++ on the test
set. Our jointly trained model clearly outperforms
not only the standard HMM but also the more com-
plex IBM 4 model. For these results, the threshold
used for posterior decoding was tuned on the devel-
opment set. ?GIZA HMM? and ?HMM, indep? are
the same algorithm but differ in implementation de-
tails. The E?F and F?E models benefit a great
deal by moving from independent to joint training,
and the combined models show a smaller improve-
ment.
Our best performing model differs from standard
IBM word alignment models in two ways. First and
most importantly, we use joint training instead of
Model E?F F?E Combined
GIZA HMM 11.5 11.5 7.0
GIZA Model 4 8.9 9.7 6.9
HMM, indep 11.2 11.5 7.2
HMM, joint 6.1 6.6 4.9
Table 2: Comparison of test set AER between vari-
ous models trained on the full 1.1 million sentences.
Model I+V I+P J+V J+P
10K sentences
Model 1 29.4 27.4 22.7 23.6
Model 2 20.1 18.2 16.5 14.9
HMM 15.2 12.1 8.9 8.4
100K sentences
Model 1 22.9 21.5 18.6 19.2
Model 2 15.1 13.1 12.9 10.2
HMM 9.2 8.0 6.0 5.3
1.1M sentences
Model 1 20.0 19.4 16.5 17.3
Model 2 12.7 11.4 11.6 9.2
HMM 7.6 6.6 5.7 5.2
Table 3: Contributions of using joint training versus
independent training and posterior decoding (with
the optimal threshold) instead of Viterbi decoding,
evaluated on the development set.
independent training, which gives us a huge boost.
The second change, which is more minor and or-
thogonal, is using posterior decoding instead of
Viterbi decoding, which also helps performance for
model 2 and HMM, but not model 1. Table 3 quan-
tifies the contribution of each of these two dimen-
sions.
Posterior decoding In our results, we have tuned
our threshold to minimize AER. It turns out that
AER is relatively insensitive to the threshold as Fig-
ure 2 shows. There is a large range from 0.2 to 0.5
where posterior decoding outperforms Viterbi de-
coding.
Initialization and convergence In addition to im-
proving performance, joint training also enjoys cer-
tain robustness properties. Specialized initialization
is absolutely crucial for an independently-trained
109
 0
 2
 4
 6
 8
 10
 12
 14
 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9
Pe
rfo
rm
an
ce
Posterior threshold
100-Precision
100-Recall
AER
Viterbi AER
Figure 2: The precision, recall, and AER as the
threshold is varied for posterior decoding in a jointly
trained pair of HMMs.
HMM model. If we initialize the HMM model with
uniform translation parameters, the HMM converges
to a completely senseless local optimum with AER
above 50%. Initializing the HMM with model 1 pa-
rameters alleviates this problem.
On the other hand, if we jointly train two HMMs
starting from a uniform initialization, the HMMs
converge to a surprisingly good solution. On the full
training set, training two HMMs jointly from uni-
form initialization yields 5.7% AER, only slightly
higher than 5.2% AER using model 1 initialization.
We suspect that the agreement term of the objective
forces the two HMMs to avoid many local optima
that each one would have on its own, since these lo-
cal optima correspond to posteriors over alignments
that would be very unlikely to agree. We also ob-
served that jointly trained HMMs converged very
quickly?in 5 iterations?and did not exhibit over-
fitting with increased iterations.
Common errors The major source of remaining
errors are recall errors that come from the shortcom-
ings of the HMM model. The E?F model gives 0
probability to any many-to-one alignments and the
F?E model gives 0 probability to any one-to-many
alignments. By enforcing agreement, the two mod-
els are effectively restricted to one-to-one (or zero)
alignments. Posterior decoding is in principle ca-
pable of proposing many-to-many alignments, but
these alignments occur infrequently since the poste-
riors are generally sharply peaked around the Viterbi
alignment. In some cases, however, we do get one-
to-many alignments in both directions.
Another common type of errors are precision er-
rors due to the models overly-aggressively prefer-
ring alignments that preserve monotonicity. Our
HMM model only uses 11 distortion parameters,
which means distortions are not sensitive to the lex-
ical context of the sentences. For example, in one
sentence, le is incorrectly aligned to the as a mono-
tonic alignment following another pair of correctly
aligned words, and then the monotonicity is broken
immediately following le?the. Here, the model is
insensitive to the fact that alignments following arti-
cles tend to be monotonic, but alignments preceding
articles are less so.
Another phenomenon is the insertion of ?stepping
stone? alignments. Suppose two edges (i, j) and
(i+4, j+4) have a very high probability of being in-
cluded in an alignment, but the words between them
are not good translations of each other. If the inter-
vening English words were null-aligned, we would
have to pay a big distortion penalty for jumping 4
positions. On the other hand, if the edge (i+2, j+2)
were included, that penalty would be mitigated. The
translation cost for forcing that edge is smaller than
the distortion cost.
4.2 BLEU evaluation
To see whether our improvement in AER also im-
proves BLEU score, we aligned 100K English-
French sentences from the Europarl corpus and
tested on 3000 sentences of length 5?15. Using
GIZA++ model 4 alignments and Pharaoh (Koehn
et al, 2003), we achieved a BLEU score of 0.3035.
By using alignments from our jointly trained HMMs
instead, we get a BLEU score of 0.3051. While this
improvement is very modest, we are currently inves-
tigating alternative ways of interfacing with phrase
table construction to make a larger impact on trans-
lation quality.
5 Related Work
Our approach is similar in spirit to co-training,
where two classifiers, complementary by the virtue
of having different views of the data, are trained
jointly to encourage agreement (Blum and Mitchell,
1998; Collins and Singer, 1999). One key difference
110
in our work is that we rely exclusively on data like-
lihood to guide the two models in an unsupervised
manner, rather than relying on an initial handful of
labeled examples.
The idea of exploiting agreement between two la-
tent variable models is not new; there has been sub-
stantial previous work on leveraging the strengths
of two complementary models. Klein and Man-
ning (2004) combine two complementary mod-
els for grammar induction, one that models con-
stituency and one that models dependency, in a man-
ner broadly similar to the current work. Aside from
investigating a different domain, one novel aspect of
this paper is that we present a formal objective and a
training algorithm for combining two generic mod-
els.
6 Conclusion
We have described an efficient and fully unsuper-
vised method of producing state-of-the-art word
alignments. By training two simple sequence-based
models to agree, we achieve substantial error re-
ductions over standard models. Our jointly trained
HMM models reduce AER by 29% over test-time
intersected GIZA++ model 4 alignments and also
increase our robustness to varying initialization reg-
imens. While AER is only a weak indicator of final
translation quality in many current translation sys-
tems, we hope that more accurate alignments can
eventually lead to improvements in the end-to-end
translation process.
Acknowledgments We thank the anonymous re-
viewers for their comments.
References
Avrim Blum and Tom Mitchell. 1998. Combining Labeled
and Unlabeled Data with Co-training. In Proceedings of the
COLT 1998.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra,
and Robert L. Mercer. 1994. The Mathematics of Statistical
Machine Translation: Parameter Estimation. Computational
Linguistics, 19:263?311.
Michael Collins and Yoram Singer. 1999. Unsupervised Mod-
els for Named Entity Classification. In Proceedings of
EMNLP 1999.
Abraham Ittycheriah and Salim Roukos. 2005. A maximum
entropy word aligner for arabic-english machine translation.
In Proceedings of HLT-EMNLP.
Dan Klein and Christopher D. Manning. 2004. Corpus-Based
Induction of Syntactic Structure: Models of Dependency and
Constituency. In Proceedings of ACL 2004.
Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Sta-
tistical Phrase-Based Translation. In Proceedings of HLT-
NAACL 2003.
E. Matusov, Zens. R., and H. Ney. 2004. Symmetric word
alignments for statistical machine translation. In Proceed-
ings of the 20th International Conference on Computational
Linguistics, August.
Robert C. Moore. 2004. Improving IBM Word Alignment
Model 1. In Proceedings of ACL 2004.
Robert C. Moore. 2005. A discriminative framework for bilin-
gual word alignment. In Proceedings of EMNLP.
Hermann Ney and Stephan Vogel. 1996. HMM-Based Word
Alignment in Statistical Translation. In COLING.
Franz Josef Och and Hermann Ney. 2003. A Systematic Com-
parison of Various Statistical Alignment Models. Computa-
tional Linguistics, 29:19?51.
Ben Taskar, Simon Lacoste-Julien, and Dan Klein. 2005. A
Discriminative Matching Approach to Word Alignment. In
Proceedings of EMNLP 2005.
L. G. Valiant. 1979. The complexity of computing the perma-
nent. Theoretical Computer Science, 8:189?201.
Appendix: Derivation of agreement EM
To simplify notation, we drop the explicit reference
to the parameters ?. Lower bound the objective in
Equation 3 by introducing a distribution q(z;x) and
using the concavity of log:
X
x
log p1(x)p2(x)
X
z
p1(z | x)p2(z | x) (4)
?
X
x,z
q(z;x) log p1(x)p2(x)p1(z | x)p2(z | x)q(z;x) (5)
=
X
x,z
q(z;x) log p1(z | x)p2(z | x)q(z;x) + C (6)
=
X
x,z
q(z;x) log p1(x, z)p2(x, z) + D, (7)
where C depends only on ? but not q and D de-
pends only q but not ?. The E-step chooses q given
a fixed ? to maximize the lower bound. Equation 6
is exactly
?
x?KL(q||p1p2) + C , which is maxi-
mized by setting q proportional to p1p2. The M-step
chooses ? given a fixed q. Equation 7 decomposes
into two separate optimization problems.
111
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 611?619,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Online EM for Unsupervised Models
Percy Liang Dan Klein
Computer Science Division, EECS Department
University of California at Berkeley
Berkeley, CA 94720
{pliang,klein}@cs.berkeley.edu
Abstract
The (batch) EM algorithm plays an important
role in unsupervised induction, but it some-
times suffers from slow convergence. In this
paper, we show that online variants (1) provide
significant speedups and (2) can even find bet-
ter solutions than those found by batch EM.
We support these findings on four unsuper-
vised tasks: part-of-speech tagging, document
classification, word segmentation, and word
alignment.
1 Introduction
In unsupervised NLP tasks such as tagging, parsing,
and alignment, one wishes to induce latent linguistic
structures from raw text. Probabilistic modeling has
emerged as a dominant paradigm for these problems,
and the EM algorithm has been a driving force for
learning models in a simple and intuitive manner.
However, on some tasks, EM can converge
slowly. For instance, on unsupervised part-of-
speech tagging, EM requires over 100 iterations to
reach its peak performance on the Wall-Street Jour-
nal (Johnson, 2007). The slowness of EM is mainly
due to its batch nature: Parameters are updated only
once after each pass through the data. When param-
eter estimates are still rough or if there is high redun-
dancy in the data, computing statistics on the entire
dataset just to make one update can be wasteful.
In this paper, we investigate two flavors of on-
line EM?incremental EM (Neal and Hinton, 1998)
and stepwise EM (Sato and Ishii, 2000; Cappe? and
Moulines, 2009), both of which involve updating pa-
rameters after each example or after a mini-batch
(subset) of examples. Online algorithms have the
potential to speed up learning by making updates
more frequently. However, these updates can be
seen as noisy approximations to the full batch up-
date, and this noise can in fact impede learning.
This tradeoff between speed and stability is famil-
iar to online algorithms for convex supervised learn-
ing problems?e.g., Perceptron, MIRA, stochastic
gradient, etc. Unsupervised learning raises two ad-
ditional issues: (1) Since the EM objective is non-
convex, we often get convergence to different local
optima of varying quality; and (2) we evaluate on
accuracy metrics which are at best loosely correlated
with the EM likelihood objective (Liang and Klein,
2008). We will see that these issues can lead to sur-
prising results.
In Section 4, we present a thorough investigation
of online EM, mostly focusing on stepwise EM since
it dominates incremental EM. For stepwise EM, we
find that choosing a good stepsize and mini-batch
size is important but can fortunately be done ade-
quately without supervision. With a proper choice,
stepwise EM reaches the same performance as batch
EM, but much more quickly. Moreover, it can even
surpass the performance of batch EM. Our results
are particularly striking on part-of-speech tagging:
Batch EM crawls to an accuracy of 57.3% after 100
iterations, whereas stepwise EM shoots up to 65.4%
after just two iterations.
2 Tasks, models, and datasets
In this paper, we focus on unsupervised induction
via probabilistic modeling. In particular, we define
a probabilistic model p(x, z; ?) of the input x (e.g.,
611
a sentence) and hidden output z (e.g., a parse tree)
with parameters ? (e.g., rule probabilities). Given a
set of unlabeled examples x(1), . . . ,x(n), the stan-
dard training objective is to maximize the marginal
log-likelihood of these examples:
`(?) =
n?
i=1
log p(x(i); ?). (1)
A trained model ?? is then evaluated on the accuracy
of its predictions: argmaxz p(z | x(i); ??) against the
true output z(i); the exact evaluation metric depends
on the task. What makes unsupervised induction
hard at best and ill-defined at worst is that the train-
ing objective (1) does not depend on the true outputs
at all.
We ran experiments on four tasks described be-
low. Two of these tasks?part-of-speech tagging and
document classification?are ?clustering? tasks. For
these, the output z consists of labels; for evalua-
tion, we map each predicted label to the true label
that maximizes accuracy. The other two tasks?
segmentation and alignment?only involve unla-
beled combinatorial structures, which can be eval-
uated directly.
Part-of-speech tagging For each sentence x =
(x1, . . . , x`), represented as a sequence of words, we
wish to predict the corresponding sequence of part-
of-speech (POS) tags z = (z1, . . . , z`). We used
a simple bigram HMM trained on the Wall Street
Journal (WSJ) portion of the Penn Treebank (49208
sentences, 45 tags). No tagging dictionary was used.
We evaluated using per-position accuracy.
Document classification For each document x =
(x1, . . . , x`) consisting of ` words,1 we wish to pre-
dict the document class z ? {1, . . . , 20}. Each doc-
ument x is modeled as a bag of words drawn inde-
pendently given the class z. We used the 20 News-
groups dataset (18828 documents, 20 classes). We
evaluated on class accuracy.
Word segmentation For each sentence x =
(x1, . . . , x`), represented as a sequence of English
phonemes or Chinese characters without spaces
separating the words, we would like to predict
1We removed the 50 most common words and words that
occurred fewer than 5 times.
a segmentation of the sequence into words z =
(z1, . . . , z|z|), where each segment (word) zi is a
contiguous subsequence of 1, . . . , `. Since the na??ve
unigram model has a degenerate maximum likeli-
hood solution that makes each sentence a separate
word, we incorporate a penalty for longer segments:
p(x, z; ?) ? ?|z|k=1 p(xzk ; ?)e?|zk|? , where ? > 1determines the strength of the penalty. For English,
we used ? = 1.6; Chinese, ? = 2.5. To speed up in-
ference, we restricted the maximum segment length
to 10 for English and 5 for Chinese.
We applied this model on the Bernstein-Ratner
corpus from the CHILDES database used in
Goldwater et al (2006) (9790 sentences) and
the Academia Sinica (AS) corpus from the first
SIGHAN Chinese word segmentation bakeoff (we
used the first 100K sentences). We evaluated using
F1 on word tokens.
To the best of our knowledge, our penalized uni-
gram model is new and actually beats the more com-
plicated model of Johnson (2008) 83.5% to 78%,
which had been the best published result on this task.
Word alignment For each pair of translated sen-
tences x = (e1, . . . , ene , f1, . . . , fnf ), we wish to
predict the word alignments z ? {0, 1}nenf . We
trained two IBM model 1s using agreement-based
learning (Liang et al, 2008). We used the first
30K sentence pairs of the English-French Hansards
data from the NAACL 2003 Shared Task, 447+37
of which were hand-aligned (Och and Ney, 2003).
We evaluated using the standard alignment error rate
(AER).
3 EM algorithms
Given a probabilistic model p(x, z; ?) and unla-
beled examples x(1), . . . ,x(n), recall we would like
to maximize the marginal likelihood of the data
(1). Let ?(x, z) denote a mapping from a fully-
labeled example (x, z) to a vector of sufficient statis-
tics (counts in the case of multinomials) for the
model. For example, one component of this vec-
tor for HMMs would be the number of times state
7 emits the word ?house? in sentence x with state
sequence z. Given a vector of sufficient statistics ?,
let ?(?) denote the maximum likelihood estimate. In
our case, ?(?) are simply probabilities obtained by
normalizing each block of counts. This closed-form
612
Batch EM
?? initialization
for each iteration t = 1, . . . , T :
??? ? 0
?for each example i = 1, . . . , n:
??s?i ?
?
z p(z | x(i); ?(?))?(x(i), z) [inference]
???? ? ?? + s?i [accumulate new]
??? ?? [replace old with new]
solution is one of the features that makes EM (both
batch and online) attractive.
3.1 Batch EM
In the (batch) EM algorithm, we alternate between
the E-step and the M-step. In the E-step, we com-
pute the expected sufficient statistics ?? across all
the examples based on the posterior over z under the
current parameters ?(?). In all our models, this step
can be done via a dynamic program (for example,
forward-backward for POS tagging).
In the M-step, we use these sufficient statistics
?? to re-estimate the parameters. Since the M-step
is trivial, we represent it implicitly by ?(?) in order
to concentrate on the computation of the sufficient
statistics. This focus will be important for online
EM, so writing batch EM in this way accentuates
the parallel between batch and online.
3.2 Online EM
To obtain an online EM algorithm, we store a sin-
gle set of sufficient statistics ? and update it after
processing each example. For the i-th example, we
compute sufficient statistics s?i. There are two main
variants of online EM algorithms which differ in ex-
actly how the new s?i is incorporated into ?.
The first is incremental EM (iEM) (Neal and Hin-
ton, 1998), in which we not only keep track of ? but
also the sufficient statistics s1, . . . , sn for each ex-
ample (? =?ni=1 si). When we process example i,
we subtract out the old si and add the new s?i.
Sato and Ishii (2000) developed another variant,
later generalized by Cappe? and Moulines (2009),
which we call stepwise EM (sEM). In sEM, we in-
terpolate between ? and s?i based on a stepsize ?k (k
is the number of updates made to ? so far).
The two algorithms are motivated in different
ways. Recall that the log-likelihood can be lower
Incremental EM (iEM)
si ? initialization for i = 1, . . . , n
???ni=1 sifor each iteration t = 1, . . . , T :
?for each example i = 1, . . . , n in random order:
??s?i ?
?
z p(z | x(i); ?(?))?(x(i), z) [inference]
???? ?+ s?i ? si; si ? s?i [replace old with new]
Stepwise EM (sEM)
?? initialization; k = 0
for each iteration t = 1, . . . , T :
?for each example i = 1, . . . , n in random order:
??s?i ?
?
z p(z | x(i); ?(?))?(x(i), z) [inference]
???? (1??k)?+ ?ks?i; k ? k+1 [towards new]
bounded as follows (Neal and Hinton, 1998):
`(?) ? L(q1, . . . , qn, ?) (2)
def=
n?
i=1
[?
z
qi(z | x(i)) log p(x(i), z; ?) +H(qi)
]
,
where H(qi) is the entropy of the distribution qi(z |
x(i)). Batch EM alternates between optimizing L
with respect to q1, . . . , qn in the E-step (represented
implicitly via sufficient statistics ??) and with re-
spect to ? in the M-step. Incremental EM alternates
between optimizing with respect to a single qi and ?.
Stepwise EM is motivated from the stochastic ap-
proximation literature, where we think of approxi-
mating the update ?? in batch EM with a single sam-
ple s?i. Since one sample is a bad approximation,
we interpolate between s?i and the current ?. Thus,
sEM can be seen as stochastic gradient in the space
of sufficient statistics.
Stepsize reduction power ? Stepwise EM leaves
open the choice of the stepsize ?k. Standard results
from the stochastic approximation literature state
that ??k=0 ?k = ? and
??
k=0 ?2k < ? are suffi-
cient to guarantee convergence to a local optimum.
In particular, if we take ?k = (k + 2)??, then any
0.5 < ? ? 1 is valid. The smaller the ?, the larger
the updates, and the more quickly we forget (decay)
our old sufficient statistics. This can lead to swift
progress but also generates instability.
Mini-batch size m We can add some stability
to sEM by updating on multiple examples at once
613
instead of just one. In particular, partition the
n examples into mini-batches of size m and run
sEM, treating each mini-batch as a single exam-
ple. Formally, for each i = 0,m, 2m, 3m, . . . , first
compute the sufficient statistics s?i+1, . . . , s?i+m on
x(i+1), . . . ,x(i+m) and then update ? using s?i+1 +
? ? ? + s?i+m. The larger the m, the less frequent
the updates, but the more stable they are. In this
way, mini-batches interpolate between a pure online
(m = 1) and a pure batch (m = n) algorithm.2
Fast implementation Due to sparsity in NLP, the
sufficient statistics of an example s?i are nonzero for
a small fraction of its components. For iEM, the
time required to update ? with s?i depends only on
the number of nonzero components of s?i. However,
the sEM update is ?? (1??k)?+?ks?i, and a na??ve
implementation would take time proportional to the
total number of components. The key to a more effi-
cient solution is to note that ?(?) is invariant to scal-
ing of ?. Therefore, we can store S = ?Q
j<k(1??j)instead of ? and make the following sparse update:
S ? S + ?kQ
j?k(1??j)
s?i, taking comfort in the fact
that ?(?) = ?(S).
For both iEM and sEM, we also need to efficiently
compute ?(?). We can do this by maintaining the
normalizer for each multinomial block (sum of the
components in the block). This extra maintenance
only doubles the number of updates we have to make
but allows us to fetch any component of ?(?) in con-
stant time by dividing out the normalizer.
3.3 Incremental versus stepwise EM
Incremental EM increases L monotonically after
each update by virtue of doing coordinate-wise as-
cent and thus is guaranteed to converge to a local
optimum of both L and ` (Neal and Hinton, 1998).
However, ` is not guaranteed to increase after each
update. Stepwise EM might not increase either L or
` after each update, but it is guaranteed to converge
to a local optimum of ` given suitable conditions on
the stepsize discussed earlier.
Incremental and stepwise EM actually coincide
under the following setting (Cappe? and Moulines,
2Note that running sEM with m = n is similar but not
equivalent to batch EM since old sufficient statistics are still
interpolated rather than replaced.
2009): If we set (?,m) = (1, 1) for sEM and ini-
tialize all si = 0 for iEM, then both algorithms make
the same updates on the first pass through the data.
They diverge thereafter as iEM subtracts out old sis,
while sEM does not even remember them.
One weakness of iEM is that its memory require-
ments grow linearly with the number of examples
due to storing s1, . . . , sn. For large datasets, these
sis might not even fit in memory, and resorting to
physical disk would be very slow. In contrast, the
memory usage of sEM does not depend on n.
The relationship between iEM and sEM (with
m = 1) is analogous to the one between exponen-
tiated gradient (Collins et al, 2008) and stochastic
gradient for supervised learning of log-linear mod-
els. The former maintains the sufficient statistics of
each example and subtracts out old ones whereas the
latter does not. In the supervised case, the added sta-
bility of exponentiated gradient tends to yield bet-
ter performance. For the unsupervised case, we will
see empirically that remembering the old sufficient
statistics offers no benefit, and much better perfor-
mance can be obtained by properly setting (?,m)
for sEM (Section 4).
4 Experiments
We now present our empirical results for batch EM
and online EM (iEM and sEM) on the four tasks de-
scribed in Section 2: part-of-speech tagging, docu-
ment classification, word segmentation (English and
Chinese), and word alignment.
We used the following protocol for all experi-
ments: We initialized the parameters to a neutral set-
ting plus noise to break symmetries.3 Training was
performed for 20 iterations.4 No parameter smooth-
ing was used. All runs used a fixed random seed for
initializing the parameters and permuting the exam-
ples at the beginning of each iteration. We report two
performance metrics: log-likelihood normalized by
the number of examples and the task-specific accu-
racy metric (see Section 2). All numbers are taken
from the final iteration.
3Specifically, for each block of multinomial probabilities
?1, . . . , ?K , we set ?k ? exp{10?3(1 + ak)}, where ak ?
U [0, 1]. Exception: for batch EM on POS tagging, we used 1
instead of 10?3; more noise worked better.
4Exception: for batch EM on POS tagging, 100 iterations
was needed to get satisfactory performance.
614
Stepwise EM (sEM) requires setting two
optimization parameters: the stepsize reduc-
tion power ? and the mini-batch size m (see
Section 3.2). As Section 4.3 will show, these
two parameters can have a large impact on
performance. As a default rule of thumb, we
chose (?,m) ? {0.5, 0.6, 0.7, 0.8, 0.9, 1.0} ?
{1, 3, 10, 30, 100, 300, 1K, 3K, 10K} to maximize
log-likelihood; let sEM` denote stepwise EM with
this setting. Note that this setting requires no labeled
data. We will also consider fixing (?,m) = (1, 1)
(sEMi) and choosing (?,m) to maximize accuracy
(sEMa).
In the results to follow, we first demonstrate that
online EM is faster (Section 4.1) and sometimes
leads to higher accuracies (Section 4.2). Next, we
explore the effect of the optimization parameters
(?,m) (Section 4.3), briefly revisiting the connec-
tion between incremental and stepwise EM. Finally,
we show the stability of our results under different
random seeds (Section 4.4).
4.1 Speed
One of the principal motivations for online EM
is speed, and indeed we found this motivation to
be empirically well-justified. Figure 1 shows that,
across all five datasets, sEM` converges to a solution
with at least comparable log-likelihood and accuracy
with respect to batch EM, but sEM` does it anywhere
from about 2 (word alignment) to 10 (POS tagging)
times faster. This supports our intuition that more
frequent updates lead to faster convergence. At the
same time, note that the other two online EM vari-
ants in Figure 1 (iEM and sEMi) are prone to catas-
trophic failure. See Section 4.3 for further discus-
sion on this issue.
4.2 Performance
It is fortunate but perhaps not surprising that step-
wise EM is faster than batch EM. But Figure 1 also
shows that, somewhat surprisingly, sEM` can actu-
ally converge to a solution with higher accuracy, in
particular on POS tagging and document classifica-
tion. To further explore the accuracy-increasing po-
tential of sEM, consider choosing (?,m) to maxi-
mize accuracy (sEMa). Unlike sEM`, sEMa does re-
quire labeled data. In practice, (?,m) can be tuned
EM sEM` sEMa ?` m` ?a ma
POS 57.3 59.6 66.7 0.7 3 0.5 3
DOC 39.1 47.8 49.9 0.8 1K 0.5 3K
SEG(en) 80.5 80.7 83.5 0.7 1K 1.0 100
SEG(ch) 78.2 77.2 78.1 0.6 10K 1.0 10K
ALIGN 78.8 78.9 78.9 0.7 10K 0.7 10K
Table 1: Accuracy of batch EM and stepwise EM, where
the optimization parameters (?,m) are tuned to either
maximize log-likelihood (sEM`) or accuracy (sEMa).
With an appropriate setting of (?,m), stepwise EM out-
performs batch EM significantly on POS tagging and
document classification.
on a small labeled set alng with any model hyper-
parameters.
Table 1 shows that sEMa improves the accuracy
compared to batch EM even more than sEM`. The
result for POS is most vivid: After one iteration of
batch EM, the accuracy is only at 24.0% whereas
sEMa is already at 54.5%, and after two iterations,
at 65.4%. Not only is this orders of magnitude faster
than batch EM, batch EM only reaches 57.3% after
100 iterations.
We get a similarly striking result for document
classification, but the results for word segmentation
and word alignment are more modest. A full un-
derstanding of this phenomenon is left as an open
problem, but we will comment on one difference be-
tween the tasks where sEM improves accuracy and
the tasks where it doesn?t. The former are ?clus-
tering? tasks (POS tagging and document classifi-
cation), while the latter are ?structural? tasks (word
segmentation and word alignment). Learning of
clustering models centers around probabilities over
words given a latent cluster label, whereas in struc-
tural models, there are no cluster labels, and it is
the combinatorial structure (the segmentations and
alignments) that drives the learning.
Likelihood versus accuracy From Figure 1, we
see that stepwise EM (sEM`) can outperform batch
EM in both likelihood and accuracy. This suggests
that stepwise EM is better at avoiding local minima,
perhaps leveraging its stochasticity to its advantage.
However, on POS tagging, tuning sEM to maxi-
mize accuracy (sEMa) results in a slower increase
in likelihood: compare sEMa in Figure 2 with sEM`
in Figure 1(a). This shouldn?t surprise us too much
given that likelihood and accuracy are only loosely
615
20 40 60 80
iterations
0.2
0.4
0.6
0.8
1.0
acc
ura
cy
20 40 60 80
iterations
-9.8
-8.8
-7.8
-6.9
-5.9
log
-lik
elih
ood
EM
sEMi
sEM`
2 4 6 8 10
iterations
0.2
0.4
0.6
0.8
1.0
acc
ura
cy
2 4 6 8 10
iterations
-9.8
-9.3
-8.8
-8.3
-7.8
log
-lik
elih
ood
EM
iEM
sEMi
sEM`
(a) POS tagging (b) Document classification
2 4 6 8 10
iterations
0.2
0.4
0.6
0.8
1.0
F 1
2 4 6 8 10
iterations
-4.8
-4.6
-4.4
-4.2
-4.0
log
-lik
elih
ood
EM
iEM
sEMi
sEM`
2 4 6 8 10
iterations
0.2
0.4
0.6
0.8
1.0
F 1
2 4 6 8 10
iterations
-9.5
-8.9
-8.4
-7.8
-7.2
log
-lik
elih
ood
EM
iEM
sEMi
sEM`
(c) Word segmentation (English) (d) Word segmentation (Chinese)
2 4 6 8 10
iterations
0.2
0.4
0.6
0.8
1.0
1?
AE
R
2 4 6 8 10
iterations
-10.9
-9.4
-7.9
-6.5
-5.0
log
-lik
elih
ood
EM
iEM
sEMi
sEM`
accuracy log-likelihood
EM sEM` EM sEM`pos 57.3 59.6 -6.03 -6.08doc 39.1 47.8 -7.96 -7.88seg(en) 80.5 80.7 -4.11 -4.11seg(ch) 78.2 77.2 -7.27 -7.28align 78.8 78.9 -5.05 -5.12
(e) Word alignment (f) Results after convergence
Figure 1: Accuracy and log-likelihood plots for batch EM, incremental EM, and stepwise EM across all five datasets.
sEM` outperforms batch EM in terms of convergence speed and even accuracy and likelihood; iEM and sEMi fail in
some cases. We did not run iEM on POS tagging due to memory limitations; we expect the performance would be
similar to sEMi, which is not very encouraging (Section 4.3).
correlated (Liang and Klein, 2008). But it does sug-
gest that stepwise EM is injecting a bias that favors
accuracy over likelihood?a bias not at all reflected
in the training objective.
We can create a hybrid (sEMa+EM) that com-
bines the strengths of both sEMa and EM: First run
sEMa for 5 iterations, which quickly takes us to a
part of the parameter space yielding good accura-
cies; then run EM, which quickly improves the like-
lihood. Fortunately, accuracy does not degrade as
likelihood increases (Figure 2).
4.3 Varying the optimization parameters
Recall that stepwise EM requires setting two opti-
mization parameters: the stepsize reduction power ?
and the mini-batch size m. We now explore the ef-
fect of (?,m) on likelihood and accuracy.
As mentioned in Section 3.2, larger mini-batches
(increasing m) stabilize parameter updates, while
larger stepsizes (decreasing ?) provide swifter
616
doc accuracy?\m 1 3 10 30 100 300 1K 3K 10K0.5 5.4 5.4 5.5 5.6 6.0 25.7 48.8 49.9 44.60.6 5.4 5.4 5.6 5.6 22.3 36.1 48.7 49.3 44.20.7 5.5 5.5 5.6 11.1 39.9 43.3 48.1 49.0 43.50.8 5.6 5.6 6.0 21.7 47.3 45.0 47.8 49.5 42.80.9 5.8 6.0 13.4 32.4 48.7 48.4 46.4 49.4 42.41.0 6.2 11.8 19.6 35.2 47.6 49.5 47.5 49.3 41.7
pos doc align
doc log-likelihood?\m 1 3 10 30 100 300 1K 3K 10K0.5 -8.875 -8.71 -8.61 -8.555 -8.505 -8.172 -7.92 -7.906 -7.9160.6 -8.604 -8.575 -8.54 -8.524 -8.235 -8.041 -7.898 -7.901 -7.9160.7 -8.541 -8.533 -8.531 -8.354 -8.023 -7.943 -7.886 -7.896 -7.9180.8 -8.519 -8.506 -8.493 -8.228 -7.933 -7.896 -7.883 -7.89 -7.9220.9 -8.505 -8.486 -8.283 -8.106 -7.91 -7.889 -7.889 -7.891 -7.9271.0 -8.471 -8.319 -8.204 -8.052 -7.919 -7.889 -7.892 -7.896 -7.937
seg(en) seg(ch)
Figure 3: Effect of optimization parameters (stepsize reduction power ? and mini-batch size m) on accuracy and
likelihood. Numerical results are shown for document classification. In the interest of space, the results for each task
are compressed into two gray scale images, one for accuracy (top) and one for log-likelihood (bottom), where darker
shades represent larger values. Bold (red) numbers denote the best ? for a given m.
20 40 60 80
iterations
0.2
0.4
0.6
0.8
1.0
acc
ura
cy
20 40 60 80
iterations
-12.7
-11.0
-9.3
-7.6
-5.9
log
-lik
elih
ood
EM
sEMa
sEMa+EM
Figure 2: sEMa quickly obtains higher accuracy than
batch EM but suffers from a slower increase in likeli-
hood. The hybrid sEMa+EM (5 iterations of EMa fol-
lowed by batch EM) increases both accuracy and likeli-
hood sharply.
progress. Remember that since we are dealing with a
nonconvex objective, the choice of stepsize not only
influences how fast we converge, but also the quality
of the solution that we converge to.
Figure 3 shows the interaction between ? and m
in terms of likelihood and accuracy. In general, the
best (?,m) depends on the task and dataset. For ex-
ample, for document classification, larger m is criti-
cal for good performance; for POS tagging, it is bet-
ter to use smaller values of ? and m.
Fortunately, there is a range of permissible set-
tings (corresponding to the dark regions in Figure 3)
that lead to reasonable performance. Furthermore,
the settings that perform well on likelihood gener-
ally correspond to ones that perform well on accu-
racy, which justifies using sEM`.
A final observation is that as we use larger mini-
batches (larger m), decreasing the stepsize more
gradually (smaller ?) leads to better performance.
Intuitively, updates become more reliable with larger
m, so we can afford to trust them more and incorpo-
rate them more aggressively.
Stepwise versus incremental EM In Section 3.2,
we mentioned that incremental EM can be made
equivalent to stepwise EM with ? = 1 and m = 1
(sEMi). Figure 1 provides the empirical support:
iEM and sEMi have very similar training curves.
Therefore, keeping around the old sufficient statis-
tics does not provide any advantage and still requires
a substantial storage cost. As mentioned before, set-
ting (?,m) properly is crucial. While we could sim-
ulate mini-batches with iEM by updating multiple
coordinates simultaneously, iEM is not capable of
exploiting the behavior of ? < 1.
4.4 Varying the random seed
All our results thus far represent single runs with a
fixed random seed. We now investigate the impact
of randomness on our results. Recall that we use
randomness for two purposes: (1) initializing the
parameters (affects both batch EM and online EM),
617
accuracy log-likelihood
EM sEM` EM sEM`
POS 56.2 ?1.36 58.8 ?0.73, 1.41 ?6.01 ?6.09
DOC 41.2 ?1.97 51.4 ?0.97, 2.82 ?7.93 ?7.88
SEG(en) 80.5 ?0.0 81.0 ?0.0, 0.42 ?4.1 ?4.1
SEG(ch) 78.2 ?0.0 77.2 ?0.0, 0.04 ?7.26 ?7.27
ALIGN 79.0 ?0.14 78.8 ?0.14, 0.25 ?5.04 ?5.11
Table 2: Mean and standard deviation over different ran-
dom seeds. For EM and sEM, the first number after ?
is the standard deviation due to different initializations
of the parameters. For sEM, the second number is the
standard deviation due to different permutations of the
examples. Standard deviation for log-likelihoods are all
< 0.01 and therefore left out due to lack of space.
and (2) permuting the examples at the beginning of
each iteration (affects only online EM).
To separate these two purposes, we used two
different seeds, Si ? {1, 2, 3, 4, 5} and Sp ?
{1, 2, 3, 4, 5} for initializing and permuting, respec-
tively. Let X be a random variable denoting either
log-likelihood or accuracy. We define the variance
due to initialization as var(E(X | Si)) (E averages
over Sp for each fixed Si) and the variance due to
permutation as E(var(X | Si)) (E averages over Si).
These two variances provide an additive decompo-
sition of the total variance: var(X) = var(E(X |
Si)) + E(var(X | Si)).
Table 2 summarizes the results across the 5 tri-
als for EM and 25 for sEM`. Since we used a very
small amount of noise to initialize the parameters,
the variance due to initialization is systematically
smaller than the variance due to permutation. sEM`
is less sensitive to initialization than EM, but addi-
tional variance is created by randomly permuting the
examples. Overall, the accuracy of sEM` is more
variable than that of EM, but not by a large amount.
5 Discussion and related work
As datasets increase in size, the demand for online
algorithms has grown in recent years. One sees
this clear trend in the supervised NLP literature?
examples include the Perceptron algorithm for tag-
ging (Collins, 2002), MIRA for dependency parsing
(McDonald et al, 2005), exponentiated gradient al-
gorithms (Collins et al, 2008), stochastic gradient
for constituency parsing (Finkel et al, 2008), just
to name a few. Empirically, online methods are of-
ten faster by an order of magnitude (Collins et al,
2008), and it has been argued on theoretical grounds
that the fast, approximate nature of online meth-
ods is a good fit given that we are interested in test
performance, not the training objective (Bottou and
Bousquet, 2008; Shalev-Shwartz and Srebro, 2008).
However, in the unsupervised NLP literature, on-
line methods are rarely seen,5 and when they are,
incremental EM is the dominant variant (Gildea and
Hofmann, 1999; Kuo et al, 2008). Indeed, as we
have shown, applying online EM does require some
care, and some variants (including incremental EM)
can fail catastrophically in face of local optima.
Stepwise EM provides finer control via its optimiza-
tion parameters and has proven quite successful.
One family of methods that resembles incremen-
tal EM includes collapsed samplers for Bayesian
models?for example, Goldwater et al (2006) and
Goldwater and Griffiths (2007). These samplers
keep track of a sample of the latent variables for
each example, akin to the sufficient statistics that we
store in incremental EM. In contrast, stepwise EM
does not require this storage and operates more in
the spirit of a truly online algorithm.
Besides speed, online algorithms are of interest
for two additional reasons. First, in some applica-
tions, we receive examples sequentially and would
like to estimate a model in real-time, e.g., in the clus-
tering of news articles. Second, since humans learn
sequentially, studying online EM might suggest new
connections to cognitive mechanisms.
6 Conclusion
We have explored online EM on four tasks and
demonstrated how to use stepwise EM to overcome
the dangers of stochasticity and reap the benefits of
frequent updates and fast learning. We also discov-
ered that stepwise EM can actually improve accu-
racy, a phenomenon worthy of further investigation.
This paper makes some progress on elucidating the
properties of online EM. With this increased under-
standing, online EM, like its batch cousin, could be-
come a mainstay for unsupervised learning.
5Other types of learning methods have been employed
successfully, for example, Venkataraman (2001) and Seginer
(2007).
618
References
L. Bottou and O. Bousquet. 2008. The tradeoffs of large
scale learning. In Advances in Neural Information
Processing Systems (NIPS).
O. Cappe? and E. Moulines. 2009. Online expectation-
maximization algorithm for latent data models. Jour-
nal of the Royal Statistics Society: Series B (Statistical
Methodology), 71.
M. Collins, A. Globerson, T. Koo, X. Carreras, and
P. Bartlett. 2008. Exponentiated gradient algorithms
for conditional random fields and max-margin Markov
networks. Journal of Machine Learning Research, 9.
M. Collins. 2002. Discriminative training methods for
hidden Markov models: Theory and experiments with
Perceptron algorithms. In Empirical Methods in Nat-
ural Language Processing (EMNLP).
J. R. Finkel, A. Kleeman, and C. Manning. 2008. Effi-
cient, feature-based, conditional random field parsing.
In Human Language Technology and Association for
Computational Linguistics (HLT/ACL).
D. Gildea and T. Hofmann. 1999. Topic-based language
models using EM. In Eurospeech.
S. Goldwater and T. Griffiths. 2007. A fully Bayesian
approach to unsupervised part-of-speech tagging. In
Association for Computational Linguistics (ACL).
S. Goldwater, T. Griffiths, and M. Johnson. 2006. Con-
textual dependencies in unsupervised word segmenta-
tion. In International Conference on Computational
Linguistics and Association for Computational Lin-
guistics (COLING/ACL).
M. Johnson. 2007. Why doesn?t EM find good HMM
POS-taggers? In Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP/CoNLL).
M. Johnson. 2008. Using adaptor grammars to identify
synergies in the unsupervised acquisition of linguistic
structure. In Human Language Technology and As-
sociation for Computational Linguistics (HLT/ACL),
pages 398?406.
J. Kuo, H. Li, and C. Lin. 2008. Mining transliterations
from web query results: An incremental approach. In
Sixth SIGHAN Workshop on Chinese Language Pro-
cessing.
P. Liang and D. Klein. 2008. Analyzing the errors
of unsupervised learning. In Human Language Tech-
nology and Association for Computational Linguistics
(HLT/ACL).
P. Liang, D. Klein, and M. I. Jordan. 2008. Agreement-
based learning. In Advances in Neural Information
Processing Systems (NIPS).
R. McDonald, K. Crammer, and F. Pereira. 2005. On-
line large-margin training of dependency parsers. In
Association for Computational Linguistics (ACL).
R. Neal and G. Hinton. 1998. A view of the EM algo-
rithm that justifies incremental, sparse, and other vari-
ants. In Learning in Graphical Models.
F. J. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29:19?51.
M. Sato and S. Ishii. 2000. On-line EM algorithm for the
normalized Gaussian network. Neural Computation,
12:407?432.
Y. Seginer. 2007. Fast unsupervised incremental parsing.
In Association for Computational Linguistics (ACL).
S. Shalev-Shwartz and N. Srebro. 2008. SVM optimiza-
tion: Inverse dependence on training set size. In Inter-
national Conference on Machine Learning (ICML).
A. Venkataraman. 2001. A statistical model for word
discovery in transcribed speech. Computational Lin-
guistics, 27:351?372.
619
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 761?768,
Sydney, July 2006. c?2006 Association for Computational Linguistics
An End-to-End Discriminative Approach to Machine Translation
Percy Liang Alexandre Bouchard-Co?te? Dan Klein Ben Taskar
Computer Science Division, EECS Department
University of California at Berkeley
Berkeley, CA 94720
{pliang, bouchard, klein, taskar}@cs.berkeley.edu
Abstract
We present a perceptron-style discriminative ap-
proach to machine translation in which large feature
sets can be exploited. Unlike discriminative rerank-
ing approaches, our system can take advantage of
learned features in all stages of decoding. We first
discuss several challenges to error-driven discrim-
inative approaches. In particular, we explore dif-
ferent ways of updating parameters given a training
example. We find that making frequent but smaller
updates is preferable to making fewer but larger up-
dates. Then, we discuss an array of features and
show both how they quantitatively increase BLEU
score and how they qualitatively interact on spe-
cific examples. One particular feature we investi-
gate is a novel way to introduce learning into the
initial phrase extraction process, which has previ-
ously been entirely heuristic.
1 Introduction
The generative, noisy-channel paradigm has his-
torically served as the foundation for most of the
work in statistical machine translation (Brown et
al., 1994). At the same time, discriminative meth-
ods have provided substantial improvements over
generative models on a wide range of NLP tasks.
They allow one to easily encode domain knowl-
edge in the form of features. Moreover, param-
eters are tuned to directly minimize error rather
than to maximize joint likelihood, which may not
correspond well to the task objective.
In this paper, we present an end-to-end dis-
criminative approach to machine translation. The
proposed system is phrase-based, as in Koehn et
al. (2003), but uses an online perceptron training
scheme to learn model parameters. Unlike mini-
mum error rate training (Och, 2003), our system is
able to exploit large numbers of specific features
in the same manner as static reranking systems
(Shen et al, 2004; Och et al, 2004). However,
unlike static rerankers, our system does not rely
on a baseline translation system. Instead, it up-
dates based on its own n-best lists. As parameter
estimates improve, the system produces better n-
best lists, which can in turn enable better updates
in future training iterations. In this paper, we fo-
cus on two aspects of the problem of discrimina-
tive translation: the inherent difficulty of learning
from reference translations, and the challenge of
engineering effective features for this task.
Discriminative learning from reference transla-
tions is inherently problematic because standard
discriminative methods need to know which out-
puts are correct and which are not. However, a
proposed translation that differs from a reference
translation need not be incorrect. It may differ
in word choice, literalness, or style, yet be fully
acceptable. Pushing our system to avoid such al-
ternate translations is undesirable. On the other
hand, even if a system produces a reference trans-
lation, it may do so by abusing the hidden struc-
ture (sentence segmentation and alignment). We
can therefore never be entirely sure whether or not
a proposed output is safe to update towards. We
discuss this issue in detail in Section 5, where we
show that conservative updates (which push the
system towards a local variant of the current pre-
diction) are more effective than more aggressive
updates (which try to directly update towards the
reference).
The second major contribution of this work is
an investigation of an array of features for our
model. We show how our features quantitatively
increase BLEU score, as well as how they qual-
itatively interact on specific examples. We first
consider learning weights for individual phrases
and part-of-speech patterns, showing gains from
each. We then present a novel way to parameter-
ize and introduce learning into the initial phrase
extraction process. In particular, we introduce
alignment constellation features, which allow us
to weight phrases based on the word alignment
pattern that led to their extraction. This kind of
761
feature provides a potential way to initially extract
phrases more aggressively and then later down-
weight undesirable patterns, essentially learning a
weighted extraction heuristic. Finally, we use POS
features to parameterize a distortion model in a
limited distortion decoder (Zens and Ney, 2004;
Tillmann and Zhang, 2005). We show that over-
all, BLEU score increases from 28.4 to 29.6 on
French-English.
2 Approach
2.1 Translation as structured classification
Machine translation can be seen as a structured
classification task, in which the goal is to learn
a mapping from an input (French) sentence x to
an output (English) sentence y. Given this setup,
discriminative methods allow us to define a broad
class of features ? that operate on (x,y). For ex-
ample, some features would measure the fluency
of y and others would measure the faithfulness of
y as a translation of x.
However, the translation task in this framework
differs from traditional applications of discrimina-
tive structured classification such as POS tagging
and parsing in a fundamental way. Whereas in
POS tagging, there is a one-to-one correspondence
between the words x and the tags y, the correspon-
dence between x and y in machine translation is
not only much more complex, but is in fact un-
known. Therefore, we introduce a hidden corre-
spondence structure h and work with the feature
vector ?(x,y,h).
The phrase-based model of Koehn et al (2003)
is an instance of this framework. In their model,
the correspondence h consists of (1) the segmen-
tation of the input sentence into phrases, (2) the
segmentation of the output sentence into the same
number of phrases, and (3) a bijection between
the input and output phrases. The feature vec-
tor ?(x,y,h) contains four components: the log
probability of the output sentence y under a lan-
guage model, the score of translating x into y
based on a phrase table, a distortion score, and a
length penalty.1 In Section 6, we vastly increase
the number of features to take advantage of the full
power of discriminative training.
Another example of this framework is the hier-
archical model of Chiang (2005). In this model
the correspondence h is a synchronous parse tree
1More components can be added to the feature vector if
additional language models or phrase tables are available.
over input and output sentences, and features in-
clude the scores of various productions used in the
tree.
Given features ? and a corresponding set of pa-
rameters w, a standard classification rule f is to
return the highest scoring output sentence y, max-
imizing over correspondences h:
f(x;w) = argmax
y,h
w ? ?(x,y,h). (1)
In the phrase-based model, computing the
argmax exactly is intractable, so we approximate
f with beam decoding.
2.2 Perceptron-based training
To tune the parameters w of the model, we use the
averaged perceptron algorithm (Collins, 2002) be-
cause of its efficiency and past success on various
NLP tasks (Collins and Roark, 2004; Roark et al,
2004). In principle, w could have been tuned by
maximizing conditional probability or maximiz-
ing margin. However, these two options require
either marginalization or numerical optimization,
neither of which is tractable over the space of out-
put sentences y and correspondences h. In con-
trast, the perceptron algorithm requires only a de-
coder that computes f(x;w).
Recall the traditional perceptron update rule on
an example (xi,yi) is
w? w + ?(xi,yt)? ?(xi,yp), (2)
where yt = yi is the target output and yp =
f(xi;w) = argmaxyw ? ?(xi,y) is the predic-
tion using the current parameters w.
We adapt this update rule to work with hidden
variables as follows:
w? w + ?(xi,yt,ht)??(xi,yp,hp), (3)
where (yp,hp) is the argmax computation in
Equation 1, and (yt,ht) is the target that we up-
date towards. If (yt,ht) is the same argmax com-
putation with the additional constraint that yt =
yi, then Equation 3 can be interpreted as a Viterbi
approximation to the stochastic gradient
EP (h|xi,yi;w)?(xi,yi,h)?EP (y,h|xi;w)?(xi,y,h)
for the following conditional likelihood objective:
P (yi | xi) ?
?
h
exp(w ? ?(xi,yi,h)).
762
      
    	
 

   
 
 


 




      
	

  

 
  
  
	 
   
      
  
	 
   
	
     

  

 
 

 
	 
  
	
     

  

 
  
 
 	
       
	

	 
   
	
   
 
	


	 
   
Proceedings of ACL-08: HLT, pages 771?779,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Learning Bilingual Lexicons from Monolingual Corpora
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick and Dan Klein
Computer Science Division, University of California at Berkeley
{ aria42,pliang,tberg,klein }@cs.berkeley.edu
Abstract
We present a method for learning bilingual
translation lexicons from monolingual cor-
pora. Word types in each language are charac-
terized by purely monolingual features, such
as context counts and orthographic substrings.
Translations are induced using a generative
model based on canonical correlation analy-
sis, which explains the monolingual lexicons
in terms of latent matchings. We show that
high-precision lexicons can be learned in a va-
riety of language pairs and from a range of
corpus types.
1 Introduction
Current statistical machine translation systems use
parallel corpora to induce translation correspon-
dences, whether those correspondences be at the
level of phrases (Koehn, 2004), treelets (Galley et
al., 2006), or simply single words (Brown et al,
1994). Although parallel text is plentiful for some
language pairs such as English-Chinese or English-
Arabic, it is scarce or even non-existent for most
others, such as English-Hindi or French-Japanese.
Moreover, parallel text could be scarce for a lan-
guage pair even if monolingual data is readily avail-
able for both languages.
In this paper, we consider the problem of learning
translations from monolingual sources alone. This
task, though clearly more difficult than the standard
parallel text approach, can operate on language pairs
and in domains where standard approaches cannot.
We take as input two monolingual corpora and per-
haps some seed translations, and we produce as out-
put a bilingual lexicon, defined as a list of word
pairs deemed to be word-level translations. Preci-
sion and recall are then measured over these bilin-
gual lexicons. This setting has been considered be-
fore, most notably in Koehn and Knight (2002) and
Fung (1995), but the current paper is the first to use
a probabilistic model and present results across a va-
riety of language pairs and data conditions.
In our method, we represent each language as a
monolingual lexicon (see figure 2): a list of word
types characterized by monolingual feature vectors,
such as context counts, orthographic substrings, and
so on (section 5). We define a generative model over
(1) a source lexicon, (2) a target lexicon, and (3) a
matching between them (section 2). Our model is
based on canonical correlation analysis (CCA)1 and
explains matched word pairs via vectors in a com-
mon latent space. Inference in the model is done
using an EM-style algorithm (section 3).
Somewhat surprisingly, we show that it is pos-
sible to learn or extend a translation lexicon us-
ing monolingual corpora alone, in a variety of lan-
guages and using a variety of corpora, even in the
absence of orthographic features. As might be ex-
pected, the task is harder when no seed lexicon is
provided, when the languages are strongly diver-
gent, or when the monolingual corpora are from dif-
ferent domains. Nonetheless, even in the more diffi-
cult cases, a sizable set of high-precision translations
can be extracted. As an example of the performance
of the system, in English-Spanish induction with our
best feature set, using corpora derived from topically
similar but non-parallel sources, the system obtains
89.0% precision at 33% recall.
1See Hardoon et al (2003) for an overview.
771
state
society
enlarge-
ment
control
import-
ance
sociedad
estado
amplifi-
caci?n
import-
ancia
control
.
.
.
.
.
.
s
t
m
Figure 1: Bilingual lexicon induction: source word types
s are listed on the left and target word types t on the
right. Dashed lines between nodes indicate translation
pairs which are in the matching m.
2 Bilingual Lexicon Induction
As input, we are given a monolingual corpus S (a
sequence of word tokens) in a source language and
a monolingual corpus T in a target language. Let
s = (s1, . . . , snS ) denote nS word types appearing
in the source language, and t = (t1, . . . , tnT ) denote
word types in the target language. Based on S and
T , our goal is to output a matching m between s
and t. We represent m as a set of integer pairs so
that (i, j) ?m if and only if si is matched with tj .
2.1 Generative Model
We propose the following generative model over
matchings m and word types (s, t), which we call
matching canonical correlation analysis (MCCA).
MCCA model
m ? MATCHING-PRIOR [matching m]
For each matched edge (i, j) ?m:
?z
i,j
? N (0, I
d
) [latent concept]
?f
S
(s
i
) ? N (W
S
z
i,j
,?
S
) [source features]
?f
T
(t
i
) ? N (W
T
z
i,j
,?
T
) [target features]
For each unmatched source word type i:
?f
S
(s
i
) ? N (0, ?
2
I
d
S
) [source features]
For each unmatched target word type j:
?f
T
(t
j
) ? N (0, ?
2
I
d
T
) [target features]
First, we generate a matching m ?M, whereM
is the set of matchings in which each word type is
matched to at most one other word type.2 We take
MATCHING-PRIOR to be uniform overM.3
Then, for each matched pair of word types (i, j) ?
m, we need to generate the observed feature vectors
of the source and target word types, fS(si) ? RdS
and fT (tj) ? RdT . The feature vector of each word
type is computed from the appropriate monolin-
gual corpus and summarizes the word?s monolingual
characteristics; see section 5 for details and figure 2
for an illustration. Since si and tj are translations of
each other, we expect fS(si) and fT (tj) to be con-
nected somehow by the generative process. In our
model, they are related through a vector zi,j ? Rd
representing the shared, language-independent con-
cept.
Specifically, to generate the feature vectors, we
first generate a random concept zi,j ? N (0, Id),
where Id is the d ? d identity matrix. The source
feature vector fS(si) is drawn from a multivari-
ate Gaussian with mean WSzi,j and covariance ?S ,
where WS is a dS ? d matrix which transforms the
language-independent concept zi,j into a language-
dependent vector in the source space. The arbitrary
covariance parameter ?S  0 explains the source-
specific variations which are not captured by WS ; it
does not play an explicit role in inference. The target
fT (tj) is generated analogously using WT and ?T ,
conditionally independent of the source given zi,j
(see figure 2). For each of the remaining unmatched
source word types si which have not yet been gen-
erated, we draw the word type features from a base-
line normal distribution with variance ?2IdS , with
hyperparameter ?2  0; unmatched target words
are similarly generated.
If two word types are truly translations, it will be
better to relate their feature vectors through the la-
tent space than to explain them independently via
the baseline distribution. However, if a source word
type is not a translation of any of the target word
types, we can just generate it independently without
requiring it to participate in the matching.
2Our choice ofM permits unmatched word types, but does
not allow words to have multiple translations. This setting facil-
itates comparison to previous work and admits simpler models.
3However, non-uniform priors could encode useful informa-
tion, such as rank similarities.
772
1.0
1.0
20.0
5.0
100.0
50.0
.
.
.
Source 
Space
Canonical 
Space
R
d
s
R
d
t
1.0
1.0
.
.
.
1.0
Target 
Space
R
d
1.0
{
{
O
r
t
h
o
g
r
a
p
h
i
c
 
F
e
a
t
u
r
e
s
C
o
n
t
e
x
t
u
a
l
 
F
e
a
t
u
r
e
s
time
tiempo
#ti
#ti
ime
mpo
me#
pe#
change
dawn
period
necessary
40.0
65.0
120.0
45.0
suficiente
per?odo
mismo
adicional
s
i
t
j
z
f
S
(s
i
)
f
T
(t
j
)
Figure 2: Illustration of our MCCA model. Each latent concept z
i,j
originates in the canonical space. The observed
word vectors in the source and target spaces are generated independently given this concept.
3 Inference
Given our probabilistic model, we would like to
maximize the log-likelihood of the observed data
(s, t):
`(?) = log p(s, t; ?) = log
?
m
p(m, s, t; ?)
with respect to the model parameters ? =
(WS ,WT ,?S ,?T ).
We use the hard (Viterbi) EM algorithm as a start-
ing point, but due to modeling and computational
considerations, we make several important modifi-
cations, which we describe later. The general form
of our algorithm is as follows:
Summary of learning algorithm
E-step: Find the maximum weighted (partial) bi-
partite matching m ?M
M-step: Find the best parameters ? by performing
canonical correlation analysis (CCA)
M-step Given a matching m, the M-step opti-
mizes log p(m, s, t; ?) with respect to ?, which can
be rewritten as
max
?
?
(i,j)?m
log p(si, tj ; ?). (1)
This objective corresponds exactly to maximizing
the likelihood of the probabilistic CCA model pre-
sented in Bach and Jordan (2006), which proved
that the maximum likelihood estimate can be com-
puted by canonical correlation analysis (CCA). In-
tuitively, CCA finds d-dimensional subspaces US ?
R
dS?d of the source and UT ? RdT?d of the tar-
get such that the components of the projections
U
>
S fS(si) and U
>
T fT (tj) are maximally correlated.
4
US and UT can be found by solving an eigenvalue
problem (see Hardoon et al (2003) for details).
Then the maximum likelihood estimates are as fol-
lows: WS = CSSUSP 1/2, WT = CTTUTP 1/2,
?S = CSS ?WSW
>
S , and ?T = CTT ?WTW
>
T ,
where P is a d? d diagonal matrix of the canonical
correlations, CSS = 1|m|
?
(i,j)?m fS(si)fS(si)
> is
the empirical covariance matrix in the source do-
main, and CTT is defined analogously.
E-step To perform a conventional E-step, we
would need to compute the posterior over all match-
ings, which is #P-complete (Valiant, 1979). On the
other hand, hard EM only requires us to compute the
best matching under the current model:5
m = argmax
m?
log p(m?, s, t; ?). (2)
We cast this optimization as a maximum weighted
bipartite matching problem as follows. Define the
edge weight between source word type i and target
word type j to be
wi,j = log p(si, tj ; ?) (3)
? log p(si; ?)? log p(tj ; ?),
4Since dS and dT can be quite large in practice and of-
ten greater than |m|, we use Cholesky decomposition to re-
represent the feature vectors as |m|-dimensional vectors with
the same dot products, which is all that CCA depends on.
5If we wanted softer estimates, we could use the agreement-
based learning framework of Liang et al (2008) to combine two
tractable models.
773
which can be loosely viewed as a pointwise mutual
information quantity. We can check that the ob-
jective log p(m, s, t; ?) is equal to the weight of a
matching plus some constant C:
log p(m, s, t; ?) =
?
(i,j)?m
wi,j + C. (4)
To find the optimal partial matching, edges with
weight wi,j < 0 are set to zero in the graph and the
optimal full matching is computed inO((nS+nT )3)
time using the Hungarian algorithm (Kuhn, 1955). If
a zero edge is present in the solution, we remove the
involved word types from the matching.6
Bootstrapping Recall that the E-step produces a
partial matching of the word types. If too few
word types are matched, learning will not progress
quickly; if too many are matched, the model will be
swamped with noise. We found that it was helpful
to explicitly control the number of edges. Thus, we
adopt a bootstrapping-style approach that only per-
mits high confidence edges at first, and then slowly
permits more over time. In particular, we compute
the optimal full matching, but only retain the high-
est weighted edges. As we run EM, we gradually
increase the number of edges to retain.
In our context, bootstrapping has a similar moti-
vation to the annealing approach of Smith and Eisner
(2006), which also tries to alter the space of hidden
outputs in the E-step over time to facilitate learn-
ing in the M-step, though of course the use of boot-
strapping in general is quite widespread (Yarowsky,
1995).
4 Experimental Setup
In section 5, we present developmental experiments
in English-Spanish lexicon induction; experiments
6Empirically, we obtained much better efficiency and even
increased accuracy by replacing these marginal likelihood
weights with a simple proxy, the distances between the words?
mean latent concepts:
wi,j = A? ||z
?
i ? z
?
j ||2, (5)
where A is a thresholding constant, z?i = E(zi,j | fS(si)) =
P 1/2U>S fS(si), and z
?
j is defined analogously. The increased
accuracy may not be an accident: whether two words are trans-
lations is perhaps better characterized directly by how close
their latent concepts are, whereas log-probability is more sensi-
tive to perturbations in the source and target spaces.
are presented for other languages in section 6. In
this section, we describe the data and experimental
methodology used throughout this work.
4.1 Data
Each experiment requires a source and target mono-
lingual corpus. We use the following corpora:
? EN-ES-W: 3,851 Wikipedia articles with both
English and Spanish bodies (generally not di-
rect translations).
? EN-ES-P: 1st 100k sentences of text from the
parallel English and Spanish Europarl corpus
(Koehn, 2005).
? EN-ES(FR)-D: English: 1st 50k sentences of
Europarl; Spanish (French): 2nd 50k sentences
of Europarl.7
? EN-CH-D: English: 1st 50k sentences of Xin-
hua parallel news corpora;8 Chinese: 2nd 50k
sentences.
? EN-AR-D: English: 1st 50k sentences of 1994
proceedings of UN parallel corpora;9 Ara-
bic: 2nd 50k sentences.
? EN-ES-G: English: 100k sentences of English
Gigaword; Spanish: 100k sentences of Spanish
Gigaword.10
Note that even when corpora are derived from par-
allel sources, no explicit use is ever made of docu-
ment or sentence-level alignments. In particular, our
method is robust to permutations of the sentences in
the corpora.
4.2 Lexicon
Each experiment requires a lexicon for evaluation.
Following Koehn and Knight (2002), we consider
lexicons over only noun word types, although this
is not a fundamental limitation of our model. We
consider a word type to be a noun if its most com-
mon tag is a noun in our monolingual corpus.11 For
7Note that the although the corpora here are derived from a
parallel corpus, there are no parallel sentences.
8LDC catalog # 2002E18.
9LDC catalog # 2004E13.
10These corpora contain no parallel sentences.
11We use the Tree Tagger (Schmid, 1994) for all POS tagging
except for Arabic, where we use the tagger described in Diab et
al. (2004).
774
 0.6 0.65 0.7 0.75 0.8 0.85
 0.9 0.95 1
 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8Precision Recall EN-ES-PEN-ES-W
Figure 3: Example precision/recall curve of our system
on EN-ES-P and EN-ES-W settings. See section 6.1.
all languages pairs except English-Arabic, we ex-
tract evaluation lexicons from the Wiktionary on-
line dictionary. As we discuss in section 7, our ex-
tracted lexicons have low coverage, particularly for
proper nouns, and thus all performance measures are
(sometimes substantially) pessimistic. For English-
Arabic, we extract a lexicon from 100k parallel sen-
tences of UN parallel corpora by running the HMM
intersected alignment model (Liang et al, 2008),
adding (s, t) to the lexicon if s was aligned to t at
least three times and more than any other word.
Also, as in Koehn and Knight (2002), we make
use of a seed lexicon, which consists of a small, and
perhaps incorrect, set of initial translation pairs. We
used two methods to derive a seed lexicon. The
first is to use the evaluation lexicon Le and select
the hundred most common noun word types in the
source corpus which have translations in Le. The
second method is to heuristically induce, where ap-
plicable, a seed lexicon using edit distance, as is
done in Koehn and Knight (2002). Section 6.2 com-
pares the performance of these two methods.
4.3 Evaluation
We evaluate a proposed lexicon Lp against the eval-
uation lexicon Le using the F1 measure in the stan-
dard fashion; precision is given by the number of
proposed translations contained in the evaluation
lexicon, and recall is given by the fraction of pos-
sible translation pairs proposed.12 Since our model
12We should note that precision is not penalized for (s, t) if
s does not have a translation in Le, and recall is not penalized
for failing to recover multiple translations of s.
Setting p0.1 p0.25 p0.33 p0.50 Best-F1
EDITDIST 58.6 62.6 61.1 ?- 47.4
ORTHO 76.0 81.3 80.1 52.3 55.0
CONTEXT 91.1 81.3 80.2 65.3 58.0
MCCA 87.2 89.7 89.0 89.7 72.0
Table 1: Performance of EDITDIST and our model with
various features sets on EN-ES-W. See section 5.
naturally produces lexicons in which each entry is
associated with a weight based on the model, we can
give a full precision/recall curve (see figure 3). We
summarize these curves with both the best F1 over
all possible thresholds and various precisions px at
recalls x. All reported numbers exclude evaluation
on the seed lexicon entries, regardless of how those
seeds are derived or whether they are correct.
In all experiments, unless noted otherwise, we
used a seed of size 100 obtained from Le and
considered lexicons between the top n = 2, 000
most frequent source and target noun word types
which were not in the seed lexicon; each system
proposed an already-ranked one-to-one translation
lexicon amongst these n words. Where applica-
ble, we compare against the EDITDIST baseline,
which solves a maximum bipartite matching prob-
lem where edge weights are normalized edit dis-
tances. We will use MCCA (for matching CCA) to
denote our model using the optimal feature set (see
section 5.3).
5 Features
In this section, we explore feature representations of
word types in our model. Recall that fS(?) and fT (?)
map source and target word types to vectors in RdS
and RdT , respectively (see section 2). The features
used in each representation are defined identically
and derived only from the appropriate monolingual
corpora. For a concrete example of a word type to
feature vector mapping, see figure 2.
5.1 Orthographic Features
For closely related languages, such as English and
Spanish, translation pairs often share many ortho-
graphic features. One direct way to capture ortho-
graphic similarity between word pairs is edit dis-
tance. Running EDITDIST (see section 4.3) on EN-
775
ES-W yielded 61.1 p0.33, but precision quickly de-
grades for higher recall levels (see EDITDIST in ta-
ble 1). Nevertheless, when available, orthographic
clues are strong indicators of translation pairs.
We can represent orthographic features of a word
type w by assigning a feature to each substring of
length ? 3. Note that MCCA can learn regular or-
thographic correspondences between source and tar-
get words, which is something edit distance cannot
capture (see table 5). Indeed, running our MCCA
model with only orthographic features on EN-ES-
W, labeled ORTHO in table 1, yielded 80.1 p0.33, a
31% error-reduction over EDITDIST in p0.33.
5.2 Context Features
While orthographic features are clearly effective for
historically related language pairs, they are more
limited for other language pairs, where we need to
appeal to other clues. One non-orthographic clue
that word types s and t form a translation pair is
that there is a strong correlation between the source
words used with s and the target words used with t.
To capture this information, we define context fea-
tures for each word type w, consisting of counts of
nouns which occur within a window of size 4 around
w. Consider the translation pair (time, tiempo)
illustrated in figure 2. As we become more con-
fident about other translation pairs which have ac-
tive period and periodico context features, we
learn that translation pairs tend to jointly generate
these features, which leads us to believe that time
and tiempo might be generated by a common un-
derlying concept vector (see section 2).13
Using context features alone on EN-ES-W, our
MCCA model (labeled CONTEXT in table 1) yielded
a 80.2 p0.33. It is perhaps surprising that context fea-
tures alone, without orthographic information, can
yield a best-F1comparable to EDITDIST.
5.3 Combining Features
We can of course combine context and orthographic
features. Doing so yielded 89.03 p0.33 (labeled
MCCA in table 1); this represents a 46.4% error re-
duction in p0.33 over the EDITDIST baseline. For the
remainder of this work, we will use MCCA to refer
13It is important to emphasize, however, that our current
model does not directly relate a word type?s role as a partici-
pant in the matching to that word?s role as a context feature.
(a) Corpus Variation
Setting p0.1 p0.25 p0.33 p0.50 Best-F1
EN-ES-G 75.0 71.2 68.3 ?- 49.0
EN-ES-W 87.2 89.7 89.0 89.7 72.0
EN-ES-D 91.4 94.3 92.3 89.7 63.7
EN-ES-P 97.3 94.8 93.8 92.9 77.0
(b) Seed Lexicon Variation
Corpus p0.1 p0.25 p0.33 p0.50 Best-F1
EDITDIST 58.6 62.6 61.1 ? 47.4
MCCA 91.4 94.3 92.3 89.7 63.7
MCCA-AUTO 91.2 90.5 91.8 77.5 61.7
(c) Language Variation
Languages p0.1 p0.25 p0.33 p0.50 Best-F1
EN-ES 91.4 94.3 92.3 89.7 63.7
EN-FR 94.5 89.1 88.3 78.6 61.9
EN-CH 60.1 39.3 26.8 ?- 30.8
EN-AR 70.0 50.0 31.1 ?- 33.1
Table 2: (a) varying type of corpora used on system per-
formance (section 6.1), (b) using a heuristically chosen
seed compared to one taken from the evaluation lexicon
(section 6.2), (c) a variety of language pairs (see sec-
tion 6.3).
to our model using both orthographic and context
features.
6 Experiments
In this section we examine how system performance
varies when crucial elements are altered.
6.1 Corpus Variation
There are many sources from which we can derive
monolingual corpora, and MCCA performance de-
pends on the degree of similarity between corpora.
We explored the following levels of relationships be-
tween corpora, roughly in order of closest to most
distant:
? Same Sentences: EN-ES-P
? Non-Parallel Similar Content: EN-ES-W
? Distinct Sentences, Same Domain: EN-ES-D
? Unrelated Corpora: EN-ES-G
Our results for all conditions are presented in ta-
ble 2(a). The predominant trend is that system per-
formance degraded when the corpora diverged in
776
content, presumably due to context features becom-
ing less informative. However, it is notable that even
in the most extreme case of disjoint corpora from
different time periods and topics (e.g. EN-ES-G),
we are still able to recover lexicons of reasonable
accuracy.
6.2 Seed Lexicon Variation
All of our experiments so far have exploited a small
seed lexicon which has been derived from the eval-
uation lexicon (see section 4.3). In order to explore
system robustness to heuristically chosen seed lexi-
cons, we automatically extracted a seed lexicon sim-
ilarly to Koehn and Knight (2002): we ran EDIT-
DIST on EN-ES-D and took the top 100 most con-
fident translation pairs. Using this automatically de-
rived seed lexicon, we ran our system on EN-ES-
D as before, evaluating on the top 2,000 noun word
types not included in the automatic lexicon.14 Us-
ing the automated seed lexicon, and still evaluat-
ing against our Wiktionary lexicon, MCCA-AUTO
yielded 91.8 p0.33 (see table 2(b)), indicating that
our system can produce lexicons of comparable ac-
curacy with a heuristically chosen seed. We should
note that this performance represents no knowledge
given to the system in the form of gold seed lexicon
entries.
6.3 Language Variation
We also explored how system performance varies
for language pairs other than English-Spanish. On
English-French, for the disjoint EN-FR-D corpus
(described in section 4.1), MCCA yielded 88.3 p0.33
(see table 2(c) for more performance measures).
This verified that our model can work for another
closely related language-pair on which no model de-
velopment was performed.
One concern is how our system performs on lan-
guage pairs where orthographic features are less ap-
plicable. Results on disjoint English-Chinese and
English-Arabic are given as EN-CH-D and EN-AR
in table 2(c), both using only context features. In
these cases, MCCA yielded much lower precisions
of 26.8 and 31.0 p0.33, respectively. For both lan-
guages, performance degraded compared to EN-ES-
14Note that the 2,000 words evaluated here were not identical
to the words tested on when the seed lexicon is derived from the
evaluation lexicon.
(a) English-Spanish
Rank Source Target Correct
1. education educaci?n Y
2. pacto pact Y
3. stability estabilidad Y
6. corruption corrupci?n Y
7. tourism turismo Y
9. organisation organizaci?n Y
10. convenience conveniencia Y
11. syria siria Y
12. cooperation cooperaci?n Y
14. culture cultura Y
21. protocol protocolo Y
23. north norte Y
24. health salud Y
25. action reacci?n N
(b) English-French
Rank Source Target Correct
3. xenophobia x?nophobie Y
4. corruption corruption Y
5. subsidiarity subsidiarit? Y
6. programme programme-cadre N
8. traceability tra?abilit? Y
(c) English-Chinese
Rank Source Target Correct
1. prices ? Y
2. network ? Y
3. population ? Y
4. reporter ? N
5. oil ? Y
Table 3: Sample output from our (a) Spanish, (b) French,
and (c) Chinese systems. We present the highest con-
fidence system predictions, where the only editing done
is to ignore predictions which consist of identical source
and target words.
D and EN-FR-D, presumably due in part to the
lack of orthographic features. However, MCCA still
achieved surprising precision at lower recall levels.
For instance, at p0.1, MCCA yielded 60.1 and 70.0
on Chinese and Arabic, respectively. Figure 3 shows
the highest-confidence outputs in several languages.
6.4 Comparison To Previous Work
There has been previous work in extracting trans-
lation pairs from non-parallel corpora (Rapp, 1995;
Fung, 1995; Koehn and Knight, 2002), but gener-
ally not in as extreme a setting as the one consid-
ered here. Due to unavailability of data and speci-
ficity in experimental conditions and evaluations, it
is not possible to perform exact comparisons. How-
777
(a) Example Non-Cognate Pairs
health salud
traceability rastreabilidad
youth juventud
report informe
advantages ventajas
(b) Interesting Incorrect Pairs
liberal partido
Kirkhope Gorsel
action reaccio?n
Albanians Bosnia
a.m. horas
Netherlands Bretan?a
Table 4: System analysis on EN-ES-W: (a) non-cognate
pairs proposed by our system, (b) hand-selected represen-
tative errors.
(a) Orthographic Feature
Source Feat. Closest Target Feats. Example Translation
#st #es, est (statue, estatua)
ty# ad#, d# (felicity, felicidad)
ogy g??a, g?? (geology, geolog??a)
(b) Context Feature
Source Feat. Closest Context Features
party partido, izquierda
democrat socialistas, demo?cratas
beijing pek??n, kioto
Table 5: Hand selected examples of source and target fea-
tures which are close in canonical space: (a) orthographic
feature correspondences, (b) context features.
ever, we attempted to run an experiment as similar
as possible in setup to Koehn and Knight (2002), us-
ing English Gigaword and German Europarl. In this
setting, our MCCA system yielded 61.7% accuracy
on the 186 most confident predictions compared to
39% reported in Koehn and Knight (2002).
7 Analysis
We have presented a novel generative model for
bilingual lexicon induction and presented results un-
der a variety of data conditions (section 6.1) and lan-
guages (section 6.3) showing that our system can
produce accurate lexicons even in highly adverse
conditions. In this section, we broadly characterize
and analyze the behavior of our system.
We manually examined the top 100 errors in the
English-Spanish lexicon produced by our system
on EN-ES-W. Of the top 100 errors: 21 were cor-
rect translations not contained in the Wiktionary
lexicon (e.g. pintura to painting), 4 were
purely morphological errors (e.g. airport to
aeropuertos), 30 were semantically related (e.g.
basketball to be?isbol), 15 were words with
strong orthographic similarities (e.g. coast to
costas), and 30 were difficult to categorize and
fell into none of these categories. Since many of
our ?errors? actually represent valid translation pairs
not contained in our extracted dictionary, we sup-
plemented our evaluation lexicon with one automat-
ically derived from 100k sentences of parallel Eu-
roparl data. We ran the intersected HMM word-
alignment model (Liang et al, 2008) and added
(s, t) to the lexicon if s was aligned to t at least
three times and more than any other word. Evaluat-
ing against the union of these lexicons yielded 98.0
p0.33, a significant improvement over the 92.3 us-
ing only the Wiktionary lexicon. Of the true errors,
the most common arose from semantically related
words which had strong context feature correlations
(see table 4(b)).
We also explored the relationships our model
learns between features of different languages. We
projected each source and target feature into the
shared canonical space, and for each projected
source feature we examined the closest projected
target features. In table 5(a), we present some of
the orthographic feature relationships learned by our
system. Many of these relationships correspond to
phonological and morphological regularities such as
the English suffix ing mapping to the Spanish suf-
fix g??a. In table 5(b), we present context feature
correspondences. Here, the broad trend is for words
which are either translations or semantically related
across languages to be close in canonical space.
8 Conclusion
We have presented a generative model for bilingual
lexicon induction based on probabilistic CCA. Our
experiments show that high-precision translations
can be mined without any access to parallel corpora.
It remains to be seen how such lexicons can be best
utilized, but they invite new approaches to the statis-
tical translation of resource-poor languages.
778
References
Francis R. Bach and Michael I. Jordan. 2006. A proba-
bilistic interpretation of canonical correlation analysis.
Technical report, University of California, Berkeley.
Peter F. Brown, Stephen Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1994. The mathematic
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19(2):263?311.
Mona Diab, Kadri Hacioglu, and Daniel Jurafsky. 2004.
Automatic tagging of arabic text: From raw text to
base phrase chunks. In HLT-NAACL.
Pascale Fung. 1995. Compiling bilingual lexicon entries
from a non-parallel english-chinese corpus. In Third
Annual Workshop on Very Large Corpora.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training
of context-rich syntactic translation models. In
COLING-ACL.
David R. Hardoon, Sandor Szedmak, and John Shawe-
Taylor. 2003. Canonical correlation analysis an
overview with application to learning methods. Tech-
nical Report CSD-TR-03-02, Royal Holloway Univer-
sity of London.
Philipp Koehn and Kevin Knight. 2002. Learning a
translation lexicon from monolingual corpora. In Pro-
ceedings of ACL Workshop on Unsupervised Lexical
Acquisition.
P. Koehn. 2004. Pharaoh: A beam search decoder
for phrase-based statistical machine translation mod-
els. In Proceedings of AMTA 2004.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In MT Summit.
H. W. Kuhn. 1955. The Hungarian method for the as-
signment problem. Naval Research Logistic Quar-
terly.
P. Liang, D. Klein, and M. I. Jordan. 2008. Agreement-
based learning. In NIPS.
Reinhard Rapp. 1995. Identifying word translation in
non-parallel texts. In ACL.
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In International Conference
on New Methods in Language Processing.
N. Smith and J. Eisner. 2006. Annealing structural bias
in multilingual weighted grammar induction. In ACL.
L. G. Valiant. 1979. The complexity of computing
the permanent. Theoretical Computer Science, 8:189?
201.
D. Yarowsky. 1995. Unsupervised word sense disam-
biguation rivaling supervised methods. In ACL.
779
Proceedings of ACL-08: HLT, pages 879?887,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Analyzing the Errors of Unsupervised Learning
Percy Liang Dan Klein
Computer Science Division, EECS Department
University of California at Berkeley
Berkeley, CA 94720
{pliang,klein}@cs.berkeley.edu
Abstract
We identify four types of errors that unsu-
pervised induction systems make and study
each one in turn. Our contributions include
(1) using a meta-model to analyze the incor-
rect biases of a model in a systematic way,
(2) providing an efficient and robust method
of measuring distance between two parameter
settings of a model, and (3) showing that lo-
cal optima issues which typically plague EM
can be somewhat alleviated by increasing the
number of training examples. We conduct
our analyses on three models: the HMM, the
PCFG, and a simple dependency model.
1 Introduction
The unsupervised induction of linguistic structure
from raw text is an important problem both for un-
derstanding language acquisition and for building
language processing systems such as parsers from
limited resources. Early work on inducing gram-
mars via EM encountered two serious obstacles: the
inappropriateness of the likelihood objective and the
tendency of EM to get stuck in local optima. With-
out additional constraints on bracketing (Pereira and
Shabes, 1992) or on allowable rewrite rules (Carroll
and Charniak, 1992), unsupervised grammar learn-
ing was ineffective.
Since then, there has been a large body of work
addressing the flaws of the EM-based approach.
Syntactic models empirically more learnable than
PCFGs have been developed (Clark, 2001; Klein
and Manning, 2004). Smith and Eisner (2005) pro-
posed a new objective function; Smith and Eis-
ner (2006) introduced a new training procedure.
Bayesian approaches can also improve performance
(Goldwater and Griffiths, 2007; Johnson, 2007;
Kurihara and Sato, 2006).
Though these methods have improved induction
accuracy, at the core they all still involve optimizing
non-convex objective functions related to the like-
lihood of some model, and thus are not completely
immune to the difficulties associated with early ap-
proaches. It is therefore important to better under-
stand the behavior of unsupervised induction sys-
tems in general.
In this paper, we take a step back and present
a more statistical view of unsupervised learning in
the context of grammar induction. We identify four
types of error that a system can make: approxima-
tion, identifiability, estimation, and optimization er-
rors (see Figure 1). We try to isolate each one in turn
and study its properties.
Approximation error is caused by a mis-match
between the likelihood objective optimized by EM
and the true relationship between sentences and their
syntactic structures. Our key idea for understand-
ing this mis-match is to ?cheat? and initialize EM
with the true relationship and then study the ways
in which EM repurposes our desired syntactic struc-
tures to increase likelihood. We present a meta-
model of the changes that EM makes and show how
this tool can shed some light on the undesired biases
of the HMM, the PCFG, and the dependency model
with valence (Klein and Manning, 2004).
Identifiability error can be incurred when two dis-
tinct parameter settings yield the same probabil-
ity distribution over sentences. One type of non-
identifiability present in HMMs and PCFGs is label
symmetry, which even makes computing a mean-
ingful distance between parameters NP-hard. We
present a method to obtain lower and upper bounds
on such a distance.
Estimation error arises from having too few train-
ing examples, and optimization error stems from
879
EM getting stuck in local optima. While it is to be
expected that estimation error should decrease as the
amount of data increases, we show that optimization
error can also decrease. We present striking experi-
ments showing that if our data actually comes from
the model family we are learning with, we can some-
times recover the true parameters by simply run-
ning EM without clever initialization. This result
runs counter to the conventional attitude that EM is
doomed to local optima; it suggests that increasing
the amount of data might be an effective way to par-
tially combat local optima.
2 Unsupervised models
Let x denote an input sentence and y denote the un-
observed desired output (e.g., a parse tree). We con-
sider a model family P = {p?(x,y) : ? ? ?}. For
example, if P is the set of all PCFGs, then the pa-
rameters ? would specify all the rule probabilities of
a particular grammar. We sometimes use ? and p?
interchangeably to simplify notation. In this paper,
we analyze the following three model families:
In the HMM, the input x is a sequence of words
and the output y is the corresponding sequence of
part-of-speech tags.
In the PCFG, the input x is a sequence of POS
tags and the output y is a binary parse tree with yield
x. We represent y as a multiset of binary rewrites of
the form (y ? y1 y2), where y is a nonterminal and
y1, y2 can be either nonterminals or terminals.
In the dependency model with valence (DMV)
(Klein and Manning, 2004), the input x =
(x1, . . . , xm) is a sequence of POS tags and the out-
put y specifies the directed links of a projective de-
pendency tree. The generative model is as follows:
for each head xi, we generate an independent se-
quence of arguments to the left and to the right from
a direction-dependent distribution over tags. At each
point, we stop with a probability parametrized by the
direction and whether any arguments have already
been generated in that direction. See Klein and Man-
ning (2004) for a formal description.
In all our experiments, we used the Wall Street
Journal (WSJ) portion of the Penn Treebank. We bi-
narized the PCFG trees and created gold dependency
trees according to the Collins head rules. We trained
45-state HMMs on all 49208 sentences, 11-state
PCFGs on WSJ-10 (7424 sentences) and DMVs
on WSJ-20 (25523 sentences) (Klein and Manning,
2004). We ran EM for 100 iterations with the pa-
rameters initialized uniformly (always plus a small
amount of random noise). We evaluated the HMM
and PCFG by mapping model states to Treebank
tags to maximize accuracy.
3 Decomposition of errors
Now we will describe the four types of errors (Fig-
ure 1) more formally. Let p?(x,y) denote the distri-
bution which governs the true relationship between
the input x and output y. In general, p? does not
live in our model family P . We are presented with
a set of n unlabeled examples x(1), . . . ,x(n) drawn
i.i.d. from the true p?. In unsupervised induction,
our goal is to approximate p? by some model p? ? P
in terms of strong generative capacity. A standard
approach is to use the EM algorithm to optimize
the empirical likelihood E? log p?(x).1 However, EM
only finds a local maximum, which we denote ??EM,
so there is a discrepancy between what we get (p??EM)
and what we want (p?).
We will define this discrepancy later, but for now,
it suffices to remark that the discrepancy depends
on the distribution over y whereas learning depends
only on the distribution over x. This is an important
property that distinguishes unsupervised induction
from more standard supervised learning or density
estimation scenarios.
Now let us walk through the four types of er-
ror bottom up. First, ??EM, the local maximum
found by EM, is in general different from ?? ?
argmax? E? log p?(x), any global maximum, which
we could find given unlimited computational re-
sources. Optimization error refers to the discrep-
ancy between ?? and ??EM.
Second, our training data is only a noisy sam-
ple from the true p?. If we had infinite data, we
would choose an optimal parameter setting under the
model, ??2 ? argmax? E log p?(x), where now the
expectation E is taken with respect to the true p? in-
stead of the training data. The discrepancy between
??2 and ?? is the estimation error.
Note that ??2 might not be unique. Let ?
?
1 denote
1Here, the expectation E?f(x) def= 1n
Pn
i=1 f(x
(i)) denotes
averaging some function f over the training data.
880
p? = true model
Approximation error (Section 4)
??1 = Best(argmax? E log p?(x))
Identifiability error (Section 5)
??2 ? argmax? E log p?(x)
Estimation error (Section 6)
?? ? argmax? E? log p?(x)
Optimization error (Section 7)
??EM= EM(E? log p?(x)) P
Figure 1: The discrepancy between what we get (??EM)
and what we want (p?) can be decomposed into four types
of errors. The box represents our model family P , which
is the set of possible parametrized distributions we can
represent. Best(S) returns the ? ? S which has the small-
est discrepancy with p?.
the maximizer of E log p?(x) that has the smallest
discrepancy with p?. Since ??1 and ?
?
2 have the same
value under the objective function, we would not be
able to choose ??1 over ?
?
2, even with infinite data or
unlimited computation. Identifiability error refers to
the discrepancy between ??1 and ?
?
2.
Finally, the model family P has fundamental lim-
itations. Approximation error refers to the discrep-
ancy between p? and p??1 . Note that ?
?
1 is not nec-
essarily the best in P . If we had labeled data, we
could find a parameter setting in P which is closer
to p? by optimizing joint likelihood E log p?(x,y)
(generative training) or even conditional likelihood
E log p?(y | x) (discriminative training).
In the remaining sections, we try to study each of
the four errors in isolation. In practice, since it is
difficult to work with some of the parameter settings
that participate in the error decomposition, we use
computationally feasible surrogates so that the error
under study remains the dominant effect.
4 Approximation error
We start by analyzing approximation error, the dis-
crepancy between p? and p??1 (the model found by
optimizing likelihood), a point which has been dis-
20 40 60 80 100iteration
-18.4
-18.0
-17.6
-17.2
-16.7
log
-lik
elih
ood
20 40 60 80 100iteration
0.2
0.4
0.6
0.8
1.0
Lab
eled
F 1
Figure 2: For the PCFG, when we initialize EM with the
supervised estimate ??gen, the likelihood increases but the
accuracy decreases.
cussed by many authors (Merialdo, 1994; Smith and
Eisner, 2005; Haghighi and Klein, 2006).2
To confront the question of specifically how
the likelihood diverges from prediction accuracy,
we perform the following experiment: we ini-
tialize EM with the supervised estimate3 ??gen =
argmax? E? log p?(x,y), which acts as a surrogate
for p?. As we run EM, the likelihood increases but
the accuracy decreases (Figure 2 shows this trend
for the PCFG; the HMM and DMV models behave
similarly). We believe that the initial iterations of
EM contain valuable information about the incor-
rect biases of these models. However, EM is chang-
ing hundreds of thousands of parameters at once in a
non-trivial way, so we need a way of characterizing
the important changes.
One broad observation we can make is that the
first iteration of EM reinforces the systematic mis-
takes of the supervised initializer. In the first E-step,
the posterior counts that are computed summarize
the predictions of the supervised system. If these
match the empirical counts, then the M-step does not
change the parameters. But if the supervised system
predicts too many JJs, for example, then the M-step
will update the parameters to reinforce this bias.
4.1 A meta-model for analyzing EM
We would like to go further and characterize the
specific changes EM makes. An initial approach is
to find the parameters that changed the most dur-
ing the first iteration (weighted by the correspond-
2Here, we think of discrepancy between p and p? as the error
incurred when using p? for prediction on examples generated
from p; in symbols, E(x,y)?ploss(y, argmaxy? p
?(y? | x)).
3For all our models, the supervised estimate is solved in
closed form by taking ratios of counts.
881
ing expected counts computed in the E-step). For
the HMM, the three most changed parameters are
the transitions 2:DT?8:JJ, START?0:NNP, and
8:JJ?3:NN.4 If we delve deeper, we can see that
2:DT?3:NN (the parameter with the 10th largest
change) fell and 2:DT?8:JJ rose. After checking
with a few examples, we can then deduce that some
nouns were retagged as adjectives. Unfortunately,
this type of ad-hoc reasoning requires considerable
manual effort and is rather subjective.
Instead, we propose using a general meta-model
to analyze the changes EM makes in an automatic
and objective way. Instead of treating parameters as
the primary object of study, we look at predictions
made by the model and study how they change over
time. While a model is a distribution over sentences,
a meta-model is a distribution over how the predic-
tions of the model change.
Let R(y) denote the set of parts of a predic-
tion y that we are interested in tracking. Each part
(c, l) ? R(y) consists of a configuration c and a lo-
cation l. For a PCFG, we define a configuration to
be a rewrite rule (e.g., c = PP?IN NP), and a loca-
tion l = [i, k, j] to be a span [i, j] split at k, where
the rewrite c is applied.
In this work, each configuration is associated with
a parameter of the model, but in general, a configu-
ration could be a larger unit such as a subtree, allow-
ing one to track more complex changes. The size of
a configuration governs how much the meta-model
generalizes from individual examples.
Let y(i,t) denote the model prediction on the i-th
training example after t iterations of EM. To sim-
plify notation, we write Rt = R(y(i,t)). The meta-
model explains how Rt became Rt+1.5
In general, we expect a part in Rt+1 to be ex-
plained by a part in Rt that has a similar location
and furthermore, we expect the locations of the two
parts to be related in some consistent way. The meta-
model uses two notions to formalize this idea: a dis-
tance d(l, l?) and a relation r(l, l?). For the PCFG,
d(l, l?) is the number of positions among i,j,k that
are the same as the corresponding ones in l?, and
r((i, k, j), (i?, k?, j?)) = (sign(i ? i?), sign(j ?
4Here 2:DT means state 2 of the HMM, which was greedily
mapped to DT.
5If the same part appears in both Rt and Rt+1, we remove
it from both sets.
j?), sign(k ? k?)) is one of 33 values. We define a
migration as a triple (c, c?, r(l, l?)); this is the unit of
change we want to extract from the meta-model.
Our meta-model provides the following genera-
tive story of how Rt becomes Rt+1: each new part
(c?, l?) ? Rt+1 chooses an old part (c, l) ? Rt with
some probability that depends on (1) the distance be-
tween the locations l and l? and (2) the likelihood of
the particular migration. Formally:
pmeta(Rt+1 | Rt) =
?
(c?,l?)?Rt+1
?
(c,l)?Rt
Z?1l? e
??d(l,l?)p(c? | c, r(l, l?)),
where Zl =
?
(c,l)?Rt e
??d(l,l?) is a normalization
constant, and ? is a hyperparameter controlling the
possibility of distant migrations (set to 3 in our ex-
periments).
We learn the parameters of the meta-model with
an EM algorithm similar to the one for IBM model
1. Fortunately, the likelihood objective is convex, so
we need not worry about local optima.
4.2 Results of the meta-model
We used our meta-model to analyze the approxima-
tion errors of the HMM, DMV, and PCFG. For these
models, we initialized EM with the supervised es-
timate ??gen and collected the model predictions as
EM ran. We then trained the meta-model on the pre-
dictions between successive iterations. The meta-
model gives us an expected count for each migra-
tion. Figure 3 lists the migrations with the highest
expected counts.
From these migrations, we can see that EM tries
to explain x better by making the corresponding y
more regular. In fact, many of the HMM migra-
tions on the first iteration attempt to resolve incon-
sistencies in gold tags. For example, noun adjuncts
(e.g., stock-index), tagged as both nouns and adjec-
tives in the Treebank, tend to become consolidated
under adjectives, as captured by migration (B). EM
also re-purposes under-utilized states to better cap-
ture distributional similarities. For example, state 24
has migrated to state 40 (N), both of which are now
dominated by proper nouns. State 40 initially con-
tained only #, but was quickly overrun with distribu-
tionally similar proper nouns such as Oct. and Chap-
ter, which also precede numbers, just as # does.
882
Iteration 0?1(A) START 4:NN24:NNP(B) 4:NN8:JJ 4:NN(C) 24:NNP 24:NNP36:NNPS
Iteration 1?2(D) 4:NN8:JJ 4:NN(E) START 4:NN24:NNP(F) 8:JJ11:RB 27:TO
Iteration 2?3(G) 24:NNP8:JJ U.S.(H) 24:NNP8:JJ 4:NN(I) 3:DT 24:NNP8:JJ
Iteration 3?4(J) 11:RB32:RP up(K) 24:NNP8:JJ U.S.(L) 19:, 11:RB32:RP
Iteration 4?5(M) 24:NNP34:$ 15:CD(N) 2:IN 24:NNP40:NNP(O) 11:RB32:RP down(a) Top HMM migrations. Example: migration (D) means a NN?NN transition is replaced by JJ?NN.
Iteration 0?1 Iteration 1?2 Iteration 2?3 Iteration 3?4 Iteration 4?5
(A) DT NN NN (D) NNP NNP NNP (G) DT JJ NNS (J) DT JJ NN (M) POS JJ NN
(B) JJ NN NN (E) NNP NNP NNP (H) MD RB VB (K) DT NNP NN (N) NNS RB VBP
(C) NNP NNP (F) DT NNP NNP (I) VBP RB VB (L) PRP$ JJ NN (O) NNS RB VBD
(b) Top DMV migrations. Example: migration (A) means a DT attaches to the closer NN.
Iteration 0?1 Iteration 1?2 Iteration 2?3 Iteration 3?4 Iteration 4?5
(A) RB 1:VP
4:S
RB 1:VP1:VP
(D) NNP 0:NP
0:NP
NNP NNP0:NP
(G) DT 0:NP
0:NP
DT NN0:NP
(J) TO VB
1:VP
TO VB2:PP
(M) CD NN
0:NP
CD NN3:ADJP
(B) 0:NP 2:PP
0:NP
1:VP 2:PP1:VP
(E) VBN 2:PP
1:VP
1:VP 2:PP1:VP
(H) 0:NP 1:VP
4:S
0:NP 1:VP4:S
(K) MD 1:VP
1:VP
MD VB1:VP
(N) VBD 0:NP
1:VP
VBD 3:ADJP1:VP
(C) VBZ 0:NP
1:VP
VBZ 0:NP1:VP
(F) 0:NP 1:VP
4:S
0:NP 1:VP4:S
(I) TO VB
1:VP
TO VB2:PP
(L) NNP NNP
0:NP
NNP NNP6:NP
(O) 0:NP NN
0:NP
0:NP NN0:NP(c) Top PCFG migrations. Example: migration (D) means a NP?NNPNP rewrite is replaced by NP?NNPNNP,where the new NNP right child spans less than the old NP right child.
Figure 3: We show the prominent migrations that occur during the first 5 iterations of EM for the HMM, DMV, and
PCFG, as recovered by our meta-model. We sort the migrations across each iteration by their expected counts under
the meta-model and show the top 3. Iteration 0 corresponds to the correct outputs. Blue indicates the new iteration,
red indicates the old.
DMV migrations also try to regularize model pre-
dictions, but in a different way?in terms of the
number of arguments. Because the stop probability
is different for adjacent and non-adjacent arguments,
it is statistically much cheaper to generate one argu-
ment rather than two or more. For example, if we
train a DMV on only DT JJ NN, it can fit the data
perfectly by using a chain of single arguments, but
perfect fit is not possible if NN generates both DT
and JJ (which is the desired structure); this explains
migration (J). Indeed, we observed that the variance
of the number of arguments decreases with more EM
iterations (for NN, from 1.38 to 0.41).
In general, low-entropy conditional distributions
are preferred. Migration (H) explains how adverbs
now consistently attach to verbs rather than modals.
After a few iterations, the modal has committed
itself to generating exactly one verb to the right,
which is statistically advantageous because there
must be a verb after a modal, while the adverb is op-
tional. This leaves the verb to generate the adverb.
The PCFG migrations regularize categories in a
manner similar to the HMM, but with the added
complexity of changing bracketing structures. For
example, sentential adverbs are re-analyzed as VP
adverbs (A). Sometimes, multiple migrations ex-
plain the same phenomenon.6 For example, migra-
tions (B) and (C) indicate that PPs that previously
attached to NPs are now raised to the verbal level.
Tree rotation is another common phenomenon, lead-
ing to many left-branching structures (D,G,H). The
migrations that happen during one iteration can also
trigger additional migrations in the next. For exam-
ple, the raising of the PP (B,C) inspires more of the
6We could consolidate these migrations by using larger con-
figurations, but at the risk of decreased generalization.
883
same raising (E). As another example, migration (I)
regularizes TO VB infinitival clauses into PPs, and
this momentum carries over to the next iteration with
even greater force (J).
In summary, the meta-model facilitates our anal-
yses by automatically identifying the broad trends.
We believe that the central idea of modeling the er-
rors of a system is a powerful one which can be used
to analyze a wide range of models, both supervised
and unsupervised.
5 Identifiability error
While approximation error is incurred when likeli-
hood diverges from accuracy, identifiability error is
concerned with the case where likelihood is indiffer-
ent to accuracy.
We say a set of parameters S is identifiable (in
terms of x) if p?(x) 6= p??(x) for every ?, ?? ? S
where ? 6= ??.7 In general, identifiability error is
incurred when the set of maximizers of E log p?(x)
is non-identifiable.8
Label symmetry is perhaps the most familiar ex-
ample of non-identifiability and is intrinsic to mod-
els with hidden labels (HMM and PCFG, but not
DMV). We can permute the hidden labels without
changing the objective function or even the nature
of the solution, so there is no reason to prefer one
permutation over another. While seemingly benign,
this symmetry actually presents a serious challenge
in measuring discrepancy (Section 5.1).
Grenager et al (2005) augments an HMM to al-
low emission from a generic stopword distribution at
any position with probability q. Their model would
definitely not be identifiable if q were a free param-
eter, since we can set q to 0 and just mix in the stop-
word distribution with each of the other emission
distributions to obtain a different parameter setting
yielding the same overall distribution. This is a case
where our notion of desired structure is absent in the
likelihood, and a prior over parameters could help
break ties.
7For our three model families, ? is identifiable in terms of
(x,y), but not in terms of x alone.
8We emphasize that non-identifiability is in terms of x, so
two parameter settings could still induce the same marginal dis-
tribution on x (weak generative capacity) while having different
joint distributions on (x,y) (strong generative capacity). Recall
that discrepancy depends on the latter.
The above non-identifiabilities apply to all param-
eter settings, but another type of non-identifiability
concerns only the maximizers of E log p?(x). Sup-
pose the true data comes from a K-state HMM. If
we attempt to fit an HMM with K + 1 states, we
can split any one of the K states and maintain the
same distribution on x. Or, if we learn a PCFG on
the same HMM data, then we can choose either the
left- or right-branching chain structures, which both
mimic the true HMM equally well.
5.1 Permutation-invariant distance
KL-divergence is a natural measure of discrepancy
between two distributions, but it is somewhat non-
trivial to compute?for our three recursive models, it
requires solving fixed point equations, and becomes
completely intractable in face of label symmetry.
Thus we propose a more manageable alternative:
d?(? || ?
?)
def
=
?
j ?j |?j ? ?
?
j |
?
j ?j
, (1)
where we weight the difference between the j-th
component of the parameter vectors by ?j , the j-
th expected sufficient statistic with respect to p?
(the expected counts computed in the E-step).9 Un-
like KL, our distance d? is only defined on distri-
butions in the model family and is not invariant to
reparametrization. Like KL, d? is asymmetric, with
the first argument holding the status of being the
?true? parameter setting. In our case, the parameters
are conditional probabilities, so 0 ? d?(? || ??) ? 1,
so we can interpret d? as an expected difference be-
tween these probabilities.
Unfortunately, label symmetry can wreak havoc
on our distance measure d?. Suppose we want to
measure the distance between ? and ??. If ?? is
simply ? with the labels permuted, then d?(? || ??)
would be substantial even though the distance ought
to be zero. We define a revised distance to correct
for this by taking the minimum distance over all la-
bel permutations:
D?(? || ?
?) = min
pi
d?(? ||pi(?
?)), (2)
9Without this factor, rarely used components could con-
tribute to the sum as much as frequently used ones, thus, making
the distance overly pessimistic.
884
where pi(??) denotes the parameter setting result-
ing from permuting the labels according to pi. (The
DMV has no label symmetries, so just d? works.)
For mixture models, we can compute D?(? || ??)
efficiently as follows. Note that each term in the
summation of (1) is associated with one of the K
labels. We can form aK?K matrixM , where each
entry Mij is the distance between the parameters in-
volving label i of ? and label j of ??. D?(? || ??) can
then be computed by finding a maximum weighted
bipartite matching on M using the O(K3) Hungar-
ian algorithm (Kuhn, 1955).
For models such as the HMM and PCFG, com-
putingD? is NP-hard, since the summation in d? (1)
contains both first-order terms which depend on one
label (e.g., emission parameters) and higher-order
terms which depend on more than one label (e.g.,
transitions or rewrites). We cannot capture these
problematic higher-order dependencies in M .
However, we can bound D?(? || ??) as follows.
We create M using only first-order terms and find
the best matching (permutation) to obtain a lower
bound D? and an associated permutation pi0 achiev-
ing it. Since D?(? || ??) takes the minimum over all
permutations, d?(? ||pi(??)) is an upper bound for
any pi, in particular for pi = pi0. We then use a local
search procedure that changes pi to further tighten
the upper bound. Let D? denote the final value.
6 Estimation error
Thus far, we have considered approximation and
identifiability errors, which have to do with flaws of
the model. The remaining errors have to do with
how well we can fit the model. To focus on these
errors, we consider the case where the true model is
in our family (p? ? P). To keep the setting as real-
istic as possible, we do supervised learning on real
labeled data to obtain ?? = argmax? E? log p(x,y).
We then throw away our real data and let p? = p?? .
Now we start anew: sample new artificial data from
??, learn a model using this artificial data, and see
how close we get to recovering ??.
In order to compute estimation error, we need to
compare ?? with ??, the global maximizer of the like-
lihood on our generated data. However, we cannot
compute ?? exactly. Let us therefore first consider the
simpler supervised scenario. Here, ??gen has a closed
form solution, so there is no optimization error. Us-
ing our distanceD? (defined in Section 5.1) to quan-
tify estimation error, we see that, for the HMM, ??gen
quickly approaches ?? as we increase the amount of
data (Table 1).
# examples 500 5K 50K 500K
D?(?? || ??gen) 0.003 6.3e-4 2.7e-4 8.5e-5
D?(?? || ??gen) 0.005 0.001 5.2e-4 1.7e-4
D?(?? || ??gen-EM) 0.022 0.018 0.008 0.002
D?(?? || ??gen-EM) 0.049 0.039 0.016 0.004
Table 1: Lower and upper bounds on the distance from
the true ?? for the HMM as we increase the number of
examples.
In the unsupervised case, we use the following
procedure to obtain a surrogate for ??: initialize EM
with the supervised estimate ??gen and run EM for
100 iterations. Let ??gen-EM denote the final param-
eters, which should be representative of ??. Table 1
shows that the estimation error of ??gen-EM is an order
of magnitude higher than that of ??gen, which is to ex-
pected since ??gen-EM does not have access to labeled
data. However, this error can also be driven down
given a moderate number of examples.
7 Optimization error
Finally, we study optimization error, which is the
discrepancy between the global maximizer ?? and
??EM, the result of running EM starting from a uni-
form initialization (plus some small noise). As be-
fore, we cannot compute ??, so we use ??gen-EM as a
surrogate. Also, instead of comparing ??gen-EM and ??
with each other, we compare each of their discrep-
ancies with respect to ??.
Let us first consider optimization error in terms
of prediction error. The first observation is that
there is a gap between the prediction accuracies
of ??gen-EM and ??EM, but this gap shrinks consider-
ably as we increase the number of examples. Fig-
ures 4(a,b,c) support this for all three model fami-
lies: for the HMM, both ??gen-EM and ??EM eventually
achieve around 90% accuracy; for the DMV, 85%.
For the PCFG, ??EM still lags ??gen-EM by 10%, but we
believe that more data can further reduce this gap.
Figure 4(d) shows that these trends are not par-
ticular to artificial data. On real WSJ data, the gap
885
500 5K 50K 500K# examples
0.6
0.7
0.8
0.9
1.0
Acc
ura
cy
500 5K 50K 500K# examples
0.6
0.7
0.8
0.9
1.0
Dir
ect
ed
F 1
500 5K 50K# examples
0.5
0.6
0.8
0.9
1.0
Lab
eled
F 1
1K 3K 10K 40K# examples
0.3
0.4
0.6
0.7
0.8
Acc
ura
cy
(a) HMM (artificial data) (b) DMV (artificial data) (c) PCFG (artificial data) (d) HMM (real data)
500 5K 50K 500K# examples
0.02
0.05
0.07
0.1
0.12
D ?(
?? |
|?) ??gen-EM
??EM (rand 1)
??EM (rand 2)
??EM (rand 3)
20 40 60 80 100iteration
-173.3
-171.4
-169.4
-167.4
-165.5
log
-lik
elih
ood
20 40 60 80 100iteration
0.2
0.4
0.6
0.8
1.0
Acc
ura
cy
Sup. init.
Unif. init.
(e) HMM (artificial data) (f) HMM log-likelihood/accuracy on 500K examples
Figure 4: Compares the performance of ??EM (EM with a uniform initialization) against ??gen-EM (EM initialized with the
supervised estimate) on (a?c) various models, (d) real data. (e) measures distance instead of accuracy and (f) shows a
sample EM run.
between ??gen-EM and ??EM also diminishes for the
HMM. To reaffirm the trends, we also measure dis-
tance D?. Figure 4(e) shows that the distance from
??EM to the true parameters ?? decreases, but the gap
between ??gen-EM and ??EM does not close as deci-
sively as it did for prediction error.
It is quite surprising that by simply running EM
with a neutral initialization, we can accurately learn
a complex model with thousands of parameters. Fig-
ures 4(f,g) show how both likelihood and accuracy,
which both start quite low, improve substantially
over time for the HMM on artificial data.
Carroll and Charniak (1992) report that EM fared
poorly with local optima. We do not claim that there
are no local optima, but only that the likelihood sur-
face that EM is optimizing can become smoother
with more examples. With more examples, there is
less noise in the aggregate statistics, so it might be
easier for EM to pick out the salient patterns.
Srebro et al (2006) made a similar observation
in the context of learning Gaussian mixtures. They
characterized three regimes: one where EM was suc-
cessful in recovering the true clusters (given lots of
data), another where EM failed but the global opti-
mum was successful, and the last where both failed
(without much data).
There is also a rich body of theoretical work on
learning latent-variable models. Specialized algo-
rithms can provably learn certain constrained dis-
crete hidden-variable models, some in terms of weak
generative capacity (Ron et al, 1998; Clark and
Thollard, 2005; Adriaans, 1999), others in term of
strong generative capacity (Dasgupta, 1999; Feld-
man et al, 2005). But with the exception of Das-
gupta and Schulman (2007), there is little theoretical
understanding of EM, let alne on complex model
families such as the HMM, PCFG, and DMV.
8 Conclusion
In recent years, many methods have improved unsu-
pervised induction, but these methods must still deal
with the four types of errors we have identified in
this paper. One of our main contributions of this pa-
per is the idea of using the meta-model to diagnose
the approximation error. Using this tool, we can bet-
ter understand model biases and hopefully correct
for them. We also introduced a method for mea-
suring distances in face of label symmetry and ran
experiments exploring the effectiveness of EM as a
function of the amount of data. Finally, we hope that
setting up the general framework to understand the
errors of unsupervised induction systems will aid the
development of better methods and further analyses.
886
References
P. W. Adriaans. 1999. Learning shallow context-free lan-
guages under simple distributions. Technical report,
Stanford University.
G. Carroll and E. Charniak. 1992. Two experiments on
learning probabilistic dependency grammars from cor-
pora. In Workshop Notes for Statistically-Based NLP
Techniques, pages 1?13.
A. Clark and F. Thollard. 2005. PAC-learnability
of probabilistic deterministic finite state automata.
JMLR, 5:473?497.
A. Clark. 2001. Unsupervised induction of stochastic
context free grammars with distributional clustering.
In CoNLL.
S. Dasgupta and L. Schulman. 2007. A probabilistic
analysis of EM for mixtures of separated, spherical
Gaussians. JMLR, 8.
S. Dasgupta. 1999. Learning mixtures of Gaussians. In
FOCS.
J. Feldman, R. O?Donnell, and R. A. Servedio. 2005.
Learning mixtures of product distributions over dis-
crete domains. In FOCS, pages 501?510.
S. Goldwater and T. Griffiths. 2007. A fully Bayesian
approach to unsupervised part-of-speech tagging. In
ACL.
T. Grenager, D. Klein, and C. D. Manning. 2005. Un-
supervised learning of field segmentation models for
information extraction. In ACL.
A. Haghighi and D. Klein. 2006. Prototype-based gram-
mar induction. In ACL.
M. Johnson. 2007. Why doesn?t EM find good HMM
POS-taggers? In EMNLP/CoNLL.
D. Klein and C. D. Manning. 2004. Corpus-based induc-
tion of syntactic structure: Models of dependency and
constituency. In ACL.
H. W. Kuhn. 1955. The Hungarian method for the as-
signment problem. Naval Research Logistic Quar-
terly, 2:83?97.
K. Kurihara and T. Sato. 2006. Variational Bayesian
grammar induction for natural language. In Interna-
tional Colloquium on Grammatical Inference.
B. Merialdo. 1994. Tagging English text with a prob-
abilistic model. Computational Linguistics, 20:155?
171.
F. Pereira and Y. Shabes. 1992. Inside-outside reestima-
tion from partially bracketed corpora. In ACL.
D. Ron, Y. Singer, and N. Tishby. 1998. On the learnabil-
ity and usage of acyclic probabilistic finite automata.
Journal of Computer and System Sciences, 56:133?
152.
N. Smith and J. Eisner. 2005. Contrastive estimation:
Training log-linear models on unlabeled data. In ACL.
N. Smith and J. Eisner. 2006. Annealing structural bias
in multilingual weighted grammar induction. In ACL.
N. Srebro, G. Shakhnarovich, and S. Roweis. 2006. An
investigation of computational and informational lim-
its in Gaussian mixture clustering. In ICML, pages
865?872.
887
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 91?99,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Learning Semantic Correspondences with Less Supervision
Percy Liang
UC Berkeley
pliang@cs.berkeley.edu
Michael I. Jordan
UC Berkeley
jordan@cs.berkeley.edu
Dan Klein
UC Berkeley
klein@cs.berkeley.edu
Abstract
A central problem in grounded language acqui-
sition is learning the correspondences between a
rich world state and a stream of text which refer-
ences that world state. To deal with the high de-
gree of ambiguity present in this setting, we present
a generative model that simultaneously segments
the text into utterances and maps each utterance
to a meaning representation grounded in the world
state. We show that our model generalizes across
three domains of increasing difficulty?Robocup
sportscasting, weather forecasts (a new domain),
and NFL recaps.
1 Introduction
Recent work in learning semantics has focused
on mapping sentences to meaning representa-
tions (e.g., some logical form) given aligned sen-
tence/meaning pairs as training data (Ge and
Mooney, 2005; Zettlemoyer and Collins, 2005;
Zettlemoyer and Collins, 2007; Lu et al, 2008).
However, this degree of supervision is unrealistic
for modeling human language acquisition and can
be costly to obtain for building large-scale, broad-
coverage language understanding systems.
A more flexible direction is grounded language
acquisition: learning the meaning of sentences
in the context of an observed world state. The
grounded approach has gained interest in various
disciplines (Siskind, 1996; Yu and Ballard, 2004;
Feldman and Narayanan, 2004; Gorniak and Roy,
2007). Some recent work in the NLP commu-
nity has also moved in this direction by relaxing
the amount of supervision to the setting where
each sentence is paired with a small set of can-
didate meanings (Kate and Mooney, 2007; Chen
and Mooney, 2008).
The goal of this paper is to reduce the amount
of supervision even further. We assume that we are
given a world state represented by a set of records
along with a text, an unsegmented sequence of
words. For example, in the weather forecast do-
main (Section 2.2), the text is the weather report,
and the records provide a structured representation
of the temperature, sky conditions, etc.
In this less restricted data setting, we must re-
solve multiple ambiguities: (1) the segmentation
of the text into utterances; (2) the identification of
relevant facts, i.e., the choice of records and as-
pects of those records; and (3) the alignment of ut-
terances to facts (facts are the meaning represen-
tations of the utterances). Furthermore, in some
of our examples, much of the world state is not
referenced at all in the text, and, conversely, the
text references things which are not represented in
our world state. This increased amount of ambigu-
ity and noise presents serious challenges for learn-
ing. To cope with these challenges, we propose a
probabilistic generative model that treats text seg-
mentation, fact identification, and alignment in a
single unified framework. The parameters of this
hierarchical hidden semi-Markov model can be es-
timated efficiently using EM.
We tested our model on the task of aligning
text to records in three different domains. The
first domain is Robocup sportscasting (Chen and
Mooney, 2008). Their best approach (KRISPER)
obtains 67% F1; our method achieves 76.5%. This
domain is simplified in that the segmentation is
known. The second domain is weather forecasts,
for which we created a new dataset. Here, the
full complexity of joint segmentation and align-
ment arises. Nonetheless, we were able to obtain
reasonable results on this task. The third domain
we considered is NFL recaps (Barzilay and Lap-
ata, 2005; Snyder and Barzilay, 2007). The lan-
guage used in this domain is richer by orders of
magnitude, and much of it does not reference the
world state. Nonetheless, taking the first unsuper-
vised approach to this problem, we were able to
make substantial progress: We achieve an F1 of
53.2%, which closes over half of the gap between
a heuristic baseline (26%) and supervised systems
(68%?80%).
91
Dataset # scenarios |w| |T | |s| |A|
Robocup 1919 5.7 9 2.4 0.8
Weather 22146 28.7 12 36.0 5.8
NFL 78 969.0 44 329.0 24.3
Table 1: Statistics for the three datasets. We report average
values across all scenarios in the dataset: |w| is the number of
words in the text, |T | is the number of record types, |s| is the
number of records, and |A| is the number of gold alignments.
2 Domains and Datasets
Our goal is to learn the correspondence between a
text w and the world state s it describes. We use
the term scenario to refer to such a (w, s) pair.
The text is simply a sequence of words w =
(w1, . . . , w|w|). We represent the world state s as
a set of records, where each record r ? s is de-
scribed by a record type r.t ? T and a tuple of
field values r.v = (r.v1, . . . , r.vm).1 For exam-
ple, temperature is a record type in the weather
domain, and it has four fields: time, min, mean,
and max.
The record type r.t ? T specifies the field type
r.tf ? {INT, STR, CAT} of each field value r.vf ,
f = 1, . . . ,m. There are three possible field
types?integer (INT), string (STR), and categori-
cal (CAT)?which are assumed to be known and
fixed. Integer fields represent numeric properties
of the world such as temperature, string fields rep-
resent surface-level identifiers such as names of
people, and categorical fields represent discrete
concepts such as score types in football (touch-
down, field goal, and safety). The field type de-
termines the way we expect the field value to be
rendered in words: integer fields can be numeri-
cally perturbed, string fields can be spliced, and
categorical fields are represented by open-ended
word distributions, which are to be learned. See
Section 3.3 for details.
2.1 Robocup Sportscasting
In this domain, a Robocup simulator generates the
state of a soccer game, which is represented by
a set of event records. For example, the record
pass(arg1=pink1,arg2=pink5) denotes a pass-
ing event; this type of record has two fields: arg1
(the actor) and arg2 (the recipient). As the game is
progressing, humans interject commentaries about
notable events in the game, e.g., pink1 passes back
to pink5 near the middle of the field. All of the
1To simplify notation, we assume that each record has m
fields, though in practice, m depends on the record type r.t.
fields in this domain are categorical, which means
there is no a priori association between the field
value pink1 and the word pink1. This degree of
flexibility is desirable because pink1 is sometimes
referred to as pink goalie, a mapping which does
not arise from string operations but must instead
be learned.
We used the dataset created by Chen and
Mooney (2008), which contains 1919 scenarios
from the 2001?2004 Robocup finals. Each sce-
nario consists of a single sentence representing a
fragment of a commentary on the game, paired
with a set of candidate records. In the annotation,
each sentence corresponds to at most one record
(possibly one not in the candidate set, in which
case we automatically get that sentence wrong).
See Figure 1(a) for an example and Table 1 for
summary statistics on the dataset.
2.2 Weather Forecasts
In this domain, the world state contains de-
tailed information about a local weather forecast
and the text is a short forecast report (see Fig-
ure 1(b) for an example). To create the dataset,
we collected local weather forecasts for 3,753
cities in the US (those with population at least
10,000) over three days (February 7?9, 2009) from
www.weather.gov. For each city and date, we
created two scenarios, one for the day forecast and
one for the night forecast. The forecasts consist of
hour-by-hour measurements of temperature, wind
speed, sky cover, chance of rain, etc., which rep-
resent the underlying world state.
This world state is summarized by records
which aggregate measurements over selected time
intervals. For example, one of the records states
the minimum, average, and maximum tempera-
ture from 5pm to 6am. This aggregation pro-
cess produced 22,146 scenarios, each containing
|s| = 36 multi-field records. There are 12 record
types, each consisting of only integer and categor-
ical fields.
To annotate the data, we split the text by punc-
tuation into lines and labeled each line with the
records to which the line refers. These lines are
used only for evaluation and are not part of the
model (see Section 5.1 for further discussion).
The weather domain is more complex than the
Robocup domain in several ways: The text w is
longer, there are more candidate records, and most
notably, w references multiple records (5.8 on av-
92
xbadPass(arg1=pink11,arg2=purple3)ballstopped()ballstopped()kick(arg1=pink11)turnover(arg1=pink11,arg2=purple3)
s
w:pink11 makes a bad pass and was picked off by purple3
(a) Robocup sportscasting
. . .rainChance(time=26-30,mode=Def)temperature(time=17-30,min=43,mean=44,max=47)windDir(time=17-30,mode=SE)windSpeed(time=17-30,min=11,mean=12,max=14,mode=10-20)precipPotential(time=17-30,min=5,mean=26,max=75)rainChance(time=17-30,mode=--)windChill(time=17-30,min=37,mean=38,max=42)skyCover(time=17-30,mode=50-75)rainChance(time=21-30,mode=--). . .
s
w:Occasional rain after 3am .Low around 43 .South wind between 11 and 14 mph .Chance of precipitation is 80 % .New rainfall amounts between aquarter and half of an inch possible .
(b) Weather forecasts
. . .rushing(entity=richie anderson,att=5,yds=37,avg=7.4,lg=16,td=0)receiving(entity=richie anderson,rec=4,yds=46,avg=11.5,lg=20,td=0)play(quarter=1,description=richie anderson ( dal ) rushed left side for 13 yards .)defense(entity=eric ogbogu,tot=4,solo=3,ast=1,sck=0,yds=0). . .
s w:. . .Former Jets player Richie Andersonfinished with 37 yards on 5 carriesplus 4 receptions for 46 yards .. . .
(c) NFL recaps
Figure 1: An example of a scenario for each of the three domains. Each scenario consists of a candidate set of records s and a
text w. Each record is specified by a record type (e.g., badPass) and a set of field values. Integer values are in Roman, string
values are in italics, and categorical values are in typewriter. The gold alignments are shown.
erage), so the segmentation of w is unknown. See
Table 1 for a comparison of the two datasets.
2.3 NFL Recaps
In this domain, each scenario represents a single
NFL football game (see Figure 1(c) for an exam-
ple). The world state (the things that happened
during the game) is represented by database tables,
e.g., scoring summary, team comparison, drive
chart, play-by-play, etc. Each record is a database
entry, for instance, the receiving statistics for a cer-
tain player. The text is the recap of the game?
an article summarizing the game highlights. The
dataset we used was collected by Barzilay and La-
pata (2005). The data includes 466 games during
the 2003?2004 NFL season. 78 of these games
were annotated by Snyder and Barzilay (2007),
who aligned each sentence to a set of records.
This domain is by far the most complicated of
the three. Many records corresponding to inconse-
quential game statistics are not mentioned. Con-
versely, the text contains many general remarks
(e.g., it was just that type of game) which are
not present in any of the records. Furthermore,
the complexity of the language used in the re-
cap is far greater than what we can represent us-
ing our simple model. Fortunately, most of the
fields are integer fields or string fields (generally
names or brief descriptions), which provide im-
portant anchor points for learning the correspon-
dences. Nonetheless, the same names and num-
bers occur in multiple records, so there is still un-
certainty about which record is referenced by a
given sentence.
3 Generative Model
To learn the correspondence between a text w and
a world state s, we propose a generative model
p(w | s) with latent variables specifying this cor-
respondence.
Our model combines segmentation with align-
ment. The segmentation aspect of our model is
similar to that of Grenager et al (2005) and Eisen-
stein and Barzilay (2008), but in those two models,
the segments are clustered into topics rather than
grounded to a world state. The alignment aspect
of our model is similar to the HMM model for
word alignment (Ney and Vogel, 1996). DeNero
et al (2008) perform joint segmentation and word
alignment for machine translation, but the nature
of that task is different from ours.
The model is defined by a generative process,
93
which proceeds in three stages (Figure 2 shows the
corresponding graphical model):
1. Record choice: choose a sequence of records
r = (r1, . . . , r|r|) to describe, where each
ri ? s.
2. Field choice: for each chosen record ri, se-
lect a sequence of fields fi = (fi1, . . . , fi|fi|),
where each fij ? {1, . . . ,m}.
3. Word choice: for each chosen field fij ,
choose a number cij > 0 and generate a se-
quence of cij words.
The observed text w is the terminal yield formed
by concatenating the sequences of words of all
fields generated; note that the segmentation of w
provided by c = {cij} is latent. Think of the
words spanned by a record as constituting an ut-
terance with a meaning representation given by the
record and subset of fields chosen.
Formally, our probabilistic model places a dis-
tribution over (r, f , c,w) and factorizes according
to the three stages as follows:
p(r, f , c,w | s) = p(r | s)p(f | r)p(c,w | r, f , s)
The following three sections describe each of
these stages in more detail.
3.1 Record Choice Model
The record choice model specifies a distribu-
tion over an ordered sequence of records r =
(r1, . . . , r|r|), where each record ri ? s. This
model is intended to capture two types of regu-
larities in the discourse structure of language. The
first is salience, that is, some record types are sim-
ply more prominent than others. For example, in
the NFL domain, 70% of scoring records are men-
tioned whereas only 1% of punting records are
mentioned. The second is the idea of local co-
herence, that is, the order in which one mentions
records tend to follow certain patterns. For ex-
ample, in the weather domain, the sky conditions
are generally mentioned first, followed by temper-
ature, and then wind speed.
To capture these two phenomena, we define a
Markov model on the record types (and given the
record type, a record is chosen uniformly from the
set of records with that type):
p(r | s) =
|r|?
i=1
p(ri.t | ri?1.t)
1
|s(ri.t)|
, (1)
where s(t)
def
= {r ? s : r.t = t} and r0.t is
a dedicated START record type.2 We also model
the transition of the final record type to a desig-
nated STOP record type in order to capture regu-
larities about the types of records which are de-
scribed last. More sophisticated models of coher-
ence could also be employed here (Barzilay and
Lapata, 2008).
We assume that s includes a special null record
whose type is NULL, responsible for generating
parts of our text which do not refer to any real
records.
3.2 Field Choice Model
Each record type t ? T has a separate field choice
model, which specifies a distribution over a se-
quence of fields. We want to capture salience
and coherence at the field level like we did at the
record level. For instance, in the weather domain,
the minimum and maximum fields of a tempera-
ture record are mentioned whereas the average is
not. In the Robocup domain, the actor typically
precedes the recipient in passing event records.
Formally, we have a Markov model over the
fields:3
p(f | r) =
|r|?
i=1
|fj |?
j=1
p(fij | fi(j?1)). (2)
Each record type has a dedicated null field with
its own multinomial distribution over words, in-
tended to model words which refer to that record
type in general (e.g., the word passes for passing
records). We also model transitions into the first
field and transitions out of the final field with spe-
cial START and STOP fields. This Markov structure
allows us to capture a few elements of rudimentary
syntax.
3.3 Word Choice Model
We arrive at the final component of our model,
which governs how the information about a par-
ticular field of a record is rendered into words. For
each field fij , we generate the number of words cij
from a uniform distribution over {1, 2, . . . , Cmax},
where Cmax is set larger than the length of the
longest text we expect to see. Conditioned on
2We constrain our inference to only consider record types
t that occur in s, i.e., s(t) 6= ?.
3During inference, we prohibit consecutive fields from re-
peating.
94
s
r
f
c,w
s
r1
f11
w1 ? ? ? w
c11
? ? ?
? ? ? ri
fi1
w ? ? ? w
ci1
? ? ? fi|fi|
w ? ? ? w
ci|fi|
? ? ? rn
? ? ? fn|fn|
w ? ? ? w|w|
cn|fn|
Record choice
Field choice
Word choice
Figure 2: Graphical model representing the generative model. First, records are chosen and ordered from the set s. Then fields
are chosen for each record. Finally, words are chosen for each field. The world state s and the words w are observed, while
(r, f , c) are latent variables to be inferred (note that the number of latent variables itself is unknown).
the fields f , the words w are generated indepen-
dently:4
p(w | r, f , c, s) =
|w|?
k=1
pw(wk | r(k).tf(k), r(k).vf(k)),
where r(k) and f(k) are the record and field re-
sponsible for generating word wk, as determined
by the segmentation c. The word choice model
pw(w | t, v) specifies a distribution over words
given the field type t and field value v. This distri-
bution is a mixture of a global backoff distribution
over words and a field-specific distribution which
depends on the field type t.
Although we designed our word choice model
to be relatively general, it is undoubtedly influ-
enced by the three domains. However, we can
readily extend or replace it with an alternative if
desired; this modularity is one principal benefit of
probabilistic modeling.
Integer Fields (t = INT) For integer fields, we
want to capture the intuition that a numeric quan-
tity v is rendered in the text as a word which
is possibly some other numerical value w due to
stylistic factors. Sometimes the exact value v is
used (e.g., in reporting football statistics). Other
times, it might be customary to round v (e.g., wind
speeds are typically rounded to a multiple of 5).
In other cases, there might just be some unex-
plained error, where w deviates from v by some
noise + = w ? v > 0 or ? = v ? w > 0. We
model + and ? as geometric distributions.5 In
4While a more sophisticated model of words would be
useful if we intended to use this model for natural language
generation, the false independence assumptions present here
matter less for the task of learning the semantic correspon-
dences because we always condition on w.
5Specifically, p(+;?+) = (1 ? ?+)+?1?+, where
?+ is a field-specific parameter; p(?;??) is defined analo-
gously.
8 9 10 11 12 13 14 15 16 17 18w
0.1
0.2
0.3
0.4
0.5
p w(
w|
v=
13)
8 9 10 11 12 13 14 15 16 17 18w
0.1
0.2
0.3
0.4
0.6
p w(
w|
v=
13)
(a) temperature.min (b) windSpeed.min
Figure 3: Two integer field types in the weather domain for
which we learn different distributions over the ways in which
a value v might appear in the text as a word w. Suppose the
record field value is v = 13. Both distributions are centered
around v, as is to be expected, but the two distributions have
different shapes: For temperature.min, almost all the mass
is to the left, suggesting that forecasters tend to report con-
servative lower bounds. For the wind speed, the mass is con-
centrated on 13 and 15, suggesting that forecasters frequently
round wind speeds to multiples of 5.
summary, we allow six possible ways of generat-
ing the word w given v:
v dve5 bvc5 round5(v) v ? ? v + +
Separate probabilities for choosing among these
possibilities are learned for each field type (see
Figure 3 for an example).
String Fields (t = STR) Strings fields are in-
tended to represent values which we expect to be
realized in the text via a simple surface-level trans-
formation. For example, a name field with value
v = Moe Williams is sometimes referenced in the
text by just Williams. We used a simple generic
model of rendering string fields: Let w be a word
chosen uniformly from those in v.
Categorical Fields (t = CAT) Unlike string
fields, categorical fields are not tied down to any
lexical representation; in fact, the identities of the
categorical field values are irrelevant. For each
categorical field f and possible value v, we have a
95
v pw(w | t, v)
0-25 , clear mostly sunny
25-50 partly , cloudy increasing
50-75 mostly cloudy , partly
75-100 of inch an possible new a rainfall
Table 2: Highest probability words for the categorical field
skyCover.mode in the weather domain. It is interesting to
note that skyCover=75-100 is so highly correlated with rain
that the model learns to connect an overcast sky in the world
to the indication of rain in the text.
separate multinomial distribution over words from
which w is drawn. An example of a categori-
cal field is skyCover.mode in the weather domain,
which has four values: 0-25, 25-50, 50-75,
and 75-100. Table 2 shows the top words for
each of these field values learned by our model.
4 Learning and Inference
Our learning and inference methodology is a fairly
conventional application of Expectation Maxi-
mization (EM) and dynamic programming. The
input is a set of scenarios D, each of which is a
text w paired with a world state s. We maximize
the marginal likelihood of our data, summing out
the latent variables (r, f , c):
max
?
?
(w,s)?D
?
r,f ,c
p(r, f , c,w | s; ?), (3)
where ? are the parameters of the model (all the
multinomial probabilities). We use the EM algo-
rithm to maximize (3), which alternates between
the E-step and the M-step. In the E-step, we
compute expected counts according to the poste-
rior p(r, f , c | w, s; ?). In the M-step, we op-
timize the parameters ? by normalizing the ex-
pected counts computed in the E-step. In our ex-
periments, we initialized EM with a uniform dis-
tribution for each multinomial and applied add-0.1
smoothing to each multinomial in the M-step.
As with most complex discrete models, the bulk
of the work is in computing expected counts under
p(r, f , c | w, s; ?). Formally, our model is a hier-
archical hidden semi-Markov model conditioned
on s. Inference in the E-step can be done using a
dynamic program similar to the inside-outside al-
gorithm.
5 Experiments
Two important aspects of our model are the seg-
mentation of the text and the modeling of the co-
herence structure at both the record and field lev-
els. To quantify the benefits of incorporating these
two aspects, we compare our full model with two
simpler variants.
? Model 1 (no model of segmentation or co-
herence): Each record is chosen indepen-
dently; each record generates one field, and
each field generates one word. This model is
similar in spirit to IBM model 1 (Brown et
al., 1993).
? Model 2 (models segmentation but not coher-
ence): Records and fields are still generated
independently, but each field can now gener-
ate multiple words.
? Model 3 (our full model of segmentation and
coherence): Records and fields are generated
according to the Markov chains described in
Section 3.
5.1 Evaluation
In the annotated data, each text w has been di-
vided into a set of lines. These lines correspond
to clauses in the weather domain and sentences in
the Robocup and NFL domains. Each line is an-
notated with a (possibly empty) set of records. Let
A be the gold set of these line-record alignment
pairs.
To evaluate a learned model, we com-
pute the Viterbi segmentation and alignment
(argmaxr,f ,c p(r, f , c | w, s)). We produce a pre-
dicted set of line-record pairsA? by aligning a line
to a record ri if the span of (the utterance corre-
sponding to) ri overlaps the line. The reason we
evaluate indirectly using lines rather than using ut-
terances is that it is difficult to annotate the seg-
mentation of text into utterances in a simple and
consistent manner.
We compute standard precision, recall, and F1
of A? with respect to A. Unless otherwise spec-
ified, performance is reported on all scenarios,
which were also used for training. However, we
did not tune any hyperparameters, but rather used
generic values which worked well enough across
all three domains.
5.2 Robocup Sportscasting
We ran 10 iterations of EM on Models 1?3. Ta-
ble 3 shows that performance improves with in-
creased model sophistication. We also compare
96
Method Precision Recall F1
Model 1 78.6 61.9 69.3
Model 2 74.1 84.1 78.8
Model 3 77.3 84.0 80.5
Table 3: Alignment results on the Robocup sportscasting
dataset.
Method F1
Random baseline 48.0
Chen and Mooney (2008) 67.0
Model 3 75.7
Table 4: F1 scores based on the 4-fold cross-validation
scheme in Chen and Mooney (2008).
our model to the results of Chen and Mooney
(2008) in Table 4.
Figure 4 provides a closer look at the predic-
tions made by each of our three models for a par-
ticular example. Model 1 easily mistakes pink10
for the recipient of a pass record because decisions
are made independently for each word. Model 2
chooses the correct record, but having no model
of the field structure inside a record, it proposes
an incorrect field segmentation (although our eval-
uation is insensitive to this). Equipped with the
ability to prefer a coherent field sequence, Model
3 fixes these errors.
Many of the remaining errors are due to the
garbage collection phenomenon familiar from
word alignment models (Moore, 2004; Liang et
al., 2006). For example, the ballstopped record
occurs frequently but is never mentioned in the
text. At the same time, there is a correlation be-
tween ballstopped and utterances such as pink2
holds onto the ball, which are not aligned to any
record in the annotation. As a result, our model
incorrectly chooses to align the two.
5.3 Weather Forecasts
For the weather domain, staged training was nec-
essary to get good results. For Model 1, we ran
15 iterations of EM. For Model 2, we ran 5 it-
erations of EM on Model 1, followed by 10 it-
erations on Model 2. For Model 3, we ran 5 it-
erations of Model 1, 5 iterations of a simplified
variant of Model 3 where records were chosen in-
dependently, and finally, 5 iterations of Model 3.
When going from one model to another, we used
the final posterior distributions of the former to ini-
Method Precision Recall F1
Model 1 49.9 75.1 60.0
Model 2 67.3 70.4 68.8
Model 3 76.3 73.8 75.0
Table 5: Alignment results on the weather forecast dataset.
[Model 1] r:f :w:
passarg2=pink10pink10 turns the ball over to purple5
[Model 2] r:f :w:
turnoverxpink10 turns the ball over arg2=purple5to purple5
[Model 3] r:f :w:
turnoverarg1=pink10pink10 xturns the ball over to arg2=purple5purple5
Figure 4: An example of predictions made by each of the
three models on the Robocup dataset.
tialize the parameters of the latter.6 We also pro-
hibited utterances in Models 2 and 3 from crossing
punctuation during inference.
Table 5 shows that performance improves sub-
stantially in the more sophisticated models, the
gains being greater than in the Robocup domain.
Figure 5 shows the predictions of the three models
on an example. Model 1 is only able to form iso-
lated (but not completely inaccurate) associations.
By modeling segmentation, Model 2 accounts for
the intermediate words, but errors are still made
due to the lack of Markov structure. Model 3
remedies this. However, unexpected structures
are sometimes learned. For example, the temper-
ature.time=6-21 field indicates daytime, which
happens to be perfectly correlated with the word
high, although high intuitively should be associ-
ated with the temperature.max field. In these cases
of high correlation (Table 2 provides another ex-
ample), it is very difficult to recover the proper
alignment without additional supervision.
5.4 NFL Recaps
In order to scale up our models to the NFL do-
main, we first pruned for each sentence the records
which have either no numerical values (e.g., 23,
23-10, 2/4) nor name-like words (e.g., those that
appear only capitalized in the text) in common.
This eliminated all but 1.5% of the record can-
didates per sentence, while maintaining an ora-
6It is interesting to note that this type of staged training
is evocative of language acquisition in children: lexical asso-
ciations are formed (Model 1) before higher-level discourse
structure is learned (Model 3).
97
[Model 1] r:f :w: cloudy , with a
windDirtime=6-21high near
temperaturemax=6363 .
windDirmode=SEeast southeast wind between
windSpeedmin=55 and
windSpeedmean=911 mph .
[Model 2] r:f :w:
rainChancemode=?cloudy ,
temperaturexwith a time=6-21high near max=6363 .
windDirmode=SEeast southeast wind xbetween 5 and
windSpeedmean=911 mph .
[Model 3] r:f :w:
skyCoverxcloudy ,
temperaturexwith a time=6-21high near max=6363 mean=56.
windDirmode=SEeast southeast xwind between
windSpeedmin=55 max=13and 11 xmph .
Figure 5: An example of predictions made by each of the three models on the weather dataset.
cle alignment F1 score of 88.7. Guessing a single
random record for each sentence yields an F1 of
12.0. A reasonable heuristic which uses weighted
number- and string-matching achieves 26.7.
Due to the much greater complexity of this do-
main, Model 2 was easily misled as it tried with-
out success to find a coherent segmentation of the
fields. We therefore created a variant, Model 2?,
where we constrained each field to generate ex-
actly one word. To train Model 2?, we ran 5 it-
erations of EM where each sentence is assumed
to have exactly one record, followed by 5 itera-
tions where the constraint was relaxed to also al-
low record boundaries at punctuation and the word
and. We did not experiment with Model 3 since
the discourse structure on records in this domain is
not at all governed by a simple Markov model on
record types?indeed, most regions do not refer to
any records at all. We also fixed the backoff prob-
ability to 0.1 instead of learning it and enforced
zero numerical deviation on integer field values.
Model 2? achieved an F1 of 39.9, an improve-
ment over Model 1, which attained 32.8. Inspec-
tion of the errors revealed the following problem:
The alignment task requires us to sometimes align
a sentence to multiple redundant records (e.g.,
play and score) referenced by the same part of the
text. However, our model generates each part of
text from only one record, and thus it can only al-
low an alignment to one record.7 To cope with this
incompatibility between the data and our notion of
semantics, we used the following solution: We di-
vided the records into three groups by type: play,
score, and other. Each group has a copy of the
model, but we enforce that they share the same
segmentation. We also introduce a potential that
couples the presence or absence of records across
7The model can align a sentence to multiple records pro-
vided that the records are referenced by non-overlapping
parts of the text.
Method Precision Recall F1
Random (with pruning) 13.1 11.0 12.0
Baseline 29.2 24.6 26.7
Model 1 25.2 46.9 32.8
Model 2? 43.4 37.0 39.9
Model 2? (with groups) 46.5 62.1 53.2
Graph matching (sup.) 73.4 64.5 68.6
Multilabel global (sup.) 87.3 74.5 80.3
Table 6: Alignment results on the NFL dataset. Graph match-
ing and multilabel are supervised results reported in Snyder
and Barzilay (2007).9
groups on the same segment to capture regular co-
occurrences between redundant records.
Table 6 shows our results. With groups, we
achieve an F1 of 53.2. Though we still trail su-
pervised techniques, which attain numbers in the
68?80 range, we have made substantial progress
over our baseline using an unsupervised method.
Furthermore, our model provides a more detailed
analysis of the correspondence between the world
state and text, rather than just producing a single
alignment decision. Most of the remaining errors
made by our model are due to a lack of calibra-
tion. Sometimes, our false positives are close calls
where a sentence indirectly references a record,
and our model predicts the alignment whereas the
annotation standard does not. We believe that fur-
ther progress is possible with a richer model.
6 Conclusion
We have presented a generative model of corre-
spondences between a world state and an unseg-
mented stream of text. By having a joint model
of salience, coherence, and segmentation, as well
as a detailed rendering of the values in the world
state into words in the text, we are able to cope
with the increased ambiguity that arises in this new
data setting, successfully pushing the limits of un-
supervision.
98
References
R. Barzilay and M. Lapata. 2005. Collective content selec-
tion for concept-to-text generation. In Human Language
Technology and Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 331?338, Vancouver,
B.C.
R. Barzilay and M. Lapata. 2008. Modeling local coher-
ence: An entity-based approach. Computational Linguis-
tics, 34:1?34.
P. F. Brown, S. A. D. Pietra, V. J. D. Pietra, and R. L. Mer-
cer. 1993. The mathematics of statistical machine trans-
lation: Parameter estimation. Computational Linguistics,
19:263?311.
D. L. Chen and R. J. Mooney. 2008. Learning to sportscast:
A test of grounded language acquisition. In International
Conference on Machine Learning (ICML), pages 128?
135. Omnipress.
J. DeNero, A. Bouchard-Co?te?, and D. Klein. 2008. Sampling
alignment structure under a Bayesian translation model.
In Empirical Methods in Natural Language Processing
(EMNLP), pages 314?323, Honolulu, HI.
J. Eisenstein and R. Barzilay. 2008. Bayesian unsupervised
topic segmentation. In Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 334?343.
J. Feldman and S. Narayanan. 2004. Embodied meaning in a
neural theory of language. Brain and Language, 89:385?
392.
R. Ge and R. J. Mooney. 2005. A statistical semantic parser
that integrates syntax and semantics. In Computational
Natural Language Learning (CoNLL), pages 9?16, Ann
Arbor, Michigan.
P. Gorniak and D. Roy. 2007. Situated language understand-
ing as filtering perceived affordances. Cognitive Science,
31:197?231.
T. Grenager, D. Klein, and C. D. Manning. 2005. Unsu-
pervised learning of field segmentation models for infor-
mation extraction. In Association for Computational Lin-
guistics (ACL), pages 371?378, Ann Arbor, Michigan. As-
sociation for Computational Linguistics.
R. J. Kate and R. J. Mooney. 2007. Learning language se-
mantics from ambiguous supervision. In Association for
the Advancement of Artificial Intelligence (AAAI), pages
895?900, Cambridge, MA. MIT Press.
P. Liang, B. Taskar, and D. Klein. 2006. Alignment by agree-
ment. In North American Association for Computational
Linguistics (NAACL), pages 104?111, New York City. As-
sociation for Computational Linguistics.
W. Lu, H. T. Ng, W. S. Lee, and L. S. Zettlemoyer. 2008. A
generative model for parsing natural language to meaning
representations. In Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 783?792.
R. C. Moore. 2004. Improving IBM word alignment model
1. In Association for Computational Linguistics (ACL),
pages 518?525, Barcelona, Spain. Association for Com-
putational Linguistics.
H. Ney and S. Vogel. 1996. HMM-based word align-
ment in statistical translation. In International Conference
on Computational Linguistics (COLING), pages 836?841.
Association for Computational Linguistics.
J. M. Siskind. 1996. A computational study of cross-
situational techniques for learning word-to-meaning map-
pings. Cognition, 61:1?38.
B. Snyder and R. Barzilay. 2007. Database-text alignment
via structured multilabel classification. In International
Joint Conference on Artificial Intelligence (IJCAI), pages
1713?1718, Hyderabad, India.
C. Yu and D. H. Ballard. 2004. On the integration of ground-
ing language and learning objects. In Association for the
Advancement of Artificial Intelligence (AAAI), pages 488?
493, Cambridge, MA. MIT Press.
L. S. Zettlemoyer and M. Collins. 2005. Learning to map
sentences to logical form: Structured classification with
probabilistic categorial grammars. In Uncertainty in Arti-
ficial Intelligence (UAI), pages 658?666.
L. S. Zettlemoyer and M. Collins. 2007. Online learn-
ing of relaxed CCG grammars for parsing to logical
form. In Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learning
(EMNLP/CoNLL), pages 678?687.
99
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 410?419,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
A Game-Theoretic Approach to Generating Spatial Descriptions
Dave Golland
UC Berkeley
Berkeley, CA 94720
dsg@cs.berkeley.edu
Percy Liang
UC Berkeley
Berkeley, CA 94720
pliang@cs.berkeley.edu
Dan Klein
UC Berkeley
Berkeley, CA 94720
klein@cs.berkeley.edu
Abstract
Language is sensitive to both semantic and
pragmatic effects. To capture both effects,
we model language use as a cooperative game
between two players: a speaker, who gener-
ates an utterance, and a listener, who responds
with an action. Specifically, we consider the
task of generating spatial references to ob-
jects, wherein the listener must accurately
identify an object described by the speaker.
We show that a speaker model that acts op-
timally with respect to an explicit, embedded
listener model substantially outperforms one
that is trained to directly generate spatial de-
scriptions.
1 Introduction
Language is about successful communication be-
tween a speaker and a listener. For example, if the
goal is to reference the target object O1 in Figure 1,
a speaker might choose one of the following two ut-
terances:
(a) right of O2 (b) on O3
Although both utterances are semantically correct,
(a) is ambiguous between O1 and O3, whereas (b)
unambiguously identifies O1 as the target object,
and should therefore be preferred over (a). In this
paper, we present a game-theoretic model that cap-
tures this communication-oriented aspect of lan-
guage interpretation and generation.
Successful communication can be broken down
into semantics and pragmatics. Most computational
Figure 1: An example of a 3D model of a room. The
speaker?s goal is to reference the target object O1 by de-
scribing its spatial relationship to other object(s). The
listener?s goal is to guess the object given the speaker?s
description.
work on interpreting language focuses on compo-
sitional semantics (Zettlemoyer and Collins, 2005;
Wong and Mooney, 2007; Piantadosi et al, 2008),
which is concerned with verifying the truth of a sen-
tence. However, what is missing from this truth-
oriented view is the pragmatic aspect of language?
that language is used to accomplish an end goal, as
exemplified by speech acts (Austin, 1962). Indeed,
although both utterances (a) and (b) are semantically
valid, only (b) is pragmatically felicitous: (a) is am-
biguous and therefore violates the Gricean maxim
of manner (Grice, 1975). To capture this maxim, we
develop a model of pragmatics based on game the-
ory, in the spirit of Ja?ger (2008) but extended to the
stochastic setting. We show that Gricean maxims
410
fall out naturally as consequences of the model.
An effective way to empirically explore the prag-
matic aspects of language is to work in the grounded
setting, where the basic idea is to map language to
some representation of the non-linguistic world (Yu
and Ballard, 2004; Feldman and Narayanan, 2004;
Fleischman and Roy, 2007; Chen and Mooney,
2008; Frank et al, 2009; Liang et al, 2009). Along
similar lines, past work has also focused on inter-
preting natural language instructions (Branavan et
al., 2009; Eisenstein et al, 2009; Kollar et al, 2010),
which takes into account the goal of the communi-
cation. This work differs from ours in that it does
not clarify the formal relationship between pragmat-
ics and the interpretation task. Pragmatics has also
been studied in the context of dialog systems. For
instance, DeVault and Stone (2007) present a model
of collaborative language between multiple agents
that takes into account contextual ambiguities.
We present our pragmatic model in a grounded
setting where a speaker must describe a target object
to a listener via spatial description (such as in the
example given above). Though we use some of the
techniques from work on the semantics of spatial de-
scriptions (Regier and Carlson, 2001; Gorniak and
Roy, 2004; Tellex and Roy, 2009), we empirically
demonstrate that having a model of pragmatics en-
ables more successful communication.
2 Language as a Game
To model Grice?s cooperative principle (Grice,
1975), we formulate the interaction between a
speaker S and a listener L as a cooperative game, that
is, one in which S and L share the same utility func-
tion. For simplicity, we focus on the production and
interpretation of single utterances, where the speaker
and listener have access to a shared context. To sim-
plify notation, we suppress writing the dependence
on the context.
The Communication Game
1. In order to communicate a target o to L, S pro-
duces an utterance w chosen according to a
strategy pS(w | o).
2. L interprets w and responds with a guess g ac-
cording to a strategy pL(g | w).
3. S and L collectively get a utility of U(o, g).
o w g
U
speaker listener
ps(w | o) pl(g | w)
target utterance guess
utility
Figure 2: Diagram representing the communication
game. A target, o, is given to the speaker that generates
an utterance w. Based on this utterance, the listener gen-
erates a guess g. If o = g, then both the listener and
speaker get a utility of 1, otherwise they get a utility of 0.
This communication game is described graphi-
on O3
1
near O3
0
right of O2
0
Figure 3: Three instances of the communication game on
the scenario in Figure 1. For each instance, the target o,
utterancew, guess g, and the resulting utilityU are shown
in their respective positions. A utility of 1 is awarded only
when the guess matches the target.
cally in Figure 2. Figure 3 shows several instances of
the communication game being played for the sce-
nario in Figure 1.
Grice?s maxim of manner encourages utterances
to be unambiguous, which motivates the following
utility, which we call (communicative) success:
U(o, g)
def
= I[o = g], (1)
where the indicator function I[o = g] is 1 if o =
g and 0 otherwise. Hence, a utility-maximizing
speaker will attempt to produce unambiguous utter-
ances because they increase the probability that the
listener will correctly guess the target.
411
Given a speaker strategy pS(w | o), a listener
strategy pL(g | w), and a prior distribution over tar-
gets p(o), the expected utility obtained by S and L is
as follows:
EU(S, L) =
?
o,w,g
p(o)pS(w|o)pL(g|w)U(o, g)
=
?
o,w
p(o)pS(w|o)pL(o|w). (2)
3 From Reflex Speaker to Rational
Speaker
Having formalized the language game, we now ex-
plore various speaker and listener strategies. First,
let us consider literal strategies. A literal speaker
(denoted S:LITERAL) chooses uniformly from the
set of utterances consistent with a target object, i.e.,
the ones which are semantically valid;1 a literal lis-
tener (denoted L:LITERAL) guesses an object con-
sistent with the utterance uniformly at random.
In the running example (Figure 1), where the tar-
get object is O1, there are two semantically valid ut-
terances:
(a) right of O2 (b) on O3
S:LITERAL selects (a) or (b) each with probability
1
2 . If S:LITERAL chooses (a), L:LITERAL will guess
the target object O1 correctly with probability 12 ; if
S:LITERAL chooses (b), L:LITERAL will guess cor-
rectly with probability 1. Therefore, the expected
utility EU(S:LITERAL, L:LITERAL) = 34 .
We say S:LITERAL is an example of a reflex
speaker because it chooses an utterance without
taking the listener into account. A general reflex
speaker is depicted in Figure 4(a), where each edge
represents a potential utterance.
Suppose we now have a model of some listener
L. Motivated by game theory, we would optimize
the expected utility (2) given pL(g | w). We call
the resulting speaker S(L) the rational speaker with
respect to listener L. Solving for this strategy yields:
pS(L)(w | o) = I[w = w?], where
w? = argmax
w?
pL(o | w
?). (3)
1Semantic validity is approximated by a set of heuristic rules
(e.g. left is all positions with smaller x-coordinates).
S
w1
o w2
w3
S(L)
w1
o
L
g1
w2 g2
g3
w3
(a) Reflex speaker (b) Rational speaker
Figure 4: (a) A reflex speaker (S) directly selects an ut-
terance based only on the target object. Each edge rep-
resents a different choice of utterance. (b) A rational
speaker (S(L)) selects an utterance based on an embed-
ded model of the listener (L). Each edge in the first layer
represents a different choice the speaker can make, and
each edge in the second layer represents a response of the
listener.
Intuitively, S(L) chooses an utterance, w?, such that,
if listener L were to interpret w?, the probability of
L guessing the target would be maximized.2 The ra-
tional speaker is depicted in Figure 4(b), where, as
before, each edge at the first level represents a possi-
ble choice for the speaker, but there is now a second
layer representing the response of the listener.
To see how an embedded model of the listener
improves communication, again consider our run-
ning example in Figure 1. A speaker can describe
the target object O1 using either w1 = on O3 or
w2 = right of O2. Suppose the embedded listener
is L:LITERAL, which chooses uniformly from the
set of objects consistent with the given utterance.
In this scenario, pL:LITERAL(O1 | w1) = 1 because
w1 unambiguously describes the target object, but
pL:LITERAL(O1 | w2) = 12 . The rational speaker
S(L:LITERAL) would therefore choose w1, achiev-
ing a utility of 1, which is an improvement over the
reflex speaker S:LITERAL?s utility of 34 .
2If there are ties, any distribution over the utterances having
the same utility is optimal.
412
4 From Literal Speaker to Learned
Speaker
In the previous section, we showed that a literal
strategy, one that considers only semantically valid
choices, can be used to directly construct a reflex
speaker S:LITERAL or an embedded listener in a
rational speaker S(L:LITERAL). This section fo-
cuses on an orthogonal direction: improving literal
strategies with learning. Specifically, we construct
learned strategies from log-linear models trained on
human annotations. These learned strategies can
then be used to construct reflex and rational speaker
variants?S:LEARNED and S(L:LEARNED), respec-
tively.
4.1 Training a Log-Linear Speaker/Listener
We train the speaker, S:LEARNED, (similarly, lis-
tener, L:LEARNED) on training examples which
comprise the utterances produced by the human an-
notators (see Section 6.1 for details on how this
data was collected). Each example consists of a 3D
model of a room in a house that specifies the 3D po-
sitions of each object and the coordinates of a 3D
camera. When training the speaker, each example is
a pair (o, w), where o is the input target object and
w is the output utterance. When training the listener,
each example is (w, g), where w is the input utter-
ance and g is the output guessed object.
For now, an utterance w consists of two parts:
? A spatial preposition w.r (e.g., right of) from a
set of possible prepositions.3
? A reference object w.o (e.g., O3) from the set
of objects in the room.
We consider more complex utterances in Section 5.
Both S:LEARNED and L:LEARNED are
parametrized by log-linear models:
pS:LEARNED(w|o; ?S) ? exp{?
>
S ?(o, w)} (4)
pL:LEARNED(g|w; ?L) ? exp{?
>
L ?(g, w)} (5)
where ?(?, ?) is the feature vector (see below), ?S
and ?L are the parameter vectors for speaker and lis-
tener. Note that the speaker and listener use the same
3We chose 10 prepositions commonly used by people to de-
scribe objects in a preliminary data gathering experiment. This
list includes multi-word units, which function equivalently to
prepositions, such as left of.
set of features, but they have different parameters.
Furthermore, the first normalization sums over pos-
sible utterances w while the second normalization
sums over possible objects g in the scene. The two
parameter vectors are trained to optimize the log-
likelihood of the training data under the respective
models.
Features We now describe the features ?(o, w).
These features draw inspiration from Landau and
Jackendoff (1993) and Tellex and Roy (2009).
Each object o in the 3D scene is represented by
its bounding box, which is the smallest rectangular
prism containing o. The following are functions of
the camera, target (or guessed object) o, and the ref-
erence object w.o in the utterance. The full set of
features is obtained by conjoining these functions
with indicator functions of the form I[w.r = r],
where r ranges over the set of valid prepositions.
? Proximity functions measure the distance be-
tween o and w.o. This is implemented as the
minimum over all the pairwise Euclidean dis-
tances between the corners of the bounding
boxes. We also have indicator functions for
whether o is the closest object, among the top
5 closest objects, and among the top 10 closest
objects to w.o.
? Topological functions measure containment be-
tween o and w.o: vol(o ? w.o)/vol(o) and
vol(o ? w.o)/vol(w.o). To simplify volume
computation, we approximate each object by a
bounding box that is aligned with the camera
axes.
? Projection functions measure the relative posi-
tion of the bounding boxes with respect to one
another. Specifically, let v be the vector from
the center of w.o to the center of o. There is a
function for the projection of v onto each of the
axes defined by the camera orientation (see Fig-
ure 5). Additionally, there is a set of indicator
functions that capture the relative magnitude of
these projections. For example, there is a indi-
cator function denoting whether the projection
of v onto the camera?s x-axis is the largest of
all three projections.
413
Figure 5: The projection features are computed by pro-
jecting a vector v extending from the center of the ref-
erence object to the center of the target object onto the
camera axes fx and fy .
5 Handling Complex Utterances
So far, we have only considered speakers and lis-
teners that deal with utterances consisting of one
preposition and one reference object. We now ex-
tend these strategies to handle more complex utter-
ances. Specifically, we consider utterances that con-
form to the following grammar:4
[noun] N ? something | O1 | O2 | ? ? ?
[relation] R ? in front of | on | ? ? ?
[conjunction] NP ? N RP?
[relativization] RP ? R NP
This grammar captures two phenomena of lan-
guage use, conjunction and relativization.
? Conjunction is useful when one spatial relation
is insufficient to disambiguate the target object.
For example, in Figure 1, right of O2 could re-
fer to the vase or the table, but using the con-
junction right of O2 and on O3 narrows down
the target object to just the vase.
? The main purpose of relativization is to refer
to objects without a precise nominal descrip-
tor. With complex utterances, it is possible to
chain relative prepositional phrases, for exam-
ple, using on something right of O2 to refer to
the vase.
4Naturally, we disallow direct reference to the target object.
Given an utterancew, we define its complexity |w|
as the number of applications of the relativization
rule, RP ? R NP, used to produce w. We had only
considered utterances of complexity 1 in previous
sections.
5.1 Example Utterances
To illustrate the types of utterances available under
the grammar, again consider the scene in Figure 1.
Utterances of complexity 2 can be generated ei-
ther using the relativization rule exclusively, or both
the conjunction and relativization rules. The rela-
tivization rule can be used to generate the following
utterances:
? on something that is right of O2
? right of something that is left of O3
Applying the conjunction rule leads to the following
utterances:
? right of O2 and on O3
? right of O2 and under O1
? left of O1 and left of O3
Note that we inserted the words that is after each N
and the word and between every adjacent pair of RPs
generated via the conjunction rule. This is to help a
human listener interpret an utterance.
5.2 Extending the Rational Speaker
Suppose we have a rational speaker S(L) defined in
terms of an embedded listener L which operates over
utterances of complexity 1. We first extend L to in-
terpret arbitrary utterances of our grammar. The ra-
tional speaker (defined in (2)) automatically inherits
this extension.
Compositional semantics allows us to define the
interpretation of complex utterances in terms of sim-
pler ones. Specifically, each node in the parse tree
has a denotation, which is computed recursively
in terms of the node?s children via a set of sim-
ple rules. Usually, denotations are represented as
lambda-calculus functions, but for us, they will be
distributions over objects in the scene. As a base
case for interpreting utterances of complexity 1, we
can use either L:LITERAL or L:LEARNED (defined
in Sections 3 and 4).
414
Given a subtree w rooted at u ? {N, NP, RP}, we
define the denotation of w, JwK, to be a distribution
over the objects in the scene in which the utterance
was generated. The listener strategy pL(g|w) = JwK
is recursively as follows:
? If w is rooted at N with a single child x, then JwK
is the uniform distribution over N (x), the set of
objects consistent with the word x.
? If w is rooted at NP, we recursively compute the
distributions over objects g for each child tree,
multiply the probabilities, and renormalize (Hin-
ton, 1999).
? Ifw is rooted at RP with relation r, we recursively
compute the distribution over objects g? for the
child NP tree. We then appeal to the base case
to produce a distribution over objects g which are
related to g? via relation r.
This strategy is defined formally as follows:
pL(g | w) ?
?
?????
?????
I[g ? N (x)] w = (N x)
k?
j=1
pL(g | wj) w = (NP w1 . . . wk)
?
g?
pL(g | (r, g?))pL(g? | w?) w = (RP (R r)w?)
(6)
Figure 6 shows an example of this bottom-
up denotation computation for the utterance
on something right of O2 with respect to the scene
in Figure 1. The denotation starts with the lowest
NP node JO2K, which places all the mass on O2
in the scene. Moving up the tree, we compute
the denotation of the RP, Jright of O2K, using the
RP case of (6), which results in a distribution that
places equal mass on O1 and O3.5 The denotation
of the N node JsomethingK is a flat distribution over
all the objects in the scene. Continuing up the tree,
the denotation of the NP is computed by taking a
product of the object distributions, and turns out
to be exactly the same split distribution as its RP
child. Finally, the denotation at the root is computed
by applying the base case to on and the resulting
distribution from the previous step.
5It is worth mentioning that this split distribution between
O1 and O3 represents the ambiguity mentioned in Section 3
when discussing the shortcomings of S:LITERAL.
Figure 6: The listener model maps an utterance to a dis-
tribution over objects in the room. Each internal NP or RP
node is a distribution over objects in the room.
Generation So far, we have defined the listener
strategy pL(g | w). Given target o, the rational
speaker S(L) with respect to this listener needs to
compute argmaxw pL(o | w) as dictated by (3). This
maximization is performed by enumerating all utter-
ances of bounded complexity.
5.3 Modeling Listener Confusion
One shortcoming of the previous approach for ex-
tending a listener is that it falsely assumes that a lis-
tener can reliably interpret a simple utterance just as
well as it can a complex utterance.
We now describe a more realistic speaker which
is robust to listener confusion. Let ? ? [0, 1] be
a focus parameter which determines the confusion
level. Suppose we have a listener L. When presented
with an utterance w, for each application of the rela-
tivization rule, we have a 1?? probability of losing
focus. If we stay focused for the entire utterance
(with probability ?|w|), then we interpret the utter-
ance according to pL. Otherwise (with probability
1 ? ?|w|), we guess an object at random according
to prnd(g | w). We then use (3) to define the rational
speaker S(L) with respect the following ?confused
listener? strategy:
p?L(g | w) = ?
|w|pL(g | w) + (1 ? ?
|w|)prnd(g | w).
(7)
As ? ? 0, the confused listener is more likely to
make a random guess, and thus there is a stronger
penalty against using more complex utterances. As
415
? ? 1, the confused listener converges to pL and the
penalty for using complex utterances vanishes.
5.4 The Taboo Setting
Notice that the rational speaker as defined so far
does not make full use of our grammar. Specifi-
cally, the rational speaker will never use the ?wild-
card? noun something nor the relativization rule in
the grammar because an NP headed by the wildcard
something can always be replaced by the object ID
to obtain a higher utility. For instance, in Figure 6,
the NP spanning something right of O2 can be re-
placed by O3.
However, it is not realistic to assume that all ob-
jects can be referenced directly. To simulate scenar-
ios where some objects cannot be referenced directly
(and to fully exercise our grammar), we introduce
the taboo setting. In this setting, we remove from
the lexicon some fraction of the object IDs which are
closest to the target object. Since the tabooed objects
cannot be referenced directly, a speaker must resort
to use of the wildcard something and relativization.
For example, in Figure 7, we enable tabooing
around the target O1. This prevents the speaker from
referring directly to O3, so the speaker is forced to
describe O3 via the relativization rule, for example,
producing something right of O2.
Figure 7: With tabooing enabled around O1, O3 can no
longer be referred to directly (represented by an X).
6 Experiments
We now present our empirical results, showing that
rational speakers, who have embedded models of lis-
Figure 8: Mechanical Turk speaker task: Given the tar-
get object (e.g., O1), a human speaker must choose an
utterance to describe the object (e.g., right of O2).
teners, can communicate more successfully than re-
flex speakers, who do not.
6.1 Setup
We collected 43 scenes (rooms) from the Google
Sketchup 3D Warehouse, each containing an aver-
age of 22 objects (household items and pieces of fur-
niture arranged in a natural configuration). For each
object o in a scene, we create a scenario, which rep-
resents an instance of the communication game with
o as the target object. There are a total of 2,860 sce-
narios, which we split evenly into a training set (de-
noted TR) and a test set (denoted TS).
We created the following two Amazon Mechani-
cal Turk tasks, which enable humans to play the lan-
guage game on the scenarios:
Speaker Task In this task, human annotators play
the role of speakers in the language game. They are
prompted with a target object o and asked to each
produce an utterance w (by selecting a preposition
w.r from a dropdown list and clicking on a reference
objectw.o) that best informs a listener of the identity
of the target object.
For each training scenario o, we asked three
speakers to produce an utterancew. The three result-
ing (o, w) pairs are used to train the learned reflex
speaker (S:LITERAL). These pairs were also used to
train the learned reflex listener (L:LITERAL), where
the target o is treated as the guessed object. See Sec-
tion 4.1 for the details of the training procedure.
Listener Task In this task, human annotators play
the role of listeners. Given an utterance generated by
a speaker (human or not), the human listener must
416
O2
O1
O3
Question: What object is right of           ?O2
Figure 9: Mechanical Turk listener task: a human listener
is prompted with an utterance generated by a speaker
(e.g., right of O2), and asked to click on an object (shown
by the red arrow).
guess the target object that the speaker saw by click-
ing on an object. The purpose of the listener task is
to evaluate speakers, as described in the next section.
6.2 Evaluation
Utility (Communicative Success) We primarily
evaluate a speaker by its ability to communicate suc-
cessfully with a human listener. For each test sce-
nario, we asked three listeners to guess the object.
We use pL:HUMAN(g | w) to denote the distribution
over guessed objects g given prompt w. For exam-
ple, if two of the three listeners guessed O1, then
pL:HUMAN(O1 | w) = 23 . The expected utility (2) is
then computed by averaging the utility (communica-
tive success) over the test scenarios TS:
SUCCESS(S) = EU(S, L:HUMAN) (8)
=
1
|TS|
?
o?TS
?
w
pS(w|o)pL:HUMAN(o|w).
Exact Match As a secondary evaluation metric,
we also measure the ability of our speaker to exactly
match an utterance produced by a human speaker.
Note that since there are many ways of describing
an object, exact match is neither necessary nor suffi-
cient for successful communication.
We asked three human speakers to each pro-
duce an utterance w given a target o. We use
pS:HUMAN(w | o) to denote this distribution; for ex-
ample, pS:HUMAN(right of O2 | o) = 13 if exactly one
of the three speakers uttered right of O2. We then
Speaker Success Exact Match
S:LITERAL [reflex] 4.62% 1.11%
S(L:LITERAL) [rational] 33.65% 2.91%
S:LEARNED [reflex] 38.36% 5.44%
S(L:LEARNED) [rational] 52.63% 14.03%
S:HUMAN 41.41% 19.95%
Table 1: Comparison of various speakers on communica-
tive success and exact match, where only utterances of
complexity 1 are allowed. The rational speakers (with
respect to both the literal listener L:LITERAL and the
learned listener L:LEARNED) perform better than their
reflex counterparts. While the human speaker (composed
of three people) has higher exact match (it is better at
mimicking itself), the rational speaker S(L:LEARNED)
actually achieves higher communicative success than the
human listener.
define the exact match of a speaker S as follows:
MATCH(S) =
1
|TS|
?
o?TS
?
w
pS:HUMAN(w | o)pS(w | o).
(9)
6.3 Reflex versus Rational Speakers
We first evaluate speakers in the setting where only
utterances of complexity 1 are allowed. Table 1
shows the results on both success and exact match.
First, our main result is that the two rational speak-
ers S(L:LITERAL) and S(L:LEARNED), which each
model a listener explicitly, perform significantly bet-
ter than the corresponding reflex speakers, both in
terms of success and exact match.
Second, it is natural that the speakers that in-
volve learning (S:LITERAL and S(L:LITERAL))
outperform the speakers that only consider the
literal meaning of utterances (S:LEARNED and
S(L:LEARNED)), as the former models capture sub-
tler preferences using features.
Finally, we see that in terms of exact match, the
human speaker S:HUMAN performs the best (this
is not surprising because human exact match is es-
sentially the inter-annotator agreement), but in terms
of communicative success, S(L:LEARNED) achieves
a higher success rate than S:HUMAN, suggesting
that the game-theoretic modeling undertaken by the
rational speakers is effective for communication,
which is ultimate goal of language.
Note that exact match is low even for the ?human
speaker?, since there are often many equally good
417
0.2 0.4 0.6 0.8 1.0?
0.49
0.5
0.51
0.52
suc
ces
s
Figure 10: Communicative success as a function of focus
parameter ? without tabooing on TSDEV. The optimal
value of ? is obtained at 0.79.
ways to evoke an object. At the same time, the suc-
cess rates for all speakers are rather low, reflecting
the fundamental difficulty of the setting: sometimes
it is impossible to unambiguously evoke the target
object via short utterances. In the next section, we
show that we can improve the success rate by al-
lowing the speakers to generate more complex utter-
ances.
6.4 Generating More Complex Utterances
We now evaluate the rational speaker
S(L:LEARNED) when it is allowed to generate
utterances of complexity 1 or 2. Recall from
Section 5.3 that the speaker depends on a focus
parameter ?, which governs the embedded listener?s
ability to interpret the utterance. We divided the test
set (TS) in two halves: TSDEV, which we used to
tune the value of ? and TSFINAL, which we used to
evaluate success rates.
Figure 10 shows the communicative success as
a function of ? on TSDEV. When ? is small, the
embedded listener is confused more easily by more
complex utterances; therefore the speaker tends to
choose mostly utterances of complexity 1. As ?
increases, the utterances increase in complexity, as
does the success rate. However, when ? approaches
1, the utterances are too complex and the success
rate decreases. The dependence between ? and av-
erage utterance complexity is shown in Figure 11.
Table 2 shows the success rates on TSFINAL for
? ? 0 (all utterances have complexity 1), ? = 1 (all
utterances have complexity 2), and ? tuned to max-
imize the success rate based on TSDEV. Setting ?
in this manner allows us to effectively balance com-
plexity and ambiguity, resulting in an improvement
in the success rate.
0.2 0.4 0.6 0.8 1.0?
1.2
1.4
1.6
1.8
2.0
ave
rag
e|w
|
Figure 11: Average utterance complexity as a function of
the focus parameter ? on TSDEV. Higher values of ?
yield more complex utterances.
Taboo Success Success Success
Amount (? ? 0) (? = 1) (? = ??) ??
0% 51.78% 50.99% 54.53% 0.79
5% 38.75% 40.83% 43.12% 0.89
10% 29.57% 29.69% 30.30% 0.80
30% 12.40% 13.04% 12.98% 0.81
Table 2: Communicative success (on TSFINAL) of the
rational speaker S(L:LEARNED) for various values of ?
across different taboo amounts. When the taboo amount
is small, small values of ? lead to higher success rates. As
the taboo amount increases, larger values of ? (resulting
in more complex utterances) are better.
7 Conclusion
Starting with the view that the purpose of language
is successful communication, we developed a game-
theoretic model in which a rational speaker gener-
ates utterances by explicitly taking the listener into
account. On the task of generating spatial descrip-
tions, we showed the rational speaker substantially
outperforms a baseline reflex speaker that does not
have an embedded model. Our results therefore sug-
gest that a model of the pragmatics of communica-
tion is an important factor to consider for generation.
Acknowledgements This work was supported by
the National Science Foundation through a Gradu-
ate Research Fellowship to the first two authors. We
also would like to acknowledge Surya Murali, the
designer of the 3D Google Sketchup models, and
thank the anonymous reviewers for their comments.
References
J. L. Austin. 1962. How to do Things with Words: The
William James Lectures delivered at Harvard Univer-
418
sity in 1955. Oxford, Clarendon, UK.
S. Branavan, H. Chen, L. S. Zettlemoyer, and R. Barzilay.
2009. Reinforcement learning for mapping instruc-
tions to actions. In Association for Computational Lin-
guistics and International Joint Conference on Natural
Language Processing (ACL-IJCNLP), Singapore. As-
sociation for Computational Linguistics.
D. L. Chen and R. J. Mooney. 2008. Learning to
sportscast: A test of grounded language acquisition.
In International Conference on Machine Learning
(ICML), pages 128?135. Omnipress.
David DeVault and Matthew Stone. 2007. Managing
ambiguities across utterances in dialogue.
J. Eisenstein, J. Clarke, D. Goldwasser, and D. Roth.
2009. Reading to learn: Constructing features from
semantic abstracts. In Empirical Methods in Natural
Language Processing (EMNLP), Singapore.
J. Feldman and S. Narayanan. 2004. Embodied meaning
in a neural theory of language. Brain and Language,
89:385?392.
M. Fleischman and D. Roy. 2007. Representing inten-
tions in a cognitive model of language acquisition: Ef-
fects of phrase structure on situated verb learning. In
Association for the Advancement of Artificial Intelli-
gence (AAAI), Cambridge, MA. MIT Press.
M. C. Frank, N. D. Goodman, and J. B. Tenenbaum.
2009. Using speakers? referential intentions to model
early cross-situational word learning. Psychological
Science, 20(5):578?585.
Peter Gorniak and Deb Roy. 2004. Grounded semantic
composition for visual scenes. In Journal of Artificial
Intelligence Research, volume 21, pages 429?470.
H. P. Grice. 1975. Syntax and Semantics; Logic and
Conversation. 3:Speech Acts:41?58.
G. Hinton. 1999. Products of experts. In International
Conference on Artificial Neural Networks (ICANN).
G. Ja?ger. 2008. Game theory in semantics and pragmat-
ics. Technical report, University of Tu?bingen.
T. Kollar, S. Tellex, D. Roy, and N. Roy. 2010. Toward
understanding natural language directions. In Human-
Robot Interaction, pages 259?266.
Barbara Landau and Ray Jackendoff. 1993. ?what?
and ?where? in spatial language and spatial cognition.
Behavioral and Brain Sciences, 16(2spatial preposi-
tions analysis, cross linguistic conceptual similarities;
comments/response):217?238.
P. Liang, M. I. Jordan, and D. Klein. 2009. Learning se-
mantic correspondences with less supervision. In As-
sociation for Computational Linguistics and Interna-
tional Joint Conference on Natural Language Process-
ing (ACL-IJCNLP), Singapore. Association for Com-
putational Linguistics.
S. T. Piantadosi, N. D. Goodman, B. A. Ellis, and J. B.
Tenenbaum. 2008. A Bayesian model of the acquisi-
tion of compositional semantics. In Proceedings of the
Thirtieth Annual Conference of the Cognitive Science
Society.
T Regier and LA Carlson. 2001. Journal of experimen-
tal psychology. general; grounding spatial language in
perception: an empirical and computational investiga-
tion. 130(2):273?298.
Stefanie Tellex and Deb Roy. 2009. Grounding spatial
prepositions for video search. In ICMI.
Y. W. Wong and R. J. Mooney. 2007. Learning syn-
chronous grammars for semantic parsing with lambda
calculus. In Association for Computational Linguis-
tics (ACL), pages 960?967, Prague, Czech Republic.
Association for Computational Linguistics.
C. Yu and D. H. Ballard. 2004. On the integration of
grounding language and learning objects. In Asso-
ciation for the Advancement of Artificial Intelligence
(AAAI), pages 488?493, Cambridge, MA. MIT Press.
L. S. Zettlemoyer and M. Collins. 2005. Learning to
map sentences to logical form: Structured classifica-
tion with probabilistic categorial grammars. In Uncer-
tainty in Artificial Intelligence (UAI), pages 658?666.
419
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 502?512,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
A Simple Domain-Independent Probabilistic Approach to Generation
Gabor Angeli
UC Berkeley
Berkeley, CA 94720
gangeli@berkeley.edu
Percy Liang
UC Berkeley
Berkeley, CA 94720
pliang@cs.berkeley.edu
Dan Klein
UC Berkeley
Berkeley, CA 94720
klein@cs.berkeley.edu
Abstract
We present a simple, robust generation system
which performs content selection and surface
realization in a unified, domain-independent
framework. In our approach, we break up
the end-to-end generation process into a se-
quence of local decisions, arranged hierar-
chically and each trained discriminatively.
We deployed our system in three different
domains?Robocup sportscasting, technical
weather forecasts, and common weather fore-
casts, obtaining results comparable to state-of-
the-art domain-specific systems both in terms
of BLEU scores and human evaluation.
1 Introduction
In this paper, we focus on the problem of generat-
ing descriptive text given a world state represented
by a set of database records. While existing gen-
eration systems can be engineered to obtain good
performance on particular domains (e.g., Dale et
al. (2003), Green (2006), Turner et al (2009), Re-
iter et al (2005), inter alia), it is often difficult
to adapt them across different domains. Further-
more, content selection (what to say: see Barzilay
and Lee (2004), Foster and White (2004), inter alia)
and surface realization (how to say it: see Ratna-
parkhi (2002), Wong and Mooney (2007), Chen and
Mooney (2008), Lu et al (2009), etc.) are typically
handled separately. Our goal is to build a simple,
flexible system which is domain-independent and
performs content selection and surface realization in
a unified framework.
We operate in a setting in which we are only given
examples consisting of (i) a set of database records
(input) and (ii) example human-generated text de-
scribing some of those records (output). We use the
model of Liang et al (2009) to automatically induce
the correspondences between words in the text and
the actual database records mentioned.
We break up the full generation process into a se-
quence of local decisions, training a log-linear clas-
sifier for each type of decision. We use a simple
but expressive set of domain-independent features,
where each decision is allowed to depend on the en-
tire history of previous decisions, as in the model
of Ratnaparkhi (2002). These long-range contextual
dependencies turn out to be critical for accurate gen-
eration.
More specifically, our model is defined in terms
of three types of decisions. The first type
chooses records from the database (macro content
selection)?for example, wind speed, in the case
of generating weather forecasts. The second type
chooses a subset of fields from a record (micro con-
tent selection)?e.g., the minimum and maximum
temperature. The third type chooses a suitable tem-
plate to render the content (surface realization)?
e.g., winds between [min] and [max] mph; templates
are automatically extracted from training data.
We tested our approach in three domains:
ROBOCUP, for sportscasting (Chen and Mooney,
2008); SUMTIME, for technical weather forecast
generation (Reiter et al, 2005); and WEATHERGOV,
for common weather forecast generation (Liang et
al., 2009). We performed both automatic (BLEU)
and human evaluation. On WEATHERGOV, we
502
s: pass(arg1=purple6, arg2=purple3)kick(arg1=purple3)badPass(arg1=purple3,arg2=pink9)turnover(arg1=purple3,arg2=pink9)
w: purple3 made a bad passthat was picked off by pink9
(a) Robocup
s: temperature(time=5pm-6am,min=48,mean=53,max=61)windSpeed(time=5pm-6am,min=3,mean=6,max=11,mode=0-10)windDir(time=5pm-6am,mode=SSW)gust(time=5pm-6am,min=0,mean=0,max=0)skyCover(time=5pm-9pm,mode=0-25)skyCover(time=2am-6am,mode=75-100)precipPotential(time=5pm-6am,min=2,mean=14,max=20)rainChance(time=5pm-6am,mode=someChance)
w: a 20 percent chance of showers after midnight . increasing clouds ,with a low around 48 southwest wind between 5 and 10 mph
(b) WeatherGov
s: wind10m(time=6am,dir=SW,min=16,max=20,gust min=0,gust max=-)wind10m(time=9pm,dir=SSW,min=28,max=32,gust min=40,gust max=-)wind10m(time=12am,dir=-,min=24,max=28,gust min=36,gust max=-)
w: sw 16 - 20 backing ssw 28 - 32 gusts 40 by mid evening easing 24 - 28 gusts 36 late evening
(c) SumTime
Figure 1: Example scenarios (a scenario is a world state s paired with a text w) for each of the three domains. Each row in the
world state denotes a record. Our generation task is to map a world state s (input) to a text w (output). Note that this mapping
involves both content selection and surface realization.
achieved a BLEU score of 51.5 on the combined task
of content selection and generation, which is more
than a two-fold improvement over a model similar
to that of Liang et al (2009). On ROBOCUP and
SUMTIME, we achieved results comparable to the
state-of-the-art. most importantly, we obtained these
results with a general-purpose approach that we be-
lieve is simpler than current state-of-the-art systems.
2 Setup and Domains
Our goal is to generate a text given a world state.
The world state, denoted s, is represented by a set
of database records. Define T to be a set of record
types, where each record type t ? T is associated
with a set of fields FIELDS(t). Each record r ? s
has a record type r.t ? T and a field value r.v[f ] for
each field f ? FIELDS(t). The text, denoted w, is
represented by a sequence of tokenized words. We
use the term scenario to denote a world state s paired
with a text w.
In this paper, we conducted experiments on three
domains, which are detailed in the following subsec-
tions. Example scenarios for each domain are de-
tailed in Figure 1.
2.1 ROBOCUP: Sportscasting
A world state in the ROBOCUP domain is a set of
event records (meaning representations in the termi-
nology of Chen and Mooney (2008)) generated by
a robot soccer simulator. For example, the record
pass(arg1=pink1,arg2=pink5) denotes a passing
event; records of this type (pass) have two fields:
arg1 (the agent) and arg2 (the recipient). As the
game progresses, human commentators talk about
some of the events in the game, e.g., purple3 made
a bad pass that was picked off by pink9.
We used the dataset created by Chen and Mooney
(2008), which contains 1919 scenarios from the
2001?2004 Robocup finals. Each scenario con-
sists of a single sentence representing a fragment
of a commentary on the game, paired with a set
of candidate records, which were recorded within
five seconds of the commentary. The records in the
ROBOCUP dataset data were aligned by Chen and
Mooney (2008). Each scenario contains on average
|s| = 2.4 records and 5.7 words. See Figure 1(a) for
an example of a scenario. Content selection in this
domain is choosing the single record to talk about,
and surface realization is talking about it.
503
2.2 SUMTIME: Technical Weather Forecasts
Reiter et al (2005) developed a generation system
and created the SUMTIME-METEO corpus, which
consists of marine wind weather forecasts used by
offshore oil rigs, generated by the output of weather
simulators. More specifically, these forecasts de-
scribe various aspects of the wind at different times
during the forecast period.
We used the version of the SUMTIME-METEO
corpus created by Belz (2008). The dataset consists
of 469 scenarios, each containing on average |s| =
2.6 records and 16.2 words. See Figure 1(c) for an
example of a scenario. This task requires no content
selection, only surface realization: The records are
given in some fixed order and the task is to generate
from each of these records in turn; of course, due
to contextual dependencies, these records cannot be
generated independently.
2.3 WEATHERGOV: Common Weather
Forecasts
In the WEATHERGOV domain, the world state con-
tains detailed information about a local weather
forecast (e.g., temperature, rain chance, etc.). The
text is a short forecast report based on this informa-
tion.
We used the dataset created by Liang et al (2009).
The world state is summarized by records which ag-
gregate measurements over selected time intervals.
The dataset consists of 29,528 scenarios, each con-
taining on average |s| = 36 records and 28.7 words.
See Figure 1(b) for an example of a scenario.
While SUMTIME and WEATHERGOV are both
weather domains, there are significant differences
between the two. SUMTIME forecasts are in-
tended to be read by trained meteorologists, and thus
the text is quite abbreviated. On the other hand,
WEATHERGOV texts are intended to be read by the
general public and thus is more English-like. Fur-
thermore, SUMTIME does not require content selec-
tion, whereas content selection is a major focus of
WEATHERGOV. Indeed, on average, only 5 of 36
records are actually mentioned in a WEATHERGOV
scenario. Also, WEATHERGOV is more complex:
The text is more varied, there are multiple record
types, and there are about ten times as many records
in each world state.
Generation Process
for i = 1, 2, . . . :
?choose a record ri ? s
?if ri = STOP: return
?choose a field set Fi ? FIELDS(ri.t)
?choose a template Ti ? TEMPLATES(ri.t, Fi)
Figure 2: Pseudocode for the generation process. The generated
text w is a deterministic function of the decisions.
3 The Generation Process
To model the process of generating a text w from a
world state s, we decompose the generation process
into a sequence of local decisions. There are two as-
pects of this decomposition that we need to specify:
(i) how the decisions are structured; and (ii) what
pieces of information govern the decisions.
The decisions are structured hierarchically into
three types of decisions: (i) record decisions, which
determine which records in the world state to talk
about (macro content selection); (ii) field set deci-
sions, which determine which fields of those records
to mention (micro content selection); and (iii) tem-
plate decisions, which determine the actual words
to use to describe the chosen fields (surface realiza-
tion). Figure 2 shows the pseudocode for the gen-
eration process, while Figure 3 depicts an example
of the generation process on a WEATHERGOV sce-
nario.
Each of these decisions is governed by a set of
feature templates (see Figure 4), which are repre-
sented as functions of the current decision and past
decisions. The feature weights are learned from
training data (see Section 4.3).
We chose a set of generic domain-independent
feature templates, described in the sections below.
These features can, in general, depend on the current
decision and all previous decisions. For example, re-
ferring to Figure 4, R2 features on the record choice
depend on all the previous record decisions, and R5
features depend on the most recent template deci-
sion. This is in contrast with most systems for con-
tent selection (Barzilay and Lee, 2004) and surface
realization (Belz, 2008), where decisions must de-
compose locally according to either a graph or tree.
The ability to use global features in this manner is
504
Worldstate skyCover1: skyCover(time=5pm-6am,mode=50-75)temperature1: temperature(time=5pm-6am,min=44,mean=49,max=60)...
Decisions
Record r1 = skyCover1 r2 = temperature1 r3 = stop
Field set F1 = {mode} F2 = {time,min}
Template T1 = ?mostly cloudy ,? T2 = ?with a low around [min] .?
Text mostly cloudy , with a low around 45 .
Specific active (nonzero) features for highlighted decisions
r2 = temperature1
(R1) Jr2.t = temperature and (r1.t, r0.t) = (skyCover, start)KJr2.t = temperature and (r1.t) = (skyCover)K(R2) Jr2.t = temperature and {r1.t} = {skyCover}K(R3) Jr2.t = temperature and rj .t 6= temperature ?j < 2K(R4) Jr2.t = temperature and r2.v[time] = 5pm-6amKJr2.t = temperature and r2.v[min] = lowKJr2.t = temperature and r2.v[mean] = lowKJr2.t = temperature and r2.v[max] = mediumK
F2 = {time,min} (F1) JF2 = {time,min}K(F2) JF2 = {time,min} and r2.v[time] = 5pm-6amK(F2) JF2 = {time,min} and r2.v[min] = lowK
T2 = ?with a low around [min]?
(W1) JBase(T2) = ?with a low around [min]?KJCoarse(T2) = ?with a [time] around [min]?K(W2) JBase(T2) = ?with a low around [min]? and r2.v[time] = 5pm-6amKJCoarse(T2) = ?with a [time] around [min]? and r2.v[time] = 5pm-6amKJBase(T2) = ?with a low around [min]? and r2.v[min] = lowKJCoarse(T2) = ?with a [time] around [min]? and r2.v[min] = lowK(W3) log plm(with | cloudy ,)
Figure 3: The generation process on an example WEATHERGOV scenario. The figure is divided into two parts: The upper part of
the figure shows the generation of text from the world state via a sequence of seven decisions (in boxes). Three of these decisions
are highlighted and the features that govern these decisions are shown in the lower part of the figure. Note that different decisions
in the generation process would result in different features being active (nonzero).
Feature TemplatesRecord R1? list of last k record types Jri.t = ? and (ri?1.t, . . . , ri?k.t) = ?K for k ? {1, 2}R2 set of previous record types Jri.t = ? and {rj .t : j < i} = ?KR3 record type already generated Jrj .t = ri.t for some j < iKR4 field values Jri.t = ? and ri.v[f ] = ?K for f ? Fields(ri.t)R5? stop under language model (LM) Jri.t = stopK? log plm(stop | previous two words generated)Field set F1? field set JFi = ?KF2 field values JFi = ? and ri.v[f ] = ?K for f ? FiTemplate W1? base/coarse generation template Jh(Ti) = ?K for h ? {Base,Coarse}W2 field values Jh(Ti) = ? and ri.v[f ] = ?K for f ? Fi, h ? {Base,Coarse}W3? first word of template under LM log plm(first word in Ti | previous two words)
Figure 4: Feature templates that govern the record, field set, and template decisions. Each line specifies the name, informal
description, and formal description of a set of features, obtained by ranging ? over possible values (for example, for Jri.t = ?K, ?
ranges over all record types T ). Notation: JeK returns 1 if the expression e is true and 0 if it is false. These feature templates are
domain-independent; that is, they are used to create features automatically across domains. Feature templates marked with ? are
included in our baseline system (Section 5.2).
one of the principal advantages of our approach.
3.1 Record Decisions
Record decisions are responsible for macro content
selection. Each record decision chooses a record ri
from the world state s according to features of the
following types:
R1 captures the discourse coherence aspect of
content selection; for example, we learn that
windSpeed tends to follow windDir (but not al-
505
ways). R2 captures an unordered notion of
coherence?simply which sets of record types are
preferable; for example, we learn that rainChance
is not generated if sleetChance already was men-
tioned. R3 is a coarser version of R2, capturing
how likely it is to propose a record of a type that has
already been generated. R4 captures the important
aspect of content selection that the records chosen
depend on their field values;1 for example, we learn
that snowChance is not chosen unless there is snow.
R5 allows the language model to indicate whether a
STOP record is appropriate; this helps prevent sen-
tences from ending abruptly.
3.2 Field Set Decisions
Field set decisions are responsible for micro con-
tent selection, i.e., which fields of a record are men-
tioned. Each field set decision chooses a subset of
fields Fi from the set of fields FIELDS(ri.t) of the
record ri that was just generated. These decisions
are made based on two types of features:
F1 captures which sets of fields are talked
about together; for example, we learn that {mean}
and {min,max} are preferred field sets for the
windSpeed record. By defining features on the en-
tire field set, we can capture any correlation structure
over the fields; in contrast, Liang et al (2009) gen-
erates a sequence of fields in which a field can only
depend on the previous one.
F2 allows the field set to be chosen based on the
values of the fields, analogously to R4.
3.3 Template Decisions
Template decisions perform surface realization. A
template is a sequence of elements, where each ele-
ment is either a word (e.g., around) or a field (e.g.,
[min]). Given the record ri and field set Fi that we
are generating from, the goal is to choose a template
Ti (Section 4.3.2 describes how we define the set
of possible templates). The features that govern the
choice of Ti are as follows:
W1 captures a priori preferences for generation
templates given field sets. There are two ways
to control this preference, BASE and COARSE.
1We map a numeric field value onto one of five categories
(very-low, low, medium, high, or very-high) based
on its value with respect to the mean and standard deviation of
values of that field in the training data.
BASE(Ti) denotes the template Ti itself, thus allow-
ing us to remember exactly which templates were
useful. To guard against overfitting, we also use
COARSE(Ti), which maps Ti to a coarsened version
of Ti, in which more words are replaced with their
associated fields (see Figure 5 for an example).
W2 captures a dependence on the values of fields
in the field set, and is analogous to R4 and F2. Fi-
nally, W3 contributes a language model probability,
to ensure smooth transitions between templates.
After Ti has been chosen, each field in the tem-
plate is replaced with a word given the correspond-
ing field value in the world state. In particular, a
word is chosen from the parameters learned in the
model of Liang et al (2009). In the example in Fig-
ure 3, the [min] field in T2 has value 44, which is
rendered to the word 45 (rounding and other noisy
deviations are common in the WEATHERGOV do-
main).
4 Learning a Probabilistic Model
Having described all the features, we now present a
conditional probabilistic model over texts w given
world states s (Section 4.1). Section 4.2 describes
how to use the model for generation, and Section 4.3
describes how to learn the model.
4.1 Model
Recall from Section 3 that the generation process
generates r1, F1, T1, r2, F2, T2, . . . , STOP. To unify
notation, denote this sequence of decisions as d =
(d1, . . . , d|d|).
Our probability model is defined as follows:
p(d | s; ?) =
|d|?
j=1
p(dj | d<j ; ?), (1)
where d<j = (d1, . . . , dj?1) is the history of de-
cisions and ? are the model parameters (feature
weights). Note that the text w (the output) is a de-
terministic function of the decisions d. We use the
features described in Section 3 to define a log-linear
model for each decision:
p(dj | d<j , s; ?) =
exp{?j(dj ,d<j , s)>?}
?
d?j?Dj
exp{?j(d?j ,d<j , s)
>?}
, (2)
where ? are all the parameters (feature weights), ?j
is the feature vector for the j-th decision, and Dj is
506
the domain of the j-th decision (either records, field
sets, or templates).
This chaining of log-linear models was used in
Ratnaparkhi (1998) for tagging and parsing, and in
Ratnaparkhi (2002) for surface realization. The abil-
ity to condition on arbitrary histories is a defining
property of these models.
4.2 Using the Model for Generation
Suppose we have learned a model with parameters ?
(how to obtain ? is discussed in Section 4.3). Given
a world state s, we would like to use our model to
generate an output text w via a decision sequence d.
In our experiments, we choose d by sequentially
choosing the best decision in a greedy fashion (until
the STOP record is generated):
dj = argmax
d?j
p(d?j | d<j , s; ?). (3)
Alternatively, instead of choosing the best decision
at each point, we can sample from the distribution:
dj ? p(dj | d<j , s; ?), which provides more diverse
generated texts at the expense of a slight degradation
in quality.
Both greedy search and sampling are very effi-
cient. Another option is to try to find the Viterbi
decision sequence, i.e., the one with the maximum
joint probability: d = argmaxd? p(d
? | s; ?). How-
ever, this computation is intractable due to features
depending arbitrarily on past decisions, making dy-
namic programming infeasible. We tried using beam
search to approximate this optimization, but we ac-
tually found that beam search performed worse than
greedy. Belz (2008) also found that greedy was more
effective than Viterbi for their model.
4.3 Learning
Now we turn our attention to learning the parame-
ters ? of our model. We are given a set of N sce-
narios {(s(i),w(i))}Ni=1 as training data. Note that
our model is defined over the decision sequence d
which contains information not present in w. In Sec-
tions 4.3.1 and 4.3.2, we show how we fill in this
missing information to obtain d(i) for each training
scenario i.
Assuming this missing information is filled, we
end up with a standard supervised learning problem,
which can be solved by maximize the (conditional)
likelihood of the training data:
max
??Rd
?
?
N?
i=1
|d(i)|?
j=1
log p(d(i)j | d
(i)
<j ; ?)
?
???||?||2, (4)
where ? > 0 is a regularization parameter. The ob-
jective function in (4) is optimized using the stan-
dard L-BFGS algorithm (Liu and Nocedal, 1989).
4.3.1 Latent Alignments
As mentioned previously, our training data in-
cludes only the world state s and generated text w,
not the full sequence of decisions d needed for train-
ing. Intuitively, we know what was generated but not
why it was generated.
We use the model of Liang et al (2009) to im-
pute the decisions d. They introduce a generative
model p(a,w|s), where the latent alignment a spec-
ifies (1) the sequence of records that were chosen,
(2) the sequence of fields that were chosen, and (3)
which words in the text were spanned by the chosen
records and fields. The model is learned in an unsu-
pervised manner using EM to produce a observing
only w and s.
An example of an alignment is given in the left
part of Figure 5. This information specifies the
record decisions and a set of fields for each record.
Because the induced alignments can be noisy, we
need to process them to obtain cleaner template de-
cisions. This is the subject of the next section.
4.3.2 Template Extraction
Given an aligned training scenario (Figure 5), we
would like to extract two types of templates.
For each record, an aligned training scenario
specifies a sequence of fields and the text that
is spanned by each field. We create a template
by abstracting fields?that is, replacing the words
spanned by a field by the field itself. We call the
resulting template COARSE. The problem with us-
ing this template directly is that fields can be noisy
due to errors from the unsupervised model.
Therefore, we also create a BASE template which
only abstracts a subset of the fields. In particular,
we define a trigger pattern which specifies a simple
condition under which a field should be abstracted.
For WEATHERGOV, we only abstract fields that
507
Records:Fields:Text:
skyCover1mode=50-75mostly cloudy ,
temperature1xwith a time=17-30low around min=4445 mean=49.
Aligned training scenario
?
skyCover temperatureCoarse ?[mode]? ?with a [time] [min] [mean]?Base ?most cloudy ,? ?with a low around [min] .?
Templates extracted
Figure 5: An example of template extraction from an imperfectly aligned training scenario. Note that these alignments are noisy
(e.g., [mean] aligns to a period). Therefore, for each record (skyCover and temperature in this case), we extract two templates:
(1) a COARSE template, which takes the text spanned by the record and abstracts away all fields in the scenario ([mode], [time],
[min], and [mean] in the example); and (2) a BASE template, which only abstracts away fields whose spanned text matches a simple
pattern (e.g., numbers in WEATHERGOV, corresponding to [min] in the example).
span numbers; for SUMTIME, fields that span num-
bers and wind directions; and for ROBOCUP, fields
that span words starting with purple or pink.
For each record ri, we define Ti so that BASE(Ti)
and COARSE(Ti) are the corresponding two ex-
tracted templates. We restrict Fi to the set of ab-
stracted fields in the COARSE template
5 Experiments
We now present an empirical evaluation of our sys-
tem on our three domains?ROBOCUP, SUMTIME,
and WEATHERGOV.
5.1 Evaluation Metrics
Automatic Evaluation To evaluate surface real-
ization (or, combined content selection and surface
realization), we measured the BLEU score (Papineni
et al, 2002) (the precision of 4-grams with a brevity
penalty) of the system-generated output with respect
to the human-generated output.
To evaluate macro content selection, we measured
the F1 score (the harmonic mean of precision and
recall) of the set of records chosen with respect to
the human-annotated set of records.
Human Evaluation We conducted a human eval-
uation using Amazon Mechanical Turk. For each
domain, we chose 100 scenarios randomly from the
test set. We ran each system under consideration on
each of these scenarios, and presented each resulting
output to 10 evaluators.2 Evaluators were given in-
structions to rank an output on the basis of English
fluency and semantic correctness on the following
scale:
2To minimize bias, we evaluated all the systems at once,
randomly shuffling the outputs of the systems. The evaluators
were not necessarily the same 10 evaluators.
Score English Fluency Semantic Correctness
5 Flawless Perfect
4 Good Near Perfect
3 Non-native Minor Errors
2 Disfluent Major Errors
1 Gibberish Completely Wrong
Evaluators were also given additional domain-
specific information: (1) the background of the
domain (e.g., that SUMTIME reports are techni-
cal weather reports); (2) general properties of the
desired output (e.g., that SUMTIME texts should
mention every record whereas WEATHERGOV texts
need not); and (3) peculiarities of the text (e.g., the
suffix ly in SUMTIME should exist as a separate to-
ken from its stem, or that pink goalie and pink1 have
the same meaning in ROBOCUP).
5.2 Systems
We evaluated the following systems on our three do-
mains:
? HUMAN is the human-generated output.
? OURSYSTEM uses all the features in Figure 4
and is trained according to Section 4.3.
? BASELINE is OURSYSTEM using a subset of
the features (those marked with ? in Fig-
ure 4). In contrast to OURSYSTEM, the in-
cluded features only depend on a local con-
text of decisions in a manner similar to
the generative model of Liang et al (2009)
and the pCRU-greedy system of Belz (2008).
BASELINE also excludes features that depend
on values of the world state.
? The existing state-of-the-art domain-specific
system for each domain.
5.3 ROBOCUP Results
Following the evaluation methodology of Chen and
Mooney (2008), we trained our system on three
508
System F1 BLEU* EnglishFluency
Semantic
Correctness
BASELINE 78.7 24.8 4.28 ? 0.78 4.15 ? 1.14
OURSYSTEM 79.9 28.8 4.34 ? 0.69 4.17 ? 1.21
WASPER-GEN 72.0 28.7 4.43 ? 0.76 4.27 ? 1.15
HUMAN ? ? 4.43 ? 0.69 4.30 ? 1.07
Table 1: ROBOCUP results. WASPER-GEN is described in
Chen and Mooney (2008). The BLEU is reported on systems
that use fixed human-annotated records (in other words, we
evaluate surface realization given perfect content selection).
Human Records:Fields:Text:
pass1arg1=purple10purple10 xpasses back to arg2=purple9purple9
Baseline Records:Fields:Text:
pass1arg1=purple10purple10 xkicks to arg2=purple9purple9
OurSystem Records:Fields:Text:
pass1arg1=purple10purple10 xpasses to arg2=purple9purple9WASPER-GENRecords:Text purple10 passes to purple9
Figure 6: Outputs of systems on an example ROBOCUP sce-
nario. There are some minor differences between the outputs.
Recall that OURSYSTEM differs from BASELINE mostly in
the addition of feature W2, which captures dependencies be-
tween field values (e.g., purple10) and the template chosen
(e.g., [arg1] passes to [arg2]). This allows us to capture value-
dependent preferences for different realizations (e.g., passes to
over kicks to). Also, HUMAN uses passes back to, but this word
choice requires knowledge of passing records in previous sce-
narios, which none of the systems have access to. It would nat-
ural, however, to add features that would capture these longer-
range dependencies in our framework.
Robocup games and tested on the fourth, averaging
over the four train/test splits. We report the average
test accuracy weighted by the number of scenarios
in a game. First, we evaluated macro content selec-
tion. Table 1 shows that OURSYSTEM significantly
outperforms BASELINE and WASPER-GEN on F1.
To compare with Chen and Mooney (2008) on
surface realization, we fixed each system?s record
decisions to the ones given by the annotated data
and enforced that all the fields of that record are
chosen. Table 1 shows that OURSYSTEM sig-
nificantly outperforms BASELINE and is compara-
ble to WASPER-GEN on BLEU. On human eval-
uation, OURSYSTEM outperforms BASELINE, but
WASPER-GEN outperforms OURSYSTEM. See
Figure 6 for example outputs from the various sys-
tems.
BLEU EnglishFluency
Semantic
Correctness
BASELINE 32.9 4.23 ? 0.71 4.26 ? 0.85
OURSYSTEM 55.1 4.25 ? 0.69 4.27 ? 0.82
OURSYSTEM-CUSTOM 62.3 4.12 ? 0.78 4.33 ? 0.91
pCRU-greedy 63.6 4.18 ? 0.71 4.49 ? 0.73
SUMTIME-Hybrid 52.7 ? ?
HUMAN ? 4.09 ? 0.83 4.37 ? 0.87
Table 2: SUMTIME results. The SUMTIME-Hybrid system
is described in (Reiter et al, 2005); pCRU-greedy, in (Belz,
2008).
5.4 SUMTIME Results
The SUMTIME task only requires micro content se-
lection and surface realization because the sequence
of records to be generated is fixed; only these as-
pects are evaluated. Following the methodology of
Belz (2008), we used five-fold cross validation.
We found that using the unsupervised model of
Liang et al (2009) to automatically produce aligned
training scenarios (Section 4.3.1) was less effec-
tive than it was in the other two domains due to
two factors: (i) there are fewer training examples
in SUMTIME and unsupervised learning typically
works better with a large amount of data; and (ii)
the alignment model does not exploit the temporal
structure in the SUMTIME world state. Therefore,
we used a small set of simple regular expressions to
produce aligned training scenarios.
Table 2 shows that OURSYSTEM signif-
icantly outperforms BASELINE as well as
SUMTIME-Hybrid, a hand-crafted system, on
BLEU. Note that OURSYSTEM is domain-
independent and has not been specifically tuned
to SUMTIME. However, OURSYSTEM is outper-
formed by the state-of-the-art statistical system
pCRU-greedy.
Custom Features One of the advantages of our
feature-based approach is that it is straightforward to
incorporate domain-specific features to capture spe-
cific properties of a domain. To this end, we define
the following set of feature templates in place of our
generic feature templates from Figure 4:
? F1?: Value of time
? F2?: Existence of gusts/wind direction/wind
speeds
? W1?: Change in wind direction (clockwise,
counterclockwise, or none)
509
Human Records:Fields:Text:
windDir1dir=nnenne min=1818 x- max=2222 gust-min=30gusts 30
windDir2xgradually decreasing min=1010 x- max=1414 time=12amby late evening
Baseline Records:Fields:Text:
windDir1dir=nnenne min=1818 x- max=2222 gust-min=30gusts 30
windDir2xincreasing min=1010 x- max=1414
OurSystem-Custom Records:Fields:Text:
windDir1dir=nnenne min=1818 x- max=2222 gust-min=30gusts 30
windDir2xgradually decreasing min=1010 x- max=1414 time=12amby late eveningpCRU-greedy Records:Text nne 18 - 22 gusts 30 easing 10 - 14 by late evening
Figure 7: Outputs of systems on an example SUMTIME scenario. Two notable differences between OURSYSTEM-CUSTOM and
BASELINE arise due to OURSYSTEM-CUSTOM?s value-dependent features. For example, OURSYSTEM-CUSTOM can choose
whether to include the time field (windDir2) or not (windDir1), depending on the value of the time (F1?), thereby improving content
selection. OURSYSTEM-CUSTOM also improves surface realization, choosing gradually decreasing over BASELINE?s increasing.
Interestingly, this improvement comes from the joint effort of two features: W2? prefers decreasing over increasing in this case,
and W5? adds the modifier gradually. An important strength of log-linear models is the ability to combine soft preferences from
many features.
? W2?: Change in wind speed
? W3?: Change in wind direction and speed
? W4?: Existence of gust min and/or max
? W5?: Time elapsed since last record
? W6?: Whether wind is a cardinal direction (N,
E, S, W)
The resulting system, which we call
OURSYSTEM-CUSTOM, obtains a BLEU score
which is comparable to pCRU-greedy.
An important aspect of our system that it is flexi-
ble and quick to deploy. According to Belz (2008),
SUMTIME-Hybrid took twelve person-months to
build, while pCRU-greedy took one month. Having
developed OURSYSTEM in a domain-independent
way, we only needed to do simple reformatting upon
receiving the SUMTIME data. Furthermore, it took
only a few days to develop the custom features
above to create OURSYSTEM-CUSTOM, which has
BLEU performance comparable to the state-of-the-
art pCRU-greedy system.
We also conducted human evaluations on the four
systems shown in Table 2. Note that this evalua-
tion is rather difficult for Mechanical Turkers since
SUMTIME texts are rather technical compared to
those in other domains. Interestingly, all systems
outperform HUMAN on English fluency; this result
corroborates the findings of Belz (2008). On se-
mantic correctness, all systems perform comparably
to HUMAN, except pCRU-greedy, which performs
slightly better. See Figure 7 for a comparison of the
outputs generated by the various systems.
F1 BLEU* EnglishFluency
Semantic
Correctness
BASELINE 22.1 22.2 4.07 ? 0.59 3.41 ? 1.16
OURSYSTEM 65.4 51.5 4.12 ? 0.74 4.22 ? 0.89
HUMAN ? ? 4.14 ? 0.71 3.85 ? 0.99
Table 3: WEATHERGOV results. The BLEU score is on joint
content selection and surface realization and is modified to not
penalize numeric deviations of at most 5.
5.5 WEATHERGOV Results
We evaluate the WEATHERGOV corpus on the joint
task of content selection and surface realization.
We split our corpus into 25,000 scenarios for train-
ing, 1,000 for development, and 3,528 for testing.
In WEATHERGOV, numeric field values are often
rounded or noisily perturbed, so it is difficult to gen-
erate precisely matching numbers. Therefore, we
used a modified BLEU score where numbers dif-
fering by at most five are treated as equal. Fur-
thermore, WEATHERGOV is evaluated on the joint
content selection and surface realization task, un-
like ROBOCUP, where content selection and surface
realization were treated separately, and SUMTIME,
where content selection was not applicable.
Table 3 shows the results. We see that
OURSYSTEM substantially outperforms BASELINE,
especially on BLEU score and semantic correctness.
This difference shows that taking non-local context
into account is very important in this domain. This
result is not surprising, since WEATHERGOV is the
most complicated of the three domains, and this
complexity is exactly where non-locality is neces-
510
Human Records:Fields:Text:
skyCover1cover=50-75mostly cloudy x,
temperature1xwith a time=5pm-6amlow xaround min=5957 x.
windDir1mode=ssesouth xwind between
windSpeed1min=75 xand max=1510 xmph .
Baseline Records:Fields:Text:
rainChance2xa chance of showers ,
nonex,
gust1xwith gusts as high as max=2120 xmph .
precipPotential1xchance of precipitation is max=1010 x% .
OurSystem Records:Fields:Text:
skyCover1xmostly cloudy ,
temperature1xwith a low around min=5959 x.
windDir1xsouth wind between
windSpeed1min=77 xand max=1515 xmph .
Figure 8: Outputs of systems on an example WEATHERGOV scenario. Most of the gains of OURSYSTEM over BASELINE come
from improved content selection. For example, BASELINE chooses rainChance because it happens to be the most common first
record type in the training data. However, since OURSYSTEM has features that depend on the value of rainChance (noChance
in this case), it has learned to disprefer talking about rain when there is no rain. Also, OURSYSTEM has additional features on the
entire history of chosen records, which enables it to choose a better sequence of records.
sary. Interestingly, OURSYSTEM even outperforms
HUMAN on semantic correctness, perhaps due to
generating more straightforward renderings of the
world state. Figure 8 describes example outputs for
each system.
6 Related Work
There has been a fair amount of work both on con-
tent selection and surface realization. In content se-
lection, Barzilay and Lee (2004) use an approach
based on local classification with edge-wise scores
between local decisions. Our model, on the other
hand, can capture higher-order constraints to enforce
global coherence.
Liang et al (2009) introduces a generative model
of the text given the world state, and in some ways is
similar in spirit to our model. Although that model
is capable of generation in principle, it was de-
signed for unsupervised induction of hidden align-
ments (which is exactly what we use it for). Even
if combined with a language model, generated text
was much worse than our baseline.
The prominent approach for surface realization
is rendering the text from a grammar. Wong and
Mooney (2007) and Chen and Mooney (2008) use
synchronous grammars that map a logical form, rep-
resented as a tree, into a parse of the text. Soricut
and Marcu (2006) uses tree structures called WIDL-
expressions (the acronym corresponds to four opera-
tions akin to the rewrite rules of a grammar) to repre-
sent the realization process, and, like our approach,
operates in a log-linear framework. Belz (2008) and
Belz and Kow (2009) also perform surface realiza-
tion from a PCFG-like grammar. Lu et al (2009)
uses a conditional random field model over trees.
Other authors have performed surface realization us-
ing various grammar formalisms, for instance CCG
(White et al, 2007), HPSG (Nakanishi et al, 2005),
and LFG (Cahill and van Genabith, 2006).
In each of the above cases, the decomposable
structure of the tree/grammar enables tractability.
However, we saw that it was important to include
features that captured long-range dependencies. Our
model is also similar in spirit to Ratnaparkhi (2002)
in the use of non-local features, but we operate at
three levels of hierarchy to include both content se-
lection and surface realization.
One issue that arises with long-range dependen-
cies is the lack of efficient algorithms for finding the
optimal text. Koller and Striegnitz (2002) perform
surface realization of a flat semantics, which is NP-
hard, so they recast the problem as non-projective
dependency parsing. Ratnaparkhi (2002) uses beam
search to find an approximate solution. We found
that a greedy approach obtained better results than
beam search; Belz (2008) found greedy approaches
to be effective as well.
7 Conclusion
We have developed a simple yet powerful generation
system that combines both content selection and sur-
face realization in a domain independent way. De-
spite our approach being domain-independent, we
were able to obtain performance comparable to the
state-of-the-art across three domains. Additionally,
the feature-based design of our approach makes it
easy to incorporate domain-specific knowledge to
increase performance even further.
511
References
R. Barzilay and L. Lee. 2004. Catching the drift: Prob-
abilistic content models, with applications to genera-
tion and summarization. In Human Language Tech-
nology and North American Association for Computa-
tional Linguistics (HLT/NAACL).
A. Belz and E. Kow. 2009. System building cost vs.
output quality in data-to-text generation. In European
Workshop on Natural Language Generation, pages
16?24.
A. Belz. 2008. Automatic generation of weather forecast
texts using comprehensive probabilistic generation-
space models. Natural Language Engineering,
14(4):1?26.
Aoife Cahill and Josef van Genabith. 2006. Robust pcfg-
based generation using automatically acquired LFG
approximations. In Association for Computational
Linguistics (ACL), pages 1033?1040, Morristown, NJ,
USA. Association for Computational Linguistics.
D. L. Chen and R. J. Mooney. 2008. Learning to
sportscast: A test of grounded language acquisition.
In International Conference on Machine Learning
(ICML), pages 128?135.
R. Dale, S. Geldof, and J. Prost. 2003. Coral: using natu-
ral language generation for navigational assistance. In
Australasian computer science conference, pages 35?
44.
M. E. Foster and M. White. 2004. Techniques for text
planning with XSLT. In Workshop on NLP and XML:
RDF/RDFS and OWL in Language Technology, pages
1?8.
N. Green. 2006. Generation of biomedical arguments for
lay readers. In International Natural Language Gen-
eration Conference, pages 114?121.
A. Koller and K. Striegnitz. 2002. Generation as de-
pendency parsing. In Association for Computational
Linguistics (ACL), pages 17?24.
P. Liang, M. I. Jordan, and D. Klein. 2009. Learning
semantic correspondences with less supervision. In
Association for Computational Linguistics and Inter-
national Joint Conference on Natural Language Pro-
cessing (ACL-IJCNLP).
D. C. Liu and J. Nocedal. 1989. On the limited mem-
ory method for large scale optimization. Mathemati-
cal Programming B, 45(3):503?528.
W. Lu, H. T. Ng, and W. S. Lee. 2009. Natural lan-
guage generation with tree conditional random fields.
In Empirical Methods in Natural Language Process-
ing (EMNLP), pages 400?409.
Hiroko Nakanishi, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic models for disambiguation of an
HPSG-based chart generator. In Parsing ?05: Pro-
ceedings of the Ninth International Workshop on Pars-
ing Technology, pages 93?102, Morristown, NJ, USA.
Association for Computational Linguistics.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
BLEU: A method for automatic evaluation of machine
translation. In Association for Computational Linguis-
tics (ACL).
A. Ratnaparkhi. 1998. Maximum entropy models for nat-
ural language ambiguity resolution. Ph.D. thesis, Uni-
versity of Pennsylvania.
A. Ratnaparkhi. 2002. Trainable approaches to surface
natural language generation and their application to
conversational dialog systems. Computer, Speech &
Language, 16:435?455.
E. Reiter, S. Sripada, J. Hunter, J. Yu, and I. Davy. 2005.
Choosing words in computer-generated weather fore-
casts. Artificial Intelligence, 167:137?169.
R. Soricut and D. Marcu. 2006. Stochastic language
generation using WIDL-expressions and its applica-
tion in machine translation and summarization. In As-
sociation for Computational Linguistics (ACL), pages
1105?1112.
R. Turner, Y. Sripada, and E. Reiter. 2009. Gener-
ating approximate geographic descriptions. In Eu-
ropean Workshop on Natural Language Generation,
pages 42?49.
Michael White, Rajakrishnan Rajkumar, and Scott Mar-
tin. 2007. Towards broad coverage surface realization
with CCG. In In Proceedings of the Workshop on Us-
ing Corpora for NLG: Language Generation and Ma-
chine Translation (UCNLG+MT).
Y. W. Wong and R. J. Mooney. 2007. Learning syn-
chronous grammars for semantic parsing with lambda
calculus. In Association for Computational Linguis-
tics (ACL), pages 960?967.
512
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1170?1179,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Feature Noising for Log-linear Structured Prediction
Sida I. Wang?, Mengqiu Wang?, Stefan Wager?,
Percy Liang, Christopher D. Manning
Department of Computer Science, ?Department of Statistics
Stanford University, Stanford, CA 94305, USA
{sidaw, mengqiu, pliang, manning}@cs.stanford.edu
swager@stanford.edu
Abstract
NLP models have many and sparse features,
and regularization is key for balancing model
overfitting versus underfitting. A recently re-
popularized form of regularization is to gen-
erate fake training data by repeatedly adding
noise to real data. We reinterpret this noising
as an explicit regularizer, and approximate it
with a second-order formula that can be used
during training without actually generating
fake data. We show how to apply this method
to structured prediction using multinomial lo-
gistic regression and linear-chain CRFs. We
tackle the key challenge of developing a dy-
namic program to compute the gradient of the
regularizer efficiently. The regularizer is a
sum over inputs, so we can estimate it more
accurately via a semi-supervised or transduc-
tive extension. Applied to text classification
and NER, our method provides a >1% abso-
lute performance gain over use of standard L2
regularization.
1 Introduction
NLP models often have millions of mainly sparsely
attested features. As a result, balancing overfitting
versus underfitting through good weight regulariza-
tion remains a key issue for achieving optimal per-
formance. Traditionally, L2 or L1 regularization is
employed, but these simple types of regularization
penalize all features in a uniform way without tak-
ing into account the properties of the actual model.
An alternative approach to regularization is to
generate fake training data by adding random noise
to the input features of the original training data. In-
tuitively, this can be thought of as simulating miss-
?Both authors contributed equally to the paper
ing features, whether due to typos or use of a pre-
viously unseen synonym. The effectiveness of this
technique is well-known in machine learning (Abu-
Mostafa, 1990; Burges and Scho?lkopf, 1997; Simard
et al, 2000; Rifai et al, 2011a; van der Maaten
et al, 2013), but working directly with many cor-
rupted copies of a dataset can be computationally
prohibitive. Fortunately, feature noising ideas often
lead to tractable deterministic objectives that can be
optimized directly. Sometimes, training with cor-
rupted features reduces to a special form of reg-
ularization (Matsuoka, 1992; Bishop, 1995; Rifai
et al, 2011b; Wager et al, 2013). For example,
Bishop (1995) showed that training with features
that have been corrupted with additive Gaussian
noise is equivalent to a form of L2 regularization in
the low noise limit. In other cases it is possible to
develop a new objective function by marginalizing
over the artificial noise (Wang and Manning, 2013;
van der Maaten et al, 2013).
The central contribution of this paper is to show
how to efficiently simulate training with artificially
noised features in the context of log-linear struc-
tured prediction, without actually having to gener-
ate noised data. We focus on dropout noise (Hinton
et al, 2012), a recently popularized form of artifi-
cial feature noise where a random subset of features
is omitted independently for each training example.
Dropout and its variants have been shown to out-
perform L2 regularization on various tasks (Hinton
et al, 2012; Wang and Manning, 2013; Wan et al,
2013). Dropout is is similar in spirit to feature bag-
ging in the deliberate removal of features, but per-
forms the removal in a preset way rather than ran-
domly (Bryll et al, 2003; Sutton et al, 2005; Smith
et al, 2005).
1170
Our approach is based on a second-order approx-
imation to feature noising developed among others
by Bishop (1995) and Wager et al (2013), which al-
lows us to convert dropout noise into a form of adap-
tive regularization. This method is suitable for struc-
tured prediction in log-linear models where second
derivatives are computable. In particular, it can be
used for multiclass classification with maximum en-
tropy models (a.k.a., softmax or multinomial logis-
tic regression) and for the sequence models that are
ubiquitous in NLP, via linear chain Conditional Ran-
dom Fields (CRFs).
For linear chain CRFs, we additionally show how
we can use a noising scheme that takes advantage
of the clique structure so that the resulting noising
regularizer can be computed in terms of the pair-
wise marginals. A simple forward-backward-type
dynamic program can then be used to compute the
gradient tractably. For ease of implementation and
scalability to semi-supervised learning, we also out-
line an even faster approximation to the regularizer.
The general approach also works in other clique
structures in addition to the linear chain when the
clique marginals can be computed efficiently.
Finally, we extend feature noising for structured
prediction to a transductive or semi-supervised set-
ting. The regularizer induced by feature noising
is label-independent for log-linear models, and so
we can use unlabeled data to learn a better regu-
larizer. NLP sequence labeling tasks are especially
well suited to a semi-supervised approach, as input
features are numerous but sparse, and labeled data
is expensive to obtain but unlabeled data is abundant
(Li and McCallum, 2005; Jiao et al, 2006).
Wager et al (2013) showed that semi-supervised
dropout training for logistic regression captures a
similar intuition to techniques such as entropy regu-
larization (Grandvalet and Bengio, 2005) and trans-
ductive SVMs (Joachims, 1999), which encourage
confident predictions on the unlabeled data. Semi-
supervised dropout has the advantage of only us-
ing the predicted label probabilities on the unlabeled
data to modulate an L2 regularizer, rather than re-
quiring more heavy-handed modeling of the unla-
beled data as in entropy regularization or expecta-
tion regularization (Mann and McCallum, 2007).
In experimental results, we show that simulated
feature noising gives more than a 1% absolute boost
yt yt+1yt?1f (yt, xt )f (yt?1, yt ) f (yt, yt+1)yt yt+1yt?1f (yt, xt )f (yt?1, yt ) f (yt, yt+1)
Figure 1: An illustration of dropout feature noising
in linear-chain CRFs with only transition features
and node features. The green squares are node fea-
tures f(yt, xt), and the orange squares are edge fea-
tures f(yt?1, yt). Conceptually, given a training ex-
ample, we sample some features to ignore (generate
fake data) and make a parameter update. Our goal is
to train with a roughly equivalent objective, without
actually sampling.
in performance over L2 regularization, on both text
classification and an NER sequence labeling task.
2 Feature Noising Log-linear Models
Consider the standard structured prediction problem
of mapping some input x ? X (e.g., a sentence)
to an output y ? Y (e.g., a tag sequence). Let
f(y, x) ? Rd be the feature vector, ? ? Rd be the
weight vector, and s = (s1, . . . , s|Y|) be a vector of
scores for each output, with sy = f(y, x) ? ?. Now
define a log-linear model:
p(y | x; ?) = exp{sy ?A(s)}, (1)
where A(s) = log
?
y exp{sy} is the log-partition
function. Given an example (x,y), parameter esti-
mation corresponds to choosing ? to maximize p(y |
x; ?).
The key idea behind feature noising is to artifi-
cially corrupt the feature vector f(y, x) randomly
1171
into some f?(y, x) and then maximize the average
log-likelihood of y given these corrupted features?
the motivation is to choose predictors ? that are ro-
bust to noise (missing words for example). Let s?,
p?(y | x; ?) be the randomly perturbed versions cor-
responding to f?(y, x). We will also assume the
feature noising preserves the mean: E[f?(y, x)] =
f(y, x), so that E[s?] = s. This can always be done
by scaling the noised features as described in the list
of noising schemes.
It is useful to view feature noising as a form of
regularization. Since feature noising preserves the
mean, the feature noising objective can be written as
the sum of the original log-likelihood plus the dif-
ference in log-normalization constants:
E[log p?(y | x; ?)] = E[s?y ?A(s?)] (2)
= log p(y | x; ?)?R(?, x), (3)
R(?, x)
def
= E[A(s?)]?A(s). (4)
Since A(?) is convex, R(?, x) is always positive by
Jensen?s inequality and can therefore be interpreted
as a regularizer. Note that R(?, x) is in general non-
convex.
Computing the regularizer (4) requires summing
over all possible noised feature vectors, which can
imply exponential effort in the number of features.
This is intractable even for flat classification. Fol-
lowing Bishop (1995) and Wager et al (2013), we
approximate R(?, x) using a second-order Taylor
expansion, which will allow us to work with only
means and covariances of the noised features. We
take a quadratic approximation of the log-partition
function A(?) of the noised score vector s? around
the the unnoised score vector s:
A(s?) u A(s) +?A(s)>(s?? s) (5)
+
1
2
(s?? s)>?2A(s)(s?? s).
Plugging (5) into (4), we obtain a new regularizer
Rq(?, x), which we will use as an approximation to
R(?, x):
Rq(?, x) =
1
2
E[(s?? s)>?2A(s)(s?? s)] (6)
=
1
2
tr(?2A(s) Cov(s?)). (7)
This expression still has two sources of potential in-
tractability, a sum over an exponential number of
noised score vectors s? and a sum over the |Y| com-
ponents of s?.
Multiclass classification If we assume that the
components of s? are independent, then Cov(s?) ?
R|Y|?|Y| is diagonal, and we have
Rq(?, x) =
1
2
?
y?Y
?y(1? ?y)Var[s?y], (8)
where the mean ?y
def
= p?(y | x) is the model prob-
ability, the variance ?y(1??y) measures model un-
certainty, and
Var[s?y] = ?
>Cov[f?(y, x)]? (9)
measures the uncertainty caused by feature noising.1
The regularizerRq(?, x) involves the product of two
variance terms, the first is non-convex in ? and the
second is quadratic in ?. Note that to reduce the reg-
ularization, we will favor models that (i) predict con-
fidently and (ii) have stable scores in the presence of
feature noise.
For multiclass classification, we can explicitly
sum over each y ? Y to compute the regularizer,
but this will be intractable for structured prediction.
To specialize to multiclass classification for the
moment, let us assume that we have a separate
weight vector for each output y applied to the same
feature vector g(x); that is, the score sy = ?y ? g(x).
Further, assume that the components of the noised
feature vector g?(x) are independent. Then we can
simplify (9) to the following:
Var[s?y] =
?
j
Var[gj(x)]?
2
yj . (10)
Noising schemes We now give some examples of
possible noise schemes for generating f?(y, x) given
the original features f(y, x). This distribution af-
fects the regularization through the variance term
Var[s?y].
? Additive Gaussian:
f?(y, x) = f(y, x) + ?, where ? ?
N (0, ?2Id?d).
1Here, we are using the fact that first and second derivatives
of the log-partition function are the mean and variance.
1172
In this case, the contribution to the regularizer
from noising is Var[s?y] =
?
j ?
2?2yj .
? Dropout:
f?(y, x) = f(y, x)  z, where  takes the el-
ementwise product of two vectors. Here, z is
a vector with independent components which
has zi = 0 with probability ?, zi = 11?? with
probability 1 ? ?. In this case, Var[s?y] =
?
j
gj(x)2?
1?? ?
2
yj .
? Multiplicative Gaussian:
f?(y, x) = f(y, x)  (1 + ?), where
? ? N (0, ?2Id?d). Here, Var[s?y] =?
j gj(x)
2?2?2yj . Note that under our second-
order approximation Rq(?, x), the multiplica-
tive Gaussian and dropout schemes are equiva-
lent, but they differ under the original regular-
izer R(?, x).
2.1 Semi-supervised learning
A key observation (Wager et al, 2013) is that
the noising regularizer R (8), while involving a
sum over examples, is independent of the output
y. This suggests estimating R using unlabeled
data. Specifically, if we have n labeled examples
D = {x1, x2, . . . , xn} and m unlabeled examples
Dunlabeled = {u1, u2, . . . , un}, then we can define a
regularizer that is a linear combination the regular-
izer estimated on both datasets, with ? tuning the
tradeoff between the two:
R?(?,D,Dunlabeled) (11)
def
=
n
n+ ?m
( n?
i=1
R(?, xi) + ?
m?
i=1
R(?, ui)
)
.
3 Feature Noising in Linear-Chain CRFs
So far, we have developed a regularizer that works
for all log-linear models, but?in its current form?
is only practical for multiclass classification. We
now exploit the decomposable structure in CRFs to
define a new noising scheme which does not require
us to explicitly sum over all possible outputs y ? Y .
The key idea will be to noise each local feature vec-
tor (which implicitly affects many y) rather than
noise each y independently.
Assume that the output y = (y1, . . . , yT ) is a se-
quence of T tags. In linear chain CRFs, the feature
vector f decomposes into a sum of local feature vec-
tors gt:
f(y, x) =
T?
t=1
gt(yt?1, yt, x), (12)
where gt(a, b, x) is defined on a pair of consecutive
tags a, b for positions t? 1 and t.
Rather than working with a score sy for each
y ? Y , we define a collection of local scores
s = {sa,b,t}, for each tag pair (a, b) and posi-
tion t = 1, . . . , T . We consider noising schemes
which independently set g?t(a, b, x) for each a, b, t.
Let s? = {s?a,b,t} be the corresponding collection of
noised scores.
We can write the log-partition function of these
local scores as follows:
A(s) = log
?
y?Y
exp
{
T?
t=1
syt?1,yt,t
}
. (13)
The first derivative yields the edge marginals under
the model, ?a,b,t = p?(yt?1 = a, yt = b | x), and
the diagonal elements of the Hessian ?2A(s) yield
the marginal variances.
Now, following (7) and (8), we obtain the follow-
ing regularizer:
Rq(?, x) =
1
2
?
a,b,t
?a,b,t(1? ?a,b,t)Var[s?a,b,t],
(14)
where ?a,b,t(1? ?a,b,t) measures model uncertainty
about edge marginals, and Var[s?a,b,t] is simply the
uncertainty due to noising. Again, minimizing the
regularizer means making confident predictions and
having stable scores under feature noise.
Computing partial derivatives So far, we have
defined the regularizer Rq(?, x) based on feature
noising. In order to minimize Rq(?, x), we need to
take its derivative.
First, note that log?a,b,t is the difference of a re-
stricted log-partition function and the log-partition
function. So again by properties of its first deriva-
tive, we have:
? log?a,b,t = Ep?(y|x,yt?1=a,yt=b)[f(y, x)] (15)
? Ep?(y|x)[f(y, x)].
1173
Using the fact that ??a,b,t = ?a,b,t? log?a,b,t and
the fact that Var[s?a,b,t] is a quadratic function in ?,
we can simply apply the product rule to derive the
final gradient?Rq(?, x).
3.1 A Dynamic Program for the Conditional
Expectation
A naive computation of the gradient ?Rq(?, x) re-
quires a full forward-backward pass to compute
Ep?(y|yt?1=a,yt=b,x)[f(y, x)] for each tag pair (a, b)
and position t, resulting in a O(K4T 2) time algo-
rithm.
In this section, we reduce the running time to
O(K2T ) using a more intricate dynamic program.
By the Markov property of the CRF, y1:t?2 only de-
pends on (yt?1, yt) through yt?1 and yt+1:T only
depends on (yt?1, yt) through yt.
First, it will be convenient to define the partial
sum of the local feature vector from positions i to
j as follows:
Gi:j =
j?
t=i
gt(yt?1, yt, x). (16)
Consider the task of computing the feature expecta-
tion Ep?(y|yt?1=a,yt=b)[f(y, x)] for a fixed (a, b, t).
We can expand this quantity into
?
y:yt?1=a,yt=b
p?(y?(t?1:t) | yt?1 = a, yt = b)G1:T .
Conditioning on yt?1, yt decomposes the sum into
three pieces:
?
y:yt?1=a,yt=b
[gt(yt?1 = a, yt = b, x) + F
a
t +B
b
t ],
where
F at =
?
y1:t?2
p?(y1:t?2 | yt?1 = a)G1:t?1, (17)
Bbt =
?
yt+1:T
p?(yt+1:T | yt = b)Gt+1:T , (18)
are the expected feature vectors summed over the
prefix and suffix of the tag sequence, respectively.
Note that F at and B
b
t are analogous to the forward
and backward messages of standard CRF inference,
with the exception that they are vectors rather than
scalars.
We can compute these messages recursively in the
standard way. The forward recurrence is
F at =
?
b
p?(yt?2 = b | yt?1 = a)
[
gt(yt?2 = b, yt?1 = a, x) + F
b
t?1
]
,
and a similar recurrence holds for the backward mes-
sages Bbt .
Running the resulting dynamic program takes
O(K2Tq) time and requires O(KTq) storage,
where K is the number of tags, T is the sequence
length and q is the number of active features. Note
that this is the same order of dependence as normal
CRF training, but there is an additional dependence
on the number of active features q, which makes
training slower.
4 Fast Gradient Computations
In this section, we provide two ways to further im-
prove the efficiency of the gradient calculation based
on ignoring long-range interactions and based on ex-
ploiting feature sparsity.
4.1 Exploiting Feature Sparsity and
Co-occurrence
In each forward-backward pass over a training ex-
ample, we need to compute the conditional ex-
pectations for all features active in that example.
Naively applying the dynamic program in Section 3
is O(K2T ) for each active feature. The total com-
plexity has to factor in the number of active fea-
tures, q. Although q only scales linearly with sen-
tence length, in practice this number could get large
pretty quickly. For example, in the NER tagging ex-
periments (cf. Section 5), the average number of
active features per token is about 20, which means
q ' 20T ; this term quickly dominates the compu-
tational costs. Fortunately, in sequence tagging and
other NLP tasks, the majority of features are sparse
and they often co-occur. That is, some of the ac-
tive features would fire and only fire at the same lo-
cations in a given sequence. This happens when a
particular token triggers multiple rare features.
We observe that all indicator features that only
fired once at position t have the same conditional ex-
pectations (and model expectations). As a result, we
can collapse such a group of features into a single
1174
feature as a preprocessing step to avoid computing
identical expectations for each of the features. Do-
ing so on the same NER tagging experiments cuts
down q/T from 20 to less than 5, and gives us a 4
times speed up at no loss of accuracy. The exact
same trick is applicable to the general CRF gradient
computation as well and gives similar speedup.
4.2 Short-range interactions
It is also possible to speed up the method by re-
sorting to approximate gradients. In our case, the
dynamic program from Section 3 together with the
trick described above ran in a manageable amount
of time. The techniques developed here, however,
could prove to be useful on larger tasks.
Let us rewrite the quantity we want to compute
slightly differently (again, for all a, b, t):
T?
i=1
Ep?(y|x,yt?1=a,yt=b)[gi(yi?1, yi, x)]. (19)
The intuition is that conditioned on yt?1, yt, the
terms gi(yi?1, yi, x) where i is far from t will be
close to Ep?(y|x)[gi(yi?1, yi, x)].
This motivates replacing the former with the latter
whenever |i? k| ? r where r is some window size.
This approximation results in an expression which
only has to consider the sum of the local feature vec-
tors from i?r to i+r, which is captured byGi?r:i+r:
Ep?(y|yt?1=a,yt=b,x)[f(y, x)]? Ep?(y|x)[f(y, x)]
? Ep?(y|yt?1=a,yt=b,x)[Gt?r:t+r] (20)
? Ep?(y|x)[Gt?r:t+r].
We can further approximate this last expression by
letting r = 0, obtaining:
gt(a, b, x)? Ep?(y|x)[gt(yt?1, yt, x)]. (21)
The second expectation can be computed from the
edge marginals.
The accuracy of this approximation hinges on the
lack of long range dependencies. Equation (21)
shows the case of r = 0; this takes almost no addi-
tional effort to compute. However, for some of our
experiments, we observed a 20% difference with the
real derivative. For r > 0, the computational savings
are more limited, but the bounded-window method
is easier to implement.
Dataset q d K Ntrain Ntest
CoNLL 20 437906 5 204567 46666
SANCL 5 679959 12 761738 82405
20news 81 62061 20 15935 3993
RCV14 76 29992 4 9625/2 9625/2
R21578 47 18933 65 5946 2347
TDT2 130 36771 30 9394/2 9394/2
Table 1: Description of datasets. q: average number
of non-zero features per example, d: total number
of features, K: number of classes to predict, Ntrain:
number of training examples, Ntest: number of test
examples.
5 Experiments
We show experimental results on the CoNLL-2003
Named Entity Recognition (NER) task, the SANCL
Part-of-speech (POS) tagging task, and several doc-
ument classification tasks.2 The datasets used are
described in Table 1. We used standard splits when-
ever available; otherwise we split the data at ran-
dom into a test set and a train set of equal sizes
(RCV14, TDT2). CoNLL has a development set
of size 51578, which we used to tune regulariza-
tion parameters. The SANCL test set is divided into
3 genres, namely answers, newsgroups, and
reviews, each of which has a corresponding de-
velopment set.3
5.1 Multiclass Classification
We begin by testing our regularizer in the simple
case of classification where Y = {1, 2, . . . ,K} for
K classes. We examine the performance of the nois-
ing regularizer in both the fully supervised setting as
well as the transductive learning setting.
In the transductive learning setting, the learner
is allowed to inspect the test features at train time
(without the labels). We used the method described
in Section 2.1 for transductive dropout.
2The document classification data are available
at http://www.csie.ntu.edu.tw/?cjlin/
libsvmtools/datasets and http://www.cad.
zju.edu.cn/home/dengcai/Data/TextData.html
3The SANCL dataset has two additional genres?emails and
weblogs?that we did not use, as we did not have access to
development sets for these genres.
1175
Dataset K None L2 Drop +Test
CoNLL 5 78.03 80.12 80.90 81.66
20news 20 81.44 82.19 83.37 84.71
RCV14 4 95.76 95.90 96.03 96.11
R21578 65 92.24 92.24 92.24 92.58
TDT2 30 97.74 97.91 98.00 98.12
Table 2: Classification performance and transduc-
tive learning results on some standard datasets.
None: use no regularization, Drop: quadratic ap-
proximation to the dropout noise (8), +Test: also use
the test set to estimate the noising regularizer (11).
5.1.1 Semi-supervised Learning with Feature
Noising
In the transductive setting, we used test data
(without labels) to learn a better regularizer. As an
alternative, we could also use unlabeled data in place
of the test data to accomplish a similar goal; this
leads to a semi-supervised setting.
To test the semi-supervised idea, we use the same
datasets as above. We split each dataset evenly into
3 thirds that we use as a training set, a test set and an
unlabeled dataset. Results are given in Table 3.
In most cases, our semi-supervised accuracies are
lower than the transductive accuracies given in Table
2; this is normal in our setup, because we used less
labeled data to train the semi-supervised classifier
than the transductive one.4
5.1.2 The Second-Order Approximation
The results reported above all rely on the ap-
proximate dropout regularizer (8) that is based on a
second-order Taylor expansion. To test the validity
of this approximation we compare it to the Gaussian
method developed by Wang and Manning (2013) on
a two-class classification task.
We use the 20-newsgroups alt.atheism vs
soc.religion.christian classification task;
results are shown in Figure 2. There are 1427 exam-
4The CoNNL results look somewhat surprising, as the semi-
supervised results are better than the transductive ones. The
reason for this is that the original CoNLL test set came from a
different distributions than the training set, and this made the
task more difficult. Meanwhile, in our semi-supervised experi-
ment, the test and train sets are drawn from the same distribu-
tion and so our semi-supervised task is actually easier than the
original one.
Dataset K L2 Drop +Unlabeled
CoNLL 5 91.46 91.81 92.02
20news 20 76.55 79.07 80.47
RCV14 4 94.76 94.79 95.16
R21578 65 90.67 91.24 90.30
TDT2 30 97.34 97.54 97.89
Table 3: Semisupervised learning results on some
standard datasets. A third (33%) of the full dataset
was used for training, a third for testing, and the rest
as unlabeled.
10?6 10?4 10?2 100 102
0.78
0.8
0.82
0.84
0.86
0.88
0.9
L2 regularization strength (?)
Ac
cu
rac
y
 
 
L2 only
L2+Gaussian dropout
L2+Quadratic dropout
Figure 2: Effect of ? in ????22 on the testset perfor-
mance. Plotted is the test set accuracy with logis-
tic regression as a function of ? for the L2 regular-
izer, Gaussian dropout (Wang and Manning, 2013)
+ additional L2, and quadratic dropout (8) + L2 de-
scribed in this paper. The default noising regularizer
is quite good, and additional L2 does not help. No-
tice that no choice of ? in L2 can help us combat
overfitting as effectively as (8) without underfitting.
ples with 22178 features, split evenly and randomly
into a training set and a test set.
Over a broad range of ? values, we find that
dropout plus L2 regularization performs far better
than using just L2 regularization for any value of
?. We see that Gaussian dropout appears to per-
form slightly better than the quadratic approxima-
tion discussed in this paper. However, our quadratic
approximation extends easily to the multiclass case
and to structured prediction in general, while Gaus-
sian dropout does not. Thus, it appears that our ap-
proximation presents a reasonable trade-off between
1176
computational efficiency and prediction accuracy.
5.2 CRF Experiments
We evaluate the quadratic dropout regularizer in
linear-chain CRFs on two sequence tagging tasks:
the CoNLL 2003 NER shared task (Tjong Kim Sang
and De Meulder, 2003) and the SANCL 2012 POS
tagging task (Petrov and McDonald, 2012) .
The standard CoNLL-2003 English shared task
benchmark dataset (Tjong Kim Sang and De Meul-
der, 2003) is a collection of documents from
Reuters newswire articles, annotated with four en-
tity types: Person, Location, Organization, and
Miscellaneous. We predicted the label sequence
Y = {LOC, MISC, ORG, PER, O}T without con-
sidering the BIO tags.
For training the CRF model, we used a compre-
hensive set of features from Finkel et al (2005) that
gives state-of-the-art results on this task. A total
number of 437906 features were generated on the
CoNLL-2003 training dataset. The most important
features are:
? The word, word shape, and letter n-grams (up to
6gram) at current position
? The prediction, word, and word shape of the pre-
vious and next position
? Previous word shape in conjunction with current
word shape
? Disjunctive word set of the previous and next 4
positions
? Capitalization pattern in a 3 word window
? Previous two words in conjunction with the word
shape of the previous word
? The current word matched against a list of name
titles (e.g., Mr., Mrs.)
The F?=1 results are summarized in Table 4. We
obtain a 1.6% and 1.1% absolute gain on the test
and dev set, respectively. Detailed results are bro-
ken down by precision and recall for each tag and are
shown in Table 6. These improvements are signifi-
cant at the 0.1% level according to the paired boot-
strap resampling method of 2000 iterations (Efron
and Tibshirani, 1993).
For the SANCL (Petrov and McDonald, 2012)
POS tagging task, we used the same CRF framework
with a much simpler set of features
? word unigrams: w?1, w0, w1
? word bigram: (w?1, w0) and (w0, w1)
F?=1 None L2 Drop
Dev 89.40 90.73 91.86
Test 84.67 85.82 87.42
Table 4: CoNLL summary of results. None: no reg-
ularization, Drop: quadratic dropout regularization
(14) described in this paper.
F?=1 None L2 Drop
newsgroups
Dev 91.34 91.34 91.47
Test 91.44 91.44 91.81
reviews
Dev 91.97 91.95 92.10
Test 90.70 90.67 91.07
answers
Dev 90.78 90.79 90.70
Test 91.00 90.99 91.09
Table 5: SANCL POS tagging F?=1 scores for the 3
official evaluation sets.
We obtained a small but consistent improvement
using the quadratic dropout regularizer in (14) over
the L2-regularized CRFs baseline.
Although the difference on SANCL is small,
the performance differences on the test sets of
reviews and newsgroups are statistically sig-
nificant at the 0.1% level. This is also interesting
because here is a situation where the features are ex-
tremely sparse, L2 regularization gave no improve-
ment, and where regularization overall matters less.
6 Conclusion
We have presented a new regularizer for learning
log-linear models such as multiclass logistic regres-
sion and conditional random fields. This regularizer
is based on a second-order approximation of fea-
ture noising schemes, and attempts to favor mod-
els that predict confidently and are robust to noise
in the data. In order to apply our method to CRFs,
we tackle the key challenge of dealing with feature
correlations that arise in the structured prediction
setting in several ways. In addition, we show that
the regularizer can be applied naturally in the semi-
supervised setting. Finally, we applied our method
to a range of different datasets and demonstrate con-
sistent gains over standard L2 regularization. Inves-
1177
Precision Recall F?=1
LOC 91.47% 91.12% 91.29
MISC 88.77% 81.07% 84.75
ORG 85.22% 84.08% 84.65
PER 92.12% 93.97% 93.04
Overall 89.84% 88.97% 89.40
(a) CoNLL dev. set with no regularization
Precision Recall F?=1
92.05% 92.84% 92.44
90.51% 83.52% 86.87
88.35% 85.23% 86.76
93.12% 94.19% 93.65
91.36% 90.11% 90.73
(b) CoNLL dev. set with L2 reg-
ularization
Precision Recall F?=1
93.59% 92.69% 93.14
93.99% 81.47% 87.28
92.48% 84.61% 88.37
94.81% 95.11% 94.96
93.85% 89.96% 91.86
(c) CoNLL dev. set with dropout
regularization
Tag Precision Recall F?=1
LOC 87.33% 84.47% 85.87
MISC 78.93% 77.12% 78.02
ORG 78.70% 79.49% 79.09
PER 88.82% 93.11% 90.92
Overall 84.28% 85.06% 84.67
(d) CoNLL test set with no regularization
Precision Recall F?=1
87.96% 86.13% 87.03
77.53% 79.30% 78.41
81.30% 80.49% 80.89
90.30% 93.33% 91.79
85.57% 86.08% 85.82
(e) CoNLL test set with L2 reg-
ularization
Precision Recall F?=1
86.26% 87.74% 86.99
81.52% 77.34% 79.37
88.29% 81.89% 84.97
92.15% 92.68% 92.41
88.40% 86.45% 87.42
(f) CoNLL test set with dropout
regularization
Table 6: CoNLL NER results broken down by tags and by precision, recall, and F?=1. Top: development
set, bottom: test set performance.
tigating how to better optimize this non-convex reg-
ularizer online and convincingly scale it to the semi-
supervised setting seem to be promising future di-
rections.
Acknowledgements
The authors would like to thank the anonymous re-
viewers for their comments. We gratefully acknowl-
edge the support of the Defense Advanced Research
Projects Agency (DARPA) Broad Operational Lan-
guage Translation (BOLT) program through IBM.
Any opinions, findings, and conclusions or recom-
mendations expressed in this material are those of
the author(s) and do not necessarily reflect the view
of the DARPA, or the US government. S. Wager is
supported by a BC and EJ Eaves SGF Fellowship.
References
Yaser S. Abu-Mostafa. 1990. Learning from hints in
neural networks. Journal of Complexity, 6(2):192?
198.
Chris M. Bishop. 1995. Training with noise is equiva-
lent to Tikhonov regularization. Neural computation,
7(1):108?116.
Robert Bryll, Ricardo Gutierrez-Osuna, and Francis
Quek. 2003. Attribute bagging: improving accuracy
of classifier ensembles by using random feature sub-
sets. Pattern recognition, 36(6):1291?1302.
Chris J.C. Burges and Bernhard Scho?lkopf. 1997. Im-
proving the accuracy and speed of support vector ma-
chines. In Advances in Neural Information Processing
Systems, pages 375?381.
Brad Efron and Robert Tibshirani. 1993. An Introduction
to the Bootstrap. Chapman & Hall, New York.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by Gibbs sam-
pling. In Proceedings of the 43rd annual meeting of
the Association for Computational Linguistics, pages
363?370.
Yves Grandvalet and Yoshua Bengio. 2005. Entropy
regularization. In Semi-Supervised Learning, United
Kingdom. Springer.
Geoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky,
Ilya Sutskever, and Ruslan R. Salakhutdinov.
2012. Improving neural networks by preventing
co-adaptation of feature detectors. arXiv preprint
arXiv:1207.0580.
Feng Jiao, Shaojun Wang, Chi-Hoon Lee, Russell
Greiner, and Dale Schuurmans. 2006. Semi-
supervised conditional random fields for improved se-
quence segmentation and labeling. In Proceedings of
the 44th annual meeting of the Association for Com-
putational Linguistics, ACL-44, pages 209?216.
Thorsten Joachims. 1999. Transductive inference for
1178
text classification using support vector machines. In
Proceedings of the International Conference on Ma-
chine Learning, pages 200?209.
Wei Li and Andrew McCallum. 2005. Semi-supervised
sequence modeling with syntactic topic models. In
Proceedings of the 20th national conference on Arti-
ficial Intelligence - Volume 2, AAAI?05, pages 813?
818.
Gideon S. Mann and Andrew McCallum. 2007. Sim-
ple, robust, scalable semi-supervised learning via ex-
pectation regularization. In Proceedings of the Inter-
national Conference on Machine Learning.
Kiyotoshi Matsuoka. 1992. Noise injection into inputs
in back-propagation learning. Systems, Man and Cy-
bernetics, IEEE Transactions on, 22(3):436?440.
Slav Petrov and Ryan McDonald. 2012. Overview of the
2012 shared task on parsing the web. Notes of the First
Workshop on Syntactic Analysis of Non-Canonical
Language (SANCL).
Salah Rifai, Yann Dauphin, Pascal Vincent, Yoshua Ben-
gio, and Xavier Muller. 2011a. The manifold tangent
classifier. Advances in Neural Information Processing
Systems, 24:2294?2302.
Salah Rifai, Xavier Glorot, Yoshua Bengio, and Pascal
Vincent. 2011b. Adding noise to the input of a model
trained with a regularized objective. arXiv preprint
arXiv:1104.3250.
Patrice Y. Simard, Yann A. Le Cun, John S. Denker, and
Bernard Victorri. 2000. Transformation invariance in
pattern recognition: Tangent distance and propagation.
International Journal of Imaging Systems and Tech-
nology, 11(3):181?197.
Andrew Smith, Trevor Cohn, and Miles Osborne. 2005.
Logarithmic opinion pools for conditional random
fields. In Proceedings of the 43rd Annual Meeting on
Association for Computational Linguistics, pages 18?
25. Association for Computational Linguistics.
Charles Sutton, Michael Sindelar, and Andrew McCal-
lum. 2005. Feature bagging: Preventing weight un-
dertraining in structured discriminative learning. Cen-
ter for Intelligent Information Retrieval, U. of Mas-
sachusetts.
Erik F. Tjong Kim Sang and Fien De Meulder. 2003.
Introduction to the conll-2003 shared task: language-
independent named entity recognition. In Proceedings
of the seventh conference on Natural language learn-
ing at HLT-NAACL 2003 - Volume 4, CONLL ?03,
pages 142?147.
Laurens van der Maaten, Minmin Chen, Stephen Tyree,
and Kilian Q. Weinberger. 2013. Learning with
marginalized corrupted features. In Proceedings of the
International Conference on Machine Learning.
Stefan Wager, Sida Wang, and Percy Liang. 2013.
Dropout training as adaptive regularization. arXiv
preprint:1307.1493.
Li Wan, Matthew Zeiler, Sixin Zhang, Yann LeCun, and
Rob Fergus. 2013. Regularization of neural networks
using dropconnect. In Proceedings of the Interna-
tional Conference on Machine learning.
Sida Wang and Christopher D. Manning. 2013. Fast
dropout training. In Proceedings of the International
Conference on Machine Learning.
1179
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1533?1544,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Semantic Parsing on Freebase from Question-Answer Pairs
Jonathan Berant Andrew Chou Roy Frostig Percy Liang
Computer Science Department, Stanford University
{joberant,akchou}@stanford.edu {rf,pliang}@cs.stanford.edu
Abstract
In this paper, we train a semantic parser that
scales up to Freebase. Instead of relying on
annotated logical forms, which is especially
expensive to obtain at large scale, we learn
from question-answer pairs. The main chal-
lenge in this setting is narrowing down the
huge number of possible logical predicates for
a given question. We tackle this problem in
two ways: First, we build a coarse mapping
from phrases to predicates using a knowledge
base and a large text corpus. Second, we
use a bridging operation to generate additional
predicates based on neighboring predicates.
On the dataset of Cai and Yates (2013), despite
not having annotated logical forms, our sys-
tem outperforms their state-of-the-art parser.
Additionally, we collected a more realistic and
challenging dataset of question-answer pairs
and improves over a natural baseline.
1 Introduction
We focus on the problem of semantic parsing nat-
ural language utterances into logical forms that can
be executed to produce denotations. Traditional se-
mantic parsers (Zelle and Mooney, 1996; Zettle-
moyer and Collins, 2005; Wong and Mooney, 2007;
Kwiatkowski et al, 2010) have two limitations: (i)
they require annotated logical forms as supervision,
and (ii) they operate in limited domains with a small
number of logical predicates. Recent developments
aim to lift these limitations, either by reducing the
amount of supervision (Clarke et al, 2010; Liang et
al., 2011; Goldwasser et al, 2011; Artzi and Zettle-
moyer, 2011) or by increasing the number of logical
Occidental College, Columbia University
Execute on Database
Type.University u Education.BarackObama
Type.University
Education
BarackObama
Which college did Obama go to ?
alignment
alignment
bridging
Figure 1: Our task is to map questions to answers via la-
tent logical forms. To narrow down the space of logical
predicates, we use a (i) coarse alignment based on Free-
base and a text corpus and (ii) a bridging operation that
generates predicates compatible with neighboring predi-
cates.
predicates (Cai and Yates, 2013). The goal of this
paper is to do both: learn a semantic parser with-
out annotated logical forms that scales to the large
number of predicates on Freebase.
At the lexical level, a major challenge in semantic
parsing is mapping natural language phrases (e.g.,
?attend?) to logical predicates (e.g., Education).
While limited-domain semantic parsers are able
to learn the lexicon from per-example supervision
(Kwiatkowski et al, 2011; Liang et al, 2011), at
large scale they have inadequate coverage (Cai and
Yates, 2013). Previous work on semantic parsing on
Freebase uses a combination of manual rules (Yahya
et al, 2012; Unger et al, 2012), distant supervision
(Krishnamurthy and Mitchell, 2012), and schema
1533
matching (Cai and Yates, 2013). We use a large
amount of web text and a knowledge base to build a
coarse alignment between phrases and predicates?
an approach similar in spirit to Cai and Yates (2013).
However, this alignment only allows us to gen-
erate a subset of the desired predicates. Aligning
light verbs (e.g., ?go?) and prepositions is not very
informative due to polysemy, and rare predicates
(e.g., ?cover price?) are difficult to cover even given
a large corpus. To improve coverage, we propose
a new bridging operation that generates predicates
based on adjacent predicates rather than on words.
At the compositional level, a semantic parser must
combine the predicates into a coherent logical form.
Previous work based on CCG requires manually
specifying combination rules (Krishnamurthy and
Mitchell, 2012) or inducing the rules from anno-
tated logical forms (Kwiatkowski et al, 2010; Cai
and Yates, 2013). We instead define a few simple
composition rules which over-generate and then use
model features to simulate soft rules and categories.
In particular, we use POS tag features and features
on the denotations of the predicted logical forms.
We experimented with two question answering
datasets on Freebase. First, on the dataset of Cai
and Yates (2013), we showed that our system out-
performs their state-of-the-art system 62% to 59%,
despite using no annotated logical forms. Second,
we collected a new realistic dataset of questions by
performing a breadth-first search using the Google
Suggest API; these questions are then answered by
Amazon Mechanical Turk workers. Although this
dataset is much more challenging and noisy, we are
still able to achieve 31.4% accuracy, a 4.5% ab-
solute improvement over a natural baseline. Both
datasets as well as the source code for SEMPRE, our
semantic parser, are publicly released and can be
downloaded from http://nlp.stanford.edu/
software/sempre/.
2 Setup
Problem Statement Our task is as follows: Given
(i) a knowledge base K, and (ii) a training set of
question-answer pairs {(xi, yi)}ni=1, output a se-
mantic parser that maps new questions x to answers
y via latent logical forms z and the knowledge base
K.
2.1 Knowledge base
Let E denote a set of entities (e.g., BarackObama),
and let P denote a set of properties (e.g.,
PlaceOfBirth). A knowledge base K is a set
of assertions (e1, p, e2) ? E ? P ? E (e.g.,
(BarackObama, PlaceOfBirth, Honolulu)).
We use the Freebase knowledge base (Google,
2013), which has 41M non-numeric entities, 19K
properties, and 596M assertions.1
2.2 Logical forms
To query the knowledge base, we use a logical lan-
guage called Lambda Dependency-Based Compo-
sitional Semantics (?-DCS)?see Liang (2013) for
details. For the purposes of this paper, we use a re-
stricted subset called simple ?-DCS, which we will
define below for the sake of completeness.
The chief motivation of ?-DCS is to produce
logical forms that are simpler than lambda cal-
culus forms. For example, ?x.?a.p1(x, a) ?
?b.p2(a, b) ? p3(b, e) is expressed compactly in
?-DCS as p1.p2.p3.e. Like DCS (Liang et al,
2011), ?-DCS makes existential quantification im-
plicit, thereby reducing the number of variables.
Variables are only used for anaphora and building
composite binary predicates; these do not appear in
simple ?-DCS.
Each logical form in simple ?-DCS is either a
unary (which denotes a subset of E) or a binary
(which denotes a subset of E ? E). The basic ?-
DCS logical forms z and their denotations JzKK are
defined recursively as follows:
? Unary base case: If e ? E is an entity (e.g.,
Seattle), then e is a unary logical form with
JzKK = {e}.
? Binary base case: If p ? P is a property (e.g.,
PlaceOfBirth), then p is a binary logical form
with JpKK = {(e1, e2) : (e1, p, e2) ? K}.
2
? Join: If b is a binary and u is a unary, then b.u
(e.g., PlaceOfBirth.Seattle) is a unary de-
noting a join and project: Jb.uKK = {e1 ? E :
?e2.(e1, e2) ? JbKK ? e2 ? JuKK}.
1In this paper, we condense Freebase names for readability
(/people/person becomes Person).
2Binaries can be also built out of lambda abstractions (e.g.,
?x.Performance.Actor.x), but as these constructions are
not central to this paper, we defer to (Liang, 2013).
1534
? Intersection: If u1 and u2 are both unaries,
then u1 u u2 (e.g., Profession.Scientist u
PlaceOfBirth.Seattle) denotes set intersec-
tion: Ju1 u u2KK = Ju1KK ? Ju2KK.
? Aggregation: If u is a unary, then count(u)
denotes the cardinality: Jcount(u)KK =
{|JuKK|}.
As a final example, ?number of dramas star-
ring Tom Cruise? in lambda calculus would
be represented as count(?x.Genre(x, Drama) ?
?y.Performance(x, y) ? Actor(y, TomCruise));
in ?-DCS, it is simply count(Genre.Drama u
Performance.Actor.TomCruise).
It is useful to think of the knowledge base K as
a directed graph in which entities are nodes and
properties are labels on the edges. Then simple ?-
DCS unary logical forms are tree-like graph patterns
which pick out a subset of the nodes.
2.3 Framework
Given an utterance x, our semantic parser constructs
a distribution over possible derivations D(x). Each
derivation d ? D(x) is a tree specifying the appli-
cation of a set of combination rules that culminates
in the logical form d.z at the root of the tree?see
Figure 2 for an example.
Composition Derivations are constructed recur-
sively based on (i) a lexicon mapping natural lan-
guage phrases to knowledge base predicates, and (ii)
a small set of composition rules.
More specifically, we build a set of derivations for
each span of the utterance. We first use the lexicon to
generate single-predicate derivations for any match-
ing span (e.g., ?born? maps to PeopleBornHere).
Then, given any logical form z1 that has been con-
structed over the span [i1 : j1] and z2 over a non-
overlapping span [i2 : j2], we generate the following
logical forms over the enclosing span [min(i1, i2) :
max(j1, j2)]: intersection z1 u z2, join z1.z2, ag-
gregation z1(z2) (e.g., if z1 = count), or bridging
z1 u p.z2 for any property p ? P (explained more in
Section 3.2).3
Note that the construction of derivations D(x)
allows us to skip any words, and in general heav-
3We also discard logical forms are incompatible according
to the Freebase types (e.g., Profession.Politician u
Type.City would be rejected).
Type.Locationu PeopleBornHere.BarackObama
Type.Location
where
was PeopleBornHere.BarackObama
BarackObama
Obama
PeopleBornHere
born
?join
intersection
lexicon
lexicon lexicon
Figure 2: An example of a derivation d of the utterance
?Where was Obama born?? and its sub-derivations, each
labeled with composition rule (in blue) and logical form
(in red). The derivation d skips the words ?was? and ???.
ily over-generates. We instead rely on features and
learning to guide us away from the bad derivations.
Modeling Following Zettlemoyer and Collins
(2005) and Liang et al (2011), we define a
discriminative log-linear model over derivations
d ? D(x) given utterances x: p?(d | x) =
exp{?(x,d)>?}
?
d??D(x) exp{?(x,d
?)>?} , where ?(x, d) is a feature
vector extracted from the utterance and the deriva-
tion, and ? ? Rb is the vector of parameters to
be learned. As our training data consists only of
question-answer pairs (xi, yi), we maximize the log-
likelihood of the correct answer (Jd.zKK = yi), sum-
ming over the latent derivation d. Formally, our
training objective is
O(?) =
n?
i=1
log
?
d?D(x):Jd.zKK=yi
p?(d | xi). (1)
Section 4 describes an approximation of this ob-
jective that we maximize to choose parameters ?.
3 Approach
Our knowledge base has more than 19,000 proper-
ties, so a major challenge is generating a manage-
able set of predicates for an utterance. We propose
two strategies for doing this. First (Section 3.1),
we construct a lexicon that maps natural language
phrases to logical predicates by aligning a large text
corpus to Freebase, reminiscent of Cai and Yates
(2013). Second, we generate logical predicates com-
patible with neighboring predicates using the bridg-
ing operation (Section 3.2). Bridging is crucial when
aligning phrases is difficult or even impossible. The
derivations produced by combining these predicates
1535
grew up in[Person,Location]born in[Person,Date]married in[Person,Date]born in[Person,Location]
DateOfBirth
PlaceOfBirth
Marriage.StartDate
PlacesLived.Location
(BarackObama,Honolulu)
(MichelleObama,Chicago)
(BarackObama,Chicago)(RandomPerson,Seattle)
F(r1) F(r2)
C(r1, r2)
Alignment featureslog-phrase-count:log(15765)log-predicate-count: log(9182)log-intersection-count: log(6048)KB-best-match: 0
Figure 3: We construct a bipartite graph over phrasesR1
and predicates R2. Each edge (r1, r2) is associated with
alignment features.
are scored using features that capture lexical, syn-
tactic and semantic regularities (Section 3.3).
3.1 Alignment
We now discuss the construction of a lexicon L,
which is a mapping from natural language phrases
to logical predicates accompanied by a set of fea-
tures. Specifically, for a phrase w (e.g., ?born in?),
L(w) is a set of entries (z, s), where z is a predicate
and s is the set of features. A lexicon is constructed
by alignment of a large text corpus to the knowledge
base (KB). Intuitively, a phrase and a predicate align
if they co-occur with many of the same entities.
Here is a summary of our alignment proce-
dure: We construct a set of typed4 phrases
R1 (e.g., ?born in?[Person,Location]) and pred-
icates R2 (e.g., PlaceOfBirth). For each
r ? R1 ? R2, we create its extension
F(r), which is a set of co-occurring entity-
pairs (e.g., F(?born in?[Person,Location]) =
{(BarackObama, Honolulu), . . . }. The lexicon is
generated based on the overlap F(r1) ? F(r2), for
r1 ? R1 and r2 ? R2.
Typed phrases 15 million triples (e1, r, e2) (e.g.,
(?Obama?, ?was also born in?, ?August 1961?))
4Freebase associates each entity with a set of types using the
Type property.
were extracted from ClueWeb09 using the ReVerb
open IE system (Fader et al, 2011). Lin et al (2012)
released a subset of these triples5 where they were
able to substitute the subject arguments with KB en-
tities. We downloaded their dataset and heuristically
replaced object arguments with KB entities by walk-
ing on the Freebase graph from subject KB entities
and performing simple string matching. In addition,
we normalized dates with SUTime (Chang and Man-
ning, 2012).
We lemmatize and normalize each text phrase
r ? R1 and augment it with a type signature
[t1, t2] to deal with polysemy (?born in? could ei-
ther map to PlaceOfBirth or DateOfBirth). We
add an entity pair (e1, e2) to the extension of
F(r[t1, t2]) if the (Freebase) type of e1 (e2) is t1
(t2). For example, (BarackObama, 1961) is added
to F(?born in?[Person, Date]). We perform a simi-
lar procedure that uses a Hearst-like pattern (Hearst,
1992) to map phrases to unary predicates. If a
text phrase r ? R1 matches the pattern ?(is|was
a|the) x IN?, where IN is a preposition, then we
add e1 to F(x). For (Honolulu, ?is a city in?,
Hawaii), we extract x = ?city ?? and add Honolulu
to F(?city?). From the initial 15M triples, we ex-
tracted 55,081 typed binary phrases (9,456 untyped)
and 6,299 unary phrases.
Logical predicates Binary logical predicates con-
tain (i) all KB properties6 and (ii) concatenations of
two properties p1.p2 if the intermediate type repre-
sents an event (e.g., the married to relation is rep-
resented by Marriage.Spouse). For unary pred-
icates, we consider all logical forms Type.t and
Profession.t for all (abstract) entities t ? E (e.g.
Type.Book and Profession.Author). The types
of logical predicates considered during alignment is
restricted in this paper, but automatic induction of
more compositional logical predicates is an interest-
ing direction. Finally, we define the extension of a
logical predicate r ? R2 to be its denotation, that is,
the corresponding set of entities or entity pairs.
Lexicon construction Given typed phrases R1,
logical predicates R2, and their extensions F , we
now generate the lexicon. It is useful to think of a
5http://knowitall.cs.washington.edu/
linked_extractions/
6We filter properties from the domains user and base.
1536
Category Description
Alignment Log of # entity pairs that occur with the
phrase r1 (|F(r1)|)
Log of # entity pairs that occur with the
logical predicate r2 (|F(r2)|)
Log of # entity pairs that occur with both
r1 and r2 (|F(r1) ? F(r2)|)
Whether r2 is the best match for r1 (r2 =
argmaxr |F(r1) ? F(r)|)
Lexicalized Conjunction of phrase w and predicate z
Text similarity Phrase r1 is equal/prefix/suffix of s2
Phrase overlap of r1 and s2
Bridging Log of # entity pairs that occur with bridg-
ing predicate b (|F(b)|)
Kind of bridging (# unaries involved)
The binary b injected
Composition # of intersect/join/bridging operations
POS tags in join/bridging and skipped
words
Size of denotation of logical form
Table 1: Full set of features. For the alignment and text sim-
ilarity, r1 is a phrase, r2 is a predicate with Freebase name s2,
and b is a binary predicate with type signature (t1, t2).
bipartite graph with left nodes R1 and right nodes
R2 (Figure 3). We add an edge (r1, r2) if (i) the
type signatures of r1 and r2 match7 and (ii) their ex-
tensions have non-empty overlap (F(r1)?F(r2) 6=
?). Our final graph contains 109K edges for binary
predicates and 294K edges for unary predicates.
Naturally, non-zero overlap by no means guaran-
tees that r1 should map to r2. In our noisy data,
even ?born in? and Marriage.EndDate co-occur 4
times. Rather than thresholding based on some cri-
terion, we compute a set of features, which are used
by the model downstream in conjunction with other
sources of information.
We compute three types of features (Table 1).
Alignment features are unlexicalized and measure
association based on argument overlap. Lexicalized
features are standard conjunctions of the phrase w
and the logical form z. Text similarity features com-
pare the (untyped) phrase (e.g., ?born?) to the Free-
base name of the logical predicate (e.g., ?People
born here?): Given the phrase r1 and the Freebase
name s2 of the predicate r2, we compute string sim-
ilarity features such as whether r1 and s2 are equal,
7Each Freebase property has a designated type signa-
ture, which can be extended to composite predicates, e.g.,
sig(Marriage.StartDate) = (Person,Date).
as well as some other measures of token overlap.
3.2 Bridging
While alignment can cover many predicates, it is un-
reliable for cases where the predicates are expressed
weakly or implicitly. For example, in ?What govern-
ment does Chile have??, the predicate is expressed
by the light verb have, in ?What actors are in Top
Gun??, it is expressed by a highly ambiguous prepo-
sition, and in ?What is Italy money?? [sic], it is
omitted altogether. Since natural language doesn?t
offer much help here, let us turn elsewhere for guid-
ance. Recall that at this point our main goal is to
generate a manageable set of candidate logical forms
to be scored by the log-linear model.
In the first example, suppose the phrases ?Chile?
and ?government? are parsed as Chile and
Type.FormOfGovernment, respectively, and we hy-
pothesize a connecting binary. The two predicates
impose strong type constraints on that binary, so we
can afford to generate all the binary predicates that
type check (see Table 2). More formally, given two
unaries z1 and z2 with types t1 and t2, we generate a
logical form z1 u b.z2 for each binary b whose type
signature is (t1, t2). Figure 1 visualizes bridging of
the unaries Type.University and Obama.
Now consider the example ?What is the
cover price of X-men?? Here, the binary
ComicBookCoverPrice is expressed explicitly, but
is not in our lexicon since the language use is rare.
To handle this, we allow bridging to generate a bi-
nary based on a single unary; in this case, based on
the unary X-Men (Table 2), we generate several bina-
ries including ComicBookCoverPrice. Generically,
given a unary z with type t, we construct a logical
form b.z for any predicate b with type (?, t).
Finally, consider the question ?Who did
Tom Cruise marry in 2006??. Suppose we
parse the phrase ?Tom Cruise marry? into
Marriage.Spouse.TomCruise, or more explicitly,
?x.?e.Marriage(x, e) ? Spouse(e, TomCruise).
Here, the neo-Davidsonian event variable e is an
intermediate quantity, but needs to be further mod-
ified (in this case, by the temporal modifier 2006).
To handle this, we apply bridging to a unary and the
intermediate event (see Table 2). Generically, given
a logical form p1.p2.z? where p2 has type (t1, ?),
and a unary z with type t, bridging injects z and
1537
# Form 1 Form 2 Bridging
1 Type.FormOfGovernment Chile Type.FormOfGovernmentu GovernmentTypeOf.Chile
2 X-Men ComicBookCoverPriceOf.X-Men
3 Marriage.Spouse.TomCruise 2006 Marriage.(Spouse.TomCruise u StartDate.2006)
Table 2: Three examples of the bridging operation. The bridging binary predicate b is in boldface.
constructs a logical form p1.(p2.z? u b.z) for each
logical predicate b with type (t1, t).
In each of the three examples, bridging gener-
ates a binary predicate based on neighboring logi-
cal predicates rather than on explicit lexical material.
In a way, our bridging operation shares with bridg-
ing anaphora (Clark, 1975) the idea of establishing
a novel relation between distinct parts of a sentence.
Naturally, we need features to distinguish between
the generated predicates, or decide whether bridging
is even appropriate at all. Given a binary b, features
include the log of the predicate count log |F(b)|, in-
dicators for the kind of bridging, an indicator on the
binary b for injections (Table 1). In addition, we add
all text similarity features by comparing the Free-
base name of b with content words in the question.
3.3 Composition
So far, we have mainly focused on the generation of
predicates. We now discuss three classes of features
pertaining to their composition.
Rule features Each derivation d is the result of ap-
plying some number of intersection, join, and bridg-
ing operations. To control this number, we define
indicator features on each of these counts. This is in
contrast to the norm of having a single feature whose
value is equal to the count, which can only repre-
sent one-sided preferences for having more or fewer
of a given operation. Indicator features stabilize the
model, preferring derivations with a well-balanced
inventory of operations.
Part-of-speech tag features To guide the compo-
sition of predicates, we use POS tags in two ways.
First, we introduce features indicating when a word
of a given POS tag is skipped, which could capture
the fact that skipping auxiliaries is generally accept-
able, while skipping proper nouns is not. Second,
we introduce features on the POS tags involved in a
composition, inspired by dependency parsing (Mc-
Donald et al, 2005). Specifically, when we combine
logical forms z1 and z2 via a join or bridging, we
include a feature on the POS tag of (the first word
spanned by) z1 conjoined with the POS tag corre-
sponding to z2. Rather than using head-modifier in-
formation from dependency trees (Branavan et al,
2012; Krishnamurthy and Mitchell, 2012; Cai and
Yates, 2013; Poon, 2013), we can learn the appro-
priate relationships tailored for downstream accu-
racy. For example, the phrase ?located? is aligned
to the predicate ContainedBy. POS features can de-
tect that if ?located? precedes a noun phrase (?What
is located in Beijing??), then the noun phrase is the
object of the predicate, and if it follows the noun
phrase (?Where is Beijing located??), then it is in
subject position.
Note that our three operations (intersection, join,
and bridging) are quite permissive, and we rely on
features, which encode soft, overlapping rules. In
contrast, CCG-based methods (Kwiatkowski et al,
2010; Kwiatkowski et al, 2011) encode the com-
bination preferences structurally in non-overlapping
rules; these could be emulated with features with
weights clamped to ??.
Denotation features While it is clear that learning
from denotations rather than logical forms is a draw-
back since it provides less information, it is less ob-
vious that working with denotations actually gives
us additional information. Specifically, we include
four features indicating whether the denotation of
the predicted logical form has size 0, 1, 2, or at least
3. This feature encodes presupposition constraints
in a soft way: when people ask a question, usually
there is an answer and it is often unique. This allows
us to favor logical forms with this property.
4 Experiments
We now evaluate our semantic parser empirically.
In Section 4.1, we compare our approach to Cai
and Yates (2013) on their recently released dataset
(henceforth, FREE917) and present results on a new
1538
dataset that we collected (henceforth, WEBQUES-
TIONS). In Section 4.2, we provide detailed experi-
ments to provide additional insight on our system.
Setup We implemented a standard beam-based
bottom-up parser which stores the k-best derivations
for each span. We use k = 500 for all our experi-
ments on FREE917 and k = 200 on WEBQUES-
TIONS. The root beam yields the candidate set D?(x)
and is used to approximate the sum in the objective
functionO(?) in (1). In experiments on WEBQUES-
TIONS, D?(x) contained 197 derivations on average.
We write the approximate objective as O(?; ??) =
?
i log
?
d?D?(xi;??):Jd.zKK=yi
p(d | xi; ?) to explic-
itly show dependence on the parameters ?? used for
beam search. We optimize the objective by initial-
izing ?0 to 0 and applying AdaGrad (stochastic gra-
dient ascent with per-feature adaptive step size con-
trol) (Duchi et al, 2010), so that ?t+1 is set based on
taking a stochastic approximation of ?O(?;?t)??
?
?
?=?t
.
We make six passes over the training examples.
We used POS tagging and named-entity recogni-
tion to restrict what phrases in the utterance could
be mapped by the lexicon. Entities must be named
entities, proper nouns or a sequence of at least two
tokens. Unaries must be a sequence of nouns, and
binaries must be either a content word, or a verb fol-
lowed by either a noun phrase or a particle. In addi-
tion, we used 17 hand-written rules to map question
words such as ?where? and ?how many? to logical
forms such as Type.Location and Count.
To compute denotations, we convert a logical
form z into a SPARQL query and execute it on our
copy of Freebase using the Virtuoso engine. On
WEBQUESTIONS, a full run over the training exam-
ples involves approximately 600,000 queries. For
evaluation, we predict the answer from the deriva-
tion with highest probability.
4.1 Main results
4.1.1 FREE917
Cai and Yates (2013) created a dataset consist-
ing of 917 questions involving 635 Freebase rela-
tions, annotated with lambda calculus forms. We
converted all 917 questions into simple ?-DCS, ex-
ecuted them on Freebase and used the resulting an-
swers to train and evaluate. To map phrases to Free-
base entities we used the manually-created entity
lexicon used by Cai and Yates (2013), which con-
tains 1,100 entries. Because entity disambiguation
is a challenging problem in semantic parsing, the en-
tity lexicon simplifies the problem.
Following Cai and Yates (2013), we held out 30%
of the examples for the final test, and performed all
development on the remaining 70%. During devel-
opment, we split the data and used 512 examples
(80%) for training and the remaining 129 (20%) for
validation. All reported development numbers are
averaged across 3 random splits. We evaluated us-
ing accuracy, the fraction of examples where the pre-
dicted answer exactly matched the correct answer.
Our main empirical result is that our system,
which was trained only on question-answer pairs,
obtained 62% accuracy on the test set, outperform-
ing the 59% accuracy reported by Cai and Yates
(2013), who trained on full logical forms.
4.1.2 WEBQUESTIONS
Dataset collection Because FREE917 requires
logical forms, it is difficult to scale up due to the
required expertise of annotating logical forms. We
therefore created a new dataset, WEBQUESTIONS,
of question-answer pairs obtained from non-experts.
To collect this dataset, we used the Google Sug-
gest API to obtain questions that begin with a wh-
word and contain exactly one entity. We started with
the question ?Where was Barack Obama born??
and performed a breadth-first search over questions
(nodes), using the Google Suggest API supplying
the edges of the graph. Specifically, we queried the
question excluding the entity, the phrase before the
entity, or the phrase after it; each query generates 5
candidate questions, which are added to the queue.
We iterated until 1M questions were visited; a ran-
dom 100K were submitted to Amazon Mechanical
Turk (AMT).
The AMT task requested that workers answer the
question using only the Freebase page of the ques-
tions? entity, or otherwise mark it as unanswerable
by Freebase. The answer was restricted to be one of
the possible entities, values, or list of entities on the
page. As this list was long, we allowed the user to
filter the list by typing. We paid the workers $0.03
per question. Out of 100K questions, 6,642 were
annotated identically by at least two AMT workers.
We again held out a 35% random subset of the
1539
Dataset # examples # word types
GeoQuery 880 279
ATIS 5,418 936
FREE917 917 2,036
WEBQUESTIONS 5,810 4,525
Table 3: Statistics on various semantic parsing datasets. Our
new dataset, WEBQUESTIONS, is much larger than FREE917
and much more lexically diverse than ATIS.
questions for the final test, and performed all devel-
opment on the remaining 65%, which was further
divided into an 80%?20% split for training and val-
idation. To map entities, we built a Lucene index
over the 41M Freebase entities.
Table 3 provides some statistics about the new
questions. One major difference in the datasets is
the distribution of questions: FREE917 starts from
Freebase properties and solicits questions about
these properties; these questions tend to be tai-
lored to the properties. WEBQUESTIONS starts from
questions completely independent of Freebase, and
therefore the questions tend to be more natural and
varied. For example, for the Freebase property
ComicGenre, FREE917 contains the question ?What
genre is Doonesbury??, while WEBQUESTIONS for
the property MusicGenre contains ?What music did
Beethoven compose??.
The number of word types in WEBQUESTIONS is
larger than in datasets such as ATIS and GeoQuery
(Table 3), making lexical mapping much more chal-
lenging. On the other hand, in terms of structural
complexity WEBQUESTIONS is simpler and many
questions contain a unary, a binary and an entity.
In some questions, the answer provided by AMT
workers is only roughly accurate, because workers
are restricted to selecting answers from the Freebase
page. For example, the answer given by workers to
the question ?What is James Madison most famous
for?? is ?President of the United States? rather than
?Authoring the Bill of Rights?.
Results AMT workers sometimes provide partial
answers, e.g., the answer to ?What movies does Tay-
lor Lautner play in?? is a set of 17 entities, out
of which only 10 appear on the Freebase page. We
therefore allow partial credit and score an answer us-
ing the F1 measure, comparing the predicted set of
entities to the annotated set of entities.
System FREE917 WebQ.
ALIGNMENT 38.0 30.6
BRIDGING 66.9 21.2
ALIGNMENT+BRIDGING 71.3 32.9
Table 4: Accuracies on the development set under different
schemes of binary predicate generation. In ALIGNMENT, bi-
naries are generated only via the alignment lexicon. In BRIDG-
ING, binaries are generated through the bridging operation only.
ALIGNMENT+BRIDGING corresponds to the full system.
As a baseline, we omit from our system the main
contributions presented in this paper?that is, we
disallow bridging, and remove denotation and align-
ment features. The accuracy on the test set of this
system is 26.9%, whereas our full system obtains
31.4%, a significant improvement.
Note that the number of possible derivations for
questions in WEBQUESTIONS is quite large. In the
question ?What kind of system of government does
the United States have?? the phrase ?United States?
maps to 231 entities in our lexicon, the verb ?have?
maps to 203 binaries, and the phrases ?kind?, ?sys-
tem?, and ?government? all map to many different
unary and binary predicates. Parsing correctly in-
volves skipping some words, mapping other words
to predicates, while resolving many ambiguities in
the way that the various predicates can combine.
4.2 Detailed analysis
We now delve deeper to explore the contributions of
the various components of our system. All ablation
results reported next were run on the development
set (over 3 random splits).
Generation of binary predicates Recall that our
system has two mechanisms for suggesting binaries:
from the alignment lexicon or via the bridging op-
eration. Table 4 shows accuracies when only one or
both is used. Interestingly, alignment alone is better
than bridging alone on WEBQUESTIONS, whereas
for FREE917, it is the opposite. The reason for this
is that FREE917 contains questions on rare pred-
icates. These are often missing from the lexicon,
but tend to have distinctive types and hence can be
predicted from neighboring predicates. In contrast,
WEBQUESTIONS contains questions that are com-
monly searched for and focuses on popular predi-
cates, therefore exhibiting larger lexical variation.
1540
System FREE917 WebQ.
FULL 71.3 32.9
-POS 70.5 28.9
-DENOTATION 58.6 28.0
Table 5: Accuracies on the development set with features re-
moved. POS and DENOTATION refer to the POS tag and deno-
tation features from Section 3.3.
System FREE917 WebQ.
ALIGNMENT 71.3 32.9
LEXICALIZED 68.5 34.2
LEXICALIZED+ALIGNMENT 69.0 36.4
Table 6: Accuracies on the development set using either
unlexicalized alignment features (ALIGNMENT) or lexicalized
features (LEXICALIZED).
For instance, when training without an align-
ment lexicon, the system errs on ?When did Nathan
Smith die??. Bridging suggests binaries that are
compatible with the common types Person and
Datetime, and the binary PlaceOfBirth is cho-
sen. On the other hand, without bridging, the sys-
tem errs on ?In which comic book issue did Kitty
Pryde first appear??, which refers to the rare pred-
icate ComicBookFirstAppearance. With bridging,
the parser can identify the correct binary, by linking
the types ComicBook and ComicBookCharacter. On
both datasets, best performance is achieved by com-
bining the two sources of information.
Overall, running on WEBQUESTIONS, the parser
constructs derivations that contain about 12,000 dis-
tinct binary predicates.
Feature variations Table 5 shows the results of
feature ablation studies. Accuracy drops when POS
tag features are omitted, e.g., in the question ?What
number is Kevin Youkilis on the Boston Red Sox? the
parser happily skips the NNPs ?Kevin Youkilis? and
returns the numbers of all players on the Boston Red
Sox. A significant loss is incurred without denota-
tion features, largely due to the parser returning log-
ical forms with empty denotations. For instance, the
question ?How many people were at the 2006 FIFA
world cup final?? is answered with a logical form
containing the property PeopleInvolved rather than
SoccerMatchAttendance, resulting in an empty de-
notation.
Next we study the impact of lexicalized versus
0 iterations 1 iterations 2 iterations
Figure 4: Beam of candidate derivations D?(x) for 50
WEBQUESTIONS examples. In each matrix, columns
correspond to examples and rows correspond to beam po-
sition (ranked by decreasing model score). Green cells
mark the positions of derivations with correct denota-
tions. Note that both the number of good derivations and
their positions improve as ? is optimized.
 0 0.2 0.4 0.6 0.8 1
 0  100  200  300  400  500oracleaccuracy
(a) FREE917
 0 0.2 0.4 0.6 0.8 1
 0  100  200oracleaccuracy
(b) WEBQUESTIONS
Figure 5: Accuracy and oracle as beam size k increases.
unlexicalized features (Table 6). In the large WE-
BQUESTIONS dataset, lexicalized features helped,
and so we added those features to our model when
running on the test set. In FREE917 lexicalized fea-
tures result in overfitting due to the small number of
training examples. Thus, we ran our final parser on
the test set without lexicalized features.
Effect of beam size An intrinsic challenge in se-
mantic parsing is to handle the exponentially large
set of possible derivations. We rely heavily on the
k-best beam approximation in the parser keeping
good derivations that lead to the correct answer. Re-
call that the set of candidate derivations D?(x) de-
pends on the parameters ?. In the initial stages of
learning, ? is far from optimal, so good derivations
are likely to fall below the k-best cutoff of inter-
nal parser beams. As a result, D?(x) contains few
derivations with the correct answer. Still, placing
these few derivations on the beam allows the train-
ing procedure to bootstrap ? into a good solution.
Figure 4 illustrates this improvement in D?(x) across
early training iterations.
Smaller choices of k yield a coarser approxima-
1541
tion in beam search. As we increase k (Figure 5), we
see a tapering improvement in accuracy. We also see
a widening gap between accuracy and oracle score,8
as including a good derivation in D?(x) is made eas-
ier but the learning problem is made more difficult.
Error analysis The accuracy on WEBQUES-
TIONS is much lower than on FREE917. We an-
alyzed WEBQUESTIONS examples and found sev-
eral main causes of error: (i) Disambiguating en-
tities in WEBQUESTIONS is much harder because
the entity lexicon has 41M entities. For example,
given ?Where did the battle of New Orleans start??
the system identifies ?New Orleans? as the target
entity rather than its surrounding noun phrase. Re-
call that all FREE917 experiments used a carefully
chosen entity lexicon. (ii) Bridging can often fail
when the question?s entity is compatible with many
binaries. For example, in ?What did Charles Bab-
bage make??, the system chooses a wrong binary
compatible with the type Person. (iii) The system
sometimes incorrectly draws verbs from subordinate
clauses. For example, in ?Where did Walt Disney
live before he died?? it returns the place of death of
Walt Disney, ignoring the matrix verb live.
5 Discussion
Our work intersects with two strands of work.
The first involves learning models of semantics
guided by denotations or interactions with the world.
Besides semantic parsing for querying databases
(Popescu et al, 2003; Clarke et al, 2010; Liang
et al, 2011), previous work has looked at inter-
preting natural language for performing program-
ming tasks (Kushman and Barzilay, 2013; Lei et
al., 2013), playing computer games (Branavan et al,
2010; Branavan et al, 2011), following navigational
instructions (Chen, 2012; Artzi and Zettlemoyer,
2013), and interacting in the real world via percep-
tion (Matuszek et al, 2012; Tellex et al, 2011; Kr-
ishnamurthy and Kollar, 2013). Our system uses
denotations rather than logical forms as a training
signal, but also benefits from denotation features,
which becomes possible in the grounded setting.
The second body of work involves connecting
natural language and open-domain databases. Sev-
8Oracle score is the fraction of examples for which D?(x)
contains any derivation with the correct denotation.
eral works perform relation extraction using dis-
tant supervision from a knowledge base (Riedel et
al., 2010; Carlson et al, 2010; Hoffmann et al,
2011; Surdeanu et al, 2012). While similar in spirit
to our alignment procedure for building the lexi-
con, one difference is that relation extraction cares
about facts, aggregating over phrases, whereas a
lexicon concerns specific phrases, thus aggregating
over facts. On the question answering side, recent
methods have made progress in building semantic
parsers for the open domain, but still require a fair
amount of manual effort (Yahya et al, 2012; Unger
et al, 2012; Cai and Yates, 2013). Our system re-
duces the amount of supervision and has a more ex-
tensive evaluation on a new dataset.
Finally, although Freebase has thousands of prop-
erties, open information extraction (Banko et al,
2007; Fader et al, 2011; Masaum et al, 2012)
and associated question answering systems (Fader
et al, 2013) work over an even larger open-ended
set of properties. The drawback of this regime is
that the noise and the difficulty in canonicaliza-
tion make it hard to perform reliable composition,
thereby nullifying one of the key benefits of se-
mantic parsing. An interesting midpoint involves
keeping the structured knowledge base but aug-
menting the predicates, for example using random
walks (Lao et al, 2011) or Markov logic (Zhang
et al, 2012). This would allow us to map atomic
words (e.g., ?wife?) to composite predicates (e.g.,
?x.Marriage.Spouse.(Gender.Femaleux)). Learn-
ing these composite predicates would drastically in-
crease the possible space of logical forms, but we
believe that the methods proposed in this paper?
alignment via distant supervision and bridging?can
provide some traction on this problem.
Acknowledgments
We would like to thank Thomas Lin, Mausam and
Oren Etzioni for providing us with open IE triples
that are partially-linked to Freebase, and also Arun
Chaganty for helpful comments. The authors grate-
fully acknowledge the support of the Defense Ad-
vanced Research Projects Agency (DARPA) Deep
Exploration and Filtering of Text (DEFT) Program
under Air Force Research Laboratory (AFRL) prime
contract no. FA8750-13-2-0040.
1542
References
Y. Artzi and L. Zettlemoyer. 2011. Bootstrapping
semantic parsers from conversations. In Empirical
Methods in Natural Language Processing (EMNLP),
pages 421?432.
Y. Artzi and L. Zettlemoyer. 2013. Weakly supervised
learning of semantic parsers for mapping instructions
to actions. Transactions of the Association for Com-
putational Linguistics (TACL), 1:49?62.
M. Banko, M. J. Cafarella, S. Soderland, M. Broadhead,
and O. Etzioni. 2007. Open information extraction
from the web. In International Joint Conference on
Artificial Intelligence (IJCAI), pages 2670?2676.
S. Branavan, L. Zettlemoyer, and R. Barzilay. 2010.
Reading between the lines: Learning to map high-level
instructions to commands. In Association for Compu-
tational Linguistics (ACL), pages 1268?1277.
S. Branavan, D. Silver, and R. Barzilay. 2011. Learning
to win by reading manuals in a Monte-Carlo frame-
work. In Association for Computational Linguistics
(ACL), pages 268?277.
S. Branavan, N. Kushman, T. Lei, and R. Barzilay. 2012.
Learning high-level planning from text. In Association
for Computational Linguistics (ACL), pages 126?135.
Q. Cai and A. Yates. 2013. Large-scale semantic parsing
via schema matching and lexicon extension. In Asso-
ciation for Computational Linguistics (ACL).
A. Carlson, J. Betteridge, B. Kisiel, B. Settles, E. R. H.
Jr, and T. M. Mitchell. 2010. Toward an architecture
for never-ending language learning. In Association for
the Advancement of Artificial Intelligence (AAAI).
A. X. Chang and C. Manning. 2012. SUTime: A library
for recognizing and normalizing time expressions. In
Language Resources and Evaluation (LREC), pages
3735?3740.
D. Chen. 2012. Fast online lexicon learning for grounded
language acquisition. In Association for Computa-
tional Linguistics (ACL).
H. H. Clark. 1975. Bridging. In Workshop on theoretical
issues in natural language processing, pages 169?174.
J. Clarke, D. Goldwasser, M. Chang, and D. Roth.
2010. Driving semantic parsing from the world?s re-
sponse. In Computational Natural Language Learn-
ing (CoNLL), pages 18?27.
J. Duchi, E. Hazan, and Y. Singer. 2010. Adaptive
subgradient methods for online learning and stochas-
tic optimization. In Conference on Learning Theory
(COLT).
A. Fader, S. Soderland, and O. Etzioni. 2011. Identifying
relations for open information extraction. In Empirical
Methods in Natural Language Processing (EMNLP).
A. Fader, L. Zettlemoyer, and O. Etzioni. 2013.
Paraphrase-driven learning for open question answer-
ing. In Association for Computational Linguistics
(ACL).
D. Goldwasser, R. Reichart, J. Clarke, and D. Roth.
2011. Confidence driven unsupervised semantic pars-
ing. In Association for Computational Linguistics
(ACL), pages 1486?1495.
Google. 2013. Freebase data dumps (2013-06-
09). https://developers.google.com/
freebase/data.
M. A. Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Interational Conference on
Computational linguistics, pages 539?545.
R. Hoffmann, C. Zhang, X. Ling, L. S. Zettlemoyer, and
D. S. Weld. 2011. Knowledge-based weak super-
vision for information extraction of overlapping rela-
tions. In Association for Computational Linguistics
(ACL), pages 541?550.
J. Krishnamurthy and T. Kollar. 2013. Jointly learning
to parse and perceive: Connecting natural language to
the physical world. Transactions of the Association for
Computational Linguistics (TACL), 1:193?206.
J. Krishnamurthy and T. Mitchell. 2012. Weakly super-
vised training of semantic parsers. In Empirical Meth-
ods in Natural Language Processing and Computa-
tional Natural Language Learning (EMNLP/CoNLL),
pages 754?765.
N. Kushman and R. Barzilay. 2013. Using semantic uni-
fication to generate regular expressions from natural
language. In Human Language Technology and North
American Association for Computational Linguistics
(HLT/NAACL), pages 826?836.
T. Kwiatkowski, L. Zettlemoyer, S. Goldwater, and
M. Steedman. 2010. Inducing probabilistic CCG
grammars from logical form with higher-order unifi-
cation. In Empirical Methods in Natural Language
Processing (EMNLP), pages 1223?1233.
T. Kwiatkowski, L. Zettlemoyer, S. Goldwater, and
M. Steedman. 2011. Lexical generalization in CCG
grammar induction for semantic parsing. In Empirical
Methods in Natural Language Processing (EMNLP),
pages 1512?1523.
N. Lao, T. Mitchell, and W. W. Cohen. 2011. Random
walk inference and learning in a large scale knowledge
base. In Empirical Methods in Natural Language Pro-
cessing (EMNLP).
T. Lei, F. Long, R. Barzilay, and M. Rinard. 2013.
From natural language specifications to program input
parsers. In Association for Computational Linguistics
(ACL).
P. Liang, M. I. Jordan, and D. Klein. 2011. Learning
dependency-based compositional semantics. In As-
1543
sociation for Computational Linguistics (ACL), pages
590?599.
P. Liang. 2013. Lambda dependency-based composi-
tional semantics. Technical report, ArXiv.
T. Lin, Mausam, and O. Etzioni. 2012. Entity link-
ing at web scale. In Knowledge Extraction Workshop
(AKBC-WEKEX).
Masaum, M. Schmitz, R. Bart, S. Soderland, and O. Et-
zioni. 2012. Open language learning for informa-
tion extraction. In Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP/CoNLL), pages 523?534.
C. Matuszek, N. FitzGerald, L. Zettlemoyer, L. Bo, and
D. Fox. 2012. A joint model of language and percep-
tion for grounded attribute learning. In International
Conference on Machine Learning (ICML).
R. McDonald, K. Crammer, and F. Pereira. 2005. Online
large-margin training of dependency parsers. In As-
sociation for Computational Linguistics (ACL), pages
91?98.
H. Poon. 2013. Grounded unsupervised semantic pars-
ing. In Association for Computational Linguistics
(ACL).
A. Popescu, O. Etzioni, and H. Kautz. 2003. Towards
a theory of natural language interfaces to databases.
In International Conference on Intelligent User Inter-
faces (IUI), pages 149?157.
S. Riedel, L. Yao, and A. McCallum. 2010. Model-
ing relations and their mentions without labeled text.
In Machine Learning and Knowledge Discovery in
Databases (ECML PKDD), pages 148?163.
M. Surdeanu, J. Tibshirani, R. Nallapati, and C. D. Man-
ning. 2012. Multi-instance multi-label learning for
relation extraction. In Empirical Methods in Natu-
ral Language Processing and Computational Natu-
ral Language Learning (EMNLP/CoNLL), pages 455?
465.
S. Tellex, T. Kollar, S. Dickerson, M. R. Walter, A. G.
Banerjee, S. J. Teller, and N. Roy. 2011. Understand-
ing natural language commands for robotic navigation
and mobile manipulation. In Association for the Ad-
vancement of Artificial Intelligence (AAAI).
C. Unger, L. Bhmann, J. Lehmann, A. Ngonga, D. Ger-
ber, and P. Cimiano. 2012. Template-based ques-
tion answering over RDF data. In World Wide Web
(WWW), pages 639?648.
Y. W. Wong and R. J. Mooney. 2007. Learning syn-
chronous grammars for semantic parsing with lambda
calculus. In Association for Computational Linguis-
tics (ACL), pages 960?967.
M. Yahya, K. Berberich, S. Elbassuoni, M. Ramanath,
V. Tresp, and G. Weikum. 2012. Natural language
questions for the web of data. In Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning (EMNLP/CoNLL), pages
379?390.
M. Zelle and R. J. Mooney. 1996. Learning to parse
database queries using inductive logic proramming. In
Association for the Advancement of Artificial Intelli-
gence (AAAI), pages 1050?1055.
L. S. Zettlemoyer and M. Collins. 2005. Learning to
map sentences to logical form: Structured classifica-
tion with probabilistic categorial grammars. In Uncer-
tainty in Artificial Intelligence (UAI), pages 658?666.
C. Zhang, R. Hoffmann, and D. S. Weld. 2012. Onto-
logical smoothing for relation extraction with minimal
supervision. In Association for the Advancement of
Artificial Intelligence (AAAI).
1544
Learning Dependency-Based
Compositional Semantics
Percy Liang?
University of California, Berkeley
Michael I. Jordan??
University of California, Berkeley
Dan Klein?
University of California, Berkeley
Suppose we want to build a system that answers a natural language question by representing its
semantics as a logical form and computing the answer given a structured database of facts. The
core part of such a system is the semantic parser that maps questions to logical forms. Semantic
parsers are typically trained from examples of questions annotated with their target logical forms,
but this type of annotation is expensive.
Our goal is to instead learn a semantic parser from question?answer pairs, where the logical
form is modeled as a latent variable. We develop a new semantic formalism, dependency-based
compositional semantics (DCS) and define a log-linear distribution over DCS logical forms. The
model parameters are estimated using a simple procedure that alternates between beam search
and numerical optimization. On two standard semantic parsing benchmarks, we show that our
system obtains comparable accuracies to even state-of-the-art systems that do require annotated
logical forms.
No rights reserved. This work was authored as part of the Contributor?s official duties as an Employee of
the United States Government and is therefore a work of the United States Government. In accordance with
17 U.S.C. 105, no copyright protection is available for such works under U.S. law.
1. Introduction
One of the major challenges in natural language processing (NLP) is building systems
that both handle complex linguistic phenomena and require minimal human effort. The
difficulty of achieving both criteria is particularly evident in training semantic parsers,
where annotating linguistic expressions with their associated logical forms is expensive
but until recently, seemingly unavoidable. Advances in learning latent-variable models,
however, have made it possible to progressively reduce the amount of supervision
? Computer Science Division, University of California, Berkeley, CA 94720, USA.
E-mail: pliang@cs.stanford.edu.
?? Computer Science Division and Department of Statistics, University of California, Berkeley, CA 94720,
USA. E-mail: jordan@cs.berkeley.edu.
? Computer Science Division, University of California, Berkeley, CA 94720, USA.
E-mail: klein@cs.berkeley.edu.
Submission received: 12 September 2011; revised submission received: 19 February 2012; accepted for
publication: 18 April 2012.
doi:10.1162/COLI a 00127
Computational Linguistics Volume 39, Number 2
required for various semantics-related tasks (Zettlemoyer and Collins 2005; Branavan
et al 2009; Liang, Jordan, and Klein 2009; Clarke et al 2010; Artzi and Zettlemoyer 2011;
Goldwasser et al 2011). In this article, we develop new techniques to learn accurate
semantic parsers from even weaker supervision.
We demonstrate our techniques on the concrete task of building a system to answer
questions given a structured database of facts; see Figure 1 for an example in the domain
of U.S. geography. This problem of building natural language interfaces to databases
(NLIDBs) has a long history in NLP, starting from the early days of artificial intelligence
with systems such as LUNAR (Woods, Kaplan, and Webber 1972), CHAT-80 (Warren
and Pereira 1982), and many others (see Androutsopoulos, Ritchie, and Thanisch [1995]
for an overview). We believe NLIDBs provide an appropriate starting point for semantic
parsing because they lead directly to practical systems, and they allow us to temporarily
sidestep intractable philosophical questions on how to represent meaning in general.
Early NLIDBs were quite successful in their respective limited domains, but because
these systems were constructed frommanually built rules, they became difficult to scale
up, both to other domains and to more complex utterances. In response, against the
backdrop of a statistical revolution in NLP during the 1990s, researchers began to build
systems that could learn from examples, with the hope of overcoming the limitations of
rule-based methods. One of the earliest statistical efforts was the CHILL system (Zelle
and Mooney 1996), which learned a shift-reduce semantic parser. Since then, there has
been a healthy line of work yielding increasingly more accurate semantic parsers by
using new semantic representations andmachine learning techniques (Miller et al 1996;
Zelle and Mooney 1996; Tang andMooney 2001; Ge andMooney 2005; Kate, Wong, and
Mooney 2005; Zettlemoyer and Collins 2005; Kate andMooney 2006;Wong andMooney
2006; Kate and Mooney 2007; Wong and Mooney 2007; Zettlemoyer and Collins 2007;
Kwiatkowski et al 2010, 2011).
Although statistical methods provided advantages such as robustness and portabil-
ity, however, their application in semantic parsing achieved only limited success. One
of the main obstacles was that these methods depended crucially on having examples
of utterances paired with logical forms, and this requires substantial human effort to
obtain. Furthermore, the annotators must be proficient in some formal language, which
drastically reduces the size of the annotator pool, dampening any hope of acquiring
enough data to fulfill the vision of learning highly accurate systems.
In response to these concerns, researchers have recently begun to explore the pos-
sibility of learning a semantic parser without any annotated logical forms (Clarke et al
Figure 1
The concrete objective: A system that answers natural language questions given a structured
database of facts. An example is shown in the domain of U.S. geography.
390
Liang, Jordan, and Klein Learning Dependency-Based Compositional Semantics
Figure 2
Our statistical methodology consists of two steps: (i) semantic parsing (p(z | x;?)): an utterance x
is mapped to a logical form z by drawing from a log-linear distribution parametrized by a
vector ?; and (ii) evaluation ([[z]]w): the logical form z is evaluated with respect to the world w
(database of facts) to deterministically produce an answer y. The figure also shows an example
configuration of the variables around the graphical model. Logical forms z are represented as
labeled trees. During learning, we are given w and (x, y) pairs (shaded nodes) and try to infer
the latent logical forms z and parameters ?.
2010; Artzi and Zettlemoyer 2011; Goldwasser et al 2011; Liang, Jordan, andKlein 2011).
It is in this vein that we develop our present work. Specifically, given a set of (x, y)
example pairs, where x is an utterance (e.g., a question) and y is the corresponding
answer, we wish to learn a mapping from x to y. What makes this mapping particularly
interesting is that it passes through a latent logical form z, which is necessary to capture
the semantic complexities of natural language. Also note that whereas the logical form
z was the end goal in much of earlier work on semantic parsing, for us it is just an
intermediate variable?a means towards an end. Figure 2 shows the graphical model
which captures the learning setting we just described: The question x, answer y, and
world/database w are all observed. We want to infer the logical forms z and the
parameters ? of the semantic parser, which are unknown quantities.
Although liberating ourselves from annotated logical forms reduces cost, it does
increase the difficulty of the learning problem. The core challenge here is program
induction: On each example (x, y), we need to efficiently search over the exponential
space of possible logical forms (programs) z and find ones that produce the target
answer y, a computationally daunting task. There is also a statistical challenge: How
do we parametrize the mapping from utterance x to logical form z so that it can be
learned from only the indirect signal y? To address these two challenges, we must first
discuss the issue of semantic representation. There are two basic questions here: (i) what
391
Computational Linguistics Volume 39, Number 2
should the formal language for the logical forms z be, and (ii) what are the compositional
mechanisms for constructing those logical forms?
The semantic parsing literature has considered many different formal languages
for representing logical forms, including SQL (Giordani and Moschitti 2009), Prolog
(Zelle and Mooney 1996; Tang and Mooney 2001), a simple functional query language
called FunQL (Kate, Wong, and Mooney 2005), and lambda calculus (Zettlemoyer and
Collins 2005), just to name a few. The construction mechanisms are equally diverse, in-
cluding synchronous grammars (Wong and Mooney 2007), hybrid trees (Lu et al 2008),
Combinatory Categorial Grammars (CCG) (Zettlemoyer and Collins 2005), and shift-
reduce derivations (Zelle and Mooney 1996). It is worth pointing out that the choice of
formal language and the construction mechanism are decisions which are really more
orthogonal than is often assumed?the former is concerned with what the logical forms
look like; the latter, with how to generate a set of possible logical forms compositionally
given an utterance. (How to score these logical forms is yet another dimension.)
Existing systems are rarely based on the joint design of the formal language and
the construction mechanism; one or the other is often chosen for convenience from
existing implementations. For example, Prolog and SQL have often been chosen as
formal languages for convenience in end applications, but they were not designed
for representing the semantics of natural language, and, as a result, the construction
mechanism that bridges the gap between natural language and formal language is
generally complex and difficult to learn. CCG (Steedman 2000) is quite popular in
computational linguistics (for example, see Bos et al [2004] and Zettlemoyer and Collins
[2005]). In CCG, logical forms are constructed compositionally using a small handful
of combinators (function application, function composition, and type raising). For a
wide range of canonical examples, CCG produces elegant, streamlined analyses, but
its success really depends on having a good, clean lexicon. During learning, there is
often a great amount of uncertainty over the lexical entries, which makes CCG more
cumbersome. Furthermore, in real-world applications, we would like to handle disflu-
ent utterances, and this further strains CCG by demanding either extra type-raising
rules and disharmonic combinators (Zettlemoyer and Collins 2007) or a proliferation of
redundant lexical entries for each word (Kwiatkowski et al 2010).
To cope with the challenging demands of program induction, we break away from
tradition in favor of a new formal language and construction mechanism, which we call
dependency-based compositional semantics (DCS). The guiding principle behind DCS
is to provide a simple and intuitive framework for constructing and representing logical
forms. Logical forms in DCS are tree structures calledDCS trees. The motivation is two-
fold: (i) DCS trees are meant to parallel syntactic dependency trees, which facilitates
parsing; and (ii) a DCS tree essentially encodes a constraint satisfaction problem, which
can be solved efficiently using dynamic programming to obtain the denotation of a DCS
tree. In addition, DCS provides a mark?execute construct, which provides a uniform
way of dealing with scope variation, a major source of trouble in any semantic for-
malism. The construction mechanism in DCS is a generalization of labeled dependency
parsing, which leads to simple and natural algorithms. To a linguist, DCS might appear
unorthodox, but it is important to keep in mind that our primary goal is effective
program induction, not necessarily to model new linguistic phenomena in the tradition
of formal semantics.
Armed with our new semantic formalism, DCS, we then define a discriminative
probabilistic model, which is depicted in Figure 2. The semantic parser is a log-linear
distribution over DCS trees z given an utterance x. Notably, z is unobserved, and we in-
stead observe only the answer y, which is obtained by evaluating z on a world/database
392
Liang, Jordan, and Klein Learning Dependency-Based Compositional Semantics
w. There are an exponential number of possible trees z, and usually dynamic program-
ming can be used to efficiently search over trees. However, in our learning setting
(independent of the semantic formalism), we must enforce the global constraint that
z produces y. This makes dynamic programming infeasible, so we use beam search
(though dynamic programming is still used to compute the denotation of a fixed DCS
tree). We estimate the model parameters with a simple procedure that alternates be-
tween beam search and optimizing a likelihood objective restricted to those beams. This
yields a natural bootstrapping procedure in which learning and search are integrated.
We evaluated our DCS-based approach on two standard benchmarks, GEO, a U.S.
geography domain (Zelle and Mooney 1996), and JOBS, a job queries domain (Tang and
Mooney 2001). On GEO, we found that our system significantly outperforms previous
work that also learns from answers instead of logical forms (Clarke et al 2010). What
is perhaps a more significant result is that our system obtains comparable accuracies to
state-of-the-art systems that do rely on annotated logical forms. This demonstrates the
viability of training accurate systems with much less supervision than before.
The rest of this article is organized as follows: Section 2 introduces DCS, our new
semantic formalism. Section 3 presents our probabilistic model and learning algorithm.
Section 4 provides an empirical evaluation of our methods. Section 5 situates this work
in a broader context, and Section 6 concludes.
2. Representation
In this section, we present the main conceptual contribution of this work, dependency-
based compositional semantics (DCS), using the U.S. geography domain (Zelle and
Mooney 1996) as a running example. To do this, we need to define the syntax and
semantics of the formal language. The syntax is defined in Section 2.2 and is quite
straightforward: The logical forms in the formal language are simply trees, which we
callDCS trees. In Section 2.3, we give a type-theoretic definition ofworlds (also known
as databases or models) with respect to which we can define the semantics of DCS trees.
The semantics, which is the heart of this article, contains two main ideas: (i) using
trees to represent logical forms as constraint satisfaction problems or extensions thereof,
and (ii) dealing with cases when syntactic and semantic scope diverge (e.g., for general-
ized quantification and superlative constructions) using a new construct which we call
mark?execute. We start in Section 2.4 by introducing the semantics of a basic version
of DCS which focuses only on (i) and then extend it to the full version (Section 2.5) to
account for (ii).
Finally, having fully specified the formal language, we describe a construction
mechanism for mapping a natural language utterance to a set of candidate DCS trees
(Section 2.6).
2.1 Notation
Operations on tuples will play a prominent role in this article. For a sequence1 v =
(v1, . . . , vk), we use |v| = k to denote the length of the sequence. For two sequences u
and v, we use u+ v = (u1, . . . ,u|u|, v1, . . . , v|v|) to denote their concatenation.
1 We use the term sequence to refer to both tuples (v1, . . . , vk ) and arrays [v1, . . . , vk]. For our purposes, there
is no functional difference between tuples and arrays; the distinction is convenient when we start to talk
about arrays of tuples.
393
Computational Linguistics Volume 39, Number 2
For a sequence of positive indices i = (i1, . . . , im), let vi = (vi1 , . . . , vim ) consist of the
components of v specified by i; we call vi the projection of v onto i. We use negative
indices to exclude components: v?i = (v(1,...,|v|)\i). We can also combine sequences of
indices by concatenation: vi,j = vi + vj. Some examples: if v = (a, b, c, d), then v2 = b,
v3,1 = (c, a), v?3 = (a, b, d), v3,?3 = (c, a, b, d).
2.2 Syntax of DCS Trees
The syntax of the DCS formal language is built from two ingredients, predicates and
relations:
 Let P be a set of predicates. We assume that P contains a special null
predicate ?, domain-independent predicates (e.g., count, <, >, and =), and
domain-specific predicates (for the U.S. geography domain, state, river,
border, etc.). Right now, think of predicates as just labels, which have yet
to receive formal semantics.
 LetR be the set of relations. Note that unlike the predicates P , which can
vary across domains, the relationsR are fixed. The full set of relations are
shown in Table 1. For now, just think of relations as labels?their semantics
will be defined in Section 2.4.
The logical forms in DCS are called DCS trees. A DCS tree is a directed rooted tree
in which nodes are labeled with predicates and edges are labeled with relations; each
node also maintains an ordering over its children. Formally:
Definition 1 (DCS trees)
Let Z be the set of DCS trees, where each z ? Z consists of (i) a predicate z.p ? P and (ii)
a sequence of edges z.e = (z.e1, . . . , z.em). Each edge e consists of a relation e.r ? R (see
Table 1) and a child tree e.c ? Z .
We will either draw a DCS tree graphically or write it compactly as ?p; r1 :c1; . . . ; rm :cm?
where p is the predicate at the root node and c1, . . . , cm are its m children connected via
edges labeled with relations r1, . . . , rm, respectively. Figure 3(a) shows an example of a
DCS tree expressed using both graphical and compact formats.
Table 1
Possible relations that appear on edges of DCS trees. Basic DCS uses only the join and aggregate
relations; the full version of DCS uses all of them.
RelationsR
Name Relation Description of semantic function
join
j
j? for j, j
? ? {1, 2, . . . } j-th component of parent = j?-th component of child
aggregate ? parent = set of feasible values of child
extract E mark node for extraction
quantify Q mark node for quantification, negation
compare C mark node for superlatives, comparatives
execute Xi for i ? {1, 2 . . . }? process marked nodes specified by i
394
Liang, Jordan, and Klein Learning Dependency-Based Compositional Semantics
Figure 3
(a) An example of a DCS tree (written in both the mathematical and graphical notations). Each
node is labeled with a predicate, and each edge is labeled with a relation. (b) A DCS tree zwith
only join relations encodes a constraint satisfaction problem, represented here as a lambda
calculus formula. For example, the root node label city corresponds to a unary predicate
city(c), the right child node label loc corresponds to a binary predicate loc() (where  is a
pair), and the edge between them denotes the constraint c1 = 1 (where the indices correspond to
the two labels on the edge). (c) The denotation of z is the set of feasible values for the root node.
A DCS tree is a logical form, but it is designed to look like a syntactic dependency
tree, only with predicates in place of words. As we?ll see over the course of this section,
it is this transparency between syntax and semantics provided by DCS which leads to a
simple and streamlined compositional semantics suitable for program induction.
2.3 Worlds
In the context of question answering, the DCS tree is a formal specification of the
question. To obtain an answer, we still need to evaluate the DCS tree with respect to
a database of facts (see Figure 4 for an example). We will use the term world to refer
Figure 4
We use the domain of U.S. geography as a running example. The figure presents an example of a
world w (database) in this domain. A world maps each predicate to a set of tuples. For example,
the depicted world wmaps the predicate loc to the set of pairs of places and their containers.
Note that functions (e.g., population) are also represented as predicates for uniformity. Some
predicates (e.g., count) map to an infinite number of tuples and would be represented implicitly.
395
Computational Linguistics Volume 39, Number 2
to this database (it is sometimes also called a model, but we avoid this term to avoid
confusion with the probabilistic model for learning that we will present in Section 3.1).
Throughout this work, we assume the world is fully observed and fixed, which is a
realistic assumption for building natural language interfaces to existing databases, but
questionable for modeling the semantics of language in general.
2.3.1 Types and Values. To define a world, we start by constructing a set of values V .
The exact set of values depends on the domain (we will continue to use U.S. geog-
raphy as a running example). Briefly, V contains numbers (e.g., 3 ? V), strings (e.g.,
Washington ? V), tuples (e.g., (3,Washington) ? V), sets (e.g., {3,Washington} ? V), and
other higher-order entities.
To be more precise, we construct V recursively. First, define a set of primitive values
V, which includes the following:
 Numeric values. Each value has the form x : t ? V, where x ? R is a real
number and t ? {number, ordinal, percent, length, . . . } is a tag. The tag
allows us to differentiate 3, 3rd, 3%, and 3 miles?this will be important in
Section 2.6.3. We simply write x for the value x :number.
 Symbolic values. Each value has the form x : t ? V, where x is a string (e.g.,
Washington) and t ? {string, city, state, river, . . . } is a tag. Again, the
tag allows us to differentiate, for example, the entitiesWashington :city
andWashington :state.
Now we build the full set of values V from the primitive values V. To define V , we
need a bit more machinery: To avoid logical paradoxes, we construct V in increasing
order of complexity using types (see Carpenter [1998] for a similar construction). The
casual reader can skip this construction without losing any intuition.
Define the set of types T to be the smallest set that satisfies the following properties:
1. The primitive type  ? T ;
2. The tuple type (t1, . . . , tk) ? T for each k ? 0 and each non-tuple type
ti ? T for i = 1, . . . , k; and
3. The set type {t} ? T for each tuple type t ? T .
Note that {}, {{}}, and (()) are not valid types.
For each type t ? T , we construct a corresponding set of values Vt:
1. For the primitive type t = , the primitive values V have already been
specified. Note that these types are rather coarse: Primitive values with
different tags are considered to have the same type .
2. For a tuple type t = (t1, . . . , tk), Vt is the cross product of the values of its
component types:
Vt = {(v1, . . . , vk) : ?i, vi ? Vti} (1)
396
Liang, Jordan, and Klein Learning Dependency-Based Compositional Semantics
3. For a set type t = {t?}, Vt contains all subsets of its element type t?:
Vt = {s : s ? Vt?} (2)
With this last condition, we ensure that all elements of a set must have the
same type. Note that a set is still allowed to have values with different tags
(e.g., {(Washington :city), (Washington :state)} is a valid set, which might
denote the semantics of the utterance things named Washington). Another
distinction is that types are domain-independent whereas tags tend to be
more domain-specific.
Let V = ?t?T Vt be the set of all possible values.
A world maps each predicate to its semantics, which is a set of tuples (see Figure 4
for an example). First, let TTUPLE ? T be the tuple types, which are the ones of the form
(t1, . . . , tk) for some k. Let V{TUPLE} denote all the sets of tuples (with the same type):
V{TUPLE}
def
=
?
t?TTUPLE
V{t} (3)
Now we define a world formally.
Definition 2 (World)
A world w : P ? V{TUPLE} ? {V} is a function that maps each non-null predicate p ?
P\{?} to a set of tuples w(p) ? V{TUPLE} and maps the null predicate ? to the set of all
values (w(?) = V).
For a set of tuplesAwith the same arity, let ARITY(A) = |x|, where x ? A is arbitrary;
if A is empty, then ARITY(A) is undefined. Now for a predicate p ? P and world w,
define ARITYw(p), the arity of predicate pwith respect to w, as follows:
ARITYw(p) =
{
1 if p = ?
ARITY(w(p)) if p = ?
(4)
The null predicate has arity 1 by fiat; the arity of a non-null predicate p is inherited from
the tuples in w(p).
Remarks. In higher-order logic and lambda calculus, we construct function types and
values, whereas in DCS, we construct tuple types and values. The two are equivalent
in representational power, but this discrepancy does point out the fact that lambda
calculus is based on function application, whereas DCS, as we will see, is based on
declarative constraints. The set type {(, )} in DCS corresponds to the function type
? (? bool). In DCS, there is no explicit bool type?it is implicitly represented by
using sets.
2.3.2 Examples. The world w maps each domain-specific predicate to a set of tuples
(usually a finite set backed by a database). For the U.S. geography domain, w has a
397
Computational Linguistics Volume 39, Number 2
predicate that maps to the set of U.S. states (state), another predicate that maps to the
set of pairs of entities and where they are located (loc), and so on:
w(state) = {(California :state), (Oregon :state), . . . } (5)
w(loc) = {(San Francisco :city,California :state), . . . } (6)
. . . (7)
To shorten notation, we use state abbreviations (e.g., CA = California :state).
The world w also specifies the semantics of several domain-independent predicates
(think of these as helper functions), which usually correspond to an infinite set of tuples.
Functions are represented in DCS by a set of input?output pairs. For example, the
semantics of the countt predicate (for each type t ? T ) contains pairs of sets S and their
cardinalities |S|:
w(countt) = {(S, |S|) : S ? V{(t)}} ? V{({(t)},)} (8)
As another example, consider the predicate averaget (for each t ? T ), which takes a
set of key?value pairs (with keys of type t) and returns the average value. For notational
convenience, we treat an arbitrary set of pairs S as a set-valued function: We let S1 = {x :
(x, y) ? S} denote the domain of the function, and abusing notation slightly, we define
the function S(x) = {y : (x, y) ? S} to be the set of values y that co-occur with the given
x. The semantics of averaget contains pairs of sets and their averages:
w(averaget) =
?
?
?
(S, z) : S ? V{(t,)}, z = |S1|
?1
?
x?S1
?
?|S(x)|?1
?
y?S(x)
y
?
?
?
?
?
? V{({(t,)},)}
(9)
Similarly, we can define the semantics of argmint and argmaxt, which each takes a set of
key?value pairs and returns the keys that attain the smallest (largest) value:
w(argmint) =
{
(S, z) : S ? V{(t,)}, z ? argmin
x?S1
min S(x)
}
? V{({(t,)},t)} (10)
w(argmaxt) =
{
(S, z) : S ? V{(t,)}, z ? argmax
x?S1
max S(x)
}
? V{({(t,)},t)} (11)
The extra min and max is needed because S(x) could contain more than one value. We
also impose that w(argmint) contains only (S, z) such that y is numeric for all (x, y) ? S;
thus argmint denotes a partial function (same for argmaxt).
These helper functions are monomorphic: For example, countt only computes
cardinalities of sets of type {(t)}. In practice, we mostly operate on sets of primitives
(t = ). To reduce notation, we omit t to refer to this version: count = count, average =
average, and so forth.
398
Liang, Jordan, and Klein Learning Dependency-Based Compositional Semantics
2.4 Semantics of DCS Trees without Mark?Execute (Basic Version)
The semantics or denotation of a DCS tree z with respect to a world w is denoted zw.
First, we define the semantics of DCS trees with only join relations (Section 2.4.1). In
this case, a DCS tree encodes a constraint satisfaction problem (CSP); this is important
because it highlights the constraint-based nature of DCS and also naturally leads to a
computationally efficient way of computing denotations (Section 2.4.2). We then allow
DCS trees to have aggregate relations (Section 2.4.3). The fragment of DCS which has
only join and aggregate relations is called basic DCS.
2.4.1 Basic DCS Trees as Constraint Satisfaction Problems. Let z be a DCS tree with only
join relations on its edges. In this case, z encodes a CSP as follows: For each node x in z,
the CSP has a variable with value a(x); the collection of these values is referred to as an
assignment a. The predicates and relations of z introduce constraints:
1. a(x) ? w(p) for each node x labeled with predicate p ? P ; and
2. a(x)j = a(y)j? for each edge (x, y) labeled with
j
j? ? R, which says that the
j-th component of a(x) must equal the j?-th component of a(y).
We say that an assignment a is feasible if it satisfies these two constraints. Next, for a node
x, defineV(x) = {a(x) : assignment a is feasible} as the set of feasible values for x?these
are the ones that are consistent with at least one feasible assignment. Finally, we define
the denotation of the DCS tree z with respect to the world w to be zw = V(x0), where
x0 is the root node of z.
Figure 3(a) shows an example of a DCS tree. The corresponding CSP has four vari-
ables c,m, , s.2 In Figure 3(b), we have written the equivalent lambda calculus formula.
The non-root nodes are existentially quantified, the root node c is ?-abstracted, and
all constraints introduced by predicates and relations are conjoined. The ?-abstraction
of c represents the fact that the denotation is the set of feasible values for c (note the
equivalence between the Boolean function ?c.p(c) and the set {c : p(c)}).
Remarks. Note that CSPs only allow existential quantification and conjunction. Why
did we choose this particular logical subset as a starting point, rather than allowing
universal quantification, negation, or disjunction? There seems to be something fun-
damental about this subset, which also appears in Discourse Representation Theory
(DRT) (Kamp and Reyle 1993; Kamp, van Genabith, and Reyle 2005). Briefly, logical
forms in DRT are called Discourse Representation Structures (DRSs), each of which
contains (i) a set of existentially quantified discourse referents (variables), (ii) a set of
conjoined discourse conditions (constraints), and (iii) nested DRSs. If we exclude nested
DRSs, a DRS is exactly a CSP.3 The default existential quantification and conjunction are
quite natural for modeling cross-sentential anaphora: New variables can be added to
2 Technically, the node is c and the variable is a(c), but we use c to denote the variable to simplify notation.
3 Unlike the CSPs corresponding to DCS trees, the CSPs corresponding to DRSs need not be
tree-structured, though economical DRT (Bos 2009) imposes a tree-like restriction on DRSs for
computational reasons.
399
Computational Linguistics Volume 39, Number 2
a DRS and connected to other variables. Indeed, DRT was originally motivated by these
phenomena (see Kamp and Reyle [1993] for more details).4
Tree-structured CSPs can capture unboundedly complex recursive structures?such
as cities in states that border states that have rivers that. . . . Trees are limited, however, in
that they are unable to capture long-distance dependencies such as those arising from
anaphora. For example, in the phrase a state with a river that traverses its capital, its binds
to state, but this dependence cannot be captured in a tree structure. A solution is
to simply add an edge between the its node and the state node that forces the two
nodes to have the same value. The result is still a well-defined CSP, though not a tree-
structured one. The situation would become trickier if we were to integrate the other
relations (aggregate, mark, and execute). We might be able to incorporate some ideas
from Hybrid Logic Dependency Semantics (Baldridge and Kruijff 2002; White 2006),
given that hybrid logic extends the tree structures of modal logic with nominals, thereby
allowing a node to freely reference other nodes. In this article, however, we will stick to
trees and leave the full exploration of non-trees for future work.
2.4.2 Computation of Join Relations. So far, we have given a declarative definition of the
denotation zw of a DCS tree z with only join relations. Now we will show how to
compute zw efficiently. Recall that the denotation is the set of feasible values for the
root node. In general, finding the solution to a CSP is NP-hard, but for trees, we can
exploit dynamic programming (Dechter 2003). The key is that the denotation of a tree
depends on its subtrees only through their denotations:

?
p;
j1
j?1
:c1; ? ? ? ;
jm
j?m
:cm
?

w
= w(p) ?
m
?
i=1
{v : vji = tj?i , t ? ciw} (12)
On the right-hand side of Equation (12), the first termw(p) is the set of values that satisfy
the node constraint, and the second term consists of an intersection across all m edges
of {v : vji = tj?i , t ? ciw}, which is the set of values v which satisfy the edge constraint
with respect to some value t for the child ci.
To further flesh out this computation, we express Equation (12) in terms of two
operations: join and project. Join takes a cross product of two sets of tuples and retains
the resulting tuples that match the join constraint:
A j,j? B = {u+ v : u ? A, v ? B,uj = vj?} (13)
Project takes a set of tuples and retains a fixed subset of the components:
A[i] = {vi : v ? A} (14)
The denotation in Equation (12) can now be expressed in terms of these join and project
operations:

?
p;
j1
j?1
:c1; ? ? ? ;
jm
j?m
:cm
?

w
= ((w(p) j1,j?1 c1w)[i] ? ? ? jm,j?m cmw)[i] (15)
4 DRT started the dynamic semantics tradition where meanings are context-change potentials, a natural
way to capture anaphora. The DCS formalism presented here does not deal with anaphora, so we give it
a purely static semantics.
400
Liang, Jordan, and Klein Learning Dependency-Based Compositional Semantics
where i = (1, . . . , ARITYw(p)). Projecting onto i retains only components corresponding
to p.
The time complexity for computing the denotation of a DCS tree zw scales linearly
with the number of nodes, but there is also a dependence on the cost of performing the
join and project operations. For details on howwe optimize these operations and handle
infinite sets of tuples (for predicates such as count), see Liang (2011).
The denotation of DCS trees is defined in terms of the feasible values of a CSP, and
the recurrence in Equation (15) is only one way of computing this denotation. In light of
the extensions to come, however, we now consider Equation (15) as the actual definition
rather than just a computational mechanism. It will still be useful to refer to the CSP in
order to access the intuition of using declarative constraints.
2.4.3 Aggregate Relation. Thus far, we have focused on DCS trees that only use join
relations, which are insufficient for capturing higher-order phenomena in language. For
example, consider the phrase number of major cities. Suppose that number corresponds
to the count predicate, and that major cities maps to the DCS tree ?city; 11 :?major??. We
cannot simply join countwith the root of this DCS tree because count needs to be joined
with the set of major cities (the denotation of ?city; 11 :?major??), not just a single city.
We therefore introduce the aggregate relation (?) that takes a DCS subtree and
reifies its denotation so that it can be accessed by other nodes in its entirety. Consider a
tree ??;? :c?, where the root is connected to a child c via ?. The denotation of the root is
simply the singleton set containing the denotation of c:
??;? :c?w = {(cw)} (16)
Figure 5(a) shows the DCS tree for our running example. The denotation of the
middle node is {(s)}, where s is all major cities. Everything above this node is an
ordinary CSP: s constrains the count node, which in turns constrains the root node to
|s|. Figure 5(b) shows another example of using the aggregate relation ?. Here, the node
right above ? is constrained to be a set of pairs of major cities and their populations.
The average predicate then computes the desired answer.
To represent logical disjunction in natural language, we use the aggregate relation
and two predicates, union and contains, which are defined in the expected way:
w(union) = {(A,B,C) : C = A ? B,A ? V{},B ? V{}} (17)
w(contains) = {(A, x) : x ? A,A ? V{}} (18)
where A,B,C ? V{} are sets of primitive values (see Section 2.3.1). Figure 5(c) shows
an example of a disjunctive construction: We use the aggregate relations to construct
two sets, one containing Oregon, and the other containing states bordering Oregon. We
take the union of these two sets; contains takes the set and reads out an element, which
then constrains the city node.
Remarks. A DCS tree that contains only join and aggregate relations can be viewed as
a collection of tree-structured CSPs connected via aggregate relations. The tree struc-
ture still enables us to compute denotations efficiently based on the recurrences in
Equations (15) and (16).
Recall that a DCS tree with only join relations is a DRS without nested DRSs. The
aggregate relation corresponds to the abstraction operator in DRT and is one way of
401
Computational Linguistics Volume 39, Number 2
Figure 5
Examples of DCS trees that use the aggregate relation (?) to (a) compute the cardinality of a set,
(b) take the average over a set, (c) represent a disjunction over two conditions. The aggregate
relation sets the parent node deterministically to the denotation of the child node. Nodes with
the special null predicate ? are represented as empty nodes.
making nested DRSs. It turns out that the abstraction operator is sufficient to obtain
the full representational power of DRT, and subsumes generalized quantification and
disjunction constructs in DRT. By analogy, we use the aggregate relation to handle
disjunction (Figure 5(c)) and generalized quantification (Section 2.5.6).
DCS restricted to join relations is less expressive than first-order logic because it
does not have universal quantification, negation, and disjunction. The aggregate rela-
tion is analogous to lambda abstraction, and in basic DCS we use the aggregate relation
to implement those basic constructs using higher-order predicates such as not, every,
and union. We can also express logical statements such as generalized quantification,
which go beyond first-order logic.
2.5 Semantics of DCS Trees with Mark?Execute (Full Version)
Basic DCS includes two types of relations, join and aggregate, but it is already quite
expressive. In general, however, it is not enough just to be able to express the meaning
of a sentence using some logical form; we must be able to derive the logical form
compositionally and simply from the sentence.
Consider the superlative constructionmost populous city, which has a basic syntactic
dependency structure shown in Figure 6(a). Figure 6(b) shows that we can in principle
402
Liang, Jordan, and Klein Learning Dependency-Based Compositional Semantics
Figure 6
Two semantically equivalent DCS trees are shown in (b) and (c). The DCS tree in (b), which uses
the join and aggregate relations in the basic DCS, does not align well with the syntactic structure
of most populous city (a), and thus is undesirable. The DCS tree in (c), by using the mark?execute
construct, aligns much better, with city rightfully dominating its modifiers. The full version of
DCS allows us to construct (c), which is preferable to (b).
already use a DCS tree with only join and aggregate relations to express the correct
semantics of the superlative construction. Note, however, that the two structures are
quite divergent?the syntactic head is city and the semantic head is argmax. This diver-
gence runs counter to a principal desideratum of DCS, which is to create a transparent
interface between coarse syntax and semantics.
In this section, we introduce mark and execute relations, which will allow us to
use the DCS tree in Figure 6(c) to represent the semantics associated with Figure 6(a);
these two are more similar than (a) and (b). The focus of this section is on this mark?
execute construct?usingmark and execute relations to give proper semantically scoped
denotations to syntactically scoped tree structures.
The basic intuition of the mark?execute construct is as follows: We mark a node
low in the tree with a mark relation; then, higher up in the tree, we invoke it with a
corresponding execute relation (Figure 7). For our example in Figure 6(c), we mark the
population node, which puts the child argmax in a temporary store; when we execute
the city node, we fetch the superlative predicate argmax from the store and invoke it.
This divergence between syntactic and semantic scope arises in other linguistic
contexts besides superlatives, such as quantification and negation. In each of these
cases, the general template is the same: A syntactic modifier low in the tree needs to
have semantic force higher in the tree. A particularly compelling case of this divergence
happenswith quantifier scope ambiguity (e.g., Some river traverses every city5), where the
5 The two meanings are: (i) there is a river x such that x traverses every city; and (ii) for every city x, some
river traverses x.
403
Computational Linguistics Volume 39, Number 2
Figure 7
The template for the mark?execute construct. A mark relation (one of E, Q, C) ?stores? the
modifier. Then an execute relation (of the form Xi for indices i) higher up ?recalls? the
modifier and applies it at the desired semantic point.
quantifiers appear in fixed syntactic positions, but the surface and inverse scope read-
ings correspond to different semantically scoped denotations. Analogously, a single syn-
tactic structure involving superlatives can also yield two different semantically scoped
denotations?the absolute and relative readings (e.g., state bordering the largest state6).
The mark?execute construct provides a unified framework for dealing all these forms
of divergence between syntactic and semantic scope. See Figures 8 and 9 for concrete
examples of this construct.
2.5.1 Denotations.We now formalize the mark?execute construct. We saw that the mark?
execute construct appears to act non-locally, putting things in a store and retrieving
them later. This means that if we want the denotation of a DCS tree to only depend
on the denotations of its subtrees, the denotations need to contain more than the set of
feasible values for the root node, as was the case for basic DCS. We need to augment de-
notations to include information about all marked nodes, because these can be accessed
by an execute relation higher up in the tree.
More specifically, let z be a DCS tree and d = zw be its denotation. The denotation
d consists of n columns. The first column always corresponds to the root node of z,
and the rest of the columns correspond to non-root marked nodes in z. In the example
in Figure 10, there are two columns, one for the root state node and the other for size
node, which is marked by C. The columns are ordered according to a pre-order traversal
of z, so column 1 always corresponds to the root node. The denotation d contains a set of
arrays d.A, where each array represents a feasible assignment of values to the columns
of d; note that we quantify over non-marked nodes, so they do not correspond to any
column in the denotation. For example, in Figure 10, the first array in d.A corresponds to
assigning (OK) to the state node (column 1) and (TX, 2.7e5) to the size node (column 2).
If there are no marked nodes, d.A is basically a set of tuples, which corresponds to a
denotation in basic DCS. For each marked node, the denotation d also maintains a store
6 The two meanings are: (i) a state that borders Alaska (which is the largest state); and (ii) a state with the
highest score, where the score of a state x is the maximum size of any state that x borders (Alaska is
irrelevant here because no states border it).
404
Liang, Jordan, and Klein Learning Dependency-Based Compositional Semantics
Figure 8
Examples of DCS trees that use the mark?execute construct with the E and Q mark relations.
(a) The head verb borders, which needs to be returned, has a direct object statesmodified by
which. (b) The quantifier no is syntactically dominated by state but needs to take wider scope.
(c) Two quantifiers yield two possible readings; we build the same basic structure, marking
both quantifiers; the choice of execute relation (X12 versus X21) determines the reading. (d) We
use two mark relations, Q on river for the negation, and E on city to force the quantifier to be
computed for each value of city.
with information to be retrieved when that marked node is executed. A store ? for a
marked node contains the following: (i) the mark relation ?.r (C in the example), (ii) the
base denotation ?.b, which essentially corresponds to denotation of the subtree rooted at
the marked node excluding the mark relation and its subtree (?size?w in the example),
and (iii) the denotation of the child of the mark relation (?argmax?w in the example).
The store of any unmarked nodes is always empty (? = ?).
Definition 3 (Denotations)
Let D be the set of denotations, where each denotation d ? D consists of
 a set of arrays d.A, where each array a = [a1, . . . , an] ? d.A is a sequence of
n tuples for some n ? 0; and
405
Computational Linguistics Volume 39, Number 2
Figure 9
Examples of DCS trees that use the mark?execute construct with the E and C relation. (a,b,c)
Comparatives and superlatives are handled as follows: For each value of the node marked
by E, we compute a number based on the node marked by C; based on this information,
a subset of the values is selected as the possible values of the root node. (d) Analog of quantifier
scope ambiguity for superlatives: The placement of the execute relation determines an absolute
versus relative reading. (e) Interaction between a quantifier and a superlative: The lower execute
relation computes the largest city for each state; the second execute relation invokes most and
enforces that the major constraint holds for the majority of states.
406
Liang, Jordan, and Klein Learning Dependency-Based Compositional Semantics
Figure 10
Example of the denotation for a DCS tree (with the compare relation C). This denotation has two
columns, one for each active node?the root node state and the marked node size.
 a sequence of n stores d.? = (d.?1, . . . , d.?n), where each store ? contains a
mark relation ?.r ? {E, Q, C, ?}, a base denotation ?.b ? D ? {?}, and a
child denotation ?.c ? D ? {?}.
Note that denotations are formally defined without reference to DCS trees (just as sets
of tuples were in basic DCS), but it is sometimes useful to refer to the DCS tree that
generates that denotation.
For notational convenience, we write d as ??A; (r1, b1, c1); . . . ; (rn, bn, cn)??. Also let
d.ri = d.?i.r, d.bi = d.?i.b, and d.ci = d.?i.c. Let d{?i = x} be the denotation which is
identical to d, except with d.?i = x; d{ri = x}, d{bi = x}, and d{ci = x} are defined
analogously. We also define a project operation for denotations: ??A;???[i] def= ??{ai : a ?
A};?i??. Extending this notation further, we use ? to denote the indices of the non-initial
columns with empty stores (i > 1 such that d.?i = ?). We can then use d[??] to represent
projecting away the non-initial columns with empty stores. For the denotation d in
Figure 10, d[1] keeps column 1, d[??] keeps both columns, and d[2,?2] swaps the two
columns.
In basic DCS, denotations are sets of tuples, which works quite well for repre-
senting the semantics of wh-questions such as What states border Texas? But what about
polar questions such as Does Louisiana border Texas? The denotation should be a simple
Boolean value, which basic DCS does not represent explicitly. Using our new deno-
tations, we can represent Boolean values explicitly using zero-column structures: true
corresponds to a singleton set containing just the empty array (dT = ??{[ ]}??) and false
is the empty set (dF = ?????).
Having described denotations as n-column structures, we now give the formal
mapping from DCS trees to these structures. As in basic DCS, this mapping is defined
recursively over the structure of the tree. We have a recurrence for each case (the first
line is the base case, and each of the others handles a different edge relation):
?p?w = ??{[v] : v ? w(p)}; ??? [base case] (19)

?
p; e;
j
j? :c
?

w
= ?p; e?w 
??
j,j?
cw [join] (20)
?p; e;? :c?w = ?p; e?w 
??
?,? ?
(
cw
)
[aggregate] (21)
407
Computational Linguistics Volume 39, Number 2
?p; e; Xi :c?w = ?p; e?w 
??
?,? xi(cw) [execute] (22)
?p; e; E :c?w =M(?p; e?w, E, cw) [extract] (23)
?p; e; C :c?w =M(?p; e?w, C, cw) [compare] (24)
?p; Q :c; e?w =M(?p; e?w, Q, cw) [quantify] (25)
We define the operations ??
j,j?
,?,Xi, andM in the remainder of this section.
2.5.2 Base Case. Equation (19) defines the denotation for a DCS tree z with a single node
with predicate p. The denotation of z has one column whose arrays correspond to the
tuples w(p); the store for that column is empty.
2.5.3 Join Relations. Equation (20) defines the recurrence for join relations. On the left-
hand side,
?
p; e;
j
j? :c
?
is a DCS tree with p at the root, a sequence of edges e followed by
a final edge with relation
j
j? connected to a child DCS tree c. On the right-hand side, we
take the recursively computed denotation of ?p; e?, the DCS tree without the final edge,
and perform a join-project-inactive operation (notated ??
j,j?
) with the denotation of the
child DCS tree c.
The join-project-inactive operation joins the arrays of the two denotations (this is
the core of the join operation in basic DCS?see Equation (13)), and then projects away
the non-initial empty columns:7
??A;??? ??
j,j?
??A?;???? = ??A??;?+ ????[??],where (26)
A?? = {a+ a? : a ? A, a? ? A?, a1j = a?1j?}
We concatenate all arrays a ? A with all arrays a? ? A? that satisfy the join condition
a1j = a
?
1j? . The sequences of stores are simply concatenated: (?+ ?
?). Finally, any non-
initial columns with empty stores are projected away by applying ?[??].
Note that the join works on column 1; the other columns are carried along for the
ride. As another piece of convenient notation, we use ? to represent all components, so
???,? imposes the join condition that the entire tuple has to agree (a1 = a
?
1).
2.5.4 Aggregate Relations. Equation (21) defines the recurrence for aggregate relations.
Recall that in basic DCS, aggregate (16) simply takes the denotation (a set of tuples) and
puts it into a set. Now, the denotation is not just a set, so we need to generalize this
operation. Specifically, the aggregate operation applied to a denotation forms a set out
of the tuples in the first column for each setting of the rest of the columns:
? (??A;???) = ??A? ? A??;??? (27)
A? = {[S(a), a2, . . . , an] : a ? A}
S(a) = {a?1 : [a
?
1, a2, . . . , an] ? A}
A?? = {[?, a2, . . . , an] : ?i ? {2, . . . ,n}, [ai] ? ?i.b.A[1],??a1, a ? A}
7 The join and project operations are taken from relational algebra.
408
Liang, Jordan, and Klein Learning Dependency-Based Compositional Semantics
The aggregate operation takes the set of arrays A and produces two sets of arrays, A?
andA??, which are unioned (note that the stores do not change). The setA? is the one that
first comes to mind: For every setting of a2, . . . , an, we construct S(a), the set of tuples a
?
1
in the first column which co-occur with a2, . . . , an in A.
There is another case, however: what happens to settings of a2, . . . , an that do not
co-occur with any value of a?1 in A? Then, S(a) = ?, but note that A
? by construction will
not have the desired array [?, a2, . . . , an]. As a concrete example, suppose A = ? and we
have one column (n = 1). Then A? = ?, rather than the desired {[?]}.
Fixing this problem is slightly tricky. There are an infinite number of a2, . . . , anwhich
do not co-occur with any a?1 inA, so for which ones do we actually include [?, a2, . . . , an]?
Certainly, the answer to this question cannot come from A, so it must come from the
stores. In particular, for each column i ? {2, . . . ,n}, we have conveniently stored a base
denotation ?i.b. We consider any ai that occurs in column 1 of the arrays of this base
denotation ([ai] ? ?i.b.A[1]). For this a2, . . . , an, we include [?, a2, . . . , an] in A?? as long as
a2, . . . , an does not co-occur with any a1. An example is given in Figure 11.
The reason for storing base denotations is thus partially revealed: The arrays rep-
resent feasible values of a CSP and can only contain positive information. When we
aggregate, we need to access possibly empty sets of feasible values?a kind of negative
information, which can only be recovered from the base denotations.
Figure 11
An example of applying the aggregate operation, which takes a denotation and aggregates the
values in column 1 for every setting of the other columns. The base denotations (b) are used to
put in {} for values that do not appear in A (in this example, AK, corresponding to the fact that
Alaska does not border any states).
409
Computational Linguistics Volume 39, Number 2
2.5.5 Mark Relations. Equations (23), (24), and (25) each processes a different mark
relation. We define a general mark operation, M(d, r, c) which takes a denotation d, a
mark relation r ? {E, Q, C} and a child denotation c, and sets the store of d in column 1
to be (r, d, c):
M(d, r, c) = d{r1 = r, b1 = d, c1 = c} (28)
The base denotation of the first column b1 is set to the current denotation d. This, in
some sense, creates a snapshot of the current denotation. Figure 12 shows an example
of the mark operation.
2.5.6 Execute Relations. Equation (22) defines the denotation of a DCS tree where the last
edge of the root is an execute relation. Similar to the aggregate case (21), we recurse on
the DCS tree without the last edge (?p; e?) and then join it to the result of applying the
execute operation Xi to the denotation of the child (cw).
The execute operation Xi is the most intricate part of DCS and is what does the
heavy lifting. The operation is parametrized by a sequence of distinct indices i that
specifies the order in which the columns should be processed. Specifically, i indexes into
the subsequence of columns with non-empty stores. We then process this subsequence
of columns in reverse order, where processing a column means performing some op-
erations depending on the stored relation in that column. For example, suppose that
columns 2 and 3 are the only non-empty columns. Then X12 processes column 3 before
column 2. On the other hand, X21 processes column 2 before column 3. We first define
Figure 12
An example of applying the mark operation, which takes a denotation and modifies the store of
the column 1. This information is used by other operations such as aggregate and execute.
410
Liang, Jordan, and Klein Learning Dependency-Based Compositional Semantics
Figure 13
An example of applying the execute operation on column 1 with the extract relation E. The
denotation prior to execution consists of two columns: column 1 corresponds to the border
node; column 2 to the state node. The join relations and predicates CA and state constrain the
arrays A in the denotation to include only the states that border California. After execution, the
non-marked column 1 is projected away, leaving only the state column with its store emptied.
the execute operation Xi for a single column i. There are three distinct cases, depending
on the relation stored in column i:
Extraction. For a denotation d with the extract relation E in column i, executing Xi(d)
involves three steps: (i) moving column i to before column 1 (?[i,?i]), (ii) projecting
away non-initial empty columns (?[??]), and (iii) removing the store (?{?1 = ?}):
Xi(d) = d[i,?i][??]{?1 = ?} if d.ri = E (29)
An example is given in Figure 13. There are two main uses of extraction.
1. By default, the denotation of a DCS tree is the set of feasible values of the
root node (which occupies column 1). To return the set of feasible values
of another node, we mark that node with E. Upon execution, the feasible
values of that node move into column 1. Extraction can be used to handle
in situ questions (see Figure 8(a)).
2. Unmarked nodes (those that do not have an edge with a mark relation) are
existentially quantified and have narrower scope than all marked nodes.
Therefore, we can make a node x have wider scope than another node y by
411
Computational Linguistics Volume 39, Number 2
marking x (with E) and executing y before x (see Figure 8(d,e) for
examples). The extract relation E (in fact, any mark relation) signifies
that we want to control the scope of a node, and the execute relation
allows us to set that scope.
Generalized Quantification.Generalized quantifiers are predicates on two sets, a restrictor
A and a nuclear scope B. For example,
w(some) = {(A,B) : A ? B > 0} (30)
w(every) = {(A,B) : A ? B} (31)
w(no) = {(A,B) : A ? B = ?} (32)
w(most) = {(A,B) : |A ? B| > 1
2
|A|} (33)
We think of the quantifier as amodifier which always appears as the child of a Q relation;
the restrictor is the parent. For example, in Figure 8(b), no corresponds to the quantifier
and state corresponds to the restrictor. The nuclear scope should be the set of all states
that Alaska borders. More generally, the nuclear scope is the set of feasible values of the
restrictor node with respect to the CSP that includes all nodes between the mark and
execute relations. The restrictor is also the set of feasible values of the restrictor node,
but with respect to the CSP corresponding to the subtree rooted at that node.8
We implement generalized quantifiers as follows: Let d be a denotation and suppose
we are executing column i. We first construct a denotation for the restrictor dA and a
denotation for the nuclear scope dB. For the restrictor, we take the base denotation in
column i (d.bi)?remember that the base denotation represents a snapshot of the restric-
tor node before the nuclear scope constraints are added. For the nuclear scope, we take
the complete denotation d (which includes the nuclear scope constraints) and extract
column i (d[i,?i][??]{?1 = ?}?see (29)). We then construct dA and dB by applying the
aggregate operation to each. Finally, we join these sets with the quantifier denotation,
stored in d.ci:
xi(d) =
((
d.ci 
??
1,1 dA
)
??2,1 dB
)
[?1] if d.ri = Q,where (34)
dA = ? (d.bi) (35)
dB = ? (d[i,?i][??]{?1 = ?}) (36)
When there is one quantifier, think of the execute relation as performing a syntactic
rewriting operation, as shown in Figure 14(b). For more complex cases, we must defer
to (34).
Figure 8(c) shows an example with two interacting quantifiers. The denotation of
the DCS tree before execution is the same in both readings, as shown in Figure 15. The
8 Defined this way, we can only handle conservative quantifiers, because the nuclear scope will always be
a subset of the restrictor. This design decision is inspired by DRT, where it provides a way of modeling
donkey anaphora. We are not treating anaphora in this work, but we can handle it by allowing pronouns
in the nuclear scope to create anaphoric edges into nodes in the restrictor. These constraints naturally
propagate through the nuclear scope?s CSP without affecting the restrictor.
412
Liang, Jordan, and Klein Learning Dependency-Based Compositional Semantics
Figure 14
(a) An example of applying the execute operation on column iwith the quantify relation Q.
Before executing, note that A = {} (because Alaska does not border any states). The restrictor (A)
is the set of all states, and the nuclear scope (B) is empty. Because the pair (A,B) does exist in
w(no), the final denotation is ??{[ ]}?? (which represents true). (b) Although the execute operation
actually works on the denotation, think of it in terms of expanding the DCS tree. We introduce
an extra projection relation [?1], which projects away the first column of the child subtree?s
denotation.
quantifier scope ambiguity is resolved by the choice of execute relation: X12 gives the
surface scope reading, X21 gives the inverse scope reading.
Figure 8(d) shows how extraction and quantification work together. First, the no
quantifier is processed for each city, which is an unprocessed marked node. Here, the
extract relation is a technical trick to give city wider scope.
Comparatives and Superlatives. Comparative and superlative constructions involve com-
paring entities, and for this we rely on a set S of entity?degree pairs (x, y), where x is an
Figure 15
Denotation of Figure 8(c) before the execute relation is applied.
413
Computational Linguistics Volume 39, Number 2
entity and y is a numeric degree. Recall that we can treat S as a function, which maps
an entity x to the set of degrees S(x) associated with x. Note that this set can contain
multiple degrees. For example, in the relative reading of state bordering the largest state,
we would have a degree for the size of each neighboring state.
Superlatives use the argmax and argmin predicates, which are defined in Section 2.3.
Comparatives use the more and less predicates: w(more) contains triples (S, x, y), where
x is ?more than? y as measured by S; w(less) is defined analogously:
w(more) = {(S, x, y) : max S(x) > max S(y)} (37)
w(less) = {(S, x, y) : min S(x) < min S(y)} (38)
We use the same mark relation C for both comparative and superlative construc-
tions. In terms of the DCS tree, there are three key parts: (i) the root x, which corresponds
to the entity to be compared, (ii) the child c of a C relation, which corresponds to the
comparative or superlative predicate, and (iii) c?s parent p, which contains the ?degree
information? (which will be described later) used for comparison. We assume that the
root is marked (usually with a relation E). This forces us to compute a comparison
degree for each value of the root node. In terms of the denotation d corresponding to the
DCS tree prior to execution, the entity to be compared occurs in column 1 of the arrays
d.A, the degree information occurs in column i of the arrays d.A, and the denotation of
the comparative or superlative predicate itself is the child denotation at column i (d.ci).
First, we define a concatenating function +i (d), which combines the columns i of d
by concatenating the corresponding tuples of each array in d.A:
+i (??A;???) = ??A?;????, where (39)
A? = {a(1...i1 )\i+ [ai1 + ? ? ?+ ai|i| ]+ a(i1...n)\i : a ? A}
?? = ?(1...i1 )\i+ [?i1 ]+ ?(i1...n)\i
Note that the store of column i1 is kept and the others are discarded. As an example:
+2,1 (??{[(1), (2), (3)], [(4), (5), (6)]};?1,?2,?3??) = ??{[(2, 1), (3)], [(5, 4), (6)]};?2,?3??
(40)
We first create a denotation d? where column i, which contains the degree infor-
mation, is extracted to column 1 (and thus column 2 corresponds to the entity to be
compared). Next, we create a denotation dS whose column 1 contains a set of entity-
degree pairs. There are two types of degree information:
1. Suppose the degree information has arity 2 (ARITY(d.A[i]) = 2). This
occurs, for example, in most populous city (see Figure 9(b)), where column i
is the population node. In this case, we simply set the degree to the
second component of population by projection (???w 
??
1,2 d
?). Now
columns 1 and 2 contain the degrees and entities, respectively. We
concatenate columns 2 and 1 (+2,1 (?)) and aggregate to produce a
denotation dS which contains the set of entity?degree pairs in column 1.
2. Suppose the degree information has arity 1 (ARITY(d.A[i]) = 1). This
occurs, for example, in state bordering the most states (see Figure 9(a)), where
414
Liang, Jordan, and Klein Learning Dependency-Based Compositional Semantics
column i is the lower marked state node. In this case, the degree of an
entity from column 2 is the number of different values that column 1 can
take. To compute this, aggregate the set of values (?
(
d?
)
) and apply the
count predicate. Now with the degrees and entities in columns 1 and 2,
respectively, we concatenate the columns and aggregate again to obtain dS.
Having constructed dS, we simply apply the comparative/superlative predicate which
has been patiently waiting in d.ci. Finally, the store of d?s column 1 was destroyed by the
concatenation operation+2,1 (() ?), so wemust restore it with ?{?1 = d.?1}. The complete
operation is as follows:
xi(d) =
(
???w 
??
1,2
(
d.ci 
??
1,1 dS
))
{?1 = d.?1} if d.?i = C, d.?1 = ?, where (41)
dS =
?
?
?
?
(
+2,1
(
???w 
??
1,2 d
?
))
if ARITY(d.A[i]) = 2
?
(
+2,1
(
???w 
??
1,2
(
?count?w 
??
1,1 ?
(
d?
)
)))
if ARITY(d.A[i]) = 1
(42)
d? = d[i,?i][??]{?1 = ?} (43)
An example of executing the C relation is shown in Figure 16(a). As with executing a
Q relation, for simple cases we can think of executing a C relation as expanding a DCS
tree, as shown in Figure 16(b).
Figure 9(a) and Figure 9(b) show examples of superlative constructions with the ar-
ity 1 and arity 2 types of degree information, respectively. Figure 9(c) shows an example
of a comparative construction. Comparatives and superlatives use the same machinery,
differing only in the predicate: argmax versus ?more; 31 :TX? (more than Texas). But both
predicates have the same template behavior: Each takes a set of entity?degree pairs and
returns any entity satisfying some property. For argmax, the property is obtaining the
highest degree; for more, it is having a degree higher than a threshold. We can handle
generalized superlatives (the five largest or the fifth largest or the 5% largest) as well by
swapping in a different predicate; the execution mechanisms defined in Equation (41)
remain the same.
We saw that the mark?execute machinery allows decisions regarding quantifier
scope to be made in a clean and modular fashion. Superlatives also have scope am-
biguities in the form of absolute versus relative readings. Consider the example in
Figure 9(d). In the absolute reading, we first compute the superlative in a narrow scope
(the largest state is Alaska), and then connect it with the rest of the phrase, resulting in
the empty set (because no states border Alaska). In the relative reading, we consider the
first state as the entity we want to compare, and its degree is the size of a neighboring
state. In this case, the lower state node cannot be set to Alaska because there are no
states bordering it. The result is therefore any state that borders Texas (the largest state
that does have neighbors). The two DCS trees in Figure 9(d) show that we can naturally
account for this form of superlative ambiguity based on where the scope-determining
execute relation is placed without drastically changing the underlying tree structure.
Remarks. These scope divergence issues are not specific to DCS?every serious semantic
formalism must address them. Generative grammar uses quantifier raising to move the
quantifier from its original syntactic position up to the desired semantic position before
semantic interpretation even occurs (Heim and Kratzer 1998). Other mechanisms such
415
Computational Linguistics Volume 39, Number 2
Figure 16
(a) Executing the compare relation C for an example superlative construction (relative reading
of state bordering the largest state from Figure 9(d)). Before executing, column 1 contains the
entity to compare, and column 2 contains the degree information, of which only the second
component is relevant. After executing, the resulting denotation contains a single column with
only the entities that obtain the highest degree (in this case, the states that border Texas). (b) For
this example, think of the execute operation as expanding the original DCS tree, although the
execute operation actually works on the denotation, not the DCS tree. The expanded DCS tree
has the same denotation as the original DCS tree, and syntactically captures the essence of the
execute?compare operation. Going through the relations of the expanded DCS tree from
bottom to top: The X2 relation swaps columns 1 and 2; the join relation keeps only the second
component ((TX, 267K) becomes (267K)); +2,1 concatenates columns 2 and 1 ([(267K), (AR)]
becomes [(AR, 267K)]); ? aggregates these tuples into a set; argmax operates on this set and
returns the elements.
as Montague?s (1973) quantifying in, Cooper storage (Cooper 1975), and Carpenter?s
(1998) scoping constructor handle scope divergence during semantic interpretation.
Roughly speaking, these mechanisms delay application of a quantifier, ?marking? its
spot with a dummy pronoun (as inMontague?s quantifying in) or putting it in a store (as
in Cooper storage), and then ?executing? the quantifier at a later point in the derivation
either by performing a variable substitution or retrieving it from the store. Continuation,
from programming languages, is another solution (Barker 2002; Shan 2004); this sets the
semantics of a quantifier to be a function from its continuation (which captures all the
semantic content of the clause minus the quantifier) to the final denotation of the clause.
416
Liang, Jordan, and Klein Learning Dependency-Based Compositional Semantics
Intuitively, continuations reverse the normal evaluation order, allowing a quantifier to
remain in situ but still outscope the rest of the clause. In fact, the mark and execute
relations of DCS are analogous to the shift and reset operators used in continuations.
One of the challenges with allowing flexible scope is that free variables can yield invalid
scopings, a well-known issuewith Cooper storage that the continuation-based approach
solves. Invalid scopings are filtered out by the construction mechanism (Section 2.6).
One difference between mark?execute in DCS and many other mechanisms is that
DCS trees (which contain mark and execute relations) are the final logical forms?the
handling of scope divergence occurs in the computing their denotations. The analog
in the other mechanisms resides in the construction mechanism?the actually final
logical form is quite simple.9 Therefore, we have essentially pushed the inevitable
complexity from the construction mechanism into the semantics of the logical form.
This is a conscious design decision: We want our construction mechanism, which maps
natural language to logical form, to be simple and not burdened with complex linguistic
issues, for our focus is on learning this mapping. Unfortunately, the denotation of our
logical forms (Section 2.5.1) do become more complex than those of lambda calculus
expressions, but we believe this is a reasonable tradeoff to make for our particular
application.
2.6 Construction Mechanism
We have thus far defined the syntax (Section 2.2) and semantics (Section 2.5) of DCS
trees, but we have only vaguely hinted at how these DCS trees might be connected
to natural language utterances by appealing to idealized examples. In this section, we
formally define the construction mechanism for DCS, which takes an utterance x and
produces a set of DCS trees ZL(x).
Because wemotivated DCS trees based on dependency syntax, it might be tempting
to take a dependency parse tree of the utterance, replace the words with predicates, and
attach some relations on the edges to produce a DCS tree. To a first approximation, this
is what we will do, but we need to be a bit more flexible for several reasons: (i) some
nodes in the DCS tree do not have predicates (e.g., children of an E relation or parent
of an Xi relation); (ii) nodes have predicates that do not correspond to words (e.g., in
California cities, there is a implicit loc predicate that bridges CA and city); (iii) some
words might not correspond to any predicates in our world (e.g., please); and (iv) the
DCS tree might not always be aligned with the syntactic structure depending on which
syntactic formalism one ascribes to. Although syntax was the inspiration for the DCS
formalism, we will not actually use it in construction.
It is also worth stressing the purpose of the construction mechanism. In linguistics,
the purpose of the construction mechanism is to try to generate the exact set of valid
logical forms for a sentence. We view the construction mechanism instead as simply a
way of creating a set of candidate logical forms. A separate step defines a distribution
over this set to favor certain logical forms over others. The construction mechanism
should therefore simply overapproximate the set of logical forms. Linguistic constraints
that are normally encoded in the construction mechanism (for example, in CCG, that
the disharmonic pair S/NP and S\NP cannot be coordinated, or that non-indefinite
quantifiers cannot extend their scope beyond clause boundaries) would be instead
9 In the continuation-based approach, this difference corresponds to the difference between assigning a
denotational versus an operational semantics.
417
Computational Linguistics Volume 39, Number 2
encoded as features (Section 3.1.1). Because feature weights are estimated from data,
one can view our approach as automatically learning the linguistic constraints relevant
to our end task.
2.6.1 Lexical Triggers. The construction mechanism assumes a fixed set of lexical triggers
L. Each trigger is a pair (s, p), where s is a sequence of words (usually one) and p is a
predicate (e.g., s = California and p = CA). We use L(s) to denote the set of predicates p
triggered by s ((s, p) ? L). We should think of the lexical triggers L not as pinning down
the precise predicate for each word, but rather as producing an overapproximation.
For example, Lmight contain {(city, city), (city, state), (city, river), . . . }, reflecting our
initial ignorance prior to learning.
We also define a set of trace predicates L(	), which can be introduced without an
overt lexical element. Their name is inspired by trace/null elements in syntax, but they
serve a more practical rather than a theoretical role here. As we shall see in Section 2.6.2,
trace predicates provide more flexibility in the construction of logical forms, allowing
us to insert a predicate based on the partial logical form constructed thus far and assess
its compatibility with the words afterwards (based on features), rather than insisting on
a purely lexically driven formalism. Section 4.1.3 describes the lexical triggers and trace
predicates that we use in our experiments.
2.6.2 Recursive Construction of DCS Trees. Given a set of lexical triggers L, we will now
describe a recursive mechanism for mapping an utterance x = (x1, . . . , xn) to ZL(x), a
set of candidate DCS trees for x. The basic approach is reminiscent of projective labeled
dependency parsing: For each span i..j of the utterance, we build a set of trees Ci,j(x).
The set of trees for the span 0..n is the final result:
ZL(x) = C0,n(x) (44)
Each set of DCS trees Ci,j(x) is constructed recursively by combining the trees of its
subspans Ci,k(x) and Ck?,j(x) for each pair of split points k, k
? (words between k and k?
are ignored). These combinations are then augmented via a function A and filtered via a
function F; these functions will be specified later. Formally, Ci,j(x) is defined recursively
as follows:
Ci,j(x) = F
(
A
(
{?p?i..j : p ? L(xi+1..j)} ?
?
i?k?k?<j
a?Ci,k(x)
b?Ck? ,j(x)
T1(a, b))
))
(45)
This recurrence has two parts:
 The base case: we take the phrase (sequence of words) over span i..j
and look up the set of predicates p in the set of lexical triggers. For each
predicate, we construct a one-node DCS tree. We also extend the definition
of DCS trees in Section 2.2 to allow each node to store the indices of the
span i..j that triggered the predicate at that node; this is denoted by ?p?i..j.
This span information will be useful in Section 3.1.1, where we will need
to talk about how an utterance x is aligned with a DCS tree z.
418
Liang, Jordan, and Klein Learning Dependency-Based Compositional Semantics
 The recursive case: T1(a, b), which we will define shortly, that takes two
DCS trees, a and b, and returns a set of new DCS trees formed by
combining a and b. Figure 17 shows this recurrence graphically.
We now focus on how to combine two DCS trees. Define Td(a, b) as the set of DCS
trees that result by making either a or b the root and connecting the other via a chain of
relations and at most d trace predicates (d is a small integer that keeps the set of DCS
trees manageable):
Td(a, b) = T
?
d
(a, b) ? T?
d
(b, a) (46)
Here, T
?
d
(a, b) is the set of DCS trees where a is the root; for T
?
d
(a, b), b is the root. The
former is defined recursively as follows:
T
?
0 (a, b) = ?, (47)
T
?
d
(a, b) =
?
r?R
p?L()
{?a.p; a.e; r :b? , ?a.p; a.e; r :?? :b??} ? T?
d?1(a, ?p; r :b?)
First, we consider all possible relations r ? R and try appending an edge to a with
relation r and child b (?a.p; a.e; r :b?); an aggregate relation ? can be inserted in addition
(?a.p; a.e; r :?? :b??). Of course, R contains an infinite number of join and execute rela-
tions, but only a small finite number of them make sense: We consider join relations
j
j? only for j ? {1, . . . , ARITY(a.p)} and j
? ? {1, . . . , ARITY(b.p)}, and execute relations Xi
for which i does not contain indices larger than the number of columns of bw. Next,
we further consider all possible trace predicates p ? L(	), and recursively try to connect
Figure 17
An example of the recursive construction of Ci,j(x), a set of DCS trees for span i..j.
419
Computational Linguistics Volume 39, Number 2
Figure 18
Given two DCS trees, a and b, T
?
1 (a, b) and T
?
1 (a, b) are the two sets of DCS trees formed by
combining a and bwith a at the root and b at the root, respectively; one trace predicate can be
inserted in between. In this example, the DCS trees which survive filtering (Section 2.6.3)
are shown.
awith the intermediate ?p; r :b?, now allowing d? 1 additional predicates. See Figure 18
for an example. In the other direction, T
?
d
is defined similarly:
T
?
0 (a, b) = ? (48)
T
?
d
(a, b) =
?
r?R
p?L()
{?b.p; r :a; b.e? , ?b.p; r :?? :a? ; b.e?} ? T?
d?1(a, ?p; r :b?)
Inserting trace predicates allows us to build logical forms with more predicates
than are explicitly triggered by the words. This ability is useful for several reasons.
Sometimes, there is a predicate not overtly expressed, especially in noun compounds
(e.g., California cities). For semantically light words such as prepositions (e.g., for) it is
difficult to enumerate all the possible predicates that they might trigger; it is simpler
computationally to try to insert trace predicates. We can even omit lexical triggers
for transitive verbs such as border because the corresponding predicate border can be
inserted as a trace predicate.
The function T1(a, b) connects two DCS trees via a path of relations and trace predi-
cates. The augmentation function A adds additional relations (specifically, E and/or Xi)
on a single DCS tree:
A(Z) =
?
z?Z
Xi?R
{z, ?z; E :???? , ?Xi :z? , ?Xi :?z; E :?????} (49)
2.6.3 Filtering using Abstract Interpretation. The construction procedure as described thus
far is extremely permissive, generating many DCS trees which are obviously wrong?
for example, ?state; 11 :?>;
2
1 ?3???, which tries to compare a state with the number 3. There
is nothing wrong with this expression syntactically: Its denotation will simply be empty
(with respect to the world). But semantically, this DCS tree is anomalous.
We cannot simply just discard DCS trees with empty denotations, because we
would incorrectly rule out ?state; 11 :?border;
2
1 ?AK???. The difference here is that even
though the denotation is empty in this world, it is possible that it might not be empty
420
Liang, Jordan, and Klein Learning Dependency-Based Compositional Semantics
in a different world where history and geology took another turn, whereas it is simply
impossible to compare cities and numbers.
Now let us quickly flesh out this intuition before falling into a philosophical dis-
cussion about possible worlds. Given a world w, we define an abstract world ?(w),
to be described shortly. We compute the denotation of a DCS tree z with respect to
this abstract world. If at any point in the computation we create an empty denotation,
we judge z to be impossible and throw it away. The filtering function F is defined as
follows:10
F(Z) = {z ? Z : ?z? subtree of z , z??(w).A = ?} (50)
Now we need to define the abstract world ?(w). The intuition is to map concrete
values to abstract values: 3 :length becomes ? :length,Oregon :state becomes ? :state,
and in general, primitive value x : t becomes ? : t. We perform abstraction on tuples
componentwise, so that (Oregon :state, 3 :length) becomes (? :state, ? :length). Our
abstraction of sets is slightly more complex: The empty set maps to the empty set, a set
containing values all with the same abstract value a maps to {a}, and a set containing
values with more than one abstract value maps to {MIXED}. Finally, a world maps each
predicate onto a set of (concrete) tuples; the corresponding abstract world maps each
predicate onto the set of abstract tuples. Formally, the abstraction function is defined as
follows:
?(x : t) = ? : t [primitive value] (51)
?((v1, . . . , vn)) = (?(v1), . . . ,?(vn)) [tuple] (52)
?(A) =
?
?
?
?
?
? if A = ?
{?(x) : x ? A} if |{?(x) : x ? A}| = 1
{MIXED} otherwise
[set] (53)
?(w) = ?p.{?(x) : x ? w(p)} [world] (54)
As an example, the abstract world might look like this:
?(w)(>) = {(? :number, ? :number, ? :number) (55)
(? :length, ? :length, ? :length), . . . }
?(w)(state) = {(? :state)} (56)
?(w)(AK) = {(? :state)} (57)
?(w)(border) = {(? :state, ? :state)} (58)
Now returning to our motivating example at the beginning of this section, we see
that the bad DCS tree has an empty abstract denotation ?state; 11 :?>;
2
1 ?3????(w) =
???; ???. The good DCS tree has a non-empty abstract denotation: ?state; 11 :?border;
2
1 ?AK????(w) = ??{(? :state)}; ???, as desired.
10 To further reduce the search space, F imposes a few additional constraints: for example, limiting the
number of columns to 2, and only allowing trace predicates between arity 1 predicates.
421
Computational Linguistics Volume 39, Number 2
Remarks. Computing denotations on an abstract world is called abstract interpretation
(Cousot and Cousot 1977) and is a very powerful framework commonly used in the
programming languages community. The idea is to obtain information about a program
(in our case, a DCS tree) without running it concretely, but rather just by running it
abstractly. It is closely related to type systems, but the type of abstractions one uses is
often much richer than standard type systems.
2.6.4 Comparison with CCG. We now compare our construction mechanism with CCG
(see Figure 19 for an example). The main difference is that our lexical triggers contain
less information than a lexical entry in a CCG. In CCG, the lexicon would have an entry
such as
major  N/N : ?f.?x.major(x) ? f (x) (59)
which gives detailed information about how this word should interact with its context.
In DCS construction, however, each lexical trigger only has the minimal amount of
information:
major  major (60)
A lexical trigger specifies a pre-theoretic ?meaning? of a word which does not commit
to any formalisms. One advantage of this minimality is that lexical triggers could be
easily obtained from non-expert supervision: One would only have to associate words
with database table names (predicates).
In some sense, the DCS construction mechanism pushes the complexity out of the
lexicon. In linguistics, this complexity usually would end up in the grammar, which
would be undesirable. We do not have to respect this tradeoff, however, because the
Figure 19
Comparison between the construction mechanisms of CCG and DCS. There are three principal
differences: First, in CCG, words are mapped onto lambda calculus expressions; in DCS, words
are just mapped onto predicates. Second, in CCG, lambda calculus expressions are built by
combining (e.g., via function application) two smaller expressions; in DCS, trees are combined
by inserting relations (and possibly other predicates between them). Third, in CCG, all words
map to logical expressions; in DCS, only a small subset of words (e.g., state and Texas) map to
predicates; the rest participate in features for scoring DCS trees.
422
Liang, Jordan, and Klein Learning Dependency-Based Compositional Semantics
construction mechanism only produces an overapproximation, which means it is possi-
ble to have both a simple ?lexicon? and a simple ?grammar.?
There is an important practical rationale for this design decision. During learning,
we never just have one clean lexical entry per word. Rather, there are often many
possible lexical entries (and to handle disfluent utterances or utterances in free word-
order languages, we might actually need many of them [Kwiatkowski et al 2010]):
major  N : ?x.major(x) (61)
major  N/N : ?f.?x.major(x) ? f (x) (62)
major  N\N : ?f.?x.major(x) ? f (x) (63)
. . . (64)
Now think of a DCS lexical trigger major  major as simply a compact representation for
a set of CCG lexical entries. Furthermore, the choice of the lexical entry is made not
at the initial lexical base case, but rather during the recursive construction by inserting
relations between DCS subtrees. It is exactly at this point that the choice can be made,
because after all, the choice is one that depends on context. The general principle is to
compactly represent the indeterminacy until one can resolve it. Compactly representing
a set of CCG lexical entries can also be done within the CCG framework by factoring
lexical entries into a lexeme and a lexical template (Kwiatkowski et al 2011).
Type raising is a combinator in CCG that traditionally converts x to ?f.f (x). In
recent work, Zettlemoyer and Collins (2007) introduced more general type-changing
combinators to allow conversion from one entity into a related entity in general (a
kind of generalized metonymy). For example, in order to parse Boston flights, Boston
is transformed to ?x.to(x, Boston). This type changing is analogous to inserting trace
predicates in DCS, but there is an important distinction: Type changing is a unary
operation and is unconstrained in that it changes logical forms into new ones without
regard for how they will be used downstream. Inserting trace predicates is a binary
operation that is constrained by the two predicates that it is mediating. In the example,
to would only be inserted to combine Boston with flight. This is another instance of
the general principle of delaying uncertain decisions until there is more information.
3. Learning
In Section 2, we defined DCS trees and a construction mechanism for producing a set
of candidate DCS trees given an utterance. We now define a probability distribution
over that set (Section 3.1) and an algorithm for estimating the parameters (Section 3.2).
The number of candidate DCS trees grows exponentially, so we use beam search to
control this growth. The final learning algorithm alternates between beam search and
optimization of the parameters, leading to a natural bootstrapping procedure which
integrates learning and search.
3.1 Semantic Parsing Model
The semantic parsing model specifies a conditional distribution over a set of candi-
date DCS trees C(x) given an utterance x. This distribution depends on a function
?(x, z) ? Rd, which takes a (x, z) pair and extracts a set of local features (see Section 3.1.1
423
Computational Linguistics Volume 39, Number 2
for a full specification). Associated with this feature vector is a parameter vector ? ? Rd.
The inner product between the two vectors, ?(x, z)?, yields a numerical score, which
intuitively measures the compatibility of the utterance x with the DCS tree z. We expo-
nentiate the score and normalize over C(x) to obtain a proper probability distribution:
p(z | x;C,?) = exp{?(x, z)??A(?; x,C)} (65)
A(?; x,C) = log
?
z?C(x)
exp{?(x, z)?} (66)
where A(?; x,C) is the log-partition function with respect to the candidate set function
C(x).
3.1.1 Features.We now define the feature vector ?(x, z) ? Rd, the core part of the seman-
tic parsing model. Each component j = 1, . . . , d of this vector is a feature, and ?(x, z)j
is the number of times that feature occurs in (x, z). Rather than working with indices,
we treat features as symbols (e.g., TRIGGERPRED[states, state]). Each feature captures
some property about (x, z) that abstracts away from the details of the specific instance
and allows us to generalize to new instances that share common features.
The features are organized into feature templates, where each feature template
instantiates a set of features. Figure 20 shows all the feature templates for a concrete
example. The feature templates are as follows:
 PREDHIT contains the single feature PREDHIT, which fires for each
predicate in z.
 PRED contains features {PRED[?(p)] : p ? P}, each of which fires on
?(p), the abstraction of predicate p, where
?(p) =
{
? : t if p = x : t
p otherwise
(67)
The purpose of the abstraction is to abstract away the details of concrete
values such as TX = Texas :state.
 PREDREL contains features {PREDREL[?(p),q] : p ? P ,q ? ({?,?}?
R)?}. A feature fires when a node x has predicate p and is connected via
some path q = (d1, r1), . . . , (dm, rm) to the lowest descendant node ywith
the property that each node between x and y has a null predicate. Each
(d, r) on the path represents an edge labeled with relation r connecting
to a left (d =?) or right (d =?) child. If x has no children, then m = 0.
The most common case is when m = 1, but m = 2 also occurs with the
aggregate and execute relations (e.g., PREDREL[count,? 11? ?] fires
for Figure 5(a)).
 PREDRELPRED contains features {PREDRELPRED[?(p),q,?(p?)] : p, p? ?
P ,q ? ({?,?}?R)?}, which are the same as PREDREL, except that we
include both the predicate p of x and the predicate p? of the descendant
node y. These features do not fire if m = 0.
424
Liang, Jordan, and Klein Learning Dependency-Based Compositional Semantics
Figure 20
For each utterance?DCS tree pair (x, z), we define a feature vector ?(x, z), whose j-th component
is the number of times a feature j occurs in (x, z). Each feature has an associated parameter ?j,
which is estimated from data in Section 3.2. The inner product of the feature vector and
parameter vector yields a compatibility score.
 TRIGGERPRED contains features {TRIGGERPRED[s, p] : s ?W?, p ? P},
whereW = {it,Texas, . . . } is the set of words. Each of these features fires
when a span of the utterance with words s triggers the predicate p?more
precisely, when a subtree ?p; e?i..j exists with s = xi+1..j. Note that these
lexicalized features use the predicate p rather than the abstracted
version ?(p).
 TRACEPRED contains features {TRACEPRED[s, p, d] : s ?W?, p ? P , d ?
{?,?}}, each of which fires when a trace predicate p has been inserted
425
Computational Linguistics Volume 39, Number 2
over a word s. The situation is the following: Suppose we have a subtree
a that ends at position k (there is a predicate in a that is triggered by a
phrase with right endpoint k) and another subtree b that begins at k?.
Recall that in the construction mechanism (46), we can insert a trace
predicate p ? L(	) between the roots of a and b. Then, for every word
xj between the spans of the two subtrees ( j = {k+ 1, . . . , k?}), the
feature TRACEPRED[xj, p, d] fires (d =? if b dominates a and d =?
if a dominates b).
 TRACEREL contains features {TRACEREL[s, d, r] : s ?W?, d ? {?,?}, r ?
R}, each of which fires when some trace predicate with parent relation r
has been inserted over a word s.
 TRACEPREDREL contains features {TRACEPREDREL[s, p, d, r] : s ?W?,
p ? P , d ? {?,?}, r ? R}, each of which fires when a predicate p is
connected via child relation r to some trace predicate over a word s.
These features are simple generic patterns which can be applied for modeling
essentially any distribution over sequences and labeled trees?there is nothing spe-
cific to DCS at all. The first half of the feature templates (PREDHIT, PRED, PREDREL,
PREDRELPRED) capture properties of the tree independent of the utterance, and
are similar to those used for syntactic dependency parsing. The other feature tem-
plates (TRIGGERPRED, TRACEPRED, TRACEREL, TRACEPREDREL) connect predicates
in the DCS tree with words in the utterance, similar to those in a model of machine
translation.
3.2 Parameter Estimation
We have now fully specified the details of the graphical model in Figure 2: Section 3.1
described semantic parsing and Section 2 described semantic evaluation. Next, we focus
on the inferential problem of estimating the parameters ? of the model from data.
3.2.1 Objective Function.We assume that our learning algorithm is given a training data
setD containing question?answer pairs (x, y). Because the logical forms are unobserved,
we work with log p(y | x;C,?), the marginal log-likelihood of obtaining the correct
answer y given an utterance x. This marginal log-likelihood sums over all z ? C(x) that
evaluate to y:
log p(y | x;C,?) = log p(z ? Cy(x) | x;C,?) (68)
= A(?; x,Cy)?A(?, x,C), where (69)
Cy(x)
def
= {z ? C(x) : zw = y} (70)
Here, Cy(x) is the set of DCS trees z with denotation y.
We call an example (x, y) ? D feasible if the candidate set of x contains a DCS
tree that evaluates to y (Cy(x) = ?). Define an objective function O(?,C) containing
two terms. The first term is the sum of the marginal log-likelihood over all feasible
426
Liang, Jordan, and Klein Learning Dependency-Based Compositional Semantics
training examples. The second term is a quadratic penalty on the parameters ? with
regularization parameter ?. Formally:
O(?,C) def=
?
(x,y)?D
Cy(x)=?
log p(y | x;C,?)? ?
2
???22 (71)
=
?
(x,y)?D
Cy(x)=?
(A(?; x,Cy)?A(?; x,C))? ?
2
???22
We would like to maximize O(?,C). The log-partition function A(?; ?, ?) is convex,
but O(?,C) is the difference of two log-partition functions and hence is not concave
(nor convex). Thus we resort to gradient-based optimization. A standard result is that
the derivative of the log-partition function is the expected feature vector (Wainwright
and Jordan 2008). Using this, we obtain the gradient of our objective function:11
?O(?,C)
??
=
?
(x,y)?D
Cy(x)=?
(
Ep(z|x;Cy,?)[?(x, z)]? Ep(z|x;C,?)[?(x, z)]
)
? ?? (72)
Updating the parameters in the direction of the gradient would move the parameters
towards the DCS trees that yield the correct answer (Cy) and away from overall can-
didate DCS trees (C). We can use any standard numerical optimization algorithm that
requires only black-box access to a gradient. Section 4.3.4 will discuss the empirical
ramifications of the choice of optimization algorithm.
3.2.2 Algorithm. Given a candidate set function C(x), we can optimize Equation (71) to
obtain estimates of the parameters ?. Ideally, we would use C(x) = ZL(x), the candidate
sets from our construction mechanism in Section 2.6, but we quickly run into the prob-
lem of computing Equation (72) efficiently. Note that ZL(x) (defined in Equation (44))
grows exponentially with the length of x. This by itself is not a show-stopper. Our
features (Section 3.1.1) decompose along the edges of the DCS tree, so it is possible
to use dynamic programming12 to compute the second expectation Ep(z|x;ZL,?)[?(x, z)]
of Equation (72). The problem is computing the first expectation Ep(z|x;ZyL ,?)
[?(x, z)],
which sums over the subset of candidate DCS trees z satisfying the constraint zw = y.
Though this is a smaller set, there is no efficient dynamic program for this set because
the constraint does not decompose along the structure of the DCS tree. Therefore, we
need to approximate ZyL , and, in fact, we will approximate ZL as well so that the two
expectations in Equation (72) are coherent.
Recall that ZL(x) was built by recursively constructing a set of DCS trees Ci,j(x)
for each span i..j. In our approximation, we simply use beam search, which truncates
each Ci,j(x) to include the (at most) K DCS trees with the highest score ?(x, z)
?. We
11 Notation: Ep(x)[f (x)] =
?
x p(x)f (x).
12 The state of the dynamic program would be the span i..j and the head predicate over that span.
427
Computational Linguistics Volume 39, Number 2
let C?i,j,?(x) denote this approximation and define the set of candidate DCS trees with
respect to the beam search:
Z?L,?(x) = C?0,n,?(x) (73)
We now have a chicken-and-egg problem: If we had good parameters ?, we
could generate good candidate sets C(x) using beam search Z?L,?(x). If we had good
candidate sets C(x), we could generate good parameters by optimizing our objective
O(?,C) in Equation (71). This problem leads to a natural solution: simply alternate
between the two steps (Figure 21). This procedure is not guaranteed to converge, due
to the heuristic nature of the beam search, but we have found it to be convergent in
practice.
Finally, we use the trained model with parameters ? to answer new questions x by
choosing the most likely answer y, summing out the latent logical form z:
F?(x)
def
= argmax
y
p(y | x;?, Z?L,?) (74)
= argmax
y
?
z?Z?L,?(x)
zw=y
p(z | x;?, Z?L,?) (75)
4. Experiments
We have now completed the conceptual part of this article?using DCS trees to rep-
resent logical forms (Section 2), and learning a probabilistic model over these trees
(Section 3). In this section, we evaluate and study our approach empirically. Our
main result is that our system can obtain comparable accuracies to state-of-the-art
systems that require annotated logical forms. All the code and data are available at
cs.stanford.edu/~pliang/software/.
4.1 Experimental Set-up
We first describe the data sets (Section 4.1.1) that we use to train and evaluate our
system. We then mention various choices in the model and learning algorithm (Sec-
tion 4.1.2). One of these choices is the lexical triggers, which are further discussed in
Section 4.1.3.
Figure 21
The learning algorithm alternates between updating the candidate sets based on beam search
and updating the parameters using standard numerical optimization.
428
Liang, Jordan, and Klein Learning Dependency-Based Compositional Semantics
4.1.1 Data sets. We tested our methods on two standard data sets, referred to in this
article as GEO and JOBS. These data sets were created by Ray Mooney?s group during
the 1990s and have been used to evaluate semantic parsers for over a decade.
U.S. Geography. The GEO data set, originally created by Zelle and Mooney (1996), con-
tains 880 questions about U.S. geography and a database of facts encoded in Prolog. The
questions in GEO ask about general properties (e.g., area, elevation, and population) of
geographical entities (e.g., cities, states, rivers, andmountains). Across all the questions,
there are 280 word types, and the length of an utterance ranges from 4 to 19 words,
with an average of 8.5 words. The questions involve conjunctions, superlatives, and
negation, but no generalized quantification. Each question is annotated with a logical
form in Prolog, for example:
Utterance: What is the highest point in Florida?
Logical form: answer(A,highest(A,(place(A),loc(A,B),const(B,stateid(florida)))))
Because our approach learns from answers, not logical forms, we evaluated the
annotated logical forms on the provided database to obtain the correct answers.
Recall that a world/database w maps each predicate p ? P to a set of tuples w(p).
Some predicates contain the set of tuples explicitly (e.g., mountain); others can be
derived (e.g., higher takes two entities x and y and returns true if elevation(x) >
elevation(y)). Other predicates are higher-order (e.g., sum, highest) in that they take
other predicates as arguments. We do not use the provided domain-specific higher-
order predicates (e.g., highest), but rather provide domain-independent higher-order
predicates (e.g., argmax) and the ordinary domain-specific predicates (e.g., elevation).
This provides more compositionality and therefore better generalization. Similarly, we
use more and elevation instead of higher. Altogether, P contains 43 predicates plus
one predicate for each value (e.g., CA).
Job Queries. The JOBS data set (Tang and Mooney 2001) contains 640 natural language
queries about job postings. Most of the questions ask for jobs matching various criteria:
job title, company, recruiter, location, salary, languages and platforms used, areas of
expertise, required/desired degrees, and required/desired years of experience. Across
all utterances, there are 388 word types, and the length of an utterance ranges from 2 to
23 words, with an average of 9.8 words.
The utterances are mostly based on conjunctions of criteria, with a sprinkling of
negation and disjunction. Here is an example:
Utterance: Are there any jobs using Java that are not with IBM?
Logical form: answer(A,(job(A),language(A,?java?),?company(A,?IBM?)))
The JOBS data set comes with a database, which we can use as the world w. When
the logical forms are evaluated on this database, however, close to half of the answers
are empty (no jobs match the requested criteria). Therefore, there is a large discrepancy
between obtaining the correct logical form (which has been the focus of most work on
semantic parsing) and obtaining the correct answer (our focus).
To bring these two into better alignment, we generated a random database as
follows: We created m = 100 jobs. For each job j, we go through each predicate p (e.g.,
company) that takes two arguments, a job, and a target value. For each of the possible
target values v, we add (j, v) to w(p) independently with probability ? = 0.8. For exam-
ple, for p = company, j = job37, we might add (job37, IBM) to w(company). The result is
429
Computational Linguistics Volume 39, Number 2
a database with a total of 23 predicates (which includes the domain-independent ones)
in addition to the value predicates (e.g., IBM).
The goal of using randomness is to ensure that two different logical forms will most
likely yield different answers. For example, consider two logical forms:
z1 = ?j.job( j) ? company( j, IBM), (76)
z2 = ?j.job( j) ? language( j, Java). (77)
Under the random construction, the denotation of z1 is S1, a random subset of the jobs,
where each job is included in S1 independently with probability ?, and the denotation
of z2 is S2, which has the same distribution as S1 but importantly is independent of S1.
Therefore, the probability that S1 = S2 is [?
2 + (1? ?)2]m, which is exponentially small
in m. This construction yields a world that is not entirely ?realistic? (a job might have
multiple employers), but it ensures that if we get the correct answer, we probably also
obtain the correct logical form.
4.1.2 Settings. There are a number of settings that control the tradeoffs between compu-
tation, expressiveness, and generalization power of our model, shown here. For now,
we will use generic settings chosen rather crudely; Section 4.3.4 will explore the effect
of changing these settings.
Lexical Triggers The lexical triggers L (Section 2.6.1) define the set of candidate DCS
trees for each utterance. There is a tradeoff between expressiveness and computa-
tional complexity: The more triggers we have, the more DCS trees we can consider
for a given utterance, but then either the candidate sets become too large or beam
search starts dropping the good DCS trees. Choosing lexical triggers is important
and requires additional supervision (Section 4.1.3).
Features Our probabilistic semantic parsing model is defined in terms of feature tem-
plates (Section 3.1.1). Richer features increase expressiveness but also might lead
to overfitting. By default, we include all the feature templates.
Number of training examples (n) An important property of any learning algorithm is
its sample complexity?how many training examples are required to obtain a
certain level of accuracy? By default, all training examples are used.
Number of training iterations (T) Our learning algorithm (Figure 21) alternates be-
tween updating candidate sets and updating parameters for T iterations. We use
T = 5 as the default value.
Beam size (K) The computation of the candidate sets in Figure 21 is based on beam
search where each intermediate state keeps at most K DCS trees. The default value
is K = 100.
Optimization algorithm To optimize the objective functionO(?,C) our default is to use
the standard L-BFGS algorithm (Nocedal 1980) with a backtracking line search for
choosing the step size.
Regularization (?) The regularization parameter ? > 0 in the objective functionO(?,C)
is another knob for controlling the tradeoff between fitting and overfitting. The
default is ? = 0.01.
4.1.3 Lexical Triggers. The lexical trigger set L (Section 2.6.1) is a set of entries (s, p), where
s is a sequence of words and p is a predicate. We run experiments on two sets of lexical
triggers: base triggers LB and augmented triggers LB+P.
430
Liang, Jordan, and Klein Learning Dependency-Based Compositional Semantics
Base Triggers. The base trigger set LB includes three types of entries:
 Domain-independent triggers: For each domain-independent predicate
(e.g., argmax), we manually specify a few words associated with that
predicate (e.g., most). The full list is shown at the top of Figure 22.
 Values: For each value x that appears in the world (specifically,
x ? vj ? w(p) for some tuple v, index j, and predicate p), LB contains an
entry (x, x) (e.g., (Boston,Boston :city)). Note that this rule implicitly
specifies an infinite number of triggers.
Regarding predicate names, we do not add entries such as (city, city),
because we want our system to be language-independent. In Turkish,
for instance, we would not have the luxury of lexicographical cues that
associate citywith s?ehir. So we should think of the predicates as just
symbols predicate1, predicate2, and so on. On the other hand, values
in the database are generally proper nouns (e.g., city names) for which
there are generally strong cross-linguistic lexicographic similarities.
 Part-of-speech (POS) triggers:13 For each domain-specific predicate p,
we specify a set of POS tags T. Implicitly, LB contains all pairs (x, p) where
the word x has a POS tag t ? T. For example, for city, we would specify
NN and NNS, which means that any word which is a singular or plural
common noun triggers the predicate city. Note that city triggers city as
desired, but state also triggers city.
The POS triggers for GEO and JOBS domains are shown in the left side of
Figure 22. Note that some predicates such as traverse and loc are
not associated with any POS tags. Predicates corresponding to verbs and
prepositions are not included as overt lexical triggers, but rather included
as trace predicates L(	). In constructing the logical forms, nouns and
adjectives serve as anchor points. Trace predicates can be inserted between
these anchors. This strategy is more flexible than requiring each predicate
to spring from some word.
Augmented Triggers.We nowdefine the augmented trigger set LB+P, which containsmore
domain-specific information than LB. Specifically, for each domain-specific predicate
(e.g., city), we manually specify a single prototype word (e.g., city) associated with
that predicate. Under LB+P, city would trigger only city because city is a prototype
word, but townwould trigger all the NN predicates (city, state, country, etc.) because
it is not a prototype word.
Prototype triggers require only a modest amount of domain-specific supervision
(see the right side of Figure 22 for the entire list for GEO and JOBS). In fact, as we?ll see
in Section 4.2, prototype triggers are not absolutely required to obtain good accuracies,
but they give an extra boost and also improve computational efficiency by reducing the
set of candidate DCS trees.
13 To perform POS tagging, we used the Berkeley Parser (Petrov et al 2006), trained on the WSJ Treebank
(Marcus, Marcinkiewicz, and Santorini 1993) and the Question Treebank (Judge, Cahill, and v. Genabith
2006)?thanks to Slav Petrov for providing the trained parser.
431
Computational Linguistics Volume 39, Number 2
Figure 22
Lexical triggers used in our experiments.
Finally, to determine triggering, we stem all words using the Porter stemmer (Porter
1980), so that mountains triggers the same predicates as mountain. We also decompose
superlatives into two words (e.g., largest is mapped to most large), allowing us to con-
struct the logical form more compositionally.
4.2 Comparison with Other Systems
We now compare our approach with existing methods. We used the same training-test
splits as Zettlemoyer and Collins (2005) (600 training and 280 test examples for GEO,
500 training and 140 test examples for JOBS). For development, we created five random
splits of the training data. For each split, we put 70% of the examples into a development
training set and the remaining 30% into a development test set. The actual test set was
only used for obtaining final numbers.
4.2.1 Systems that Learn from Question?Answer Pairs.We first compare our system (hence-
forth, LJK11) with Clarke et al (2010) (henceforth, CGCR10), which is most similar to
our work in that it also learns from question?answer pairs without using annotated
logical forms. CGCR10 works with the FunQL language and casts semantic parsing as
integer linear programming (ILP). In each iteration, the learning algorithm solves the
432
Liang, Jordan, and Klein Learning Dependency-Based Compositional Semantics
Table 2
Results on GEO with 250 training and 250 test examples. Our system (LJK11 with base triggers
and no logical forms) obtains higher test accuracy than CGCR10, even when CGCR10 is trained
using logical forms.
System Accuracy (%)
CGCR10 w/answers (Clarke et al 2010) 73.2
CGCR10 w/logical forms (Clarke et al 2010) 80.4
LJK11 w/base triggers (Liang, Jordan, and Klein 2011) 84.0
LJK11 w/augmented triggers (Liang, Jordan, and Klein 2011) 87.6
ILP to predict the logical form for each training example. The examples with correct
predictions are fed to a structural support vector machine (SVM) and the model param-
eters are updated.
Though similar in spirit, there are some important differences between CGCR10
and our approach. They use ILP instead of beam search and structural SVM instead of
log-linear models, but the main difference is which examples are used for learning. Our
approach learns on any feasible example (Section 3.2.1), one where the candidate set
contains a logical form that evaluates to the correct answer. CGCR10 uses a much more
stringent criterion: The highest scoring logical formmust evaluate to the correct answer.
Therefore, for their algorithm to progress, the model already must be non-trivially good
before learning even starts. This is reflected in the amount of prior knowledge and
initialization that CGCR10 uses before learning starts: WordNet features, syntactic parse
trees, and a set of lexical triggers with 1.42 words per non-value predicate. Our system
with base triggers requires only simple indicator features, POS tags, and 0.5 words per
non-value predicate.
CGCR10 created a version of GEO which contains 250 training and 250 test exam-
ples. Table 2 compares the empirical results of this split. We see that our system (LJK11)
with base triggers significantly outperforms CGCR10 (84% vs. 73.2%), and it even
outperforms the version of CGCR10 that is trained using logical forms (84.0% vs. 80.4%).
If we use augmented triggers, we widen the gap by another 3.6 percentage points.14
4.2.2 State-of-the-Art Systems. We now compare our system (LJK11) with state-of-the-
art systems, which all require annotated logical forms (except PRECISE). Here is a brief
overview of the systems:
 COCKTAIL (Tang and Mooney 2001) uses inductive logic programming to
learn rules for driving the decisions of a shift-reduce semantic parser. It
assumes that a lexicon (mapping from words to predicates) is provided.
 PRECISE (Popescu, Etzioni, and Kautz 2003) does not use learning, but
instead relies on matching words to strings in the database using various
heuristics based on WordNet and the Charniak parser. Like our work, it
also uses database type constraints to rule out spurious logical forms. One
of the unique features of PRECISE is that it has 100% precision?it refuses
to parse an utterance which it deems semantically intractable.
14 Note that the numbers for LJK11 differ from those presented in Liang, Jordan, and Klein (2011), which
reports results based on 10 different splits rather than the set-up used by CGCR10.
433
Computational Linguistics Volume 39, Number 2
 SCISSOR (Ge and Mooney 2005) learns a generative probabilistic model
that extends the Collins (1999) models with semantic labels, so
that syntactic and semantic parsing can be done jointly.
 SILT (Kate, Wong, and Mooney 2005) learns a set of transformation rules
for mapping utterances to logical forms.
 KRISP (Kate and Mooney 2006) uses SVMs with string kernels to drive the
local decisions of a chart-based semantic parser.
 WASP (Wong and Mooney 2006) uses log-linear synchronous grammars to
transform utterances into logical forms, starting with word alignments
obtained from the IBM models.
 ?-WASP (Wong and Mooney 2007) extends WASP to work with logical
forms that contain bound variables (lambda abstraction).
 LNLZ08 (Lu et al 2008) learns a generative model over hybrid trees,
which are logical forms augmented with natural language words.
IBM model 1 is used to initialize the parameters, and a discriminative
reranking step works on top of the generative model.
 ZC05 (Zettlemoyer and Collins 2005) learns a discriminative log-linear
model over CCG derivations. Starting with a manually constructed
domain-independent lexicon, the training procedure grows the lexicon
by adding lexical entries derived from associating parts of an utterance
with parts of the annotated logical form.
 ZC07 (Zettlemoyer and Collins 2007) extends ZC05 with extra
(disharmonic) combinators to increase the expressive power of the model.
 KZGS10 (Kwiatkowski et al 2010) uses a restricted higher-order
unification procedure, which iteratively breaks up a logical form into
smaller pieces. This approach gradually adds lexical entries of increasing
generality, thus obviating the need for the manually specified templates
used by ZC05 and ZC07 for growing the lexicon. IBM model 1 is used to
initialize the parameters.
 KZGS11 (Kwiatkowski et al 2011) extends KZGS10 by factoring lexical
entries into a template plus a sequence of predicates that fill the slots of
the template. This factorization improves generalization.
With the exception of PRECISE, all other systems require annotated logical forms,
whereas our system learns only from annotated answers. On the other hand, our system
does rely on a fewmanually specified lexical triggers, whereasmany of the later systems
essentially require no manually crafted lexica. For us, the lexical triggers play a crucial
role in the initial stages of learning because they constrain the set of candidate DCS
trees; otherwise we would face a hopelessly intractable search problem. The other
systems induce lexica using unsupervised word alignment (Wong and Mooney 2006,
2007; Kwiatkowski et al 2010, 2011) and/or on-line lexicon learning (Zettlemoyer and
Collins 2005, 2007; Kwiatkowski et al 2010, 2011). Unfortunately, we cannot use these
automatic techniques because they rely on having annotated logical forms.
Table 3 shows the results for GEO. Semantic parsers are typically evaluated on
the accuracy of the logical forms: precision (the accuracy on utterances which are
434
Liang, Jordan, and Klein Learning Dependency-Based Compositional Semantics
Table 3
Results on GEO: Logical form accuracy (LF) and answer accuracy (Answer) of the various
systems. The first group of systems are evaluated using 10-fold cross-validation on all 880
examples; the second are evaluated on the 680+ 200 split of Zettlemoyer and Collins (2005).
Our system (LJK11) with base triggers obtains comparable accuracy to past work, whereas
with augmented triggers, our system obtains the highest overall accuracy.
System LF (%) Answer (%)
COCKTAIL (Tang and Mooney 2001) 79.4 ?
PRECISE (Popescu, Etzioni, and Kautz 2003) 77.5 77.5
SCISSOR (Ge and Mooney 2005) 72.3 ?
SILT (Kate, Wong, and Mooney 2005) 54.1 ?
KRISP (Kate and Mooney 2006) 71.7 ?
WASP (Wong and Mooney 2006) 74.8 ?
?-WASP (Wong and Mooney 2007) 86.6 ?
LNLZ08 (Lu et al 2008) 81.8 ?
ZC05 (Zettlemoyer and Collins 2005) 79.3 ?
ZC07 (Zettlemoyer and Collins 2007) 86.1 ?
KZGS10 (Kwiatkowski et al 2010) 88.2 88.9
KZGS11 (Kwiatkowski et al 2010) 88.6 ?
LJK11 w/base triggers (Liang, Jordan, and Klein 2011) ? 87.9
LJK11 w/augmented triggers (Liang, Jordan, and Klein 2011) ? 91.4
successfully parsed) and recall (the accuracy on all utterances). We only focus on recall
(a lower bound on precision) and simply use the word accuracy to refer to recall.15 Our
system is evaluated only on answer accuracy because our model marginalizes out the
latent logical form. All other systems are evaluated on the accuracy of logical forms. To
calibrate, we also evaluated KZGS10 on answer accuracy and found that it was quite
similar to its logical form accuracy (88.9% vs. 88.2%).16 This does not imply that our
system would necessarily have a high logical form accuracy because multiple logical
forms can produce the same answer, and our system does not receive a training signal
to tease them apart. Even with only base triggers, our system (LJK11) outperforms all
but two of the systems, falling short of KZGS10 by only one percentage point (87.9% vs.
88.9%).17 With augmented triggers, our system takes the lead (91.4% vs. 88.9%).
Table 4 shows the results for JOBS. The two learning-based systems (COCKTAIL
and ZC05) are actually outperformed by PRECISE, which is able to use strong database
type constraints. By exploiting this information and doing learning, we obtain the best
results.
4.3 Empirical Properties
In this section, we try to gain intuition into properties of our approach. All experiments
in this section were performed on random development splits. Throughout this section,
?accuracy? means development test accuracy.
15 Our system produces a logical form for every utterance, and thus our precision is the same as our recall.
16 The 88.2% corresponds to 87.9% in Kwiatkowski et al (2010). The difference is due to using a slightly
newer version of the code.
17 The 87.9% and 91.4% correspond to 88.6% and 91.1% in Liang, Jordan, and Klein (2011). These differences
are due to minor differences in the code.
435
Computational Linguistics Volume 39, Number 2
Table 4
Results on JOBS: Both PRECISE and our system use database type constraints, which results in a
decisive advantage over the other systems. In addition, LJK11 incorporates learning and
therefore obtains the highest accuracies.
System LF (%) Answer (%)
COCKTAIL (Tang and Mooney 2001) 79.4 ?
PRECISE (Popescu, Etzioni, and Kautz 2003) 88.0 88.0
ZC05 (Zettlemoyer and Collins 2005) 79.3 ?
LJK11 w/base triggers (Liang, Jordan, and Klein 2011) ? 90.7
LJK11 w/augmented triggers (Liang, Jordan, and Klein 2011) ? 95.0
4.3.1 Error Analysis. To understand the type of errors our system makes, we examined
one of the development runs, which had 34 errors on the test set. We classified these
errors into the following categories (the number of errors in each category is shown in
parentheses):
 Incorrect POS tags (8): GEO is out-of-domain for our POS tagger, so the
tagger makes some basic errors that adversely affect the predicates that
can be lexically triggered. For example, the questionWhat states border
states . . . is tagged as WP VBZ NN NNS . . . , which means that the first states
cannot trigger state. In another example, major river is tagged as NNP
NNP, so these cannot trigger the appropriate predicates either, and thus
the desired DCS tree cannot even be constructed.
 Non-projectivity (3): The candidate DCS trees are defined by a projective
construction mechanism (Section 2.6) that prohibits edges in the DCS
tree from crossing. This means we cannot handle utterances such as
largest city by area, because the desired DCS tree would have city
dominating area dominating argmax. To construct this DCS tree,
we could allow local reordering of the words.
 Unseen words (2): We never saw at least or sea level at training time.
The former has the correct lexical trigger, but not a sufficiently large
feature weight (0) to encourage its use. For the latter, the problem is
more structural: We have no lexical triggers for 0 :length, and only
adding more lexical triggers can solve this problem.
 Wrong lexical triggers (7): Sometimes the error is localized to a single
lexical trigger. For example, the model incorrectly thinksMississippi
is the state rather than the river, and that Rochester is the city in
New York rather than the name, even though there are contextual
cues to disambiguate in these cases.
 Extra words (5): Sometimes, words trigger predicates that should be
ignored. For example, for population density, the first word triggers
population, which is used rather than density.
 Over-smoothing of DCS tree (9): The first half of our features (Figure 20)
are defined on the DCS tree alone; these produce a form of smoothing
that encourages DCS trees to look alike regardless of the words. We found
436
Liang, Jordan, and Klein Learning Dependency-Based Compositional Semantics
several instances where this essential tool for generalization went too
far. For example, in state of Nevada, the trace predicate border is inserted
between the two nouns, because it creates a structure more similar to
that of the common question what states border Nevada?
4.3.2 Visualization of Features.Having analyzed the behavior of our system for individual
utterances, let us move from the token level to the type level and analyze the learned
parameters of our model. We do not look at raw feature weights, because there are
complex interactions between them not represented by examining individual weights.
Instead, we look at expected feature counts, which we think are more interpretable.
Consider a group of ?competing? features J, for example J = {TRIGGERPRED[city,
p] : p ? P}. We define a distribution q(?) over J as follows:
q( j) =
Nj
?
j??J Nj?
, where (78)
Nj =
?
(x,y)?D
Ep(z|x,Z?L,?,?)[?(x, z)]
Think of q( j) as a marginal distribution (because all our features are positive) that
represents the relative frequencies with which the features j ? J fire with respect to
our training data set D and trained model p(z | x, Z?L,?,?). To appreciate the difference
between what this distribution and raw feature weights capture, suppose we had two
features, j1 and j2, which are identical (?(x, z)j1 ? ?(x, z)j2 ). The weights would be split
across the two features, but the features would have the same marginal distribution
(q(j1) = q(j2)). Figure 23 shows some of the feature distributions learned.
4.3.3 Learning, Search, Bootstrapping. Recall from Section 3.2.1 that a training example
is feasible (with respect to our beam search) if the resulting candidate set contains a
DCS tree with the correct answer. Infeasible examples are skipped, but an example may
become feasible in a later iteration. A natural question is how many training examples
are feasible in each iteration. Figure 24 shows the answer: Initially, only around 30% of
the training examples are feasible; this is not surprising given that all the parameters
are zero, so our beam search is essentially unguided. Training on just these examples
improves the parameters, however, and over the next few iterations, the number of
feasible examples steadily increases to around 97%.
In our algorithm, learning and search are deeply intertwined. Search is of course
needed to learn, but learning also improves search. The general approach is similar in
spirit to Searn (Daume, Langford, andMarcu 2009), althoughwe do not have any formal
guarantees at this point.
Our algorithm also has a bootstrapping flavor. The ?easy? examples are processed
first, where easy is defined by the ability of beam search to generate the correct answer.
This bootstrapping occurs quite naturally: Unlikemost bootstrapping algorithms, we do
not have to set a confidence threshold for accepting new training examples, something
that can be quite tricky to do. Instead, our threshold falls out of the discrete nature of
the beam search.
4.3.4 Effect of Various Settings. So far, we have used our approach with default settings
(Section 4.1.2). How sensitive is the approach to these choices? Table 5 shows the impact
of the feature templates. Figure 25 shows the effect of the number of training examples,
437
Computational Linguistics Volume 39, Number 2
Figure 23
Learned feature distributions. In a feature group (e.g., TRIGGERPRED[city, ?]), each feature is
associated with the marginal probability that the feature fires according to Equation (78). Note
that we have successfully learned that citymeans city, but incorrectly learned that sparsemeans
elevation (due to the confounding fact that Alaska is the most sparse state and has the highest
elevation).
number of training iterations, beam size, and regularization parameter. The overall
conclusion is that there are no big surprises: Our default settings could be improved
on slightly, but these differences are often smaller than the variation across different
development splits.
We now consider the choice of optimization algorithm to update the parameters
given candidate sets (see Figure 21). Thus far, we have been using L-BFGS (Nocedal
1980), which is a batch algorithm. Each iteration, we construct the candidate
Figure 24
The fraction of feasible training examples increases steadily as the parameters, and thus the
beam search improves. Each curve corresponds to a run on a different development split.
438
Liang, Jordan, and Klein Learning Dependency-Based Compositional Semantics
Table 5
There are two classes of feature templates: lexical features (TRIGGERPRED,TRACE*) and
non-lexical features (PREDREL,PREDRELPRED). The lexical features are relatively much more
important for obtaining good accuracy (76.4% vs. 23.1%), but adding the non-lexical features
makes a significant contribution as well (84.7% vs. 76.4%).
Features Accuracy (%)
PRED 13.4? 1.6
PRED + PREDREL 18.4? 3.5
PRED + PREDREL + PREDRELPRED 23.1? 5.0
PRED + TRIGGERPRED 61.3? 1.1
PRED + TRIGGERPRED + TRACE* 76.4? 2.3
PRED + PREDREL + PREDRELPRED + TRIGGERPRED + TRACE* 84.7? 3.5
sets C(t)(x) for all the training examples before solving the optimization problem
argmax?O(?,C
(t) ). We now consider an on-line algorithm, stochastic gradient descent
(SGD) (Robbins and Monro 1951), which updates the parameters after computing
the candidate set for each example. In particular, we iteratively scan through the
training examples in a random order. For each example (x, y), we compute the
candidate set using beam search. We then update the parameters in the direction of
the gradient of the marginal log-likelihood for that example (see Equation (72)) with
step size t??:
?(t+1) ? ?(t) + t??
(
? log p(y | x; Z?L,?(t) ,?)
??
?
?
?
?=?(t)
)
(79)
The trickiest aspect of using SGD is selecting the correct step size: A small ? leads to
quick progress but also instability; a large ? leads to the opposite. We let L-BFGS and
SGD both take the same number of iterations (passes over the training set). Figure 26
shows that a very small value of ? (less than 0.2) is best for our task, even though
only values between 0.5 and 1 guarantee convergence. Our setting is slightly different
because we are interleaving the SGD updates with beam search, which might also
lead to unpredictable consequences. Furthermore, the non-convexity of the objective
function exacerbates the unpredictability (Liang and Klein 2009). Nonetheless, with
a proper ?, SGD converges much faster than L-BFGS and even to a slightly better
solution.
5. Discussion
The work we have presented in this article addresses three important themes. The
first theme is semantic representation (Section 5.1): How do we parametrize the mapping
from utterances to their meanings? The second theme is program induction (Section 5.2):
How do we efficiently search through the space of logical structures given a weak
feedback signal? Finally, the last theme is grounded language (Section 5.3): Howdowe use
constraints from the world to guide learning of language and conversely use language
to interact with the world?
439
Computational Linguistics Volume 39, Number 2
Figure 25
(a) The learning curve shows test accuracy as the number of training examples increases; about
300 examples suffices to get around 80% accuracy. (b) Although our algorithm is not guaranteed
to converge, the test accuracy is fairly stable (with one exception) with more training
iterations?hardly any overfitting occurs. (c) As the beam size increases, the accuracy increases
monotonically, although the computational burden also increases. There is a small gain from our
default setting of K = 100 to the more expensive K = 300. (d) The accuracy is relatively
insensitive to the choice of the regularization parameter for a wide range of values. In fact, no
regularization is also acceptable. This is probably because the features are simple, and the lexical
triggers and beam search already provide some helpful biases.
5.1 Semantic Representation
Since the late nineteenth century, philosophers and linguists have worked on elucidat-
ing the relationship between an utterance and its meaning. One of the pillars of formal
semantics is Frege?s principle of compositionality, that the meaning of an utterance
is built by composing the meaning of its parts. What these parts are and how they
are composed is the main question. The dominant paradigm, which stems from the
seminal work of Richard Montague (1973) in the early 1970s, states that parts are
lambda calculus expressions that correspond to syntactic constituents, and composition
is function application.
440
Liang, Jordan, and Klein Learning Dependency-Based Compositional Semantics
Figure 26
(a) Given the same number of iterations, compared to default batch algorithm (L-BFGS),
the on-line algorithm (stochastic gradient descent) is slightly better for aggressive step
sizes (small ?) and worse for conservative step sizes (large ?). (b) The on-line algorithm
(with an appropriate choice of ?) obtains a reasonable accuracy much faster than L-BFGS.
Consider the compositionality principle from a statistical point of view, where we
construe compositionality as factorization. Factorization, the way a statistical model
breaks into features, is necessary for generalization: It enables us to learn from pre-
viously seen examples and interpret new utterances. Projecting back to Frege?s orig-
inal principle, the parts are the features (Section 3.1.1), and composition is the DCS
construction mechanism (Section 2.6) driven by parameters learned from training
examples.
Taking the statistical view of compositionality, finding a good semantic represen-
tation becomes designing a good statistical model. But statistical modeling must also
deal with the additional issue of language acquisition or learning, which presents
complications: In absorbing training examples, our learning algorithm must inevitably
traverse through intermediate models that are wrong or incomplete. The algorithms
must therefore tolerate this degradation, and do so in a computationally efficient way.
For example, in the line of work on learning probabilistic CCGs (Zettlemoyer and
Collins 2005, 2007; Kwiatkowski et al 2010), many candidate lexical entries must be
entertained for each word even when polysemy does not actually exist (Section 2.6.4).
To improve generalization, the lexicon can be further factorized (Kwiatkowski et al
2011), but this is all done within the constraints of CCG. DCS represents a departure
from this tradition, which replaces a heavily lexicalized constituency-based formalism
with a lightly-lexicalized dependency-based formalism. We can think of DCS as a shift
in linguistic coordinate systems, which makes certain factorizations or features more
accessible. For example, we can define features on paths between predicates in a DCS
tree which capture certain lexical patterns much more easily than in a lambda calculus
expression or a CCG derivation.
DCS has a family resemblance to a semantic representation called natural logic form
(Alshawi, Chang, and Ringgaard 2011), which is also motivated by the benefits of work-
ing with dependency-based logical forms. The goals and the detailed structure of the
two semantic formalisms are different, however. Alshawi, Chang, and Ringgaard (2011)
focus on parsing complex sentences in an open domain where a structured database
or world does not exist. Whereas they do equip their logical forms with a full model-
theoretic semantics, the logical forms are actually closer to dependency trees: Quantifier
scope is left unspecified, and the predicates are simply the words.
441
Computational Linguistics Volume 39, Number 2
Perhaps not immediately apparent is the fact that DCS draws an important idea
from Discourse Representation Theory (DRT) (Kamp and Reyle 1993)?not from the
treatment of anaphora and presupposition which it is known for, but something closer
to its core. This is the idea of having a logical form where all variables are existentially
quantified and constraints are combined via conjunction?a Discourse Representation
Structure (DRS) in DRT, or a basic DCS tree with only join relations. Computationally,
these logical structures conveniently encode CSPs. Linguistically, it appears that existen-
tial quantifiers play an important role and should be treated specially (Kamp and Reyle
1993). DCS takes this core and focuses on semantic compositionality and computation,
whereas DRT focuses more on discourse and pragmatics.
In addition to the statistical view of DCS as a semantic representation, it is use-
ful to think about DCS from the perspective of programming language design. Two
programming languages can be equally expressive, but what matters is how simple it
is to express a desired type of computation in a given language. In some sense, we
designed the DCS formal language to make it easy to represent computations expressed
by natural language. An important part of DCS is themark?execute construct, a uniform
framework for dealing with the divergence between syntactic and semantic scope. This
construct allows us to build simple DCS tree structures and still handle the complexities
of phenomena such as quantifier scope variation. Compared to lambda calculus, think
of DCS as a higher-level programming language tailored to natural language, which
results in simpler programs (DCS trees). Simpler programs are easier for us to work
with and easier for an algorithm to learn.
5.2 Program Induction
Searching over the space of programs is challenging. This is the central computational
challenge of program induction, that of inferring programs (logical forms) from their
behavior (denotations). This problem has been tackled by different communities in
various forms: program induction in AI, programming by demonstration in Human?
Computer Interaction, and program synthesis in programming languages. The core
computational difficulty is that the supervision signal?the behavior?is a complex
function of the program that cannot be easily inverted. What program generated the
output Arizona, Nevada, and Oregon?
Perhaps somewhat counterintuitively, program induction is easier if we infer pro-
grams for not a single task but for multiple tasks. The intuition is that when the tasks
are related, the solution to one task can help another task, both computationally in
navigating the program space and statistically in choosing the appropriate program if
there are multiple feasible possibilities (Liang, Jordan, and Klein 2010). In our semantic
parsing work, we want to infer a logical form for each utterance (task). Clearly the tasks
are related because they use the same vocabulary to talk about the same domain.
Natural language also makes program induction easier by providing side informa-
tion (words) which can be used to guide the search. There have been several papers
that induce programs in this setting: Eisenstein et al (2009) induce conjunctive for-
mulae from natural language instructions, Piantadosi et al (2008) induce first-order
logic formulae using CCG in a small domain assuming observed lexical semantics,
and Clarke et al (2010) induce logical forms in semantic parsing. In the ideal case, the
words would determine the program predicates, and the utterance would determine
the entire program compositionally. But of course, this mapping is not given and must
be learned.
442
Liang, Jordan, and Klein Learning Dependency-Based Compositional Semantics
5.3 Grounded Language
In recent years, there has been an increased interest in connecting language with the
world.18 One of the primary issues in grounded language is alignment?figuring out
what fragments of utterances refer to what aspects of the world. In fact, semantic
parsers trained on examples of utterances and annotated logical form (those discussed
in Section 4.2.2) need to solve the task of aligning words to predicates. Some can learn
from utterances paired with a set of logical forms, one of which is correct (Kate and
Mooney 2007; Chen and Mooney 2008). Liang, Jordan, and Klein (2009) tackle the even
more difficult alignment problem of segmenting and aligning a discourse to a database
of facts, where many parts on either side are irrelevant.
If we know how the world relates to language, we can leverage structure in the
world to guide the learning and interpretation of language.We saw that type constraints
from the database/world reduce the set of candidate logical forms and lead to more
accurate systems (Popescu, Etzioni, and Kautz 2003; Liang, Jordan, and Klein 2011).
Even for syntactic parsing, information from the denotation of an utterance can be
helpful (Schuler 2003).
One of the exciting aspects about using the world for learning language is that
it opens the door to many new types of supervision. We can obtain answers given a
world, which are cheaper to obtain than logical forms (Clarke et al 2010; Liang, Jordan,
and Klein 2011). Other researchers have also pushed in this direction in various ways:
learning a semantic parser based on bootstrapping and estimating the confidence of its
own predictions (Goldwasser et al 2011), learning a semantic parser from user interac-
tions with a dialog system (Artzi and Zettlemoyer 2011), and learning to execute natural
language instructions from just a reward signal using reinforcement learning (Branavan
et al 2009; Branavan, Zettlemoyer, and Barzilay 2010; Branavan, Silver, and Barzilay
2011). In general, supervision from the world is indirectly related to the learning task,
but it is often much more plentiful and natural to obtain.
The benefits can also flow from language to the world. For example, previous work
learned to interpret language to troubleshoot aWindows machine (Branavan et al 2009;
Branavan, Zettlemoyer, and Barzilay 2010), win a game of Civilization (Branavan, Silver,
and Barzilay 2011), play a legal game of solitaire (Eisenstein et al 2009; Goldwasser and
Roth 2011), and navigate a map by following directions (Vogel and Jurafsky 2010; Chen
and Mooney 2011). Even when the objective in the world is defined independently of
language (e.g., in Civilization), language can provide a useful bias towards the non-
linguistic end goal.
6. Conclusions
The main conceptual contribution of this article is a new semantic formalism,
dependency-based compositional semantics (DCS), and techniques to learn a semantic
parser from question?answer pairs where the intermediate logical form (a DCS tree) is
induced in an unsupervised manner. Our final question?answering system was able to
match the accuracies of state-of-the-art systems that learn from annotated logical forms.
There is currently a significant conceptual gap between our question?answering
system (which can be construed as a natural language interface to a database) and
18 Here, world need not refer to the physical world, but could be any virtual world. The point is that the
world has non-trivial structure and exists extra-linguistically.
443
Computational Linguistics Volume 39, Number 2
open-domain question?answering systems. The former focuses on understanding a
question compositionally and computing the answer compositionally, whereas the lat-
ter focuses on retrieving and ranking answers from a large unstructured textual corpus.
The former has depth; the latter has breadth. Developing methods that can both model
the semantic richness of language and scale up to an open-domain setting remains an
open challenge.
We believe that it is possible to push our approach in the open-domain direction.
Neither DCS nor the learning algorithm is tied to having a clean rigid database, which
could instead be a database generated from a noisy information extraction process. The
key is to drive the learning with the desired behavior, the question?answer pairs. The
latent variable is the logical form or program, which just tries to compute the desired
answer by piecing together whatever information is available. Of course, there aremany
open challenges ahead, but with the proper combination of linguistic, statistical, and
computational insight, we hope to eventually build systems with both breadth and
depth.
Acknowledgments
We thank Luke Zettlemoyer and Tom
Kwiatkowski for providing us with data
and answering questions, as well as the
anonymous reviewers for their detailed
feedback. P. L. was supported by an NSF
Graduate Research Fellowship.
References
Alshawi, H., P. Chang, and M. Ringgaard.
2011. Deterministic statistical mapping
of sentences to underspecified
semantics. In International Conference
on Compositional Semantics (IWCS),
pages 15?24, Oxford.
Androutsopoulos, I., G. D. Ritchie, and
P. Thanisch. 1995. Natural language
interfaces to databases?an introduction.
Journal of Natural Language Engineering,
1:29?81.
Artzi, Y. and L. Zettlemoyer. 2011.
Bootstrapping semantic parsers from
conversations. In Empirical Methods in
Natural Language Processing (EMNLP),
pages 421?432, Edinburgh.
Baldridge, J. and G. M. Kruijff. 2002.
Coupling CCG with hybrid logic
dependency semantics. In Association
for Computational Linguistics (ACL),
pages 319?326, Philadelphia, PA.
Barker, C. 2002. Continuations and the
nature of quantification. Natural
Language Semantics, 10:211?242.
Bos, J. 2009. A controlled fragment of
DRT. InWorkshop on Controlled Natural
Language, pages 1?5.
Bos, J., S. Clark, M. Steedman, J. R. Curran,
and J. Hockenmaier. 2004. Wide-coverage
semantic representations from a CCG
parser. In International Conference on
Computational Linguistics (COLING),
pages 1240?1246, Geneva.
Branavan, S., H. Chen, L. S. Zettlemoyer, and
R. Barzilay. 2009. Reinforcement learning
for mapping instructions to actions. In
Association for Computational Linguistics and
International Joint Conference on Natural
Language Processing (ACL-IJCNLP),
pages 82?90, Singapore.
Branavan, S., D. Silver, and R. Barzilay. 2011.
Learning to win by reading manuals in a
Monte-Carlo framework. In Association
for Computational Linguistics (ACL),
pages 268?277.
Branavan, S., L. Zettlemoyer, and R. Barzilay.
2010. Reading between the lines: Learning
to map high-level instructions to
commands. In Association for Computational
Linguistics (ACL), pages 1268?1277,
Portland, OR.
Carpenter, B. 1998. Type-Logical Semantics.
MIT Press, Cambridge, MA.
Chen, D. L. and R. J. Mooney. 2008. Learning
to sportscast: A test of grounded language
acquisition. In International Conference on
Machine Learning (ICML), pages 128?135,
Helsinki.
Chen, D. L. and R. J. Mooney. 2011.
Learning to interpret natural language
navigation instructions from observations.
In Association for the Advancement
of Artificial Intelligence (AAAI),
pages 128?135, Cambridge, MA.
Clarke, J., D. Goldwasser, M. Chang,
and D. Roth. 2010. Driving semantic
parsing from the world?s response.
In Computational Natural Language
Learning (CoNLL), pages 18?27,
Uppsala.
444
Liang, Jordan, and Klein Learning Dependency-Based Compositional Semantics
Collins, M. 1999. Head-Driven Statistical
Models for Natural Language Parsing.
Ph.D. thesis, University of Pennsylvania.
Cooper, R. 1975.Montague?s semantic theory
and transformational syntax. Ph.D. thesis,
University of Massachusetts at Amherst.
Cousot, P. and R. Cousot. 1977. Abstract
interpretation: A unified lattice model for
static analysis of programs by construction
or approximation of fixpoints. In Principles
of Programming Languages (POPL),
pages 238?252, Los Angeles, CA.
Daume, H., J. Langford, and D. Marcu.
2009. Search-based structured prediction.
Machine Learning Journal (MLJ), 75:297?325.
Dechter, R. 2003. Constraint Processing.
Morgan Kaufmann.
Eisenstein, J., J. Clarke, D. Goldwasser,
and D. Roth. 2009. Reading to learn:
Constructing features from semantic
abstracts. In Empirical Methods in
Natural Language Processing (EMNLP),
pages 958?967, Singapore.
Ge, R. and R. J. Mooney. 2005. A statistical
semantic parser that integrates syntax
and semantics. In Computational Natural
Language Learning (CoNLL), pages 9?16,
Ann Arbor, MI.
Giordani, A. and A. Moschitti. 2009.
Semantic mapping between natural
language questions and SQL queries
via syntactic pairing. In International
Conference on Applications of Natural
Language to Information Systems,
pages 207?221, Saarbru?cken.
Goldwasser, D., R. Reichart, J. Clarke,
and D. Roth. 2011. Confidence driven
unsupervised semantic parsing. In
Association for Computational Linguistics
(ACL), pages 1486?1495, Barcelona.
Goldwasser, D. and D. Roth. 2011. Learning
from natural instructions. In International
Joint Conference on Artificial Intelligence
(IJCAI), pages 1794?1800, Portland, OR.
Heim, I. and A. Kratzer. 1998. Semantics in
Generative Grammar. Wiley-Blackwell,
Oxford.
Judge, J., A. Cahill, and J. v. Genabith.
2006. Question-bank: Creating a
corpus of parse-annotated questions.
In International Conference on Computational
Linguistics and Association for Computational
Linguistics (COLING/ACL), pages 497?504,
Sydney.
Kamp, H. and U. Reyle. 1993. From Discourse
to Logic: An Introduction to the
Model-theoretic Semantics of Natural
Language, Formal Logic and Discourse
Representation Theory. Kluwer, Dordrecht.
Kamp, H., J. van Genabith, and U. Reyle.
2005. Discourse representation theory.
In Handbook of Philosophical Logic,
Kluwer, Dordrecht.
Kate, R. J. and R. J. Mooney. 2006. Using
string-kernels for learning semantic
parsers. In International Conference on
Computational Linguistics and Association for
Computational Linguistics (COLING/ACL),
pages 913?920, Sydney.
Kate, R. J. and R. J. Mooney. 2007.
Learning language semantics from
ambiguous supervision. In Association
for the Advancement of Artificial
Intelligence (AAAI), pages 895?900,
Cambridge, MA.
Kate, R. J., Y. W. Wong, and R. J. Mooney.
2005. Learning to transform natural to
formal languages. In Association for the
Advancement of Artificial Intelligence
(AAAI), pages 1062?1068.
Kwiatkowski, T., L. Zettlemoyer,
S. Goldwater, and M. Steedman. 2010.
Inducing probabilistic CCG grammars
from logical form with higher-order
unification. In Empirical Methods in
Natural Language Processing (EMNLP),
pages1223?1233, Cambridge, MA.
Kwiatkowski, T., L. Zettlemoyer,
S. Goldwater, and M. Steedman. 2011.
Lexical generalization in CCG grammar
induction for semantic parsing. In
Empirical Methods in Natural Language
Processing (EMNLP), pages 1512?1523,
Cambridge, MA.
Liang, P. 2011. Learning Dependency-Based
Compositional Semantics. Ph.D. thesis,
University of California at Berkeley.
Liang, P., M. I. Jordan, and D. Klein. 2009.
Learning semantic correspondences
with less supervision. In Association for
Computational Linguistics and International
Joint Conference on Natural Language
Processing (ACL-IJCNLP), pages 91?99,
Singapore.
Liang, P., M. I. Jordan, and D. Klein. 2010.
Learning programs: A hierarchical
Bayesian approach. In International
Conference on Machine Learning (ICML),
pages 639?646, Haifa.
Liang, P., M. I. Jordan, and D. Klein.
2011. Learning dependency-based
compositional semantics. In Association
for Computational Linguistics (ACL),
pages 590?599, Portland, OR.
Liang, P. and D. Klein. 2009. Online EM for
unsupervised models. In North American
Association for Computational Linguistics
(NAACL), pages 611?619, Boulder, CO.
445
Computational Linguistics Volume 39, Number 2
Lu, W., H. T. Ng, W. S. Lee, and L. S.
Zettlemoyer. 2008. A generative model for
parsing natural language to meaning
representations. In Empirical Methods in
Natural Language Processing (EMNLP),
pages 783?792, Honolulu, HI.
Marcus, M. P., M. A. Marcinkiewicz, and
B. Santorini. 1993. Building a large
annotated corpus of English: The Penn
Treebank. Computational Linguistics,
19:313?330.
Miller, S., D. Stallard, R. Bobrow, and
R. Schwartz. 1996. A fully statistical
approach to natural language interfaces.
In Association for Computational Linguistics
(ACL), pages 55?61, Santa Cruz, CA.
Montague, R. 1973. The proper treatment
of quantification in ordinary English.
In J. Hiutikka, J. Moravcsik, and
P. Suppes, editors, Approaches to Natural
Language, pages 221?242, Dordrecht,
The Netherlands.
Nocedal, J. 1980. Updating quasi-Newton
matrices with limited storage.Mathematics
of Computation, 35:773?782.
Petrov, S., L. Barrett, R. Thibaux, and
D. Klein. 2006. Learning accurate,
compact, and interpretable tree
annotation. In International Conference on
Computational Linguistics and Association for
Computational Linguistics (COLING/ACL),
pages 433?440, Sydney.
Piantadosi, S. T., N. D. Goodman, B. A. Ellis,
and J. B. Tenenbaum. 2008. A Bayesian
model of the acquisition of compositional
semantics. In Proceedings of the Thirtieth
Annual Conference of the Cognitive Science
Society, pages 1620?1625, Washington, DC.
Popescu, A., O. Etzioni, and H. Kautz. 2003.
Towards a theory of natural language
interfaces to databases. In International
Conference on Intelligent User Interfaces
(IUI), pages 149?157, Miami, FL.
Porter, M. F. 1980. An algorithm for suffix
stripping. Program, 14:130?137.
Robbins, H. and S. Monro. 1951. A stochastic
approximation method. Annals of
Mathematical Statistics, 22(3):400?407.
Schuler, W. 2003. Using model-theoretic
semantic interpretation to guide statistical
parsing and word recognition in a spoken
language interface. In Association for
Computational Linguistics (ACL),
pages 529?536, Sapporo.
Shan, C. 2004. Delimited continuations in
natural language. Technical report, ArXiv.
Available at http://arvix.org/abs/
cs.CL/0404006.
Steedman, M. 2000. The Syntactic Process.
MIT Press, Cambridge, MA.
Tang, L. R. and R. J. Mooney. 2001. Using
multiple clause constructors in inductive
logic programming for semantic parsing.
In European Conference on Machine Learning,
pages 466?477, Freiburg.
Vogel, A. and D. Jurafsky. 2010. Learning
to follow navigational directions.
In Association for Computational Linguistics
(ACL), pages 806?814, Uppsala.
Wainwright, M. and M. I. Jordan. 2008.
Graphical models, exponential families,
and variational inference. Foundations and
Trends in Machine Learning, 1:1?307.
Warren, D. and F. Pereira. 1982. An efficient
easily adaptable system for interpreting
natural language queries. Computational
Linguistics, 8:110?122.
White, M. 2006. Efficient realization of
coordinate structures in combinatory
categorial grammar. Research on Language
and Computation, 4:39?75.
Wong, Y. W. and R. J. Mooney. 2006.
Learning for semantic parsing with
statistical machine translation. In North
American Association for Computational
Linguistics (NAACL), pages 439?446,
New York, NY.
Wong, Y. W. and R. J. Mooney. 2007.
Learning synchronous grammars for
semantic parsing with lambda calculus.
In Association for Computational Linguistics
(ACL), pages 960?967, Prague.
Woods, W. A., R. M. Kaplan, and
B. N. Webber. 1972. The lunar sciences
natural language information system:
Final report. Technical Report 2378,
Bolt Beranek and Newman Inc.,
Cambridge, MA.
Zelle, M. and R. J. Mooney. 1996. Learning to
parse database queries using inductive
logic programming. In Association for the
Advancement of Artificial Intelligence
(AAAI), pages 1050?1055, Cambridge, MA.
Zettlemoyer, L. S. and M. Collins. 2005.
Learning to map sentences to logical
form: Structured classification with
probabilistic categorial grammars.
In Uncertainty in Artificial Intelligence
(UAI), pages 658?666.
Zettlemoyer, L. S. and M. Collins. 2007.
Online learning of relaxed CCG grammars
for parsing to logical form. In Empirical
Methods in Natural Language Processing
and Computational Natural Language
Learning (EMNLP/CoNLL), pages 678?687,
Prague.
446

Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 573?581,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Type-Based MCMC
Percy Liang
UC Berkeley
pliang@cs.berkeley.edu
Michael I. Jordan
UC Berkeley
jordan@cs.berkeley.edu
Dan Klein
UC Berkeley
klein@cs.berkeley.edu
Abstract
Most existing algorithms for learning latent-
variable models?such as EM and existing
Gibbs samplers?are token-based, meaning
that they update the variables associated with
one sentence at a time. The incremental na-
ture of these methods makes them suscepti-
ble to local optima/slow mixing. In this paper,
we introduce a type-based sampler, which up-
dates a block of variables, identified by a type,
which spans multiple sentences. We show im-
provements on part-of-speech induction, word
segmentation, and learning tree-substitution
grammars.
1 Introduction
A long-standing challenge in NLP is the unsu-
pervised induction of linguistic structures, for ex-
ample, grammars from raw sentences or lexicons
from phoneme sequences. A fundamental property
of these unsupervised learning problems is multi-
modality. In grammar induction, for example, we
could analyze subject-verb-object sequences as ei-
ther ((subject verb) object) (mode 1) or (subject
(verb object)) (mode 2).
Multimodality causes problems for token-based
procedures that update variables for one example at
a time. In EM, for example, if the parameters al-
ready assign high probability to the ((subject verb)
object) analysis, re-analyzing the sentences in E-step
only reinforces the analysis, resulting in EM getting
stuck in a local optimum. In (collapsed) Gibbs sam-
pling, if all sentences are already analyzed as ((sub-
ject verb) object), sampling a sentence conditioned
2 1 2 2 1
2 2 2 1 2
1 2 1 2 2
2 1 2 2 1
2 2 2 1 2
1 2 1 2 2
2 1 2 2 1
2 2 2 1 2
1 2 1 2 2
(a) token-based (b) sentence-based (c) type-based
Figure 1: Consider a dataset of 3 sentences, each of
length 5. Each variable is labeled with a type (1 or 2). The
unshaded variables are the ones that are updated jointly
by a sampler. The token-based sampler updates the vari-
able for one token at a time (a). The sentence-based sam-
pler updates all variables in a sentence, thus dealing with
intra-sentential dependencies (b). The type-based sam-
pler updates all variables of a particular type (1 in this ex-
ample), thus dealing with dependencies due to common
parameters (c).
on all others will most likely not change its analysis,
resulting in slow mixing.
To combat the problems associated with token-
based algorithms, we propose a new sampling algo-
rithm that operates on types. Our sampler would, for
example, be able to change all occurrences of ((sub-
ject verb) object) to (subject (verb object)) in one
step. These type-based operations are reminiscent of
the type-based grammar operations of early chunk-
merge systems (Wolff, 1988; Stolcke and Omohun-
dro, 1994), but we work within a sampling frame-
work for increased robustness.
In NLP, perhaps the the most simple and popu-
lar sampler is the token-based Gibbs sampler,1 used
in Goldwater et al (2006), Goldwater and Griffiths
(2007), and many others. By sampling only one
1In NLP, this is sometimes referred to as simply the col-
lapsed Gibbs sampler.
573
variable at a time, this sampler is prone to slow mix-
ing due to the strong coupling between variables.
A general remedy is to sample blocks of coupled
variables. For example, the sentence-based sampler
samples all the variables associated with a sentence
at once (e.g., the entire tag sequence). However, this
blocking does not deal with the strong type-based
coupling (e.g., all instances of a word should be
tagged similarly). The type-based sampler we will
present is designed exactly to tackle this coupling,
which we argue is stronger and more important to
deal with in unsupervised learning. Figure 1 depicts
the updates made by each of the three samplers.
We tested our sampler on three models: a
Bayesian HMM for part-of-speech induction (Gold-
water and Griffiths, 2007), a nonparametric
Bayesian model for word segmentation (Goldwater
et al, 2006), and a nonparametric Bayesian model of
tree substitution grammars (Cohn et al, 2009; Post
and Gildea, 2009). Empirically, we find that type-
based sampling improves performance and is less
sensitive to initialization (Section 5).
2 Basic Idea via a Motivating Example
The key technical problem we solve in this paper is
finding a block of variables which are both highly
coupled and yet tractable to sample jointly. This
section illustrates the main idea behind type-based
sampling on a small word segmentation example.
Suppose our dataset x consists of n occurrences
of the sequence a b. Our goal is infer z =
(z1, . . . , zn), where zi = 0 if the sequence is one
word ab, and zi = 1 if the sequence is two, a
and b. We can model this situation with a simple
generative model: for each i = 1, . . . , n, gener-
ate one or two words with equal probability. Each
word is drawn independently based on probabilities
? = (?a, ?b, ?ab) which we endow with a uniform
prior ? ? Dirichlet(1, 1, 1).
We marginalize out ? to get the following standard
expression (Goldwater et al, 2009):
p(z | x) ?
1(m)1(m)1(n?m)
3(n+m)
def
= g(m), (1)
where m =
?n
i=1 zi is the number of two-word se-
quences and a(k) = a(a + 1) ? ? ? (a + k ? 1) is the
200 400 600 8001000
m
-1411.4
-1060.3
-709.1
-358.0
-6.8
log
g(m
)
2 4 6 8 10
iteration
200
400
600
800
1000
m
Token
Type
(a) bimodal posterior (b) sampling run
Figure 2: (a) The posterior (1) is sharply bimodal (note
the log-scale). (b) A run of the token-based and type-
based samplers. We initialize both samplers with m = n
(n = 1000). The type-based sampler mixes instantly
(in fact, it makes independent draws from the posterior)
whereas the token-based sampler requires five passes
through the data before finding the high probability re-
gion m u 0.
ascending factorial.2 Figure 2(a) depicts the result-
ing bimodal posterior.
A token-based sampler chooses one zi to update
according to the posterior p(zi | z?i,x). To illus-
trate the mixing problem, consider the case where
m = n, i.e., all sequences are analyzed as two
words. From (1), we can verify that p(zi = 0 |
z?i,x) = O( 1n). When n = 1000, this means that
there is only a 0.002 probability of setting zi = 0,
a very unlikely but necessary first step to take to es-
cape this local optimum. Indeed, Figure 2(b) shows
how the token-based sampler requires five passes
over the data to finally escape.
Type-based sampling completely eradicates the
local optimum problem in this example. Let us take
a closer look at (1). Note that p(z | x) only depends
on a single integer m, which only takes one of n+ 1
values, not on the particular z. This shows that the
zis are exchangeable. There are
(n
m
)
possible val-
ues of z satisfying m =
?
i zi, each with the same
probability g(m). Summing, we get:
p(m | x) ?
?
z:m=
P
i zi
p(x, z) =
(
n
m
)
g(m). (2)
A sampling strategy falls out naturally: First, sample
the number m via (2). Conditioned on m, choose
2The ascending factorial function arises from marginaliz-
ing Dirichlet distributions and is responsible the rich-gets-richer
phenomenon: the larger n is, more we gain by increasing it.
574
the particular z uniformly out of the
(n
m
)
possibili-
ties. Figure 2(b) shows the effectiveness of this type-
based sampler.
This simple example exposes the fundamental
challenge of multimodality in unsupervised learn-
ing. Both m = 0 and m = n are modes due to the
rich-gets-richer property which arises by virtue of
all n examples sharing the same parameters ?. This
sharing is a double-edged sword: It provides us with
clustering structure but also makes inference hard.
Even though m = n is much worse (by a factor ex-
ponential in n) than m = 0, a na??ve algorithm can
easily have trouble escaping m = n.
3 Setup
We will now present the type-based sampler in full
generality. Our sampler is applicable to any model
which is built out of local multinomial choices,
where each multinomial has a Dirichlet process prior
(a Dirichlet prior if the number of choices is finite).
This includes most probabilistic models in NLP (ex-
cluding ones built from log-linear features).
As we develop the sampler, we will pro-
vide concrete examples for the Bayesian hidden
Markov model (HMM), the Dirichlet process uni-
gram segmentation model (USM) (Goldwater et al,
2006), and the probabilistic tree-substitution gram-
mar (PTSG) (Cohn et al, 2009; Post and Gildea,
2009).
3.1 Model parameters
A model is specified by a collection of multino-
mial parameters ? = {?r}r?R, where R is an in-
dex set. Each vector ?r specifies a distribution over
outcomes: outcome o has probability ?ro.
? HMM: Let K is the number of states. The set
R = {(q, k) : q ? {T,E}, k = 1, . . . ,K}
indexes the K transition distributions {?(T,k)}
(each over outcomes {1, . . . ,K}) and K emis-
sion distributions {?(E,k)} (each over the set of
words).
? USM: R = {0}, and ?0 is a distribution over (an
infinite number of) words.
? PTSG: R is the set of grammar symbols, and
each ?r is a distribution over labeled tree frag-
ments with root label r.
R index set for parameters
? = {?r}r?R multinomial parameters
? = {?r}r?R base distributions (fixed)
S set of sites
b = {bs}s?S binary variables (to be sampled)
z latent structure (set of choices)
z?s choices not depending on site s
zs:b choices after setting bs = b
?zs:b zs:b\z?s: new choices from bs = b
S ? S sites selected for sampling
m # sites in S assigned bs = 1
n = {nro} counts (sufficient statistics of z)
Table 1: Notation used in this paper. Note that there is a
one-to-one mapping between z and (b,x). The informa-
tion relevant for evaluating the likelihood is n. We use
the following parallel notation: n?s = n(z?s),ns:b =
n(zs:b),?ns = n(?zs).
3.2 Choice representation of latent structure z
We represent the latent structure z as a set of local
choices:3
? HMM: z contains elements of the form
(T, i, a, b), denoting a transition from state
a at position i to state b at position i + 1; and
(E, i, a, w), denoting an emission of word w
from state a at position i.
? USM: z contains elements of the form (i, w), de-
noting the generation of word w at character po-
sition i extending to position i+ |w| ? 1.
? PTSG: z contains elements of the form (x, t), de-
noting the generation of tree fragment t rooted at
node x.
The choices z are connected to the parameters ?
as follows: p(z | ?) =
?
z?z ?z.r,z.o. Each choice
z ? z is identified with some z.r ? R and out-
come z.o. Intuitively, choice z was made by drawing
drawing z.o from the multinomial distribution ?z.r.
3.3 Prior
We place a Dirichlet process prior on ?r (Dirichlet
prior for finite outcome spaces): ?r ? DP(?r, ?r),
where ?r is a concentration parameter and ?r is a
fixed base distribution.
3We assume that z contains both a latent part and the ob-
served input x, i.e., x is a deterministic function of z.
575
Let nro(z) = |{z ? z : z.r = r, z.o = o}| be the
number of draws from ?r resulting in outcome o, and
nr? =
?
o nro be the number of times ?r was drawn
from. Let n(z) = {nro(z)} denote the vector of
sufficient statistics associated with choices z. When
it is clear from context, we simply write n for n(z).
Using these sufficient statistics, we can write p(z |
?) =
?
r,o ?
nro(z)
ro .
We now marginalize out ? using Dirichlet-
multinomial conjugacy, producing the following ex-
pression for the likelihood:
p(z) =
?
r?R
?
o (?ro?ro)
(nro(z))
?r(nr?(z))
, (3)
where a(k) = a(a+1) ? ? ? (a+k?1) is the ascending
factorial. (3) is the distribution that we will use for
sampling.
4 Type-Based Sampling
Having described the setup of the model, we now
turn to posterior inference of p(z | x).
4.1 Binary Representation
We first define a new representation of the latent
structure based on binary variables b so that there is
a bijection between z and (b,x); z was used to de-
fine the model, b will be used for inference. We will
use b to exploit the ideas from Section 2. Specifi-
cally, let b = {bs}s?S be a collection of binary vari-
ables indexed by a set of sites S.
? HMM: If the HMM hasK = 2 states, S is the set
of positions in the sequence. For each s ? S , bs
is the hidden state at s. The extension to general
K is considered at the end of Section 4.4.
? USM: S is the set of non-final positions in the
sequence. For each s ? S , bs denotes whether
a word boundary exists between positions s and
s+ 1.
? PTSG: S is the set of internal nodes in the parse
tree. For s ? S, bs denotes whether a tree frag-
ment is rooted at node s.
For each site s ? S, let zs:0 and zs:1 denote the
choices associated with the structures obtained by
setting the binary variable bs = 0 and bs = 1, re-
spectively. Define z?s
def
= zs:0 ? zs:1 to be the set
of choices that do not depend on the value of bs, and
n?s
def
= n(z?s) be the corresponding counts.
? HMM: z?s includes all but the transitions into
and out of the state at s plus the emission at s.
? USM: z?s includes all except the word ending at
s and the one starting at s+ 1 if there is a bound-
ary (bs = 1); except the word covering s if no
boundary exists (bs = 0).
? PTSG: z?s includes all except the tree fragment
rooted at node s and the one with leaf s if bs = 1;
except the single fragment containing s if bs = 0.
4.2 Sampling One Site
A token-based sampler considers one site s at a time.
Specifically, we evaluate the likelihoods of zs:0 and
zs:1 according to (3) and sample bs with probability
proportional to the likelihoods. Intuitively, this can
be accomplished by removing choices that depend
on bs (resulting in z?s), evaluating the likelihood re-
sulting from setting bs to 0 or 1, and then adding the
appropriate choices back in.
More formally, let ?zs:b
def
= zs:b\z?s be the new
choices that would be added if we set bs = b ?
{0, 1}, and let ?ns:b
def
= n(?zs:b) be the corre-
sponding counts. With this notation, we can write
the posterior as follows:
p(bs = b | b\bs) ? (4)
?
r?R
?
o (?ro?ro + n
?s
ro )
(?ns:bro )
(?r + n
?s
r? )
(?ns:br? )
.
The form of the conditional (4) follows from the
joint (3) via two properties: additivity of counts
(ns:b = n?s + ?ns:b) and a simple property of as-
cending factorials (a(k+?) = a(k)(a+ k)(?)).
In practice, most of the entries of ?ns:b are zero.
For the HMM, ns:bro would be nonzero only for
the transitions into the new state (b) at position s
(zs?1 ? b), transitions out of that state (b? zs+1),
and emissions from that state (b? xs).
4.3 Sampling Multiple Sites
We would like to sample multiple sites jointly as in
Section 2, but we cannot choose any arbitrary subset
S ? S, as the likelihood will in general depend on
the exact assignment of bS
def
= {bs}s?S , of which
576
a b c a a b c a b c b
(a) USM
1 1 2 2 1 1 2 2
a b a b c b b e
(b) HMM
a
b
a a
b c
d e
c
d
b c
e
a b
(c) PTSG
Figure 3: The type-based sampler jointly samples all vari-
ables at a set of sites S (in green boxes). Sites in S are
chosen based on types (denoted in red). (a) HMM: two
sites have the same type if they have the same previous
and next states and emit the same word; they conflict un-
less separated by at least one position. (b) USM: two sites
have the same type if they are both of the form ab|c or
abc; note that occurrences of the same letters with other
segmentations do not match the type. (c) PTSG: analo-
gous to the USM, only for tree rather than sequences.
there are an exponential number. To exploit the ex-
changeability property in Section 2, we need to find
sites which look ?the same? from the model?s point
of view, that is, the likelihood only depends on bS
via m
def
=
?
s?S bs.
To do this, we need to define two notions, type and
conflict. We say sites s and s? have the same type if
the counts added by setting either bs or bs? are the
same, that is, ?ns:b = ?ns
?:b for b ? {0, 1}. This
motivates the following definition of the type of site
s with respect to z:
t(z, s)
def
= (?ns:0,?ns:1), (5)
We say that s and s? have the same type if t(z, s) =
t(z, s?). Note that the actual choices added (?zs:b
and ?zs
?:b) are in general different as s and s? cor-
respond to different parts of the latent structure, but
the model only depends on counts and is indifferent
to this. Figure 3 shows examples of same-type sites
for our three models.
However, even if all sites in S have the same
type, we still cannot sample bS jointly, since chang-
ing one bs might change the type of another site s?;
indeed, this dependence is reflected in (5), which
shows that types depend on z. For example, s, s? ?
S conflict when s? = s + 1 in the HMM or when
s and s? are boundaries of one segment (USM) or
one tree fragment (PTSG). Therefore, one additional
concept is necessary: We say two sites s and s? con-
flict if there is some choice that depends on both bs
and bs? ; formally, (z\z?s) ? (z\z?s
?
) 6= ?.
Our key mathematical result is as follows:
Proposition 1 For any set S ? S of non-conflicting
sites with the same type,
p(bS | b\bS) ? g(m) (6)
p(m | b\bS) ?
(
|S|
m
)
g(m), (7)
for some easily computable g(m), where m =
?
s?S bs.
We will derive g(m) shortly, but first note from
(6) that the likelihood for a particular setting of bS
depends on bS only via m as desired. (7) sums
over all
(|S|
m
)
settings of bS with m =
?
s?S bs.
The algorithmic consequences of this result is that
to sample bS , we can first compute (7) for each
m ? {0, . . . , |S|}, sample m according to the nor-
malized distribution, and then choose the actual bS
uniformly subject to m.
Let us now derive g(m) by generalizing (4).
Imagine removing all sites S and their dependent
choices and adding in choices corresponding to
some assignment bS . Since all sites in S are non-
conflicting and of the same type, the count contribu-
tion ?ns:b is the same for every s ? S (i.e., sites
in S are exchangeable). Therefore, the likelihood
of the new assignment bS depends only on the new
counts:
?nS:m
def
= m?ns:1 + (|S| ?m)?ns:0. (8)
Using these new counts in place of the ones in (4),
we get the following expression:
g(m) =
?
r?R
?
o (?ro?ro + nro(z
?S))
(?nS:mro )
?r + nr?(z?S)
(?nS:mr? )
. (9)
4.4 Full Algorithm
Thus far, we have shown how to sample bS given
a set S ? S of non-conflicting sites with the same
type. To complete the description of the type-based
577
Type-Based Sampler
for each iteration t = 1, . . . , T :
?for each pivot site s0 ? S:
??S ? TB(z, s0) (S is the type block centered at s0)
??decrement n and remove from z based on bS
??sample m according to (7)
??sample M ? S with |M | = m uniformly at random
??set bs = I[s ?M ] for each s ? S
??increment n and add to z accordingly
Figure 4: Pseudocode for the general type-based sampler.
We operate in the binary variable representation b of z.
Each step, we jointly sample |S| variables (of the same
type).
sampler, we need to specify how to choose S. Our
general strategy is to first choose a pivot site s0 ? S
uniformly at random and then set S = TB(z, s0) for
some function TB. Call S the type block centered at
s0. The following two criteria on TB are sufficient
for a valid sampler: (A) s0 ? S, and (B) the type
blocks are stable, which means that if we change bS
to any b?S (resulting in a new z
?), the type block cen-
tered at s0 with respect to z? does not change (that
is, TB(z?, s0) = S). (A) ensures ergodicity; (B),
reversibility.
Now we define TB as follows: First set S = {s0}.
Next, loop through all sites s ? S with the same type
as s0 in some fixed order, adding s to S if it does
not conflict with any sites already in S. Figure 4
provides the pseudocode for the full algorithm.
Formally, this sampler cycles over |S| transition
kernels, one for each pivot site. Each kernel (in-
dexed by s0 ? S) defines a blocked Gibbs move,
i.e. sampling from p(bTB(z,s0) | ? ? ? ).
Efficient Implementation There are two oper-
ations we must perform efficiently: (A) looping
through sites with the same type as the pivot site s0,
and (B) checking whether such a site s conflicts with
any site in S. We can perform (B) in O(1) time by
checking if any element of ?zs:bs has already been
removed; if so, there is a conflict and we skip s. To
do (A) efficiently, we maintain a hash table mapping
type t to a doubly-linked list of sites with type t.
There is anO(1) cost for maintaining this data struc-
ture: When we add or remove a site s, we just need
to add or remove neighboring sites s? from their re-
spective linked lists, since their types depend on bs.
For example, in the HMM, when we remove site s,
we also remove sites s?1 and s+1.
For the USM, we use a simpler solution: main-
tain a hash table mapping each word w to a list of
positions where w occurs. Suppose site (position) s
straddles words a and b. Then, to perform (A), we
retrieve the list of positions where a, b, and ab occur,
intersecting the a and b lists to obtain a list of posi-
tions where a b occurs. While this intersection is
often much smaller than the pre-intersected lists, we
found in practice that the smaller amount of book-
keeping balanced out the extra time spent intersect-
ing. We used a similar strategy for the PTSG, which
significantly reduces the amount of bookkeeping.
Skip Approximation Large type blocks mean
larger moves. However, such a block S is also sam-
pled more frequently?once for every choice of a
pivot site s0 ? S. However, we found that empir-
ically, bS changes very infrequently. To eliminate
this apparent waste, we use the following approxi-
mation of our sampler: do not consider s0 ? S as
a pivot site if s0 belongs to some block which was
already sampled in the current iteration. This way,
each site is considered roughly once per iteration.4
Sampling Non-Binary Representations We can
sample in models without a natural binary represen-
tation (e.g., HMMs with with more than two states)
by considering random binary slices. Specifically,
suppose bs ? {1, . . . ,K} for each site s ? S .
We modify Figure 4 as follows: After choosing a
pivot site s0 ? S , let k = bs0 and choose k
? uni-
formly from {1, . . . ,K}. Only include sites in one
of these two states by re-defining the type block to
be S = {s ? TB(z, s0) : bs ? {k, k?}}, and sam-
ple bS restricted to these two states by drawing from
p(bS | bS ? {k, k?}|S|, ? ? ? ). By choosing a random
k? each time, we allow b to reach any point in the
space, thus achieving ergodicity just by using these
binary restrictions.
5 Experiments
We now compare our proposed type-based sampler
to various alternatives, evaluating on marginal like-
4A site could be sampled more than once if it belonged to
more than one type block during the iteration (recall that types
depend on z and thus could change during sampling).
578
lihood (3) and accuracy for our three models:
? HMM: We learned a K = 45 state HMM on
the Wall Street Journal (WSJ) portion of the Penn
Treebank (49208 sentences, 45 tags) for part-of-
speech induction. We fixed ?r to 0.1 and ?r to
uniform for all r.
For accuracy, we used the standard metric based
on greedy mapping, where each state is mapped
to the POS tag that maximizes the number of cor-
rect matches (Haghighi and Klein, 2006). We did
not use a tagging dictionary.
? USM: We learned a USM model on the
Bernstein-Ratner corpus from the CHILDES
database used in Goldwater et al (2006) (9790
sentences) for word segmentation. We fixed ?0 to
0.1. The base distribution ?0 penalizes the length
of words (see Goldwater et al (2009) for details).
For accuracy, we used word token F1.
? PTSG: We learned a PTSG model on sections 2?
21 of the WSJ treebank.5 For accuracy, we used
EVALB parsing F1 on section 22.6 Note this is a
supervised task with latent-variables, whereas the
other two are purely unsupervised.
5.1 Basic Comparison
Figure 5(a)?(c) compares the likelihood and accu-
racy (we use the term accuracy loosely to also in-
clude F1). The initial observation is that the type-
based sampler (TYPE) outperforms the token-based
sampler (TOKEN) across all three models on both
metrics.
We further evaluated the PTSG on parsing. Our
standard treebank PCFG estimated using maximum
likelihood obtained 79% F1. TOKEN obtained an F1
of 82.2%, and TYPE obtained a comparable F1 of
83.2%. Running the PTSG for longer continued to
5Following Petrov et al (2006), we performed an initial pre-
processing step on the trees involving Markovization, binariza-
tion, and collapsing of unary chains; words occurring once are
replaced with one of 50 ?unknown word? tokens, using base
distributions {?r} that penalize the size of trees, and sampling
the hyperparameters (see Cohn et al (2009) for details).
6To evaluate, we created a grammar where the rule proba-
bilities are the mean values under the PTSG distribution: this
involves taking a weighted combination (based on the concen-
tration parameters) of the rule counts from the PTSG samples
and the PCFG-derived base distribution. We used the decoder
of DeNero et al (2009) to parse.
improve the likelihood but actually hurt parsing ac-
curacy, suggesting that the PTSG model is overfit-
ting.
To better understand the gains from TYPE
over TOKEN, we consider three other alterna-
tive samplers. First, annealing (TOKENanneal) is
a commonly-used technique to improve mixing,
where (3) is raised to some inverse temperature.7
In Figure 5(a)?(c), we see that unlike TYPE,
TOKENanneal does not improve over TOKEN uni-
formly: it hurts for the HMM, improves slightly for
the USM, and makes no difference for the PTSG. Al-
though annealing does increase mobility of the sam-
pler, this mobility is undirected, whereas type-based
sampling increases mobility in purely model-driven
directions.
Unlike past work that operated on types (Wolff,
1988; Brown et al, 1992; Stolcke and Omohun-
dro, 1994), type-based sampling makes stochastic
choices, and moreover, these choices are reversible.
Is this stochasticity important? To answer this, we
consider a variant of TYPE, TYPEgreedy: instead
of sampling from (7), TYPEgreedy considers a type
block S and sets bs to 0 for all s ? S if p(bS =
(0, . . . , 0) | ? ? ? ) > p(bS = (1, . . . , 1) | ? ? ? ); else
it sets bs to 1 for all s ? S. From Figure 5(a)?(c),
we see that greediness is disastrous for the HMM,
hurts a little for USM, and makes no difference on
the PTSG. These results show that stochasticity can
indeed be important.
We consider another block sampler, SENTENCE,
which uses dynamic programming to sample all
variables in a sentence (using Metropolis-Hastings
to correct for intra-sentential type-level coupling).
For USM, we see that SENTENCE performs worse
than TYPE and is comparable to TOKEN, suggesting
that type-based dependencies are stronger and more
important to deal with than intra-sentential depen-
dencies.
5.2 Initialization
We initialized all samplers as follows: For the USM
and PTSG, for each site s, we place a boundary (set
bs = 1) with probability ?. For the HMM, we set bs
to state 1 with probability ? and a random state with
7We started with a temperature of 10 and gradually de-
creased it to 1 during the first half of the run, and kept it at 1
thereafter.
579
3 6 9 12
time (hr.)
-1.1e7
-0.9e7
-9.1e6
-7.9e6
-6.7e6
log
-lik
elih
ood
3 6 9 12
time (hr.)
0.1
0.2
0.4
0.5
0.6
acc
ura
cy
2 4 6 8
time (min.)
-3.7e5
-3.2e5
-2.8e5
-2.4e5
-1.9e5
log
-lik
elih
ood Token
Tokenanneal
Typegreedy
Type
Sentence
2 4 6 8
time (min.)
0.1
0.2
0.4
0.5
0.6
F 1
3 6 9 12
time (hr.)
-6.2e6
-6.0e6
-5.8e6
-5.7e6
-5.5e6
log
-lik
elih
ood
(a) HMM (b) USM (c) PTSG
0.2 0.4 0.6 0.8 1.0
?
-7.1e6
-7.0e6
-6.9e6
-6.8e6
-6.7e6
log
-lik
elih
ood
0.2 0.4 0.6 0.8 1.0
?
0.2
0.3
0.4
0.5
0.6
acc
ura
cy
0.2 0.4 0.6 0.8 1.0
?
-3.5e5
-3.1e5
-2.7e5
-2.3e5
-1.9e5
log
-lik
elih
ood
0.2 0.4 0.6 0.8 1.0
?
0.2
0.3
0.4
0.5
0.6
F 1
0.2 0.4 0.6 0.8 1.0
?
-5.7e6
-5.6e6
-5.6e6
-5.5e6
-5.5e6
log
-lik
elih
ood
(d) HMM (e) USM (f) PTSG
Figure 5: (a)?(c): Log-likelihood and accuracy over time. TYPE performs the best. Relative to TYPE, TYPEgreedy
tends to hurt performance. TOKEN generally works worse. Relative to TOKEN, TOKENanneal produces mixed results.
SENTENCE behaves like TOKEN. (d)?(f): Effect of initialization. The metrics were applied to the current sample after
15 hours for the HMM and PTSG and 10 minutes for the USM. TYPE generally prefers larger ? and outperform the
other samplers.
probability 1 ? ?. Results in Figure 5(a)?(c) were
obtained by setting ? to maximize likelihood.
Since samplers tend to be sensitive to initializa-
tion, it is important to explore the effect of initial-
ization (parametrized by ? ? [0, 1]). Figure 5(d)?(f)
shows that TYPE is consistently the best, whereas
other samplers can underperform TYPE by a large
margin. Note that TYPE favors ? = 1 in general.
This setting maximizes the number of initial types,
and thus creates larger type blocks and thus enables
larger moves. Larger type blocks also mean more
dependencies that TOKEN is unable to deal with.
6 Related Work and Discussion
Block sampling, on which our work is built, is a clas-
sical idea, but is used restrictively since sampling
large blocks is computationally expensive. Past
work for clustering models maintained tractabil-
ity by using Metropolis-Hastings proposals (Dahl,
2003) or introducing auxiliary variables (Swendsen
and Wang, 1987; Liang et al, 2007). In contrast,
our type-based sampler simply identifies tractable
blocks based on exchangeability.
Other methods for learning latent-variable models
include EM, variational approximations, and uncol-
lapsed samplers. All of these methods maintain dis-
tributions over (or settings of) the latent variables of
the model and update the representation iteratively
(see Gao and Johnson (2008) for an overview in the
context of POS induction). However, these methods
are at the core all token-based, since they only up-
date variables in a single example at a time.8
Blocking variables by type?the key idea of
this paper?is a fundamental departure from token-
based methods. Though type-based changes have
also been proposed (Brown et al, 1992; Stolcke and
Omohundro, 1994), these methods operated greed-
ily, and in Section 5.1, we saw that being greedy led
to more brittle results. By working in a sampling
framework, we were able bring type-based changes
to fruition.
8While EM technically updates all distributions over latent
variables in the E-step, this update is performed conditioned on
model parameters; it is this coupling (made more explicit in
collapsed samplers) that makes EM susceptible to local optima.
580
References
P. F. Brown, V. J. D. Pietra, P. V. deSouza, J. C. Lai, and
R. L. Mercer. 1992. Class-based n-gram models of
natural language. Computational Linguistics, 18:467?
479.
T. Cohn, S. Goldwater, and P. Blunsom. 2009. Inducing
compact but accurate tree-substitution grammars. In
North American Association for Computational Lin-
guistics (NAACL), pages 548?556.
D. B. Dahl. 2003. An improved merge-split sampler for
conjugate Dirichlet process mixture models. Techni-
cal report, Department of Statistics, University of Wis-
consin.
J. DeNero, M. Bansal, A. Pauls, and D. Klein. 2009.
Efficient parsing for transducer grammars. In North
American Association for Computational Linguistics
(NAACL), pages 227?235.
J. Gao and M. Johnson. 2008. A comparison of
Bayesian estimators for unsupervised hidden Markov
model POS taggers. In Empirical Methods in Natural
Language Processing (EMNLP), pages 344?352.
S. Goldwater and T. Griffiths. 2007. A fully Bayesian
approach to unsupervised part-of-speech tagging. In
Association for Computational Linguistics (ACL).
S. Goldwater, T. Griffiths, and M. Johnson. 2006. Con-
textual dependencies in unsupervised word segmenta-
tion. In International Conference on Computational
Linguistics and Association for Computational Lin-
guistics (COLING/ACL).
S. Goldwater, T. Griffiths, and M. Johnson. 2009. A
Bayesian framework for word segmentation: Explor-
ing the effects of context. Cognition, 112:21?54.
A. Haghighi and D. Klein. 2006. Prototype-driven learn-
ing for sequence models. In North American Associ-
ation for Computational Linguistics (NAACL), pages
320?327.
P. Liang, M. I. Jordan, and B. Taskar. 2007. A
permutation-augmented sampler for Dirichlet process
mixture models. In International Conference on Ma-
chine Learning (ICML).
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning accurate, compact, and interpretable tree an-
notation. In International Conference on Computa-
tional Linguistics and Association for Computational
Linguistics (COLING/ACL), pages 433?440.
M. Post and D. Gildea. 2009. Bayesian learning of a
tree substitution grammar. In Association for Com-
putational Linguistics and International Joint Confer-
ence on Natural Language Processing (ACL-IJCNLP).
A. Stolcke and S. Omohundro. 1994. Inducing prob-
abilistic grammars by Bayesian model merging. In
International Colloquium on Grammatical Inference
and Applications, pages 106?118.
R. H. Swendsen and J. S. Wang. 1987. Nonuniversal
critical dynamics in MC simulations. Physics Review
Letters, 58:86?88.
J. G. Wolff. 1988. Learning syntax and meanings
through optimization and distributional analysis. In
Categories and processes in language acquisition,
pages 179?215.
581
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 590?599,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Learning Dependency-Based Compositional Semantics
Percy Liang
UC Berkeley
pliang@cs.berkeley.edu
Michael I. Jordan
UC Berkeley
jordan@cs.berkeley.edu
Dan Klein
UC Berkeley
klein@cs.berkeley.edu
Abstract
Compositional question answering begins by
mapping questions to logical forms, but train-
ing a semantic parser to perform this mapping
typically requires the costly annotation of the
target logical forms. In this paper, we learn
to map questions to answers via latent log-
ical forms, which are induced automatically
from question-answer pairs. In tackling this
challenging learning problem, we introduce a
new semantic representation which highlights
a parallel between dependency syntax and effi-
cient evaluation of logical forms. On two stan-
dard semantic parsing benchmarks (GEO and
JOBS), our system obtains the highest pub-
lished accuracies, despite requiring no anno-
tated logical forms.
1 Introduction
What is the total population of the ten largest cap-
itals in the US? Answering these types of complex
questions compositionally involves first mapping the
questions into logical forms (semantic parsing). Su-
pervised semantic parsers (Zelle and Mooney, 1996;
Tang and Mooney, 2001; Ge and Mooney, 2005;
Zettlemoyer and Collins, 2005; Kate and Mooney,
2007; Zettlemoyer and Collins, 2007; Wong and
Mooney, 2007; Kwiatkowski et al, 2010) rely on
manual annotation of logical forms, which is expen-
sive. On the other hand, existing unsupervised se-
mantic parsers (Poon and Domingos, 2009) do not
handle deeper linguistic phenomena such as quan-
tification, negation, and superlatives.
As in Clarke et al (2010), we obviate the need
for annotated logical forms by considering the end-
to-end problem of mapping questions to answers.
However, we still model the logical form (now as a
latent variable) to capture the complexities of lan-
guage. Figure 1 shows our probabilistic model:
(parameters) (world)
? w
x z y
(question) (logical form) (answer)state with thelargest area x1
11
c
argmax
area
state
?? Alaska
z ? p?(z | x)
y = JzKw
Semantic Parsing Evaluation
Figure 1: Our probabilistic model: a question x is
mapped to a latent logical form z, which is then evaluated
with respect to a world w (database of facts), producing
an answer y. We represent logical forms z as labeled
trees, induced automatically from (x, y) pairs.
We want to induce latent logical forms z (and pa-
rameters ?) given only question-answer pairs (x, y),
which is much cheaper to obtain than (x, z) pairs.
The core problem that arises in this setting is pro-
gram induction: finding a logical form z (over an
exponentially large space of possibilities) that pro-
duces the target answer y. Unlike standard semantic
parsing, our end goal is only to generate the correct
y, so we are free to choose the representation for z.
Which one should we use?
The dominant paradigm in compositional se-
mantics is Montague semantics, which constructs
lambda calculus forms in a bottom-up manner. CCG
is one instantiation (Steedman, 2000), which is used
by many semantic parsers, e.g., Zettlemoyer and
Collins (2005). However, the logical forms there
can become quite complex, and in the context of
program induction, this would lead to an unwieldy
search space. At the same time, representations such
as FunQL (Kate et al, 2005), which was used in
590
Clarke et al (2010), are simpler but lack the full ex-
pressive power of lambda calculus.
The main technical contribution of this work is
a new semantic representation, dependency-based
compositional semantics (DCS), which is both sim-
ple and expressive (Section 2). The logical forms in
this framework are trees, which is desirable for two
reasons: (i) they parallel syntactic dependency trees,
which facilitates parsing and learning; and (ii) eval-
uating them to obtain the answer is computationally
efficient.
We trained our model using an EM-like algorithm
(Section 3) on two benchmarks, GEO and JOBS
(Section 4). Our system outperforms all existing
systems despite using no annotated logical forms.
2 Semantic Representation
We first present a basic version (Section 2.1) of
dependency-based compositional semantics (DCS),
which captures the core idea of using trees to rep-
resent formal semantics. We then introduce the full
version (Section 2.2), which handles linguistic phe-
nomena such as quantification, where syntactic and
semantic scope diverge.
We start with some definitions, using US geogra-
phy as an example domain. Let V be the set of all
values, which includes primitives (e.g., 3, CA ? V)
as well as sets and tuples formed from other values
(e.g., 3, {3, 4, 7}, (CA, {5}) ? V). Let P be a set
of predicates (e.g., state, count ? P), which are
just symbols.
A world w is mapping from each predicate p ?
P to a set of tuples; for example, w(state) =
{(CA), (OR), . . . }. Conceptually, a world is a rela-
tional database where each predicate is a relation
(possibly infinite). Define a special predicate ? with
w(?) = V . We represent functions by a set of input-
output pairs, e.g., w(count) = {(S, n) : n = |S|}.
As another example, w(average) = {(S, x?) :
x? = |S1|?1
?
x?S1 S(x)}, where a set of pairs S
is treated as a set-valued function S(x) = {y :
(x, y) ? S} with domain S1 = {x : (x, y) ? S}.
The logical forms in DCS are called DCS trees,
where nodes are labeled with predicates, and edges
are labeled with relations. Formally:
Definition 1 (DCS trees) Let Z be the set of DCS
trees, where each z ? Z consists of (i) a predicate
RelationsR
j
j? (join) E (extract)
? (aggregate) Q (quantify)
Xi (execute) C (compare)
Table 1: Possible relations appearing on the edges of a
DCS tree. Here, j, j? ? {1, 2, . . . } and i ? {1, 2, . . . }?.
z.p ? P and (ii) a sequence of edges z.e1, . . . , z.em,
each edge e consisting of a relation e.r ? R (see
Table 1) and a child tree e.c ? Z .
We write a DCS tree z as ?p; r1 : c1; . . . ; rm : cm?.
Figure 2(a) shows an example of a DCS tree. Al-
though a DCS tree is a logical form, note that it looks
like a syntactic dependency tree with predicates in
place of words. It is this transparency between syn-
tax and semantics provided by DCS which leads to
a simple and streamlined compositional semantics
suitable for program induction.
2.1 Basic Version
The basic version of DCS restrictsR to join and ag-
gregate relations (see Table 1). Let us start by con-
sidering a DCS tree z with only join relations. Such
a z defines a constraint satisfaction problem (CSP)
with nodes as variables. The CSP has two types of
constraints: (i) x ? w(p) for each node x labeled
with predicate p ? P; and (ii) xj = yj? (the j-th
component of x must equal the j?-th component of
y) for each edge (x, y) labeled with jj? ? R.
A solution to the CSP is an assignment of nodes
to values that satisfies all the constraints. We say a
value v is consistent for a node x if there exists a
solution that assigns v to x. The denotation JzKw (z
evaluated on w) is the set of consistent values of the
root node (see Figure 2 for an example).
Computation We can compute the denotation
JzKw of a DCS tree z by exploiting dynamic pro-
gramming on trees (Dechter, 2003). The recurrence
is as follows:
J
?
p; j1j?1 :c1; ? ? ? ;
jm
j?m
:cm
?
K
w
(1)
= w(p) ?
m?
i=1
{v : vji = tj?i , t ? JciKw}.
At each node, we compute the set of tuples v consis-
tent with the predicate at that node (v ? w(p)), and
591
Example: major city in California
z = ?city; 11 :?major? ; 11 :?loc; 21 :?CA???
1
1
1
1
major
2
1
CA
loc
city ?c?m?`?s .
city(c) ? major(m)?
loc(`) ? CA(s)?c1 = m1 ? c1 = `1 ? `2 = s1
(a) DCS tree (b) Lambda calculus formula
(c) Denotation: JzKw = {SF, LA, . . . }
Figure 2: (a) An example of a DCS tree (written in both
the mathematical and graphical notation). Each node is
labeled with a predicate, and each edge is labeled with a
relation. (b) A DCS tree z with only join relations en-
codes a constraint satisfaction problem. (c) The denota-
tion of z is the set of consistent values for the root node.
for each child i, the ji-th component of v must equal
the j?i-th component of some t in the child?s deno-
tation (t ? JciKw). This algorithm is linear in the
number of nodes times the size of the denotations.1
Now the dual importance of trees in DCS is clear:
We have seen that trees parallel syntactic depen-
dency structure, which will facilitate parsing. In
addition, trees enable efficient computation, thereby
establishing a new connection between dependency
syntax and efficient semantic evaluation.
Aggregate relation DCS trees that only use join
relations can represent arbitrarily complex compo-
sitional structures, but they cannot capture higher-
order phenomena in language. For example, con-
sider the phrase number of major cities, and suppose
that number corresponds to the count predicate.
It is impossible to represent the semantics of this
phrase with just a CSP, so we introduce a new ag-
gregate relation, notated ?. Consider a tree ??:c?,
whose root is connected to a child c via ?. If the de-
notation of c is a set of values s, the parent?s denota-
tion is then a singleton set containing s. Formally:
J??:c?Kw = {JcKw}. (2)
Figure 3(a) shows the DCS tree for our running
example. The denotation of the middle node is {s},
1Infinite denotations (such as J<Kw) are represented as im-
plicit sets on which we can perform membership queries. The
intersection of two sets can be performed as long as at least one
of the sets is finite.
number ofmajor cities
12
11
?
11major
city
??
count
??
average population ofmajor cities
12
11
?
11
11major
city
population
??
average
??
(a) Counting (b) Averaging
Figure 3: Examples of DCS trees that use the aggregate
relation (?) to (a) compute the cardinality of a set and (b)
take the average over a set.
where s is all major cities. Having instantiated s as
a value, everything above this node is an ordinary
CSP: s constrains the count node, which in turns
constrains the root node to |s|.
A DCS tree that contains only join and aggre-
gate relations can be viewed as a collection of tree-
structured CSPs connected via aggregate relations.
The tree structure still enables us to compute deno-
tations efficiently based on (1) and (2).
2.2 Full Version
The basic version of DCS described thus far han-
dles a core subset of language. But consider Fig-
ure 4: (a) is headed by borders, but states needs
to be extracted; in (b), the quantifier no is syntacti-
cally dominated by the head verb borders but needs
to take wider scope. We now present the full ver-
sion of DCS which handles this type of divergence
between syntactic and semantic scope.
The key idea that allows us to give semantically-
scoped denotations to syntactically-scoped trees is
as follows: We mark a node low in the tree with a
mark relation (one of E, Q, or C). Then higher up in
the tree, we invoke it with an execute relation Xi to
create the desired semantic scope.2
This mark-execute construct acts non-locally, so
to maintain compositionality, we must augment the
2Our mark-execute construct is analogous to Montague?s
quantifying in, Cooper storage, and Carpenter?s scoping con-
structor (Carpenter, 1998).
592
California borders which states?
x1
2 111CA
e
??
state
border
?? Alaska borders no states.
x1
2 111AK
q
no
state
border
?? Some river traverses every city.
x12
2 111
q
some
river
q
every
city
traverse
??
x21
2 111
q
some
river
q
every
city
traverse
??
(narrow) (wide)
city traversed by no rivers
x12
1 2e?? 11
q
no
river
traverse
city
??
(a) Extraction (e) (b) Quantification (q) (c) Quantifier ambiguity (q,q) (d) Quantification (q,e)
state borderingthe most states
x12
1 1e?? 21
c
argmax
state
border
state
??
state borderingmore states than Texas
x12
1 1e?? 21
c
31TX
more
state
border
state
??
state borderingthe largest state
11
21
x12
1 1e??
c
argmax
size
state
??
border
state
x12
1 1e?? 21
11
c
argmax
size
state
border
state
??
(absolute) (relative)
Every state?slargest city is major.
x1
x2
1 111
21
q
every
state
loc
c
argmax
size
city
major
??
(e) Superlative (c) (f) Comparative (c) (g) Superlative ambiguity (c) (h) Quantification+Superlative (q,c)
Figure 4: Example DCS trees for utterances in which syntactic and semantic scope diverge. These trees reflect the
syntactic structure, which facilitates parsing, but importantly, these trees also precisely encode the correct semantic
scope. The main mechanism is using a mark relation (E, Q, or C) low in the tree paired with an execute relation (Xi)
higher up at the desired semantic point.
denotation d = JzKw to include any information
about the marked nodes in z that can be accessed
by an execute relation later on. In the basic ver-
sion, d was simply the consistent assignments to the
root. Now d contains the consistent joint assign-
ments to the active nodes (which include the root
and all marked nodes), as well as information stored
about each marked node. Think of d as consisting
of n columns, one for each active node according to
a pre-order traversal of z. Column 1 always corre-
sponds to the root node. Formally, a denotation is
defined as follows (see Figure 5 for an example):
Definition 2 (Denotations) Let D be the set of de-
notations, where each d ? D consists of
? a set of arrays d.A, where each array a =
[a1, . . . , an] ? d.A is a sequence of n tuples
(ai ? V?); and
? a list of n stores d.? = (d.?1, . . . , d.?n),
where each store ? contains a mark relation
?.r ? {E, Q, C, ?}, a base denotation ?.b ?
D?{?}, and a child denotation ?.c ? D?{?}.
We write d as ??A; (r1, b1, c1); . . . ; (rn, bn, cn)??. We
use d{ri = x} to mean d with d.ri = d.?i.r = x
(similar definitions apply for d{?i = x}, d{bi = x},
and d{ci = x}).
The denotation of a DCS tree can now be defined
recursively:
J?p?Kw = ??{[v] : v ? w(p)}; ???, (3)
J
?
p; e; jj? :c
?
K
w
= Jp; eKw ./j,j? JcKw, (4)
J?p; e; ?:c?Kw = Jp; eKw ./?,? ? (JcKw) , (5)
J?p; e; Xi :c?Kw = Jp; eKw ./?,? Xi(JcKw), (6)
J?p; e; E :c?Kw = M(Jp; eKw, E, c), (7)
J?p; e; C :c?Kw = M(Jp; eKw, C, c), (8)
J?p; Q :c; e?Kw = M(Jp; eKw, Q, c). (9)
593
11
21
11
c
argmax
size
state
border
state
J?Kw
column 1 column 2
A:
(OK)(NM)(NV)? ? ?
(TX,2.7e5)(TX,2.7e5)(CA,1.6e5)? ? ?r: ? cb: ? J?size?Kwc: ? J?argmax?KwDCS tree Denotation
Figure 5: Example of the denotation for a DCS tree with
a compare relation C. This denotation has two columns,
one for each active node?the root node state and the
marked node size.
The base case is defined in (3): if z is a sin-
gle node with predicate p, then the denotation of z
has one column with the tuples w(p) and an empty
store. The other six cases handle different edge re-
lations. These definitions depend on several opera-
tions (./j,j? ,?,Xi,M) which we will define shortly,
but let us first get some intuition.
Let z be a DCS tree. If the last child c of z?s
root is a join ( jj?), aggregate (?), or execute (Xi) re-
lation ((4)?(6)), then we simply recurse on z with c
removed and join it with some transformation (iden-
tity, ?, or Xi) of c?s denotation. If the last (or first)
child is connected via a mark relation E, C (or Q),
then we strip off that child and put the appropriate
information in the store by invoking M.
We now define the operations ./j,j? ,?,Xi,M.
Some helpful notation: For a sequence v =
(v1, . . . , vn) and indices i = (i1, . . . , ik), let vi =
(vi1 , . . . , vik) be the projection of v onto i; we write
v?i to mean v[1,...,n]\i. Extending this notation to
denotations, let ??A;???[i] = ??{ai : a ? A};?i??.
Let d[??] = d[?i], where i are the columns with
empty stores. For example, for d in Figure 5, d[1]
keeps column 1, d[??] keeps column 2, and d[2,?2]
swaps the two columns.
Join The join of two denotations d and d? with re-
spect to components j and j? (? means all compo-
nents) is formed by concatenating all arrays a of d
with all compatible arrays a? of d?, where compat-
ibility means a1j = a?1j? . The stores are also con-
catenated (?+??). Non-initial columns with empty
stores are projected away by applying ?[1,??]. The
full definition of join is as follows:
??A;??? ./j,j? ??A
?;???? = ??A??;? + ????[1,??],
A?? = {a + a? : a ? A,a? ? A?, a1j = a
?
1j?}. (10)
Aggregate The aggregate operation takes a deno-
tation and forms a set out of the tuples in the first
column for each setting of the rest of the columns:
? (??A;???) = ??A? ?A??;??? (11)
A? = {[S(a), a2, . . . , an] : a ? A}
S(a) = {a?1 : [a
?
1, a2, . . . , an] ? A}
A?? = {[?, a2, . . . , an] : ??a1,a ? A,
?2 ? i ? n, [ai] ? d.bi[1].A}.
2.2.1 Mark and Execute
Now we turn to the mark (M) and execute (Xi)
operations, which handles the divergence between
syntactic and semantic scope. In some sense, this is
the technical core of DCS. Marking is simple: When
a node (e.g., size in Figure 5) is marked (e.g., with
relation C), we simply put the relation r, current de-
notation d and child c?s denotation into the store of
column 1:
M(d, r, c) = d{r1 = r, b1 = d, c1 = JcKw}. (12)
The execute operation Xi(d) processes columns
i in reverse order. It suffices to define Xi(d) for a
single column i. There are three cases:
Extraction (d.ri = E) In the basic version, the
denotation of a tree was always the set of con-
sistent values of the root node. Extraction al-
lows us to return the set of consistent values of a
marked non-root node. Formally, extraction sim-
ply moves the i-th column to the front: Xi(d) =
d[i,?(i, ?)]{?1 = ?}. For example, in Figure 4(a),
before execution, the denotation of the DCS tree
is ??{[(CA, OR), (OR)], . . . }; ?; (E, J?state?Kw, ?)??;
after applying X1, we have ??{[(OR)], . . . }; ???.
Generalized Quantification (d.ri = Q) Gener-
alized quantifiers are predicates on two sets, a re-
strictor A and a nuclear scope B. For example,
w(no) = {(A,B) : A ? B = ?} and w(most) =
{(A,B) : |A ?B| > 12 |A|}.
In a DCS tree, the quantifier appears as the
child of a Q relation, and the restrictor is the par-
ent (see Figure 4(b) for an example). This in-
formation is retrieved from the store when the
594
quantifier in column i is executed. In particu-
lar, the restrictor is A = ? (d.bi) and the nu-
clear scope is B = ? (d[i,?(i, ?)]). We then
apply d.ci to these two sets (technically, denota-
tions) and project away the first column: Xi(d) =
((d.ci ./1,1 A) ./2,1 B) [?1].
For the example in Figure 4(b), the de-
notation of the DCS tree before execution is
???; ?; (Q, J?state?Kw, J?no?Kw)??. The restrictor
set (A) is the set of all states, and the nuclear scope
(B) is the empty set. Since (A,B) exists in no, the
final denotation, which projects away the actual pair,
is ??{[ ]}?? (our representation of true).
Figure 4(c) shows an example with two interact-
ing quantifiers. The quantifier scope ambiguity is
resolved by the choice of execute relation; X12 gives
the narrow reading and X21 gives the wide reading.
Figure 4(d) shows how extraction and quantification
work together.
Comparatives and Superlatives (d.ri = C) To
compare entities, we use a set S of (x, y) pairs,
where x is an entity and y is a number. For su-
perlatives, the argmax predicate denotes pairs of
sets and the set?s largest element(s): w(argmax) =
{(S, x?) : x? ? argmaxx?S1 maxS(x)}. For com-
paratives, w(more) contains triples (S, x, y), where
x is ?more than? y as measured by S; formally:
w(more) = {(S, x, y) : maxS(x) > maxS(y)}.
In a superlative/comparative construction, the
root x of the DCS tree is the entity to be compared,
the child c of a C relation is the comparative or su-
perlative, and its parent p contains the information
used for comparison (see Figure 4(e) for an exam-
ple). If d is the denotation of the root, its i-th column
contains this information. There are two cases: (i) if
the i-th column of d contains pairs (e.g., size in
Figure 5), then let d? = J???Kw ./1,2 d[i,?i], which
reads out the second components of these pairs; (ii)
otherwise (e.g., state in Figure 4(e)), let d? =
J???Kw ./1,2 J?count?Kw ./1,1 ? (d[i,?i]), which
counts the number of things (e.g., states) that occur
with each value of the root x. Given d?, we construct
a denotation S by concatenating (+i) the second and
first columns of d? (S = ? (+2,1 (d?{?2 = ?})))
and apply the superlative/comparative: Xi(d) =
(J???Kw ./1,2 (d.ci ./1,1 S)){?1 = d.?1}.
Figure 4(f) shows that comparatives are handled
using the exact same machinery as superlatives. Fig-
ure 4(g) shows that we can naturally account for
superlative ambiguity based on where the scope-
determining execute relation is placed.
3 Semantic Parsing
We now turn to the task of mapping natural language
utterances to DCS trees. Our first question is: given
an utterance x, what trees z ? Z are permissible? To
define the search space, we first assume a fixed set
of lexical triggers L. Each trigger is a pair (x, p),
where x is a sequence of words (usually one) and p
is a predicate (e.g., x = California and p = CA).
We use L(x) to denote the set of predicates p trig-
gered by x ((x, p) ? L). Let L() be the set of
trace predicates, which can be introduced without
an overt lexical trigger.
Given an utterance x = (x1, . . . , xn), we define
ZL(x) ? Z , the set of permissible DCS trees for
x. The basic approach is reminiscent of projective
labeled dependency parsing: For each span i..j, we
build a set of trees Ci,j and set ZL(x) = C0,n. Each
set Ci,j is constructed recursively by combining the
trees of its subspans Ci,k and Ck?,j for each pair of
split points k, k? (words between k and k? are ig-
nored). These combinations are then augmented via
a functionA and filtered via a functionF , to be spec-
ified later. Formally, Ci,j is defined recursively as
follows:
Ci,j = F
(
A
(
L(xi+1..j) ?
?
i?k?k?<j
a?Ci,k
b?Ck?,j
T1(a, b))
))
.
(13)
In (13), L(xi+1..j) is the set of predicates triggered
by the phrase under span i..j (the base case), and
Td(a, b) = ~Td(a, b) ? ~T d(b, a), which returns all
ways of combining trees a and b where b is a de-
scendant of a (~Td) or vice-versa ( ~T d). The former is
defined recursively as follows: ~T0(a, b) = ?, and
~Td(a, b) =
?
r?R
p?L()
{?a; r :b?} ? ~Td?1(a, ?p; r :b?).
The latter ( ~T k) is defined similarly. Essentially,
~Td(a, b) allows us to insert up to d trace predi-
cates between the roots of a and b. This is use-
ful for modeling relations in noun compounds (e.g.,
595
California cities), and it also allows us to underspec-
ify L. In particular, our L will not include verbs or
prepositions; rather, we rely on the predicates corre-
sponding to those words to be triggered by traces.
The augmentation function A takes a set of trees
and optionally attaches E and Xi relations to the
root (e.g., A(?city?) = {?city? , ?city; E :??}).
The filtering function F rules out improperly-typed
trees such as ?city; 00 :?state??. To further reduce
the search space, F imposes a few additional con-
straints, e.g., limiting the number of marked nodes
to 2 and only allowing trace predicates between ar-
ity 1 predicates.
Model We now present our discriminative se-
mantic parsing model, which places a log-linear
distribution over z ? ZL(x) given an utter-
ance x. Formally, p?(z | x) ? e?(x,z)
>?,
where ? and ?(x, z) are parameter and feature vec-
tors, respectively. As a running example, con-
sider x = city that is in California and z =
?city; 11 :?loc;
2
1 :?CA???, where city triggers city
and California triggers CA.
To define the features, we technically need to
augment each tree z ? ZL(x) with alignment
information?namely, for each predicate in z, the
span in x (if any) that triggered it. This extra infor-
mation is already generated from the recursive defi-
nition in (13).
The feature vector ?(x, z) is defined by sums of
five simple indicator feature templates: (F1) a word
triggers a predicate (e.g., [city, city]); (F2) a word
is under a relation (e.g., [that, 11]); (F3) a word is un-
der a trace predicate (e.g., [in, loc]); (F4) two pred-
icates are linked via a relation in the left or right
direction (e.g., [city, 11, loc, RIGHT]); and (F5) a
predicate has a child relation (e.g., [city, 11]).
Learning Given a training dataset D con-
taining (x, y) pairs, we define the regu-
larized marginal log-likelihood objective
O(?) =
?
(x,y)?D log p?(JzKw = y | x, z ?
ZL(x)) ? ????22, which sums over all DCS trees z
that evaluate to the target answer y.
Our model is arc-factored, so we can sum over all
DCS trees in ZL(x) using dynamic programming.
However, in order to learn, we need to sum over
{z ? ZL(x) : JzKw = y}, and unfortunately, the
additional constraint JzKw = y does not factorize.
We therefore resort to beam search. Specifically, we
truncate each Ci,j to a maximum of K candidates
sorted by decreasing score based on parameters ?.
Let Z?L,?(x) be this approximation of ZL(x).
Our learning algorithm alternates between (i) us-
ing the current parameters ? to generate the K-best
set Z?L,?(x) for each training example x, and (ii)
optimizing the parameters to put probability mass
on the correct trees in these sets; sets contain-
ing no correct answers are skipped. Formally, let
O?(?, ??) be the objective function O(?) with ZL(x)
replaced with Z?L,??(x). We optimize O?(?, ??) by
setting ?(0) = ~0 and iteratively solving ?(t+1) =
argmax? O?(?, ?
(t)) using L-BFGS until t = T . In all
experiments, we set ? = 0.01, T = 5, andK = 100.
After training, given a new utterance x, our system
outputs the most likely y, summing out the latent
logical form z: argmaxy p?(T )(y | x, z ? Z?L,?(T )).
4 Experiments
We tested our system on two standard datasets, GEO
and JOBS. In each dataset, each sentence x is an-
notated with a Prolog logical form, which we use
only to evaluate and get an answer y. This evalua-
tion is done with respect to a world w. Recall that
a world w maps each predicate p ? P to a set of
tuples w(p). There are three types of predicates in
P: generic (e.g., argmax), data (e.g., city), and
value (e.g., CA). GEO has 48 non-value predicates
and JOBS has 26. For GEO, w is the standard US
geography database that comes with the dataset. For
JOBS, if we use the standard Jobs database, close to
half the y?s are empty, which makes it uninteresting.
We therefore generated a random Jobs database in-
stead as follows: we created 100 job IDs. For each
data predicate p (e.g., language), we add each pos-
sible tuple (e.g., (job37, Java)) to w(p) indepen-
dently with probability 0.8.
We used the same training-test splits as Zettle-
moyer and Collins (2005) (600+280 for GEO and
500+140 for JOBS). During development, we fur-
ther held out a random 30% of the training sets for
validation.
Our lexical triggers L include the following: (i)
predicates for a small set of ? 20 function words
(e.g., (most, argmax)), (ii) (x, x) for each value
596
System Accuracy
Clarke et al (2010) w/answers 73.2
Clarke et al (2010) w/logical forms 80.4
Our system (DCS with L) 78.9
Our system (DCS with L+) 87.2
Table 2: Results on GEO with 250 training and 250
test examples. Our results are averaged over 10 random
250+250 splits taken from our 600 training examples. Of
the three systems that do not use logical forms, our two
systems yield significant improvements. Our better sys-
tem even outperforms the system that uses logical forms.
predicate x in w (e.g., (Boston, Boston)), and
(iii) predicates for each POS tag in {JJ, NN, NNS}
(e.g., (JJ, size), (JJ, area), etc.).3 Predicates
corresponding to verbs and prepositions (e.g.,
traverse) are not included as overt lexical trig-
gers, but rather in the trace predicates L().
We also define an augmented lexicon L+ which
includes a prototype word x for each predicate ap-
pearing in (iii) above (e.g., (large, size)), which
cancels the predicates triggered by x?s POS tag. For
GEO, there are 22 prototype words; for JOBS, there
are 5. Specifying these triggers requires minimal
domain-specific supervision.
Results We first compare our system with Clarke
et al (2010) (henceforth, SEMRESP), which also
learns a semantic parser from question-answer pairs.
Table 2 shows that our system using lexical triggers
L (henceforth, DCS) outperforms SEMRESP (78.9%
over 73.2%). In fact, although neither DCS nor
SEMRESP uses logical forms, DCS uses even less su-
pervision than SEMRESP. SEMRESP requires a lex-
icon of 1.42 words per non-value predicate, Word-
Net features, and syntactic parse trees; DCS requires
only words for the domain-independent predicates
(overall, around 0.5 words per non-value predicate),
POS tags, and very simple indicator features. In
fact, DCS performs comparably to even the version
of SEMRESP trained using logical forms. If we add
prototype triggers (use L+), the resulting system
(DCS+) outperforms both versions of SEMRESP by
a significant margin (87.2% over 73.2% and 80.4%).
3We used the Berkeley Parser (Petrov et al, 2006) to per-
form POS tagging. The triggers L(x) for a word x thus include
L(t) where t is the POS tag of x.
System GEO JOBS
Tang and Mooney (2001) 79.4 79.8
Wong and Mooney (2007) 86.6 ?
Zettlemoyer and Collins (2005) 79.3 79.3
Zettlemoyer and Collins (2007) 81.6 ?
Kwiatkowski et al (2010) 88.2 ?
Kwiatkowski et al (2010) 88.9 ?
Our system (DCS with L) 88.6 91.4
Our system (DCS with L+) 91.1 95.0
Table 3: Accuracy (recall) of systems on the two bench-
marks. The systems are divided into three groups. Group
1 uses 10-fold cross-validation; groups 2 and 3 use the in-
dependent test set. Groups 1 and 2 measure accuracy of
logical form; group 3 measures accuracy of the answer;
but there is very small difference between the two as seen
from the Kwiatkowski et al (2010) numbers. Our best
system improves substantially over past work, despite us-
ing no logical forms as training data.
Next, we compared our systems (DCS and DCS+)
with the state-of-the-art semantic parsers on the full
dataset for both GEO and JOBS (see Table 3). All
other systems require logical forms as training data,
whereas ours does not. Table 3 shows that even DCS,
which does not use prototypes, is comparable to the
best previous system (Kwiatkowski et al, 2010), and
by adding a few prototypes, DCS+ offers a decisive
edge (91.1% over 88.9% on GEO). Rather than us-
ing lexical triggers, several of the other systems use
IBM word alignment models to produce an initial
word-predicate mapping. This option is not avail-
able to us since we do not have annotated logical
forms, so we must instead rely on lexical triggers
to define the search space. Note that having lexical
triggers is a much weaker requirement than having
a CCG lexicon, and far easier to obtain than logical
forms.
Intuitions How is our system learning? Initially,
the weights are zero, so the beam search is essen-
tially unguided. We find that only for a small frac-
tion of training examples do the K-best sets contain
any trees yielding the correct answer (29% for DCS
on GEO). However, training on just these exam-
ples is enough to improve the parameters, and this
29% increases to 66% and then to 95% over the next
few iterations. This bootstrapping behavior occurs
naturally: The ?easy? examples are processed first,
where easy is defined by the ability of the current
597
model to generate the correct answer using any tree.
Our system learns lexical associations between
words and predicates. For example, area (by virtue
of being a noun) triggers many predicates: city,
state, area, etc. Inspecting the final parameters
(DCS on GEO), we find that the feature [area, area]
has a much higher weight than [area, city]. Trace
predicates can be inserted anywhere, but the fea-
tures favor some insertions depending on the words
present (for example, [in, loc] has high weight).
The errors that the system makes stem from mul-
tiple sources, including errors in the POS tags (e.g.,
states is sometimes tagged as a verb, which triggers
no predicates), confusion of Washington state with
Washington D.C., learning the wrong lexical asso-
ciations due to data sparsity, and having an insuffi-
ciently large K.
5 Discussion
A major focus of this work is on our semantic rep-
resentation, DCS, which offers a new perspective
on compositional semantics. To contrast, consider
CCG (Steedman, 2000), in which semantic pars-
ing is driven from the lexicon. The lexicon en-
codes information about how each word can used in
context; for example, the lexical entry for borders
is S\NP/NP : ?y.?x.border(x, y), which means
borders looks right for the first argument and left
for the second. These rules are often too stringent,
and for complex utterances, especially in free word-
order languages, either disharmonic combinators are
employed (Zettlemoyer and Collins, 2007) or words
are given multiple lexical entries (Kwiatkowski et
al., 2010).
In DCS, we start with lexical triggers, which are
more basic than CCG lexical entries. A trigger for
borders specifies only that border can be used, but
not how. The combination rules are encoded in the
features as soft preferences. This yields a more
factorized and flexible representation that is easier
to search through and parametrize using features.
It also allows us to easily add new lexical triggers
without becoming mired in the semantic formalism.
Quantifiers and superlatives significantly compli-
cate scoping in lambda calculus, and often type rais-
ing needs to be employed. In DCS, the mark-execute
construct provides a flexible framework for dealing
with scope variation. Think of DCS as a higher-level
programming language tailored to natural language,
which results in programs (DCS trees) which are
much simpler than the logically-equivalent lambda
calculus formulae.
The idea of using CSPs to represent semantics is
inspired by Discourse Representation Theory (DRT)
(Kamp and Reyle, 1993; Kamp et al, 2005), where
variables are discourse referents. The restriction to
trees is similar to economical DRT (Bos, 2009).
The other major focus of this work is program
induction?inferring logical forms from their deno-
tations. There has been a fair amount of past work on
this topic: Liang et al (2010) induces combinatory
logic programs in a non-linguistic setting. Eisen-
stein et al (2009) induces conjunctive formulae and
uses them as features in another learning problem.
Piantadosi et al (2008) induces first-order formu-
lae using CCG in a small domain assuming observed
lexical semantics. The closest work to ours is Clarke
et al (2010), which we discussed earlier.
The integration of natural language with denota-
tions computed against a world (grounding) is be-
coming increasingly popular. Feedback from the
world has been used to guide both syntactic parsing
(Schuler, 2003) and semantic parsing (Popescu et
al., 2003; Clarke et al, 2010). Past work has also fo-
cused on aligning text to a world (Liang et al, 2009),
using text in reinforcement learning (Branavan et al,
2009; Branavan et al, 2010), and many others. Our
work pushes the grounded language agenda towards
deeper representations of language?think grounded
compositional semantics.
6 Conclusion
We built a system that interprets natural language
utterances much more accurately than existing sys-
tems, despite using no annotated logical forms. Our
system is based on a new semantic representation,
DCS, which offers a simple and expressive alter-
native to lambda calculus. Free from the burden
of annotating logical forms, we hope to use our
techniques in developing even more accurate and
broader-coverage language understanding systems.
Acknowledgments We thank Luke Zettlemoyer
and Tom Kwiatkowski for providing us with data
and answering questions.
598
References
J. Bos. 2009. A controlled fragment of DRT. In Work-
shop on Controlled Natural Language, pages 1?5.
S. Branavan, H. Chen, L. S. Zettlemoyer, and R. Barzilay.
2009. Reinforcement learning for mapping instruc-
tions to actions. In Association for Computational Lin-
guistics and International Joint Conference on Natural
Language Processing (ACL-IJCNLP), Singapore. As-
sociation for Computational Linguistics.
S. Branavan, L. Zettlemoyer, and R. Barzilay. 2010.
Reading between the lines: Learning to map high-level
instructions to commands. In Association for Compu-
tational Linguistics (ACL). Association for Computa-
tional Linguistics.
B. Carpenter. 1998. Type-Logical Semantics. MIT Press.
J. Clarke, D. Goldwasser, M. Chang, and D. Roth.
2010. Driving semantic parsing from the world?s re-
sponse. In Computational Natural Language Learn-
ing (CoNLL).
R. Dechter. 2003. Constraint Processing. Morgan Kauf-
mann.
J. Eisenstein, J. Clarke, D. Goldwasser, and D. Roth.
2009. Reading to learn: Constructing features from
semantic abstracts. In Empirical Methods in Natural
Language Processing (EMNLP), Singapore.
R. Ge and R. J. Mooney. 2005. A statistical semantic
parser that integrates syntax and semantics. In Compu-
tational Natural Language Learning (CoNLL), pages
9?16, Ann Arbor, Michigan.
H. Kamp and U. Reyle. 1993. From Discourse to Logic:
An Introduction to the Model-theoretic Semantics of
Natural Language, Formal Logic and Discourse Rep-
resentation Theory. Kluwer, Dordrecht.
H. Kamp, J. v. Genabith, and U. Reyle. 2005. Discourse
representation theory. In Handbook of Philosophical
Logic.
R. J. Kate and R. J. Mooney. 2007. Learning lan-
guage semantics from ambiguous supervision. In As-
sociation for the Advancement of Artificial Intelligence
(AAAI), pages 895?900, Cambridge, MA. MIT Press.
R. J. Kate, Y. W. Wong, and R. J. Mooney. 2005.
Learning to transform natural to formal languages. In
Association for the Advancement of Artificial Intel-
ligence (AAAI), pages 1062?1068, Cambridge, MA.
MIT Press.
T. Kwiatkowski, L. Zettlemoyer, S. Goldwater, and
M. Steedman. 2010. Inducing probabilistic CCG
grammars from logical form with higher-order unifi-
cation. In Empirical Methods in Natural Language
Processing (EMNLP).
P. Liang, M. I. Jordan, and D. Klein. 2009. Learning se-
mantic correspondences with less supervision. In As-
sociation for Computational Linguistics and Interna-
tional Joint Conference on Natural Language Process-
ing (ACL-IJCNLP), Singapore. Association for Com-
putational Linguistics.
P. Liang, M. I. Jordan, and D. Klein. 2010. Learning
programs: A hierarchical Bayesian approach. In In-
ternational Conference on Machine Learning (ICML).
Omnipress.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning accurate, compact, and interpretable tree an-
notation. In International Conference on Computa-
tional Linguistics and Association for Computational
Linguistics (COLING/ACL), pages 433?440. Associa-
tion for Computational Linguistics.
S. T. Piantadosi, N. D. Goodman, B. A. Ellis, and J. B.
Tenenbaum. 2008. A Bayesian model of the acquisi-
tion of compositional semantics. In Proceedings of the
Thirtieth Annual Conference of the Cognitive Science
Society.
H. Poon and P. Domingos. 2009. Unsupervised semantic
parsing. In Empirical Methods in Natural Language
Processing (EMNLP), Singapore.
A. Popescu, O. Etzioni, and H. Kautz. 2003. Towards
a theory of natural language interfaces to databases.
In International Conference on Intelligent User Inter-
faces (IUI).
W. Schuler. 2003. Using model-theoretic semantic inter-
pretation to guide statistical parsing and word recog-
nition in a spoken language interface. In Association
for Computational Linguistics (ACL). Association for
Computational Linguistics.
M. Steedman. 2000. The Syntactic Process. MIT Press.
L. R. Tang and R. J. Mooney. 2001. Using multiple
clause constructors in inductive logic programming for
semantic parsing. In European Conference on Ma-
chine Learning, pages 466?477.
Y. W. Wong and R. J. Mooney. 2007. Learning syn-
chronous grammars for semantic parsing with lambda
calculus. In Association for Computational Linguis-
tics (ACL), pages 960?967, Prague, Czech Republic.
Association for Computational Linguistics.
M. Zelle and R. J. Mooney. 1996. Learning to parse
database queries using inductive logic proramming. In
Association for the Advancement of Artificial Intelli-
gence (AAAI), Cambridge, MA. MIT Press.
L. S. Zettlemoyer and M. Collins. 2005. Learning to
map sentences to logical form: Structured classifica-
tion with probabilistic categorial grammars. In Uncer-
tainty in Artificial Intelligence (UAI), pages 658?666.
L. S. Zettlemoyer and M. Collins. 2007. Online learn-
ing of relaxed CCG grammars for parsing to logical
form. In Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing (EMNLP/CoNLL), pages 678?687.
599
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 391?401,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Zero-shot Entity Extraction from Web Pages
Panupong Pasupat
Computer Science Department
Stanford University
ppasupat@cs.stanford.edu
Percy Liang
Computer Science Department
Stanford University
pliang@cs.stanford.edu
Abstract
In order to extract entities of a fine-grained
category from semi-structured data in web
pages, existing information extraction sys-
tems rely on seed examples or redundancy
across multiple web pages. In this paper,
we consider a new zero-shot learning task
of extracting entities specified by a natural
language query (in place of seeds) given
only a single web page. Our approach de-
fines a log-linear model over latent extrac-
tion predicates, which select lists of enti-
ties from the web page. The main chal-
lenge is to define features on widely vary-
ing candidate entity lists. We tackle this by
abstracting list elements and using aggre-
gate statistics to define features. Finally,
we created a new dataset of diverse queries
and web pages, and show that our system
achieves significantly better accuracy than
a natural baseline.
1 Introduction
We consider the task of extracting entities of
a given category (e.g., hiking trails) from web
pages. Previous approaches either (i) assume that
the same entities appear on multiple web pages,
or (ii) require information such as seed examples
(Etzioni et al, 2005; Wang and Cohen, 2009;
Dalvi et al, 2012). These approaches work well
for common categories but encounter data sparsity
problems for more specific categories, such as the
products of a small company or the dishes at a lo-
cal restaurant. In this context, we may have only a
single web page that contains the information we
need and no seed examples.
In this paper, we propose a novel task, zero-
shot entity extraction, where the specification
of the desired entities is provided as a natural
language query. Given a query (e.g., hiking
seedsAvalon Super LoopHilton Area traditional
answersAvalon Super LoopHilton AreaWildlands Loop...
queryhiking trailsnear Baltimore our system
answersAvalon Super LoopHilton AreaWildlands Loop...
web pagesweb pagesweb pages
web page
Figure 1: Entity extraction typically requires ad-
ditional knowledge such as a small set of seed ex-
amples or depends on multiple web pages. In our
setting, we take as input a natural language query
and extract entities from a single web page.
trails near Baltimore) and a web page (e.g.,
http://www.everytrail.com/best/
hiking-baltimore-maryland), the goal is
to extract all entities corresponding to the query
on that page (e.g., Avalon Super Loop, etc.).
Figure 1 summarizes the task setup.
The task introduces two challenges. Given a
single web page to extract entities from, we can
no longer rely on the redundancy of entities across
multiple web pages. Furthermore, in the zero-shot
learning paradigm (Larochelle et al, 2008), where
entire categories might be unseen during training,
the system must generalize to new queries and web
pages without the additional aid of seed examples.
To tackle these challenges, we cast the task as
a structured prediction problem where the input
is the query and the web page, and the output is
a list of entities, mediated by a latent extraction
predicate. To generalize across different inputs,
we rely on two types of features: structural fea-
tures, which look at the layout and placement of
the entities being extracted; and denotation fea-
391
tures, which look at the list of entities as a whole
and assess their linguistic coherence. When defin-
ing features on lists, one technical challenge is be-
ing robust to widely varying list sizes. We ap-
proach this challenge by defining features over a
histogram of abstract tokens derived from the list
elements.
For evaluation, we created the OPENWEB
dataset comprising natural language queries from
the Google Suggest API and diverse web pages re-
turned from web search. Despite the variety of
queries and web pages, our system still achieves
a test accuracy of 40.5% and an accuracy at 5 of
55.8%.
2 Problem statement
We define the zero-shot entity extraction task as
follows: let x be a natural language query (e.g.,
hiking trails near Baltimore), and w be a web
page. Our goal is to construct a mapping from
(x,w) to a list of entities y (e.g., [Avalon Super
Loop, Patapsco Valley State Park, . . . ]) which are
extracted from the web page.
Ideally, we would want our data to be anno-
tated with the correct entity lists y, but this would
be very expensive to obtain. We instead define
each training and test example as a triple (x,w, c),
where the compatibility function c maps each y to
c(y) ? {0, 1} denoting the (approximate) correct-
ness of the list y. In this paper, an entity list y is
compatible (c(y) = 1) when the first, second, and
last elements of y match the annotation; otherwise,
it is incompatible (c(y) = 0).
2.1 Dataset
To experiment with a diverse set of queries and
web pages, we created a new dataset, OPENWEB,
using web pages from Google search results.
1
We
use the method from Berant et al (2013) to gen-
erate search queries by performing a breadth-first
search over the query space. Specifically, we
use the Google Suggest API, which takes a par-
tial query (e.g., ?list of movies?) and out-
puts several complete queries (e.g., ?list of hor-
ror movies?). We start with seed partial queries
?list of ? ? where ? is one or two initial let-
ters. In each step, we call the Google Suggest API
on the partial queries to obtain complete queries,
1
The OPENWEB dataset and our code base are available
for download at http://www-nlp.stanford.edu/
software/web-entity-extractor-ACL2014.
Full query New partial queries
list of X IN Y list of X
where IN is a preposition list of X
(list of [hotels]
X
in [Guam]
Y
) list of X IN
list of IN Y
list of X CC Y list of X
where CC is a conjunction list of X
(list of [food]
X
and [drink]
Y
) list of Y
list of Y
list of X w list of w
(list of [good 2012]
X
[movies]
w
) list of w
list of X
Table 1: Rules for generating new partial queries
from complete queries. (X and Y are sequences
of words; w is a single word.)
and then apply the transformation rules in Table 1
to generate more partial queries from complete
queries. We run the procedure until we obtained
100K queries.
Afterwards, we downloaded the top 2?3 Google
search results of each query, sanitized the web
pages, and randomly submitted 8000 query / web
page pairs to Amazon Mechanical Turk (AMT).
Each AMT worker must either mark the web page
as irrelevant or extract the first, second, and last
entities from the page. We only included exam-
ples where at least two AMT workers agreed on
the answer.
The resulting OPENWEB dataset consists of
2773 examples from 2269 distinct queries.
Among these queries, there are 894 headwords
ranging from common categories (e.g., movies,
companies, characters) to more specific ones (e.g.,
enzymes, proverbs, headgears). The dataset con-
tains web pages from 1438 web domains, of which
83% appear only once in our dataset.
Figure 2 shows some queries and web pages
from the OPENWEB dataset. Besides the wide
range of queries, another main challenge of the
dataset comes from the diverse data representa-
tion formats, including complex tables, grids, lists,
headings, and paragraphs.
3 Approach
Figure 3 shows the framework of our system.
Given a query x and a web page w, the system
generates a set Z(w) of extraction predicates z
which can extract entities from semi-structured
data in w. Section 3.1 describes extraction pred-
icates in more detail. Afterwards, the system
chooses z ? Z(w) that maximizes the model
probability p
?
(z | x,w), and then executes z on
392
Queriesairlines of italynatural causes of global warminglsu football coachesbf3 submachine gunsbadminton tournamentsfoods high in dhatechnical colleges in south carolinasongs on glee season 5singers who use auto tunesan francisco radio stationsactors from boston
Examples (web page, query)
airlines of italy natural causes of global warming lsu football coaches
Figure 2: Some examples illustrating the diversity of queries and web pages from the OPENWEB dataset.
x Generation w
Z
Model
z Execution
y
hiking trailsnear Baltimore
html
head
...
body
...
/html[1]/body[1]/table[2]/tr/td[1]
[Avalon Super Loop, Hilton Area, ...]
Figure 3: An overview of our system. The system
uses the input query x and web page w to produce
a list of entities y via an extraction predicate z.
w to get the list of entities y = JzK
w
. Section 3.2
describes the model and the training procedure,
while Section 3.3 presents the features used in our
model.
3.1 Extraction predicates
We represent each web page w as a DOM tree, a
common representation among wrapper induction
and web information extraction systems (Sahuguet
and Azavant, 1999; Liu et al, 2000; Crescenzi et
al., 2001). The text of any DOM tree node that is
shorter than 140 characters is a candidate entity.
However, without further restrictions, the number
of possible entity lists grows exponentially with
the number of candidate entities.
To make the problem tractable, we introduce an
extraction predicate z as an intermediate represen-
tation for extracting entities from w. In our sys-
tem, we let an extraction predicate be a simplified
XML path (XPath) such as
/html[1]/body[1]/table[2]/tr/td[1]
Informally, an extraction predicate is a list of
path entries. Each path entry is either a tag (e.g.,
tr), which selects all children with that tag; or a
tag and an index i (e.g., td[1]), which selects
only the ith child with that tag. The denotation
y = JzK
w
of an extraction predicate z is the list of
entities selected by the XPath. Figure 4 illustrates
the execution of the extraction predicate above on
a DOM tree.
In the literature, many information extraction
systems employ more versatile extraction predi-
cates (Wang and Cohen, 2009; Fumarola et al,
2011). However, despite the simplicity, we are
able to find an extraction predicate that extracts
a compatible entity list in 69.7% of the develop-
ment examples. In some examples, we cannot ex-
tract a compatible list due to unrecoverable issues
such as incorrect annotation. Section 4.4 provides
a detailed analysis of these issues. Additionally,
extraction predicates can be easily extended to in-
crease the coverage. For example, by introduc-
ing new index types [1:] (selects all but the first
node) and [:-1] (selects all but the last node),
we can increase the coverage to 76.2%.
Extraction predicate generation. We generate
a set Z(w) of extraction predicates for a given
web page w as follows. For each node in
the DOM tree, we find an extraction predicate
which selects only that node, and then gener-
alizes the predicate by removing any subset of
the indices of the last k path entries. For in-
stance, when k = 2, an extraction predicate
ending in .../tr[5]/td[2] will be general-
ized to .../tr[5]/td[2], .../tr/td[2],
.../tr[5]/td, and .../tr/td. In all ex-
periments, we use k = 8, which gives at most 2
8
generalized predicates for each original predicate.
This generalization step allows the system to se-
lect multiple nodes with the same structure (e.g.,
393
DOM tree w
html
head body
table
tr
td
Home..
td
Expl..
td
Mobi..
td
Crea..
h1
Hiki..
table
tr
th
Name..
th
Loca..
tr
td
Aval..
td
12.7..
... tr
td
Gove..
td
3.1 ..Extraction predicate z/html[1]/body[1]/table[2]/tr/td[1]
Rendered web page
Home Explore Mobile Apps Create TripHiking near Baltimore, MarylandName LengthAvalon Super Loop 12.7 milesHilton Area 7.8 milesAvalon Loop 9.4 milesWildlands Loop 4.4 milesMckeldin Area 16.7 milesGreenbury Point 3.7 milesGoverner Bridge Natural Area 3.1 miles
Figure 4: A simplified example of a DOM tree w and an extraction predicate z, which selects a list of
entity strings y = JzK
w
from the page (highlighted in red).
table cells from the same column or list items from
the same section of the page).
Out of all generalized extraction predicates, we
retain the ones that extract at least two entities
from w. Note that several extraction predicates
may select the same list of nodes and thus produce
the same list of entities.
The procedure above gives a manageable num-
ber of extraction predicates. Among the devel-
opment examples of the OPENWEB dataset, we
generate an average of 8449 extraction predicates
per example, which evaluate to an average of 1209
unique entity lists.
3.2 Modeling
Given a query x and a web page w, we define
a log-linear distribution over all extraction predi-
cates z ? Z(w) as
p
?
(z | x,w) ? exp{?
>
?(x,w, z)}, (1)
where ? ? R
d
is the parameter vector and
?(x,w, z) is the feature vector, which will be de-
fined in Section 3.3.
To train the model, we find a parameter vec-
tor ? that maximizes the regularized log marginal
probability of the compatibility function being sat-
isfied. In other words, given training data D =
{(x
(i)
, w
(i)
, c
(i)
)}
n
i=1
, we find ? that maximizes
n
?
i=1
log p
?
(c
(i)
= 1 | x
(i)
, w
(i)
)?
?
2
???
2
2
where
p
?
(c = 1 | x,w) =
?
z?Z(w)
p
?
(z | x,w) ? c(JzK
w
).
Note that c(JzK
w
) = 1 when the entity list y =
JzK
w
selected by z is compatible with the annota-
tion; otherwise, c(JzK
w
) = 0.
We use AdaGrad, an online gradient descent
with an adaptive per-feature step size (Duchi et al,
2010), making 5 passes over the training data. We
use ? = 0.01 obtained from cross-validation for
all experiments.
3.3 Features
To construct the log-linear model, we define a fea-
ture vector ?(x,w, z) for each query x, web page
w, and extraction predicate z. The final feature
vector is the concatenation of structural features
?
s
(w, z), which consider the selected nodes in
the DOM tree, and denotation features ?
d
(x, y),
which look at the extracted entities.
We will use the query hiking trails near Balti-
more and the web page in Figure 4 as a running
example. Figure 5 lists some features extracted
from the example.
3.3.1 Recipe for defining features on lists
One main focus of our work is finding good fea-
ture representations for a list of objects (DOM tree
nodes for structural features and entity strings for
denotation features). One approach is to define the
feature vector of a list to be the sum of the feature
vectors of individual elements. This is commonly
done in structured prediction, where the elements
are local configurations (e.g., rule applications in
parsing). However, this approach raises a normal-
ization issue when we have to compare and rank
lists of drastically different sizes.
As an alternative, we propose a recipe for gen-
erating features from a list as follows:
394
html
head body
table
...
h1 table
tr
th th
tr
td td
tr
td td
... tr
td td
Structural feature Value
Features on selected nodes:
TAG-MAJORITY = td 1
INDEX-ENTROPY 0.0
Features on parent nodes:
CHILDRENCOUNT-MAJORITY = 2 1
PARENT-SINGLE 1
INDEX-ENTROPY 1.0
HEADHOLE (The first node is skipped) 1
Features on grandparent nodes:
PAGECOVERAGE 0.6
. . . . . .
Selected entitiesAvalon Super LoopHilton AreaAvalon LoopWildlands LoopMckeldin AreaGreenbury PointGoverner Bridge Natural Area
Denotation feature Value
WORDSCOUNT-MEAN 2.42
PHRASESHAPE-MAJORITY = Aa Aa 1
PHRASESHAPE-MAJORITYRATIO 0.71
WORDSHAPE-MAJORITY = Aa 1
PHRASEPOS-MAJORITY = NNP NN 1
LASTWORD-ENTROPY 0.74
WORDPOS = NN (normalized count) 0.53
. . . . . .
Figure 5: A small subset of features from the example hiking trails near Baltimore in Figure 4.
A B C D E
1 2 0 1 0
0 1 2histogram
Abstraction
Aggregation EntropyMajorityMajorityRatioSingle(Mean)(Variance)
Figure 6: The recipe for defining features on a
list of objects: (i) the abstraction step converts list
elements into abstract tokens; (ii) the aggregation
step defines features using the histogram of the ab-
stract tokens.
Step 1: Abstraction. We map each list element
into an abstract token. For example, we can map
each DOM tree node onto an integer equal to the
number of children, or map each entity string onto
its part-of-speech tag sequence.
Step 2: Aggregation. We create a histogram of
the abstract tokens and define features on proper-
ties of the histogram. Generally, we use ENTROPY
(entropy normalized to the maximum value of 1),
MAJORITY (mode), MAJORITYRATIO (percent-
age of tokens sharing the majority value), and
SINGLE (whether all tokens are identical). For
abstract tokens with finitely many possible values
(e.g., part-of-speech), we also use the normalized
histogram count of each possible value as a fea-
ture. And for real-valued abstract tokens, we also
use the mean and the standard deviation. In the
actual system, we convert real-valued features (en-
tropy, histogram count, mean, and standard devia-
tion) into indicator features by binning.
Figure 6 summarizes the steps explained above.
We use this recipe for defining both structural and
denotation features, which are discussed below.
3.3.2 Structural features
Although different web pages represent data in
different formats, they still share some common
hierarchical structures in the DOM tree. To cap-
ture this, we define structural features ?
s
(w, z),
which consider the properties of the selected nodes
in the DOM tree, as follows:
Features on selected nodes. We apply our
recipe on the list of nodes in w selected by z using
the following abstract tokens:
? TAG, ID, CLASS, etc. (HTML attributes)
? CHILDRENCOUNT and SIBLINGSCOUNT
(number of children and siblings)
? INDEX (position among its siblings)
? PARENT (parent node; e.g., PARENT-SINGLE
means that all nodes share the same parent.)
Additionally, we define the following features
based on the coverage of all selected nodes:
? NOHOLE, HEADHOLE, etc. (node coverage
in the same DOM tree level; e.g., HEAD-
HOLE activates when the first sibling of the
selected nodes is not selected.)
395
? PAGECOVERAGE (node coverage relative to
the entire tree; we use depth-first traversal
timestamps to estimate the fraction of nodes
in the subtrees of the selected nodes.)
Features on ancestor nodes. We also define the
same feature set on the list of ancestors of the se-
lected nodes in the DOM tree. In our experiments,
we traverse up to 5 levels of ancestors and define
features from the nodes in each level.
3.3.3 Denotation features
Structural features are not powerful enough to dis-
tinguish between entity lists appearing in similar
structures such as columns of the same table or
fields of the same record. To solve this ambiguity,
we introduce denotation features ?
d
(x, y) which
considers the coherence or appropriateness of the
selected entity strings y = JzK
w
.
We observe that the correct entities often share
some linguistic statistics. For instance, entities in
many categories (e.g., people and place names)
usually have only 2?3 word tokens, most of which
are proper nouns. On the other hand, random
words on the web page tend to have more diverse
lengths and part-of-speech tags.
We apply our recipe on the list of selected enti-
ties using the following abstract tokens:
? WORDSCOUNT (number of words)
? PHRASESHAPE (abstract shape of the phrase;
e.g., Barack Obama becomes Aa Aa)
? WORDSHAPE (abstract shape of each word;
the number of abstract tokens will be the total
number of words over all selected entities)
? FIRSTWORD and LASTWORD
? PHRASEPOS and WORDPOS (part-of-
speech tags for whole phrases and individual
words)
4 Experiments
In this section we evaluate our system on the
OPENWEB dataset.
4.1 Evaluation metrics
Accuracy. As the main metric, we use a notion
of accuracy based on compatibility; specifically,
we define the accuracy as the fraction of examples
where the system predicts a compatible entity list
as defined in Section 2. We also report accuracy
at 5, the fraction of examples where the top five
predictions contain a compatible entity list.
Path suffix pattern (multiset) Count
{a, table, tbody, td[
*
], tr} 1792
{a, tbody, td[
*
], text, tr} 1591
{a, table[
*
], tbody, td[
*
], tr} 1325
{div, table, tbody, td[
*
], tr} 1259
{b, div, div, div, div[
*
]} 1156
{div[
*
], table, tbody, td[
*
], tr} 1059
{div, table[
*
], tbody, td[
*
], tr} 844
{table, tbody, td[
*
], text, tr} 828
{div[
*
], table[
*
], tbody, td[
*
], tr} 793
{a, table, tbody, td, tr} 743
Table 2: Top 10 path suffix patterns found by the
baseline learner in the development data. Since
we allow path entries to be permuted, each suffix
pattern is represented by a multiset of path entries.
The notation [
*
] denotes any path entry index.
To see how our compatibility-based accuracy
tracks exact correctness, we sampled 100 web
pages which have at least one valid extraction
predicate and manually annotated the full list of
entities. We found that in 85% of the examples,
the longest compatible list y is the correct list of
entities, and many lists in the remaining 15% miss
the correct list by only a few entities.
Oracle. In some examples, our system cannot
find any list of entities that is compatible with the
gold annotation. The oracle score is the fraction
of examples in which the system can find at least
one compatible list.
4.2 Baseline
As a baseline, we list the suffixes of the cor-
rect extraction predicates in the training data, and
then sort the resulting suffix patterns by frequency.
To improve generalization, we treat path entries
with different indices (e.g., td[1] vs. td[2]) as
equivalent and allow path entries to be permuted.
Table 2 lists the top 10 suffix patterns from the de-
velopment data. At test time, we choose an extrac-
tion predicate with the most frequent suffix pat-
tern. The baseline should work considerably well
if the web pages were relatively homogeneous.
4.3 Main results
We held out 30% of the dataset as test data. For the
results on development data, we report the average
across 10 random 80-20 splits. Table 3 shows the
results. The system gets an accuracy of 41.1% and
40.5% for the development and test data, respec-
tively. If we consider the top 5 lists of entities, the
accuracy increases to 58.4% on the development
data and 55.8% on the test data.
396
Development data Test data
Acc A@5 Acc A@5
Baseline 10.8 ? 1.3 25.6 ? 2.0 10.3 20.9
Our system 41.1 ? 3.4 58.4 ? 2.7 40.5 55.8
Oracle 68.7 ? 2.4 68.7 ? 2.4 66.6 66.6
Table 3: Main results on the OPENWEB dataset
using the default set of features. (Acc = accuracy,
A@5 = accuracy at 5)
4.4 Error analysis
We now investigate the errors made by our system
using the development data. We classify the er-
rors into two types: (i) coverage errors, which are
when the system cannot find any entity list satis-
fying the compatibility function; and (ii) ranking
errors, which are when a compatible list of entities
exists, but the system outputs an incompatible list.
Tables 4 and 5 show the breakdown of cover-
age and ranking errors from an experiment on the
development data.
Analysis of coverage errors. From Table 4,
about 36% of coverage errors happen when the
extraction predicate for the correct entities also
captures unrelated parts of the web page (Reason
C1). For example, many Wikipedia articles have
the See Also section that lists related articles in an
unordered list (/ul/li/a), which causes a prob-
lem when the entities are also represented in the
same format.
Another main source of errors is the in-
consistency in HTML tag usage (Reason C2).
For instance, some web pages use <b> and
<strong> tags for bold texts interchangeably,
or switch between <b><a>...</a></b> and
<a><b>...</b></a> across entities. We ex-
pect that this problem can be solved by normaliz-
ing the web page, using an alternative web page
representation (Cohen et al, 2002; Wang and Co-
hen, 2009; Fumarola et al, 2011), or leveraging
more expressive extraction predicates (Dalvi et al,
2011).
One interesting source of errors is Reason C3,
where we need to filter the selected entities to
match the complex requirement in the query. For
example, the query tech companies in China re-
quires the system to select only the company
names with China in the corresponding location
column. To handle such queries, we need a deeper
understanding of the relation between the linguis-
tic structure of the query and the hierarchical
structure of the web page. Tackling this error re-
Setting Acc A@5
All features 41.1 ? 3.4 58.4 ? 2.7
Oracle 68.7 ? 2.4 68.7 ? 2.4
(Section 4.5)
Structural features only 36.2 ? 1.9 54.5 ? 2.5
Denotation features only 19.8 ? 2.5 41.7 ? 2.7
(Section 4.6)
Structural + query-denotation 41.7 ? 2.5 58.1 ? 2.4
Query-denotation features only 25.0 ? 2.3 48.0 ? 2.7
Concat. a random web page +
structural + denotation 19.3 ? 2.6 41.2 ? 2.3
Concat. a random web page +
structural + query-denotation 29.2 ? 1.7 49.2 ? 2.2
(Section 4.7)
Add 1 seed entity 52.9 ? 3.0 66.5 ? 2.5
Table 6: System accuracy with different feature
and input settings on the development data. (Acc
= accuracy, A@5 = accuracy at 5)
quires compositionality and is critical to general-
ize to more complex queries.
Analysis of ranking errors. From Table 5, a
large number of errors are attributed to the system
selecting non-content elements such as navigation
links and content headings (Reason R1). Feature
analysis reveals that both structural and linguis-
tic statistics of these non-content elements can be
more coherent than those of the correct entities.
We suspect that since many of our features try to
capture the coherence of entities, the system some-
times erroneously favors the more homonogenous
non-content parts of the page. To disfavor these
parts, One possible solution is to add visual fea-
tures that capture how the web page is rendered
and favor more salient parts of the page. (Liu et al,
2003; Song et al, 2004; Zhu et al, 2005; Zheng et
al., 2007).
4.5 Feature variations
We now investigate the contribution of each fea-
ture type. The ablation results on the development
set over 10 random splits are shown in Table 6.
We observe that denotation features improves ac-
curacy on top of structural features.
Table 7 shows an example of an error that is
eliminated by each feature type. Generally, if
the entities are represented as records (e.g., rows
of a table), then denotation features will help the
system select the correct field from each record.
On the other hand, structural features prevent the
system from selecting random entities outside the
main part of the page.
397
Reason Short example Count
C1 Answers and contextual elements are selected
by the same extraction predicate.
Select entries in See Also section in addition to the con-
tent because they are all list entries.
48
C2 HTML tag usage is inconsistent. The page uses both b and strong for headers. 16
C3 The query applies to only some sections of the
matching entities.
Need to select only companies in China from the table
of all Asian companies.
20
C4 Answers are embedded in running text. Answers are in a comma-separated list. 13
C5 Text normalization issues. Selected Silent Night Lyrics instead of Silent Night. 19
C6 Other issues. Incorrect annotation. / Entities are permuted when the
web page is rendered. / etc.
18
Total 134
Table 4: Breakdown of coverage errors from the development data.
Reason Short example Count
R1 Select non-content strings. Select navigation links, headers, footers, or sidebars. 25
R2 Select entities from a wrong field. Select book authors instead of book names. 22
R3 Select entities from the wrong section(s). For the query schools in Texas, select all schools on the
page, or select the schools in Alabama instead.
19
R4 Also select headers or footers. Select the table header in addition to the answers. 7
R5 Select only entities with a particular formatting. From a list of answers, select only anchored (a) entities. 4
R6 Select headings instead of the contents or vice
versa.
Select the categories of rums in h2 tags instead of the
rum names in the tables.
2
R7 Other issues. Incorrect annotation. / Multiple sets of answers appear
on the same page. / etc.
9
Total 88
Table 5: Breakdown of ranking errors from the development data.
All features Structural only Denotation only
The Sun CIRC: 2,279,492 Paperboy Australia
Daily Mail CIRC: 1,821,684 Paperboy UK
Daily Mirror CIRC: 1,032,144 Paperboy Home Page
. . . . . . . . .
Table 7: System outputs for the query UK news-
papers with different feature sets. Without deno-
tation features, the system selects the daily circu-
lation of each newspaper instead of the newspaper
names. And without structural features, the sys-
tem selects the hidden navigation links from the
top of the page.
4.6 Incorporating query information
So far, note that all our features depend only on
the extraction predicate z and not the input query
x. Remarkably, we were still able to obtain rea-
sonable results. One explanation is that since we
obtained the web pages from a search engine, the
most prominent entities on the web pages, such as
entities in table cells in the middle of the page, are
likely to be good independent of the query.
However, different queries often denote enti-
ties with different linguistic properties. For exam-
ple, queries mayors of Chicago and universities in
Chicago will produce entities of different lengths,
part-of-speech sequences, and word distributions.
This suggests incorporating features that depend
on the query.
To explore the potential of query informa-
tion, we conduct the following oracle experi-
ment. We replace each denotation feature f(y)
with a corresponding query-denotation feature
(f(y), g(x)), where g(x) is the category of the
query x. We manually classified all queries in our
dataset into 7 categories: person, media title, loca-
tion/organization, abtract entity, word/phrase, ob-
ject name, and miscellaneous.
Table 8 shows some examples where adding
these query-denotation features improves the se-
lected entity lists by favoring answers that are
more suitable to the query category. However, Ta-
ble 6 shows that these new features do not signifi-
cantly improve the accuracy of our original system
on the development data.
We suspect that any gains offered by the query-
denotation features are subsumed by the structural
features. To test this hypothesis, we conducted
two experiments, the results of which are shown
in Table 6. First, we removed structural features
and found that using query-denotation features im-
proves accuracy significantly over using denota-
tion features alone from 19.8% to 25.0%. Second,
we created a modified dataset where the web page
in each example is a concatenation of the orig-
inal web page and an unrelated web page. On
398
Query euclid?s elements book titles soft drugs professional athletes with concussions
Default
features
?Prematter?, ?Book I.?,
?Book II.?, ?Book III.?, . . .
?Hard drugs?, ?Soft drugs?,
?Some drugs cannot be
classified that way?, . . .
?Pistons-Knicks Game Becomes Site
of Incredible Dance Battle?, ?Toronto
Mayor Rob Ford Attends . . . ?, . . .
Structural
+ Query-
Denotation
(category = media title)
?Book I. The fundamentals . . . ?,
?Book II. Geometric algebra?, . . .
(category = object name)
?methamphetamine?,
?psilocybin?, ?caffeine?
(category = person)
?Mike Richter?, ?Stu Grimson?,
?Geoff Courtnall?, . . .
Table 8: System outputs after changing denotation features into query-denotation features.
this modified dataset, the prominent entities may
not be the answers to the query. Here, query-
denotation features improves accuracy over deno-
tation features alone from 19.3% to 29.2%.
4.7 Comparison with other problem settings
Since zero-shot entity extraction is a new task,
we cannot directly compare our system with other
systems. However, we can mimic the settings of
other tasks. In one experiment, we augment each
input query with a single seed entity (the second
annotated entity in our experiments); this setting
is suggestive of Wang and Cohen (2009). Table 6
shows that this augmentation increases accuracy
from 41.1% to 52.9%, suggesting that our sys-
tem can perform substantially better with a small
amount of additional supervision.
5 Discussion
Our work shares a base with the wrapper induc-
tion literature (Kushmerick, 1997) in that it lever-
ages regularities of web page structures. However,
wrapper induction usually focuses on a small set
of web domains, where the web pages in each do-
main follow a fixed template (Muslea et al, 2001;
Crescenzi et al, 2001; Cohen et al, 2002; Arasu
and Garcia-Molina, 2003). Later work in web data
extraction attempts to generalize across different
web pages, but relies on either restricted data for-
mats (Wong et al, 2009) or prior knowledge of
web page structures with respect to the type of data
to extract (Zhang et al, 2013).
In our case, we only have the natural language
query, which presents the more difficult problem
of associating the entity class in the query (e.g.,
hiking trails) to concrete entities (e.g., Avalon Su-
per Loop). In contrast to information extraction
systems that extract homogeneous records from
web pages (Liu et al, 2003; Zheng et al, 2009),
our system must choose the correct field from each
record and also identify the relevant part of the
page based on the query.
Another related line of work is information ex-
traction from text, which relies on natural lan-
guage patterns to extract categories and relations
of entities. One classic example is Hearst pat-
terns (Hearst, 1992; Etzioni et al, 2005), which
can learn new entities and extraction patterns from
seed examples. More recent approaches also
leverage semi-structured data to obtain more ro-
bust extraction patterns (Mintz et al, 2009; Hoff-
mann et al, 2011; Surdeanu et al, 2012; Riedel
et al, 2013). Although our work focuses on semi-
structured web pages rather than raw text, we use
linguistic patterns of queries and entities as a sig-
nal for extracting appropriate answers.
Additionally, our efforts can be viewed as build-
ing a lexicon on the fly. In recent years, there
has been a drive to scale semantic parsing to large
databases such as Freebase (Cai and Yates, 2013;
Berant et al, 2013; Kwiatkowski et al, 2013).
However, despite the best efforts of information
extraction, such databases will always lag behind
the open web. For example, Berant et al (2013)
found that less than 10% of naturally occurring
questions are answerable by a simple Freebase
query. By using the semi-structured data from the
web as a knowledge base, we hope to increase fact
coverage for semantic parsing.
Finally, as pointed out in the error analysis, we
need to filter or aggregate the selected entities for
complex queries (e.g., tech companies in China for
a web page with all Asian tech companies). In fu-
ture work, we would like to explore the issue of
compositionality in queries by aligning linguistic
structures in natural language with the relative po-
sition of entities on web pages.
Acknowledgements
We gratefully acknowledge the support of the
Google Natural Language Understanding Focused
Program. In addition, we would like to thank
anonymous reviewers for their helpful comments.
399
References
A. Arasu and H. Garcia-Molina. 2003. Extracting
structured data from web pages. In ACM SIGMOD
international conference on Management of data,
pages 337?348.
J. Berant, A. Chou, R. Frostig, and P. Liang. 2013.
Semantic parsing on Freebase from question-answer
pairs. In Empirical Methods in Natural Language
Processing (EMNLP).
Q. Cai and A. Yates. 2013. Large-scale semantic pars-
ing via schema matching and lexicon extension. In
Association for Computational Linguistics (ACL).
W. W. Cohen, M. Hurst, and L. S. Jensen. 2002. A
flexible learning system for wrapping tables and lists
in HTML documents. In World Wide Web (WWW),
pages 232?241.
V. Crescenzi, G. Mecca, P. Merialdo, et al 2001.
Roadrunner: Towards automatic data extraction
from large web sites. In VLDB, volume 1, pages
109?118.
N. Dalvi, R. Kumar, and M. Soliman. 2011. Auto-
matic wrappers for large scale web extraction. Pro-
ceedings of the VLDB Endowment, 4(4):219?230.
B. Dalvi, W. Cohen, and J. Callan. 2012. Websets:
Extracting sets of entities from the web using unsu-
pervised information extraction. In Web Search and
Data Mining (WSDM), pages 243?252.
J. Duchi, E. Hazan, and Y. Singer. 2010. Adaptive sub-
gradient methods for online learning and stochastic
optimization. In Conference on Learning Theory
(COLT).
O. Etzioni, M. Cafarella, D. Downey, A. Popescu,
T. Shaked, S. Soderland, D. S. Weld, and A. Yates.
2005. Unsupervised named-entity extraction from
the web: An experimental study. Artificial Intelli-
gence, 165(1):91?134.
F. Fumarola, T. Weninger, R. Barber, D. Malerba, and
J. Han. 2011. Extracting general lists from web doc-
uments: A hybrid approach. Modern Approaches in
Applied Intelligence Springer.
M. A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Interational
Conference on Computational linguistics, pages
539?545.
R. Hoffmann, C. Zhang, X. Ling, L. S. Zettlemoyer,
and D. S. Weld. 2011. Knowledge-based weak su-
pervision for information extraction of overlapping
relations. In Association for Computational Lin-
guistics (ACL), pages 541?550.
N. Kushmerick. 1997. Wrapper induction for informa-
tion extraction. Ph.D. thesis, University of Washing-
ton.
T. Kwiatkowski, E. Choi, Y. Artzi, and L. Zettlemoyer.
2013. Scaling semantic parsers with on-the-fly on-
tology matching. In Empirical Methods in Natural
Language Processing (EMNLP).
H. Larochelle, D. Erhan, and Y. Bengio. 2008. Zero-
data learning of new tasks. In AAAI, volume 8,
pages 646?651.
L. Liu, C. Pu, and W. Han. 2000. XWRAP: An XML-
enabled wrapper construction system for web infor-
mation sources. In Data Engineering, 2000. Pro-
ceedings. 16th International Conference on, pages
611?621.
B. Liu, R. Grossman, and Y. Zhai. 2003. Mining data
records in web pages. In Proceedings of the ninth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, pages 601?606.
M. Mintz, S. Bills, R. Snow, and D. Jurafsky. 2009.
Distant supervision for relation extraction without
labeled data. In Association for Computational Lin-
guistics (ACL), pages 1003?1011.
I. Muslea, S. Minton, and C. A. Knoblock. 2001. Hi-
erarchical wrapper induction for semistructured in-
formation sources. Autonomous Agents and Multi-
Agent Systems, 4(1):93?114.
S. Riedel, L. Yao, and A. McCallum. 2013. Re-
lation extraction with matrix factorization and uni-
versal schemas. In North American Association for
Computational Linguistics (NAACL).
A. Sahuguet and F. Azavant. 1999. WysiWyg web
wrapper factory (W4F). In WWW Conference.
R. Song, H. Liu, J. Wen, and W. Ma. 2004. Learning
block importance models for web pages. In World
Wide Web (WWW), pages 203?211.
M. Surdeanu, J. Tibshirani, R. Nallapati, and C. D.
Manning. 2012. Multi-instance multi-label learning
for relation extraction. In Empirical Methods in Nat-
ural Language Processing and Computational Nat-
ural Language Learning (EMNLP/CoNLL), pages
455?465.
R. C. Wang and W. W. Cohen. 2009. Character-level
analysis of semi-structured documents for set expan-
sion. In Empirical Methods in Natural Language
Processing (EMNLP), pages 1503?1512.
Y. W. Wong, D. Widdows, T. Lokovic, and K. Nigam.
2009. Scalable attribute-value extraction from semi-
structured text. In IEEE International Conference
on Data Mining Workshops, pages 302?307.
Z. Zhang, K. Q. Zhu, H. Wang, and H. Li. 2013. Au-
tomatic extraction of top-k lists from the web. In
International Conference on Data Engineering.
S. Zheng, R. Song, and J. Wen. 2007. Template-
independent news extraction based on visual consis-
tency. In AAAI, volume 7, pages 1507?1513.
400
S. Zheng, R. Song, J. Wen, and C. L. Giles. 2009. Ef-
ficient record-level wrapper induction. In Proceed-
ings of the 18th ACM conference on Information and
knowledge management, pages 47?56.
J. Zhu, Z. Nie, J. Wen, B. Zhang, and W. Ma. 2005.
2D conditional random fields for web information
extraction. In International Conference on Machine
Learning (ICML), pages 1044?1051.
401
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1415?1425,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Semantic Parsing via Paraphrasing
Jonathan Berant
Stanford University
joberant@stanford.edu
Percy Liang
Stanford University
pliang@cs.stanford.edu
Abstract
A central challenge in semantic parsing is
handling the myriad ways in which knowl-
edge base predicates can be expressed.
Traditionally, semantic parsers are trained
primarily from text paired with knowledge
base information. Our goal is to exploit
the much larger amounts of raw text not
tied to any knowledge base. In this pa-
per, we turn semantic parsing on its head.
Given an input utterance, we first use a
simple method to deterministically gener-
ate a set of candidate logical forms with
a canonical realization in natural language
for each. Then, we use a paraphrase model
to choose the realization that best para-
phrases the input, and output the corre-
sponding logical form. We present two
simple paraphrase models, an association
model and a vector space model, and train
them jointly from question-answer pairs.
Our system PARASEMPRE improves state-
of-the-art accuracies on two recently re-
leased question-answering datasets.
1 Introduction
We consider the semantic parsing problem of map-
ping natural language utterances into logical forms
to be executed on a knowledge base (KB) (Zelle
and Mooney, 1996; Zettlemoyer and Collins,
2005; Wong and Mooney, 2007; Kwiatkowski
et al, 2010). Scaling semantic parsers to large
knowledge bases has attracted substantial atten-
tion recently (Cai and Yates, 2013; Berant et al,
2013; Kwiatkowski et al, 2013), since it drives
applications such as question answering (QA) and
information extraction (IE).
Semantic parsers need to somehow associate
natural language phrases with logical predicates,
e.g., they must learn that the constructions ?What
What party did Clay establish?
paraphrase model
What political party founded by Henry Clay? ... What event involved the people Henry Clay?
Type.PoliticalParty u Founder.HenryClay ... Type.Event u Involved.HenryClay
Whig Party
Figure 1: Semantic parsing via paraphrasing: For each
candidate logical form (in red), we generate canonical utter-
ances (in purple). The model is trained to paraphrase the in-
put utterance (in green) into the canonical utterances associ-
ated with the correct denotation (in blue).
does X do for a living??, ?What is X?s profes-
sion??, and ?Who is X??, should all map to the
logical predicate Profession. To learn these map-
pings, traditional semantic parsers use data which
pairs natural language with the KB. However, this
leaves untapped a vast amount of text not related
to the KB. For instance, the utterances ?Where is
ACL in 2014?? and ?What is the location of ACL
2014?? cannot be used in traditional semantic
parsing methods, since the KB does not contain
an entity ACL2014, but this pair clearly contains
valuable linguistic information. As another refer-
ence point, out of 500,000 relations extracted by
the ReVerb Open IE system (Fader et al, 2011),
only about 10,000 can be aligned to Freebase (Be-
rant et al, 2013).
In this paper, we present a novel approach for
semantic parsing based on paraphrasing that can
exploit large amounts of text not covered by the
KB (Figure 1). Our approach targets factoid ques-
tions with a modest amount of compositionality.
Given an input utterance, we first use a simple de-
terministic procedure to construct a manageable
set of candidate logical forms (ideally, we would
generate canonical utterances for all possible logi-
cal forms, but this is intractable). Next, we heuris-
1415
utterance
underspecifiedlogicalform
canonicalutterance
logicalform
ontologymatching
paraphrase
direct(traditional)
(Kwiatkowski et al 2013)
(this work)
Figure 2: The main challenge in semantic parsing is cop-
ing with the mismatch between language and the KB. (a)
Traditionally, semantic parsing maps utterances directly to
logical forms. (b) Kwiatkowski et al (2013) map the utter-
ance to an underspecified logical form, and perform ontology
matching to handle the mismatch. (c) We approach the prob-
lem in the other direction, generating canonical utterances for
logical forms, and use paraphrase models to handle the mis-
match.
tically generate canonical utterances for each log-
ical form based on the text descriptions of predi-
cates from the KB. Finally, we choose the canoni-
cal utterance that best paraphrases the input utter-
ance, and thereby the logical form that generated
it. We use two complementary paraphrase mod-
els: an association model based on aligned phrase
pairs extracted from a monolingual parallel cor-
pus, and a vector space model, which represents
each utterance as a vector and learns a similarity
score between them. The entire system is trained
jointly from question-answer pairs only.
Our work relates to recent lines of research
in semantic parsing and question answering.
Kwiatkowski et al (2013) first maps utterances to
a domain-independent intermediate logical form,
and then performs ontology matching to produce
the final logical form. In some sense, we ap-
proach the problem from the opposite end, using
an intermediate utterance, which allows us to em-
ploy paraphrasing methods (Figure 2). Fader et
al. (2013) presented a QA system that maps ques-
tions onto simple queries against Open IE extrac-
tions, by learning paraphrases from a large mono-
lingual parallel corpus, and performing a single
paraphrasing step. We adopt the idea of using
paraphrasing for QA, but suggest a more general
paraphrase model and work against a formal KB
(Freebase).
We apply our semantic parser on two datasets:
WEBQUESTIONS (Berant et al, 2013), which
contains 5,810 question-answer pairs with
common questions asked by web users; and
FREE917 (Cai and Yates, 2013), which has
917 questions manually authored by annota-
tors. On WEBQUESTIONS, we obtain a relative
improvement of 12% in accuracy over the
state-of-the-art, and on FREE917 we match the
current best performing system. The source
code of our system PARASEMPRE is released
at http://www-nlp.stanford.edu/
software/sempre/.
2 Setup
Our task is as follows: Given (i) a knowledge
base K, and (ii) a training set of question-answer
pairs {(x
i
, y
i
)}
n
i=1
, output a semantic parser that
maps new questions x to answers y via latent log-
ical forms z. Let E denote a set of entities (e.g.,
BillGates), and let P denote a set of properties
(e.g., PlaceOfBirth). A knowledge base K is a
set of assertions (e
1
, p, e
2
) ? E ? P ? E (e.g.,
(BillGates, PlaceOfBirth, Seattle)). We use
the Freebase KB (Google, 2013), which has 41M
entities, 19K properties, and 596M assertions.
To query the KB, we use a logical language
called simple ?-DCS. In simple ?-DCS, an
entity (e.g., Seattle) is a unary predicate
(i.e., a subset of E) denoting a singleton set
containing that entity. A property (which is a
binary predicate) can be joined with a unary
predicate; e.g., Founded.Microsoft denotes
the entities that are Microsoft founders. In
PlaceOfBirth.Seattle u Founded.Microsoft,
an intersection operator allows us to denote
the set of Seattle-born Microsoft founders.
A reverse operator reverses the order of ar-
guments: R[PlaceOfBirth].BillGates
denotes Bill Gates?s birthplace (in con-
trast to PlaceOfBirth.Seattle). Lastly,
count(Founded.Microsoft) denotes set cardinal-
ity, in this case, the number of Microsoft founders.
The denotation of a logical form z with respect to
a KB K is given by JzK
K
. For a formal description
of simple ?-DCS, see Liang (2013) and Berant et
al. (2013).
3 Model overview
We now present the general framework for seman-
tic parsing via paraphrasing, including the model
and the learning algorithm. In Sections 4 and 5,
we provide the details of our implementation.
Canonical utterance construction Given an ut-
terance x and the KB, we construct a set of candi-
1416
date logical forms Z
x
, and then for each z ? Z
x
generate a small set of canonical natural language
utterances C
z
. Our goal at this point is only to gen-
erate a manageable set of logical forms containing
the correct one, and then generate an appropriate
canonical utterance from it. This strategy is feasi-
ble in factoid QA where compositionality is low,
and so the size of Z
x
is limited (Section 4).
Paraphrasing We score the canonical utter-
ances in C
z
with respect to the input utterance x
using a paraphrase model, which offers two ad-
vantages. First, the paraphrase model is decoupled
from the KB, so we can train it from large text cor-
pora. Second, natural language utterances often do
not express predicates explicitly, e.g., the question
?What is Italy?s money?? expresses the binary
predicate CurrencyOf with a possessive construc-
tion. Paraphrasing methods are well-suited for
handling such text-to-text gaps. Our framework
accommodates any paraphrasing method, and in
this paper we propose an association model that
learns to associate natural language phrases that
co-occur frequently in a monolingual parallel cor-
pus, combined with a vector space model, which
learns to score the similarity between vector rep-
resentations of natural language utterances (Sec-
tion 5).
Model We define a discriminative log-linear
model that places a probability distribution over
pairs of logical forms and canonical utterances
(c, z), given an utterance x:
p
?
(c, z | x) =
exp{?(x, c, z)
>
?}
?
z
?
?Z
x
,c
?
?C
z
exp{?(x, c
?
, z
?
)
>
?}
,
where ? ? R
b
is the vector of parameters to be
learned, and ?(x, c, z) is a feature vector extracted
from the input utterance x, the canonical utterance
c, and the logical form z. Note that the candidate
set of logical forms Z
x
and canonical utterances
C
x
are constructed during the canonical utterance
construction phase.
The model score decomposes into two terms:
?(x, c, z)
>
? = ?
pr
(x, c)
>
?
pr
+ ?
lf
(x, z)
>
?
lf
,
where the parameters ?
pr
define the paraphrase
model (Section 5), which is based on features ex-
tracted from text only (the input and canonical ut-
terance). The parameters ?
lf
correspond to seman-
tic parsing features based on the logical form and
input utterance, and are briefly described in this
section.
Many existing paraphrase models introduce la-
tent variables to describe the derivation of c from
x, e.g., with transformations (Heilman and Smith,
2010; Stern and Dagan, 2011) or alignments
(Haghighi et al, 2005; Das and Smith, 2009;
Chang et al, 2010). However, we opt for a sim-
pler paraphrase model without latent variables in
the interest of efficiency.
Logical form features The parameters ?
lf
corre-
spond to the following features adopted from Be-
rant et al (2013). For a logical form z, we extract
the size of its denotation JzK
K
. We also add all bi-
nary predicates in z as features. Moreover, we ex-
tract a popularity feature for predicates based on
the number of instances they have in K. For Free-
base entities, we extract a popularity feature based
on the entity frequency in an entity linked subset
of Reverb (Lin et al, 2012). Lastly, Freebase for-
mulas have types (see Section 4), and we conjoin
the type of z with the first word of x, to capture the
correlation between a word (e.g., ?where?) with
the Freebase type (e.g., Location).
Learning As our training data consists of
question-answer pairs (x
i
, y
i
), we maximize the
log-likelihood of the correct answer. The proba-
bility of an answer y is obtained by marginaliz-
ing over canonical utterances c and logical forms
z whose denotation is y. Formally, our objective
function O(?) is as follows:
O(?) =
n
?
i=1
log p
?
(y
i
| x
i
)? ????
1
,
p
?
(y | x) =
?
z?Z
x
:y=JzK
K
?
c?C
z
p
?
(c, z | x).
The strength ? of the L
1
regularizer is set based
on cross-validation. We optimize the objective by
initializing the parameters ? to zero and running
AdaGrad (Duchi et al, 2010). We approximate
the set of pairs of logical forms and canonical ut-
terances with a beam of size 2,000.
4 Canonical utterance construction
We construct canonical utterances in two steps.
Given an input utterance x, we first construct a
set of logical forms Z
x
, and then generate canon-
ical utterances from each z ? Z
x
. Both steps are
performed with a small and simple set of deter-
ministic rules, which suffices for our datasets, as
1417
they consist of factoid questions with a modest
amount of compositional structure. We describe
these rules below for completeness. Due to its so-
porific effect though, we advise the reader to skim
it quickly.
Candidate logical forms We consider logical
forms defined by a set of templates, summarized
in Table 1. The basic template is a join of a bi-
nary and an entity, where a binary can either be
one property p.e (#1 in the table) or two proper-
ties p
1
.p
2
.e (#2). To handle cases of events in-
volving multiple arguments (e.g., ?Who did Brad
Pitt play in Troy??), we introduce the template
p.(p
1
.e
1
u p
2
.e
2
) (#3), where the main event is
modified by more than one entity. Logical forms
can be further modified by a unary ?filter?, e.g.,
the answer to ?What composers spoke French??
is a set of composers, i.e., a subset of all people
(#4). Lastly, we handle aggregation formulas for
utterances such as ?How many teams are in the
NCAA?? (#5).
To construct candidate logical forms Z
x
for a
given utterance x, our strategy is to find an en-
tity in x and grow the logical form from that en-
tity. As we show later, this procedure actually pro-
duces a set with better coverage than construct-
ing logical forms recursively from spans of x, as
is done in traditional semantic parsing. Specifi-
cally, for every span of x, we take at most 10 en-
tities whose Freebase descriptions approximately
match the span. Then, we join each entity e with
all type-compatible
1
binaries b, and add these log-
ical forms to Z
x
(#1 and #2).
To construct logical forms with multiple en-
tities (#3) we do the following: For any logical
form z = p.p
1
.e
1
, where p
1
has type signa-
ture (t
1
, ?), we look for other entities e
2
that
were matched in x. Then, we add the logical
form p.(p
1
.e
1
u p
2
.e
2
), if there exists a binary
p
2
with a compatible type signature (t
1
, t
2
),
where t
2
is one of e
2
?s types. For example, for
the logical form Character.Actor.BradPitt,
if we match the entity Troy in x, we obtain
Character.(Actor.BradPitt u Film.Troy).
We further modify logical forms by intersecting
with a unary filter (#4): given a formula z with
some Freebase type (e.g., People), we look at
all Freebase sub-types t (e.g., Composer), and
1
Entities in Freebase are associated with a set of types,
and properties have a type signature (t
1
, t
2
) We use these
types to compute an expected type t for any logical form z.
check whether one of their Freebase descriptions
(e.g., ?composer?) appears in x. If so, we
add the formula Type.t u z to Z
x
. Finally, we
check whether x is an aggregation formula by
identifying whether it starts with phrases such as
?how many? or ?number of? (#5).
On WEBQUESTIONS, this results in 645 for-
mulas per utterance on average. Clearly, we can
increase the expressivity of this step by expand-
ing the template set. For example, we could han-
dle superlative utterances (?What NBA player is
tallest??) by adding a template with an argmax
operator.
Utterance generation While mapping general
language utterances to logical forms is hard, we
observe that it is much easier to generate a canoni-
cal natural language utterances of our choice given
a logical form. Table 2 summarizes the rules used
to generate canonical utterances from the template
p.e. Questions begin with a question word, are fol-
lowed by the Freebase description of the expected
answer type (d(t)), and followed by Freebase de-
scriptions of the entity (d(e)) and binary (d(p)).
To fill in auxiliary verbs, determiners, and prepo-
sitions, we parse the description d(p) into one of
NP, VP, PP, or NP VP. This determines the gen-
eration rule to be used.
Each Freebase property p has an explicit prop-
erty p
?
equivalent to the reverse R[p] (e.g.,
ContainedBy and R[Contains]). For each logical
form z, we also generate using equivalent logical
forms where p is replaced with R[p
?
]. Reversed
formulas have different generation rules, since en-
tities in these formulas are in the subject position
rather than object position.
We generate the description d(t) from the Free-
base description of the type of z (this handles #4).
For the template p
1
.p
2
.e (#2), we have a similar
set of rules, which depends on the syntax of d(p
1
)
and d(p
2
) and is omitted for brevity. The tem-
plate p.(p
1
.e
1
u p
2
.e
2
) (#3) is generated by ap-
pending the prepositional phrase in d(e
2
), e.g,
?What character is the character of Brad Pitt in
Troy??. Lastly, we choose the question phrase
?How many? for aggregation formulas (#5), and
?What? for all other formulas.
We also generate canonical utterances using
an alignment lexicon, released by Berant et al
(2013), which maps text phrases to Freebase bi-
nary predicates. For a binary predicate b mapped
from text phrase d(b), we generate the utterance
1418
# Template Example Question
1 p.e Directed.TopGun Who directed Top Gun?
2 p
1
.p
2
.e Employment.EmployerOf.SteveBalmer Where does Steve Balmer work?
3 p.(p
1
.e
1
u p
2
.e
2
) Character.(Actor.BradPitt u Film.Troy) Who did Brad Pitt play in Troy?
4 Type.t u z Type.Composer u SpeakerOf.French What composers spoke French?
5 count(z) count(BoatDesigner.NatHerreshoff) How many ships were designed by
Nat Herreshoff?
Table 1: Logical form templates, where p, p
1
, p
2
are Freebase properties, e, e
1
, e
2
are Freebase entities, t is a Freebase type,
and z is a logical form.
d(p) Categ. Rule Example
p.e NP WH d(t) has d(e) as NP ? What election contest has George Bush as winner?
VP WH d(t) (AUX) VP d(e) ? What radio station serves area New-York?
PP WH d(t) PP d(e) ? What beer from region Argentina?
NP VP WH d(t) VP the NP d(e) ? What mass transportation system served the area Berlin?
R(p).e NP WH d(t) is the NP of d(e) ? What location is the place of birth of Elvis Presley?
VP WH d(t) AUX d(e) VP ? What film is Brazil featured in?
PP WH d(t) d(e) PP ? What destination Spanish steps near travel destination?
NP VP WH NP is VP by d(e) ? What structure is designed by Herod?
Table 2: Generation rules for templates of the form p.e and R[p].e based on the syntactic category of the property description.
Freebase descriptions for the type, entity, and property are denoted by d(t), d(e) and d(p) respectively. The surface form of the
auxiliary AUX is determined by the POS tag of the verb inside the VP tree.
WH d(t) d(b) d(e) ?. On the WEBQUESTIONS
dataset, we generate an average of 1,423 canonical
utterances c per input utterance x. In Section 6,
we show that an even simpler method of gener-
ating canonical utterances by concatenating Free-
base descriptions hurts accuracy by only a modest
amount.
5 Paraphrasing
Once the candidate set of logical forms paired with
canonical utterances is constructed, our problem
is reduced to scoring pairs (c, z) based on a para-
phrase model. The NLP paraphrase literature is
vast and ranges from simple methods employing
surface features (Wan et al, 2006), through vec-
tor space models (Socher et al, 2011), to latent
variable models (Das and Smith, 2009; Wang and
Manning, 2010; Stern and Dagan, 2011).
In this paper, we focus on two paraphrase mod-
els that emphasize simplicity and efficiency. This
is important since for each question-answer pair,
we consider thousands of canonical utterances as
potential paraphrases. In contrast, traditional para-
phrase detection (Dolan et al, 2004) and Recog-
nizing Textual Entailment (RTE) tasks (Dagan et
al., 2013) consider examples consisting of only a
single pair of candidate paraphrases.
Our paraphrase model decomposes into an as-
sociation model and a vector space model:
?
pr
(x, c)
>
?
pr
= ?
as
(x, c)
>
?
as
+ ?
vs
(x, c)
>
?
vs
.
x : What type of music did Richard Wagner play
c : What is the musical genres of Richard Wagner
Figure 3: Token associations extracted for a paraphrase
pair. Blue and dashed (red and solid) indicate positive (neg-
ative) score. Line width is proportional to the absolute value
of the score.
5.1 Association model
The goal of the association model is to deter-
mine whether x and c contain phrases that are
likely to be paraphrases. Given an utterance x =
?x
0
, x
1
, .., x
n?1
?, we denote by x
i:j
the span from
token i to token j. For each pair of utterances
(x, c), we go through all spans of x and c and
identify a set of pairs of potential paraphrases
(x
i:j
, c
i
?
:j
?
), which we call associations. (We will
describe how associations are identified shortly.)
We then define features on each association; the
weighted combination of these features yields a
score. In this light, associations can be viewed
as soft paraphrase rules. Figure 3 presents exam-
ples of associations extracted from a paraphrase
pair and visualizes the learned scores. We can see
that our model learns a positive score for associ-
ating ?type? with ?genres?, and a negative score
for associating ?is? with ?play?.
We define associations in x and c primarily by
looking up phrase pairs in a phrase table con-
structed using the PARALEX corpus (Fader et al,
2013). PARALEX is a large monolingual parallel
1419
Category Description
Assoc. lemma(x
i:j
) ? lemma(c
i
?
:j
?
)
pos(x
i:j
) ? pos(c
i
?
:j
?
)
lemma(x
i:j
) = lemma(c
i
?
:j
?
)?
pos(x
i:j
) = pos(c
i
?
:j
?
)?
lemma(x
i:j
) and lemma(c
i
?
:j
?
) are synonyms?
lemma(x
i:j
) and lemma(c
i
?
:j
?
) are derivations?
Deletions Deleted lemma and POS tag
Table 3: Full feature set in the association model. x
i:j
and
c
i
?
:j
?
denote spans from x and c. pos(x
i:j
) and lemma(x
i:j
)
denote the POS tag and lemma sequence of x
i:j
.
corpora, containing 18 million pairs of question
paraphrases from wikianswers.com, which
were tagged as having the same meaning by users.
PARALEX is suitable for our needs since it fo-
cuses on question paraphrases. For example, the
phrase ?do for a living? occurs mostly in ques-
tions, and we can extract associations for this
phrase from PARALEX. Paraphrase pairs in PAR-
ALEX are word-aligned using standard machine
translation methods. We use the word alignments
to construct a phrase table by applying the con-
sistent phrase pair heuristic (Och and Ney, 2004)
to all 5-grams. This results in a phrase table with
approximately 1.3 million phrase pairs. We let A
denote this set of mined candidate associations.
For a pair (x, c), we also consider as candidate
associations the set B (represented implicitly),
which contains token pairs (x
i
, c
i
?
) such that x
i
and c
i
?
share the same lemma, the same POS tag,
or are linked through a derivation link on WordNet
(Fellbaum, 1998). This allows us to learn para-
phrases for words that appear in our datasets but
are not covered by the phrase table, and to han-
dle nominalizations for phrase pairs such as ?Who
designed the game of life?? and ?What game de-
signer is the designer of the game of life??.
Our model goes over all possible spans of x
and c and constructs all possible associations from
A and B. This results in many poor associations
(e.g., ?play? and ?the?), but as illustrated in Fig-
ure 3, we learn weights that discriminate good
from bad associations. Table 3 specifies the full
set of features. Note that unlike standard para-
phrase detection and RTE systems, we use lexi-
calized features, firing approximately 400,000 fea-
tures on WEBQUESTIONS. By extracting POS
features, we obtain soft syntactic rules, e.g., the
feature ?JJ N ? N? indicates that omitting ad-
jectives before nouns is possible. Once associa-
tions are constructed, we mark tokens in x and c
that were not part of any association, and extract
deletion features for their lemmas and POS tags.
Thus, we learn that deleting pronouns is accept-
able, while deleting nouns is not.
To summarize, the association model links
phrases of two utterances in multiple overlapping
ways. During training, the model learns which
associations are characteristic of paraphrases and
which are not.
5.2 Vector space model
The association model relies on having a good set
of candidate associations, but mining associations
suffers from coverage issues. We now introduce
a vector space (VS) model, which assigns a vec-
tor representation for each utterance, and learns a
scoring function that ranks paraphrase candidates.
We start by constructing vector representations
of words. We run the WORD2VEC tool (Mikolov et
al., 2013) on lower-cased Wikipedia text (1.59 bil-
lion tokens), using the CBOW model with a win-
dow of 5 and hierarchical softmax. We also ex-
periment with publicly released word embeddings
(Huang et al, 2012), which were trained using
both local and global context. Both result in k-
dimensional vectors (k = 50). Next, we construct
a vector v
x
? R
k
for each utterance x by simply
averaging the vectors of all content words (nouns,
verbs, and adjectives) in x.
We can now estimate a paraphrase score for two
utterances x and c via a weighted combination of
the components of the vector representations:
v
>
x
Wv
c
=
k
?
i,j=1
w
ij
v
x,i
v
c,j
where W ? R
k?k
is a parameter matrix. In terms
of our earlier notation, we have ?
vs
= vec(W ) and
?
vs
(x, c) = vec(v
x
v
>
c
), where vec(?) unrolls a ma-
trix into a vector. In Section 6, we experiment with
W equal to the identity matrix, constraining W to
be diagonal, and learning a full W matrix.
The VS model can identify correct paraphrases
in cases where it is hard to directly associate
phrases from x and c. For example, the answer
to ?Where is made Kia car?? (from WEBQUES-
TIONS), is given by the canonical utterance ?What
city is Kia motors a headquarters of??. The as-
sociation model does not associate ?made? and
?headquarters?, but the VS model is able to de-
termine that these utterances are semantically re-
lated. In other cases, the VS model cannot distin-
guish correct paraphrases from incorrect ones. For
1420
Dataset # examples # word types
FREE917 917 2,036
WEBQUESTIONS 5,810 4,525
Table 4: Statistics on WEBQUESTIONS and FREE917.
example, the association model identifies that the
paraphrase for ?What type of music did Richard
Wagner Play?? is ?What is the musical genres
of Richard Wagner??, by relating phrases such as
?type of music? and ?musical genres?. The VS
model ranks the canonical utterance ?What com-
position has Richard Wagner as lyricist?? higher,
as this utterance is also in the music domain. Thus,
we combine the two models to benefit from their
complementary nature.
In summary, while the association model aligns
particular phrases to one another, the vector space
model provides a soft vector-based representation
for utterances.
6 Empirical evaluation
In this section, we evaluate our system on WE-
BQUESTIONS and FREE917. After describing the
setup (Section 6.1), we present our main empirical
results and analyze the components of the system
(Section 6.2).
6.1 Setup
We use the WEBQUESTIONS dataset (Berant et
al., 2013), which contains 5,810 question-answer
pairs. This dataset was created by crawling
questions through the Google Suggest API, and
then obtaining answers using Amazon Mechani-
cal Turk. We use the original train-test split, and
divide the training set into 3 random 80%?20%
splits for development. This dataset is character-
ized by questions that are commonly asked on the
web (and are not necessarily grammatical), such
as ?What character did Natalie Portman play in
Star Wars?? and ?What kind of money to take to
Bahamas??.
The FREE917 dataset contains 917 questions,
authored by two annotators and annotated with
logical forms. This dataset contains questions on
rarer topics (for example, ?What is the engine
in a 2010 Ferrari California?? and ?What was
the cover price of the X-men Issue 1??), but the
phrasing of questions tends to be more rigid com-
pared to WEBQUESTIONS. Table 4 provides some
statistics on the two datasets. Following Cai and
Yates (2013), we hold out 30% of the data for the
final test, and perform 3 random 80%-20% splits
of the training set for development. Since we train
from question-answer pairs, we collect answers by
executing the gold logical forms against Freebase.
We execute ?-DCS queries by converting them
into SPARQL and executing them against a copy
of Freebase using the Virtuoso database engine.
We evaluate our system with accuracy, that is, the
proportion of questions we answer correctly. We
run all questions through the Stanford CoreNLP
pipeline (Toutanova and Manning, 2003; Finkel et
al., 2005; Klein and Manning, 2003).
We tuned the L
1
regularization strength, devel-
oped features, and ran analysis experiments on the
development set (averaging across random splits).
On WEBQUESTIONS, without L
1
regularization,
the number of non-zero features was 360K; L
1
regularization brings it down to 17K.
6.2 Results
We compare our system to Cai and Yates (2013)
(CY13), Berant et al (2013) (BCFL13), and
Kwiatkowski et al (2013) (KCAZ13). For
BCFL13, we obtained results using the SEMPRE
package
2
and running Berant et al (2013)?s sys-
tem on the datasets.
Table 5 presents results on the test set. We
achieve a substantial relative improvement of 12%
in accuracy on WEBQUESTIONS, and match the
best results on FREE917. Interestingly, our system
gets an oracle accuracy of 63% on WEBQUES-
TIONS compared to 48% obtained by BCFL13,
where the oracle accuracy is the fraction of ques-
tions for which at least one logical form in the
candidate set produced by the system is correct.
This demonstrates that our method for construct-
ing candidate logical forms is reasonable. To fur-
ther examine this, we ran BCFL13 on the devel-
opment set, allowing it to use only predicates from
logical forms suggested by our logical form con-
struction step. This improved oracle accuracy on
the development set to 64.5%, but accuracy was
32.2%. This shows that the improvement in accu-
racy should not be attributed only to better logical
form generation, but also to the paraphrase model.
We now perform more extensive analysis of our
system?s components and compare it to various
baselines.
Component ablation We ablate the association
model, the VS model, and the entire paraphrase
2
http://www-nlp.stanford.edu/software/sempre/
1421
FREE917 WEBQUESTIONS
CY13 59.0 ?
BCFL13 62.0 35.7
KCAZ13 68.0 ?
This work 68.5 39.9
Table 5: Results on the test set.
FREE917 WEBQUESTIONS
Our system 73.9 41.2
?VSM 71.0 40.5
?ASSOCIATION 52.7 35.3
?PARAPHRASE 31.8 21.3
SIMPLEGEN 73.4 40.4
Full matrix 52.7 35.3
Diagonal 50.4 30.6
Identity 50.7 30.4
JACCARD 69.7 31.3
EDIT 40.8 24.8
WDDC06 71.0 29.8
Table 6: Results for ablations and baselines on develop-
ment set.
model (using only logical form features). Table 5
shows that our full system obtains highest accu-
racy, and that removing the association model re-
sults in a much larger degradation compared to re-
moving the VS model.
Utterance generation Our system generates
relatively natural utterances from logical forms us-
ing simple rules based on Freebase descriptions
(Section 4). We now consider simply concate-
nating Freebase descriptions. For example, the
logical form R[PlaceOfBirth].ElvisPresley
would generate the utterance ?What location Elvis
Presley place of birth??. Row SIMPLEGEN in Ta-
ble 6 demonstrates that we still get good results in
this setup. This is expected given that our para-
phrase models are not sensitive to the syntactic
structure of the generated utterance.
VS model Our system learns parameters for a
full W matrix. We now examine results when
learning parameters for a full matrix W , a diago-
nal matrix W , and when setting W to be the iden-
tity matrix. Table 6 (third section) illustrates that
learning a full matrix substantially improves accu-
racy. Figure 4 gives an example for a correct para-
phrase pair, where the full matrix model boosts
the overall model score. Note that the full ma-
trix assigns a high score for the phrases ?official
language? and ?speak? compared to the simpler
models, but other pairs are less interpretable.
Baselines We also compared our system to the
following implemented baselines:
Full do people czech republic speakoffical 0.7 8.09 15.34 21.62 24.44
language 3.86 -3.13 7.81 2.58 14.74
czech 0.67 16.55 2.76
republic -8.71 12.47 -10.75
Diagonal do people czech republic speakoffical 2.31 -0.72 1.88 0.27 -0.49
language 0.27 4.72 11.51 12.33 11
czech 1.4 8.13 5.21
republic -0.16 6.72 9.69
Identity do people czech republic speakoffical 2.26 -1.41 0.89 0.07 -0.58
language 0.62 4.19 11.91 10.78 12.7
czech 2.88 7.31 5.42
republic -1.82 4.34 9.44
Figure 4: Values of the paraphrase score v
>
x
i
Wv
c
i
?
for all
content word tokens x
i
and c
i
?
, where W is an arbitrary full
matrix, a diagonal matrix, or the identity matrix. We omit
scores for the words ?czech? and ?republic? since they ap-
pear in all canonical utterances for this example.
? JACCARD: We compute the Jaccard score
between the tokens of x and c and define
?
pr
(x, c) to be this single feature.
? EDIT: We compute the token edit distance
between x and c and define ?
pr
(x, c) to be
this single feature.
? WDDC06: We re-implement 13 features
from Wan et al (2006), who obtained close to
state-of-the-art performance on the Microsoft
Research paraphrase corpus.
3
Table 6 demonstrates that we improve perfor-
mance over all baselines. Interestingly, JACCARD
and WDDC06 obtain reasonable performance
on FREE917 but perform much worse on WE-
BQUESTIONS. We surmise this is because ques-
tions in FREE917 were generated by annotators
prompted by Freebase facts, whereas questions
in WEBQUESTIONS originated independently of
Freebase. Thus, word choice in FREE917 is of-
ten close to the generated Freebase descriptions,
allowing simple baselines to perform well.
Error analysis We sampled examples from the
development set to examine the main reasons
PARASEMPRE makes errors. We notice that in
many cases the paraphrase model can be further
improved. For example, PARASEMPRE suggests
3
We implement all features that do not require depen-
dency parsing.
1422
that the best paraphrase for ?What company did
Henry Ford work for?? is ?What written work
novel by Henry Ford?? rather than ?The em-
ployer of Henry Ford?, due to the exact match
of the word ?work?. Another example is the
question ?Where is the Nascar hall of fame??,
where PARASEMPRE suggests that ?What hall of
fame discipline has Nascar hall of fame as halls
of fame?? is the best canonical utterance. This
is because our simple model allows to associate
?hall of fame? with the canonical utterance three
times. Entity recognition also accounts for many
errors, e.g., the entity chosen in ?where was the
gallipoli campaign waged?? is Galipoli and not
GalipoliCampaign. Last, PARASEMPRE does not
handle temporal information, which causes errors
in questions like ?Where did Harriet Tubman live
after the civil war??
7 Discussion
In this work, we approach the problem of seman-
tic parsing from a paraphrasing viewpoint. A
fundamental motivation and long standing goal
of the paraphrasing and RTE communities has
been to cast various semantic applications as para-
phrasing/textual entailment (Dagan et al, 2013).
While it has been shown that paraphrasing meth-
ods are useful for question answering (Harabagiu
and Hickl, 2006) and relation extraction (Romano
et al, 2006), this is, to the best of our knowledge,
the first paper to perform semantic parsing through
paraphrasing. Our paraphrase model emphasizes
simplicity and efficiency, but the framework is ag-
nostic to the internals of the paraphrase method.
On the semantic parsing side, our work is most
related to Kwiatkowski et al (2013). The main
challenge in semantic parsing is coping with the
mismatch between language and the KB. In both
Kwiatkowski et al (2013) and this work, an inter-
mediate representation is employed to handle the
mismatch, but while they use a logical represen-
tation, we opt for a text-based one. Our choice
allows us to benefit from the parallel monolingual
corpus PARALEX and from word vectors trained
on Wikipedia. We believe that our approach is
particularly suitable for scenarios such as factoid
question answering, where the space of logical
forms is somewhat constrained and a few gener-
ation rules suffice to reduce the problem to para-
phrasing.
Our work is also related to Fader et al (2013),
who presented a paraphrase-driven question an-
swering system. One can view this work as a
generalization of Fader et al along three dimen-
sions. First, Fader et al use a KB over natu-
ral language extractions rather than a formal KB
and so querying the KB does not require a gener-
ation step ? they paraphrase questions to KB en-
tries directly. Second, they suggest a particular
paraphrasing method that maps a test question to a
question for which the answer is already known in
a single step. We propose a general paraphrasing
framework and instantiate it with two paraphrase
models. Lastly, Fader et al handle queries with
only one property and entity whereas we general-
ize to more types of logical forms.
Since our generated questions are passed to
a paraphrase model, we took a very simple ap-
proach, mostly ensuring that we preserved the se-
mantics of the utterance without striving for the
most fluent realization. Research on generation
(Dale et al, 2003; Reiter et al, 2005; Turner et
al., 2009; Piwek and Boyer, 2012) typically fo-
cuses on generating natural utterances for human
consumption, where fluency is important.
In conclusion, the main contribution of this pa-
per is a novel approach for semantic parsing based
on a simple generation procedure and a paraphrase
model. We achieve state-of-the-art results on two
recently released datasets. We believe that our ap-
proach opens a window of opportunity for learn-
ing semantic parsers from raw text not necessarily
related to the target KB. With more sophisticated
generation and paraphrase, we hope to tackle com-
positionally richer utterances.
Acknowledgments
We thank Kai Sheng Tai for performing the er-
ror analysis. Stanford University gratefully ac-
knowledges the support of the Defense Advanced
Research Projects Agency (DARPA) Deep Ex-
ploration and Filtering of Text (DEFT) Program
under Air Force Research Laboratory (AFRL)
contract no. FA8750-13-2-0040. Any opinions,
findings, and conclusion or recommendations ex-
pressed in this material are those of the authors and
do not necessarily reflect the view of the DARPA,
AFRL, or the US government. The second author
is supported by a Google Faculty Research Award.
1423
References
J. Berant, A. Chou, R. Frostig, and P. Liang. 2013.
Semantic parsing on Freebase from question-answer
pairs. In Empirical Methods in Natural Language
Processing (EMNLP).
Q. Cai and A. Yates. 2013. Large-scale semantic pars-
ing via schema matching and lexicon extension. In
Association for Computational Linguistics (ACL).
M. Chang, D. Goldwasser, D. Roth, and V. Srikumar.
2010. Discriminative learning over constrained la-
tent representations. In North American Association
for Computational Linguistics (NAACL).
I. Dagan, D. Roth, M. Sammons, and F. M. Zanzotto.
2013. Recognizing Textual Entailment: Models and
Applications. Morgan and Claypool Publishers.
R. Dale, S. Geldof, and J. Prost. 2003. Coral: using
natural language generation for navigational assis-
tance. In Australasian computer science conference,
pages 35?44.
D. Das and N. A. Smith. 2009. Paraphrase identifica-
tion as probabilistic quasi-synchronous recognition.
In Association for Computational Linguistics (ACL),
pages 468?476.
B. Dolan, C. Quirk, and C. Brockett. 2004. Unsuper-
vised construction of large paraphrase corpora: Ex-
ploiting massively parallel news sources. In Inter-
national Conference on Computational Linguistics
(COLING).
J. Duchi, E. Hazan, and Y. Singer. 2010. Adaptive sub-
gradient methods for online learning and stochastic
optimization. In Conference on Learning Theory
(COLT).
A. Fader, S. Soderland, and O. Etzioni. 2011. Identi-
fying relations for open information extraction. In
Empirical Methods in Natural Language Processing
(EMNLP).
A. Fader, L. Zettlemoyer, and O. Etzioni. 2013.
Paraphrase-driven learning for open question an-
swering. In Association for Computational Linguis-
tics (ACL).
C. Fellbaum. 1998. WordNet: An Electronic Lexical
Database. MIT Press.
J. R. Finkel, T. Grenager, and C. Manning. 2005. In-
corporating non-local information into information
extraction systems by Gibbs sampling. In Associ-
ation for Computational Linguistics (ACL), pages
363?370.
Google. 2013. Freebase data dumps (2013-06-
09). https://developers.google.com/
freebase/data.
A. Haghighi, A. Y. Ng, and C. D. Manning. 2005.
Robust textual inference via graph matching. In
Empirical Methods in Natural Language Processing
(EMNLP).
S. Harabagiu and A. Hickl. 2006. Methods for using
textual entailment in open-domain question answer-
ing. In Association for Computational Linguistics
(ACL).
M. Heilman and N. A. Smith. 2010. Tree edit
models for recognizing textual entailments, para-
phrases, and answers to questions. In Human Lan-
guage Technology and North American Association
for Computational Linguistics (HLT/NAACL), pages
1011?1019.
E. H. Huang, R. Socher, C. D. Manning, and A. Y. Ng.
2012. Improving word representations via global
context and multiple word prototypes. In Associa-
tion for Computational Linguistics (ACL).
D. Klein and C. Manning. 2003. Accurate unlexical-
ized parsing. In Association for Computational Lin-
guistics (ACL), pages 423?430.
T. Kwiatkowski, L. Zettlemoyer, S. Goldwater, and
M. Steedman. 2010. Inducing probabilistic CCG
grammars from logical form with higher-order uni-
fication. In Empirical Methods in Natural Language
Processing (EMNLP), pages 1223?1233.
T. Kwiatkowski, E. Choi, Y. Artzi, and L. Zettlemoyer.
2013. Scaling semantic parsers with on-the-fly on-
tology matching. In Empirical Methods in Natural
Language Processing (EMNLP).
P. Liang. 2013. Lambda dependency-based composi-
tional semantics. Technical report, ArXiv.
T. Lin, Mausam, and O. Etzioni. 2012. Entity linking
at web scale. In Knowledge Extraction Workshop
(AKBC-WEKEX).
T. Mikolov, K. Chen, G. Corrado, and Jeffrey. 2013.
Efficient estimation of word representations in vec-
tor space. Technical report, ArXiv.
F. J. Och and H. Ney. 2004. The alignment template
approach to statistical machine translation. Compu-
tational Linguistics, 30:417?449.
P. Piwek and K. E. Boyer. 2012. Varieties of question
generation: Introduction to this special issue. Dia-
logue and Discourse, 3:1?9.
E. Reiter, S. Sripada, J. Hunter, J. Yu, and I. Davy.
2005. Choosing words in computer-generated
weather forecasts. Artificial Intelligence, 167:137?
169.
L. Romano, M. kouylekov, I. Szpektor, I. Dagan,
and A. Lavelli. 2006. Investigating a generic
paraphrase-based approach for relation extraction.
In Proceedings of ECAL.
R. Socher, E. H. Huang, J. Pennin, C. D. Manning, and
A. Ng. 2011. Dynamic pooling and unfolding re-
cursive autoencoders for paraphrase detection. In
Advances in Neural Information Processing Systems
(NIPS), pages 801?809.
1424
A. Stern and I. Dagan. 2011. A confidence model
for syntactically-motivated entailment proofs. In
Recent Advances in Natural Language Processing,
pages 455?462.
K. Toutanova and C. D. Manning. 2003. Feature-
rich part-of-speech tagging with a cyclic depen-
dency network. In Human Language Technology
and North American Association for Computational
Linguistics (HLT/NAACL).
R. Turner, Y. Sripada, and E. Reiter. 2009. Generating
approximate geographic descriptions. In European
Workshop on Natural Language Generation, pages
42?49.
S. Wan, M. Dras, R. Dale, and C. Paris. 2006. Using
dependency-based features to take the ?para-farce?
out of paraphrase. In Australasian Language Tech-
nology Workshop.
M. Wang and C. D. Manning. 2010. Probabilistic tree-
edit models with structured latent variables for tex-
tual entailment and question answering. In The In-
ternational Conference on Computational Linguis-
tics, pages 1164?1172.
Y. W. Wong and R. J. Mooney. 2007. Learning
synchronous grammars for semantic parsing with
lambda calculus. In Association for Computational
Linguistics (ACL), pages 960?967.
M. Zelle and R. J. Mooney. 1996. Learning to parse
database queries using inductive logic proramming.
In Association for the Advancement of Artificial In-
telligence (AAAI), pages 1050?1055.
L. S. Zettlemoyer and M. Collins. 2005. Learning to
map sentences to logical form: Structured classifica-
tion with probabilistic categorial grammars. In Un-
certainty in Artificial Intelligence (UAI), pages 658?
666.
1425
