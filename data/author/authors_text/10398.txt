Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 529?536
Manchester, August 2008
Linguistically-based sub-sentential alignment for terminology extraction
from a bilingual automotive corpus
Lieve Macken and Els Lefever and Veronique Hoste
Language and Translation Technology Team
Ghent University College
Belgium
{Lieve.Macken, Els.Lefever, Veronique.Hoste}@hogent.be
Abstract
We present a sub-sentential alignment
system that links linguistically motivated
phrases in parallel texts based on lexical
correspondences and syntactic similarity.
We compare the performance of our sub-
sentential alignment system with different
symmetrization heuristics that combine the
GIZA++ alignments of both translation di-
rections. We demonstrate that the aligned
linguistically motivated phrases are a use-
ful means to extract bilingual terminology
and more specifically complex multiword
terms.
1 Introduction
This research has been carried out in the frame-
work of a customer project for PSA Peugeot
Citro?en. The final goal of the project is a re-
duction and terminological unification process of
PSA?s database, which contains all text strings that
are used for compiling user manuals. French being
the source language, all French entries have been
translated to some extent into the twenty different
languages that are part of the customer?s portfolio.
Two sub-projects have been defined:
1. automatic terminology extraction for all lan-
guages taking French as the pivot language
2. improved consistency of the database entries,
e.g. through the automatic replacement of
synonyms by the preferred term (decided in
(1))
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
This paper presents a novel terminology extrac-
tion method applied to the French-English part of
the database.
There is a long tradition of research into
bilingual terminology extraction (Kupiec, 1993),
(Gaussier, 1998). In most systems, candidate terms
are first identified in the source language based on
predefined PoS patterns ? for French, N N, N Prep
N, and N Adj are typical patterns. In a second step,
the translation candidates are extracted from the
bilingual corpus based on word alignments. In re-
cent work, Itagaki et al (2007) use the phrase table
derived from the GIZA++ alignments to identify
the translations.
We use a different and more flexible approach.
We developed a sub-sentential alignment system
that links linguistically motivated phrases in paral-
lel texts based on lexical correspondences and syn-
tactic similarity. Rather than predefining terms as
sequences of PoS patterns, we first generate candi-
date terms starting from the aligned phrases. In a
second step, we use a general purpose corpus and
the n-gram frequency of the automotive corpus to
determine the specificity of the terms.
The remainder of this paper is organized as fol-
lows: Section 2 describes the corpus. In Section 3,
we present our linguistically-based sub-sentential
alignment system and in Section 4 we describe
how we use the aligned phrases for terminology
extraction.
2 Automotive corpus
For developing our terminology extraction mod-
ule, we have used the French-English sentence-
aligned database that contains 363,651 entries.
These entries can be full sentences, parts of sen-
tences, as well as isolated words and are aligned
across languages by means of a unique ID. The
529
PoS tagging Lemmatisation PoS after Lemmas
error rate error rate update after update
French 4.50 % 2.29 % 1.92 % 1.22 %
English 5.16 % 3.13 % 2.66 % 3.03 %
Table 1: Part-of-Speech tagging and lemmatisation
error rate on the test sentences
average sentence length of a database entry is 9
words.
2.1 Linguistic annotation
In order to ensure consistent processing of the lan-
guages in the corpus (e.g. Italian, Spanish, Ger-
man), we have used the freely availabe TreeTag-
ger tool (Schmid, 1994) for performing tokeni-
sation, part-of-speech tagging and lemmatisation
of the corpus. In order to evaluate the domain-
adaptability of the tagger, we have manually val-
idated the quality of the TreeTagger output for a
training set of 12,200 tokens (about 1,200 sen-
tences). We have used this validated set to derive
a list of hard coded PoS tags (e.g. the French word
vis can be a noun or verb, but is always a noun
in our corpus) as well as post-processing rules
for remediating erroneous PoS tags. We addition-
ally annotated 350 test sentences (about 3,500 to-
kens). Table 1 shows the error rate figures for PoS-
tagging and lemmatisation before and after updat-
ing the default output of the TreeTagger tool.
We further enriched the corpora with chunk in-
formation. During text chunking, syntactically re-
lated words are combined into non-overlapping
chunks based on PoS information. We devel-
oped rule-based chunkers for English and French.
The rule-based chunkers contain distituency rules,
i.e. the rules add a chunk boundary when two part-
of-speech codes cannot occur in the same con-
stituent. The following example shows a French-
English sentence pair divided in non-overlapping
chunks:
Fr: valable | uniquement | pour la ceinture | de
s?ecurit?e avant lat?erale | du c?ot?e passager
En: applies | only | to the outer seat belt | on the
passenger side
We manually indicated chunk boundaries in the
350-sentences test corpus and evaluated the rule-
based chunkers by running the CoNLL-evalscript
(Tjong Kim Sang and Buchholz, 2000). We ob-
tained precision scores of 89% and 87% and recall
scores of 91% and 91% for French and English re-
spectively.
# Words # Sentence pairs
Short (< 8 words) 4,496 404
Medium (8-19 words) 4,493 212
Long (> 19 words) 4,498 97
Total 13,487 713
Development corpus 4,423 231
Table 2: Number of words and sentence pairs in
the three test corpora and the development corpus
2.2 Test corpora
As we expect that sentence length has an impact
on the alignment performance, we created three
test corpora with varying sentence length. We dis-
tinguished short sentences (2-7 words), medium-
length sentences (8-19 words) and long sentences
(> 19 words). Each test corpus contains approxi-
mately 4,500 words.
We also compiled a development corpus con-
taining sentences of varying sentence length to de-
bug the system and to determine the value of the
thresholds used in the system. The formal charac-
teristics of the test corpora and the training corpus
are given in Table 2.
3 Sub-sentential alignment
Sub-sentential alignments ? and the underlying
word alignments ? are used in the context of
Machine Translation to create phrase tables for
phrase-based statistical machine translation sys-
tems (Koehn et al, 2007). A stand-alone sub-
sentential alignment module however, is also use-
ful for human translators if incorporated in CAT-
tools, e.g. sophisticated bilingual concordance sys-
tems, or in sub-sentential translation memory sys-
tems (Gotti et al, 2005). A quite obvious applica-
tion of a sub-sentential alignment system is the cre-
ation of bilingual dictionaries and terminology ex-
traction from bilingual corpora (Melamed, 2000),
(Itagaki et al, 2007).
In the context of statistical machine translation,
GIZA++ is one of the most widely used word
alignment toolkits. GIZA++ implements the IBM
models and is used in Moses (Koehn et al, 2007)
to generate the initial source-to-target and target-
to-source word alignments after which some sym-
metrization heuristics combine the alignments of
both translation directions.
We present an alternative ? linguistically-based
? approach, that starts from a lexical probabilistic
bilingual dictionary generated by IBM Model One.
530
3.1 Architecture
The basic idea behind our approach is that ? at least
for European languages ? translations conveying
the same meaning use to a certain extent the same
building blocks from which this meaning is com-
posed: i.e. we assume that to a large extent noun
and prepositional phrases, verb phrases and adver-
bial phrases in one language directly map to simi-
lar constituents in the other language
1
. The extent
to which our basic assumption holds depends on
the translation strategy that was used. Text types
that are typically translated in a more literal way
(e.g. user manuals) will contain more direct corre-
spondences.
We conceive our sub-sentential aligner as a cas-
cade model consisting of two phases. The objec-
tive of the first phase is to link anchor chunks,
i.e. chunks that can be linked with a very high pre-
cision. Those anchor chunks are linked based on
lexical clues and syntactic similarity. In the sec-
ond phase, we will try to model the more complex
translational correspondences based on observed
translation shift patterns. The anchor chunks of the
first phase will be used to limit the search space in
the second phase.
As the application at hand is terminology ex-
traction, we are interested in alignments with very
high precision. As the automotive corpus contains
rather literal translations, we expect that a high per-
centage of anchor chunks can be retrieved using
only the first phase of our approach.
The sub-sentential alignment system takes as
input sentence-aligned texts, together with addi-
tional linguistic annotations (part-of-speech codes
and lemmas) for the source and the target text.
In the first step of the process, the source and
target sentences are divided into chunks based on
PoS information, and lexical correspondences are
retrieved from a bilingual dictionary. During an-
chor chunk alignment, the sub-sentential aligner
links chunks based on lexical correspondences and
chunk similarity.
3.2 Bilingual Dictionary
We used the Perl implementation of IBM Model
One that is part of the Microsoft Bilingual Sen-
tence Aligner (Moore, 2002) to derive a bilingual
dictionary from a parallel corpus. IBM Model One
1
The more syntax-aware SMT systems assume that to a
certain extent syntactic relationships in one language directly
map to syntactic relationships in the other, which Hwa (2002)
calls the Direct Correspondence Assumption.
is a purely lexical model: it only takes into account
word frequencies of source and target sentences
2
.
The IBMmodels allow only 1:n word mappings,
and are therefore asymmetric. To overcome this
problem, we ran the model in two directions: from
French to English and from English to French. To
get high-accuracy links, only the words pairs oc-
curring in both the French-English and English-
French word lists were retained, and the probabil-
ities were averaged. To get rid of the noise pro-
duced by the translation model, only the entries
with an averaged value of at least 0.1 were re-
tained. This value was set experimentally
3
.
The resulting bilingual dictionary contains
28,990 English-French word pairs. The bilingual
dictionary is used to create the lexical link matrix
for each sentence pair.
3.3 Lexical Link Matrix
For each source and target word in each sentence
pair, all translations for the word form and the
lemma are retrieved from the bilingual dictionary.
In the process of building the lexical link ma-
trix, function words are neglected. Given the fre-
quency of function words in a sentence, linking
function words based on lexical information alone,
often results in erroneous alignments. For that
reason no lexical links are created for the follow-
ing word classes: determiners, prepositions, co-
ordinating conjunctions, possessive pronouns and
punctuation symbols.
For all content words, if a source word occurs in
the set of possible translations of a target word, or
if a target word occurs in the set of possible transla-
tions of the source words, a lexical link is created.
Identical strings in source and target language are
also linked.
3.4 Anchor chunks
Anchor chunk alignment comprises two steps. In
a first step, we select candidate anchor chunks; in
a second step we test the syntactic similarity of the
candidate anchor chunks.
3.4.1 Selecting candidate anchor chunks
The candidate anchor chunks are selected based on
the information available in the lexical link matrix.
2
The higher numbered IBM Models build on IBM Model
One and take into account word order (distortion) and model
the probability that a source word aligns to n target words
(fertility).
3
Lowering this threshold significantly decreased precision
scores of the sub-sentential alignment system.
531
For each source chunk a candidate target chunk is
constructed. The candidate target chunk is built by
concatenating all target chunks from a begin index
until an end index. The begin index points to the
first target chunk with a lexical link to the source
chunk under consideration. The end index points
to the last target chunk with a lexical link to the
source chunk under consideration. In this way, 1:1
and 1:n candidate target chunks are built.
The process of selecting candidate chunks as de-
scribed above, is performed a second time starting
from the target sentence. In this way additional n:1
candidates are constructed.
3.4.2 Testing chunk similarity
For each selected candidate pair, a similarity test
is performed. Chunks are considered to be similar
if at least a certain percentage of words of source
and target chunk(s) are either linked by means of
a lexical link or can be linked on the basis of cor-
responding part-of-speech codes. All word classes
can be linked based on PoS codes.
In addition to linking words based on PoS codes,
a small set of predefined language-dependent rules
were implemented to handle function words. For
example:
? Extra function words (determiners and prepo-
sitions) in source or target language are linked
together with their noun to the noun?s transla-
tion.
? In French, the preposition de is contracted
with the definitive articles le and les to du and
des respectively. The contracted determiners
are linked to an English preposition and de-
terminer.
The percentage of words that have to be linked was
empirically set at 85%.
3.5 Remaining chunks
In a second step, chunks consisting of one function
word ? mostly punctuation marks and conjunctions
? are linked based on corresponding part-of-speech
codes if its left or right neighbour on the diagonal
is an anchor chunk. Corresponding final punctua-
tion marks are also linked.
In a final step, additional candidates are con-
structed by selecting non-anchor chunks in the
source and target sentence that have correspond-
ing left and right anchor chunks as neigbours. The
anchor chunks of the first step are used as contex-
tual information to link n:m chunks or chunks for
which no lexical link was found in the lexical link
matrix.
In Figure 1, the chunks [Fr: gradient] ? [En: gra-
dient] and the final punctuation mark have been
retrieved in the first step as anchor chunk. In the
last step, the n:m chunk [Fr: de remont?ee p?edale
d? embrayage] ? [En: of rising of the clutch pedal]
is selected as candidate anchor chunk because it is
enclosed within anchor chunks.
Figure 1: n:m candidate chunk: ?A? stands for an-
chor chunks, ?L? for lexical links, ?P? for words
linked on the basis of corresponding PoS codes and
?R? for words linked by language-dependent rules.
As the contextual clues (the left and right neig-
bours of the additional candidate chunks are an-
chor chunks) provide some extra indication that the
chunks can be linked, the similarity test for the fi-
nal candidates was somewhat relaxed: the percent-
age of words that have to be linked was lowered
to 0.80 and a more relaxed PoS matching function
was used:
? Verbs and nouns can be linked
Fr: pour permettre de vidanger proprement le circuit
En: to permit clean draining of the system
? Adjectives and nouns can be linked
Fr: l? entr
?
ee d? air
En: incoming air
? Past participles can be linked to past tense
4
3.6 Evaluation
All translational correspondences were manually
indicated in the three test corpora (see section 2.2).
4
The English PoS tagger often tags a past participle erro-
neously as a past tense.
532
We adapted the annotation guidelines of Macken
(2007) to the French-English language pair, and
used three different types of links: regular links
for straightforward correspondences, fuzzy links
for translation-specific shifts of various kinds, and
null links for words for which no correspondence
could be indicated. Figure 2 shows an example.
Figure 2: Manual reference: regular links are indi-
cated by x?s, fuzzy links and null links by 0?s
To evaluate the system?s performance, we used the
evaluation methodology of Och and Ney (2003).
Och and Ney distinguished sure alignments (S)
and possible alignments (P) and introduced the fol-
lowing redefined precision and recall measures:
precision =
|A ? P |
|A|
, recall =
|A ? S|
|S|
(1)
and the alignment error rate (AER):
AER(S, P ;A) = 1 ?
|A ? P | + |A ? S|
|A| + |S|
(2)
We consider all regular links of the manual ref-
erence as sure alignments and all fuzzy and null
links as possible alignments to compare the output
of our system with the manual reference.
We trained statistical translation models using
Moses. Moses uses the GIZA++ toolkit (IBM
Model 1-4) in both translation directions (source
to target, target to source) and allows for different
symmetrization heuristics to combine the align-
ments of both translation directions. We used three
different heuristics: grow-diag-final (default), in-
tersection and union.
SHORT MEDIUM LONG
p r e p r e p r e
? .99 .83 .10 .98 .73 .16 .99 .77 .13
? .95 .92 .07 .91 .86 .11 .91 .89 .10
Gdf .95 .91 .07 .93 .85 .11 .94 .88 .09
Ling. .96 .93 .06 .94 .88 .09 .92 .87 .10
Table 3: Precision (p), recall (r) and align-
ment error rate (e) for three symmetrization
heuristics based on the GIZA++ alignments
(intersection(?), union (?), Grow-diag-final
(Gdf)) vs the linguistically-based system (Ling.)
for the three test corpora
Table 3 compares the alignment quality of our
linguistically-based system with the purely statisti-
cal approaches. Overall, the results confirm our as-
sumption that shorter sentences are easier to align
than longer sentences. As expected, the intersec-
tion heuristic aligns words with a very high preci-
sion (98-99%). We further observe that the align-
ment error rate of the linguistically-based system
is the lowest for the short and medium-length sen-
tences, but that on the long sentences the default
symmetrization heuristic yields the best results.
Manual inspection of the alignments revealed that
in some long sentences, the linguistically-based
system misaligns repeated terms in long sentences,
a phenomenon that occured frequently in the long
sentence corpus. As expected, the linguistically-
based system scores better on function words.
Overall, on this data set, the linguistically-based
system yields results that are comparable to the re-
sults obtained by the complex and computationally
expensive chain of IBM models.
4 Terminology extraction
As described in Section 1, we generate candidate
terms starting from the aligned anchor chunks. In
a second step, we use a general purpose corpus and
the n-gram frequency of the automotive corpus to
determine the specificity of the terms.
4.1 Generating candidate terms
English and French use a different compounding
strategy. In English, the most frequently used com-
pounding strategy is the concatenation of nouns,
while in French prepositional phrases are concate-
nated. The following example illustrates the dif-
ferent compounding strategy:
Fr: une proc?edure d? initialisation du calculateur
de bo??te de vitesses automatique
533
En: an automatic gearbox ECU initialisation pro-
cedure
We start from the anchor chunks as they are the
minimal chunks that could be linked together. We
implemented two heuristics to generate additional
French candidate terms: a first heuristic strips off
adjectives and a second heuristic considers each N
+ PP pattern as candidate term.
For each French candidate term, the English
translation is constructed on the basis of the word
alignments. The following candidate terms are
generated for our example:
1 proc?edure d? initialisation
du calculateur de bo??te de
vitesses automatique
automatic gearbox ECU
initialisation procedure
2 proc?edure d? initialisation
du calculateur de bo??te de
vitesses
gearbox ECU initialisa-
tion procedure
3 proc?edure d? initialisation initialisation procedure
4 initialisation du calcula-
teur
ECU initialisation
5 calculateur de bo??te de
vitesses
gearbox ECU
6 bo??te de vitesses automa-
tique
automatic gearbox
7 bo??te de vitesses gearbox
8 proc?edure procedure
9 initialisation initialisation
10 calculateur ECU
11 automatique automatic
4.2 Filtering of candidate terms
As our terminology extraction module is meant to
generate a bilingual automotive lexicon, every en-
try in our lexicon should refer to a concept or ac-
tion that is relevant in an automotive context. This
also means we want to include the minimal se-
mantical units (e.g. seat belt) as well as the larger
semantical units (e.g. outer front seat belt) of a
parent-child term relationship. In order to decide
on which terms should be kept in our lexicon, we
have combined two algorithms: Log-Likelihood
for single word entries and Mutual Expectation
Measure for multiword entries.
4.2.1 Log-Likelihood Measure
In order to detect single word terms that are dis-
tinctive enough to be kept in our bilingual lexi-
con, we have applied the Log-Likelihood measure
(LL). This metric considers frequencies of words
weighted over two different corpora (in our case
a technical automotive corpus and the more gen-
eral purpose corpus ?Le Monde?), in order to as-
sign high LL-values to words having much higher
or lower frequencies than expected. Daille (1995)
has determined empirically that LL is an accurate
measure for finding the most surprisingly frequent
words in a corpus. Low LL values on the other
hand allow to retrieve common vocabulary with
high frequencies in both corpora. We have cre-
ated a frequency list for both corpora and calcu-
lated the Log-Likelihood values for each word in
this frequency list. In the formula below, N cor-
responds to the number of words in the corpus,
whereas the observed values O correspond to the
real frequencies of a word in the corpus. The for-
mula for calculating both the expected values (E)
and the Log-Likelihood have been described in de-
tail by (Rayson and Garside, 2000).
E
i
=
N
i
?
i
O
i
?
i
N
i
(3)
We used the resulting Expected values for calcu-
lating the Log-Likelihood:
?2ln? = 2
?
i
O
i
ln(
O
i
E
i
) (4)
Manual inspection of the Log-Likelihood fig-
ures confirmed our hypothesis that more domain-
specific terms in our corpus got high LL-values.
As we are mainly interested in finding distinc-
tive terms in the automotive corpus, we have only
kept those terms showing positive Expected Val-
ues in our domain-specific corpus combined with
user-defined Log-Likelihood values. Examples of
French-English translation pairs that are filtered
out using the LL values are:
Fr: tout ? En: entire
Fr: propre ? En: clean
Fr: interdits ? En: prohibited
Fr: nombre ? En: number
4.2.2 Mutual Expectation Measure
Dias and Kaalep (2003) have developed the Mu-
tual Expectation measure for evaluating the degree
of cohesiveness between words in a text. We have
applied this metric on our list of multiword terms,
to exclude multiword terms which components do
not occur together more often than expected by
chance. In a first step, we have calculated all n-
gram frequencies (up to 8-grams) for our English
and French sentences. We use these frequencies to
derive the Normalised Expectation (NE) values for
all multiword entries, as specified by the formula
of Dias and Kaalep:
534
NE =
prob(n? gram)
1
n
?
prob(n? 1 ? grams)
(5)
The Normalised Expectation value expresses the
cost, in terms of cohesiveness, of the possible
loss of one word in an n-gram. The higher the
frequency of the n-1-grams, the smaller the NE,
and the smaller the chance that it is a valid mul-
tiword expression. As simple n-gram frequency
also seems to be a valid criterion for multiword
term identification (Daille, 1995), the NE values
are multiplied by the n-gram frequency to obtain
the final Mutual Expectation (ME) value.
We have calculated Mutual Expectation values
for all French and English multiword terms and
filtered out incomplete or erroneous terms having
very low ME values. The following example has
been filtered out:
Fr: permettant d?alimenter le circuit d?eau arri`ere
En: to supply the rear water circuit
Incomplete term:
eau arri`ere - rear water (should be Fr: circuit
d?eau arri`ere - En: rear water circuit)
4.3 Evaluation of the Terminology Extraction
Module
To evaluate the terminology extraction module,
we used all sentences of the three test corpora
(see Section 2.2). We compared the performance
of our algorithm with the output of a commer-
cial state-of-the-art terminology extraction pro-
gram SDL MultiTerm Extract
5
. MultiTerm first
extracts source language terms and identifies in
a separate step the term translations. MultiTerm
makes use of basic vocubulary lists to exclude gen-
eral vocabulary words from the candidate term list.
We ran MultiTerm Extract with the default settings
on 70,000 aligned sentences
6
of the automotive
corpus. The extracted terms of our system have
been filtered by applying Log-Likelihood thresh-
olds (for single word terms) and Mutual Expec-
tation thresholds (for multiword terms). Tabel 4
shows the number of terms after each reduction
phase.
The output of both systems has been manually
labeled taking into account the following guide-
lines:
5
www.translationzone.com/en/products/sdlmultitermextract
6
70,000 sentences was the maximum size of the corpus
that could be processed within MultiTerm Extract.
# extracted # entries # entries
entries after after
ME filtering LL filtering
Anchor chunk approach 2778 2688 2549
Multiterm Extract 1337 N/A N/A
Table 4: Figures after Log-Likelihood and Mutual
Expectation reduction
Anchor chunk approach Correct Not correct Maybe correct
Multiwords 78.5% 19% 2.5%
Single words 89.5% 9.5% 1%
All terms 83% 15% 2%
Multiterm Extract Correct Not correct Maybe correct
Multiwords 51% 48.5% 0.5%
Single words 83% 16% 1%
All terms 66% 33.5% 0.5%
Table 5: Results Term Extraction Module
? judge the quality of the bilingual entry as a
whole, meaning that the French and English
terms should express the same concept
? each entry should form a semantic unit and
refer to an existing concept or action in the
automotive context
During manual validation, the following three
labels have been used: OK (valid entry), NOK (not
a valid entry) and MAYBE (when the annotator
was not sure about the correct label). Table 5
lists the results of both our system and MultiTerm
Extract and illustrates that our linguistically
based alignment approach works particularly well
for the extraction of more complex multiword
expressions.
Error analysis on the errors made by the anchor
chunk approach revealed the following error types:
1. compounds that are only partially retrieved
in one of the two languages:
ceinture outer seat belt
(valable uniquement pour
la ceinture de s
?
ecurit
?
e
avant lat?erale)
(applies only to the outer
seat belt)
2. fuzzy word links (different grammatical
and syntactical structures, paraphrases etc)
that result in bad lexicon entries:
fusibles no fuse
(montage avec vide-
poches inf?erieur fixe sans
rangement des fusibles)
(fitting with fixed lower
storage compartment with
no fuse storage)
3. translation errors in the parallel corpus:
automatique additional
(tableau de commande
climatisation automa-
tique)
(additional air condition-
ing unit control panel)
535
4. ambiguous words that cause PoS and
chunking errors (in the corpus avant is usu-
ally used as an adjective, but in the example it
has a prepositional function as avant de):
c?ables avant cables
(rep?erer la position des
c?ables avant de les
d?eclipper)
(mark the position of the
cables before unclipping
them)
5 Conclusions and future work
We presented a sub-sentential alignment system
that links linguistically motivated phrases in paral-
lel texts based on lexical correspondences and syn-
tactic similarity. Overall, the obtained alignment
scores are comparable to the scores of the state-of-
the-art statistical approach that is used in Moses.
The results show that the aligned linguistically
motivated phrases are a useful means to extract
bilingual terminology for French-English. In the
short term, we will test our methodology on other
language pairs, i.e. French-Dutch, French-Spanish
and French-Swedish. We will also compare our
work with other bilingual term extraction pro-
grams.
6 Acknowledgement
We would like to thank PSA Peugeot Citro?en for
funding this project.
References
Daille, B. 1995. Study and implementation of com-
bined techniques for automatic extraction of termi-
nology. In Klavans, J. and P. Resnik, editors, The
Balancing Act: Combining Symbolic and Statistical
Approaches to Language, pages 49?66. MIT Press,
Cambridge, Massachusetts; London, England.
Dias, G. and H. Kaalep. 2003. Automatic Extraction of
Multiword Units for Estonian: Phrasal Verbs. Lan-
guages in Development, 41:81?91.
Gaussier, E. 1998. Flow Network Models for Word
Alignment and Terminology Extraction from Bilin-
gual Corpora . In 36th Annual Meeting of the Associ-
ation for Computational Linguistics and 17th Inter-
national Conference on Computational Linguistics
(Proceedings of COLING-ACL ?98), pages 444?450,
Universit?e de Montr?eal, Montreal, Quebec, Canada.
Gotti, F., P. Langlais, E. Macklovitch, D. Bourigault,
B. Robichaud, and C. Coulombe. 2005. 3GTM: a
third-generation translation memory. In Proceedings
of the 3rd Computational Linguistics in the North-
East (CLiNE) Workshop, Gatineau, Qu?ebec.
Hwa, R., P. Resnik, A. Weinberg, and O. Kolak. 2002.
Evaluating translational correspondence using anno-
tation projection. In Proceedings of the 40th An-
nual Meeting of the Association for Computational
Linguistics (ACL), pages 392?399, Philadelphia, PA,
USA.
Itagaki, M., T. Aikawa, and X. He. 2007. Auto-
matic Validation of Terminology Consistency with
Statistical Method. In Maegaard, Bente, editor,
Machine Translation Summit XI, pages 269?274,
Copenhagen, Denmark. European Associaton for
Machine Translation.
Koehn, P., H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open Source Toolkit
for Statistical Machine Translation. In Proceedings
of the ACL 2007 Demo and Poster Sessions, pages
177?180, Prague, Czech Republic.
Kupiec, J. 1993. An algorithm for finding noun phrase
correspondences in bilingual corpora. In Proceed-
ings of the 31st Annual Meeting of the Association
for Computational Linguistics.
Macken, L. 2007. Analysis of translational corre-
spondence in view of sub-sentential alignment. In
Proceedings of the METIS-II Workshop on New Ap-
proaches to Machine Translation, pages 97?105,
Leuven, Belgium.
Melamed, I. Dan. 2000. Models of translational equiv-
alence among words. Computational Linguistics,
26(2):221?249.
Moore, R. C. 2002. Fast and accurate sentence align-
ment of bilingual corpora. In Proceedings of the 5th
Conference of the Association for Machine Transla-
tion in the Americas, Machine Translation: from re-
search to real users, pages 135?244, Tiburon, Cali-
fornia.
Och, F. J. and H. Ney. 2003. A systematic comparison
of various statistical alignment models. Computa-
tional Linguistics, 29(1):19?51.
Rayson, P. and R. Garside. 2000. Comparing cor-
pora using frequency profiling. In Proceedings of
the workshop on Comparing Corpora, 38th annual
meeting of the Association for Computational Lin-
guistics (ACL 2000), pages 1?6.
Schmid, H. 1994. Probabilistic part-of-speech tagging
using decision trees. In International Conference on
New Methods in Language Processing, Manchester,
UK.
Tjong Kim Sang, Erik F. and Sabine Buchholz.
2000. Introduction to the CoNLL-2000 Shared Task:
Chunking. In CoNLL-2000 and LLL-2000, pages
127?132, Lisbon, Portugal.
536
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 496?504,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Language-independent bilingual terminology extraction from a
multilingual parallel corpus
Els Lefever1,2, Lieve Macken1,2 and Veronique Hoste1,2
1LT3
School of Translation Studies
University College Ghent
Groot-Brittannie?laan 45
9000 Gent, Belgium
2Department of Applied Mathematics
and Computer Science
Ghent University
Krijgslaan281-S9
9000 Gent, Belgium
{Els.Lefever, Lieve.Macken, Veronique.Hoste}@hogent.be
Abstract
We present a language-pair independent
terminology extraction module that is
based on a sub-sentential alignment sys-
tem that links linguistically motivated
phrases in parallel texts. Statistical filters
are applied on the bilingual list of candi-
date terms that is extracted from the align-
ment output.
We compare the performance of both
the alignment and terminology extrac-
tion module for three different language
pairs (French-English, French-Italian and
French-Dutch) and highlight language-
pair specific problems (e.g. different com-
pounding strategy in French and Dutch).
Comparisons with standard terminology
extraction programs show an improvement
of up to 20% for bilingual terminology ex-
traction and competitive results (85% to
90% accuracy) for monolingual terminol-
ogy extraction, and reveal that the linguis-
tically based alignment module is particu-
larly well suited for the extraction of com-
plex multiword terms.
1 Introduction
Automatic Term Recognition (ATR) systems are
usually categorized into two main families. On the
one hand, the linguistically-based or rule-based
approaches use linguistic information such as PoS
tags, chunk information, etc. to filter out stop
words and restrict candidate terms to predefined
syntactic patterns (Ananiadou, 1994), (Dagan and
Church, 1994). On the other hand, the statistical
corpus-based approaches select n-gram sequences
as candidate terms that are filtered by means of
statistical measures. More recent ATR systems
use hybrid approaches that combine both linguis-
tic and statistical information (Frantzi and Anani-
adou, 1999).
Most bilingual terminology extraction systems
first identify candidate terms in the source lan-
guage based on predefined source patterns, and
then select translation candidates for these terms
in the target language (Kupiec, 1993).
We present an alternative approach that gen-
erates candidate terms directly from the aligned
words and phrases in our parallel corpus. In a sec-
ond step, we use frequency information of a gen-
eral purpose corpus and the n-gram frequencies
of the automotive corpus to determine the term
specificity. Our approach is more flexible in the
sense that we do not first generate candidate terms
based on language-dependent predefined PoS pat-
terns (e.g. for French, N N, N Prep N, and N
Adj are typical patterns), but immediately link lin-
guistically motivated phrases in our parallel cor-
pus based on lexical correspondences and syntac-
tic similarity.
This article reports on the term extraction ex-
periments for 3 language pairs, i.e. French-Dutch,
French-English and French-Italian. The focus was
on the extraction of automative lexicons.
The remainder of this paper is organized as fol-
lows: Section 2 describes the corpus. In Section 3
we present our linguistically-based sub-sentential
alignment system and in Section 4 we describe
how we generate and filter our list of candidate
terms. We compare the performance of our sys-
tem with both bilingual and monolingual state-of-
the-art terminology extraction systems. Section 5
concludes this paper.
496
2 Corpus
The focus of this research project was on the au-
tomatic extraction of 20 bilingual automative lex-
icons. All work was carried out in the framework
of a customer project for a major French automo-
tive company. The final goal of the project is to
improve vocabulary consistency in technical texts
across the 20 languages in the customer?s portfo-
lio. The French database contains about 400,000
entries (i.e. sentences and parts of sentences with
an average length of 9 words) and the translation
percentage of the database into 19 languages de-
pends on the target market.
For the development of the alignment and termi-
nology extraction module, we created three paral-
lel corpora (Italian, English, Dutch) with French
as a central language. Figures about the size of
each parallel corpus can be found in table 1.
Target Lang. # Sentence pairs # words
French Italian 364,221 6,408,693
French English 363,651 7,305,151
French Dutch 364,311 7,100,585
Table 1: Number of sentence pairs and total num-
ber of words in the three parallel corpora
2.1 Preprocessing
We PoS-tagged and lemmatized the French, En-
glish and Italian corpora with the freely available
TreeTagger tool (Schmid, 1994) and we used Tad-
Pole (Van den Bosch et al, 2007) to annotate the
Dutch corpus.
In a next step, chunk information was added
by a rule-based language-independent chunker
(Macken et al, 2008) that contains distituency
rules, which implies that chunk boundaries are
added between two PoS codes that cannot occur
in the same constituent.
2.2 Test and development corpus
As we presume that sentence length has an impact
on the alignment performance, and thus on term
extraction, we created three test sets with vary-
ing sentence lengths. We distinguished short sen-
tences (2-7 words), medium-length sentences (8-
19 words) and long sentences (> 19 words). Each
test corpus contains approximately 9,000 words;
the number of sentence pairs per test set can be
found in table 2. We also created a development
corpus with sentences of varying length to debug
the linguistic processing and the alignment mod-
ule as well as to define the thresholds for the sta-
tistical filtering of the candidate terms (see 4.1).
# Words # Sentence pairs
Short (< 8 words) +- 9,000 823
Medium (8-19 words) +- 9,000 386
Long (> 19 words) +- 9,000 180
Development corpus +-5,000 393
Table 2: Number of words and sentence pairs in
the test and development corpora
3 Sub-sentential alignment module
As the basis for our terminology extraction sys-
tem, we used the sub-sentential alignment sys-
tem of (Macken and Daelemans, 2009) that links
linguistically motivated phrases in parallel texts
based on lexical correspondences and syntactic
similarity. In the first phase of this system, anchor
chunks are linked, i.e. chunks that can be linked
with a very high precision. We think these anchor
chunks offer a valid and language-independent al-
ternative to identify candidate terms based on pre-
defined PoS patterns. As the automotive corpus
contains rather literal translations, we expect that a
high percentage of anchor chunks can be retrieved.
Although the architecture of the sub-sentential
alignment system is language-independent, some
language-specific resources are used. First, a
bilingual lexicon to generate the lexical correspon-
dences and second, tools to generate additional
linguistic information (PoS tagger, lemmatizer and
a chunker). The sub-sentential alignment system
takes as input sentence-aligned texts, together with
the additional linguistic annotations for the source
and the target texts.
The source and target sentences are divided into
chunks based on PoS information, and lexical cor-
respondences are retrieved from a bilingual dic-
tionary. In order to extract bilingual dictionaries
from the three parallel corpora, we used the Perl
implementation of IBM Model One that is part of
the Microsoft Bilingual Sentence Aligner (Moore,
2002).
In order to link chunks based on lexical clues
and chunk similarity, the following steps are taken
for each sentence pair:
1. Creation of the lexical link matrix
2. Linking chunks based on lexical correspon-
dences and chunk similarity
497
3. Linking remaining chunks
3.1 Lexical Link Matrix
For each source and target word, all translations
for the word form and the lemma are retrieved
from the bilingual dictionary. In the process of
building the lexical link matrix, function words are
neglected. For all content words, a lexical link is
created if a source word occurs in the set of pos-
sible translations of a target word, or if a target
word occurs in the set of possible translations of
the source words. Identical strings in source and
target language are also linked.
3.2 Linking Anchor chunks
Candidate anchor chunks are selected based on the
information available in the lexical link matrix.
The candidate target chunk is built by concatenat-
ing all target chunks from a begin index until an
end index. The begin index points to the first target
chunk with a lexical link to the source chunk un-
der consideration. The end index points to the last
target chunk with a lexical link to the source chunk
under consideration. This way, 1:1 and 1:n candi-
date target chunks are built. The process of select-
ing candidate chunks as described above, is per-
formed a second time starting from the target sen-
tence. This way, additional n:1 candidates are con-
structed. For each selected candidate pair, a simi-
larity test is performed. Chunks are considered to
be similar if at least a certain percentage of words
of source and target chunk(s) are either linked by
means of a lexical link or can be linked on the basis
of corresponding part-of-speech codes. The per-
centage of words that have to be linked was em-
pirically set at 85%.
3.3 Linking Remaining Chunks
In a second step, chunks consisting of one function
word ? mostly punctuation marks and conjunc-
tions ? are linked based on corresponding part-of-
speech codes if their left or right neighbour on the
diagonal is an anchor chunk. Corresponding final
punctuation marks are also linked.
In a final step, additional candidates are con-
structed by selecting non-anchor chunks in the
source and target sentence that have correspond-
ing left and right anchor chunks as neigbours. The
anchor chunks of the first step are used as contex-
tual information to link n:m chunks or chunks for
which no lexical link was found in the lexical link
matrix.
In Figure 1, the chunks [Fr: gradient] ? [En:
gradient] and the final punctuation mark have been
retrieved in the first step as anchor chunk. In the
last step, the n:m chunk [Fr: de remonte?e pe?dale
d? embrayage] ? [En: of rising of the clutch pedal]
is selected as candidate anchor chunk because it is
enclosed within anchor chunks.
Figure 1: n:m candidate chunk: ?A? stands for an-
chor chunks, ?L? for lexical links, ?P? for words
linked on the basis of corresponding PoS codes
and ?R? for words linked by language-dependent
rules.
As the contextual clues (the left and right neig-
bours of the additional candidate chunks are an-
chor chunks) provide some extra indication that
the chunks can be linked, the similarity test for
the final candidates was somewhat relaxed: the
percentage of words that have to be linked was
lowered to 0.80 and a more relaxed PoS matching
function was used.
3.4 Evaluation
To test our alignment module, we manually indi-
cated all translational correspondences in the three
test corpora. We used the evaluation methodology
of Och and Ney (2003) to evaluate the system?s
performance. They distinguished sure alignments
(S) and possible alignments (P) and introduced the
following redefined precision and recall measures
(where A refers to the set of alignments):
precision =
|A ? P |
|A|
, recall =
|A ? S|
|S|
(1)
and the alignment error rate (AER):
AER(S, P ;A) = 1?
|A ? P |+ |A ? S|
|A|+ |S|
(2)
498
Table 3 shows the alignment results for the three
language pairs. (Macken et al, 2008) showed that
the results for French-English were competitive to
state-of-the-art alignment systems.
SHORT MEDIUM LONG
p r e p r e p r e
Italian .99 .93 .04 .95 .89 .08 .95 .89 .07
English .97 .91 .06 .95 .85 .10 .92 .85 .12
Dutch .96 .83 .11 .87 .73 .20 .87 .67 .24
Table 3: Precision (p), recall (r) and alignment er-
ror rate (e) for our sub-sentential alignment sys-
tem evaluated on French-Italian, French-English
and French-Dutch
As expected, the results show that the align-
ment quality is closely related to the similarity be-
tween languages. As shown in example (1), Ital-
ian and French are syntactically almost identical
? and hence easier to align, English and French
are still close but show some differences (e.g dif-
ferent compounding strategy and word order) and
French and Dutch present a very different lan-
guage structure (e.g. in Dutch the different com-
pound parts are not separated by spaces, separable
verbs, i.e. verbs with prefixes that are stripped off,
occur frequently (losmaken as an infinitive versus
maak los in the conjugated forms) and a different
word order is adopted).
(1) Fr: de?clipper le renvoi de ceinture de se?curite?.
(En: unclip the mounting of the belt of safety)
It: sganciare il dispositivo di riavvolgimento della
cintura di sicurezza.
(En: unclip the mounting of the belt of satefy)
En: unclip the seat belt mounting.
Du: maak de oprolautomaat van de autogordel los.
(En: clip the mounting of the seat-belt un)
We tried to improve the low recall for French-
Dutch by adding a decompounding module to our
alignment system. In case the target word does
not have a lexical correspondence in the source
sentence, we decompose the Dutch word into its
meaningful parts and look for translations of the
compound parts. This implies that, without de-
compounding, in example 2 only the correspon-
dences doublure ? binnenpaneel, arc ? dakverste-
viging and arrie`re ? achter will be found. By de-
composing the compound into its meaningful parts
(binnenpaneel = binnen + paneel, dakversteviging
= dak + versteviging) and retrieving the lexical
links for the compound parts, we were able to link
the missing correspondence: pavillon ? dakverste-
viging.
(2) Fr: doublure arc pavillon arrie`re.
(En: rear roof arch lining)
Du: binnenpaneel dakversteviging achter.
We experimented with the decompounding mod-
ule of (Vandeghinste, 2008), which is based on
the Celex lexical database (Baayen et al, 1993).
The module, however, did not adapt well to the
highly technical automotive domain, which is re-
flected by its low recall and the low confidence
values for many technical terms. In order to adapt
the module to the automotive domain, we imple-
mented a domain-dependent extension to the de-
compounding module on the basis of the devel-
opment corpus. This was done by first running the
decompounding module on the Dutch sentences to
construct a list with possible compound heads, be-
ing valid compound parts in Dutch. This list was
updated by inspecting the decompounding results
on the development corpus. While decomposing,
we go from right to left and strip off the longest
valid part that occurs in our preconstructed list
with compound parts and we repeat this process
on the remaining part of the word until we reach
the beginning of the word.
Table 4 shows the impact of the decompound-
ing module, which is more prominent for short
and medium sentences than for long sentences. A
superficial error analysis revealed that long sen-
tences combine a lot of other French ? Dutch
alignment difficulties next to the decompounding
problem (e.g. different word order and separable
verbs).
SHORT MEDIUM LONG
p r e p r e p r e
Dutch
no dec .95 .76 .16 .88 .67 .24 .88 .64 .26
dec .96 .83 .11 .87 .73 .20 .87 .67 .24
Table 4: Precision (p), recall (r) and alignment er-
ror rate (e) for French-Dutch without and with de-
compounding information
4 Term extraction module
As described in Section 1, we generate candi-
date terms from the aligned phrases. We believe
these anchor chunks offer a more flexible approach
499
because the method is language-pair independent
and is not restricted to a predefined set of PoS pat-
terns to identify valid candidate terms. In a second
step, we use a general-purpose corpus and the n-
gram frequency of the automotive corpus to deter-
mine the specificity of the candidate terms.
The candidate terms are generated in several
steps, as illustrated below for example (3).
(3) Fr: Tableau de commande de climatisation automa-
tique
En: Automatic air conditioning control panel
1. Selection of all anchor chunks (minimal
chunks that could be linked together) and lex-
ical links within the anchor chunks:
tableau de commande control panel
climatisation air conditioning
commande control
tableau panel
2. combine each NP + PP chunk:
commande de climatisa-
tion automatique
automatic air condition-
ing control
tableau de commande de
climatisation automatique
automatic air condition-
ing control panel
3. strip off the adjectives from the anchor
chunks:
commande de climatisa-
tion
air conditioning control
tableau de commande de
climatisation
air conditioning control
panel
4.1 Filtering candidate terms
To filter our candidate terms, we keep following
criteria in mind:
? each entry in the extracted lexicon should re-
fer to an object or action that is relevant for
the domain (notion of termhood that is used
to express ?the degree to which a linguis-
tic unit is related to domain-specific context?
(Kageura and Umino, 1996))
? multiword terms should present a high de-
gree of cohesiveness (notion of unithood that
expresses the ?degree of strength or stability
of syntagmatic combinations or collocations?
(Kageura and Umino, 1996))
? all term pairs should contain valid translation
pairs (translation quality is also taken into
consideration)
To measure the termhood criterion and to fil-
ter out general vocabulary words, we applied
Log-Likelihood filters on the French single-word
terms. In order to filter on low unithood values,
we calculated the Mutual Expectation Measure for
the multiword terms in both source and target lan-
guage.
4.1.1 Log-Likelihood Measure
The Log-Likehood measure (LL) should allow us
to detect single word terms that are distinctive
enough to be kept in our bilingual lexicon (Daille,
1995). This metric considers word frequencies
weighted over two different corpora (in our case a
technical automotive corpus and the more general
purpose corpus ?Le Monde?1), in order to assign
high LL-values to words having much higher or
lower frequencies than expected. We implemented
the formula for both the expected values and the
Log-Likelihood values as described by (Rayson
and Garside, 2000).
Manual inspection of the Log-Likelihood fig-
ures confirmed our hypothesis that more domain-
specific terms in our corpus were assigned high
LL-values. We experimentally defined the thresh-
old for Log-Likelihood values corresponding to
distinctive terms on our development corpus. Ex-
ample (4) shows some translation pairs which are
filtered out by applying the LL threshold.
(4) Fr: cependant ? En: however ? It: tuttavia ? Du:
echter
Fr: choix ? En: choice ? It: scelta ? Du: keuze
Fr: continuer ? En: continue ? It: continuare ? Du:
verdergaan
Fr: cadre ? En: frame ? It: cornice ? Du: frame
(erroneous filtering)
Fr: alle?gement ? En: lightening ? It: alleggerire ?
Du: verlichten (erroneous filtering)
4.1.2 Mutual Expectation Measure
The Mutual Expectation measure as described by
Dias and Kaalep (2003) is used to measure the
degree of cohesiveness between words in a text.
This way, candidate multiword terms whose com-
ponents do not occur together more often than ex-
pected by chance get filtered out. In a first step,
we have calculated all n-gram frequencies (up to
8-grams) for our four automotive corpora and then
used these frequencies to derive the Normalised
1http://catalog.elra.info/product info.php?products id=438
500
Expectation (NE) values for all multiword entries,
as specified by the formula of Dias and Kaalep:
NE =
prob(n? gram)
1
n
?
prob(n? 1? grams)
(3)
The Normalised Expectation value expresses the
cost, in terms of cohesiveness, of the possible loss
of one word in an n-gram. The higher the fre-
quency of the n-1-grams, the smaller the NE, and
the smaller the chance that it is a valid multiword
expression. The final Mutual Expectation (ME)
value is then obtained by multiplying the NE val-
ues by the n-gram frequency. This way, the Mu-
tual Expectation between n words in a multiword
expression is based on the Normalised Expecta-
tion and the relative frequency of the n-gram in
the corpus.
We calculated Mutual Expectation values for all
candidate multiword term pairs and filtered out in-
complete or erroneous terms having ME values be-
low an experimentally set threshold (being below
0.005 for both source and target multiword or be-
low 0.0002 for one of the two multiwords in the
translation pair). The following incomplete can-
didate terms in example (5) were filtered out by
applying the ME filter:
(5) Fr: fermeture embout - En: end closing - It:
chiusura terminale - Du: afsluiting deel
(should be: Fr: fermeture embout de brancard - En:
chassis member end closing panel - It: chiusura ter-
minale del longherone - Du: afsluiting voorste deel
van langsbalk)
4.2 Evaluation
The terminology extraction module was tested on
all sentences from the three test corpora. The out-
put was manually labeled and the annotators were
asked to judge both the translational quality of the
entry (both languages should refer to the same ref-
erential unit) as well as the relevance of the term
in an automotive context. Three labels were used:
OK (valid entry), NOK (not a valid entry) and
MAYBE (in case the annotator was not sure about
the relevance of the term).
First, the impact of the statistical filtering was
measured on the bilingual term extraction. Sec-
ondly, we compared the output of our system with
the output of a commercial bilingual terminology
extraction module and with the output of a set of
standard monolingual term extraction modules.
Since the annotators labeled system output, the
reported scores all refer to precision scores. In fu-
ture work, we will develop a gold standard corpus
which will enable us to also calculate recall scores.
4.2.1 Impact of filtering
Table 5 shows the difference in performance for
both single and multiword terms with and with-
out filtering. Single-word filtering seems to have a
bigger impact on the results than multiword filter-
ing. This can be explained by the fact that our can-
didate multiword terms are generated from anchor
chunks (chunks aligned with a very high preci-
sion) that already answer to strict syntactical con-
straints. The annotators also mentioned the diffi-
culty of judging the relevance of single word terms
for the automotive domain (no clear distinction be-
tween technical and common vocabulary).
NOT FILTERED FILTERED
OK NOK MAY OK NOK MAY
FR-EN
Sing w 82% 17% 1% 86.5% 12% 1.5%
Mult w 81% 16.5% 2.5% 83% 14.5% 2.5%
FR-IT
Sing w 80.5% 19% 0.5% 84.5% 15% 0.5%
Mult w 69% 30% 1.0% 72% 27% 1.0%
FR-DU
Sing w 72% 25% 3% 75% 22% 3%
Mult w 83% 15% 2% 84% 14% 2%
Table 5: Impact of statistical filters on Single and
Multiword terminology extraction
4.2.2 Comparison with bilingual terminology
extraction
We compared the three filtered bilingual lexi-
cons (French versus English-Italian-Dutch) with
the output of a commercial state-of-the-art termi-
nology extraction program SDL MultiTerm Ex-
tract2. MultiTerm is a statistically based system
that first generates a list of candidate terms in the
source language (French in our case) and then
looks for translations of these terms in the target
language. We ran MultiTerm with its default set-
tings (default noise-silence threshold, default stop-
word list, etc.) on a large portion of our parallel
corpus that also contains all test sentences3. We
ran our system (where term extraction happens on
a sentence per sentence basis) on the three test
sets.
2www.translationzone.com/en/products/sdlmultitermextract
370,000 sentences seemed to be the maximum size of
the corpus that could be easily processed within MultiTerm
Extract.
501
Table 6 shows that even after applying statistical
filters, our term extraction module retains a much
higher number of candidate terms than MultiTerm.
# Extracted terms # Terms after filtering MultiTerm
FR-EN 4052 3386 1831
FR-IT 4381 3601 1704
FR-DU 3285 2662 1637
Table 6: Number of terms before and after apply-
ing Log-Likelihood and ME filters
Table 7 lists the results of both systems and
shows the differences in performance for single
and multiword terms. Following observations can
be made:
? The performance of both systems is compa-
rable for the extraction of single word terms,
but our system clearly outperforms Multi-
Term when it comes to the extraction of more
complex multiword terms.
? Although the alignment results for French-
Italian were very good, we do not achieve
comparable results for Italian multiword ex-
traction. This can be due to the fact that the
syntactic structure is very similar in both lan-
guages. As a result, smaller syntactic chunks
are linked. However one can argue that, just
because of the syntactic resemblance of both
languages, the need for complex multiword
terms is less prominent in closely related lan-
guages as translators can just paste smaller
noun phrases together in the same order in
both languages. If we take the following ex-
ample for instance:
de?poser ? l? embout ? de brancard
togliere ? il terminale ? del sotto-
porta
we can recompose the larger compound
l?embout de brancard or il terminale del sot-
toporta by translating the smaller parts in the
same order (l?embout ? il terminale and de
brancard ? del sottoporta
? Despite the worse alignment results for
Dutch, we achieve good accuracy results on
the multiword term extraction. Part of that
can be explained by the fact that French and
Dutch use a different compounding strategy:
whereas French compounds are created by
concatenating prepositional phrases, Dutch
usually tends to concatenate noun phrases
(even without inserting spaces between the
different compound parts). This way we can
extract larger Dutch chunks that correspond
to several French chunks, for instance:
Fr: feu re?gulateur ? de pression
carburant.
Du: brandstofdrukregelaar.
ANCHOR CHUNK APPROACH MULTITERM
OK NOK MAY OK NOK MAY
FR-EN
Sing w 86.5% 12% 1.5% 77% 21% 2%
Mult w 83% 14.5% 2.5% 47% 51% 2%
Total 84.5% 13.5% 2 % 64% 34% 2%
FR-IT
Sing w 84.5% 15% 0.5% 85% 14% 1%
Mult w 72% 27% 1.0% 65% 34% 1%
Total 77.5% 22% 1% 76.5% 22.5% 1%
FR-DU
Sing w 75% 22% 3% 64.5% 33% 2.5%
Mult w 84% 14% 2% 49.5% 49.5% 1%
Total 79.5% 20% 2.5% 58% 40% 2%
Table 7: Precision figures for our term extraction
system and for SDL MultiTerm Extract
4.2.3 Comparison with monolingual
terminology extraction
In order to have insights in the performance of
our terminology extraction module, without con-
sidering the validity of the bilingual terminology
pairs, we contrasted our extracted English terms
with state-of-the art monolingual terminology sys-
tems. As we want to include both single words and
multiword terms in our technical automotive lex-
icon, we only considered ATR systems which ex-
tract both categories. We used the implementation
for these systems from (Zhang et al, 2008) which
is freely available at1.
We compared our system against 5 other ATR
systems:
1. Baseline system (Simple Term Frequency)
2. Weirdness algorithm (Ahmad et al, 2007)
which compares term frequencies in the tar-
get and reference corpora
3. C-value (Frantzi and Ananiadou, 1999)
which uses term frequencies as well as
unit-hood filters (to measure the collocation
strength of units)
1http://www.dcs.shef.ac.uk/?ziqizhang/resources/tools/
502
4. Glossex (Kozakov et al, 2004) which uses
term frequency information from both the tar-
get and reference corpora and compares term
frequencies with frequencies of the multi-
word components
5. TermExtractor (Sclano and Velardi, 2007)
which is comparable to Glossex but intro-
duces the ?domain consensus? which ?sim-
ulates the consensus that a term must gain in
a community before being considered a rele-
vant domain term?
For all of the above algorithms, the input auto-
motive corpus is PoS tagged and linguistic filters
(selecting nouns and noun phrases) are applied to
extract candidate terms. In a second step, stop-
words are removed and the same set of extracted
candidate terms (1105 single words and 1341 mul-
tiwords) is ranked differently by each algorithm.
To compare the performance of the ranking algo-
rithms, we selected the top terms (300 single and
multiword terms) produced by all algorithms and
compared these with our top candidate terms that
are ranked by descending Log-likelihood (calcu-
lated on the BNC corpus) and Mutual Expectation
values. Our filtered list of unique English automo-
tive terms contains 1279 single words and 1879
multiwords in total. About 10% of the terms do
not overlap between the two term lists. All can-
didate terms have been manually labeled by lin-
guists. Table 8 shows the results of this compari-
son.
SINGLE WORD TERMS MULTIWORD TERMS
OK NOK MAY OK NOK MAY
Baseline 80% 19.5% 0.5% 84.5% 14.5% 1%
Weirdness 95.5% 3.5% 1% 96% 2.5% 1.5%
C-value 80% 19.5% 0.5% 94% 5% 1%
Glossex 94.5% 4.5% 1% 85.5% 14% 0.5%
TermExtr. 85% 15% 0% 79% 20% 1%
AC 85.5% 14.5% 0% 90% 8% 2%
approach
Table 8: Results for monolingual Term Extraction
on the English part of the automotive corpus
Although our term extraction module has been tai-
lored towards bilingual term extraction, the results
look competitive to monolingual state-of-the-art
ATR systems. If we compare these results with
our bilingual term extraction results, we can ob-
serve that we gain more in performance for mul-
tiwords than for single words, which might mean
that the filtering and ranking based on the Mutual
Expectation works better than the Log-Likelihood
ranking.
An error analysis of the results leads to the fol-
lowing insights:
? All systems suffer from partial retrieval of
complex multiwords (e.g. ATR management
ecu instead of engine management ecu, AC
approach chassis leg end piece closure in-
stead of chassis leg end piece closure panel).
? We manage to extract nice sets of multiwords
that can be associated with a given concept,
which could be nice for automatic ontology
population (e.g. AC approach gearbox cas-
ing, gearbox casing earth, gearbox casing
earth cable, gearbox control, gearbox control
cables, gearbox cover, gearbox ecu, gearbox
ecu initialisation procedure, gearbox fixing,
gearbox lower fixings, gearbox oil, gearbox
oil cooler protective plug).
? Sometimes smaller compounds are not ex-
tracted because they belong to the same syn-
tactic chunk (E.g we extract passenger com-
partment assembly, passenger compartment
safety, passenger compartment side panel,
etc. but not passenger compartment as such).
5 Conclusions and further work
We presented a bilingual terminology extraction
module that starts from sub-sentential alignments
in parallel corpora and applied it on three differ-
ent parallel corpora that are part of the same auto-
motive corpus. Comparisons with standard termi-
nology extraction programs show an improvement
of up to 20% for bilingual terminology extraction
and competitive results (85% to 90% accuracy) for
monolingual terminology extraction. In the near
future we want to experiment with other filtering
techniques, especially to measure the domain dis-
tinctiveness of terms and work on a gold standard
for measuring recall next to accuracy. We will
also investigate our approach on languages which
are more distant from each other (e.g. French ?
Swedish).
Acknowledgments
We would like to thank PSA Peugeot Citroe?n for
funding this project.
503
References
K. Ahmad, L. Gillam, and L. Tostevin. 2007. Uni-
versity of surrey participation in trec8: Weirdness
indexing for logical document extrapolation and
rerieval (wilder). In Proceedings of the Eight Text
REtrieval Conference (TREC-8).
S. Ananiadou. 1994. A methodology for automatic
term recognition. In Proceedings of the 15th con-
ference on computational linguistics, pages 1034?
1038.
R.H. Baayen, R. Piepenbrock, and H. van Rijn. 1993.
The celex lexical database on cd-rom.
I. Dagan and K. Church. 1994. Termight: identifying
and translating technical terminology. In Proceed-
ings of Applied Language Processing, pages 34?40.
B. Daille. 1995. Study and implementation of com-
bined techniques for automatic extraction of termi-
nology. In J. Klavans and P. Resnik, editors, The
Balancing Act: Combining Symbolic and Statistical
Approaches to Language, pages 49?66. MIT Press,
Cambridge, Massachusetts; London, England.
G. Dias and H. Kaalep. 2003. Automatic extraction
of multiword units for estonian: Phrasal verbs. Lan-
guages in Development, 41:81?91.
K.T. Frantzi and S. Ananiadou. 1999. the c-value/nc-
value domain independent method for multiword
term extraction. journal of Natural Language Pro-
cessing, 6(3):145?180.
K. Kageura and B. Umino. 1996. Methods of au-
tomatic term recognition: a review. Terminology,
3(2):259?289.
L. Kozakov, Y. Park, T.-H Fin, Y. Drissi, Y.N. Do-
ganata, and T. Confino. 2004. Glossary extraction
and knowledge in large organisations via semantic
web technologies. In Proceedings of the 6th Inter-
national Semantic Web Conference and he 2nd Asian
Semantic Web Conference (Se-mantic Web Chal-
lenge Track).
J. Kupiec. 1993. An algorithm for finding noun phrase
correspondences in bilingual corpora. In Proceed-
ings of the 31st Annual Meeting of the Association
for Computational Linguistics.
L. Macken and W. Daelemans. 2009. Aligning lin-
guistically motivated phrases. In van Halteren H.
Verberne, S. and P.-A. Coppen, editors, Selected Pa-
pers from the 18th Computational Linguistics in the
Netherlands Meeting, pages 37?52, Nijmegen, The
Netherlands.
L. Macken, E. Lefever, and V. Hoste. 2008.
Linguistically-based sub-sentential alignment for
terminology extraction from a bilingual automotive
corpus. In Proceedings of the 22nd International
Conference on Computational Linguistics (Coling
2008), pages 529?536, Manchester, United King-
dom.
R. C. Moore. 2002. Fast and accurate sentence align-
ment of bilingual corpora. In Proceedings of the 5th
Conference of the Association for Machine Trans-
lation in the Americas, Machine Translation: from
research to real users, pages 135?244, Tiburon, Cal-
ifornia.
F. J. Och and H. Ney. 2003. A systematic comparison
of various statistical alignment models. Computa-
tional Linguistics, 29(1):19?51.
P. Rayson and R. Garside. 2000. Comparing cor-
pora using frequency profiling. In Proceedings of
the workshop on Comparing Corpora, 38th annual
meeting of the Association for Computational Lin-
guistics (ACL 2000), pages 1?6.
H. Schmid. 1994. Probabilistic part-of-speech tagging
using decision trees. In International Conference on
New Methods in Language Processing, Manchester,
UK.
F. Sclano and P. Velardi. 2007. Termextractor: a web
application to learn the shared terminology of emer-
gent web communities. In Proceedings of the 3rd
International Conference on Interoperability for En-
terprise Software and Applications (I-ESA 2007).
A. Van den Bosch, G.J. Busser, W. Daelemans, and
S. Canisius. 2007. An efficient memory-based mor-
phosyntactic tagger and parser for dutch. In Selected
Papers of the 17th Computational Linguistics in the
Netherlands Meeting, pages 99?114, Leuven, Bel-
gium.
V. Vandeghinste. 2008. A Hybrid Modular Machine
Translation System. LoRe-MT: Low Resources Ma-
chine Translation. Ph.D. thesis, Centre for Compu-
tational Linguistics, KULeuven.
Z. Zhang, J. Iria, C. Brewster, and F. Ciravegna. 2008.
A comparative evaluation of term recognition algo-
rithms. In Proceedings of the sixth international
conference of Language Resources and Evaluation
(LREC 2008).
504
Proceedings of the EACL 2012 Workshop on Computational Linguistics and Writing, pages 1?8,
Avignon, France, April 23, 2012. c?2012 Association for Computational Linguistics
From Character to Word Level:  Enabling the Linguistic Analyses of Inputlog Process Data 
  Mari?lle Leijten  Lieve Macken Flanders Research Foundation University of Antwerp Department of Management Belgium marielle.leijten@ua.ac.be 
LT3, Language and Translation Technology Team, University College Ghent and Ghent University  Belgium  lieve.macken@hogent.be     Veronique Hoste  LT3, Language and Translation Technology Team, University College Ghent and Ghent University  Belgium  veronique.hoste@hogent.be  
Eric Van Horenbeeck University of Antwerp Department of Management Belgium eric.vanhorenbeeck@ua.ac.be   
Luuk Van Waes University of Antwerp Department of Management Belgium luuk.vanwaes@ua.ac.be  
 
 Abstract Keystroke-logging tools are widely used in writing process research.  These applications are designed to capture each character and mouse movement as isolated events as an indicator of cognitive processes.  The current research project explores the possibilities of aggregating the logged process data from the letter level (keystroke) to the word level by merging them with existing lexica and using NLP tools.  Linking writing process data to lexica and using NLP tools enables researchers to analyze the data on a higher, more complex level. In this project the output data of Inputlog are segmented on the sentence level and then tokenized.  However, by definition writing process data do not always represent clean and grammatical text.  Coping with this problem was one of the 
main challenges in the current project.  Therefore, a parser has been developed that extracts three types of data from the S-notation: word-level revisions, deleted fragments, and the final writing product.  The within-word typing errors are identified and excluded from further analyses.  At this stage the Inputlog process data are enriched with the following linguistic information: part-of-speech tags, lemmas, chunks, syllable boundaries and word frequencies.  1 Introduction Keystroke-logging is a popular method in writing research (Sullivan & Lindgren, 2006) to study the underlying cognitive processes (Berninger, 2012). Various keystroke-logging programs have been developed, each with a different focus1.  The programs differ in the events that are logged                                                            1 A detailed overview of available keystroke logging programs can be found on http://www.writingpro.eu/ logging_programs.php. 
1
(keyboard and/or mouse, speech recognition), in the environment that is logged (a program-specific text editor, MS Word or all Windows-based applications), in their combination with other logging tools (e.g., eye tracking and usability tools like Morae) and the analytic detail of the output files.  Examples of keystroke-logging tools are: ? Scriptlog: Text editor, Eyetracking (Str?mqvist, Holmqvist, Johansson, Karlsson, & Wengelin, 2006),  ? Inputlog: Windows environment, speech recognition (Leijten & Van Waes, 2006),  ? Translog: Text editor, integration of dictionaries (Jakobsen, 2006) (Wengelin et al, 2009).   Keystroke loggers? data output is mainly based on capturing each character and mouse movement as isolated events.  In the current research project2 we explore the possibilities of aggregating the logged process data from the letter level (keystroke) to the word level by merging them with existing lexica and using NLP tools. Linking writing process data to lexica and using NLP tools enables us to analyze the data on a higher, more complex level.  By doing so we would like to stimulate interdisciplinary research, and relate findings in the domain of writing research to other domains (e.g., Pragmatics, CALL, Translation studies, Psycholinguistics).  We argue that the enriched process data combined with temporal information (time stamps, action times and pauses) will further facilitate the analysis of the logged data and address innovative research questions.  For instance, Is there a developmental shift in the pausing behaviors of writers related to word classes, e.g., before adjectives as opposed to before nouns (cf. cognitive development in language production)?  Do translation segments correspond to linguistic units (e.g., comparing speech recognition and keyboarding)?  Which linguistic shifts characterize substitutions as a sub type of revisions (e.g., linguistic categories, frequency)?  A more elaborate example of a research question in which the linguistic information has added value is: Is the text prodcution of causal markers more cognitive demanding than the production of temporal markers?  In reading                                                            2 FWO-Merging writing process data with lexica -2009-2012 
research, evidence is found that it takes readers longer to process sentences or paragraphs that contain causal markers than temporal markers.  Does the same hold for the production of these linguistic markers?  Based on the linguistic information added to the writing process data researchers are now able to easily select causal and temporal markers and compare the process data from various perspectives (cf. step 4 - linguistic analyses). The work described in this paper is based on the output of Inputlog3, but it can also be applied to the output of other keystroke-logging programs.  To promote more linguistically-oriented writing process research, Inputlog aggregates the logged process data from the character level (keystroke) to the word level.  In a subsequent step, we use various Natural Language Processing (NLP) tools to further annotate the logged process data with different kinds of linguistic information: part-of-speech tags, lemmata, chunk boundaries, syllable boundaries, and word frequency.  The remainder of this paper is structured as follows.  Section 2 describes the output of Inputlog, and section 3 describes an intermediate level of analysis.  Section 4 describes the flow of the linguistic analyses and the various linguistic annotations.  Section 5 wraps up with some concluding remarks and suggestions for future research. 2 Inputlog Inputlog is a word-processor independent keystroke-logging program that not only registers keystrokes, mouse movements, clicks and pauses in MS Word, but also in any other Windows-based software applications.  Keystroke-logging programs store the complete sequence of keyboard and/or mouse events in chronological order.  Figure 1 represents ?Volgend jaar? (?Next Year?) at the character and mouse action level. The keyboard strokes, mouse movements, and mouse clicks are represented in a readable output for each action (e.g., ?SPACE? refers to the spacebar, LEFT Click is a left mouse click, and ?Movement? is a synthesized representation of a continuous mouse movement).  Additionally, timestamps indicate when keys are pressed and released, and when mouse movements are made.  For each keystroke in MSWord the position of                                                            3 http://www.inputlog.net/ 
2
the character in the document is represented as well as the total length of the document at that specific moment.  This enables researchers to take the non-linearity of the writing process into account, which is the result of the execution of revisions during the text production.   
 Figure 1 Example of general analysis Inputlog.  To represent the non-linearity of the writing process the S-notation is used.  The S-notation (Kollberg & Severinson Eklundh, 2002) contains information about the revision types (insertion or deletion), the order of the revisions and the place in the text where the writing process was interrupted. The S-notation can be automatically generated from the keystroke-logging data and has become a standard in the representation of the non-linearity in writing processes.  Figure 2 shows an example of the S-notation.  The text is taken from an experiment with master students Multilingual Professional Communica-tion who were asked to write a (Dutch) tweet about a conference (VWEC).  The S-notation shows the final product and the process needed.    Volgend?jaar?organiseert?{#|4}3VWEC?een?{boeiend?|9}8congres?[over?']1|1[met?als?thema|10]9{over}10?'Corporate?Communication{'|8}7.[.]2|2[?Wat?levert?het?op?'.|7]6?Blijf?[ons?volgen?op|5]4{op?de?hoogte?via|6}5?www.vwec2012.be.|3? Figure 2. Example of S-notation.  The following conventions are used in S-notation: ? |i: a break in the writing process with sequential number i; ? {insertion}i: an insertion occurring after break i;  ? [deletion]i: a deletion occurring after break i. The example in Figure 2 can be read as follows:  
The writer formulates in one segment ?Volgend jaar organiseert VWEC een congres over? (?Next year VWEC organises a conference on?).  She decides to delete ?over? (index 1) and then adds the remainder of her first draft ?met al thema ?Corporate Communication.  Wat levert het op??.?  (?themed ?Corporate Communication.  What is in it for us??.?)  She deletes a full stop and ends with ?Blijf ons volgen op www.vwec2012.be.? (?Follow us on www.vwec2012.be?).  The third revision is the addition of the hashtag before VWEC.  Then she rephrases ?ons volgen op? into ?op de hoogte via.?  She notices that her tweet is too long (max. 140 characters) and she decides to delete the subtitle of the conference.  She adds the adjective ?boeiend? (?interesting?) to conference and ends by deleting ?met al thema? (?themed?). 3 Intermediate level At the intermediate level, Inputlog data can also be used to analyze data at the digraph level, for instance, to study interkey intervals (or digraph latency) in relation to typing speed, keyboard efficiency of touch typists and others, dyslexia and keyboard fluency, biometric verification etc.  For this type of research, logging data can be leveled up to an intermediate level in which two consecutive events are treated as a unit (e.g., un-ni-it). Grabowski?s research on the internal structure of students? keyboard skills in different writing tasks is a case in point (Grabowski, 2008).  He studied whether there are patterns of overall keyboard behavior and whether such patterns are stable across different (copying) tasks.  Across tasks, typing speed turned out to be the most stable characteristic of a keyboard user.  Another example is the work by Nottbush and his colleagues.  Focusing on linguistic aspects of interkey intervals, their research (Nottbusch, 2010; Sahel, Nottbusch, Grimm, & Weingarten, 2008) shows that the syllable boundaries within words have an effect on the temporal keystroke succession.  Syllable boundaries lead to increased interkey intervals at the digraph level.  In recent research Inputlog data has also been used to analyze typing errors at this level (Van Waes & Leijten, 2010).  As will be demonstrated in the next section, typing errors complicate the analysis of logging data at the word and sentence level because the linear reconstruction is disrupted.  For this purpose a large experimental corpus based on a controlled copying task was 
3
analyzed, focusing on five digraphs with different characteristics (frequency, keyboard distribution, left-right coordination).  The results of a multilevel analysis show that there is no correlation between the frequency of a digraph and the chance that a typing error occurs.  However, typing errors show a limited variation: pressing the adjacent key explains more than 40% of the errors, both for touch typists and others; the chance that a typing error is made is related to the characteristics of the digraph, and the individual typing style.  Moreover, the median pausing time preceding a typing error tends to be longer than the median interkey transitions of the intended digraph typed correctly.  These results illustrate that further research should make it possible to identify and isolate typing errors in logged process data and build an algorithm to filter them during data preparation.  This would benefit parsing at a later stage (see section 4).  4 Flow of linguistic analyses As explained above, writing process data gathered via the traditional keystroke-logging tools are represented at the character level and produce non-linear data (containing sentence fragments, unfinished sentences/words and spelling errors).  These two characteristics are the main obstacles that we need to cope with to analyze writing process data on a higher level.  In this section we explain the flow of the linguistic analyses.  4.1 Step 1 - aggregate letter to word level Natural Language Processing tools, such as part-of-speech taggers, lemmatizers and chunkers are trained on (completed) sentences and words.  Therefore, to use the standard NLP tools to enrich the process data with linguistic information, in a first step, words, word groups, and sentences are extracted from the process data.   The S-notation was used as a basis to further segment the data into sentences and tokenize them.  A dedicated sentence segmenting and tokenizer module was developed to conduct this process.  This dedicated module can cope with the specific S-notation annotations such as insertion, deletion and break markers.   
4.2 Step 2 ? parsing the S-notation As mentioned before, standard NLP tools are designed to work with clean, grammatically correct text.  We thus decided to treat word-level revisions differently than higher-level revisions and to distinguish deleted fragments from the final writing product. We developed a parser that extracts three types of data from the S-notation: word-level revisions, deleted fragments, and the final writing product.  The word-level revisions can be extracted from the S-notation by retaining all words with word-internal square or curly brackets (see excerpt 1).  (1 - word level revision)  Delet[r]ion    incorrect: Deletrion; correct: deletion   In{s}ertion    incorrect: Inertion; correct: insertion  Conceptually, the deleted fragments can be extracted from the S-notation by retaining only the words and phrases that are surrounded by word-external square brackets (2); and the final product data can be obtained by deleting everything in between square brackets from the S-notation.  In practice, the situation is more complicated as insertions and deletions can be nested. An example of the three different data types extracted from the S-notation is presented in the excerpt below.  To facilitate the readability of the resulting data, the indices are omitted (3).   (2 - deleted fragments)  Volgend?jaar?organiseert?{#}VWEC?een?{boeiend?}congres?[over?'][met?als?thema]{over}?'Corporate?Communication{'}.[.][?Wat?levert?het?op?'.]?Blijf?[ons?volgen?op]{op?de?hoogte?via|}?www.vwec2012.be.|?  (3 - final writing product)  Volgend?jaar?organiseert?{#}VWEC?een?{boeiend?}congres?[over?'][met?als?thema]{over}?'Corporate?Communication{'}.[.][?Wat?levert?het?op?'.]?Blijf?[ons?volgen?op]{op?de?hoogte?via|}?www.vwec2012.be.|?  English translation Next year #VWEC organises an interesting conference about Corporate Communication. Follow us on www.vwec2012.be  In sum, the output of Inputlog data is segmented in sentences and tokenized.  The S-notation is divided into three types of revisions 
4
and the within-word typing errors are excluded from further analyses. Although the set-up of the Inputlog extension is largely language-independent, the NLP tools used are language-dependent.  As proof-of-concept, we provide evidence from English and Dutch (See Figure 3).   
 Figure 3 Flow of the linguistic analyses.  4.3 Step 3 ? enriching process data with linguistic information As standard NLP tools are trained on clean data, these tools are not suited for processing input containing spelling errors.  Therefore, we only enrich the final product data and the deleted fragments with different kinds of linguistic annotations.  As part-of-speech taggers typically use the surrounding local context to determine the proper part-of-speech tag for a given word (typically a window of two to three words and/or tags is used), the deletions in context are extracted from the S-notation to be processed by the part-of-speech tagger.  The deleted fragments in context consist of the whole text string without the insertions and are only used to optimize the results of the linguistic annotation.  (4 - deleted fragments in context)  Volgend?jaar?organiseert?{#}VWEC?een?{boeiend?}congres?[over?'][met?als?thema]{over}?'Corporate?Communication{'}.[.][?Wat?levert?het?op?'.]?Blijf?[ons?volgen?op]{op?de?hoogte?via|}?www.vwec2012.be.|?  For the shallow linguistic analysis, we used the LT3 shallow parsing tools suite consisting of:  ? a part-of-speech tagger (LeTsTAG),  ? a lemmatizer (LeTsLEMM), and  ? a chunker (LeTsCHUNK). 
The LT3 tools are platform-independent and hence run on Windows. Part of speech tags The English PoS tagger uses the Penn Treebank tag set, which contains 45 distinct tags.  The Dutch part-of-speech tagger uses the CGN tag set codes (Van Eynde, Zavrel, & Daelemans, 2000), which is characterized by a high level of granularity.  Apart from the word class, the CGN tag set codes a wide range of morpho-syntactic features as attributes to the word class.  In total, 316 distinct tags are discerned.  Lemmata During lemmatization, for each orthographic token, the base form (lemma) is generated.  For verbs, the base form is the infinitive; for most other words, this base form is the stem, i.e., the word form without inflectional affixes.  The lemmatizers make use of the predicted PoS codes to disambiguate ambiguous word forms, e.g., Dutch ?landen? can be an infinitive (base form ?landen?) or plural form of a noun (base form ?land?).  The lemmatizers were trained on the English and Dutch parts of the Celex lexical database respectively (Baayen, Piepenbrock, & van Rijn, 1993).   Chunks During text chunking syntactically related consecutive words are combined into non-overlapping, non-recursive chunks on the basis of a fairly superficial analysis.  The chunks are represented by means of IOB-tags.  In the IOB-tagging scheme, each token belongs to one of the following three types: I (inside), O (outside) and B (begin); the B- en I-tags are followed by the chunk type, e.g., B-VP, I-VP.  We adapted the IOB-tagging scheme and added end tag (E) to explicitly mark the end of a chunk.  Accuracy sores of part-of-speech taggers and lemmatizers typically fluctuate around 97% to 98%; accuracy scores of 95% to 96% are obtained for chunking. After annotation, the final writing product, deleted fragments, and word-level corrections are aligned and the indices are restored.  Figures 4 and 5 show how we enriched the logged process data with different kinds of linguistic information: lemmata, part-of-speech tags, and chunk boundaries.   We further added some word-level annotations on the final writing product and the deletions, 
5
viz., syllable boundaries and word frequencies (see last two columns in Figures 4 and 5). Syllable boundaries: The syllabification tools were trained on Celex (http://lt3.hogent.be/en/tools/timbl-syllabification).  Syllabification was approached as a classification task: a large instance base of syllabified data is presented to a classification algorithm, which automatically learns from it the patterns needed to syllabify unseen data.  Accuracy scores for syllabification reside in the range of 92% to 95%.  Word Frequency Frequency lists for Dutch and English were compiled on the basis of Wikipedia pages, which were extracted from the XML dump of the Dutch and English Wikipedia of December 2011.  We used the Wikipedia Extractor developed by Medialab4 to extract the text from the wiki files.  The Wikipedia text files were further tokenized and enriched with part-of-speech tags and                                                            4 http://medialab.di.unipi.it/wiki/Wikipedia_Extractor 
 Figure 4 Final writing product and word-level revisions enriched with linguistic information. 
Figure 5 Deleted fragments enriched with linguistic information. 
6
lemmata.  The Wikipedia frequency lists can thus group different word forms belonging to one lemma.  The current version of the Dutch frequency list has been compiled on the basis of nearly 100 million tokens coming from 395,673 Wikipedia pages, which is almost half of the Dutch Wikipedia dump of December 2011.  Frequencies are presented as absolute frequencies. 4.4 Step 4 - combining process data with linguistic information In a final step we combine the process data with the linguistic information.  Based on the time information provided by Inputlog, researchers can calculate various measures, e.g., length of a pause within, before and after lemmata, part-of-speech tags, and at chunk boundaries.  As an example Table 1 shows the mean pausing time before and after the adjectives and nouns in the tweet.  Of course, this is a very small-scale example, but it shows the possibilities of exploring writing process data from a linguistic perspective.   mean pause before mean pause  after mean pause within  ADJ 1880 671 148 NOUN 728 1455 232     B (begin) 1412 1174 164 E (end) 685 1353 148 I  (inside) 730 1034 144 Table 1. Example of process data and linguistic information  In this example the mean pausing time before adjectives is twice as long as before nouns.  The pausing time after such a segment shows the opposite proportion.  Also pauses in the beginning of chunks are more than twice as long as in the middle of a chunk.  5 Future research In this paper we presented how writing process data can be enriched with linguistic information.  The annotated output facilitates the linguistic analysis of the logged data and provides a valuable basis for more linguistically-oriented writing process research.  We hope that this perspective will further enrich writing process research. 
5.1 Additional annotations and analyses In a first phase we only focused on English and Dutch, but the method can be easily applied to other languages as well provided that the linguistic tools are available for a Windows platform.  For the moment, the linguistic annotations are limited to part-of-speech tags, lemmata, chunk information, syllabification, and word frequency information, but can be extended, e.g., by n-gram frequencies to capture collocations.  By aggregating the logged process data from the character level (keystroke) to the word level, general statistics (e.g., total number of deleted or inserted words, pause length before nouns preceded by an adjective or not) can be generated easily from the output of Inputlog as well. 5.2 Technical flow of Inputlog & linguistic tools At this point Inputlog is a standalone program that needs to be installed on the same local machine that is used to produce the texts.  This makes sense as long as the heaviest part of the work is the logging of a writing process.  However, extending the scope from a character based analysis device to a system that supplements fine-grained production and process information to various NLP tools is a compelling reason to rethink the overall architecture of the software. It is not feasible to install the necessary linguistic software with its accompanying databases on every device.  By decoupling the capturing part from the analytics a research group will have a better view on the use of its hard- and software resources while also allowing to solve potential copyright issues.  Inputlog is now pragmatically Windows-based, but with the new architecture any tool on any OS will be capable to exchange data and results.  It will be possible to add an NLP module that receives Inputlog data through a communication layer.  A workflow procedure then presents the data in order to the different NLP packages and collects the final output.  Because all data traffic is done with XML files, cooperation between software with different creeds becomes conceivable.  Finally, the module has an administration utility handling the necessary user authentication and permits.   
7
Acknowledgements This study is partially funded by a research grant of the Flanders Research Foundation (FWO 2009-2012). References Baayen, R. H., R. Piepenbrock, & H. van Rijn. (1993). The CELEX lexical database on CD-ROM. Philadelphia, PA: Linguistic Data Consortium. Baayen, R. H., Piepenbrock, R., & van Rijn, H. (1993). The CELEX lexical database on CD-ROM. Philadelphia, PA: Linguistic Data Consortium. Berninger, V. (2012). Past, Present, and Future Contributions of Cognitive Writing Research to Cognitive Psychology: Taylor and Francis. Grabowski, J. (2008). The internal structure of university students? keyboard skills. Journal of Writing Research, 1(1), 27-52.  Jakobsen, A. L. (2006). Translog: Research methods in translation. In K. P. H. Sullivan & E. Lindgren (Eds.), Computer Keystroke Logging and Writing: Methods and Applications (pp. 95-105). Oxford: Elsevier. Kollberg, P., & Severinson Eklundh, K. (2002). Studying writers' revising patterns with S-notation analysis. In T. Olive & C. M. Levy (Eds.), Contemporary Tools and Techniques for Studying Writing (pp. 89-104). Dordrecht: Kluwer Academic Publishers. Leijten, M., & Van Waes, L. (2006). Inputlog: New Perspectives on the Logging of On-Line Writing. In K. P. H. Sullivan & E. Lindgren (Eds.), Computer Keystroke Logging and Writing: Methods and Applications (pp. 73-94). Oxford: Elsevier. Nottbusch, G. (2010). Grammatical planning, execution, and control in written sentence production. Reading and Writing, 23(7), 777-801.  Sahel, S., Nottbusch, G., Grimm, A., & Weingarten, R. (2008). Written production of German compounds: Effects of lexical frequency and semantic transparency. Written Language and Literacy, 11(2), 211-228.  Str?mqvist, S., Holmqvist, K., Johansson, V., Karlsson, H., & Wengelin, A. (2006). What keystroke logging can reveal about writing. In K. P. H. Sullivan & E. Lindgren (Eds.), Computer Keystroke Logging and Writing: Methods and Applications (pp. 45-71). Oxford: Elsevier. Sullivan, K. P. H., & Lindgren, E. (2006). Computer Key-Stroke Logging and Writing. Oxford: Elsevier Science. Van Eynde, F., Zavrel, J., & Daelemans, W. (2000). Part of Speech Tagging and Lemmatisation for the Spoken Dutch Corpus. Paper presented at the Proceedings of the second International Conference on Language Resources and Evaluation (LREC), Athens, Greece. 
Van Waes, L., & Leijten, M. (2010). The dynamics of typing errors in text production. Paper presented at the SIG Writing 2010, 12th International Conference of the Earli Special Interest Group on Writing, Heidelberg.  Wengelin, A., Torrance, M., Holmqvist, K., Simpson, S., Galbraith, D., Johansson, V., & Johansson, R. (2009). Combined eyetracking and keystroke-logging methods for studying cognitive processes in text production. Behavior Research Methods, 41(2), 337-351.    
8
