Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 206?209,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
JU: A Supervised Approach to Identify Semantic Relations from Paired 
Nominals 
 
Santanu Pal         Partha Pakray             Dipankar Das          Sivaji Bandyopadhyay
Department of Computer Science & Engineering, Jadavpur University, Kolkata, India 
santanupersonal1@gmail.com,parthapakray@gmail.com,     
dipankar.dipnil2005@gmail.com,sivaji_cse_ju@yahoo.com 
Abstract 
This article presents the experiments carried 
out at Jadavpur University as part of the 
participation in Multi-Way Classification of 
Semantic Relations between Pairs of Nomi-
nals in the SemEval 2010 exercise. Separate 
rules for each type of the relations are iden-
tified in the baseline model based on the 
verbs and prepositions present in the seg-
ment between each pair of nominals. Inclu-
sion of WordNet features associated with 
the paired nominals play an important role 
in distinguishing the relations from each 
other. The Conditional Random Field (CRF) 
based machine-learning framework is 
adopted for classifying the pair of nominals.  
Application of dependency relations, 
Named Entities (NE) and various types of 
WordNet features along with several com-
binations of these features help to improve 
the performance of the system. Error analy-
sis suggests that the performance can be im-
proved by applying suitable strategies to 
differentiate each paired nominal in an al-
ready identified relation. Evaluation result 
gives an overall macro-averaged F1 score of 
52.16%.     
1 Introduction 
Semantic Relations describe the relations between 
concepts or meanings that are crucial but hard to 
identify. The present shared task aims to develop 
the systems for automatically recognizing semantic 
relations between pairs of nominals. Nine relations 
such as Cause-Effect, Instrument-Agency, Product-
Producer, Content-Container, Entity-Origin, En-
tity-Destination, Component-Whole, Member-
Collection and Message-Topic are given for Se-
mEval-2010 Task #8 (Hendrix et al, 2010). The 
relation that does not belong to any of the nine re-
lations is tagged as Other. The first five relations 
also featured in the previous SemEval-2007 Task 
#4.  
The present paper describes the approach of 
identifying semantic relations between pair of 
nominals. The baseline system is developed based 
on the verbs and prepositions present in the senten-
tial segment between the two nominals. Some 
WordNet (Miller, 1990) features are also used in 
the baseline for extracting the relation specific at-
tributes (e.g. Content type hypernym feature used 
for extracting the relation of Content-Container). 
The performance of the baseline system is limited 
due to the consideration of only the verb and 
preposition words in between the two nominals 
along with a small set of WordNet features. Hence, 
the Conditional Random Field (CRF) (McCallum 
et al, 2001) based framework is considered to ac-
complish the present task. The incorporation of 
different lexical features (e.g. WordNet hyponyms, 
Common-parents, distance), Named Entities (NE) 
and syntactic features (direct or transitive depend-
ency relations of parsing) has noticeably improved 
the performance of the system. It is observed that 
nominalization feature plays an effective role for 
identifying as well as distinguishing the relations. 
The test set containing 2717 sentences is evaluated 
against four different training sets. Some of the 
relations, e.g. Cause-Effect, Member-Collection 
perform well in comparison to other relations in all 
the four test results. Reviewing of the confusion 
matrices suggests that the system performance can 
be improved by reducing the errors that occur in 
distinguishing the two individual nominals in each 
relation. 
The rest of the paper is organized as follows. 
The pre-processing of resources and the baseline 
system are described in Section 2 and Section 3 
respectively. Development of CRF-based model is 
discussed in Section 4. Experimental results along 
206
with error analysis are specified in Section 5. Fi-
nally Section 6 concludes the paper. 
2 Resource Pre-Processing 
The annotated training corpus containing 8000 sen-
tences was made available by the respective task 
organizers. The objective is to evaluate the effec-
tiveness of the system in terms of identifying se-
mantic relations between pair of nominals. The 
rule-based baseline system is evaluated against the 
whole training corpus. But, for in-house experi-
ments regarding CRF based framework, the devel-
opment data is prepared by randomly selecting 500 
sentences from the 8000 training sentences. Rest 
7500 sentences are used for training of the CRF-
model. The format of one example entry in training 
file is as follows.  
"The system as described above has its greatest 
application in an arrayed <e1>configuration</e1> 
of antenna <e2>elements</e2>."  
Component-Whole (e2, e1)  
Comment: Not a collection: there is structure 
here, organisation. 
    Each of the training sentences is annotated by 
the paired nominals tagged as <e1> and <e2>. 
The relation of the paired nominals and a comment 
portion describing the detail of the input type fol-
lows the input sentence. 
The sentences are filtered and passed through 
Stanford Dependency Parser (Marneffe et al, 
2006) to identify direct as well as transitive de-
pendencies between the nominals. The direct de-
pendency is identified based on the simultaneous 
presence of both nominals, <e1> as well as <e2> 
in the same dependency relation whereas the tran-
sitive dependencies are verified if <e1> and <e2> 
are connected via one or more intermediate de-
pendency relations.  
Each of the sentences is passed through a Stan-
ford Named Entity Recognizer (NER)1 for identi-
fying the named entities. The named entities are 
the useful hints to separately identify the relations 
like Entity-Origin and Entity-Destination from 
other relations as the Origin and Destination enti-
ties are tagged by the NER frequently than other 
entities. 
Different seed lists are prepared for different 
types of verbs. For example, the lists for causal 
                                                          
1  http://nlp.stanford.edu/software/CRF-NER.shtml 
and motion verbs are developed by processing the 
XML files of English VerbNet (Kipper-Schuler, 
2005). The list of the causal and motion verbs are 
prepared by collecting the member verbs if their 
corresponding class contain the semantic type  
?CAUSE? or ?MOTION?. The other verb lists are 
prepared manually by reviewing the frequency of 
verbs in the training corpus. The WordNet stem-
mer is used to identify the root forms of the verbs.   
3 Baseline Model 
The baseline model is developed based on the 
similarity clues present in the phrasal pattern con-
taining verbs and prepositions. Different rules are 
identified separately for the nine different rela-
tions. A few WordNet features such as hypernym, 
meronym, distance and Common-Parents are 
added into the rule-based baseline model. Some of 
the relation specific rules are mentioned below. 
For example, if any of the nominals contain 
their meronym property as ?whole? and if the hy-
pernym tree for one of the nominals contains the 
word ?whole?, the relation is identified as a Com-
ponent-Whole relation.   But, the ordering of the 
nominals <e1> and <e2> is done based on the 
combination of ?has?, ?with? and ?of? with other 
word level components.  
The relations Cause-Effect, Entity-Destination 
are identified based on the causal verbs (cause, 
lead etc.) and motion verbs (go, run etc.) respec-
tively. One of the main criteria for extracting these 
relations is to verify the presence of causal and 
motion verbs in between the text segment of <e1> 
and <e2>. Different types of specific relaters (as, 
because etc.) are identified from the text segment 
as well. It is observed that such specific causal re-
laters help in distinguishing other relations from 
Cause-Effect.  
If one of the nominals is described as instrument 
type in its hypernym tree, the corresponding rela-
tion is identified as Instrument-Agency but the base 
level filtering criterion is applied if both the nomi-
nals belong to instrument type. On the other hand, 
if any of the nominals belong to the hypernym tree 
as content or container or hold type, it returns the 
relation Content-Container as a probable answer. 
Similarly, if both of them belong to the same type, 
the condition is fixed as false criterion for that par-
ticular category. The nominals identified as the 
part of collective nouns and associated with 
207
phrases like "of", "in", "from" between <e1> and 
<e2> contain the relation of Member-Collection. 
The relations e.g. Message-Topic uses seed list of 
verbs that satisfy the communication type in the 
hypernym tree and Product-Producer relation con-
cerns the hypernym feature as Product type. 
But, the identification of the proper ordering of 
the entities in the relation, i.e., whether the relation 
is valid between <e1, e2> or <e2, e1> is done by 
considering the passive sense of the sentence with 
the help of the keyword ?by? as well as by some 
passive dependency relations.  
The evaluation of the rule-based baseline sys-
tem on the 8000 training data gives an average F1-
score of 22.45%. The error analysis has shown that 
use of lexical features only is not sufficient to ana-
lyze the semantic relation between two nominals 
and the performance can be improved by adopting 
strategies for differentiating the nominals of a par-
ticular pair. 
4 CRF-based Model 
To improve the baseline system performance, 
CRF-based machine learning framework 
(McCallum et al, 2001) is considered for classify-
ing the semantic relations that exist among the or-
dered pair of nominals. Identification of appropri-
ate features plays a crucial role in any machine-
learning framework. The following features are 
identified heuristically by manually reviewing the 
corpus and based on the frequency of different 
verbs in different relations. 
? 11 WordNet features (Synset, Synonym, 
Gloss, Hyponym, Nominalization, Holo-
nym, Common-parents, WordNet distance, 
Sense ID, Sense count, Meronym) 
? Named Entities (NE) 
? Direct Dependency 
? Transitive Dependency 
? 9 separate verb list containing relation spe-
cific verbs, each for 9 different semantic 
relations  
Different singleton features and their combinations 
are generated from the training corpus. Instead of 
considering the whole sentence as an input to the 
CRF-based system, only the pairs of nominals are 
passed for classification. The previous and next 
token of the current token with respect to each of 
the relations are added in the template to identify 
their co-occurrence nature that in turn help in the 
classification process. Synsets containing synony-
mous verbs of the same and different senses are 
considered as individual features.   
4.1 Feature Analysis  
The importance of different features varies accord-
ing to the genre of the relations. For example, the 
Common-parents WordNet feature plays an effec-
tive role in identifying the Content-Container and 
Product-Producer relations. If the nominals in a 
pair share a common Sense ID and Sense Count  
then this is considered as a feature. The combina-
tion of multiple features in comparison with a sin-
gle feature generally shows a reasonable perform-
ance enhancement of the present classification sys-
tem. Evaluation on the development data for the 
various feature combinations has shown that the 
nominalization feature effectively performs for all 
the relations. WordNet distance feature is used for 
capturing the relations like Content-Container and 
Component-Whole. The direct and transitive de-
pendency syntactic features contribute in identify-
ing the relation as well as identify the ordering of 
the entities <e1> and <e2> in the relation. 
The Named-Entity (NE) relation plays an impor-
tant role in distinguishing the relations, e.g., Entity-
Origin and Entity-Destination from other relations. 
The person tagged NEs have been excluded from 
the present task as such NEs are not present in the  
Entity-Origin and Entity-Destination relations. It 
has been observed that the relation specific verbs 
supply useful clues to the training phrase for dif-
ferentiating relations among nominals.   
The system is trained on 7500 sentences and the 
evaluation is carried out on 500 development sen-
tences achieving an F1-Score of 57.56% F1-Score. 
The tuning on the development set has been carried 
out based on the performance produced by the 
individual features that effectively contains 
WordNet relations. In addition to that, the 
combination of dependency features with verb 
feature plays an contributory role on the system 
evaluation results. 
208
Table 1: Precision, Recall and F1-scores (in %) of semantic relations in (9+1) way directionality-based evaluation 
 
5 Experimental Results 
The active feature list is prepared after achieving 
the best possible F1-score of 61.82% on the devel-
opment set of 500 sentences. The final training of 
the CRF-based model is carried out on four differ-
ent sets containing 1000, 2000, 4000 and 8000 sen-
tences. These four training sets are prepared by 
extracting sentences from the beginning of the 
training corpus and the final evaluation is carried 
out on 2717 test sentences as provided by the or-
ganizers. The results on the four test sets termed as 
TD1, TD2, TD3 and TD4 are shown in Table 1. 
The error analysis is done based on the information 
present in the confusion matrices. The fewer occur-
rence of Entity-Destination (e2, e1) instance in the 
training corpus plays the negative role in identify-
ing the relation. Mainly, the strategy used for as-
signing the order among the entities, i.e., either 
<e1, e2> or <e2, e1> in the already identified re-
lations is the main cause of errors of the system. 
The Entity-Origin, Product-Producer and Mes-
sage-Topic relations suffer from overlapping prob-
lem with other relations. Each of the tested nomi-
nal pairs is tagged with more than one relation. 
But, selecting the first output tag produced by CRF 
is considered as the final relational tag for each of 
the nominal pairs. Hence, a distinguishing strategy 
needs to be adopted for fine-grained selection.  
6 Conclusion and Future Task 
In our approach to automatic classification of se-
mantic relations between nominals, the system 
achieves its best performance using the lexical fea-
ture such as nominalization of WordNet and syn-
tactic information such as dependency relations. 
These facts lead us to conclude that semantic fea-
tures from WordNet, in general, play a key role in 
the classification task. The present system aims for 
assigning class labels to discrete word level entities 
but the context feature is not taken into considera-
tion. The future task is to evaluate the performance 
of the system by capturing the context present be-
tween the pair of nominals.  
References  
Andrew McCallum, Fernando Pereira and John 
Lafferty. 2001. Conditional Random Fields: Prob-
abilistic Models for Segmenting and labeling Se-
quence Data. ICML-01, 282 ? 289. 
George A. Miller. 1990. WordNet: An on-line lexical 
database. International Journal of Lexicography, 
3(4): 235?312. 
Karin Kipper-Schuler. 2005. VerbNet. A broad-
coverage, comprehensive verb lexicon. Ph.D. thesis, 
University of Pennsylvania, Philadelphia, PA. 
Marie-Catherine de Marneffe, Bill MacCartney, and 
Christopher D. Manning. 2006. Generating Typed 
Dependency Parses from Phrase Structure Parses. 
(LREC 2006). 
Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva, 
Preslav Nakov, Diarmuid ?O S?eaghdha, Sebastian 
Padok , Marco Pennacchiotti, Lorenza Romano, Stan 
Szpakowicz. 2010. SemEval-2010 Task 8: Multi-
Way Classification of Semantic Relations Between 
Pairs of Nominals. 5th SIGLEX Workshop. 
TD1 TD2 TD3 TD4 Relations 
Prec. Recall F1 Prec. Recall F1 Prec. Recall F1 Prec. Recall F1 
Cause-Effect 76.33 65.85 70.70 78.55 65.85 71.64 79.86 68.90 73.98 79.26 72.26 75.60
Component-Whole 49.25 31.41 38.36 48.76 37.82 42.60 50.77 42.31 46.15 58.40 49.04 53.31
Content-Container 31.35 30.21 30.77 37.93 34.38 36.07 40.65 32.81 36.31 51.15 34.90 41.49
   Entity-Destination 37.58 62.67 46.98 43.43 63.36 51.53 43.09 63.01 51.18 
 
47.07 60.62 52.99
Entity-Origin 62.50 46.51 53.33 61.95 49.22 54.86 60.18 52.71 56.20 64.02 53.10 58.05
Instrument-Agency 19.46 23.08 21.11 21.18 27.56 23.96 26.43 23.72 25.00 32.48 24.36 27.84
Member-Collection 50.97 67.81 58.20 54.82 70.82 61.80 59.93 72.53 65.63 66.80 71.67 69.15
Message-Topic 41.70 41.38 41.54 50.23 42.15 45.83 52.81 46.74 49.59 57.78 49.81 53.50
Product-Producer 52.94 7.79 13.58 48.94 9.96 16.55 59.09 16.88 26.26 53.17 29.00 37.54
Other 21.10 27.09 23.72 24.48 33.70 28.36 26.28 37.44 30.88 26.64 42.07 32.62
Average F1 score 42.62 44.98 47.81 52.16 
209
Proceedings of the Multiword Expressions: From Theory to Applications (MWE 2010), pages 37?45,
Beijing, August 2010
Automatic Extraction of Complex Predicates in Bengali  
Dipankar Das     Santanu Pal      Tapabrata Mondal       Tanmoy Chakraborty   
 
  Sivaji Bandyopadhyay 
Department of Computer Science and Engineering 
Jadavpur University 
dipankar.dipnil2005@gmail.com, 
santanupersonal1@gmail.com, 
tapabratamondal@gmail.com, its_tanmoy@yahoo.co.in, 
sivaji_cse_ju@yahho.com 
 
 
Abstract 
This paper presents the automatic ex-
traction of Complex Predicates (CPs) 
in Bengali with a special focus on 
compound verbs (Verb + Verb) and 
conjunct verbs (Noun /Adjective + 
Verb). The lexical patterns of com-
pound and conjunct verbs are extracted 
based on the information of shallow 
morphology and available seed lists of 
verbs. Lexical scopes of compound and 
conjunct verbs in consecutive sequence 
of Complex Predicates (CPs) have 
been identified. The fine-grained error 
analysis through confusion matrix 
highlights some insufficiencies of lexi-
cal patterns and the impacts of different 
constraints that are used to identify the 
Complex Predicates (CPs). System 
achieves F-Scores of 75.73%, and 
77.92% for compound verbs and 
89.90% and 89.66% for conjunct verbs 
respectively on two types of Bengali 
corpus.      
1 Introduction 
Complex Predicates (CPs) contain [verb] + 
verb (compound verbs) or [noun/ 
adjective/adverb] +verb (conjunct verbs) 
combinations in South Asian languages (Hook, 
1974). To the best of our knowledge, Bengali  
 
 
is not only a language of South Asia but also 
the sixth popular language in the World 1 , 
second in India and the national language of 
Bangladesh. The identification of Complex 
Predicates (CPs) adds values for building 
lexical resources (e.g. WordNet (Miller et al, 
1990; VerbNet (Kipper-Schuler, 2005)), 
parsing strategies and machine translation 
systems.  
Bengali is less computerized compared to 
English due to its morphological enrichment. 
As the identification of Complex Predicates 
(CPs) requires the knowledge of morphology, 
the task of automatically extracting the Com-
plex Predicates (CPs) is a challenge. Complex 
Predicates (CPs) in Bengali consists of two 
types, compound verbs (CompVs) and conjunct 
verbs (ConjVs). 
The compound verbs (CompVs) (e.g. ? ?? 
? ?? mere phela ?kill?, ???? ???? bolte laglo 
?started saying?) consist of two verbs. The first 
verb is termed as Full Verb (FV) that is present 
at surface level either as conjunctive participial 
form -e ?e or the infinitive form -?  ?te. The 
second verb bears the inflection based on 
Tense, Aspect and Person. The second verbs 
that are termed as Light Verbs (LV) are 
polysemous, semantically bleached and 
confined into some definite candidate seeds 
(Paul, 2010).  
On the other hand, each of the Bengali con-
junct verbs (ConjVs) (e.g. ???? ??? bharsha 
                                                 
1http://www.ethnologue.com/ethno_docs/distributio
n.asp?by=size 
37
kara ?to depend?, ???? ??? jhakjhak kara ?to 
glow?) consists of noun or adjective followed 
by a Light Verb (LV). The Light Verbs (LVs) 
bear the appropriate inflections based on 
Tense, Aspect and Person.   
According to the definition of multi-word 
expressions (MWEs)(Baldwin and Kim, 2010), 
the absence of conventional meaning of the 
Light Verbs in Complex Predicates (CPs) 
entails us to consider the Complex Predicates 
(CPs) as MWEs (Sinha, 2009). But, there are 
some typical examples of Complex Predicates 
(CPs), e.g. ? ?? ??? dekha kara ?see-do? that 
bear the similar lexical pattern as Full Verb 
(FV)+ Light Verb (LV) but both of the Full 
Verb (FV) and Light Verb (LV) loose their 
conventional meanings and generate a 
completely different meaning (?to meet? in this 
case).  
In addition to that, other types of predicates 
such as ???? ? ? niye gelo ?take-go? (took and 
went), ???? ? ? diye gelo ?give-go? (gave and 
went) follows the similar lexical patterns 
FV+LV as of Complex Predicates (CPs) but 
they are not mono-clausal. Both the Full Verb 
(FV) and Light Verb (LV) behave like 
independent syntactic entities and they belong 
to non-Complex Predicates (non-CPs). The 
verbs are also termed as Serial Verb (SV) 
(Mukherjee et al, 2006). 
Butt (1993) and Paul (2004) have also 
mentioned the following criteria that are used 
to check the validity of complex predicates 
(CPs) in Bengali. The following cases are the 
invalid criteria of complex predicates (CPs). 
1. Control Construction (CC): ????? ??? 
likhte bollo ?asked to write?, ????? ???? 
??? likhte badhyo korlo ?forced to 
write? 
2. Modal Control Construction (MCC): 
? ?? ??? jete hobe ?have to go? ? ?? ??? 
khete hobe ?have to eat? 
3. Passives (Pass) : ??? ??? dhora porlo 
?was caught?, ???? ?? mara holo ?was 
beaten? 
4. Auxiliary Construction (AC): ??? ??? 
bose ache ?is sitting?, ???? ??? niye chilo 
?had taken?. 
Sometimes, the successive sequence of the 
Complex Predicates (CPs) shows a problem of 
deciding the scopes of individual Complex 
Predicates (CPs) present in that sequence. For 
example the sequence, u?? ??? ? ???? uthe pore 
dekhlam ?rise-wear-see? (rose and saw) seems 
to contain two Complex Predicates (CPs) (u?? 
??? uthe pore ?rose? and ??? ? ???? pore 
dekhlam ?wore and see?). But there is actually 
one Complex Predicate (CP). The first one u?? 
??? uthe pore ?rose? is a compound verb 
(CompV) as well as a Complex Predicate (CP). 
Another one is ? ???? dekhlam ?saw? that is a 
simple verb. As the sequence is not mono-
clausal, the Complex Predicate (CP) u?? ??? 
uthe pore ?rose? associated with ? ???? dekhlam 
?saw? is to be separated by a lexical boundary. 
Thus the determination of lexical scopes of 
Complex Predicates (CPs) from a long con-
secutive sequence is indeed a crucial task.      
 The present task therefore not only aims to 
extract the Complex Predicates (CPs) 
containing compound and conjunct verbs but 
also to resolve the problem of deciding the 
lexical scopes automatically. The compound 
verbs (CompVs) and conjunct verbs (ConjVs) 
are extracted from two separate Bengali 
corpora based on the morphological 
information (e.g. participle forms, infinitive 
forms and inflections) and list of Light Verbs 
(LVs). As the Light Verbs (LVs) in the 
compound verbs (CompVs) are limited in 
number, fifteen predefined verbs (Paul, 2010) 
are chosen as Light Verbs (LVs) for framing 
the compound verbs (CompVs).  A manually 
prepared seed list that is used to frame the 
lexical patterns for conjunct verbs (ConjVs) 
contains frequently used Light Verbs (LVs).  
An automatic method is designed to identify 
the lexical scopes of compound and conjunct 
verbs in the long sequences of Complex 
Predicates (CPs). The identification of lexical 
scope of the Complex Predicates (CPs) 
improves the performance of the system as the 
number of identified Complex Predicates 
(CPs) increases.  
Manual evaluation is carried out on two 
types of Bengali corpus. The experiments are 
carried out on 800 development sentences 
from two corpora but the final evaluation is 
carried out on 1000 sentences. Overall, the 
system achieves F-Scores of 75.73%, and 
77.92% for compound verbs and 89.90% and 
89.66% for conjunct verbs respectively.  
38
The error analysis shows that not only the 
lexical patterns but also the augmentation of 
argument structure agreement (Das, 2009), the 
analysis of Non-MonoClausal Verb (NMCV) or 
Serial Verb, Control Construction (CC), 
Modal Control Construction (MCC), Passives 
(Pass) and Auxiliary Construction (AC) (Butt, 
1993; Paul, 2004) are also necessary to 
identify the Complex Predicates (CPs). The 
error analysis shows that the system suffers in 
distinguishing the Complex Predicates (CPs) 
from the above constraint constructions.  
The rest of the paper is organized as fol-
lows. Section 2 describes the related work 
done in this area. The automatic extraction of 
compound and conjunct verbs is described in 
Section 3. In Section 4, the identification of 
lexical scopes of the Complex Predicates 
(CPs) is mentioned. Section 5 discusses the 
results of evaluation along with error analysis. 
Finally, Section 6 concludes the paper. 
2 Related Work 
The general theory of complex predicate is 
discussed in Alsina (1996). Several attempts 
have been organized to identify complex 
predicates in South Asian languages (Abbi, 
1991; Bashir, 1993; Verma, 1993) with a spe-
cial focus to Hindi (Burton-Page, 1957; Hook, 
1974), Urdu (Butt, 1995), Bengali (Sarkar, 
1975; Paul, 2004), Kashmiri (Kaul, 1985) and 
Oriya (Mohanty, 1992). But the automatic ex-
traction of Complex Predicates (CPs) has been 
carried out for few languages, especially 
Hindi. 
The task described in (Mukherjee et al, 
2006) highlights the development of a database 
based on the hypothesis that an English verb is 
projected onto a multi-word sequence in Hindi. 
The simple idea of projecting POS tags across 
an English-Hindi parallel corpus considers the 
Complex Predicate types, adjective-verb (AV), 
noun-verb (NV), adverb-verb (Adv-V), and 
verb-verb (VV) composites. A similar task 
(Sinha, 2009) presents a simple method for 
detecting Complex Predicates of all kinds us-
ing a Hindi-English parallel corpus. His simple 
strategy exploits the fact that Complex Predi-
cate is a multi-word expression with a meaning 
that is distinct from the meaning of the Light 
Verb. In contrast, the present task carries the 
identification of Complex Predicates (CPs) 
from monolingual Bengali corpus based on 
morphological information and lexical pat-
terns. 
The analysis of V+V complex predicates 
termed as lexical compound verbs (LCpdVs) 
and the linguistic tests for their detection in 
Hindi are described in (Chakrabarti et al, 
2008). In addition to compound verbs, the pre-
sent system also identifies the conjunct verbs 
in Bengali. But, it was observed that the identi-
fication of Hindi conjunct verbs that contain 
noun in the first slot is puzzling and therefore a 
sophisticated solution was proposed in (Das, 
2009) based on the control agreement strategy 
with other overtly case marked noun phrases. 
The present task also agrees with the above 
problem in identifying conjunct verbs in Ben-
gali although the system satisfactorily identi-
fies the conjunct verbs (ConjVs). 
Paul (2003) develops a constraint-based 
mechanism within HPSG framework for com-
posing Indo-Aryan compound verb construc-
tions with special focus on Bangla (Bengali) 
compound verb sequences. Postulating seman-
tic relation of compound verbs, another work 
(Paul, 2009) proposed a solution of providing 
lexical link between the Full verb and Light 
Verb to store the Compound Verbs in Indo 
WordNet without any loss of generalization. 
To the best of our knowledge, ours is the first 
attempt at automatic extraction of Complex 
Predicates (CPs) in Bengali.  
3 Identification of Complex Predi-
cates (CPs) 
The compound verbs (CompVs) and conjunct 
verbs (ConjVs) are identified from the shallow 
parsed result using a lexical pattern matching 
technique. 
3.1 Preparation of Corpora 
Two types of Bengali corpus have been con-
sidered to carry out the present task. One cor-
pus is collected from a travel and tourism do-
main and another from an online web archive 
of Rabindranath Rachanabali 2 . Rabindra 
Rachanabali corpus is a large collection of 
short stories of Rabindranath Tagore. The for-
                                                 
2 www.rabindra-rachanabali.nltr.org 
39
mer EILMT travel and tourism corpus is ob-
tained from the consortium mode project ?De-
velopment of English to Indian Languages 
Machine Translation (EILMT 3) System?. The 
second type of corpus is retrieved from the 
web archive and pre-processed accordingly. 
Each of the Bengali corpora contains 400 and 
500 development and test sentences respec-
tively.   
The sentences are passed through an open 
source Bengali shallow parser 4. The shallow 
parser gives different morphological informa-
tion (root, lexical category of the root, gender, 
number, person, case, vibhakti, tam, suffixes 
etc.) that help in identifying the lexical patterns 
of Complex Predicates (CPs).  
3.2 Extracting Complex Predicates (CPs) 
Manual observation shows that the Complex 
Predicates (CPs) contain the lexical pattern 
{[XXX] (n/adj) [YYY] (v)} in the shallow 
parsed sentences where XXX and YYY repre-
sent any word. But, the lexical category of the 
root word of XXX is either noun (n) or adjec-
tive (adj) and the lexical category of the root 
word of YYY is verb (v). The shallow parsed 
sentences are pre-processed to generate the 
simplified patterns. An example of similar 
lexical pattern of the shallow parsed result and 
its simplified output is shown in Figure 1.  
 
((NP  a????  NN  <fs 
?f='a???? ,n,,sg,,d,?? ? ,?? ? '>  ))              
   
((VGF  ???????      VM       <fs 
?f='?r,v,,,5,,? ,? '>     )) 
a???? |no?n|a???? /NN/NP/ 
(a???? ^n^*^sg^*^d^?? ^?? ? )_ 
???????|v??b|???????/VM/VGF/              
(?r^v^*^*^1^*^? ^? ) 
         
 Figure 1. Example of a pre-processed shallow 
parsed result. 
 
                                                 
3 The EILMT project is funded by the Department of 
Information Technology (DIT), Ministry of Communica-
tions and Information Technology (MCIT), Government 
of India. 
4http://ltrc.iiit.ac.in/showfile.php?filename=downloads/sh
allow_parser.php 
The corresponding lexical categories of the 
root words a???? adhyan ?study? (e.g. noun 
for ?n?) and '?r  kar, ?do? (e.g. verb for ?v?) are 
shown in bold face in Figure 1. The f ollowing 
example is of conjunct verb (ConjV).  
The extraction of Bengali compound verbs 
(CompVs) is straightforward rather than con-
junct verbs (ConjVs). The lexical pattern of 
compound verb is {[XXX](v) [YYY] (v)} where 
the lexical or basic POS categories of the root 
words of  ?XXX? and ?YYY? are only verb. If 
the basic POS tags of the root forms of ?XXX? 
and ?YYY? are verbs (v) in shallow parsed sen-
tences, then only the corresponding lexical 
patterns are considered as the probable candi-
dates of compound verbs (CompVs).  
Example 1: 
??i??|v??b|??i??/VM/VGNF/? ? ^v^*^*^?ny^*^i??^i??)
#??????|v??b|??????/VM/VGF/(??^v^*^*^1^*^?^?) 
Example 1 is a compound verb (CompV) but 
Example 2 is not. In Example 2, the lexical 
category or the basic POS of the Full Verb 
(FV) is noun (n) and hence the pattern is dis-
carded as non-compound verb (non-CompV). 
Example 2: 
?k? |noun|?k? /NN/NP/(?k? ^n^*^*^*^*^*^pos
lcat="NM") #  
?????|verb|?????/VM/VGNF/(?r^v^*^*^any^*^i??
^i??) 
Bengali, like any other Indian languages, is 
morphologically very rich. Different suffixes 
may be attached to a Light Verb (LVs) (in this 
case [YYY]) depending on the various features 
such as Tense, Aspect, and Person.  
In case of extracting compound verbs 
(CompVs), the Light Verbs are identified from 
a seed list (Paul, 2004). The list of Light Verbs 
is specified in Table 1. The dictionary forms of 
the Light Verbs are stored in this list. As the 
Light Verbs contain different suffixes, the pri-
mary task is to identify the root forms of the 
Light Verbs (LVs) from shallow parsed result. 
Another table that stores the root forms and the 
corresponding dictionary forms of the Light 
Verbs is used in the present task. The table 
contains a total number of 378 verb entries 
including Full Verbs (FVs) and Light Verbs 
(LVs). The dictionary forms of the Light Verbs 
(LVs) are retrieved from the Table. 
On the other hand, the conjunctive particip-
ial form -e/i?? -e/iya or the infinitive form -
? /i?? ?te/ite are attached with the Full Verbs 
40
(FVs) (in this case [XXX]) in compound verbs 
(CompVs). i?? / iya and i??/ ite are also used 
for conjunctive participial form -e ?e or the 
infinitive form -?  ?te respectively in litera-
ture. The participial and infinitive forms are 
checked based on the morphological informa-
tion (e.g. suffixes of the verb) given in the 
shallow parsed results. In Example 1, the Full 
Verb (FV) contains -i?? -iya suffix. If the dic-
tionary forms of the Light Verbs (LVs) are pre-
sent in the list of Light Verbs and the Full 
Verbs (FVs) contain the suffixes of -e/i?? -
e/iya or ? /i?? ?te/ite, both verbs are combined 
to frame the patterns of compound verbs 
(CompVs). 
 
aSa ?come?          d?Ra ?stand? 
rakha ?keep?       ana ?bring?  
deoya ?give?        pOra ?fall?  
paTha ?send?       bERano ?roam? 
neoya ?take?        tola ?lift? 
bOSa ?sit?           oTha ?rise? 
jaoya ?go?           chaRa ?leave? 
phEla ?drop?       mOra ?die? 
 
Table 1. List of Light Verbs for compound 
verbs. 
The identification of conjunct verbs 
(ConjVs) requires the lexical pattern (Noun / 
Adjective + Light Verb) where a noun or an 
adjective is followed by a Light Verb (LV). The 
dictionary forms of the Light Verbs (LVs) that 
are frequently used as conjunct verbs (ConjVs) 
are prepared manually. The list of Light Verbs 
(LVs) is given in Table 2. The detection of 
Light Verbs (LVs) for conjunct verbs (ConjVs) 
is similar to the detection of the Light Verbs 
(LVs) for compound verbs (CompVs) as de-
scribed earlier in this section.  If the basic POS 
of the root of the first words ([XXX]) is either 
?noun? or ?adj? (n/adj) and the basic POS of 
the following word ([YYY]) is ?verb? (v), the 
patterns are considered as conjunct verbs 
(ConjVs). The Example 2 is an example of 
conjunct verb (ConjV). 
For example, ???? ??? (jhakjhak kara ?to 
glow?), ???? ??? (taktak ?to glow?), ?? ??? ??? 
(chupchap kara ?to silent?) etc are identified as 
conjunct verbs (ConjVs) where the basic POS 
of the former word is an adjective (adj) fol-
lowed by ??? kara ?to do?, a common Light 
Verb.  
deoya ?give?        kara  ?do?    
neoya ?take?        laga  ?start?            
paoya ?pay?         kata  ?cut?   
  
Table 2. List of Light Verbs for conjunct verbs. 
 
Example 3:  
  ????|?dj|???? /JJ/JJP/(???? ^?dj) # 
????|v??b|????/VM/VGF/(?r^v^*^*^5^*^?^?) 
But, the extraction of conjunct verbs 
(ConjVs) that have a ?noun+verb? construction 
is descriptively and theoretically puzzling 
(Das, 2009). The identification of lexical pat-
terns is not sufficient to recognize the com-
pound verbs (CompVs). For example, ?i ? ??? 
boi deoya ?give book? and ???? ? ??? bharsa 
deyoa ?to assure? both contain similar lexical 
pattern (noun+verb) and same Light Verb ? ??? 
deyoa. But, ???? ? ??? bharsa deyoa ?to assure? 
is a conjunct verb (ConjV) whereas ?i ? ??? boi 
deoya ?give book? is not a conjunct verb 
(ConjV). Linguistic observation shows that the 
inclusion of this typical category into conjunct 
verbs (ConjVs) requires the additional knowl-
edge of syntax and semantics.  
In connection to conjunct verbs (ConjVs), 
(Mohanty, 2010) defines two types of conjunct 
verbs (ConjVs), synthetic and analytic. A syn-
thetic conjunct verb is one in which both the 
constituents form an inseparable whole from 
the semantic point of view or semantically 
non-compositional in nature. On the other 
hand, an analytic conjunct verb is semantically 
compositional. Hence, the identification of 
conjunct verbs requires knowledge of seman-
tics rather than only the lexical patterns. 
It is to be mentioned that sometimes, the 
negative markers (?? no, ??i nai) are attached 
with the Light Verbs u?????  uthona ?do not get 
up? ? ?????  phelona ?do not throw?. Negative 
attachments are also considered in the present 
task while checking the suffixes of Light Verbs 
(LVs). 
4 Identification of Lexical Scope for 
Complex Predicates (CPs) 
The identification of lexical scopes of the 
Complex Predicates (CPs) from their succes-
sive sequences shows that multiple Complex 
41
Predicates (CPs) can occur in a long sequence. 
An automatic method is employed to identify 
the Complex Predicates (CPs) along with their 
lexical scopes. The lexical category or basic 
POS tags are obtained from the parsed sen-
tences. 
If the compound and conjunct verbs occur 
successively in a sequence, the left most two 
successive tokens are chosen to construct the 
Complex Predicate (CP). If successive verbs 
are present in a sequence and the dictionary 
form of the second verb reveals that the verb is 
present in the lists of compound Light Verbs 
(LV), then that Light Verb (LV) may be a part 
of a compound verb (CompV). For that reason, 
the immediate previous word token is chosen 
and tested for its basic POS in the parsed result. 
If the basic POS of the previous word is ?verb 
(v)? and any suffixes of either conjunctive par-
ticipial form -e/i?? -e/iya or the infinitive form 
-? /i?? ?te/ite is attached to the previous verb, 
the two successive verbs are grouped together 
to form a compound verb (CompV) and the 
lexical scope is fixed for the Complex Predi-
cate (CP).  
If the previous verb does not contain -e/i?? 
-e/iya or -? /i?? ?te/ite inflections, no com-
pound verb (CompV) is framed with these two 
verbs. But, the second Light Verb (LV) may be 
a part of another Complex Predicate (CP). This 
Light Verb (LV) is now considered as the Full 
Verb (FV) and its immediate next verb is 
searched in the list of compound Light Verbs 
(LVs) and the formation of compound verbs 
(CompVs) progresses similarly.  If the verb is 
not in the list of compound Light Verbs, the 
search begins by considering the present verb 
as Full Verb (FV) and the search goes in a 
similar way. 
The following examples are given to illus-
trate the formation of compound verbs 
(CompVs) and find the lexical scopes of the 
compound verbs (CompVs). 
 
???      ????       ????     ???      ? ??? 
(ami)       (chalte)      (giye)    (pore)    (gelam). 
I <fell down while walking>. 
 
Here, ?chalte giye pore gelam? is a verb 
group. The two left most verbs ???? ???? chalte 
giye are picked and the dictionary form of the 
second verb is searched in the list of com-
pound Light Verbs. As the dictionary form 
(jaoya ?go?) of the verb ???? giye is present in 
the list of compound Light Verbs (as shown in 
Table 1), the immediate previous verb ???? 
chalte is checked for inflections -e/i?? -e/iya 
or -? /i?? ?te/ite.  As the verb ???? chalte con-
tains the inflection -?  -te , the verb group ???? 
???? chalte giye is a compound verb (CompV) 
where ???? giye is a Light Verb and ???? chalte 
is the Full Verb with inflection (-?  -te).  Next 
verb group, ???   ? ??? pore gelam is identified 
as compound verb (CompV) in a similar way 
(??+ (-e) por+ (-e) + ? ??? gelam (jaoya ?go?)).  
Another example is given as follows.  
 
???    u??      ???       ? ????       ?   
(ami)   (uthe)      (pore)      (dekhlam)    (je)  
?? ??      e????        ? i 
(tumi)     (ekhane)       (nei) 
I <get up and saw> that you are not here 
 
Here, u?? ??? ? ???? uthe pore dekhlam is 
another verb group. The immediate next verb 
of u?? uthe is ??? pore that is chosen and its 
dictionary form is searched in the list of com-
pound Light Verbs (LV) similarly. As the dic-
tionary form (???  pOra) of the verb ??? pore 
is present in the list of Light Verbs and the 
verb u??   uthe contains the inflection -e ?e, 
the consecutive verbs frame a compound verb 
(CompV) u?? ??? where u?? uthe is a Full Verb 
with inflection -e ?e and ??? pore is a Light 
Verb. The final verb ? ????      dekhlam is 
chosen and as there is no other verb present, 
the verb ? ???? dekhlam is excluded from any 
formation of compound verb (CompV) by con-
sidering it as a simple verb.  
Similar technique is adopted for identifying 
the lexical scopes of conjunct verbs (ConjVs). 
The method seems to be a simple pattern 
matching technique in a left-to-right fashion 
but it helps in case of conjunct verbs (ConjVs). 
As the noun or adjective occur in the first slot 
of conjunct verbs (ConjVs) construction, the 
search starts from the point of noun or adjec-
tive. If the basic POS of a current token is ei-
ther ?noun? or ?adjective? and the dictionary 
form of the next token with the basic POS 
?verb (v)? is in the list of conjunct Light Verbs 
(LVs), then the two consecutive tokens are 
42
combined to frame the pattern of a conjunct 
verb (ConjV). 
For example, the identification of lexical 
scope of a conjunct verb (ConjV) from a se-
quence such as u????  ???? ? ??? uparjon korte 
gelam ?earn-do-go? (went to earn) identifies 
the conjunct verb (ConjV) u????  ???? uparjon 
korte. There is another verb group ???? ? ??? 
korte gelam that seems to be a compound verb 
(CompV) but is excluded by considering ?? ??? 
gelam as a simple verb. 
5 Evaluation 
The system is tested on 800 development sen-
tences and finally applied on a collection of 
500 sentences from each of the two Bengali 
corpora. As there is no annotated corpus avail-
able for evaluating Complex Predicates (CPs), 
the manual evaluation of total 1000 sentences 
from the two corpora is carried out in the pre-
sent task.  
The recall, precision and F-Score are con-
sidered as the standard metrics for the present 
evaluation. The extracted Complex Predicates 
(CPs) contain compound verb (CompV) and 
conjunct verbs (ConjVs). Hence, the metrics 
are measured for both types of verbs individu-
ally. The separate results for two separate cor-
pora are shown in Table 3 and Table 4 respec-
tively. The results show that the system identi-
fies the Complex Predicates (CPs) satisfacto-
rily from both of the corpus. In case of Com-
pound Verbs (CompVs), the precision value is 
higher than the recall. The lower recall value 
of Compound Verbs (CompVs) signifies that 
the system fails to capture the other instances 
from overlapping sequences as well as non-
Complex predicates (non-CPs).  
But, it is observed that the identification of 
lexical scopes of compound verbs (CompVs) 
and conjunct verbs (ConjVs) from long se-
quence of successive Complex Predicates 
(CPs) increases the number of Complex Predi-
cates (CPs) entries along with compound verbs 
(CompVs) and conjunct verbs (ConjVs). The 
figures shown in bold face in Table 3 and Ta-
ble 4 for the Travel and Tourism corpus and 
Short Story corpus of Rabindranath Tagore 
indicates the improvement of identifying lexi-
cal scopes of the Complex Predicates (CPs).  
In comparison to other similar language 
such as Hindi (Mukerjee et al, 2006) (the re-
ported precision and recall are 83% and 46% 
respectively), our results (84.66% precision 
and 83.67% recall) are higher in case of ex-
tracting Complex Predicates (CPs). The reason 
may be of resolving the lexical scope and han-
dling the morphosyntactic features using shal-
low parser.  
In addition to Non-MonoClausal Verb 
(NMCV) or Serial Verb, the other criteria 
(Butt, 1993; Paul, 2004) are used in our pre-
sent diagnostic tests to identify the complex 
predicates (CPs). The frequencies of 
Compound Verb (CompV), Conjunct Verb 
(ConjV) and the instances of other constraints 
of non Complex Predicates (non-CPs) are 
shown in Figure 2. It is observed that the num-
bers of instances of Conjunct Verb (ConjV), 
Passives (Pass), Auxiliary Construction (AC) 
and Non-MonoClausal Verb (NMCV) or Serial 
Verb are comparatively high than other in-
stances in both of the corpus. 
 
EILMT  Recall Precision F-
Score 
Compound  
Verb 
(CompV) 
65.92% 
70.31% 
 
80.11% 
82.06% 
72.32% 
75.73%
Conjunct 
Verb 
(ConjV) 
94.65% 
96.96% 
80.44% 
83.82% 
86.96% 
89.90%
 
Table 3. Recall, Precision and F-Score of the 
system for acquiring the CompVs and ConjVs 
from EILMT Travel and Tourism Corpus. 
 
Rabindra 
Rachana-
bali 
Recall Precision F-
Score 
Compound  
Verb 
(CompV) 
68.75% 
72.22% 
 
81.81% 
84.61% 
74.71% 
77.92%
Conjunct 
Verb 
(ConjV) 
94.11% 
95.23% 
83.92% 
84.71% 
88.72% 
89.66%
 
Table 4. Recall, Precision and F-Score of the 
system for acquiring the CompVs and ConjVs 
from Rabindra Rachanabali corpus. 
 
43
 
 CompV ConjV NMCV CC MCC Pass AC 
CompV 0.76 0.00 0.02 0.00 0.00 0.03 0.02 
ConjV 0.04 0.72 0.03 0.01 0.02 0.02 0.02 
NMCV 0.17 0.18 0.65 0.00 0.02 0.02 0.02 
CC 0.01 0.00 0.00 0.56 0.01 0.02 0.02 
MCC 0.00 0.00 0.00 0.07 0.65 0.00 0.02 
Pass 0.12 0.01 0.00 0.00 0.00 0.78 0.00 
AC 0.06 0.07 0.04 0.00 0.00 0.08 0.54 
Table 5. Confusion Matrix for CPs and constraints of non-CPs (in %).  
 
0
50
100
150
200
CompV
ConjV
CC MCC
Pass
AC NMCV
EILMTRabindra
  
Figure 2. The frequencies of Complex Predi-
cates (CPs) and different constrains of non-
Complex Predicates (non-CPs). 
 
The error analysis is conducted on both of 
the corpus. Considering both corpora as a 
whole single corpus, the confusion matrix is 
developed and shown in Table 5. The bold face 
figures in Table 5 indicate that the percentages 
of non-Complex Predicates (non-CPs) such as 
Non-MonoClausal Verbs (NMCV), Passives 
(Pass) and Auxiliary Construction (AC) that 
are identified as compound verbs (CompVs). 
The reason is the frequencies of the non-
Complex Predicates (non-CPs) that are rea-
sonably higher in the corpus.  In case of con-
junct verbs (ConjVs), the Non-MonoClausal 
Verbs (NMCV) and Auxiliary Construction 
(AC) occur as conjunct verbs (ConjVs).  The 
system also suffers from clausal detection that 
is not attempted in the present task. The Pas-
sives (Pass) and Auxiliary Construction (AC) 
requires the knowledge of semantics with ar-
gument structure knowledge. 
6 Conclusion 
In this paper, we have presented a study of 
Bengali Complex Predicates (CPs) with a spe-
cial focus on compound verbs, proposed auto-
matic methods for their extraction from a cor-
pus and diagnostic tests for their evaluation. 
The problem arises in case of distinguishing 
Complex Predicates (CPs) from Non-Mono-
Clausal verbs, as only the lexical patterns are 
insufficient to identify the verbs. In future task, 
the subcategorization frames or argument 
structures of the sentences are to be identified 
for solving the issues related to the errors of 
the present system.  
References 
Abbi, Anvita. 1991. Semantics of Explicator Com-
pound Verbs. In South Asian Languages, Lan-
guage Sciences, 13(2): 161-180. 
Alsina, Alex. 1996. Complex Predicates: Structure 
and Theory. Center for the Study of Language 
and Information Publications, Stanford, CA. 
Bashir, Elena. 1993. Causal chains and compound 
verbs. In M. K. Verma ed. (1993) Complex 
Predicates in South Asian Languages, Manohar 
Publishers and Distributors, New Delhi. 
Burton-Page, John. 1957. Compound and conjunct 
verbs in Hindi. Bulletin of the School of Oriental 
and African Studies, 19: 469-78. 
Butt, Miriam. 1995. The Structure of Complex 
Predicates in Urdu. Doctoral Dissertation, Stan-
ford University. 
Chakrabarti, Debasri, Mandalia Hemang, Priya 
Ritwik, Sarma Vaijayanthi, Bhattacharyya Push-
pak. 2008. Hindi Compound Verbs and their 
Automatic Extraction. International Conference 
on Computational Linguistics ?2008, pp. 27-30. 
44
Das, Pradeep Kumar. 2009. The form and function 
of Conjunct verb construction in Hindi. Global 
Association of Indo-ASEAN Studies, Daejeon, 
South Korea. 
Hook, Peter. 1974. The Compound Verbs in Hindi. 
The Michigan Series in South and South-east 
Asian Language and Linguistics. The University 
of Michigan. 
Kaul, Vijay Kumar. 1985. The Compound Verb in 
Kashmiri. Unpublished Ph.D. dissertation. Ku-
rukshetra University. 
Kipper-Schuler, Karin. 2005. VerbNet: A broad-
coverage,  comprehensive verb lexicon. Ph.D. 
thesis, Computer and Information Science Dept., 
University of Pennsylvania, Philadelphia,PA  
Miller, George, Richard Beckwith, Christiane Fell-
baum, Derek Gross and Katherine Miller. 1990. 
Five Papers on WordNet. CSL Report 43, Cogni-
tive Science Laboratory, Princeton University, 
Princeton. 
Mohanty, Gopabandhu. 1992. The Compound 
Verbs in Oriya. Ph. D. dissertation, Deccan Col-
lege Post-Graduate and Research Institute, Pune. 
Mohanty, Panchanan. 2010. WordNets for Indian 
Languages: Some Issues. Global WordNet Con-
ference-2010, pp. 57-64. 
Mukherjee, Amitabha, Soni Ankit and Raina Achla 
M. 2006. Detecting Complex Predicates in Hindi 
using POS Projection across Parallel Corpora. 
Multiword Expressions: Identifying and Exploit-
ing Underlying Properties Association for Com-
putational Linguistics, pp. 28?35, Sydney.  
Paul, Soma. 2010. Representing Compound Verbs 
in Indo WordNet. Golbal Wordnet Conference-
2010, pp. 84-91. 
Paul, Soma. 2004. An HPSG Account of Bangla 
Compound Verbs with LKB Implementation. 
Ph.D dissertation, University of Hyderabad, Hy-
derabad. 
Paul, Soma. 2003. Composition of Compound 
Verbs in Bangla. Multi-Verb constructions. 
Trondheim  Summer School. 
Sarkar, Pabitra. 1975. Aspects of Compound Verbs 
in Bengali. Unpublished M.A. dissertation, Chi-
cago University. 
Sinha, R. Mahesh, K. 2009. Mining Complex 
Predicates In Hindi Using A Parallel Hindi-
English Corpus. Multiword Expression Work-
shop, Association of Computational Linguistics-
International Joint Conference on Natural Lan-
guage Processing-2009, pp. 40-46, Singapore. 
Timothy, Baldwin, Su Nam Kim. 2010. Multiword 
Expressions. In Nitin Indurkhya and Fred J. 
Damerau (eds.) Handbook of Natural Language 
Processing, Second Edition, Chapman & 
Hall/CRC, London, UK, pp. 267-292. 
Verma, Manindra K.1993. Complex Predicates in 
South Asian Languages. Manohar Publishers and 
Distributors, New Delhi. 
 
45
Proceedings of the Multiword Expressions: From Theory to Applications (MWE 2010), pages 46?54,
Beijing, August 2010
Handling Named Entities and Compound Verbs in             
Phrase-Based Statistical Machine Translation 
Santanu Pal*, Sudip Kumar Naskar?, Pavel Pecina?,  
Sivaji Bandyopadhyay* and Andy Way? 
*Dept. of Comp. Sc. & Engg. 
Jadavpur University 
santanupersonal1@gmail.com, sivaji_cse_ju@yahoo.com 
?CNGL, School of Computing 
Dublin City University 
{snaskar, ppecina, away}@computing.dcu.ie 
 
Abstract 
Data preprocessing plays a crucial role in 
phrase-based statistical machine transla-
tion (PB-SMT). In this paper, we show 
how single-tokenization of two types of 
multi-word expressions (MWE), namely 
named entities (NE) and compound 
verbs, as well as their prior alignment 
can boost the performance of PB-SMT. 
Single-tokenization of compound verbs 
and named entities (NE) provides sig-
nificant gains over the baseline PB-SMT 
system. Automatic alignment of NEs 
substantially improves the overall MT 
performance, and thereby the word 
alignment quality indirectly. For estab-
lishing NE alignments, we transliterate 
source NEs into the target language and 
then compare them with the target NEs. 
Target language NEs are first converted 
into a canonical form before the com-
parison takes place. Our best system 
achieves statistically significant im-
provements (4.59 BLEU points absolute, 
52.5% relative improvement) on an Eng-
lish?Bangla translation task. 
1 Introduction 
Statistical machine translation (SMT) heavily 
relies on good quality word alignment and 
phrase alignment tables comprising translation 
knowledge acquired from a bilingual corpus. 
Multi-word expressions (MWE) are defined 
as ?idiosyncratic interpretations that cross word 
boundaries (or spaces)? (Sag et al, 2002). Tradi-
tional approaches to word alignment following 
IBM Models (Brown et al, 1993) do not work 
well with multi-word expressions, especially 
with NEs, due to their inability to handle many-
to-many alignments. Firstly, they only carry out 
alignment between words and do not consider 
the case of complex expressions, such as multi-
word NEs. Secondly, the IBM Models only al-
low at most one word in the source language to 
correspond to a word in the target language 
(Marcu, 2001, Koehn et al, 2003). 
In another well-known word alignment ap-
proach, Hidden Markov Model (HMM: Vogel et 
al., 1996), the alignment probabilities depend on 
the alignment position of the previous word. It 
does not explicitly consider many-to-many 
alignment either. 
We address this many-to-many alignment 
problem indirectly. Our objective is to see how 
to best handle the MWEs in SMT. In this work, 
two types of MWEs, namely NEs and compound 
verbs, are automatically identified on both sides 
of the parallel corpus. Then, source and target 
language NEs are aligned using a statistical 
transliteration method. We rely on these auto-
matically aligned NEs and treat them as transla-
tion examples. Adding bilingual dictionaries, 
which in effect are instances of atomic transla-
tion pairs, to the parallel corpus is a well-known 
practice in domain adaptation in SMT (Eck et 
al., 2004; Wu et al, 2008). We modify the paral-
lel corpus by converting the MWEs into single 
tokens and adding the aligned NEs in the parallel 
corpus in a bid to improve the word alignment, 
and hence the phrase alignment quality. This 
46
preprocessing results in improved MT quality in 
terms of automatic MT evaluation metrics. 
The remainder of the paper is organized as 
follows. In section 2 we discuss related work. 
The System is described in Section 3.  Section 4 
includes the results obtained, together with some 
analysis. Section 5 concludes, and provides ave-
nues for further work. 
2 Related Work 
Moore (2003) presented an approach for si-
multaneous NE identification and translation. He 
uses capitalization cues for identifying NEs on 
the English side, and then he applies statistical 
techniques to decide which portion of the target 
language corresponds to the specified English 
NE. Feng et al (2004) proposed a Maximum 
Entropy model based approach for English?
Chinese NE alignment which significantly out-
performs IBM Model4 and HMM. They consid-
ered 4 features: translation score, transliteration 
score, source NE and target NE's co-occurrence 
score, and the distortion score for distinguishing 
identical NEs in the same sentence. Huang et al 
(2003) proposed a method for automatically ex-
tracting NE translingual equivalences between 
Chinese and English based on multi-feature cost 
minimization. The costs considered are translit-
eration cost, word-based translation cost, and NE 
tagging cost. 
Venkatapathy and Joshi (2006) reported a dis-
criminative approach of using the compositional-
ity information about verb-based multi-word 
expressions to improve word alignment quality. 
(Ren et al, 2009) presented log likelihood ratio-
based hierarchical reducing algorithm to auto-
matically extract bilingual MWEs, and investi-
gated the usefulness of these bilingual MWEs in 
SMT by integrating bilingual MWEs into Moses 
(Koehn et al, 2007) in three ways. They ob-
served the highest improvement when they used 
an additional feature to represent whether or not 
a bilingual phrase contains bilingual MWEs. 
This approach was generalized in Carpuat and 
Diab (2010). In their work, the binary feature 
was replaced by a count feature representing the 
number of MWEs in the source language phrase. 
Intuitively, MWEs should be both aligned in 
the parallel corpus and translated as a whole. 
However, in the state-of-the-art PB-SMT, it 
could well be the case that constituents of an 
MWE are marked and aligned as parts of con-
secutive phrases, since PB-SMT (or any other 
approaches to SMT) does not generally treat 
MWEs as special tokens. Another problem SMT 
suffers from is that verb phrases are often 
wrongly translated, or even sometimes deleted in 
the output in order to produce a target sentence 
considered good by the language model. More-
over, the words inside verb phrases seldom show 
the tendency of being aligned one-to-one; the 
alignments of the words inside source and target 
verb phrases are mostly many-to-many, particu-
larly so for the English?Bangla language pair. 
These are the motivations behind considering 
NEs and compound verbs for special treatment 
in this work. 
By converting the MWEs into single tokens, 
we make sure that PB-SMT also treats them as a 
whole. The objective of the present work is two-
fold; firstly to see how treatment of NEs and 
compound verbs as a single unit affects the 
overall MT quality, and secondly whether prior 
automatic alignment of these single-tokenized 
MWEs can bring about any further improvement 
on top of that. 
We carried out our experiments on an Eng-
lish?Bangla translation task, a relatively hard 
task with Bangla being a morphologically richer 
language. 
3 System Description 
3.1 PB-SMT 
Translation is modeled in SMT as a decision 
process, in which the translation Ie1 = e1 . . . ei . . 
. eI of a source sentence
Jf1 = f1 . . . fj . . . fJ is 
chosen to maximize (1): 
)().|(maxarg)|(maxarg 111
,
11
, 11
IIJ
eI
JI
eI
ePefPfeP
II
=      (1)  
where )|( 11
IJ efP  and )( 1
IeP  denote respec-
tively the translation model and the target lan-
guage model (Brown et al, 1993). In log-linear 
phrase-based SMT, the posterior probability 
)|( 11
JI feP  is directly modeled as a log-linear 
combination of features (Och and Ney, 2002), 
that usually comprise M translational features, 
and the language model, as in (2): 
 
47
?
=
=
M
m
KIJ
mm
JI sefhfeP
1
11111 ),,()|(log ?  
)(log 1
I
LM eP?+        (2)     
where k
k sss ...11 =  denotes a segmentation of the 
source and target sentences respectively into the 
sequences of phrases )?,...,?( 1 kee  and )?,...,?( 1 kff  
such that (we set i0 = 0) (3): 
,1 Kk ???  sk = (ik, bk, jk), 
          
kk iik
eee ...? 11 +?= , 
         
kk jbk
fff ...? = .          (3) 
and each feature mh?  in (2) can be rewritten as in 
(4): 
?
=
=
K
k
kkkm
KIJ
m sefhsefh
1
111 ),?,?(?),,(                  (4) 
where mh? is a feature that applies to a single 
phrase-pair. It thus follows (5): 
? ??
= ==
=
K
k
K
k
kkkkkkm
M
m
m sefhsefh
1 11
),?,?(?),?,?(??      (5) 
where m
M
m
mhh ??
1
?
=
= ? .            
3.2 Preprocessing of the Parallel Corpus 
The initial English?Bangla parallel corpus is 
cleaned and filtered using a semi-automatic 
process. We employed two kinds of multi-word 
information: compound verbs and NEs. Com-
pound verbs are first identified on both sides of 
the parallel corpus. Chakrabarty et al (2008) 
analyzed and identified a category of V+V com-
plex predicates called lexical compound verbs 
for Hindi. We adapted their strategy for identifi-
cation of compound verbs in Bangla. In addition 
to V+V construction, we also consider N+V and 
ADJ+V structures. 
NEs are also identified on both sides of trans-
lation pairs. NEs in Bangla are much harder to 
identify than in English (Ekbal and Bandyop-
adhyay, 2009). This can be attributed to the fact 
that (i) there is no concept of capitalization in 
Bangla; and (ii) Bangla common nouns are often 
used as proper names. In Bangla, the problem is 
compounded by the fact that suffixes (case 
markers, plural markers, emphasizers, specifiers) 
are also added to proper names, just like to any 
other common nouns. As a consequence, the ac-
curacy of Bangla NE recognizers (NER) is much 
poorer compared to that for English. Once the 
compound verbs and the NEs are identified on 
both sides of the parallel corpus, they are con-
verted into and replaced by single tokens. When 
converting these MWEs into single tokens, we 
replace the spaces with underscores (?_?). Since 
there are already some hyphenated words in the 
corpus, we do not use hyphenation for this pur-
pose; besides, the use of a special word separator 
(underscore in our case) facilitates the job of 
deciding which single-token (target language) 
MWEs to detokenize into words comprising 
them, before evaluation. 
3.3 Transliteration  Using Modified Joint 
Source-Channel Model 
Li et al (2004) proposed a generative framework 
allowing direct orthographical mapping of trans-
literation units through a joint source-channel 
model, which is also called n-gram translitera-
tion model. They modeled the segmentation of 
names into transliteration units (TU) and their 
alignment preferences using maximum likeli-
hood via EM algorithm (Dempster et al, 1977). 
Unlike the noisy-channel model, the joint 
source-channel model tries to capture how 
source and target names can be generated simul-
taneously by means of contextual n-grams of the 
transliteration units. For K aligned TUs, they 
define the bigram model as in (6): 
 )...,,...,(),( 2121 KK bbbeeePBEP =  
  ),...,,,( 21 KbebebeP ><><><=  
   ? ><><= K
=k
k bebeP
1
1-k
1 ),|,(         (6) 
where E refers to the English name and B the 
transliteration in Bengali, while ei and bi refer to 
the ith English and Bangla segment (TU) respec-
tively. 
Ekbal et al (2006) presented a modification to 
the joint source-channel model to incorporate 
different contextual information into the model 
for Indian languages. They used regular expres-
sions and language-specific heuristics based on 
consonant and vowel patterns to segment names 
into TUs. Their modified joint source-channel 
model, for which they obtained improvement 
48
over the original joint source-channel model, 
essentially considers a trigram model for the 
source language and a bigram model for the tar-
get, as in (7). 
 ? +><><= K
=k
kk ebebePBEP
1
11-k ),,|,(),(   (7) 
Ekbal et al (2006) reported a word agreement 
ratio of 67.9% on an English?Bangla translit-
eration task. In the present work, we use the 
modified joint source-channel model of (Ekbal 
et al, 2006) to translate names for establishing 
NE alignments in the parallel corpus. 
3.4 Automatic Alignment of NEs through 
Transliteration 
We first create an NE parallel corpus by extract-
ing the source and target (single token) NEs 
from the NE-tagged parallel translations in 
which both sides contain at least one NE. For 
example, we extract the NE translation pairs 
given in (9) from the sentence pair shown in (8), 
where the NEs are shown as italicized. 
(8a) Kirti_Mandir , where Mahatma_Gandhi 
was born , today houses a photo exhibition on 
the life and times of the Mahatma , a library, a 
prayer hall and other memorabilia . 
(8b) ??????_??n? , ?????? ???t?_??n? ??n????? , 
???????? ?????? ???t?? ???? o ??i ????? 
?????????? e??? ??tp????????? , e??? ??i?b?? o 
e??? p?????? ?? e?? a????? s ????????? ??????t 
??? ? 
(9a) Kirti_Mandir Mahatma_Gandhi Mahatma 
(9b) ??????_??n? ???t?_??n? ???t?? 
Then we try to align the source and target NEs 
extracted from a parallel sentence, as illustrated 
in (9). If both sides contain only one NE then the 
alignment is trivial, and we add such NE pairs to 
seed another parallel NE corpus that contains 
examples having only one token in both side. 
Otherwise, we establish alignments between the 
source and target NEs using transliteration. We 
use the joint source-channel model of translitera-
tion (Ekbal et al, 2006) for this purpose.  
If both the source and target side contains n 
number of NEs, and the alignments of n-1 NEs 
can be established through transliteration or by 
means of already existing alignments, then the 
nth alignment is trivial. However, due to the rela-
tive performance difference of the NERs for the 
source and target language, the number of NEs 
identified on the source and target sides is al-
most always unequal (see Section 4). Accord-
ingly, we always use transliteration to establish 
alignments even when it is assumed to be trivial. 
Similarly, for multi-word NEs, intra-NE word 
alignments are established through translitera-
tion or by means of already existing alignments. 
For a multi-word source NE, if we can align all 
the words inside the NE with words inside a tar-
get NE, then we assume they are translations of 
each other. Due to the relatively poor perform-
ance of the Bangla NER, we also store the im-
mediate left and right neighbouring words for 
every NE in Bangla, just in case the left or the 
right word is a valid part of the NE but is not 
properly tagged by the NER. 
As mentioned earlier, since the source side 
NER is much more reliable than the target side 
NER, we transliterate the English NEs, and try 
to align them with the Bangla NEs. For aligning 
(capitalized) English words to Bangla words, we 
take the 5 best transliterations produced by the 
transliteration system for an English word, and 
compare them against the Bangla words. Bangla 
NEs often differ in their choice of matras (vowel 
modifiers). Thus we first normalize the Bangla 
words, both in the target NEs and the transliter-
ated ones, to a canonical form by dropping the 
matras, and then compare the results. In effect, 
therefore, we just compare the consonant se-
quences of every transliteration candidate with 
that of a target side Bangla word; if they match, 
then we align the English word with the Bangla 
word. 
???? (? + ??+ ? + ?) -- ????? (? + ?? + ? + ?? + ?) 
      (10) 
The example in (10) illustrates the procedure. 
Assume, we are trying to align ?Niraj? with 
???????. The transliteration system produces 
?????? from the English word ?Niraj? and we 
compare ?????? with ???????. Since the conso-
nant sequences match in both words, ?????? is 
considered a spelling variation of ???????, and 
the English word ?Niraj? is aligned to the 
Bangla word ???????. 
In this way, we achieve word-level align-
ments, as well as NE-level alignments. (11) 
shows the alignments established from (8). The 
word-level alignments help to establish new 
49
word / NE alignments. Word and NE alignments 
obtained in this way are added to the parallel 
corpus as additional training data. 
(11a) Kirti-Mandir  ? ??????-??n?  
(11b) Kirti ? ?????? 
(11c) Mandir  ? ??n? 
(11d) Mahatma-Gandhi ? ???t?-??n?  
(11e) Mahatma ? ???t? 
(11f) Gandhi ? ??n? 
(11g) Mahatma ? ???t?? 
3.5 Tools and Resources Used 
A sentence-aligned English?Bangla parallel 
corpus containing 14,187 parallel sentences from 
a travel and tourism domain was used in the pre-
sent work. The corpus was obtained from the 
consortium-mode project ?Development of Eng-
lish to Indian Languages Machine Translation 
(EILMT) System? 1. 
The Stanford Parser2 and the CRF chunker3 
were used for identifying compound verbs in the 
source side of the parallel corpus. The Stanford 
NER4 was used to identify NEs on the source 
side (English) of the parallel corpus. 
The sentences on the target side (Bangla) 
were POS-tagged by using the tools obtained 
from the consortium mode project ?Develop-
ment of Indian Languages to Indian Languages 
Machine Translation (ILILMT) System?. NEs in 
Bangla are identified using the NER system of 
Ekbal and Bandyopadhyay (2008). We use the 
Stanford Parser, Stanford NER and the NER for 
Bangla along with the default model files pro-
vided, i.e., with no additional training. 
The effectiveness of the MWE-aligned paral-
lel corpus developed in the work is demonstrated 
by using the standard log-linear PB-SMT model 
as our baseline system: GIZA++ implementation 
of IBM word alignment model 4, phrase-
extraction heuristics described in (Koehn et al, 
2003), minimum-error-rate training (Och, 2003) 
on a held-out development set, target language 
model with Kneser-Ney smoothing (Kneser and 
                                                 
1 The EILMT and ILILMT projects are funded by the De-
partment of Information Technology (DIT), Ministry of 
Communications and Information Technology (MCIT), 
Government of India. 
2 http://nlp.stanford.edu/software/lex-parser.shtml 
3 http://crfchunker.sourceforge.net/ 
4 http://nlp.stanford.edu/software/CRF-NER.shtml 
Ney, 1995) trained with SRILM (Stolcke, 2002), 
and Moses decoder (Koehn et al, 2007). 
4 Experiments and Results 
We randomly extracted 500 sentences each for 
the development set and testset from the initial 
parallel corpus, and treated the rest as the train-
ing corpus. After filtering on maximum allow-
able sentence length of 100 and sentence length 
ratio of 1:2 (either way), the training corpus con-
tained 13,176 sentences. In addition to the target 
side of the parallel corpus, a monolingual Bangla 
corpus containing 293,207 words from the tour-
ism domain was used for the target language 
model. We experimented with different n-gram 
settings for the language model and the maxi-
mum phrase length, and found that a 4-gram 
language model and a maximum phrase length 
of 4 produced the optimum baseline result. We 
therefore carried out the rest of the experiments 
using these settings. 
English Bangla In training set 
T U T U 
Compound verbs 4,874 2,289 14,174 7,154
Single-word NEs 4,720 1,101 5,068 1,175
2-word NEs 4,330 2,961 4,147 3,417
>2 word NEs 1,555 1,271 1,390 1,278
Total NEs 10,605 5,333 10,605 5,870
Total NE words 22,931 8,273 17,107 9,106
Table 1.  MWE statistics (T - Total occur-
rence, U ? Unique). 
Of the 13,676 sentences in the training and 
development set, 13,675 sentences had at least 
one NE on both sides, only 22 sentences had 
equal number of NEs on both sides, and 13,654 
sentences had an unequal number of NEs. Simi-
larly, for the testset, all the sentences had at least 
one NE on both sides, and none had an equal 
number of NEs on both sides. It gives an indica-
tion of the relative performance differences of 
the NERs. 6.6% and 6.58% of the source tokens 
belong to NEs in the training and testset respec-
tively. These statistics reveal the high degree of 
NEs in the tourism domain data that demands 
special treatment. Of the 225 unique NEs ap-
pearing on the source side of the testset, only 65 
NEs are found in the training set.  
50
Experiments Exp BLEU METEOR NIST WER PER TER 
Baseline 1 8.74 20.39 3.98 77.89 62.95 74.60
NEs of any length as Single 
Token (New-MWNEaST) 
2 9.15 18.19 3.88 77.81 63.85 74.61
NEs of length >2 as  
Single Tokens (MWNE-
aST) 
3 8.76 18.78 3.86 78.31 63.78 75.15
 
 
NEs as Single  
Tokens  
(NEaST) 
2-Word NEs as Single To-
kens (2WNEaST) 
4 9.13 17.28 3.92 78.12 63.15 74.85
Compound Verbs as  Single Tokens 
(CVaST) ? 
5 9.56 15.35 3.96 77.60 63.06 74.46
Alignment of NEs of any 
length (New-MWNEA) ? 
6 13.33 24.06 4.44 74.79 60.10 71.25
Alignment of NEs of length 
upto 2 (New-2WNEA) ? 
7 10.35 20.93 4.11 76.49 62.20 73.05
Alignment of NEs of length 
>2 (MWNEA) ? 
8 12.39 23.13 4.36 75.51 60.58 72.06
 
 
 
 
NE Alignment 
(NEA) 
Alignment of NEs of length 
2 (2WNEA) ? 
9 11.2 23.14 4.26 76.13 60.72 72.57
New-MWNEaST 10 8.62 16.64 3.73 78.41 65.21 75.47
MWNEaST 11 8.74 14.68 3.84 78.40 64.05 75.40
 
CVaST 
+NEaST 2WNEaST 12 8.85 16.60 3.86 78.17 63.90 75.33
New-MWNEA? 13 11.22 21.02 4.16 75.99 61.96 73.06
New-2WNEA? 14 10.07 17.67 3.98 77.08 63.35 74.18
MWNEA? 15 10.34 16.34 4.07 77.12 62.38 73.88
 
CVaST +NEA 
2WNEA? 16 10.51 18.92 4.08 76.77 62.28 73.56
Table 2.  Evaluation results for different experimental setups (The ??? marked systems produce 
statistically significant improvements on BLEU over the baseline system).
Table 1 shows the MWE statistics of the 
parallel corpus as identified by the NERs. The 
average NE length in the training corpus is 
2.16 for English and 1.61 for Bangla. As can 
be seen from Table 1, 44.5% and 47.8% of the 
NEs are single-word NEs in English and 
Bangla respectively, which suggests that prior 
alignment of the single-word NEs, in addition 
to multi-word NE alignment, should also be 
beneficial to word and phrase alignment. 
Of all the NEs in the training and develop-
ment sets, the transliteration-based alignment 
process was able to establish alignments of 
4,711 single-word NEs, 4,669 two-word NEs 
and 1,745 NEs having length more than two. 
It is to be noted that, some of the single-word 
NE alignments, as well as two-word NE 
alignments, result from multi-word NE align-
ment. 
We analyzed the output of the NE align-
ment module and observed that longer NEs 
were aligned better than the shorter ones, 
which is quite intuitive, as longer NEs have 
more tokens to be considered for intra-NE 
alignment. Since the NE alignment process is 
based on transliteration, the alignment method 
does not work where NEs involve translation 
or acronyms. We also observed that English 
multi-word NEs are sometimes fused together 
into single-word NEs. 
We performed three sets of experiments: 
treating compound verbs as single tokens, 
treating NEs as single tokens, and the combi-
nation thereof. Again for NEs, we carried out 
three types of preprocessing: single-
tokenization of (i) two-word NEs, (ii) more 
than two-word NEs, and (iii) NEs of any 
length. We make distinctions among these 
three to see their relative effects. The devel-
opment and test sets, as well as the target lan-
guage monolingual corpus (for language mod-
eling), are also subjected to the same preproc-
essing of single-tokenizing the MWEs. For 
NE alignment, we performed experiments us-
ing 4 different settings: alignment of (i) NEs 
of length up to two, (ii) NEs of length two, 
51
(iii) NEs of length greater than two, and (iv) 
NEs of any length. Before evaluation, the sin-
gle-token (target language) underscored 
MWEs are expanded back to words compris-
ing the MWEs. 
Since we did not have the gold-standard 
word alignment, we could not perform intrin-
sic evaluation of the word alignment. Instead 
we carry out extrinsic evaluation on the MT 
quality using the well known automatic MT 
evaluation metrics: BLEU (Papineni et al, 
2002), METEOR (Banerjee and Lavie, 2005), 
NIST (Doddington, 2002), WER, PER and 
TER (Snover et al, 2006). As can be seen 
from the evaluation results reported in Table 
2, baseline Moses without any preprocessing 
of the dataset produces a BLEU score of 8.74. 
The low score can be attributed to the fact that 
Bangla, a morphologically rich language, is 
hard to translate into. Moreover, Bangla being 
a relatively free phrase order language (Ekbal 
and Bandyopadhyay, 2009) ideally requires 
multiple set of references for proper evalua-
tion. Hence using a single reference set does 
not justify evaluating translations in Bangla. 
Also the training set was not sufficiently large 
enough for SMT. Treating only longer than 2-
word NEs as single tokens does not help im-
prove the overall performance much, while 
single tokenization  of two-word NEs as single 
tokens produces some improvements (.39 
BLEU points absolute, 4.5% relative). Con-
sidering compound verbs as single tokens 
(CVaST) produces a .82 BLEU point im-
provement (9.4% relative) over the baseline. 
Strangely, when both compound verbs and 
NEs together are counted as single tokens, 
there is hardly any improvement. By contrast, 
automatic NE alignment  (NEA) gives a huge 
impetus to system performance, the best of 
them (4.59 BLEU points absolute, 52.5% rela-
tive improvement) being the alignment of NEs 
of any length that produces the best scores 
across all metrics. When NEA is combined 
with CVaST, the improvements are substan-
tial, but it can not beat the individual im-
provement on NEA. The (?) marked systems 
produce statistically significant improvements 
as measured by bootstrap resampling method 
(Koehn, 2004) on BLEU over the baseline 
system. Metric-wise individual best scores are 
shown in bold in Table 2. 
5 Conclusions and Future Work 
In this paper, we have successfully shown 
how the simple yet effective preprocessing of 
treating two types of MWEs, namely NEs and 
compound verbs, as single-tokens, in conjunc-
tion with prior NE alignment can boost the 
performance of PB-SMT system on an Eng-
lish?Bangla translation task. Treating com-
pound verbs as single-tokens provides signifi-
cant gains over the baseline PB-SMT system. 
Amongst the MWEs, NEs perhaps play the 
most important role in MT, as we have clearly 
demonstrated through experiments that auto-
matic alignment of NEs by means of translit-
eration improves the overall MT performance 
substantially across all automatic MT evalua-
tion metrics. Our best system yields 4.59 
BLEU points improvement over the baseline, 
a 52.5% relative increase. We compared a 
subset of the output of our best system with 
that of the baseline system, and the output of 
our best system almost always looks better in 
terms of either lexical choice or word order-
ing. The fact that only 28.5% of the testset 
NEs appear in the training set, yet prior auto-
matic alignment of the NEs brings about so 
much improvement in terms of MT quality, 
suggests that it not only improves the NE 
alignment quality in the phrase table, but word 
alignment and phrase alignment quality must 
have also been improved significantly. At the 
same time, single-tokenization of MWEs 
makes the dataset sparser, but yet improves 
the quality of MT output to some extent. Data-
driven approaches to MT, specifically for 
scarce-resource language pairs for which very 
little parallel texts are available, should benefit 
from these preprocessing methods. Data 
sparseness is perhaps the reason why single-
tokenization of NEs and compound verbs, 
both individually and in collaboration, did not 
add significantly to the scores. However, a 
significantly large parallel corpus can take 
care of the data sparseness problem introduced 
by the single-tokenization of MWEs. 
The present work offers several avenues for 
further work. In future, we will investigate 
how these automatically aligned NEs can be 
52
used as anchor words to directly influence the 
word alignment process. We will look into 
whether similar kinds of improvements can be 
achieved for larger datasets, corpora from dif-
ferent domains and for other language pairs. 
We will also investigate how NE alignment 
quality can be improved, especially where 
NEs involve translation and acronyms. We 
will also try to perform morphological analy-
sis or stemming on the Bangla side before NE 
alignment. We will also explore whether dis-
criminative approaches to word alignment can 
be employed to improve the precision of the 
NE alignment. 
Acknowledgements 
This research is partially supported by the Sci-
ence Foundation Ireland (Grant 07/CE/I1142) 
as part of the Centre for Next Generation Lo-
calisation (www.cngl.ie) at Dublin City Uni-
versity, and EU projects PANACEA (Grant 
7FP-ITC-248064) and META-NET (Grant 
FP7-ICT-249119). 
References 
Banerjee, Satanjeev, and Alon Lavie. 2005. An 
Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgments. In 
proceedings of the ACL-2005 Workshop on In-
trinsic and Extrinsic Evaluation Measures for 
MT and/or Summarization, pp. 65-72. Ann Ar-
bor, Michigan., pp. 65-72. 
Brown, Peter F., Stephen A. Della Pietra, Vincent 
J. Della Pietra, and Robert L. Mercer. 1993. The 
mathematics of statistical machine translation: 
parameter estimation. Computational Linguis-
tics, 19(2):263-311. 
Carpuat, Marine, and Mona Diab. 2010. Task-
based Evaluation of Multiword Expressions: a 
Pilot Study in Statistical Machine Translation. 
In Proceedings of Human Language Technology 
conference and the North American Chapter of 
the Association for Computational Linguistics 
conference (HLT-NAACL 2010), Los Angeles, 
CA, pp. 242-245. 
Chakrabarti, Debasri, Hemang Mandalia, Ritwik 
Priya, Vaijayanthi Sarma, and Pushpak Bhat-
tacharyya. 2008. Hindi compound verbs and 
their automatic extraction. In Proceedings 
of  the 22nd International Conference on Com-
putational Linguistics (Coling 2008), Posters 
and demonstrations, Manchester, UK, pp. 27-
30. 
Dempster, A.P., N.M. Laird, and D.B. Rubin. 
1977). Maximum Likelihood from Incomplete 
Data via the EM Algorithm. Journal of the 
Royal Statistical Society, Series B (Methodo-
logical) 39 (1): 1?38. 
Doddington, George. 2002. Automatic evaluation 
of machine translation quality using n-gram 
cooccurrence statistics. In Proceedings of the 
Second International Conference on Human 
Language Technology Research (HLT-2002), 
San Diego, CA, pp. 128-132. 
Eck, Matthias, Stephan Vogel, and Alex Waibel. 
2004. Improving statistical machine translation 
in the medical domain using the Unified Medi-
cal Language System. In Proceedings of  the 
20th International Conference on Computational 
Linguistics (COLING 2004), Ge-
neva, Switzerland, pp. 792-798. 
Ekbal, Asif, and Sivaji Bandyopadhyay. 2009. 
Voted NER system using appropriate unlabeled 
data. In proceedings of the ACL-IJCNLP-2009 
Named Entities Workshop (NEWS 2009), 
Suntec, Singapore, pp. 202-210. 
Ekbal, Asif, and Sivaji Bandyopadhyay. 2008. 
Maximum Entropy Approach for Named Entity 
Recognition in Indian Languages. International 
Journal for Computer Processing of Lan-
guages (IJCPOL), Vol. 21(3):205-237. 
Feng, Donghui, Yajuan Lv, and Ming Zhou. 2004. 
A new approach for English-Chinese named en-
tity alignment. In Proceedings of the 2004 Con-
ference on Empirical Methods in Natural Lan-
guage Processing (EMNLP-2004), Barcelona, 
Spain, pp. 372-379. 
Huang, Fei, Stephan Vogel, and Alex Waibel. 
2003. Automatic extraction of named entity 
translingual equivalence based on multi-feature 
cost minimization. In Proceedings of the ACL-
2003 Workshop on Multilingual and Mixed-
language Named Entity Recognition, 2003, 
Sapporo, Japan, pp. 9-16. 
Kneser, Reinhard, and Hermann Ney. 1995. Im-
proved backing-off for m-gram language model-
ing. In Proceedings of the IEEE Internation 
Conference on Acoustics, Speech, and Signal 
Processing (ICASSP), vol. 1, pp. 181-184. De-
troit, MI. 
Koehn, Philipp, Franz Josef Och, and Daniel 
Marcu. 2003. Statistical phrase-based transla-
tion. In Proceedings of HLT-NAACL 2003: 
53
conference combining Human Language Tech-
nology conference series and the North Ameri-
can Chapter of the Association for Computa-
tional Linguistics conference series,  Edmonton, 
Canada, pp. 48-54. 
Koehn, Philipp, Hieu Hoang, Alexandra Birch, 
Chris Callison-Burch, Marcello Federico, Ni-
cola Bertoldi, Brooke Cowan, Wade Shen, 
Christine Moran, Richard Zens, Chris Dyer, 
Ond?ej Bojar, Alexandra Constantin, and Evan 
Herbst. 2007. Moses: open source toolkit for 
statistical machine translation. In Proceedings of 
the 45th Annual meeting of the Association for 
Computational Linguistics (ACL 2007): Pro-
ceedings of demo and poster sessions, Prague, 
Czech Republic, pp. 177-180. 
Koehn, Philipp. 2004. Statistical significance tests 
for machine translation evaluation. In  EMNLP-
2004: Proceedings of the 2004 Conference on 
Empirical Methods in Natural Language Proc-
essing, 25-26 July 2004, Barcelona, Spain, pp. 
388-395. 
Marcu, Daniel. 2001. Towards a Unified Approach 
to Memory- and Statistical-Based Machine 
Translation. In Proceedings of the 39th Annual 
Meeting of the Association for Computational 
Linguistics (ACL 2001), Toulouse, France, pp. 
386-393. 
Moore, Robert C. 2003. Learning translations of 
named-entity phrases from parallel corpora. In 
Proceedings of 10th Conference of the Euro-
pean Chapter of the Association for Computa-
tional Linguistics (EACL 2003), Budapest, 
Hungary; pp. 259-266. 
Och, Franz J. 2003. Minimum error rate training in 
statistical machine translation. In Proceedings of 
the 41st Annual Meeting of the Association for 
Computational Linguistics (ACL-2003), Sap-
poro, Japan, pp. 160-167. 
Papineni, Kishore, Salim Roukos, Todd Ward, and 
Wei-Jing Zhu. 2002. BLEU: a method for 
automatic evaluation of machine translation. In 
Proceedings of the 40th Annual Meeting of the 
Association for Computational Linguistics 
(ACL-2002), Philadelphia, PA, pp. 311-318. 
Ren, Zhixiang, Yajuan L?, Jie Cao, Qun Liu, and 
Yun Huang. 2009. Improving statistical ma-
chine translation using domain bilingual multi-
word expressions. In Proceedings of the 2009 
Workshop on Multiword Expressions, ACL-
IJCNLP 2009, Suntec, Singapore, pp. 47-54. 
Sag, Ivan A., Timothy Baldwin, Francis Bond, 
Ann Copestake and Dan Flickinger. 2002. Mul-
tiword expressions: A pain in the neck for NLP. 
In Proceedings of the 3rd International Confer-
ence on Intelligent Text Processing and Compu-
tational Linguistics (CICLing-2002), Mexico 
City, Mexico, pp. 1-15. 
Snover, Matthew, Bonnie Dorr, Richard Schwartz, 
Linnea Micciulla, and John Makhoul. 2006. A 
study of translation edit rate with targeted hu-
man annotation. In Proceedings of the 7th Con-
ference of the Association for Machine Transla-
tion in the Americas (AMTA 2006), Cambridge, 
MA, pp. 223-231. 
Vogel, Stephan, Hermann Ney, and Christoph 
Tillmann. 1996. HMM-based word alignment in 
statistical translation. In Proceedings of the 16th 
International Conference on Computational 
Linguistics (COLING 1996), Copenhagen, pp. 
836-841. 
Venkatapathy, Sriram, and Aravind K. Joshi. 2006. 
Using information about multi-word expres-
sions for the word-alignment task. In Proceed-
ings of Coling-ACL 2006: Workshop on Multi-
word Expressions: Identifying and Exploiting 
Underlying Properties, Sydney, pp. 20-27. 
Wu, Hua Haifeng Wang, and Chengqing Zong. 
2008. Domain adaptation for statistical machine 
translation with domain dictionary and mono-
lingual corpora. In Proceedings of the 22nd In-
ternational Conference on Computational Lin-
guistics (COLING 2008),  Manchester, UK, pp. 
993-1000. 
54
Proceedings of the Workshop on Distributional Semantics and Compositionality (DiSCo?2011), pages 38?42,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
Shared task system description: Measuring the Compositionality of 
Bigrams using Statistical Methodologies 
Tanmoy Chakraborty, Santanu Pal, Tapabrata Mondal, Tanik Saikh, 
 Sivaji Bandyopadhyay 
Department of Computer Science and Engineering 
Jadavpur University 
its_tanmoy@yahoo.co.in, santanu.pal.ju@gmail.com, 
tapabratamondal@gmail.com, tanik4u@gmail.com, 
sivaji_cse_ju@yahoo.com 
 
Abstract 
The measurement of relative 
compositionality of bigrams is crucial to 
identify Multi-word Expressions 
(MWEs) in Natural Language 
Processing (NLP) tasks. The article 
presents the experiments carried out as 
part of the participation in the shared 
task ?Distributional Semantics and 
Compositionality (DiSCo)? organized as 
part of the DiSCo workshop in ACL-
HLT 2011. The experiments deal with 
various collocation based statistical 
approaches to compute the relative 
compositionality of three types of 
bigram phrases (Adjective-Noun, Verb-
subject and Verb-object combinations). 
The experimental results in terms of both 
fine-grained and coarse-grained 
compositionality scores have been 
evaluated with the human annotated gold 
standard data. Reasonable results have 
been obtained in terms of average point 
difference and coarse precision.  
1 Introduction 
The present work examines the relative 
compositionality of Adjective-Noun (ADJ-NN; 
e.g., blue chip), Verb-subject (V-SUBJ; where 
noun acting as a subject of a verb, e.g., name 
imply) and Verb-object (V-OBJ; where noun 
acting as an object of a verb, e.g., beg question) 
combinations using collocation based statistical 
approaches. Measuring the relative 
compositionality is useful in applications such as 
machine translation where the highly non-
compositional collocations can be handled in a 
special way (Hwang and Sasaki, 2005). 
Multi-word expressions (MWEs) are 
sequences of words that tend to co-occur more 
frequently than chance and are either 
idiosyncratic or decomposable into multiple 
simple words (Baldwin, 2006). Deciding 
idiomaticity of MWEs is highly important for 
machine translation, information retrieval, 
question answering, lexical acquisition, parsing 
and language generation. Compositionality 
refers to the degree to which the meaning of a 
MWE can be predicted by combining the 
meanings of its components. Unlike syntactic 
compositionality (e.g. by and large), semantic 
compositionality is continuous (Baldwin, 2006).   
Several studies have been carried out for 
detecting compositionality of noun-noun MWEs 
using WordNet hypothesis (Baldwin et al, 
2003), verb-particle constructions using 
statistical similarities (Bannard et al, 2003; 
McCarthy et al, 2003) and verb-noun pairs 
using Latent Semantic Analysis (Katz and 
Giesbrecht, 2006).  
Our contributions are two-fold: firstly, we 
experimentally show that collocation based 
statistical compositionality measurement can 
assist in identifying the continuum of 
compositionality of MWEs. Secondly, we show 
that supervised weighted parameter tuning 
results in accuracy that is comparable to the best 
manually selected combination of parameters.  
38
2 Proposed Methodologies 
The present task was to identify the numerical 
judgment of compositionality of individual 
phrase. The statistical co-occurrence features 
used in this experiment are described.     
Frequency:  If two words occur together 
quite frequently, the lexical meaning of the 
composition may be different from the 
combination of their individual meanings. The 
frequency of an individual phrase is directly 
used in the following methods. 
Point-wise Information (PMI): An 
information-theoretic motivated measure for 
discovering interesting collocations is point-wise 
mutual information (Church and Hanks, 1990). 
It is originally defined as the mutual information 
between particular events X and Y and in our 
case the occurrence of particular words, as 
follows: 
   = log ,. ? log ,.   1  
PMI represents the amount of information 
provided by the occurrence of the event 
represented by X about the occurrence of the 
event represented by Y. 
T-test:  T-test has been widely used for 
collocation discovery. This statistical test tells us 
the probability of a certain constellation 
(Nugues, 2006). It looks at the mean and 
variance of a sample of measurements. The null 
hypothesis is that the sample is drawn from a 
distribution with mean. T-score is computed 
using the equation (2): 
,  = Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 93?100,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Bootstrapping Method for Chunk Alignment in Phrase Based SMT 
 
Santanu Pal Sivaji Bandyopadhyay 
  
Department of Computer Science and Engi-
neering 
Department of Computer Science and Engi-
neering 
Jadavpur University Jadavpur University 
santanu.pal.ju@gmail.com sivaji_cse@yahoo.com 
 
 
 
 
 
 
Abstract 
The processing of parallel corpus plays 
very crucial role for improving the over-
all performance in Phrase Based Statisti-
cal Machine Translation systems (PB-
SMT). In this paper the automatic align-
ments   of different kind of chunks have 
been studied that boosts up the word 
alignment as well as the machine transla-
tion quality. Single-tokenization of 
Noun-noun MWEs, phrasal preposition 
(source side only) and reduplicated 
phrases (target side only) and the align-
ment of named entities and complex 
predicates provide the best SMT model 
for bootstrapping. Automatic bootstrap-
ping on the alignment of various chunks 
makes significant gains over the previous 
best English-Bengali PB-SMT system. 
The source chunks are translated into the 
target language using the PB-SMT sys-
tem and the translated chunks are com-
pared with the original target chunk. The 
aligned chunks increase the size of the 
parallel corpus. The processes are run in 
a bootstrapping manner until all the 
source chunks have been aligned with the 
target chunks or no new chunk alignment 
is identified by the bootstrapping process. 
The proposed system achieves significant 
improvements (2.25 BLEU over the best 
System and 8.63 BLEU points absolute 
over the baseline system, 98.74% relative 
improvement over the baseline system) 
on an English- Bengali translation task.  
1 Introduction 
The objective of the present research work is to 
analyze effects of chunk alignment in English ? 
Bengali parallel corpus in a Phrase Based Statis-
tical Machine Translation system. The initial sen-
tence level aligned English-Bengali corpus is 
cleaned and filtered using a semi-automatic 
process. More effective chunk level alignments 
are carried out by bootstrapping on the training 
corpus to the PB-SMT system. 
The objective in the present task is to align the 
chunks in a bootstrapping manner using a Single 
tokenized MWE aligned SMT model and then 
modifying the model by inserting the aligned 
chunks to the parallel corpus after  each iteration 
of the bootstrapping process, thereby enhancing 
the performance of the SMT system. In turn, this 
method deals with the many-to-many word 
alignments in the parallel corpus. Several types 
of MWEs like phrasal prepositions and Verb-
object combinations are automatically identified 
on the source side while named-entities and 
complex predicates are identified on both sides 
of the parallel corpus. In the target side only, 
identification of the Noun-noun MWEs and re-
duplicated phrases are carried out. Simple rule-
based and statistical approaches have been used 
to identify these MWEs. The parallel corpus is 
modified by considering the MWEs as single 
tokens. Source and target language NEs are 
aligned using a statistical transliteration tech-
nique. These automatically aligned NEs and 
Complex predicates are treated as translation ex-
amples, i.e., as additional entries in the phrase 
table (Pal.et al2010, 2011). Using this aug-
mented phrase table each individual source 
chunk is translated into the target chunk and then 
validated with the target chunks on the target 
side. The validated source-target chunks are con-
93
sidered as further parallel examples, which in 
effect are instances of atomic translation pairs to 
the parallel corpus. This is a well-known practice 
in domain adaptation in SMT (Eck et al, 2004; 
Wu et al, 2008).  The preprocessing of the paral-
lel corpus results in improved MT quality in 
terms of automatic MT evaluation metrics. 
The remainder of the paper is organized as fol-
lows. Section 2 briefly elaborates the related 
work. The PB-SMT system is described in Sec-
tion 3. The resources used in the present work 
are described in Section 4. The various experi-
ments carried out and the corresponding evalua-
tion results have been reported in Section 5. The 
conclusions are drawn in Section 6 along with 
future work roadmap.  
2 Related work 
A multi lingual filtering algorithm generates bi-
lingual chunk alignment from Chinese-English 
parallel corpus (Zhou.et al 2004). The algorithm 
has  three steps, first, the most frequent bilingual 
chunks are extracted from the parallel corpus, 
second, a clustering algorithm has been used for 
combining chunks which are participating for 
alignment and finally one English chunk is gen-
erated corresponding to a Chinese chunk by ana-
lyzing the highest co-occurrences of English 
chunks. Bilingual knowledge can be extracted 
using chunk alignment (Zhou.et al 2004). The 
alignment strategies include the comparison of 
dependency relations between source and target 
sentences. The dependency related candidates are 
then compared with the bilingual dictionary and 
finally the chunk is aligned using the extracted 
dependency related words. Ma.et al (2007) sim-
plified the task of automatic word alignment as 
several consecutive words together correspond to 
a single word in the opposite language by using 
the word aligner itself, i.e., by bootstrapping on 
its output. Zhu and Chang (2008) extracted a dic-
tionary from the aligned corpus, used the dic-
tionary to re-align the corpus and then extracted 
the new dictionary from the new alignment re-
sult. The process goes on until the threshold is 
reached.  
An automatic extraction of bilingual MWEs is 
carried out by Ren et al (2009), using a log like-
lihood ratio based hierarchical reducing algo-
rithm to investigate the usefulness of bilingual 
MWEs in SMT by integrating bilingual MWEs 
into the Moses decoder (Koehn et al, 2007). The 
system has observed the highest improvement 
with an additional feature that identifies whether 
or not a bilingual phrase contains bilingual 
MWEs. This approach was generalized in Car-
puat and Diab (2010) where the binary feature is 
replaced by a count feature which is representing 
the number of MWEs in the source language 
phrase. 
MWEs on the source and the target sides 
should be both aligned in the parallel corpus and 
translated as a whole. However, in the state-of-
the-art PB-SMT systems, the constituents of an 
MWE are marked and aligned as parts of con-
secutive phrases, since PB-SMT (or any other 
approaches to SMT) does not generally treat 
MWEs as special tokens. Another problem with 
SMT systems is the wrong translation of some 
phrases. Sometimes some phrases are not found 
in the output sentence. Moreover, the source and 
target phrases are mostly many-to-many, particu-
larly so for the English?Bengali language pair.  
The main objective of the present work is to see 
whether prior automatic alignment of chunks can 
bring any improvement in the overall perform-
ance of the MT system.  
3 PB-SMT System Description 
The system follows three steps; the first step is 
prepared an SMT system with improved word 
alignment that produces a best SMT model for 
bootstrapping. And the second step is produced a 
chunk level parallel corpus by using the best 
SMT model. These chunk level parallel corpuses 
are added with the training corpus to generate the 
new SMT model in first iteration. And finally the 
whole process repeats to achieve better chunk 
level alignments as well as the better SMT 
model. 
3.1 SMT System with improved Word 
Alignment 
The initial English-Bengali parallel corpus is 
cleaned and filtered using a semi-automatic 
process.  Complex predicates are first extracted 
on both sides of the parallel corpus. The analysis 
and identification of various complex predicates 
like, compound verbs (Verb + Verb), conjunct 
verbs (Noun /Adjective/Adverb + Verb) and se-
rial verbs (Verb + Verb + Verb) in Bengali are 
done following the strategy in Das.et al (2010). 
 Named-Entities and complex predicates are 
aligned following a similar technique as reported 
in Pal.et al(2011). Reduplicated phrases do not 
occur very frequently in the English corpus; 
some of them (like correlatives, semantic redu-
plications) are not found in English (Chakraborty 
94
and Bandyopadhyay, 2010).  But reduplication 
plays a crucial role on the target Bengali side as 
they occur with high frequency. These redupli-
cated phrases are considered as a single-token so 
that they may map to a single word on the source 
side. Phrasal prepositions and verb object combi-
nations are also treated as single tokens. Once the 
compound verbs and the NEs are identified on 
both sides of the parallel corpus, they are assem-
bled into single tokens. When converting these 
MWEs into single tokens, the spaces are replaced 
with underscores (?_?). Since there are already 
some hyphenated words in the corpus, hyphena-
tion is not used for this purpose. Besides, the use 
of a special word separator (underscore in this 
case) facilitates the job of deciding which single-
token MWEs to be de-tokenized into its constitu-
ent words, before evaluation. 
3.1.1 MWE Identification on Source Side 
 The UCREL1 Semantic analysis System 
(USAS) developed by Lancaster University 
(Rayson.et al 2004) has been adopted for MWE 
identification.  The USAS is a software tool for 
the automatic semantic analysis of English spo-
ken and written data. Various types of Multi-
Word Units (MWU) that are identified by the 
USAS software include: verb-object combina-
tions (e.g. stubbed out), noun phrases (e.g. riding 
boots), proper names (e.g. United States of 
America), true idioms (e.g. living the life of Ri-
ley) etc. In English, Noun-Noun (NN) com-
pounds, i.e., noun phrases occur with high fre-
quency and high lexical and semantic variability 
(Tanaka.et al 2003). The USAS software has a 
reported precision value of 91%. 
3.1.2 MWE Identification on Target Side 
Compound nouns are identified on the target 
side. Compound nouns are nominal compounds 
where two or more nouns are combined to form a 
single phrase such as ?golf club? or ?computer 
science department? (Baldwin.et al 2010). Each 
element in a compound noun can function as a 
lexeme in independent of the other lexemes in 
different context. The system uses Point-wise 
Mutual Information (PMI), Log-likelihood Ratio 
(LLR) and Phi-coefficient, Co-occurrence meas-
urement and Significance function (Agarwal.et 
al, 2004) measures for identification of com-
pound nouns. Final evaluation has been carried 
out by combining the results of all the methods. 
A predefined cut-off score has been considered 
                                                        
1  http://www.comp.lancs.ac.uk/ucrel 
and the candidates having scores above the 
threshold value have been considered as MWEs. 
The repetition of noun, pronoun, adjective and 
verb are generally classified as two categories: 
repetition at the (a) expression level and at the 
(b) contents or semantic level. In case of Bengali, 
The expression-level reduplication are classified 
into five fine-grained subcategories:  (i) Ono-
matopoeic expressions (khat khat, knock knock), 
(ii) Complete Reduplication (bara-bara, big big), 
(iii) Partial Reduplication (thakur-thukur, God), 
(iv) Semantic Reduplication (matha-mundu, 
head) and (v) Correlative Reduplication 
(maramari, fighting). 
For identifying reduplications, simple rules 
and morphological properties at lexical level 
have been used (Chakraborty and Bandyop-
adhyay, 2010). The Bengali monolingual dic-
tionary has been used for identification of seman-
tic reduplications.  
An NE and Complex Predicates parallel cor-
pus is created by extracting the source and the 
target (single token) NEs from the NE-tagged 
parallel corpus and aligning the NEs using the 
strategies as applied in (Pal.et al 2010, 2011).  
3.1.3 Verb Chunk / Complex Predicate 
Alignment 
Initially, it is assumed that all the members of the 
English verb chunk in an aligned sentence pair 
are aligned with the members of the Bengali 
complex predicates. Verb chunks are aligned 
using a statistical aligner. A pattern generator 
extracts patterns from the source and the target 
side based on the correct alignment list.  The root 
form of the main verb, auxiliary verb present in 
the verb chunk and the associated tense, aspect 
and modality information are extracted for the 
source side token. Similarly, root form of the 
Bengali verb and the associated vibhakti (inflec-
tion) are identified on the target side token. Simi-
lar patterns are extracted for each alignment in 
the doubtful alignment list.  
Each pattern alignment for the entries in the 
doubtful alignment list is checked with the pat-
terns identified in the correct alignment list. If 
both the source and the target side patterns for a 
doubtful alignment match with the source and the 
target side patterns of a correct alignment, then 
the doubtful alignment is considered as a correct 
one.  
The doubtful alignment list is checked again to 
look for a single doubtful alignment for a sen-
tence pair. Such doubtful alignments are consid-
ered as correct alignment. 
95
The above alignment list as well as NE 
aligned lists are added with the parallel corpus 
for creating the SMT model for chunk alignment. 
The system has reported 15.12 BLEU score for 
test corpus and 6.38 (73% relative) point im-
provement over the baseline system (Pal.et al 
2011). 
3.2 Automatic chunk alignment 
 
3.2.1 Source chunk extraction 
The source corpus is preprocessed after identify-
ing the MWEs using the UCREL tool and single 
tokenizing the extracted MWEs. The source sen-
tences of the parallel corpus have been parsed 
using Stanford POS tagger and then the chunks 
of the sentences are extracted using CRF chun-
ker2 The CRF chunker detects the chunk bounda-
ries of noun, verb, adjective, adverb and preposi-
tional chunks from the sentences. After detection 
of the individual chunks by the CRF chunker, the 
boundary of the prepositional phrase chunks are 
expanded by examining the series of  noun 
chunks separated by conjunctions such as 
'comma', 'and' etc. or a single noun chunk fol-
lowed by a preposition.  For each individual 
chunk, the head words are identified. A synony-
mous bag of words is generated for each head 
word. These bags of words produce more alter-
native chunks which are decoded using the best 
SMT based system (Section 3.1). Additional 
translated target chunks for a single source chunk 
are generated. 
 
CRF Chunker output 
 
bodies/NNS/B-NP of/IN/B-PP all/DT/B-NP 
ages/NNS/I-NP ,/,/O colors/NNS/I-NP and/CC/O 
sizes/NNS/I-NP don/VB/B-VP the/DT/B-NP 
very/JJ/I-NP minimum/NN/I-NP in/IN/B-PP beach-
wear/NN/B-NP and/CC/O idle/VB/B-VP away/RP/B-
PRT the/DT/B-NP days/NNS/I-NP on/IN/B-PP 
the/DT/B-NP sun/NN/I-NP kissed/VBN/I-NP co-
pacabana/NN/I-NP and/CC/O ipanema/NN/I-NP 
beaches/NNS/I-NP ././O  
 
Noun chunk Expansion and boundary detection 
 
(bodies/NNS/B-NP) (of/IN/B-PP) (all/DT/B-NP 
ages/NNS/I-NP ,/,/I-NP colors/NNS/I-NP and/CC/I-
NP sizes/NNS/I-NP) (don/VB/B-VP) (the/DT/B-NP 
very/JJ/I-NP minimum/NN/I-NP) (in/IN/B-PP) 
(beachwear/NN/B-NP) (and/CC/B-O) (idle/VB/B-VP) 
(away/RP/B-PRT) (the/DT/B-NP days/NNS/I-NP) 
                                                        
2  http://crfchunker.sourceforge.net/ 
(on/IN/B-PP) (the/DT/B-NP sun/NN/I-NP 
kissed/VBN/I-NP copacabana/NN/I-NP and/CC/I-NP 
ipanema/NN/I-NP beaches/NNS/I-NP) (././B-O) 
 
Prepositional phrase expansion and extraction 
 
bodies 
of all ages , colors and sizes 
don 
the very minimum 
in beachwear 
and 
idle 
away 
the days 
on the sun kissed copacabana and ipanema 
beaches  
 
 
Figure 1.System architecture of the Automatic chunk 
alignment model  
3.2.2 Target chunk extraction 
The target side of the parallel corpus is cleaned 
and parsed using the shallow parser developed by 
the consortia mode project ?Development of In-
dian Language to Indian Language Machine 
Translation (IL-ILMT) System Phase II? funded 
by Department of Information Technology, Gov-
ernment of India. The individual chunks are ex-
tracted from the parsed output. The individual 
chunk boundary is expanded if any noun chunk 
contains only single word and several noun 
chunks occur consecutively.  The content of the 
individual chunks are examined by checking 
their POS categories. At the time of boundary 
expansion, if the system detects other POS cate-
gory words except noun or conjunction then the 
expansion process stops immediately and new 
chunk boundary beginning is identified. The IL-
ILMT system generates the head word for each 
individual chunk. The chunks for each sentence 
are stored in a separate list. This list is used as a 
96
validation resource for validate the output of the 
statistical chunk aligner. 
3.2.3 Source-Target chunk Alignment 
The extracted source chunks are translated using 
the generated SMT model. The translated chunks 
as well as their alternatives are validated with the 
original target chunk. During validation check-
ing, if any match is found between the translated 
chunk and the target chunk then the source chunk 
is directly aligned with the original target chunk. 
Otherwise, the source chunk is ignored in the 
current iteration for any possible alignment. The 
source chunk will be considered in the next 
alignment. After the current iteration is com-
pleted, two lists are produced: a chunk level 
alignment list and an unaligned source chunk list. 
The produced alignment lists are added with the 
parallel corpus as the additional training corpus 
to produce new SMT model for the next iteration 
process. The next iteration process translates the 
source chunks that are in the unaligned list pro-
duced by the previous iteration. This process 
continues until the unaligned source chunk list is 
empty or no further alignment is identified.  
3.2.4 Source-Target chunk Validation 
The translated target chunks are validated with 
the original target list of the same sentence. The 
extracted noun, verb, adjective, adverb and 
prepositional chunks of the source side may not 
have a one to one correspondence with the target 
side except for the verb chunk. There is no con-
cept of prepositional chunks on the target side. 
Some time adjective or adverb chunks may be 
treated as noun chunk on the target side. So, 
chunk level validation for individual categories 
of chunks is not possible. Source side verb 
chunks are compared with the target side verb 
chunks while all the other chunks on the source 
side are compared with all the other chunks on 
the target side. Head words are extracted for each 
source chunk and the translated head words are 
actually compared on the target side taking into 
the consideration the synonymous target words. 
When the validation system returns positive, the 
source chunk is aligned with the identified origi-
nal target chunk.  
4 Tools and Resources used 
A sentence-aligned English-Bengali parallel cor-
pus containing 14,187 parallel sentences from the 
travel and tourism domain has been used in the 
present work. The corpus has been collected 
from the consortium-mode project ?Development 
of English to Indian Languages Machine Trans-
lation (EILMT) System Phase II3?. The Stanford 
Parser4, Stanford NER, CRF chunker5 and the 
Wordnet 3.06 have been used for identifying 
complex predicates in the source English side of 
the parallel corpus.  
The sentences on the target side (Bengali) are 
parsed and POS-tagged by using the tools ob-
tained from the consortium mode project ?De-
velopment of Indian Language to Indian Lan-
guage Machine Translation (IL-ILMT) System 
Phase II?. NEs in Bengali are identified using the 
NER system of Ekbal and Bandyopadhyay 
(2008).  
The effectiveness of the MWE-aligned and 
chunk aligned parallel corpus is demonstrated by 
using the standard log-linear PB-SMT model as 
our baseline system: GIZA++ implementation of 
IBM word alignment model 4, phrase-extraction 
heuristics described in (Koehn et al, 2003), 
minimum-error-rate training (Och, 2003) on a 
held-out development set, target language model 
trained using SRILM toolkit  (Stolcke, 2002) 
with Kneser-Ney smoothing (Kneser and Ney, 
1995) and the Moses decoder (Koehn et al, 
2007). 
5 Experiments and Evaluation Results 
We have randomly identified 500 sentences each 
for the development set and the test set from the 
initial parallel corpus. The rest are considered as 
the training corpus. The training corpus was fil-
tered with the maximum allowable sentence 
length of 100 words and sentence length ratio of 
1:2 (either way). Finally the training corpus con-
tains 13,176 sentences. In addition to the target 
side of the parallel corpus, a monolingual Ben-
gali corpus containing 293,207 words from the 
tourism domain was used for the target language 
model. The experiments have been carried out 
with different n-gram settings for the language 
model and the maximum phrase length and found 
that a 4-gram language model and a maximum 
phrase length of 4 produce the optimum baseline 
result. The rest of the experiments have been car-
ried out using these settings. 
                                                        
3    The EILMT and ILILMT projects are funded by 
the Department of Information Technology (DIT), Ministry 
of Communications and Information Technology (MCIT), 
Government of India. 
4    http://nlp.stanford.edu/software/lex-parser.shtml 
5    http://crfchunker.sourceforge.net/ 
6    http://wordnet.princeton.edu/ 
97
The system continues with the various pre-
processing of the corpus. The hypothesis is that 
as more and more MWEs and chunks are identi-
fied and aligned properly, the system shows the 
improvement in the translation procedure. Table 
1 shows the MWE statistics of the parallel train-
ing corpus. It is observed from Table 1 that NEs 
occur with high frequency in both sides com-
pared to other types of MWEs. It suggests that 
prior alignment of the NEs and complex predi-
cates plays a role in improving the system per-
formance. 
 
 
English Bengali Training set 
T U T U 
CPs 4874 2289 14174 7154 
redupli-
cated word 
- - 85 50 
Noun-noun 
compound 
892 711 489 300 
Phrasal 
preposition 
982 779 - - 
Phrasal 
verb 
549 532 - - 
Total NE 
words 
22931 8273 17107 9106 
 
Table 1. MWE Statistics. (T - Total occurrence, 
U ? Unique, CP ? complex predicates, NE ? 
Named Entities) 
 
 
Single tokenization of NEs and MWEs of any 
length on both the sides followed by GIZA++ 
alignment has given a huge impetus to system 
performance (6.38 BLEU points absolute, 73% 
relative improvement over the baseline). In the 
source side, the system treats the phrasal preposi-
tions, verb-object combinations and noun-noun 
compounds as a single token. In the target side, 
single tokenization of reduplicated phrases and 
noun-noun compounds has been done followed 
by alignments using the GIZA++ tool. From the 
observation of Table 2, during first iteration there 
are 81821 chunks are identified from the source 
corpus and 14534 has been aligned by the sys-
tem. For iteration 2, there are 67287 source 
chunks are remaining to align. At the final itera-
tion almost 65% of the source chunks have been 
aligned. 
 
 
 
Training 
set 
English Bengali 
Iteration T U T U 
1 81821 70321 65429 59627 
2 67287 62575 50895 47139 
final 32325 31409 15933 15654 
 
Table 2. Chunk Statistics. (T - Total occurrence, 
U ? Unique) 
 
The system performance improves when the 
alignment list of NEs and complex predicates as 
well as sentence level aligned chunk are incorpo-
rated in the baseline best system. It achieves the 
BLEU score of 17.37 after the final iteration. 
This is the best result obtained so far with respect 
to the baseline system (8.63 BLEU points abso-
lute, 98.74% relative improvement in Table 3). It 
may be observed from Table 3 that baseline 
Moses without any preprocessing of the dataset 
produces a BLEU score of 8.74. 
 
Experiments Exp BLEU NIST 
Baseline 1 8.74 3.98 
Best System (Alignment 
of NEs and Complex 
Predicates and Single 
Tokenization of various 
MWEs) 
2 15.12 4.48 
Iteration 1 3 15.87 4.49 
Iteration 2 4 16.28 4.51 
Iteration 3 5 16.40 4.51 
Iteration 4 6 16.68 4.52 
Base-
line 
Best 
Sys-
tem + 
Chunk 
Align
ment 
Final Iteration? 7 17.37 4.55 
 
Table 3.  Evaluation results for different experi-
mental setups. (The ??? marked systems produce 
statistically significant improvements on BLEU 
over the baseline system) 
 
Intrinsic evaluation of the chunk alignment 
could not be performed as gold-standard word 
alignment was not available. Thus, extrinsic 
evaluation was carried out on the MT quality 
using the well known automatic MT evaluation 
metrics: BLEU (Papineni et al, 2002) and NIST 
(Doddington, 2002). Bengali is a morphologi-
cally rich language and has relatively free phrase 
order. Proper evaluation of the English-Bengali 
98
MT evaluation ideally requires multiple set of 
reference translations. Moreover, the training set 
was smaller in size.  
6. Conclusions and Future work 
A methodology has been presented in this paper  
to show how the simple yet effective preprocess-
ing of various types of MWEs and alignment of 
NEs, complex predicates and chunks can boost 
the performance of PB-SMT system on an Eng-
lish?Bengali translation task. The best system 
yields 8.63 BLEU points improvement over the 
baseline, a 98.74% relative increase.  A subset of 
the output from the best system has been com-
pared with that of the baseline system, and the 
output of the best system almost always looks 
better in terms of either lexical choice or word 
ordering. It is observed that only 28.5% of the 
test set NEs appear in the training set, yet prior 
automatic alignment of the NEs complex predi-
cates and chunk improves the translation quality. 
This suggests that not only the NE alignment 
quality in the phrase table but also the word 
alignment and phrase alignment quality improves 
significantly. At the same time, single-
tokenization of MWEs makes the dataset sparser, 
but improves the quality of MT output to some 
extent. Data-driven approaches to MT, specifi-
cally for scarce-resource language pairs for 
which very little parallel texts are available, 
should benefit from these preprocessing meth-
ods. Data sparseness is perhaps the reason why 
single-tokenization of NEs and compound verbs, 
both individually and in collaboration, did not 
add significantly to the scores. However, a sig-
nificantly large parallel corpus can take care of 
the data sparseness problem introduced by the 
single-tokenization of MWEs. 
Acknowledgement 
The work has been carried out with support from 
the consortium-mode project ?Development of 
English to Indian Languages Machine Transla-
tion (EILMT) System funded by Department of 
Information Technology, Government of India. 
References 
Agarwal, Aswini, Biswajit Ray, Monojit Choudhury, 
Sudeshna Sarkar and Anupam Basu. Automatic 
Extraction of Multiword Expressions in Bengali: 
An Approach for Miserly Resource Scenario. In 
Proc. of International Conference on Natural Lan-
guage Processing (ICON), pp. 165-174.( 2004) 
Baldwin, Timothy and Su Nam Kim Multiword Ex-
pressions, in Nitin Indurkhya and Fred J. Damerau 
(eds.) Handbook of Natural Language Processing,  
Second Edition, CRC Press, Boca Raton, USA, pp. 
267?292 (2010) 
Banerjee, Satanjeev, and Alon Lavie.. An Automatic 
Metric for MT Evaluation with Improved Correla-
tion with Human Judgments. In proceedings of the 
ACL-2005 Workshop on Intrinsic and Extrinsic 
Evaluation Measures for MT and/or Summariza-
tion, pp. 65-72. Ann Arbor, Michigan., pp. 65-72. 
(2005) 
Carpuat, Marine, and Mona Diab. Task-based Evalua-
tion of Multiword Expressions: a Pilot Study in 
Statistical Machine Translation. In Proc. of Human 
Language Technology conference and the North 
American Chapter of the Association for Computa-
tional Linguistics conference (HLT-NAACL 
2010), Los Angeles, CA (2010) 
Chakraborty, Tanmoy and Sivaji Bandyopadhyay. 
Identification of Reduplication in Bengali Corpus 
and their Semantic Analysis: A Rule Based Ap-
proach. In proc. of the 23rd International Confer-
ence on Computational Linguistics (COLING 
2010), Workshop on Multiword Expressions: from 
Theory to Applications (MWE 2010). Beijing, 
China. (2010) 
Das, Dipankar, Santanu Pal, Tapabrata Mondal, Tan-
moy Chakraborty, Sivaji Bandyopadhyay. Auto-
matic Extraction of Complex Predicates in Bengali 
In proc. of the workshop on Multiword expression: 
from theory to application (MWE-2010), The 23rd 
International conference of computational linguis-
tics (Coling 2010),Beijing, Chaina, pp. 37-
46.(2010) 
Doddington, George. Automatic evaluation of ma-
chine translation quality using n-gram cooccur-
rence statistics. In Proc. of the Second International 
Conference on Human Language Technology Re-
search (HLT-2002), San Diego, CA, pp. 128-
132(2002) 
Eck, Matthias, Stephan Vogel, and Alex Waibel. Im-
proving statistical machine translation in the medi-
cal domain using the Unified Medical Language 
System. In Proc. of the 20th International Confer-
ence on Computational Linguistics (COLING 
2004), Geneva, Switzerland, pp. 792-798 (2004) 
Ekbal, Asif, and Sivaji Bandyopadhyay. Voted NER 
system using appropriate unlabeled data. In proc. 
of the ACL-IJCNLP-2009 Named Entities Work-
shop (NEWS 2009), Suntec, Singapore, pp.202-
210 (2009). 
Huang, Young-Sook, Kyonghee Paik, Yutaka Sasaki, 
?Bilingual Knowledge Extraction Using Chunk 
Alignment?, PACLIC 18, Tokiyo, pp. 127-138, 
(2004). 
99
Kneser, Reinhard, and Hermann Ney. Improved back-
ing-off for m-gram language modeling. In Proc. of 
the IEEE Internation Conference on Acoustics, 
Speech, and Signal Processing (ICASSP), vol. 1, 
pp. 181?184. Detroit, MI. (1995) 
Koehn, Philipp, Franz Josef Och, and Daniel Marcu. 
Statistical phrase-based translation. In Proc. of 
HLT-NAACL 2003: conference combining Human 
Language Technology conference series and the 
North American Chapter of the Association for 
Computational Linguistics conference series,  Ed-
monton, Canada, pp. 48-54. (2003) 
Koehn, Philipp, Hieu Hoang, Alexandra Birch, Chris 
Callison-Burch, Marcello Federico, Nicola Ber-
toldi, Brooke Cowan, Wade Shen, Christine 
Moran, Richard Zens, Chris Dyer, Ond?ej Bojar, 
Alexandra Constantin, and Evan Herbst. Moses: 
open source toolkit for statistical machine transla-
tion. In Proc. of the 45th Annual meeting of the 
Association for Computational Linguistics (ACL 
2007): Proc. of demo and poster sessions, Prague, 
Czech Republic, pp. 177-180. (2007) 
Koehn, Philipp. Statistical significance tests for ma-
chine translation evaluation. In  EMNLP-2004: 
Proc. of the 2004 Conference on Empirical Meth-
ods in Natural Language Processing, 25-26 July 
2004, Barcelona, Spain, pp 388-395. (2004) 
Ma, Yanjun, Nicolas Stroppa, AndyWay. Proceedings 
of the 45th Annual Meeting of the Association of 
Computational Linguistics, ,Prague, Czech Repub-
lic, June 2007, pp. 304?311 (2007). 
Moore, Robert C. Learning translations of named-
entity phrases from parallel corpora. In Proc. of 
10th Conference of the European Chapter of the 
Association for Computational Linguistics (EACL 
2003), Budapest, Hungary; pp. 259-266. (2003) 
Och, Franz J. Minimum error rate training in statisti-
cal machine translation. In Proc. of the 41st Annual 
Meeting of the Association for Computational Lin-
guistics (ACL-2003), Sapporo, Japan, pp. 160-167. 
(2003) 
Pal Santanu, Sudip Kumar Naskar, Pavel Pecina, 
Sivaji Bandyopadhyay and Andy Way. Handling 
Named Entities and Compound Verbs in Phrase-
Based Statistical Machine Translation, In proc. of 
the workshop on Multiword expression: from the-
ory to application (MWE-2010), The 23rd Interna-
tional conference of computational linguistics (Col-
ing 2010),Beijing, Chaina, pp. 46-54 (2010) 
Pal, Santanu Tanmoy Chakraborty , Sivaji Bandyop-
adhyay, ?Handling Multiword Expressions in 
Phrase-Based Statistical Machine Translation?, 
Machine Translation Summit XIII(2011),Xiamen, 
China, pp. 215-224 (2011) 
Papineni, Kishore, Salim Roukos, Todd Ward, and 
Wei-Jing Zhu. BLEU: a method for automatic 
evaluation of machine translation. In Proc. of the 
40th Annual Meeting of the Association for Com-
putational Linguistics (ACL-2002), Philadelphia, 
PA, pp. 311-318 (2002) 
Rayson, Paul, Dawn Archer, Scott Piao, and Tony 
McEnery. The UCREL Semantic Analysis System. 
In proc. Of LREC-04 Workshop: Beyond Named 
Entity Recognition Semantic Labeling for NLP 
Tasks, pages 7-12, Lisbon, Porugal (2004) 
Ren, Zhixiang, Yajuan L?, Jie Cao, Qun Liu, and Yun 
Huang. Improving statistical machine translation 
using domain bilingual multiword expressions. 
In Proc. of the 2009 Workshop on Multiword Ex-
pressions, ACL-IJCNLP 2009, Suntec, Singapore, 
pp. 47-54 (2009). 
Stolcke, A. SRILM?An Extensible Language Mod-
eling Toolkit. Proc. Intl. Conf. on Spoken Lan-
guage Processing, vol. 2, pp. 901?904, Denver 
(2002). 
Tanaka, Takaaki and Timothy Baldwin. Noun- Noun 
Compound Machine Translation: A Feasibility 
Study on Shallow Processing. In Proc. of the Asso-
ciation for Computational Linguistics- 2003, 
Workshop on Multiword Expressions: Analysis, 
Acquisition and Treatment, Sapporo, Japan, pp. 
17?24 (2003) 
Wu, Hua Haifeng Wang, and Chengqing Zong. Do-
main adaptation for statistical machine translation 
with domain dictionary and monolingual cor-
pora. In Proc. of the 22nd International Conference 
on Computational Linguistics (COLING 
2008),  Manchester, UK, pp. 993-1000 (2008) 
Xuan-Hieu Phan, "CRFChunker: CRF English Phrase 
Chunker", http://crfchunker.sourceforge.net/, 
(2006) 
Zhou, Yu, chengquing Zong, Bo Xu, ?Bilingual 
Chunk Aliment in Statistical Machine Translation?,  
IEEE International Conference on Systems, Man 
and Cybernetics, pp. 1401-1406, (2004)
 
100
The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 201?207,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Detection and Correction of Preposition and  Determiner Errors in English: HOO 2012   Pinaki Bhaskar Aniruddha Ghosh Santanu Pal Sivaji Bandyopadhyay Department of Computer Science and Engineering, Jadavpur University 188, Raja S. C. Mallick Road Kolkata ? 700032, India pinaki.bhaskar @gmail.com arghyaonline @gnail.com santanu.pal.ju @gmail.com sivaji_cse_ju @yahoo.com       Abstract 
This paper reports on our work in the HOO 2012 shared task. The task is to automatically detect, recognize and correct the errors in the use of prepositions and determiners in a set of given test documents in English. For that, we have developed a hybrid system of an n-gram statistical model along with some rule-based techniques. The system has been trained on the HOO shared task?s training datasets and run on the test set given. We have submitted one run, which has demonstrated an F-score of 7.1, 6.46 and 2.58 for detection, recognition and correction respectively before revision and F-score of 8.22, 7.59 and 3.16 for detec-tion, recognition and correction respectively after revision. 
1 Introduction Writing research papers or theses in English is a very challenging task for those researchers and scientists whose first language or mother tongue is not English. Depicting their research works proper-ly in English is a hard job for them. Generally their papers, which are submitted to conferences, may be rejected not because of their research works but because of the English writing, which makes the papers harder for the reviewer to understand the intentions of author. This kind of problem will be faced in any field where someone has to provide 
material in a language other than his/her first lan-guage. The mentoring service of Association for Com-putational Linguistics (ACL) is one part of a re-sponse. This service can address a wider range of problems than those related purely to writing. The aim of this service is that a research paper should be judged only on its research content. The organizer of ?Help Our Own? (HOO) pro-posed and initiated a shared task in 2011 (Dale and Kilgarriff, 2010), which attempts to tackle the problem by developing tools or techniques for the non-native speaker of English, which will automat-ically correct the English prose of the papers so that they can be accepted. This tools and tech-niques may also help native English speakers. This task is simply expressed as text-to-text generation or Natural language Generation (NLG). In the 2011 shared task, all possible errors were covered which made the task enormously huge. In 2012, the task is more specific and only deals with de-terminers and prepositions as described in (Dale and Kilgarriff, 2011). For this shared task, HOO, we have developed two models, one is rule-based model and the other is the statistical model for both determiners and prepositions. Then we have combined both these models and developed our system for HOO 2012. 2 Related Work The English language belongs to the Germanic languages branch of the Indo-European language family, widely spoken on six continents. The HOO 
201
shared task is organized to help authors with writ-ing tasks. Identifying grammatical and linguistic errors in text is an open challenge to researchers. In recent times, researchers (Heidorn, 2000) have provided quite a benchmark for spell checker and grammar checkers, which is commonly available. In this task it is aimed to correct errors beyond the scope of these commonly available checkers i.e. detection and correction of jarring errors at part-of-speech (POS) level, syntax level and semantic lev-el. Earlier Heidorn (1975) developed augmented phrase structure grammar. (Tetreault et. al., 2008) has dealt with error pattern with preposition by non-native speakers. Meurers and Wunsch (2010) showed a surface based state-of-the-art machine learning technique, which deals with some fre-quently used prepositions. (Elghafari et al, 2010) worked on Data-Driven Prediction of Prepositions in English. Boyd et al (2011) used an n-gram based machine-learning approach. Last year we have also participated in this shared task; our sys-tem report was reported in (Bhaskar et. al., 2011).  3 Corpus Statistics There are two sets of data, training set and test set provided by the organizer. The training set has 1000 documents, which are collected from the FCE dataset. The publicly available dataset was in the native FCE format. So, the organizer first convert-ed it to the HOO data format. Then CUP annota-tors found the errors and marked them up in the dataset. This year the task is only about the errors related to prepositions and determiners. So the or-ganizer set only six types of errors, listed in table 1, which were dealt with this year. Hence, the other errors were discarded and replace with its corre-sponding standoff annotation in the training set. The training set consists of 1000 documents of to-tal 374680 words, which means 375 words per document. All the standoff annotations of training set were provided and an example of the standoff annotation is shown in the figure 1. Table 2 gives the error statistics of training set as reported in (Dale et. al., 2012). The test dataset has another 100 documents, which contain total of 18013 words at an average of 180 words per document. The test data was pro-cessed as the training data was done, but the stand-off annotation of the test documents was not provided before the task completion. The docu-
ments were provided in XML format as shown in the figure 2.   Error Type Tag Original Correction Replacement Preposition  RT He was born on January He was born in January Missing Preposition  MT Because it reminds me my child-hood. 
Because it reminds me of my child-hood. Unwanted Preposition  UT Regarding to the accom-modation Regarding the accom-modation Replacement Determiner  RD I used to go-ing with my friends to the camp. 
I used to go-ing with my friends to a camp. Missing Determiner  MD That will be nice to go on 1st of July That will be nice to go on the 1st of July Unwanted Determiner  UD The most suitable time for shopping is weekend when parents don't work and children haven't got a school. 
The most suitable time for shopping is weekend when parents don't work and children haven't got school.  Table 1. Examples of the six types of error.   Error Type # Training # Test # before Revised # after Revised UT  822  43 39 MT  1104 57 56 RT  2618 136 148 Prep  4545 236 243 UD  1048 53 62 MD  2230 125 131 RD  609 39 37 Det  3887 217 230 Total  8432 453 473 Words/  Error  44.18 39.77 38.08  Table 2. Error Statistics in the Training set. 
202
 <edit end="779" file="0004" in-dex="0008" part="1" start="775" type="UD">   <original>the </original>     <corrections>       <correction>         <empty/>       </correction>   </corrections> </edit>  <edit end="1041" file="0004" in-dex="0010" part="1" start="1039" type="RT">   <original>in</original>     <corrections>       <correction>at</correction>   </corrections> </edit>   Figure 1: An example of a standoff error annotation  4 System Description  The task is consisted of two coarse parts ? Preposi-tion and Determiner detection, recognition and cor-rection. In our previous year?s hybrid model, to resolve preposition errors, a rule-based model was developed and for determiner errors, a linear statis-tical method was used. There was no linear statisti-cal model for prepositions. So this year we have induced a statistical model to incorporate larger coverage of preposition error detection, which is not detected by the appropriate preposition list de-scribed in section 4.1.2. To resolve preposition errors and determiner er-rors we have built a hybrid model for both of them and used a voting technique among the rule based and statistical model for determiners and rule based post processing for prepositions. The system architecture is shown in the figure 3.  
 <?xml version="1.0" encod-ing="utf-8"?> <HOO version="2.1">   <HEAD sortkey="" source-type="FCE">     <CANDIDATE>       <AGE>20-30</AGE>     </CANDIDATE>   </HEAD>   <BODY>     <PART id="1">       <P>Dear Chris</P>       <P>I was great ?</P>       .       .       .     </PART>   </BODY> </HOO>   Figure 2: An example of the XML format of documents  4.1 Preposition Error Detection 
4.1.1 Statistical Model for Preposition An n-gram based linear statistical model is used. From the training corpus, it was trained with 3, 5 and 7-gram models. After testing, the 5-gram mod-el is performing best as from 3-gram, the statistical model fails to classify since probability distance is too small among the probable set to distinguish proper one while in 7-gram it fails to score high as training data set is relatively small and there are no similar occurrences. For the statistical model, dif-ferent linguistic information is taken as features. Initially, surface words are only considered which actually is similar to fingerprinting technique. Due to different inflected forms, the system fails to identify possible cases for a similar type of error with different inflected forms. Hence the root form of the word is included as a feature. Chunk infor-mation is included as a feature. The preposition with same word varies with if following word is animate or inanimate. As example, collaborate with SB collaborate in/on ST
203
  Figure 3. System Architecture  The text is parsed using the Stanford Dependen-cy parser1 to retrieve animate and inanimate infor-mation. After including animate and inanimate information the system didn?t improve much as training data set is quite small and animate infor-mation is not correct for names. Hence, this feature is discarded from the statistical model. 4.1.2 Appropriate Preposition List An appropriate preposition list consists of list of words along with preposition. The list is prepared in different corpus and training data. In the list, all possible formation with a word and preposition is stored. Let us take an example: admit ST to SB admit to From corpus, two patterns for admit are found. Between admit and preposition something (ST) 
                                                            1 http://nlp.stanford.edu/software/lex-parser.shtml 
may come. Hence both of the entries are combined and formed in a regular expression format. 
admit (ST)* to SB 4.1.3 Rule Based model for Preposition Rule based post processing was applied on output of statistical model. For the rule based post pro-cessing, an appropriate preposition list was pre-pared manually. The list contains 1567 entries. The list is associated with animate and inanimate in-formation. Hence, we aim to use dependency par-ser to identify subject object relation. Since the test data was in XML format, raw text was extracted from the XML document and the extracted sen-tences were parsed using Stanford dependency par-ser. After parsing the document with the dependency parser, subject and object information was extract-ed. From all the sentences, proposition are detected and cross-validated with the appropriate preposi-tion list. The preposition is dependent of the local association of the word around it. For the baseline
204
model, we have found that due to the list being small, few errors are being detected. Hence from the training corpus, the appropriate proposition list is enriched. The list is prepared in regular expres-sion format. Here is an example: ask * out + invite on a date  In the above example, + means the two phrases have a similar meaning and * means one or more words can appear between the two words. Hence, when a match is found from the appropriate propo-sition list with the first word or the preposition, the words local to it are validated. Since the task is about correcting preposition errors, only words are matched with the list.  grateful to SB for ST  In the above example, ST means something or an object and SB means somebody or a subject, this information being retrieved from the depend-ency parser.  4.2 Determiner Error Detection At the beginning of the determiner error detection task, we found that generation of list of rules to detect and correct the probable linguistic errors is a non-exhaustive set. Hence, we have decided to use a statistical model. After the statistical model, a rule based system is implemented with a few rules for the determiner devised from grammar books as for certain patterns statistical model fails to identi-fy. 4.2.1 Statistical Model for Determiner Similarly to preposition error detection, here a 5-gram linear statistical model is used. As same au-thors are prone to repeat same types of mistakes, we have decided to list out the errors from the training corpus documents. We have listed the er-rors document wise. In the training corpus, age information of author is mentioned. Hence docu-ments are grouped according to age. After a close inspection of the document wise error list, the age group is prone to make similar type of errors, which depicts the attributes of the age group. Our statistical model is trained with every set of train-ing data grouped by age separately. Hence differ-ent statistical models are prepared for different age 
groups. Now statistical model are applied accord-ing to the age group. It is found that age wise train-ing incurred better result than single statistical model over whole data.  4.2.2 Rule Based Model for Determiner It is found that statistical models works best for detecting the a and an determiner whereas perfor-mance drops for the determiner. Hence, rules for the are crafted manually from grammar books. A few rules for a and an are defined based on the first letter of the following word.  Among the determiners, usage of the is the most complicated one. For the rule based system differ-ent lists like nation, nationalities, unique objects, etc are produced. A few of the rules, which have been developed for the the determiner are men-tioned below. 1. In most cases, if a sentence starts with a proper noun or common noun the is dropped. 2. Before a country name, the is dropped except if starts with kingdom or republic. 3. They system checks whether a common noun is appeared in a previous line of the docu-ment, i.e. it has already been referred to, in which case the is added. 4. If subject and object belong to same class i.e. they share the same hyponym class, the is added to the subject. 5. In case of superlatives like best, worst etc. the is added. 6. Before numerals, the is added. 7. Before unique things, the is added. Unique-ness is defined if a thing has single embod-iment like moon etc. 8.  It is found that if some geographical location is mentioned at a position other than start of sentence, the is added. For different rules word lists are prepared such as a unique things list, superlatives, common nouns, country names, citizenships etc.  For a and an determiner correction, a list of dif-ferent phonemes is prepared. Rule based system 
205
trims the first two characters and maps them into a phoneme to decide between a and an. 4.2.3 Voting Technique The voting technique is used on the output of the rule based model and the statistical model. For a and an determiners, statistical model works best, especially in missing determiner and unnecessary determiner but for wrong determiner the rule based model performs better. For the determiner, the sta-tistical model identified missing determiner and unnecessary determiner cases to some extent whereas list based rule-based system elevates the accuracy.  5 Evaluation The system was evaluated for its performance in detecting, recognizing and correcting preposition and determiner errors in English documents. Sepa-rate scores were calculated for detection, recogni-tion and correction for both the errors of preposition and determiner separately and then combined scores were also calculated. For all re-sults, the organizer has provided three measures: Precision, Recall and F-Score. The precise defini-tions of these measures as implemented in the evaluation tool, and further details on the evalua-tion process are provided in (Dale and Narroway, 2012) and elaborated on at the HOO website.4 Each team was allowed to submit up to 10 sepa-rate runs over the test data, thus allowing them to have different configurations of their systems eval-
                                                            4 See www.correcttext.org/hoo2012. 
uated. Teams were asked to indicate whether they had used only publicly available data to train their systems, or whether they had made use of privately held data. We have submitted only one run (JU_run1) which has demonstrated F-scores of 7.1, 6.46 and 2.58 for detection, recognition and correc-tion respectively before revision. And after revi-sion it has demonstrated F-scores of 8.22, 7.59 and 3.16 for detection, recognition and correction re-spectively. Table 3 shows all the results of our run. We had used only publicly available data to train our systems, which are provided by the organizer as training set; we didn?t use any privately held data. 6 Conclusion and Future Works Our system has achieved F-scores of 8.22, 7.59 and 3.16 in detection, recognition and correction respectively. Our system failed to detect and cor-rect many syntactic and semantic errors like wrong a determiner. Since the data consists of mostly mail conversation, it retains huge number of spelling mistakes, which misdirected the statistical, and rule based model to detect probable errors. For the determiner, if the size of the produced lists in-creases, better accuracy can be achieved with the rule-based system. Co-reference is another issue to identify, as the determiner is used mostly subse-quent references. Anaphora resolution might there-fore be of some help.    
 
    Element Task Before Revision After Revision Precision Recall F-score Precision Recall F-score 
Preposition Detection 6.10 7.63 6.78 7.12 8.61 7.79 Recognition 5.42 6.78 6.03 6.44 7.79 7.05 Correction 3.05 3.81 3.39 3.73 4.51 4.08 
Determiner Detection 7.73 6.45 7.04 9.39 7.42 8.29 Recognition 7.73 6.45 7.04 9.39 7.42 8.29 Correction 1.66 1.38 1.51 2.21 1.75 1.95 
Combined Detection 6.93 7.28 7.10 8.19 8.25 8.22 Recognition 6.30 6.62 6.46 7.56 7.61 7.59 Correction 2.52 2.65 2.58 3.15 3.17 3.16  Table 3. Results for Preposition, Determiner and Combined (preposition and determiner) errors.  
206
Acknowledgments We acknowledge the support of the IFCPAR fund-ed Indo-French project ?An Advanced Platform for Question Answering Systems? and the DIT, Gov-ernment of India funded project ?Development of English to Indian Language Machine Translation (EILMT) System Phase II?. References  Adriane Boyd and Detmar Meurers. Data-Driven Cor-rection of FunctionWords in Non-Native English. In 2011 Generation Challenges, HOO: Helping Our Own in the Proceedings of the 13th European Work-shop on Natural Language Generation (ENLG), 28th ? 30th September, 2011, Nancy, France. Anas Elghafari, Detmar Meurers and Holger Wunsch, 2010. Exploring the Data-Driven Prediction of Prep-ositions in English. In the Proceedings of the 23rd In-ternational Conference on Computational Linguistics, Beijing, China, 2010. George Heidorn. 2000. Intelligent writing assistance. In R Dale, H Moisl, and H Somers, editors, Handbook of Natural Language Processing, pages 181?207. Marcel Dekker Inc.  GE Heidorn. 1975. Augmented phrase structure gram-mars. In: BL Webber, RC Schank, eds. Theoretical Issues in Natural Language Processing. Assoc. for Computational Linguistics, pp.1-5.  J R Tetreault and M S Chodorow. 2008. The ups and downs of preposition error detection in ESL writing. In Proceedings of the 22nd International Conference on Computational Linguistics, pp-865-872, Manches-ter,2008. Pinaki Bhaskar, Aniruddha Ghosh, Santanu Pal and Sivaji Bandyopadhyay. May I correct the English of your paper!!!. In 2011 Generation Challenges, HOO: Helping Our Own in the Proceedings of the 13th Eu-ropean Workshop on Natural Language Generation (ENLG), pp 250-253, 28th ? 30th September, 2011, Nancy, France. Robert Dale and A Kilgarriff. 2010. Helping Our Own: Text massaging for computational linguistics as a new shared task. In Proceedings of the 6th Interna-tional Natural Language Generation Conference, Dublin, Ireland, pages 261?266, 7th-9th July 2010. Robert Dale and A Kilgarriff. 2011. Helping our own: The HOO 2011 pilot shared task. In Proceedings of the 13th European Workshop on Natural Language Generation (ENLG), 28th ? 30th September, 2011, Nancy, France. 
Robert Dale and George Narroway. 2012. A framework for evaluating text correction. In Proceedings of the Eighth International Conference on Language Re-sources and Evaluation (LREC2012), 21?27 May 2012. Robert Dale, Ilya Anisimoff and George Narroway (2012) HOO 2012: A Report on the Preposition and Determiner Error Correction Shared Task.  In Pro-ceedings of the Seventh Workshop on Innovative Use of NLP for Building Educational Applications, Mon-treal, Canada, 7th June 2012.             
207
Proceedings of the 6th Workshop on Building and Using Comparable Corpora, pages 69?76,
Sofia, Bulgaria, August 8, 2013. c?2013 Association for Computational Linguistics
Improving MT System Using Extracted Parallel Fragments of Text 
from Comparable Corpora 
 
 
Rajdeep Gupta, Santanu Pal, Sivaji Bandyopadhyay 
Department of Computer Science & Engineering 
Jadavpur University 
Kolkata ? 700032, India 
{rajdeepgupta20, santanu.pal.ju}@gmail.com,  
sivaji_cse_ju@yahoo.com 
 
Abstract 
In this article, we present an automated ap-
proach of extracting English-Bengali parallel 
fragments of text from comparable corpora 
created using Wikipedia documents. Our ap-
proach exploits the multilingualism of Wiki-
pedia. The most important fact is that this ap-
proach does not need any domain specific cor-
pus. We have been able to improve the BLEU 
score of an existing domain specific English-
Bengali machine translation system by 
11.14%. 
1 Introduction 
Recently comparable corpora have got great at-
tention in the field of NLP. Extracting parallel 
fragments of texts, paraphrases or sentences from 
comparable corpora are particularly useful for 
any statistical machine translation system (SMT) 
(Smith et al 2010) as the size of the parallel cor-
pus plays major role in any SMT performance. 
Extracted parallel phrases from comparable cor-
pora are added with the training corpus as addi-
tional data that is expected to facilitate better per-
formance of machine translation systems specifi-
cally for those language pairs which have limited 
parallel resources available. In this work, we try 
to extract English-Bengali parallel fragments of 
text from comparable corpora. We have devel-
oped an aligned corpus of English-Bengali doc-
ument pairs using Wikipedia. Wikipedia is a 
huge collection of documents in many different 
languages. We first collect an English document 
from Wikipedia and then follow the inter-
language link to find the same document in Ben-
gali (obviously, if such a link exists). In this way, 
we create a small corpus. We assume that such 
English-Bengali document pairs from Wikipedia 
are already comparable since they talk about the 
same entity. Although each English-Bengali 
document pair talks about the same entity, most 
of the times they are not exact translation of each 
other. And as a result, parallel fragments of text 
are rarely found in these document pairs. The 
bigger the size of the fragment the less probable 
it is to find its parallel version in the target side. 
Nevertheless, there is always chance of getting 
parallel phrase, tokens or even sentences in com-
parable documents. The challenge is to find those 
parallel texts which can be useful in increasing 
machine translation performance. 
In our present work, we have concentrated on 
finding small fragments of parallel text instead of 
rigidly looking for parallelism at entire sentential 
level. Munteanu and Marcu (2006) believed that 
comparable corpora tend to have parallel data at 
sub-sentential level. This approach is particularly 
useful for this type of corpus under 
consideration, because there is a very little 
chance of getting exact translation of bigger 
fragments of text in the target side. Instead, 
searching for parallel chunks would be more 
logical. If a sentence in the source side has a 
parallel sentence in the target side, then all of its 
chunks need to have their parallel translations in 
the target side as well. 
It is to be noted that, although we have 
document level alignment in our corpus, it is 
somehow ad-hoc i.e. the documents in the corpus 
do not belong to any particular domain. Even 
with such a corpus we have been able to improve 
the performance of an existing machine 
translation system built on tourism domain. This 
also signifies our contribution towards domain 
adaptation of machine translation systems. 
The rest of the paper is organized as follows. 
Section 2 describes the related work. Section 3 
describes the preparation of the comparable 
corpus. The system architecture is described in 
section 4. Section 5 describes the experiments we 
69
conducted and presents the results. Finally the 
conclusion is drawn in section 6. 
2 Related Work 
There has been a growing interest in approaches 
focused on extracting word translations from 
comparable corpora (Fung and McKeown, 1997; 
Fung and Yee, 1998; Rapp, 1999; Chiao and 
Zweigenbaum, 2002; Dejean et al, 2002; Kaji, 
2005; Gamallo, 2007; Saralegui et al, 2008). 
Most of the strategies follow a standard method 
based on context similarity. The idea behind this 
method is as follows: A target word t is the 
translation of a source word s if the words with 
which t co-occurs are translations of words with 
which s co-occurs. The basis of the method is to 
find the target words that have the most similar 
distributions with a given source word. The 
starting point of this method is a list of bilingual 
expressions that are used to build the context 
vectors of all words in both languages. This list 
is usually provided by an external bilingual 
dictionary. In Gamallo (2007), however, the 
starting list is provided by bilingual correlations 
which are previously extracted from a parallel 
corpus. In Dejean (2002), the method relies on a 
multilingual thesaurus instead of an external 
bilingual dictionary. In all cases, the starting list 
contains the ?seed expressions? required to build 
context vectors of the words in both languages. 
The works based on this standard approach 
mainly differ in the coefficients used to measure 
the context vector similarity. 
Otero et al (2010) showed how Wikipedia 
could be used as a source of comparable corpora 
in different language pairs. They downloaded the 
entire Wikipedia for any two language pair and 
transformed it into a new collection: 
CorpusPedia. However, in our work we have 
showed that only a small ad-hoc corpus 
containing Wikipedia articles could be proved to 
be beneficial for existing MT systems. 
3 Tools and Resources Used 
A sentence-aligned English-Bengali parallel 
corpus containing 22,242 parallel sentences from 
a travel and tourism domain was used in the 
preparation of the baseline system. The corpus 
was obtained from the consortium-mode project 
?Development of English to Indian Languages 
Machine Translation (EILMT) System?. The 
Stanford Parser and the CRF chunker were used 
for identifying individual chunks in the source 
side of the parallel corpus. The sentences on the 
target side (Bengali) were POS-tagged/chunked 
by using the tools obtained from the consortium 
mode project ?Development of Indian Languages 
to Indian Languages Machine Translation 
(ILILMT) System?.  
For building the comparable corpora we have 
focused our attention on Wikipedia documents. 
To collect comparable English-Bengali 
document pairs we designed a crawler. The 
crawler first visits an English page, saves the raw 
text (in HTML format), and then finds the cross-
lingual link (if exists) to find the corresponding 
Bengali document. Thus, we get one English-
Bengali document pair. Moreover, the crawler 
visits the links found in each document and 
repeats the process. In this way, we develop a 
small aligned corpus of English-Bengali 
comparable document pairs. We retain only the 
textual information and all the other details are 
discarded. It is evident that the corpus is not 
confined to any particular domain. The challenge 
is to exploit this kind of corpus to help machine 
translation systems improve. The advantage of 
using such corpus is that it can be prepared easily 
unlike the one that is domain specific. 
The effectiveness of the parallel fragments of 
text developed from the comparable corpora in 
the present work is demonstrated by using the 
standard log-linear PB-SMT model as our 
baseline system: GIZA++ implementation of 
IBM word alignment model 4, phrase extraction 
heuristics described in (Koehn et al, 2003), 
minimum-error-rate training (Och, 2003) on a 
held-out development set, target language model 
with Kneser-Ney smoothing (Kneser and Ney, 
1995) trained with SRILM (Stolcke, 2002), and 
Moses decoder (Koehn et al, 2007). 
4 System Architecture 
4.1 PB-SMT(Baseline System) 
Translation is modeled in SMT as a decision 
process, in which the translation e1
I = e1..ei..eI of 
a source sentence f1
J = f1..fj..fJ  is chosen to 
maximize (1) 
)().|(maxarg)|(maxarg 111,11, 11
IIJ
eI
JI
eI
ePefPfeP
II
?
     (1)  
where )|( 11 IJ efP  and )( 1IeP  denote 
respectively the translation model and the target 
language model (Brown et al, 1993). In log-
linear phrase-based SMT, the posterior 
probability )|( 11 JI feP  is directly modeled as a 
log-linear combination of features (Och and Ney, 
70
2002), that usually comprise of M translational 
features, and the language model, as in (2): 
?
?
?
M
m
KIJ
mm
JI sefhfeP
1
11111 ),,()|(log ?
)(log 1ILM eP??        (2)     
where kk sss ...11 ?  denotes a segmentation of the 
source and target sentences respectively into the 
sequences of phrases )?,...,?( 1 kee  and )?,...,?( 1 kff  
such that (we set i0 = 0) (3): 
,1 Kk ???  sk = (ik, bk, jk), 
          kk iik eee ...? 11??? , 
         kk jbk fff ...? ? .          (3) 
and each feature mh?  in (2) can be rewritten as in 
(4): 
?
?
?
K
k
kkkm
KIJ
m sefhsefh
1
111 ),?,?(?),,(
                 (4) 
where mh? is a feature that applies to a single 
phrase-pair. It thus follows (5): 
? ??
? ??
?K
k
K
k
kkkkkkm
M
m
m sefhsefh
1 11
),?,?(?),?,?(??
     (5) 
where 
m
M
m
mhh ??
1
?
?
? ?
. 
4.2 Chunking of English Sentences 
We have used CRF-based chunking algorithm to 
chunk the English sentences in each document. 
The chunking breaks the sentences into linguistic 
phrases. These phrases may be of different sizes. 
For example, some phrases may be two words 
long and some phrases may be four words long. 
According to the linguistic theory, the interme-
diate constituents of the chunks do not usually 
take part in long distance reordering when it is 
translated, and only intra chunk reordering oc-
curs. Some chunks combine together to make a 
longer phrase. And then some phrases again 
combine to make a sentence. The entire process 
maintains the linguistic definition of a sentence. 
Breaking the sentences into N-grams would have 
always generated phrases of length N but these 
phrases may not be linguistic phrases. For this 
reason, we avoided breaking the sentences into 
N-grams. 
The chunking tool breaks each English sentence 
into chunks. The following is an example of how 
the chunking is done. 
Sentence: India , officially the Republic of India , 
is a country in South Asia. 
After Chunking: (India ,) (officially) (the 
Republic ) (of) (India , ) (is) (a country ) (in 
South Asia ) (.) 
We have further merged the chunks to form 
bigger chunks. The idea is that, we may 
sometimes find the translation of the merged 
chunk in the target side as well, in which case, 
we would get a bigger fragment of parallel text. 
The merging is done in two ways: 
Strict Merging: We set a value ?V?. Starting 
from the beginning, chunks are merged such that 
the number of tokens in each merged chunk does 
not exceed V. 
 
 
Figure 1. Strict-Merging Algorithm. 
 
Figure 1 describes the pseudo-code for strict 
merging. 
For example, in our example sentence the 
merged chunks will be as following, where V=4: 
(India , officially) (the Republic of ) (India , is) 
(a country) (in South Asia .) 
 
 
Figure 2. Window-Based Merging Algorithm. 
Procedure Window_Merging() 
begin 
Set_Chunk?Set of all English Chunks 
L?Number of chunks in Set_Chunk 
for i = 0 to L-1 
 Words?Set of tokens in i-th Chunk in Set_Chunk 
 Cur_wc?number of tokens in Words 
Ol?i-th chunk in Set_Chunk 
for j = (i+1) to (L-1) 
  C?j-th chunk in Set_Chunk 
  w?set of tokens in C 
  l?number of tokens in w 
  if(Cur_wc + l ? V) 
   Append C at the end of Ol 
   Add l to Cur_wc 
  end if 
 end for 
 Output Ol as the next merged chunk 
end for 
end   
 
Procedure Strict_Merge() 
begin 
Oline ? null 
Cur_wc ? 0 
repeat 
Iline?Next Chunk 
Length?Number of Tokens in Iline 
if(Cur_wc + Length > V) 
Output Oline as the next merged chunk 
  Cur_wc?Length 
 else 
  Append Iline at the end of Oline 
  Add Length to Cur_wc 
 end if 
while (there are more chunks) 
end 
71
 
Figure 3. System Architecture for Finding Parallel Fragments
Window-Based Merging: In this type of 
chunking also, we set a value ?V?, and for each 
chunk we try to merge as many chunks as 
possible so that the number of tokens in the 
merged chunk never exceeds V. 
So, we slide an imaginary window over the 
chunks. For example, for our example sentence 
the merged chunks will be as following, where V 
= 4 : 
(India , officially) (officially the Republic of) 
(the Republic of) (of India , is) (India , is) (is a 
country) (a country) (in South Asia .) 
The pseudo-code of window-based merging is   
described in Figure 2. 
4.3 Chunking of Bengali Sentences 
Since to the best of our knowledge, there is no 
good quality chunking tool for Bengali we did 
not use chunking explicitly. Instead, strict 
merging is done with consecutive V number of 
tokens whereas window-based merging is done 
sliding a virtual window over each token and 
merging tokens so that the number of tokens 
does not exceed V. 
4.4 Finding Parallel Chunks 
After finding the merged English chunks they are 
translated into Bengali using a machine 
translation system that we have already 
developed. This is also the same machine 
translation system whose performance we want 
to improve. Chunks of each of the document 
pairs are then compared to find parallel chunks. 
Each translated source chunk (translated from 
English to Bengali) is compared with all the 
target chunks in the corresponding Bengali-
chunk document. When a translated source 
chunk is considered, we try to align each of its 
token to some token in the target chunk. Overlap 
between token two Bengali chunks B1 and B2, 
where B1 is the translated chunk and B2 is the 
chunk in the Bengali document, is defined as 
follows: 
Overlap(B1,B2) = Number of tokens in B1 for 
which an alignment can be found in B2.  
It is to be noted that Overlap(B1,B2) ? 
Overlap(B2 ,B1). Overlap between chunks is 
found in both ways (from translated source 
chunk to target and from target to translated 
source chunk). If 70% alignment is found in both 
the overlap measures then we declare them as 
parallel. Two issues are important here: the com-
parison of two Bengali tokens and in case an 
alignment is found, which token to retrieve 
(source or target) and how to reorder them. We 
address these two issues in the next two sections. 
4.5 Comparing Bengali Tokens 
For our purpose, we first divide the two tokens 
into their matra (vowel modifiers) part and 
consonant part keeping the relative orders of 
characters in each part same. For example, 
Figure 4 shows the division of the word . 
 
English 
Documents 
English 
Chunks 
Merging 
Translation 
Bengali 
Documents 
Bengali 
Chunks 
Find Parallel Chunks and Reorder  
Merging 
72
 
 
 
 
 
Figure 4. Division of a Bengali Word. 
 
Respective parts of the two words are then 
compared. Orthographic similarities like 
minimum edit distance ratio, longest common 
subsequence ratio, and length of the strings are 
used for the comparison of both parts. 
Minimum Edit Distance Ratio: It is defined 
as  follows: 
 
 
where |B| is the length of the string B and ED is 
the minimum edit distance or levenshtein 
distance calculated as the minimum number of 
edit operations ? insert, replace, delete ? needed 
to transform B1 into B2. 
Longest Common Subsequence Ratio: It is 
defined as follows: 
 
 
 
where LCS is the longest common subsequence 
of two strings. 
Threshold for matching is set empirically. We 
differentiate between shorter strings and larger 
strings. The idea is that, if the strings are short 
we cannot afford much difference between them 
to consider them as a match. In those cases, we 
check for exact match. Also, the threshold for 
consonant part is set stricter because our 
assumption is that consonants contribute more 
toward the word?s pronunciation. 
4.6 Reordering of Source Chunks 
When a translated source chunk is compared 
with a target chunk it is often found that the 
ordering of the tokens in the source chunk and 
the target chunk is different. The tokens in the 
target chunk have a different permutation of 
positions with respect to the positions of tokens 
in the source chunk. In those cases, we reordered 
the positions of the tokens in the source chunk so 
as to reflect the positions of tokens in the target 
chunk because it is more likely that the tokens 
will usually follow the ordering as in the target 
chunk. For example, the machine translation 
output of the English chunk ?from the Atlantic 
Ocean? is ? theke  atlantic  
 (mahasagar)?. We found a target 
chunk ?  (atlantic)  (maha-
sagar)  (theke)  (ebong)? with which 
we could align the tokens of the source chunk 
but in different relative order. Figure 5 shows the 
alignment of tokens.  
 
Figure 5. Alignment of Bengali Tokens. 
 
We reordered the tokens of the source chunk 
and the resulting chunk was ?  
 ?.Also, the token ? ? in the 
target chunk could not find any alignment and 
was discarded. The system architecture of the 
present system is described in figure 3. 
5 Experiments And Results 
5.1 Baseline System 
We randomly extracted 500 sentences each for 
the development set and test set from the initial 
parallel corpus, and treated the rest as the 
training corpus. After filtering on the maximum 
allowable sentence length of 100 and sentence 
length ratio of 1:2 (either way), the training 
corpus contained 22,492 sentences.  
 
V=4 V=7 
Number of English 
Chunks(Strict-Merging) 
579037 376421 
Number of English 
Chunks(Window-
Merging) 
890080 949562 
Number of Bengali 
Chunks(Strict-Merging) 
69978 44113 
Number of Bengali 
Chunks(Window-
Merging) 
230025 249330 
Table 1. Statistics of the Comparable Corpus 
 
V=4 V=7 
Number of Parallel 
Chunks(Strict-Merging) 
1032 1225 
Number of Parallel 
Chunks(Window-Merging) 
1934 2361 
Table 2. Number of Parallel Chunks found 
 
 
Kolkata  
matra
73
 BLEU NIST 
Baseline System(PB-SMT) 10.68 4.12 
Baseline + Parallel 
Chunks(Strict-
Merging) 
V=4 10.91 4.16 
V=7 11.01 4.16 
Baseline + Parallel 
Chunks(Window-
Merging) 
V=4 11.55 4.21 
V=7 11.87 4.29 
 
Table 3.Evaluation of the System 
 
In addition to the target side of the parallel cor-
pus, a monolingual Bengali corpus containing 
406,422 words from the tourism domain was 
used for the target language model. We 
experimented with different n-gram settings for 
the language model and the maximum phrase 
length, and found that a 5-gram language model 
and a maximum phrase length of 7 produced the 
optimum baseline result. We therefore carried 
out the rest of the experiments using these 
settings. 
5.2 Improving Baseline System 
The comparable corpus consisted of 582 English-
Bengali document pairs.  
We experimented with the values V=4 and 
V=7 while doing the merging of chunks both in 
English and Bengali. All the single token chunks 
were discarded. Table 1 shows some statistics 
about the merged chunks for V=4 and V=7.It is 
evident that number of chunks in English 
documents is far more than the number of chunks 
in Bengali documents. This immediately 
suggests that Bengali documents are less 
informative than English documents. When the 
English merged chunks were passed to the 
translation module some of the chunks could not 
be translated into Bengali. Also, some chunks 
could be translated only partially, i.e. some 
tokens could be translated while some could not 
be. Those chunks were discarded. Finally, the 
number of (Strict-based) English merged-chunks 
and number of (Window-based) English merged-
chunks were 285756 and 594631 respectively. 
Two experiments were carried out separately. 
Strict-based  merged English chunks were 
compared with Strict-Based merged Bengali 
chunks. Similarly, window-based merged Eng-
lish chunks were compared with window-based 
merged Bengali chunks. While searching for 
parallel chunks each translated source chunk was 
compared with all the target chunks in the 
corresponding document. Table 2 displays the 
number of parallel chunks found. Compared to 
the number of chunks in the original documents 
the number of parallel chunks found was much 
less. Nevertheless, a quick review of the parallel 
list revealed that most of the chunks were of 
good quality. 
5.3 Evaluation 
We carried out evaluation of the MT quality 
using two automatic MT evaluation metrics: 
BLEU (Papineni et al, 2002) and NIST 
(Doddington, 2002). Table 3 presents the ex-
perimental results. For the PB-SMT experiments, 
inclusion of the extracted strict merged parallel 
fragments from comparable corpora as additional 
training data presented some improvements over 
the PB-SMT baseline. Window based extracted 
fragments are added separately with parallel cor-
pus and that also provides some improvements 
over the PB baseline; however inclusion of win-
dow based extracted phrases in baseline system 
with phrase length 7 improves over both strict 
and baseline in term of BLEU score and NIST 
score. 
Table 3 shows the performance of the PB-
SMT system that shows an improvement over 
baseline with both strict and window based 
merging even if,  we change their phrase length 
from 4 to 7. Table 3 shows that the best 
improvement is achieved when we add parallel 
chunks as window merging with phrase length 7. 
It gives 1.19 BLEU point, i.e., 11.14% relative 
improvement over baseline system. The NIST 
score could be improved  up to 4.12%. Bengali is 
a morphologically rich language and has 
74
relatively free phrase order. The strict based 
extraction does not reflect much improvement 
compared to the window based extraction 
because strict-merging (Procedure Strict_Merge) 
cannot cover up all the segments on either side, 
so very few parallel extractions have been found 
compared to window based extraction.  
6 Conclusion 
In this work, we tried to find English-Bengali 
parallel fragments of text from a comparable 
corpus built from Wikipedia documents. We 
have successfully improved the performance of 
an existing machine translation system. We have 
also shown that out-of-domain corpus happened 
to be useful for training of a domain specific MT 
system. The future work consists of working on 
larger amount of data. Another focus could be on 
building ad-hoc comparable corpus from WEB 
and using it to improve the performance of an 
existing out-of-domain MT system. This aspect 
of work is particularly important because the 
main challenge would be of domain adaptation. 
Acknowledgements 
This work has been partially supported by a grant 
from the English to Indian language Machine 
Translation (EILMT) project funded by the 
Department of Information and Technology 
(DIT), Government of India.  
Reference 
Chiao, Y. C., & Zweigenbaum, P. (2002, August). 
Looking for candidate translational equivalents in 
specialized, comparable corpora. In Proceedings of 
the 19th international conference on Computation-
al linguistics-Volume 2 (pp. 1-5). Association for 
Computational Linguistics. 
D?jean, H., Gaussier, ?., & Sadat, F. (2002). Bilin-
gual terminology extraction: an approach based on 
a multilingual thesaurus applicable to comparable 
corpora. In Proceedings of the 19th International 
Conference on Computational Linguistics COLING 
(pp. 218-224). 
Doddington, G. (2002, March). Automatic evaluation 
of machine translation quality using n-gram co-
occurrence statistics. In Proceedings of the second 
international conference on Human Language 
Technology Research (pp. 138-145). Morgan 
Kaufmann Publishers Inc.. 
Fung, P., & McKeown, K. (1997, August). Finding 
terminology translations from non-parallel corpora. 
In Proceedings of the 5th Annual Workshop on 
Very Large Corpora (pp. 192-202). 
Fung, P., & Yee, L. Y. (1998, August). An IR ap-
proach for translating new words from nonparallel, 
comparable texts. In Proceedings of the 17th inter-
national conference on Computational linguistics-
Volume 1 (pp. 414-420). Association for Computa-
tional Linguistics. 
Hiroyuki, K. A. J. I. (2005). Extracting translation 
equivalents from bilingual comparable corpora. 
IEICE Transactions on information and systems, 
88(2), 313-323. 
 Kneser, R., & Ney, H. (1995, May). Improved back-
ing-off for m-gram language modeling. In Acous-
tics, Speech, and Signal Processing, 1995. 
ICASSP-95., 1995 International Conference on 
(Vol. 1, pp. 181-184). IEEE. 
Koehn, P., Hoang, H., Birch, A., Callison-Burch, C., 
Federico, M., Bertoldi, N., ... & Herbst, E. (2007, 
June). Moses: Open source toolkit for statistical 
machine translation. In Proceedings of the 45th 
Annual Meeting of the ACL on Interactive Poster 
and Demonstration Sessions (pp. 177-180). 
Association for Computational Linguistics. 
Koehn, P., Och, F. J., & Marcu, D. (2003, May). Sta-
tistical phrase-based translation. In Proceedings of 
the 2003 Conference of the North American Chap-
ter of the Association for Computational Linguis-
tics on Human Language Technology-Volume 1 
(pp. 48-54). Association for Computational Lin-
guistics. 
 Munteanu, D. S., & Marcu, D. (2006, July). Extract-
ing parallel sub-sentential fragments from non-
parallel corpora. In Proceedings of the 21st Inter-
national Conference on Computational Linguistics 
and the 44th annual meeting of the Association for 
Computational Linguistics (pp. 81-88). Association 
for Computational Linguistics.. 
Och, F. J. (2003, July). Minimum error rate training in 
statistical machine translation. In Proceedings of 
the 41st Annual Meeting on Association for Com-
putational Linguistics-Volume 1 (pp. 160-167). As-
sociation for Computational Linguistics. 
Och, F. J., & Ney, H. (2000). Giza++: Training of 
statistical translation models. 
Otero, P. G. (2007). Learning bilingual lexicons from 
comparable english and spanish corpora. Proceed-
ings of MT Summit xI, 191-198. 
Otero, P. G., & L?pez, I. G. (2010). Wikipedia as 
multilingual source of comparable corpora. In Pro-
ceedings of the 3rd Workshop on Building and Us-
ing Comparable Corpora, LREC (pp. 21-25). 
 Papineni, K., Roukos, S., Ward, T., & Zhu, W. J. 
(2002, July). BLEU: a method for automatic evalu-
ation of machine translation. In Proceedings of the 
40th annual meeting on association for computa-
75
tional linguistics (pp. 311-318). Association for 
Computational Linguistics. 
Rapp, R. (1999, June). Automatic identification of 
word translations from unrelated English and Ger-
man corpora. In Proceedings of the 37th annual 
meeting of the Association for Computational Lin-
guistics on Computational Linguistics (pp. 519-
526). Association for Computational Linguistics. 
Saralegui, X., San Vicente, I., & Gurrutxaga, A. 
(2008). Automatic generation of bilingual lexicons 
from comparable corpora in a popular science do-
main. In LREC 2008 workshop on building and us-
ing comparable corpora. 
 Smith, J. R., Quirk, C., & Toutanova, K. (2010, 
June).Extracting parallel sentences from 
comparable corpora using document level 
alignment. In Human Language Technologies: The 
2010 Annual Conference of the North American 
Chapter of the Association for Computational 
Linguistics (pp. 403-411). Association for 
Computational Linguistics. 
Stolcke, A. (2002, September). SRILM-an extensible 
language modeling toolkit. In Proceedings of the 
international conference on spoken language 
processing (Vol. 2, pp. 901-904). 
 
 
 
76
Proceedings of the Second Workshop on Hybrid Approaches to Translation, pages 94?101,
Sofia, Bulgaria, August 8, 2013. c?2013 Association for Computational Linguistics
A Hybrid Word Alignment Model for Phrase-Based Statistical Ma-
chine Translation 
 
 
Santanu Pal*, Sudip Kumar Naskar? and Sivaji Bandyopadhyay* 
*Department of Computer Science & Engineering 
Jadavpur University, Kolkata, India 
santanu.pal.ju@gmail.com, sivaji_cse_ju@yahoo.com 
?
 Department of Computer & System Sciences 
Visva-Bharati University, Santiniketan, India 
sudip.naskar@gmail.com 
 
  
 
Abstract 
This paper proposes a hybrid word alignment 
model for Phrase-Based Statistical Machine 
translation (PB-SMT). The proposed hybrid 
alignment model provides most informative 
alignment links which are offered by both un-
supervised and semi-supervised word align-
ment models. Two unsupervised word align-
ment models (GIZA++ and Berkeley aligner) 
and a rule based aligner are combined togeth-
er. The rule based aligner only aligns named 
entities (NEs) and chunks. The NEs are 
aligned through transliteration using a joint 
source-channel model. Chunks are aligned 
employing a bootstrapping approach by trans-
lating the source chunks into the target lan-
guage using a baseline PB-SMT system and 
subsequently validating the target chunks us-
ing a fuzzy matching technique against the 
target corpus. All the experiments are carried 
out after single-tokenizing the multi-word 
NEs.  Our best system provided significant 
improvements over the baseline as measured 
by BLEU.  
1 Introduction 
Word alignment is the backbone of PB-SMT sys-
tem or any data driven approaches to Machine 
Translation (MT) and it has received a lot of at-
tention in the area of statistical machine transla-
tion (SMT) (Brown et al, 1993; Och and Ney, 
2003; Koehn et al, 2003). Word alignment is not 
an end task in itself and is usually used as an in-
termediate step in SMT. Word alignment is de-
fined as the detection of corresponding alignment 
of words from parallel sentences that are transla-
tion of each other. Statistical machine translation 
usually suffers from many-to-many word links 
which existing statistical word alignment algo-
rithms can not handle well.  
The unsupervised word alignment models are 
based on IBM models 1?5 (Brown et al, 1993) 
and the HMM model (Ney and Vogel, 1996; Och 
and Ney, 2003). Models 3, 4 and 5 are based on 
fertility based models which are asymmetric. To 
improve alignment quality, the Berkeley Aligner 
is based on the symmetric property by intersect-
ing alignments induced in each translation direc-
tion. 
In the present work, we propose improvement 
of word alignment quality by combining three 
word alignment tables (i) GIZA++ alignment (ii) 
Berkeley Alignment and (iii) rule based align-
ment. Our objective is to perceive the effective-
ness of the Hybrid model in word alignment by 
improving the quality of translation in the SMT 
system. In the present work, we have implement-
ed a rule based alignment model by considering 
several types of chunks which are automatically 
extracted on the source side. Each individual 
source chunk is translated using a baseline PB-
SMT system and validated with the target chunks 
on the target side. The validated source-target 
chunks are added in the rule based alignment 
table. Work has been carried out into three direc-
tions: (i) three alignment tables are combined 
together by taking their union; (ii) extra align-
ment pairs are added into the alignment table. 
This is a well-known practice in domain adapta-
tion in SMT (Eck et al, 2004; Wu et al, 2008); 
(iii) the alignment table is updated through semi-
supervised alignment technique. 
94
The remainder of the paper is organized as fol-
lows. Section 2 discusses related work. The pro-
posed hybrid word alignment model is described 
in Section 3. Section 4 presents the tools and re-
sources used for the various experiments. Section 
5 includes the results obtained, together with 
some analysis. Section 6 concludes and provides 
avenues for further work. 
2 Related Works  
Zhou et al (2004) proposed a multi lingual filter-
ing algorithm that generates bilingual chunk 
alignment from Chinese-English parallel corpus. 
The algorithm has three steps, first, from the par-
allel corpus; the most frequent bilingual chunks 
are extracted. Secondly, the participating chunks 
for alignments are combined into a cluster and 
finally one English chunk is generated corre-
sponding to a Chinese chunk by analyzing the 
highest co-occurrences of English chunks. Bilin-
gual knowledge can be extracted using chunk 
alignment (Zhou et. al., 2004). Pal et, al. (2012) 
proposed a bootstrapping method for chunk 
alignment; they used an SMT based model for 
chunk translation and then aligned the source-
target chunk pairs after validating the translated 
chunk. Ma et. al. (2007) simplified the task of 
automatic word alignment as several consecutive 
words together correspond to a single word in the 
opposite language by using the word aligner it-
self, i.e., by bootstrapping on its output. A Max-
imum Entropy model based approach for Eng-
lish?Chinese NE alignment which significantly 
outperforms IBM Model4 and HMM has been 
proposed by Feng et al (2004). They considered 
4 features: translation score, transliteration score, 
source NE and target NE's co-occurrence score 
and the distortion score for distinguishing identi-
cal NEs in the same sentence. Moore (2003) pre-
sented an approach where capitalization cues 
have been used for identifying NEs on the Eng-
lish side. Statistical techniques are applied to de-
cide which portion of the target language corre-
sponds to the specified English NE, for simulta-
neous NE identification and translation. 
To improve the learning process of unlabeled 
data using labeled data (Chapelle et al, 2006), 
the semi-supervised learning method is the most 
useful learning technique. Semi-supervised 
learning is a broader area of Machine Learning. 
Researchers have begun to explore semi-
supervised word alignment models that use both 
labeled and unlabeled data. Fraser and Marcu 
(2006) proposed a semi-supervised training algo-
rithm. The weighting parameters are learned 
from discriminative error training on labeled da-
ta, and the parameters are estimated by maxi-
mum-likelihood EM training on unlabeled data. 
They have also used a log-linear model which is 
trained on the available labeled data to improve 
performance. Interpolating human alignments 
with automatic alignments has been proposed by 
Callison-Burch et al (2004), where the align-
ments of higher quality have gained much higher 
weight than the lower-quality alignments. Wu et 
al. (2006) have developed two separate models 
of standard EM algorithm which learn separately 
from both labeled and unlabeled data. Two mod-
els are then interpolated as a learner in the semi-
supervised Ada-Boost algorithm to improve 
word alignment. Ambati et al (2010) proposed 
active learning query strategies to identify highly 
uncertain or most informative alignment links 
under an unsupervised word alignment model. 
Intuitively, multiword NEs on the source and 
the target sides should be both aligned in the par-
allel corpus and translated as a whole. However, 
in the state-of-the-art PB-SMT systems, the con-
stituents of multiword NE are marked and 
aligned as parts of consecutive phrases, since 
PB-SMT (or any other approaches to SMT) does 
not generally treat multiword NEs as special to-
kens. This is the motivations behind considering 
NEs for special treatment in this work by con-
verting into single tokens that makes sure that 
PB-SMT also treats them as a whole 
Another problem with SMT systems is the er-
roneous word alignment. Sometimes some words 
are not translated in the SMT output sentence 
because of the mapping to NULL token or erro-
neous mapping during word alignment. Verb 
phrase translation also creates major problems. 
The words inside verb phrases are generally not 
aligned one-to-one; the alignments of the words 
inside source and target verb phrases are mostly 
many-to-many particularly so for the English?
Bengali language pair.  
The first objective of the present work is to see 
how single tokenization and alignment of NEs on 
both the sides affects the overall MT quality. The 
second objective is to see whether Hybrid word 
alignment model of both unsupervised and semi-
supervised techniques enhance the quality of 
translation in the SMT system rather than the 
single tokenized NE level parallel corpus applied 
to the hybrid model.  
We carried out the experiments on English?
Bengali translation task. Bengali shows high 
morphological richness at lexical level. Lan-
95
guage resources in Bengali are not widely avail-
able. 
3 Hybrid Word Alignment Model 
The hybrid word alignment model is described as 
the combination of three word alignment models 
as follows: 
3.1 Word Alignment Using GIZA++ 
GIZA++ (Och and Ney, 2003) is a statistical 
word alignment tool which incorporates all the 
IBM 1-5 models. GIZA++ facilitates fast devel-
opment of statistical machine translation (SMT) 
systems. In case of low-resource language pairs 
the quality of word alignments is typically quite 
low and it also deviates from the independence 
assumptions made by the generative models. 
Although huge amount of parallel data enables 
the model parameters to acquire better estimation, 
a large number of language pairs still lacks from 
the unavailability of sizeable amount of parallel 
data. GIZA++ has some draw-backs. It allows at 
most one source word to be aligned with each 
foreign word. To resolve this issue, some tech-
niques have already been applied such as: the 
parallel corpus is aligned bidirectionally; then the 
two alignment tables are reconciled using differ-
ent heuristics e.g., intersection, union, and most 
recently grow-diagonal-final and grow-diagonal-
final-and heuristics have been applied. In spite of 
these heuristics, the word alignment quality for 
low-resource language pairs is still low and calls 
for further improvement. We describe our ap-
proach of improving word alignment quality in 
the following three subsections. 
3.2 Word Alignment Using Berkley Aligner 
The recent advancements in word alignment is 
implemented in Berkeley Aligner (Liang et al, 
2006) which allows both unsupervised and su-
pervised approach to align word from parallel 
corpus. We initially train the parallel corpus us-
ing unsupervised technique. We make a few 
manual corrections to the alignment table pro-
duced by the unsupervised aligner. Then we ap-
ply this corrected alignment table as gold stand-
ard training data for the supervised aligner. The 
Berkeley aligner is an extension of the Cross Ex-
pectation Maximization word aligner. Berkeley 
aligner is a very useful word aligner because it 
allows for supervised training, enabling us to 
derive knowledge from already aligned parallel 
corpus or we can use the same corpus by updat-
ing the alignments using some rule based meth-
ods. Our approach deals with the latter case. The 
supervised technique of Berkeley aligner helps 
us to align those words which could not be 
aligned by rule based word aligner.  
3.3 Rule Based Word Alignment 
The proposed Rule based aligner aligns Named 
Entities (NEs) and chunks. For NE alignment, 
we first identify NEs from the source side (i.e. 
English) using Stanford NER.  The NEs on the 
target side (i.e. Bengali) are identified using a 
method described in (Ekbal and Bandyopadhyay, 
2009). The accuracy of the Bengali Named Enti-
ty recognizers (NER) is much poorer compared 
to that of English NER due to several reasons: (i) 
there is no capitalization cue for NEs in Bengali; 
(ii) most of the common nouns in Bengali are 
frequently used as proper nouns; (iii) suffixes 
(case markers, plural markers, emphasizers, 
specifiers) get attached to proper names as well 
in Bengali. Bengali shallow parser 1  has been 
used to improve the performance of NE identifi-
cation by considering proper names as NE.  
Therefore, NER and shallow parser are jointly 
employed to detect NEs from the Bengali sen-
tences. The source NEs are then transliterated 
using a modified joint source-channel model 
(Ekbal et al, 2006) and aligned to their target 
side equivalents following the approach of Pal et 
al. (2010). The target side equivalents NEs are 
transformed into canonical form after omitting 
their ?matras?. Similarly Bengali NEs are also 
transformed into canonical forms as Bengali NEs 
may differ in their choice of matras (vowel mod-
ifiers). The transliterated NEs are then matched 
with the corresponding parallel target NEs and 
finally we align the NEs if match is found.   
After identification of multiword NEs on both 
sides, we pre-processed the corpus by replacing 
space with the underscore character (?_?). We 
have used underscore (?_?) instead of hyphen (?-
?) since there already exists some hyphenated 
words in the corpus.  The use of the underscore 
(?_?) character also facilitates to de-tokenize the 
single-tokenized NEs after decoding. 
For chunk alignment, the source sentences of 
the parallel corpus are parsed using Stanford 
POS tagger. The chunks of the sentences are ex-
tracted using CRF chunker2. The chunker detects 
the boundaries of noun, verb, adjective, adverb 
                                                 
1 
http://ltrc.iiit.ac.in/showfile.php?filename=downloads/shallo
w_parser.php 
2 http://crfchunker.sourceforge.net/ 
96
and prepositional chunks from the sentences. In 
case of prepositional phrase chunks, we have 
taken a special attention: we have expanded the 
prepositional phrase chunk by examining a single 
noun chunk followed by a preposition or a series 
of noun chunks separated by conjunctions such 
as 'comma', 'and' etc.  For each individual chunk, 
the head word is identified. Similarly target side 
sentences are parsed using a shallow parser. The 
individual target side Bengali chunks are extract-
ed from the parsed sentences.  The head words 
for all individual chunks on the target side are 
also marked. If the translated head word of a 
source chunk matches with the headword of a 
target chunk then we hypothesize that these two 
chunks are translations of each other.  
The extracted source chunks are translated us-
ing a baseline SMT model trained on the same 
corpus. The translated chunks are validated 
against the target chunks found in the corre-
sponding target sentence. During the validation 
process, if any match is found between the trans-
lated chunk and a target chunk then the source 
chunk is directly aligned with the original target 
chunk. Otherwise, the source chunk is ignored in 
the current iteration for any possible alignment 
and is considered in the next iterations. 
 
 
 
 
 
 
Figure 1.a: Rule based alignments 
 
 
 
 
 
 
Figure 1.b: Gold standard alignments 
 
Figure 1: Establishing alignments through Rule 
based methods. 
 
The extracted chunks on the source side may 
not have a one to one correspondence with the 
target side chunks. The alignment validation pro-
cess is focused on the proper identification of the 
head words and not between the translated 
source chunk and target chunk. The matching 
process has been carried out using a fuzzy 
matching technique. If both sides contain only 
one chunk after aligning the remaining chunks 
then the alignment is trivial. After aligning the 
individual chunks, we also establish word align-
ments between the matching words in those 
aligned chunks. Thus we get a sentence level 
source-target word alignment table.  
Figure 1 shows how word alignments are es-
tablished between a source-target sentence pair 
using the rule based method. Figure 1.a shows 
the alignments obtained through rule based 
method. The solid links are established through 
transliteration (for NEs) and translation. The dot-
ted arrows are also probable candidates for intra-
chunk word alignments; however they are not 
considered in the present work. Figure 1.b shows 
the gold standard alignments for this sentence 
pair.  
3.4  Hybrid Word alignment Model  
The hybrid word alignment method combines 
three different kinds of word alignments ? Gi-
za++ word alignment with grow-diag-final-and 
(GDFA) heuristic, Berkeley aligner and rule 
based aligner. We have followed two different 
strategies to combine the three different word 
alignment tables.  
 
Union 
In the union method all the alignment tables are 
united together and duplicate entries are removed. 
 
ADD additional Alignments  
In this method we consider either of the align-
ments generated by GIZA++ GDFA (A1) or 
Berkeley aligner (A2) as the standard alignment 
as the rule based aligner fails to align all words 
in the parallel sentences. From the three set of 
alignments A1, A2 and A3, we propose an 
alignment combination method as described in 
algorithm 1. 
 
ALGORITHM: 1 
 
Step 1: Choose either A1 or A2 as the standard 
alignment (SA). 
Step 2: Correct the alignments in SA using the 
alignment table of A3. 
Step 3: if A2 is considered as SA then find addi-
tional alignment from A1 and A3 using intersec-
tion method (A1?A3) otherwise find additional 
alignment from A2 and A3 (using A2?A3).   
Step 4: Add additional entries with SA. 
[Jaipur] [golapi sohor name] [porichito] [.] 
[Jaipur] [is known] [as [Pink City]] [.] 
 
[Jaipur] [golapi sohor name] [porichito] [.] 
[Jaipur] [is known] [as [Pink City]] [.] 
 
97
3.5 Berkeley Semi-supervised Alignment 
The correctness of the alignments is verified by 
manually checking the performance of the vari-
ous alignment system. We start with the com-
bined alignment table which is produced by Al-
gorithm 1. Iinitially, we take a subset of the 
alignments by manually inspecting from the 
combined alignment table. Then we train the 
Barkley supervised aligner with this labeled data. 
A subset of the unlabeled data from the com-
bined alignment table is tested with the super-
vised model. The output is then added as addi-
tional labeled training data for the supervised 
training method for the next iteration. Using this 
bootstrapping approach, the amount of labeled 
training data for the supervised aligner is gradu-
ally increased. The process is continued until 
there are no more unlabelled training data. In this 
way we tune the whole alignment table for the 
entire parallel corpus. The process is carried out 
in a semi-supervised manner. 
4 Tools and resources Used  
A sentence-aligned English-Bengali parallel cor-
pus containing 23,492 parallel sentences from 
the travel and tourism domain has been used in 
the present work. The corpus has been collected 
from the consortium-mode project ?Development 
of English to Indian Languages Machine Trans-
lation (EILMT) System - Phase II? 3. The Stan-
ford Parser4 and CRF chunker5 have been used 
for identifying chunks and Stanford NER has 
been used to identify named entities in the source 
side of the parallel corpus.  
The target side (Bengali) sentences are parsed 
by using the tools obtained from the consortium 
mode project ?Development of Indian Language 
to Indian Language Machine Translation (IL-
ILMT) System - Phase II6?. 
The effectiveness of the present work has been 
tested by using the standard log-linear PB-SMT 
model as our baseline system: phrase-extraction 
heuristics described in (Koehn et al, 2003), , 
MERT (minimum-error-rate training) (Och, 
2003) on a held-out development set, target 
                                                 
3  The EILMT project is funded by the Department of Elec-
tronics and Information Technology (DEITY), Ministry of 
Communications and Information Technology (MCIT), 
Government of India. 
4 http://nlp.stanford.edu/software/lex-parser.shtml 
5 http://crfchunker.sourceforge.net/ 
6   The IL-ILMT project is funded by the Department of 
Electronics and Information Technology (DEITY), Ministry 
of Communications and Information Technology (MCIT), 
Government of India. 
language model trained using SRILM toolkit 
(Stolcke, 2002) with Kneser-Ney smoothing 
(Kneser and Ney, 1995) and the Moses decoder 
(Koehn et al, 2007) have been used in the 
present study. 
5 Experiments and Results 
We have randomly selected 500 sentences each 
for the development set and the test set from the 
initial parallel corpus. The rest are considered as 
the training corpus. The training corpus was fil-
tered with the maximum allowable sentence 
length of 100 words and sentence length ratio of 
1:2 (either way). Finally the training corpus con-
tained 22,492 sentences. In addition to the target 
side of the parallel corpus, a monolingual Benga-
li corpus containing 488,026 words from the 
tourism domain was used for building the target 
language model. We experimented with different 
n-gram settings for the language model and the 
maximum phrase length and found that a 4-gram 
language model and a maximum phrase length of 
7 produced the optimum baseline result. We car-
ried out the rest of the experiments using these 
settings. 
We experimented with the system  over 
various combinations of word alignment models. 
Our hypothesis focuses mainly on the theme that 
proper alignment of words will result in 
improvement of the system performance in terms 
of translation quality.  
141,821 chunks were identified from the 
source corpus, of which 96,438 (68%) chunks 
were aligned by the system. 39,931 and 28,107 
NEs were identified from the source and target 
sides of the parallel corpus respectively, of which 
22,273 NEs are unique in English and 22,010 
NEs in Bengali. A total of 14,023 NEs have been 
aligned through transliteration.  
The experiments have been carried out with 
various experimental settings: (i) single 
tokenization of NEs on both sides of the parallel 
corpus, (ii) using Berkeley Aligner with 
unsupervised training, (iii) union of the three 
alignment models: rule based, GIZA++ with 
GDFA and Berkeley Alignment, (iv) 
hybridization of the three alignment models and 
(v) supervised Berkeley Aligner. Eextrinsic 
evaluation was carried out on the MT quality 
using BLEU (Papineni et al, 2002) and NIST 
(Doddington, 2002). 
98
 
 
Experiment Exp 
no. 
BLEU NIST 
Baseline system using GIZA++ with GDFA 1 10.92 4.13 
PB-SMT system using Berkeley Aligner 2 11.42 4.16 
Union of all Alignments 3 11.12 4.14 
PB-SMT System with Hybrid Alignment by considering (a) 
GIZA++ as the standard alignment) (b) Berkeley alignment 
as the standard alignment) 
4a? 15.38 4.30 
4b? 15.92 4.36 
Single tokenized NE + Exp 1 5 11.68 4.17 
Single tokenized NE + Exp 2 6 11.82 4.19 
Single tokenized NE + (a) Exp 4a (b) Exp 4b 7a? 16.58 4.45 
7b? 17.12 4.49 
PB-SMT System with semi-supervised Berkeley Aligner + 
Single tokenized NE 
8? 20.87 4.71 
 
Table: 1 Evaluation results for different experimental setups. (The ??? marked systems produce statis-
tically significant improvements on BLEU over the baseline system) 
 
 
The baseline system (Exp 1) is the state-of-art 
PB-SMT system where GIZA++ with grow-diag-
final-and has been used as the word alignment 
model. Experiment 2 provides better results than 
experiment 1 which signifies that Berkeley 
Aligner performs better than GIZA++ for the 
English-Bengali translation task. The union of all 
thee alignments (Exp 3) provides better scores 
than the baseline; however it cannot beat the re-
sults obtained with the Berkeley Aligner alone. 
Hybrid alignment model with GIZA++ as the  
standard alignment (Exp 4a) produces statistical-
ly significant improvements over the baseline. 
Similarly the use of Berkeley Aligner as the 
standard alignment for hybrid alignment model 
(Exp 4b) also results in statistically significant 
improvements over Exp 2. These two experi-
ments (Exp 4a and 4b) demonstrate the effec-
tiveness of the hybrid alignment model. It is to 
be noticed that hybrid alignment model works 
better with the Berkeley Aligner than with 
GIZA++. 
Single-tokenization of the NEs (Exp 5, 6, 7a 
and 7b) improves the system performance to 
some extent over the corresponding experiments 
without single-tokenization (Exp 1, 2, 4a and 
4b); however, these improvements are not statis-
tically significant. The Berkeley semi-supervised 
alignment method using a bootstrapping ap-
proach together with single-tokenization of NEs 
provided the overall best performance in terms of 
both BLEU and NIST and the corresponding im-
provement is statistically significant on BLEU 
over rest of the experiments. 
6 Conclusion and Future Work 
The paper proposes a hybrid word alignment 
model for PB-SMT. The paper also shows how 
effective pre-processing of NEs in the parallel 
corpus and direct incorporation of their align-
ment in the word alignment model can improve 
SMT system performance. In data driven ap-
proaches to MT, specifically for scarce resource 
data, this approach can help to upgrade the state-
of-art machine translation quality as well as the 
word alignment quality. . The hybrid model with 
the use of the semi-supervised technique of the 
Berkeley word aligner in a bootstrapping manner, 
together with single tokenization of NEs, pro-
vides substantial improvements (9.95 BLEU 
points absolute, 91.1% relative) over the base-
line. On manual inspection of the output we 
found that our best system provides more accu-
99
rate lexical choice as well as better word order-
ing than the baseline system.  
As future work we would like to explore how 
to get the best out of multiple word alignments. 
Furthermore, integrating the knowledge about 
multi-word expressions into the word alignment 
models is another future direction for this work. 
 
Acknowledgement 
 
The work has been carried out with support from 
the project ?Development of English to Indian 
Languages Machine Translation (EILMT) Sys-
tem - Phase II? funded by Department of Infor-
mation Technology, Government of India. 
References  
Alexander Fraser and Daniel Marcu. 2006. Semi-
supervised training for statistical word alignment. 
In ACL-44: Proceedings of the 21st International 
Conference on Computational Linguistics and the 
44th annual meeting of the Association for Compu-
tational Linguistics (ACL-2006), Morristown, NJ, 
USA. pages 769?776. 
Brown, Peter F., Stephen A. Della Pietra, Vincent J. 
Della Pietra, and Robert L. Mercer. 1993. The 
mathematics of statistical machine translation: pa-
rameter estimation. Computational Linguistics, 
19(2):263-311. 
Chris Callison-Burch, David Talbot, and Miles Os-
borne. 2004. Statistical machine translation with 
word- and sentence-aligned parallel corpora. In 
ACL 2004, page 175, Morristown, NJ, USA. Asso-
ciation for Computational Linguistics. 
Dempster, A.P., N.M. Laird, and D.B. Rubin. 1977). 
Maximum Likelihood from Incomplete Data via 
the EM Algorithm. Journal of the Royal Statistical 
Society, Series B (Methodological) 39 (1): 1?38. 
Doddington, George. 2002. Automatic evaluation of 
machine translation quality using n-gram cooccur-
rence statistics. In Proceedings of the Second In-
ternational Conference on Human Language Tech-
nology Research (HLT-2002), San Diego, CA, pp. 
128-132. 
Eck, Matthias, Stephan Vogel, and Alex Waibel. 
2004. Improving statistical machine translation in 
the medical domain using the Unified Medical 
Language System. In Proc. of the 20th Internation-
al Conference on Computational Linguistics (COL-
ING 2004), Geneva, Switzerland, pp. 792-798. 
Ekbal, Asif, and Sivaji Bandyopadhyay. 2008. Maxi-
mum Entropy Approach for Named Entity Recog-
nition in Indian Languages. International Journal 
for Computer Processing of Languages (IJCPOL), 
Vol. 21 (3), 205-237. 
Ekbal, Asif, and Sivaji Bandyopadhyay. 2009. Voted 
NER system using appropriate unlabeled data. In 
proceedings of the ACL-IJCNLP-2009 Named En-
tities Workshop (NEWS 2009), Suntec, Singapore, 
pp.202-210. 
Feng, Donghui, Yajuan Lv, and Ming Zhou. 2004. A 
new approach for English-Chinese named entity 
alignment. In Proceedings of the 2004 Conference 
on Empirical Methods in Natural Language Pro-
cessing (EMNLP-2004), Barcelona, Spain, pp. 
372-379. 
Feng, Donghui, Yajuan Lv, and Ming Zhou. 2004. A 
new approach for English-Chinese named entity 
alignment. In Proceedings of the 2004 Conference 
on Empirical Methods in Natural Language Pro-
cessing (EMNLP-2004), Barcelona, Spain, pp. 
372-379. 
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment 
models. Computational Linguistics, pages 19?51. 
Huang, Fei, Stephan Vogel, and Alex Waibel. 2003. 
Automatic extraction of named entity translingual 
equivalence based on multi-feature cost minimiza-
tion. In Proceedings of the ACL-2003 Workshop 
on Multilingual and Mixed-language Named Entity 
Recognition, 2003, Sapporo, Japan, pp. 9-16. 
HuaWu, HaifengWang, and Zhanyi Liu. 2006. Boost-
ing statistical word alignment using labeled and un-
labeled data. In Proceedings of the COLING/ACL 
on Main conference poster sessions, pages 913?
920, Morristown, NJ, USA. Association for Com-
putational Linguistics.  
Kneser, Reinhard, and Hermann Ney. 1995. Improved 
backing-off for m-gram language modeling. In 
Proceedings of the IEEE Internation Conference on 
Acoustics, Speech, and Signal Processing 
(ICASSP), vol. 1, pp. 181?184. Detroit, MI. 
Koehn, Philipp, Franz Josef Och, and Daniel Marcu. 
2003. Statistical phrase-based translation. In Pro-
ceedings of HLT-NAACL 2003: conference com-
bining Human Language Technology conference 
series and the North American Chapter of the As-
sociation for Computational Linguistics conference 
series,  Edmonton, Canada, pp. 48-54. 
Koehn, Philipp, Hieu Hoang, Alexandra Birch, Chris 
Callison-Burch, Marcello Federico, Nicola Ber-
toldi, Brooke Cowan, Wade Shen, Christine Mo-
ran, Richard Zens, Chris Dyer, Ond?ej Bojar, Alex-
andra Constantin, and Evan Herbst. 2007. Moses: 
open source toolkit for statistical machine transla-
tion. In Proceedings of the 45th Annual meeting of 
the Association for Computational Linguistics 
(ACL 2007): Proceedings of demo and poster ses-
sions, Prague, Czech Republic, pp. 177-180. 
Koehn, Philipp. 2004. Statistical significance tests for 
machine translation evaluation. In  EMNLP-2004: 
100
Proceedings of the 2004 Conference on Empirical 
Methods in Natural Language Processing, 25-26 
July 2004, Barcelona, Spain, pp 388-395. 
O. Chapelle, B. Sch?olkopf, and A. Zien, editors. 
2006. Semi-Supervised Learning. MIT Press, 
Cambridge, MA. 
Och, Franz J. 2003. Minimum error rate training in 
statistical machine translation. In Proceedings of 
the 41st Annual Meeting of the Association for 
Computational Linguistics (ACL-2003), Sapporo, 
Japan, pp. 160-167. 
Pal, Santanu, Sivaji Bandyopadhyay. 2012, ?Boot-
strapping Chunk Alignment in Phrase-Based Sta-
tistical Machine Translation?, Joint Workshop on 
Exploiting Synergies between Information Retriev-
al and Machine Translation (ESIRMT) and Hybrid 
Approaches to Machine Translation (HyTra), 
EACL-2012, Avignon, France, pp. 93-100 . 
Pal, Santanu., Sudip Kumar Naskar, Pavel Pecina, 
Sivaji Bandyopadhyay and Andy Way. 2010, Han-
dling Named Entities and Compound Verbs in 
Phrase-Based Statistical Machine Translation, In 
proc. of the workshop on Multiword expression: 
from theory to application (MWE-2010), The 23rd 
International conference of computational linguis-
tics (Coling 2010),Beijing, Chaina, pp. 46-54. 
Papineni, Kishore, Salim Roukos, Todd Ward, and 
Wei-Jing Zhu. 2002. BLEU: a method for automat-
ic evaluation of machine translation. In Proceed-
ings of the 40th Annual Meeting of the Association 
for Computational Linguistics (ACL-2002), Phila-
delphia, PA, pp. 311-318. 
Percy Liang, Ben Taskar, Dan Klein. 2006.  6th Pro-
ceedings of the main conference on Human Lan-
guage Technology Conference of the North Ameri-
can Chapter of the Association of Computational 
Linguistics, HLT-NAACL-2006, Pages 104-111 
Stolcke, A. SRILM?An Extensible Language Mod-
eling Toolkit. Proc. Intl. Conf. on Spoken Lan-
guage Processing, vol. 2, pp. 901?904, Denver 
(2002). 
Vamshi Ambati, Stephan Vogel, Jaime Carbonell. 
2010, 10th Proceedings of the NAACL HLT 2010 
Workshop on Active Learning for Natural Lan-
guage Processing (ALNLP-2010), Pages 10-17. 
Vogel, Stephan, Hermann Ney, and Christoph Till-
mann. 1996. HMM-based word alignment in statis-
tical translation. In Proc. of the 16th International 
Conference on Computational Linguistics (COL-
ING 1996), Copenhagen, pp. 836-841. 
Wu, Hua Haifeng Wang, and Chengqing Zong. 2008. 
Domain adaptation for statistical machine transla-
tion with domain dictionary and monolingual cor-
pora. In Proc. of the 22nd International Conference 
on Computational Linguistics (COLING 2008),  
Manchester, UK, pp. 993-1000. 
X. Zhu. 2005. Semi-Supervised Learning Literature 
Survey. Technical Report 1530, Computer Scienc-
es, University of Wisconsin-Madison. 
http://www.cs.wisc.edu/_jerryzhu/pub/ssl_survey.p
df. 
 
101
Proceedings of the 3rd Workshop on Hybrid Approaches to Translation (HyTra) @ EACL 2014, pages 48?57,
Gothenburg, Sweden, April 27, 2014. c?2014 Association for Computational Linguistics
Automatic Building and Using Parallel Resources for SMT from 
Comparable Corpora 
Santanu Pal1, Partha Pakray2, Sudip Kumar Naskar3 
1Universit?t Des Saarlandes, Saarbr?cken, Germany 
2Computer & Information Science,  
Norwegian University of Science and Technology, Trondheim, Norway 
3Department of Computer Science & Engineering,  
Jadavpur University, Kolkata, India 
1santanu.pal@uni-saarland.de, 
2partha.pakray@idi.ntnu.no, 
3sudip.naskar@cse.jdvu.ac.in 
 
 
Abstract 
Building parallel resources for corpus 
based machine translation, especially 
Statistical Machine Translation (SMT), 
from comparable corpora has recently 
received wide attention in the field 
Machine Translation research. In this 
paper, we propose an automatic approach 
for extraction of parallel fragments from 
comparable corpora. The comparable 
corpora are collected from Wikipedia 
documents and this approach exploits the 
multilingualism of Wikipedia. The 
automatic alignment process of parallel 
text fragments uses a textual entailment 
technique and Phrase Based SMT (PB-
SMT) system.  The parallel text 
fragments extracted thus are used as 
additional parallel translation examples 
to complement the training data for a PB-
SMT system. The additional training data 
extracted from comparable corpora 
provided significant improvements in 
terms of translation quality over the 
baseline as measured by BLEU. 
1 Introduction 
Comparable corpora have recently attracted huge 
interest in natural language processing research. 
Comparable corpora are now considered as a rich 
resource for acquiring parallel resources such as 
parallel corpus or parallel text fragments,. 
Parallel text extracted from comparable corpora 
can take an important role in improving the 
quality of machine translation (MT) (Smith et al. 
2010).  Parallel text extracted from comparable 
corpora are typically added with the training 
corpus as additional training material which is 
expected to facilitate better performance of SMT 
systems specifically for low density language 
pairs. 
In the present work, we try to extract 
English?Bengali parallel fragments of text from 
comparable corpora. We have collected 
document aligned corpus of English?Bengali 
document pairs from Wikipedia which provides a 
huge collection of documents in many different 
languages. For automatic alignment of parallel 
fragments we have used two-way textual 
entailment (TE) system and a baseline SMT 
system.  
Textual entailment (TE), introduced by 
(Dagan and Glickman, 2004), is defined as a 
directional relationship between pairs of text 
expressions, denoted by the entailing text (T) and 
the entailed hypothesis (H). T entails H if the 
meaning of H can be inferred from the meaning 
of T. Textual Entailment has many applications 
in NLP tasks, such as summarization, 
information extraction, question answering, 
48
information retrieval, machine translation, etc. In 
machine translation, textual entailment can be 
applied to MT evaluation (Pado et al., 2009). A 
number of research works have been carried out 
on cross-lingual Textual entailment using MT 
(Mehdad et al.,2010; Negri et al., 2010; Neogi et 
al., 2012). However, to the best of our 
knowledge, the work presented here is the first 
attempt towards employing textual entailment for 
the purpose of extracting parallel text fragments 
from comparable corpora which in turn are used 
to improve MT system.  
Munteanu and Marcu (2006) suggested that 
comparable corpora tend to have parallel data at 
sub-sentential level. Hence, instead of finding 
sentence level parallel resource from comparable 
corpora, in the present work we mainly focus on 
finding parallel fragments of text. 
We carried out the task of automatic alignment 
of parallel fragments using three steps: (i) mining 
comparable corpora form Wikipedia, (ii) 
sentence level alignment using two-way TE and 
a baseline Bengali?English SMT system, and 
finally (iii) clustering the parallel sentence 
aligned comparable corpora using textual 
entailment and then aligning parallel fragments 
of text by textual entailment and a baseline 
Bengali?English SMT system.  
Although, we have collected document 
aligned comparable corpora, the documents in 
the corpus do not belong to any particular 
domain. Even with such a corpus we have been 
able to improve the performance of an existing 
machine translation system which was built on 
tourism domain data. This also signifies the 
contribution of this work towards domain 
adaptation of MT systems. 
The rest of the paper is organized as follows. 
Section 2 describes the related work. Section 3 
describes the mining process of the comparable 
corpora. The two-way TE system architecture is 
described in section 4. Section 5 describes the 
automatic alignment technique of parallel 
fragment of texts. Section 6 describes the tools 
and resources used for this work. The 
experiments and evaluation results are presented 
in section 7. Section 8 concludes and presents 
avenues for future work. 
2 Related Work  
Comparable corpora have been used in many 
research areas in NLP, especially in machine 
translation. Several earlier works have studied 
the use of comparable corpora in machine 
translation. However, most of these approaches 
(Fung and McKeown, 1997; Fung and Yee, 1998; 
Rapp, 1999; Chiao and Zweigenbaum, 2002; 
Dejean et al., 2002; Kaji, 2005; Otero, 2007; 
Saralegui et al., 2008; Gupta et al., 2013) are 
specifically focused on extracting word 
translations from comparable corpora. Most of 
the strategies follow a standard method based on 
the context vector similarity measure such as 
finding the target words that have the most 
similar distributions with a given source word. In 
most of the cases, a starting list contains the 
?seed expressions? and this list is required to 
build the context vectors of the words in both the 
languages. A bilingual dictionary can be used as 
a starting list. The bilingual list can also be 
prepared form parallel corpus using bilingual 
correlation method (Otero, 2007). Instead of a 
bilingual list, multilingual thesaurus could also 
be used for this purpose (Dejean, 2002).  
Wikipedia is a multilingual encyclopedia 
available in different languages and it can be 
used as a source of comparable corpora. Otero et 
al. (2010) stored the entire Wikipedia for any 
two languages and transformed it into a new 
collection: CorpusPedia. Our work shows that 
only a small ad-hoc corpus containing Wikipedia 
articles could prove to be beneficial for existing 
MT systems. 
In the NIST shared task on Recognizing 
Textual Entailment Challenge (RTE), several 
methods have been proposed to tackle the textual 
entailment problem. Most of these systems use 
some form of lexical matching, e.g., n-gram, 
word similarity, etc. and even simple word 
overlap. A number of systems represent the texts 
as parse trees (e.g., syntactic or dependency trees) 
49
before the actual task. Some of the systems use 
semantic features (e.g., logical inference, 
Semantic Role Labelling) for solving the text and 
hypothesis entailment problem. MacCartney et al. 
(2006) proposed a new architecture for textual 
inference in which finding a good alignment is 
separated from evaluating entailment. Agichtein 
et al. (2008) presented a supervised machine 
learning approach to train a classifier over a 
variety of lexical, syntactic, and semantic metrics. 
Malakasiotis (2009) used string similarity 
measures applied to shallow abstractions of the 
input sentences and a Maximum Entropy 
classifier to learn how to combine the resulting 
features.  
In the present work, we used the textual 
entailment system of Pakray et al. (2011) which 
performed well on various RTE tasks and 
datasets, as well as other NLP tasks like question 
answering, summarization, etc. We integrated a 
new module to by using reVerb 1  tool and 
optimized all the features produced by different 
modules. 
The main objective of the present work is to 
investigate whether textual entailment can be 
used to establish alignments between text 
fragments in comparable corpora and whether 
the parallel text fragments extracted thus can 
improve MT system performance. 
3 Mining Comparable Corpora 
We collected comparable corpora from 
Wikipedia - online collaborative encyclopedia 
available in a wide variety of languages. English 
Wikipedia contains largest volume of data such 
as millions of articles; there are many language 
editions with at least 100,000 articles. Wikipedia 
links articles on the same topic in different 
languages using ?interwiki? linking facility. 
Wikipedia is an enormously useful re-source for 
extracting parallel resources as the documents in 
different languages are already aligned. We first 
collect an English document from Wikipedia and 
then find the same document in Bengali if there 
                                                        
1 http://reverb.cs.washington.edu/ 
exists any inter-language link. Extracted 
English?Bengali document pairs from Wikipedia 
are already comparable since they are written 
about the same entity. Although each 
English?Bengali document pairs are comparable 
and they discuss about the same topic, most of 
the times they are not exact translation of each 
other; as a result parallel fragments of text are 
rarely found in these document pairs. The bigger 
the size of the fragment may result less probable 
parallel version will be found in the target side. 
Nevertheless, there is always chance of getting 
parallel phrase, tokens or even sentences in 
comparable documents.   
We designed a crawler to collect comparable 
corpora for English?Bengali document pairs. 
Based on an initial seed keyword list, the crawler 
first visits each English page of Wikipedia, saves 
the raw text (in HTML format), and then follows 
the cross-lingual link for each English page and 
collects the corresponding Bengali document. In 
this way, we collect English?Bengali comparable 
documents in the tourism domain. We retain only 
the textual information and all the other details 
are discarded. We extract English and Bengali 
sentences from each document. The extracted 
sentences from each English document are not 
parallel with the corresponding Bengali 
document. Moreover, Bengali documents are 
contained limited information compare to the 
English document. We align sentences of 
English?Bengali from these comparable corpora 
through a baseline PB-SMT system. A Bengali-
English baseline PB-SMT system has been 
developed which was trained on 
English?Bengali tourism domain corpus. We 
translated Bengali sentences into English. The 
translated sentence is then examined for 
entailment in the English comparable document 
by using two-way TE system proposed in section 
4. If it is more than 50% entailed with the target 
document then the target sentence is directly 
fetched form the comparable English document 
and the source-target sentence pair are saved in a 
list. In this way, we extract parallel sentences 
from comparable corpora. These parallel 
sentences except those are 100% entailed may 
50
not be completely parallel but they are 
comparable. So, we created a parallel fragment 
list which is proposed in section 5.  
4 Two-way Textual Entailment System 
A two-way automatic textual entailment (TE) 
recognition system that uses lexical, syntactic 
and semantic features has been described in this 
section. The system architecture has been shown 
in Figure 1. The TE system has used the Support 
Vector Machine (SVM) technique that uses 
thirty-one features for training purpose. In lexical 
module there are eighteen features and eleven 
features from syntactic module, one feature by 
using reVerb and one feature from semantic 
module. 
 
Fig.1 Two way TE architecture 
4.1 Lexical Module 
In this module six lexical comparisons and 
seventeen lexical distance comparisons between 
text and hypothesis has used.  
Six lexical comparisons are WordNet 
(Fellbaum, 1998) based unigram match, bigram 
match, longest common sub-sequence, skip-gram, 
stemming and named entity matching.  We have 
calculated weight from each of these six 
comparisons in equation (1). 
weight =
number - of - common - tokens - between - text - and - hypothesis?
number - of - tokens - in - hypothesis?  
(1) 
The API for WordNet Searching (JAWS) 2 
provides Java applications with the ability to 
retrieve data from the WordNet 2.1 database. 
For Named entity detection we have used Text 
Tokenization Toolkit (LT-TTT2)3 (Grover et. al., 
1999). The LT-TTT2 named entity component 
has been used.  
For lexical distance measure, we have used 
features of Vector Space Measures (Euclidean 
distance, Block distance, Minkowsky distance, 
Cosine similarity, Matching Coefficient), Set-
based Similarities (Dice, Jaccard, Overlap, 
Harmonic), Edit Distance Measures (Levenshtein 
distance, Smith-Waterman distance, Jaro 
Distance). Lexical distance measurement has 
used the libraries SimMetrics 4 , SimPack 5  and 
SecondString6. SimMetrics is a Similarity Metric 
Library, e.g., from edit distance (Levenshtein, 
Gotoh, Jaro etc) to other metrics, (e.g Soundex, 
Chapman). 
4.2 Syntactic Module  
The syntactic module compares the dependency 
relations in both hypothesis and text. The system 
extracts syntactic structures from the text-
hypothesis pairs using Combinatory Categorial 
Grammar (C&C CCG) Parser 7  and Stanford 
Parser 8  and compares the corresponding 
structures to determine if the entailment relation 
is established. Two different systems have been 
implemented one system used Stanford Parser 
output and another system used C&C CCG 
Parser. The system accepts pairs of text snippets 
(text and hypothesis) at the input and gives score 
for each comparison. Some of the important 
comparisons on the dependency structures of the 
text and the hypothesis are Subject-subject 
comparison, WordNet Based Subject-Verb 
                                                        
2 http://lyle.smu.edu/~tspell/jaws/index.html 
3 http://www.ltg.ed.ac.uk/software/lt-ttt2 
4 http://sourceforge.net/projects/simmetrics/ 
5https://files.ifi.uzh.ch/ddis/oldweb/ddis/research/simpack/in
dex.html 
6 http://sourceforge.net/projects/secondstring/ 
7 http://svn.ask.it.usyd.edu.au/trac/candc/wiki 
8 http://nlp.stanford.edu/software/lex-parser.shtml 
51
Comparison, Subject-Subject Comparison, 
Object-Verb Comparison, WordNet Based 
Object-Verb Comparison, Cross Subject-Object 
Comparison Number Comparison, Noun 
Comparison, Prepositional Phrase Comparison, 
Determiner Comparison and other relation 
Comparison.  
4.3 reVerb Module  
ReVerb 9  is a tool, which extracts binary 
relationships from English sentences.  The 
extraction format is in Table 1. 
Extraction Format arg1 rel arg2 
Example A person is playing a guitar 
reVerb Extracts arg1= {A person}  rel = {is 
playing} arg2 = {a guitar} 
 
Table 1: Example by reVerb Tool 
The system parsed the text and the hypothesis 
by reverb tool. Each of the relations compares 
between text and hypothesis and calculates a 
score for each pair. 
4.4 Semantic Module 
The semantic module based on the Universal 
Networking Language (UNL) (Uchida and Zhu, 
2001). The UNL can express information or 
knowledge in semantic network form with hyper-
nodes. The UNL is like a natural language for 
computers to represent and process human 
knowledge. There are two modules in UNL 
system - En-converter and De-converter module. 
The process of representing natural language 
sentences in UNL graphs is called En-converting 
and the process of generating natural language 
sentences out of UNL graphs is called De-
converting. An En-Converter is a language 
independent parser, which provides a framework 
for morphological, syntactic, and semantic 
analysis synchronously. The En-Converter is 
based on a word dictionary and a set of 
enconversion grammar rules. It analyses 
sentences according to the en-conversion rules. 
A De-Converter is a language independent 
                                                        
9 http://reverb.cs.washington.edu/ 
generator, which provides a framework for 
syntactic and morphological generation 
synchronously. 
An example UNL relation for a sentence 
?Pfizer is accused of murdering 11 children? is 
shown in Table 2. 
[S:00] 
{org:en} Pfizer is accused of murdering 11 children 
{/org} 
{unl} 
obj(accuse(icl>do,equ>charge,cob>abstract_thing,agt>per
son,obj>person).@entry 
.@present,pfizer.@topic) 
qua:01(child(icl>juvenile>thing).@pl,11) 
obj:01(murder(icl>kill>do,agt>thing,obj>living_thing).@
entry,child(icl>juvenile 
>thing).@pl) 
cob(accuse(icl>do,equ>charge,cob>abstract_thing,agt>per
son,obj>person).@entr 
y.@present,:01) 
{/unl}  
[/S] 
 
Table 2: Example of UNL  
The system converts the text and the 
hypothesis into UNL relations by En-Converter. 
Then it compares the UNL relations in both the 
text and the hypothesis and gives a score for each 
comparison. 
4.5 Feature Extraction Module 
The features are listed in Table 3: 
Name of Features No of features 
Lexical Module 18 
Syntactic Module 11 
reVerb Module 1 
Semantic Module 1 
 
Table 3: Features for SVM 
4.6 Support Vector Machines (SVM) 
Support Vector Machines (SVMs) 10  are 
supervised learning models used for 
classification and regression analysis. The basic 
SVM takes a set of input data and predicts, for 
                                                        
10 http://en.wikipedia.org/wiki/Support_vector_machine 
52
each given input, which of two possible classes 
form the output, making it a non-probabilistic 
binary linear classifier.   
The SVM based our Textual Entailment 
system has used the following data sets: RTE-1 
development and RTE-1 annotated test set, RTE-
2 development set and RTE-2 annotated test set, 
RTE-3 development set and RTE-3 annotated 
test set to deal with the two-way classification 
task. The system has used the LIBSVM -- A 
Library for Support Vector Machines 11  for the 
classifier to learn from this data set. 
5 Alignment of Parallel fragments using 
proposed TE system 
We have extracted parallel fragment from the 
parallel sentence aligned comparable resource 
list as well as the training data. Initially, we 
make cluster on the English side of this list with 
the help of two-way TE method. More than 50% 
entailed sentences have been considered to take a 
part of the same cluster. The TE system divides 
the complete set of comparable resources list into 
some smaller sets of cluster. Each cluster 
contains at least two English sentences. Each 
English cluster is corresponding to the set 
comparable Bengali sentences. So in this way we 
have developed a number of English Bengali 
parallel clusters. We intersect between the both 
English and Bengali sentences which are 
belonging to the same clusters.     
We try to align the English and Bengali 
fragments extracted from a parallel sentence 
aligned comparable resource list. If both sides 
contain only one fragment then the alignment is 
trivial, and we add such fragment pairs to seed 
another parallel fragment corpus that contains 
examples having only one token in both side. 
Otherwise, we establish alignments between the 
English and Bengali fragments using translation. 
If both the English and Bengali side contains n 
number of fragments, and the alignments of n-1 
fragments can be established through translation 
                                                        
11 http://www.csie.ntu.edu.tw/~cjlin/libsvm/ 
or by means of already existing alignments, then 
the nth alignment is trivial.  
These parallel fragments of text, extracted 
from the comparable corpora are added with the 
tourism domain training corpus to enhance the 
performance of the baseline PB-SMT system. 
6 Tools and Resources 
A sentence-aligned English?Bengali parallel 
corpus contains 23,492 parallel sentences from 
the travel and tourism domain has been used in 
the present work. The corpus has been collected 
from the consortium-mode project ?Development 
of English to Indian Languages Machine 
Translation (EILMT) System 12 ?. The Stanford 
Parser 13  and CRF chunker 14  (Xuan-Hieu Phan, 
2006) have been used for parsing and chunking 
in the source side of the parallel corpus, 
respectively.  
The experiments were carried out using the 
standard log-linear PB-SMT model as our 
baseline system: GIZA++ implementation of 
IBM word alignment model 4, phrase-extraction 
heuristics described in (Koehn et al., 2003), 
minimum-error-rate training (Och, 2003) on a 
held-out development set, target language model 
trained using SRILM toolkit (Stolcke, 2002) with 
Kneser-Ney smoothing (Kneser and Ney, 1995) 
and the Moses decoder (Koehn et al., 2007) have 
been used in the present study. 
7 Experiments and Results 
We randomly identified 500 sentences each for 
the development set and the test set from the 
initial parallel corpus. The rest is considered as 
the training corpus. The training corpus was 
filtered with the maximum allowable sentence 
length of 100 words and sentence length ratio of 
1:2 (either way). Finally the training corpus 
                                                        
12 The EILMT project is funded by the Department of 
Electronics and Information Technology (DEITY), Ministry 
of Communications and Information Technology (MCIT), 
Government of India. 
13 http://nlp.stanford.edu/software/lex-parser.shtml 
14 http://crfchunker.sourceforge.net/ 
53
contained 22,492 sentences. In addition to the 
target side of the parallel corpus, we used a 
monolingual Bengali corpus containing 488,026 
words from the tourism domain for building the 
target language model. Experiments were carried 
out with different n-gram settings for the 
language model and the maximum phrase length 
and it was found that a 4-gram language model 
and a maximum phrase length of 7 produce the 
optimum baseline result on both the development 
and the test set. We carried out the rest of the 
experiments using these settings. 
The collected comparable corpus consisted of 
5582 English?Bengali document pairs. It is 
evident from Table 4 that English documents are 
more informative than the Bengali documents as 
the number of sentences in English documents is 
much higher than those in the Bengali documents. 
When the Bengali fragments of texts were passed 
to the Bengali?English translation module some 
of them could not be translated into English and 
also, some of them could be translated only 
partially. Therefore, some of the tokens were 
translated while some were not. Some of those 
partially translated text fragments were aligned 
through textual entailment; however, most of 
them were discarded. As can be seen from Table 
4, 9,117 sentences were entailed in the English 
side, of which the system was able to establish 
cross-lingual entailment for 2,361 
English?Bengali sentence pairs.  
 No. of 
English 
sentence 
No. of 
Bengali 
sentence 
Extraction from 
Comparable corpora 
579037 169978 
more than 50% Entailed 
English Sentences 
9117 - 
more than 50% Entailed 
(sentence aligned 
comparable) 
2361 2361 
parallel fragment of texts 
from sentence aligned 
comparable list 
3937 3937 
 
Table 4: Statistics of the sentence aligned comparable 
list and the aligned parallel text fragments.  
Finally, the textual entailment based alignment 
procedure was able to align 3937 parallel 
fragments as reported in Table 4. Manual 
inspection of the parallel list revealed that most 
of the aligned texts were of good quality. 
We carried out evaluation of the MT quality 
using four automatic MT evaluation metrics: 
BLEU (Papineni et al., 2002), METEOR 
(Banerjee and Lavie, 2005), NIST (Doddington, 
2002) and TER (Snover et al., 2006). Table 5 
shows the performance of the PB-SMT systems 
built on the initial training corpus and the larger 
training corpus containing parallel text fragments 
extracted from the comparable corpora. Treating 
the parallel text fragments extracted from the 
comparable corpora as additional training 
material results in significant improvement in 
terms of BLEU (1.73 points, 15.84% relative) 
over the baseline system. Similar improvements 
are also obtained for the other metrics.  The low 
evaluation scores could be attributed to the fact 
that Bengali is a morphologically rich language 
and has a relatively free phrase order; besides 
there were only one set of reference translations 
for the testset. 
Experiments BLEU NIST METEOR TER 
Baseline 10.92 4.16 0.3073 75.34 
Baseline  + 
parallel 
fragments of 
texts as 
additional 
training 
material 
12.65 4.32 0.3144 73.00 
 
Table 5: Evaluation results 
8 Conclusion and Future Work 
In this paper, we have successfully extracted 
English?Bengali parallel fragments of text from 
comparable corpora using textual entailment 
techniques. The parallel text fragments extracted 
thus were able to bring significant improvements 
in the performance of an existing machine 
translation system. For low density language 
pairs, this approach can help to improve the 
state-of-art machine translation quality. A 
manual inspection on a subset of the output 
revealed that the additional training material 
54
extracted from comparable corpora effectively 
resulted in better lexical choice and less OOV 
words than the baseline output.  As the collected 
parallel text does not belong to any particular 
domain, this work also signifies that out of 
domain data is also useful to enhance the 
performance of a domain specific MT system. 
This aspect of the work would be useful for 
domain adaptation in MT. As future work, we 
would like to carry out experiments on larger 
datasets.  
Acknowledgments 
The research leading to these results has received 
funding from the EU project EXPERT ?the 
People Programme (Marie Curie Actions) of the 
European Union's Seventh Framework 
Programme FP7/2007-2013<tel:2007-2013>/ 
under REA grant agreement no. [317471]. We 
acknowledge the support from Department of 
Computer and Information Science, Norwegian 
University of Science and Technology and also 
support from ABCDE fellowship programme 
2012-1013. 
References  
Banerjee, Satanjeev and Alon Lavie. 2005. METEOR: 
An Automatic Metric for MT Evaluation with 
Improved Correlation with Human Judgments. 
Proceedings of the ACL Workshop on Intrinsic and 
Extrinsic Evaluation Measures for Machine 
Translation and/or Summarization, Ann Arbor, 
Michigan, pages 65?72. 
Chiao, Yun-Chuang and Pierre Zweigenbaum. 2002. 
Looking for candidate translational equivalents in 
specialized, comparable corpora. In Proceedings of 
the 19th international conference on Computational 
linguistics, Volume 2, Association for 
Computational Linguistics, pages 1-5. 
Dagan, Ido and Oren Glickman. 2004. Probabilistic 
textual entailment: generic applied modeling of 
language variability, In PASCAL Workshop on 
Learning Methods for Text Understanding and 
Mining, Grenoble, France. 
De Marneffe, Marie-Catherine, Bill MacCartney, 
Trond Grenager, Daniel Cer, Anna Rafferty, and 
Christopher D. Manning. 2006. Learning to 
distinguish valid textual entailments. In B. Magnini 
and I. Dagan (eds.), Proceedings of the Second 
PASCAL Recognizing Textual Entailment 
Challenge. Venice: Springer, pages 74?79. 
D?jean, Herv?, ?ric Gaussier, and Fatia Sadat. 2002. 
Bilingual terminology extraction: an approach 
based on a multilingual thesaurus applicable to 
comparable corpora. In Proceedings of the 19th 
International Conference on Computational 
Linguistics COLING, Pages 218-224. 
 Doddington, George. 2002. Automatic evaluation of 
machine translation quality using n-gram co-
occurrence statistics. In Proceedings of the second 
international conference on Human Language 
Technology Research . Morgan Kaufmann 
Publishers Inc, pages. 138-145. 
Fung, Pascale and Kathleen McKeown. 1997. Finding 
terminology translations from non-parallel corpora. 
In Proceedings of the 5th Annual Workshop on 
Very Large Corpora, pages 192-202. 
Fung, Pascale and Lo Yuen Yee. 1998. An IR 
approach for translating new words from 
nonparallel, comparable texts. In Proceedings of 
the 17th international conference on Computational 
linguistics-Volume 1, Association for 
Computational Linguistics, pages 414-420. 
Gupta, Rajdeep, Santanu Pal, and Sivaji 
Bandyopadhyay. 2013. Improving MT System 
Using Extracted Parallel Fragments of Text from 
Comparable Corpora. In proceedings of 6th 
workshop of Building and Using Comparable 
Corpora (BUCC), ACL, Sofia, Bulgaria, Pages 69-
76. 
Kneser, Reinhard and Hermann Ney. 1995. Improved 
backing-off for n-gram language modeling. In 
Proceedings of the IEEE International Conference 
on Acoustics, Speech and Signal Processing, 
volume I. pages 181-184. 
Koehn, Philipp, Hieu Hoang, Alexandra Birch, Chris 
Callison-Burch, Marcello Federico,Nicola Bertoldi, 
Brooke Cowan, Wade Shen, Christine Moran, 
Richard Zens, Chris Dyer, Ond rej Bojar, 
Alexandra Constantin, and Evan Herbst. Moses: 
open source toolkit for statistical machine 
translation. In Proceedings of the 45th Annual 
Meeting of the ACL on Interactive Poster and 
Demonstration Sessions. Association for 
Computational Linguistics, pages 177-180. 
Koehn, Philipp, Franz Josef Och, and Daniel Marcu. 
2003. Statistical phrase-based translation. In 
55
Proceedings of the 2003 Conference of the North 
American Chapter of the Association for 
Computational Linguistics on Human Language 
Technology-Volume 1, Association for 
Computational Linguistics, pages 48-54. 
Mehdad, Yashar, Matteo Negri, and Marcello 
Federico. 2010. Towards Cross-Lingual Textual 
entailment. In Proceedings of the 11th Annual 
Conference of the North American Chapter of the 
Association for Computational Linguistics,  
NAACL-HLT 2010. LA, USA.  
Munteanu,  Dragos Stefan and Daniel Marcu. 2006. 
Extracting parallel sub-sentential fragments from 
non-parallel corpora. In Proceedings of the 21st 
International Conference on Computational 
Linguistics and the 44th annual meeting of the 
Association for Computational Linguistics, 
Association for Computational Linguistics, pages 
81-88. 
Negri, Matteo, and Yashar Mehdad. 2010. Creating a 
Bilingual Entailment Corpus through Translations 
with Mechanical Turk: $100 for a 10-day Rush. In 
Proceedings of the NAACL-HLT 2010, Creating 
Speech and Text Language Data With Amazon's 
Mechanical Turk Workshop. LA, USA.  
Neogi, Snehasis, Partha Pakray, Sivaji 
Bandyopadhyay, and Alexander Gelbukh. 2012. 
JU_CSE_NLP: Language Independent Cross-
lingual Textual Entailment System. (*SEM) First 
Joint Conference on Lexical and Computational 
Semantics, Collocated with NAACL-HLT 2012, 
Montreal, Canada.  
Och, F. Josef. 2003. Minimum error rate training in 
statistical machine translation. In Proceedings of 
the 41st Annual Meeting on Association for 
Computational Linguistics-Volume 1, Association 
for Computational Linguistics, pages 160-167. 
Och, F. Josef and Herman Ney. 2000. Giza++: 
Training of statistical translation models. 
Otero, P. Gamallo. 2007. Learning bilingual lexicons 
from comparable english and spanish corpora. 
Proceedings of MT Summit xI, pages 191-198. 
Otero, P. Gamallo and Isaac Gonz?lez L?pez. 2010. 
Wikipedia as multilingual source of comparable 
corpora. In Proceedings of the 3rd Workshop on 
Building and Using Comparable Corpora, LREC, 
pages 21-25. 
 Papineni, Kishore, Salim Roukos, Todd Ward, and 
Wei-Jing Zhu. 2002. BLEU: a method for 
automatic evaluation of machine translation. In 
Proceedings of the 40th annual meeting on 
association for computational linguistics, 
Association for Computational Linguistics, pages 
311-318. 
Prodromos Malakasiotis. 2009. "AUEB at TAC 2009", 
In TAC 2009 Workshop, National Institute of 
Standards and Technology Gaithersburg, Maryland 
USA. 
Rapp, Reinhard. 1999. Automatic identification of 
word translations from unrelated English and 
German corpora. In Proceedings of the 37th annual 
meeting of the Association for Computational 
Linguistics on Computational Linguistics, 
Association for Computational Linguistics, pages 
519-526. 
Saralegui, X., San Vicente, I., and Gurrutxaga, A. 
2008. Automatic generation of bilingual lexicons 
from comparable corpora in a popular science 
domain. In LREC 2008 workshop on building and 
using comparable corpora. 
Pado, Sebastian, Michel Galley, Dan Jurafsky, and 
Christopher D. Manning. 2009. Textual entailment 
features for machine translation evaluation. In 
Proceedings of the EACL Workshop on Statistical 
Machine Translation, Athens, Greece, pages 37?41. 
 Smith, R. Jason, Chris Quirk, and Kristina Toutanova. 
2010. Extracting parallel sentences from 
comparable corpora using document level 
alignment. In Human Language Technologies: The 
2010 Annual Conference of the North American 
Chapter of the Association for Computational 
Linguistics, Association for Computational 
Linguistics, pages 403-411. 
Snover, Matthew, Bonnie Dorr, Richard Schwartz, 
Linnea Micciulla, and John Makhoul. 2006. A 
study of translation edit rate with targeted human 
annotation. Proceedings of Association for 
Machine Translation in the Americas, Cambridge, 
Massachusetts, USA, pages 223?231. 
Pakray, Partha, Snehasis Neogi, Pinaki Bhaskar, 
Soujanya Poria, Sivaji Bandyopadhyay, and 
Alexander Gelbukh. 2011. A Textual Entailment 
System using Anaphora Resolution. System Report, 
Text Analysis Conference Recognizing Textual 
Entailment Track (TAC RTE) Notebook, 
November 14-15, 2011, National Institute of 
56
Standards and Technology, Gaithersburg, 
Maryland USA 
Stolcke, Andreas. 2002. SRILM-an extensible 
language modeling toolkit. In Proceedings of the 
international conference on spoken language 
processing, Volume 2, pages 901-904. 
Wang, Rui and G?nter Neumann. 2007. Recognizing 
Textual Entailment Using Sentence Similarity 
based on Dependency Tree Skeletons. In 
Proceedings of the third PASCAL Recognising 
Textual Entailment Challenge. 
Xuan-Hieu Phan. 2006. CRFChunker: CRF English 
Phrase Chunker , http://crfchunker.sourceforge.net/. 
57
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 201?206,
Baltimore, Maryland USA, June 26?27, 2014.
c?2014 Association for Computational Linguistics
Manawi: Using Multi-Word Expressions and Named Entities to Improve
Machine Translation
Liling Tan and Santanu Pal
Applied Linguistics, Translation and Interpretation Department
Universit?at des Saarlandes
liling.tan@uni-saarland.de
santanu.pal@uni-saarland.de
Abstract
We describe the Manawi
1
(mAnEv) sys-
tem submitted to the 2014 WMT transla-
tion shared task. We participated in the
English-Hindi (EN-HI) and Hindi-English
(HI-EN) language pair and achieved 0.792
for the Translation Error Rate (TER)
score
2
for EN-HI, the lowest among the
competing systems. Our main innova-
tions are (i) the usage of outputs from
NLP tools, viz. billingual multi-word ex-
pression extractor and named-entity rec-
ognizer to improve SMT quality and (ii)
the introduction of a novel filter method
based on sentence-alignment features. The
Manawi system showed the potential of
improving translation quality by incorpo-
rating multiple NLP tools within the MT
pipeline.
1 Introduction
In this paper, we present Saarland University
(USAAR) submission to Workshop for Machine
Translation 2014 (WMT 2014) using the Manawi
MT system. We participated in the generic trans-
lation shared task for the English-Hindi (EN-HI)
and Hindi-English (HI-EN) language pairs.
Our Manawi system showcased the incorpora-
tion of NLP tools output within the MT pipeline; a
bilingual MWE extractor and a bilingual NE rec-
ognizer for English and Hindi were implemented.
The output from these NLP tools was appended to
the training corpus prior to the SMT model train-
ing with the MOSES toolkit (Koehn et al., 2007).
The resulting system achieves the lowest Transla-
tion Error Rate (TER) among competing systems
for the English-Hindi language pair.
1
Multi-word expression And Named-entity And
Wikipedia titles (Manawi)
2
Lower TER often results in better translation
The rest of the paper is structured as follow:
Section 2 describes the implementation of the NLP
tools; Section 3 outlines the corpus pre-processing
before the MT training process; Section 4 de-
scribes the MT system setup; Section 5 describes
a simple post-processing component to handle
Out-Of-Vocabulary words; Section 6 presents the
WMT shared task results for the Manawi system
and Section 6 concludes the paper.
2 NLP Tools Implementation
2.1 Bilingual MWE in MT
Multi-Word Expressions (MWE) are defined as
?idiosyncratic interpretations that cross word
boundaries? (Sag et al., 2002). MWE can be made
up of collocations (e.g. seem ridiculous : behuda
dikhai), frozen expressions (e.g. exception han-
dling : apavada sancalaka) or name entities (e.g.
Johnny Cash : Johni Kesh). Jackendoff (1997)
claims that the frequency of MWE and the fre-
quency of single words in a speaker?s lexicon are
almost equivalent.
Bilingual MWE has shown to be useful for
a variety of NLP applications such as multilin-
gual information retrieval (Vechtomova, 2005)
and Crosslingual/Multilingual Word Sense Dis-
ambiguation (Tan and Bond, 2013; Finlayson and
Kulkarni, 2011). For machine translation, vari-
ous studies had introduced bilingual MWE to im-
prove MT system performance. Lambert (2005)
introduced bilingual MWE by grouping them as
a single token before training alignment models
and they showed that it improved alignment and
translation quality. Ren et al. (2009) integrated
an in-domain bilingual MWE using log likelihood
ratio based hierarchical reducing algorithm and
gained +0.61 BLEU score. Similarly, Santanu et
al. (2010) single tokenized MWE before training a
phrase-based SMT model and achieved 50% im-
provement in BLEU score.
201
In order to improve the word alignment quality,
Venkatapathy and Joshi (2006) reported a discrim-
inative approach to use the compositionality infor-
mation of verb-based multi-word expressions. Pal
et al. (2011) discussed the effects of incorporating
prior alignment of MWE and NEs directly or indi-
rectly into Phrase-based SMT systems.
2.2 Bilingual MWE Extraction
Monolingual MWE extraction revolves around
three approaches (i) rule-based methods relying
on morphosyntactic patterns, (ii) statistical meth-
ods which use association/frequency measures to
determine ngrams as MWE and (iii) hybrid ap-
proaches that combine the rule-based and statis-
tical methods.
However, where bilingual MWE extraction
techniques are concerned, they operate around
two main modus operandi (i) extracting mono-
lingual MWE separately and aligning them at
word/phrasal level afterwards or (ii) aligning par-
allel text at word/phrasal level and then extracting
MWE.
We implemented a language independent bilin-
gual MWE extractor, (Muwee), that produces a
parallel dictionary of MWE without the need for
any word/phrasal-level alignment. Muwee makes
use of the fact that the number of highly collocated
MWE should be the same for each sentences pair.
Muwee first extracts MWE separately from the
source and target sentences; the MWE are ex-
tracted based on bigrams that reports a Point-
wise Mutual Information (PMI) score of above
10. Then for each parallel sentence, if the number
of MWE are equivalent for the source and target,
the bigrams are joint together as a string and con-
tiguous duplicate words are deleted. The removal
of contiguous duplicate words is grounded on the
fact that linguistically motivated MWE that forms
grammatical phrases had shown to improve SMT
performances (Pal et al., 2013). Figure 1 presents
an example of the MWE extraction process.
Figure 1: Muwee Extraction Process
2.3 Named-entity Recognition
Named-Entity (NE) recognition is the task of iden-
tifying entities such as names of people, organi-
zations and locations. Given a perfect MWE ex-
traction system, NEs would have been captured by
MWE extraction. However, the state-of-art MWE
extractors have yet been perfected.
To compliment the MWE extracted by Muwee,
we implemented a bilingual NE extractor by
combining outputs from the (i) Stanford English
NE Recognizer (NER)
3
and (ii) a Do-It-Yourself
(DIY) Hindi NER using CRF++ toolkit
4
with an-
notated data from NER-SSEA 2008 shared task
(Rajeev Sangal and Singh, 2008). We trained a
Conditional Random Field classifier for the Hindi
NER using unigram features, bigram features and
a context window of two words to the left and to
the right. And we used the DIY Hindi NER and
Stanford NER tool to monolingually annotate the
NEs from training corpus for the EN-HI / HI-EN
language pair.
Similar to the Muwee bilingual extraction cri-
teria, if the number of NEs are the same on the
source and target language, the NEs were joint to-
gether as a string. We note that sometimes the
bilingual NER output contains more than one NE
per sentence. For example, our bilingual NER ex-
tractor outputs ?Kalpna Chawla Gurdeep Pand-
her?, which contains two NEs ?Kalpna Chawla?
and ?Gurdeep Pandher?. Although the resulting
bilingual NE does not provide a perfect NE dic-
tionary, it filters out NEs from the sentence and
improves word alignments at the start of the MT
pipeline.
3 Corpus Preprocessing
The performance of any data driven SMT depends
on the quality of training data. Previous stud-
ies had shown that filtering out low quality sen-
tence pairs improves the quality of machine trans-
lation. For instance, the Moore-Lewis filter re-
moves sentence pairs based on source-side cross-
entropy differences (Moore and Lewis, 2010) and
the Edinburgh?s MT system used the Modified
Moore-Lewis filtering (Axelrod et al., 2011) in
WMT 2013 shared task (Durrani et al., 2013).
CNGL-DCU system extended the Moore-Lewis
filter by incorporating lemmas and named enti-
3
http://nlp.stanford.edu/software/CRF-NER.shtml
4
http://crfpp.googlecode.com
202
ties in their definition of perplexity
5
(Rubino et al.,
2013; Toral, 2013).
The RWTH Aachen system filtered the Com-
mon Crawl Corpus by keeping only sentence pairs
that contains at least 70% of the word from a
known vocabulary dataset extracted from the other
corpora in the WMT 2013 shared task (Peitz et
al., 2013). The Docent system from Uppsala Uni-
versity also performed data cleaning on the Com-
mon Crawl dataset prior to SMT but they were
using more aggressive conditions by (i) remov-
ing documents that were identified correctly us-
ing a language identification module and (ii) re-
moving documents that falls below a threshold
value of alignment points and sentence length ra-
tio (Stymne et al., 2013). Our approach to data
cleaning is similar to the Uppsala?s system but in-
stead of capitalizing on word-alignments features,
we were cleaning the data based on sentence align-
ment features.
3.1 GaCha Filtering: Filter by Character
Mean Ratio
Stymne et al. (2013) improved translation qual-
ity by cleaning the Common Crawl corpus during
the WMT 2013 shared task. They filtered out doc-
uments exceeding 60 words and cleaned the re-
mainder of the corpus by exploiting the number
of alignment points in word alignments between
sentence pairs. Their hypothesis was that sentence
pairs with very few alignment points in the inter-
section would mostly likely not be parallel. This
is based on the fact that when using GIZA++ (Och
and Ney, 2003), the intersection of alignments is
more sparse than the standard SMT symmetriza-
tion heuristics like grow-diag-final-and (Koehn,
2005).
Different from Stymne et al., our hypothesis for
non-parallelness adheres to sentence level align-
ment criteria as defined in the Gale-Church algo-
rithm (Gale and Church, 1993). If a sentence pair
is parallel, the ratio of the number of characters in
the source and target sentence should be coherent
to the global ratio of the number of source-target
characters in a fully parallel corpus. The Gale-
Church algorithm had its parameters tuned to suit
European languages and Tan (2013) had demon-
strated that sentence-level alignments can be im-
proved by using corpus specific parameters. When
5
The exponent of cross-entropy may be regarded as per-
plexity
using variable parameters to the Gale-Church al-
gorithm, Tan showed that instead of the default
parameters set in the original Gale-Church algo-
rithm, using mean ratio of the noisy corpus can
also improve sentence level alignments although
the ratio from a clean corpus would achieve even
better alignments.
Given the premises of the sentence level align-
ment hypothesis, we clean the training corpus by
first calculating the global mean ratio of the num-
ber of characters of source sentence to target sen-
tence and then filter out sentence pairs that exceeds
or fall below 20% of the global ratio. We call this
method, GaCha filtering; this cleaning method is
more aggressive than cleaning methods described
by Stymne et al. but it filters out noisy sen-
tence level alignments created by non-language
specific parameters used by sentence aligners such
as Gale-Church algorithm.
3.2 Filtering Noise in HindEnCorp
After manual inspection 100 random sentence
pairs from the HindEnCorp (Bojar et al., 2014),
we found that documents were often misaligned
at sentence level or contains HTML special char-
acters. To further reduce the noise in the Hin-
dEnCorp, the Manawi system was only trained
a subset of the HindEnCorp from the follow-
ing sources (i) DanielPipes, (ii) TIDES and (iii)
EILMT. Lastly, we filtered the training data on al-
lowing a maximum of 100 tokens per language per
sentence.
Finally, the cleaned data contained 87,692 sen-
tences, only ?36% of the original HindEnCorp
training data.
4 System Setup
Data: To train the baseline translation model,
we have used the cleaned subset of the data as
described in Section 3. For the Manawi model,
we added the NLP outputs from the MWE and
NE extractors presented in Section 2. To train the
monolingual language model, we used the Hindi
sentences from the HindEnCorp.
System: We used the standard log-linear
Phrase based SMT model provided from the
MOSES toolkit.
Configuration: We experimented with various
maximum phrase length for the translation and n-
203
Manawi Submissions (EN-HI) BLEU BLEU TER
(cased)
PB-SMT + MWE + NE 9.9 7.1 0.869
PB-SMT + MWE + NE + Wiki (Manawi) 7.7 7.6 0.864
Manawi + GaCha Filter 8.9 8.9 0.818
Manawi + GaCha Filter + Handle OOV 8.8 8.8 0.800
Manawi + GaCha Filter + Remove OOV 8,9 8.8 0.792
Table 1: Manawi System Submissions @ WMT 2014 Translation Shared Task for English-Hindi
Manawi Submissions (HI-EN) BLEU BLEU TER
(cased)
PB-SMT + MWE + NE + Wiki (Manawi) 7.7 7.6 0.864
Manawi + GaCha Filter 8.9 8.9 0.818
Table 2: Manawi System Submissions @ WMT 2014 Translation Shared Task for Hindi-English
gram settings for the language model. And we
found that using a maximum phrase length of 5
and 4-gram language model produced best result
in terms of BLEU and TER for our baseline model
(i.e. without the incorporation of outputs from the
NLP tools). The other experimental settings were:
? GIZA++ implementation of IBM word align-
ment model 4 with grow-diagonal-final-and
heuristics for performing word alignment and
phrase-extraction (Koehn et al., 2003)
? Minimum Error Rate Training (MERT) (Och,
2003) on a held-out development set, target
language model with Kneser-Ney smoothing
(Kneser and Ney, 1995) using language mod-
els trained with SRILM (Stolcke, 2002)
? Reordering model
6
was trained on bidirec-
tional (i.e. using both forward and back-
ward models) and conditioned on both source
and target language. The reordering model
is built by calculating the probabilities of the
phrase pair being associated with the given
orientation.
Innovation: We demonstrated the incorporation
of multiple NLP tools outputs in the SMT pipline
by simply using automatically extracted bilingual
MWE and NEs as additional parallel data to the
cleaned data and ran the translation and statistical
model as per the baseline configurations.
6
For reordering we used lexicalized reordering model,
which consists of three different types of reordering by
conditioning the orientation of previous and next phrases-
monotone (m), swap (s) and discontinuous (d).
5 Post-processing
The MOSES decoder produces translations with
Out-Of-Vocabulary (OOV) words that were not
translated from the source language. The Manawi
system post-processed the decoder output by (i)
handling OOV words by replacing each OOV
word with the most probable translation using the
lexical files generated by GIZA++ and (ii) remov-
ing OOV words from the decoded outputs.
6 Results
Table 1 summarizes the Manawi system sub-
missions for the English-Hindi language pair for
WMT 2014 generic translation shared task. The
basic Manawi system is a Phrase-based SMT
(PB-SMT) setup using extracted MWE and NEs
and Wikipedia titles as additional parallel data (i.e.
PB-SMT+MWE+NE+Wiki in Table 1). The ba-
sic Manawi system achieved 7.7 BLEU score and
0.864 TER.
After filtering the data before training the trans-
lation model, the Manawi system performed bet-
ter at 8.9 BLEU and 0.818 TER. By adding the
post-processing component, we achieved the low-
est TER score among competing team at 0.792.
7 Conclusion
The Manawi system showed how simple yet ef-
fective pre-processing and integration of output
from NLP tools improves the performance of MT
systems. Using GaCha filtering to remove noisy
data and using automatically extracted MWE and
NEs as additional parallel data improve word and
phrasal alignments at the start of the MT pipeline
204
which eventually improves the quality of machine
translation. The best setup for the Manawi system
achieved the best TER score among the competing
system.
Also, the incremental improvements made by
step-wise implementation of (i) filtering, (ii) in-
corporating outputs from NLP tools and (iii) post-
processing showed that individual components of
the Manawi can be integrated into other MT sys-
tems without detrimental effects.
Acknowledgments
The research leading to these results has received
funding from the People Programme (Marie
Curie Actions) of the European Union?s Seventh
Framework Programme FP7/2007-2013/ under
REA grant agreement n
?
317471.
The authors of this paper also thank our col-
leagues J?org Knappen and Jos?e M.M. Mart??nez
for their help in setting up the server that made the
Manawi system possible.
References
Amittai Axelrod, Xiaodong He, and Jianfeng Gao.
2011. Domain adaptation via pseudo in-domain data
selection. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
pages 355?362. Association for Computational Lin-
guistics.
Ond?rej Bojar, Vojt?ech Diatka, Pavel Rychl?y, Pavel
Stra?n?ak, Ale?s Tamchyna, and Dan Zeman. 2014.
Hindi-English and Hindi-only Corpus for Machine
Translation. In Proceedings of the Ninth Interna-
tional Language Resources and Evaluation Confer-
ence (LREC?14), Reykjavik, Iceland, may. ELRA,
European Language Resources Association. in prep.
Nadir Durrani, Barry Haddow, Kenneth Heafield, and
Philipp Koehn. 2013. Edinburghs machine transla-
tion systems for european language pairs. In Pro-
ceedings of the Eighth Workshop on Statistical Ma-
chine Translation, pages 112?119.
Mark Alan Finlayson and Nidhi Kulkarni. 2011. De-
tecting multi-word expressions improves word sense
disambiguation. In Proceedings of the Workshop on
Multiword Expressions: From Parsing and Gener-
ation to the Real World, MWE ?11, pages 20?24,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
William A Gale and Kenneth W Church. 1993. A
program for aligning sentences in bilingual corpora.
Computational linguistics, 19(1):75?102.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, et al. 2007. Moses: Open source
toolkit for statistical machine translation. In Pro-
ceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
pages 177?180. Association for Computational Lin-
guistics.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. MT summit, 5:79?
86.
Patrik Lambert. 2005. Data inferred multi-word ex-
pressions for statistical machine translation. In In
MT Summit X.
Robert C Moore and William Lewis. 2010. Intelligent
selection of language model training data. In Pro-
ceedings of the ACL 2010 Conference Short Papers,
pages 220?224. Association for Computational Lin-
guistics.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational linguistics, 29(1):19?51.
Santanu Pal, Tanmoy Chakraborty, and Sivaji Bandy-
opadhyay. 2011. Handling multiword expressions
in phrase-based statistical machine translation. In In
Proceedings of the 13th Machine Translation Sum-
mit, pages 215?224. MT Summit 2011.
Santanu Pal, Mahammed Hasanuzzaman, Sudip Ku-
mar Naskar, and Sivaji Bandyopadhyay.
2013. Impact of linguistically motivated
shallow phrases in pb-smt. In ICON 2013
http://sivajibandyopadhyay.com/publications/Icon-
v1.3-camera.pdf. ICON 2013.
Stephan Peitz, Jan-Thorsten Peter Saab Mansour,
Christoph Schmidt, Joern Wuebker, Matthias Huck,
Markus Freitag, and Hermann Ney. 2013. The rwth
aachen machine translation system for wmt 2013. In
Proceedings of the Eighth Workshop on Statistical
Machine Translation, pages 191?197.
Dipti Misra Sharma Rajeev Sangal and Anil Kumar
Singh, editors. 2008. Proceedings of the IJCNLP-
08 Workshop on Named Entity Recognition for South
and South East Asian Languages. Asian Federation
of Natural Language Processing, Hyderabad, India,
January.
Zhixiang Ren, Yajuan L?u, Jie Cao, Qun Liu, and Yun
Huang. 2009. Improving statistical machine trans-
lation using domain bilingual multiword expres-
sions. In Proceedings of the Workshop on Multiword
Expressions: Identification, Interpretation, Disam-
biguation and Applications, MWE ?09, pages 47?
54, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
205
Raphael Rubino, Antonio Toral, S Cort?es Va?llo, Jun
Xie, Xiaofeng Wu, Stephen Doherty, and Qun Liu.
2013. The cngl-dcu-prompsit translation systems
for wmt13. In Proceedings of the Eighth Workshop
on Statistical Machine Translation, pages 211?216.
Ivan A Sag, Timothy Baldwin, Francis Bond, Ann
Copestake, and Dan Flickinger. 2002. Multiword
expressions: A pain in the neck for nlp. In Compu-
tational Linguistics and Intelligent Text Processing,
pages 1?15. Springer Berlin Heidelberg.
Pal Santanu, Sudip Kumar Naskar, Pavel Pecina, Sivaji
Bandyopadhyay, and Andy Way. 2010. Handling
named entities and compound verbs in phrase-based
statistical machine translation. In 23rd International
Conference of Computational Linguistics (Coling
2010), Beijing, Chaina, pages 46?54.
Sara Stymne, Christian Hardmeier, J?org Tiedemann,
and Joakim Nivre. 2013. Tunable distortion lim-
its and corpus cleaning for smt. In Proceedings of
the Eighth Workshop on Statistical Machine Trans-
lation, pages 225?231.
Liling Tan and Francis Bond. 2013. Xling: Match-
ing query sentences to a parallel corpus using topic
models for word sense disambiguation.
Liling Tan. 2013. Gachalign: Gale-church sentence-
level alignments with variable parameters [soft-
ware]. Retrieved from https://db.tt/LLrul4zP and
https://code.google.com/p/gachalign/.
Antonio Toral. 2013. Hybrid selection of language
model training data using linguistic information and
perplexity. ACL 2013, page 8.
Olga Vechtomova. 2005. The role of multi-word units
in interactive information retrieval. In ECIR, pages
403?420.
Sriram Venkatapathy and Aravind K Joshi. 2006. Us-
ing information about multi-word expressions for
the word-alignment task. In Proceedings of the
Workshop on Multiword Expressions: Identifying
and Exploiting Underlying Properties, pages 20?27.
Association for Computational Linguistics.
206
