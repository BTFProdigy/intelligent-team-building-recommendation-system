Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 423?431,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Who, What, When, Where, Why?  
Comparing Multiple Approaches to the Cross-Lingual 5W Task 
Kristen Parton*, Kathleen R. McKeown*, Bob Coyne*, Mona T. Diab*,  
Ralph Grishman?, Dilek Hakkani-T?r?, Mary Harper?, Heng Ji?, Wei Yun Ma*,  
Adam Meyers?, Sara Stolbach*, Ang Sun?, Gokhan Tur?, Wei Xu? and Sibel Yaman? 
 
*Columbia University 
New York, NY, USA 
{kristen, kathy, 
coyne, mdiab, ma, 
sara}@cs.columbia.edu 
 
?New York University 
New York, NY, USA 
{grishman, meyers, 
asun, xuwei} 
@cs.nyu.edu 
?International Computer 
Science Institute 
Berkeley, CA, USA 
{dilek, sibel} 
@icsi.berkeley.edu 
 
?Human Lang. Tech. Ctr. of 
Excellence, Johns Hopkins 
and U. of Maryland, 
College Park  
mharper@umd.edu 
?City University of  
New York 
New York, NY, USA 
hengji@cs.qc.cuny.edu 
 
 
?SRI International 
Palo Alto, CA, USA 
gokhan@speech.sri.com 
 
 
  
Abstract 
Cross-lingual tasks are especially difficult 
due to the compounding effect of errors in 
language processing and errors in machine 
translation (MT). In this paper, we present an 
error analysis of a new cross-lingual task: the 
5W task, a sentence-level understanding task 
which seeks to return the English 5W's (Who, 
What, When, Where and Why) corresponding 
to a Chinese sentence. We analyze systems 
that we developed, identifying specific prob-
lems in language processing and MT that 
cause errors. The best cross-lingual 5W sys-
tem was still 19% worse than the best mono-
lingual 5W system, which shows that MT 
significantly degrades sentence-level under-
standing. Neither source-language nor target-
language analysis was able to circumvent 
problems in MT, although each approach had 
advantages relative to the other. A detailed 
error analysis across multiple systems sug-
gests directions for future research on the 
problem. 
1 Introduction 
In our increasingly global world, it is ever more 
likely for a mono-lingual speaker to require in-
formation that is only available in a foreign lan-
guage document. Cross-lingual applications ad-
dress this need by presenting information in the 
speaker?s language even when it originally ap-
peared in some other language, using machine 
translation (MT) in the process. In this paper, we 
present an evaluation and error analysis of a 
cross-lingual application that we developed for a 
government-sponsored evaluation, the 5W task. 
The 5W task seeks to summarize the informa-
tion in a natural language sentence by distilling it 
into the answers to the 5W questions: Who, 
What, When, Where and Why. To solve this 
problem, a number of different problems in NLP 
must be addressed: predicate identification, ar-
gument extraction, attachment disambiguation, 
location and time expression recognition, and 
(partial) semantic role labeling. In this paper, we 
address the cross-lingual 5W task: given a 
source-language sentence, return the 5W?s trans-
lated (comprehensibly) into the target language. 
Success in this task requires a synergy of suc-
cessful MT and answer selection.  
The questions we address in this paper are: 
? How much does machine translation (MT) 
degrade the performance of cross-lingual 
5W systems, as compared to monolingual 
performance? 
? Is it better to do source-language analysis 
and then translate, or do target-language 
analysis on MT? 
? Which specific problems in language 
processing and/or MT cause errors in 5W 
answers?  
In this evaluation, we compare several differ-
ent approaches to the cross-lingual 5W task, two 
that work on the target language (English) and 
one that works in the source language (Chinese). 
423
A central question for many cross-lingual appli-
cations is whether to process in the source lan-
guage and then translate the result, or translate 
documents first and then process the translation. 
Depending on how errorful the translation is, 
results may be more accurate if models are de-
veloped for the source language. However, if 
there are more resources in the target language, 
then the translate-then-process approach may be 
more appropriate. We present a detailed analysis, 
both quantitative and qualitative, of how the ap-
proaches differ in performance.  
We also compare system performance on hu-
man translation (which we term reference trans-
lations) and MT of the same data in order to de-
termine how much MT degrades system per-
formance. Finally, we do an in-depth analysis of 
the errors in our 5W approaches, both on the 
NLP side and the MT side. Our results provide 
explanations for why different approaches suc-
ceed, along with indications of where future ef-
fort should be spent. 
2 Prior Work 
The cross-lingual 5W task is closely related to 
cross-lingual information retrieval and cross-
lingual question answering (Wang and Oard 
2006; Mitamura et al 2008). In these tasks, a 
system is presented a query or question in the 
target language and asked to return documents or 
answers from a corpus in the source language. 
Although MT may be used in solving this task, it 
is only used by the algorithms ? the final evalua-
tion is done in the source language. However, in 
many real-life situations, such as global business, 
international tourism, or intelligence work, users 
may not be able to read the source language. In 
these cases, users must rely on MT to understand 
the system response. (Parton et al 2008) exam-
ine the case of ?translingual? information re-
trieval, where evaluation is done on translated 
results in the target language. In cross-lingual 
information extraction (Sudo et al 2004) the 
evaluation is also done on MT, but the goal is to 
learn knowledge from a large corpus, rather than 
analyzing individual sentences.  
The 5W task is also closely related to Seman-
tic Role Labeling (SRL), which aims to effi-
ciently and effectively derive semantic informa-
tion from text. SRL identifies predicates and 
their arguments in a sentence, and assigns roles 
to each argument. For example, in the sentence 
?I baked a cake yesterday.?, the predicate 
?baked? has three arguments. ?I? is the subject of 
the predicate, ?a cake? is the object and ?yester-
day? is a temporal argument.  
Since the release of large data resources anno-
tated with relevant levels of semantic informa-
tion, such as the FrameNet (Baker et al, 1998) 
and PropBank corpora (Kingsbury and Palmer, 
2003), efficient approaches to SRL have been 
developed (Carreras and Marquez, 2005). Most 
approaches to the problem of SRL follow the 
Gildea and Jurafsky (2002) model. First, for a 
given predicate, the SRL system identifies its 
arguments' boundaries. Second, the Argument 
types are classified depending on an adopted 
lexical resource such as PropBank or FrameNet. 
Both steps are based on supervised learning over 
labeled gold standard data. A final step uses heu-
ristics to resolve inconsistencies when applying 
both steps simultaneously to the test data.  
Since many of the SRL resources are English, 
most of the SRL systems to date have been for 
English. There has been work in other languages 
such as German and Chinese (Erk 2006; Sun 
2004; Xue and Palmer 2005). The systems for 
the other languages follow the successful models 
devised for English, e.g. (Gildea and Palmer, 
2002; Chen and Rambow, 2003; Moschitti, 2004; 
Xue and Palmer, 2004; Haghighi et al, 2005). 
3 The Chinese-English 5W Task 
3.1 5W Task Description 
We participated in the 5W task as part of the 
DARPA GALE (Global Autonomous Language 
Exploitation) project. The goal is to identify the 
5W?s (Who, What, When, Where and Why) for a 
complete sentence. The motivation for the 5W 
task is that, as their origin in journalism suggests, 
the 5W?s cover the key information nuggets in a 
sentence. If a system can isolate these pieces of 
information successfully, then it can produce a 
pr?cis of the basic meaning of the sentence. Note 
that this task differs from QA tasks, where 
?Who? and ?What? usually refer to definition 
type questions. In this task, the 5W?s refer to se-
mantic roles within a sentence, as defined in Ta-
ble 1.  
In order to get al 5W?s for a sentence correct, 
a system must identify a top-level predicate, ex-
tract the correct arguments, and resolve attach-
ment ambiguity. In the case of multiple top-level 
predicates, any of the top-level predicates may be 
chosen. In the case of passive verbs, the Who is 
the agent (often expressed as a ?by clause?, or 
not stated), and the What should include the syn-
tactic subject.  
424
Answers are judged Correct1 if they identify a 
correct null argument or correctly extract an ar-
gument that is present in the sentence. Answers 
are not penalized for including extra text, such as 
prepositional phrases or subordinate clauses, 
unless the extra text includes text from another 
answer or text from another top-level predicate. 
In sentence 2a in Table 2, returning ?bought and 
cooked? for the What would be Incorrect. Simi-
larly, returning ?bought the fish at the market? 
for the What would also be Incorrect, since it 
contains the Where. Answers may also be judged 
Partial, meaning that only part of the answer was 
returned. For example, if the What contains the 
predicate but not the logical object, it is Partial.  
Since each sentence may have multiple correct 
sets of 5W?s, it is not straightforward to produce 
a gold-standard corpus for automatic evaluation. 
One would have to specify answers for each pos-
sible top-level predicate, as well as which parts 
of the sentence are optional and which are not 
allowed. This also makes creating training data 
for system development problematic. For exam-
ple, in Table 2, the sentence in 2a and 2b is the 
same, but there are two possible sets of correct 
answers. Since we could not rely on a gold-
standard corpus, we used manual annotation to 
judge our 5W system, described in section 5. 
3.2 The Cross-Lingual 5W Task 
In the cross-lingual 5W task, a system is given a 
sentence in the source language and asked to 
produce the 5W?s in the target language. In this 
task, both machine translation (MT) and 5W ex-
traction must succeed in order to produce correct 
answers. One motivation behind the cross-lingual 
5W task is MT evaluation. Unlike word- or 
phrase-overlap measures such as BLEU, the 5W 
evaluation takes into account ?concept? or ?nug-
get? translation. Of course, only the top-level 
predicate and arguments are evaluated, so it is 
not a complete evaluation. But it seeks to get at 
the understandability of the MT output, rather 
than just n-gram overlap. 
Translation exacerbates the problem of auto-
matically evaluating 5W systems. Since transla-
tion introduces paraphrase, rewording and sen-
tence restructuring, the 5W?s may change from 
one translation of a sentence to another transla-
tion of the same sentence. In some cases, roles 
may swap. For example, in Table 2, sentences 1a 
and 1b could be valid translations of the same 
                                                 
1
 The specific guidelines for determining correctness 
were formulated by BAE.  
Chinese sentence. They contain the same infor-
mation, but the 5W answers are different. Also, 
translations may produce answers that are textu-
ally similar to correct answers, but actually differ 
in meaning. These differences complicate proc-
essing in the source followed by translation. 
 
Example: On Tuesday, President Obama met with 
French President Sarkozy in Paris to discuss the 
economic crisis. 
W Definition Example  
answer 
WHO Logical subject of the 
top-level predicate in 
WHAT, or null. 
President 
Obama 
WHAT One of the top-level 
predicates in the sen-
tence, and the predi-
cate?s logical object. 
met with 
French Presi-
dent Sarkozy 
WHEN ARGM-TMP of the 
top-level predicate in 
WHAT, or null. 
On Tuesday 
WHERE ARGM-LOC of the 
top-level predicate in 
WHAT, or null. 
in Paris 
WHY ARGM-CAU of the 
top-level predicate in 
WHAT, or null. 
to discuss the 
economic crisis 
Table 1. Definition of the 5W task, and 5W answers 
from the example sentence above. 
4 5W System 
We developed a 5W combination system that 
was based on five other 5W systems. We se-
lected four of these different systems for evalua-
tion: the final combined system (which was our 
submission for the official evaluation), two sys-
tems that did analysis in the target-language 
(English), and one system that did analysis in the 
source language (Chinese). In this section, we 
describe the individual systems that we evalu-
ated, the combination strategy, the parsers that 
we tuned for the task, and the MT systems.  
 Sentence WHO WHAT 
1a Mary bought a cake 
from Peter. 
Mary bought a 
cake 
1b Peter sold Mary a 
cake. 
Peter sold Mary 
2a I bought the fish at 
the market yesterday 
and cooked it today. 
I bought the 
fish 
[WHEN: 
yesterday] 
2b I bought the fish at 
the market yesterday 
and cooked it today. 
I cooked it 
[WHEN: 
today] 
Table 2. Example 5W answers. 
425
4.1 Latent Annotation Parser 
For this work, we have re-implemented and en-
hanced the Berkeley parser (Petrov and Klein 
2007) in several ways: (1) developed a new 
method to handle rare words in English and Chi-
nese; (2) developed a new model of unknown 
Chinese words based on characters in the word; 
(3) increased robustness by adding adaptive 
modification of pruning thresholds and smooth-
ing of word emission probabilities. While the 
enhancements to the parser are important for ro-
bustness and accuracy, it is even more important 
to train grammars matched to the conditions of 
use. For example, parsing a Chinese sentence 
containing full-width punctuation with a parser 
trained on half-width punctuation reduces accu-
racy by over 9% absolute F. In English, parsing 
accuracy is seriously compromised by training a 
grammar with punctuation and case to process 
sentences without them.  
We developed grammars for English and Chi-
nese trained specifically for each genre by sub-
sampling from available treebanks (for English, 
WSJ, BN, Brown, Fisher, and Switchboard; for 
Chinese, CTB5) and transforming them for a 
particular genre (e.g., for informal speech, we 
replaced symbolic expressions with verbal forms 
and remove punctuation and case) and by utiliz-
ing a large amount of genre-matched self-labeled 
training parses. Given these genre-specific 
parses, we extracted chunks and POS tags by 
script. We also trained grammars with a subset of 
function tags annotated in the treebank that indi-
cate case role information (e.g., SBJ, OBJ, LOC, 
MNR) in order to produce function tags.   
4.2 Individual 5W Systems 
The English systems were developed for the 
monolingual 5W task and not modified to handle 
MT. They used hand-crafted rules on the output 
of the latent annotation parser to extract the 5Ws.  
English-function used the function tags from 
the parser to map parser constituents to the 5Ws. 
First the Who, When, Where and Why were ex-
tracted, and then the remaining pieces of the sen-
tence were returned as the What. The goal was to 
make sure to return a complete What answer and 
avoid missing the object. 
English-LF, on the other hand, used a system 
developed over a period of eight years (Meyers 
et al 2001) to map from the parser?s syntactic 
constituents into logical grammatical relations 
(GLARF), and then extracted the 5Ws from the 
logical form. As a back-up, it also extracted 
GLARF relations from another English-treebank 
trained parser, the Charniak parser (Charniak 
2001). After the parses were both converted to 
the 5Ws, they were then merged, favoring the 
system that: recognized the passive, filled more 
5W slots or produced shorter 5W slots (provid-
ing that the WHAT slot consisted of more than 
just the verb). A third back-up method extracted 
5Ws from part-of-speech tag patterns. Unlike 
English-function, English-LF explicitly tried to 
extract the shortest What possible, provided there 
was a verb and a possible object, in order to 
avoid multiple predicates or other 5W answers.  
Chinese-align uses the latent annotation 
parser (trained for Chinese) to parse the Chinese 
sentences. A dependency tree converter (Johans-
son and Nuges 2007) was applied to the constitu-
ent-based parse trees to obtain the dependency 
relations and determine top-level predicates. A 
set of hand-crafted dependency rules based on 
observation of Chinese OntoNotes were used to 
map from the Chinese function tags into Chinese 
5Ws.  Finally, Chinese-align used the alignments 
of three separate MT systems to translate the 
5Ws: a phrase-based system, a hierarchical 
phrase-based system, and a syntax augmented 
hierarchical phrase-based system. Chinese-align 
faced a number of problems in using the align-
ments, including the fact that the best MT did not 
always have the best alignment. Since the predi-
cate is essential, it tried to detect when verbs 
were deleted in MT, and back-off to a different 
MT system. It also used strategies for finding 
and correcting noisy alignments, and for filtering 
When/Where answers from Who and What.  
4.3 Hybrid System 
A merging algorithm was learned based on a de-
velopment test set. The algorithm selected all 
5W?s from a single system, rather than trying to 
merge W?s from different systems, since the 
predicates may vary across systems. For each 
document genre (described in section 5.4), we 
ranked the systems by performance on the devel-
opment data. We also experimented with a vari-
ety of features (for instance, does ?What? include 
a verb). The best-performing features were used 
in combination with the ranked list of priority 
systems to create a rule-based merger. 
4.4 MT Systems 
The MT Combination system used by both of the 
English 5W systems combined up to nine sepa-
rate MT systems. System weights for combina-
tion were optimized together with the language 
426
model score and word penalty for a combination 
of BLEU and TER (2*(1-BLEU) + TER). Res-
coring was applied after system combination us-
ing large language models and lexical trigger 
models. Of the nine systems, six were phrased-
based systems (one of these used chunk-level 
reordering of the Chinese, one used word sense 
disambiguation, and one used unsupervised Chi-
nese word segmentation), two were hierarchical 
phrase-based systems, one was a string-to-
dependency system, one was syntax-augmented, 
and one was a combination of two other systems. 
Bleu scores on the government supplied test set 
in December 2008 were 35.2 for formal text, 
29.2 for informal text, 33.2 for formal speech, 
and 27.6 for informal speech. More details may 
be found in (Matusov et al 2009). 
5 Methods 
5.1 5W Systems 
For the purposes of this evaluation2, we com-
pared the output of 4 systems: English-Function, 
English-LF, Chinese-align, and the combined 
system. Each English system was also run on 
reference translations of the Chinese sentence. 
So for each sentence in the evaluation corpus, 
there were 6 systems that each provided 5Ws. 
5.2 5W Answer Annotation 
For each 5W output, annotators were presented 
with the reference translation, the MT version, 
and the 5W answers. The 5W system names 
were hidden from the annotators. Annotators had 
to select ?Correct?, ?Partial? or ?Incorrect? for 
each W. For answers that were Partial or Incor-
rect, annotators had to further specify the source 
of the error based on several categories (de-
scribed in section 6). All three annotators were 
native English speakers who were not system 
developers for any of the 5W systems that were 
being evaluated (to avoid biased grading, or as-
signing more blame to the MT system). None of 
the annotators knew Chinese, so all of the judg-
ments were based on the reference translations. 
After one round of annotation, we measured 
inter-annotator agreement on the Correct, Partial, 
or Incorrect judgment only. The kappa value was 
0.42, which was lower than we expected. An-
other surprise was that the agreement was lower 
                                                 
2
 Note that an official evaluation was also performed by 
DARPA and BAE. This evaluation provides more fine-
grained detail on error types and gives results for the differ-
ent approaches. 
for When, Where and Why (?=0.31) than for 
Who or What (?=0.48). We found that, in cases 
where a system would get both Who and What 
wrong, it was often ambiguous how the remain-
ing W?s should be graded. Consider the sentence: 
?He went to the store yesterday and cooked lasa-
gna today.? A system might return erroneous 
Who and What answers, and return Where as ?to 
the store? and When as ?today.? Since Where 
and When apply to different predicates, they 
cannot both be correct. In order to be consistent, 
if a system returned erroneous Who and What 
answers, we decided to mark the When, Where 
and Why answers Incorrect by default. We added 
clarifications to the guidelines and discussed ar-
eas of confusion, and then the annotators re-
viewed and updated their judgments.  
After this round of annotating, ?=0.83 on the 
Correct, Partial, Incorrect judgments. The re-
maining disagreements were genuinely ambigu-
ous cases, where a sentence could be interpreted 
multiple ways, or the MT could be understood in 
various ways. There was higher agreement on 
5W?s answers from the reference text compared 
to MT text, since MT is inherently harder to 
judge and some annotators were more flexible 
than others in grading garbled MT. 
5.3 5W Error Annotation 
In addition to judging the system answers by the 
task guidelines, annotators were asked to provide 
reason(s) an answer was wrong by selecting from 
a list of predefined errors. Annotators were asked 
to use their best judgment to ?assign blame? to 
the 5W system, the MT, or both. There were six 
types of system errors and four types of MT er-
rors, and the annotator could select any number 
of errors. (Errors are described further in section 
6.) For instance, if the translation was correct, 
but the 5W system still failed, the blame would 
be assigned to the system. If the 5W system 
picked an incorrectly translated argument (e.g., 
?baked a moon? instead of ?baked a cake?), then 
the error would be assigned to the MT system. 
Annotators could also assign blame to both sys-
tems, to indicate that they both made mistakes.  
Since this annotation task was a 10-way selec-
tion, with multiple selections possible, there were 
some disagreements. However, if categorized 
broadly into 5W System errors only, MT errors 
only, and both 5W System and MT errors, then 
the annotators had a substantial level of agree-
ment (?=0.75 for error type, on sentences where 
both annotators indicated an error).  
427
5.4 5 W Corpus 
The full evaluation corpus is 350 documents, 
roughly evenly divided between four genres: 
formal text (newswire), informal text (blogs and 
newsgroups), formal speech (broadcast news) 
and informal speech (broadcast conversation). 
For this analysis, we randomly sampled docu-
ments to judge from each of the genres. There 
were 50 documents (249 sentences) that were 
judged by a single annotator. A subset of that set, 
with 22 documents and 103 sentences, was 
judged by two annotators. In comparing the re-
sults from one annotator to the results from both 
annotators, we found substantial agreement. 
Therefore, we present results from the single an-
notator so we can do a more in-depth analysis. 
Since each sentence had 5W?s, and there were 6 
systems that were compared, there were 7,500 
single-annotator judgments over 249 sentences. 
6 Results 
Figure 1 shows the cross-lingual performance 
(on MT) of all the systems for each 5W. The best 
monolingual performance (on human transla-
tions) is shown as a dashed line (% Correct 
only). If a system returned Incorrect answers for 
Who and What, then the other answers were 
marked Incorrect (as explained in section 5.2). 
For the last 3W?s, the majority of errors were due 
to this (details in Figure 1), so our error analysis 
focuses on the Who and What questions. 
6.1 Monolingual 5W Performance 
To establish a monolingual baseline, the Eng-
lish 5W system was run on reference (human) 
translations of the Chinese text. For each partial 
or incorrect answer, annotators could select one 
or more of these reasons: 
? Wrong predicate or multiple predicates. 
? Answer contained another 5W answer. 
? Passive handled wrong (WHO/WHAT). 
? Answer missed. 
? Argument attached to wrong predicate. 
Figure 1 shows the performance of the best 
monolingual system for each 5W as a dashed 
line. The What question was the hardest, since it 
requires two pieces of information (the predicate 
and object). The When, Where and Why ques-
tions were easier, since they were null most of 
the time. (In English OntoNotes 2.0, 38% of sen-
tences have a When, 15% of sentences have a 
Where, and only 2.6% of sentences have a Why.) 
The most common monolingual system error on 
these three questions was a missed answer, ac-
counting for all of the Where errors, all but one 
Why error and 71% of the When errors. The re-
maining When errors usually occurred when the 
system assumed the wrong sense for adverbs 
(such as ?then? or ?just?). 
 Missing Other 
5W 
Wrong/Multiple 
Predicates 
Wrong 
REF-func 37 29 22 7 
REF-LF 54 20 17 13 
MT-func 18 18 18 8 
MT-LF 26 19 10 11 
Chinese 23 17 14 8 
Hybrid 13 17 15 12 
Table 3. Percentages of Who/What errors attributed to 
each system error type. 
The top half of Table 3 shows the reasons at-
tributed to the Who/What errors for the reference 
corpus. Since English-LF preferred shorter an-
swers, it frequently missed answers or parts of 
Figure 1. System performance on each 5W. ?Partial? indicates that part of the answer was missing. Dashed lines 
show the performance of the best monolingual system (% Correct on human translations). For the last 3W?s, the 
percent of answers that were Incorrect ?by default? were: 30%, 24%, 27% and 22%, respectively, and 8% for the 
best monolingual system 
60 60 56 66
36 40 38 42
56 59 59 64 63
70 66 73 68 75 71 78
19201914
0
10
20
30
40
50
60
70
80
90
100
En
g-
fu
nc
En
g-
LF
Ch
in
es
e
Hy
br
id
En
g-
fu
nc
En
g-
LF
Ch
in
es
e
Hy
br
id
En
g-
fu
nc
En
g-
LF
Ch
in
es
e
Hy
br
id
En
g-
fu
nc
En
g-
LF
Ch
in
es
e
Hy
br
id
En
g-
fu
nc
En
g-
LF
Ch
in
es
e
Hy
br
id
WHO WHAT WHEN WHERE WHY
Partia l
Correct
90
75 81
83 90
Best 
mono-
lingual
428
answers. English-LF also had more Partial an-
swers on the What question: 66% Correct and 
12% Partial, versus 75% Correct and 1% Partial 
for English-function. On the other hand, English-
function was more likely to return answers that 
contained incorrect extra information, such as 
another 5W or a second predicate. 
6.2 Effect of MT on 5W Performance 
The cross-lingual 5W task requires that systems 
return intelligible responses that are semantically 
equivalent to the source sentence (or, in the case 
of this evaluation, equivalent to the reference).  
As can be seen in Figure 1, MT degrades the 
performance of the 5W systems significantly, for 
all question types, and for all systems. Averaged 
over all questions, the best monolingual system 
does 19% better than the best cross-lingual sys-
tem. Surprisingly, even though English-function 
outperformed English-LF on the reference data, 
English-LF does consistently better on MT. This 
is likely due to its use of multiple back-off meth-
ods when the parser failed.  
6.3 Source-Language vs. Target-Language 
The Chinese system did slightly worse than ei-
ther English system overall, but in the formal 
text genre, it outperformed both English systems.  
Although the accuracies for the Chinese and 
English systems are similar, the answers vary a 
lot. Nearly half (48%) of the answers can be an-
swered correctly by both the English system and 
the Chinese system. But 22% of the time, the 
English system returned the correct answer when 
the Chinese system did not. Conversely, 10% of 
the answers were returned correctly by the Chi-
nese system and not the English systems. The 
hybrid system described in section 4.2 attempts 
to exploit these complementary advantages. 
After running the hybrid system, 61% of the 
answers were from English-LF, 25% from Eng-
lish-function, 7% from Chinese-align, and the 
remaining 7% were from the other Chinese 
methods (not evaluated here). The hybrid did 
better than its parent systems on all 5Ws, and the 
numbers above indicate that further improvement 
is possible with a better combination strategy.  
6.4 Cross-Lingual 5W Error Analysis 
For each Partial or Incorrect answer, annotators 
were asked to select system errors, translation 
errors, or both. (Further analysis is necessary to 
distinguish between ASR errors and MT errors.) 
The translation errors considered were: 
? Word/phrase deleted. 
? Word/phrase mistranslated. 
? Word order mixed up. 
? MT unreadable. 
Table 4 shows the translation reasons attrib-
uted to the Who/What errors. For all systems, the 
errors were almost evenly divided between sys-
tem-only, MT-only and both, although the Chi-
nese system had a higher percentage of system-
only errors. The hybrid system was able to over-
come many system errors (for example, in Table 
2, only 13% of the errors are due to missing an-
swers), but still suffered from MT errors. 
Table 4. Percentages of Who/What errors by each 
system attributed to each translation error type. 
Mistranslation was the biggest translation 
problem for all the systems. Consider the first 
example in Figure 3. Both English systems cor-
rectly extracted the Who and the When, but for 
Mistrans-
lation 
Deletion Word 
Order 
Unreadable 
MT-func 34 18 24 18 
MT-LF 29 22 21 14 
Chinese 32 17 9 13 
Hybrid 35 19 27 18 
MT: After several rounds of reminded, I was a little bit 
Ref: After several hints, it began to come back to me. 
 ??????,?????????? 
MT: The Guizhou province, within a certain bank robber, under the watchful eyes of a weak woman, and, with a 
knife stabbed the woman. 
Ref: I saw that in a bank in Guizhou Province, robbers seized a vulnerable young woman in front of a group of 
onlookers and stabbed the woman with a knife. 
 ?????????,?????????,???????,??,???????? 
MT: Woke up after it was discovered that the property is not more than eleven people do not even said that the 
memory of the receipt of the country into the country. 
Ref: Well, after waking up, he found everything was completely changed. Apart from having additional eleven 
grandchildren, even the motherland as he recalled has changed from a socialist country to a capitalist country. 
 ?????????????,?????????,????????????????????????? 
Figure 3 Example sentences that presented problems for the 5W systems. 
 
429
What they returned ?was a little bit.? This is the 
correct predicate for the sentence, but it does not 
match the meaning of the reference. The Chinese 
5W system was able to select a better translation, 
and instead returned ?remember a little bit.? 
Garbled word order was chosen for 21-24% of 
the target-language system Who/What errors, but 
only 9% of the source-language system 
Who/What errors. The source-language word 
order problems tended to be local, within-phrase 
errors (e.g., ?the dispute over frozen funds? was 
translated as ?the freezing of disputes?). The tar-
get-language system word order problems were 
often long-distance problems. For example, the 
second sentence in Figure 3 has many phrases in 
common with the reference translation, but the 
overall sentence makes no sense. The watchful 
eyes actually belong to a ?group of onlookers? 
(deleted). Ideally, the robber would have 
?stabbed the woman? ?with a knife,? rather than 
vice versa. Long-distance phrase movement is a 
common problem in Chinese-English MT, and 
many MT systems try to handle it (e.g., Wang et 
al. 2007). By doing analysis in the source lan-
guage, the Chinese 5W system is often able to 
avoid this problem ? for example, it successfully 
returned ?robbers? ?grabbed a weak woman? for 
the Who/What of this sentence. 
Although we expected that the Chinese system 
would have fewer problems with MT deletion, 
since it could choose from three different MT 
versions, MT deletion was a problem for all sys-
tems. In looking more closely at the deletions, 
we noticed that over half of deletions were verbs 
that were completely missing from the translated 
sentence. Since MT systems are tuned for word-
based overlap measures (such as BLEU), verb 
deletion is penalized equally as, for example, 
determiner deletion. Intuitively, a verb deletion 
destroys the central meaning of a sentence, while 
a determiner is rarely necessary for comprehen-
sion. Other kinds of deletions included noun 
phrases, pronouns, named entities, negations and 
longer connecting phrases.  
Deletion also affected When and Where. De-
leting particles such as ?in? and ?when? that in-
dicate a location or temporal argument caused 
the English systems to miss the argument. Word 
order problems in MT also caused attachment 
ambiguity in When and Where. 
The ?unreadable? category was an option of 
last resort for very difficult MT sentences. The 
third sentence in Figure 3 is an example where 
ASR and MT errors compounded to create an 
unparseable sentence.  
7 Conclusions 
In our evaluation of various 5W systems, we dis-
covered several characteristics of the task. The 
What answer was the hardest for all systems, 
since it is difficult to include enough information 
to cover the top-level predicate and object, with-
out getting penalized for including too much. 
The challenge in the When, Where and Why 
questions is due to sparsity ? these responses 
occur in much fewer sentences than Who and 
What, so systems most often missed these an-
swers. Since this was a new task, this first 
evaluation showed clear issues on the language 
analysis side that can be improved in the future. 
The best cross-lingual 5W system was still 
19% worse than the best monolingual 5W sys-
tem, which shows that MT significantly degrades 
sentence-level understanding. A serious problem 
in MT for systems was deletion. Chinese con-
stituents that were never translated caused seri-
ous problems, even when individual systems had 
strategies to recover. When the verb was deleted, 
no top level predicate could be found and then all 
5Ws were wrong.  
One of our main research questions was 
whether to extract or translate first. We hypothe-
sized that doing source-language analysis would 
be more accurate, given the noise in Chinese 
MT, but the systems performed about the same. 
This is probably because the English tools (logi-
cal form extraction and parser) were more ma-
ture and accurate than the Chinese tools.  
Although neither source-language nor target-
language analysis was able to circumvent prob-
lems in MT, each approach had advantages rela-
tive to the other, since they did well on different 
sets of sentences. For example, Chinese-align 
had fewer problems with word order, and most 
of those were due to local word-order problems.  
Since the source-language and target-language 
systems made different kinds of mistakes, we 
were able to build a hybrid system that used the 
relative advantages of each system to outperform 
all systems. The different types of mistakes made 
by each system suggest features that can be used 
to improve the combination system in the future. 
Acknowledgments 
This work was supported in part by the Defense 
Advanced Research Projects Agency (DARPA) 
under contract number HR0011-06-C-0023. Any 
opinions, findings and conclusions or recom-
mendations expressed in this material are the 
authors' and do not necessarily reflect those of 
the sponsors. 
430
References  
Collin F. Baker, Charles J. Fillmore, and John B. 
Lowe. 1998. The Berkeley FrameNet project. In 
COLING-ACL '98: Proceedings of the Conference, 
held at the University of Montr?al, pages 86?90. 
Xavier Carreras and Llu?s M?rquez. 2005. Introduc-
tion to the CoNLL-2005 shared task: Semantic role 
labeling. In Proceedings of the Ninth Conference 
on Computational Natural Language Learning 
(CoNLL-2005), pages 152?164. 
Eugene Charniak. 2001. Immediate-head parsing for 
language models. In Proceedings of the 39th An-
nual Meeting on Association For Computational 
Linguistics (Toulouse, France, July 06 - 11, 2001).   
John Chen and Owen Rambow. 2003. Use of deep 
linguistic features for the recognition and labeling 
of semantic arguments. In Proceedings of the 2003 
Conference on Empirical Methods in Natural Lan-
guage Processing, Sapporo, Japan. 
Katrin Erk and Sebastian Pado. 2006. Shalmaneser ? 
a toolchain for shallow semantic parsing. Proceed-
ings of LREC. 
Daniel Gildea and Daniel Jurafsky. 2002. Automatic 
labeling of semantic roles. Computational Linguis-
tics, 28(3):245?288. 
Daniel Gildea and Martha Palmer. 2002. The neces-
sity of parsing for predicate argument recognition. 
In Proceedings of the 40th Annual Conference of 
the Association for Computational Linguistics 
(ACL-02), Philadelphia, PA, USA. 
Mary Harper and Zhongqiang Huang. 2009. Chinese 
Statistical Parsing, chapter to appear. 
Aria Haghighi, Kristina Toutanova, and Christopher 
Manning. 2005. A joint model for semantic role la-
beling. In Proceedings of the Ninth Conference on 
Computational Natural Language Learning 
(CoNLL-2005), pages 173?176.  
Paul Kingsbury and Martha Palmer. 2003. Propbank: 
the next level of treebank. In Proceedings of Tree-
banks and Lexical Theories. 
Evgeny Matusov, Gregor Leusch, & Hermann Ney: 
Learning to combine machine translation systems.  
In: Cyril Goutte, Nicola Cancedda, Marc Dymet-
man, & George Foster (eds.) Learning machine 
translation. (Cambridge, Mass.: The MIT Press, 
2009); pp.257-276. 
Adam Meyers, Ralph Grishman, Michiko Kosaka and 
Shubin Zhao.  2001. Covering Treebanks with 
GLARF. In Proceedings of the ACL 2001 Work-
shop on Sharing Tools and Resources. Annual 
Meeting of the ACL. Association for Computa-
tional Linguistics, Morristown, NJ, 51-58. 
Teruko Mitamura, Eric Nyberg, Hideki Shima, 
Tsuneaki Kato, Tatsunori Mori, Chin-Yew Lin, 
Ruihua Song, Chuan-Jie Lin, Tetsuya Sakai, 
Donghong Ji, and Noriko Kando. 2008. Overview 
of the NTCIR-7 ACLIA Tasks: Advanced Cross-
Lingual Information Access. In Proceedings of the 
Seventh NTCIR Workshop Meeting. 
Alessandro Moschitti, Silvia Quarteroni, Roberto 
Basili, and Suresh Manandhar. 2007. Exploiting 
syntactic and shallow semantic kernels for question 
answer classification. In Proceedings of the 45th 
Annual Meeting of the Association of Computa-
tional Linguistics, pages 776?783.  
Kristen Parton, Kathleen R. McKeown, James Allan, 
and Enrique Henestroza. Simultaneous multilingual 
search for translingual information retrieval. In 
Proceedings of ACM 17th Conference on Informa-
tion and Knowledge Management (CIKM), Napa 
Valley, CA, 2008. 
Slav Petrov and Dan Klein. 2007. Improved Inference 
for Unlexicalized Parsing. North American Chapter 
of the Association for Computational Linguistics 
(HLT-NAACL 2007). 
Sudo, K., Sekine, S., and Grishman, R. 2004. Cross-
lingual information extraction system evaluation. 
In Proceedings of the 20th international Confer-
ence on Computational Linguistics. 
Honglin Sun and Daniel Jurafsky. 2004. Shallow Se-
mantic Parsing of Chinese. In Proceedings of 
NAACL-HLT. 
Cynthia A. Thompson, Roger Levy, and Christopher 
Manning. 2003. A generative model for semantic 
role labeling. In 14th European Conference on Ma-
chine Learning. 
Nianwen Xue and Martha Palmer. 2004. Calibrating 
features for semantic role labeling. In Dekang Lin 
and Dekai Wu, editors, Proceedings of EMNLP 
2004, pages 88?94, Barcelona, Spain, July. Asso-
ciation for Computational Linguistics. 
Xue, Nianwen and Martha Palmer. 2005. Automatic 
semantic role labeling for Chinese verbs. InPro-
ceedings of the Nineteenth International Joint Con-
ference on Artificial Intelligence, pages 1160-1165.  
Chao Wang, Michael Collins, and Philipp Koehn. 
2007. Chinese Syntactic Reordering for Statistical 
Machine Translation. Proceedings of the 2007 Joint 
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), 737-745. 
Jianqiang Wang and Douglas W. Oard, 2006. "Com-
bining Bidirectional Translation and Synonymy for 
Cross-Language Information Retrieval," in 29th 
Annual International ACM SIGIR Conference on 
Research and Development in Information Re-
trieval, pp. 202-209. 
431
Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 49?54,
Baltimore, Maryland USA, June 23-24, 2014.
c?2014 Association for Computational Linguistics
WELT: Using Graphics Generation in Linguistic Fieldwork
Morgan Ulinski
?
mulinski@cs.columbia.edu
Anusha Balakrishnan
?
ab3596@columbia.edu
Daniel Bauer
?
bauer@cs.columbia.edu
Bob Coyne
?
coyne@cs.columbia.edu
Julia Hirschberg
?
julia@cs.columbia.edu
Owen Rambow
?
rambow@ccls.columbia.edu
?
Department of Computer Science
?
CCLS
Columbia University
New York, NY, USA
Abstract
We describe the WordsEye Linguistics
tool (WELT), a novel tool for the docu-
mentation and preservation of endangered
languages. WELT is based on Words-
Eye (Coyne and Sproat, 2001), a text-to-
scene tool that automatically generates 3D
scenes from written input. WELT has two
modes of operation. In the first mode, En-
glish input automatically generates a pic-
ture which can be used to elicit a de-
scription in the target language. In the
second mode, the linguist formally docu-
ments the grammar of an endangered lan-
guage, thereby creating a system that takes
input in the endangered language and gen-
erates a picture according to the grammar;
the picture can then be used to verify the
grammar with native speakers. We will
demonstrate WELT?s use on scenarios in-
volving Arrernte and Nahuatl.
1 Introduction
Although languages have appeared and disap-
peared throughout history, today languages are
facing extinction at an unprecedented pace. Over
40% of the estimated 7,000 languages in the world
are at risk of disappearing. When languages die
out, we lose access to an invaluable resource for
studying the culture, history, and experience of
peoples around the world (Alliance for Linguistic
Diversity, 2013). Efforts to document languages
and develop tools in support of collecting data on
them become even more important with the in-
creasing rate of extinction. Bird (2009) empha-
sizes a particular need to make use of computa-
tional linguistics during fieldwork.
To address this issue, we are developing the
WordsEye Linguistics Tool, or WELT. In the first
mode of operation, we provide a field linguist with
tools for running custom elicitation sessions based
on a collection of 3D scenes. In the second, input
in an endangered language generates a picture rep-
resenting the input?s meaning according to a for-
mal grammar.
WELT provides important advantages for elic-
itation over the pre-fabricated sets of static pic-
tures commonly used by field linguists today. The
field worker is not limited to a fixed set of pictures
but can, instead, create and modify scenes in real
time, based on the informants? answers. This al-
lows them to create additional follow-up scenes
and questions on the fly. In addition, since the
pictures are 3D scenes, the viewpoint can easily
be changed, allowing exploration of linguistic de-
scriptions based on different frames of reference.
This will be particularly useful in eliciting spatial
descriptions. Finally, since scenes and objects can
easily be added in the field, the linguist can cus-
tomize the images used for elicitation to be maxi-
mally relevant to the current informants.
WELT also provides a means to document the
semantics of a language in a formal way. Lin-
guists can customize their studies to be as deep or
shallow as they wish; however, we believe that a
major advantage of documenting a language with
WELT is that it enables such studies to be much
more precise. The fully functioning text-to-scene
system created as a result of this documentation
will let linguists easily test the theories they de-
velop with native speakers, making changes to
grammars and semantics in real time. The result-
ing text-to-scene system can be an important tool
for language preservation, spreading interest in the
language among younger generations of the com-
munity and recruiting new speakers.
We will demonstrate the features of WELT
for use in fieldwork, including designing elic-
itation sessions, building scenes, recording au-
dio, and adding descriptions and glosses to a
scene. We will use examples from sessions we
49
have conducted with a native speaker of Nahu-
atl, an endangered language spoken in Mexico.
We will demonstrate how to document seman-
tics with WELT, using examples from Arrernte,
an Australian aboriginal language spoken in Alice
Springs. We will also demonstrate a basic Arrernte
text-to-scene system created in WELT.
In the following sections, we will mention re-
lated work (Section 2), discuss the WordsEye sys-
tem that WELT is based on (Section 3), describe
WELT in more detail, highlighting the functional-
ity that will appear in our demonstration (Section
4), and briefly mention our future plans for WELT
(Section 5).
2 Related Work
One of the most widely-used computer toolkits for
field linguistics is SIL Fieldworks. FieldWorks is
a collection of software tools; the most relevant
for our research is FLEx, Fieldworks Language
Explorer. FLEx includes tools for eliciting and
recording lexical information, dictionary develop-
ment, interlinearization of texts, analysis of dis-
course features, and morphological analysis. An
important part of FLEx is its ?linguist-friendly?
morphological parser (Black and Simons, 2006),
which uses an underlying model of morphology
familiar to linguists, is fully integrated into lexicon
development and interlinear text analysis, and pro-
duces a human-readable grammar sketch as well
as a machine-interpretable parser.
Several computational tools aim to simplify the
formal documentation of syntax by eliminating
the need to master particular grammar formalisms.
First is the PAWS starter kit (Black and Black,
2012), a system that prompts linguists with a series
of guided questions about the target language and
uses their answers to produce a PC-PATR gram-
mar (McConnel, 1995). The LinGO Grammar
Matrix (Bender et al., 2002) is a similar tool de-
veloped for HPSG that uses a type hierarchy to
represent cross-linguistic generalizations.
The most commonly used resource for for-
mally documenting semantics across languages
is FrameNet (Filmore et al., 2003). FrameNets
have been developed for many languages, includ-
ing Spanish, Japanese, and Portuguese. Most
start with English FrameNet and adapt it for the
new language; a large portion of the frames end
up being substantially the same across languages
(Baker, 2008). ParSem (Butt et al., 2002) is a
collaboration to develop parallel semantic repre-
sentations across languages, by developing seman-
tic structures based on LFG. Neither of these re-
sources, however, are targeted at helping non-
computational linguists formally document a lan-
guage, as compared to the morphological parser in
FLEx or the syntactic documentation in PAWS.
3 WordsEye Text-to-Scene System
WordsEye (Coyne and Sproat, 2001) is a system
for automatically converting natural language text
into 3D scenes representing the meaning of that
text. WordsEye supports language-based control
of spatial relations, textures and colors, collec-
tions, facial expressions, and poses; it handles
simple anaphora and coreference resolution, al-
lowing for a variety of ways of referring to ob-
jects. The system assembles scenes from a library
of 2,500 3D objects and 10,000 images tied to an
English lexicon of about 15,000 nouns.
The system includes a user interface where the
user can type simple sentences that are processed
to produce a 3D scene. The user can then modify
the text to refine the scene. In addition, individual
objects and their parts can be selected and high-
lighted with a bounding box to focus attention.
Several thousand real-world people have used
WordsEye online (http://www.wordseye.com). It
has also been used as a tool in education, to en-
hance literacy (Coyne et al., 2011b). In this paper,
we describe how we are using WordsEye to create
a comprehensive tool for field linguistics.
Vignette Semantics and VigNet To interpret in-
put text, WordsEye uses a lexical resource called
VigNet (Coyne et al., 2011a). VigNet is inspired
by and based on FrameNet (Baker et al., 1998),
a resource for lexical semantics. In FrameNet,
lexical items are grouped together in frames ac-
cording to their shared semantic structure. Every
frame contains a number of frame elements (se-
mantic roles) which are participants in this struc-
ture. The English FrameNet defines the mapping
between syntax and semantics for a lexical item by
providing lists of valence patterns that map syntac-
tic functions to frame elements.
VigNet extends FrameNet in two ways in or-
der to capture ?graphical semantics?,? the knowl-
edge needed to generate graphical scenes from
language. First, graphical semantics are added
to the frames by adding primitive graphical (typ-
ically, spatial) relations between the frame ele-
50
ment fillers. Second, VigNet distinguishes be-
tween meanings of words that are distinguished
graphically. For example, the specific objects
and spatial relations in the graphical semantics for
cook depend on the object being cooked and on
the culture in which it is being cooked (cooking
turkey in Baltimore vs. cooking an egg in Alice
Springs), even though at an abstract level cook an
egg in Alice Springs and cook a turkey in Bal-
timore are perfectly compositional semantically.
Frames augmented with graphical semantics are
called vignettes.
4 WordsEye Linguistics Tool (WELT)
In this section, we describe the two modes of
WELT, focusing on the aspects of our system that
will appear in our demonstration.
4.1 Tools for Linguistic Fieldwork
WELT includes tools that allow linguists to elicit
language with WordsEye. Each elicitation session
is organized around a set of WordsEye scenes. We
will demonstrate how a linguist would use WELT
in fieldwork, including (1) creating an elicitation
session, either starting from scratch, or by import-
ing scenes from a previous session; (2) building
scenes in WordsEye, saving them to a WELT ses-
sion, and modifying scenes previously added to
the session, either overwriting the original scene or
saving the changes as a new scene; (3) adding tex-
tual descriptions, glosses, and notes to a scene; and
(4) recording audio, which is automatically synced
to open scenes, and playingit back tto review any
given scene. A screen shot of the scene annotation
window is included in Figure 1.
To test the fieldwork capabilities of WELT,
we created a set of scenes based on the Max
Planck topological relations picture series (Bower-
man and Pederson, 1992). We used these scenes to
elicit descriptions from a native Nahuatl speaker;
some examples of scenes and descriptions are in-
cluded in Figure 2.
4.2 Formal Documentation of a Language
WELT also provides the means to formally doc-
ument the semantics of a language and create a
text-to-scene system for that language. The formal
documentation allows precise description of the
lexical semantics of a language. We will demon-
strate both the user interface for documenting se-
mantics, as well as a text-to-scene system for Ar-
Figure 1: WELT interface for annotating a scene
rernte created with WELT.
When a sentence is processed by WordsEye, it
goes through three main stages: (1) morphological
analysis and syntactic parsing, (2) semantic anal-
ysis, and (3) graphical realization. We will walk
through these modules in the context of WELT,
discussing (a) the formal documentation required
for that component, (b) the processing of an ex-
ample sentence through that component, and (c)
the parts of that component that will feature in our
demonstration. We will use the Arrernte sentence
shown in (1) as a running example.
(1) artwe le goal arrerneme
man ERG goal put.nonpast
The man kicks a goal.
Morphology and Syntax WELT first parses a
sentence into its morphology and syntax. Since
the focus of WELT is documentation of semantics,
the exact mechanisms for parsing the morphology
and syntax may vary. To document Arrernte, we
are using XFST (Karttunen et al., 1997) to model
the morphology and XLE (Crouch et al., 2006) to
model the syntax in the LFG formalism (Kaplan
and Bresnan, 1982). These are mature systems
that we believe are sufficient for the formal doc-
umentation of morphology and syntax. In future,
we will provide interfaces to the third-party tools
so that common information, like the lexicon, can
51
(a) in amat? t?akentija se kutSara
the paper cover one spoon
(b) in kwawit? t?apanawi t?akoja se mansana
the stick pass.thru in.middle one apple
Figure 2: Nahuatl examples elicited with WELT
be shared.
Running each word of the sentence through
the morphological analyzer in XFST transforms
the verb arrerneme into ?arrerne+NONPAST.? The
other tokens in the sentence remain unchanged.
Parsing the sentence with XLE gives the c-
structure shown in Figure 3(a) and the f-structure
shown in Figure 3(b). The f-structure will be
passed on to the semantics module.
(a)
(b)
Figure 3: C-structure (a) and f-structure (b) for
artwe le goal arrerneme.
We have added one additional feature to the
morphology and syntax module of WELT?s text-
to-scene system: an interface for selecting an f-
structure from multiple options produced by XLE,
in case the grammar is ambiguous. This way, a
linguist can use the WELT text-to-scene system
to verify their semantic documentation even if the
syntactic documentation is fairly rough. We will
demonstrate this feature when demonstrating the
Arrernte text-to-scene system.
Semantics The WELT semantics is represented
using VigNet, which has been developed for
WordsEye based on English. We will assume that
large parts of VigNet are language-independent
(for instance, the set of low-level graphical rela-
tions used to express the graphical semantics is
based on physics and human anatomy and does not
depend on language). Therefore, it should not be
necessary to create a completely new VigNet for
every language that will be used in WELT. In fu-
ture, we will develop tools for modifying VigNet
to handle linguistic and cultural differences as they
occur.
In order to use VigNet with other languages,
we need to map between the formal syntax of the
language being studied and the (English) lexical
semantics required currently by VigNet. One in-
stance showing why this is necessary occurs in our
example Arrrente sentence. When discussing foot-
ball in English, one would say that someone kicks
a goal or makes a goal. In Arrente, one would say
goal arrerneme, which translates literally to ?put
a goal.? Although the semantics of both sentences
are the same, the entry for ?put? in the English
VigNet does not include this meaning, but the Ar-
rernte text-to-scene system needs to account for it.
To address such instances, we have created an
interface for a linguist to specify a set of rules that
map from syntax to semantics. The rules take syn-
tactic f-structures as input and output a high-level
semantic representation compatible with VigNet.
The left-hand side of a rule consists of a set of con-
ditions on the f-structure elements and the right-
hand side consists of the semantic structure that
should be returned. Figure 4(a) is an example of
a rule mapping Arrernte syntax to semantics, cre-
ated in WELT.
In addition to these rules, the linguist creates a
simple table mapping lexical items into VigNet se-
mantic concepts, so that nouns can be converted to
graphical objects. We have created a mapping for
the lexical items in the Arrernte grammar; a partial
mapping is shown in Table 1.
We now describe the semantic processing of our
example Arrernte sentence, assuming a set of rules
consisting solely of the one in Figure 4(a) and the
noun mapping in Table 1. The f-structure in Fig-
52
(a) (b)
Figure 4: Syntax-semantics rule (a) and semantic category browser (b) from WELT
Lexical Item artwe panikane angepe akngwelye apwerte tipwele
VigNet Concept PERSON.N CUP.N CROW.N DOG.N ROCK-ITEM.N TABLE.N
Table 1: A mapping from nouns (lexical items) to VigNet semantic concepts
ure 3(b) has main predicate arrerne with two ar-
guments; the object is goal. Therefore, it matches
the left-hand-side of our rule. The output of
the rule specifies predicate CAUSE MOTION.KICK
with three arguments. The latter two are straight-
forward; the Theme is the VigNet object FOOTY-
BALL.N, and the Goal is FOOTYGOAL.N. To deter-
mine the Agent, we need to find the VigNet con-
cept corresponding to var-1, which occupies the
subject position in the f-structure. The subject in
our f-structure is artwe, and according to Table 1,
it maps to the VigNet concept PERSON.N. The re-
sulting semantic representation is augmented with
its graphical semantics, taken from the vignette
for CAUSE MOTION.KICK (vignette definition not
shown for lack of space). The final representation
is shown in Figure 5, with lexical semantics at the
top and graphical semantics below. The Words-
Eye system then builds the scene from these con-
straints and renders it in 3D.
CAUSE_MOTION.KICK
FOOTYBALL
Theme
FOOTYGOAL
Goal
PERSON
Agent
20 ft
FRONT-OF
Dist
ORIENT-TOPOSITION-BETWEEN
Figure GroundGoal Ground
IN-POSE
FigureSource SubjectFigure
kick
Value
Figure 5: The semantics (lexical and graphical) for
sentence (1)
WELT provides an interface for creating rules
by defining the tree structures for the left-hand-
side and right-hand-side of the rule. Every node on
the left-hand-side can optionally contain boolean
logic, if for example we want to allow the sub-
ject to be [(artwe ?man? OR arhele ?woman?) AND
NOT ampe ?child?]; so rules can be as simple or
complex as desired. Rules need not specify lexical
items directly; it is also possible to refer to more
general semantic categories. For example, a rule
could select for all verbs of motion, or specify a
particular constraint on the subject or object. In
figure 4(a), for instance, we may want to only al-
low animate subjects.
Semantic categories are chosen through a
browser that allows the user to search through all
the semantic categories defined in VigNet. For ex-
ample, if we want to find the semantic category
to use as a constraint on our example subject, we
might start by searching for human. This takes us
to a portion of a tree of semantic concepts cen-
tered around HUMAN.N. The semantic categories
are displayed one level at a time, so we initially
see only the concepts directly above and directly
below the word we searched for. From there, it?s
easy to select the concepts we are interested in,
and go up or down the tree until we find the one we
want. Below HUMAN.N are HUMAN-FEMALE.N
and HUMAN-MALE.N, but we are more interested
in the more general categories above the node. A
screen shot showing the result of this search is
shown in Figure 4(b). Above HUMAN.N is HU-
MANOID.N; above that, ANIMATE-BEING.N. Do-
ing a quick check of further parents and chil-
dren, we can see that for the subject of ?put goal,?
we would probably want to choose ANIMATE-
BEING.N over LIVING-THING.N.
The table mapping lexical items to VigNet con-
cepts is built in a similar way; the lexicon is au-
tomatically extracted from the LFG grammar, and
the user can search and browse semantic concepts
to find the appropriate node for each lexical item.
We will demonstrate the WELT user inter-
53
face which supports the creation of syntax-to-
semantics rules, creates the mapping between
nouns in the lexicon and VigNet concepts, and ver-
ifies the rules using the WELT text-to-scene sys-
tem. We will show examples from our documenta-
tion of Arrernte and demonstrate entering text into
the Arrernte text-to-scene system to generate pic-
tures.
5 Summary and Future Work
We have described a novel tool for linguists work-
ing with endangered languages. It provides a new
way to elicit data from informants, an interface
for formally documenting the lexical semantics of
a language, and allows the creation of a text-to-
scene system for any language.
This project is in its early stages, so we are plan-
ning many additional features and improvements.
For both modes of WELT, we want to generate pic-
tures appropriate for the target culture. To han-
dle this, we will add the ability to include cus-
tom objects and modify VigNet with new vignettes
or new graphical semantics for existing vignettes.
We also plan to build tools to import and export
the work done in WELT in order to facilitate col-
laboration among linguists working on similar lan-
guages or cultures. Sharing sets of scenes will al-
low linguists to reuse work and avoid duplicated
effort. Importing different versions of VigNet will
make it easier to start out with WELT on a new
language if it is similar to one that has already
been studied. We might expect, for instance, that
other Australian aboriginal languages will require
the same kinds of cultural modifications to VigNet
that we make for Arrernte, or that two languages
in the same family might also have similar syntax
to semantics rules.
Acknowledgments
This material is based upon work supported by
the National Science Foundation under Grant No.
1160700.
References
Alliance for Linguistic Diversity. 2013. The En-
dangered Languages Project. http://www.
endangeredlanguages.com/.
C. Baker, J. Fillmore, and J. Lowe. 1998. The Berkeley
FrameNet project. In 36th Meeting of the Associa-
tion for Computational Linguistics and 17th Inter-
national Conference on Computational Linguistics
(COLING-ACL?98), pages 86?90, Montr?eal.
C. Baker. 2008. FrameNet, present and future. In The
First International Conference on Global Interoper-
ability for Language Resources, pages 12?17.
E. Bender, D. Flickinger, and S. Oepen. 2002. The
Grammar Matrix. In J. Carroll, N. Oostdijk, and
R. Sutcliffe, editors, Workshop on Grammar En-
gineering and Evaluation at the 19th International
Conference on Computational Linguistics, pages 8?
14, Taipei, Taiwan.
S. Bird. 2009. Natural language processing and
linguistic fieldwork. Computational Linguistics,
35(3):469?474.
C. Black and H.A. Black. 2012. Grammars for the
people, by the people, made easier using PAWS and
XLingPaper. In Sebastian Nordoff, editor, Elec-
tronic Grammaticography, pages 103?128. Univer-
sity of Hawaii Press, Honolulu.
H.A. Black and G.F. Simons. 2006. The SIL Field-
Works Language Explorer approach to morphologi-
cal parsing. In Computational Linguistics for Less-
studied Languages: Texas Linguistics Society 10,
Austin, TX, November.
M. Bowerman and E. Pederson. 1992. Topological re-
lations picture series. In S. Levinson, editor, Space
stimuli kit 1.2, page 51, Nijmegen. Max Planck In-
stitute for Psycholinguistics.
M. Butt, H. Dyvik, T.H. King, H. Masuichi, and
C. Rohrer. 2002. The parallel grammar project. In
2002 Workshop on Grammar Engineering and Eval-
uation - Volume 15, COLING-GEE ?02, pages 1?
7, Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
B. Coyne and R. Sproat. 2001. WordsEye: An au-
tomatic text-to-scene conversion system. In SIG-
GRAPH.
B. Coyne, D. Bauer, and O. Rambow. 2011a. Vignet:
Grounding language in graphics using frame seman-
tics. In ACL Workshop on Relational Models of Se-
mantics (RELMS), Portland, OR.
B. Coyne, C. Schudel, M. Bitz, and J. Hirschberg.
2011b. Evaluating a text-to-scene generation system
as an aid to literacy. In SlaTE (Speech and Language
Technology in Education) Workshop at Interspeech,
Venice.
D. Crouch, M. Dalrymple, R. Kaplan, T. King,
J. Maxwell, and P. Newman, 2006. XLE Doc-
umentation. http://www2.parc.com/isl/
groups/nltt/xle/doc/xle.
C. Filmore, C. Johnson, and M. Petruck. 2003. Back-
ground to FrameNet. In International Journal of
Lexicography, pages 235?250.
R.M. Kaplan and J.W. Bresnan. 1982. Lexical-
functional grammar: A formal system for grammat-
ical representation. In J.W. Bresnan, editor, The
Mental Representation of Grammatical Relations.
MIT Press, Cambridge, Mass., December.
L. Karttunen, T. Ga?al, and A. Kempe. 1997. Xerox
finite-state tool. Technical report, Xerox Research
Centre Europe, Grenoble.
S. McConnel, 1995. PC-PATR Reference Manual.
Summer Institute for Linguistics.
54
Collecting Semantic Data by Mechanical Turk for the Lexical
Knowledge Resource of a Text-to-Picture Generating System
Masoud Rouhizadeh* Margit Bowler* Richard Sproat*Bob Coyne**
*Center for Spoken Language Understanding, Oregon Health and Science University
**Department of Computer Science, Columbia University
Abstract
WordsEye is a system for automatically converting natural language text into 3D scenes repre-
senting the meaning of that text. At the core of WordsEye is the Scenario-Based Lexical Knowledge
Resource (SBLR), a unified knowledge base and representational system for expressing lexical and
real-world knowledge needed to depict scenes from text. To enrich a portion of the SBLR, we need to
fill out some contextual information about its objects, including information about their typical parts,
typical locations and typical objects located near them. This paper explores our proposed method-
ology to achieve this goal. First we try to collect some semantic information by using Amazon?s
Mechanical Turk (AMT). Then, we manually filter and classify the collected data and finally, we
compare the manual results with the output of some automatic filtration techniques which use several
WordNet similarity and corpus association measures.
1 Introduction
WordsEye (Coyne and Sproat, 2001), (Coyne et al, 2010) is a system for automatically converting natural
language text into 3D scenes representing the meaning of that text. A version of WordsEye has been
tested online (www.wordseye.com) with several thousand real-world users. The system works by first
parsing each input sentence into a dependency structure. These dependency structures are then processed
to resolve anaphora and other coreferences. The lexical items and dependency links are then converted
to semantic nodes and roles drawing on lexical valence patterns and other information in the Scenario-
Based Lexical Knowledge Resource (SBLR) (Coyne et al, 2010). The resulting semantic relations are
then converted to a final set of graphical constraints representing the position, orientation, size, color,
texture, and poses of objects in the scene. Finally, the scene is composed from these constraints and
rendered in OpenGL (http://www.opengl.org).
The SBLR is the core of the text-to-scene conversion mechanism. It is a unified knowledge base and
representational system for expressing lexical and real-world knowledge needed to depict scenes from
text. The SBLR will ultimately include information on the semantic categories of words; the semantic
relations between predicates (verbs, nouns, adjectives, and prepositions) and their arguments; the types
of arguments different predicates typically take; additional contextual knowledge about the visual scenes
various events and activities occur in; and the relationship between this linguistic information and the 3D
objects in our objects library.
To enrich a portion of the SBLR we need to fill out some contextual information about several
hundred objects in WordsEye?s database, including information about their typical parts, typical location
and typical objects nearby them. Such information can in principle be extracted from online corpora
(e.g. Sproat (2001)), but such data is invariably noisy and requires hand editing. Furthermore, precisely
because much of the information is common sense it is rarely explicitly stated in text. Ontologies of
common sense information such as Cyc are effectively useless for extracting such information.
This paper explores our proposed methodology to achieve this goal. First we try to collect some
semantic information by Amazon?s Mechanical Turk (AMT). Then, we manually filter and classify the
380
collected data and finally, we compare the manual results with the output of some automatic filtration
techniques which use WN similarity and corpus association measures.
2 Data collection from Amazon?s Mechanical Turk
Amazon?s Mechanical Turk is an online marketplace that provides a way to pay people small amounts
of money to perform tasks that are simple for humans but difficult for computers. Examples of these
Human Intelligence Tasks (HITs) range from labeling images to moderating blog comments to providing
feedback on the relevance of results for a search query. The highly accurate, cheap and efficient results
of several NLP tasks (Callison-Burch and Dredze, 2010) have encouraged us to explore using AMT.
We designed three separate tasks to collect information about typical nearby objects, typical location
and typical parts of the objects of our library. For task 1, we asked the workers to name 10 common
objects that they might typically find around or near a given object. For task 2, we asked the workers to
name 10 locations in which they might typically find a given object and in task 3, we asked the workers
to list 10 parts of a given object. Given that some objects might not consist of 10 parts, (i.e, they are
very simple objects), we wanted the worker to name as many parts as possible. We collected 17,200
responses from the AMT tasks and paid $106.90 overall for completion of the three tasks. Table 1 shows
a summary of the AMT tasks, payments, and completion time.
Task TW UI AA RPA EHR ACT
Objects 342 6850 2? $0.05 $1.54 5
Locations 342 6850 2? $0.05 $1.26 5
Parts 245 3500 1? $0.07 $2.29 5
TW: Number of Target Words; UI: Number of User Inputs; AA: Average Time Per Assignment;
RPA: Reward Per Assignment; EHR: Effective Hourly Rate; ACT: Approximate Completion Days
Table 1: Summary of AMT tasks, payments and the completion time
The data that we collected in this step was in raw format. The next step was filtering out undesirable
data entered by the workers and mapping it into entities and relations contained within the SBLR.
3 Manual filtering and classifying the data
Data collected from AMT tasks was manually filtered via removal of undesirable target item-response
item pairs and classified via definition of the relations between the remaining target item-response item
pairs. Response items given in their plural form were lemmatized to the singular form of the word.
A total of 34 relations were defined within the Amazon Mechanical Turk data. Defining relations was
completed manually and determined by pragmatic cues about the relationship held between the target
item-response item pair. Restricting AMT workers to those within the United States ensured that actions
or items which might differ in their typically found location by cultural or geographical context were
restricted to the location(s) generally agreed upon by English speakers within the United States.
Generic, widely applicable relations were used in the general case for all sets of Mechanical Turk data
(e.g. the containment relation containing.r was used for generic instances of containment; the next-to.r
relation was used for target item-response item pairs for which the orientation of the items with respect to
one another was not a defining characteristic of their relationship). Finer distinctions were made within
these generic relations, e.g. habitat.r and residence.r within the overarching containment relation, which
specified that the relation held between two items was that of habitat or residence, respectively. More
semantically explicit relations were used for target item-response item pairs that tended to occur in more
specific relations. Specific relations of this type included those spatial relations from the following target
item-response item-relation triples:
javelin - dirt - embedded-in.r
binoculars - case - true-containing.r
381
Another subsection of relations included functional relations such as those within the following
triples:
harmonica - hand - human-grip.r
earmuffs - head - wearing.r
Relation labels for meronymic (part-whole) relations were based off of already defined part-whole
classifications (Priss, 1996).
3.1 Data and results for each AMT task
Target item-response item pairs were usually rejected for misinterpretation of the potentially ambiguous
target item (e.g. misinterpreting mobile as a cell phone rather than as a decorative hanging structure,
prompting mobile - ear as an object-nearby object pair). Target item-response item pairs were also dis-
carded if the interpretation of the target item, though viable, was not contained within the SBLR library.
This was especially prevalent in instances where the target item was a plant or animal (e.g. crawfish)
that could be interpreted as either a live plant/animal or as food. With the exception of mushroom, the
SBLR does not contain the edible interpretation of these nouns; in the object-nearby object task, target
item-response item pairs such as crawfish - plate were discarded.
In the object-location task, the most common relation labels were derivatives of the generic spatial
containment relation. The containing.r relation accounted for 38.01% of all labeled target-response pairs;
habitat.r accounted for 11.02%, and on-surface.r accounted for 10.6%.
In the part-whole task, AMT workers provided responses that were predominantly labeled by part-
whole relations. When AMT responses were not relevant for part-whole relations, they tended to fall
under the generic containment relation. The object-part.r relation accounted for 79.12% of all labeled
target-response pairs; stuff-object.r accounted for 16.33%, and containing.r accounted for 1.48%.
As with the part-whole task, responses in the nearby objects task that were not relevant for the next-
to.r relation usually fell under the generic spatial containment relation. In the object-nearby object task,
the next-to.r relation was the most frequently utilized relation label, accounting for 75.66% of all target-
response pairs labeled. The on-surface.r relation was the second most common relation, with 5.69%,
and containing.r accounted for 4.44% of all labeled target-response pairs
4 Automatic filtering undesirable data
Manual processing of the data is a time-consuming and expensive approach. As a result, we are inves-
tigating different automatic techniques to filter out the undesirable responses from AMT, using current
manually annotated data as a gold standard for evaluation of automatic approaches.
4.1 WordNet Similarity measures
In the first approach, we computed some lexical similarity scores for the target and the response items
based on the followingWN similarity measures. (It should be noted that not all of the target and responses
were present in WN. For such words, we used their nearest hypernyms).
WN Path Distance Similarity between each target word and each received response for that target
word. This score denotes how similar two word senses are, based on the shortest path that connects
the senses in the is-a (hypernym/hypnoym) taxonomy. We selected the maximum similarity score of
different senses of the target and the respond words.
Resnik Similarity between each target word and each of the received responses for that target word.
This score denotes how similar the two word senses are, based on the Information Content (IC) of the
Least Common Subsumer (most specific ancestor node) (Resnik, 1999).
The Average Pairwise Similarity Score which is computed based on WN path distance similarity
score. If we assumeW1,W2...Wn to be n responses for target word T; and Sij to be theWN path distance
similarity between Wi and Wj , then the average pairwise similarity score for Wi will be Si1+Si2+...+Sinn .
This will provide us the average similarity of each response (i.e Wi) with the other responses (i.e. Wj
382
so that i6= j). In this way we will reward the responses that are more semantically related to each other
(regardless of their similarity to the target word).
The WN Matrix Similarity which is a bag of words similarity matrix based on WN path distance
similarities. For target word T we have the following similarity matrix:
1 + S12 + ...+ S1n
S21 + 1 + ...+ S2n
...
Sn1 + Sn2 + ...+ Snn
In this matrix row i is the similarity vector of Wi represented as ~Vi = [Si1 + Si2 + ...+ Sin]. We
use cosine similarity to calculate the similarity measure of two words. So, the similarity measure of
Wi and Wj is the cosine of ~Vi and ~Vj and is computed by CSij = cos(?) = Vi.Vj||Vi||.||Vj || . Then the WN
matrix similarity score of Wi will be CSi1+CSi2+...+CSinn . The more two words are semantically related
to similar set of words, the higher cosine similarity they will have. If a word is related to many different
words in the set, it will obtain higher WN matrix similarity score.
4.2 Corpus association measures
The next approach for filtering the raw data was finding association measures of target-response pairs
using Google?s 1-trillion 5-gram web corpus (LDC2006T13), by counting the frequency of each target
and response word in unigram and bigram portions of the corpus and then the number of times the two
words co-occur within a +/- 4-word window in the 5-gram portion of the corpus. We also computed the
sentential co-occurrences of each target-response pair (i.e. the number of sentences in which the target
or the response words appear and the number of sentences in which both words occur together) on the
English Gigaword corpus (LDC2007T07) which is a 1 billion word corpus of articles marked up from
English press texts (mainly the New York Times). Based on these counts, we used log-likelihood and
log-odds ratio (Dunning, 1993) to compute the association between the two words.
4.3 Discussion and evaluation of automatic filtaration techniques
The collected responses of each AMT task were ranked separately by each of the above similarity and
association measures. We classify the ranked responses into ?keep? (higher-scoring) and ?reject? (lower-
scoring) classes by defining a specific threshold for each list. Then we evaluated the accuracy of each
filtration approach by computing their precision and recall on correct ?keep? items (see table 2). In this
table the baseline score shows the accuracy of the responses of each AMT task before using automatic
filtration techniques. It should be added that collecting data by using AMT is rather cheap and fast, so
we are more interested in higher precision (achieving highly accurate data) than higher recall. Lower
recall means we lose some data, which is not too expensive to collect.
Baseline Log-likelihood Log-odds WN Path Dist sim Resnik sim WN Pairwise sim WN Matrix sim
Pre Rec Pre Rec Pre Rec Pre Rec Pre Rec Pre Rec Pre Rec
LOC 0.5527 1.0 0.7832 0.6690 0.7851 0.6684 0.5624 0.9724 0.5674 0.9784 0.6115 0.3657 0.4832 1.0
PAR 0.7887 1.0 0.7921 0.4523 0.8321 0.5022 0.8073 1.0 0.8234 1.0 0.9045 0.2859 0.9010 0.2516
OBJ 0.8934 1.0 0.9015 1.0 0.9286 0.9144 0.9123 1.0 0.9185 1.0 0.9855 0.3215 0.8925 1.0
Table 2: The accuracy of automatic filtering approaches
As can be seen in table 2, within the object-location data set, we gained the best precision (0.7832) by
using log-odds with relatively high recall (0.6690). Target-response pairs that were approved or rejected
contrary to automatic predictions were due primarily to the specificity of the response location.
In the part-whole task, the best precision (0.9010) was achieved by using WN matrix similarities
but again we lost a noticeable portion of data (recall= 0.2516). Rejected target-response pairs from the
higher-scoring part-whole set were often due to responses that named attributes, rather than parts, of
the target item (e.g. croissant - flaky). Many responses were too general (e.g, gong - material). Many
target-response pairs would have fallen under the next-to.r relation rather than any of the meronymic
383
relations. The majority of the approved target-response pairs from the lower-scoring part-whole set were
due to obvious, ?common sense responses that would usually be inferred rather than explicitly stated,
particularly body parts (e.g, bunny - brain).
The baseline accuracy of the nearby objects task is quite high (precision=0.8934, recall=1.0), and
we gain the best precision by using WN average pairwise similarity (0.9855) by removing lower-scoring
part of AMT responses (recall=0.3215). The high precision in all automatic techniques is due primarily
to the fact that the open-ended nature of the task resulted in a large number of target-response pairs that,
while not pertinent to the next-to.r relation, could be labeled by other relations. Again, the open-ended
nature of the nearby objects task resulted in the lowest percentage of rejected high-scoring pairs.
5 Conclusions
In this paper, we investigated the use Amazon?s Mechanical Turk for collecting semantic information for
a portion of our lexical knowledge resource. Manual evaluation of the AMT responses (baseline results
in table 2) confirms that we can collect highly accurate data in a cheap and efficient way by using AMT.
The accuracy of automatic filtration techniques sounds promising as we were able to filter out some
undesirable data, most of the time without loosing so much of collected responses.
Overall, we have shown a method which is very good in collecting semantic information and some
other methods which are very good at filtering out word pairs that are undesirable in this particular context
(i.e locations, nearby object and parts of our object library). This approach seems to have the potential
to be extended for more contexts. For the future work, we are planning to apply this methodology to
collect semantic information about action verbs, such as information about the locations of the action,
the participants, their relation to each other, the background objects and so on.
References
Callison-Burch, C. and M. Dredze (2010). Creating speech and language data with amazon?s mechanical
turk. In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with
Amazon?s Mechanical Turk, Los Angeles, CA, USA, pp. 1?12.
Coyne, B., O. Rambow, J. Hirschberg, and R. Sproat (2010). Frame semantics in text-to-scene gen-
eration. In R. Setchi, I. Jordanov, R. Howlett, and L. Jain (Eds.), Knowledge-Based and Intelligent
Information and Engineering Systems, Volume 6279 of Lecture Notes in Computer Science, pp. 375?
384. Springer Berlin / Heidelberg.
Coyne, B. and R. Sproat (2001). Wordseye: An automatic text-to-scene conversion system. In Proceed-
ings of the 28th annual conference on Computer graphics and interactive techniques, Los Angeles,
CA, USA, pp. 487? 496.
Coyne, B., R. Sproat, and J. Hirschberg (2010). Spatial relations in text-to-scene conversion. In Compu-
tational Models of Spatial Language Interpretation, Workshop at Spatial Cognition 2010, Mt. Hood,
OR, USA, pp. 9?16.
Dunning, T. E. (1993). Accurate methods for the statistics of surprise and coincidence. Computational
Linguistics 19(1), 61?74.
Priss, U. (1996). Classification of meronymy by methods of relational concept analysis. In Online
proceedings of the 1996 Midwest Artificial Intelligence Conference, Bloomington, IN, USA.
Resnik, P. (1999). Semantic similarity in a taxonomy: An information-based measure and its application
to problems of ambiguity in natural language. Journal of Artificial Intelligence Research, 95?130.
Sproat, R. (2001). Inferring the environment in a text-to-scene conversion system. In Proceedings of The
First International Conference on Knowledge Capture, Victoria, BC, Canada, pp. 147?154.
384
Proceedings of the ACL 2011 Workshop on Relational Models of Semantics (RELMS 2011), pages 28?36,
Portland, Oregon, USA, June 23, 2011. c?2011 Association for Computational Linguistics
VigNet: Grounding Language in Graphics using Frame Semantics
Bob Coyne and Daniel Bauer and Owen Rambow
Columbia University
New York, NY 10027, USA
{coyne, bauer, rambow}@cs.columbia.edu
Abstract
This paper introduces Vignette Semantics, a
lexical semantic theory based on Frame Se-
mantics that represents conceptual and graph-
ical relations. We also describe a lexical re-
source that implements this theory, VigNet,
and its application in text-to-scene generation.
1 Introduction
Our goal is to build a comprehensive text-to-
graphics system. When considering sentences such
as John is washing an apple and John is washing
the floor, we discover that rather different graphical
knowledge is needed to generate static scenes rep-
resenting the meaning of these two sentences (see
Figure 1): the human actor is assuming different
poses, he is interacting differently with the thing be-
ing washed, and the water, present in both scenes,
is supplied differently. If we consider the types of
knowledge needed for scene generation, we find that
we cannot simply associate a single set of knowl-
edge with the English verb wash. The question
arises: how can we organize this knowledge and
associate it with lexical items, so that the resulting
lexical knowledge base both is usable in a wide-
coverage text-to-graphics system, and can be pop-
ulated with the required knowledge using limited re-
sources?
In this paper, we present a new knowledge base
that we use for text-to-graphics generation. We dis-
tinguish three types of knowledge needed for our
task. The first is conceptual knowledge, which is
knowledge about concepts, often evoked by words.
For example, if I am told John bought an apple, then
I know that that event necessarily also involved the
seller and money. Second, we need world knowl-
Figure 1: Mocked-up scenes using the WASH-SMALL-
FRUIT vignette (?John washes the apple?) and WASH-
FLOOR-W-SPONGE vignette (?John washes the floor?).
edge. For example, apples grow on trees in cer-
tain geographic locations at certain times of the year.
Third, we need grounding knowledge, which tells
us how concepts are related to sensory experiences.
In our application, we model grounding knowledge
with a database of 3-dimensional graphical models.
We will refer to this type of grounding knowledge
as graphical knowledge. An example of grounding
knowledge is knowing that several specific graphical
models represent apple trees.
Conceptual knowledge is already the object of ex-
tensive work in frame semantics; FrameNet (Rup-
penhofer et al, 2010) is an extensive (but not com-
plete) relational semantic encoding of lexical mean-
ing in a frame-semantic conceptual framework. We
use this prior work, both the theory and the resource,
in our work. The encoding of world knowledge has
been the topic of much work in Artificial Intelli-
gence. Our specific contribution in this paper is the
integration of the representation for world knowl-
edge and graphical knowledge into a frame-semantic
approach. In order to integrate these knowledge
types, we extend FrameNet in three manners.
1. Frames describe complex relations between
their frame elements, but these relations, i.e.
28
the internal structure of a frame, is not explic-
itly formulated in frame semantics. FrameNet
frames do not have any intensional meaning
besides the informal English definition of the
frames (and what is expressed by so-called
?frame-to-frame relations?). From the point
of view of graphics generation, internal struc-
ture is necessary. While for many applications
a semantic representation can remain vague, a
scene must contain concrete objects and spatial
relations between them.
2. Some frames are not semantically specific
enough. For example, there is a frame
SELF MOTION, which includes both walk and
swim; these verbs clearly need different graph-
ical realizations, but they are also different
from a general semantic point of view. While
this situation could be remedied by extend-
ing the inventory of frames by adding WALK
and SWIM frames, which would inherit from
SELF MOTION, the situation is more complex.
Consider wash an apple and wash the floor,
discussed above. While the core meaning of
wash is the same in both phrases, the graphi-
cal realization is again very different. However,
we cannot simply create two new frames, since
at some level (though not the graphical level)
the meaning is indeed compositional. We thus
need a new mechanism.
3. FrameNet is a lexical resource that illustrates
how language can be used to refer to frames,
which are abstract definitions of concepts, and
their frame elements. It is not intended to be
a formalism for deep semantic interpretation.
The FrameNet annotations show the frame ele-
ments of frames (e.g. the goal frame element of
the SELF MOTION frame) being filled with text
passages (e.g. into the garden) rather than with
concrete semantic objects (e.g. an ?instance?
of a LOCALE BY USE frame evoked by gar-
den). Because such objects are needed in or-
der to fully represent the meaning of a sentence
and to assert world knowledge, we introduce
semantic nodes which are discourse referents
of lexical items (whereas frames describe their
meanings).
In this paper, we present VigNet, a resource which
extends FrameNet to incorporate world and graph-
ical knowledge. We achieve this goal by address-
ing the three issues above. We first extend frames
by adding more information to them (specifically,
about decomposition relevant to graphical ground-
ing and more precise selectional restrictions). We
call a frame with graphical information a vignette.
We then extend the structure defined by FrameNet
by adding new frames and vignettes, for example
for wash an apple. The result we call VigNet. Fi-
nally, we extend VigNet with a system of nodes
which instantiate frames; these nodes we call se-
mantic nodes. They get their meaning only from the
frames they instantiate. All three extensions are con-
servative extensions of frames and FrameNet. The
semantic theory that VigNet instantiates we call Vi-
gnette Semantics and we believe it to be a conser-
vative extension (and thus in the spirit of) frame se-
mantics.
This paper is structured as follows. In Section 2,
we review frame semantics and FrameNet. Section 3
presents a more detailed description of VigNet, and
we provide examples in Section 4. Since VigNet is
intended to be used in a large-coverage system, the
population of VigNet with knowledge is a crucial is-
sue which we address in Section 5. We discuss re-
lated work in Section 6 and conclude in Section 7.
2 Frame Semantics and FrameNet
Frame Semantics (FS; Fillmore (1982)) is based on
the idea that the meaning of a word can only be fully
understood in context of the entire conceptual struc-
ture surrounding it, called the word?s frame. When
the meaning of a word is evoked in a hearer?s mind
all related concepts are activated simultaneously and
we can rely on this structure to transfer information
in a conversation. Frames can describe states-of-
affairs, events or complex objects. Each frame con-
tains a set of specific frame elements (FEs), which
are labeled semantic argument slots describing par-
ticipants in the frame. For instance, the word buy
evokes the frame for a commercial transaction sce-
nario, which includes a buyer and a seller that ex-
change money for goods. A speaker is aware of what
typical buyers, sellers, and goods are. He may also
have a mental prototype of the visual scenario itself
29
(e.g. standing at a counter in a store). In FS the
role of syntactic theory and the lexicon is to explain
how the syntactic dependents of a word that realizes
a frame (i.e. arguments and adjuncts) are mapped to
frame elements via valence patterns.
FrameNet (FN; Baker et al (1998), Ruppenhofer
et al (2010)) is a lexical resource based on FS.
Frames in FN (around 1000) 1 are defined in terms
of their frame elements, relations to other frames
and semantic types of FEs. Beyond this, the mean-
ing of the frame (how the FEs are related to each
other) is only described in natural language. FN
contains about 11,800 lexical units, which are pair-
ings of words and frames. These come with anno-
tated example sentences (about 150,000) to illustrate
their valence patterns. FN contains a network of
directed frame-to-frame relations. In the INHERI-
TANCE relation a child-frame inherits all semantic
properties from the superframe. The frame rela-
tions SUBFRAME and PRECEDES refer to sub-events
and events following in temporal order respec-
tively. The parent frame?s FEs are mapped to the
child?s FEs. For instance CAUSE TO WAKE inher-
its from TRANSITIVE ACTION and its sleeper FE
maps to agent. Other relations include PERSPEC-
TIVE ON, CAUSATIVE OF, and INCHOATIVE OF.
Frame relations captures important semantic facts
about frames. For instance the hierarchical organi-
zation of INHERITANCE allows to view an event on
varying levels of specificity. Finally, FN contains
a small ontology of semantic types for frame ele-
ments, which can be interpreted as selectional re-
strictions (e.g. an agent frame element must be
filled by a sentient being).
3 Vignette Semantics
In Section 1, we motivated VigNet by the need
for a resource that allows us to relate language to
a grounded semantics, where for us the graphical
representation is a stand-in for grounding. We de-
scribed three reasons for extending FrameNet to Vi-
gNet: we need more meaning in a frame, we need
more frames and more types of frames, and we need
to instantiate frames in a clean manner. We discuss
these refinements in more detail in this section.
1Numbers refer to FrameNet 1.5
? Vignettes are frames that are decomposed into
graphical primitives and can be visualized.
Like other fames they are motivated by frame
semantics; they correspond to a conceptual
structure evoked by the lexical units which are
associated with it.
? VigNet includes individual frames for each
(content) lexical item. This provides finer-
grained semantics than given with FrameNet
frames themselves. These lexically-coupled
frames leverage the existing structure of their
parent frames. For example, the SELF MOTION
frame contains lexical items for run and swim
which have very different meaning even though
they share the same frame and FEs (such as
SOURCE, GOAL, and PATH). We therefore
define frames for RUN and SWIM which in-
herit from SELF MOTION. We assume also that
frames and lexical items that are missing from
FrameNet are defined and linked to the rest of
FrameNet as needed.
? Even more specific frames are created to rep-
resent composed vignettes. These are vi-
gnettes that ground meaning in different ways
than the primitive vignette that they special-
ize. The only motivation for their existence
is the graphical grounding. For example, we
cannot determine how to represent washing an
apple from the knowledge of how to repre-
sent generic washing and an apple. So we de-
fine a new vignette specifically for washing a
small fruit. From the point of view of lexi-
cal semantics, it uses two lexical items (wash
and apple) and their interpretation, but for us,
since we are interested in grounding, it is a
single vignette. Note that it is not necessary
to create specific vignettes for every concrete
verb/argument combination. Because vignettes
are visually inspired relatively few general vi-
gnettes (e.g. manipulate an object on a fixture)
suffices to visualize many possible scenarios.
? A new type of frame-to-frame relation, which
we call SUBFRAME-PARALLEL is used to de-
compose vignettes into a set of more primitive
semantic relations between their arguments.
Unlike FrameNet?s SUBFRAME relation which
30
represents temporally sequential subframes, in
SUBFRAME-PARALLEL, the subframes are all
active at the same time, provide a conceptual
and spatial decomposition of the frame, and can
serve as spatial constraints on the frame ele-
ments. A frame is called a vignette if it can
be decomposed into graphical primitives using
SUBFRAME-PARALLEL relations. For instance
in the vignette WASH-SMALL-OBJ for washing
a small object in a sink, the washer has to be
in front of the sink. We assert a SUBFRAME-
PARALLEL relation between WASH-SMALL-
OBJ and FRONTOF, mapping the washer FE
to the figure FE and sink to ground.
? FrameNet has a very limited number of seman-
tic types that are used to restrict the values
of FEs. Vignette semantics uses selectional
restrictions to differentiate between vignettes
that have the same parent. For example, the
vignette invoked for washing a small object in
a sink would restrict the semantic type of the
theme (the entity being washed) to anything
small, or, more generally, to any object that is
washed in this way (apples, hard-boiled eggs,
etc). The vignette used for washing a vehicle in
a driveway with a hose would restrict its theme
to some set of large objects or vehicle types.
Selectional restrictions are asserted using the
same mechanism as decompositions.
? As mentioned in Section 1, in FrameNet an-
notations frame elements (FEs) are filled with
text spans. Therefore, while frame seman-
tics in general is a deep semantic theory,
FrameNet annotations only represent shallow
semantics and it is not immediately obvious
how FrameNet can be used to build a full se-
mantic representations of a sentence. In Vi-
gnette semantics, when a frame is evoked by
a lexical item, it is instantiated as a semantic
node. Its FEs are then bound not to subphrases,
but to semantic nodes which are the instantia-
tions of the frames evoked by those subphrases.
Section 3.1 investigates semantic nodes in more de-
tail. Section 3.2 illustrates different types of vi-
gnettes (objects, actions, locations) and how they are
defined using the SUBFRAME PARALLEL relation.
In Section 3.3 we discuss selectional restrictions.
3.1 Semantic Nodes and Relational Knowledge
The intuition behind semantic nodes is that they rep-
resent objects, events or situations. They can also
represent plurals or generics. For instance we could
have semantic node city, denoting the class of cities
and a semantic node paris, that denotes the city
Paris. Note that there is also a frame CITY and a
frame PARIS that contain the conceptual structure
associated with the words city and Paris. Frames
represent the linguistic and the conceptual aspect
of knowledge; the intensional meaning of a word.
They provide knowledge to answer questions such
as ?What is an apple?? or ?How do you wash an ap-
ple??. In contrast, semantic nodes are extensional,
i.e. denotations. They represent the knowledge to
answer questions such as ?In what season are apples
harvested?? or ?How did Percy wash that apple just
now??.
As mentioned above semantic nodes allow us to
build full meaning representations of entire sen-
tences in discourse. Therefore, while frame defi-
nitions are fixed, semantic nodes can be added dy-
namically during discourse understanding or gener-
ation to model the instances of frames that language
is evoking. We call such nodes temporary seman-
tic nodes. They they are closely related to the dis-
course referents of Discourse Representation Theory
(Kamp, 1981) and related concepts in other theories.
In contrast, persistent semantic nodes are used to
store world knowledge which is distinct from the
conceptual knowledge encoded within frames and
their relations; for example, the frame for moon will
not encode the fact that the moon?s circumference is
6,790 miles, but we may record that using a knowl-
edge based of external assertions semantic nodes are
given their meaning by corresponding frames (CIR-
CUMFERENCE, MILE, etc.). A temporary semantic
node can become persistent by being retained in the
knowledge base.
3.2 Vignette Types and their Decomposition
A vignette is a frame in the FrameNet sense that is
decomposed to a set of more primitive frames us-
ing the SUBFRAME-PARALLEL frame-to-frame re-
lation. The frame elements (FEs) of a vignette are
31
defined as in FrameNet, except that our grounding
in the graphical representation gives us a new, strong
criterion to choose what the FEs are: they are the ob-
jects necessarily involved in the visual scene associ-
ated with that vignette. The subframes represent the
spatial and other relations between the FEs. The re-
sulting semantic relations specify how the scene el-
ements are spatially arranged. This mechanism cov-
ers several different cases.
For actions, we conceptually freeze the action in
time, much as in a comic book panel, and repre-
sent it in a vignette with a set of objects, spatial
relations between those objects, and poses charac-
teristic for the humans (and other pliable beings) in-
volved in that action. Action vignettes will typically
be specialized to composed vignettes, so that the ap-
plicability of different vignettes with the same par-
ent frame will depend on the values of the FEs of
the parent. In the process of creating composed vi-
gnettes, FEs are often added because additional ob-
jects are required to play auxiliary roles. As a re-
sult, the FEs of an action vignette are the union of
the semantic roles of the important participants and
props involved in that enactment of the action with
the FEs of the parent frame. For instance the follow-
ing vignette describes one concrete way of washing
a small fruit. Note that we have included a new FE
sink which is not motivated in the frame WASH.2
Note also that this vignette also contains a selec-
tional restriction on its theme, which we will dis-
cuss in the next subsection and which is not shown
here.
WASH-SMALL-FRUIT(washer, theme, sink)
FRONTOF(figure:washer, figure:sink)
FACING(figure:washer, figure:sink)
GRASP(grasper:washer, theme:theme)
REACH(reacher:washer, target:sink)
In this notation the head row contains the vignette
name and its FEs in parentheses. For readability we
will often omit FEs that are part of the vignette but
not restricted or used in any mentioned relation. The
lower box contains the vignette decomposition and
implicitly specifies SUBFRAME-PARALLEL frame-
to-frame relations. In the decomposition of a vi-
gnette V we use the notation F(a:b, ? ? ? ) to indicate
that the FE a of frame F is mapped to the FE b of V.
2FrameNet does not currently contain a WASH frame, but if
it did, it would not contain an FE sink.
When V is instantiated the semantic node binding to
a must also be able to bind to b in F.
Locations are represented by vignettes which ex-
press constraints between a set of objects character-
istic for the given location. The FEs of location vi-
gnettes include these constituent objects. For exam-
ple, one type of living room (of many possible ones)
might contain a couch, a coffee table, and a fireplace
in a certain arrangement.
LIVING-ROOM 42(left wall, far wall, couch,
coffee table, fireplace)
TOUCHING(figure:couch, ground:left wall)
FACING(figure:couch, ground:right wall)
FRONTOF(figure:coffee table, ground: sofa)
EMBEDDED(figure:fire-place, ground:far wall)
Even ordinary physical objects will have certain
characteristic parts with size, shape, and spatial re-
lations that can be expressed by vignettes. For ex-
ample, an object type such as a kind of stop sign can
be defined as a two-foot-wide, red, hexagonal metal
sheet displaying the word ?STOP? positioned on the
top of a 6 foot high post.
STOP-SIGN(sign-part, post-part, texture)
MATERIAL(theme:sign-part, material:METAL)
MATERIAL(theme:post-part, material:METAL)
DIAMETER(theme:sign-part, diameter:2 feet)
HEIGHT(theme:post-part, height:6 feet)
ONTOP(figure:sign-part, ground:post-part)
TEXTURE(theme:sign-part, texture:?STOP?)
In addition, many real-world objects do not corre-
spond to lexical items but are elaborations on them
or combinations. These sublexical entities can be
represented by vignettes as well. For example, one
such 3D object in our text-to-scene system is a goat
head mounted on a piece of wood. This object is
represented by a vignette with two FEs (ghead,
gwood) representing the goat?s head and the wood.
The vignette decomposes into ON(ghead, gwood).
While there can be many vignettes for a single
lexical item, representing the many ways a location,
action, or object can be constituted, vignettes need
not be specialized for every particular situation and
can be more or less general. In one exteme creat-
ing vignettes for every verb/argument combination
would clearly lead to a combinatorial explosion and
is not feasible. In the other extreme we can define
rather general vignettes. For example, a vignette
32
USE-TOOL for using a tool on a theme can be repre-
sented by the user GRASPING the tool and REACH-
ING towards the theme. These vignettes can be
used in decompositions of more concrete vignettes
(e.g. HAMMER-NAIL-INTO-WALL). They can also
be used directly if no other more concrete vignette
can be applied (because it does not exist or its selec-
tional restrictions cannot be satisfied). In this way
by defining a small set of such vignettes we can vi-
sualize approximate scenes for a large number of de-
scriptions.
3.3 Selectional Restrictions on Frame Elements
To define a frame we need to specify selectional re-
strictions on the semantic type of its FEs. Instead
of relying on a fixed inventory of semantic types,
we assert conceptual knowledge and external asser-
tions over persistent semantic types. This allows us
to use VigNet?s large set of frames to represent such
knowledge. For example, an apple can be defined as
a small round fruit.
APPLE(self)
SHAPEOF(figure:self, shape:spherical)
SIZEOF(figure:self, size:small)
APPLE is simply a frame that contains a self FE,
which allows us to make assertions about the con-
cept (i.e. about any semantic node bound to the
self FE). Frame elements of this type are not un-
usual in FrameNet, where they are mainly used for
frames containing common nouns (for instance the
Substance FE contains a substance FE). In Vi-
gNet we implicitly use self in all frames, including
frames describing situations and events.
We use the same mechanism to define specialized
compound vignettes such as WASH SMALL FRUIT.
We extend WASH in the following way to restrict
it to small fruits (we abreviate F(self:a) as a=F for
readability).
WASH-SMALL-FRUIT(washer, theme, sink)
% selectional restrictions
sink=SINK, washer=PERSON,
theme=x, x=FRUIT,
SIZEOF(figure:x,size:small)
% decomposition
FRONTOF(figure:washer, figure:sink)
FACING(figure:washer, figure:sink)
GRASP(grasper:washer, theme:theme)
REACH(reacher:washer, target:sink)
4 Examples
In this section we give further examples of visual
action vignettes for the verb wash. The selectional
restrictions and graphical decomposition of these vi-
gnettes vary depending on the type of object be-
ing washed. The first example shows a vignette for
washing a vehicle.
WASH-VEHICLE(washer, theme, instr, location)
washer=PERSON, theme=VEHICLE,
instr=HOSE, location=DRIVEWAY
ONSURFACE(figure:theme, ground:location)
FRONTOF(figure:washer, ground:theme)
FACING(figure:washer, ground:theme)
GRASP(grasper:washer, theme:instrument)
AIM(aimer:washer, theme:instr, target:theme)
The following two vignettes represent a case where
the object being washed alone does not determine
which vignette to apply. If the instrument is unspec-
ified one or the other could be used. We illustrate
one option in figure 1 (right).
WASH-FLOOR-W-SPONGE(washer,theme,instr)
washer=PERSON, theme=FLOOR,
instr=SPONGE
KNEELING(agent:washer),
GRASP(grasper:washer, theme:instr),
REACH(reacher:washer, target:theme)
WASH-FLOOR-W-MOP(washer, theme, instr)
washer=PERSON, theme=FLOOR, instr=MOP
GRASP(grasper:washer, theme:instr),
REACHWITH(reacher:washer, target:theme,
instr:instr)
It is easy to come up with other concrete vi-
gnettes for wash (washing windows, babies, hands,
dishes...). As mentioned in section 3.2 more gen-
eral vignettes can be defined for very broad object
classes. In choosing vignettes, the most specific will
be used (looking at type matching hierarchies), so
general vignettes will only be chosen when more
specific ones are unavailable. The following generic
vignette describes washing any large object.
WASH-LARGE-OBJECT(washer, theme instrument)
washer=PERSON, theme=OBJECT,
instrument=SPONGE,
SIZEOF(figure:theme, size:large)
FACING(figure:washer, ground:theme)
GRASP(grasper:washer, theme:instrument)
REACH(reacher:washer, target:theme)
33
In our final example, a vignette for picking fruit uses
the following assertion of world knowledge about
particular types of fruit and the trees they come
from:
SOURCE-OF(theme:x, source:y), APPLE(self:x),
APPLETREE(self:y)
In matching the vignette to the verb frame and its ar-
guments, the source frame element is bound to the
type of tree for the given theme (fruit).
PICK-FRUIT(picker, theme, source)
picker=PERSON, theme=FRUIT, source=TREE,
SOURCEOF(theme:theme, source:source)
UNDERCANOPY(figure:picker, canopy:source)
GRASP(grasper:picker, theme:theme)
REACH(reacher:picker, target:source.branch)
5 VigNet
We are developing VigNet as a general purpose re-
source, but with the specific goal of using it in text-
to-scene generation. In this section we first describe
various methods to populate VigNet. We then sketch
how we create graphical representations from Vi-
gNet meaning representations.
5.1 Populating VigNet
VigNet is being populated using several approaches:
? Amazon Mechanical Turk is being used to ac-
quire scene elements for location and action vi-
gnettes as well as the spatial relations among
those elements. For locations, Turkers are
shown representative pictures of different lo-
cations as well as variants of similar locations,
thereby providing distinct vignettes for each lo-
cation. We also use Mechanical Turk to acquire
general purpose relational information for ob-
jects and actions such as default locations, ma-
terials, contents, and parts.
? We extract relations such as typical locations
for actions from corpora based on co-occurance
patterns of location and action terms. This is
based on ideas described in (Sproat, 2001). We
also rely on corpora to induce new lexical units
and selectional preferences.
? A large set of semantic nodes and frames for
nouns has been imported from the noun lexicon
of the WordsEye text-to-scene system (Coyne
and Sproat, 2001). This lexicon currently con-
tains 15,000 lexical items and is tied to a li-
brary of 2,200 3D objects and 10,000 images
Semantic relations between these nodes include
parthood, containment, size, style (e.g. antique
or modern), overall shape, material, as well as
spatial tags denoting important spatial regions
on the object. We also import graphically-
oriented vignettes from WordsEye. These are
used to capture the meaning of sub-lexical 3D
objects such as the mounted goat head de-
scribed earlier.
? Finally, we intend to use WordsEye itself to al-
low users to visualize vignettes as they define
them, as a way to improve vignette accuracy
and relevancy to the actual use of the system.
While the population of VigNet is not the fo-
cus of this paper, it is our goal to create a usable
resource that can be populated with a reasonable
amount of effort. We note that opposed to resources
like FrameNet that require skilled lexicographers,
we only need simple visual annotation that can eas-
ily be done by untrained Mechanical Turkers. In
addition, as described in section 3.2, vignettes de-
fined at more abstract levels of the frame hierar-
chy can be used and composed to cover large num-
bers of frames in a plausible manner. This allows
more specific vignettes to be defined where the dif-
ferences are most significant. VigNet is is focused
on visually-oriented language involving tangible ob-
jects. However, abstract, process-oriented language
and relations such as negation can be depicted icon-
ically with general vignettes. Examples of these can
be seen in the figurative and metaphorical depictions
shown in (Coyne and Sproat, 2001).
5.2 Using VigNet in Text-to-Scene Generation
To compose a scene from text input such as the
man is washing the apple it is necessary to parse
the sentence into a semantic representation (evoking
frames for each content word) and to then resolve
the language-level semantics to a set of graphical
entities and relations. To create a low-level graph-
ical representation all frame elements need to be
filled with appropriate semantic nodes. Frames sup-
port the selection of these nodes by specifying con-
straints on them using selectional restrictions. The
34
SUBFRAME-PARALLEL decomposition of vignettes
then ultimately relates these nodes using elementary
spatial vignettes (FRONTOF, ON, ...).
Note that it is possible to describe scenes directly
using these vignettes (such as The man is in front of
the sink. He is holding an apple.), as was used to
create the mock-ups in figure 1.
Vignettes can be directly applied or composed to-
gether. Composing vignettes involves unifying their
frame elements. For example, in washing an ap-
ple, the WASH-SMALL-FRUIT vignette uses a sink.
From world knowledge we know (via instances of
the TYPICAL-LOCATION frame) that washing food
typically takes place in the KITCHEN. To create a
scene we compose the two vignettes together by uni-
fying the sink in the location vignette with the sink
in the action vignette.
6 Related Work
The grounding of natural language to graphical re-
lations has been investigated in very early text-to-
scene systems (Boberg, 1972), (Simmons, 1975),
(Kahn, 1979), (Adorni et al, 1984), and then later
in Put (Clay and Wilhelms, 1996), and WordsEye
(Coyne and Sproat, 2001). Other systems, such as
CarSim (Dupuy et al, 2001), Jack (Badler et al,
1998), and CONFUCIUS (Ma and McKevitt, 2006)
target animation and virtual environments rather
than scene construction. A graphically grounded
lexical-semantic resource such as VigNet would be
of use to these and related domains. The concept of
vignettes as graphical realizations of more general
frames was introduced in (Coyne et al, 2010).
In addition to FrameNet, much work has been
done in developing theories and resources for lexi-
cal semantics and common-sense knowledge. Verb-
Net (Kipper et al, 2000) focuses on verb subcat pat-
terns grouped by Levin verb classes (Levin, 1993),
but also grounds verb semantics into a small num-
ber of causal primitives representing temporal con-
straints tied to causality and state changes. VerbNet
lacks the ability to compose semantic constraints
or use arbitrary semantic relations in those con-
straints. Conceptual Dependency theory (Schank
and Abelson, 1977) specifies a small number of
state-change primitives into which all verbs are re-
duced. Event Logic (Siskind, 1995) decomposes ac-
tions into intervals describing state changes and al-
lows visual grounding by specifying truth conditions
for a small set of spatial primitives (a similar for-
malism is used by Ma and McKevitt (2006)). (Bai-
ley et al, 1998) and related work proposes a rep-
resentation in many ways similar to ours, in which
lexical items are paired with a detailed specifica-
tion of actions in terms of elementary body poses
and movements. In contrast to these temporally-
oriented approaches, VigNet grounds semantics in
spatial constraints active at a single moment in time.
This allows for and emphasizes contextual reason-
ing rather than causal reasoning. In addition, VigNet
emphasizes a holistic frame semantic perspective,
rather than emphasizing decomposition alone. Sev-
eral resources for common-sense knowledge exist or
have been proposed. In OpenMind and ConceptNet
(Havasi et al, 2007) online crowd-sourcing is used
to collect a large set of common-sense assertions.
These assertions are normalized into a set of a cou-
ple dozen relations. The Cyc project is using the web
to augment its large ontology and knowledge base of
common sense knowledge (Matuszek et al, 2005).
PRAXICON (Pastra, 2008) is a grounded concep-
tual resources that integrates motor-sensoric, visual,
pragmatic and lexical knowledge (via WordNet). It
targets the embodied robotics community and does
not directly focus on scene generation. It also fo-
cuses on individual lexical items, while VigNet, like
FrameNet, takes syntactic context into account.
7 Conclusion
We have described a new semantic paradigm that we
call vignette semantics. Vignettes are extensions of
FrameNet frames and represent the specific ways in
which semantic frames can be realized in the world.
Mapping frames to vignettes involves translating be-
tween high-level frame semantics and the lower-
level relations used to compose a scene. Knowledge
about objects, both in terms of their semantic types
and the affordances they provide is used to make that
translation. FrameNet frames, coupled with seman-
tic nodes representing entity classes, provide a pow-
erful relational framework to express such knowl-
edge. We are developing a new resource VigNet
which will implement this framework and be used
in our text-to-scene generation system.
35
References
G. Adorni, M. Di Manzo, and F. Giunchiglia. 1984. Nat-
ural Language Driven Image Generation. In Proceed-
ings of COLING 1984, pages 495?500, Stanford, CA.
N. Badler, R. Bindiganavale, J. Bourne, M. Palmer, J. Shi,
and W. Schule. 1998. A parameterized action rep-
resentation for virtual human agents. In Workshop
on Embodied Conversational Characters, Tahoe City,
CA.
D. Bailey, N. Chang, J. Feldman, and S. Narayanan.
1998. Extending Embodied Lexical Development. In
Proceedings of the Annual Meeting of the Cognitive
Science Society, Madison, WI.
C. Baker, C. Fillmore, and J. Lowe. 1998. The Berkeley
Framenet Project. In Proceedings of COLING 1998,
pages 86?90.
R. Boberg. 1972. Generating line drawings from ab-
stract scene descriptions. Master?s thesis, Dept. of
Elec. Eng, MIT, Cambridge, MA.
S. R. Clay and J. Wilhelms. 1996. Put: Language-based
interactive manipulation of objects. IEEE Computer
Graphics and Applications, 16(2):31?39.
B. Coyne and R. Sproat. 2001. WordsEye: An automatic
text-to-scene conversion system. In Proceedings of
the Annual Conference on Computer Graphics, pages
487?496, Los Angeles, CA.
B. Coyne, O. Rambow, J. Hirschberg, and R. Sproat.
2010. Frame Semantics in Text-to-Scene Generation.
In Proceedings of the KES?10 workshop on 3D Visual-
isation of Natural Language, Cardiff, Wales.
S. Dupuy, A. Egges, V. Legendre, and P. Nugues. 2001.
Generating a 3D Simulation Of a Car Accident from a
Written Description in Natural Language: The CarSim
System. In Proceedings of ACL Workshop on Tem-
poral and Spatial Information Processing, pages 1?8,
Toulouse, France.
C. J. Fillmore. 1982. Frame semantics. In Linguistic
Society of Korea, editor, Linguistics in the Morning
Calm, pages 111?137. Hanshin Publishing Company,
Seoul.
C. Havasi, R. Speer, and J. Alonso. 2007. ConceptNet 3:
a Flexible, Multilingual Semantic Network for Com-
mon Sense Knowledge. In Proceedings of RANLP
2007, Borovets, Bulgaria.
K. Kahn. 1979. Creation of Computer Animation from
Story Descriptions. Ph.D. thesis, MIT, AI Lab, Cam-
bridge, MA.
H. Kamp. 1981. A Theory of Truth and Semantic Rep-
resentation. In Groenendijk, J. and Janssen, T. and
Stokhof, M., editor, Formal Methods in the Study of
Language, pages 277?322. de Gruyter, Amsterdam.
K. Kipper, H. T. Dang, and M. Palmer. 2000. Class-
Based Construction of a Verb Lexicon. In Proceedings
of AAAI 2000, Austin, TX.
B. Levin. 1993. English verb classes and alternations:
a preliminary investigation. University Of Chicago
Press.
M. Ma and P. McKevitt. 2006. Virtual human anima-
tion in natural language visualisation. Artificial Intel-
ligence Review, 25:37?53, April.
C. Matuszek, M. Witbrock, R. C. Kahlert, J. Cabral,
D. Schneider, P. Shah, and D. Lenat. 2005. Search-
ing for Common Sense: Populating Cyc from the Web.
In Proceedings of AAAI 2005, pages 1430?1435, Pitts-
burgh, PA.
K. Pastra. 2008. PRAXICON: The Development of a
Grounding Resource. In Proceedings of the Interna-
tional Workshop on Human-Computer Conversation,
Bellagio, Italy.
J. Ruppenhofer, M. Ellsworth, M. Petruck, C. R. John-
son, and J. Scheffczyk. 2010. Framenet II: Extended
Theory and Practice. ICSI Berkeley.
R. C. Schank and R. Abelson. 1977. Scripts, Plans,
Goals, and Understanding. Earlbaum, Hillsdale, NJ.
R. Simmons. 1975. The CLOWNS Microworld. In Pro-
ceedings of the Workshop on Theoretical Issues in Nat-
ural Language Processing, pages 17?19, Cambridge,
MA.
J. M. Siskind. 1995. Grounding language in perception.
Artificial Intelligence Review, 8:371?391.
R. Sproat. 2001. Inferring the environment in a text-to-
scene conversion system. In International Conference
on Knowledge Capture, Victoria, BC.
36
Proceedings of the 2014 Workshop on the Use of Computational Methods in the Study of Endangered Languages, pages 6?14,
Baltimore, Maryland, USA, 26 June 2014.
c?2014 Association for Computational Linguistics
Documenting Endangered Languages with the WordsEye Linguistics Tool
Morgan Ulinski
?
mulinski@cs.columbia.edu
Anusha Balakrishnan
?
ab3596@columbia.edu
Daniel Bauer
?
bauer@cs.columbia.edu
Bob Coyne
?
coyne@cs.columbia.edu
Julia Hirschberg
?
julia@cs.columbia.edu
Owen Rambow
?
rambow@ccls.columbia.edu
?
Department of Computer Science
?
CCLS
Columbia University
New York, NY, USA
Abstract
In this paper, we describe how field lin-
guists can use the WordsEye Linguistics
Tool (WELT) to study endangered lan-
guages. WELT is a tool under devel-
opment for eliciting endangered language
data and formally documenting a lan-
guage, based on WordsEye (Coyne and
Sproat, 2001), a text-to-scene generation
tool that produces 3D scenes from text in-
put. First, a linguist uses WELT to create
elicitation materials and collect language
data. Next, he or she uses WELT to for-
mally document the language. Finally, the
formal models are used to create a text-
to-scene system that takes input in the en-
dangered language and generates a picture
representing its meaning.
1 Introduction
Although languages have appeared and disap-
peared throughout history, today languages are
facing extinction at an unprecedented pace. Over
40% of the estimated 7,000 languages in the world
are at risk of disappearing. When languages die,
we lose access to an invaluable resource for study-
ing the culture, history, and experience of people
who spoke them (Alliance for Linguistic Diversity,
2013). Efforts to document languages and develop
tools to support these efforts become even more
important with the increasing rate of extinction.
Bird (2009) emphasizes a particular need to make
use of computational linguistics during fieldwork.
To address this issue, we are developing the
WordsEye Linguistics Tool, WELT. In one mode
of operation, we provide field linguists with tools
for building elicitation sessions based on custom
3D scenes. In another, we provide a way to for-
mally document the endangered language. For-
mal hypotheses can be verified using a text-to-
scene system that takes input in the endangered
language, analyzes it based on the formal model,
and generates a picture representing the meaning.
WELT provides important advantages to field
linguists for elicitation over the current practice of
using a set of pre-fabricated static pictures. Using
WELT the linguist can create and modify scenes
in real time, based on informants? responses, cre-
ating follow-up questions and scenes to support
them. Since the pictures WELT supports are 3D
scenes, the viewpoint can easily be changed, al-
lowing exploration of linguistic descriptions based
on different frames of reference, as for elicitations
of spatial descriptions. Finally, since scenes and
objects can easily be added in the field, the lin-
guist can customize the images used for elicitation
to be maximally relevant to the current informants.
Creating a text-to-scene system for an endan-
gered language with WELT also has advantages.
First, WELT allows documentation of the seman-
tics of a language in a formal way. Linguists can
customize the focus of their studies to be as deep
or shallow as they wish; however, we believe that a
major advantage of documenting a language with
WELT is that it enables studies that are much more
precise. The fact that a text-to-scene system is cre-
ated from this documentation will allow linguists
to test the theories they develop with native speak-
ers, making changes to grammars and semantics
in real time. The resulting text-to-scene system
can also be an important tool for language preser-
vation, spreading interest in the language among
younger generations of the community and re-
cruiting new speakers.
In this paper, we discuss the WELT toolkit and
its intended use, with examples from Arrernte and
Nahuatl. In Section 2 we discuss prior work on
field linguistics computational tools. In Section 3
we present an overview of the WELT system. We
describe using WELT for elicitation in Section 4
and describe the tools for language documentation
in Section 5. We conclude in Section 6.
6
2 Related Work
Computational tools for field linguistics fall into
two categories: tools for native speakers to use
directly, without substantial linguist intervention,
and tools for field linguists to use. Tools intended
for native speakers include the PAWS starter kit
(Black and Black, 2009), which uses the answers
to a series of guided questions to produce a draft
of a grammar. Similarly, Bird and Chiang (2012)
describe a simplified workflow and supporting MT
software that lets native speakers produce useable
documentation of their language on their own.
One of the most widely-used toolkits in the lat-
ter category is SIL FieldWorks (SIL FieldWorks,
2014), or specifically, FieldWorks Language Ex-
plorer (FLEx). FLEx includes tools for elicit-
ing and recording lexical information, dictionary
development, interlinearization of texts, analysis
of discourse features, and morphological analy-
sis. An important part of FLEx is its ?linguist-
friendly? morphological parser (Black and Si-
mons, 2006), which uses an underlying model
of morphology familiar to linguists, is fully in-
tegrated into lexicon development and interlin-
ear text analysis, and produces a human-readable
grammar sketch as well as a machine-interpretable
parser. The morphological parser is constructed
?stealthily? in the background, and can help a lin-
guist by predicting glosses for interlinear texts.
Linguist?s Assistant (Beale, 2011) provides a
corpus of semantic representations for linguists to
use as a guide for elicitation. After eliciting the
language data, a linguist writes rules translating
these semantic representations into surface forms.
The result is a description of the language that can
be used to generate text from documents that have
been converted into the semantic representation.
Linguists are encouraged to collect their own elic-
itations and naturally occurring texts and translate
them into the semantic representation.
The LinGO Grammar Matrix (Bender et al.,
2002) facilitates formal modeling of syntax by
generating basic HPSG ?starter grammars? for
languages from the answers to a typological ques-
tionnaire. Extending a grammar beyond the proto-
type, however, does require extensive knowledge
of HPSG, making this tool more feasibly used by
grammar engineers and computational linguists.
For semantics, the most common resource for for-
mal documentation across languages is FrameNet
(Filmore et al., 2003); FrameNets have been de-
veloped for many languages, including Spanish,
Japanese, and Portuguese. However, FrameNet is
also targeted toward computational linguists.
In general, we also lack tools for creating cus-
tom elicitation materials. With WELT, we hope to
fill some of the gaps in the range of available field
linguistics tools. WELT will enable the creation of
custom elicitation material and facilitate the man-
agement sessions with an informant. WELT will
also enable formal documentation of the semantics
of a language without knowledge of specific com-
putational formalisms. This is similar to the way
FLEx allows linguists to create a formal model of
morphology while also documenting the lexicon
of a language and glossing interlinear texts.
3 Overview of WELT Workflow
In this section, we briefly describe the workflow
for using WELT; a visual representation is pro-
vided in Figure 1. Since we are still in the early
stages of our project, this workflow has not been
tested in practice. The tools for scene creation and
elicitation are currently useable, although more
features will be added in the future. The tools for
modeling and documentation are still in develop-
ment; although some functionality has been imple-
mented, we are still testing it with toy grammars.
First, WELT will be used to prepare a set of 3D
scenes to be used to elicit targeted descriptions or
narratives. An important part of this phase will be
the cultural adaptation of the graphical semantics
used in WordsEye, so that scenes will be relevant
to the native speakers a linguist works with. We
will discuss cultural adaptation in more detail in
Section 4.1. Next, the linguist will work with an
informant to generate language data based on pre-
pared 3D scenes. This can be a dynamic process;
as new questions come up, a linguist can easily
modify existing scenes or create new ones. WELT
also automatically syncs recorded audio with open
scenes and provides an interface for the linguist to
write notes, textual descriptions, and glosses. We
will discuss creating scenes and eliciting data with
WELT in Section 4.2. After the elicitation session,
the linguist can use WELT to review the data col-
lected, listen to the audio recorded for each scene,
and revise notes and glosses. The linguist can then
create additional scenes to elicit more data or be-
gin the formal documentation of the language.
Creating a text-to-scene system with WELT re-
quires formal models of the morphology, syntax,
7
Define	 ?Lexicon	 ?
Cultural	 ?
Adapta?on	 ?of	 ?
VigNet	 ?
Create	 ?Scenes	 ? Collect	 ?Data	 ?from	 ?informant	 ?
Clean-??up	 ?notes/
glosses	 ?
Modify	 ?&	 ?add	 ?vigne?es	 ?
Define	 ?syntax	 ?to	 ?seman?cs	 ?rules	 ?
Define	 ?Morphology	 ?
Define	 ?Syntax	 ?
L2	 ?
Lexicon	 ?
L2	 ?Syntax-??
Seman?cs	 ?rules	 ?VigNet	 ?Resources 
Output	 ?&	 ?
Collabora?on	 ? Prepare	 ?L2	 ?scenes	 ?
Verify	 ?with	 ?
informant	 ?
XLE	 ? FieldWorks	 ?Tools WELT	 ?
Figure 1: WELT workflow
and semantics of a language. Since the focus
of WELT is on semantics, the formalisms used
to model morphology and syntax may vary. We
are using FieldWorks to document Nahuatl mor-
phology, XFST (Beesley and Karttunen, 2003) to
model Arrernte morphology, and XLE (Crouch et
al., 2011) to model syntax in the LFG formal-
ism (Kaplan and Bresnan, 1982). We will provide
tools to export WELT descriptions and glosses
into FLEx format and to export the lexicon cre-
ated during documentation into FLEx and XLE.
WELT will provide user interfaces for modeling
the syntax-semantics interface, lexical semantics,
and graphical semantics of a language. We will
discuss these in more detail in Section 5.3.
Once models of morphology, syntax, and se-
mantics are in place (note that these can be work-
ing models, and need not be complete), WELT
puts the components together into a text-to-scene
system that takes input in the endangered language
and uses the formal models to generate pictures.
This system can be used to verify theories with in-
formants and revise grammars. As new questions
arise, WELT can also continue to be used to create
elicitation materials and collect linguistic data.
Finally, we will create a website for WELT so
linguists can share resources such as modified ver-
sions of VigNet, 3D scenes, language data col-
lected, and formal grammars. This will allow
comparison of analyses across languages, as well
as facilitate the documentation of other languages
that are similar linguistically or spoken by cul-
turally similar communities. In addition, sharing
the resulting text-to-scene systems with a wider
audience can generate interest in endangered lan-
guages and, if shared with endangered-language-
speaking communities, encourage younger mem-
bers of the community to use the language.
4 Elicitation with WELT
WELT organizes elicitation sessions around a set
of 3D scenes, which are created by inputting En-
glish text into WordsEye. Scenes can be imported
and exported between sessions, so that useful
scenes can be reused and data compared. WELT
also provides tools for recording audio (which is
automatically synced with open scenes), textual
descriptions, glosses, and notes during a session.
Screenshots are included in Figure 2.
4.1 Cultural Adaptation of VigNet
To interpret input text, WordsEye uses VigNet
(Coyne et al., 2011), a lexical resource based on
FrameNet (Baker et al., 1998). As in FrameNet,
lexical items are grouped in frames according to
shared semantic structure. A frame contains a set
of frame elements (semantic roles). FrameNet de-
fines the mapping between syntax and semantics
for a lexical item with valence patterns that map
syntactic functions to frame elements.
VigNet extends FrameNet in order to capture
?graphical semantics?, a set of graphical con-
straints representing the position, orientation, size,
color, texture, and poses of objects in the scene,
8
Figure 2: Screenshots of WELT elicitation interfaces
which is used to construct and render a 3D
scene. Graphical semantics are added to frames by
adding primitive graphical (typically, spatial) rela-
tions between frame element fillers. VigNet distin-
guishes between meanings of words that are dis-
tinguished graphically. For example, the specific
objects (e.g., implements) and spatial relations in
the graphical semantics for cook depend on the
object being cooked and on the culture in which
it is being cooked (cooking turkey in Baltimore
vs. cooking an egg in Alice Springs), even though
at an abstract level cook an egg in Alice Springs
and cook a turkey in Baltimore are perfectly com-
positional semantically. Frames augmented with
graphical semantics are called vignettes.
Vignette Tailoring: Without digressing into a dis-
cussion on linguistic relativity, we assume that
large parts of VigNet are language- and culture-
independent. The low-level graphical relations
used to express graphical semantics are based on
physics and human anatomy and do not depend on
language. However, the graphical semantics for a
vignette may be culture-specific, and some new vi-
gnettes will need to be added for a culture. In the
U.S., for example, the sentence The woman boiled
the water might invoke a scene with a pot of wa-
ter on a stove in a kitchen. Among the Arrernte
people, it would instead invoke a woman sitting
on the ground in front of a kettle on a campfire.
Figure 3 shows an illustration from the Eastern
and Central Arrernte Picture Dictionary (Broad,
2008) of the sentence Ipmenhe-ipmenhele kwatye
urinpe-ilemele iteme, ?My grandmother is boiling
the water.? The lexical semantics for the English
verb boil and the Arrente verb urinpe-ileme are
the same, the relation APPLY-HEAT.BOIL. How-
ever, the vignettes map to different, culture-typical
graphical semantics. The vignettes for our exam-
ple are shown in Figure 4.
Figure 3: Illustration from Broad (2008).
To handle cultural differences like these, a lin-
guist will use WELT to extend VigNet with new
9
Figure 4: Vignettes for the woman boils the water.
The high-level semantics of APPLY-HEAT.BOIL
are decomposed into sets of objects and primitive
graphical relations that depend on cultural context.
graphical semantics for existing vignettes that
need to be modified, and new vignettes for scenar-
ios not already covered. We will create interfaces
so that VigNet can easily be adapted.
Custom WordsEye Objects: Another way to
adapt WordsEye to a culture or region is to add rel-
evant 3D objects to the database. WordsEye also
supports 2D-cutout images, which is an easy way
to add new material without 3D modeling. We
have created a corpus of 2D and 3D models for
WordsEye that are specifically relevant to aborig-
inal speakers of Arrernte, including native Aus-
tralian plants and animals and culturally relevant
objects and gestures. Many of the pictures we cre-
ated are based on images from IAD Press, used
with permission, which we enhanced and cropped
in PhotoShop. Some scenes that use these images
are included in Figure 5. Currently, each new ob-
ject has to be manually incorporated into Words-
Eye, but we will create tools to allow WELT users
to easily add pictures and objects.
New objects will also need to be incorporated
into the semantic ontology. VigNet?s ontology
consists of semantic concepts that are linked to-
gether with ISA relations. The ontology supports
multiple inheritance, allowing a given concept to
be a sub-type of more than one concept. For exam-
ple, a PRINCESS.N is a subtype of both FEMALE.N
and ARISTOCRAT.N, and a BLACK-WIDOW.N is a
subtype of SPIDER.N and POISONOUS-ENTITY.N.
Concepts are often linked to corresponding lexi-
cal items. If a lexical item has more than one
word sense, the different word senses would be
represented by different concepts. In addition, ev-
ery graphical object in VigNet is represented by
a unique concept. For example, a particular 3D
model of a dog would be a linked to the general
DOG.N concept by the ISA relation. The semantic
concepts in VigNet include the graphical objects
available in WordsEye as well as concepts tied to
related lexical items. While WordsEye might only
have a handful of graphical objects for dogs, Vi-
gNet will have concepts representing all common
types of dogs, even if there is no graphical object
associated with them. We will provide interfaces
both for adding new objects and for modifying the
semantic concepts in VigNet to reflect the differ-
ing lexical semantics of a new language.
4.2 Preparing Scenes and Eliciting Data
The next step in the workflow is the preparation
of scenes and elicitation of descriptions. To test
creating elicitation materials with WELT, we built
a set of scenes based on the Max Planck topolog-
ical relations picture series (Bowerman and Ped-
erson, 1992). In creating these, we used a feature
of WordsEye that allows highlighting specific ob-
jects (or parts of objects) in a scene. We used these
scenes to elicit descriptions from a native Nahuatl
speaker; some examples are included in Figure 6.
(a) in tapamet? t?atsakwa se kali
the fence/wall around the house
(b) in tsopelik katsekotok t?atsint?a in t?apetS
the candy sticking under the table
Figure 6: Nahuatl examples elicited with WELT
One topic we will explore with WELT is the re-
lationship in Arrernte between case and semantic
interpretation of a sentence. It is possible to signif-
icantly alter a sentence?s meaning by changing the
case on an argument. For example, the sentences
in (1) from Wilkins (1989) show that adding dative
10
Figure 5: WordsEye scenes using custom 2D gum tree and dingo from our corpus
case to the direct object of the sentence changes
the meaning from shooting and hitting the kanga-
roo to shooting at the kangaroo and not hitting it.
Wilkins calls this the ?dative of attempt.?
(1) a. re aherre tyerre-ke
he kangaroo shot-pc
He shot the kangaroo.
b. re aherre-ke tyerre-ke
he kangaroo-DAT shot-pc
He shot at the kangaroo (but missed).
In order to see how this example generalizes,
we will create pairs of pictures, one in which the
object of the sentence is acted upon, and one in
which the object fails to be acted upon. Figure 7
shows a pair of scenes contrasting an Australian
football player scoring a goal with a player aiming
at the goal but missing the shot. Sentences (2) and
(3) are two ways of saying ?score a goal? in Ar-
rernte; we want to see if a native Arrernte speaker
would use goal-ke in place of goal in this context.
(2) artwe le goal arrerne-me
man ERG goal put-NP
The man kicks a goal.
(3) artwe le goal kick-eme-ile-ke
man ERG goal kick-VF-TV-PST
The man kicked a goal.
5 Modeling a Language with WELT
WELT includes tools for documenting the seman-
tics of the language. It also uses this documenta-
tion to automatically generate a text-to-scene sys-
tem for the language. Because WELT is centered
around the idea of 3D scenes, the formal docu-
mentation will tend to focus on the parts of the se-
mantics that can be represented graphically. Note
that this can include figurative concepts as well,
although the visual representation of these may be
culture-specific. However, linguists do not need
to be limited by the graphical output; WELT can
be used to document other aspects of semantics as
well, but linguists will not be able to verify these
theories using the text-to-scene system.
To explain the necessary documentation, we
briefly describe the underlying architecture of
WordsEye, and how we are adapting it to sup-
port text-to-scene systems for other languages.
The WordsEye system parses each input sentence
into a labeled syntactic dependency structure, then
converts it into a lexical-semantic structure using
lexical valence patterns and other lexical and se-
mantic information. The resulting set of seman-
tic relations is converted to a ?graphical seman-
tics?, the knowledge needed to generate graphical
scenes from language.
To produce a text-to-scene system for a new lan-
guage, WELT must replace the English linguistic
processing modules with models for the new lan-
guage. The WELT processing pipeline is illus-
trated in Figure 8, with stages of the pipeline on
top and required resources below. In this section,
we will discuss creating the lexicon, morphologi-
cal and syntactic parsers, and syntax-to-semantics
rules. The vignettes and 3D objects will largely
have been done during cultural adaptation of Vi-
gNet; additional modifications needed to handle
the semantics can be defined using the same tools.
5.1 The Lexicon
The lexicon in WELT is a list of word forms
mapped to semantic concepts. The process of
building the lexicon begins during elicitation.
WELT?s elicitation interface includes an option to
display each object in the scene individually be-
fore progressing to the full scene. When an object
is labeled and glossed in this way, the word and
the semantic concept represented by the 3D ob-
ject are immediately added to the lexicon. Word
forms glossed in scene descriptions will also be
added to the lexicon, but will need to be mapped
to semantic concepts later. WELT will provide
11
Figure 7: WordsEye scenes to elicit the ?dative of attempt.?
Morph	 ? Lexical	 ?Seman?cs	 ?
Graphical	 ?
Seman?cs	 ? Scene	 ?
Input	 ?
Text	 ?
Processing	 ?Pipeline	 ?
VigNet	 ?
Vigne?s	 ? 2D/3D	 ?objects	 ?Lexicon	 ?
Syntax	 ?
Morphological	 ?
Analyzer	 ?
Syntac?c	 ?
Parser	 ?
Syntax-??Seman?cs	 ?
Rules	 ?
Figure 8: WELT architecture
tools for completing the lexicon by modifying
the automatically-added items, adding new lexical
items, and mapping each lexical item to a seman-
tic concept in VigNet. Figure 9(a) shows a partial
mapping of the nouns in our Arrernte lexicon.
WELT includes a visual interface for search-
ing VigNet?s ontology for semantic concepts and
browsing through the hierarchy to select a partic-
ular category. Figure 9(b) shows a portion of the
ontology that results from searching for cup. Here,
we have decided to map panikane to CUP.N. Se-
mantic categories are displayed one level at a time,
so initially only the concepts directly above and
below the search term are shown. From there, it is
simple to click on relevant concepts and navigate
the graph to find an appropriate semantic category.
To facilitate the modeling of morphology and syn-
tax, WELT will also export the lexicon into for-
mats compatible with FieldWorks and XLE, so the
list of word forms can be used as a starting point.
5.2 Morphology and Syntax
As mentioned earlier, the focus of our work on
WELT is on modeling the interface between syn-
tax, lexical semantics, and graphical semantics.
Therefore, although WELT requires models of
morphology and syntax to generate a text-to-scene
system, we are relying on third-party tools to build
those models. For morphology, a very good tool
already exists in FLEx, which allows the creation
Lexical VigNet
Item Concept
artwe PERSON.N
panikane CUP.N
angepe CROW.N
akngwelye DOG.N
tipwele TABLE.N
(a) (b)
Figure 9: (a) Arrernte lexical items mapped to Vi-
gNet concepts; (b) part of the VigNet ontology
of a morphological parser without knowledge of
any particular grammatical formalism. For syn-
tax, we are using XLE for our own work while
researching other options that would be more ac-
cessible to non-computational linguists. It is im-
portant to note, though, that the modeling done in
WELT does not require a perfect syntactic parser.
In fact, one can vastly over-generate syntax and
still accurately model semantics. Therefore, the
syntactic grammars provided as models do not
need to be complex. However, the question of syn-
tax is still an open area of research in our project.
5.3 Semantics
To use the WordsEye architecture, the system
needs to be able to map between the formal syntax
of the endangered language and a representation of
semantics compatible with VigNet. To accomplish
12
Figure 10: Creating syntax-semantics rules in WELT
this, WELT includes an interface for the linguist to
specify a set of rules that map from syntax to (lex-
ical) semantics. Since we are modeling Arrernte
syntax with LFG, the rules currently take syntactic
f-structures as input, but the system could easily be
modified to accommodate other formalisms. The
left-hand side of a rule consists of a set of con-
ditions on the f-structure elements and the right-
hand side is the desired semantic structure. Rules
are specified by defining a tree structure for the
left-hand (syntax) side and a DAG for the right-
hand (semantics) side.
As an example, we will construct a rule to
process sentence (2) from Section 4.2, artwe le
goal arrerneme. For this sentence, our Arrernte
grammar produces the f-structure in Figure 11.
We create a rule that selects for predicate ar-
rerne with object goal and any subject. Figure
10 shows the construction of this rule in WELT.
Note that var-1 on the left-hand side becomes
VIGNET(var-1) on the right-hand side; this in-
dicates that the lexical item found in the input is
mapped into a semantic concept using the lexicon.
Figure 11: F-structure for sentence 2, Section 4.2.
The rule shown in Figure 10 is a very sim-
ple example. Nodes on the left-hand side of
the rule can also contain boolean logic, if we
wanted to allow the subject to be [(artwe ?man? OR
arhele ?woman?) AND NOT ampe ?child?]. Rules
need not specify lexical items directly but may
refer to more general semantic categories. For
example, our rule could require a particular se-
mantic category for VIGNET(var-1), such as
ANIMATE-BEING.N. These categories are chosen
through the same ontology browser used to cre-
ate the lexicon. Finally, to ensure that our sen-
tence can be converted into graphics, we need
to make sure that a vignette definition exists for
CAUSE MOTION.KICK so that the lexical seman-
tics on the right-hand side of our rule can be aug-
mented with graphical semantics; the vignette def-
inition is given in Figure 12. The WordsEye sys-
tem will use the graphical constraints in the vi-
gnette to build a scene and render it in 3D.
Figure 12: Vignette definition for
CAUSE MOTION.KICK
6 Summary
We have described a novel tool under develop-
ment for linguists working with endangered lan-
guages. It will provide a new way to elicit data
from informants, an interface for formally docu-
menting the lexical semantics of a language, and
allow the creation of a text-to-scene system for any
language. In this paper, we have focused specifi-
cally on the workflow that a linguist would fol-
low while studying an endangered language with
WELT. WELT will provide useful tools for field
linguistics and language documentation, from cre-
ating elicitation materials, to eliciting data, to for-
mally documenting a language. In addition, the
text-to-scene system that results from document-
ing an endangered language with WELT will be
valuable for language preservation, generating in-
terest in the wider world, as well as encouraging
younger members of endangered language com-
munities to use the language.
Acknowledgments
This material is based upon work supported by
the National Science Foundation under Grant No.
1160700.
13
References
Alliance for Linguistic Diversity. 2013. The En-
dangered Languages Project. http://www.
endangeredlanguages.com.
C. Baker, J. Fillmore, and J. Lowe. 1998. The Berkeley
FrameNet project. In 36th Meeting of the Associa-
tion for Computational Linguistics and 17th Inter-
national Conference on Computational Linguistics
(COLING-ACL?98), pages 86?90, Montr?eal.
Stephen Beale. 2011. Using Linguist?s Assistant for
Language Description and Translation. In IJCNLP
2011 System Demonstrations, pages 5?8.
Kenneth R. Beesley and Lauri Karttunen. 2003. Finite-
State Morphology Homepage. http://www.
fsmbook.com.
E. Bender, D. Flickinger, and S. Oepen. 2002. The
Grammar Matrix. In J. Carroll, N. Oostdijk, and
R. Sutcliffe, editors, Workshop on Grammar En-
gineering and Evaluation at the 19th International
Conference on Computational Linguistics, pages 8?
14, Taipei, Taiwan.
S. Bird and D. Chiang. 2012. Machine translation for
language preservation. In COLING 2012: Posters,
pages 125?134, Mumbai, December.
S. Bird. 2009. Natural language processing and
linguistic fieldwork. Computational Linguistics,
35(3):469?474.
Cheryl A Black and H Andrew Black. 2009. PAWS:
Parser and Writer for Syntax. In SIL Forum for Lan-
guage Fieldwork 2009-002.
H.A. Black and G.F. Simons. 2006. The SIL Field-
Works Language Explorer approach to morphologi-
cal parsing. In Computational Linguistics for Less-
studied Languages: Texas Linguistics Society 10,
Austin, TX, November.
M. Bowerman and E. Pederson. 1992. Topological re-
lations picture series. In S. Levinson, editor, Space
stimuli kit 1.2, page 51, Nijmegen. Max Planck In-
stitute for Psycholinguistics.
N. Broad. 2008. Eastern and Central Arrernte Picture
Dictionary. IAD Press.
B. Coyne and R. Sproat. 2001. WordsEye: An au-
tomatic text-to-scene conversion system. In SIG-
GRAPH.
B. Coyne, D. Bauer, and O. Rambow. 2011. Vignet:
Grounding language in graphics using frame seman-
tics. In ACL Workshop on Relational Models of Se-
mantics (RELMS), Portland, OR.
D. Crouch, M. Dalrymple, R. Kaplan, T. King,
J. Maxwell, and P. Newman. 2011. XLE Doc-
umentation. http://www2.parc.com/isl/
groups/nltt/xle/doc/xle_toc.html.
C. Filmore, C. Johnson, and M. Petruck. 2003. Back-
ground to FrameNet. In International Journal of
Lexicography, pages 235?250.
R.M. Kaplan and J.W. Bresnan. 1982. Lexical-
functional grammar: A formal system for grammat-
ical representation. In J.W. Bresnan, editor, The
Mental Representation of Grammatical Relations.
MIT Press, Cambridge, Mass., December.
SIL FieldWorks. 2014. SIL FieldWorks. http://
fieldworks.sil.org.
D. Wilkins. 1989. Mparntwe Arrernte (Aranda): Stud-
ies in the structure and semantics of grammar. Ph.D.
thesis, Australian National University.
14
