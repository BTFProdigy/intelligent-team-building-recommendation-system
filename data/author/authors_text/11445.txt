Proceedings of the 2009 Workshop on Language Generation and Summarisation, ACL-IJCNLP 2009, pages 15?22,
Suntec, Singapore, 6 August 2009.
c
?2009 ACL and AFNLP
Unsupervised Induction of Sentence Compression Rules
Jo
?
ao Cordeiro
CLT and Bioinformatics
University of Beira Interior
Covilh?a, Portugal
jpaulo@di.ubi.pt
Ga
?
el Dias
CLT and Bioinformatics
University of Beira Interior
Covilh?a, Portugal
ddg@di.ubi.pt
Pavel Brazdil
LIAAD
University of Porto
Porto, Portugal
pbrazdil@liaad.up.pt
Abstract
In this paper, we propose a new unsu-
pervised approach to sentence compres-
sion based on shallow linguistic process-
ing. For that purpose, paraphrase extrac-
tion and alignment is performed over web
news stories extracted automatically from
the web on a daily basis to provide struc-
tured data examples to the learning pro-
cess. Compression rules are then learned
through the application of Inductive Logic
Programming techniques. Qualitative and
quantitative evaluations suggests that this
is a worth following approach, which
might be even improved in the future.
1 Introduction
Sentence compression, simplification or summa-
rization has been an active research subject dur-
ing this decade. A set of approaches involving
machine learning algorithms and statistical mod-
els have been experimented and documented in the
literature and several of these are described next.
1.1 Related Work
In (Knight & Marcu, 2002) two methods were
proposed, one is a probabilistic model - the
noisy channel model - where the probabili-
ties for sentence reduction (P{S
compress
|S)}
1
) are estimated from a training set of 1035
(Sentence, Sentence
compress
) pairs, manually
crafted, while considering lexical and syntacti-
cal features. The other approach learns syntac-
tic tree rewriting rules, defined through four op-
erators: SHIFT, REDUCE DROP and ASSIGN.
Sequences of these operators are learned from the
training set, and each sequence defines a complete
1
In the original paper the P (t|s) notation is used, where t
is the sentence in the target language and s the original sen-
tence in the source language.
transformation from an original sentence to the
compressed version.
In the work of (Le Nguyen & Ho, 2004)
two sentence reduction algorithms were also pro-
posed. The first one is based on template-
translation learning, a method inherited from the
machine translation field, which learns lexical
transformation rules
2
, by observing a set of 1500
(Sentence, Sentence
reduced
) pair, selected from
a news agency and manually tuned to obtain the
training data. Due to complexity difficulties found
for the application of this big lexical ruleset, they
proposed an improvement where a stochastic Hid-
den Markov Model is trained to help in the deci-
sion of which sequence of possible lexical reduc-
tion rules should be applied to a specific case.
An unsupervised approach was included in the
work of (Turner & Charniak, 2005), where train-
ing data are automatically extracted from the Penn
Treebank corpus, to fit a noisy channel model,
similar to the one used by (Knight & Marcu,
2002). Although it seems an interesting approach
to provide new training instances, it still be depen-
dent upon data manually labeled.
More recently, the work of (Clarke & Lapata,
2006) devise a different and quite curious ap-
proach, where the sentence compression task is
defined as an optimization goal, from an Integer
Programming problem. Several constraints are de-
fined, according to language models, linguistic,
and syntactical features. Although this is an unsu-
pervised approach, without using any paralel cor-
pus, it is completely knowledge driven, like a set
of crafted rules and heuristics incorporated into a
system to solve a certain problem.
1.2 Our Proposal
In this paper, we propose a new approach to
this research field, which follows an unsupervised
methodology to learn sentence compression rules
2
Those rules are named there as template-reduction rules.
15
based on shallow linguistic processing. We de-
signed a system composed of four main steps
working in pipeline, where the first three are re-
sponsible for data extraction and preparation and
in the last one the induction process takes place.
The first step gathers web news stories from re-
lated news events collected on a daily basis from
which paraphrases are extracted. In the second
step, word alignment between two sentences of
a paraphrase is processed. In the third step, spe-
cial regions from these aligned paraphrases, called
bubbles, are extracted and conveniently prepro-
cessed to feed the induction process. The whole
sequence is schematized in figure 1.
Figure 1: The Pipeline Architecture.
The induction process generates sentence re-
duction rules which have the following general
structure: L
cond
?X
cond
?R
cond
? suppress(X).
This means that the sentence segment X will
be eliminated if certain conditions hold over left
(L), middle (X) and right (R) segments
3
. In
Figure 2, we present seven different rules which
have been automatically induced from our archi-
tecture. These rules are formed by the conjunc-
tion of several literals, and they define constraints
under which certain sentence subparts may be
deleted, therefore compressing or simplifying the
sentence. The X symbol stands for the segment
3
For the sake of simplicity and compact representation,
we will omit the rule consequent, which is always the same
(?? suppress(X)?), whenever a rule is presented.
Z
(X)
= 1 ? L
c
= NP ?X
1
= JJ ?R
1
= IN (1)
Z
(X)
= 1 ? L
c
= NP ?X
1
= RB ?R
1
= IN (2)
Z
(X)
= 2 ? L
1
= and ?X
1
= the ?R
1
= JJ (3)
Z
(X)
= 2 ? L
1
= the ?X
2
= of ?R
1
= NN (4)
Z
(X)
= 2 ? L
1
= the ?X
c
= NP ?R
1
= NN (5)
Z
(X)
= 3 ? L
c
= PP ?X
1
= the ?R
c
= NP (6)
Z
(X)
= 3 ? L
c
= NP ?X
1
= and ?R
2
= V B (7)
Figure 2: Learned Sentence Compression Rules.
to be dropped, L
(?)
and R
(?)
are conditions over
the left and right contexts respectively. The nu-
meric subscripts indicate the positions
4
where a
segment constraint holds and the c subscript stands
for a syntactic chunk type. The Z
(?)
function com-
putes the length of a given segment, by counting
the number of words it contains. For instance, the
first rule means that a word
5
will be eliminated if
we have a NP (Noun Phrase) chunk in the left
context, and a preposition or subordinating con-
junction, in the right context (R
1
= IN ). The rule
also requires that the elimination word must be an
adjective, as we have X
1
= JJ .
This rule would be applied to the following seg-
ment
6
[NP mutual/jj funds/nns information/nn]
[ADJP available/jj] [PP on/in] [NP
reuters.com/nn]
and would delete the word available giving rise
to the simplified segment:
[NP mutual/jj funds/nns information/nn]
[PP on/in] [NP reuters.com/nn].
Comparatively to all existing works, we propose
in this paper a framework capable to extract com-
pression rules in a real world environment. More-
over, it is fully unsupervised as, at any step of the
process, examples do not need to be labeled.
In the remaining of the paper, we will present
the overall architecture which achieves precision
4
The position starts with 1 and is counted from left to
right, on the word segments, except for the left context, where
it is counted reversely.
5
As we have Z
(X)
= 1, the candidate segment size to
eliminate is equal to one.
6
The segment is marked with part-of-speech tags (POS)
and chunked with a shallow parser. Both transformations
were made with the OpenNLP toolkit.
16
values up to 85.72%, correctness up to 4.03 in 5
and utility up to 85.72%.
2 Data Preparation
Creating relevant training sets, with some thou-
sands examples is a difficult task, as well as is the
migration of such a system to process other lan-
guages. Therefore, we propose an unsupervised
methodology to automatically create a training set
of aligned paraphrases, from electronically avail-
able texts on the web. This step is done through
step one and step two of Figure 1, and the details
are described in the next two subsections.
2.1 Paraphrase Extraction
Our system collects web news stories on a daily
basis, and organized them into clusters, which
are exclusively related to different and unique
events, happening each day: ?a company acqui-
sition?, ?a presidential speech?, ?a bomb attack?,
etc. Usually, such clusters contain near 30 small
or medium news articles, collected from differ-
ent media sources. This environment proves to be
very fruitful for paraphrase extraction, since we
have many sentences conveying similar informa-
tion yet written in a different form.
A few unsupervised metrics have been applied
to automatic paraphrase identification and extrac-
tion (Barzilay & Lee, 2003; Dolan et al, 2004).
However, these unsupervised methodologies show
a major drawback by extracting quasi-exact or
even exact match pairs of sentences as they rely
on classical string similarity measures such as the
Edit Distance in the case of (Dolan et al, 2004)
and Word N-gram Overlap for (Barzilay & Lee,
2003). Such pairs are useless for our purpose,
since we aim to identify asymmetrical paraphrase
pairs to be used for sentence compression rule
induction, as explained in (Cordeiro et al, Oct
2007). There we proposed a new metric, the
Sumo-Metric, specially designed for asymmetrical
entailed pairs identification, and proved better per-
formance over previous established metrics, even
in the specific case when tested with the Microsoft
Paraphrase Research Corpus (Dolan et al, 2004),
which contains mainly symmetrical cases. For a
given sentence pair, having each sentence x and
y words, and with ? exclusive links between the
sentences, the Sumo-Metric is defined in Equation
8 and 9.
S(S
a
, S
b
) =
8
>
>
<
>
>
:
S(x, y, ?) if S(x, y, ?) < 1.0
0 if ? = 0
e
?k?S(x,y,?)
otherwise
(8)
where
S(x, y, ?) = ? log
2
(
x
?
) + ? log
2
(
y
?
) (9)
with ?, ? ? [0, 1] and ?+ ? = 1.
We have shown (Cordeiro et al, Oct 2007) that
Sumo-Metric outperforms all state-of-the-art met-
rics over all tested corpora and allows to identify-
ing similar sentences with high probability to be
paraphrases. In Figure 3, we provide the reader
with an example of an extracted paraphrase.
(1) To the horror of their fans, Miss Ball
and Arnaz were divorced in 1960.
(2) Ball and Arnaz divorced in 1960.
Figure 3: An Assymetrical Paraphrase
2.2 Paraphrase Alignment
From a corpus of asymmetrical paraphrases, we
then use biology-based gene alignment algorithms
to align the words contained in each of the two
sentences within each paraphrase. For that pur-
pose, we implemented two well established algo-
rithms, one identifying local alignments (Smith
& Waterman, 1981) and the other one computing
global alignments (Needleman & Wunsch, 1970).
We also proposed a convenient dynamic strategy
(Cordeiro et al, 2007), which chooses the best
alignment algorithm to be applied to a specific
case at runtime.
The difference between local and global se-
quence alignments is illustrated below, where we
use letters, instead of words, to better fit our paper
space constraints. Suppose that we have the fol-
lowing two sequences: [D,H,M,S,T,P,R,Q,I,S]
and [T,P,Q,I,S,D,H,S] a global alignment
would produce the following pair.
D H M S T P R Q I S _ _ _
_ _ _ _ T P _ Q I S D H S
For the same two sequences, a local alignment
strategy could generate two or more aligned sub-
sequences as follows.
17
|D H M S| |T P R Q I S|
|D H _ S| |T P _ Q I S|
Hence, at this stage of the process, we end with a
corpus of aligned
7
asymmetrical paraphrases. In
Figure 4, we present the alignment of the para-
phrase of Figure 3.
(1) To the horror of their fans ,
(2) __ ___ ______ __ _____ ____ _
(1) Miss Ball and Arnaz were divorced in 1960.
(2) ____ Ball and Arnaz ____ divorced in 1960.
Figure 4: An Aligned Paraphrase
The next section describes how we use this
structured data to extract instances which are go-
ing to feed a learning system.
3 Bubble Extraction
In order to learn rewriting rules, we have focus
our experiences on a special kind of data, se-
lected from the corpus of aligned sentences, and
we named this data as Bubbles
8
. Given two word
aligned sentences, a bubble is a non-empty seg-
ment aligned with an empty segment of the other
sentence of the paraphrase, sharing a ?strong? con-
text. In Figure 5, we show different examples of
bubbles.
the situation here in chicago with the workers
the situation ____ in chicago with the workers
obama talks exclusively with tom brokaw on meet
obama talks ___________ with tom brokaw on meet
Ball and Arnaz were divorced in 1960
Ball and Arnaz ____ divorced in 1960
america is in the exact same seat as sweigert and
america is in ___ _____ same seat as sweigert and
after a while at the regents park gym, the president
after a while at ___ _______ ____ gym, the president
Figure 5: Examples of Bubbles
To extract a bubble, left and right contexts of
equally aligned words must occur, and the proba-
bility of such extraction depends on the contexts
size as well as the size of the region aligned with
the empty space. The main idea is to eliminate
cases where the bubble middle sequence is too
large when compared to the size of left and right
contexts. More precisely, we use the condition in
7
By ?aligned? we mean, from now on, word alignment
between paraphrase sentence pairs.
8
There are other possible regions to explore, but due to
the complexity of this task, we decided to initially work only
with bubbles
Equation 10 to decide whether a bubble should be
extracted or not.
Z
(L)
? Z
(X)
+ Z
(R)
? 0 (10)
whereL andR stand for the left and right contexts,
respectively, and X is the middle region. The Z
(?)
function computes the length of a given segment,
in terms of number of words. For example, in the
first and last examples of Figure 5, we have: 2 ?
1+5 = 6 ? 0 and 4?3+4 = 5 ? 0. In this case,
both bubbles will be extracted. This condition is
defined to prevent from extracting eccentric cases,
as the ones shown in the examples shown in Figure
6, where the conditions respectively fail: 0 ? 8 +
3 = ?5 < 0 and 1? 7 + 2 = ?4 < 0.
To the horror of their fans , Miss Ball and Arnaz
__ ___ ______ __ _____ ____ _ ____ Ball and Arnaz
will vote __ ___ _______ ____ __ _____ __ friday .
____ vote on the amended bill as early as friday .
Figure 6: Examples of Rejected Bubbles
Indeed, we favor examples with high common
contexts and few deleted words to enhance the in-
duction process.
So far, we only consider bubbles where the
middle region is aligned with a void segment
(X
transf
?? ?). However, more general transforma-
tions will be investigated in the future. Indeed, any
transformation X
transf
?? Y , where Y 6= ?, having
Z
(X)
> Z
(Y )
, may be a relevant compression ex-
ample.
Following this methodology, we obtain a huge
set of examples, where relevant sentence transfor-
mations occur. To have an idea about the amount
of data we are working with, from a set of 30 days
web news stories (133.5 MB of raw text), we iden-
tified and extracted 596678 aligned paraphrases,
from which 143761 bubbles were obtained.
In the next section, we show how we explore
Inductive Logic Programming (ILP) techniques to
generalize regularities and find conditions to com-
press sentence segments.
4 The Induction of Compression Rules
Many different algorithms exist to induce knowl-
edge from data. In this paper, we use Inductive
Logic Programming (ILP) (Muggleton, 1991) and
it was a choice based on a set of relevant fea-
tures like: the capacity to generate symbolic and
18
relational knowledge; the possibility to securely
avoid negative instances; the ability to mix differ-
ent types of attribute and to have more control over
the theory search process.
Unlike (Clarke & Lapata, 2006), we aim at
inducing human understandable knowledge, also
known as symbolic knowledge. For that pur-
pose, ILP satisfies perfectly this goal by produc-
ing clauses based on first order logic. Moreover,
most of the learning algorithms require a com-
plete definition and characterization of the feature
set, prior to the learning process, where any at-
tribute must be specified. This is a conceptual bot-
tleneck to many learning problems such as ours,
since we need to combine different types of at-
tributes i.e. lexical, morpho-syntactic and syntac-
tical. With ILP, we only need to define a set of pos-
sible features and the induction process will search
throughout this set.
4.1 The Aleph System
The Aleph system(Srinivasan, 2000) is an empir-
ical ILP system, initially designed to be a pro-
totype for exploring ILP ideas. It has become a
quite mature ILP implementation, used in many
research projects, ranging form Biology to NLP. In
fact, Aleph is the successor of several and ?more
primitive? ILP systems, like: Progol (Muggleton,
1999), FOIL (Quinlan, 1990), and Indlog (Cama-
cho, 1994), among others, and may be appropri-
ately parametrized to emulate any of those older
systems.
One interesting advantage in Aleph is the possi-
bility to learn exclusively from positive instances,
contrarily to what is required by most learning sys-
tems. Moreover, there is theoretical research work
(Muggleton, 1996) demonstrating that the increase
in the learning error tend to be negligible with the
absence of negative examples, as the number of
learning instances increases. This is a relevant
issue, for many learning domains, and specially
ours, where negative examples are not available.
4.2 Learning Instances
In our problem, we define predicates that charac-
terize possible features to be considered during the
induction process. Regarding the structure of our
learning instances (bubbles), we define predicates
which restrict left and right context sequences as
well as the aligned middle sequence. In particu-
lar, we limit the size of our context sequences to
a maximum of three words and, so far, only use
bubbles in which the middle sequence has a max-
imum length of three
9
words. The notion of con-
texts from bubbles is clarified with the next exam-
ple.
L2 L1 X1 X2 X3 R1 R2 R3 R4
L2 L1 __ __ __ R1 R2 R3 R4
For such a case, we consider [L1, L2] as the left
context, [R1, R2, R3] as the right context, and
[X1, X2, X3] as the aligned middle sequence.
Such an example is represented with a Prolog term
with arity 5 (bub/5) in the following manner:
bub(ID, t(3,0), [L1,L2],
[X1,X2,X3]--->[],
[R1,R2,R3]).
The ID is the identifier of the sequence instance,
t/2 defines the ?transformation dimension?, in
this case from 3 words to 0. The third and fifth
arguments are lists with the left and right con-
texts, respectively, and the fourth argument con-
tains the list with the elements deleted from the
middle sequence. It is important to point out that
every L
i
, X
i
andR
i
are structures with 3 elements
such as word/POS/Chunk. For example, the
word president would be represented by the
expanded structure president/nn/np.
4.3 Feature Space
As mentioned previously, with an ILP system, and
in particular with Aleph, the set of attributes is
defined through a set of conditions, expressed in
the form of predicates. These predicates are the
building blocks that will be employed to construct
rules, during the induction process. Hence, our at-
tribute search space is defined using Prolog pred-
icates, which define the complete set of possibil-
ities for rule body construction. In our problem,
we let the induction engine seek generalization
conditions for the bubble main regions (left, mid-
dle, and right). Each condition may be from one
of the four types: dimensional, lexical, POS, and
chunk. Dimensional conditions simply express
the aligned sequence transformation dimensional-
ity. Lexical conditions impose a fixed position to
match a given word. The POS condition is similar
to the lexical one, but more general, as the position
must match a specific part-of-speech tag. Likely,
chunk conditions bind a region to be equal to a
particular chunk type. For example, by looking
9
They represent 83.47% from the total number of ex-
tracted bubbles.
19
at Figure 2, the attentive reader may have noticed
that these three conditions are present in rule 7. In
terms of Aleph declaration mode, these conditions
are defined as follows.
:- modeh(1,rule(+bub)).
:- modeb(1,transfdim(+bub,n(#nat,#nat))).
:- modeb(3,chunk(+bub,#side,#chk)).
:- modeb(
*
,inx(+bub,#side,#k,#tword)).
:- determination(rule/1,transfdim/2).
:- determination(rule/1,chunk/3).
:- determination(rule/1,inx/4).
The inx/4 predicate defines lexical and POS
type conditions, the chunk/3 predicate de-
fines chunking conditions and the transfdim/2
predicate defines the transformation dimension-
ality, which is in the form transfdim(N,0)
with N>0, according to the kind of bubbles we are
working with.
4.4 The Rule Value Function
The Aleph system implements many different
evaluation
10
functions which guide the theory
search process, allowing the basic procedure for
theory construction to be altered. In order to bet-
ter fit to our problem, we define a new evaluation
function calculated as the geometrical mean be-
tween the coverage percentage and the rule size
value, as shown in Equation 11 whereR is the can-
didate rule and Cov(R) is the proportion of posi-
tive instances covered by R and the LV (?) func-
tion defines the rule value in terms of its length,
returning a value in the [0, 1] interval.
V alue(R) =
p
Cov(R)? LV (R) (11)
The V alue(?) function guides the induction
process, by preferring not too general rules having
maximum possible coverage value. As shown in
Figure 7, the V alue(?) function gives preferences
to rules with 3, 4 and 5 literals.
5 Results
The automatic evaluation of a system is always the
best way to do it, due to its objectivity and scal-
ability. However, in many cases it is unfeasible
for several practical reasons, like the unavailability
of data or the difficulty to prepare an appropriate
10
In the Aleph terminology, this function is named as the
?cost? function, despite the fact that it really computes the
value in the sense that the grater the value, the more likely it
is to be chosen.
0
10
20
30
40
50
60
70
80
90
100
10
25
50
90
60
40
20
1 2 3 4 5 6 7
n
o
clauses
value
Figure 7: Rule length value function
dataset. Some supervised learning approach use
manually labeled test sets to evaluated their sys-
tems. However, these are small test sets, for exam-
ple, (Knight & Marcu, 2002) use a set of 1035 sen-
tences to train the system and only 32 sentences
to test it, which is a quite small test set. As a
consequence, it is also important to propose more
through evaluation. In order to assess as clearly
as possible the performance of our methodology
on large datasets, we propose a set of qualitative
and quantitative evaluations based on three differ-
ent measures: Utility, Ngram simplification and
Correctness.
5.1 Evaluation
A relevant issue, not very commonly discussed, is
the Utility of a learned theory. In real life prob-
lems, people may be more interested in the vol-
ume of data processed than the quality of the re-
sults. Maybe, between a system which is 90%
precise and processes only 10% of data, and a sys-
tem with 70% precision, processing 50% of data,
the user would prefer the last one. The Utility
may be a stronger than the Recall measure, used
for the evaluation of supervised learning systems,
because the later measures how many instances
were well identified or processed from the test set
only, and the former takes into account the whole
universe. For example, in a sentence compres-
sion system, it is important to know how many
sentences would be compressed, from the whole
possible set of sentences encountered in electronic
news papers, or in classical literature books, or
both. This is what we mean here by Utility.
The Ngram-Simplification methodology is an
automatic extrinsic test, performed to perceive
how much a given sentence reduction ruleset
would simplify sentences in terms of syntactical
complexity. The answer is not obvious at first
sight, because even smaller sentences can contain
20
more improbable syntactical subsequences than
their uncompressed versions. To evaluate the syn-
tactical complexity of a sentence, we use a 4 ?
gram model and compute a relative
11
sequence
probability as defined in Equation 12 where
~
W =
[t
1
, t
2
, ..., t
m
] is the sequence of part-of-speech
tags for a given sentence with size m.
P{
~
W} =
?
m?n
Y
k=n
P{t
k
| t
k?1
, ..., t
k?n
}
?
1
m
(12)
The third evaluation is qualitative. We measure
the quality of the learned rules when applied to
sentence reduction. The objective is to assess how
correct is the application of the reduction rules.
This evaluation was made through manual annota-
tion for a statistically representative random sam-
ple of compressed sentences. A human judged
the adequacy and Correctness of each compres-
sion rule to a given sentence segment, in a scale
from 1 to 5, where 1 means that it is absolutely in-
correct and inadequate, and 5 that the compression
rule fits perfectly to the situation (sentence) being
analyzed.
To perform our evaluation, a sample of 300 sen-
tences were randomly extracted, where at least one
compression rule had been applied. This eval-
uation set may be subdivided into three subsets,
where 100 instances came from rules with Z
(X)
=
1 (BD1), 100 from rules with Z
(X)
= 2 (BD2),
and the other 100 from rules with Z
(X)
= 3
(BD3). Another random sample, also with 100
cases has been extracted to evaluate our base-line
(BL) which consists in the direct application of
the bubble set to make compressions. This means
that no learning process is performed. Instead, we
store the complete bubble set as if they were rules
by themselves (in the same manner as (Le Nguyen
& Ho, 2004) do).
Table 1 compiles the comparative results
for Correctness, Precision, Utility and Ngram-
simplification for all datasets. In particular,
Ngram-simplification in percentage is the pro-
portion of test cases where P{reduced(
~
W )} ?
P{
~
W}.
Table 1 provides evidence of the improvement
achieved with the induction rules, in comparison
with the base line, on each test parameter: Cor-
rectness, Utility and Ngram-simplification. Con-
11
Because it is raised to the inverse power of m, which is
the number of words in the sentence.
Parameter BL BD1 BD2 BD3
Correctness: 2.93 3.56 4.03 4.01
Precision: 58.60% 71.20% 80.60% 80.20%
Utility: 8.65% 32.67% 85.72% 26.86%
NG-Simpl: 47.39% 89.33% 90.03% 89.23%
Table 1: Results with Four Evaluation Parameters.
sidering the three experiences, BD1, BD2, and
BD3, as a unique evaluation run, we obtained a
mean Correctness quality of 3.867 (i.e. 77.33%
Precision), a mean Utility of 48.45%, and a mean
Ngram-simplification equal to 89.53%, which are
significantly better than the base line.
Moreover, best results overall are obtained for
BD2 with 80.6% Precision, 85.72% Utility and
90.03% Ngram-simplification which means that
we can expect a reduction of two words with high
quality for a great number of sentences. In partic-
ular, Figure 2 shows examples of learned rules.
5.2 Time Complexity
In the earlier
12
days of ILP, the computation time
spent by their systems was a serious difficult ob-
stacle, disabling its implementation for real life
problems. However, nowadays these time ef-
ficiency issues have been overcome, opening a
wide range of application possibilities, for many
problems, from Biology to Natural Language Pro-
cessing. The graph in figure 8, shows that even
with considerable big datasets, our learning sys-
tem (based on Aleph) evidences acceptable feasi-
ble computation time.
0
20
40
60
80
100
120
140
12
27
0
53
0
120
0
10 20 30 40 50 60
10
3
bubbles
seconds
Figure 8: Time spent during the induction process,
for datasets with size expressed in thousands of
bubbles.
To give an idea about the size of an induced
rule set, and taking as an example the learned rules
12
In the 1990-2000 decade.
21
with Z
(X)
= 2, these were learned from a dataset
containing 37271 t(2, 0) bubbles, and in the final
5806 sentence reduction rules were produced.
6 Conclusion and Future Directions
Sentence Compression is an active research topic,
where several relevant contributions have recently
been proposed. However, we believe that many
milestones still need to be reached. In this pa-
per, we propose a new framework in the form of
a pipeline, which processes huge sets of web news
articles and retrieves compression rules in an un-
supervised way. For that purpose, we extract and
align paraphrases, explore and select specific text
characteristics called bubbles and finally induce a
set of logical rules for sentence reduction in a real-
world environment. Although we have only con-
sidered bubbles having Z
(X)
? 3, a sentence may
have a compression length greater than this value,
since several compression rules may be applied to
a single sentence.
Our results evidence good practical applicabil-
ity, both in terms of Utility, Precision and Ngram-
simplification. In particular, we assess results up
to 80.6% Precision, 85.72% Utility and 90.03%
Ngram-simplification for reduction rules of two
word length. Moreover, results were compared to
a base line set of rules produced without learning
and the difference reaches a maximum improve-
ment using Inductive Logic Programming of 22%.
Acknowledgments
This work was supported by the VIPACCESS
project - Ubiquitous Web Access for Visually Im-
paired People. Funding Agency: Fundac??ao para
a Ci?encia e a Tecnologia (Portugal). Reference:
PTDC/PLP/72142/2006.
References
Barzilay R. and Lee L.. 2003. Learning to paraphrase:
An unsupervised approach using multiple-sequence
alignment. In HLT-NAACL 2003: Main Proceed-
ings, pages 16?23.
Camacho R. 1994. Learning stage transition rules with
Indlog. Gesellschaft f?ur Mathematik und Datenver-
arbeitung MBH., Volume 237 of GMD- Studien, pp.
273-290.
Clarke J., and Lapata M. 2006. Constraint-based Sen-
tence Compression: An Integer Programming Ap-
proach. 21st International Conference on Compu-
tational Linguistics and 44th Annual Meeting of the
Association for Computational Linguistics.
Cordeiro J. and Dias G. and Cleuziou G. 2007. Bi-
ology Based Alignments of Paraphrases for Sen-
tence Compression. In Proceedings of the Workshop
on Textual Entailment and Paraphrasing (ACL-
PASCAL / ACL2007), Prague, Czech Republic.
Cordeiro J. and Dias G. and Brazdir P. October
2007. New Functions for Unsupervised Asymmet-
rical Paraphrase Detection. In Journal of Software.,
Volume:2, Issue:4, Page(s): 12-23. Academy Pub-
lisher. Finland. ISSN: 1796-217X.
Dolan W.B. and Quirck C. and Brockett C. 2004. Un-
supervised construction of large paraphrase corpora:
Exploiting massively parallel news sources. In Pro-
ceedings of 20th International Conference on Com-
putational Linguistics (COLING 2004).
Knight K. and Marcu D. 2002. Summarization be-
yond sentence extraction: A probabilistic approach
to sentence compression. Artificial Intelligence,
139(1):91-107.
Muggleton S. 1991. Inductive Logic Programming.
New Generation Computing, 8 (4):295-318.
Muggleton S. 1996. Learning from positive data. Pro-
ceedings of the Sixth International Workshop on In-
ductive Logic Programming (ILP-96), LNAI 1314,
Berlin, 1996. Springer-Verlag.
Muggleton S. 1999. Inductive logic programming: Is-
sues, results and the challenge of learning language
in logic. Artificial Intelligence, 114 (1-2), 283?296.
Le Nguyen M., Horiguchi S., A. S., and Ho B. T. 2004.
Example-based sentence reduction using the hidden
markov model. ACM Transactions on Asian Lan-
guage Information Processing (TALIP), 3(2):146-
158.
Needleman SB, Wunsch CD. 1970. A general method
applicable to the search for similarities in the amino
acid sequence of two proteins. Journal of Molecular
Biology, 48 (3): 443?53.
Quinlan J. R. 1990. Learning Logical Deinitions from
Relations. Machine Learning., 5 (3), 239-266. 33,
39, 41.
Smith TF, Waterman MS. 1981. Identification of Com-
mon Molecular Subsequences. Journal of Molecu-
lar Biology, 147: 195?197.
Srinivasan A. 2000. The Aleph Manual, Technical
Report. Computing Laboratory, Oxford University,
UK.
Turner J, Charniak E. 2005. Supervised and Unsuper-
vised Learning for Sentence Compression. Proceed-
ings of the 43rd Annual Meeting of the ACL, pages
290-297.
22
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 403?411,
Beijing, August 2010
Paraphrase Alignment for Synonym Evidence Discovery
Gintare? Grigonyte?
Saarland University
g.grigonyte@hmf.vdu.lt
Joa?o Cordeiro, Gae?l Dias,
Rumen Moraliyski
HULTIG
University of Beira Interior
{jpaulo,ddg,rumen}@di.ubi.pt
Pavel Brazdil
LIAAD/FEP
University of Porto
pbrazdil@liaad.up.pt
Abstract
We describe a new unsupervised approach
for synonymy discovery by aligning para-
phrases in monolingual domain corpora.
For that purpose, we identify phrasal
terms that convey most of the concepts
within domains and adapt a methodol-
ogy for the automatic extraction and align-
ment of paraphrases to identify para-
phrase casts from which valid synonyms
are discovered. Results performed on two
different domain corpora show that gen-
eral synonyms as well as synonymic ex-
pressions can be identified with a 67.27%
precision.
1 Introduction
Synonymy is a specific type of a semantic re-
lationship. According to (Sowa and Siekmann,
1994), a synonym is a word (or concept) that
means the same or nearly the same as another
word (or concept). It has been observed that
words are similar if their contexts are similar (Fre-
itag et al, 2005) and so synonymy detection has
received a lot of attention during the last decades.
However, words used in the same context are
not necessarily synonyms and can embody dif-
ferent semantic relationships such as hyponyms,
meronyms or co-hyponyms (Heylen et al, 2008).
In this paper, we introduce a new unsupervised
methodology for synonym detection by extract-
ing and aligning paraphrases on normalized do-
main corpora1. In particular, we study a specific
structure within aligned paraphrases, paraphrase
1By normalized, we intend that phrasal terms have been
previously identified.
casts, from which valid synonyms are discovered.
In fact, we propose a new approach based on the
idea that synonyms are substitutable words within
a given domain corpus. Results performed on two
different domain corpora, the Corpus of Computer
Security (COCS) and the Corpus of Cancer Re-
search (COCR), show that general synonyms as
well as synonymic expressions can be identified
with a 67.27% precision performance.
2 Related Work
Automatic synonymy detection has been tackled
in a variety of ways which we explain as follows.
2.1 Pattern-based Approaches
This approach to information extraction is based
on a technique called selective concept extraction
as defined by (Riloff, 1993). Selective concept
extraction is a form of text skimming that selec-
tively processes relevant text while effectively ig-
noring surrounding text that is thought to be ir-
relevant to the domain. The pioneer of pattern-
based approaches (Hearst, 1992) has introduced
lexico-syntactic patterns to automatically acquire
given word semantic relationships. Specific pat-
terns like ?X and other Y? or ?X such as Y? were
used for hypernym-hyponym detection. Later, the
idea was extended and adapted for synonymy by
other researchers such as (Roark and Charniak,
1998), (Caraballo, 1999) and (Maynard and Pe-
ters, 2009). In general, manual pattern definition
is time consuming and requires linguistic skills.
Usually, systems based on lexico-syntactic pat-
terns perform with very high precision, but low
recall due to the fact that these patterns are rare.
However, recent work by (Ohshima and Tanaka,
403
2009) on Web data reported high recall figures.
To avoid manual encoding of patterns, many su-
pervised approaches have been proposed as sum-
marized in (Stevenson and Greenwood, 2006).
2.2 Distributional Similarity
Distributional similarity for capturing semantic
relatedness is relying on the hypothesis that se-
mantically similar words share similar contexts.
These methods vary in the level of supervision
from unsupervised to semi-supervised or to su-
pervised. The first type of methods includes the
work of (Hindle, 1990), (Lin, 1998) and (Heylen
et al, 2008) who used unsupervised methods
for detecting word similarities based on shallow-
parsed corpora. Others have proposed unsuper-
vised methodologies to solve TOEFL-like tests,
instead of discovering synonyms (Turney, 2001),
(Terra and Clarke, 2003) and (Freitag et al, 2005).
Other researchers, such as (Girju et al, 2004),
(Muller et al, 2006), (Wu and Zhou, 2003) and
(Wei et al, 2009), have used language or knowl-
edge resources to enhance the representation of
the vector space model. Unlike the pattern-based
approach, the distributional similarity-based ap-
proach shows low precision compared to high re-
call.
One obvious way to verify all the possible con-
nections between words of the vocabulary em-
ploys an exhaustive search. However, compari-
son based on word usage can only highlight those
terms that are highly similar in meaning. This
method of representation is usually unable to dis-
tinguish between middle strength and weak se-
mantic relations, or detect the relationship be-
tween hapax-legomena.
2.3 Hybrid Approaches
More recently, approaches combining patterns
and distributional similarity appeared to bring the
best of the two methodologies. (Hagiwara et
al., 2009) describe experiments that involve train-
ing various synonym classifiers. (Giovannetti et
al., 2008) use syntactically parsed text and man-
ually composed patterns together with distribu-
tional similarity for detecting semantically related
words. Finally, (Turney, 2008) proposes a super-
vised machine learning approach for discovering
synonyms, antonyms, analogies and associations.
For that purpose, feature vectors are based on fre-
quencies of patterns and classified by a SVM.
2.4 Our Approach
(Van der Plas and Tiedemann, 2006) state that
?People use multiple ways to express the same
idea. These alternative ways of conveying the
same information in different ways are referred
to by the term paraphrase and in the case of
single words or phrasal terms sharing the same
meaning, we speak of synonyms?. Based on this,
we propose that in order to discover pairs of se-
mantically related words (in the best case syn-
onyms) that may be used in figurative or rare
sense, and as consequence impossible to be iden-
tified by the distributional similarity approach,
we need to have them highlighted by their lo-
cal specific environment. Here we differ from
the pattern-based approach that use local general
environment. We propose to align paraphrases
from domain corpora and discover words that are
possibly substitutable for one another in a given
context (paraphrase casts), and as such are syn-
onyms or near-synonyms. Comparatively to exist-
ing approaches, we propose an unsupervised and
language-independent methodology which does
not depend on linguistic processing2, nor manual
definition of patterns or training sets and leads to
higher precision when compared to distributional
similarity-based approaches.
3 Normalization of the Corpora
The main goal of our research is to build knowl-
edge resources in different domains that can ef-
fectively be used in different NLP applications.
As such, precision takes an important part in the
overall process of our methodology. For that pur-
pose, we first identify the phrasal terms (or multi-
word units) present in the corpora. Indeed, it has
been shown in many works that phrasal terms con-
vey most of the specific contents of a given do-
main. Our approach to term extraction is based
on linguistic pattern matching and Inverse Doc-
ument Frequency (IDF) measurements for term
2We will see in the next section that we will use linguistic
resources to normalize our corpora, but the methodology can
be applied to any raw text.
404
quality assurance as explained in (Avizienis et al,
2009). For that purpose, we present a domain in-
dependent hybrid term extraction framework that
includes the following steps. First, the text is
morphologically annotated with the MPRO sys-
tem (Maas et al, 2009). Then grammar rules for
morphological disambiguation, syntactic parsing
and noun phrase detection are applied based on
finite-state automata technology, KURD (Carl and
Schmidt-Wigger, 1998). Following this, a vari-
ant and non-basic term form detection is applied,
as well as stop words removal. Then, combining
rich morphological and shallow syntactical analy-
sis with pattern matching techniques allows us to
extract a wide span of candidate terms which are
finally filtered with the well-known IDF measure.
4 Paraphrase Identification
A few unsupervised metrics have been applied to
automatic paraphrase identification and extraction
(Barzilay and McKeown, 2001) and (Dolan et al,
2004). However, these unsupervised methodolo-
gies show a major drawback by extracting quasi-
exact or even exact match pairs of sentences as
they rely on classical string similarity measures.
Such pairs are useless for our research purpose.
More recently, (Cordeiro et al, 2007a) proposed
the sumo metric specially designed for asymmet-
rical entailed pair identification in corpora which
obtained better performance than previously es-
tablished metrics, even in corpora with exclu-
sively symmetrical entailed paraphrases as in the
Microsoft Paraphrase Research Corpus (Dolan et
al., 2004). This function states that for a given
sentence pair ?Sa, Sb?, having m and n words in
each sentence and ? lexical exclusive links (word
overlaps, see figure 1) between them, its lexi-
cal connection strength is computed as defined in
Equations 1 and 2.
Sumo(Sa, Sb) =
?
?
?
S(m,n, ?) if S(m,n, ?) < 1
0 if ? = 0
e?kS(m,n,?) otherwise
(1)
where
S(m,n, ?) = ? log2(m? ) + ? log2(
n
? )
?, ? ? [0, 1], ? + ? = 1
(2)
Figure 1: 4 exclusive links between Sa and Sb.
To obtain a paraphrase corpus, we compute all
sentence pairs similarities Sumo(Sa, Sb) and se-
lect only those pairs exceeding a given threshold,
in our case threshold = 0.85, which is quite re-
strictive, ensuring the selection of pairs strongly
connected3.
However, to take into account the normalization
of the corpus, little adjustments had to be inte-
grated in the methodology proposed in (Cordeiro
et al, 2007a). Indeed, the original Sumo(., .)
function was under-weighting links that occurred
between phrasal terms such as ?molecular labo-
ratory? or ?renal cancer?. So, instead of counting
the number of lexical links among sentences, each
link weights differently according to the word
length in the connection, hence connections of
longer words will result in a larger value. For ex-
ample, in figure 1, instead of having ? = 4, we
count ? = 3 + 8 + 7 + 4 = 22. This adjust-
ment is important as multi-word units are treated
as longer words in the corpus. This modification
has also, as a side effect, under-evaluation of func-
tional words which usually follow the Zipf?s Law
and give more importance to meaningful words in
the paraphrase extraction process.
5 Paraphrase Alignment
In order to usefully explore the evidence syn-
onymy from paraphrases, sentence alignment
techniques must be applied to paraphrases in or-
der to identify paraphrase casts, i.e., substitutable
word pairs as shown in figure 2. As we can see,
the paraphrase cast includes three parts: the left
segment (context), a middle segment and the right
segment (context). In our figure the left and right
segments (contexts) are identical.
In the context of DNA sequence alignment,
two main algorithms have been proposed: (1) the
Needleman-Wunsch algorithm (Needleman and
3Further details about the sumo metric are available in
(Cordeiro et al, 2007a).
405
Figure 2: A paraphrase cast.
Wunsch, 1970) based on dynamic programming
which outputs a unique global alignment and (2)
the Smith-Waterman (SW) algorithm (Smith and
Waterman, 1981) which is an adaptation of the
previous algorithm and outputs local alignments.
In the context of NLP, (Cordeiro et al, 2007a)
proposed a combination of both algorithms de-
pending on the structure of paraphrase. How-
ever, since any local alignment is a candidate for
paraphrase casts, the SW algorithm revealed it-
self more appropriate and was always chosen. The
SW alignment algorithm uses dynamic program-
ming to compute the optimal local alignments be-
tween two sequences4. This process requires first
the definition of an alignment matrix (function),
which governs the likelihood of alignment of two
symbols. Thus we first build a matrix H such that
H(i, 0) = 0 and H(0, j) = 0, for 0 ? i ? m,
and 0 ? j ? n, where m and n are the number of
words in the paraphrase sentences. The rest of the
H elements are recursively calculated as in Equa-
tion 3 where gs(., .) is the gap-scoring function
and Sai (resp. Sbj ) represents the ith (resp. jth)
word of sentence Sa (resp. Sb).
H(i, j) = max
?
????
????
0
H(i? 1, j ? 1) + gs(Sai , Sbj ) MMisatch
H(i? 1, j) + gs(Sai , ) Deletion
H(i, j ? 1) + gs( , Sbj ) Insertion
(3)
Obviously, this algorithm is based on an align-
ment function which exploits the alignment like-
lihood between two alphabet symbols. For DNA
sequence alignments, this function is defined as
a mutation matrix, scoring gene mutation and gap
alignments. In our case, we define the gap-scoring
4In our case, the two sequences are the two sentences of
a paraphrase
function gs(., .) in Equations 4 and 5 which prior-
itize the alignment of specific domain key terms
i.e., single match, or key expressions i.e., phrasal
match, (reward 50), as well as lexically similar5
words such as ?programme? and ?programming?
for example. In particular, for these similar words
an adaptation of the well known Edit Distance is
used, the c(., .) function (5), which is explained in
more detail in (Cordeiro et al, 2007b).
gs(Sai , Sbj ) =
?
???????
???????
?1 if (Sai = ?) and (Sbj 6= ?)
?1 if (Sbj = ?) and (Sai 6= ?)
10 Single Match
50 Phrasal Match
c(Sai , Sbj ) Mismatch
(4)
where
c(Sai , Sbj ) = ?
edist(Sai , Sbj )
+maxseq(Sai , Sbj )
(5)
To obtain local alignments, the SW algorithm is
applied, using the alignment function defined with
H(., .) in 3. The alignment of the paraphrase in
figure 2 would give the result in figure 3.
Figure 3: An alignment.
6 Paraphrase Casts
In order to discover synonyms, we are looking for
special patterns from the aligned paraphrase sen-
tences, which naturally give us more evidence for
the existence of equivalent terms or expressions.
Due to the topological aspect of such patterns, we
decided to name them paraphrase casts, or just
casts as shown in figure 2. As we have mentioned
earlier, the paraphrase cast includes three parts:
the left segment (contextL), a middle segment and
the right segment (contextR). In the following ex-
ample the left and right segments (contexts) are
identical, but the middle segment includes differ-
ent misaligned sequences of words, represented
by wordSa and wordSb.
contextL wordSa ----- contextR
contextL ----- wordSb contextR
5This is why we have in equation 3 the label ?Mismatch?,
where ?mismatch? means different yet lexically near words.
406
We can attribute different levels of confidence
to different paraphrase casts. Indeed, the larger
the contexts and the smaller the misaligned se-
quences are, the more likely it is for single or
phrasal terms to be synonyms or near-synonyms.
Note that in the cast shown in figure 3, each con-
text has a significant size, with four words on
each side, and the misaligned segments are in fact
equivalent expressions i.e. ?paper? is a synonym
of ?research article?. In the analyzed domain
these expressions are equivalent and interchange-
able and appear to be interchangeable in other do-
mains. For the purpose of this paper, we only
take into account the casts where the misaligned
sequences of words contain only one word or one
multi-word unit in each sentence. That is, we have
a one-to-one match. However, no constraints are
imposed on the contexts6. So, all casts are com-
puted and analyzed for synonym discovery and re-
sults are provided in the next section.
7 Experiments
To evaluate our methodology we have used
two different corpora, both from scientific do-
mains built from abstracts of publications (see
Table 1). The corpus of computer secu-
rity (COCS) is a collection of 4854 abstracts
on computer security extracted from the IEEE
(http://ieee.rkbexplorer.com/) repository7. The
corpus of cancer research (COCR) contains 3334
domain specific abstracts of scientific publica-
tions extracted from the PubMed8 on three types
of cancer: (1) the mammary carcinoma register
(COCR1) consisting of 1500 abstracts, (2) the
nephroblastoma register (COCR2) consisting of
1500 abstracts, and (3) the rhabdoid tumor regis-
ter (COCR3) consisting of 334 abstracts. From
the paraphrase casts, we were able to automat-
ically extract, without further processing, single
synonymous word pairs, as well as synonymic
multi-word units, as can be seen in Table 2. For
that purpose we have used specific paraphrase
casts, whose aim was to privilege precision to
6This issue will be discussed in the next section.
7An example of an abstract can be viewed at:
http://ieee.rkbexplorer.com/description/publication-
00534618
8http://www.ncbi.nlm.nih.gov/pubmed
Corpus COCS COCR1 COCR2 COCR3
Tokens 412.265 336.745 227.477 46.215
Sentences 18.974 15.195 10.575 2.321
Aligned Pairs 589 994 511 125
Casts without filter 320 10.217 2.520 48
Casts with filter 202 361 292 16
Table 1: Corpora
recall. In particular, we have removed all casts
which contained numbers or special characters i.e.
casts with filter in Table 1. However, no con-
straints were imposed on the frequency of the
casts. Indeed, all casts were included even if
their overall frequency was just one. Although
Synonym (COCS) Complementary
frequency tuning frequency control
attack consequences attack impact
error-free operation error free operation
pseudo code pseudo algorithm
tolerance resilience
package loss message loss
adjustable algorithm context-aware algorithm
helpful comment valuable comment
Synonym (COCR) Complementary
childhood renal tumor childhood kidney tumor
hypertrophy growth
doxorubicin vincristine
carcinomas of the kidney sarcoma of the kidney
metastasis neoplasm
renal tumor renal malignancy
neoplastic thrombus tumor thrombus
vincristine adriamycin
Table 2: Synonyms for COCS
most of the word relationships were concerned
with synonymy, the other ones were not just er-
rors, but lexically related words, namely examples
of antonymy, hyperonymy/hyponymy and associ-
ations as shown in Table 3. In the evaluation, we
Antonym Complementary
positive sentinel nodes negative sentinel nodes
higher bits lower bits
older version newer version
Hypernym Hyponym
Multi-Tasking Virtual Machine Java Virtual Machine
therapy chemotherapy
hormone breast cancer estrogen breast cancer
Association Complementary
performance reliability
statistical difference significant difference
relationship correlation
Table 3: Other Word Semantic Relationships.
have focused on the precision of the method. The
evaluation of the extracted pairs was performed
manually by two domain experts. In fact, four
407
different evaluations were carried out depending
on whether the adapted S(., .) measure was used
(or not) and whether the normalization of the cor-
pora was used (or not). The best results were ob-
tained in all cases for the adapted S(., .) measure
with the multi-word units. Table 4 shows also the
worst results for the COCS as a baseline (COCS
(1)), i.e. non-adapted S(., .) and non-normalized
corpus. For the rest of the experiments we always
present the results with the adapted S(., .) mea-
sure and normalized corpus.
Corpus COCS (1) COCS (2)
Precision 54.62% 71.29%
Extracted Synonyms 130 144
Errors 108 58
Corpus COCR1 COCR2 COCR3
Precision 69.80% 61.30% 75.00%
Extracted Synonyms 252 178 12
Errors 109 111 4
Table 4: Overall Results
7.1 Discussion
Many results have been published in the literature,
especially tackling the TOEFL synonym detection
problem which aims at identifying the correct syn-
onym among a small set of alternatives (usually
four). For that purpose, the best precision rate has
been reached by (Turney et al, 2003) with 97.50%
who have exploited many resources, both statis-
tical and linguistic. However, our methodology
tackles a different problem. Indeed, our goal
is to automatically extract synonyms from texts.
The published works toward this direction have
not reached so good results. One of the latest stud-
ies was conducted by (Heylen et al, 2008) who
used distributional similarity measures to extract
synonyms from shallow parsed corpora with the
help of unsupervised methods. They report that
?the dependency-based model finds a tightly re-
lated neighbor for 50% of the target words and a
true synonym for 14%?. So, it means that by com-
paring all words in a corpus with all other words,
one can expect to find a correct semantic relation-
ship in 50% of the cases and a correct synonym
in just 14%. In that perspective, our approach
reaches higher results. Moreover, (Heylen et al,
2008) use a frequency cut-off which only selects
features that occur at least five times together with
the target word. In our case, no frequency thresh-
old is imposed to enable extraction of synonyms
with low frequency, such as hapax legomena. This
situation is illustrated in figure 4. We note that
most of the candidate pairs appear only once in
the corpus.
1 2 3 4 5
SynonymousNon?synonymous
Frequency of candidate pairs
Log nu
mber 
of can
didate
 pairs
01
23
45
6
Figure 4: Synonyms Frequency Distribution.
In order to assess the quality of our results,
we calculated the similarity between all extracted
pairs of synonyms following the distributional
analysis paradigm as in (Moraliyski and Dias,
2007) who build context9 feature vectors for noun
synonyms. In particular, we used the cosine sim-
ilarity measure and the Loglike association mea-
sure (Dunning, 1993) as the weighting scheme of
the context features. The distribution of the simi-
larity measure for all noun synonyms (62 pairs) is
shown in figure 5.
Similarity
Frequ
ency
0.0 0.2 0.4 0.6 0.8 1.00
510
1520
2530
Figure 5: Synonym Pairs Similarity Distribution.
The results clearly show that all extracted syn-
onyms are highly correlated in terms of context.
9In this case, the contexts are the surrounding nouns,
verbs and adjectives in the closest chunks of a shallow parsed
corpus.
408
Nearly half of the cases have similarities higher
than 0.5. It is important to notice that a spe-
cific corpus10 was built to calculate as sharply as
possible the similarity measures as it is done in
(Moraliyski and Dias, 2007). Indeed, based on
the COCS and the COCR most statistics were in-
significant leading to zero-valued features. This
situation is well-known as it is one of the major
drawbacks of the distributional analysis approach
which needs huge quantities of texts to make se-
cure decisions. So we note that applying the distri-
butional analysis approach to such small corpora
would have led to rather poor results. Even with
an adapted corpus, figure 5 (left-most bar) shows
that there are no sufficient statistics for 30 pairs of
synonyms. Although the quality of the extracted
pairs of synonyms is high, the precision remains
relatively low with 67.27% precision on average.
As we pointed out in the previous section, we did
not make any restrictions to the left and right con-
texts of the casts. However, the longer these con-
texts are, compared to the misaligned sequence of
words, the higher the chance is that we find a cor-
rect synonym. Table 5 shows the average lengths
of both cast contexts for synonyms and erroneous
pairings, in terms of words (WCL) and charac-
ters (CCL). We also provide the ratio (R) between
the character lengths of the middle segment (i.e.
misaligned character sequences) and the charac-
ter lengths of the cast contexts (i.e. right and left
sizes of equally aligned character sequences). It is
Type WCL CCL R
Synonyms 2.70 11.67 0.70
Errors 2.45 8.05 0.55
Table 5: Average Casts Contexts Lengths
clear that a more thorough study of the effects of
the left and right contexts should be carried out to
improve precision of our approach, although this
may be to the detriment of recall. Based on the
results of the ratio R11, we note that the larger the
cast context is compared to the cast content, the
more likely it is that the misaligned words are syn-
onyms.
10This corpus contains 125.888.439 words.
11These results are statistically relevant with p? value <
0.001 using the Wilcoxon Rank-Sum Test.
8 Conclusions
In this paper we have introduced a new unsu-
pervised methodology for synonym detection that
involves extracting and aligning paraphrases on
normalized domain corpora. In particular, we
have studied a specific structure within aligned
paraphrases, paraphrase casts, from which valid
synonyms were discovered. The overall preci-
sion was 71.29% for the computer security do-
main and 66.06% for the cancer research domain.
This approach proved to be promising for ex-
tracting synonymous words and synonymic multi-
word units. Its strength is the ability to effectively
work with small domain corpora, without super-
vised training, nor definition of specific language-
dependent patterns. Moreover, it is capable to
extract synonymous pairs with figurative or rare
senses which would be impossible to identify us-
ing the distributional similarity approach. Finally,
our approach is completely language-independent
as it can be applied to any raw text, not obli-
gatorily normalized corpora, although the results
for domain terminology may be improved by the
identification of phrasal terms.
However, further improvements of the method
should be considered. A measure of quality of the
paraphrase casts is necessary to provide a mea-
sure of confidence to the kind of extracted word
semantic relationship. Indeed, the larger the con-
texts and the smaller the misaligned sequences
are, the more likely it is for single or phrasal terms
to be synonyms or near-synonyms. Further work
should also be carried out to differentiate the ac-
quired types of semantically related pairs. As it
is shown in Table 3, some of the extracted pairs
were not synonymic, but lexically related words
such as antonyms, hypernyms/hyponyms and as-
sociations. A natural follow-up solution for dis-
criminating between semantic types of extracted
pairs could involve context-based classification of
acquired casts pairs. In particular, (Turney, 2008)
tackled the problem of classifying different lexi-
cal information such as synonymy, antonymy, hy-
pernymy and association by using context words.
In order to propose a completely unsupervised
methodology, we could also follow the idea of
(Dias et al, 2010) to automatically construct small
409
TOEFL-like tests based on sets of casts which
could be handled with the help of different dis-
tributional similarities.
Acknowledgments
We thank anonymous reviewers whose comments
helped to improve this paper. We also thank
IFOMIS institute (Saarbrucken) and the ReSIST
project for allowing us to use the COCR and
COCS corpora.
References
Avizienis, A., Grigonyte, G., Haller, J., von Henke,
F., Liebig, T., and Noppens, O. 2009. Organiz-
ing Knowledge as an Ontology of the Domain of
Resilient Computing by Means of NLP - An Experi-
ence Report. In Proc. of the 22th Int. FLAIRS Conf.
AAAI Press, pp. 474-479.
Barzilay, R. and McKeown, K. R. 2001. Extracting
Paraphrases from a Parallel Corpus. In Proc. of the
39th meeting of ACL, pp. 50-57.
Caraballo, S. A. 1999. Automatic Construction of a
Hypernym-labeled Noun Hierarchy from Text. In
Proc. of 37th meeting of ACL 1999, pp 120-126.
Carl, M., and Schmidt-Wigger, A. 1998. Shallow Post
Morphological Processing with KURD. In Proc. of
the Conf. on New Methods in Language Processing.
Cordeiro, J.P., Dias, G. and Brazdil, P. 2007. Learning
Paraphrases from WNS Corpora. In Proc. of the
20th Int. FLAIRS Conf. AAAI Press, pp. 193-198.
Cordeiro, J.P., Dias, G. and Cleuziou, G. 2007. Biol-
ogy Based Alignments of Paraphrases for Sentence
Compression. In Proc. of the 20th meeting of ACL,
workshop PASCAL, pp. 177-184.
Dias, G., Moraliyski, R., Cordeiro, J.P., Doucet, A. and
Ahonen-Myka, H. 2010. Automatic Discovery of
Word Semantic Relations using Paraphrase Align-
ment and Distributional Lexical Semantics Analy-
sis. In Journal of Natural Language Engineering,
Cambridge University Press. ISSN 1351-3249, pp.
1-26.
Dolan, B., Quirk, C. and Brockett, C. 2004. Un-
supervised Construction of Large Paraphrase Cor-
pora: Exploiting Massively Parallel News Sources.
In Proc. of the 20th int. Conf. on Computational
Linguistics.
Dunning T. D 1993. Accurate Methods for the Statis-
tics of Surprise and Coincidence. In Computational
Linguistics, 19(1), pp. 61-74.
Freitag, D., Blume, M., Byrnes, J., Chow, E., Kapa-
dia, S., Rohwer, R. and Wang, Z. 2005. New Ex-
periments in Distributional Representations of Syn-
onymy. In Proc. of 9th conf. on Computational Nat-
ural Language Learning, pp. 25-32.
Giovannetti, E., Marchi, S. and Montemagni, S.
2008. Combining Statistical Techniques and
Lexico-Syntactic Patterns for Semantic Relations
Extraction from Text. In Proc. of the 5th Workshop
on Semantic Web Applications and Perspectives.
Girju, R., Giuglea, A. M., Olteanu, M., Fortu, O.,
Bolohan, O. and Moldovan, D. 2004. Support Vec-
tor Machines Applied to the Classification of Se-
mantic Relations in Nominalized Noun Phrases. In
Proc. of the HLT-NAACL Workshop on Computa-
tional Lexical Semantics, pp. 68-75.
Hagiwara, M. O. Y. and Katsuhiko, T. 2009. Su-
pervised Synonym Acquisition using Distributional
Features and Syntactic Patterns. In Information and
Media Technologies 4(2), pp. 558-582.
Hearst, M. A. 1992. Automatic Acquisition of Hy-
ponyms from Large Text Corpora. In Proc. of the
14th conf. on Computational Linguistics, pp. 539-
545.
Heylen, K., Peirsman, Y., Geeraerts, D. and Speelman,
D. 2008. Modelling Word Similarity: an Evalua-
tion of Automatic Synonymy Extraction Algorithms.
In Proc. of the 6th LREC.
Hindle, D. 1990. Noun Classification from Predicate-
Argument Structures. In Proc. of the 28th meeting
of ACL, pp. 268-275.
Inkpen, D. 2007. A Statistical Model for Near-
Synonym choice. In ACM Trans. Speech Lang. Pro-
cess. 4(1), 1-17.
Kiefer, J. 1953. Sequential Minimax Search for a
Maximum. In Proc. of the American Mathematical
Society 4, pp. 502-506.
Lin, D. 1998. Automatic Retrieval and Clustering of
Similar Words. In Proc. of the 17th Int. Conf. on
Computational Linguistics, pp. 768-774.
Maas, D., Rosener, Ch., and Theofilidis, A. 2009.
Morphosyntactic and Semantic Analysis of Text:
The MPRO Tagging Procedure. Workshop on sys-
tems and frameworks for computational morphol-
ogy. Springer., pp. 76-87.
Maynard, D. F. A. and Peters, W. 2009. Using lexico-
syntactic Ontology Design Patterns for Ontology
Creation and Population. In Proc. of the Workshop
on Ontology Patterns.
410
Moraliyski, R., and Dias, G. 2007. One Sense per
Discourse for Synonym Detection. In Proc. of the
Int. Conf. On Recent Advances in NLP, Bulgaria,
pp. 383-387.
Muller, P., Hathout, N. and Gaume, B. 2006. Synonym
Extraction Using a Semantic Distance on a Dictio-
nary. In Proc. of the 1st Workshop on Graph-Based
Methods for NLP, pp. 65-72.
Needleman, S. B. and Wunsch, C. D. 1970. A General
Method Applicable to the Search for Similarities in
the Amino Acid Sequence of two Proteins. In Jour-
nal of Molecular Biology 48(3), pp. 443-453.
Ohshima, H. and Tanaka, K. 2009. Real Time Ex-
traction of Related Terms by Bi-directional lexico-
syntactic Patterns from the Web. In Proc. of the 3rd
Int. Conf. on Ubiquitous Information Management
and Communication, pp. 441-449.
Riloff, E. 1993. Automatically Constructing a Dic-
tionary for Information Extraction Tasks. In Proc.
of the 11th Nat. Conf. on Artificial Intelligence, pp.
811-816.
Roark, B. and Charniak, E. 1998. Noun-phrase
Co-occurrence Statistics for Semiautomatic Seman-
tic Lexicon Construction. In Proc. of the 17th
Int. Conf. on Computational Linguistics, pp. 1110-
1116.
Smith, T. and Waterman, M. 1981. Identification of
Common Molecular Subsequences. In Journal of
Molecular Biology 147, pp. 195-197.
Sowa, J. F. and Siekmann, J. H. 1994. Concep-
tual Structures: Current Practices. Springer-Verlag
New York, Inc., Secaucus, NJ, USA.
Stevenson, M. and Greenwood, M. 2006. Compar-
ing Information Extraction Pattern Models. In Proc.
of the Workshop on Information Extraction Beyond
the Document, ACL, pp. 29-35.
Terra, E. and Clarke, C. 2003. Frequency Estimates
for Statistical Word Similarity Measures. In Proc.
of HTL/NAACL 2003, pp. 165-172.
Turney, P. D. 2001. Mining the Web for Synonyms:
PMI-IR versus LSA on TOEFL. Lecture Notes in
Computer Science, 2167, pp. 491-502.
Turney, P. D., Littman, M. L., Bigham, J. and Shnay-
der, V. 2003. Combining Independent Modules in
Lexical Multiple-choice Problems. In Recent Ad-
vances in NLP III: Selected Papers, pp. 101-110.
Turney, P. D. 2008. A Uniform Approach to Analogies,
Synonyms, Antonyms and Associations. In Proc .of
the 22nd Int. Conf. on Computational Linguistics,
pp. 905-912.
Van der Plas, L. and Tiedemann, J. 2006. Finding Syn-
onyms Using Automatic Word Alignment and Mea-
sures of Distributional Similarity. In Proc. of the
21st Int. Conf. on Computational Linguistics, pp.
866-873.
Wei, X., Peng, F., Tseng, H., Lu, Y. and Dumoulin,
B. 2009. Context Sensitive Synonym Discovery for
Web Search Queries. In Proc. of the 18th ACM
conference on Information and Knowledge Man-
agement, pp. 1585-1588.
Wu, H. and Zhou, M. 2003. Optimizing Synonym
Extraction Using Monolingual and Bilingual Re-
sources. In Proc. of the 2nd Int. Workshop on Para-
phrasing, pp. 72-79.
411
