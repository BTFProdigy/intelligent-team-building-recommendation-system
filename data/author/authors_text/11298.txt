Proceedings of the 12th Conference of the European Chapter of the ACL, pages 86?93,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Syntactic Phrase Reordering for English-to-Arabic Statistical Machine
Translation
Ibrahim Badr Rabih Zbib
Computer Science and Artificial Intelligence Lab
Massachusetts Institute of Technology
Cambridge, MA 02139, USA
{iab02, rabih, glass}@csail.mit.edu
James Glass
Abstract
Syntactic Reordering of the source lan-
guage to better match the phrase struc-
ture of the target language has been
shown to improve the performance of
phrase-based Statistical Machine Transla-
tion. This paper applies syntactic reorder-
ing to English-to-Arabic translation. It in-
troduces reordering rules, and motivates
them linguistically. It also studies the ef-
fect of combining reordering with Ara-
bic morphological segmentation, a pre-
processing technique that has been shown
to improve Arabic-English and English-
Arabic translation. We report on results in
the news text domain, the UN text domain
and in the spoken travel domain.
1 Introduction
Phrase-based Statistical Machine Translation has
proven to be a robust and effective approach to
machine translation, providing good performance
without the need for explicit linguistic informa-
tion. Phrase-based SMT systems, however, have
limited capabilities in dealing with long distance
phenomena, since they rely on local alignments.
Automatically learned reordering models, which
can be conditioned on lexical items from both the
source and the target, provide some limited re-
ordering capability when added to SMT systems.
One approach that explicitly deals with long
distance reordering is to reorder the source side
to better match the target side, using predefined
rules. The reordered source is then used as input
to the phrase-based SMT system. This approach
indirectly incorporates structure information since
the reordering rules are applied on the parse trees
of the source sentence. Obviously, the same re-
ordering has to be applied to both training data and
test data. Despite the added complexity of parsing
the data, this technique has shown improvements,
especially when good parses of the source side ex-
ist. It has been successfully applied to German-to-
English and Chinese-to-English SMT (Collins et
al., 2005; Wang et al, 2007).
In this paper, we propose the use of a similar
approach for English-to-Arabic SMT. Unlike most
other work on Arabic translation, our work is in
the direction of the more morphologically com-
plex language, which poses unique challenges. We
propose a set of syntactic reordering rules on the
English source to align it better to the Arabic tar-
get. The reordering rules exploit systematic differ-
ences between the syntax of Arabic and the syntax
of English; they specifically address two syntac-
tic constructs. The first is the Subject-Verb order
in independent sentences, where the preferred or-
der in written Arabic is Verb-Subject. The sec-
ond is the noun phrase structure, where many dif-
ferences exist between the two languages, among
them the order of adjectives, compound nouns
and genitive constructs, as well as the way defi-
niteness is marked. The implementation of these
rules is fairly straightforward since they are ap-
plied to the parse tree. It has been noted in previ-
ous work (Habash, 2007) that syntactic reordering
does not improve translation if the parse quality is
not good enough. Since in this paper our source
language is English, the parses are more reliable,
and result in more correct reorderings. We show
that using the reordering rules results in gains in
the translation scores and study the effect of the
training data size on those gains.
This paper also investigates the effect of using
morphological segmentation of the Arabic target
86
in combination with the reordering rules. Mor-
phological segmentation has been shown to benefit
Arabic-to-English (Habash and Sadat, 2006) and
English-to-Arabic (Badr et al, 2008) translation,
although the gains tend to decrease with increas-
ing training data size.
Section 2 provides linguistic motivation for the
paper. It describes the rich morphology of Arabic,
and its implications on SMT. It also describes the
syntax of the verb phrase and noun phrase in Ara-
bic, and how they differ from their English coun-
terparts. In Section 3, we describe some of the rel-
evant previous work. In Section 4, we present the
preprocessing techniques used in the experiments.
Section 5 describes the translation system, the data
used, and then presents and discusses the experi-
mental results from three domains: news text, UN
data and spoken dialogue from the travel domain.
The final section provides a brief summary and
conclusion.
2 Arabic Linguistic Issues
2.1 Arabic Morphology
Arabic has a complex morphology compared to
English. The Arabic noun and adjective are in-
flected for gender and number; the verb is inflected
in addition for tense, voice, mood and person.
Various clitics can attach to words as well: Con-
junctions, prepositions and possessive pronouns
attach to nouns, and object pronouns attach to
verbs. The example below shows the decompo-
sition into stems and clitics of the Arabic verb
phrase wsyqAblhm1 and noun phrase wbydh, both
of which are written as one word:
(1) a. w+
and
s+
will
yqAbl
meet-3SM
+hm
them
and he will meet them
b. w+
and
b+
with
yd
hand
+h
his
and with his hand
An Arabic corpus will, therefore, have more
surface forms than an equivalent English corpus,
and will also be sparser. In the LDC news corpora
used in this paper (see Section 5.2), the average
English sentence length is 33 words compared to
the Arabic 25 words.
1All examples in this paper are writ-
ten in the Buckwalter Transliteration System
(http://www.qamus.org/transliteration.htm)
Although the Arabic language family consists
of many dialects, none of them has a standard
orthography. This affects the consistency of the
orthography of Modern Standard Arabic (MSA),
the only written variety of Arabic. Certain char-
acters are written inconsistently in different data
sources: Final ?y? is sometimes written as ?Y? (Alif
mqSwrp), and initial Alif hamza (The Buckwal-
ter characters ?<? and ?{?) are written as bare alif
(A). Arabic is usually written without the diacritics
that denote short vowels. This creates an ambigu-
ity at the word level, since a word can have more
than one reading. These factors adversely affect
the performance of Arabic-to-English SMT, espe-
cially in the English-to-Arabic direction.
Simple pattern matching is not enough to per-
form morphological analysis and decomposition,
since a certain string of characters can, in princi-
ple, be either an affixed morpheme or part of the
base word itself. Word-level linguistic information
as well as context analysis are needed. For exam-
ple the written form wly can mean either ruler or
and for me, depending on the context. Only in the
latter case should it be decomposed.
2.2 Arabic Syntax
In this section, we describe a number of syntactic
facts about Arabic which are relevant to the
reordering rules described in Section 4.2.
Clause Structure
In Arabic, the main sentence usually has
the order Verb-Subject-Object (VSO). The order
Subject-Verb-Object (SVO) also occurs, but is less
frequent than VSO. The verb agrees with the sub-
ject in gender and number in the SVO order, but
only in gender in the VSO order (Examples 2c and
2d).
(2) a. Akl
ate-3SM
Alwld
the-boy
AltfAHp
the-apple
the boy ate the apple
b. Alwld
the-boy
Akl
ate-3SM
AltfAHp
the-apple
the boy ate the apple
c. Akl
ate-3SM
AlAwlAd
the-boys
AltfAHAt
the-apples
the boys ate the apples
d. AlAwlAd
the-boys
AklwA
ate-3PM
AltfAHAt
the-apples
the boys ate the apples
87
In a dependent clause, the order must be SVO,
as illustrated by the ungrammaticality of Exam-
ple 3b below. As we discuss in more detail later,
this distinction between dependent and indepen-
dent clauses has to be taken into account when the
syntactic reordering rules are applied.
(3) a. qAl
said-3SM
An
that
Alwld
the-boy
Akl
ate
AltfAHp
the-apple
he said that the boy ate the apple
b. *qAl
said-3SM
An
that
Akl
ate
Alwld
the-boy
AltfAHp
the-apple
he said that the boy ate the apple
Another pertinent fact is that the negation parti-
cle has to always preceed the verb:
(4) lm
not
yAkl
eat-3SM
Alwld
the-boy
AltfAHp
the-apple
the boy did not eat the apple
Noun Phrase
The Arabic noun phrase can have constructs
that are quite different from English. The adjective
in Arabic follows the noun that it modifies, and it
is marked with the definite article, if the head noun
is definite:
(5) AlbAb
the-door
Alkbyr
the-big
the big door
The Arabic equivalent of the English posses-
sive, compound nouns and the of -relationship is
the Arabic idafa construct, which compounds two
or more nouns. Therefore, N1?s N2 and N2 of N1
are both translated as N2 N1 in Arabic. As Exam-
ple 6b shows, this construct can also be chained
recursively.
(6) a. bAb
door
Albyt
the-house
the house?s door
b. mftAH
key
bAb
door
Albyt
the-house
The key to the door of the house
Example 6 also shows that an idafa construct is
made definite by adding the definite article Al- to
the last noun in the noun phrase. Adjectives follow
the idafa noun phrase, regardless of which noun in
the chain they modify. Thus, Example 7 is am-
biguous in that the adjective kbyr (big) can modify
any of the preceding three nouns. The same is true
for relative clauses that modify a noun.
(7) mftAH
key
bAb
door
Albyt
the-house
Alkbyr
the-big
These and other differences between the Arabic
and English syntax are likely to affect the qual-
ity of automatic alignments, since corresponding
words will occupy positions in the sentence that
are far apart, especially when the relevant words
(e.g. the verb and its subject) are separated by sub-
ordinate clauses. In such cases, the lexicalized dis-
tortion models used in phrase-based SMT do not
have the capability of performing reorderings cor-
rectly. This limitation adversely affects the trans-
lation quality.
3 Previous Work
Most of the work in Arabic machine translation
is done in the Arabic-to-English direction. The
other direction, however, is also important, since
it opens the wealth of information in different do-
mains that is available in English to the Arabic
speaking world. Also, since Arabic is a morpho-
logically richer language, translating into Arabic
poses unique issues that are not present in the
opposite direction. The only works on English-
to-Arabic SMT that we are aware of are Badr et
al. (2008), and Sarikaya and Deng (2007). Badr
et al show that using segmentation and recom-
bination as pre- and post- processing steps leads
to significant gains especially for smaller train-
ing data corpora. Sarikaya and Deng use Joint
Morphological-Lexical Language Models to re-
rank the output of an English-to-Arabic MT sys-
tem. They use regular expression-based segmen-
tation of the Arabic so as not to run into recombi-
nation issues on the output side.
Similarly, for Arabic-to-English, Lee (2004),
and Habash and Sadat (2006) show that vari-
ous segmentation schemes lead to improvements
that decrease with increasing parallel corpus size.
They use a trigram language model and the Ara-
bic morphological analyzer MADA (Habash and
Rambow, 2005) respectively, to segment the Ara-
bic side of their corpora. Other work on Arabic-
to-English SMT tries to address the word reorder-
ing problem. Habash (2007) automatically learns
syntactic reordering rules that are then applied to
the Arabic side of the parallel corpora. The words
are aligned in a sentence pair, then the Arabic sen-
tence is parsed to extract reordering rules based on
how the constituents in the parse tree are reordered
on the English side. No significant improvement is
88
shown with reordering when compared to a base-
line that uses a non-lexicalized distance reordering
model. This is attributed in the paper to the poor
quality of parsing.
Syntax-based reordering as a preprocessing step
has been applied to many language pairs other
than English-Arabic. Most relevant to the ap-
proach in this paper are Collins et al (2005)
and Wang et al (2007). Both parse the source
side and then reorder the sentence based on pre-
defined, linguistically motivated rules. Signifi-
cant gain is reported for German-to-English and
Chinese-to-English translation. Both suggest that
reordering as a preprocessing step results in bet-
ter alignment, and reduces the reliance on the dis-
tortion model. Popovic and Ney (2006) use sim-
ilar methods to reorder German by looking at the
POS tags for German-to-English and German-to-
Spanish. They show significant improvements on
test set sentences that do get reordered as well
as those that don?t, which is attributed to the im-
provement of the extracted phrases. (Xia and
McCord, 2004) present a similar approach, with
a notable difference: the re-ordering rules are au-
tomatically learned from aligning parse trees for
both the source and target sentences. They report
a 10% relative gain for English-to-French trans-
lation. Although target-side parsing is optional
in this approach, it is needed to take full advan-
tage of the approach. This is a bigger issue when
no reliable parses are available for the target lan-
guage, as is the case in this paper. More generally,
the use of automatically-learned rules has the ad-
vantage of readily applicable to different language
pairs. The use of deterministic, pre-defined rules,
however, has the advantage of being linguistically
motivated, since differences between the two lan-
guages are addressed explicitly. Moreover, the im-
plementation of pre-defined transfer rules based
on target-side parses is relatively easy and cheap
to implement in different language pairs.
Generic approaches for translating from En-
glish to more morphologically complex languages
have been proposed. Koehn and Hoang (2007)
propose Factored Translation Models, which ex-
tend phrase-based statistical machine translation
by allowing the integration of additional morpho-
logical features at the word level. They demon-
strate improvements for English-to-German and
English-to-Czech. Tighter integration of fea-
tures is claimed to allow for better modeling of
the morphology and hence is better than using
pre-processing and post-processing techniques.
Avramidis and Koehn (2008) enrich the English
side by adding a feature to the Factored Model that
models noun case agreement and verb person con-
jugation, and show that it leads to a more gram-
matically correct output for English-to-Greek and
English-to-Czech translation. Although Factored
Models are well equipped for handling languages
that differ in terms of morphology, they still use
the same distortion reordering model as a phrase-
based MT system.
4 Preprocessing Techniques
4.1 Arabic Segmentation and Recombination
It has been shown previously work (Badr et al,
2008; Habash and Sadat, 2006) that morphologi-
cal segmentation of Arabic improves the transla-
tion performance for both Arabic-to-English and
English-to-Arabic by addressing the problem of
sparsity of the Arabic side. In this paper, we use
segmented and non-segmented Arabic on the tar-
get side, and study the effect of the combination of
segmentation with reordering.
As mentioned in Section 2.1, simple pattern
matching is not enough to decompose Arabic
words into stems and affixes. Lexical information
and context are needed to perform the decompo-
sition correctly. We use the Morphological Ana-
lyzer MADA (Habash and Rambow, 2005) to de-
compose the Arabic source. MADA uses SVM-
based classifiers of features (such as POS, num-
ber, gender, etc.) to score the different analyses
of a given word in context. We apply morpho-
logical decomposition before aligning the training
data. We split the conjunction and preposition pre-
fixes, as well as possessive and object pronoun suf-
fixes. We then glue the split morphemes into one
prefix and one suffix, such that any given word is
split into at most three parts: prefix+ stem +suffix.
Note that plural markers and subject pronouns are
not split. For example, the word wlAwlAdh (?and
for his children?) is segmented into wl+ AwlAd
+P:3MS.
Since training is done on segmented Arabic, the
output of the decoder must be recombined into its
original surface form. We follow the approach of
Badr et. al (2008) in combining the Arabic out-
put, which is a non-trivial task for several reasons.
First, the ending of a stem sometimes changes
when a suffix is attached to it. Second, word end-
89
ings are normalized to remove orthographic incon-
sistency between different sources (Section 2.1).
Finally, some words can recombine into more than
one grammatically correct form. To address these
issues, a lookup table is derived from the training
data that maps the segmented form of the word to
its original form. The table is also useful in re-
combining words that are erroneously segmented.
If a certain word does not occur in the table, we
back off to a set of manually defined recombina-
tion rules. Word ambiguity is resolved by picking
the more frequent surface form.
4.2 Arabic Reordering Rules
This section presents the syntax-based rules used
for re-ordering the English source to better match
the syntax of the Arabic target. These rules are
motivated by the Arabic syntactic facts described
in Section 2.2.
Much like Wang et al (2007), we parse the En-
glish side of our corpora and reorder using prede-
fined rules. Reordering the English can be done
more reliably than other source languages, such
as Arabic, Chinese and German, since the state-
of-the-art English parsers are considerably better
than parsers of other languages. The following
rules for reordering at the sentence level and the
noun phrase level are applied to the English parse
tree:
1. NP: All nouns, adjectives and adverbs in the
noun phrase are inverted. This rule is moti-
vated by the order of the adjective with re-
spect to its head noun, as well as the idafa
construct (see Examples 6 and 7 in Section
2.2. As a result of applying this rule, the
phrase the blank computer screen becomes
the screen computer blank .
2. PP: All prepositional phrases of the form
N1ofN2 ...ofNn are transformed to
N1N2 ...Nn . All N i are also made indefi-
nite, and the definite article is added to Nn ,
the last noun in the chain. For example, the
phrase the general chief of staff of the armed
forces becomes general chief staff the armed
forces. We also move all adjectives in the
top noun phrase to the end of the construct.
So the real value of the Egyptian pound
becomes value the Egyptian pound real. This
rule is motivated by the idafa construct and
its properties (see Example 6).
3. the: The definite article the is replicated be-
fore adjectives (see Example 5 above). So the
blank computer screen becomes the blank the
computer the screen. This rule is applied af-
ter NP rule abote. Note that we do not repli-
cate the before proper names.
4. VP: This rule transforms SVO sentences to
VSO. All verbs are reordered on the condi-
tion that they have their own subject noun
phrase and are not in the participle form,
since in these cases the Arabic subject occurs
before the verb participle. We also check that
the verb is not in a relative clause with a that
complementizer (Example 3 above). The fol-
lowing example illustrates all these cases: the
health minister stated that 11 police officers
were wounded in clashes with the demonstra-
tors? stated the health minister that 11 po-
lice officers were wounded in clashes with the
demonstrators. If the verb is negated, the
negative particle is moved with the verb (Ex-
ample 4. Finally, if the object of the reordered
verb is a pronoun, it is reordered with the
verb. Example: the authorities gave us all
the necessary help becomes gave us the au-
thorities all the necessary help.
The transformation rules 1, 2 and 3 are applied
in this order, since they interact although they do
not conflict. So, the real value of the Egyptian
pound ? value the Egyptian the pound the real
The VP reordering rule is independent.
5 Experiments
5.1 System description
For the English source, we first tokenize us-
ing the Stanford Log-linear Part-of-Speech Tag-
ger (Toutanova et al, 2003). We then proceed
to split the data into smaller sentences and tag
them using Ratnaparkhi?s Maximum Entropy Tag-
ger (Ratnaparkhi, 1996). We parse the data us-
ing the Collins Parser (Collins, 1997), and then
tag person, location and organization names us-
ing the Stanford Named Entity Recognizer (Finkel
et al, 2005). On the Arabic side, we normalize
the data by changing final ?Y? to ?y?, and chang-
ing the various forms of Alif hamza to bare Alif,
since these characters are written inconsistently in
some Arabic sources. We then segment the data
using MADA according to the scheme explained
in Section 4.1.
90
The English source is aligned to the seg-
mented Arabic target using the standard
MOSES (MOSES, 2007) configuration of
GIZA++ (Och and Ney, 2000), which is IBM
Model 4, and decoding is done using the phrase-
based SMT system MOSES. We use a maximum
phrase length of 15 to account for the increase
in length of the segmented Arabic. We also
use a lexicalized bidirectional reordering model
conditioned on both the source and target sides,
with a distortion limit set to 6. We tune using
Och?s algorithm (Och, 2003) to optimize weights
for the distortion model, language model, phrase
translation model and word penalty over the
BLEU metric (Papineni et al, 2001). For the
segmented Arabic experiments, we experiment
with tuning using non-segmented Arabic as a
reference. This is done by recombining the output
before each tuning iteration is scored and has been
shown by Badr et. al (2008) to perform better than
using segmented Arabic as reference.
5.2 Data Used
We report results on three domains: newswire text,
UN data and spoken dialogue from the travel do-
main. It is important to note that the sentences
in the travel domain are much shorter than in the
news domain, which simplifies the alignment as
well as reordering during decoding. Also, since
the travel domain contains spoken Arabic, it is
more biased towards the Subject-Verb-Object sen-
tence order than the Verb-Subject-Object order
more common in the news domain. Also note
that since most of our data was originally intended
for Arabic-to-English translation, our test and tun-
ing sets have only one reference, and therefore,
the BLEU scores we report are lower than typi-
cal scores reported in the literature on Arabic-to-
English.
The news training data consists of several LDC
corpora2. We construct a test set by randomly
picking 2000 sentences. We pick another 2000
sentences randomly for tuning. Our final training
set consists of 3 million English words. We also
test on the NIST MT 05 ?test set while tuning on
both the NIST MT 03 and 04 test sets. We use the
first English reference of the NIST test sets as the
source, and the Arabic source as our reference. For
2LDC2003E05 LDC2003E09 LDC2003T18
LDC2004E07 LDC2004E08 LDC2004E11 LDC2004E72
LDC2004T18 LDC2004T17 LDC2005E46 LDC2005T05
LDC2007T24
Scheme
RandT MT 05
S NoS S NoS
Baseline 21.6 21.3 23.88 23.44
VP 21.9 21.5 23.98 23.58
NP 21.9 21.8
NP+PP 21.8 21.5 23.72 23.68
NP+PP+VP 22.2 21.8 23.74 23.16
NP+PP+VP+The 21.3 21.0
Table 1: Translation Results for the News Domain
in terms of the BLEU Metric.
the language model, we use 35 million words from
the LDC Arabic Gigaword corpus, plus the Arabic
side of the 3 million word training corpus. Exper-
imentation with different language model orders
shows that the optimal model orders are 4-grams
for the baseline system and 6-grams for the seg-
mented Arabic. The average sentence length is 33
for English, 25 for non-segmented Arabic and 36
for segmented Arabic.
To study the effect of syntactic reordering on
larger training data sizes, we use the UN English-
Arabic parallel text (LDC2003T05). We experi-
ment with two training data sizes: 30 million and
3 million words. The test and tuning sets are
comprised of 1500 and 500 sentences respectively,
chosen at random.
For the spoken domain, we use the BTEC 2007
Arabic-English corpus. The training set consists
of 200K words, the test set has 500 sentences and
the tuning set has 500 sentences. The language
model consists of the Arabic side of the training
data. Because of the significantly smaller data
size, we use a trigram LM for the baseline, and
a 4-gram for segmented Arabic. In this case, the
average sentence length is 9 for English, 8 for Ara-
bic, and 10 for segmented Arabic.
5.3 Translation Results
The translation scores for the News domain are
shown in Table 1. The notation used in the table is
as follows:
? S: Segmented Arabic
? NoS: Non-Segmented Arabic
? RandT: Scores for test set where sentences
were picked at random from NEWS data
? MT 05: Scores for the NIST MT 05 test set
The reordering notation is explained in Section
4.2. All results are in terms of the BLEU met-
91
S NoS
Short Long Short Long
Baseline 22.57 25.22 22.40 24.33
VP 22.95 25.05 22.95 24.02
NP+PP 22.71 24.76 23.16 24.067
NP+PP+VP 22.84 24.62 22.53 24.56
Table 2: Translation Results depending on sen-
tence length for NIST test set.
Scheme Score % Oracle reord
VP 25.76 59%
NP+PP 26.07 58%
NP+PP+VP 26.17 53%
Table 3: Oracle scores for combining baseline sys-
tem with other reordered systems.
ric. It is important to note that the gain that we
report in terms of BLEU are more significant that
comparable gains on test sets that have multiple
references, since our test sets have only one refer-
ence. Any amount of gain is a result of additional
n-gram precision with one reference. We note that
the gain achieved from the reordering of the non-
segmented and segmented systems are compara-
ble. Replicating the before adjectives hurts the
scores, possibly because it increases the sentence
length noticeably, and thus deteriorates the align-
ments? quality. We note that the gains achieved by
reordering on the NIST test set are smaller than
the improvements on the random test set. This is
due to the fact that the sentences in the NIST test
set are longer, which adversely affects the parsing
quality. The average English sentence length is 33
words in the NIST test set, while the random test
set has an average sentence length of 29 words.
Table 2 shows the reordering gains of the non-
segmented Arabic by sentence length. Short sen-
tences are sentences that have less that 40 words of
English, while long sentences have more than 40
words. Out of the 1055 sentence in the NIST test
set 719 are short and 336 are long. We also report
oracle scores in Table 3 for combining the base-
line system with the reordering systems, as well
as the percentage of oracle sentences produced by
the reordered system. The oracle score is com-
puted by starting with the reordered system?s can-
didate translations and iterating over all the sen-
tences one by one: we replace each sentence with
its corresponding baseline system translation then
Scheme 30M 3M
Baseline 32.17 28.42
VP 32.46 28.60
NP+PP 31.73 28.80
Table 4: Translation Results on segmentd UN data
in terms of the BLEU Metric.
compute the total BLEU score of the entire set. If
the score improves, then the sentence in question
is replaced with the baseline system?s translation,
otherwise it remains unchanged and we move on
to the next one.
In Table 4, we report results on the UN corpus
for different training data sizes. It is important to
note that although gains from VP reordering stay
constant when scaled to larger training sets, gains
from NP+PP reordering diminish. This is due to
the fact that NP reordering tend to be more local-
ized then VP reorderings. Hence with more train-
ing data the lexicalized reordering model becomes
more effective in reordering NPs.
In Table 5, we report results on the BTEC
corpus for different segmentation and reordering
scheme combinations. We should first point out
that all sentences in the BTEC corpus are short,
simple and easy to align. Hence, the gain intro-
duced by reordering might not be enough to offset
the errors introduced by the parsing. We also note
that spoken Arabic usually prefers the Subject-
Verb-Object sentence order, rather than the Verb-
Subject-Object sentence order of written Arabic.
This explains the fact that no gain is observed
when the verb phrase is reordered. Noun phrase
reordering produces a significant gain with non-
segmented Arabic. Replicating the definite arti-
cle the in the noun phrase does not create align-
ment problems as is the case with the newswire
data, since the sentences are considerably shorter,
and hence the 0.74 point gain observed on the seg-
mented Arabic system. That gain does not trans-
late to the non-segmented Arabic system since in
that case the definite article Al remains attached to
its head word.
6 Conclusion
This paper presented linguistically motivated rules
that reorder English to look like Arabic. We
showed that these rules produce significant gains.
We also studied the effect of the interaction be-
tween Arabic morphological segmentation and
92
Scheme S NoS
Baseline 29.06 25.4
VP 26.92 23.49
NP 27.94 26.83
NP+PP 28.59 26.42
The 29.8 25.1
Table 5: Translation Results for the Spoken Lan-
guage Domain in the BLEU Metric.
syntactic reordering on translation results, as well
as how they scale to bigger training data sizes.
Acknowledgments
We would like to thank Michael Collins, Ali Mo-
hammad and Stephanie Seneff for their valuable
comments.
References
Eleftherios Avramidis, and Philipp Koehn 2008. En-
riching Morphologically Poor Languages for Statis-
tical Machine Translation. In Proc. of ACL/HLT.
Ibrahim Badr, Rabih Zbib, and James Glass 2008. Seg-
mentation for English-to-Arabic Statistical Machine
Translation. In Proc. of ACL/HLT.
Michael Collins 1997. Three Generative, Lexicalized
Models for Statistical Parsing. In Proc. of ACL.
Michael Collins, Philipp Koehn, and Ivona Kucerova
2005. Clause Restructuring for Statistical Machine
Translation. In Proc. of ACL.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning 2005. Incorporating Non-local Informa-
tion into Information Extraction Systems by Gibbs
Sampling. In Proc. of ACL.
Nizar Habash, 2007. Syntactic Preprocessing for Sta-
tistical Machine Translation. In Proc. of the Ma-
chine Translation Summit (MT-Summit).
Nizar Habash and Owen Rambow, 2005. Arabic Tok-
enization, Part-of-Speech Tagging and Morphologi-
cal Disambiguation in One Fell Swoop. In Proc. of
ACL.
Nizar Habash and Fatiha Sadat, 2006. Arabic Pre-
processing Schemes for Statistical Machine Trans-
lation. In Proc. of HLT.
Philipp Koehn and Hieu Hoang, 2007. Factored
Translation Models. In Proc. of EMNLP/CNLL.
Young-Suk Lee, 2004. Morphological Analysis
for Statistical Machine Translation. In Proc. of
EMNLP.
MOSES, 2007. A Factored Phrase-based Beam-
search Decoder for Machine Translation. URL:
http://www.statmt.org/moses/.
Franz Och 2003. Minimum Error Rate Training in
Statistical Machine Translation. In Proc. of ACL.
Franz Och and Hermann Ney 2000. Improved Statisti-
cal Alignment Models. In Proc. of ACL.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu 2001. BLUE: a Method for Automatic
Evaluation of Machine Translation. In Proc. of
ACL.
Maja Popovic and Hermann Ney 2006. POS-based
Word Reordering for Statistical Machine Transla-
tion. In Proc. of NAACL LREC.
Adwait Ratnaparkhi 1996. A Maximum Entropy
Model for Part-of-Speech Tagging. In Proc. of
EMNLP.
Ruhi Sarikaya and Yonggang Deng 2007. Joint
Morphological-Lexical Language Modeling for Ma-
chine Translation. In Proc. of NAACL HLT.
Kristina Toutanova, Dan Klein, Christopher Manning,
and Yoram Singer. 2003. Feature-Rich Part-of-
Speech Tagging with a Cyclic Dependency Network.
In Proc. of NAACL HLT.
Chao Wang, Michael Collins, and Philipp Koehn 2007.
Chinese Syntactic Reordering for Statistical Ma-
chine Translation. In Proc. of EMNLP.
Fei Xia and Michael McCord 2004. Improving a
Statistical MT System with Automatically Learned
Rewrite Patterns. In COLING.
93
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 153?156,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Segmentation for English-to-Arabic Statistical Machine Translation
Ibrahim Badr Rabih Zbib
Computer Science and Artificial Intelligence Lab
Massachusetts Institute of Technology
Cambridge, MA 02139, USA
{iab02, rabih, glass}@csail.mit.edu
James Glass
Abstract
In this paper, we report on a set of ini-
tial results for English-to-Arabic Statistical
Machine Translation (SMT). We show that
morphological decomposition of the Arabic
source is beneficial, especially for smaller-size
corpora, and investigate different recombina-
tion techniques. We also report on the use
of Factored Translation Models for English-
to-Arabic translation.
1 Introduction
Arabic has a complex morphology compared to
English. Words are inflected for gender, number,
and sometimes grammatical case, and various cli-
tics can attach to word stems. An Arabic corpus
will therefore have more surface forms than an En-
glish corpus of the same size, and will also be more
sparsely populated. These factors adversely affect
the performance of Arabic?English Statistical Ma-
chine Translation (SMT). In prior work (Lee, 2004;
Habash and Sadat, 2006), it has been shown that
morphological segmentation of the Arabic source
benefits the performance of Arabic-to-English SMT.
The use of similar techniques for English-to-Arabic
SMT requires recombination of the target side into
valid surface forms, which is not a trivial task.
In this paper, we present an initial set of experi-
ments on English-to-Arabic SMT. We report results
from two domains: text news, trained on a large cor-
pus, and spoken travel conversation, trained on a sig-
nificantly smaller corpus. We show that segmenting
the Arabic target in training and decoding improves
performance. We propose various schemes for re-
combining the segmented Arabic, and compare their
effect on translation. We also report on applying
Factored Translation Models (Koehn and Hoang,
2007) for English-to-Arabic translation.
2 Previous Work
The only previous work on English-to-Arabic SMT
that we are aware of is by Sarikaya and Deng (2007).
It uses shallow segmentation, and does not make
use of contextual information. The emphasis of that
work is on using Joint Morphological-Lexical Lan-
guage Models to rerank the output.
Most of the related work, though, is on Arabic-to-
English SMT. Lee (2004) uses a trigram language
model to segment Arabic words. She then pro-
ceeds to deleting or merging some of the segmented
morphemes in order to make the segmented Arabic
source align better with the English target. Habash
and Sadat (2006) use the Arabic morphological an-
alyzer MADA (Habash and Rambow, 2005) to seg-
ment the Arabic source; they propose various seg-
mentation schemes. Both works show that the im-
provements obtained from segmentation decrease as
the corpus size increases. As will be shown later, we
observe the same trend, which is due to the fact that
the model becomes less sparse with more training
data.
There has been work on translating from En-
glish to other morphologically complex languages.
Koehn and Hoang (2007) present Factored Transla-
tion Models as an extension to phrase-based statisti-
cal machine translation models. Factored models al-
low the integration of additional morphological fea-
153
tures, such as POS, gender, number, etc. at the word
level on both source and target sides. The tighter in-
tegration of such features was claimed to allow more
explicit modeling of the morphology, and is better
than using pre-processing and post-processing tech-
niques. FactoredModels demonstrate improvements
when used to translate English to German or Czech.
3 Arabic Segmentation and
Recombination
As mentioned in Section 1, Arabic has a relatively
rich morphology. In addition to being inflected for
gender, number, voice and case, words attach to var-
ious clitics for conjunction (w+ ?and?)1, the definite
article (Al+ ?the?), prepositions (e.g. b+ ?by/with?,
l+ ?for?, k+ ?as?), possessive pronouns and object
pronouns (e.g. +ny ?me/my?, +hm ?their/them?). For
example, the verbal form wsnsAEdhm and the nomi-
nal formwbsyAratnA can be decomposed as follows:
(1) a. w+
and+
s+
will+
n+
we+
sAEd
help
+hm
+them
b. w+
and+
b+
with+
syAr
car
+At
+PL
+nA
+our
Also, Arabic is usually written without the diacritics
that denote the short vowels, and different sources
write a few characters inconsistently. These issues
create word-level ambiguity.
3.1 Arabic Pre-processing
Due to the word-level ambiguity mentioned above,
but more generally, because a certain string of char-
acters can, in principle, be either an affixed mor-
pheme or part of the base word, morphological
decomposition requires both word-level linguistic
information and context analysis; simple pattern
matching is not sufficient to detect affixed mor-
phemes. To perform pre-translation morphologi-
cal decomposition of the Arabic source, we use the
morphological analyzer MADA. MADA uses SVM-
based classifiers for features (such as POS, number
and gender, etc.) to choose among the different anal-
yses of a given word in context.
We first normalize the Arabic by changing final
?Y? to ?y? and the various forms of Alif hamza to bare
1In this paper, Arabic text is written using Buckwalter
transliteration
Alif. We also remove diacritics wherever they occur.
We then apply one of two morphological decompo-
sition schemes before aligning the training data:
1. S1: Decliticization by splitting off each con-
junction clitic, particle, definite article and
pronominal clitic separately. Note that plural
and subject pronoun morphemes are not split.
2. S2: Same as S1, except that the split clitics are
glued into one prefix and one suffix, such that
any given word is split into at most three parts:
prefix+ stem +suffix.
For example the word wlAwlAdh (?and for his kids?)
is segmented to w+ l+ AwlAd +P:3MS according to
S1, and to wl+ AwlAd +P:3MS according to S2.
3.2 Arabic Post-processing
As mentioned above, both training and decoding use
segmented Arabic. The final output of the decoder
must therefore be recombined into a surface form.
This proves to be a non-trivial challenge for a num-
ber of reasons:
1. Morpho-phonological Rules: For example, the
feminine marker ?p? at the end of a word
changes to ?t? when a suffix is attached to the
word. So syArp +P:1S recombines to syArty
(?my car?)
2. Letter Ambiguity: The character ?Y? (Alf
mqSwrp) is normalized to ?y?. In the recom-
bination step we need to be able to decide
whether a final ?y? was originally a ?Y?. For
example, mdy +P:3MS recombines to mdAh
?its extent?, since the ?y? is actually a Y; but fy
+P:3MS recombines to fyh ?in it?.
3. Word Ambiguity: In some cases, a word can
recombine into 2 grammatically correct forms.
One example is the optional insertion of nwn
AlwqAyp (protective ?n?), so the segmented
word lkn +O:1S can recombine to either lknny
or lkny, both grammatically correct.
To address these issues, we propose two recombina-
tion techniques:
1. R: Recombination rules defined manually. To
resolve word ambiguity we pick the grammat-
ical form that appears more frequently in the
154
training data. To resolve letter ambiguity we
use a unigram language model trained on data
where the character ?Y? had not been normal-
ized. We decide on the non-normalized from of
the ?y? by comparing the unigram probability of
the word with ?y? to its probability with ?Y?.
2. T: Uses a table derived from the training set
that maps the segmented form of the word to its
original form. If a segmented word has more
than one original form, one of them is picked
at random. The table is useful in recombin-
ing words that are split erroneously. For ex-
ample, qrDAy, a proper noun, gets incorrectly
segmented to qrDAn +P:1S which makes its re-
combination without the table difficult.
3.3 Factored Models
For the Factored Translation Models experiment, the
factors on the English side are the POS tags and the
surface word. On the Arabic side, we use the sur-
face word, the stem and the POS tag concatenated
to the segmented clitics. For example, for the word
wlAwlAdh (?and for his kids?), the factored words are
AwlAd and w+l+N+P:3MS. We use two language
models: a trigram for surface words and a 7-gram
for the POS+clitic factor. We also use a genera-
tion model to generate the surface form from the
stem and POS+clitic, a translation table from POS
to POS+clitics and from the English surface word to
the Arabic stem. If the Arabic surface word cannot
be generated from the stem and POS+clitic, we back
off to translating it from the English surface word.
4 Experiments
The English source is aligned to the segmented Ara-
bic target using GIZA++ (Och and Ney, 2000), and
the decoding is done using the phrase-based SMT
system MOSES (MOSES, 2007). We use a max-
imum phrase length of 15 to account for the in-
crease in length of the segmented Arabic. Tuning
is done using Och?s algorithm (Och, 2003) to op-
timize weights for the distortion model, language
model, phrase translation model and word penalty
over the BLEU metric (Papineni et al, 2001). For
our baseline system the tuning reference was non-
segmented Arabic. For the segmented Arabic exper-
iments we experiment with 2 tuning schemes: T1
Scheme Training Set Tuning Set
Baseline 34.6% 36.8%
R 4.04% 4.65%
T N/A 22.1%
T + R N/A 1.9%
Table 1: Recombination Results. Percentage of sentences
with mis-combined words.
uses segmented Arabic for reference, and T2 tunes
on non-segmented Arabic. The Factored Translation
Models experiments uses the MOSES system.
4.1 Data Used
We experiment with two domains: text news and
spoken dialogue from the travel domain. For the
news training data we used corpora from LDC2. Af-
ter filtering out sentences that were too long to be
processed by GIZA (> 85 words) and duplicate sen-
tences, we randomly picked 2000 development sen-
tences for tuning and 2000 sentences for testing. In
addition to training on the full set of 3 million words,
we also experimented with subsets of 1.6 million
and 600K words. For the language model, we used
20 million words from the LDC Arabic Gigaword
corpus plus 3 million words from the training data.
After experimenting with different language model
orders, we used 4-grams for the baseline system and
6-grams for the segmented Arabic. The English
source is downcased and the punctuations are sepa-
rated. The average sentence length is 33 for English,
25 for non-segmented Arabic and 36 for segmented
Arabic.
For the spoken language domain, we use the
IWSLT 2007 Arabic-English (Fordyce, 2007) cor-
pus which consists of a 200,000 word training set, a
500 sentence tuning set and a 500 sentence test set.
We use the Arabic side of the training data to train
the language model and use trigrams for the baseline
system and a 4-grams for segmented Arabic. The av-
erage sentence length is 9 for English, 8 for Arabic,
and 10 for segmented Arabic.
2Since most of the data was originally intended for Arabic-
to-English translation our test and tuning sets have only one
reference
155
4.2 Recombination Results
To test the different recombination schemes de-
scribed in Section 3.2, we run these schemes on
the training and development sets of the news data,
and calculate the percentage of sentences with re-
combination errors (Note that, on average, there
is one mis-combined word per mis-combined sen-
tence). The scores are presented in Table 1. The
baseline approach consists of gluing the prefix and
suffix without processing the stem. T +Rmeans that
the words seen in the training set were recombined
using scheme T and the remainder were recombined
using scheme R. In the remaining experiments we
use the scheme T + R.
4.3 Translation Results
The 1-reference BLEU score results for the news
corpus are presented in Table 2; those for IWSLT are
in Table 3. We first note that the scores are generally
lower than those of comparable Arabic-to-English
systems. This is expected, since only one refer-
ence was used to evaluate translation quality and
since translating to a more morphologically com-
plex language is a more difficult task, where there
is a higher chance of translating word inflections in-
correctly. For the news corpus, the segmentation of
Arabic helps but the gain diminishes as the training
data size increases, since the model becomes less
sparse. This is consistent with the larger gain ob-
tained from segmentation for IWSLT. The segmen-
tation scheme S2 performs slightly better than S1.
The tuning scheme T2 performs better for the news
corpus, while T1 is better for the IWSLT corpus.
It is worth noting that tuning without segmentation
hurts the score for IWSLT, possibly because of the
small size of the training data. Factored models per-
form better than our approach with the large train-
ing corpus, although at a significantly higher cost in
terms of time and required resources.
5 Conclusion
In this paper, we showed that making the Arabic
match better to the English through segmentation,
or by using additional translation model factors that
model grammatical information is beneficial, espe-
cially for smaller domains. We also presented sev-
eral methods for recombining the segmented Arabic
Large Medium Small
Training Size 3M 1.6M 0.6M
Baseline 26.44 20.51 17.93
S1 + T1 tuning 26.46 21.94 20.59
S1 + T2 tuning 26.81 21.93 20.87
S2 + T1 tuning 26.86 21.99 20.44
S2 + T2 tuning 27.02 22.21 20.98
Factored Models + tuning 27.30 21.55 19.80
Table 2: BLEU (1-reference) scores for the News data.
No Tuning T1 T2
Baseline 26.39 24.67
S1 29.07 29.82
S2 29.11 30.10 28.94
Table 3: BLEU (1-reference) scores for the IWSLT data.
target. Our results suggest that more sophisticated
techniques, such as syntactic reordering, should be
attempted.
Acknowledgments
We would like to thank Ali Mohammad, Michael Collins and
Stephanie Seneff for their valuable comments.
References
Cameron S. Fordyce 2007. Overview of the 2007 IWSLT Eval-
uation Campaign . In Proc. of IWSLT 2007.
Nizar Habash and Owen Rambow, 2005. Arabic Tokenization,
Part-of-Speech Tagging and Morphological Disambiguation
in One Fell Swoop. In Proc. of ACL.
Nizar Habash and Fatiha Sadat, 2006. Arabic Preprocessing
Schemes for Statistical Machine Translation. In Proc. of
HLT.
Philipp Koehn and Hieu Hoang, 2007. Factored Translation
Models. In Proc. of EMNLP/CNLL.
Young-Suk Lee, 2004. Morphological Analysis for Statistical
Machine Translation. In Proc. of EMNLP.
MOSES, 2007. A Factored Phrase-based Beam-
search Decoder for Machine Translation. URL:
http://www.statmt.org/moses/.
Franz Och, 2003. Minimum Error Rate Training in Statistical
Machine Translation. In Proc. of ACL.
Franz Och and Hermann Ney, 2000. Improved Statistical
Alignment Models. In Proc. of ACL.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
Zhu, 2001. Bleu: a Method for Automatic Evaluation of
Machine Translation. In Proc. of ACL.
Ruhi Sarikaya and Yonggang Deng 2007. Joint
Morphological-Lexical Language Modeling for Machine
Translation. In Proc. of NAACL HLT.
156
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 556?566,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Factored Soft Source Syntactic Constraints for Hierarchical Machine
Translation
Zhongqiang Huang
Raytheon BBN Technologies
50 Moulton St
Cambridge, MA, USA
zhuang@bbn.com
Jacob Devlin
Raytheon BBN Technologies
50 Moulton St
Cambridge, MA, USA
jdevlin@bbn.com
Rabih Zbib
Raytheon BBN Technologies
50 Moulton St
Cambridge, MA, USA
rzbib@bbn.com
Abstract
This paper describes a factored approach to
incorporating soft source syntactic constraints
into a hierarchical phrase-based translation
system. In contrast to traditional approaches
that directly introduce syntactic constraints to
translation rules by explicitly decorating them
with syntactic annotations, which often ex-
acerbate the data sparsity problem and cause
other problems, our approach keeps transla-
tion rules intact and factorizes the use of syn-
tactic constraints through two separate mod-
els: 1) a syntax mismatch model that asso-
ciates each nonterminal of a translation rule
with a distribution of tags that is used to
measure the degree of syntactic compatibil-
ity of the translation rule on source spans; 2)
a syntax-based reordering model that predicts
whether a pair of sibling constituents in the
constituent parse tree of the source sentence
should be reordered or not when translated to
the target language. The features produced
by both models are used as soft constraints
to guide the translation process. Experiments
on Chinese-English translation show that the
proposed approach significantly improves a
strong string-to-dependency translation sys-
tem on multiple evaluation sets.
1 Introduction
Hierarchical phrase-based translation models (Chi-
ang, 2007) are widely used in machine translation
systems due to their ability to achieve local flu-
ency through phrasal translation and handle non-
local phrase reordering using synchronous context-
free grammars. A large number of previous works
have tried to introduce grammaticality to the trans-
lation process by incorporating syntactic constraints
into hierarchical translation models. Despite some
differences in the granularity of syntax units (e.g.,
tree fragments (Galley et al, 2004; Liu et al, 2006),
treebank tags (Shen et al, 2008; Chiang, 2010), and
extended tags (Zollmann and Venugopal, 2006)),
most previous work incorporates syntax into hier-
archical translation models by explicitly decorating
translation rules with syntactic annotations. These
approaches inevitably exacerbate the data sparsity
problem and cause other problems such as increased
grammar size, worsened derivational ambiguity, and
unavoidable parsing errors (Hanneman and Lavie,
2013).
In this paper, we propose a factored approach
that incorporates soft source syntactic constraints
into a hierarchical string-to-dependency translation
model (Shen et al, 2008). The general ideas are ap-
plicable to other hierarchical models as well. Instead
of enriching translation rules with explicit syntactic
annotations, we keep the original translation rules
intact, and factorize the use of source syntactic con-
straints through two separate models.
The first is a syntax mismatch model that intro-
duces source syntax into the nonterminals of transla-
tion rules, and measures the degree of syntactic com-
patibility between a translation rule and the source
spans it is applied to during decoding. When a hi-
erarchical translation rule is extracted from a par-
allel training sentence pair, we determine a tag for
each nonterminal based on the dependency parse of
the source sentence. Instead of fragmenting rule
statistics by directly labeling nonterminals with tags,
556
we keep the original string-to-dependency transla-
tion rules intact and associate each nonterminal with
a distribution of tags. That distribution is then used
to measure the syntactic compatibility between the
syntactic context from which the translation rule is
extracted and the syntactic analysis of a test sen-
tence.
The second is a syntax-based reordering model
that takes advantage of phrasal cohesion in transla-
tion (Fox, 2002). The reordering model takes a pair
of sibling constituents in the source parse tree as in-
put, and uses source syntactic clues to predict the
ordering distribution (straight vs. inverted) of their
translations on the target side. The resulting order-
ing distribution is used in the decoder at the word
pair level to guide the translation process. This sep-
arate reordering model allows us to utilize source
syntax to improve reordering in hierarchical trans-
lation models without having to explicitly annotate
translation rules with source syntax.
Our results show that both the syntax mismatch
model and the syntax-based reordering model are
able to achieve significant gains over a strong
Chinese-English MT baseline. The rest of the pa-
per is organized as follows. Section 2 discusses
related work in the literature. Section 3 provides
an overview of our baseline string-to-dependency
translation system. Section 4 describes the details
of the syntax mismatch and syntax-based reordering
models. Experimental results are presented in Sec-
tion 5. The last section concludes the paper.
2 Related Work
Attempts to use rich syntactic annotations do not
always result in improved performance when com-
pared to purely hierarchical models that do not
use linguistic guidance. For example, as shown
in (Mi and Huang, 2008), tree-to-string translation
models (Huang et al, 2006) only start to outper-
form purely hierarchical models when significant ef-
forts were made to alleviate parsing errors by using
forest-based approaches in both rule extraction and
decoding. Using only syntactic phrases is too re-
strictive in phrasal translation as many useful phrase
pairs are not syntactic constituents (Koehn et al,
2003). The syntax-augmented translation model
of Zollmann and Venugopal (2006) annotates non-
terminals in hierarchical rules with thousands of ex-
tended syntactic categories in order to capture the
syntactic variations of phrase pairs. This results
in exacerbated data sparsity problems, partially due
to the requirement of exact matches in nonterminal
substitutions between translation rules in the deriva-
tion. Several solutions were proposed. Shen et
al. (2009) and Chiang (2010) used soft match fea-
tures to explicitly model the substitution of nonter-
minals with different labels; Venugopal et al (2009)
used a preference grammar to soften the syntactic
constraints through the use of a preference distribu-
tion of syntactic categories; and recently Hanneman
and Lavie (2013) proposed a clustering approach
to reduce the number of syntactic categories. Our
proposed syntax mismatch model associates non-
terminals with a distribution of tags. It is simi-
lar to the preference grammar in (Venugopal et al,
2009); however, we use treebank tags and focus on
the syntactic compatibility between translation rules
and the source sentence. The work of Huang et al
(2010) is most similar to ours, with the main differ-
ence being that their syntactic categories are latent
and learned automatically in a data driven fashion
while we simply use treebank tags based on depen-
dency parsing. Marton and Resnik (2008) also ex-
ploited soft source syntax constraints without mod-
ifying translation rules. However, they focused on
the quality of translation spans based on the syn-
tactic analysis of the source sentence, while our
method explicitly models the syntactic compatibil-
ity between translation rules and source spans.
Most research on reordering in machine transla-
tion focuses on phrase-based translation models as
they are inherently weak at non-local reordering.
Previous efforts to improve reordering for phrase-
based systems can be largely classified into two cat-
egories. Approaches in the first category try to re-
order words in the source sentence in a preprocess-
ing step to reduce reordering in both word alignment
and MT decoding. The reordering decisions are ei-
ther made using manual or automatically learned
rules (Collins et al, 2005; Xia and McCord, 2004;
Xia and McCord, 2004; Genzel, 2010) based on the
syntactic analysis of the source sentence, or con-
structed through an optimization procedure that uses
feature-based reordering models trained on a word-
aligned parallel corpus (Tromble and Eisner, 2009;
557
Khapra et al, 2013). Approaches in the second cate-
gory try to explicitly model phrase reordering in the
translation process. These approaches range from
simple distance based distortion models (Koehn et
al., 2003) that globally penalizes reordering based
on the distorted distance, to lexicalized reordering
models (Koehn et al, 2005; Al-Onaizan and Pap-
ineni, 2006) that assign reordering preferences of
adjacent phrases for individual phrases, and to hi-
erarchical reordering models (Galley and Manning,
2008; Cherry, 2013) that handle reordering prefer-
ences beyond adjacent phrases. Although hierarchi-
cal translation models are capable of handling non-
local reordering, their accuracy is far from perfect.
Xu et al (2009) showed that the syntax-augmented
hierarchical model (Zollmann and Venugopal, 2006)
also benefits from reordering source words in a pre-
processing step. Explicitly adding syntax to trans-
lation rules helps with reordering in general, but it
introduces additional complexities, and is still lim-
ited by the context-free nature of hierarchical rules.
Our work exploits an alternative direction that uses
an external reordering model to improve word re-
ordering of hierarchical models. Gao et al (2011),
Xiong et al (2012), and Li et al (2013) also studied
external reordering models for hierarchical models.
However, they focused on specific word pairs such
as a word and its dependents or a predicate and its
arguments, while our proposed general framework
considers all word pairs in a sentence. Our syntax-
based reordering model exploits phrasal cohesion in
translation (Fox, 2002) by modeling the reordering
of sibling constituents in the source parse tree, which
is similar to the recent work of Yang et al (2012).
However, the latter focuses on finding the optimal
reordering of sibling constituents before MT decod-
ing, while our proposed model generates reordering
features that are used together with other MT fea-
tures to determine the optimal reordering during MT
decoding.
3 String-to-Dependency Translation
Our baseline translation system is based on a string-
to-dependency translation model similar to the im-
plementation in (Shen et al, 2008). It is an extension
of the hierarchical translation model of Chiang et al
(2006) that requires the target side of a phrase pair
to have a well-formed dependency structure, defined
as either of the two types:
? fixed structure: a single rooted dependency
sub-tree with each child being a complete con-
stituent. In this case, the phrase has a unique
head word inside the phrase, i.e., the root of
the dependency sub-tree. Each dependent of
the head word, together with all of its descen-
dants, is either completely inside the phrase or
completely outside the phrase. For example,
the phrase give him in Figure 1 (a) has a fixed
dependency structure with head word give.
? floating structure: a sequence of siblings with
each being a complete constituent. In this case,
the phrase is composed of a sequence of sibling
constituents whose common parent is outside
the phrase. For example, the phrase him that
brown coat in Figure 1 is a floating structure
whose common parent give is not in the phrase.
Requiring the target side to have a well-formed
dependency structure is less restrictive than requir-
ing it to be a syntactic constituent, allowing more
translation rules to be extracted. However, it still
results in fewer rules than pure hierarchical transla-
tion models and might hurt MT performance. The
well-formed dependency structure on the target side
makes it possible to introduce syntax features dur-
ing decoding. Shen et al (2008) obtained signif-
icant improvements from including a dependency
language model score in decoding, outweighing the
negative effect of the dependency constraint. Shen et
al. (2009) proposed an approach to label each non-
terminal, which can be either on the left-hand-side
(LHS) or the right-hand-side (RHS) of the rule, with
the head POS tag of the underlying target phrase if
it has a fixed dependency structure1, and measure
the mismatches between nonterminal labels when a
RHS nonterminal of a rule is substantiated with the
LHS nonterminal of another rule during decoding.
This also resulted in further improvements in MT
performance. Figure 1 (c) shows an example string-
to-dependency translation rule in our baseline sys-
tem.
1Nonterminals corresponding to floating structures keep
their default label ?X? as experiments show that it is not bene-
ficial to label them differently.
558
X :  give  X
2 
 X
1
 
X  :  X
1
    X
2
(b) pure hierarchical rule
VV  :  give  PRP
2  
NN
1
 
X  :  X
1
    X
2
(c) string-to-dependency rule
??  ??  ?  ??    ?
 give   him
   
that  brown  coat 
(a) word alignments
Figure 1: An example of extracting a string-to-
dependency translation rule from word alignments. The
nonterminals on the target side of the hierarchical rule
(b) all correspond to fixed dependency structures and so
they are labeled by the respective head tag in the string-
to-dependency rule (c).
4 Factored Syntactic Constraints
Although the string-to-dependency formulation
helps to improve the grammaticality of translations,
it lacks the ability to incorporate source syntax into
the translation process. We next describe a factored
approach to address this problem by utilizing source
syntax through two models: one that introduces syn-
tactic awareness to translation rules themselves, and
another that focuses on reordering based on the syn-
tactic analysis of the source.
4.1 Syntax Mismatch Model
A straightforward method to introduce awareness
of source syntax to translation rules is to apply
the same well-formed dependency constraint and
head POS annotation on the target side of string-
to-dependency translation rules to the source side.
However, as discussed earlier, this would signifi-
cantly reduce the number of rules that can be ex-
tracted, exacerbate data sparsity, and cause other
problems, especially given that the target side is al-
ready constrained by the dependency requirement.
A relaxed method is to bypass the dependency
constraint and only annotate source nonterminals
whose underlying phrase is a fixed dependency
structure with the head POS tag of the phrase. This
method would still extract all of the rules that can
be extracted from the baseline string-to-dependency
VV  :  give  PRP
2  
NN
1
 
X  :  X
1
    X
2
2
4
VV : 0.7
NN : 0.1
X : 0.2
3
5
2
4
NN : 0.8
VV : 0.1
X : 0.1
3
5
2
4
PN : 0.5
NN : 0.4
X : 0.1
3
5
VV
(a) nonterminal tag distributions
source:
gross:
NN
PN
span tag:
(b) source span tags

?
his
? ?
pen
?
me


give
dependency:
Figure 2: Example distribution of tags for nonterminals
on the source side (a) and example tags for source spans
(b)
translation model, but the extra annotation on non-
terminals can split a rule into multiple rules, with the
only difference being the nonterminal labels on the
source side. Unfortunately, our experiments have
shown that even this moderate annotation results
in significantly lower translation quality due to the
fragmentation of translation rules, and the increased
derivational ambiguity. We have also tried to include
some source tag mismatch features (with details de-
scribed later) to measure the syntactic compatibility
between the nonterminal labels of a translation rule
and the corresponding tags of source spans. This im-
proves translation accuracy, but not enough to com-
pensate for the performance drop caused by annotat-
ing source nonterminals.
Our proposed method introduces syntax to trans-
lation rules without sacrificing performance. Instead
of imposing dependency constraints or explicitly an-
notating source nonterminals, we keep the original
string-to-dependency translation rules intact and as-
sociate each nonterminal on the source side with a
distribution of tags. The tags are determined based
on the dependency structure of training samples. If
the underlying source phrase of a nonterminal is a
fixed dependency structure in a training sample, we
use the head POS tag of the phrase as the tag. Oth-
erwise, we use the default tag ?X? to denote float-
559
Feature Condition Value
f1 ts = X P(tr = X)
f2 ts = X P(tr ? X)
f3 ts ? X P(tr = X)
f4 ts ? X P(tr = ts)
f5 ts ? X P(tr ? X, tr ? ts)
Table 1: Source tag mismatch features. The default value
of each feature is zero if the source span tag ts does not
match the condition
ing structures and dependency structures that are not
well formed. As a result, we still extract the same
set of rules as in the baseline string-to-dependency
translation model, and also obtain a distribution of
tags for each nonterminal. Figure 2 (a) illustrates the
example tag distributions of a string-to-dependency
translation rule. The tag distributions provide an ap-
proximation of the source syntax of the training data
from which the translation rules are extracted. They
are used to measure the syntactic compatibility be-
tween a translation rule and the source spans it is
applied to. At decoding time, we parse the source
sentence and assign each span a tag in the same way
as it is done during rule extraction, as shown in the
example in Figure 2 (b). When a translation rule is
used to expand a derivation, for each nonterminal
(which can be on the LHS or RHS) on the source
side of the rule, five source tag mismatch features
are computed based on the distribution of tags P(tr)
on the rule nonterminal, and the tag ts on the cor-
responding source span. The features are defined in
Table 1. We use soft features instead of hard syn-
tactic constraints, and allow the tuning process to
choose the appropriate weight for each feature. As
shown in Section 5, these source syntax mismatch
features help to improve the baseline system.
4.2 Syntax-based Reordering Model
Most previous research on reordering models has fo-
cused on improving word reordering for statistical
phrase-based translation systems (e.g., (Collins et
al., 2005; Al-Onaizan and Papineni, 2006; Tromble
and Eisner, 2009)). There has been less work on im-
proving the reordering of hierarchical phrase-based
translation systems (see (Xu et al, 2009; Gao et al,
2011; Xiong et al, 2012) for a few exceptions), ex-
cept through explicit syntactic annotation of transla-
tion rules. It is generally assumed that hierarchical
models are inherently capable of handling both lo-
cal and non-local reorderings. However, many hier-
archical translation rules are noisy and have limited
context, and so may not be able to produce transla-
tions in the right order.
We propose a general framework that incorpo-
rates external reordering information into the decod-
ing process of hierarchical translation models. To
simplify the presentation, we make the assumption
that every source word translates to one or more
target words, and that the translations for a pair
of source words is either straight or inverted. We
discuss the general case later. Given a sentence
w1,?,wn, suppose we have a separate reordering
model that predicts Porder(oij), the probability distri-
bution of ordering oij ? {straight, inverted} between
the translations of any source word pair (wi,wj).
We can measure the goodness of a given hypothe-
sis h with respect to the ordering predicted by the
reordering model as the sum of log probabilities2
for ordering each pair of source words, as defined
in Equation 1:
forder(h) = ?
1?i<j?n
log Porder(oij = ohij) (1)
where ohij is the ordering between the translations of
source word pair (wi,wj) in hypothesis h. The re-
ordering score forder(h) can be computed efficiently
through recursion during hierarchical decoding as
follows:
? Base case: for phrasal (i.e. non-hierarchical)
rules, the ordering of translations for any word
pair covered by the source phrase can be deter-
mined based on the word alignment of the rule.
The value of the reordering score can be simply
computed according to Equation 1.
? Recursive case: when a hierarchical rule is used
to expand a partial derivation, two types of
word pairs are encountered: a) word pairs that
are covered exclusively by one of the nonter-
minals on the RHS of the rule, and b) other
2In practice, the log probability is thresholded to avoid neg-
ative infinity, which would otherwise result in a hard constraint.
560
(a)
VV  :  give  PRP
2  
NN
1
 
X  :  X
1
    X
2
source:
gross:
 
his
 
pen

me

give
(b)
word pair translation order
(?, ) inverted
(?, ?) inverted
(?, ) inverted
(?, ?) inverted
(?, ) inverted
(?, ?) inverted
(, ?) straight
(?, ?) previously considered
(?, ?) previously considered
(?, ?) previously considered
Figure 3: An example rule application (a) with the trans-
lation order of new source word pairs covered by the rule
shown in (b). The translation order of word pairs covered
by X1 is previously considered and is thus not shown.
word pairs. The reordering scores of the for-
mer would be already computed in previous
rule applications, and can simply be retrieved
from the partial derivation. Word pairs of the
latter case are new word pairs introduced by the
hierarchical rule, and their ordering can be de-
termined based on the alignment of the hierar-
chical rule. The value of the reordering score
of the new derivation is the sum of the reorder-
ing scores retrieved from the partial derivations
for the nonterminals and the reordering scores
of the new word pairs.
Figure 3 shows an example of determining the
ordering of translations when applying a string-to-
dependency rule. The alignment in the translation
rule is able to fully determine the translation order
for all new word pairs introduced by the rule. For
example, ??/pen? is covered by X1 in the rule and
the translation order for X1 and ??/give? is inverted
on the target side. Since ??/pen? is translated to-
gether with other words covered by X1 as a group,
we can determine that the translation order between
the source word pair ??/pen? and ??/give? is also
inverted on the target side. The words ??/his?,
???, ?? /pen? are all covered by the same nonter-
Reordering features
The syntactic production rule
The syntactic labels of the nodes in the context
The head POS tags of the nodes in the context
The dep. labels of the nodes in the context
The seq. of dep. labels connecting the two nodes
The length of the nodes in the context
Table 2: Features in the reordering model
minal X1 and thus their pairwise reordering scores
have already been considered in previous rule appli-
cations.
In practice, not all source words in a translation
rule are translated to a target word; sometimes there
is no clear ordering between the translations of two
source words. In such cases we use a binary discount
feature instead of the reordering feature.
This reordering framework relies on an external
model to provide the ordering probability distribu-
tion of source word pairs. In this paper, we inves-
tigate a simple maximum-entropy reordering model
based on the syntactic parse tree of the source sen-
tence. This allows us to take advantage of the source
syntax to improve reordering without using syntactic
annotations in translation rules. The syntax-based
reordering model attempts to predict the reordering
probability of a pair of sibling constituents in the
source parse tree, building on the fact that syntac-
tic phrases tend to move in a group during transla-
tion (Fox, 2002). The reordering model is trained on
a word-aligned corpus. For each pair of sibling con-
stituents in the source parse tree, we determine the
translation order on the target side based on word
alignments. If there is a clear ordering3, i.e., either
straight or inverted, on the target side, we include
the context of the constituent pair and its translation
order as a sample for training or evaluating the max-
imum entropy reordering model. Table 2 lists the
features of the reordering model.
The ordering distributions of source word pairs
are determined based on the ordering distributions
of sibling constituent pairs. For each pair of sib-
3If the translations overlap with other, the non-overlapping
parts are used to determine the translation order.
561
ling constituents4 in the parse tree of a source sen-
tence, we compute its distribution of translation or-
der using the reordering model. The distribution is
shared among all word pairs covered by the respec-
tive constituents, which guarantees that the order-
ing distribution of any source word pair is computed
exactly once. The ordering distributions of source
word pairs are then used through the general reorder-
ing framework in the decoder to guide the decoding
process.
5 Experiments
5.1 Experimental Setup
Our main experiments use the Chinese-English par-
allel training data and development sets released by
the LDC, and made available to the DARPA GALE
and BOLT programs. We train the translation model
on 100 million words of parallel data. We use a 8 bil-
lion words of English monolingual data to train two
language models: a trigram language model used in
chart decoding, and a 5-gram language model used
in n-best rescoring. The systems are tuned and eval-
uated on a mixture of newswire and web forum text
from the development sets available for the DARPA
GALE and BOLT programs, with up to 4 indepen-
dent references for each source sentence. We also
evaluate our final systems on both newswire and
web text from the NIST MT06 and MT08 evalua-
tions using an experimental setup compatible with
the NIST MT12 Chinese-English constrained track.
In this setup, the translation and language models
are trained on 35 million words of parallel data and
3.8 billion words of English monolingual data, re-
spectively. The systems are tuned on the MT02-
05 development sets. All systems are tuned and
evaluated on IBM BLEU (Papineni et al, 2002).
The baseline string-to-dependency translation sys-
tem uses more than 10 core features and a large num-
ber of sparse binary features similar to the method
described in (Chiang et al, 2009). It achieves trans-
lation accuracies comparable to the top ranked sys-
tems in the NIST MT12 evaluation.
4Note that the constituent pairs used to train the reordering
model are filtered to only contain these with clear ordering on
the target side, while no such pre-filtering is applied to con-
stituent pairs when applying the reordering model in translation.
We leave it to future work to address this mismatch problem.
GIZA++ (Och and Ney, 2003) is used for auto-
matic word alignment in all of the experiments. We
use Charniak?s parser (Charniak and Johnson, 2005)
on the English side to obtain string-to-dependency
translation rules, and use a latent variable PCFG
parser (Huang and Harper, 2009) to parse the source
side of the parallel training data as well as the
test sentences for extracting syntax mismatch and
reordering features. For both languages, depen-
dency structures are read off constituency trees us-
ing manual head word percolation rules. We use
a lexicon-based longest-match-first word segmenter
to tokenize source Chinese sentences. Since the
source tokenization used in our MT system is dif-
ferent from the treebank tokenization used to train
the Chinese parser, the source sentences are first to-
kenized using the treebank-trained Stanford Chinese
segmenter (Tseng et al, 2005), then parsed with
the Chinese parser, and finally projected to MT tok-
enization based on the character alignment between
the tokens. The syntax-based reordering model is
trained on a set of Chinese-English manual word
alignment corpora released by the LDC5.
5.2 Syntax Mismatch Model
We first conduct experiments on the GALE/BOLT
data sets to evaluate different strategies of incor-
porating source syntax into string-to-dependency
translation rules. As mentioned in Section 4.1, con-
straining the source side of translation rules to only
well-formed dependency structures is too restrictive
given that our baseline system already has depen-
dency constraint on the target side. We evaluate
the relaxed method that only annotates source non-
terminals with the head POS tag of the underlying
phrase if the phrase is a fixed dependency structure.
As shown in Table 3, nonterminal annotation results
in a big drop in performance, decreasing the BLEU
score of the baseline from 27.82 to 25.54. This sug-
gests that it is undesirable to further fragment the
translation rules. Introducing the syntax mismatch
features described in Section 4.1 helps to improve
5The alignment corpora are LDC2012E24, LDC2012E72,
LDC2012E95, and LDC2013E02. The reordering model can
also be trained on automatically aligned data; however, our ex-
periments show that using manual alignments results in a bet-
ter accuracy for the reordering model itself and more improve-
ments for the MT system.
562
BLEU
baseline 27.82
+ tag annotation only 25.54
+ tag annotation, mismatch feat. 25.90
+ tag distribution, mismatch feat. 28.23
Table 3: Effects of tag annotation, tag distribution, and
syntax mismatch features on MT performance on the
GALE/BOLT data set.
BLEU from 25.54 to 25.90. This improvement is
not large enough to compensate for the performance
drop caused by annotating the nonterminals.
Our proposed approach, on the other hand, does
not modify the translation rules in the baseline sys-
tem, but only associates each nonterminal with a dis-
tribution of tags. For that reason, it does not suffer
from the aforementioned problem. It achieves ex-
actly the same performance as the baseline system
if no source syntactic constraints are imposed dur-
ing decoding. When the source syntax mismatch
features are used, the proposed approach is able to
achieve a gain of 0.41 in BLEU over the baseline
system. Table 4 lists the learned weights of the syn-
tax mismatch features after MT tuning. The nega-
tive weights of f1 and f2 mean that the MT system
penalizes source spans that do not have a fixed de-
pendency structure, and it assigns a higher penalty
to rules whose nonterminals have a high probability
of being extracted from source phrases that do not
have a fixed dependency structure. When the source
span has a fixed dependency structure, the MT sys-
tem prefers translation rules that have a high proba-
bility of matching the tag on the source span (feature
f4) over the ones that do not match (features f3 and
f5). This result is consistent with our expectations
of the syntax mismatch features.
Feature Description Weight
f1 ts = X, tr = X ?1.543
f2 ts = X, tr ? X ?0.676
f3 ts ? X, tr = X 0.380
f4 ts ? X, tr ? X, tr = ts 1.677
f5 ts ? X, tr ? X, tr ? ts 0.232
Table 4: Learned syntax mismatch feature weight
5.3 Syntax-based Reordering Model
Before evaluating the syntax-based reordering
model, we would like to establish the upper bound
improvement that could be achieved using the gen-
eral reordering framework for hierarchical transla-
tion models. Towards that goal, we conduct an ora-
cle experiment on the GALE/BOLT development set
that uses the oracle translation order from the ref-
erence as the external reordering model. For each
source sentence in the development set, we pair it
with the first reference translation (out of up to 4 in-
dependent translations). We then add the sentence
pairs from the development set to the parallel train-
ing data and run GIZA++ to obtain word alignments.
We consider the GIZA++ word alignments for the
development set to be all correct, and use it to de-
termine the oracle order in the reference translation.
For the ordering distribution, we set the log proba-
bility of the reference translation order to 0 and the
reverse order to -1 to avoid negative infinity. As
shown in Table 5, the system tuned and evaluated
with the oracle reordering model significantly out-
performs the baseline by a large margin of 2.32 in
BLEU on the GALE/BOLT test set. This suggests
that there is room for potential improvement by us-
ing a fairly trained reordering model.
BLEU
baseline 27.82
+ oracle reorder 30.14
+ syntax reorder 28.40
Table 5: Effects of external reordering features on MT
performance on the GALE/BOLT test set.
We next evaluate the syntax-based reordering
model. We train the model on manually aligned
Chinese-English corpora. Since the tokenization
used in the manual alignment corpora is different
from the tokenization used in our MT system, the
manual alignment is projected to the MT tokeniza-
tion based on the character alignment between the
tokens. Some extraneously tagged alignment links
in the manual alignment corpora are not useful for
machine translation and are thus removed before
projecting the alignment. As described in Sec-
tion 4.2, the syntax-based reordering method mod-
563
els the translation order of sibling constituent pairs
in the source parse tree. As a result of strong phrasal
cohesion (Fox, 2002), we find that 94% of con-
stituent pairs have a clear ordering on the target
side. We only retain these constituent pairs for train-
ing and evaluating the reordering model. In order
to evaluate the accuracy of the maximum entropy
reordering model, we divide the manual alignment
corpora into 2/3 for training and 1/3 for evaluation.
A baseline that only chooses the majority order (i.e.
straight) has an accuracy of 69%, while the syntax-
based reordering model improves the accuracy to
79%.
The final reordering model used in MT is trained
on all of the samples extracted from the manual
alignment corpora. As shown in Table 5, the syntax-
based reordering feature improves the baseline by
0.58 in BLEU, which is a good improvement given
our strong baseline. Table 6 lists the number of
shifting errors in TER measurement (Snover et al,
2006) of various systems on the GALE/BOLT test
set. The syntax-based reordering model achieves a
6.1% reduction in the number of shifting errors in
the baseline system, and its combination with the
syntax mismatch model achieves an additional re-
duction of 0.6%. This suggests that the proposed
method helps to improve word reordering in transla-
tion.
Shifting errors
baseline 3205
+ syntax mismatch 3089
+ syntax reorder 3010
+ syntax mismatch and reorder 2990
Table 6: Number of shifting errors in TER measurement
of multiple systems on the GALE/BOLT test set
5.4 Final Results
Table 7 shows the final results on the GALE/BOLT
test set, as well as the NIST MT06 and MT08 test
sets. Both the syntax mismatch and the syntax-based
reordering features improve the baseline system, re-
sulting in moderate to significant gains in all of the
five test sets. The two features are complementary
to each other and their combination results in better
improvement in four out of the five test sets com-
pared to adding them separately. In three out of the
five test sets, the improvement from the combina-
tion of the two features is statistically significant at
the 95% confidence level over the baseline, with the
largest absolute improvement of 1.43 in BLEU ob-
tained on MT08 web.
6 Conclusion
In this paper, We have discussed problems resulting
from explicitly decorating translation rules with syn-
tactic annotations. We presented a factored approach
to incorporate soft source syntax mismatch and re-
ordering constraints to hierarchical machine transla-
tion, and showed how our models avoid the pitfalls
of the explicit decoration approach. Experiments on
Chinese-English translation show that the proposed
approach significantly improves a strong string-to-
dependency translation baseline on multiple evalu-
ation sets. There are many directions in which this
work can be continued. The syntax mismatch model
can be extended to dynamically adjust the transla-
tion distribution based on the syntactic compatibil-
ity between a translation rule and a source sentence.
It also might be beneficial to look beyond syntactic
constituent pairs when modeling reordering, given
that phrasal cohesion does not always hold in trans-
lation. The general framework that uses an external
reordering model in hierarchical models via features
can also be naturally extended to use multiple re-
ordering models.
Acknowledgments
This work was supported in part by DARPA/IPTO
Contract No. HR0011-12-C-0014 under the BOLT
Program. The views expressed are those of the au-
thors and do not reflect the official policy or position
of the Department of Defense or the U.S. Govern-
ment. The authors would like to thank the anony-
mous reviewers for their helpful comments.
References
Yaser Al-Onaizan and Kishore Papineni. 2006. Distor-
tion models for statistical machine translation. In Pro-
ceedings of the Annual Meeting of the Association for
Computational Linguistics.
564
MT06 news MT06 web MT08 news MT08 web GALE/BOLT
baseline 43.76 36.13 40.52 27.78 27.82
+ syntax mismatch 43.89 36.72 40.82+ 28.54- 28.23-
+ syntax reorder 44.01 36.40 41.23/ 28.95/ 28.40/
+ syntax mismatch and reorder 44.28+ 36.43+ 41.14- 29.21/ 28.62/
improvement over baseline +0.52 +0.30 +0.62 +1.43 +0.8
Table 7: Results on Chinese-English MT. The symbols +, -, and / indicate that the system is better than the baseline
at the 85%, 95%, and 99% confidence levels, respectively, as defined in (Koehn, 2004).
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proceedings of the Annual Meeting of the As-
sociation for Computational Linguistics.
Colin Cherry. 2013. Improved reordering for phrase-
based translation using sparse features. In Proceed-
ings of the Conference of the North American Chapter
of the Association for Computational Linguistics.
David Chiang, Mona Diab, Nizar Habash, Owen Ram-
bow, and Safiullah Shareef. 2006. Parsing Arabic di-
alects. In Conference of the European Chapter of the
Association for Computational Linguistics.
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 new features for statistical machine translation.
In Proceedings of the Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics.
David Chiang. 2010. Learning to translate with source
and target syntax. In Proceedings of the Annual Meet-
ing of the Association for Computational Linguistics.
Michael Collins, Philipp Koehn, and Ivona Kuc?erova?.
2005. Clause restructuring for statistical machine
translation. In Proceedings of the Annual Meeting of
the Association for Computational Linguistics.
Heidi Fox. 2002. Phrasal cohesion and statistical ma-
chine translation. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing.
Michel Galley and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. In Proceedings of the Conference on Empir-
ical Methods in Natural Language Processing.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2004. What?s in a translation rule. In Pro-
ceedings of the Conference of the North American
Chapter of the Association for Computational Linguis-
tics on Human Language Technology.
Yang Gao, Philipp Koehn, and Alexandra Birch. 2011.
Soft dependency constraints for reordering in hierar-
chical phrase-based translation. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing.
Dmitriy Genzel. 2010. Automatically learning source-
side reordering rules for large scale machine transla-
tion. In Proceedings of the International Conference
on Computational Linguistics.
Greg Hanneman and Alon Lavie. 2013. Improving
syntax-augmented machine translation by coarsening
the label set. In Proceedings of the Conference of the
North American Chapter of the Association for Com-
putational Linguistics.
Zhongqiang Huang and Mary Harper. 2009. Self-
training PCFG grammars with latent annotations
across languages. In Proceedings of the Conference
on Empirical Methods in Natural Language Process-
ing.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
A syntax-directed translator with extended domain of
locality. In Proceedings of the Workshop on Computa-
tionally Hard Problems and Joint Inference in Speech
and Language Processing.
Zhongqiang Huang, Martin C?mejrek, and Bowen Zhou.
2010. Soft syntactic constraints for hierarchical
phrase-based translation using latent syntactic distri-
butions. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing.
Mitesh M. Khapra, Ananthakrishnan Ramanathan, and
Karthik Visweswariah. 2013. Improving reordering
performance using higher order and structural features.
In Proceedings of the Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of the Conference of the North American Chap-
ter of the Association for Computational Linguistics
on Human Language Technology.
Philipp Koehn, Amittai Axelrod, Alexandra Birch, Chris
Callison-Burch, Miles Osborne, and David Talbot.
2005. Edinburgh system description for the 2005 iwslt
565
speech translation evaluation. In International Work-
shop on Spoken Language Translation.
Philipp Koehn. 2004. Pharaoh: A bean search decoder
for phrase-based statistical machine translation mod-
els. In Proceedings of the Conference of Association
for Machine Translation in the Americas.
Junhui Li, Philip Resnik, and Hal Daume. 2013. Mod-
eling syntactic and semantic structures in hierarchi-
cal phrase-based translation. In Proceedings of the
Annual Meeting of the Association for Computational
Linguistics.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine trans-
lation. In Proceedings of the Annual Meeting of the
Association for Computational Linguistics.
Yuval Marton and Philip Resnik. 2008. Soft syntactic
constraints for hierarchical phrased-based translation.
In Proceedings of the Annual Meeting of the Associa-
tion for Computational Linguistics.
Haitao Mi and Liang Huang. 2008. Forest-based transla-
tion rule extraction. In Proceedings of the Conference
on Empirical Methods in Natural Language Process-
ing.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computional Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalua-
tion of machine translation. In Proceedings of the An-
nual Meeting on Association for Computational Lin-
guistics.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proceedings of the Annual Meeting of the Association
for Computational Linguistics.
Libin Shen, Jinxi Xu, Bing Zhang, Spyros Matsoukas,
and Ralph Weischedel. 2009. Effective use of linguis-
tic and contextual information for statistical machine
translation. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of Association for Machine Translation
in the Americas.
Roy Tromble and Jason Eisner. 2009. Learning linear or-
dering problems for better translation. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing.
Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel
Jurafsky, and Christopher Manning. 2005. A condi-
tional random field word segmenter. In Proceedings
of the SIGHAN Workshop on Chinese Language Pro-
cessing.
Ashish Venugopal, Andreas Zollmann, Noah A. Smith,
and Stephan Vogel. 2009. Preference grammars: soft-
ening syntactic constraints to improve statistical ma-
chine translation. In Proceedings of the Conference
of the North American Chapter of the Association for
Computational Linguistics.
Fei Xia and Michael McCord. 2004. Improving a sta-
tistical mt system with automatically learned rewrite
patterns. In Proceedings of the International Confer-
ence on Computational Linguistics.
Deyi Xiong, Min Zhang, and Haizhou Li. 2012. Model-
ing the translation of predicate-argument structure for
smt. In Proceedings of the Annual Meeting of the As-
sociation for Computational Linguistics.
Peng Xu, Jaeho Kang, Michael Ringgaard, and Franz
Och. 2009. Using a dependency parser to improve smt
for subject-object-verb languages. In Proceeding of
the Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics.
Nan Yang, Mu Li, Dongdong Zhang, and Nenghai Yu.
2012. A ranking-based approach to word reordering
for statistical machine translation. In Proceedings of
the Annual Meeting of the Association for Computa-
tional Linguistics.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
Proceedings of the Workshop on Statistical Machine
Translation.
566
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 49?59,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Machine Translation of Arabic Dialects
Rabih Zbib, Erika Malchiodi, Jacob Devlin, David Stallard, Spyros Matsoukas,
Richard Schwartz, John Makhoul, Omar F. Zaidan?, Chris Callison-Burch?
Raytheon BBN Technologies, Cambridge MA
?Microsoft Research, Redmond WA
?Johns Hopkins University, Baltimore MD
Abstract
Arabic Dialects present many challenges for
machine translation, not least of which is the
lack of data resources. We use crowdsourc-
ing to cheaply and quickly build Levantine-
English and Egyptian-English parallel cor-
pora, consisting of 1.1M words and 380k
words, respectively. The dialectal sentences
are selected from a large corpus of Arabic web
text, and translated using Amazon?s Mechan-
ical Turk. We use this data to build Dialec-
tal Arabic MT systems, and find that small
amounts of dialectal data have a dramatic im-
pact on translation quality. When translating
Egyptian and Levantine test sets, our Dialec-
tal Arabic MT system performs 6.3 and 7.0
BLEU points higher than a Modern Standard
Arabic MT system trained on a 150M-word
Arabic-English parallel corpus.
1 Introduction
The Arabic language is a well-known example of
diglossia (Ferguson, 1959), where the formal vari-
ety of the language, which is taught in schools and
used in written communication and formal speech
(religion, politics, etc.) differs significantly in its
grammatical properties from the informal varieties
that are acquired natively, which are used mostly for
verbal communication. The spoken varieties of the
Arabic language (which we refer to collectively as
Dialectal Arabic) differ widely among themselves,
depending on the geographic distribution and the
socio-economic conditions of the speakers, and they
diverge from the formal variety known as Mod-
ern Standard Arabic (MSA) (Embarki and Ennaji,
2011). Significant differences in the phonology,
morphology, lexicon and even syntax render some
of these varieties mutually incomprehensible.
The use of Dialectal Arabic has traditionally been
confined to informal personal speech, while writ-
ing has been done almost exclusively using MSA
(or its ancestor Classical Arabic). This situation is
quickly changing, however, with the rapid prolifer-
ation of social media in the Arabic-speaking part
of the world, where much of the communication
is composed in dialect. The focus of the Arabic
NLP research community, which has been mostly on
MSA, is turning towards dealing with informal com-
munication, with the introduction of the DARPA
BOLT program. This new focus presents new chal-
lenges, the most obvious of which is the lack of di-
alectal linguistic resources. Dialectal text, which is
usually user-generated, is also noisy, and the lack
of standardized orthography means that users often
improvise spelling. Dialectal data also includes a
wider range of topics than formal data genres, such
as newswire, due to its informal nature. These chal-
lenges require innovative solutions if NLP applica-
tions are to deal with Dialectal Arabic effectively.
In this paper:
? We describe a process for cheaply and quickly
developing parallel corpora for Levantine-
English and Egyptian-English using Amazon?s
Mechanical Turk crowdsourcing service (?3).
? We use the data to perform a variety of machine
translation experiments showing the impact of
morphological analysis, the limited value of
adding MSA parallel data, the usefulness of
cross-dialect training, and the effects of trans-
lating from dialect to MSA to English (?4).
We find that collecting dialect translations has a low
cost ($0.03/word) and that relatively small amounts
of data has a dramatic impact on translation quality.
When trained on 1.5M words of dialectal data, our
system performs 6.3 to 7.0 BLEU points higher than
when it is trained on 100 times more MSA data from
a mismatching domain.
49
2 Previous Work
Existing work on natural language processing of Di-
alectal Arabic text, including machine translation, is
somewhat limited. Previous research on Dialectal
Arabic MT has focused on normalizing dialectal in-
put words into MSA equivalents before translating
to English, and they deal with inputs that contain
a limited fraction of dialectal words. Sawaf (2010)
normalized the dialectal words in a hybrid (rule-
based and statistical) MT system, by performing a
combination of character- and morpheme-level map-
pings. They then translated the normalized source
to English using a hybrid MT or alternatively a
Statistical MT system. They tested their method
on proprietary test sets, observing about 1 BLEU
point (Papineni et al, 2002) increase on broadcast
news/conversation and about 2 points on web text.
Salloum and Habash (2011) reduced the proportion
of dialectal out-of-vocabulary (OOV) words also by
mapping their affixed morphemes to MSA equiva-
lents (but did not perform lexical mapping on the
word stems). They allowed for multiple morpho-
logical analyses, passing them on to the MT system
in the form of a lattice. They tested on a subset of
broadcast news and broadcast conversation data sets
consisting of sentences that contain at least one re-
gion marked as non-MSA, with an initial OOV rate
against an MSA training corpus of 1.51%. They
obtained a 0.62 BLEU point gain. Abo Bakr et
al. (2008) suggested another hybrid system to map
Egyptian Arabic to MSA, using morphological anal-
ysis on the input and an Egyptian-MSA lexicon.
Other work that has focused on tasks besides MT
includes that of Chiang et al (2006), who built a
parser for spoken Levantine Arabic (LA) transcripts
using an MSA treebank. They used an LA-MSA
lexicon in addition to morphological and syntac-
tic rules to map the LA sentences to MSA. Riesa
and Yarowsky (2006) built a statistical morphologi-
cal segmenter for Iraqi and Levantine speech tran-
scripts, and showed that they outperformed rule-
based segmentation with small amounts of training.
Some tools exist for preprocessing and tokenizing
Arabic text with a focus on Dialectal Arabic. For ex-
ample, MAGEAD (Habash and Rambow, 2006) is a
morphological analyzer and generator that can ana-
lyze the surface form of MSA and dialect words into
their root/pattern and affixed morphemes, or gener-
ate the surface form in the opposite direction.
Amazon?s Mechanical Turk (MTurk) is becom-
ing an essential tool for creating annotated resources
for computational linguistics. Callison-Burch and
Dredze (2010) provide an overview of various tasks
for which MTurk has been used, and offer a set of
best practices for ensuring high-quality data.
Zaidan and Callison-Burch (2011a) studied the
quality of crowdsourced translations, by quantifying
the quality of non-professional English translations
of 2,000 Urdu sentences that were originally trans-
lated by the LDC. They demonstrated a variety of
mechanisms that increase the translation quality of
crowdsourced translations to near professional lev-
els, with a total cost that is less than one tenth the
cost of professional translation.
Zaidan and Callison-Burch (2011b) created the
Arabic Online Commentary (AOC) dataset, a 52M-
word monolingual dataset rich in dialectal content.
Over 100k sentences from the AOC were annotated
by native Arabic speakers on MTurk to identify the
dialect level (and dialect itself) in each, and the col-
lected labels were used to train automatic dialect
identification systems. Although a large number
of dialectal sentences were identified (41% of sen-
tences), none were passed on to a translation phase.
3 Data Collection and Annotation
Following Zaidan and Callison-Burch (2011a,b), we
use MTurk to identify Dialectal Arabic data and to
create a parallel corpus by hiring non-professional
translators to translate the sentences that were la-
beled as being dialectal. We had Turkers perform
three steps for us: dialect classification, sentence
segmentation, and translation.
Since Dialectal Arabic is much less common in
written form than in spoken form, the first challenge
is to simply find instances of written Dialectal Ara-
bic. We draw from a large corpus of monolingual
Arabic text (approximately 350M words) that was
harvested from the web by the LDC, largely from
weblog and online user groups.1 Before present-
ing our data to annotators, we filter it to identify
1Corpora: LDC2006E32, LDC2006E77, LDC2006E90,
LDC2007E04, LDC2007E44, LDC2007E102, LDC2008E41,
LDC2008E54, LDC2009E14, LDC2009E93.
50
M
ag
hr
eb
i
E
gy
Ir
aq
i
G
ul
f
Ot
he
r
L
ev
Figure 1: One possible breakdown of spoken Arabic into
dialect groups: Maghrebi, Egyptian, Levantine, Gulf and
Iraqi. Habash (2010) gives a breakdown along mostly
the same lines. We used this map as an illustration for
annotators in our dialect classification task (Section 3.1),
with Arabic names for the dialects instead of English.
segments most likely to be dialectal (unlike Zaidan
and Callison-Burch (2011b), who did no such pre-
filtering). We eliminate documents with a large per-
centage of non-Arabic or MSA words. We then
retain documents that contain some number of di-
alectal words, using a set of manually selected di-
alectal words that was assembled by culling through
the transcripts of the Levantine Fisher and Egyp-
tian CallHome speech corpora. After filtering, the
dataset contained around 4M words, which we used
as a starting point for creating our Dialectal Arabic-
English parallel corpus.
3.1 Dialect Classification
To refine the document set beyond our keyword fil-
tering heuristic and to label which dialect each doc-
ument is written in, we hire Arabic annotators on
MTurk to perform classification similar to Zaidan
and Callison-Burch (2011b). Annotators were asked
to classify the filtered documents for being in MSA
or in one of four regional dialects: Egyptian, Lev-
antine, Gulf/Iraqi or Maghrebi, and were shown the
map in Figure 1 to explain what regions each of the
dialect labels corresponded to. We allowed an addi-
tional ?General? dialect option for ambiguous docu-
ments. Unlike Zaidan and Callison-Burch, our clas-
sification was applied to whole documents (corre-
sponding to a user online posting) instead of individ-
ual sentences. To perform quality control, we used
a set of documents for which correct labels were
known. We presented these 20% of the time, and
Dialect Classification HIT $10,064
Sentence Segmentation HIT $1,940
Translation HIT $32,061
Total cost $44,065
Num words translated 1,516,856
Cost per word 2.9 cents/word
Table 1: The total costs for the three MTurk subtasks in-
volved with the creation of our Dialectal Arabic-English
parallel corpus.
eliminated workers who did not correctly classify
them (2% of labels).
Identifying the dialect of a text snippet can be
challenging in the absence of phonetic cues. We
therefore required 3 classifications from different
workers for every document, and accepted a dialect
label if at least two of them agreed. The dialect dis-
tribution of the final output was: 43% Gulf/Iraqi,
28% Levantine, 11% Egyptian, and 16% could not
be classified. MSA and the other labels accounted
for 2%. We decided to translate only the Levantine
and Egyptian documents, since the pool of MTurk
workers contained virtually no workers from Iraq or
the Gulf region.
3.2 Sentence Segmentation
Since the data we annotated was mostly user-
generated informal web content, the existing punc-
tuation was often insufficient to determine sentence
boundaries. Since sentence boundaries are impor-
tant for correct translation, we segmented passages
into individual sentences using MTurk. We only re-
quired sentences longer than 15 words to be seg-
mented, and allowed Turkers to split and rejoin at
any point between the tokens. The instructions were
simply to ?divide the Arabic text into individual sen-
tences, where you believe it would be appropriate
to insert a period.? We also used a set of correctly
segmented passages for quality control, and scored
Turkers using a metric based on the precision and
recall of correct segmentation points. The rejection
rate was 1.2%.
3.3 Translation to English
Following Zaidan and Callison-Burch (2011a), we
hired non-professional translators on MTurk to
translate the Levantine and Egyptian sentences into
51
Sentence Arabic English
Data Set Pairs Tokens Tokens
MSA-150MW 8.0M 151.4M 204.4M
Dialect-1500KW 180k 1,545,053 2,257,041
MSA-1300KW 71k 1,292,384 1,752,724
MSA-Web-Tune 6,163 145,260 184,185
MSA-Web-Test 5,454 136,396 172,357
Lev-Web-Tune 2,600 20,940 27,399
Lev-Web-Test 2,600 21,092 27,793
Egy-Web-Test 2,600 23,671 33,565
E-Facebook-Tune 3,351 25,130 34,753
E-Facebook-Test 3,188 25,011 34,244
Table 2: Statistics about the training/tuning/test datasets
used in our experiments. The token counts are calculated
before MADA segmentation.
English. Among several quality control measures,
we rendered the Arabic sentences as images to pre-
vent Turkers from simply copying the Arabic text
into translation software. We still spot checked the
translations against the output of Google Translate
and Bing Translator. We also rejected gobbledygook
garbage translations that have a high percentage of
words not found in an English lexicon.
We quantified the quality of an individual Turker?s
translations in two ways: first by asking native Ara-
bic speaker judges to score a sample of the Turker?s
translations, and second by inserting control sen-
tences for which we have good reference translations
and measuring the Turker?s METEOR (Banerjee and
Lavie, 2005) and BLEU-1 scores (Papineni et al,
2002).2 The rejection rate of translation assignments
was 5%. We promoted good translators to a re-
stricted access ?preferred worker queue?. They were
paid at a higher rate, and were required to translate
control passages only 10% of the time as opposed
to 20% for general Turkers, thus providing us with a
higher translation yield for unseen data.
Worker turnout was initially slow, but increased
quickly as our reputation for being reliable payers
was established; workers started translating larger
volumes and referring their acquaintances. We had
121 workers who each completed 20 or more trans-
lation assignments. We eventually reached and sus-
tained a rate of 200k words of acceptable quality
2BLEU-1 provided a more reliable correlation with human
judgment in this case that the regular BLEU score (which uses
n-gram orders 1, . . . , 4), given the limited size of the sample
measured.
translated per week. Unlike Zaidan and Callison-
Burch (2011a), who only translated 2,000 Urdu sen-
tences, we translated sufficient volumes of Dialectal
Arabic to train machine translation systems. In total,
we had 1.1M words of Levantine and 380k words of
Egyptian translated into English, corresponding to
about 2.3M words on the English side.
Table 1 outlines the costs involved with creating
our parallel corpus. The total cost was $44k, or
$0.03/word ? an order of magnitude cheaper than
professional translation.
4 Experiments in Dialectal Arabic-English
Machine Translation
We performed a set of experiments to contrast sys-
tems trained using our dialectal parallel corpus with
systems trained on a (much larger) MSA-English
parallel corpus. All experiments use the same meth-
ods for training, decoding and parameter tuning, and
we only varied the corpora used for training, tun-
ing and testing. The MT system we used is based
on a phrase-based hierarchical model similar to that
of Shen et al (2008). We used GIZA++ (Och and
Ney, 2003) to align sentences and extract hierar-
chical rules. The decoder used a log-linear model
that combines the scores of multiple feature scores,
including translation probabilities, smoothed lexi-
cal probabilities, a dependency tree language model,
in addition to a trigram English language model.
Additionally, we used 50,000 sparse, binary-valued
source and target features based on Chiang et al
(2009). The English language model was trained on
7 billion words from the Gigaword and from a web
crawl. The feature weights were tuned to maximize
the BLEU score on a tuning set using the Expected-
BLEU optimization procedure (Devlin, 2009).
The Dialectal Arabic side of our corpus consisted
of 1.5M words (1.1M Levantine and 380k Egyp-
tian). Table 2 gives statistics about the various
train/tune/test splits we used in our experiments.
Since the Egyptian set was so small, we split it only
to training/test sets, opting not to have a tuning set.
The MSA training data we used consisted of Arabic-
English corpora totaling 150M tokens (Arabic side).
The MSA train/tune/test sets were constructed for
the DARPA GALE program.
We report translation quality in terms of BLEU
52
Simple Segment MADA Segment
Training Tuning BLEU OOV BLEU OOV ?BLEU ?OOV
MSA-Web-Test
MSA-150MW MSA-Web 26.21 1.69% 27.85 0.48% +1.64 -1.21%
MSA-1300KW 21.24 7.20% 25.23 1.95% +3.99 -5.25%
Egyptian-Web-Test
Dialect-1500KW Levantine-Web 18.55 6.31% 20.66 2.85% +2.11 -3.46%
Levantine-Web-Test
Dialect-1500KW Levantine-Web 17.00 6.22% 19.29 2.96% +2.29 -3.26%
Table 3: Comparison of the effect of morphological segmentation when translating MSA web text and Dialectal
Arabic web text. The morphological segmentation uniformly improves translation quality, but the improvements are
more dramatic for MSA than for Dialectal Arabic when comparing similarly-sized training corpora.
Training Tuning BLEU OOV BLEU OOV BLEU OOV
Egyptian-Web-Test Levantine-Web-Test MSA-Web-Test
MSA-150MW MSA-Web 14.76 4.42% 11.83 5.53% 27.85 0.48%
MSA-150MW Lev-Web 14.34 4.42% 12.29 5.53% 24.63 0.48%
MSA-150MW+Dial-1500KW 20.09 2.04% 19.11 2.27% 24.30 0.45%
Dialect-1500KW 20.66 2.85% 19.29 2.96% 15.53 3.70%
Egyptian-360KW 19.04 4.62% 11.21 9.00% - -
Levantine-360KW 14.05 7.11% 16.36 5.24% - -
Levantine-1100KW 17.79 4.83% 19.29 3.31% - -
Table 4: A comparison of translation quality of Egyptian, Levantine, andMSAweb text, using various training corpora.
The highest BLEU scores are achieved using the full set of dialectal data (which combines Levantine and Egyptian),
since the Egyptian alone is sparse. For Levantine, adding Egyptian has no effect. In both cases, adding MSA to the
dialectal data results in marginally worse translations.
score.3 In addition, we also report the OOV rate of
the test set relative to the training corpus in each ex-
perimental setups.
4.1 Morphological Decomposition
Arabic has a complex morphology compared to En-
glish. Preprocessing the Arabic source by morpho-
logical segmentation has been shown to improve the
performance of Arabic MT (Lee, 2004; Habash and
Sadat, 2006) by decreasing the size of the source vo-
cabulary, and improving the quality of word align-
ments. The morphological analyzers that underlie
most segmenters were developed for MSA, but the
different dialects of Arabic share many of the mor-
phological affixes of MSA, and it is therefore not
unreasonable to expect MSA segmentation to also
improve Dialect Arabic to English MT. To test this,
3We also computed TER (Snover et al, 2006) andMETEOR
scores, but omit them because they demonstrated similar trends.
we ran experiments using the MADA morpholog-
ical analyzer (Habash and Rambow, 2005). Table
3 shows the effect of applying segmentation to the
text, for both MSA and Dialectal Arabic. The BLEU
score improves uniformly, although the improve-
ments are most dramatic for smaller datasets, which
is consistent with previous work (Habash and Sadat,
2006). Morphological segmentation gives a smaller
gain on dialectal input, which could be due to two
factors: the segmentation accuracy likely decreases
since we are using an unmodified MSA segmenter,
and there is higher variability in the written form of
dialect compared to MSA. Given the significant, al-
beit smaller gain on dialectal input, we use MADA
segmentation in all our experiments.
4.2 Effect of Dialectal Training Data Size
We next examine how the size of the dialectal train-
ing data affects MT performance, and whether it is
useful to combine it with MSA training data. We
53
oh
 ti
me
 (s
pa
ce
 om
itt
ed
). 
Ap
pe
are
d w
ith
in
 a 
po
em
.
11
yA
zm
n

?
lik
e y
ou
 (c
or
ru
pti
on
 of
 M
SA
 m
vl
k)
.
10
m
tlk
"#
$
by
 m
uc
h (
co
rru
pti
on
 of
 M
SA
 bk
vy
r).
11
bk
ty
r
&'$
()
I m
iss
 yo
u (
sp
ok
en
 to
 a 
fe
ma
le)
 ?
Eg
yp
tia
n.
14
w
H
$t
yn
y
/0
'$1
2?
Th
e l
as
t n
am
e (
Al
-N
a'o
om
) o
f a
 fo
ru
m 
ad
mi
n.
16
A
ln
E
w
m
?:;
0<?
a l
oo
ot 
(c
or
ru
pti
on
 of
 M
SA
 kv
yr
A
).
17
kt
yy
yr
&''
'$?
rea
lly
/fo
r r
ea
l ?
Le
va
nti
ne
.
31
E
nj
d
DE
0F
En
gli
sh
 E
qu
iva
len
t
Co
un
t
TL
Ar
ab
ic
Table 5: The most frequent OOV?s (with counts ? 10) of the dialectal test sets against the MSA training data.
Source (EGY):  ? ? ??	
?   ? ! !
Transliteration: Ant btEml lh AElAn wlA Ayh?!!
MSA-Sys. Output: You are working for a declarationand not?
Dial-Sys. Output: You are making the advertisementfor him or what?
Reference: Are you promoting it or what?!!
Source (EGY):  01?. ??78 6 35 34? ?
 9:;? <=>
Transliteration: nfsY Atm}n Elyh bEd mA $Af
AlSwrh dy
MSA-Sys. Output: Myself feel to see this image.
Dial-Sys. Output: I wish to check on him afterhe saw this picture.
Reference: I wish to be sure that he is fineafter he saw this images
Source (LEV):  ?0??? E7770 ?F? G7H
Transliteration: lhyk Aljw ktyyyr kwwwl
MSA-Sys. Output: God you the atmosphere.
Dial-Sys. Output: this is why the weather is so cool
Reference: This is why the weather is so cool
Source (LEV):  ?L M
 G3 0?;
Transliteration: Twl bAlk Em nmzH
MSA-Sys. Output: Do you think about a joke long.
Dial-Sys. Output: Calm down we are kidding
Reference: calm down, we are kidding
Figure 2: Examples of improvement in MT output when
training on our Dialectal Arabic-English parallel corpus
instead of an MSA-English parallel corpus.
Source (EGY):   	
 	  ? 
Transliteration: qAltlp Tb tEAlY nEd ,
MSA-Sys. Output: Medicine almighty promise.
Dial-Sys. Output: She said, OK, come and then
Reference: She told him, OK, lets count them ,
Source (LEV):  "#$%& 
#'01 ?-%. ! -,%+? ?? ?2 
Transliteration: fbqrA w>HyAnA bqDyhA Em
>tslY mE rfqAty
MSA-Sys. Output: I read and sometimes with gowith my uncle.
Dial-Sys. Output: So I read, and sometimes I spendtrying to make my self comfortwith my friends
Reference: So i study and sometimes I spendthe time having fun with my friends
Source (LEV):  ?@ ?< ??' => +? &#:9? B:C12D E?
?? %$?+G 
Transliteration: Allh ysAmHkn hlq kl wAHd TAlb
qrb bykwn bdw Erws
MSA-Sys. Output: God now each student near the
Bedouin bride.
Dial-Sys. Output: God forgive you, each one is aclose student would want the bride
Reference: God forgive you. Is every oneasking to be close, want a bride!
Figure 3: Examples of ambiguous words that are trans-
lated incorrectly by the MSA-English system, but cor-
rectly by the Dialectal Arabic-English system.
54
!"
!#
!$
!%
"&
""
&' "&&' #&&' %&&' !(&&'
!
"#
$
%
!"#$%&'()*#"+"+,(-./0(/1(2/*345
)*+,-./0123
-./0123
Egyptian web test
!"
!#
!$
!%
"&
""
&' "&&' #&&' %&&' !(&&'
!
"#
$
!"#$%&'()*#"+"+,(-./0(/1(2/*345
)*+,-./0123
-./0123
Levantine web test
Figure 4: Learning curves showing the effects of increas-
ing the size of dialectal training data, when combined
with the 150M-word MSA parallel corpus, and when
used alone. Adding the MSA training data is only use-
ful when the dialectal data is scarce (200k words).
started with a baseline system trained on the 150M-
word MSA parallel corpus, and added various sized
portions of the dialect parallel corpus to it. Figure 4
shows the resulting learning curve, and compares it
to the learning curve for a system trained solely on
the dialectal parallel corpus. When only 200k words
of dialectal data are available, combining it with the
150M-word MSA corpus results in improved BLEU
scores, adding 0.8?1.5 BLEU points. When 400k
words or more of dialectal data are available, the
MSA training data ceases to provide any gain, and
in fact starts to hurt the performance.
The performance of a system trained on the 1.5M-
word dialectal data is dramatically superior to a sys-
tem that uses only the 150M-word MSA data: +6.32
BLEU points on the Egyptian test set, or 44% rela-
tive gain, and +7.00 BLEU points on the Levantine
test set, or 57% relative gain (fourth line vs. second
line of Table 4). In Section 4.4, we show that those
gains are not an artifact of the similarity between test
and training datasets, or of using the same translator
pool to translate both sets.
Inspecting the difference in the outputs of the Di-
alectal vs. MSA systems, we see that the improve-
ment in score is a reflection of a significant improve-
ment in the quality of translations. Figure 2 shows
a few examples of sentences whose translations im-
prove significantly using the Dialectal system. Fig-
ure 3 shows a particularly interesting category of ex-
amples. Many words are homographs, with different
meanings (and usually different pronunciations) in
MSA vs. one or more dialects. The bolded tokens
in the sentences in Figure 3 are examples of such
words. They are translated incorrectly by the MSA
system, while the dialect system translates them cor-
rectly.4 If we examine the most frequent OOVwords
against the MSA training data (Table 5), we find a
number of corrupted MSA words and names, but
that a majority of OOVs are dialect words.
4.3 Cross-Dialect Training
Since MSA training data appeared to have little ef-
fect when translating dialectal input, we next inves-
tigated the effect of training data from one dialect on
translating the input of another dialect. We trained a
system with the 360k-word Egyptian training subset
of our dialectal parallel corpus, and another system
with a similar amount of Levantine training data. We
used each system to translate the test set of the other
dialect. As expected, a system performs better when
it translates a test set in the same dialect that it was
trained on (Table 4).
That said, since the Egyptian training set is so
small, adding the (full) Levantine training data im-
proves performance (on the Egyptian test set) by
1.62 BLEU points, compared to using only Egyp-
tian training data. In fact, using the Levantine
training data by itself outperforms the MSA-trained
system on the Egyptian test set by more than 3
BLEU points. (For the Levantine test set, adding
the Egyptian training data has no affect, possibly
due to the small amount of Egyptian data.) This
may suggest that the mismatch between dialects is
less severe than the mismatch between MSA and
dialects. Alternatively, the differences may be due
to the changes in genre from the MSA parallel cor-
pus (which is mainly formal newswire) to the news-
groups and weblogs that mainly comprise the dialec-
tal corpus.
4The word nfsY of Figure 2 (first word of second example)
is also a homograph, as it means myself in MSA and I wish in
Dialectal Arabic.
55
Training Tuning BLEU OOV
MSA-150MW Levantine-Web 13.80 4.16%
MSA-150MW+Dialect-1500KW 16.71 2.43%
Dialect-1500KW 15.75 3.79%
MSA-150MW Egyptian-Facebook 15.80 4.16%
MSA-150MW+Dialect-1500KW 18.50 2.43%
Dialect-1500KW 17.90 3.79%
Dialect-1000KW (random selection) Egyptian-Facebook 17.09 4.64%
Dialect-1000KW (no Turker overlap) 17.10 4.60%
Table 6: Results on a truly independent test set, consisting of data harvested from Egyptian Facebook pages that are
entirely distinct from the our dialectal training set. The improvements over the MSA baseline are still considerable:
+2.9 BLEU points when no Facebook data is available for tuning and +2.7 with a Facebook tuning set.
4.4 Validation on Independent Test Data
To eliminate the possibility that the gains are solely
due to similarity between the test/training sets in the
dialectal data, we ran experiments using the same
dialectal training data, but using truly independent
test/tuning data sets selected at random from a larger
set of monolingual data that we collected from pub-
lic Egyptian Facebook pages. This data consists of
a set of original user postings and the subsequent
comments on each, giving the data a more conversa-
tional style than our other test sets. The postings
deal with current Egyptian political affairs, sports
and other topics. The test set we selected consisted
of 25,011 words (3,188 comments and 427 postings
from 86 pages), and the tuning set contained 25,130
words (3,351 comments and 415 conversations from
58 pages). We obtained reference translations for
those using MTurk as well.
Table 6 shows that using the 1.5M-word dialect
parallel corpus for training yields a 2 point BLEU
improvement over using the 150M-word MSA cor-
pus. Adding the MSA training data does yield an
improvement, though of less than a single BLEU
point. It remains true that training on 1.5M words
of dialectal data is better than training on 100 times
more MSA parallel data. The system performance
is sensitive to the tuning set choice, and improves
when it matches the test set in genre and origin.
To eliminate another potential source of artificial
bias, we also performed an experiment where we
removed any training translation contributed by a
Turker who translated any sentence in the Egyptian
Facebook set, to eliminate translator bias. For this,
we were left with 1M words of dialect training data.
This gave the same BLEU score as when training
with a randomly selected subset of the same size
(bottom part of Table 6).
4.5 Mapping from Dialectal Arabic to MSA
Before Translating to English
Given the large amount of linguistic resources that
have been developed for MSA over the past years,
and the extensive research that was conducted on
machine translation from MSA to English and other
languages, an obvious research question is whether
Dialectal Arabic is best translated to English by first
pivoting through MSA, rather than directly. The
proximity of Dialectal Arabic to MSA makes the
mapping in principle easier than general machine
translation, and a number of researchers have ex-
plored this direction (Salloum and Habash, 2011).
In this scenario, the dialectal source would first be
automatically transformed to MSA, using either a
rule-based or statistical mapping module.
The Dialectal Arabic-English parallel corpus we
created presents a unique opportunity to compare
the MSA-pivoting approach against direct transla-
tion. First, we collected equivalent MSA data for
the Levantine Web test and tuning sets, by asking
Turkers to transform dialectal passages to valid and
fluent MSA. Turkers were shown example transfor-
mations, and we encouraged fewer changes where
applicable (e.g. morphological rather than lexical
mapping), but allowed any editing operation in gen-
eral (deletion, substitution, reordering). Sample sub-
missions were independently shown to native Ara-
bic speaking judges, who confirmed they were valid
MSA. A lowOOV rate also indicated the correctness
of the mappings. By manually transforming the test
56
Training BLEU OOV BLEU OOV ?BLEU ?OOV
Direct dialect trans Map to MSA then trans
MSA-150MW 12.29 5.53% 14.59 1.53% +2.30 -4.00%
MSA-150MW+Dialect-200KW 15.37 3.59% 15.53 1.22% +0.16 -2.37%
MSA-150MW+Dialect-400KW 16.62 3.06% 16.25 1.13% -0.37 -1.93%
MSA-150MW+Dialect-800KW 17.83 2.63% 16.69 1.04% -1.14 -1.59%
MSA-150MW+Dialect-1500KW 19.11 2.27% 17.20 0.98% -1.91 -1.29%
Table 7: A comparison of the effectiveness of performing Levantine-to-MSA mapping before translating into English,
versus translating directly from Levantine into English. The mapping from Levantine to MSA was done manually, so it
is an optimistic estimate of what might be done automatically. Although initially helpful to the MSA baseline system,
the usefulness of pivoting through MSA drops as more dialectal data is added, eventually hurting performance.
dialectal sentence into MSA, we establish an opti-
mistic estimate of what could be done automatically.
Table 7 compares direct translation versus piv-
oting to MSA before translating, using the base-
line MSA-English MT system.5 The performance
of the system improves by 2.3 BLEU points with
dialect-to-MSA pivoting, compared to attempting to
translate the untransformed dialectal input directly.
As we add more dialectal training data, the BLEU
score when translating the untransformed dialect
test set improves rapidly (as seen previously in the
MSA+Dialect learning curve in Figure 4), while the
improvement is less rapid when the text is first trans-
formed to MSA. Direct translation becomes a better
option than mapping to MSA once 400k words of di-
alectal data are added, despite the significantly lower
OOV rate with MSA-mapping. This indicates that
simple vocabulary coverage is not sufficient, and
data domain mismatch, quantified by more complex
matching patterns, is more important.
5 Conclusion
We have described a process for building a Dialec-
tal Arabic-English parallel corpus, by selecting pas-
sages with a relatively high percentage of non-MSA
words from a monolingual Arabic web text corpus,
then using crowdsourcing to classify them by di-
alect, segment them into individual sentences and
translate them to English. The process was success-
fully scaled to the point of reaching and sustaining a
rate of 200k translated words per week, at 1/10 the
cost of professional translation. Our parallel corpus,
consisting of 1.5M words, was produced at a total
5The systems in each column of the table are tuned consis-
tently, using their corresponding tuning sets.
cost of $40k, or roughly $0.03/word.
We used the parallel corpus we constructed to
analyze the behavior of a Dialectal Arabic-English
MT system as a function of the size of the dialec-
tal training corpus. We showed that relatively small
amounts of training data render larger MSA corpora
from different data genres largely ineffective for this
test data. In practice, a system trained on the com-
bined Dialectal-MSA data is likely to give the best
performance, since informal Arabic data is usually
a mixture of Dialectal Arabic and MSA. An area of
future research is using the output of a dialect clas-
sifier, or other features to bias the translation model
towards the Dialectal or the MSA parts of the data.
We also validated the models built from the di-
alectal corpus by using them to translate an inde-
pendent data set collected from Egyptian Facebook
public pages. We finally investigated using MSA
as a ?pivot language? for Dialectal Arabic-English
translation, by simulating automatic dialect-to-MSA
mapping using MTurk. We obtained limited gains
from mapping the input to MSA, even when the
mapping is of good quality, and only at lower train-
ing set sizes. This suggests that the mismatch be-
tween training and test data is an important aspect of
the problem, beyond simple vocabulary coverage.
The aim of this paper is to contribute to setting
the direction of future research on Dialectal Arabic
MT. The gains we observed from using MSA mor-
phological segmentation can be further increased
with dialect-specific segmenters. Input preprocess-
ing can also be used to decrease the noise of the
user-generated data. Topic adaptation is another im-
portant problem to tackle if the large MSA linguistic
resources already developed are to be leveraged for
Dialectal Arabic-English MT.
57
Acknowledgments
This work was supported in part by DARPA/IPTO
Contract No. HR0011-12-C-0014 under the BOLT
Program, and in part by the EuroMatrixPlus project
funded by the European Commission (7th Frame-
work Programme). The views expressed are those
of the authors and do not reflect the official policy
or position of the Department of Defense or the U.S.
Government. Distribution Statement A (Approved
for Public Release, Distribution Unlimited).
References
Hitham M. Abo Bakr, Khaled Shaalan, and Ibrahim
Ziedan. 2008. A hybrid approach for converting writ-
ten Egyptian colloquial dialect into diacritized Arabic.
In The 6th International Conference on Informatics
and Systems, INFOS2008, Cairo, Egypt.
Satanjeev Banerjee and Alon Lavie. 2005. Meteor:
An automatic metric for MT evaluation with improved
correlation with human judgments. In In Proc. of ACL
2005 Workshop on Intrinsic and Extrinsic Evaluation
Measures for MT and/or Summarization, Ann Arbor,
Michigan.
Chris Callison-Burch and Mark Dredze. 2010. Creating
speech and language data with Amazon?s Mechanical
Turk. In Proceedings of the NAACL HLT 2010 Work-
shop on Creating Speech and Language Data with
Amazon?s Mechanical Turk, pages 1?12, Los Angeles,
June.
David Chiang, Mona Diab, Nizar Habash, Owen Ram-
bow, and Safiullah Shareef. 2006. Parsing Arabic di-
alects. In Proceedings of the Conference of the Eu-
ropean Chapter of the Association for Computational
Linguistics, Trento, Italy.
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 new features for statistical machine translation.
In NAACL ?09: Proceedings of the 2009 Human Lan-
guage Technology Conference of the North American
Chapter of the Association for Computational Linguis-
tics, Boulder, Colorado.
Jacob Devlin. 2009. Lexical features for statistical ma-
chine translation. Master?s thesis, University of Mary-
land, December.
Mohamed Embarki and Moha Ennaji, editors. 2011.
Modern Trends in Arabic Dialectology. The Red Sea
Press.
Charles A. Ferguson. 1959. Diglossia. Word, 15:325?
340.
Nizar Habash and Owen Rambow. 2005. Arabic tok-
enization, part-of-speech tagging and morphological
disambiguation in one fell swoop. In Proceedings of
the 43th Annual Meeting of the Association for Com-
putational Linguistics (ACL), Ann Arbor, Michigan.
Nizar Habash and Owen Rambow. 2006. MAGEAD: A
morphological analyzer and generator for the Arabic
dialects. In Proceedings of the 44th Annual Meeting of
the Association for Computational Linguistics (ACL),
Sydney, Australia.
Nizar Habash and Fatiha Sadat. 2006. Arabic prepro-
cessing schemes for statistical machine translation. In
Proceedings of the 2006 Human Language Technol-
ogy Conference of the North American Chapter of the
Association for Computational Linguistics, New York,
New York.
Nizar Y. Habash. 2010. Introduction to Arabic Natural
Language Processing. Morgan & Claypool.
Young-Suk Lee. 2004. Morphological analysis for
statistical machine translation. In HLT-NAACL ?04:
Proceedings of HLT-NAACL 2004, Boston, Mas-
sachusetts.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalua-
tion of machine translation. In Proceedings of the 40th
Annual Meeting of the Association for Computational
Linguistics (ACL), Philadelphia, PA.
Jason Riesa and David Yarowsky. 2006. Minimally
supervised morphological segmentation with applica-
tions to machine translation. In Proceedings of the 7th
Conf. of the Association for Machine Translation in the
Americas (AMTA 2006), Cambridge, MA.
Wael Salloum and Nizar Habash. 2011. Dialectal to stan-
dard Arabic paraphrasing to improve Arabic-English
statistical machine translation. In Proceedings of the
2011 Conference of Empirical Methods in Natural
Language Processing, Edinburgh, Scotland, UK.
Hassan Sawaf. 2010. Arabic dialect handling in hybrid
machine translation. In Proceedings of the 9th Conf. of
the Association for Machine Translation in the Ameri-
cas (AMTA 2010), Denver, Colorado.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proceedings of the 46th Annual Meeting of the As-
sociation for Computational Linguistics (ACL), pages
577?585, Columbus, Ohio.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea
Micciulla, and Ralph Weischedel. 2006. A study of
translation error rate with targeted human annotation.
In Proceedings of the 7th Conf. of the Association for
Machine Translation in the Americas (AMTA 2006),
pages 223?231, Cambridge, MA.
58
Omar F. Zaidan and Chris Callison-Burch. 2011a.
The Arabic online commentary dataset: an annotated
dataset of informal Arabic with high dialectal content.
In Proceedings of the 49th Annual Meeting of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies, pages 37?41, Portland, Oregon,
June.
Omar F. Zaidan and Chris Callison-Burch. 2011b.
Crowdsourcing translation: Professional quality from
non-professionals. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies, pages 1220?
1229, Portland, Oregon, June.
59
Proceedings of NAACL-HLT 2013, pages 612?616,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Systematic Comparison of Professional and Crowdsourced Reference
Translations for Machine Translation
Rabih Zbib, Gretchen Markiewicz, Spyros Matsoukas,
Richard Schwartz, John Makhoul
Raytheon BBN Technologies
Cambridge, MA 02138, USA
{rzbib,gmarkiew,smatsouk,schwartz,makhoul}@bbn.com
Abstract
We present a systematic study of the effect of
crowdsourced translations on Machine Trans-
lation performance. We compare Machine
Translation systems trained on the same data
but with translations obtained using Amazon?s
Mechanical Turk vs. professional translations,
and show that the same performance is ob-
tained from Mechanical Turk translations at
1/5th the cost. We also show that adding a Me-
chanical Turk reference translation of the de-
velopment set improves parameter tuning and
output evaluation.
1 Introduction
Online crowdsourcing services have been shown to
be a cheap and effective data annotation resource
for various Natural Language Processing (NLP)
tasks (Callison-Burch and Dredze, 2010; Zaidan and
Callison-Burch, 2011a; Zaidan and Callison-Burch,
2011b). The resulting quality of annotations is high
enough to be used for training statistical NLP mod-
els, with a saving in cost and time of up to an or-
der of magnitude. Statistical Machine Translation
(SMT) is one of the NLP tasks that can benefit from
crowdsourced annotations. With appropriate quality
control mechanisms, reference translations collected
by crowdsourcing have been successfully used for
training and evaluating SMT systems (Zbib et al,
2012; Zaidan and Callison-Burch, 2011b).
In this work, we used Amazon?s Mechanical Turk
(MTurk) to obtain alternative reference translations
of four Arabic-English parallel corpora previously
released by the Linguistic Data Consortium (LDC)
for the DARPA BOLT program. This data, totaling
over 500K Arabic tokens, was originally collected
from web discussion forums and translated profes-
sionally to English. We used alternative MTurk
translations of the same data to train and evalua-
tion MT systems; and conducted the first systematic
study that quantifies the effect of the reference trans-
lation process on MT output. We found that:
? Mechanical Turk can be used to translate
enough data for training an MT system at
1/10th the price of professional translation, and
at a much faster rate.
? Training MT systems on MTurk reference
translations gives the same performance as
training with professional translations at 20%
of the cost.
? A second translation of the development set ob-
tained via MTurk improves parameter tuning
and output evaluation.
2 Previous Work
There have been several publications on crowd-
sourcing data annotation for NLP. Callison-Burch
and Dredze (2010) give an overview of the NAACL-
2010 Workshop on using Mechanical Turk for data
annotation. They describe tasks for which MTurk
can be used, and summarize a set of best practices.
They also include references to the workshop con-
tributions.
Zaidan and Callison-Burch (2011a) created a
monolingual Arabic data set rich in dialectal con-
tent from user commentaries on newspaper web-
sites. They hired native Arabic speakers on MTurk
612
to identify the dialect level and used the collected la-
bels to train automatic dialect identification systems.
They did not translate the collected data, however.
Zaidan and Callison-Burch (2011b) obtained mul-
tiple translations of the NIST 2009 Urdu-English
evaluation set using MTurk. They trained a statis-
tical model on a set of features to select among the
multiple translations. They showed that the MTurk
translations selected by their model approached the
range of quality of professional translations, and that
the selected MTurk translations can be used reliably
to score the outputs of different MT systems submit-
ted to the NIST evaluation. Unlike our work, they
did not investigate the use of crowdsourced trans-
lations for training or parameter tuning. Zbib et al
(2012) trained a Dialectal Arabic to English MT sys-
tem using Mechanical Turk translations. But the
data they translated on MTurk does not have profes-
sional translations to conduct the systematic com-
parison we do in this paper.
It is well known that scoring MT output against
multiple references improves MT scores such as
BLEU significantly, since it increases the chance of
matching n-grams between the MT output and the
references. Tuning system parameter with multi-
ple references also improves machine translation for
the same reason Madnani et al (2007) and Madnani
et al (2008) showed that tuning on additional ref-
erences obtained by automatic paraphrasing helps
when only few tuning references are available.
3 Data Translation
The data we used are Arabic-English parallel cor-
pora released by the LDC for the DARPA BOLT
Phase 1 program1. The data was collected from
Egyptian online discussion forums, and consists of
separate discussion threads, each composed of an
initial user posting and multiple reply postings. The
data tends to be bimodal: the first posting in the
thread is often formal and expressed in Modern
Standard Arabic, while the subsequent threads use
a less formal style, and contain colloquial Egyptian
dialect. The data was manually segmented into sen-
tence units, and translated professionally.
We used non-professional translators hired on
MTurk to get second translations. We used several
1Corpora: LDC2012E15, LDC2012E19, LDC2012E55
measures to control the quality of translations and
detect cheaters. Those include the rendering of Ara-
bic sentences as images, comparing the output to
Google Translate and Bing Translator, and other au-
tomatic checks. The quality of individual worker?s
translations was quantified by asking a native Ara-
bic speaker judge to score a sample of the Turker?s
translations. The translation task unit (aka Human
Intelligence Task or HIT) consisted of a sequence
of contiguous sentences from a discussion thread
amounting to between 40 and 60 words. The in-
structions were simply to translate the Arabic source
fully and accurately, and to take surrounding sen-
tence segments into account to help resolve ambigu-
ities. The HIT rewards were set to 2.5? per word.
At the end of the effort, we had 26 different work-
ers translate 567K Arabic tokens in 4 weeks. The
resulting translations were less fluent than their pro-
fessional counterparts, and 10% shorter on average.
The following section presents results of MT exper-
iments using the MTurk translations.
4 MT Experiments
The MT system used is based on a string-to-
dependency-tree hierarchical model of Shen et
al. (2008). Sentence alignment was done using
GIZA++ (Och and Ney, 2003). Decoder fea-
tures include translation probabilities, smoothed lex-
ical probabilities, and a dependency tree language
model. Additionally, we used 50,000 sparse, binary-
valued source and target features based on Chiang
et al (2009). The English language model was
trained on 7 billion words from the LDC Gigaword
corpus and from a web crawl. We used expected
BLEU maximization (Devlin, 2009) to tune feature
weights.
We defined a tuning set (3581 segments, 43.3K
tokens) and a test set (4166 segments, 47.7K to-
kens) using LDC2012E30, the corpus designated
as a development set by the LDC, augmented with
around 50K Words held out from LDC2012E15
and LDC2012E19, to make a development set large
enough to tune the large number of feature weights2.
The remaining data was used for training. We de-
fined three nested training sets containing 100K,
200K and 400K Arabic tokens respectively, with
2Only full forum threads were held out
613
Training Web-forum Only Newswire(10MW)+Web-forum
100KW 200KW 400KW 0KW 100KW 200KW 400KW
Prof. refs 17.71 20.23 22.61 22.82 24.05 24.85 25.19
MTurk refs 16.41 18.43 20.08 22.82 23.79 24.20 24.51
Two Training refs 19.03 21.19 23.06 22.82 24.26 25.19 25.38
Add?l Training data - 19.80 21.53 22.82 - 24.31 25.16
Table 1: Comparison of the effect of web forum training data when using professional and MTurk reference transla-
tions. All results use professional references for the tuning and test sets.
two versions of each set: one with the professional
reference translations for the target, and the other
with the same source data, but the MTurk transla-
tions. We defined two versions of the test and tuning
sets similarly. We report translation results in terms
of lower-case BLEU scores (Papineni et al, 2002).
4.1 Training Data References
We first study the effect of training data refer-
ences, varying the amount of training data and type
of translations, while using the same professional
translation references for tuning and scoring. The
first set of baseline experiments were trained on
web forum data only, using professional transla-
tions. The first line of Table 1 shows that doubling of
the training data adds 2.5 then 2.3 BLEU points. We
repeated the experiments, but with MTurk training
references, and saw that the scores are lower by 1.3-
2.5 BLEU points, depending on the size of training
data, and that the gain obtained from doubling the
training data decreases to 2.0 and 1.6 BLEU points.
The lower MT scores and slower learning curve of
the MTurk systems are both due to the lower quality
of the translations, and to the mismatch with the pro-
fessional development set translations (we discuss
this issue further in ?4.3). However, by interpolation
of the MT scores, we find that the same MT perfor-
mance can be obtained by using twice the amount of
MTurk translated data as professional data. Consid-
ering that the MTurk translations is 10 times cheaper
than professional translations (2.5? versus 25-30?),
this constitutes a cost ratio of 5x.
We repeated the above experiments, but this time
added 10 million words of parallel data from the
NIST MT 2012 corpora (mostly news) for training.
We weighted the web forum part of the training data
by a factor of 5. Note from the results in the right
half of Table 1 that the newswire data improves the
BLEU score by 2.5 to 6.3 BLEU points, depend-
ing on the size of the web forum data. This signif-
icant improvement is because some of the web fo-
rum user postings are formal and written in MSA
(?3). More relevant to our aims is the comparison
when we vary the web forum training references in
the presence of the newswire training. The differ-
ence between the MTurk translation systems and the
professional translation drops to 0.26-0.68 points.
We conclude that in a domain adaptation scenario,
where out-of-domain training data (i.e. newswire)
already exists, crowdsourced translations for the in-
domain (i.e. web forum) training data can be used
with little to no loss in MT performance.
4.2 More Data vs. Multiple Translations
To our knowledge no previous work has compared
using multiple reference translations for training
data versus using additional training data of the same
size. We studied this question by using both transla-
tions on the target side of the training data. Using the
MTurk translations in addition to the professional
translations in training gave a gain of 0.4 to 1.3
BLEU points (bottom half of Table 1). The gain was
smaller in the presence of the GALE newswire data.
When we compared with using the same amount of
different training data instead of multiple references,
we saw that training on new data with crowdsourced
translations is better: training on two translations of
100KW gives 19.03, compared to 19.80 when train-
ing on a single translation of 200KW. The advantage
of different-source data drops to 0.34 points when
we start with 200KW. With a larger initial corpus,
the additional source coverage of new data is not as
critical, and the advantage of more variety on the
target-side of the extracted translation rules becomes
more competitive. This coverage is even less criti-
cal in the presence of the news data, where the ad-
614
Training Tuning Test Training Data Size
100KW 200KW 400KW 400KW(no lex)
Prof. Prof. Prof. 17.71 20.23 22.61 20.01
Prof. Prof. Prof.+MTurk 22.53 25.75 28.38 25.42
Prof. Prof. (len=0.95) Prof.+MTurk 23.63 26.84 29.54 26.17
Prof. Prof.+MTurk Prof.+MTurk 25.26 28.44 30.94 27.22
MTurk MTurk MTurk 16.66 18.47 20.35 17.75
MTurk MTurk Prof.+MTurk 23.83 26.45 28.66 25.44
MTurk MTurk (len=1.05) Prof.+MTurk 23.73 26.19 28.74 25.87
MTurk Prof.+MTurk Prof.+MTurk 24.91 27.66 29.78 26.45
Table 2: Effect of Tuning and Scoring References on MT.
vantage of new web forum source data disappears
(lower-right quadrant of Table 1).
4.3 Development Data References
So far, we have focused on varying training data
conditions, and kept the tuning and evaluation con-
ditions fixed. But since we have re-translated the
tuning and test sets on MTurk as well, we can study
the effect of their reference translations on MT. As
Table 2 shows, scoring the MT output using both
reference translations, the BLEU scores increase by
over 5 points (and more for the MTurk-trained sys-
tem). This increase by itself is not remarkable. What
is important to note is that the gain obtained by dou-
bling the amount of training data is larger when mea-
sured using the multiple reference test set. We also
ran experiments with 400KW training data, but with
the lexical smoothing features (Koehn et al, 2003;
Devlin, 2009) turned off. The bigger gains show that
improvements in the MT output (from additional
training or new features) can be better measured us-
ing a second MTurk reference of the test set.
Finally, we study the effect of tuning the system
parameters using both translation references. Look-
ing at the system trained on the professional trans-
lations, we see a gain of 2.5 to 2.7 BLEU points
from adding the MTurk references to the tuning set.
But as we mentioned earlier, the MTurk transla-
tions are shorter than the professional translations
by around 10% on average. Tuning on both ref-
erences, therefore, shortens the system output by
around 5%. To neutralize the effect of length mis-
match, we compared to a fairer baseline tuned on
the professional references only, but we tuned the
output-to-reference length ratio to be 0.95 (thus pro-
ducing a shorter output). In this case, we see a gain
of 1.4 points from adding the MTurk references to
the tuning set.
We also used the multiple-reference tuning set
to retune the systems trained on MTurk transla-
tions. Comparing that to a baseline that is tuned and
scored using MTurk references only, we see a gain
of around 1%. Note, however, that in this case the
length mismach is reversed, and the output of the
multiple-reference system is around 5% longer than
that of the baseline. If we compare with a baseline
that is tuned with a length ratio of 1.05 (to produce a
longer output), we see the gain shrink only slightly.
To sum up this section, a second set of refer-
ence translations obtained via MTurk makes mea-
surements of improvement on the test set more re-
liable. Also, a second set of references for tuning
improves the output of the MT systems trained on
either professional or MTurk references.
5 Conclusion
We compared professional and crowdsourced trans-
lations of the same data for training, tuning and scor-
ing Arabic-English SMT systems. We showed that
the crowdsourced translations yield the same MT
performance as professional translations for as lit-
tle as 20% of the cost. We also showed that a sec-
ond crowsourced reference translation of the devel-
opment set alows for a more accurate evaluation of
MT output.
Acknowledgments
This work was supported in part by DARPA/IPTO
Contract No. HR0011-12-C-0014 under the BOLT
615
Program. The views expressed are those of the au-
thors and do not reflect the official policy or position
of the Department of Defense or the U.S. Govern-
ment. Distribution Statement A (Approved for Pub-
lic Release, Distribution Unlimited).
References
Chris Callison-Burch and Mark Dredze. 2010. Creating
speech and language data with Amazon?s Mechanical
Turk. In Proceedings of the NAACL HLT 2010 Work-
shop on Creating Speech and Language Data with
Amazon?s Mechanical Turk, pages 1?12, Los Angeles,
June.
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 new features for statistical machine translation.
In NAACL ?09: Proceedings of the 2009 Human Lan-
guage Technology Conference of the North American
Chapter of the Association for Computational Linguis-
tics, Boulder, Colorado.
Jacob Devlin. 2009. Lexical features for statistical ma-
chine translation. Master?s thesis, University of Mary-
land, December.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proceedings of the 2003
Human Language Technology Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 48?54, Edmonton, Canada.
Nitin Madnani, Necip Fazil, Ayan, Philip Resnik, and
Bonnie Dorr. 2007. Using paraphrases for parameter
tuning in statistical machine translation. In Proceed-
ings of the Second Workshop on Statistical Machine
Translation, pages 120?127, Prague, Czech Republic.
Association for Computational Linguistics.
Nitin Madnani, Philip Resnik, Bonnie Dorr, and Richard
Schwartz. 2008. Are multiple reference transla-
tions necessary? investigating the value of paraphrased
reference translations in parameter optimization. In
Proceedings of the 8th Conf. of the Association for
Machine Translation in the Americas (AMTA 2008),
Waikiki, Hawaii, USA.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalua-
tion of machine translation. In Proceedings of the 40th
Annual Meeting of the Association for Computational
Linguistics (ACL), Philadelphia, PA.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proceedings of the 46th Annual Meeting of the As-
sociation for Computational Linguistics (ACL), pages
577?585, Columbus, Ohio.
Omar F. Zaidan and Chris Callison-Burch. 2011a.
The Arabic online commentary dataset: an annotated
dataset of informal Arabic with high dialectal content.
In Proceedings of the 49th Annual Meeting of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies, pages 37?41, Portland, Oregon,
June.
Omar F. Zaidan and Chris Callison-Burch. 2011b.
Crowdsourcing translation: Professional quality from
non-professionals. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies, pages 1220?
1229, Portland, Oregon, June.
Rabih Zbib, Erika Malchiodi, Jacob Devlin, David
Stallard, Spyros Matsoukas, Richard Schwartz, John
Makhoul, Omar F. Zaidan, and Chris Callison-Burch.
2012. Machine translation of arabic dialects. In The
2012 Conference of the North American Chapter of the
Association for Computational Linguistics, Montreal,
June. Association for Computational Linguistics.
616
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1370?1380,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Fast and Robust Neural Network Joint Models for Statistical Machine
Translation
Jacob Devlin, Rabih Zbib, Zhongqiang Huang,
Thomas Lamar, Richard Schwartz, and John Makhoul
Raytheon BBN Technologies, 10 Moulton St, Cambridge, MA 02138, USA
{jdevlin,rzbib,zhuang,tlamar,schwartz,makhoul}@bbn.com
Abstract
Recent work has shown success in us-
ing neural network language models
(NNLMs) as features in MT systems.
Here, we present a novel formulation for
a neural network joint model (NNJM),
which augments the NNLM with a source
context window. Our model is purely lexi-
calized and can be integrated into any MT
decoder. We also present several varia-
tions of the NNJM which provide signif-
icant additive improvements.
Although the model is quite simple, it
yields strong empirical results. On the
NIST OpenMT12 Arabic-English condi-
tion, the NNJM features produce a gain of
+3.0 BLEU on top of a powerful, feature-
rich baseline which already includes a
target-only NNLM. The NNJM features
also produce a gain of +6.3 BLEU on top
of a simpler baseline equivalent to Chi-
ang?s (2007) original Hiero implementa-
tion.
Additionally, we describe two novel tech-
niques for overcoming the historically
high cost of using NNLM-style models
in MT decoding. These techniques speed
up NNJM computation by a factor of
10,000x, making the model as fast as a
standard back-off LM.
This work was supported by DARPA/I2O Contract No.
HR0011-12-C-0014 under the BOLT program (Approved for
Public Release, Distribution Unlimited). The views, opin-
ions, and/or findings contained in this article are those of the
author and should not be interpreted as representing the of-
ficial views or policies, either expressed or implied, of the
Defense Advanced Research Projects Agency or the Depart-
ment of Defense.
1 Introduction
In recent years, neural network models have be-
come increasingly popular in NLP. Initially, these
models were primarily used to create n-gram neu-
ral network language models (NNLMs) for speech
recognition and machine translation (Bengio et al,
2003; Schwenk, 2010). They have since been ex-
tended to translation modeling, parsing, and many
other NLP tasks.
In this paper we use a basic neural network ar-
chitecture and a lexicalized probability model to
create a powerful MT decoding feature. Specifi-
cally, we introduce a novel formulation for a neu-
ral network joint model (NNJM), which augments
an n-gram target language model with an m-word
source window. Unlike previous approaches to
joint modeling (Le et al, 2012), our feature can be
easily integrated into any statistical machine trans-
lation (SMT) decoder, which leads to substantially
larger improvements than k-best rescoring only.
Additionally, we present several variations of this
model which provide significant additive BLEU
gains.
We also present a novel technique for training
the neural network to be self-normalized, which
avoids the costly step of posteriorizing over the
entire vocabulary in decoding. When used in con-
junction with a pre-computed hidden layer, these
techniques speed up NNJM computation by a fac-
tor of 10,000x, with only a small reduction on MT
accuracy.
Although our model is quite simple, we obtain
strong empirical results. We show primary results
on the NIST OpenMT12 Arabic-English condi-
tion. The NNJM features produce an improvement
of +3.0 BLEU on top of a baseline that is already
better than the 1st place MT12 result and includes
1370
a powerful NNLM. Additionally, on top of a sim-
pler decoder equivalent to Chiang?s (2007) origi-
nal Hiero implementation, our NNJM features are
able to produce an improvement of +6.3 BLEU ?
as much as all of the other features in our strong
baseline system combined.
We also show strong improvements on the
NIST OpenMT12 Chinese-English task, as well as
the DARPA BOLT (Broad Operational Language
Translation) Arabic-English and Chinese-English
conditions.
2 Neural Network Joint Model (NNJM)
Formally, our model approximates the probability
of target hypothesis T conditioned on source sen-
tence S. We follow the standard n-gram LM de-
composition of the target, where each target word
t
i
is conditioned on the previous n ? 1 target
words. To make this a joint model, we also condi-
tion on source context vector S
i
:
P (T |S) ? ?
|T |
i=1
P (t
i
|t
i?1
, ? ? ? , t
i?n+1
, S
i
)
Intuitively, we want to define S
i
as the window
that is most relevant to t
i
. To do this, we first say
that each target word t
i
is affiliated with exactly
one source word at index a
i
. S
i
is then them-word
source window centered at a
i
:
S
i
= s
a
i
?
m?1
2
, ? ? ? , s
a
i
, ? ? ? , s
a
i
+
m?1
2
This notion of affiliation is derived from the
word alignment, but unlike word alignment, each
target word must be affiliated with exactly one
non-NULL source word. The affiliation heuristic
is very simple:
(1) If t
i
aligns to exactly one source word, a
i
is
the index of the word it aligns to.
(2) If t
i
align to multiple source words, a
i
is the
index of the aligned word in the middle.
1
(3) If t
i
is unaligned, we inherit its affiliation
from the closest aligned word, with prefer-
ence given to the right.
2
An example of the NNJM context model for a
Chinese-English parallel sentence is given in Fig-
ure 1.
For all of our experiments we use n = 4 and
m = 11. It is clear that this model is effectively
an (n+m)-gram LM, and a 15-gram LM would be
1
We arbitrarily round down.
2
We have found that the affiliation heuristic is robust to
small differences, such as left vs. right preference.
far too sparse for standard probability models such
as Kneser-Ney back-off (Kneser and Ney, 1995)
or Maximum Entropy (Rosenfeld, 1996). Fortu-
nately, neural network language models are able
to elegantly scale up and take advantage of arbi-
trarily large context sizes.
2.1 Neural Network Architecture
Our neural network architecture is almost identi-
cal to the original feed-forward NNLM architec-
ture described in Bengio et al (2003).
The input vector is a 14-word context vector
(3 target words, 11 source words), where each
word is mapped to a 192-dimensional vector us-
ing a shared mapping layer. We use two 512-
dimensional hidden layers with tanh activation
functions. The output layer is a softmax over the
entire output vocabulary.
The input vocabulary contains 16,000 source
words and 16,000 target words, while the out-
put vocabulary contains 32,000 target words. The
vocabulary is selected by frequency-sorting the
words in the parallel training data. Out-of-
vocabulary words are mapped to their POS tag (or
OOV, if POS is not available), and in this case
P (POS
i
|t
i?1
, ? ? ? ) is used directly without fur-
ther normalization. Out-of-bounds words are rep-
resented with special tokens <src>, </src>,
<trg>, </trg>.
We chose these values for the hidden layer size,
vocabulary size, and source window size because
they seemed to work best on our data sets ? larger
sizes did not improve results, while smaller sizes
degraded results. Empirical comparisons are given
in Section 6.5.
2.2 Neural Network Training
The training procedure is identical to that of an
NNLM, except that the parallel corpus is used
instead of a monolingual corpus. Formally, we
seek to maximize the log-likelihood of the train-
ing data:
L =
?
i
log(P (x
i
))
where x
i
is the training sample, with one sample
for every target word in the parallel corpus.
Optimization is performed using standard back
propagation with stochastic gradient ascent (Le-
Cun et al, 1998). Weights are randomly initial-
ized in the range of [?0.05, 0.05]. We use an ini-
tial learning rate of 10
?3
and a minibatch size of
1371
Figure 1: Context vector for target word ?the?, using a 3-word target history and a 5-word source window
(i.e., n = 4 and m = 5). Here, ?the? inherits its affiliation from ?money? because this is the first aligned
word to its right. The number in each box denotes the index of the word in the context vector. This
indexing must be consistent across samples, but the absolute ordering does not affect results.
128.
3
At every epoch, which we define as 20,000
minibatches, the likelihood of a validation set is
computed. If this likelihood is worse than the pre-
vious epoch, the learning rate is multiplied by 0.5.
The training is run for 40 epochs. The training
data ranges from 10-30M words, depending on the
condition. We perform a basic weight update with
no L2 regularization or momentum. However, we
have found it beneficial to clip each weight update
to the range of [-0.1, 0.1], to prevent the training
from entering degenerate search spaces (Pascanu
et al, 2012).
Training is performed on a single Tesla K10
GPU, with each epoch (128*20k = 2.6M samples)
taking roughly 1100 seconds to run, resulting in
a total training time of ?12 hours. Decoding is
performed on a CPU.
2.3 Self-Normalized Neural Network
The computational cost of NNLMs is a significant
issue in decoding, and this cost is dominated by
the output softmax over the entire target vocabu-
lary. Even class-based approaches such as Le et
al. (2012) require a 2-20k shortlist vocabulary, and
are therefore still quite costly.
Here, our goal is to be able to use a fairly
large vocabulary without word classes, and to sim-
ply avoid computing the entire output layer at de-
code time.
4
To do this, we present the novel
technique of self-normalization, where the output
layer scores are close to being probabilities with-
out explicitly performing a softmax.
Formally, we define the standard softmax log
3
We do not divide the gradient by the minibatch size. For
those who do, this is equivalent to using an initial learning
rate of 10
?3
? 128 ? 10
?1
.
4
We are not concerned with speeding up training time, as
we already find GPU training time to be adequate.
likelihood as:
log(P (x)) = log
(
e
U
r
(x)
Z(x)
)
= U
r
(x)? log(Z(x))
Z(x) = ?
|V |
r
?
=1
e
U
r
?
(x)
where x is the sample, U is the raw output layer
scores, r is the output layer row corresponding to
the observed target word, and Z(x) is the softmax
normalizer.
If we could guarantee that log(Z(x)) were al-
ways equal to 0 (i.e., Z(x) = 1) then at decode
time we would only have to compute row r of the
output layer instead of the whole matrix. While
we cannot train a neural network with this guaran-
tee, we can explicitly encourage the log-softmax
normalizer to be as close to 0 as possible by aug-
menting our training objective function:
L =
?
i
[
log(P (x
i
))? ?(log(Z(x
i
))? 0)
2
]
=
?
i
[
log(P (x
i
))? ? log
2
(Z(x
i
))
]
In this case, the output layer bias weights are
initialized to log(1/|V |), so that the initial net-
work is self-normalized. At decode time, we sim-
ply use U
r
(x) as the feature score, rather than
log(P (x)). For our NNJM architecture, self-
normalization increases the lookup speed during
decoding by a factor of ?15x.
Table 1 shows the neural network training re-
sults with various values of the free parameter
?. In all subsequent MT experiments, we use
? = 10
?1
.
We should note that Vaswani et al (2013) im-
plements a method called Noise Contrastive Es-
timation (NCE) that is also used to train self-
normalized NNLMs. Although NCE results in
faster training time, it has the downside that there
1372
Arabic BOLT Val
? log(P (x)) | log(Z(x))|
0 ?1.82 5.02
10
?2
?1.81 1.35
10
?1
?1.83 0.68
1 ?1.91 0.28
Table 1: Comparison of neural network likelihood
for various ? values. log(P (x)) is the average
log-likelihood on a held-out set. | log(Z(x))| is
the mean error in log-likelihood when using U
r
(x)
directly instead of the true softmax probability
log(P (x)). Note that ? = 0 is equivalent to the
standard neural network objective function.
is no mechanism to control the degree of self-
normalization. By contrast, our ? parameter al-
lows us to carefully choose the optimal trade-off
between neural network accuracy and mean self-
normalization error. In future work, we will thor-
oughly compare self-normalization vs. NCE.
2.4 Pre-Computing the Hidden Layer
Although self-normalization significantly im-
proves the speed of NNJM lookups, the model
is still several orders of magnitude slower than a
back-off LM. Here, we present a ?trick? for pre-
computing the first hidden layer, which further in-
creases the speed of NNJM lookups by a factor of
1,000x.
Note that this technique only results in a signif-
icant speedup for self-normalized, feed-forward,
NNLM-style networks with one hidden layer. We
demonstrate in Section 6.6 that using one hidden
layer instead of two has minimal effect on BLEU.
For the neural network described in Section 2.1,
computing the first hidden layer requires mul-
tiplying a 2689-dimensional input vector
5
with
a 2689 ? 512 dimensional hidden layer matrix.
However, note that there are only 3 possible posi-
tions for each target word, and 11 for each source
word. Therefore, for every word in the vocabu-
lary, and for each position, we can pre-compute
the dot product between the word embedding and
the first hidden layer. These are computed offline
and stored in a lookup table, which is <500MB in
size.
Computing the first hidden layer now only re-
quires 15 scalar additions for each of the 512
hidden rows ? one for each word in the input
5
2689 = 14 words ? 192 dimensions + 1 bias
vector, plus the bias. This can be reduced to
just 5 scalar additions by pre-summing each 11-
word source window when starting a test sen-
tence. If our neural network has only one hid-
den layer and is self-normalized, the only remain-
ing computation is 512 calls to tanh() and a sin-
gle 513-dimensional dot product for the final out-
put score.
6
Thus, only ?3500 arithmetic opera-
tions are required per n-gram lookup, compared
to ?2.8M for self-normalized NNJM without pre-
computation, and ?35M for the standard NNJM.
7
Neural Network Speed
Condition lookups/sec sec/word
Standard 110 10.9
+ Self-Norm 1500 0.8
+ Pre-Computation 1,430,000 0.0008
Table 2: Speed of the neural network computa-
tion on a single CPU thread. ?lookups/sec? is the
number of unique n-gram probabilities that can be
computed per second. ?sec/word? is the amortized
cost of unique NNJM lookups in decoding, per
source word.
Table 2 shows the speed of self-normalization
and pre-computation for the NNJM. The decoding
cost is based on a measurement of ?1200 unique
NNJM lookups per source word for our Arabic-
English system.
8
By combining self-normalization and pre-
computation, we can achieve a speed of 1.4M
lookups/second, which is on par with fast back-
off LM implementations (Tanaka et al, 2013).
We demonstrate in Section 6.6 that using the self-
normalized/pre-computed NNJM results in only
a very small BLEU degradation compared to the
standard NNJM.
3 Decoding with the NNJM
Because our NNJM is fundamentally an n-gram
NNLM with additional source context, it can eas-
ily be integrated into any SMT decoder. In this
section, we describe the considerations that must
be taken when integrating the NNJM into a hierar-
chical decoder.
6
tanh() is implemented using a lookup table.
7
3500 ? 5? 512 + 2? 513; 2.8M ? 2? 2689? 512 +
2 ? 513; 35M ? 2 ? 2689 ? 512 + 2 ? 513 ? 32000. For
the sake of a fair comparison, these all use one hidden layer.
A second hidden layer adds 0.5M floating point operations.
8
This does not include the cost of duplicate lookups
within the same test sentence, which are cached.
1373
3.1 Hierarchical Parsing
When performing hierarchical decoding with an
n-gram LM, the leftmost and rightmost n ? 1
words from each constituent must be stored in the
state space. Here, we extend the state space to
also include the index of the affiliated source word
for these edge words. This does not noticeably in-
crease the search space. We also train a separate
lower-order n-gram model, which is necessary to
compute estimate scores during hierarchical de-
coding.
3.2 Affiliation Heuristic
For aligned target words, the normal affiliation
heuristic can be used, since the word alignment
is available within the rule. For unaligned words,
the normal heuristic can also be used, except when
the word is on the edge of a rule, because then the
target neighbor words are not necessarily known.
In this case, we infer the affiliation from the rule
structure. Specifically, if unaligned target word t
is on the right edge of an arc that covers source
span [s
i
, s
j
], we simply say that t is affiliated with
source word s
j
. If t is on the left edge of the arc,
we say it is affiliated with s
i
.
4 Model Variations
Recall that our NNJM feature can be described
with the following probability:
?
|T |
i=1
P (t
i
|t
i?1
, t
i?2
, ? ? ? , s
a
i
, s
a
i
?1
, s
a
i
+1
, ? ? ? )
This formulation lends itself to several natural
variations. In particular, we can reverse the trans-
lation direction of the languages, as well as the di-
rection of the language model.
We denote our original formulation as a source-
to-target, left-to-right model (S2T/L2R). We can
train three variations using target-to-source (T2S)
and right-to-left (R2L) models:
S2T/R2L
?
|T |
i=1
P (t
i
|t
i+1
, t
i+2
, ? ? ? , s
a
i
, s
a
i
?1
, s
a
i
+1
, ? ? ? )
T2S/L2R
?
|S|
i=1
P (s
i
|s
i?1
, s
i?2
, ? ? ? , t
a
?
i
, t
a
?
i
?1
, t
a
?
i
+1
, ? ? ? )
T2S/R2L
?
|S|
i=1
P (s
i
|s
i+1
, s
i+2
, ? ? ? , t
a
?
i
, t
a
?
i
?1
, t
a
?
i
+1
, ? ? ? )
where a
?
i
is the target-to-source affiliation, de-
fined analogously to a
i
.
The T2S variations cannot be used in decoding
due to the large target context required, and are
thus only used in k-best rescoring. The S2T/R2L
variant could be used in decoding, but we have not
found this beneficial, so we only use it in rescor-
ing.
4.1 Neural Network Lexical Translation
Model (NNLTM)
One issue with the S2T NNJM is that the prob-
ability is computed over every target word, so it
does not explicitly model NULL-aligned source
words. In order to assign a probability to every
source word during decoding, we also train a neu-
ral network lexical translation model (NNLMT).
Here, the input context is the 11-word source
window centered at s
i
, and the output is the tar-
get token t
s
i
which s
i
aligns to. The probabil-
ity is computed over every source word in the in-
put sentence. We treat NULL as a normal target
word, and if a source word aligns to multiple target
words, it is treated as a single concatenated token.
Formally, the probability model is:
?
|S|
i=1
P (t
s
i
|s
i
, s
i?1
, s
i+1
, ? ? ? )
This model is trained and evaluated like our
NNJM. It is easy and computationally inexpensive
to use this model in decoding, since only one neu-
ral network computation must be made for each
source word.
In rescoring, we also use a T2S NNLTM model
computed over every target word:
?
|T |
i=1
P (s
t
i
|t
i
, t
i?1
, t
i+1
, ? ? ? )
5 MT System
In this section, we describe the MT system used in
our experiments.
5.1 MT Decoder
We use a state-of-the-art string-to-dependency hi-
erarchical decoder (Shen et al, 2010). Our base-
line decoder contains a large and powerful set of
features, which include:
? Forward and backward rule probabilities
? 4-gram Kneser-Ney LM
? Dependency LM (Shen et al, 2010)
? Contextual lexical smoothing (Devlin, 2009)
? Length distribution (Shen et al, 2010)
? Trait features (Devlin and Matsoukas, 2012)
? Factored source syntax (Huang et al, 2013)
? 7 sparse feature types, totaling 50k features
(Chiang et al, 2009)
? LM adaptation (Snover et al, 2008)
1374
We also perform 1000-best rescoring with the
following features:
? 5-gram Kneser-Ney LM
? Recurrent neural network language model
(RNNLM) (Mikolov et al, 2010)
Although we consider the RNNLM to be part
of our baseline, we give it special treatment in the
results section because we would expect it to have
the highest overlap with our NNJM.
5.2 Training and Optimization
For Arabic word tokenization, we use the MADA-
ARZ tokenizer (Habash et al, 2013) for the BOLT
condition, and the Sakhr
9
tokenizer for the NIST
condition. For Chinese tokenization, we use a sim-
ple longest-match-first lexicon-based approach.
For word alignment, we align all of the train-
ing data with both GIZA++ (Och and Ney, 2003)
and NILE (Riesa et al, 2011), and concatenate the
corpora together for rule extraction.
For MT feature weight optimization, we use
iterative k-best optimization with an Expected-
BLEU objective function (Rosti et al, 2010).
6 Experimental Results
We present MT primary results on Arabic-English
and Chinese-English for the NIST OpenMT12 and
DARPA BOLT conditions. We also present a set
of auxiliary results in order to further analyze our
features.
6.1 NIST OpenMT12 Results
Our NIST system is fully compatible with the
OpenMT12 constrained track, which consists of
10M words of high-quality parallel training for
Arabic, and 25M words for Chinese.
10
The
Kneser-Ney LM is trained on 5B words of data
from English GigaWord. For test, we use
the ?Arabic-To-English Original Progress Test?
(1378 segments) and ?Chinese-to-English Orig-
inal Progress Test + OpenMT12 Current Test?
(2190 segments), which consists of a mix of
newswire and web data.
11
All test segments have
4 references. Our tuning set contains 5000 seg-
ments, and is a mix of the MT02-05 eval set as
well as held-out parallel training.
9
http://www.sakhr.com
10
We also make weak use of 30M-100M words of UN data
+ ISI comparable corpora, but this data provides almost no
benefit.
11
http://www.nist.gov/itl/iad/mig/openmt12results.cfm
NIST MT12 Test
Ar-En Ch-En
BLEU BLEU
OpenMT12 - 1st Place 49.5 32.6
OpenMT12 - 2nd Place 47.5 32.2
OpenMT12 - 3rd Place 47.4 30.8
? ? ? ? ? ? ? ? ?
OpenMT12 - 9th Place 44.0 27.0
OpenMT12 - 10th Place 41.2 25.7
Baseline (w/o RNNLM) 48.9 33.0
Baseline (w/ RNNLM) 49.8 33.4
+ S2T/L2R NNJM (Dec) 51.2 34.2
+ S2T NNLTM (Dec) 52.0 34.2
+ T2S NNLTM (Resc) 51.9 34.2
+ S2T/R2L NNJM (Resc) 52.2 34.3
+ T2S/L2R NNJM (Resc) 52.3 34.5
+ T2S/R2L NNJM (Resc) 52.8 34.7
?Simple Hier.? Baseline 43.4 30.1
+ S2T/L2R NNJM (Dec) 47.2 31.5
+ S2T NNLTM (Dec) 48.5 31.8
+ Other NNJMs (Resc) 49.7 32.2
Table 3: Primary results on Arabic-English and
Chinese-English NIST MT12 Test Set. The first
section corresponds to the top and bottom ranked
systems from the evaluation, and are taken from
the NIST website. The second section corresponds
to results on top of our strongest baseline. The
third section corresponds to results on top of a
simpler baseline. Within each section, each row
includes all of the features from previous rows.
BLEU scores are mixed-case.
Results are shown in the second section of Ta-
ble 3. On Arabic-English, the primary S2T/L2R
NNJM gains +1.4 BLEU on top of our baseline,
while the S2T NNLTM gains another +0.8, and
the directional variations gain +0.8 BLEU more.
This leads to a total improvement of +3.0 BLEU
from the NNJM and its variations. Considering
that our baseline is already +0.3 BLEU better than
the 1st place result of MT12 and contains a strong
RNNLM, we consider this to be quite an extraor-
dinary improvement.
12
For the Chinese-English condition, there is an
improvement of +0.8 BLEU from the primary
NNJM and +1.3 BLEU overall. Here, the base-
line system is already +0.8 BLEU better than the
12
Note that the official 1st place OpenMT12 result was our
own system, so we can assure that these comparisons are ac-
curate.
1375
best MT12 system. The smaller improvement on
Chinese-English compared to Arabic-English is
consistent with the behavior of our baseline fea-
tures, as we show in the next section.
6.2 ?Simple Hierarchical? NIST Results
The baseline used in the last section is a highly-
engineered research system, which uses a wide
array of features that were refined over a num-
ber of years, and some of which require linguis-
tic resources. Because of this, the baseline BLEU
scores are much higher than a typical MT system
? especially a real-time, production engine which
must support many language pairs.
Therefore, we also present results using a
simpler version of our decoder which emulates
Chiang?s original Hiero implementation (Chiang,
2007). Specifically, this means that we don?t
use dependency-based rule extraction, and our de-
coder only contains the following MT features: (1)
rule probabilities, (2) n-gram Kneser-Ney LM, (3)
lexical smoothing, (4) target word count, (5) con-
cat rule penalty.
Results are shown in the third section of Table 3.
The ?Simple Hierarchical? Arabic-English system
is -6.4 BLEU worse than our strong baseline, and
would have ranked 10th place out of 11 systems
in the evaluation. When the NNJM features are
added to this system, we see an improvement of
+6.3 BLEU, which would have ranked 1st place in
the evaluation.
Effectively, this means that for Arabic-English,
the NNJM features are equivalent to the combined
improvements from the string-to-dependency
model plus all of the features listed in Section 5.1.
For Chinese-English, the ?Simple Hierarchical?
system only degrades by -3.2 BLEU compared
to our strongest baseline, and the NNJM features
produce a gain of +2.1 BLEU on top of that.
6.3 BOLT Web Forum Results
DARPA BOLT is a major research project with the
goal of improving translation of informal, dialec-
tical Arabic and Chinese into English. The BOLT
domain presented here is ?web forum,? which was
crawled from various Chinese and Egyptian Inter-
net forums by LDC. The BOLT parallel training
consists of all of the high-quality NIST training,
plus an additional 3 million words of translated
forum data provided by LDC. The tuning and test
sets consist of roughly 5000 segments each, with
2 references for Arabic and 3 for Chinese.
Results are shown in Table 4. The baseline here
uses the same feature set as the strong NIST sys-
tem. On Arabic, the total gain is +2.6 BLEU,
while on Chinese, the gain is +1.3 BLEU.
BOLT Test
Ar-En Ch-En
BLEU BLEU
Baseline (w/o RNNLM) 40.2 30.6
Baseline (w/ RNNLM) 41.3 30.9
+ S2T/L2R NNJM (Dec) 42.9 31.9
+ S2T NNLTM (Dec) 43.2 31.9
+ Other NNJMs (Resc) 43.9 32.2
Table 4: Primary results on Arabic-English and
Chinese-English BOLT Web Forum. Each row
includes the aggregate features from all previous
rows.
6.4 Effect of k-best Rescoring Only
Table 5 shows performance when our S2T/L2R
NNJM is used only in 1000-best rescoring, com-
pared to decoding. The primary purpose of this is
as a comparison to Le et al (2012), whose model
can only be used in k-best rescoring.
BOLT Test
Ar-En
Without With
RNNLM RNNLM
BLEU BLEU
Baseline 40.2 41.3
S2T/L2R NNJM (Resc) 41.7 41.6
S2T/L2R NNJM (Dec) 42.8 42.9
Table 5: Comparison of our primary NNJM in de-
coding vs. 1000-best rescoring.
We can see that the rescoring-only NNJM per-
forms very well when used on top of a baseline
without an RNNLM (+1.5 BLEU), but the gain on
top of the RNNLM is very small (+0.3 BLEU).
The gain from the decoding NNJM is large in both
cases (+2.6 BLEU w/o RNNLM, +1.6 BLEU w/
RNNLM). This demonstrates that the full power of
the NNJM can only be harnessed when it is used
in decoding. It is also interesting to see that the
RNNLM is no longer beneficial when the NNJM
is used.
1376
6.5 Effect of Neural Network Configuration
Table 6 shows results using the S2T/L2R NNJM
with various configurations. We can see that re-
ducing the source window size, layer size, or vo-
cab size will all degrade results. Increasing the
sizes beyond the default NNJM has almost no ef-
fect (102%). Also note that the target-only NNLM
(i.e., Source Window=0) only obtains 33% of the
improvements of the NNJM.
BOLT Test
Ar-En
BLEU % Gain
?Simple Hier.? Baseline 33.8 -
S2T/L2R NNJM (Dec) 38.4 100%
Source Window=7 38.3 98%
Source Window=5 38.2 96%
Source Window=3 37.8 87%
Source Window=0 35.3 33%
Layers=384x768x768 38.5 102%
Layers=192x512 38.1 93%
Layers=128x128 37.1 72%
Vocab=64,000 38.5 102%
Vocab=16,000 38.1 93%
Vocab=8,000 37.3 83%
Activation=Rectified Lin. 38.5 102%
Activation=Linear 37.3 76%
Table 6: Results with different neural net-
work architectures. The ?default? NNJM in
the second row uses these parameters: SW=11,
L=192x512x512, V=32,000, A=tanh. All mod-
els use a 3-word target history (i.e., 4-gram LM).
?Layers? refers to the size of the word embedding
followed by the hidden layers. ?Vocab? refers to
the size of the input and output vocabularies. ?%
Gain? is the BLEU gain over the baseline relative
to the default NNJM.
6.6 Effect of Speedups
All previous results use a self-normalized neural
network with two hidden layers. In Table 7, we
compare this to using a standard network (with
two hidden layers), as well as a pre-computed neu-
ral network.
13
The ?Simple Hierarchical? base-
line is used here because it more closely approx-
imates a real-time MT engine. For the sake of
speed, these experiments only use the S2T/L2R
NNJM+S2T NNLTM.
13
The difference in score for self-normalized vs. pre-
computed is entirely due to two vs. one hidden layers.
Each result from Table 7 corresponds to a row
in Table 2 of Section 2.4. We can see that go-
ing from the standard model to the pre-computed
model only reduces the BLEU improvement from
+6.4 to +6.1, while increasing the NNJM lookup
speed by a factor of 10,000x.
BOLT Test
Ar-En
BLEU Gain
?Simple Hier.? Baseline 33.8 -
Standard NNJM 40.2 +6.4
Self-Norm NNJM 40.1 +6.3
Pre-Computed NNJM 39.9 +6.1
Table 7: Results for the standard NNs vs. self-
normalized NNs vs. pre-computed NNs.
In Table 2 we showed that the cost of unique
lookups for the pre-computed NNJM is only
?0.001 seconds per source word. This does not
include the cost of n-gram creation or cached
lookups, which amount to ?0.03 seconds per
source word in our current implementation.
14
However, the n-grams created for the NNJM can
be shared with the Kneser-Ney LM, which reduces
the cost of that feature. Thus, the total cost in-
crease of using the NNJM+NNLTM features in
decoding is only ?0.01 seconds per source word.
In future work we will provide more detailed
analysis regarding the usability of the NNJM in a
low-latency, high-throughput MT engine.
7 Related Work
Although there has been a substantial amount of
past work in lexicalized joint models (Marino et
al., 2006; Crego and Yvon, 2010), nearly all of
these papers have used older statistical techniques
such as Kneser-Ney or Maximum Entropy. How-
ever, not only are these techniques intractable to
train with high-order context vectors, they also
lack the neural network?s ability to semantically
generalize (Mikolov et al, 2013) and learn non-
linear relationships.
A number of recent papers have proposed meth-
ods for creating neural network translation/joint
models, but nearly all of these works have ob-
tained much smaller BLEU improvements than
ours. For each related paper, we will briefly con-
14
In our decoder, roughly 95% of NNJM n-gram lookups
within the same sentence are duplicates.
1377
trast their methodology with our own and summa-
rize their BLEU improvements using scores taken
directly from the cited paper.
Auli et al (2013) use a fixed continuous-space
source representation, obtained from LDA (Blei
et al, 2003) or a source-only NNLM. Also, their
model is recurrent, so it cannot be used in decod-
ing. They obtain +0.2 BLEU improvement on top
of a target-only NNLM (25.6 vs. 25.8).
Schwenk (2012) predicts an entire target phrase
at a time, rather than a word at a time. He obtains
+0.3 BLEU improvement (24.8 vs. 25.1).
Zou et al (2013) estimate context-free bilingual
lexical similarity scores, rather than using a large
context. They obtain an +0.5 BLEU improvement
on Chinese-English (30.0 vs. 30.5).
Kalchbrenner and Blunsom (2013) implement
a convolutional recurrent NNJM. They score a
1000-best list using only their model and are able
to achieve the same BLEU as using all 12 standard
MT features (21.8 vs 21.7). However, additive re-
sults are not presented.
The most similar work that we know of is Le et
al. (2012). Le?s basic procedure is to re-order the
source to match the linear order of the target, and
then segment the hypothesis into minimal bilin-
gual phrase pairs. Then, he predicts each target
word given the previous bilingual phrases. How-
ever, Le?s formulation could only be used in k-
best rescoring, since it requires long-distance re-
ordering and a large target context.
Le?s model does obtain an impressive +1.7
BLEU gain on top of a baseline without an NNLM
(25.8 vs. 27.5). However, when compared to
the strongest baseline which includes an NNLM,
Le?s best models (S2T + T2S) only obtain an +0.6
BLEU improvement (26.9 vs. 27.5). This is con-
sistent with our rescoring-only result, which indi-
cates that k-best rescoring is too shallow to take
advantage of the power of a joint model.
Le?s model also uses minimal phrases rather
than being purely lexicalized, which has two main
downsides: (a) a number of complex, hand-crafted
heuristics are required to define phrase boundaries,
which may not transfer well to new languages, (b)
the effective vocabulary size is much larger, which
substantially increases data sparsity issues.
We should note that our best results use six sep-
arate models, whereas all previous work only uses
one or two models. However, we have demon-
strated that we can obtain 50%-80% of the to-
tal improvement with only one model (S2T/L2R
NNJM), and 70%-90% with only two models
(S2T/L2R NNJM + S2T NNLTM). Thus, the one
and two-model conditions still significantly out-
perform any past work.
8 Discussion
We have described a novel formulation for a neural
network-based machine translation joint model,
along with several simple variations of this model.
When used as MT decoding features, these models
are able to produce a gain of +3.0 BLEU on top of
a very strong and feature-rich baseline, as well as
a +6.3 BLEU gain on top of a simpler system.
Our model is remarkably simple ? it requires no
linguistic resources, no feature engineering, and
only a handful of hyper-parameters. It also has no
reliance on potentially fragile outside algorithms,
such as unsupervised word clustering. We con-
sider the simplicity to be a major advantage. Not
only does this suggest that it will generalize well to
new language pairs and domains, but it also sug-
gests that it will be straightforward for others to
replicate these results.
Overall, we believe that the following factors set
us apart from past work and allowed us to obtain
such significant improvements:
1. The ability to use the NNJM in decoding
rather than rescoring.
2. The use of a large bilingual context vector,
which is provided to the neural network in
?raw? form, rather than as the output of some
other algorithm.
3. The fact that the model is purely lexicalized,
which avoids both data sparsity and imple-
mentation complexity.
4. The large size of the network architecture.
5. The directional variation models.
One of the biggest goals of this work is to quell
any remaining doubts about the utility of neural
networks in machine translation. We believe that
there are large areas of research yet to be explored.
For example, creating a new type of decoder cen-
tered around a purely lexicalized neural network
model. Our short term ideas include using more
interesting types of context in our input vector
(such as source syntax), or using the NNJM to
model syntactic/semantic structure of the target.
1378
References
Michael Auli, Michel Galley, Chris Quirk, and Geof-
frey Zweig. 2013. Joint language and translation
modeling with recurrent neural networks. In Pro-
ceedings of the 2013 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1044?
1054, Seattle, Washington, USA, October. Associa-
tion for Computational Linguistics.
Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Re-
search, 3:1137?1155.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. J. Mach. Learn.
Res., 3:993?1022, March.
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 new features for statistical machine transla-
tion. In HLT-NAACL, pages 218?226.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Josep Maria Crego and Franc?ois Yvon. 2010. Factored
bilingual n-gram language models for statistical ma-
chine translation. Machine Translation, 24(2):159?
175.
Jacob Devlin and Spyros Matsoukas. 2012. Trait-
based hypothesis selection for machine translation.
In Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
NAACL HLT ?12, pages 528?532, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Jacob Devlin. 2009. Lexical features for statistical
machine translation. Master?s thesis, University of
Maryland.
Nizar Habash, Ryan Roth, Owen Rambow, Ramy Es-
kander, and Nadi Tomeh. 2013. Morphological
analysis and disambiguation for dialectal arabic. In
HLT-NAACL, pages 426?432.
Zhongqiang Huang, Jacob Devlin, and Rabih Zbib.
2013. Factored soft source syntactic constraints for
hierarchical machine translation. In EMNLP, pages
556?566.
Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent
continuous translation models.
Reinhard Kneser and Hermann Ney. 1995. Im-
proved backing-off for m-gram language modeling.
In Acoustics, Speech, and Signal Processing, 1995.
ICASSP-95., 1995 International Conference on, vol-
ume 1, pages 181?184. IEEE.
Hai-Son Le, Alexandre Allauzen, and Franc?ois Yvon.
2012. Continuous space translation models with
neural networks. In Proceedings of the 2012 Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, NAACL HLT ?12, pages 39?
48, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Yann LeCun, L?eon Bottou, Genevieve B Orr, and
Klaus-Robert M?uller. 1998. Efficient backprop. In
Neural networks: Tricks of the trade, pages 9?50.
Springer.
Jos?e B Marino, Rafael E Banchs, Josep M Crego, Adri`a
De Gispert, Patrik Lambert, Jos?e AR Fonollosa, and
Marta R Costa-Juss`a. 2006. N-gram-based machine
translation. Computational Linguistics, 32(4):527?
549.
Tomas Mikolov, Martin Karafi?at, Lukas Burget, Jan
Cernock?y, and Sanjeev Khudanpur. 2010. Recur-
rent neural network based language model. In IN-
TERSPEECH, pages 1045?1048.
Tomas Mikolov, Wen tau Yih, and Geoffrey Zweig.
2013. Linguistic regularities in continuous space
word representations. In HLT-NAACL, pages 746?
751.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio.
2012. On the difficulty of training recurrent neural
networks. arXiv preprint arXiv:1211.5063.
Jason Riesa, Ann Irvine, and Daniel Marcu. 2011.
Feature-rich language-independent syntax-based
alignment for statistical machine translation. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP ?11,
pages 497?507, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Ronald Rosenfeld. 1996. A maximum entropy ap-
proach to adaptive statistical language modeling.
Computer, Speech and Language, 10:187?228.
Antti Rosti, Bing Zhang, Spyros Matsoukas, and
Rich Schwartz. 2010. BBN system descrip-
tion for WMT10 system combination task. In
WMT/MetricsMATR, pages 321?326.
Holger Schwenk. 2010. Continuous-space language
models for statistical machine translation. Prague
Bull. Math. Linguistics, 93:137?146.
Holger Schwenk. 2012. Continuous space translation
models for phrase-based statistical machine transla-
tion. In COLING (Posters), pages 1071?1080.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2010.
String-to-dependency statistical machine transla-
tion. Computational Linguistics, 36(4):649?671,
December.
1379
Matthew Snover, Bonnie Dorr, and Richard Schwartz.
2008. Language and translation model adaptation
using comparable corpora. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP ?08, pages 857?866,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Makoto Tanaka, Yasuhara Toru, Jun-ya Yamamoto, and
Mikio Norimatsu. 2013. An efficient language
model using double-array structures.
Ashish Vaswani, Yinggong Zhao, Victoria Fossum,
and David Chiang. 2013. Decoding with large-
scale neural language models improves translation.
In Proceedings of the 2013 Conference on Em-
pirical Methods in Natural Language Processing,
pages 1387?1392, Seattle, Washington, USA, Oc-
tober. Association for Computational Linguistics.
Will Y Zou, Richard Socher, Daniel Cer, and Christo-
pher D Manning. 2013. Bilingual word embeddings
for phrase-based machine translation. In Proceed-
ings of the 2013 Conference on Empirical Methods
in Natural Language Processing, pages 1393?1398.
1380
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 428?437,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Decision Trees for Lexical Smoothing in Statistical Machine
Translation
Rabih Zbib
?
and Spyros Matsoukas and Richard Schwartz and John Makhoul
BBN Technologies, 10 Moulton Street, Cambridge, MA 02138, USA
? Massachusetts Institute of Technology, 77 Massachusetts Ave., Cambridge, MA 02139, USA
Abstract
We present a method for incorporat-
ing arbitrary context-informed word at-
tributes into statistical machine trans-
lation by clustering attribute-qualified
source words, and smoothing their
word translation probabilities using bi-
nary decision trees. We describe two
ways in which the decision trees are
used in machine translation: by us-
ing the attribute-qualified source word
clusters directly, or by using attribute-
dependent lexical translation probabil-
ities that are obtained from the trees,
as a lexical smoothing feature in the de-
coder model. We present experiments
using Arabic-to-English newswire data,
and using Arabic diacritics and part-of-
speech as source word attributes, and
show that the proposed method im-
proves on a state-of-the-art translation
system.
1 Introduction
Modern statistical machine translation (SMT)
models, such as phrase-based SMT or hierar-
chical SMT, implicitly incorporate source lan-
guage context. It has been shown, however,
that such systems can still benefit from the
explicit addition of lexical, syntactic or other
kinds of context-informed word features (Vick-
rey et al, 2005; Gimpel and Smith, 2008;
Brunning et al, 2009; Devlin, 2009). But the
benefit obtained from the addition of attribute
information is in general countered by the in-
crease in the model complexity, which in turn
results in a sparser translation model when es-
timated from the same corpus of data. The
increase in model sparsity usually results in a
deterioration of translation quality.
In this paper, we present a method for using
arbitrary types of source-side context-informed
word attributes, using binary decision trees to
deal with the sparsity side-effect. The deci-
sion trees cluster attribute-dependent source
words by reducing the entropy of the lexi-
cal translation probabilities. We also present
another method where, instead of clustering
the attribute-dependent source words, the de-
cision trees are used to interpolate attribute-
dependent lexical translation probability mod-
els, and use those probabilities to compute a
feature in the decoder log-linear model.
The experiments we present in this paper
were conducted on the translation of Arabic-
to-English newswire data using a hierarchical
system based on (Shen et al, 2008), and using
Arabic diacritics (see section 2.3) and part-of-
speech (POS) as source word attributes. Pre-
vious work that attempts to use Arabic dia-
critics in machine translation runs against the
sparsity problem, and appears to lose most of
the useful information contained in the dia-
critics when using partial diacritization (Diab
et al, 2007). Using the methods proposed
in this paper, we manage to obtain consistent
improvements from diacritics against a strong
baseline. The methods we propose, though,
are not restrictive to Arabic-to-English trans-
lation. The same techniques can also be used
with other language pairs and arbitrary word
attribute types. The attributes we use in the
described experiments are local; but long dis-
tance features can also be used.
In the next section, we review relevant pre-
vious work in three areas: Lexical smoothing
and lexical disambiguation techniques in ma-
chine translation; using decision trees in nat-
ural language processing, and especially ma-
chine translation; and Arabic diacritics. We
present a brief exposition of Arabic orthogra-
428
phy, and refer to previous work on automatic
diacritization of Arabic text. Section 3 de-
scribes the procedure for constructing the deci-
sion trees, and the two methods for using them
in machine translation. In section 4 we de-
scribe the experimental setup and present ex-
perimental results. Finally, section 5 concludes
the paper and discusses future directions.
2 Previous Work
2.1 Lexical Disambiguation and
Lexical Smoothing
Various ways have been proposed to improve
the lexical translation choices of SMT systems.
These approaches typically incorporate local
context information, either directly or indi-
rectly.
The use of Word Sense Disambiguation
(WSD) has been proposed to enhance ma-
chine translation by disambiguating the source
words (Cabezas and Resnick, 2005; Carpuat
and Wu, 2007; Chan et al, 2007). WSD
usually requires that the training data be la-
beled with senses, which might not be avail-
able for many languages. Also, WSD is tra-
ditionally formulated as a classification prob-
lem, and therefore does not naturally lend it-
self to be integrated into the generative frame-
work of machine translation. Carpuat and Wu
(2007) formulate the SMT lexical disambigua-
tion problem as a WSD task. Instead of learn-
ing from word sense corpora, they use the SMT
training data, and use local context features to
enhance the lexical disambiguation of phrase-
based SMT.
Sarikaya et al (2007) incorporate context
more directly by using POS tags on the target
side to model word context. They augmented
the target words with POS tags of the word
itself and its surrounding words, and used the
augmented words in decoding and for language
model rescoring. They reported gains on Iraqi-
Arabic-to-English translation.
Finally, using word-to-word context-free lex-
ical translation probabilities has been shown
to improve the performance of machine trans-
lation systems, even those using much more
sophisticated models. This feature, usually
called lexical smoothing, has been used in
phrase-based systems (Koehn et al, 2003).
Och et al (2004) also found that including
IBM Model 1 (Brown et al, 1993) word prob-
abilities in their log-linear model works better
than most other higher-level syntactic features
at improving the baseline. The incorporation
of context on the source or target side en-
hances the gain obtained from lexical smooth-
ing. Gimpel and Smith (2008) proposed us-
ing source-side lexical features in phrase-based
SMT by conditioning the phrase probabilities
on those features. They used word context,
syntactic features or positional features. The
features were added as components into the
log-linear decoder model, each with a tunable
weight. Devlin (2009) used context lexical fea-
tures in a hierarchical SMT system, interpolat-
ing lexical counts based on multiple contexts.
It also used target-side lexical features.
The work in the paper incorporates con-
text information based on the reduction of the
translation probability entropy.
2.2 Decision Trees
Decision trees have been used extensively in
various areas of machine learning, typically
as a way to cluster patterns in order to im-
prove classification (Duda et al, 2000). They
have, for instance, been long used success-
fully in speech recognition to cluster context-
dependent phoneme model states (Young et
al., 1994).
Decision trees have also been used in ma-
chine translation, although to a lesser extent.
In this respect, our work is most similar to
(Brunning et al, 2009), where the authors ex-
tended word alignment models for IBM Model
1 and Hidden Markov Model (HMM) align-
ments. They used decision trees to cluster the
context-dependent source words. Contexts be-
longing to the same cluster were grouped to-
gether during Expectation Maximization (EM)
training, thus providing a more robust proba-
bility estimate. While Brunning et al (2009)
used the source context clusters for word align-
ments, we use the attribute-dependent source
words directly in decoding. The approach we
propose can be readily used with any align-
ment model.
Stroppa et al (2007) presented a general-
ization of phrase-based SMT (Koehn et al,
2003) that also takes into account source-
side context information. They conditioned
the target phrase probability on the source
429
phrase as well as source phrase context, such
as bordering words, or part-of-speech of bor-
dering words. They built a decision tree for
each source phrase extracted from the train-
ing data. The branching of the tree nodes
was based on the different context features,
branching on the most class-discriminative fea-
tures first. Each node is associated with the
set of aligned target phrases and correspond-
ing context-conditioned probabilities. The de-
cision tree thus smoothes the phrase probabil-
ities based on the different features, allowing
the model to back off to less context, or no
context at all depending on the presence of
that context-dependent source phrase in the
training data. The model, however, did not
provide for a back-off mechanism if the phrase
pair was not found in the extracted phrase ta-
ble. The method presented in this paper differs
in various aspects. We use context-dependent
information at the source word level, rather
than the phrase level, thus making it readily
applicable to any translation model and not
just phrase-based translation. By incorporat-
ing context at the word level, we can decode
directly with attribute-augmented source data
(see section 3.2).
2.3 Arabic Diacritics
Since an important part of the experiments
described in this paper use diacritized Arabic
source, we present a brief description of Arabic
orthography, and specifically diacritics.
The Arabic script, like that of most other
Semitic languages, only represents consonants
and long vowels using letters
1
. Short vowels
can be written as small marks written above
or below the preceding consonant, called di-
acritics. The diacritics are, however, omit-
ted from written text, except in special cases,
thus creating an additional level of lexical am-
biguity. Readers can usually guess the cor-
rect pronunciation of words in non-diacritized
text from the sentence and discourse context.
Grammatical case on nouns and adjectives are
also marked using diacritics at the end of
words. Arabic MT systems use undiacritized
text, since most available Arabic data is undi-
acritized.
1
Such writing systems are sometimes referred to as
Abjads (See Daniels, Peter T., et al eds. The World's
Writing Systems Oxford. (1996), p.4.)
Automatic diacritization of Arabic has been
done with high accuracy, using various genera-
tive and discriminative modeling techniques.
For example, Ananthakrishnan et al (2005)
used a generative model that incorporates
word level n-grams, sub-word level n-grams
and part-of-speech information to perform di-
acritization. Nelken and Shieber (2005) mod-
eled the generative process of dropping dia-
critics using weighted transducers, then used
Viterbi decoding to find the most likely gener-
ator. Zitouni et al (2006) presented a method
based on maximum entropy classifiers, us-
ing features like character n-grams, word n-
grams, POS and morphological segmentation.
Habash and Rambow (2007) determined vari-
ous morpho-syntactic features of the word us-
ing SVM classifiers, then chose the correspond-
ing diacritization. The experiments in this
paper use the automatic diacritizer by Sakhr
Software. The diacritizer determines word di-
acritics through rule-based morphological and
syntactic analysis. It outputs a diacritization
for both the internal stem and case ending
markers of the word, with an accuracy of 97%
for stem diacritization and 91% for full dia-
critization (i.e., including case endings).
There has been work done on using dia-
critics in Automatic Speech Recognition, e.g.
(Vergyri and Kirchhoff, 2004). However, the
only previous work on using diacritization for
MT is (Diab et al, 2007), which used the di-
acritization system described in (Habash and
Rambow, 2007). It investigated the effect
of using full diacritization as well as partial
diacritization on MT results. The authors
found that using full diacritics deteriorates MT
performance. They used partial diacritiza-
tion schemes, such as diacritizing only passive
verbs, keeping the case endings diacritics, or
only gemination diacritics. They also saw no
gain in most configurations. The authors ar-
gued that the deterioration in performance is
caused by the increase in the size of the vo-
cabulary, which in turn makes the translation
model sparser; as well as by errors during the
automatic diacritization process.
430
3 Decision Trees for Source Word
Attributes
3.1 Growing the Decision Tree
In this section, we describe the procedure
for growing the decision trees using context-
informed source word attributes.
The attribute-qualified source-side of the
parallel training data is first aligned to the
target-side data. If S is the set of attribute-
dependent forms of source word s, and tj is a
target word aligned to si ? S, then we define:
p (tj |si) =
count(si,tj)
count(si)
(1)
where count(si, tj) is the count of alignment
links between si and tj .
A separate binary decision tree is grown for
each source word. We start by including all the
attribute-dependent forms of the source word
at the root of the tree. We split the set of at-
tributes at each node into two child nodes, by
choosing the splitting that maximizes the re-
duction in weighted entropy of the probability
distribution in (1). In other words, at node n,
we choose the partition (S?1 , S
?
2) such that:
(S?1 , S
?
2) =
argmax
(S1,S2)
S1?S2=S
{h(S)? (h(S1) + h(S2))}
(2)
where h(S) is the entropy of the probabil-
ity distribution p(tj |si ? S), weighted by the
number of samples in the training data of the
source words in S. We only split a node if the
entropy is reduced by more than a threshold
?h. This step is repeated recursively until the
tree cannot be grown anymore.
Weighting the entropy by the source word
counts gives more weight to the context-
dependent source words with a higher number
of samples in the training data, sine the lex-
ical translation probability estimates for fre-
quent words can be trusted better. The ratio-
nale behind the splitting criterion used is that
the split that reduces the entropy of the lexical
translation probability distribution the most
is also the split that best separates the list of
forms of the source word in terms of the target
word translation. For a source word that has
multiple meanings, depending on its context,
the decision tree will tend to implicitly sepa-
rate those meanings using the information in
the lexical translation probabilities.
Although we describe this method as grow-
ing one decision tree for each word, and using
one attribute type at a time, a decision tree
can clearly be constructed for multiple words,
and more than one attribute type can be used
in the same decision tree.
3.2 Trees for Source Word Clustering
The source words could be augmented to ex-
plicitly incorporate the word attributes (dia-
critics or other attribute types). The aug-
mented source will be less ambiguous if the
attributes do in fact contain disambiguating
information. This, in principle, helps machine
translation performance. The flip side is that
the resulting increase in vocabulary size in-
creases the translation model sparsity, usually
with a detrimental effect on translation.
To mitigate the effect of the increase in vo-
cabulary, decision trees can be use to cluster
the attribute-augmented source words. More
specifically, a decision tree is grown for each
source word as described in the previous sec-
tion, using a predefined entropy threshold ?h.
When the tree cannot be expanded anymore,
its leaf nodes will contain a multi-set parti-
tioning of the list of attribute-dependent forms
of that source word. Each of the clusters is
treated as an equivalence class, and all forms
in that class are mapped to a unique form (e.g.
an arbitrarily chosen member of the cluster).
The mappings are used to map the tokens in
the parallel training data before alignment is
run on the mapped data. The test data is
also mapped consistently. This clustering pro-
cedure will only keep the attribute-dependent
forms of the source words that decrease the un-
certainty in the translation probabilities, and
are thus useful for translation.
The experiments we report on use diacritics
as an attribute type. The various diacritized
forms of a source word are thus used to train
the decision trees. The resulting clusters are
used to map the data into a subset of the vo-
cabulary that is used in translation training
and decoding (see section 4.2 for results). Di-
acritics are obviously specific to Arabic. But
this method can be used with other attribute
types, by first appending the source words with
431
{sijo
na,s
ijni}sjn
{sijo
na,s
ijni,
sajo
na,s
ajon
u,sa
jana
} {saj
ana
}
{saj
ona
,sajo
nu}
Figure 1: Decision tree for source word sjn using
diacritics as an attribute.
their context (e.g. attach to each source word
its part-of-speech tag or context), and then
training decision trees and mapping the source
side of the data.
Figure 1 shows an example of a decision
tree for the Arabic word sjn
2
using diacritics
as a source attribute. The root contains the
various diacritized forms (sijona `prison AC-
CUSATIVE', sijoni `prison DATIVE', sajona
`imprisonment ACCUSATIVE.', sajoni `im-
prisonment ACCUSATIVE.', sajana `he im-
prisoned '). The leaf nodes contain the
attribute-dependent clusters.
3.3 Trees for Lexical Smoothing
As mentioned in section 2.1, lexical smoothing,
computed from word-to-word translation prob-
abilities, is a useful feature, even in SMT sys-
tems that use sophisticated translation mod-
els. This is likely due to the robustness of
context-free word-to-word translation proba-
bility estimates compared to the probabilities
of more complicated models. In those models,
the rules and probabilities are estimated from
much larger sample spaces.
In our system, the lexical smoothing feature
is computed as follows:
f(U)=
?
tj?T (U)
(
1?
?
si?{S(U)?NULL}
(1?p?(tj |si))
)
(3)
where U is the modeling unit specific to the
translation model used. For a phrase-based
system, U is the phrase pair, and for a hierar-
chical system U is the translation rule. S (U)
2
Examples are written using Buckwalter transliter-
ation.
sjn
{sijo
na,s
ijni,
sajo
na,s
ajon
u,sa
jana
} {s
ajan
a}
{sijo
na}
{sijo
ni}
{saj
ona
}
{saj
onu
}
{sijo
na}
{sijo
ni}Figure 2: Decision tree for source word sjn grown
fully using diacritics.
is the set of terminals on the source side of U,
and T (U) is the set of terminals on its tar-
get. The NULL term in the equation above
accounts for unaligned target words, which we
found in our experiments to be beneficial. One
way of interpreting equation (3) is that f (U)
is the probability that for each target word tj
in U, tj is a likely translation of at least one
word si on the source side. The feature value
is then used as a component in the log-linear
model, with a tunable weight.
In this work, we generalize the lexical
smoothing feature to incorporate the source
word attributes. A tree is grown for each
source word as described in section 3.1, but
using an entropy threshold ?h = 0. In other
words, the tree is grown all the way until each
leaf node contains one attribute-dependent
form of the source word. Each node in the
tree contains a cluster of attribute-dependent
forms of the source word, and a corresponding
attribute-dependent lexical translation prob-
ability distribution. The lexical translation
probability models at the root nodes are those
of the regular attribute-independent lexical
translation probabilities. The models at the
leaf nodes are the most fine-grained, since they
are conditioned on only one attribute value.
Figure 2 shows a fully grown decision tree for
the same source word as the example in Figure
1.
The lexical probability distribution at the
leafs are from sparser data than the original
distributions, and are therefore less robust. To
address this, the attribute-dependent lexical
432
smoothing feature is estimated by recursively
interpolating the lexical translation probabil-
ities up the tree. The probability distribu-
tion pn at each node n is interpolated with
the probability of its parent node as follows:
pn =
{
pn if n is root,
wnpn + (1? wn)pm otherwise
where m is the parent of n
(4)
A fraction of the parent probability mass is
thus given to the probability of the child node.
If the probability estimate of an attribute-
dependent form of a source word with a cer-
tain target word t is not reliable, or if the
probability estimate is 0 (because the source
word in this context is not aligned with t),
then the model gracefully backs off by using
the probability estimates from other attribute-
dependent lexical translation probability mod-
els of the source word.
The interpolation weight is a logistic regres-
sion function of the source word count at a
node n:
wn =
1
1 + e???? log(count(Sn))
(5)
The weight varies depending on the count
of the attribute-qualified source word in each
node, thus reflecting the confidence in the es-
timates of each node's distribution. The two
global parameters of the function, a bias ? and
a scale ? are tuned to maximize the likelihood
of a set of alignment counts from a heldout
data set of 179K sentences. The tuning is done
using Powell's method (Brent, 1973).
During decoding, we use the probability dis-
tribution at the leaves to compute the feature
value f(R) for each hierarchical rule R. We
train and decode using the regular, attribute-
independent source. The source word at-
tributes are used in the decoder only to in-
dex the interpolated probability distribution
needed to compute f (R).
4 Experiments
4.1 Experimental Setup
As mentioned before, the experiments we re-
port on use a string-to-dependency-tree hier-
archical translation system based on the model
described in (Shen et al, 2008). Forward and
Likelihood %
baseline -1.29 -
Diacs.
dec. trees
-1.25 +2.98%
POS dec.
trees
-1.24 +3.41%
Table 1: Normalized likelihood of the test set algn-
ments without decision trees, then with decision trees
using diacritics and part-of-speech respectively.
backward context-free lexical smoothing are
used as decoder features in all the experiments.
Other features such as rule probabilities and
dependency tree language model (Shen et al,
2008) are also used. We use GIZA++ (Och
and Ney, 2003) for word alignments. The de-
coder model parameters are tuned using Mini-
mum Error Rate training (Och, 2003) to max-
imize the IBM BLEU score (Papineni et al,
2002).
For training the alignments, we use 27M
words from the Sakhr Arabic-English Paral-
lel Corpus (SSUSAC27). The language model
uses 7B words from the English Gigaword and
from data collected from the web. A 3-gram
language model is used during decoding. The
decoder produces an N-best list that is re-
ranked using a 5-gram language model.
We tune and test on two separate data sets
consisting of documents from the following col-
lections: the newswire portion of NIST MT04,
MT05, MT06, and MT08 evaluation sets, the
GALE Phase 1 (P1) and Phase 2 (P2) evalu-
ation sets, and the GALE P2 and P3 develop-
ment sets. The tuning set contains 1994 sen-
tences and the test set contains 3149 sentences.
The average length of sentences is 36 words.
Most of the documents in the two data sets
have 4 reference translations, but some have
only one. The average number of reference
translations per sentence is 3.94 for the tun-
ing set and 3.67 for the test set.
In the next section, we report on measure-
ments of the likelihood of test data, and de-
scribe the translation experiments in detail.
4.2 Results
In order to assess whether the decision trees
are in fact helpful in decreasing the uncer-
tainty in the lexical translation probabilities
433
54.254.354.454.554.654.754.854.955
MT Score  in BLEU
5454.154.254.354.454.554.654.754.854.955
0
25
50
100
Entr
opy T
hresh
old
Figure 3: BLEU scores of the clustering experiments
as a function of the entropy threshold on tuning set.
on unseen data, we compute the likelihood
of the test data with respect to these prob-
abilities with and without the decision tree
splitting. We align the test set with its ref-
erence using GIZA++, and then obtain the
link count l_count(si, tj) for each alignment
link i = (si,ti) in the set of alignment links I.
We calculate the normalized likelihood of the
alignments:
L = log
?
?
(
?
i
p(ti | si)
l_count(si,ti)
) 1
|I|
?
?
=
1
|I|
?
i?I
l_count(si, ti) log p? (ti | si) (6)
where p? (ti | si) is the probability for the word
pair (ti, si) in equation (4). If the same in-
stance of source word si is aligned to two tar-
get words ti and tj , then these two links are
counted separately. If a source in the test set
is out-of-vocabulary, or if a word pair (ti, si)
is aligned in the test alignment but not in the
training alignments (and thus has no probabil-
ity estimate), then it is ignored in the calcula-
tion of the log-likelihood.
Table 1 shows the likelihood for the baseline
case, where one lexical translation probability
distribution is used per source word. It also
shows the likelihoods calculated using the lex-
ical distributions in the leaf nodes of the de-
cision trees, when either diacritics or part-of-
speech are used as an attribute type. The table
shows an increase in the likelihood of 2.98% us-
ing diacritics, and 3.41% using part-of-speech.
The translation result tables present MT
scores in two different metrics: Translation
Edit Rate (Snover et al, 2006) and IBM
TER BLEU
Test
baseline 40.14 52.05
full diacritics 40.31 52.39
+0.17 +0.34
dec. trees, diac (?h = 50) 39.75 52.60
-0.39 +0.55
Table 2: Results of experiments using decision trees
to cluster source words.
BLEU. The reader is reminded that a higher
BLEU score and a lower TER are desired. The
tables also show the difference in scores be-
tween the baseline and each experiment. It is
worth noting that the gains reported are rela-
tive to a strong baseline that uses a state-of-
the-art system with many features, and a fairly
large training corpus.
The decision tree clustering experiment as
described in section 3.2 depends on a global
parameter, namely the threshold in entropy re-
duction ?h. We tune this parameter manually
on a tuning set. Figure 3 shows the BLEU
scores as a function of the threshold value, with
diacritics as an attribute type. The most gain
is obtained for an entropy threshold of 50.
The fully diacritized data has an average of
1.78 diacritized forms per source word. The av-
erage weighted by the number of occurrences is
6.28, which indicates that words with more di-
acritized forms tend to occur more frequently.
After clustering using a value of ?h = 50,
the average number of diacritized forms be-
comes 1.11, and the occurrence weighted av-
erage becomes 3.69. The clustering proce-
dure thus seems to eliminate most diacritized
forms, which likely do not contain helpful dis-
ambiguating information.
Table 2 lists the detailed results of experi-
ments using diacritics. In the first experiment,
we show that using full diacritization results in
a small gain on the BLEU score and no gain on
TER, which is somewhat consistent with the
result obtained by Diab et al (2007). The next
experiment shows the results of clustering the
diacritized source words using decision trees
for the entropy threshold of 50. The TER loss
of the full diacritics becomes a gain, and the
BLEU gain increases. This confirms our spec-
ulation that the use of fully diacritized data in-
434
TER BLEU
Test
baseline 40.14 52.05
dec. trees, diacs 39.75 52.55
-0.39 +0.50
dec. trees, POS 40.05 52.40
-0.09 +0.35
dec. trees, diacs, no interpolation 39.98 52.09
-0.16 +0.04
Table 3: Results of experiments using the word attribute-dependent lexical smoothing feature.
creases the model sparsity, which undoes most
of the benefit obtained from the disambiguat-
ing information that the diacritics contain. Us-
ing the decision trees to cluster the diacritized
source data prunes diacritized forms that do
not decrease the entropy of the lexical trans-
lation probability distributions. It thus finds
a sweet-spot between the negative effect of in-
creasing the vocabulary size and the positive
effect of disambiguation.
In our experiments, using diacritics with
case endings gave consistently better score
than using diacritics with no case endings, de-
spite the fact that they result in a higher vo-
cabulary size. One possible explanation is that
diacritics not only help in lexical disambigua-
tion, but they might also be indirectly help-
ing in phrase reordering, since the diacritics on
the final letter indicate the word's grammatical
function.
The results from using decision trees to in-
terpolate attribute-dependent lexical smooth-
ing features are summarized in table 3. In
the first experiment, we show the results of
using diacritics to estimate the interpolated
lexical translation probabilities. The results
show a gain of +0.5 BLEU points and 0.39
TER points. The gain is statistically signifi-
cant with a 95% confidence level. Using part-
of-speech as an attribute gives a smaller, but
still statistically significant gain. We also ran
a control experiment, where we used diacritic-
dependent lexical translation probabilities ob-
tained from the decision trees, but did not per-
form the probability interpolation of equation
(4). The gains mostly disappear, especially on
BLEU, showing the importance of the inter-
polation step for the proper estimation of the
lexical smoothing feature.
5 Conclusion and Future Directions
We presented in this paper a new method for
incorporating explicit context-informed word
attributes into SMT using binary decision
trees. We reported on experiments on Arabic-
to-English translation using diacritized Ara-
bic and part-of-speech as word attributes, and
showed that the use of these attributes in-
creases the likelihood of source-target word
pairs of unseen data. We proposed two spe-
cific ways in which the results of the decision
tree training process are used in machine trans-
lation, and showed that they result in better
translation results.
For future work, we plan on using multi-
ple source-side attributes at the same time.
Different attributes could have different dis-
ambiguating information, which could pro-
vide more benefit than using any of the at-
tributes alone. We also plan on investigat-
ing the use of multi-word trees; trees for word
clusters can for instance be grown instead
of growing a separate tree for each source
word. Although the experiments presented
in this paper use local word attributes, noth-
ing in principle prevents this method from be-
ing used with long-distance sentence context,
or even with document-level or discourse-level
features. Our future plans include the investi-
gation of using such features as well.
Acknowledgment
This work was supported by DARPA/IPTO
Contract No. HR0011-06-C-0022 under the
GALE program.
The views, opinions, and/or findings con-
tained in this article are those of the author
and should not be interpreted as representing
the official views or policies, either expressed
435
or implied, of the Defense Advanced Research
Projects Agency or the Department of Defense.
A pproved for Public Release, Distribution Un-
limited.
References
S. Ananthakrishnan, S. Narayanan, and S. Ban-
galore. 2005. Automatic diacritization of ara-
bic transcripts for automatic speech recognition.
Kanpur, India.
R. Brent. 1973. Algorithms for Minimization
Without Derivatives. Prentice-Hall.
P. Brown, V. Della Pietra, S. Della Pietra, and
R. L. Mercer. 1993. The mathematics of sta-
tistical machine translation: Parameter estima-
tion. Computational Linguistics, 19:263311.
J. Brunning, A. de Gispert, and W. Byrne. 2009.
Context-dependent alignment models for statis-
tical machine translation. In NAACL '09: Pro-
ceedings of the 2009 Human Language Technol-
ogy Conference of the North American Chapter
of the Association for Computational Linguis-
tics, pages 110118.
C. Cabezas and P. Resnick. 2005. Using WSD
techniques for lexical selection in statistical ma-
chine translation. In Technical report, Insti-
tute for Advanced Computer Studies (CS-TR-
4736, LAMP-TR-124, UMIACS-TR-2005-42),
College Park, MD.
M. Carpuat and D. Wu. 2007. Improving statis-
tical machine translation using word sense dis-
ambiguation. In EMNLP-CoNLL-2007: Pro-
ceedings of the 2007 Joint Conference on Em-
pirical Methods in Natural Language Processing
and Computational Natural Language Learning,
Prague, Czech Republic.
Y. Chan, H. Ng, and D. Chiang. 2007. Word
sense disambiguation improves statistical ma-
chine translation. In Proceedings of the 45th
Annual Meeting of the Association for Compu-
tational Linguistics (ACL).
J. Devlin. 2009. Lexical features for statistical
machine translation. Master's thesis, University
of Maryland, December 2009.
M. Diab, M. Ghoneim, and N. Habash. 2007. Ara-
bic diacritization in the context of statistical ma-
chine translation. InMT Summit XI, pages 143
149, Copenhagen, Denmark.
R. O. Duda, P. E. Hart, and D. G. Stork. 2000.
Pattern Classification. Wiley-Interscience Pub-
lication.
K. Gimpel and N. A. Smith. 2008. Rich source-
side context for statistical machine translation.
In StatMT '08: Proceedings of the Third Work-
shop on Statistical Machine Translation, pages
917, Columbus, Ohio.
N. Habash and O. Rambow. 2007. Arabic diacriti-
zation through full morphological tagging. In
Proceedings of the 2007 Human Language Tech-
nology Conference of the North American Chap-
ter of the Association for Computational Lin-
guistics, pages 5356, Rochester, New York.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statis-
tical phrase-based translation. In Proceedings of
the 2003 Human Language Technology Confer-
ence of the North American Chapter of the As-
sociation for Computational Linguistics, pages
4854, Edmonton, Canada.
R. Nelken and S. M. Shieber. 2005. Arabic dia-
critization using weighted finite-state transduc-
ers. In Proceedings of the 2005 ACL Workshop
on Computational Approaches to Semitic Lan-
guages, Ann Arbor, Michigan.
F. J. Och and H. Ney. 2003. A systematic com-
parison of various statistical alignment models.
Computational Linguistics, 29(1):1951.
F. J. Och, D. Gildea, S. Khudanpur, A. Sarkar,
K. Yamada, A. Fraser, S. Kumar, L. Shen,
D. Smith, K. Eng, V. Jain, Z. Jin, and D. R.
Radev. 2004. A smorgasbord of features for sta-
tistical machine translation. In HLT-NAACL,
pages 161168.
F. J. Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings
of the 41st Annual Meeting of the Association
for Computational Linguistics (ACL), Sapporo,
Japan.
K. Papineni, S. Roukos, T. Ward, and W. Zhu.
2002. Bleu: a method for automatic evalua-
tion of machine translation. In Proceedings of
the 40th Annual Meeting of the Association for
Computational Linguistics (ACL), Philadelphia,
PA.
Ruhi Sarikaya, Yonggang Deng, and Yuqing Gao.
2007. Context dependent word modeling for sta-
tistical machine translation using part-of-speech
tags. In Proceedings of INTERSPEECH 2007fs,
Antwerp, Belgium.
L. Shen, J. Xu, and R. Weischedel. 2008. A new
string-to-dependency machine translation algo-
rithm with a target dependency language model.
In Proceedings of the 46th Annual Meeting of
the Association for Computational Linguistics
(ACL), Columbus, Ohio.
M. Snover, B. Dorr, R. Schwartz, J. Makhoul, and
L. Micciulla. 2006. A study of translation error
436
rate with targeted human annotation. In Pro-
ceedings of the 7th Conf. of the Association for
Machine Translation in the Americas (AMTA
2006), pages 223231, Cambridge, MA.
N. Stroppa, A. van den Bosch, and A Way.
2007. Exploiting source similarity for SMT us-
ing context-informed features. In Proceedings of
the 11th International Conference on Theoreti-
cal and Methodological Issues in Machine Trans-
lation (TMI-07), pages 231240.
D. Vergyri and K. Kirchhoff. 2004. Automatic
diacritization of arabic for acoustic modeling in
speech recognition. In Semitic '04: Proceedings
of the Workshop on Computational Approaches
to Arabic Script-based Languages, pages 6673,
Geneva, Switzerland.
D. Vickrey, L. Biewald, M. Teyssier, and D. Koller.
2005. Word-sense disambiguation for machine
translation. In HLT '05: Proceedings of the
conference on Human Language Technology and
Empirical Methods in Natural Language Pro-
cessing, Vancouser, BC, Canada.
S.J. Young, J.J. Odell, and P.C. Woodland. 1994.
Tree-based state tying for high accuracy acoustic
modelling. In HLT'94: Proceedings of the Work-
shop on Human Language Technology, pages
307312.
I. Zitouni, J. S. Sorensen, and Ruhi Sarikaya. 2006.
Maximum entropy based restoration of arabic
diacritics. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics
and the 44th annual meeting of the Association
for Computational Linguistics, pages 577584,
Sydney, Australia.
437
