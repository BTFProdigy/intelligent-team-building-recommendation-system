Exploiting semantic information for manual anaphoric annotation in
Cast3LB corpus
Borja Navarro, Rube?n Izquierdo, Maximiliano Saiz-Noeda
Departmento de Lenguajes y Sistemas Informa?ticos
Universidad de Alicante
Ap. Correo 99, E-03080
Alicante, Spain
{borja,ruben,max}@dlsi.ua.es
Abstract
This paper presents the discourse annotation fol-
lowed in Cast3LB, a Spanish corpus annotated with
several information sources (morphological, syntac-
tic, semantic and coreferential) at syntactic, seman-
tic and discourse level. 3LB annotation scheme has
been developed for three languages (Spanish, Cata-
lan and Basque). Human annotators have used a
set of tagging techniques and protocols. Several
tools have provided them with a friendly annotation
scheme. At discourse level, anaphoric and coref-
erence expressions are annotated. One of the most
interesting contributions to this annotation scenario
is the enriched anaphora resolution module that is
based on the previously defined semantic annotation
phase to expand the discourse information and use
it to suggest the correct antecedent of an anaphora
to the annotator. This paper describes the relevance
of the semantic tags in the discourse annotation in
Spanish corpus Cast3LB and shows both levels and
tools in the mentioned discourse annotation scheme.
1 Introduction
Cast3LB corpus is annotated (Navarro et al, 2003)
at three linguistic levels: sentence level (syntac-
tic), lexical level (semantic) and discourse level. At
discourse level, it is annotated with anaphoric and
coreferential information. In order to improve the
time-consuming and tedious task of the manually
annotation, a semiautomatic and interactive process
is followed: first, an anaphora resolution system se-
lects each anaphora and its antecedent from a list
of candidates; then, the human annotator decides
wether or not accept the suggestion.
With this approach, the correctness of the
anaphora resolution system is a key factor in the
quest of an efficient annotation process. For this
reason, we use the linguistic information of the
previous annotation tasks (morphological, syntac-
tic and, mainly, semantic information) to improve
the anaphora resolution system. In this paper we
will focus on the use of semantic information in the
anaphora and coreferential manual annotation task.
Next section presents the project overview and
the three annotation levels. Following sections
present the semantic annotation and the way it
serves to the discourse annotations. Last section
presents annotation tools used to annotate the cor-
pus at semantic and coreferential level.
2 Cast3LB corpus: annotation project
overview
Cast3Lb project is part of the general project 3LB1.
The main objective of this general project is to
develop three corpora annotated with syntactic,
semantic and pragmatic/coreferential information:
one for Catalan (Cat3LB), one for Basque (Eus3LB)
and one for Spanish (Cast3LB).
The Spanish corpus Cast3LB is a part of the
CLIC-TALP corpus, which is made up of 100.000
words from the LexEsp corpus (Sebastia?n et al,
2000) plus 25.000 words coming from the EFE
Spanish Corpus, given by the Agencia EFE (the of-
ficial news agency) for research purposes. The EFE
corpus fragments are comparable among the lan-
guages of the general project (Catalan, Basque and
Spanish).
We have selected this corpus because it contains
a large variety of Spanish texts (newspapers, novels,
scientific papers. . . ), both from Spain and South-
America, so it is a good representation of the cur-
rent state of the Spanish language. Moreover, the
automatic morphological annotation of this corpus
has been manually checked (Civit, 2003).
The spirit of the annotation scheme is to build a
flexible system portable to different romance lan-
guages and to potential new cases that might appear,
but consistent with all annotation levels and annota-
tion data.
At the syntactic level we follow the constituency
annotation scheme. Main principles of syntactic an-
notation are the following (Civit et al, 2003): a)
1Project partially funded by Spanish Government FIT-150-
500-2002-244.
only the explicit elements are annotated (except for
elliptical subjects); b) we do not alter the surface
word order of the elements; c) we do not follow any
specific theoretical framework; d) we do not take
into account the verbal phrase, rather, the main con-
stituents of the sentence become the daughters of
the root node; e) this syntactic information is en-
riched by the functional information of the main
phrases, but we have not taken into account the pos-
sibility of double functions.
At the semantic level, we annotate the sense of
the nouns, verbs and some adjectives, following an
all words approach. The specific sense (or senses)
of each one is assigned by means of the EuroWord-
Net offset number (Vossen, 1998). Also, due to
some words are not available in EuroWordNet or
do not have the suitable sense, we have created two
new tags to mark this circumstance.
At the discourse level, we mark the coreference of
nominal phrases and some elliptical elements. The
coreference expressions taken into account are per-
sonal pronouns, clitics, elliptical subjects and some
elliptical adjectives. The definite descriptions are
not marked. The possible antecedents considered
are the nominal phrases or other coreferential ex-
pressions.
3 Semantic annotation
As we said before, main objective of Cast3LB
project at semantic level is to develop an ?all words?
corpus with the specific sense (or senses) of nouns,
verbs and adjectives.
Our proposal is based on the SemCor corpus
(Miller, 1990). This corpus is formed by a por-
tion of the Brown corpus and the novel The Red
Badge of Courage. Altogether, it is formed by ap-
proximately 250.000 words, where nouns, verbs,
adjectives and adverbs have been manually anno-
tated with WordNet senses (Miller, 1990). Another
corpus with WordNet-based semantic annotation is
the DSO corpus (Ng and Lee, 1996). In this cor-
pus, the most frequent English ambiguous nouns
and verbs had been annotated with the correct sense
(121 nouns and 70 verbs). The corpus is formed by
192.800 sentences from the Brown Corpus and the
Wall Street Journal, and it has also been manually
annotated. Finally, the SENSEVAL forum has de-
veloped a few sense annotated corpora for the eval-
uation of Word Sense Disambiguation systems (Kil-
garriff and Palmer, 2000), some of which also use
WordNet as a lexical resource.
We have decided to use Spanish WordNet for sev-
eral reasons. First of all, Spanish WordNet is, up to
now, the more commonly used lexical resource in
Word Sense Disambiguation tasks. Secondly, it is
one of the most complete lexical resources currently
available for Spanish. Finally, as part of EuroWord-
Net, the lexical structure of Spanish and the lexical
structure of Catalan and Basque are related. There-
fore, the annotated senses of the three corpora of
3LB project can also be related.
The tag used to mark a word sense is its off-
set number, that is, its identification number in Eu-
roWordNet?s InterLingua Index. The corpus has
42291 lexical words, where 20461 are nouns, 13471
are verbs and 8543 are adjectives.
On other hand, not all nouns, verbs, adjectives
and adverbs are annotated, due to EuroWordNet
does not contain them. Possible lacks in this sense
are (i) the synset, (ii) the word, (iii) the synset and
the word, and (iv) the link between the synset and
the word.
In order to deal with these cases we have defined
two more tags in EuroWordNet:
? C1S: the word is found, but not its correct
sense (due to a sense lack, or because there is
no link between the word and the synset).
? C2S: the word is not found (because it is not
there, or because both the word and the synset
are missing).
It is possible to distinguish two methods for se-
mantically annotate a corpus. The first one is linear
(or ?textual?) method (Kilgarriff, 1998), where the
human annotator marks the sentences token by to-
ken up to the end of the corpus. In this strategy the
annotator must read and analyze the sense of each
word every time it appears in the corpus. The sec-
ond annotation method is transversal (or ?lexical?)
(Kilgarriff, 1998), where he/she annotates word-
type by word-type, all the occurrences of each word
in the corpus one by one. With this method, the
annotator must read and analyze all the senses of a
word only once.
We have followed in Cast3LB the transversal pro-
cess. The main advantage of this method is that
we can focus our attention on the sense structure of
one word and deal with its specific semantic prob-
lems: its main sense or senses, its specific senses. . . .
Then we check the context of the single word each
time it appears and select the corresponding sense.
Through this approach, semantic features of each
word is taken into consideration only once, and the
whole corpus achieves greater consistency. Through
the linear process, however, the annotator must re-
member the sense structure of each word and their
specific problems each time the word appears in
the corpus, making the annotation process much
more complex, and increasing the possibilities of
low consistency and disagreement between the an-
notators.
Nevertheless, the transversal method finds its dis-
advantage in the annotation of large corpus, be-
cause no fragment of the corpus is available until
the whole corpus is completed. To avoid this, we
have selected a fragment of the whole corpus and
annotated it by means of the linear process.
Everybody agrees that semantic annotation is a
tedious and difficult task. From a general point of
view, the main problem in the semantic annotation
is the subjectivity of the human annotator when it
comes to the selection of the correct sense, because
there are usually more than one sense for a word,
and, due to the WorNet?s granularity, more than one
could be correct for a given word. Another impor-
tant problem in the semantic annotation is the poor
agreement between different annotators, due to the
ambiguity and/or vagueness of many words.
In order to overcome these problems, the annota-
tion process has been carried out in two steps. In the
first step, a subset of ambiguous words have been
annotated twice by two annotators. With this dou-
ble annotation we have developed a disagreement
typology and an annotation handbook, where all the
possible causes of ambiguity have been described
and common solutions have been adopted for the
rest of cases. In the second step the remaining cor-
pus is annotated following the criteria adopted in the
annotation handbook.
Our final aim is to obtain useful resources for
Word Sense Disambiguation (WSD) systems in
Spanish. This semantically annotated corpus will
be used as a training corpus for the development of
unsupervised systems and as a reference in general
evaluation tasks. At the end of the project, we will
have a large amount of words with an unambiguous
sense tag in a real context.
As well as this final application, we exploit this
semantic information in the anaphoric annotation
task. In (Saiz-Noeda, 2002), how to apply seman-
tic information in anaphora resolution systems is
showed and evaluated. We take this proposal, but
applied to manual anaphora annotation.
Due to the corpus has been annotated with syn-
tactic information, and the sense of each word is
marked with the offset number of EuroWordNet,
it is possible to extract semantic features of each
verb and noun through the ontological concepts of
the EuroWordNet?s Top Ontology. Furthermore, the
corpus has been annotated with syntactic roles, so
it is possible to extract syntactic patterns formed by
the verb and its main complements: subject-verb,
verb-direct objects, verb-indirect objects.
As we will show bellow, these patterns are use-
ful in order to select the specific antecedent of an
anaphora, according to semantic compatibility cri-
teria between the antecedent and the verb of the sen-
tence where the anaphora appears.
4 Discourse annotation: anaphora and
coreference
At discourse level, our objective is to annotate the
anaphora and the coreference, in order to develop
useful resources for anaphora resolution systems.
We agreed to annotate the anaphoric elements
and their antecedents. These anaphoric elements are
the anaphoric ellipsis, the pronominal anaphora and
the coreferential chains.
Specifically, in each one, we mark:
? Anaphoric ellipsis:
? The elliptical subject, made explicit in the
syntactic annotation step. Being a noun
phrase, it could also be an antecedent too.
Unlike English, where it is possible an
expletive pronoun as subject, in Spanish
it is very common an elliptical nominal
phrase as subject of the sentence. This is
why we have decide to include this kind
of anaphora in the annotation process.
? Elliptical head of nominal phrases with
an adjective complement. In English,
this construction is the ?one anaphora?.
In Spanish, however, the anaphoric con-
struction is made up by an elliptical head
noun and an adjective complement.
? Anaphora: Two kinds of pronouns:
? The tonic personal pronouns in the third
person. They can appear in subject func-
tion or in object function.
? The atonic pronouns, specifically the
clitic pronouns that appear in the subcate-
gorization frame of the main verb.
? Finally, there are sets of anaphoric and ellipti-
cal units that corefer to the same entity. These
units form coreferential chains. They must be
marked in order to show the cohesion and co-
herence of the text. They are annotated by
means of the identification of the same an-
tecedent.
We do not annotate the definite descriptions.
They consist of nominal phrases that can refer (or
not) to an antecedent. We do not mark them because
they outline specific problems that make this task
very difficult: firstly, there are not clear criteria that
allow us to distinguish between coreferential and
not coreferential nominal phrases; secondly, there
are not a clear typology for definite descriptions;
and finally, there are not a clear typology of rela-
tionships between the definite description and their
antecedents. These problems could further increase
the time-consuming in the annotation process and
widen the gap of disagreement between the human
annotators.
This proposal of annotation scheme is based on
the one used in the MUC (Message Understanding
Conference) (Hirschman, 1997) as well as in the
works of Gaizauskas (Gaizauskas and Humphreys,
1996) and Mitkov (Mitkov et al, 2002): this is
the mostly used scheme in coreferential annotation
(Mitkov, 2002).
In the anaphoric annotation, two linguistic ele-
ments must be marked: the anaphoric expression
and its antecedent. In the antecedent we annotate
the following information:
? A reference tag that shows the presence of an
antecedent (?REF?),
? An identification number (?ID?),
? The minimum continuous substring that could
be considerer correct (?MIN?).
In the coreferential expression, we annotate:
? The presence of a coreferential expression
(?COREF?),
? An identification number (?ID?),
? The type of anaphoric expression: elliptical
subject, elliptical head of noun phrase, tonic
pronoun or atonic pronoun (?TYPE?),
? The antecedent, through its identification num-
ber (?REF?),
? Finally, a status tag where the annotators shows
their confidence in the annotation (?STA-
TUS?).
As previously mentioned in this paper, the main
problem in the anaphoric annotation is the low
agreement between human annotators. There is usu-
ally less agreement in anaphoric annotation than in
syntactic annotation ((Mitkov, 2002), 141). In order
to reduce this low agreement, we annotate only the
clearest type of anaphoric units (pronouns, ellipti-
cal subjects and elliptical nominal heads), and we
introduce the lowest necessary information. More-
over, with the tag ?STATUS?, the human annotator
can show his confidence in the anaphoric unit and
the antecedent marked. However, at the moment, as
occurs in the semantic annotation, we do not have
enough data on the agreement between annotators.
4.1 Manual annotation with an Enriched
Anaphora Resolution System
As we said before, we follow a manual anaphora an-
notation with the help of a Enriched Anaphora Res-
olution System: our idea is to check the automatic
annotation of the anaphora resolution system and to
correct mistakes in the annotation process.
In manual anaphora and coreferential annota-
tion, the human annotator first locates a possible
anaphora, and then must read back the text until
the antecedent appears. With an anaphora resolu-
tion system it is possible to automatize this pro-
cess: the system selects possible anaphoric ele-
ments, their possible antecedents, and decides the
main candidate. The human annotator must only
check the suggestion. The process is more useful
because the most tedious task (to select a possible
anaphora, to read back looking for the antecedent,
etc.) is made up by the system. When the human
annotator checks the solution, he does not read back
for antecedents, he goes directly to the possible an-
tecedents.
However, the anaphora resolution system must be
very accurate. In order to automatically specify the
antecedent of an anaphora and ensure the correct-
ness of the system, we use all the linguistic infor-
mation previously annotated in the corpus: morpho-
logical, syntactic and semantic. In this knowledge-
based anaphora resolution system, the linguistic in-
formation is used through a set of restrictions and
preferences. Following this strategy, the system re-
jects possible antecedents until only one is selected.
The key point is the linguistic information used in
restrictions and preferences.
We have developed a semantically enriched
anaphora resolution system in order to aid the dis-
course annotation level. EuroWordNet synsets are
the base of the semantic information added to the
resolution process. The fact of counting with a se-
mantically annotated corpus such as Cast3Lb facil-
itates the use of the anaphora resolution method,
based on a natural way of understanding the human
process for anaphora resolution.
The specific use of semantic information is re-
lated to the sematic compatibility between the possi-
ble antecedent (a noun) and the verb of the sentence
in which the anaphoric pronoun appears. Due to the
pronoun replaces a lexical word (the antecedent),
the semantic information of the antecedent must
be compatible with the semantic restrictions of the
verb. In other words, the anaphoric expression takes
the semantic features of the antecedent, so they must
be compatible with the semantic restrictions of the
verb.
In this way, verbs like ?eat? or ?drink? will be
specially compatible with animal subjects and eat-
able and drinkable objects than others.
In our case, the semantic features of the lexi-
cal words have been extracted form the ontologi-
cal concepts of EuroWorNet, that is, the Top On-
tology. All the synsets in EuroWordnet are seman-
tically described through a set of base concepts (the
more general concepts). In the EuroWorNet?s Top
Ontology, these base concepts are classified in the
three orders of Lyons (Lyons, 1977), according to
basic semantic distinctions. So through the top on-
tology, all the synsets of EuroWordNet are seman-
tically described with concepts like ?human?, ?an-
imal?, ?artifact?, etc. With this, we have extracted
subject-verb, verb-direct object and/or verb-indirect
object semantic patterns.
From this semantic patters, rules about the se-
mantic compatibility between nouns and verbs have
been extracted. These rules are applied to the
anaphora resolution as preferences. Based on the
patterns, the system calculates the compatibility
between the verb of the sentence in which the
anaphora appears and the antecedent. So the possi-
ble antecedents with low compatibility are rejected,
and the antecedents with high compatibility are se-
lected. These semantic preferences, plus the syn-
tactic and morphological restrictions and prefer-
ences, are used to select the correct antecedent of
the anaphora.
Furthermore, semantic information is also used in
some rules. There are two kind of rules:
? ?NO? rules: NO(v#sense,c,r) defines the in-
compatibility between the verb v (and it sense)
and any name which contains ?c? in its ontolog-
ical concept list, being ?r? the syntactic func-
tion that relates them.
? ?MUST? rules: MUST(v#sense,c,r) defines
the incompatibility between the verb v (and its
sense) and all the names that don?t contain ?c?
in their ontological concept list, being ?r? the
syntactic function that relates them.
At the final annotation step, the annotator checks
if the antecedent selected is the correct one or not,
and, in each case, confirms the annotation or cor-
rects it.
5 Tools
5.1 3LB-SAT
3LB-SAT (Semantic Annotation Tool) is a tool for
the semantic tagging of multilingual corpora. Main
features of this tool are:
? it is word-oriented,
? it allows different format for input corpus; ba-
sically, the main formats used in corpus anno-
tation: treebank format (TBF) and XML for-
mat;
? it uses EuroWordNet as a lexical resource.
For the XML format a DTD has been defined, that
allows to describe the information structure in each
file of the corpus.
In the annotation process, monosemic words are
automatically annotated. So, 3LB-SAT is used to
annotated only the polysemic words. When a file
is loaded, all lemmas of the file are shown (Fig-
ure 1). The tool uses different colors to indicate the
state of the annotation process: (i) no occurrence of
the lemma in the file has been annotated, (ii) some
occurrences of the lemma in the file have been an-
notated, or (iii) all the occurrences have been an-
notated. When the annotator selects a lemma, all
its occurrences are shown. The selection of one of
them shows all possible senses, and the annotator
chooses the correct one for this specific context.
Figure 1: 3LB-SAT semantic annotation tool.
5.2 3LB-RAT
3LB-RAT (Reference Annotation Tool) is a tool de-
veloped in 3LB project for the annotation and su-
pervision of anaphora and coreferences at discourse
level.
The tool provides the annotator with two working
ways: manual and semiautomatic. In the first one,
the tool locates and shows all possible anaphoric
and coreference elements and their possible an-
tecedents. The annotator chooses one of these pos-
sible antecedents and indicates the certainty degree
on this selection (standby, certain or uncertain).
There are some exceptional cases that the tool al-
ways offers:
? cases of cataphora,
? possible syntactic mistakes (that will be used to
review and to correct the syntactic annotation),
? the possibility of a non-located antecedent,
? the possibility that an antecedent doesn?t ap-
pear explicitly in the text,
? the possibility of non-anaphora, that is, the sys-
tem has not correctly located an anaphoric ex-
pression.
In the semiautomatic way, the tool solves each
coreference by means of the enriched resolution
anaphora method previously explained. So the sys-
tem proposes and shows the most suitable candidate
to the annotator. The annotator can choose the solu-
tion that the resolution method offers in all cases, or
choose another solution (manually).
3LB-RAT has been developed in Python lan-
guage, which guarantees the portability to any Win-
dows or Unix platform. It deals with XML files:
it is designed to work and to understand the format
used by the 3LB-SAT tool, but it is able to accept
any other XML specification.
As we said before, the tool uses syntactic, mor-
phologic and semantic information for the specifi-
cation of an anaphora and its antecedent. The se-
mantic information used by the tool is limited to
ontology concepts and synonymous. From the se-
mantically annotated text, three tables are created,
one for each syntactic function: subject, direct ob-
ject and indirect object. In these tables the appear-
ance frequency of nouns with verbs (with their cor-
rect senses) is stored. These tables are the base to
construct the semantic compatibility patterns, which
indicate the compatibility between the ontological
concept related with the possible antecedent and the
verb of the sentence where the anaphoric expression
appears. In order to calculate this information, the
occurrence frequency and the conceptual generality
degree in the ontology are considered. In this case,
a higher punctuation is given to the most concrete
concepts. For example, ?Human? concept gives us
further information than ?Natural? concept. These
patterns are used in the semantic preferences appli-
cation. For a specific candidate, its semantic com-
patibility is calculated from the compatible ontolog-
ical concepts on the patterns. The candidates with
greater compatibility are preferred.
When the annotator selects a XML file to open,
the possible anaphoric elements of the text and their
candidates are located, and each anaphora is solved.
The system shows two lists (Figure 2): the lower
list shows each anaphora located and its solution.
When the annotator selects one of these elements,
in the upper box appears the possible candidates list
besides the solution suggested by the system. At the
same time, in the plain text, the anaphora and the
selected candidates are shown with different colors.
The annotator can choose any suggested option and
the certainty degree of this election, or accept the
solution given by the system.
Figure 2: 3LB-RAT anaphoric annotation tool.
6 Conclusions
The main contribution of this paper is the applica-
tion of semantic information to a manual anaphora
annotation process, based on the semantic relation
between the anaphoric element and its antecedent at
discourse level.
The semantic and anaphoric annotation scheme
of the Spanish corpus Cast3LB has been presented,
and how anaphoric annotation has been improved
with the semantic information annotated in previous
steps. The annotation process is based on the help
of an anaphora resolution system: first, the system
detects the anaphora and its antecedent, and then the
human annotator checks the correctness of the auto-
matic annotation process and solves possible mis-
takes. The system uses all the linguistic informa-
tion previously annotated in the corpus, including
the semantic information, in order to evaluate the
semantic compatibility between the antecedent and
the verb of the sentence in which the anaphora ap-
pears.
Acknowledgements
The authors would like to thank Bele?n and Raquel
for their work in the manual annotation process.
References
M. Civit, Ma. A. Mart??, B. Navarro, N. Buf??,
B. Ferna?ndez, and R. Marcos. 2003. Issues in the
Syntactic Annotation of Cast3LB. In 4th Inter-
national Workshop on Linguistically Interpreted
Corpora (LINC03), EACL03, Budapest.
M. Civit. 2003. Criterios de etiquetacio?n y desam-
biguacio?n morfosinta?ctica de corpus en Espan?ol.
Sociedad Espaola para el Procesamiento del
Lenguaje Natural, Alicante.
R. Gaizauskas and K. Humphreys. 1996. Quantita-
tive evaluation of coreference algorithms in an in-
formation extraction system. In S. P. Botley and
A. M. McEnery, editors, Corpus-based and Com-
putational Approaches to Discourse Anaphora,
pages 143?167. John Benjamins, Amsterdam.
L. Hirschman. 1997. MUC-7 coreference task def-
inition Message Understanding Conference Pro-
ceedings.
A. Kilgarriff and M. Palmer. 2000. Computer and
the Humanities. Special Issue on SENSEVAL,
volume 34.
A. Kilgarriff. 1998. Gold standard datasets for
evaluating word sense disambiguation programs.
Computer Speech and Language. Special Use on
Evaluation, 12(4):453?472.
J. Lyons. 1977. Semantics. Cambridge University
Press, London.
G. A. Miller. 1990. Wordnet: An on-line lexical
database. Intenational Journal of Lexicography,
3(4):235?312.
R. Mitkov, R. Evans, C. Orasan, C. Barbu,
L. Jones, and V.Sotirova. 2002. Coreference
and anaphora: developing annotating tools, an-
notated resources and annotation strategies. In
Proceedings of the Discourse, Anaphora and Ref-
erence Resolution Conference (DAARC 2000),
Lancaster, UK.
R. Mitkov. 2002. Anaphora resolution. Pearson,
London.
B. Navarro, M. Civit, Ma. A. Mart??, B. Ferna?ndez,
and R. Marcos. 2003. Syntactic, semantic and
pragmatic annotation in Cast3LB. In Proceed-
ings of the Shallow Processing of Large Corpora.
A Corpus Linguistics WorkShop, Lancaster, UK.
H. T. Ng and H. B. Lee. 1996. Integrating Mul-
tiple Knowledge Sources to Disambiguate Word
Sense: An Exemplar-Based Approach. In Pro-
ceedings of the 34th Annual Meeting of the As-
sociation for Computational Linguistics, Santa
Cruz, California.
M. Saiz-Noeda. 2002. Influencia y aplicacio?n de
papeles sinta?cticos e informacio?n sema?ntica en la
resolucio?n de la ana?fora pronominal en espan?ol.
Ph.D. thesis, Universidad de Alicante, Alicante.
N. Sebastia?n, Ma. A. Mart??, M. F. Carreiras, and
F. Cuetos. 2000. 2000 LEXESP: Le?xico Informa-
tizado del Espan?ol. Edicions de la Universitat de
Barcelona, Barcelona.
P. Vossen. 1998. A Multilingual Database with Lex-
ical Networks. Kluwer Academic Publisher.
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 334?337,
Prague, June 2007. c?2007 Association for Computational Linguistics
UA-ZBSA: A Headline Emotion Classification through Web Information
Zornitsa Kozareva, Borja Navarro, Sonia Va?zquez, Andre?s Montoyo
DLSI, University of Alicante
Carretera de San Vicente S/N
Alicante, Spain
03080
zkozareva,borja,svazquez,montoyo@dlsi.ua.es
Abstract
This paper presents a headline emotion clas-
sification approach based on frequency and
co-occurrence information collected from
the World Wide Web. The content words of
a headline (nouns, verbs, adverbs and adjec-
tives) are extracted in order to form different
bag of word pairs with the joy, disgust, fear,
anger, sadness and surprise emotions. For
each pair, we compute the Mutual Informa-
tion Score which is obtained from the web
occurrences of an emotion and the content
words. Our approach is based on the hypoth-
esis that group of words which co-occur to-
gether across many documents with a given
emotion are highly probable to express the
same emotion.
1 Introduction
The subjective analysis of a text is becoming impor-
tant for many Natural Language Processing (NLP)
applications such as Question Answering, Informa-
tion Extraction, Text Categorization among others
(Shanahan et al, 2006). The resolution of this prob-
lem can lead to a complete, realistic and coher-
ent analysis of the natural language, therefore ma-
jor attention is drawn to the opinion, sentiment and
emotion analysis, and to the identification of be-
liefs, thoughts, feelings and judgments (Quirk et al,
1985), (Wilson and Wiebe, 2005).
The aim of the Affective Text task is to clas-
sify a set of news headlines into six types of emo-
tions: ?anger?, ?disgust?, ?fear?, ?joy?, ?sadness?
and ?surprise?. In order to be able to conduct
such multi-category analysis, we believe that first
we need a comprehensive theory of what a human
emotion is, and then we need to understand how the
emotion is expressed and transmitted within the nat-
ural language. These aspects rise the need of syn-
tactic, semantic, textual and pragmatic analysis of
a text (Polanyi and Zaenen, 2006). However, some
of the major drawbacks in this field are related to
the manual or automatic acquisition of subjective ex-
pressions, as well as to the lack of resources in terms
of coverage.
For this reason, our current emotion classification
approach is based on frequency and co-occurrence
bag of word counts collected from the World Wide
Web. Our hypothesis is that words which tend to co-
occur across many documents with a given emotion
are highly probable to express this emotion.
The rest of the paper is organized as follows. In
Section 2 we review some of the related work, in
Section 3 we describe our web-based emotion classi-
fication approach for which we show a walk-through
example in Section 4. A discussion of the obtained
results can be found in Section 5 and finally we con-
clude in Section 6.
2 Related work
Our approach for emotion classification is based on
the idea of (Hatzivassiloglou and McKeown, 1997)
and is similar to those of (Turney, 2002) and (Tur-
ney and Littman, 2003). According to Hatzivas-
siloglou and McKeown (1997), adjectives with the
same polarity tended to appear together. For exam-
ple the negative adjectives ?corrupt and brutal? co-
334
occur very often.
The idea of tracing polarity through adjective co-
occurrence is adopted by Turney (2002) for the bi-
nary (positive and negative) classification of text re-
views. They take two adjectives, for instance ?ex-
cellent? and ?poor? in a way that the first adjective
expresses positive meaning, meanwhile the second
one expresses negative. Then, they extract all ad-
jectives from the review text and combine them with
?excellent? and ?poor?. The co-occurrences of these
words are searched on the web, and then the Mutual
Information score for the two groups of adjectives
is measured. When the adjective of the review ap-
pear more often with ?excellent?, then the review is
classified as positive, and when the adjectives appear
more often with ?poor?, then the review is classified
as negative.
Following Hatzivassiloglou and McKeown (1997)
and Turney (2002), we decided to observe how often
the words from the headline co-occur with each one
of the six emotions. This study helped us deduce
information according to which ?birthday? appears
more often with ?joy?, while ?war? appears more
often with ?fear?.
Some of the differences between our approach
and those of Turney (2002) are mentioned below:
? objectives: Turney (2002) aims at binary text
classification, while our objective is six class
classification of one-liner headlines. Moreover,
we have to provide a score between 0 and 100
indicating the presence of an emotion, and not
simply to identify what the emotion in the text
is. Apart from the difficulty introduced by the
multi-category classification, we have to deal
with a small number of content words while
Turney works with large list of adjectives.
? word class: Turney (2002) measures polarity
using only adjectives, however in our approach
we consider the noun, the verb, the adverb and
the adjective content words. The motivation
of our study comes from (Polanyi and Zaenen,
2006), according to which each content word
can express sentiment and emotion. In addition
to this issue we saw that most of the headlines
contain only nouns and verbs, because they ex-
press objectivity.
? search engines: Turney (2002) uses the Al-
tavista web browser, while we consider and
combine the frequency information acquired
from three web search engines.
? word proximity: For the web searches, Tur-
ney (2002) uses the NEAR operator and con-
siders only those documents that contain the
adjectives within a specific proximity. In our
approach, as far as the majority of the query
words appear in the documents, the frequency
count is considered.
? queries: The queries of Turney (2002) are made
up of a pair of adjectives, and in our approach
the query contains the content words of the
headline and an emotion.
There are other emotion classification approaches
that use the web as a source of information. For
instance, (Taboada et al, 2006) extracted from the
web co-occurrences of adverbs, adjectives, nouns
and verbs. Gamon and Aue (2005) were looking
for adjectives that did not co-occur at sentence level.
(Baroni and Vegnaduzzo, 2004) and (Grefenstette
et al, 2004) gathered subjective adjectives from the
web calculating the Mutual Information score.
Other important works on sentiment analysis are
those of (Wilson et al, 2005) and (Wiebe et al,
2005; Wilson and Wiebe, 2005), who used linguistic
information such as syntax and negations to deter-
mine polarity. Kim and Hovy (2006) integrated verb
information from FrameNet and incorporated it into
semantic role labeling.
3 Web co-occurrences
In order to determine the emotions of a
headline, we measure the Pointwise Mu-
tual Information (MI) of ei and cwj as
MI(ei, cwj) = log2 hits(ei,cwj)hits(ei)hits(cwj) , where ei ?
{anger, disgust, fear, joy, sadness, surprise}
and cwj are the content words of the headline j.
For each headline, we have six MI scores which
indicate the presence of the emotion. MI is used
in our experiments because it provides information
about the independence of an emotion and a bag of
words.
To collect the frequency and co-occurrence counts
of the headline words, we need large and massive
335
data repositories. To surmount the data sparsity
problem, we used as corpus the World Wide Web
which is constantly growing and daily updated.
Our statistical information is collected from three
web search engines: MyWay1, AlltheWeb2 and Ya-
hoo3. It is interesting to note that the emotion dis-
tribution provided by each one of the search engines
for the same headline has different scores. For this
reason, we decided to compute an intermediate MI
score as aMI =
?n
s=1 MI(ei,cwj)
s .
In the trail data, besides the MI score of an emo-
tion and all headline content words, we have calcu-
lated the MI for an emotion and each one of the con-
tent words. This allowed us to determine the most
sentiment oriented word in the headline and then we
use this predominant emotion to weight the associ-
ation sentiment score for the whole text. Unfortu-
nately, we could not provide results for the test data
set, due to the high number of emotion-content word
pairs and the increment in processing time and re-
turned responses of the search engines.
4 Example for Emotion Classification
As a walk through example, we use the Mortar as-
sault leaves at least 18 dead headline which is taken
from the trial data. The first step in our emotion clas-
sification approach consists in the determination of
the part-of-speech tags for the one-liner. The non-
content words are stripped away, and the rest of the
words are taken for web queries. To calculate the MI
score of a headline, we query the three search en-
gines combining ?mortar, assault, leave, dead? with
the anger, joy, disgust, fear, sadness and surprise
emotions. The obtained results are normalized in a
range from 0 to 100 and are shown in Table 1.
MyWay AllWeb Yahoo Av. G.Stand.
anger 19 22 24 22 22
disgust 5 6 7 6 2
fear 44 50 53 49 60
joy 15 19 20 18 0
sadness 28 36 36 33 64
surprise 4 5 6 5 0
Table 1: Performance of the web-based emotion
classification for a trail data headline
1www.myway.com
2www.alltheweb.com
3www.yahoo.com
As can be seen from the table, the three search
engines provide different sentiment distribution for
the same headline, therefore in our final experiment
we decided to calculate intermediate MI. Comparing
our results to those of the gold standard, we can say
that our approach detects significantly well the fear,
sadness and angry emotions.
5 Results and Discussion
Table 2 shows the obtained results for the affective
test data. The low performance of our approach
is explainable by the minimal knowledge we have
used. An interesting conclusion deduced from the
trail and test emotion data is that the system detects
better the negative feelings such as anger, disgust,
fear and sadness, in comparison to the positive emo-
tions such as joy and surprise. This makes us believe
that according to the web most of the word-emotion
combinations we queried are related to the expres-
sion of negative emotions.
UA-ZBSA Fine-grained Coarse-grained
Pearson Acc. P. R.
Anger 23.20 86.40 12.74 21.66
Disgust 16.21 97.30 0.00 0.00
Fear 23.15 75.30 16.23 26.27
Joy 2.35 81.80 40.00 2.22
Sadness 12.28 88.90 25.00 0.91
Surprise 7.75 84.60 13.70 16.56
Table 2: Performance of the web-based emotion
classification for the whole test data set
In the test run, we could not apply the emotion-
word weighting, however we believe that it has
a significant impact over the final performance.
Presently, we were looking for the distribution of all
content words and the emotions, but in the future we
would like to transform all words into adjectives and
then conduct web queries.
Furthermore, we would like to combine the re-
sults from the web emotion classification with the
polarity information given by SentiWordNet4. A-
priory we want to disambiguate the headline content
words and to determine the polarities of the words
and their corresponding senses. For instance, the ad-
jective ?new? has eleven senses, where new#a#3 and
new#a#5 express negativism, new#a#4 and new#a#9
positivism and the rest of the senses are objective.
4http://sentiwordnet.isti.cnr.it/
336
So far we did not consider the impact of valence
shifter (Polanyi and Zaenen, 2006) and we were un-
able to detect that a negative adverb or adjective
transforms the emotion from positive into negative
and vice versa. We are also interested in studying
how to conduct queries not as a bag of words but
bind by syntactic relations (Wilson et al, 2005).
6 Conclusion
Emotion classification is a challenging and difficult
task in Natural Language Processing. For our first
attempt to detect the amount of angry, fear, sadness,
surprise, disgust and joy emotions, we have pre-
sented a simple web co-occurrence approach. We
have combined the frequency count information of
three search engines and we have measured the Mu-
tual Information score between a bag of content
words and emotion.
According to the yielded results, the presented ap-
proach can determine whether one sentiment is pre-
dominant or not, and most of the correct sentiment
assignments correspond to the negative emotions.
However, we need to improve the approach in many
aspects and to incorporate more knowledge-rich re-
sources, as well as to tune the 0-100 emotion scale.
Acknowledgements
This research has been funded by QALLME number
FP6 IST-033860 and TEX-MESS number TIN2006-
15265-C06-01.
References
Marco Baroni and Stefano Vegnaduzzo. 2004. Identi-
fying subjective adjectives through web-based mutual
information. In Ernst Buchberger, editor, Proceedings
of KONVENS 2004, pages 17?24.
Michael Gamon and Anthony Aue. 2005. Automatic
identification of sentiment vocabulary: exploiting low
association with known sentiment terms. In Proceed-
ings of the Workshop on Feature Engineering for Ma-
chine Learning in Natural Language Processing (ACL
2005), pages 57?64.
Gregory Grefenstette, Yan Qu, James G. Shanahana, and
David A. Evans. 2004. Coupling niche browsers and
affect analysis for an opinion mining application. In
Proceeding of RIAO-04.
Vasileios Hatzivassiloglou and Kathleen R. McKeown.
1997. Predicting the semantic orientation of adjec-
tives. In Proceedings of the eighth conference on Eu-
ropean chapter of the Association for Computational
Linguistics (EACL).
Soo-Min Kim and Eduard Hovy. 2006. Extracting opin-
ions, opinion holders, and topics expressed in online
news media text. In Proceedings of the Workshop on
Sentiment and Subjectivity in Text, pages 1?8.
Livia Polanyi and Annie Zaenen. 2006. Contextual va-
lence shifter. In James G. Shanahan, Yan Qu, and
Janyce Wiebe, editors, Computing Attitude and Affect
in Text: Theory and Applications, chapter 1, pages 1?
10. Springer.
R. Quirk, S. Greenbaum, G. Leech, and J. Svartvik. 1985.
A Comprehensive Grammar of the English Language.
Longman.
James G. Shanahan, Yan Qu, and Janyce Wiebe. 2006.
Computing Attitude and Affect in Text: Theory and Ap-
plications. Springer.
Maite Taboada, Caroline Anthony, and Kimberly Voll.
2006. Methods for creating semantic orientation
databases. In Proceeding of LREC-06, the 5th Interna-
tional Conference on Language Resources and Evalu-
ation, pages 427?432.
Peter D. Turney and Michael L. Littman. 2003. Measur-
ing praise and criticism: Inference of semantic orien-
tation from association. ACM Transactions on Infor-
mation Systems, 21(4):315?346.
Peter D. Turney. 2002. Thumbs up or thumbs down?
semantic orientation applied to unsupervised classifi-
cation of reviews. In Proceedings of the 40th Annual
Meeting of the Association for Computational Linguis-
tics (ACL), pages 417?424.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.
Annotating expressions of opinions and emotions in
language. Language Resources and Evaluation (for-
merly Computers and the Humanities), 39(2-3):165?
210.
Theresa Wilson and Janyce Wiebe. 2005. Annotating
attributions and private states. In Ann Arbor, editor,
Proceedings of the Workshop on Frontiers in Corpus
Annotation II: Pie in the Sky, pages 53?60.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In Proceedings of the conference
on Human Language Technology and Empirical Meth-
ods in Natural Language Processing, pages 347?354.
337
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 725?733,
Beijing, August 2010
TimeML Events Recognition and Classification:
Learning CRF Models with Semantic Roles
Hector Llorens, Estela Saquete, Borja Navarro-Colorado
Natural Language Processing Group
University of Alicante
{hllorens,stela,borja}@dlsi.ua.es
Abstract
This paper analyzes the contribution of se-
mantic roles to TimeML event recognition
and classification. For that purpose, an
approach using conditional random fields
with a variety of morphosyntactic features
plus semantic roles features is developed
and evaluated. Our system achieves an
F1 of 81.4% in recognition and a 64.2%
in classification. We demonstrate that the
application of semantic roles improves the
performance of the presented system, es-
pecially for nominal events.
1 Introduction
Event recognition and classification has been
pointed out to be very important to improve com-
plex natural language processing (NLP) applica-
tions such as automatic summarization (Daniel et
al., 2003) and question answering (QA) (Puste-
jovsky, 2002). Natural language (NL) texts often
describe sequences of events in a time line. In the
context of summarization, extracting such events
may aid in obtaining better summaries when these
have to be focused on specific happenings. In
the same manner, the access to such information
is crucial for QA systems attempting to address
questions about events.
The analysis of events as well as the classifica-
tion of the different forms they adopt in NL text is
not a new issue (Vendler, 1967). It relates not only
to linguistics but different scientific areas such as
philosophy, psychology, etc.
In NLP, different definitions of event can be
found regarding the target application.
On the one hand, in topic detection and track-
ing (Allan, 2002), event is defined as an instance
of a topic identified at document level describing
something that happen (e.g., ?wars?). The aim of
this task is to cluster documents on the same topic,
that is to say, the same event.
On the other hand, information extraction (IE)
provides finer granularity event definitions. IE
proposes standard schemes to annotate the indi-
vidual events within the scope of a document.
STAG scheme (2000) was aimed to identify events
in news and their relationship with points in a tem-
poral line. More recently, TimeML (Pustejovsky
et al, 2003a) presented a rich specification for an-
notating events in NL text extending the features
of the previous one.
This paper is focused on the TimeML view of
events. TimeML defines events as situations that
happen or occur, or elements describing states
or circumstances in which something obtains or
holds the truth. These events are generally ex-
pressed by tensed or untensed verbs, nominaliza-
tions, adjectives, predicative clauses or preposi-
tional phrases. TimeML guidelines define seven
classes of events:
? Reporting. Action of a person or organization declar-
ing or narrating an event (e.g., ?say?)
? Perception. Physical perception of another event (e.g.,
?see?, ?hear?)
? Aspectual. Aspectual predication of another event
(e.g., ?start?, ?continue?)
? I Action. Intensional action (e.g., ?try?)
? I State. Intensional state (e.g., ?feel?, ?hope?)
? State. Circumstance in which something holds the
truth (e.g., ?war?, ?in danger?)
? Occurrence. Events that describe things that happen
(e.g., ?erupt?, ?arrive?)
The following sentence shows an example of an
occurrence event and a state event.
It?s <EVENT class="OCCURRENCE">turning</EVENT>
out to be another <EVENT class="STATE">bad</EVENT>
financial week.
725
The automatic annotation of events has been
addressed with different data-driven approaches.
Current approaches are mainly based on mor-
phosyntactic information. Our hypothesis is that
semantic roles, as higher language level analysis
information, may be useful as additional feature
to improve the performance of such approaches.
Within this setting, the main objective of this
paper is to analyze (1) the contribution of seman-
tic roles, as additional feature, and (2) the influ-
ence of conditional random fields (CRFs), as ma-
chine learning (ML) technique, in the events auto-
matic recognition and classification task.
This paper is structured as follows. Firstly,
related work in the task is reviewed in Section
2. The next section provides a detailed descrip-
tion of our proposal to address event recognition
and classification. After that, Section 4 includes
an evaluation of the proposal, and a comparative
analysis of the results. Finally, conclusions are
drawn in Section 5.
2 Related Work
There is only one corpus available annotated with
TimeML events: TimeBank (Pustejovsky et al,
2003b). Hence, all the approaches regarding
TimeML events extraction have been evaluated
using this corpus.
EVITA system (Saur?? et al, 2005) recognizes
events by combining linguistic and statistical tech-
niques. The main features used to manually
encode event recognition rules are the follow-
ing: part-of-speech (PoS) tagging, lemmatizing,
chunking, lexical lookup and contextual pars-
ing. Furthermore, WordNet information com-
bined with Bayesian learned disambiguation was
used to identify nominal events. EVITA obtained
74.03% precision, 87.31% recall, and 80.12%
F?=1 in event recognition over TimeBank.
Boguraev and Ando (2005) present an evalu-
ation on automatic TimeML events annotation.
They set out the task as a classification prob-
lem and used a robust risk minimization (RRM)
classifier to solve it. The F?=1 results obtained
by a 5-fold cross validation over TimeBank were
78.6% for recognition and 61.3% for classifica-
tion. Moreover, they evaluated the impact of ap-
plying word-profiling techniques over their ap-
proach to exploit unannotated data. Using this ad-
ditional information, the F?=1 results improved to
80.3% and 64.0%. In this evaluation, neither pre-
cision nor recall were given.
STEP (Bethard and Martin, 2006) is a system
for TimeML event recognition and classification.
This approach uses a rich set of textual, morpho-
logical, dependency andWordNet hypernymy fea-
tures to build a Support Vector Machine (SVM)
model. The model was trained using 9/10 of the
TimeBank. The test, carried out using the remain-
ing 1/10 of the corpus, obtained a 82.0% preci-
sion, 70.6% recall and 75.9% F?=1 for recognition
and a 66.7% precision, 51.2% recall and 57.9%
F?=1 for classification.
Finally, March and Baldwin (2008) present an
evaluation on event recognition using a multi-
class classifier (BSVM). The main features used
to train the classifier are word and PoS context
window, stop words removal and feature general-
ization through words grouping (numbers, named
entities, etc.). The result for the best feature com-
bination in a 10-fold cross validation over Time-
Bank was 76.4% F?=1.
It is worth mentioning that there are two ver-
sions of the TimeBank corpus, 1.1 and 1.2. The
latest version is the current gold standard. Both
versions consist of the same documents1, mainly
news articles and transcribed broadcast news from
different domains. EVITA is the only reference
which used TimeBank 1.2 while the rest of re-
viewed references used TimeBank 1.1.
3 Our proposal: semantic roles
enhancing a CRF model
In this section, the motivation for our proposal,
and our specific approach are presented.
3.1 Motivation
The next two subsections describe the main
feature (semantic roles) and the ML algorithm
(CRFs) we selected to address event recognition
and classification; and the reasons why we think
they could be useful in that task.
1Except 3 documents removed in TimeBank 1.2
726
3.1.1 Semantic roles
Semantic role labeling (SRL) has achieved im-
portant results in the last years (Gildea and Juraf-
sky, 2002). For each predicate in a sentence, se-
mantic roles identify all constituents, determining
their arguments (agent, patient, etc.) and their ad-
juncts (locative, temporal, etc.). Currently, there
exist different role sets aimed to cover opposed re-
quirements. They range from more specific, such
as FrameNet (Baker et al, 1998), to more general
like PropBank (Palmer et al, 2005). Figure 1 il-
lustrates a semantic role labeled sentence.
Figure 1: Semantic roles example
Many research efforts into the application of se-
mantic roles demonstrated that this information is
useful for different NLP purposes (Melli et al,
2006). Focusing on TimeML, semantic roles have
been applied to temporal expressions recognition
(Llorens et al, 2009), and temporal links classi-
fication (Hage`ge and Tannier, 2007). However,
they have not been used to recognize and classify
TimeML events.
Semantic roles provide structural relations of
the predicates in which events may participate.
Beyond syntactic relations expressed by means of
the different types of phrases, semantic roles give
further information about semantic relations be-
tween the arguments of a predicate. Therefore,
as richer information, roles may better distinguish
tokens to be candidate events. In addition, differ-
ent semantic role settings may represent specific
event classes.
Example 1 shows four sentences annotated with
PropBank semantic roles (in square brackets) in
which the noun ?control? participates. In the sen-
tences 1 and 2, ?control? does not represent an
event, while in the sentences 3 and 4, it repre-
sents an state event. It can be seen that the noun
?control?, when it is contained by A1 role it may
represent an event. However, it is not an event
when contained by A0 or AM-MNR roles. The
analysis may also take into account the governing
verb. In the example, we could specify that ?con-
trol? represents an event when contained by A1
role of ?seek? and ?obtain? verbs; and the oppo-
site for the A0 role of ?emerge? and the AM-MNR
of ?had?.
(1) 1. ?[Control procedures A0] will emerge?
2. ?[Iraq A0] had [thousands of Americans A1] [under
its control AM-MNR]?
3. ?[Crane Co. A0] may obtain [control of Milton Roy
Corp. A1]?
4. ?[Pattison?s A0] decided to seek [control A1]?
Our hypothesis is that semantic roles, as ad-
ditional information, may help in the recogni-
tion and classification of events. The information
about the role of a token and the verb it depends
on, or the set of roles of the sentence, could be
useful for determining whether a token or a se-
quence of tokens is an event or not. Due to the
fact that roles represent high level information in
NL text, they are more independent from word to-
kens. Hence, roles may aid in learning more gen-
eral models that could improve the results of ap-
proaches focused on lower level information.
3.1.2 CRF probabilistic model
Conditional Random Fields is a popular and ef-
ficient ML technique for supervised sequence la-
beling (Lafferty et al, 2001). CRFs are undirected
graphical models, a special case of conditionally-
trained finite state machines. A key advantage of
CRFs is their flexibility to include a wide variety
of arbitrary, non-independent features of the input.
We see the task set out in this paper as a se-
quence labeling problem. Assume X is a random
variable over data sequences to be labeled, and Y
is a random variable over the corresponding label
sequences (hidden), being all Y components (Yi)
members of a finite label alphabet ?. X might
range over NL sentences and Y range over event
annotations of those sentences, with ? the set of
possible event IOB22 labels. The following ex-
ample illustrates the event recognition problem.
(2) X Y
was ?
another ? B-EVENT
bad ? ? = I-EVENT
week ? O
2IOB2 format: (B)egin, (I)nside, and (O)utside
727
The variables X and Y are jointly distributed
over both label and observation sequences. How-
ever, unlike Hidden Markov Models (generative)
in which p(X,Y ), CRFs (discriminative) con-
struct a conditional model from paired observa-
tion and label sequences: p(Y |X). Graphically,
CRFs are represented by undirected graphs, G =
(V,E) such that Y = (Yv), v  V , so that Y is
indexed by the vertices of G. Then (X,Y ) is a
conditional random field if Yv variables obey the
Markov property with respect to the graph when
conditioned on X:
P (Yv|X,Yw, v 6= w) = P (Yv|X,Yw, v ? w),
where v ? w means that Yv and Yw are connected
neighbors in G.
To extend the problem to event classification,
the alphabet ? must be extended with the event
classes (state, aspectual, etc.).
CRFs have been successfully applied to many
sequence labeling tasks (Sha and Pereira, 2003;
McCallum and Li, 2003).
From our point of view, the task addressed in
this paper is well suited for this ML technique.
Events may depend on structural properties of NL
sentences. Not only the word sequence, but mor-
phological, syntactic and semantic information is
related with the event structure (Tenny and Puste-
jovsky, 2000).
For example, sequences of verbs may represent
i action+occurrence or aspectual+occurrence
events (see Example 3).
(3) ?The president will <EVENT class="i action"> try
</EVENT> to <EVENT class="occurrence"> assist
</EVENT> to the <EVENT class="occurrence">
conference </EVENT>?
?Saddam will <EVENT class="aspectual"> begin
</EVENT> <EVENT class="occurrence"> withdrawing
</EVENT> troops from Iranian territory on Friday?
In addition, for instance, many state event in-
stances are represented by ?to be? plus a variable
quality (see Example 4).
(4) ?It is <EVENT class="occurrence"> turning
</EVENT> out to be another <EVENT class="state">
bad </EVENT> financial week.?
Given this analysis, our hypothesis is that CRFs
will be useful in the recognition of events in which
the sequential and structural properties are rele-
vant.
3.2 Approach description
This paper proposes CRFs as learning method
to infer an event recognition and classification
model. Our system includes CRF++ toolkit3 for
training and testing our approach. The learning
process was done using CRF-L2 algorithm and
hyper-parameter C=1.
The definition of the features is crucial for the
architecture of the system. The features used in
our approach are grouped in two feature sets. On
the one hand, general features, which comprise
morphosyntactic and ontological information. On
the other hand, semantic roles features, which are
the main focus of this paper.
The general features used to train our CRF
model are described regarding each language
analysis level.
? Morphological: The lemma and PoS con-
text, in a 5-window (-2,+2), was employed.
This basic linguistic feature showed good re-
sults in different NLP tasks, as well as in
event recognition and classification (March
and Baldwin, 2008). Tokenization, PoS and
lemmatization were obtained using TreeTag-
ger (Schmid, 1994).
? Syntactic: Different events are contained in
particular types of phrases and syntactic de-
pendencies. This feature tries to tackle this
by considering syntactic information. Char-
niak parser (Charniak and Johnson, 2005)
was used to obtain the syntactic tree.
? Lexical semantics: WordNet (Fellbaum,
1998) top ontology classes have been widely
used to represent word meaning at ontologi-
cal level, and demonstrated its worth in many
tasks. We obtained the four top classes for
each word.
The specific semantic roles features used to en-
hance the training framework of the CRF model
were developed considering PropBank role set.
PropBank was applied in our system due to the
high coverage it offers in contrast to FrameNet.
In order to get PropBank semantic roles, the CCG
3http://crfpp.sourceforge.net/
728
SRL tool (Punyakanok et al, 2004) was used for
labeling the corpus.
? Role: For each token, we considered the role
regarding the verb the token depends on. Se-
mantic roles information may be useful for
distinguish particular lemmas that are events
only when appearing under a precise role.
? Governing verb: The verb to which the cur-
rent token holds a particular role. This may
distinguish tokens appearing under the influ-
ence of different verbs.
? Role+verb combination: The previous two
features were combined to capture the rela-
tion between them. This introduces new clas-
sification information by distinguishing roles
depending on different verbs. The impor-
tance of this falls especially on the numbered
roles of PropBank (A0, A1, ...) holding dif-
ferent meanings when depending on different
verbs.
? Role configuration: This consists of the set
of roles depending on the verb the token de-
pends on. This may be particularly useful
for distinguish different sentence settings and
thus, whether a token denotes an event in a
particular sentence type.
The system consists of two main processes.
Firstly, given TimeML annotated text, it obtains
the defined features plus the IOB2 tags of the an-
notated events. Then, using this data the system
learns (trains) a model for event recognition and
a model for event classification. Secondly, given
plain text, it automatically gets the defined fea-
tures using the described tools. With this data,
the system applies the learned models to recog-
nize and classify TimeML events.
4 Evaluation
In this section, firstly, the corpus, criteria and mea-
sures are defined. Secondly, the results obtained
by our approach are presented. After that, the con-
tribution of our approach is measured through dif-
ferent experiments: (1) general contribution, (2)
semantic roles contribution, and (3) CRFs contri-
bution. And finally, our approach is compared to
the state of the art systems.
4.1 Corpus, criteria and measures
For the evaluation, the TimeBank 1.2 corpus
(7881 events) was used without modification. All
the results reported in this evaluation were ob-
tained using a 5-fold cross validation. The n-fold
train-test sets were built sorting the corpus files
alphabetically and then sequentially select each
set regarding the documents size. It is important
to highlight the latter because if the n-folds were
made regarding the number of documents, the sets
had not been homogeneous due to the differences
in TimeBank document sizes.
Only annotations matching the exact event span
were considered as correct in recognition and
classification, requiring also the class matching in
the second case.
The following measures were used to score the
evaluated approaches.
? Precision correct annotationstotal approach annotations
? Recall correct annotationtotal corpus annotations
? F?=1 2 ? precision ? recallprecision + recall
4.2 Our approach results
Table 1 shows the results obtained by our ap-
proach for both recognition and classification of
events. The last column (BF) indicates the best
F?=1 results obtained in the individual folds.
Precision Recall F?=1 BF
Recognition 83.43 79.54 81.40 82.43
Classification 68.84 60.15 64.20 69.68
Table 1: Our approach (CRF+Roles) results
The results show a high F?=1 score in both
recognition and classification, showing a good
balance between precision and recall. This indi-
cates that our approach is appropriate to address
this task.
Focusing on classification task, Table 2 shows
the detailed scores for each event class.
Looking at the specific class results, reporting
obtained the best results. This is due to the fact
that 80% of reporting events are represented by
lemmas ?say? and ?report? with PoS ?VBD? and
?VBZ?. Occurrence, perception, aspectual and
i state obtained classification results over 50%.
729
Class (instances) Precision Recall F?=1
Reporting (1021) 91.90 89.18 90.51
Perception (48) 65.93 66.83 66.37
Aspectual (258) 81.35 47.00 59.57
I Action (673) 51.40 29.30 37.32
I State (582) 68.44 43.70 53.34
State (1107) 50.01 24.84 33.19
Occurrence (4192) 66.73 72.07 69.29
Table 2: CRF+Roles 5-fold detailed results
Although perception and aspectual are quite re-
stricted to some lemmas, they obtained results be-
low reporting. This is due to the fact that Time-
Bank contains very few examples of these classes.
I action and state show poorer results. In the
case of the former, this is because some non-
intensional verbs (e.g., ?look?) appear in the cor-
pus as i action under certain conditions, for exam-
ple, when there is modality or these verbs appear
in conditional sentences. This suggests the neces-
sity of incorporating a word sense disambiguation
(WSD) technique. Our approach did not take into
account this information and thus the results are
lower for this event class. In the case of state, the
reasons for the low performance are the richness
of this event class by means of lemmas, PoS, and
phrases.
Finally, Table 3 shows the results of our ap-
proach by word class.
Precision Recall F?=1
Verb 91.56 92.15 91.33
Recognition Noun 72.67 48.26 58.42
Adj. 66.78 38.09 48.35
Verb 73.86 74.21 73.51
Classification Noun 62.73 41.33 49.53
Adj. 55.69 31.12 40.41
Table 3: CRF+Roles 5-fold word class results
It may be seen that the best results in both
recognition and classification are obtained in verb
events, followed by noun and adjective.
4.3 Contribution analysis
This subsection details the contribution of each as-
pect of our approach through three comparative
experiments.
First experiment: general contribution
This experiment measures the general contribu-
tion of our approach by comparing its results with
a baseline. TimeBank was analyzed to find a ba-
sic general rule to annotate events. The events are
mainly denoted by verbs, pertaining to occurrence
class. Hence, we propose a baseline that annotates
all verbs as occurrence events. Table 4 shows re-
sults obtained by this baseline for both recognition
and classification of events.
Prec. Recall F?=1
Our approach Recog. 83.43 79.54 81.40
Class. 68.84 60.15 64.20
Baseline Recog. 72.50 65.20 68.60
Class. 46.01 53.19 49.34
Table 4: Our approach vs Baseline results
Given the simplicity of the baseline, the results
obtained are quite high. However, our approach
F?=1 significantly improves baseline by 19% for
recognition and 30% for classification.
Second experiment: roles contribution
The main objective of this paper is to determine
the impact of semantic roles in this task. To quan-
tify it, a non-roles version of our approach was
evaluated. This version only uses the general fea-
tures described in section 3. Table 5 shows the
results obtained.
Precision Recall F?=1
Our approach Recog. 83.43 79.54 81.40
Class. 68.84 60.15 64.20
Non-roles Recog. 82.96 74.81 78.67
Class. 67.53 54.80 60.50
Table 5: Our approach vs Non-roles results
Comparing these results with the ones obtained
by our full featured approach, the application
of roles improved especially the recall. Specifi-
cally, recall improved by 6% and 10% for recog-
nition and classification respectively. The main
improvement was achieved by state and occur-
rence classes (60% of the total improvement), es-
pecially, nominal events of that classes that con-
centrate around the 70% of the total contribution.
To illustrate corpus examples that have been
improved by roles, Example 5 shows two sen-
tences containing state events that were correctly
tagged by the roles approach and missed by the
730
non-roles. In the examples, the TimeML events
annotation and below the semantic roles annota-
tion is reported.
(5) ?There are still few buyers and the mood is <EVENT
class=STATE>gloomy</EVENT>?
?[There A0] are [still AM-TMP] [few buyers A1] and [the
mood A0] is [gloomy AM-MNR]?
?Security is now <EVENT>better</EVENT>?
?[Security A0] is [now AM-TMP] [better AM-MNR]?
In these cases, AM-MNR role information lead to
a correct state event recognition.
Third experiment: CRFs contribution
In order to measure the CRFs contribution to this
task, an extra experiment was carried out. This
consisted of comparing, under the same setting,
CRFs with a popular learning technique: support
vector machines (SVM). As in Bethard and Mar-
tin (2006), YamCha4 software was used (parame-
ters: C=1 and polynomial degree=2).
Table 6 shows the results obtained by the SVM-
based approach in recognition and Table 7 reports
the improvement (CRFs over SVM) distribution
in the different word classes.
Precision Recall F?=1
Our approach (CRF) 83.43 79.54 81.40
SVM 80.00 75.10 77.40
Table 6: Our approach (CRF) vs SVM results
Verb Noun Adj. Adv. Prep.
General 22% 71% 5% 1% 1%
Table 7: CRF improvement distribution among
the word classes
These results verify that CRF improves SVM
F?=1 by 5% in this task. Furthermore, especially
noun events take advantage of using CRF.
Finally, Figure 2 illustrates the results of our ap-
proach over the described experiments.
4.4 Comparison with the state of the art
Most systems found in the literature are data-
driven approaches using morphosyntactic fea-
tures. SVM based approaches (Bethard and Mar-
tin, 2006; March and Baldwin, 2008) achieved,
4http://chasen.org/?taku/software/YamCha/
Figure 2: F?=1 Results
approximately, 76% and 58% F?=1 in event
recognition and classification respectively. Bogu-
raev and Ando (2005) used a robust risk mini-
mization classifier to address this task and ob-
tained 78.6% and 61% (without exploiting unan-
notated data). These results are very similar to the
ones obtained by our non-roles approach. This
suggests that using, apart from morphosyntactic
features, additional features based on semantic
roles could improve the approaches.
EVITA system (Saur?? et al, 2005) combines
linguistic and statistical techniques. On the one
hand, it consists of a set of manually encoded rules
based on morphosyntactic information. On the
other hand, it includes a Bayesian learned disam-
biguation module to identify nominal events. The
later was trained and tested using the whole cor-
pus, therefore, the results could be inflated by this
fact. For that reason, Bethard and Martin (2006)
presented an EVITA implementation (Sim-Evita)
to compare the results. Sim-Evita obtains an 73%
and 51% F?=1 in event recognition and classifica-
tion respectively. These results suggest that data-
driven improve rule-based approaches.
Only STEP evaluation showed detailed classifi-
cation results. We agree that state events are the
most complex and heterogeneous ones. Focus-
ing on such events, our F?=1 results (33%) im-
prove Bethard?s (25%) by 32%. Regarding the
results obtained for each word class. Bethard?s
results presented good performance on classify-
ing verb events (71%), but lower results in noun
events (34%). Our approach results for noun
events (49%) improve theirs by 44%. This sug-
gests that the application of semantic roles en-
ables our approach on making more general pre-
dictions. In this manner, our system may recog-
731
nize unseen nominal event instances as long as
they share, with the seen instances, some semantic
roles features.
5 Conclusions and Further Work
This paper presented an approach for the recogni-
tion and classification of TimeML events consist-
ing of a CRF model learned using semantic roles
as main feature. In addition to morphosyntactic
features, the model was enhanced including ex-
tra semantic information, semantic role labeling,
used for other applications with satisfactory re-
sults, but never employed before for this purpose.
Our proposal was evaluated using the gold stan-
dard corpus, TimeBank 1.2, and the results ob-
tained were analyzed and compared to measure
the impact of both semantic roles and CRFs in the
described task.
The obtained F?=1 results demonstrated that
semantic roles are useful to recognize (81.43%)
and classify (64.20%) TimeML events, improv-
ing the presented baseline by 19% for recogni-
tion and 30% for classification. Specifically, Se-
mantic roles employed as additional feature im-
proved the recall of the non-roles version by 6%
and 10% for recognition and classification respec-
tively. This indicates that roles features led to
more general models capable of better annotat-
ing unseen instances. The roles contribution was
more significant in state and occurrence classes of
noun events, concentrating around the 70% of the
improvement.
Furthermore, it was verified that CRFs achieve
higher results than models learned using other
ML techniques such as SVM (5% improvement),
contributing especially to nominal events. This
demonstrated that CRF models are appropriate to
face the task.
Finally, to the extent our results are compara-
ble to state of the art evaluations, ours outper-
form the F?=1 scores in both recognition and clas-
sification. Especially, our approach showed bet-
ter performance than related works in state (32%
improvement) and nominal events (44% improve-
ment). Hence, the extension of the current ap-
proaches with semantic roles features could bene-
fit their performance.
The main difficulties found in the task ad-
dressed in this paper are related to i action and
state events. In the former, we detected that
modality and the word senses are important and
must be treated to distinguish such events. In
the later, although they were improved by our
approach, state events are still the most com-
plex class of events due to their richness in con-
trast to the reduced size of the training data. We
agree with related literature that event classifi-
cation results are still below other tasks perfor-
mance, which indicates that this task is inherently
complex and more training data may lead to sig-
nificant improvements.
As further work we propose, firstly, improv-
ing the i action results by taking into account the
modality considering the AM-MOD role, and the
word senses using a WSD technique. Secondly,
the application of FrameNet role set (finer granu-
larity) to determine which kind of roles are better
to improve the current event annotation systems.
Acknowledgments
This paper has been supported by the Spanish
Government, projects TIN-2006-15265-C06-01, TIN-
2009-13391-C04-01 and PROMETEO/2009/119, where
Hector Llorens is funded (BES-2007-16256).
References
Allan, James. 2002. Topic Detection and Tracking:
Event-Based Information Organization. Kluwer
Academic Publishers, Norwell, MA, USA.
Baker, Collin F., Charles J. Fillmore, and John B.
Lowe. 1998. The Berkeley FrameNet Project. In
COLING-ACL, pages 86?90.
Bethard, Steven and James H. Martin. 2006. Identi-
fication of event mentions and their semantic class.
In EMNLP: Proceedings of the Conference on Em-
pirical Methods in NLP, pages 146?154. ACL.
Boguraev, Branimir and Rie Kubota Ando. 2005. Ef-
fective Use of TimeBank for TimeML Analysis. In
Annotating, Extracting and Reasoning about Time
and Events 05151.
Charniak, Eugene and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative
reranking. In 43rd Annual Meeting of the ACL.
Daniel, Naomi, Dragomir Radev, and Timothy Allison.
2003. Sub-event based multi-document summariza-
732
tion. In HLT-NAACL Text summarization workshop,
pages 9?16. ACL.
Fellbaum, Christiane. 1998. WordNet: An Electronic
Lexical Database (Language, Speech, and Commu-
nication). MIT Press.
Gildea, Daniel and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational Linguis-
tics, 28(3):245?288.
Hage`ge, Caroline and Xavier Tannier. 2007. XRCE-
T: XIP temporal module for TempEval campaign.
In TempEval (SemEval), pages 492?495, Prague,
Czech Republic. ACL.
Lafferty, John D., Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling
sequence data. In Proceedings of the 18th ICML,
pages 282?289. Morgan Kaufmann.
Llorens, Hector, Borja Navarro, and Estela Saquete.
2009. Using Semantic Networks to Identify Tem-
poral Expressions from Semantic Roles. In VI
RANLP, pages 219?224.
March, Olivia and Timothy Baldwin. 2008. Auto-
matic event reference identification. In ALTA 2008,
pages 79?87, Australia.
McCallum, Andrew and Wei Li. 2003. Early results
for named entity recognition with conditional ran-
dom fields, feature induction and web-enhanced lex-
icons. In HLT-NAACL, pages 188?191.
Melli, G., Y. Liu Z. Shi, Y. Wang, and F. Popowich.
2006. Description of SQUASH, the SFU Question
Answering Summary Handler for the DUC-2006
Summarization Task. In DUC.
Palmer, Martha, Daniel Gildea, and Paul Kingsbury.
2005. The Proposition Bank: An Annotated Corpus
of Semantic Roles. Computational Linguistics, 31.
Punyakanok, Vasin, Dan Roth, W. Yih, D. Zimak, and
Y. Tu. 2004. Semantic role labeling via generalized
inference over classifiers. InHLT-NAACL (CoNLL),
pages 130?133. ACL.
Pustejovsky, James, Jose? M. Castan?o, Robert Ingria,
Roser Saur??, Robert Gaizauskas, Andrea Setzer, and
Graham Katz. 2003a. TimeML: Robust Specifica-
tion of Event and Temporal Expressions in Text. In
IWCS-5.
Pustejovsky, James, Patrik Hanks, Roser Saur??, A. See,
Robert Gaizauskas, Andrea Setzer, Dragomir R.
Radev, Beth Sundheim, David Day, Lisa Ferro, and
M. Lazo. 2003b. The TIMEBANK Corpus. In Cor-
pus Linguistics, pages 647?656.
Pustejovsky, James. 2002. TERQAS: Time and Event
Recognition for Question Answering Systems. In
ARDA Workshop.
Saur??, Roser, Robert Knippen, Marc Verhagen, and
James Pustejovsky. 2005. Evita: A robust event
recognizer for qa systems. In HLT/EMNLP. ACL.
Schmid, Helmut. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of the
International Conference on New Methods in Lan-
guage Processing, pages 44?49.
Setzer, Andrea and Robert Gaizauskas. 2000.
Annotating Events and Temporal Information in
Newswire Texts. In LREC 2000, pages 1287?1294.
Sha, Fei and Fernando C. N. Pereira. 2003. Shal-
low parsing with conditional random fields. InHLT-
NAACL.
Tenny, Carol and James Pustejovsky. 2000. Events as
Grammatical Objects. The Converging Perspectives
of Lexical Semantics and Syntax. CSLI.
Vendler, Zeno, 1967. Linguistics and philosophy,
chapter Verbs and times, pages 97?121. Cornell
University Press, Ithaca, NY.
733
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 284?291,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
TIPSem (English and Spanish):
Evaluating CRFs and Semantic Roles in TempEval-2
Hector Llorens, Estela Saquete, Borja Navarro
University of Alicante
Alicante, Spain
{hllorens,stela,borja}@dlsi.ua.es
Abstract
This paper presents TIPSem, a system to
extract temporal information from natural
language texts for English and Spanish.
TIPSem, learns CRF models from training
data. Although the used features include
different language analysis levels, the ap-
proach is focused on semantic informa-
tion. For Spanish, TIPSem achieved the
best F1 score in all the tasks. For English,
it obtained the best F1 in tasks B (events)
and D (event-dct links); and was among
the best systems in the rest.
1 Introduction
The automatic treatment of time expressions,
events and their relations over natural language
text consists of making temporal elements ex-
plicit through a system that identifies and anno-
tates them following a standard scheme. This in-
formation is crucial for other natural language pro-
cessing (NLP) areas, such as summarization or
question answering. The relevance of temporal in-
formation has been reflected in specialized confer-
ences (Schilder et al, 2007) and evaluation forums
(Verhagen et al, 2007).
We present a system to tackle the six different
tasks related to multilingual temporal information
treatment proposed in TempEval-2. Particularly,
in this evaluation exercise, TimeML (Pustejovsky
et al, 2003) is adopted as temporal annotation
scheme. In this manner, the tasks require partic-
ipating systems to automatically annotate differ-
ent TimeML elements. Firstly, task A consists
of determining the extent of time expressions as
defined by the TimeML TIMEX3 tag, as well as
the attributes ?type? and ?value?. Secondly, task
B addresses the recognition and classification of
events as defined by TimeML EVENT tag. Fi-
nally, tasks C to F comprise the categorization of
different temporal links (TimeML LINKs). Figure
1 illustrates the TimeML elements in a sentence.
Figure 1: TimeML example
In the context of TempEval-2, we tackle all
tasks for English and Spanish with a data-driven
system. This consists of CRF models inferred
from lexical, syntactic and semantic information
of given training data.
Our main approach, TIPSem (Temporal
Information Processing based on Semantic in-
formation), is focused on semantic roles and
semantic networks. Furthermore, we present
a secondary approach, TIPSem-B (TIPSem-
Baseline), which contrary to the former does not
consider semantic information.
The main objectives of this paper are (1) evalu-
ating the performance of TIPSem comparing it to
other participating systems and (2) measuring the
contribution of semantic information to different
TempEval-2 tasks though the comparison between
our systems: TIPSem and TIPSem-B.
This paper is structured as follows. Our ap-
proach to address the TempEval-2 tasks is moti-
vated in Section 2 and described in Section 3. The
results obtained in the evaluation are shown and
analyzed in Section 4. Finally, conclusions are
drawn in Section 5.
2 Approach motivation
The next two subsections describe the two main
characteristics of our approach, CRFs and seman-
tic roles, and the reasons why we think they could
be useful to tackle TimeML annotation.
284
2.1 CRF probabilistic model
Conditional Random Fields is a popular and effi-
cient ML technique for supervised sequence label-
ing (Lafferty et al, 2001). In the recognition prob-
lem raised by TempEval-2 tasks A and B, assume
X is a random variable over data sequences to be
labeled, and Y is a random variable over the corre-
sponding label sequences, being all Y components
(Y
i
) members of a finite label alphabet ?. X might
range over the sentences and Y range over possi-
ble annotations of those sentences, with ? the set
of possible event IOB21 labels. The following ex-
ample illustrates the problem.
(1) X Y
That ? B-TIMEX3
was ? B-EVENT
another ? ? = I-TIMEX3
bad ? I-EVENT
week ? O
Then, CRFs construct a conditional model from
paired observations and label sequences: p(Y |X).
To extend the problem to classification, X is re-
placed with elements to be classified and ? is re-
placed with the possible classes, for instance, in
task A X = {TIMEX3 instances in text} and
? = {DATE, DURATION, SET, TIME}.
From our point of view, CRFs are well suited
to address TempEval-2 tasks. Firstly, TimeML
elements depend on structural properties of sen-
tences. Not only the word sequence, but mor-
phological, syntactic and semantic structure is re-
lated with them. Secondly, some TIMEX3 and
EVENT elements are denoted by sequences of
words, therefore the CRFs are very appropriate.
2.2 Semantic roles
Semantic role labeling (SRL) has achieved impor-
tant results in the last years (Gildea and Jurafsky,
2002; Moreda et al, 2007). For each predicate in a
sentence, semantic roles identify all constituents,
determining their arguments (agent, patient, etc.)
and their adjuncts (locative, temporal, etc.). Fig-
ure 2 illustrates a semantic role labeled sentence.
Figure 2: Semantic roles example
Semantic roles provide structural relations of
the predicates in which TimeML elements may
1IOB2 format: (B)egin, (I)nside, and (O)utside
participate. Beyond syntactic relations expressed
by means of the different types of phrases, seman-
tic roles give further information about semantic
relations between the arguments of a predicate.
Due to the fact that roles represent high level in-
formation, they are more independent from word
tokens. Hence, roles may aid in learning more
general models that could improve the results of
approaches focused on lower level information.
3 Our approach: TIPSem
As defined in previous section, this paper pro-
poses CRF as learning method to infer models to
face the TempEval-2 tasks. Specifically, CRF++
toolkit2 was used for training and testing our ap-
proach. The learning process was done using
the parameters: CRF-L2 algorithm and hyper-
parameter C=1.
In order to set out the approach architecture and
select the features for learning, we divided the
tasks proposed in the evaluation exercise into four
groups: recognition, classification, normalization
and link-categorization. Each group represents a
kind of problem to be resolved. Recognition prob-
lem is present in TIMEX3 and EVENT bounding
(tasks A and B). Classification problem appears in
TIMEX3 type and EVENT class attributes (tasks
A and B). Normalization arises in TIMEX3 value
attribute (task A). And link-categorization is ap-
plied to different kind of link relations (tasks C to
F). Each group uses a particular feature set to learn
an annotation model. The features of these sets are
grouped in two subsets. On the one hand, general
features, which are widely used in different NLP
fields and represent lower language analysis lev-
els. On the other hand, semantic features, which
are a novelty in the task and our main focus.
TIPSem system uses all the features defined
above. However, to measure the influence of se-
mantic information in temporal information treat-
ment, TIPSem-B system was implemented ex-
cluding the semantic features.
3.1 Recognition
In recognition, the features are obtained at token
level, that is to say, each token has its own set of
features.
Regarding each language analysis level, the
general features used to train our CRF model are:
2http://crfpp.sourceforge.net/
285
? Morphological: The lemma and part-of-
speech (PoS) context, in a 5-window (-2,+2),
was employed due to the good results it
achieved in other NLP tasks. Tokenization,
PoS and lemmatization were obtained using
TreeTagger (Schmid, 1994) for English, and
were got from AnCora (Taule? et al, 2008) for
Spanish.
? Syntactic: Different TimeML elements are
contained in particular types of phrases. This
feature tries to capture this fact by consider-
ing phrase level syntactic information. The
syntactic tree was obtained using Charniak
parser (Charniak and Johnson, 2005) for En-
glish, and AnCora for Spanish.
? Polarity, tense and aspect: These were ob-
tained using PoS and a set of handcrafted
rules (e.g., will+verb ? future).
The semantic level features used to enhance the
training framework of the CRF model are:
? Role: For each token, we considered the
role regarding the verb the token depends on.
To get semantic roles, CCG SRL tool (Pun-
yakanok et al, 2004) was used for English,
and AnCora for Spanish.
? Governing verb: The verb to which the cur-
rent token holds a particular role. This may
distinguish tokens appearing under the influ-
ence of different verbs.
? Role+verb combination: The previous two
features were combined to capture the rela-
tion between them. This introduces addi-
tional information by distinguishing roles de-
pending on different verbs. The importance
of this falls especially on the numbered roles
(A0, A1, etc.) meaning different things when
depending on different verbs.
? Role configuration: This feature is only
present in verb tokens heading a sentence or
sub-sentence. This consists of the set of roles
depending on the verb. This may be particu-
larly useful for distinguish different sentence
settings.
? Lexical semantics: WordNet (Fellbaum,
1998) top ontology classes have been widely
used to represent word meaning at ontologi-
cal level, and demonstrated its worth in many
tasks. TIPSem uses the top four classes
for each word. For Spanish, EuroWordNet
(Vossen, 1998) was used.
In this manner, given a list of tokens and its fea-
tures, the trained recognition model will assign to
each token one of the valid labels. For instance,
in the case of TIMEX3 recognition: B-TIMEX3,
I-TIMEX3 or O.
3.2 Classification
Classification features, used to get TIMEX3 types
and EVENT classes, are basically the same as the
ones used for recognition. However, the main
difference is that the features are not obtained
at token level but at TIMEX3 or EVENT level.
This implies that the word context is set to the
extent of each element (TIMEX3 and EVENT),
as well as all the features have as many values
as tokens comprises the element (e.g., element-
tokens=?next Monday?, PoS-feature=?JJ+NNP?).
Hence, following this description, the classifica-
tion models will assign to each element one of the
valid classes. For example, in the case of TIMEX3
typing: DATE, DURATION, SET or TIME.
3.3 Normalization
As in classification the features are obtained at
TIMEX3 level. Furthermore, word-spelled num-
bers contained in the TIMEX3 extent are trans-
lated to their numerical value (e.g., ?three days?
? ?3 days?).
Normalization process consists of two main
steps: (1) obtain the normalization type and (2)
apply the corresponding normalization rules.
The first step applies a CRF model that uses the
same features as the previous two plus TIMEX3
pattern. This new feature consists of the tokens
comprised by the TIMEX3 but replacing num-
bers by NUM, temporal units, such as years or
days, by TUNIT, months by MONTH, and week-
days by WEEKDAY. In other words, ?next Mon-
day? would result in ?next WEEKDAY? and ?June
1999? in ?MONTH NUM?. Once the model is
trained, for each new TIMEX3 it assigns a normal-
ization type. We define seven normalization types:
Period, ISO, ISO set, ISO function, present ref,
past ref and future ref.
The second step uses as input the output of the
first one. Each normalization type has its own nor-
malization rules.
286
? Period: Apply rules to convert period-like
TIMEX3 (?3 days?) into P NUM TUNIT
normalized period (?P3D?).
? ISO: Apply rules to convert any-format ex-
plicit date or time into a valid ISO 8601 stan-
dard date.
? ISO set: Apply rules to get a valid ISO-
like set from a TIMEX3 set (?monthly? ?
XXXX-XX).
? ISO function: This is the most complex
type. The system applies different functions
to get a valid ISO date or time in a valid gran-
ularity from DCT3 dates. Here, time direc-
tion indicators like ?next? or ?previous?, as
well as verbal tenses are used.
? Present ref, past ref and future ref: these
are already normalized.
3.4 Link-categorization
Each one of link-related tasks (C to F) has its own
link-categorization features. Nevertheless, all link
types share some of them.
? Task C: For categorizing the relation be-
tween an EVENT and a TIMEX3, the system
takes into account the following features:
? Heading preposition if the event or the
TIMEX3 are contained by a preposi-
tional phrase as in ?before the meeting?,
where ?meeting? is the event and ?be-
fore? the heading preposition.
? Syntactic relation of the event and the
TIMEX3 in the sentence. This feature
may be evaluated as: same sentence,
same subsentence or same phrase.
? Time position. If the event is not di-
rectly linked with the relation TIMEX3
but related to another TIMEX3, the time
position represents whether the event
is before, overlap or after the relation
TIMEX3.
? Interval. This feature is 0 unless there
appears some interval indicator token
near the TIMEX3. This is useful to
identify overlap-and-after and overlap-
and-before categories.
? TIMEX3 type.
3Date Creation Time
? Semantic roles if the event or the
TIMEX3 are contained by a tempo-
ral subordination (labeled with tempo-
ral role), for example, in ?after he left
home?, ?left? is the event and ?after? the
subordinating element (role feature).
? Task D: To determine the relationship be-
tween an event and the DCT, TIPSem uses
the same features as in task C except in-
terval. In addition, all the features related
to TIMEX3 are now related to the closer
TIMEX3 (if exists) in the event sentence. In
this manner, the time position is calculated by
comparing DCT and that TIMEX3.
? Task E: Relations between two main events
are categorized using only four features: the
tense and aspect of the two events, the syn-
tactic relation between them, and the time po-
sition, calculated using the closer TIMEX3 to
each event.
? Task F: For categorizing subordinated
events, TIPSem uses the subordinating
element of temporal roles containing each
event (if present), the heading preposition of
a prepositional phrases containing each event
(if present), as well as the tense and aspect.
To illustrate the system architecture, Figure 3
summarizes the strategies that TIPSem follows
to tackle the tasks proposed in the TempEval-2
framework.
Figure 3: TIPSem architecture
287
4 Evaluation
The test corpus consists of 17K words for English
and 10K words for Spanish, in which approxi-
mately a half part correspond to tasks A and B,
and the other half to tasks C, D, E and F. The per-
formance is measured using precision, recall and
F
?=1
metrics. A scoring script is provided. This
counts correct instances at token level for tasks A
and B, and at temporal link level for the rest.
Next subsections show the results obtained by
TIPSem system in each one of the TempEval-2
tasks for English (EN) and Spanish (ES). More-
over, a final subsection illustrates the F
?=1
results
in three comparative graphs. In tasks A and B, pre-
cision, recall and F
?=1
are given. In tasks C to E,
links tasks precision, recall and F
?=1
are the same
because our system does not consider the NONE
value. Hence, only F
?=1
is given. Tasks E and
F were not considered for Spanish in TempEval-
2 evaluation and thus Spanish is excluded from
those subsections.
For each task, scores in which our system ob-
tained the first place in the evaluation exercise are
in bold. Furthermore, in all cases the best score
obtained by participating systems is reported. Fi-
nally, the influence of semantic information in
terms of improvement is indicated and analyzed
through the comparison with TIPSem-B system,
which exclude the features related with semantics.
4.1 Task A: TIMEX3
Table 1 shows the results obtained by our ap-
proaches in TIMEX3 recognition, typing and ISO
8601 normalization (value).
System lang Prec. Rec. F
?=1
type value
TIPSem EN 0.92 0.80 0.85 0.92 0.65
TIPSem ES 0.95 0.87 0.91 0.91 0.78
TIPSem-B EN 0.88 0.60 0.71 0.88 0.59
TIPSem-B ES 0.97 0.81 0.88 0.99 0.75
Table 1: Task A - English and Spanish
As shown in results, TIPSem obtains the best re-
sults for Spanish in all measures except for ?value?
attribute, in which the best system obtained a 0.83.
Another system obtained the same recall (0.87)
but a lower precision (0.90), and thus a F
?=1
of
(0.88) below TIPSem score (0.91). For English,
our main approach obtained the best precision.
However, another system obtained the best recall
(0.91). The best F
?=1
was 0.86. Regarding type
attribute, TIPSem obtained values closer to best
system (0.98). Finally, in normalization, which
is the only attribute that is not annotated by a
purely data-driven process, best system surpassed
TIPSem in 0.20.
These results indicate that CRFs represent an
appropriate ML technique to learn models for an-
notating TIMEX3. Furthermore, they show that
normalization process used by TIPSem could be
improved using other techniques.
Specifically, the usage of semantic information
improved the capability of learned models to gen-
eralize rules. For instance in time expressions, if
an unseen instance is contained by a temporal role
is a clear candidate to be a time expression. Hence,
they improve system recall (33% EN, 7% ES).
4.2 Task B: EVENT
Table 2 shows the results obtained by our ap-
proaches in recognizing and classifying events.
System lang Prec. Recall F
?=1
class
TIPSem EN 0.81 0.86 0.83 0.79
TIPSem ES 0.90 0.86 0.88 0.66
TIPSem-B EN 0.83 0.81 0.82 0.79
TIPSem-B ES 0.92 0.85 0.88 0.66
Table 2: Task B - English and Spanish
In this tasks, TIPSem obtained the best re-
sults in TempEval-2 for Spanish and English in
both recognition and classification. Although for
English another system achieved the best recall
(0.88), it obtained a lower precision (0.55); and
thus a 0.68 F
?=1
. This indicates that our approach
obtains the best F
?=1
(0.83) with a well-balanced
precision and recall.
Again, the usage of semantic information im-
proves the capability of learned models to gen-
eralize, which improves the recall (6% EN, 1%
ES). For events, the improvement is lower than for
TIMEX3 because, contrary to TIMEX3, they are
not clearly defined by specific roles. In this case,
features like role configuration, semantic classes,
or role-governing verb are more useful.
Other attributes present in events such as polar-
ity, mood and tense obtained values of about 90%.
However, to get the values for these attributes the
system applies a set of handcrafted rules and then
the results are not relevant for our approach.
4.3 Task C: LINKS - Events and TIMEXs
Table 3 shows the results obtained by our ap-
proaches in categorizing EVENT-TIMEX3 links.
288
System lang F
?=1
TIPSem EN 0.55
TIPSem ES 0.81
TIPSem-B EN 0.54
TIPSem-B ES 0.81
Table 3: Task C - English and Spanish
TIPSem was the only system participating in
this task for Spanish. Nevertheless, 0.81 is a high
score comparing it to English best score (0.63).
Our system, for English, is 8 points below top
scored system.
In this task, the application of semantic roles in-
troduced an improvement of 2% in F
?=1
.
4.4 Task D: LINKS - Events and DCTs
Table 4 shows the results obtained by our ap-
proaches in categorizing events with respect to the
creation time of a document.
System lang F
?=1
TIPSem EN 0.82
TIPSem ES 0.59
TIPSem-B EN 0.81
TIPSem-B ES 0.59
Table 4: Task D - English and Spanish
Task D is successfully covered by TIPSem ob-
taining the best results in the evaluation.
It seems that the relation of events with doc-
ument creation time strongly depends on tense
and aspect, as well as the event position in time
with respect to DCT when defined by neighboring
TIMEX3.
Furthermore, the learned CRF models take ad-
vantage of using temporal semantic roles informa-
tion. Specifically, the usefulness of semantic roles
in this task was quantified to 2%.
4.5 Task E: LINKS - Main events
Table 5 shows the results obtained by our ap-
proaches in categorizing main events relations in
text.
System lang F
?=1
TIPSem EN 0.55
TIPSem-B EN 0.55
Table 5: Task E - English
In this task, our system obtains the second
place. However, the top scored achieved a 0.56.
Again, the tense and aspect features, as well as
the events position in time resulted useful to tackle
this task. In this case, semantic roles information
is not used so TIPSem and TIPSem-B are equiva-
lent.
4.6 Task F: LINKS - Subordinated events
Table 6 shows the results obtained by our ap-
proaches in categorizing events relations with the
events they syntactically govern.
System lang F
?=1
TIPSem EN 0.59
TIPSem-B EN 0.60
Table 6: Task F - English
Categorizing subordinated events TIPSem ob-
tained the second place. Best score was 0.66. In
this task, the application of roles did not help and
decreased the score in one point. The cause may
be that for this task roles are not relevant but noisy.
In this case, some extra information extending se-
mantic roles is needed to turn them into a useful
feature.
4.7 Comparative graphs
This subsection presents the TIPSem F
?=1
re-
sults in three graphs. Figure 4 illustrates the re-
sults for English indicating the higher and lower
scores achieved by TempEval-2 participating sys-
tems. Figure 5 shows the same for Spanish but,
due to the fact that TIPSem was the only partici-
pant in tasks B, C and D, the graph includes En-
glish min. and max. scores as indirect assessment.
Finally, Figure 6, compares the TIPSem results for
English and Spanish.
Figure 4: English F
?=1
comparative
Figure 4 shows how TIPSem achieved, in gen-
eral, a high performance for English.
289
Figure 5: Spanish F
?=1
indirect assessment
For Spanish we can only report indirect assess-
ment comparing the results to English scores. It
can be seen that the quality of the results is similar
for tasks A and B, but seems to be inverted in tasks
C and D.
Figure 6: TIPSem EN - ES F
?=1
comparative
Finally, in this graph comparing TIPSem re-
sults, we observe that our approach achieved simi-
lar performance for both languages in tasks A and
B. This indicates that for this tasks, the approach is
valid for both languages. However, as in the previ-
ous graph, it seems that for English TIPSem per-
forms worse in task C and better in task D while
for Spanish it does right the opposite.
The train and test corpora were reviewed to an-
alyze this fact. On the one hand, the reason for
the high performance in task C for Spanish was
the high amount of ?overlap? instances in both
corpora. This trained the CRF model for catego-
rizing event-timex links as ?overlap? in most of
cases. On the other hand, the cause of the Spanish
low performance in task D is ?vague? links. The
features defined in TIPSem cannot distinguish be-
tween ?overlap? and ?vague?. Due to the fact that
?vague? links are quite popular in Spanish test set,
the results decreased. This did not affect to En-
glish results because of the spareness of ?vague?
links.
5 Conclusions and Further Work
This paper presented a system for automatically
treating temporal information of natural language
texts as required in the TempEval-2 evaluation ex-
ercise, in particular, following TimeML specifica-
tions.
Our system, TIPSem, is a data-driven approach
and consists of different CRF models learned us-
ing semantic information as main feature. CRFs
were used taking into account that data-driven ap-
proaches have obtained good results in many NLP
tasks, and due to their appropriateness in sequence
labeling problems and problems in which struc-
tural properties are relevant, as those proposed in
TempEval-2. Furthermore, the models were en-
hanced using semantic information. Roles have
been applied in other NLP fields with successful
results, but never employed before for this pur-
pose. With these two main characteristics, we de-
signed a complete learning environment selecting,
in addition to roles, different language analysis
level properties as features to train the models.
The results obtained for English and Spanish
in the evaluation exercise were satisfactory and
well-balanced between precision and recall. For
Spanish, TIPSem achieved the best F
?=1
in all
tasks. For English, it obtained the best F
?=1
in
event recognition and classification (task B), and
event and document creation time links catego-
rization (task D). Furthermore, in general, all the
results of TIPSem were very competitive and were
among the top scored systems. This verifies that
our approach is appropriate to address TempEval-
2 tasks.
Regarding multilinguality, the approach was
proven to be valid for different languages (English
and Spanish). This was also verified for Catalan
language by earlier versions of TIPSem (Llorens
et al, 2009). In fact, the data-driven part of the
system could be considered language independent
because it has been applied to different languages
and could be applied to other languages without
adaptation, provided that there are tools available
to get the morphosyntactic and semantic informa-
tion required by the approach. It has to be high-
290
lighted that to apply TIPSem-B only morphosyn-
tactic information is required. Only the normaliza-
tion of time expressions is a language dependent
process in our system and requires the construc-
tion of a set of rules for each target language.
The contribution of semantic information to
temporal information treatment was more signif-
icant in recall and the improvement was concen-
trated in tasks A and B (approx. 12% recall im-
provement). Although, TIPSem-B achieved lower
results they are high enough to confirm that that
most of temporal elements strongly depends on
lexical and morphosyntactic information.
The main errors and difficulties of our approach
in this evaluation exercise are related to TIMEX3
normalization (value attribute). A pure ML ap-
proach for solving this problem is not trivial, at
least, using our approach philosophy. The treat-
ment of normalization functions is an inherently
complex task and requires many training data to be
automatically learned. This required us to include
in the system some handcrafted rules to enable the
system for this task.
As further work we propose improving the
TIMEX3 normalization by replacing handcrafted
normalization rules with machine learned ones
by combining statistic techniques and multilingual
temporal knowledge resources (ontologies). Fur-
thermore, link-categorization will be analyzed in
more detail in order to include more features to im-
prove the models. Finally, the suggested language
independence of the approach will be tested using
TempEval-2 available data for other languages.
Acknowledgments
This paper has been supported by the Spanish
Government, projects TIN-2006-15265-C06-01, TIN-
2009-13391-C04-01 and PROMETEO/2009/119, where
Hector Llorens is funded under a FPI grant (BES-
2007-16256).
References
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative
reranking. In 43rd Annual Meeting of the ACL.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database (Language, Speech, and Commu-
nication). MIT Press.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational Linguis-
tics, 28(3):245?288.
John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling se-
quence data. In Proceedings of the 18th ICML,
pages 282?289. Morgan Kaufmann.
Hector Llorens, Borja Navarro, and Estela Saquete.
2009. Deteccio?n de Expresiones Temporales
TimeML en Catala?n mediante Roles Sema?nticos y
Redes Sema?nticas. In Procesamiento del Lenguaje
Natural (SEPLN), number 43, pages 13?21.
Paloma Moreda, Borja Navarro, and Manuel Palomar.
2007. Corpus-based semantic role approach in in-
formation retrieval. Data Knowledge Engineering,
61(3):467?483.
Vasin Punyakanok, Dan Roth, W. Yih, D. Zimak, and
Y. Tu. 2004. Semantic role labeling via generalized
inference over classifiers. In HLT-NAACL (CoNLL),
pages 130?133. ACL.
James Pustejovsky, Jose? M. Castan?o, Robert Ingria,
Roser Saur??, Robert Gaizauskas, Andrea Setzer, and
Graham Katz. 2003. TimeML: Robust Specifica-
tion of Event and Temporal Expressions in Text. In
IWCS-5.
Frank Schilder, Graham Katz, and James Pustejovsky.
2007. Annotating, Extracting and Reasoning About
Time and Events (Dagstuhl 2005), volume 4795 of
LNCS. Springer.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of the
International Conference on New Methods in Lan-
guage Processing, pages 44?49.
Mariona Taule?, M. Antonia Mart??, and Marta Recasens.
2008. AnCora: Multilevel Annotated Corpora for
Catalan and Spanish. In ELRA, editor, LREC, Mar-
rakech, Morocco.
Marc Verhagen, Robert Gaizauskas, Mark Hepple,
Frank Schilder, Graham Katz, and James Puste-
jovsky. 2007. Semeval-2007 task 15: Tempeval
temporal relation identification. In Proceedings of
the 4th International Workshop on Semantic Evalu-
ations, pages 75?80, Prague. ACL.
Piek Vossen. 1998. EuroWordNet: a multilingual
database with lexical semantic networks. Kluwer
Academic Publishers, MA, USA.
291
