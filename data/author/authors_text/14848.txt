An Unsupervised Method for Detecting Grammatical Errors 
Martin Chodorow 
Hunter College of CUNY 
695 Park Avenue 
New York, NY 
martin.chodorow @.hunter.cuny.edu 
Claudia Leacock 
Educational Testing Service 
Rosedale Road 
Princeton, NJ 
cleacock@ets.org 
Abstract 
We present an unsupervised method for 
detecting rammatical errors by inferring 
negative evidence from edited textual 
corpora. The system was developed and 
tested using essay-length responses to 
prompts on the Test of English as a 
Foreign Language (TOEFL). The error- 
recognition system, ALEK, performs with 
about 80% precision and 20% recall. 
Introduction 
A good indicator of whether aperson knows the 
meaning of a word is the ability to use it 
appropriately in a sentence (Miller and Gildea, 
1987). Much information about usage can be 
obtained from quite a limited context: Choueka 
and Lusignan (1985) found that people can 
typically recognize the intended sense of a 
polysemous word by looking at a narrow 
window of one or two words around it. 
Statistically-based computer programs have 
been able to do the same with a high level of 
accuracy (Kilgarriff and Palmer, 2000). The goal 
of our work is to automatically identify 
inappropriate usage of specific vocabulary 
words in essays by looking at the local 
contextual cues around a target word. We have 
developed a statistical system, ALEK (Assessing 
Le____xical Knowledge), that uses statistical 
analysis for this purpose. 
A major objective of this research is to avoid the 
laborious and costly process of collecting errors 
(or negative vidence) for each word that we 
wish to evaluate. Instead, we train ALEK on a 
general corpus of English and on edited text 
containing example uses of the target word. The 
system identifies inappropriate usage based on 
differences between the word's local context 
cues in an essay and the models of context it has 
derived from the corpora of well-formed 
sentences. 
A requirement for ALEK has been that all steps 
in the process be automated, beyond choosing 
the words to be tested and assessing the results. 
Once a target word is chosen, preprocessing, 
building a model of the word's appropriate 
usage, and identifying usage errors in essays is 
performed without manual intervention. 
ALEK has been developed using the Test of 
English as a Foreign Language (TOEFL) 
administered bythe Educational Testing 
Service. TOEFL is taken by foreign students 
who are applying to US undergraduate and 
graduate-level programs. 
1 Background 
Approaches to detecting errors by non-native 
writers typically produce grammars that look for 
specific expected error types (Schneider and 
McCoy, 1998; Park, Palmer and Washburn, 
1997). Under this approach, essays written by 
ESL students are collected and examined for 
errors. Parsers are then adapted to identify those 
error types that were found in the essay 
collection. 
We take a different approach, initially viewing 
error detection as an extension of the word sense 
disambiguation (WSD) problem. Corpus-based 
WSD systems identify the intended sense of a 
polysemous word by (1) collecting a set of 
example sentences for each of its various enses 
and (2) extracting salient contextual cues from 
these sets to (3) build a statistical model for each 
sense. They identify the intended sense of a 
word in a novel sentence by extracting its 
contextual cues and selecting the most similar 
word sense model (e.g., Leacock, Chodorow and 
Miller (1998), Yarowsky (1993)). 
Golding (1995) showed how methods used for 
WSD (decision lists and Bayesian classifiers) 
could be adapted to detect errors resulting from 
140 
common spelling confusions among sets such as 
there, their, and they 're. He extracted contexts 
from correct usage of each confusable word in a 
training corpus and then identified a new 
occurrence as an error when it matched the 
wrong context. 
However, most grammatical errors are not the 
result of simple word confusions. This 
complicates the task of building a model of 
incorrect usage. One approach we considered 
was to proceed without such a model: represent 
appropriate word usage (across enses) in a 
single model and compare a novel example to 
that model. The most appealing part of this 
formulation was that we could bypass the 
knowledge acquisition bottleneck. All 
occurrences of the word in a collection of edited 
text could be automatically assigned to a single 
training set representing appropriate usage. 
Inappropriate usage would be signaled by 
contextual cues that do not occur in training. 
Unfortunately, this approach was not effective 
for error detection. An example of a word usage 
error is often very similar to the model of 
appropriate usage. An incorrect usage can 
contain two or three salient contextual elements 
as well as a single anomalous element. The 
problem of error detection does not entail 
finding similarities to appropriate usage, rather it 
requires identifying one element among the 
contextual cues that simply does not fit. 
2 ALEK Architecture 
What kinds of anomalous elements does ALEK 
identify? Writers sometimes produce rrors that 
violate basic principles of English syntax (e.g., a 
desks), while other mistakes how a lack of 
information about a specific vocabulary item 
(e.g., a knowledge). In order to detect hese two 
types of problems, ALEK uses a 30-million 
word general corpus of English from the San 
Jose Mercury News (hereafter referred to as the 
general corpus) and, for each target word, a set 
of 10,000 example sentences from North 
American newspaper text I (hereafter referred to 
as the word-specific corpus). 
i The corpora re extracted from the ACL-DCI 
corpora. In selecting the sentences for the word 
ALEK infers negative vidence from the 
contextual cues that do not co-occur with the 
target word - either in the word specific corpus 
or in the general English one. It uses two kinds 
of contextual cues in a +2 word window around 
the target word: function words (closed-class 
items) and part-of-speech tags (Brill, 1994). The 
Brill tagger output is post-processed to "enrich" 
some closed class categories of its tag set, such 
as subject versus object pronoun and definite 
versus indefinite determiner. The enriched tags 
were adapted from Francis and Ku~era (I 982). 
After the sentences have been preprocessed, 
ALEK counts sequences of adjacent part-of- 
speech tags and function words (such as 
determiners, prepositions, and conjunctions). For 
example, the sequence a/ATfull-time/JJjob/NN 
contributes one occurrence ach to the bigrams 
AT+J J, JJ+NN, a+JJ, and to the part-of-speech tag 
trigram AT+JJ+NN. Each individual tag and 
function word also contributes to its own 
unigram count. These frequencies form the basis 
for the error detection measures. 
From the general corpus, ALEK computes a 
mutual information measure to determine which 
sequences of part-of-speech tags and function 
words are unusually rare and are, therefore, 
likely to be ungrammatical in English (e.g., 
singular determiner preceding plural noun, as in 
*a desks). Mutual information has often been 
used to detect combinations of words that occur 
more frequently than we would expect based on 
the assumption that the words are independent. 
Here we use this measure for the opposite 
purpose - to find combinations that occur less 
often than expected. ALEK also looks for 
sequences that are common in general but 
unusual in the word specific corpus (e.g., the 
singular determiner a preceding a singular noun 
is common in English but rare when the noun is 
specific orpora, we tried to minimize the mismatch 
between the domains of newspapers and TOEFL 
essays. For example, in the newspaper domain, 
concentrate is usually used as a noun, as in orange 
juice concentrate but in TOEFL essays it is a verb 
91% of the time. Sentence selection for the word 
specific orpora was constrained to reflect he 
distribution of part-of-speech tags for the target word 
in a random sample of TOEFL essays. 
141 
knowledge). These divergences between the two 
corpora reflect syntactic properties that are 
peculiar to the target word. 
2.1 Measures based on the general 
corpus: 
The system computes mutual information 
comparing the proportion of observed 
occurrences ofbigrams in the general corpus to 
the proportion expected based on the assumption 
of independence, as shown below: 
P(A) ? P(B)) 
Here, P(AB) is the probability of the occurrence 
of the AB bigram, estimated from its frequency 
in the general corpus, and P(A) and P(B) are the 
probabilities of the first and second elements of 
the bigram, also estimated from the general 
corpus. Ungrammatical sequences should 
produce bigram probabilities that are much 
smaller than the product of the unigram 
probabilities (the value of MI will be negative). 
Trigram sequences are also used, but in this case 
the mutual information computation compares 
the co-occurrence of ABC to a model in which 
A and C are assumed to be conditionally 
independent given B (see Lin, 1998). 
M/= log 2 P( B) x P( A I B ) x P(C I B) 
Once again, a negative value is often indicative 
of a sequence that violates a rule of English. 
2.2 Comparing the word-specific orpus 
to the general corpus: 
ALEK also uses mutual information to compare 
the distributions of tags and function words in 
the word-specific corpus to the distributions that 
are expected based on the general corpus. The 
measures for bigrams and trigrams are similar to 
those given above except that the probability in 
the numerator isestimated from the word- 
specific orpus and the probabilities in the 
denominator come from the general corpus. To 
return to a previous example, the phrase a
knowledge contains the tag bigram for singular 
determiner followed by singular noun (AT Nil). 
This sequence ismuch less common in the 
word-specific corpus for knowledge than would 
be expected from the general corpus unigram 
probabilities of AT and NN. 
In addition to bigram and trigram measures, 
ALEK compares the target word's part-of- 
speech tag in the word-specific corpus and in the 
general corpus. Specifically, it looks at the 
conditional probability of the part-of-speech tag 
given the major syntactic ategory (e.g., plural 
noun given noun) in both distributions, by 
computing the following value. 
( P=p~c~c _ corm(taglcategory) I 
o g 2 / ~ ~  
t. p=o,e,o, _ co, =(tag I ) 
For example, in the general corpus, about half of 
all noun tokens are plural, but in the training set 
for the noun knowledge, the plural knowledges 
occurs rarely, if at all. 
The mutual information measures provide 
candidate errors, but this approach overgenerates 
- it finds rare, but still quite grammatical, 
sequences. To reduce the number of false 
positives, no candidate found by the MI 
measures i considered an error if it appears in 
the word-specific corpus at least wo times. This 
increases ALEK's precision at the price of 
reduced recall. For example, aknowledge will 
not be treated as an error because it appears in 
the training corpus as part of the longer a 
knowledge of sequence (as in a knowledge of 
mathematics). 
ALEK also uses another statistical technique for 
finding rare and possibly ungrammatical t g and 
function word bigrams by computing the %2 (chi 
square) statistic for the difference between the 
bigram proportions found in the word-specific 
and in the general corpus: 
~ Pgeneral_corpu~ i -egerneral_corpus ) / Nword specific 
The %2 measure faces the same problem of 
overgenerating errors. Due to the large sample 
sizes, extreme values can be obtained even 
though effect size may be minuscule. To reduce 
false positives, ALEK requires that effect sizes 
be at least in the moderate-to-small r nge 
(Cohen and Cohen, 1983). 
142 
Direct evidence from the word specific corpus 
can also be used to control the overgeneration f 
errors. For each candidate rror, ALEK 
compares the larger context in which the bigram 
appears to the contexts that have been analyzed 
in the word-specific corpus. From the word- 
specific corpus, ALEK forms templates, 
sequences ofwords and tags that represent the 
local context of the target. If a test sentence 
contains a low probability bigram (as measured 
by the X2 test), the local context of the target is 
compared to all the templates of which it is a 
part. Exceptions to the error, that is longer 
grammatical sequences that contain rare sub- 
sequences, are found by examining conditional 
probabilities. To illustrate this, consider the 
example of a knowledge and a knowledge of. 
The conditional probability of of  given a 
knowledge is high, as it accounts for almost all 
of the occurrences ofa knowledge in the word- 
specific corpus. Based on this high conditional 
probability, the system will use the template for 
a knowledge of to keep it from being marked as 
an error. Other function words and tags in the +1 
position have much lower conditional 
probability, so for example, a knowledge iswill 
not be treated as an exception to the error. 
2.3 Validity of  the n-gram measures 
TOEFL essays are graded on a 6 point scale, 
where 6 demonstrates "clear competence" in 
writing on rhetorical and syntactic levels and 1 
demonstrates "incompetence in writing". If low 
probability n-grams ignal grammatical errors, 
then we would expect TOEFL essays that 
received lower scores to have more of these n- 
grams. To test this prediction, we randomly 
selected from the TOEFL pool 50 essays for 
each of the 6 score values from 1.0 to 6.0. For 
Score 
1.0 
% of bigrams 
3.6 
% O f trigrams 
1.4 
2.0 3.4 0.8 
3.0 2.6 0.6 
4.0 1.9 0.3 
5.0 1.3 0.4 
6.0 1.5 0.3 
Table 1: Percent of n-grams with mutual 
information <-3.60, by score point 
each score value, all 50 essays were 
concatenated to form a super-essay. In every 
super-essay, for each adjacent pair and triple of 
tags containing a noun, verb, or adjective, the 
bigram and trigram mutual information values 
were computed based on the general corpus. 
Table 1 shows the proportions ofbigrams and 
trigrams with mutual information less than 
-3.60. As predicted, there is a significant 
negative correlation between the score and the 
proportion of low probability bigrams (rs = -.94, 
n=6, p<.01, two-tailed) and trigrams (r~= -.84, 
n=6, p<.05, two-tailed). 
2.4 System development 
ALEK was developed using three target words 
that were extracted from TOEFL essays: 
concentrate, interest, and knowledge. These 
words were chosen because they represent 
different parts of speech and varying degrees of 
polysemy. Each also occurred in at least 150 
sentences inwhat was then a small pool of 
TOEFL essays. Before development began, each 
occurrence of these words was manually labeled 
as an appropriate or inappropriate usage - 
without aking into account grammatical errors 
that might have been present elsewhere in the 
sentence but which were not within the target 
word's scope. 
Critical values for the statistical measures were 
set during this development phase. The settings 
were based empirically on ALEK's performance 
so as to optimize precision and recall on the 
three development words. Candidate rrors were 
those local context sequences that produced a 
mutual information value of less than -3.60 
based on the general corpus; mutual information 
of less than -5.00 for the specific/general 
comparisons; ora X2 value greater than 12.82 
with an effect size greater than 0.30. Precision 
and recall for the three words are shown below. 
Target word n Precision Recall 
Concentrate 169 .875 .280 
Interest 416 .840 .330 
Knowledge 761 .918 .570 
Table 2: Development Words 
143 
Test Word Precision Recall Total Recall Test Word Precision Recall Total Recall 
(estimated) (estimated) 
Affect .848 .762 .343 .768 .666 .104 
Area 
Aspect 
Benefit 
.752 
.792 
.744 
.846 
.717 
.709 
.205 
.217 
.276 
Energy 
Function 
Individual 
Job 
.800 
.576 
.714 
.742 
.168 
.302 
.728 .679 .103 
Career .736 .671 .110 Period .832 .670 .102 
Communicate .784 .867 .274 Pollution .912 .780 .310 
Concentrate .848 .791 .415 Positive .784 .700 .091 
Conclusion .944 .756 .119 Role ' .728 .674 .098 
Culture .704 .656 .083 Stress .768 .578 .162 
.816 .728 
.779 
Economy .666 .674 
.716 
Technology ~
Mean 
.235 .093 
.190 
Table 3: Precision and recall for 20 test words 
3 Experimental Design and Results 
ALEK was tested on 20 words. These words 
were randomly selected from those which met 
two criteria: (1) They appear in a university 
word list ('Nation, 1990) as words that a student 
in a US university will be expected to encounter 
and (2) there were at least 1,000 sentences 
containing the word in the TOEFL essay pool. 
To build the usage model for each target word, 
10,000 sentences containing it were extracted 
from the North American News Corpus. 
Preprocessing included etecting sentence 
boundaries and part-of-speech tagging. As in the 
development system, the model of general 
English was based on bigram and trigram 
frequencies of function words and part-of- 
speech tags from 30-million words of the San 
Jose Mercury News. 
For each test word, all of the test sentences were 
marked by ALEK as either containing an error 
or not containing an error. The size of the test set 
for each word ranged from 1,400 to 20,000 with 
a mean of 8,000 sentences. 
3.1 Results 
To evaluate the system, for each test word we 
randomly extracted 125 sentences that ALEK 
classified as containing no error (C-set) and 125 
sentences which it labeled as containing an error 
(E-set). These 250 sentences were presented to
a linguist in a random order for blind evaluation. 
The linguist, who had no part in ALEK's 
development, marked each usage of  the target 
word as incorrect or correct and in the case of 
incorrect usage indicated how far from the target 
one would have to look in order to recognise that 
there was an error. For example, in the case of 
"an period" the error occurs at a distance of one 
word from period. When the error is an 
omission, as in "lived in Victorian period", the 
distance is where the missing word should have 
appeared. In this case, the missing determiner is 
2 positions away from the target. When more 
than one error occurred, the distance of the one 
closest o the target was marked. 
Table 3 lists the precision and recall for the 20 
test words. The column labelled "Recall" is the 
proportion of human-judged rrors in the 250- 
sentence sample that were detected by ALEK. 
"Total Recall" is an estimate that extrapolates 
from the human judgements of the sample to the 
entire test set. We illustrate this with the results 
for pollution. The human judge marked as 
incorrect usage 91.2% of the sample from 
ALEK's E-set and 18.4% of the sample from its 
C-set. To estimate overall incorrect usage, we 
computed a weighted mean of these two rates, 
where the weights reflected the proportion of 
sentences that were in the E-set and C-set. The 
E-set contained 8.3% of the pollution sentences 
and the C-set had the remaining 91.7%. With the 
human judgements as the gold standard, the 
estimated overall rate of incorrect usage is (.083 
x .912 + .917 x .184) = .245. ALEK's estimated 
recall is the proportion of sentences in the E-set 
times its precision, divided by the overall 
estimated error rate (.083 ? .912) / .245 = .310. 
144 
The precision results vary from word to word. 
Conclusion and pollution have precision in the 
low to middle 90's while individual's precision 
is 57%. Overall, ALEK's predictions are about 
78% accurate. The recall is limited in part by the 
fact that the system only looks at syntactic 
information, while many of the errors are 
semantic. 
3.2 Analysis of  Hits and Misses 
Nicholls (1999) identifies four error types: an 
unnecessary word (*affect o their emotions), a 
missing word (*opportunity of job.), a word or 
phrase that needs replacing (*every jobs), a word 
used in the wrong form (*pollutions). ALEK 
recognizes all of these types of errors. For closed 
class words, ALEK identified whether a word 
was missing, the wrong word was used (choice), 
and when an extra word was used. Open class 
words have a fourth error category, form, 
including inappropriate compounding and verb 
agreement. During the development stage, we 
found it useful to add additional error categories. 
Since TEOFL graders are not supposed to take 
punctuation i to account, punctuation errors 
were only marked when they caused the judge to 
"garden path" or initially misinterpret the 
sentence. Spelling was marked either when a 
function word was misspelled, causing part-of- 
speech tagging errors, or when the writer's 
intent was unclear. 
The distributions of categories for hits and 
misses, shown in Table 4, are not strikingly 
different. However, the hits are primarily 
syntactic in nature while the misses are both 
semantic (as in open-class:choice) and syntactic 
(as in closed-class:missing). 
ALEK is sensitive to open-class word 
confusions (affect vs effect) where the part of 
speech differs or where the target word is 
confused with another word (*ln this aspect,... 
instead ofln this respect, ...). In both cases, the 
system recognizes that the target is in the wrong 
syntactic environment. Misses can also be 
syntactic - when the target word is confused 
with another word but the syntactic environment 
fails to trigger an error. In addition, ALEK does 
not recognize semantic errors when the error 
involves the misuse of an open-class word in 
Category % Hits % Misses 
Closed-class - choice 22.5 15.5 
--extra 15.5 13.0 
-missing 
.Open-class - choice 
8.0 8.5 
12.0 19.0 
- extra .5 1.0 
- missing 15 
- form 
1.5 
28.0 28.5 
Punctuation 5.5 1.5 
1.5 
5.5 
Sentence fragment 
Spelling/typing error 
Word order .5 
2.0 
8.5 
1.0 
Table 4: Hits and misses based on a random sample 
of 200 hits and 200 misses 
combination with the target (for example, make 
in "*they make benefits"). 
Closed class words typically are either selected 
by or agree with a head word. So why are there 
so many misses, especially with prepositions? 
The problem is caused in part by polysemy - 
when one sense of the word selects apreposition 
that another sense does not. When concentrate is
used spatially, it selects the preposition i , as 
"the stores were concentrated in the downtown 
area". When it denotes mental activity, it selects 
the preposition on, as in "Susan concentrated on
her studies". Since ALEK trains on all senses of 
concentrate, it does not detect he error in 
"*Susan concentrated in her studies". Another 
cause is that adjuncts, especially temporal and 
locative adverbials, distribute freely in the word- 
specific corpora, as in "Susan concentrated in 
her room." This second problem is more 
tractable than the polysemy problem - and 
would involve training the system to recognize 
certain types of adjuncts. 
3.3 Analysis of  False Positives 
False positives, when ALEK "identifies" an 
error where none exists, fall into six major 
categories. The percentage of each false positive 
type in a random sample of 200 false positives is 
shown in Table 5. 
Domain mismatch: Mismatch of the 
newspaper-domain word-specific corpora nd 
essay-domain test corpus. One notable 
difference is that some TOEFL essay prompts 
call for the writer's opinion. Consequently, 
145 
Error Type % Occurrence 
Domain mismatch 12.5 
17.0 Tagger 
Syntactic 
Free distribution 
14.5 
16.5 
Punctuation 12.0 
Infrequent tags 
Other 
9.0 
18.5 
Table 5. Distribution of false positive types 
TOEFL essays often contain first person 
references, whereas newspaper a ticles are 
written in the third person. We need to 
supplement the word-specific corpora with 
material that more closely resembles the test 
corpus. 
Tagger: Incorrect analysis by the part-of-speech 
tagger. When the part-of-speech tag is wrong, 
ALEK often recognizes the resulting n-gram as 
anomalous. Many of these errors are caused by 
training on the Brown corpus instead of a corpus 
of essays. 
Syntactic analysis: Errors resulting from using 
part-of-speech tags instead of supertags or a full 
parse, which would give syntactic relations 
between constituents. For example, ALEK false 
alarms on arguments of ditransitive verbs such 
as offer and flags as an error "you benefits" in 
"offers you benefits". 
Free distribution: Elements that distribute 
freely, such as adverbs and conjunctions, as well 
as temporal and locative adverbial phrases, tend 
to be identified as errors when they occur in 
some positions. 
Punctuation: Most notably omission of periods 
and commas. Since these errors are not 
indicative of one's ability to use the target word, 
they were not considered as errors unless they 
caused the judge to misanalyze the sentence. 
Infrequent tags. An undesirable r sult of our 
"enriched" tag set is that some tags, e.g., the 
post-determiner last, occur too infrequently in
the corpora to provide reliable statistics. 
Solutions to some of these problems will clearly 
be more tractable than to others. 
4 Comparison of Results 
Comparison of these results to those of other 
systems i  difficult because there is no generally 
accepted test set or performance baseline. Given 
this limitation, we compared ALEK's 
performance toa widely used grammar checker, 
the one incorporated in Microsoft's Word97. We 
created files of sentences used for the three 
development words concentrate, interest, and 
knowledge, and manually corrected any errors 
outside the local context around the target before 
checking them with Word97. The performance 
for concentrate showed overall precision of 0.89 
and recall of 0.07. For interest, precision was 
0.85 with recall of 0.11. In sentences containing 
knowledge, precision was 0.99 and recall was 
0.30. Word97 correctly detected the 
ungrammaticality ofknowledges as well as a 
knowledge, while it avoided flagging a
knowledge of. 
In summary, Word97's precision in error 
detection is impressive, but the lower recall 
values indicate that it is responding tofewer 
error types than does ALEK. In particular, 
Word97 is not sensitive to inappropriate 
selection of prepositions for these three words 
(e.g., *have knowledge on history, *to 
concentrate at science). Of course, Word97 
detects many kinds of errors that ALEK does 
not. 
Research as been reported on grammar 
checkers pecifically designed for an ESL 
population. These have been developed by hand, 
based on small training and test sets. Schneider 
and McCoy (1998) developed a system tailored 
to the error productions of American Sign 
Language signers. This system was tested on 79 
sentences containing determiner and agreement 
errors, and 101 grammatical sentences. We 
calculate that their precision was 78% with 54% 
recall. Park, Palmer and Washburn (1997) 
adapted a categorial grammar to recognize 
"classes of errors \[that\] dominate" in the nine 
essays they inspected. This system was tested on 
eight essays, but precision and recall figures are 
not reported. 
5 Conclusion 
The unsupervised techniques that we have 
presented for inferring negative vidence are 
effective in recognizing rammatical errors in 
written text. 
146 
Preliminary results indicate that ALEK's error 
detection is predictive of TOEFL scores. If 
ALEK accurately detects usage errors, then it 
should report more errors in essays with lower 
scores than in those with higher scores. We have 
already seen in Table 1 that there is a negative 
correlation between essay score and two of 
ALEK's component measures, the general 
corpus n-grams. However, the data in Table 1 
were not based on specific vocabulary items and 
do not reflect overall system performance, which 
includes the other measures as well. 
Table 6 shows the proportion of test word 
occurrences that were classified by ALEK as 
containing errors within two positions of the 
target at each of 6 TOEFL score points. As 
predicted, the correlation is negative (rs = -1.00, 
n = 6, p < .001, two-tailed). These data support 
the validity of the system as a detector of 
inappropriate usage, even when only a limited 
number of words are targeted and only the 
immediate context of each target is examined. 
Score 
1 
2 
3 
4 
5 
6 
ALEK Human 
.091 . . . . .  
.085 .375 
.067 .268 
.057 .293 
.048 .232 
.041 .164 
Table 6: Proportion of  test word occurrences, by 
score point, classified as containing an error by 
ALEK and by a human judge 
For comparison, Table 6 also gives the estimated 
proportions of inappropriate usage by score 
point based on the human judge's classification. 
Here, too, there is a negative correlation: rs = 
-.90, n = 5, p < .05, two-tailed. 
Although the system recognizes a wide range of 
error types, as Table 6 shows, it detects only 
about one-fifth as many errors as a human judge 
does. To improve recall, research needs to focus 
on the areas identified in section 3.2 and, to 
improve precision, efforts should be directed at 
reducing the false positives described in 3.3. 
ALEK is being developed as a diagnostic tool 
for students who are learning English as a 
foreign language. However, its techniques could 
be incorporated into a grammar checker for 
native speakers. 
Acknowledgments 
We thank Susanne Wolff for evaluating the test 
sentences, and Robert Kantor, Ken Sheppard and 3 
anonymous reviewers for their helpful suggestions. 
References 
Brill, E. 1994. Some advances in rule-based part-of- 
speech tagging. Proceedings of the Twelfth 
National Conference on Artificial Intelligence, 
Seattle, AAAI. 
Choueka, Y. and S. Lusignan. 1985. Disambiguation 
by short contexts. Computers and the Humanities, 
19:147-158. 
Cohen, J. and P. Cohen. 1983. Applied Multiple 
Regression~Correlation Analysis for the 
Behavioral Sciences. Hillsdale, N J: Erlbaum. 
Francis, W. and H. Ku~era. 1982. Frequency 
Analysis of English Usage: Lexicon and Grammar. 
Boston, Houghton Mifflin. 
Golding, A. 1995. A Bayesian hybrid for context- 
sensitive spelling correction. Proceedings of the 3 ~a 
Workshop on Very Large Corpora. Cambridge, 
MA. 39--53. 
Kilgarriff, A. and M. Palmer. 2000. Introduction to 
the special issue on SENSEVAL. Computers and 
the Humanities, 34:1----2. 
Leacock, C., M. Chodorow and G.A. Miller. 1998. 
1998. Using corpus tatistics and WordNet's 
lexical relations for sense identification. 
Computational Linguistics, 24:1. 
Lin, D. 1998. Extracting collocations from text 
corpora. First Workshop on Computational 
Terminology. Montreal, Canada. 
Miller, G.A. and P. Gildea. 1987. How children learn 
words. Scientific American, 257. 
Nation, I.S.P. 1990. Teaching and learning 
vocabulary. New York: Newbury House. 
Nicholls, D. 1999. The Cambridge Learner Corpus -
Error coding and analysis. Summer Workshop on 
Learner Corpora. Tokyo 
Park, J.C., M. Palmer and G. Washburn. 1997. 
Checking rammatical mistakes for English-as-a- 
second-language (ESL) students. Proceedings of 
KSEA-NERC. New Brunswick, NJ. 
Schneider, D.A. and K.F. McCoy. 1998. Recognizing 
syntactic errors in the writing of second language 
learners. Proceedings of Coling-ACL-98, Montr6al. 
Yarowsky, D. 1993. One sense per collocation. 
Proceedings of the ARPA Workshop on Human 
Language Technology. San Francisco. Morgan 
Kaufman. 
147 
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 865?872
Manchester, August 2008
The Ups and Downs of Preposition Error Detection in ESL Writing
Joel R. Tetreault
Educational Testing Service
660 Rosedale Road
Princeton, NJ, USA
JTetreault@ets.org
Martin Chodorow
Hunter College of CUNY
695 Park Avenue
New York, NY, USA
martin.chodorow@hunter.cuny.edu
Abstract
In this paper we describe a methodology
for detecting preposition errors in the writ-
ing of non-native English speakers. Our
system performs at 84% precision and
close to 19% recall on a large set of stu-
dent essays. In addition, we address the
problem of annotation and evaluation in
this domain by showing how current ap-
proaches of using only one rater can skew
system evaluation. We present a sampling
approach to circumvent some of the issues
that complicate evaluation of error detec-
tion systems.
1 Introduction
The long-term goal of our work is to develop a
system which detects errors in grammar and us-
age so that appropriate feedback can be given to
non-native English writers, a large and growing
segment of the world?s population. Estimates are
that in China alone as many as 300 million peo-
ple are currently studying English as a second lan-
guage (ESL). Usage errors involving prepositions
are among the most common types seen in the
writing of non-native English speakers. For ex-
ample, (Izumi et al, 2003) reported error rates for
English prepositions that were as high as 10% in
a Japanese learner corpus. Errors can involve in-
correct selection (?we arrived to the station?), ex-
traneous use (?he went to outside?), and omission
(?we are fond null beer?). What is responsible
for making preposition usage so difficult for non-
native speakers?
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
At least part of the difficulty seems to be due to
the great variety of linguistic functions that prepo-
sitions serve. When a preposition marks the ar-
gument of a predicate, such as a verb, an ad-
jective, or a noun, preposition selection is con-
strained by the argument role that it marks, the
noun which fills that role, and the particular predi-
cate. Many English verbs also display alternations
(Levin, 1993) in which an argument is sometimes
marked by a preposition and sometimes not (e.g.,
?They loaded the wagon with hay? / ?They loaded
hay on the wagon?). When prepositions introduce
adjuncts, such as those of time or manner, selec-
tion is constrained by the object of the preposition
(?at length?, ?in time?, ?with haste?). Finally, the
selection of a preposition for a given context also
depends upon the intended meaning of the writer
(?we sat at the beach?, ?on the beach?, ?near the
beach?, ?by the beach?).
With so many sources of variation in English
preposition usage, we wondered if the task of se-
lecting a preposition for a given context might
prove challenging even for native speakers. To
investigate this possibility, we randomly selected
200 sentences from Microsoft?s Encarta Encyclo-
pedia, and, in each sentence, we replaced a ran-
domly selected preposition with a blank line. We
then asked two native English speakers to perform
a cloze task by filling in the blank with the best
preposition, given the context provided by the rest
of the sentence. Our results showed only about
75% agreement between the two raters, and be-
tween each of our raters and Encarta.
The presence of so much variability in prepo-
sition function and usage makes the task of the
learner a daunting one. It also poses special chal-
lenges for developing and evaluating an NLP error
detection system. This paper addresses both the
865
development and evaluation of such a system.
First, we describe a machine learning system
that detects preposition errors in essays of ESL
writers. To date there have been relatively few
attempts to address preposition error detection,
though the sister task of detecting determiner er-
rors has been the focus of more research. Our sys-
tem performs comparably with other leading sys-
tems. We extend our previous work (Chodorow et
al., 2007) by experimenting with combination fea-
tures, as well as features derived from the Google
N-Gram corpus and Comlex (Grishman et al,
1994).
Second, we discuss drawbacks in current meth-
ods of annotating ESL data and evaluating error
detection systems, which are not limited to prepo-
sition errors. While the need for annotation by
multiple raters has been well established in NLP
tasks (Carletta, 1996), most previous work in error
detection has surprisingly relied on only one rater
to either create an annotated corpus of learner er-
rors, or to check the system?s output. Some gram-
matical errors, such as number disagreement be-
tween subject and verb, no doubt show very high
reliability, but others, such as usage errors involv-
ing prepositions or determiners are likely to be
much less reliable. Our results show that relying
on one rater for system evaluation can be problem-
atic, and we provide a sampling approach which
can facilitate using multiple raters for this task.
In the next section, we describe a system that
automatically detects errors involving incorrect
preposition selection (?We arrived to the station?)
and extraneous preposition usage (?He went to
outside?). In sections 3 and 4, we discuss the
problem of relying on only one rater for exhaus-
tive annotation and show how multiple raters can
be used more efficiently with a sampling approach.
Finally, in section 5 we present an analysis of com-
mon preposition errors that non-native speakers
make.
2 System
2.1 Model
We have used a Maximum Entropy (ME) classi-
fier (Ratnaparkhi, 1998) to build a model of correct
preposition usage for 34 common English prepo-
sitions. The classifier was trained on 7 million
preposition contexts extracted from parts of the
MetaMetrics Lexile corpus that contain textbooks
and other materials for high school students. Each
context was represented by 25 features consisting
of the words and part-of-speech (POS) tags found
in a local window of +/- two positions around the
preposition, plus the head verb of the preceding
verb phrase (PV), the head noun of the preceding
noun phrase (PN), and the head noun of the follow-
ing noun phrase (FH), among others. In analyz-
ing the contexts, we used only tagging and heuris-
tic phrase-chunking, rather than parsing, so as to
avoid problems that a parser might encounter with
ill-formed non-native text
1
. In test mode, the clas-
sifier was given the context in which a preposition
occurred, and it returned a probability for each of
the 34 prepositions.
2.2 Other Components
While the ME classifier constitutes the core of the
system, it is only one of several processing com-
ponents that refines or blocks the system?s output.
Since the goal of an error detection system is to
provide diagnostic feedback to a student, typically
a system?s output is heavily constrained so that it
minimizes false positives (i.e., the system tries to
avoid saying a writer?s preposition is used incor-
rectly when it is actually right), and thus does not
mislead the writer.
Pre-Processing Filter: A pre-processing pro-
gram skips over preposition contexts that contain
spelling errors. Classifier performance is poor in
such cases because the classifier was trained on
well-edited text, i.e., without misspelled words. In
the context of a diagnostic feedback and assess-
ment tool for writers, a spell checker would first
highlight the spelling errors and ask the writer to
correct them before the system analyzed the prepo-
sitions.
Post-Processing Filter: After the ME clas-
sifier has output a probability for each of the 34
prepositions but before the system has made its fi-
nal decision, a series of rule-based post-processing
filters block what would otherwise be false posi-
tives that occur in specific contexts. The first filter
prevents the classifier from marking as an error a
case where the classifier?s most probable preposi-
tion is an antonym of what the writer wrote, such
as ?with/without? and ?from/to?. In these cases,
resolution is dependent on the intent of the writer
and thus is outside the scope of information cap-
1
For an example of a common ungrammatical sentence
from our corpus, consider: ?In consion, for some reasons,
museums, particuraly known travel place, get on many peo-
ple.?
866
tured by the current feature set. Another problem
for the classifier involves differentiating between
certain adjuncts and arguments. For example, in
the sentence ?They described a part for a kid?, the
system?s top choices were of and to. The benefac-
tive adjunct introduced by for is difficult for the
classifier to learn, perhaps because it so freely oc-
curs in many locations within a sentence. A post-
processing filter prevents the system from marking
as an error a prepositional phrase that begins with
for and has an object headed by a human noun (a
WordNet hyponym of person or group).
Extraneous Use Filter: To cover extraneous
use errors, we developed two rule-based filters:
1) Plural Quantifier Constructions, to handle cases
such as ?some of people? and 2) Repeated Prepo-
sitions, where the writer accidentally repeated the
same preposition two or more times, such as ?can
find friends with with?. We found that extrane-
ous use errors usually constituted up to 18% of all
preposition errors, and our extraneous use filters
handle a quarter of that 18%.
Thresholding: The final step for the preposi-
tion error detection system is a set of thresholds
that allows the system to skip cases that are likely
to result in false positives. One of these is where
the top-ranked preposition and the writer?s prepo-
sition differ by less than a pre-specified amount.
This was also meant to avoid flagging cases where
the system?s preposition has a score only slightly
higher than the writer?s preposition score, such as:
?My sister usually gets home around 3:00? (writer:
around = 0.49, system: by = 0.51). In these cases,
the system?s and the writer?s prepositions both fit
the context, and it would be inappropriate to claim
the writer?s preposition was used incorrectly. An-
other system threshold requires that the probabil-
ity of the writer?s preposition be lower than a pre-
specified value in order for it to be flagged as an
error. The thresholds were set so as to strongly fa-
vor precision over recall due to the high number of
false positives that may arise if there is no thresh-
olding. This is a tactic also used for determiner
selection in (Nagata et al, 2006) and (Han et al,
2006). Both thresholds were empirically set on a
development corpus.
2.3 Combination Features
ME is an attractive choice of machine learning al-
gorithm for a problem as complex as preposition
error detection, in no small part because of the
availability of ME implementations that can han-
dle many millions of training events and features.
However, one disadvantage of ME is that it does
not automatically model the interactions among
features as some other approaches do, such as sup-
port vector machines (Jurafsky and Martin, 2008).
To overcome this, we have experimented with aug-
menting our original feature set with ?combination
features? which represent richer contextual struc-
ture in the form of syntactic patterns.
Table 1 (first column) illustrates the four com-
bination features used for the example context
?take our place in the line?. The p denotes a
preposition, so N-p-N denotes a syntactic context
where the preposition is preceded and followed
by a noun phrase. We use the preceding noun
phrase (PN) and following head (FH) from the
original feature set for the N-p-N feature. Column
3 shows one instantiation of combination features:
Combo:word. For the N-p-N feature, the cor-
responding Combo:word instantiation is ?place-
line? since ?place? is the PN and ?line? is the
FH. We also experimented with using combina-
tions of POS tags (Combo:tag) and word+tag com-
binations (Combo:word+tag). So for the example,
the Combo:tag N-p-N feature would be ?NN-NN?,
and the Combo:word+tag N-p-N feature would be
place NN+line NN (see the fourth column of Ta-
ble 1). The intuition with the Combo:tag features
is that the Combo:word features have the potential
to be sparse, and these capture more general pat-
terns of usage.
We also experimented with other features such
as augmenting the model with verb-preposition
preferences derived from Comlex (Grishman et al,
1994), and querying the Google Terabyte N-gram
corpus with the same patterns used in the combina-
tion features. The Comlex-based features did not
improve the model, and though the Google N-gram
corpus represents much more information than our
7 million event model, its inclusion improved per-
formance only marginally.
2.4 Evaluation
In our initial evaluation of the system we col-
lected a corpus of 8,269 preposition contexts,
error-annotated by two raters using the scheme de-
scribed in Section 3 to serve as a gold standard. In
this study, we focus on two of the three types of
preposition errors: using the incorrect preposition
and using an extraneous preposition. We compared
867
Class Components Combo:word Features Combo:tag Features
p-N FH line NN
N-p-N PN-FH place-line NN-NN
V-p-N PV-PN take-line VB-NN
V-N-p-N PV-PN-FH take-place-line VB-NN-NN
Table 1: Feature Examples for take our place in the line
different models: the baseline model of 25 features
and baseline with combination features added. The
precision and recall for the top performing mod-
els are shown in Table 2. These results do not in-
clude the extraneous use filter; this filter generally
increased precision by as much as 2% and recall
by as much as 5%.
Evaluation Metrics In the tasks of determiner
and preposition selection in well-formed, native
texts (such as (Knight and Chander, 1994), (Min-
nen et al, 2000), (Turner and Charniak, 2007) and
(Gamon et al, 2008)), the evaluation metric most
commonly used is accuracy. In these tasks, one
compares the system?s output on a determiner or
preposition to the gold standard of what the writer
originally wrote. However, in the tasks of deter-
miner and preposition error detection, precision
and recall are better metrics to use because one
is only concerned with a subset of the preposi-
tions (or determiners), those used incorrectly, as
opposed to all of them in the selection task. In
essence, accuracy has the problem of distorting
system performance.
Results The baseline system (described in
(Chodorow et al, 2007)) performed at 79.8% pre-
cision and 11.7% recall. Next we tested the differ-
ent combination models: word, tag, word+tag, and
all three. Surprisingly, three of the four combina-
tion models: tag, word+tag, all, did not improve
performance of the system when added to the
model, but using just the +Combo:word features
improved recall by 1%. We use the +Combo:word
model to test our sampling approach in section 4.
As a final test, we tuned our training corpus of
7 million events by removing any contexts with
unknown or misspelled words, and then retrained
the model. This ?purge? resulted in a removal
of nearly 200,000 training events. With this new
training corpus, the +Combo:tag feature showed
the biggest improvement over the baseline, with
an improvement in both precision (+2.3%) and re-
call (+2.4%) to 82.1% and 14.1% respectively (last
line of Table 2. While this improvement may seem
small, it is in part due to the difficulty of the prob-
lem, but also the high baseline system score that
was established in our prior work (Chodorow et
al., 2007).
It should be noted that with the inclusion
of the extraneous use filter, performance of the
+Combo:tag rose to 84% precision and close to
19% recall.
Model Precision Recall
Baseline 79.8% 11.7%
+Combo:word 79.8% 12.8%
+Combo:tag (with purge) 82.1% 14.1%
Table 2: Best System Results on Incorrect Selec-
tion Task
2.5 Related Work
Currently there are only a handful of approaches
that tackle the problem of preposition error detec-
tion in English learner texts. (Gamon et al, 2008)
used a language model and decision trees to de-
tect preposition and determiner errors in the CLEC
corpus of learner essays. Their system performs at
79% precision (which is on par with our system),
however recall figures are not presented thus mak-
ing comparison difficult. In addition, their eval-
uation differs from ours in that they also include
errors of omission, and their work focuses on the
top twelve most frequent prepositions, while ours
has greater coverage with the top 34. (Izumi et
al., 2003) and (Izumi et al, 2004) used an ME ap-
proach to classify different grammatical errors in
transcripts of Japanese interviews. They do not
present performance of prepositions specifically,
but overall performance for the 13 error types
they target reached 25% precision and 7% recall.
(Eeg-Olofsson and Knuttson, 2003) created a rule-
based approach to detecting preposition errors in
Swedish language learners (unlike the approaches
presented here, which focus on English language
learners), and their system performed at 25% ac-
curacy. (Lee and Seneff, 2006) used a language
model to tackle the novel problem of preposition
selection in a dialogue corpus. While their perfor-
mance results are quite high, 88% precision and
868
78% recall, it should be noted that their evaluation
was on a small corpus with a highly constrained
domain, and focused on a limited number of prepo-
sitions, thus making direct comparison with our
approach difficult.
Although our recall figures may seem low, es-
pecially when compared to other NLP tasks such
as parsing and anaphora resolution, this is really a
reflection of how difficult the task is. For example,
in the problem of preposition selection in native
text, a baseline using the most frequent preposition
(of) results in precision and recall of 26%. In addi-
tion, the cloze tests presented earlier indicate that
even in well-formed text, agreement between na-
tive speakers on preposition selection is only 75%.
In texts written by non-native speakers, rater dis-
agreement increases, as will be shown in the next
section.
3 Experiments with Multiple Raters
While developing an error detection system for
prepositions is certainly challenging, given the re-
sults from our work and others, evaluation also
poses a major challenge. To date, single human
annotation has typically been the gold standard for
grammatical error detection, such as in the work
of (Izumi et al, 2004), (Han et al, 2006), (Nagata
et al, 2006), (Eeg-Olofsson and Knuttson, 2003)
2
.
Another method for evaluation is verification ((Ga-
mon et al, 2008), where a human rater checks over
a system?s output. The drawbacks of this approach
are: 1. every time the system is changed, a rater
is needed to re-check the output, and 2. it is very
hard to estimate recall. What these two evaluation
methods have in common is that they side-step the
issue of annotator reliability.
In this section, we show how relying on only one
rater can be problematic for difficult error detec-
tion tasks, and in section 4, we propose a method
(?the sampling approach?) for efficiently evaluat-
ing a system that does not require the amount of
effort needed in the standard approach to annota-
tion.
3.1 Annotation
To create a gold-standard corpus of error anno-
tations for system evaluation, and also to deter-
mine whether multiple raters are better than one,
2
(Eeg-Olofsson and Knuttson, 2003) had a small evalua-
tion on 40 preposition contexts and it is unclear whether mul-
tiple annotators were used.
we trained two native English speakers with prior
NLP annotation experience to annotate preposition
errors in ESL text. The training was very exten-
sive: both raters were trained on 2000 preposi-
tion contexts and the annotation manual was it-
eratively refined as necessary. To summarize the
procedure, the two raters were shown sentences
randomly selected from student essays with each
preposition highlighted in the sentence. They
marked each context (?2-word window around the
preposition, plus the commanding verb) for gram-
mar and spelling errors, and then judged whether
the writer used an incorrect preposition, a correct
preposition, or an extraneous preposition. Finally,
the raters suggested prepositions that would best
fit the context, even if there were no error (some
contexts can license multiple prepositions).
3.2 Reliability
Each rater judged approximately 18,000 prepo-
sitions contexts, with 18 sets of 100 contexts
judged by both raters for purposes of comput-
ing kappa. Despite the rigorous training regimen,
kappa ranged from 0.411 to 0.786, with an overall
combined value of 0.630. Of the prepositions that
Rater 1 judged to be errors, Rater 2 judged 30.2%
to be acceptable. Conversely, of the prepositions
Rater 2 judged to be erroneous, Rater 1 found
38.1% acceptable. The kappa of 0.630 shows the
difficulty of this task and also shows how two
highly trained raters can produce very different
judgments. Details on our annotation and human
judgment experiments can be found in (Tetreault
and Chodorow, 2008).
Variability in raters? judgments translates to
variability of system evaluation. For instance, in
our previous work (Chodorow et al, 2007), we
found that when our system?s output was com-
pared to judgments of two different raters, there
was a 10% difference in precision and a 5% differ-
ence in recall. These differences are problematic
when evaluating a system, as they highlight the po-
tential to substantially over- or under-estimate per-
formance.
4 Sampling Approach
The results from the previous section motivate the
need for a more refined evaluation. They sug-
gest that for certain error annotation tasks, such as
preposition usage, it may not be appropriate to use
only one rater and that if one uses multiple raters
869
for error annotation, there is the possibility of cre-
ating an adjudicated set, or at least calculating the
variability of the system?s performance. However,
annotation with multiple raters has its own disad-
vantages as it is much more expensive and time-
consuming. Even using one rater to produce a
sizeable evaluation corpus of preposition errors is
extremely costly. For example, if we assume that
500 prepositions can be annotated in 4 hours us-
ing our annotation scheme, and that the base rate
for preposition errors is 10%, then it would take at
least 80 hours for a rater to find and mark 1000 er-
rors. In this section, we propose a more efficient
annotation approach to circumvent this problem.
4.1 Methodology
Figure 1: Sampling Approach Example
The sampling procedure outlined here is in-
spired by the one described in (Chodorow and Lea-
cock, 2000) for the task of evaluating the usage of
nouns, verbs and adjectives. The central idea is
to skew the annotation corpus so that it contains a
greater proportion of errors.
Here are the steps in the procedure:
1. Process a test corpus of sentences so that each
preposition in the corpus is labeled ?OK? or
?Error? by the system.
2. Divide the processed corpus into two sub-
corpora, one consisting of the system?s ?OK?
prepositions and the other of the system?s
?Error? prepositions. For the hypothetical
data in Figure 1, the ?OK? sub-corpus con-
tains 90% of the prepositions, and the ?Error?
sub-corpus contains the remaining 10%.
3. Randomly sample cases from each sub-
corpus and combine the samples into an an-
notation set that is given to a ?blind? human
rater. We generally use a higher sampling
rate for the ?Error? sub-corpus because we
want to ?enrich? the annotation set with a
larger proportion of errors than is found in the
test corpus as a whole. In Figure 1, 75% of
the ?Error? sub-corpus is sampled while only
16% of the ?OK? sub-corpus is sampled.
4. For each case that the human rater judges to
be an error, check to see which sub-corpus it
came from. If it came from the ?OK? sub-
corpus, then the case is a Miss (an error that
the system failed to detect). If it came from
the ?Error? sub-corpus, then the case is a Hit
(an error that the system detected). If the rater
judges a case to be a correct usage and it came
from the ?Error? sub-corpus, then it is a False
Positive (FP).
5. Calculate the proportions of Hits and FPs in
the sample from the ?Error? sub-corpus. For
the hypothetical data in Figure 1, these val-
ues are 600/750 = 0.80 for Hits, and 150/750
= 0.20 for FPs. Calculate the proportion of
Misses in the sample from the ?OK? sub-
corpus. For the hypothetical data, this is
450/1500 = 0.30 for Misses.
6. The values computed in step 5 are conditional
proportions based on the sub-corpora. To cal-
culate the overall proportions in the test cor-
pus, it is necessary to multiply each value
by the relative size of its sub-corpus. This
is shown in Table 3, where the proportion of
Hits in the ?Error? sub-corpus (0.80) is mul-
tiplied by the relative size of the ?Error? sub-
corpus (0.10) to produce an overall Hit rate
(0.08). Overall rates for FPs and Misses are
calculated in a similar manner.
7. Using the values from step 6, calculate Preci-
sion (Hits/(Hits + FP)) and Recall (Hits/(Hits
+ Misses)). These are shown in the last two
rows of Table 3.
Estimated Overall Rates
Sample Proportion * Sub-Corpus Proportion
Hits 0.80 * 0.10 = 0.08
FP 0.20 * 0.10 = 0.02
Misses 0.30 * 0.90 = 0.27
Precision 0.08/(0.08 + 0.02) = 0.80
Recall 0.08/(0.08 + 0.27) = 0.23
Table 3: Sampling Calculations (Hypothetical)
870
This method is similar in spirit to active learning
((Dagan and Engelson, 1995) and (Engelson and
Dagan, 1996)), which has been used to iteratively
build up an annotated corpus, but it differs from
active learning applications in that there are no it-
erative loops between the system and the human
annotator(s). In addition, while our methodology
is used for evaluating a system, active learning is
commonly used for training a system.
4.2 Application
Next, we tested whether our proposed sam-
pling approach provides good estimates of a sys-
tem?s performance. For this task, we used the
+Combo:word model to separate a large corpus
of student essays into the ?Error? and ?OK? sub-
corpora. The original corpus totaled over 22,000
prepositions which would normally take several
weeks for two raters to double annotate and then
adjudicate. After the two sub-corpora were propor-
tionally sampled, this resulted in an annotation set
of 752 preposition contexts (requiring roughly 6
hours for annotation), which is substantially more
manageable than the full corpus. We had both
raters work together to make judgments for each
preposition.
It is important to note that while these are not
the exact same essays used in the previous evalua-
tion of 8,269 preposition contexts, they come from
the same pool of student essays and were on the
same topics. Given these strong similarities, we
feel that one can compare scores between the two
approaches. The precision and recall scores for
both approaches are shown in Table 4 and are ex-
tremely similar, thus suggesting that the sampling
approach can be used as an alternative to exhaus-
tive annotation.
Precision Recall
Standard Approach 80% 12%
Sampling Approach 79% 14%
Table 4: Sampling Results
It is important with the sampling approach to use
appropriate sample sizes when drawing from the
sub-corpora, because the accuracy of the estimates
of hits and misses will depend upon the propor-
tion of errors in each sub-corpus as well as on the
sample sizes. The OK sub-corpus is expected to
have even fewer errors than the overall base rate,
so it is especially important to have a relatively
large sample from this sub-corpus. The compari-
son study described above used an OK sub-corpus
sample that was twice as large as the Error sub-
corpus sample (about 500 contexts vs. 250 con-
texts).
In short, the sampling approach is intended to
alleviate the burden on annotators when faced with
the task of having to rate several thousand errors
of a particular type in order to produce a sizeable
error corpus. On the other hand, one advantage
that exhaustive annotation has over the sampling
method is that it makes possible the comparison
of multiple systems. With the sampling approach,
one would have to resample and annotate for each
system, thus multiplying the work needed.
5 Analysis of Learner Errors
One aspect of automatic error detection that usu-
ally is under-reported is an analysis of the errors
that learners typically make. The obvious benefit
of this analysis is that it can focus development of
the system.
From our annotated set of preposition errors,
we found that the most common prepositions
that learners used incorrectly were in (21.4%), to
(20.8%) and of (16.6%). The top ten prepositions
accounted for 93.8% of all preposition errors in our
learner corpus.
Next, we ranked the common preposition ?con-
fusions?, the common mistakes made for each
preposition. The top ten most common confusions
are listed in Table 5, where null refers to cases
where no preposition is licensed (the writer used
an extraneous preposition). The most common of-
fenses were actually extraneous errors (see Table
5): using to and of when no preposition was li-
censed accounted for 16.8% of all errors.
It is interesting to note that the most common
usage errors by learners overwhelmingly involved
the ten most frequently occurring prepositions in
native text. This suggests that our effort to handle
the 34 most frequently occurring prepositions may
be overextended and that a system that is specif-
ically trained and refined on the top ten preposi-
tions may provide better diagnostic feedback to a
learner.
6 Conclusions
This paper has two contributions to the field of
error detection in non-native writing. First, we
discussed a system that detects preposition errors
with high precison (up to 84%) and is competitive
871
Writer?s Prep. Rater?s Prep. Frequency
to null 9.5%
of null 7.3%
in at 7.1%
to for 4.6%
in null 3.2%
of for 3.1%
in on 3.1%
of in 2.9%
at in 2.7%
for to 2.5%
Table 5: Common Preposition Confusions
with other leading methods. We used an ME
approach augmented with combination features
and a series of thresholds. This system is currently
incorporated in the Criterion writing evaluation
service. Second, we showed that the standard ap-
proach to evaluating NLP error detection systems
(comparing a system?s output with a gold-standard
annotation) can greatly skew system results when
the annotation is done by only one rater. However,
one reason why a single rater is commonly used
is that building a corpus of learner errors can be
extremely costly and time consuming. To address
this efficiency issue, we presented a sampling
approach that produces results comparable to
exhaustive annotation. This makes using multiple
raters possible since less time is required to
assess the system?s performance. While the work
presented here has focused on prepositions, the
arguments against using only one rater, and for
using a sampling approach generalize to other
error types, such as determiners and collocations.
Acknowledgements We would first like to
thank our two annotators Sarah Ohls and Waverly
VanWinkle for their hours of hard work. We would
also like to acknowledge the three anonymous re-
viewers and Derrick Higgins for their helpful com-
ments and feedback.
References
Carletta, J. 1996. Assessing agreement on classifica-
tion tasks: The kappa statistic. Computational Lin-
guistics, pages 249?254.
Chodorow, M. and C. Leacock. 2000. An unsupervised
method for detecting grammatical errors. In NAACL.
Chodorow, M., J. Tetreault, and N-R. Han. 2007. De-
tection of grammatical errors involving prepositions.
In Proceedings of the Fourth ACL-SIGSEM Work-
shop on Prepositions.
Dagan, I. and S. Engelson. 1995. Committee-based
sampling for training probabilistic classifiers. In
Proceedings of ICML, pages 150?157.
Eeg-Olofsson, J. and O. Knuttson. 2003. Automatic
grammar checking for second language learners - the
use of prepositions. In Nodalida.
Engelson, S. and I. Dagan. 1996. Minimizing manual
annotation cost in supervised training from corpora.
In Proceedings of ACL, pages 319?326.
Gamon, M., J. Gao, C. Brockett, A. Klementiev, W. B.
Dolan, D. Belenko, and L. Vanderwende. 2008. Us-
ing contextual speller techniques and language mod-
eling for esl error correction. In IJCNLP.
Grishman, R., C. Macleod, and A. Meyers. 1994.
Comlex syntax: Building a computational lexicon.
In COLING.
Han, N-R., M. Chodorow, and C. Leacock. 2006. De-
tecting errors in english article usage by non-native
speakers. Natural Language Engineering, 12:115?
129.
Izumi, E., K. Uchimoto, T. Saiga, T. Supnithi, and
H. Isahara. 2003. Automatic error detection in the
Japanese leaners? English spoken data. In ACL.
Izumi, E., K. Uchimoto, and H. Isahara. 2004. The
overview of the sst speech corpus of Japanese learner
English and evaluation through the experiment on
automatic detection of learners? errors. In LREC.
Jurafsky, D. and J. Martin. 2008. Speech and Language
Processing (2nd Edition). Prentice Hall. To Appear.
Knight, K. and I. Chander. 1994. Automated postedit-
ing of documents. In Conference on Artificial Intel-
ligence.
Lee, J. and S. Seneff. 2006. Automatic grammar cor-
rection for second-language learners. In Interspeech.
Levin, B. 1993. English verb classes and alternations:
a preliminary investigation. Univ. of Chicago Press.
Minnen, G., F. Bond, and A. Copestake. 2000.
Memory-based learning for article generation. In
CoNLL.
Nagata, R., A. Kawai, K. Morihiro, and N. Isu. 2006.
A feedback-augmented method for detecting errors
in the writing of learners of English. In Proceedings
of the ACL/COLING.
Ratnaparkhi, A. 1998. Maximum Entropy Models for
natural language ambiguity resolution. Ph.D. thesis,
University of Pennsylvania.
Tetreault, J. and M. Chodorow. 2008. Native Judg-
ments of non-native usage: Experiments in preposi-
tion error detection. In COLING Workshop on Hu-
man Judgments in Computational Linguistics.
Turner, J. and E. Charniak. 2007. Language modeling
for determiner selection. In HLT/NAACL.
872
Towards Automatic Classification of Discourse Elements in Essays 
 
Jill Burstein 
ETS Technologies 
MS 18E 
Princeton, NJ 08541 
USA 
Jburstein@ 
etstechnologies.com 
Daniel Marcu 
ISI/USC 
4676 Admiralty 
Way 
Marina del Rey, 
CA, USA 
Marcu@isi.edu 
Slava Andreyev  
ETS Technologies 
MS 18E 
Princeton, NJ 08541 
USA 
sandreyev@ 
etstechnologies.com 
Martin Chodorow 
Hunter College, The 
City University of 
 New York 
New York, NY USA 
Martin.chodorow@ 
hunter.cuny.edu 
 
 
Abstract 
Educators are interested in essay 
evaluation systems that include 
feedback about writing features that 
can facilitate the essay revision 
process. For instance, if the thesis 
statement of a student?s essay could be 
automatically identified, the student 
could then use this information to 
reflect on the thesis statement with 
regard to its quality, and its relationship 
to other discourse elements in the 
essay. Using a relatively small corpus 
of manually annotated data, we use 
Bayesian classification to identify 
thesis statements.  This method yields 
results that are much closer to human 
performance than the results produced 
by two baseline systems.  
 
1 Introduction 
 
Automated essay scoring technology can 
achieve agreement with a single human judge 
that is comparable to agreement between two 
single human judges (Burstein, et al1998; Foltz, 
et al1998; Larkey, 1998; and Page and 
Peterson, 1995). Unfortunately, providing 
students with just a score (grade) is insufficient 
for instruction. To help students improve their 
writing skills, writing evaluation systems need 
to provide feedback that is specific to each 
individual?s writing and that is applicable to 
essay revision. 
The factors that contribute to improvement 
of student writing include refined sentence 
structure, variety of appropriate word usage, and 
organizational structure. The improvement of  
organizational structure is believed to be critical 
in the essay revision process toward overall 
improvement of essay quality.  Therefore, it 
would be desirable to have a system that could 
indicate as feedback to students, the discourse 
elements in their essays. Such a system could 
present to students a guided list of questions to 
consider about the quality of the discourse.  
For instance, it has been suggested by writing 
experts that if the thesis statement1 of a student?s 
essay could be automatically provided, the 
student could then use this information to reflect 
on the thesis statement and its quality. In 
addition, such an instructional application could 
utilize the thesis statement to discuss other types 
of discourse elements in the essay, such as the 
relationship between the thesis statement and the 
conclusion, and the connection between the 
thesis statement and the main points in the 
essay.  In the teaching of writing, in order to 
facilitate the revision process, students are often 
presented with ?Revision Checklists.? A revision 
checklist is a list of questions posed to the 
student to help the student reflect on the quality 
of his or her writing. Such a list might pose 
questions such as: 
a) Is the intention of my thesis statement 
clear? 
                                                           
1
 A thesis statement is generally defined as the 
sentence that explicitly identifies the purpose of the 
paper or previews its main ideas. See the Literacy 
Education On-line (LEO) site at 
http://leo.stcloudstate.edu. 
 (Annotator 1) ?In my opinion student should do what they want to do because they feel everything 
and they can't have anythig they feel because they probably feel to do just because other people do it not they 
want it. 
(Annotator 2) I think doing what students want is good for them. I sure they want to achieve in the 
highest place but most of the student give up. They they don?t get what they want. To get what they want, they 
have to be so strong and take the lesson from their parents Even take a risk, go to the library, and study hard by 
doing different thing. 
Some student they do not get what they want because of their family. Their family might be careless 
about their children so this kind of student who does not get support, loving from their family might not get 
what he wants. He just going to do what he feels right away. 
So student need a support from their family they has to learn from them and from their background. I 
learn from my background I will be the first generation who is going to gradguate from university that is what I 
want.? 
 
Figure 1: Sample student essay with human annotations of thesis statements. 
 
b) Does my thesis statement respond 
directly to the essay question?  
c) Are the main points in my essay 
clearly stated? 
d) Do the main points in my essay relate 
to my original thesis statement?  
If these questions are expressed in general 
terms, they are of little help; to be useful, they 
need to be grounded and need to refer 
explicitly to the essays students write 
(Scardamalia and Bereiter, 1985; White 1994). 
The ability to automatically identify and 
present to students the discourse elements in 
their essays can help them focus and reflect on 
the critical discourse structure of the essays.  
In addition, the ability for the application to 
indicate to the student that a discourse element 
could not be located, perhaps due to the ?lack 
of clarity? of this element, could also be 
helpful. Assuming that such a capability was 
reliable, this would force the writer to think 
about the clarity of an intended discourse 
element, such as a thesis statement. 
Using a relatively small corpus of essay 
data where thesis statements have been 
manually annotated, we built a Bayesian 
classifier using the following features:  
sentence position; words commonly used in 
thesis statements; and discourse features, 
based on Rhetorical Structure Theory (RST) 
parses (Mann and Thompson, 1988 and 
Marcu, 2000).  Our results indicate that this 
classification technique may be used toward 
automatic identification of thesis statements in 
essays.  Furthermore, we show that this 
method generalizes across essay topics. 
 
2 What Are Thesis Statements? 
 
A thesis statement is defined as the sentence that 
explicitly identifies the purpose of the paper or 
previews its main ideas (see footnote 1). This 
definition seems straightforward enough, and 
would lead one to believe that even for people to 
identify the thesis statement in an essay would be 
clear-cut.  However, the essay in Figure 1 is a 
common example of the kind of first-draft writing 
that our system has to handle. Figure 1 shows a 
student response to the essay question:  
Often in life we experience a conflict in 
choosing between something we "want" to do 
and something we feel we "should" do.  In your 
opinion, are there any circumstances in which 
it is better for people to do what they  "want" to 
do rather than what they feel they "should" do?  
Support your position with evidence from your 
own experience or your observations of other 
people.  
The writing in Figure 1 illustrates one kind of 
challenge in automatic identification of discourse 
elements, such as thesis statements.  In this case, 
the two human annotators independently chose 
different text as the thesis statement (the two texts 
highlighted in bold and italics in Figure 1).  In this 
kind of first-draft writing, it is not uncommon for 
writers to repeat ideas, or express more than one 
general opinion about the topic, resulting in text 
that seems to contain multiple thesis statements. 
Before building a system that automatically 
identifies thesis statements in essays, we wanted to 
determine whether the task was well-defined. In 
collaboration with two writing experts, a simple 
discourse-based annotation protocol was 
developed to manually annotate discourse 
elements in essays for a single essay topic.  
This was the initial attempt to annotate essay 
data using discourse elements generally 
associated with essay structure, such as thesis 
statement, concluding statement, and topic 
sentences of the essay?s main ideas. The 
writing experts defined the characteristics of 
the discourse labels.  These experts then 
annotated 100 essay responses to one English 
Proficiency Test (EPT) question, called Topic 
B, using a PC-based interface implemented in 
Java. 
We computed the agreement between the 
two human annotators using the kappa 
coefficient (Siegel and Castellan, 1988), a 
statistic used extensively in previous empirical 
studies of discourse.  The kappa statistic 
measures pairwise agreement among a set of 
coders who make categorial judgments, 
correcting for chance expected agreement. 
The kappa agreement between the two 
annotators with respect to the thesis statement 
labels was 0.733 (N=2391, where 2391 
represents the total number of sentences 
across all annotated essay responses).  This 
shows high agreement based on research in 
content analysis (Krippendorff, 1980) that 
suggests that values of kappa higher than 0.8 
reflect very high agreement and values higher 
than 0.6 reflect good agreement.  The 
corresponding z statistic was 27.1, which 
reflects a confidence level that is much higher 
than 0.01, for which the corresponding z value 
is 2.32 (Siegel and Castellan, 1988). 
 In the early stages of our project, it was 
suggested to us that thesis statements reflect 
the most important sentences in essays.  In 
terms of summarization, these sentences 
would represent indicative, generic summaries 
(Mani and Maybury, 1999; Marcu, 2000). To 
test this hypothesis (and estimate the adequacy 
of using summarization technology for 
identifying thesis statements), we carried out 
an additional experiment. The same 
annotation tool was used with two different 
human judges, who were asked this time to 
identify the most important sentence of each 
essay. The agreement between human judges 
on the task of identifying summary sentences 
was significantly lower: the kappa was 0.603 
(N=2391). Tables 1a and 1b summarize the results 
of the annotation experiments. 
Table 1a shows the degree of agreement 
between human judges on the task of identifying 
thesis statements and generic summary sentences. 
The agreement figures are given using the kappa 
statistic and the relative precision (P), recall (R), 
and F-values (F), which reflect the ability of one 
judge to identify the sentences labeled as thesis 
statements or summary sentences by the other 
judge. The results in Table 1a show that the task of 
thesis statement identification is much better 
defined than the task of identifying important 
summary sentences. In addition, Table 1b indicates 
that there is very little overlap between thesis and 
generic summary sentences: just 6% of the 
summary sentences were labeled by human judges 
as thesis statement sentences. This strongly 
suggests that there are critical differences between 
thesis statements and summary sentences, at least 
in first-draft essay writing. It is possible that thesis 
statements reflect an intentional facet (Grosz and 
Sidner, 1986) of language, while summary 
sentences reflect a semantic one (Martin, 1992). 
More detailed experiments need to be carried out 
though before proper conclusions can be derived.  
Table 1a: Agreement between human judges on 
thesis and summary sentence identification. 
Metric Thesis 
Statements 
Summary 
Sentences 
Kappa 0.733 0.603 
P (1 vs. 2) 0.73 0.44 
R (1 vs. 2) 0.69 0.60 
F (1 vs. 2) 0.71 0.51 
 
Table 1b: Percent overlap between human labeled 
thesis statements and summary sentences. 
 Thesis statements  vs. 
Summary sentences 
Percent Overlap 0.06 
 
The results in Table 1a provide an estimate for 
an upper bound of a thesis statement identification 
algorithm. If one can build an automatic classifier 
that identifies thesis statements at recall and 
precision levels as high as 70%, the performance 
of such a classifier will be indistinguishable from 
the performance of humans. 
 
3 A Bayesian Classifier for 
Identifying Thesis Statements 
 
3.1 Description of the Approach 
 
We initially built a Bayesian classifier for 
thesis statements using essay responses to one 
English Proficiency Test (EPT) test question: 
Topic B.  
McCallum and Nigam (1998) discuss two 
probabilistic models for text classification that 
can be used to train Bayesian independence 
classifiers. They describe the multinominal 
model as being the more traditional approach 
for statistical language modeling (especially in 
speech recognition applications), where a 
document is represented by a set of word 
occurrences, and where probability estimates 
reflect the number of word occurrences in a 
document. In using the alternative, 
multivariate Bernoulli model, a document is 
represented by both the absence and presence 
of features. On a text classification task, 
McCallum and Nigam (1998) show that the 
multivariate Bernoulli model performs well 
with small vocabularies, as opposed to the 
multinominal model which performs better 
when larger vocabularies are involved.  
Larkey (1998) uses the multivariate Bernoulli 
approach for an essay scoring task, and her 
results are consistent with the results of 
McCallum and Nigam (1998) (see also Larkey 
and Croft (1996) for descriptions of additional 
applications). In Larkey (1998), sets of essays 
used for training scoring models typically 
contain fewer than 300 documents.  
Furthermore, the vocabulary used across these 
documents tends to be restricted.   
Based on the success of Larkey?s 
experiments, and McCallum and Nigam?s 
findings that the multivariate Bernoulli model 
performs better on texts with small 
vocabularies, this approach would seem to be 
the likely choice when dealing with data sets 
of essay responses. Therefore, we have 
adopted this approach in order to build a thesis 
statement classifier that can select from an 
essay the sentence that is the most likely 
candidate to be labeled as thesis statement.2   
                                                           
2
 In our research, we trained classifiers using a 
classical Bayes approach too, where two classifiers 
were built: a thesis classifier and a non-thesis 
In our experiments, we used three general 
feature types to build the classifier: sentence 
position; words commonly occurring in thesis 
statements; and RST labels from outputs generated 
by an existing rhetorical structure parser (Marcu, 
2000).  
We trained the classifier to predict thesis 
statements in an essay. Using the multivariate 
Bernoulli formula, below, this gives us the log 
probability that a sentence (S) in an essay belongs 
to the class (T) of sentences that are thesis 
statements.  We found that it helped performance 
to use a Laplace estimator to deal with cases where 
the probability estimates were equal to zero. 
 
i i
i ii
log(P(T | S)) =
log(P(T)) +
log(P(A | T) /P(A)),
log(P(A | T) /P(A )),
i
i
if S contains A
if S does not contain A
?????
?
 
 
In this formula, P(T) is the prior probability that a 
sentence is in class T, P(Ai|T) is the conditional 
probability of a sentence having feature Ai , given 
that the sentence is in T, and P(Ai) is the prior 
probability that a sentence contains feature Ai, 
P( iA |T) is the conditional probability that a 
sentence does not have feature Ai, given that it is 
in T, and P( iA ) is the prior probability that a 
sentence does not contain feature Ai.  
 
3.2 Features Used to Classify Thesis 
Statements 
3.2.1 Positional Feature 
We found that the likelihood of a thesis statement 
occurring at the beginning of essays was quite high 
in the human annotated data. To account for this, 
we used one feature that reflected the position of 
each sentence in an essay. 
                                                                                           
classifier. In the classical Bayes implementation, each 
classifier was trained only on positive feature evidence, 
in contrast to the multivariate Bernoulli approach that 
trains classifiers both on the absence and presence of 
features. Since the performance of the classical Bayes 
classifiers was lower than the performance of the 
Bernoulli classifier, we report here only the 
performance of the latter. 
 3.2.2 Lexical Features 
All words from human annotated thesis 
statements were used to build the Bayesian 
classifier. We will refer to these words as the 
thesis word list.  From the training data, a 
vocabulary list was created that included one 
occurrence of each word used in all resolved 
human annotations of thesis statements.  All 
words in this list were used as independent 
lexical features. We found that the use of 
various lists of stop words decreased the 
performance of our classifier, so we did not 
use them. 
3.2.3 Rhetorical Structure Theory 
Features 
According to RST (Mann and Thompson, 
1988), one can associate a rhetorical structure 
tree to any text. The leaves of the tree 
correspond to elementary discourse units and 
the internal nodes correspond to contiguous 
text spans. Each node in a tree is characterized 
by a status (nucleus or satellite) and a 
rhetorical relation, which is a relation that 
holds between two non-overlapping text 
spans.  The distinction between nuclei and 
satellites comes from the empirical 
observation that the nucleus expresses what is 
more essential to the writer?s intention than the 
satellite; and that the nucleus of a rhetorical 
relation is comprehensible independent of the 
satellite, but not vice versa.  When spans are 
equally important, the relation is multinuclear. 
Rhetorical relations reflect semantic, 
intentional, and textual relations that hold 
between text spans as is illustrated in Figure 2. 
For example, one text span may elaborate on 
another text span; the information in two text 
spans may be in contrast; and the information 
in one text span may provide background for 
the information presented in another text span. 
Figure 2 displays in the style of Mann and 
Thompson (1988) the rhetorical structure tree 
of a text fragment. In Figure 2, nuclei are 
represented using straight lines; satellites 
using arcs. Internal nodes are labeled with 
rhetorical relation names.  
We built RST trees automatically for each 
essay using the cue-phrase-based discourse parser 
of Marcu (2000). We then associated with each 
sentence in an essay a feature that reflected the 
status of its parent node (nucleus or satellite), and 
another feature that reflected its rhetorical relation. 
For example, for the last sentence in Figure 2 we 
associated the status satellite and the relation 
elaboration because that sentence is the satellite 
of an elaboration relation.  For sentence 2, we 
associated the status nucleus and the relation 
elaboration because that sentence is the nucleus 
of an elaboration relation.  
We found that some rhetorical relations 
occurred more frequently in sentences annotated as 
thesis statements. Therefore, the conditional 
probabilities for such relations were higher and 
provided evidence that certain sentences were 
thesis statements.  The Contrast relation shown in 
Figure 2, for example, was a rhetorical relation 
that occurred more often in thesis statements.  
Arguably, there may be some overlap between 
words in thesis statements, and rhetorical relations 
used to build the classifier. The RST relations, 
however, capture long distance relations between 
text spans, which are not accounted by the words 
in our thesis word list.  
  
3.3 Evaluation of the Bayesian classifier 
 
We estimated the performance of our system using 
a six-fold cross validation procedure. We 
partitioned the 93 essays that were labeled by both 
human annotators with a thesis statement into six 
groups. (The judges agreed that 7 of the 100 essays 
they annotated had no thesis statement.) We 
trained six times on 5/6 of the labeled data and 
evaluated the performance on the other 1/6 of the 
data. 
The evaluation results in Table 2 show the average 
performance of our classifier with respect to the 
resolved annotation (Alg. wrt. Resolved), using 
traditional recall (R), precision (P), and F-value (F) 
metrics. For purposes of comparison, Table 2 also 
shows the performance of two baselines: the 
random baseline    classifies    the     thesis   
statements  
 Figure 2:  Example of RST tree.
randomly; while the position baseline assumes 
that the thesis statement is given by the first 
sentence in each essay. 
Table 2: Performance of the thesis statement 
classifier.  
System vs. system P R F 
Random baseline 
wrt. Resolved 
0.06 0.05 0.06 
Position baseline wrt. 
Resolved 
0.26 0.22 0.24  
Alg. wrt. Resolved 0.55 0.46 0.50  
1 wrt. 2 0.73 0.69 0.71  
1 wrt. Resolved 0.77 0.78 0.78  
2 wrt. Resolved 0.68 0.74 0.71  
 
4 Generality of the Thesis Statement 
Identifier 
In commercial settings, it is crucial that a 
classifier such as the one discussed in Section 3 
generalizes across different test questions. New 
test questions are introduced on a regular basis; 
so it is important that a classifier that works well 
for a given data set works well for other data 
sets as well, without requiring additional 
annotations and training.  
For the thesis statement classifier it was 
important to determine whether the positional, 
lexical, and RST-specific features are topic 
independent, and thus generalizable to new test 
questions.  If so, this would indicate that we 
could annotate thesis statements across a number 
of topics, and re-use the algorithm on additional 
topics, without further annotation. We asked a 
writing expert to manually annotate the thesis 
statement in approximately 45 essays for 4 
additional test questions: Topics A, C, D and E.  
The annotator completed this task using the 
same interface that was used by the two 
annotators in Experiment 1.  
To test generalizability for each of the five 
EPT questions, the thesis sentences selected by a 
writing expert were used for building the 
classifier.  Five combinations of 4 prompts were 
used to build the classifier in each case, and the 
resulting classifier was then cross-validated on 
the fifth topic, which was treated as test data.  
To evaluate the performance of each of the 
classifiers, agreement was calculated for each 
?cross-validation? sample (single topic) by 
comparing the algorithm selection to our writing 
expert?s thesis statement selections.  For 
example, we trained on Topics A, C, D, and E, 
using the thesis statements selected manually.  
This classifier was then used to select, 
automatically, thesis statements for Topic B.  In 
the evaluation, the algorithm?s selection was 
compared to the manually selected set of thesis 
statements for Topic B, and agreement was 
calculated. Table 3 illustrates that in all but one 
case, agreement exceeds both baselines from 
Table 2.  In this set of manual annotations, the 
human judge almost always selected one 
sentence as the thesis statement.  This is why 
Precision, Recall, and the F-value are often 
equal in Table 3. 
Table 3: Cross-topic generalizability of the thesis 
statement classifier. 
Training 
Topics 
CV Topic P R  F  
ABCD   E 0.36 0.36 0.36 
ABCE   D 0.49 0.49 0.49 
ABDE   C 0.45 0.45 0.45 
ACDE   B 0.60 0.59 0.59 
BCDE   A 0.25 0.24 0.25 
Mean  0.43 0.43 0.43 
 5 Discussion and Conclusions 
 
The results of our experimental work indicate 
that the task of identifying thesis statements in 
essays is well defined. The empirical evaluation 
of our algorithm indicates that with a relatively 
small corpus of manually annotated essay data, 
one can build a Bayes classifier that identifies 
thesis statements with good accuracy. The 
evaluations also provide evidence that this 
method for automated thesis selection in essays 
is generalizable.  That is, once trained on a few 
human annotated prompts, it can be applied to 
other prompts given a similar population of 
writers, in this case, writers at the college 
freshman level.  The larger implication is that 
we begin to see that there are underlying 
discourse elements in essays that can be 
identified, independent of the topic of the test 
question. For essay evaluation applications this 
is critical since new test questions are 
continuously being introduced into on-line essay 
evaluation applications.  
Our results compare favorably with results 
reported by Teufel and Moens (1999) who also 
use Bayes classification techniques to identify 
rhetorical arguments such as aim and 
background in scientific texts, although the texts 
we are working with are extremely noisy. 
Because EPT essays are often produced for 
high-stake exams, under severe time constraints, 
they are often ungrammatical, repetitive, and 
poorly organized at the discourse level. 
Current investigations indicate that this 
technique can be used to reliably identify other 
essay-specific discourse elements, such as, 
concluding statements, main points of 
arguments, and supporting ideas.  In addition, 
we are exploring how we can use estimated 
probabilities as confidence measures of the 
decisions made by the system. If the confidence 
level associated with the identification of a 
thesis statement is low, the system would 
instruct the student that no explicit thesis 
statement has been found in the essay. 
 
Acknowledgements 
 
We would like to thank our annotation 
experts, Marisa Farnum, Hilary Persky, Todd 
Farley, and Andrea King. 
 
References 
 
Burstein, J., Kukich, K. Wolff, S. Lu, C. 
Chodorow, M, Braden-Harder, L. and Harris 
M.D. (1998). Automated Scoring Using A 
Hybrid Feature Identification Technique. 
Proceedings of ACL, 206-210. 
Foltz, P. W., Kintsch, W., and Landauer, T.. 
(1998). The Measurement of Textual Coherence 
with Latent Semantic Analysis. Discourse 
Processes, 25(2&3), 285-307. 
Grosz B. and Sidner, C. (1986). Attention, 
Intention, and the Structure of Discourse. 
Computational Linguistics, 12 (3), 175-204. 
Krippendorff K. (1980). Content Analysis: 
An Introduction to Its Methodology. Sage Publ. 
Larkey, L. and Croft, W. B. (1996).  
Combining Classifiers in Text Categorization. 
Proceedings of  SIGIR,  289-298. 
Larkey, L. (1998). Automatic Essay Grading 
Using Text Categorization Techniques.  
Proceedings of SIGIR, pages 90-95. 
Mani, I. and Maybury, M. (1999). Advances 
in Automatic Text Summarization. The MIT 
Press. 
Mann, W.C. and Thompson, S.A.(1988). 
Rhetorical Structure Theory: Toward a 
Functional Theory of Text Organization. Text 
8(3), 243?281. 
Martin, J. (1992). English Text. System and 
Structure. John Benjamin Publishers.  
 Marcu, D. (2000). The Theory and Practice 
of Discourse Parsing and Summarization. The 
MIT Press.  
McCallum, A. and Nigam, K. (1998). A 
Comparison of Event Models for Naive Bayes 
Text Classification. The AAAI-98 Workshop on 
"Learning for Text Categorization".  
Page, E.B. and Peterson, N. (1995). The 
computer moves into essay grading: updating 
the ancient test. Phi Delta Kappa, March, 561-
565. 
Scardamalia, M. and Bereiter, C. (1985). 
Development of Dialectical Processes in 
Composition.  In Olson, D. R., Torrance, N. and 
Hildyard, A. (eds), Literacy, Language, and 
Learning: The nature of consequences of 
reading and writing.  Cambridge University 
Press. 
Siegel S. and Castellan, N.J. (1988). 
Nonparametric Statistics for the Behavioral 
Sciences. McGraw-Hill. 
Teufel , S. and Moens, M. (1999). Discourse-
level argumentation in scientific articles. 
Proceedings of the ACL99 Workshop on 
Standards and Tools for Discourse Tagging. 
White E.M. (1994). Teaching and Assessing 
Writing. Jossey-Bass Publishers, 103-108. 
Proceedings of the 4th ACL-SIGSEM Workshop on Prepositions, pages 25?30,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Detection of Grammatical Errors Involving Prepositions
Martin Chodorow
Hunter College of CUNY
695 Park Avenue
New York, NY, 10021
mchodoro@hunter.cuny.edu
Joel R. Tetreault and Na-Rae Han
Educational Testing Services
Rosedale Road
Princeton, NJ, 08541
jtetreault|nzhan@ets.org
Abstract
This paper presents ongoing work on the de-
tection of preposition errors of non-native
speakers of English. Since prepositions
account for a substantial proportion of all
grammatical errors by ESL (English as a
Second Language) learners, developing an
NLP application that can reliably detect
these types of errors will provide an invalu-
able learning resource to ESL students. To
address this problem, we use a maximum
entropy classifier combined with rule-based
filters to detect preposition errors in a corpus
of student essays. Although our work is pre-
liminary, we achieve a precision of 0.8 with
a recall of 0.3.
1 Introduction
The National Clearinghouse for English Language
Acquisition (2002) estimates that 9.6% of the stu-
dents in the US public school population speak a
language other than English and have limited En-
glish proficiency. Clearly, there is a substantial and
increasing need for tools for instruction in English
as a Second Language (ESL).
In particular, preposition usage is one of the most
difficult aspects of English grammar for non-native
speakers to master. Preposition errors account for
a significant proportion of all ESL grammar errors.
They represented the largest category, about 29%,
of all the errors by 53 intermediate to advanced ESL
students (Bitchener et al, 2005), and 18% of all er-
rors reported in an intensive analysis of one Japanese
writer (Murata and Ishara, 2004). Preposition errors
are not only prominent among error types, they are
also quite frequent in ESL writing. Dalgish (1985)
analyzed the essays of 350 ESL college students
representing 15 different native languages and re-
ported that preposition errors were present in 18%
of sentences in a sample of text produced by writ-
ers from first languages as diverse as Korean, Greek,
and Spanish.
The goal of the research described here is to pro-
vide software for detecting common grammar and
usage errors in the English writing of non-native En-
glish speakers. Our work targets errors involving
prepositions, specifically those of incorrect preposi-
tion selection, such as arrive to the town, and those
of extraneous prepositions, as in most of people.
We present an approach that combines machine
learning with rule-based filters to detect preposition
errors in a corpus of ESL essays. Even though this
is work in progress, we achieve precision of 0.8 with
a recall of 0.3. The paper is structured as follows: in
the next section, we describe the difficulty in learn-
ing English preposition usage; in Section 3, we dis-
cuss related work; in Sections 4-7 we discuss our
methodology and evaluation.
2 Problem of Preposition Usage
Why are prepositions so difficult to master? Perhaps
it is because they perform so many complex roles. In
English, prepositions appear in adjuncts, they mark
the arguments of predicates, and they combine with
other parts of speech to express new meanings.
The choice of preposition in an adjunct is largely
constrained by its object (in the summer, on Friday,
25
at noon) and the intended meaning (at the beach,
on the beach, near the beach, by the beach). Since
adjuncts are optional and tend to be flexible in their
position in a sentence, the task facing the learner is
quite complex.
Prepositions are also used to mark the arguments
of a predicate. Usually, the predicate is expressed
by a verb, but sometimes it takes the form of an ad-
jective (He was fond of beer), a noun (They have
a thirst for knowledge), or a nominalization (The
child?s removal from the classroom). The choice of
the preposition as an argument marker depends on
the type of argument it marks, the word that fills the
argument role, the particular word used as the pred-
icate, and whether the predicate is a nominalization.
Even with these constraints, there are still variations
in the ways in which arguments can be expressed.
Levin (1993) catalogs verb alternations such as They
loaded hay on the wagon vs. They loaded the wagon
with hay, which show that, depending on the verb,
an argument may sometimes be marked by a prepo-
sition and sometimes not.
English has hundreds of phrasal verbs, consist-
ing of a verb and a particle (some of which are
also prepositions). To complicate matters, phrasal
verbs are often used with prepositions (i.e., give up
on someone; give in to their demands). Phrasal
verbs are particularly difficult for non-native speak-
ers to master because of their non-compositionality
of meaning, which forces the learner to commit them
to rote memory.
3 Related Work
If mastering English prepositions is a daunting task
for the second language learner, it is even more
so for a computer. To our knowledge, only three
other groups have attempted to automatically de-
tect errors in preposition usage. Eeg-Olofsson et al
(2003) used 31 handcrafted matching rules to detect
extraneous, omitted, and incorrect prepositions in
Swedish text written by native speakers of English,
Arabic, and Japanese. The rules, which were based
on the kinds of errors that were found in a training
set of text produced by non-native Swedish writers,
targeted spelling errors involving prepositions and
some particularly problematic Swedish verbs. In a
test of the system, 11 of 40 preposition errors were
correctly detected.
Izumi et al (2003) and (2004) used error-
annotated transcripts of Japanese speakers in an
interview-based test of spoken English to train a
maximum entropy classifier (Ratnaparkhi, 1998) to
recognize 13 different types of grammatical and lex-
ical errors, including errors involving prepositions.
The classifier relied on lexical and syntactic features.
Overall performance for the 13 error types reached
25.1% precision with 7.1% recall on an independent
test set of sentences from the same source, but the
researchers do not separately report the results for
preposition error detection. The approach taken by
Izumi and colleagues is most similar to the one we
have used, which is described in the next section.
More recently, (Lee and Seneff, 2006) used a
language model and stochastic grammar to replace
prepositions removed from a dialogue corpus. Even
though they reported a precision of 0.88 and recall
of 0.78, their evaluation was on a very restricted do-
main with only a limited number of prepositions,
nouns and verbs.
4 The Selection Model
A preposition error can be a case of incorrect prepo-
sition selection (They arrived to the town), use of a
preposition in a context where it is prohibited (They
came to inside), or failure to use a preposition in a
context where it is obligatory (e.g., He is fond this
book). To detect the first type of error, incorrect
selection, we have employed a maximum entropy
(ME) model to estimate the probability of each of
34 prepositions, based on the features in their lo-
cal contexts. The ME Principle says that the best
model will satisfy the constraints found in the train-
ing, and for those situations not covered in the train-
ing, the best model will assume a distribution of
maximum entropy. This approach has been shown
to perform well in combining heterogeneous forms
of evidence, as in word sense disambiguation (Rat-
naparkhi, 1998). It also has the desirable property of
handling interactions among features without having
to rely on the assumption of feature independence,
as in a Naive Bayesian model.
Our ME model was trained on 7 million ?events?
consisting of an outcome (the preposition that ap-
peared in the training text) and its associated con-
26
text (the set of feature-value pairs that accompa-
nied it). These 7 million prepositions and their con-
texts were extracted from the MetaMetrics corpus of
1100 and 1200 Lexile text (11th and 12th grade) and
newspaper text from the San Jose Mercury News.
The sentences were then POS-tagged (Ratnaparkhi,
1998) and then chunked into noun phrases and verb
phrases by a heuristic chunker.
The maximum entropy model was trained with
25 contextual features. Some of the features repre-
sented the words and tags found at specific locations
adjacent to the preposition; others represented the
head words and tags of phrases that preceded or fol-
lowed the preposition. Table 1 shows a subset of the
feature list.
Some features had only a few values while oth-
ers had many. PHR pre is the ?preceding phrase?
feature that indicates whether the preposition was
preceded by a noun phrase (NP) or a verb phrase
(VP). In the example in Table 2, the preposition
into is preceded by an NP. In a sentence that be-
gins After the crowd was whipped up into a frenzy
of anticipation, the preposition into is preceded by
a VP. There were only two feature#value pairs for
this feature: PHR pre#NP and PHR pre#VP. Other
features had hundreds or even thousands of differ-
ent values because they represented the occurrence
of specific words that preceded or followed a prepo-
sition. Any feature#value pairs which occurred with
very low frequency in the training (less than 10 times
in the 7 million contexts) were eliminated to avoid
the need for smoothing their probabilities. Lemma
forms of words were used as feature values to fur-
ther reduce the total number and to allow the model
to generalize across inflectional variants. Even after
incorporating these reductions, the number of val-
ues was still very large. As Table 1 indicates, TGR,
the word sequence including the preposition and the
two words to its right, had 54,906 different values.
Summing across all features, the model contained a
total of about 388,000 feature#value pairs. Table 2
shows an example of where some of the features are
derived from.
5 Evaluation on Grammatical Text
The model was tested on 18,157 preposition con-
texts extracted from 12 files randomly selected from
a portion of 1100 Lexile text (11th grade) that had
not been used for training. For each context, the
model predicted the probability of each preposi-
tion given the contextual representation. The high-
est probability preposition was then compared to
the preposition that had actually been used by the
writer. Because the test corpus consisted of pub-
lished, edited text, we assumed that this material
contained few, if any, errors. In this and subsequent
tests, the model was used to classify each context as
one of 34 classes (prepositions).
Results of the comparison between the classifier
and the test set showed that the overall proportion
of agreement between the text and the classifier was
0.69. The value of kappa was 0.64. When we ex-
amined the errors, we discovered that, frequently,
the classifier?s most probable preposition (the one
it assigned) differed from the second most probable
by just a few percentage points. This corresponded
to a situation in which two or more prepositions
were likely to be found in a given context. Con-
sider the context They thanked him for his consider-
ation this matter, where either of or in could fill
the blank. Because the classifier was forced to make
a choice in this and other close cases, it incurred a
high probability of making an error. To avoid this
situation, we re-ran the test allowing the classifier
to skip any preposition if its top ranked and sec-
ond ranked choices differed by less than a specified
amount. In other words, we permitted it to respond
only when it was confident of its decision. When
the difference between the first and second ranked
choices was 0.60 or greater, 50% of the cases re-
ceived no decision, but for the remaining half of the
test cases, the proportion of agreement was 0.90 and
kappa was 0.88. This suggests that a considerable
improvement in performance can be achieved by us-
ing a more conservative approach based on a higher
confidence level for the classifier.
6 Evaluation on ESL Essays
To evaluate the ME model?s suitability for analyzing
ungrammatical text, 2,000 preposition contexts were
extracted from randomly selected essays written on
ESL tests by native speakers of Chinese, Japanese,
and Russian. This set of materials was used to look
for problems that were likely to arise as a conse-
27
Feature Description No. of values with freq ? 10
BGL Bigram to left; includes preceding word and POS 23,620
BGR Bigram to right; includes following word and POS 20,495
FH Headword of the following phrase 19,718
FP Following phrase 40,778
PHR pre Preceding phrase type 2
PN Preceding noun 18,329
PNMod Adjective modifying preceding noun 3,267
PNP Preceding noun phrase 29,334
PPrep Preceding preposition 60
PV Preceding verb 5,221
PVP Preceding verb phrase 23,436
PVtag POS tag of the preceding verb 24
PVword Lemma of the preceding verb 5,221
PW Lemma of the preceding word 2,437
TGL Trigram to left; includes two preceding words and POS 44,446
TGR Trigram to right; includes two following words and POS 54,906
Table 1: Some features used in ME Model
After whipping the crowd up into a frenzy of anticipation...
PVword PN PW FH
BGL BGR
??TGL?? ??TGR??
Table 2: Locations of some features in the local context of a preposition
quence of the mismatch between the training cor-
pus (edited, grammatical text) and the testing corpus
(ESL essays with errors of various kinds). When the
model was used to classify prepositions in the ESL
essays, it became obvious, almost immediately, that
a number of new performance issues would have to
be addressed.
The student essays contained many misspelled
words. Because misspellings were not in the train-
ing, the model was unable to use the features associ-
ated with them (e.g., FHword#frinzy) in its decision
making. The tagger was also affected by spelling
errors, so to avoid these problems, the classifier
was allowed to skip any context that contained mis-
spelled words in positions adjacent to the preposi-
tion or in its adjacent phrasal heads. A second prob-
lem resulted from punctuation errors in the student
writing. This usually took the form of missing com-
mas, as in I disagree because from my point of view
there is no evidence. In the training corpus, commas
generally separated parenthetical expressions, such
as from my point of view, from the rest of the sen-
tence. Without the comma, the model selected of
as the most probable preposition following because,
instead of from. A set of heuristics was used to lo-
cate common sites of comma errors and skip these
contexts.
There were two other common sources of clas-
sification error: antonyms and benefactives. The
model very often confused prepositions with op-
posite meanings (like with/without and from/to), so
when the highest probability preposition was an
antonym of the one produced by the writer, we
blocked the classifier from marking the usage as an
error. Benefactive phrases of the form for + per-
son/organization (for everyone, for my school) were
also difficult for the model to learn, most likely be-
cause, as adjuncts, they are free to appear in many
different places in a sentence and the preposition is
not constrained by its object, resulting in their fre-
quency being divided among many different con-
texts. When a benefactive appeared in an argument
position, the model?s most probable preposition was
generally not the preposition for. In the sentence
They described a part for a kid, the preposition of
has a higher probability. The classifier was pre-
vented from marking for + person/organization as
a usage error in such contexts.
To summarize, the classifier consisted of the ME
model plus a program that blocked its application
28
Rater 1 vs. Classifier vs. Classifier vs.
Rater 2 Rater 1 Rater 2
Agreement 0.926 0.942 0.934
Kappa 0.599 0.365 0.291
Precision N/A 0.778 0.677
Recall N/A 0.259 0.205
Table 3: Classifer vs. Rater Statistics
in cases of misspelling, likely punctuation errors,
antonymous prepositions, and benefactives. An-
other difference between the training corpus and the
testing corpus was that the latter contained grammat-
ical errors. In the sentence, This was my first experi-
ence about choose friends, there is a verb error im-
mediately following the preposition. Arguably, the
preposition is also wrong since the sequence about
choose is ill-formed. When the classifier marked the
preposition as incorrect in an ungrammatical con-
text, it was credited with correctly detecting a prepo-
sition error.
Next, the classifier was tested on the set of 2,000
preposition contexts, with the confidence threshold
set at 0.9. Each preposition in these essays was
judged for correctness of usage by one or two human
raters. The judged rate of occurrence of preposition
errors was 0.109 for Rater 1 and 0.098 for Rater 2,
i.e., about 1 out of every 10 prepositions was judged
to be incorrect. The overall proportion of agreement
between Rater1 and Rater 2 was 0.926, and kappa
was 0.599.
Table 3 (second column) shows the results for the
Classifier vs. Rater 1, using Rater 1 as the gold stan-
dard. Note that this is not a blind test of the clas-
sifier inasmuch as the classifier?s confidence thresh-
old was adjusted to maximize performance on this
set. The overall proportion of agreement was 0.942,
but kappa was only 0.365 due to the high level of
agreement expected by chance, as the Classifier used
the response category of ?correct? more than 97%
of the time. We found similar results when com-
paring the judgements of the Classifier to Rater 2:
agreement was high and kappa was low. In addition,
for both raters, precision was much higher than re-
call. As noted earlier, the table does not include the
cases that the classifier skipped due to misspelling,
antonymous prepositions, and benefactives.
Both precision and recall are low in these com-
parisons to the human raters. We are particularly
concerned about precision because the feedback that
students receive from an automated writing analy-
sis system should, above all, avoid false positives,
i.e., marking correct usage as incorrect. We tried to
improve precision by adding to the system a naive
Bayesian classifier that uses the same features found
in Table 1. As expected, its performance is not as
good as the ME model (e.g., precision = 0.57 and
recall = 0.29 compared to Rater 1 as the gold stan-
dard), but when the Bayesian classifier was given a
veto over the decision of the ME classifier, overall
precision did increase substantially (to 0.88), though
with a reduction in recall (to 0.16). To address the
problem of low recall, we have targeted another type
of ESL preposition error: extraneous prepositions.
7 Prepositions in Prohibited Contexts
Our strategy of training the ME classifier on gram-
matical, edited text precluded detection of extrane-
ous prepositions as these did not appear in the train-
ing corpus. Of the 500-600 errors in the ESL test set,
142 were errors of this type. To identify extraneous
preposition errors we devised two rule-based filters
which were based on analysis of the development
set. Both used POS tags and chunking information.
Plural Quantifier Constructions This filter ad-
dresses the second most common extraneous prepo-
sition error where the writer has added a preposi-
tion in the middle of a plural quantifier construction,
for example: some of people. This filter works by
checking if the target word is preceded by a quanti-
fier (such as ?some?, ?few?, or ?three?), and if the
head noun of the quantifier phrase is plural. Then, if
there is no determiner in the phrase, the target word
is deemed an extraneous preposition error.
Repeated Prepositions These are cases such as
people can find friends with with the same interests
where a preposition occurs twice in a row. Repeated
prepositions were easily screened by checking if the
same lexical item and POS tag were used for both
words.
These filters address two types of extraneous
preposition errors, but there are many other types
(for example, subcategorization errors, or errors
with prepositions inserted incorrectly in the begin-
ning of a sentence initial phrase). Even though these
filters cover just one quarter of the 142 extraneous
29
errors, they did improve precision from 0.778 to
0.796, and recall from 0.259 to 0.304 (comparing
to Rater 1).
8 Conclusions and Future Work
We have presented a combined machine learning
and rule-based approach that detects preposition er-
rors in ESL essays with precision of 0.80 or higher
(0.796 with the ME classifier and Extraneous Prepo-
sition filters; and 0.88 with the combined ME and
Bayesian classifiers). Our work is novel in that we
are the first to report specific performance results for
a preposition error detector trained and evaluated on
general corpora.
While the training for the ME classifier was done
on a separate corpus, and it was this classifier that
contributed the most to the high precision, it should
be noted that some of the filters were tuned on the
evaluation corpus. Currently, we are in the course
of annotating additional ESL essays for preposition
errors in order to obtain a larger-sized test set.
While most NLP systems are a balancing act be-
tween precision and recall, the domain of designing
grammatical error detection systems is distinguished
in its emphasis on high precision over high recall.
Essentially, a false positive, i.e., an instance of an er-
ror detection system informing a student that a usage
is incorrect when in fact it is indeed correct, must be
reduced at the expense of a few genuine errors slip-
ping through the system undetected. Given this, we
chose to set the threshold for the system so that it en-
sures high precision which in turn resulted in a recall
figure (0.3) that leaves us much room for improve-
ment. Our plans for future system development in-
clude:
1. Using more training data. Even a cursory ex-
amination of the training corpus reveals that there
are many gaps in the data. Seven million seems
like a large number of examples, but the selection
of prepositions is highly dependent on the presence
of other specific words in the context. Many fairly
common combinations of Verb+Preposition+Noun
or Noun+Preposition+Noun are simply not attested,
even in a sizable corpus. Consistent with this, there
is a strong correlation between the relative frequency
of a preposition and the classifier?s ability to predict
its occurrence in edited text. That is, prediction is
better for prepositions that have many examples in
the training set and worse for those with fewer ex-
amples. This suggests the need for much more data.
2. Combining classifiers. Our plan is to use the
output of the Bayesian model as an input feature for
the ME classifier. We also intend to use other classi-
fiers and let them vote.
3. Using semantic information. The ME
model in this study contains no semantic informa-
tion. One way to extend and improve its cover-
age might be to include features of verbs and their
noun arguments from sources such as FrameNet
(http://framenet.icsi.berkeley.edu/), which detail the
semantics of the frames in which many English
words appear.
References
J. Bitchener, S. Young, and D. Cameron. 2005. The ef-
fect of different types of corrective feedback on esl stu-
dent writing. Journal of Second Language Writing.
G. Dalgish. 1985. Computer-assisted esl research and
courseware development. Computers and Composi-
tion.
J. Eeg-Olofsson and O. Knuttson. 2003. Automatic
grammar checking for second language learners - the
use of prepositions. In Nodalida.
National Center for Educational Statistics. 2002. Public
school student counts, staff, and graduate counts by
state: School year 2000-2001.
E. Izumi, K. Uchimoto, T. Saiga, T. Supnithi, and H. Isa-
hara. 2003. Automatic error detection in the japanese
leaners? english spoken data. In ACL.
E. Izumi, K. Uchimoto, and H. Isahara. 2004. The
overview of the sst speech corpus of japanese learner
english and evaluation through the experiment on au-
tomatic detection of learners? errors. In LREC.
J. Lee and S. Seneff. 2006. Automatic grammar correc-
tion for second-language learners. In Interspeech.
B. Levin. 1993. English verb classes and alternations: a
preliminary investigation. Univ. of Chicago Press.
M. Murata and H. Ishara. 2004. Three english learner
assistance systems using automatic paraphrasing tech-
niques. In PACLIC 18.
A. Ratnaparkhi. 1998. Maximum Entropy Models for
natural language ambiguity resolution. Ph.D. thesis,
University of Pennsylvania.
30
Proceedings of the Third Linguistic Annotation Workshop, ACL-IJCNLP 2009, pages 60?63,
Suntec, Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Human Evaluation of Article and Noun Number Usage:
Influences of Context and Construction Variability
John Lee
Spoken Language Systems
MIT CSAIL
Cambridge, MA 02139, USA
jsylee@csail.mit.edu
Joel Tetreault
Educational Testing Service
Princeton, NJ 08541
jtetreault@ets.org
Martin Chodorow
Hunter College of CUNY
New York, NY 10021
martin.chodorow@
hunter.cuny.edu
Abstract
Evaluating systems that correct errors in
non-native writing is difficult because of
the possibility of multiple correct answers
and the variability in human agreement.
This paper seeks to improve the best prac-
tice of such evaluation by analyzing the
frequency of multiple correct answers and
identifying factors that influence agree-
ment levels in judging the usage of articles
and noun number.
1 Introduction
In recent years, systems have been developed with
the long-term goal of detecting, in the writing of
non-native speakers, usage errors involving arti-
cles, prepositions and noun number (Knight and
Chander, 1994; Minnen et al, 2000; Lee, 2004;
Han et al, 2005; Peng and Araki, 2005; Brockett
et al, 2006; Turner and Charniak, 2007). These
systems should, ideally, be evaluated on a cor-
pus of learners? writing, annotated with accept-
able corrections. However, since such corpora are
expensive to compile, many researchers have in-
stead resorted to measuring the accuracy of pre-
dicting what a native writer originally wrote in
well-formed text. This type of evaluation effec-
tively makes the assumption that there is one cor-
rect form of native usage per context, which may
not always be the case.
Two studies have already challenged this ?sin-
gle correct construction? assumption by compar-
ing the output of a system to the original text.
In (Tetreault and Chodorow, 2008), two human
judges were presented with 200 sentences and, for
each sentence, they were asked to select which
preposition (either the writer?s preposition, or the
system?s) better fits the context. In 28% of the
cases where the writer and the system differed, the
human raters found the system?s prediction to be
equal to or better than the writer?s original prepo-
sition. (Lee and Seneff, 2006) found similar re-
sults on the sentence level in a task that evaluated
many different parts of speech.
Percentage Article Number Example
42.5% null singular stone
22.7% the singular the stone
17.6% null plural stones
11.4% a/an singular a stone
5.7% the plural the stones
Table 1: Distribution of the five article-number
constructions of head nouns, based on 8 million
examples extracted from the MetaMetrics Lexile
Corpus. The various constructions are illustrated
with the noun ?stone?.
2 Research Questions
It is clear that using what the author wrote as
the gold standard can underestimate the sys-
tem?s performance, and that multiple correct an-
swers should be annotated. Using this annotation
scheme, however, raises two questions that have
not yet been thoroughly researched: (1) what is the
human agreement level on such annotation? (2)
what factors might influence the agreement level?
In this paper, we consider two factors: the context
of a word, and the variability of its usage.
In the two studies cited above, the human judges
were shown only the target sentence and did not
take into account any constraint on the choice of
word that might be imposed by the larger con-
text. For PP attachment, human performance im-
proves when given more context (Ratnaparkhi et
al., 1994). For other linguistic phenomena, such
as article/number selection for nouns, a larger con-
text window of at least several sentences may be
required, even though some automatic methods for
exploiting context have not been shown to boost
performance (Han et al, 2005).
The second factor, variability of usage, may be
60
Three years ago John Small, a sheep farmer in the Mendip
Hills, read an editorial in his local newspaper which claimed
that foxes never killed lambs. He drove down to the pa-
per?s office and presented [?], killed the night before,
to the editor.
NO-CONTEXT IN-CONTEXT
lamb: no no
a lamb: yes yes*
the lamb: yes no
lambs: yes yes
the lambs: yes no
Table 2: An example of a completed annotation
item.
expressed as the entropy of the distribution of the
word?s constructions. Table 1 shows the over-
all distribution of five article/number constructions
for head nouns, i.e. all permissible combinations
of number (singular or plural), and article (?a/an?,
?the?, or the ?null article?). A high entropy noun
such as ?stone? can appear freely in all of these, ei-
ther as a count noun or a non-count noun. This
contrasts with a low entropy noun such as ?pollu-
tion? which is mostly limited to two construction
types (?pollution? and ?the pollution?).
In this paper, we analyze the effects of varying
context and noun entropy on human judgments of
the acceptability of article-number constructions.
As a result of this study, we hope to advance the
best practice in annotation for evaluating error de-
tection systems. ?3 describes our annotation task.
In ?4, we test the ?single correct construction? as-
sumption for article and noun number. In ?5, we
investigate to what extent context and entropy con-
strain the range of acceptable constructions and in-
fluence the level of human agreement.
3 Annotation Design
3.1 Annotation Scheme
Two native speakers of English participated in
an annotation exercise, which took place in two
stages: NO-CONTEXT and IN-CONTEXT. Both
stages used a common set of sentences, each con-
taining one noun to be annotated. That noun was
replaced by the symbol [?], and the five possible
constructions, as listed in Table 1, were displayed
below the sentence to be judged.
In the NO-CONTEXT stage, only the sentence
in question and the five candidate constructions
(i.e., the bolded parts in Table 2) were shown to
the raters. They were asked to consider each of
the five constructions, and to select yes if it would
null a the
anaphoric not anaphoric
singular 2 2 2 2
plural 2 n/a 2 2
Table 3: For each noun, two sentences were se-
lected from each configuration of number, article
and anaphor.
yield a good sentence in some context, and no oth-
erwise1.
The IN-CONTEXT stage began after a few days?
break. The raters were presented with the same
sentences, but including the context, which con-
sisted of the five preceding sentences, some of
which are shown in Table 2. The raters were again
asked to select yes if the choice would yield a
good sentence given the context, and no other-
wise. Among the yes constructions, they were
asked to mark with an asterisk (yes*) the con-
struction(s) most likely to have been used in the
original text.
3.2 Annotation Example
In Table 2, ?lambs? are mentioned in the context,
but only in the generic sense. Therefore, the [?]
in the sentence must be indefinite, resulting in yes
for both ?a lamb? and ?lambs?. Of these two con-
structions, the singular was judged more likely to
have been the writer?s choice.
If the context is removed, then the [?] in the
sentence could be anaphoric, and so ?the lamb?
and ?the lambs? are also possible. Finally, regard-
less of context, the null singular ?lamb? is not ac-
ceptable.
3.3 Item Selection
All items were drawn from the Grade 10 material
in the 2.5M-sentence MetaMetrics Lexile corpus.
To avoid artificially inflating the agreement level,
we excluded noun phrases whose article or num-
ber can be predicted with very high confidence,
such as proper nouns, pronouns and non-count
nouns. Noun phrases with certain words, such as
non-article determiners (e.g., this car), possessive
pronouns (e.g., his car), cardinal numbers (e.g.,
one car) or quantifiers (e.g., some cars), also fall
into this category. Most of these preclude the arti-
cles a and the.
1Originally, a third response category was offered to the
rater to mark constructions that fell in a grey area between
yes and no. This category was merged with yes.
61
Rater NO-CONTEXT IN-CONTEXT
yes no yes no
R1 62.4% 37.6% 29.3% 70.7%
R2 51.8% 48.2% 39.2% 60.8%
Table 4: Breakdown of the annotations by rater
and by stage. See ?4 for a discussion.
Once these easy cases were filtered out, the head
nouns in the corpus were divided into five sets ac-
cording to their dominant construction. Each set
was then ranked according to the entropy of the
distribution of their constructions. Low entropy
typically means that there is one particular con-
struction whose frequency dwarfs the others?, such
as the singular definite for ?sun?. High entropy
means that the five constructions are more evenly
represented in the corpus; these are mostly generic
objects that can be definite or indefinite, singular
or plural, such as ?stone?. For each of the five
constructions, the three nouns with the highest en-
tropies, and three with the lowest, were selected.
This yielded a total of 15 ?high-entropy? and 15
?low-entropy? nouns.
For each noun, 14 sentences were drawn ac-
cording to the breakdown in Table 3, ensuring a
balanced representation of the article and num-
ber used in the original text, and the presence of
anaphoric references2. A total of 368 items3 were
generated.
4 Multiple Correct Constructions
We first establish the reliability of the annotation
by measuring agreement with the original text,
then show how and when multiple correct con-
structions can arise. All results in this section are
from the IN-CONTEXT stage.
Since the items were drawn from well-formed
text, each noun?s original construction should be
marked yes. The two raters assigned yes to the
original construction 80% and 95% of the time,
respectively. These can be viewed as the upper
bound of system performance if we assume there
can be only one correct construction. A stricter
ceiling can be obtained by considering how of-
ten the yes* constructions overlap with the orig-
2For practical reasons, we have restricted the study of con-
text to direct anaphoric references, i.e., where the same head
noun has already occurred in the context.
3In theory, there should be 420 items, but some of the con-
figurations in Table 3 are missing for certain nouns, mostly
the low-entropy ones.
NO-CONTEXT IN-CONTEXT
R1:? R2:? yes no yes no
yes 846 302 462 77
no 108 584 260 1041
Table 5: The confusion tables of the two raters for
the two stages.
inal one4. The yes* items overlapped with the
original 72% and 83% of the time, respectively.
These relatively high figures serve as evidence of
the quality of the annotation.
Both raters frequently found more than one
valid construction ? 18% of the time if only
considering yes*, and 49% if considering both
yes and yes*. The implication for auto-
matic system evaluation is that one could po-
tentially underestimate a system?s performance
by as much as 18%, if not more. For both
raters, the most frequent combinations of yes*
constructions were {null-plural,the-plural}, {a-
singular,the-singular}, {a-singular,null-plural},
and {the-singular,the-plural}. From the stand-
point of designing a grammar-checking system,
a system should be less confident in proposing
change from one construction to another within
the same construction pair.
5 Sources of Variation in Agreement
It is unavoidable for agreement levels to be af-
fected by how accepting or imaginative the in-
dividual raters are. In the NO-CONTEXT stage,
Rater 1 awarded more yes?s than Rater 2, per-
haps attributable to her ability to imagine suitable
contexts for some of the less likely constructions.
In the IN-CONTEXT stage, Rater 1 used yesmore
sparingly than Rater 2. This reflects their different
judgments on where to draw the line among con-
structions in the grey area between acceptable and
unacceptable.
We have identified, however, two other factors
that led to variations in the agreement level: the
amount of context available, and the distribution
of the noun itself in the English language. Careful
consideration of these factors should lead to better
agreement.
Availability of Context As shown in Table 4, for
both raters, the context sharply reduced the num-
ber of correct constructions. The confusion tables
4Both raters assigned yes* to an average of 1.2 construc-
tions per item.
62
for the two raters are shown in Table 5. For the
NO-CONTEXT stage, they agreed 78% of the time
and the kappa statistic was 0.55. When context is
provided, human judgment can be expected to in-
crease. Indeed, for the IN-CONTEXT stage, agree-
ment rose to 82% and kappa to 0.605.
Another kind of context ? previous mention
of the noun ? also increases agreement. Among
nouns originally constructed with ?the?, the kappa
statistics for those with direct anaphora was 0.63,
but only 0.52 for those without6.
Most previous research on article-number pre-
diction has only used features extracted from the
target sentence. These results suggest that using
features from a wider context should improve
performance.
Noun Construction Entropy For the low-entropy
nouns, we found a marked difference in human
agreement among the constructions depending on
their frequencies. For the most frequent construc-
tion in a noun?s distribution, the kappa was 0.78;
for the four remaining constructions, which are
much more rare, the kappa was only 0.527. They
probably constitute ?border-line? cases for which
the line between yes and no was often hard to
draw, leading to the lower kappa.
Entropy can thus serve as an additional factor
when a system decides whether or not to mark a
usage as an error. For low-entropy nouns, the sys-
tem should be more confident of predicting a fre-
quent construction, but more wary of suggesting
the other constructions.
6 Conclusions & Future Work
We conducted a human annotation exercise on ar-
ticle and noun number usage, making two obser-
vations that can help improve the evaluation pro-
cedure for this task. First, although the context
substantially reduces the range of acceptable an-
swers, there are still often multiple acceptable an-
swers given a context; second, the level of human
agreement is influenced by the availability of the
5This kappa value is on the boundary between ?moderate?
and ?substantial? agreement on the scale proposed in (Landis
and Koch, 1977). The difference between the kappa values
for the NO-CONTEXT and IN-CONTEXT stages approaches
statistical significance, z = 1.71, p < 0.10.
6The difference between these kappa values is statistically
significant, z = 2.06, p < 0.05.
7The two kappa values are significantly different, z =
4.35, p < 0.001.
context and the distribution of the noun?s construc-
tions.
These observations should help improve not
only the evaluation procedure but also the design
of error correction systems for articles and noun
number. Entropy, for example, can be incorpo-
rated into the estimation of a system?s confidence
in its prediction. More sophisticated contextual
features, beyond simply noting that a noun has
been previously mentioned (Han et al, 2005; Lee,
2004), can also potentially reduce uncertainty and
improve system performance.
Acknowledgments
We thank the two annotators, Sarah Ohls and Wa-
verely VanWinkle.
References
C. Brockett, W. Dolan, and M. Gamon. 2006. Cor-
recting ESL Errors using Phrasal SMT Techniques.
Proc. ACL.
N.-R. Han, M. Chodorow, and C. Leacock. 2005.
Detecting Errors in English Article Usage by Non-
Native Speakers. Natural Language Engineering,
1(1):1?15.
K. Knight and I. Chander. 1994. Automated Postedit-
ing of Documents. Proc. AAAI.
J. R. Landis and G. G. Koch. 1977. The Measurement
of Observer Agreement for Categorical Data. Bio-
metrics 33:159?174.
J. Lee. 2004. Automatic Article Restoration. Proc.
HLT-NAACL Student Research Workshop.
J. Lee and S. Seneff. 2006. Automatic Grammar Cor-
rection for Second-Language Learners. Proc. Inter-
speech.
G. Minnen, F. Bond, and A. Copestake. 2000.
Memory-based Learning for Article Generation.
Proc. CoNLL/LLL.
J. Peng and K. Araki. 2005. Correction of Arti-
cle Errors in Machine Translation Using Web-based
Model. Proc. IEEE NLP-KE.
A. Ratnaparkhi, J. Reynar, and S. Roukos. 1994. A
Maximum Entropy Model for Prepositional Phrase
Attachment. Proc. ARPA Workshop on Human Lan-
guage Technology.
J. Tetreault and M. Chodorow. 2008. Native Judg-
ments of Non-Native Usage. Proc. COLING Work-
shop on Human Judgements in Computational Lin-
guistics.
J. Turner and E. Charniak. 2007. Language Modeling
for Determiner Selection. Proc. HLT-NAACL.
63
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 950?961, Dublin, Ireland, August 23-29 2014.
Lexical Chaining for Measuring Discourse Coherence Quality in
Test-taker Essays
Swapna Somasundaran
Educational Testing Service
660 Rosedale Road
Princeton, NJ 08540
ssomasundaran@ets.org
Jill Burstein
Educational Testing Service
660 Rosedale Road
Princeton, NJ 08540
jburstein@ets.org
Martin Chodorow
Hunter College, CUNY
695 Park Avenue
New York, NY 10065
martin.chodorow@hunter.cuny.edu
Abstract
This paper presents an investigation of lexical chaining (Morris and Hirst, 1991) for measuring
discourse coherence quality in test-taker essays. We hypothesize that attributes of lexical chains,
as well as interactions between lexical chains and explicit discourse elements, can be harnessed
for representing coherence. Our experiments reveal that performance achieved by our new lexical
chain features is better than that of previous discourse features used for this task, and that the best
system performance is achieved when combining lexical chaining features with complementary
discourse features, such as those provided by a discourse parser based on rhetorical structure
theory, and features that reflect errors in grammar, word usage, and mechanics.
1 Introduction
Coherence, the reader?s ability to construct meaning from a document, is greatly influenced by the pres-
ence and organization of cohesive elements in the text (Halliday and Hasan, 1976; Moe, 1979). The
lexical chain (Morris and Hirst, 1991) is one such element. It consists of a sequence of related words that
contribute to the continuity of meaning based on word repetition, synonymy and similarity. In this paper
we explore how lexical chains can be employed to measure coherence in essays. Specifically, our goal
is to investigate how attributes of lexical chains can encode discourse coherence quality, such as adher-
ence to the essay topic, elaboration, usage of varied vocabulary, and sound organization of thoughts and
ideas. To do this, we build lexical chains and extract linguistically-motivated features from them. The
number of chains and their properties, such as length, density and link strength, can potentially reveal
discourse qualities related to focus and elaboration. In addition, features that capture the interactions
between chains and explicit discourse cues, such as transition words, can show if the cohesive elements
in text have been organized in a coherent fashion.
The main contributions of this paper are as follows: We use lexical chaining features to train a dis-
course coherence classifier on annotated essays from six different essay-writing tasks which differ in
essay genre and/or test-taker population. We then perform experiments to measure the effect of the fea-
tures when they are used alone and when they are combined with state-of-the-art features to classify the
coherence quality of essays. Our results indicate that lexical chaining features yield better results than
discourse features previously explored for this task and that the best performing feature combinations
contain lexical chaining features. We also show that lexical chaining features can improve system per-
formance across multiple genres of writing and populations. Our efforts result in the creation of a higher
performing state-of-the-art feature set for measuring coherence in test-taker writing.
The rest of the paper is organized as follows: In Section 2, we describe our intuitions about lexical
chains and how they can be used for measuring discourse coherence quality in essays. Section 3 describes
our data, and Section 4 describes our experiments in predicting discourse coherence quality. We discuss
related work in Section 5 and conclude in Section 6.
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
950
2 Lexical Chains and Discourse Coherence Quality
According to Morris and Hirst (1991), lexical cohesion is the result of chains of related words that con-
tribute to the continuity of lexical meaning. These sequences are characterized by the relations between
the words, as well as by their distance and density within a given span. Lexical chains do not stop at
sentence boundaries ? they can connect a pair of adjacent words or range over an entire text. Morris
and Hirst also observe that lexical chains tend to delineate portions of text that have a strong unity of
meaning. In this paper, we use this underlying principle of cohesion to detect the quality of coherence
in a discourse. Specifically, we employ lexical chains to quantify and represent expectations for coher-
ent discourse in test-taker essays. Presumably, violations of these expectations would indicate lack of
(or poor) coherence. We believe lexical chains have the potential to reveal the following characteristics
about discourse coherence in essays:
Text unity: Textual continuity is vital for the reader?s ability to construct meaning from the text (Halli-
day and Hasan, 1976). Coherent essays generally maintain focus over the main theme, so lexical chains
constructed over such essays will have chains representing the central topic running through most of the
length of the essay. These types of chains would presumably represent the main claim or position in
persuasive texts, the main object or person in descriptive texts, and the main story-line in narrative texts.
On the other hand, incoherent texts that jump from one topic to another, or do not adhere to a central
idea, will exhibit no chains or chains with very few member words.
Elaboration and Detailing: A function of elaboration in discourse is to overcome misunderstanding or
lack of understanding, and to enrich the understanding of the reader by expressing the same thought from
a different perspective (Hobbs, 1979). Good writers usually initiate topics, ideas or claims and provide
clear elaborations or reasons. That is, a sequence of many related words and phrases will be evoked to
explain an idea or provide an account of the writer?s reasoning. This development and detailing will be
exhibited by lexical chains with a good number of member words.
Variety: While cohesiveness is vital for coherence, too much repetition of the same word can, in fact,
harm the discourse quality (Witte and Faigley, 1981). Using a variety of words to express an idea or
elaborate on a topic is generally a characteristic of skillful writing. Lexical chains corresponding to such
writing will have a variety of similar words within the same chain.
Organization: In addition to cohesion (as represented by lexical chains in our case), one other factor
must be present for text to have coherence ? organization (Moe, 1979; Perfetti and Lesgold, 1977). Thus,
it is important to organize ideas using clear discourse transitions. Transitions from one topic to another,
or from a topic to its subtopic, should be clearly cued in order to assist the reader?s understanding of
the discourse. Consequently, in coherent writing, we would expect lexical chain patterns to synchronize
with discourse cues. For example, we would expect some chains to start after a new (sub) topic initiation
cue, such as ?Secondly? or ?Finally?, and at least some chains (corresponding to the previous topic) to
end immediately before the cue. Similarly, we would expect at least some chains to cross over discourse
cues indicating elaboration or reason (e.g. ?because?) due to topic continuity.
2.1 Features for Measuring Discourse Coherence
In order to measure discourse coherence quality, we create features based on attributes of lexical chains
extracted from essays. These features are then used to train a machine learning model, using essays
manually labeled for overall discourse coherence quality.
2.1.1 Lexical Chain Construction
Lexical chains in a text are composed of words and terms that are related. Based on Hirst and St-
Onge (1995), these relations can be exact repetitions, called extra-strong relations, close synonyms,
called strong relations, or words with weaker semantic relations, called medium-strong relations. We
implement the lexical chaining program described in Hirst and St. Onge (1995), where if a word or
phrase is potentially chainable, it is considered a candidate node for existing chains. First, an extra-
strong relation is sought throughout all existing chains, and if one is found, the word is added to it. If
not, strong relations are sought, but for these, the search scope is limited to the words in any chain that is
951
no farther away than the previous six sentences in the text; the search ends as soon as a strong relation is
found. Finally, if no relationship has yet been found, medium-strong relations are sought with the search
scope limited to words in chains that are no farther away than the previous three sentences. If the node
cannot be added to any existing chains, it forms its own single-node chain.
In this work, nouns are the focus of the lexical chains. Nouns, adjective-noun and noun-noun structures
are identified as potential chain participants. Lin?s thesaurus (Lin, 1998) is used to measure similarity
between words and phrases. Candidate pairs receiving similarity scores greater than 0.8 are considered
to have an extra-strong relationship (word repetition receives a similarity score of 1), pairs with similarity
greater than 0.172 are considered to have a strong relation, and pairs with similarity scores greater than
0.099 are considered to have a medium-strong relation. These thresholds were chosen after qualitative
inspection of a separate development data set of essays, and are also based on a previous finding (Burstein
et al., 2012) that 0.172 is the mean similarity value across different parts of speech in the Lin thesaurus.
We created two feature sets to capture the intuitions described above. The first set, LEX-1, encodes
the characteristics of text unity, elaboration and variety, while the second, LEX-2, encodes organization.
2.1.2 LEX-1 feature set
In order to capture text unity and detailing, we create features such as: total number of chains in the
essay, average chain size, number (and percentage) of large chains (chains having more than four nodes
are considered to be large chains
1
). As discussed previously, essays that show ample chaining might do
so because they adhere to themes and their development, while the presence of large, dense chains might
be an indicator that a topic is being discussed in detail. To represent variety, we employ features such
as number (and percentage) of chains that have a variety of words (chains containing more than one
word/phrase type are considered to have variety), as well as number (and percentage) of large chains
with a variety of words. To encode the characteristics of cohesive relationships, we look at the nature
of the links. Examples of these features are: number and percentage of each link type, number (and
percentage) of links of each type in large chains as well as in small chains. Corresponding to each
feature that uses counts (e.g. total number of chains) we also created normalized versions of the numbers
to account for the essay length. LEX-1 has a total of 38 features.
2.1.3 LEX-2 feature set
LEX-2 features capture the interactions between discourse transitions, indicated by explicit discourse
cues, and lexical chaining patterns. For this, we use a discourse cue tagger described in Burstein et al.
(1998) that was specifically developed for tagging discourse cues in the essay genre. Using patterns and
syntactic rules, the tagger automatically identifies words and phrases used as discourse cues, and assigns
them a discourse tag. Each tag has a primary component, indicating whether an argument (or topic) is
being initialized (arg-init) or developed (arg-dev), and a secondary component indicating the specific
type of discourse initialization (e.g. CLAIM, SUMMARY), or development (e.g. CLAIM, CONTRAST).
Examples of the discourse tags and their cues are: arg-init:SUMMARY (e.g. all in all, in conclusion,
in summary, overall), arg-init:TRANSITION (e.g. let us), arg-init:PARALLEL (e.g. firstly, similarly,
finally), arg-dev:CONTRAST (e.g. nonetheless, however, on the contrary, rather than, even if ), arg-
dev:EVIDENCE (e.g. because of, since), arg-dev:INFERENCE (e.g. as a result, consequently, there-
fore), arg-dev:DETAIL (e.g. as well as, in this case, in addition, such as), arg-dev:REFORMULATION
(e.g. in other words, that is).
For each discourse cue tagged in the text, we replace the cue with its tag and measure the number of
chains that (1) start after it, (2) end before it, and (3) continue over it (chains having nodes before and
after the tag). We create such features for the tags in the original form (e.g. arg-dev:DETAIL), as well
as for the primary component alone (e.g. arg-dev) and the secondary component alone (e.g. DETAIL).
This alleviates the data sparseness that we see with certain tags, and results in a total of 138 tags for the
LEX-2 feature set.
1
This number was chosen after inspecting chains in a separate development data set.
952
3 Data
We use essays from different essay-writing tasks, representing different genres, writing proficiency and
populations. Specifically, our essays consist of the following six subsets:
1. PE-G-N: Persuasive/Expository essays written by graduate school applicants who are a mix of
native and non-native speakers. (e.g. ?As people rely more and more on technology to solve prob-
lems, the ability of humans to think for themselves will surely deteriorate. Discuss the extent to
which you agree or disagree ... ? ) [n= 145 essays]
2. AC-G-N: Argumentation critique essays written by graduate school applicants who are a mix of
native and non-native speakers. (?Examine the stated and/or unstated assumptions of the argument.
Be sure to explain how the argument depends on the assumptions and what the implications are if
the assumptions prove unwarranted ...?) [n= 138 essays]
3. PE-UG-NN: Persuasive/Expository essays written by undergraduate and graduate school appli-
cants, who are non-native speakers. [n= 146 essays]
4. CS-UG-NN: Contrastive summary essays written by undergraduate and graduate school applicants
who are non-native speakers. Here, the prompt focuses on a specific type of summarization, where
ideas from an audio lecture are to be contrasted with ideas from a written passage. [n= 147 essays]
5. S-G-N: Subject matter essays written by graduates in a professional licensure exam who are a mix
of native and non-native speakers. [n= 150 essays]
6. M-K12-N: A Mix of expository, persuasive, descriptive and narrative essays written by K-12 school
students who are a mix of native and non-native speakers. [n= 150 essays]
Of the total of 876 essays, 40 essays were used for system development, and the rest were used for
cross-validation experiments. Each essay in the data set was manually annotated for overall discourse
coherence quality by annotators not involved in this research. The discourse coherence score was as-
signed using a 4-point scale (with score point 4 for excellent discourse coherence). Twenty percent of
the essays were double annotated and the rest were annotated by one of the annotators. Inter-annotator
agreement over the doubly annotated essays, calculated using quadratic weighted kappa (QWK), was
0.61 (substantial agreement). The data distribution for each score point was: 1% for score 1, 9% for
score 2, 27% for score 3, 63% for score 4.
4 Experiments
4.1 Baseline Features
A review by Burstein et al. (2013a) describes the several systems that measure discourse coherence
quality across various text genres including test-taker essays. Features used to evaluate the discourse
coherence quality systems in this study include those previously discussed in Burstein et al. (described
below). In addition to comparing our features with previously explored features, our goal is to see if the
state-of-the-art feature set can be extended with the use of lexical chaining features.
Entity-grid transition probabilities (entity). Entity-grid transition probabilities (Barzilay and Lap-
ata, 2008) are intended to address unity, progression and coherence by tracking nouns and pronouns in
text. An entity grid is constructed in which all entities (nouns and pronouns) are represented by their
syntactic roles in a sentence (i.e., Subject, Object, Other). Entity grid transitions track how the same
word appears in a syntactic role across adjacent sentences.
Type/Token Ratios for Entities (type/token). These are modified entity-grid transition probabilities.
While the entity grid only captures, for example ?Subject-Subject? transitions, type/token ratios capture
the proportion of unique words that make such transitions. Higher ratios indicate that more concepts are
being introduced in a given syntactic role, and lower ratios indicate fewer concepts.
953
RST-derived features (RST). Rhetorical relations (Mann and Thompson, 1988) derived from an RST
parser (Marcu, 2000) are used to evaluate if and how certain rhetorical relations, combinations of rhetor-
ical relations, or rhetorical relation tree structures contribute to discourse coherence quality. These in-
clude: (a) relative frequencies of n-gram rhetorical relations in the context of the RST parse tree structure
(unigrams, or occurrences of a single relation (e.g., ThemeShift); bigrams, (e.g., ?ThemeShift -> Elab-
oration?); and trigrams, (e.g., ?ThemeShift -> Elaboration -> Circumstance?)); (b) relative proportions
of leaf-parent relation types in the tree structure; and (c) counts of root node relation types in the trees.
Maximum LSA Value for Distant Sentence Pairs (maxLSA). This feature set is the maximum
Latent Semantic Analysis (LSA) similarity score found between pairs of sentences that are separated
by at least 5 intervening sentences in the essay. It captures reintroduction of topics later in an essay,
consistent with a backward inference strategy (Van den Broek et al., 1993; Van den Broek, 2012). LSA
has also been employed to measure semantic relatedness between texts for discourse coherence (Foltz et
al., 1998).
Grammatical error features (gramErr). These features address errors in grammar that could inter-
fere with a reader?s ability to construct meaning and have been used in previous studies (e.g. (Attali and
Burstein, 2006; Burstein et al., 2013b)). Specifically, they are based on more than 30 kinds of errors in
grammar, such as subject-verb agreement errors, in word usage, such as missing article errors, and in
spelling. We use e-rater
r
, an essay scoring engine developed by Educational Testing Service (ETS), to
detect the grammar errors. Aggregate counts of these errors are used as features for predicting discourse
coherence.
Program Features (program). This is a single feature for identifying the data type listed in Section
3. Genre and population play an important role with respect to discourse coherence ? essays written
by more advanced writers, such as those at the graduate level, are typically more coherent than essays
written by populations where English is a second language, or by K-12 school students. Note that the
program feature is not linguistically motivated ? it does not capture the writing construct or a writing
skill. However, it is a strong feature as it can reliably bias the system to change its expectations about the
discourse quality based on population and task.
4.2 Principal Components Analysis
To reduce the number of lexical chain features, a Principal Components Analysis (PCA) was calculated
on an independent set of 6000 essays randomly sampled from the six task types. For 38 LEX-1 features,
a 4-component solution accounted for about 0.70 of the feature variance. An 8-component solution
explained about 0.30 of the feature variance for the 138 LEX-2 features. (While the variance was lower
for this PCA solution, the components were fairly clean.) The component scores were then computed
for the 876 essays in our annotated data set. The 4-component scores were used as LEX-1 features, and
the 8-component scores were used as LEX-2 features. PCA was used for lexical chaining features in
order to reduce the number of features used to build the models rather than using a much larger number
of correlated features. PCA was not applied to features from previous work, as we wanted to reproduce
their performance.
4.3 Results
A 10-fold cross-validation was run with an unscaled, gradient boosting regressor
2
tuned using quadratic
weighted kappa
3
. Specifically, we used the standard Gradient Boosting Regressor in the scikit-learn
toolkit
4
(Pedregosa et al., 2011). The learner was trained to assign 4-point coherence quality scores
using different combinations of the feature sets described in sections 2.1 and 4.1. In addition to each of
the individual features in Section 4.1, we tested two baseline feature combinations: Baseline-1, a system
using all discourse-based features from Section 4.1, and Baseline-2, a system using all features described
in Section 4.1.
2
We experimented with SVMs and Random Forest learners too, but the best results were obtained with the regressor.
3
The software for the regressor can be found at https://github.com/EducationalTestingService/skll/
4
http://scikit-learn.org/stable/modules/generated/sklearn.ensemble
954
Performance was calculated using Quadratic Weighted Kappa (QWK) (Cohen, 1968), which measures
the agreement between the system score and the human-annotated coherence score. QWK corrects for
chance agreement, and it penalizes large disagreements more than small disagreements. The formula for
QWK is as follows:
? = 1?
k
?
i=1
k
?
j=1
w
ij
o
ij
k
?
i=1
k
?
j=1
w
ij
e
ij
where k is the total number of categories (4 in our case), o
ij
is the observed value in cell i, j of the
confusion matrix between system predictions and human scores, e
ij
is the expected value for cell i, j,
and w
ij
is the weight given to the discrepancy between category
i
and category
j
. The expected value
e
ij
is calculated as:
e
ij
=
k
?
j=1
o
ij
k
?
i=1
o
ij
k
?
i=1
k
?
j=1
o
ij
For quadratic weighted kappa, w
ij
is calculated as:
w
ij
= 1?
(i? j)
2
(k ? 1)
2
where i and j are categories, and k is the total number of categories. We use QWK as it is the standard
evaluation metric used in automated essay scoring, and it also helps us to compare our results with
previous work.
Table 1 reports the results for our proposed features and for each individual feature set investigated in
previous work. Here, feature sets explicitly targeting discourse phenomena are grouped under Discourse-
based Features. The features grouped under Non-Discourse Features also capture coherence quality;
however they are based on grammatical errors or data type information. The best performing system in
each group is shown in bold. We see that the full set of lexical chaining features (LEX-1 + LEX-2) is the
best performing discourse-based feature set. It performs better than each of the other discourse-based
features used alone, and also better than Baseline-1, which uses a combination of all discourse-based
features from previous work. Notice that the performance of each discourse-based system is below the
performance of both gramErr and program, indicating that they can play an important role in predicting
text coherence.
While grammar (gramErr) and data type (program) are powerful features, it is also important to incor-
porate capabilities for detecting and evaluating discourse-specific phenomena to ensure construct rele-
vance, as the grading guidelines for essays specify the need for proper organization of ideas (e.g.?sustains
a well-focused, well-organized analysis, connecting ideas logically?). Lack of construct relevance has
been a major criticism of automated scoring methods (Deane, 2013; Shermis and Burstein, 2013). Ad-
ditionally, discourse-relevant features will allow for interpretable, useful, explicit feedback to students
regarding discourse coherence and its breakdown.
In Table 1 we also see that no individual discourse-based system outperforms Baseline-2, compris-
ing all features from the state-of-the-art (Section 4.1). In fact, the human-system agreement obtained
by Baseline-2 surpasses the human-human agreement (QWK of 0.61) reported in Section 3. This phe-
nomenon is not uncommon in essay scoring. For example, Bridgeman et al. (2012) performed detailed
analyses and found that across all test populations, human-automated system score correlations surpassed
human-human score correlations.
Because the gramErr and program features contain information that is complementary to discourse-
based features, we combined the discourse features, first with gramErr features, and then with
gramErr+program features. Table 2 reports the results from these experiments. The best performing sys-
tem for each column is in bold, and all features with QWK higher than Baseline-2 are in italics. Here,
955
Feature set QWK
Discourse-based features
LEX-1+ LEX-2 0.316
LEX-1 0.176
LEX-2 0.246
entity 0.249
type/token 0.178
RST 0.295
maxLSA 0.171
Baseline-1 0.302
Non-Discourse Features
gramErr 0.592
program 0.387
Baseline-2 0.631
Table 1: Performance of individual feature sets.
Feature set +gramErr +gramErr
+program
LEX-1+ LEX-2 0.608 0.646
LEX-1 0.611 0.650
LEX-2 0.577 0.654
entity 0.621 0.609
type/token 0.600 0.623
RST 0.612 0.649
maxLSA 0.592 0.650
gramErr+program 0.644
Table 2: Performance (QWK), of individ-
ual discourse-based features when gramErr is
added (column 2) and gramErr and program
are added (column 3)
we see that, when combined with gramErr+program, the full set of lexical chaining features (LEX-
1+LEX-2), as well as LEX-1 and LEX-2 individually, perform above Baseline-2. Surprisingly, we find
that when some individual discourse features from previous work are combined with gramErr+program,
they achieve better performance than Baseline-2 indicating that using the full combination of discourse
features may not result in the best system. In the last row, we see that the combination of gramErr
and program features alone (gramErr+program) is more competitive that Baseline-2, underscoring their
usefulness for detecting coherence quality.
Finally, we performed full ablation studies to see which feature set combination produces the best
system for identifying discourse coherence quality. Different combinations of the 8 feature sets resulted
in 255 different systems, which we ranked based on their performance. Table 3 lists some of the systems,
with their respective ranks and QWK values.
First, we observe that the best performing system contains the full set of lexical chaining features
and achieves a QWK of 0.673. In fact, all of the top-5 performing systems contain either LEX-1 or
LEX-2. The best performance produced by a system not containing any lexical chaining features ranks
eighth (gramErr+ maxLSA+ program+ RST). Notice that gramErr+program is at rank 31, Baseline-2
is at rank 61, and Baseline-1 is at rank 235. Interestingly, RST features are also seen in all of the top-5
systems, indicating that RST features and lexical chaining features capture complementary information
about discourse quality. Surprisingly, maxLSA features, which have the same underlying principle of
cohesion in text as lexical chains, are in some of the top-performing feature combinations (at ranks 4
and 5), indicating that, in addition to how ideas and themes are presented throughout the essay, the
re-introduction of topics is also important.
We tested the statistical significance of the performance differences between our best system (gramErr
+ LEX-2+ LEX-1+ maxLSA+ program+ RST, at rank 1 in Table 3) and three other systems (Baseline-1,
Baseline-2 and gramErr+program) by drawing 10,000 bootstrap samples (Berg-Kirkpatrick et al., 2012)
from our manually scored essays. For each sample, QWKs were calcuated between the human scores and
the predictions of our best system, and between the human scores and each of the other three systems?
predictions. For each sample, the differences in QWKs were recorded, and the distributions of differences
were used for significance testing. Results show that our best performing system is significantly better
than Baseline-1 (p < 0.001) and Baseline-2 (p < 0.01), and it marginally outperformed the system with
gramErr+program features (p < 0.06).
These results show that lexical chaining information is a reliable indicator of discourse quality, and
that it can be combined synergistically with other complementary features to extend the state-of-the-art
for measuring discourse coherence quality.
956
Feature set QWK Rank
gramErr + LEX-2+ LEX-1+ maxLSA+ program+ RST 0.673 1
gramErr+ LEX-1+ program+ RST 0.661 2
gramErr+ LEX-2+ program+ RST 0.661 3
gramErr+ LEX-2+ maxLSA+ program+ RST 0.660 4
gramErr+ LEX-1+ maxLSA+ program+ RST 0.659 5
gramErr+ maxLSA+ program+ RST 0.656 8
gramErr+ program 0.644 31
Baseline-2: entity+ gramErr+ RST+ maxLSA+ program+ type/token 0.631 61
Baseline-1: entity+ RST+ maxLSA+ type/token 0.302 235
Table 3: Performance (QWK), and ranks of systems using different feature combinations
4.4 Analysis by Data Type
In the previous section we saw that features based on lexical chaining are able to successfully encode and
predict the quality of discourse coherence. We now examine if this impact is uniform across all essay
genres and populations of writers. Table 4 shows the performance of gramErr +program (in column
2), the best performing features and their respective performance (Best system, columns 3 and 4), and
the best feature set when lexical chaining features are removed, with their respective performance (Best
Minus LEX-1 and LEX-2, columns 5 and 6). Here we use gramErr +program as an additional baseline,
as it was found to be more competitive than both Baseline-1 and Baseline-2.
Program gramErr Best system Best Minus LEX-1 and LEX-2
+prog Features QWK Features QWK
CS-UG-NN 0.418 gramErr+ maxLSA+ RST 0.523 gramErr+ maxLSA+ RST 0.523
PE-UG-NN 0.406 gramErr + LEX-2 + maxLSA + RST 0.468 gramErr 0.406
PE-G-N 0.614 gramErr + LEX-1 + maxLSA 0.676 gramErr + maxLSA 0.650
AC-G-N 0.744 gramErr + LEX-2 + maxLSA 0.839 gramErr + maxLSA + type/token 0.766
S-G-N 0.414 entity + gramErr+ LEX-1+ RST+
type/token
0.532 gramErr+ RST+ type/token 0.487
M-K12-N 0.635 gramErr + LEX-2 + maxLSA 0.656 gramErr + maxLSA + RST +
type/token
0.649
Table 4: Performance of feature sets by data type. Best performance is shown in bold.
In general, for all data types, addition of discourse features produces improvement over just using a
combination of gramErr and program features. Also, the addition of lexical chaining features produces
performance improvement for most data types. Specifically, there is substantial improvement in perfor-
mance for persuasive writing (PE-UG-NN and PE-G-N), expository subject writing (S-G-N) and writing
involving critical argumentation (AC-G-N). M-K12-N, which is composed of a mix of genres and writing
proficiency, shows a minor improvement. Interestingly, for contrastive summarization (CS-UG-NN), the
best system for predicting discourse coherence does not employ any lexical chaining features. For this
type of writing, the best feature set using lexical chaining features achieved a QWK of 0.465, which im-
proves over gramErr+program but is lower than the best performing feature set. This is perhaps because
the discourse phenomena targeted by our lexical chaining features (topical detailing, variety and organi-
zation) are already provided for the writer in the source document and the audio lecture, i.e., the materials
that are to be referred to in writing this type of essay. Thus, other features play a more prominent role,
such as the RST features that capture local discourse organization which is needed, for example, when
drawing a contrast between two sources of conflicting information.
957
5 Related Work
5.1 Discourse coherence quality
A number of models for measuring the quality of discourse coherence have been based on Centering
Theory (Grosz et al., 1995). For example, Barzilay and Lapata (2008) construct entity grids based
on syntactic subjects and objects. Their algorithm keeps track of the distribution of entity transitions
between adjacent sentences and computes a value for all transition types based on their proportion of
occurrence in a text. The algorithm has been evaluated with three tasks using well-formed newspaper
corpora: text ordering, summary coherence evaluation, and readability assessment. Along similar lines,
Rus and Niraula (2012) find centered paragraphs based on prominent syntactic roles. Similarly, Milt-
sakaki and Kukich (2000) use manually marked centering information and find that higher numbers of
Rough Shifts within paragraphs are indicative of a lack of coherence. Using well-formed texts, Pitler
and Nenkova (2008) show that a text coherence detection system yields the best performance when it
includes features using the Barzilay and Lapata (2008) entity grids, syntactic features, discourse rela-
tions from the Penn Discourse Treebank (Prasad et al., 2008), and vocabulary and length features. Wang,
Harrington, and White (2012) combine the approaches from Barzilay and Lapata (2008), and Miltsakaki
and Kukich (2000) to detect coherence breakdown points. The biggest difference between our approach
and the approaches based on Centering Theory is that we do not use syntactically prominent items or try
to establish a center. Instead, multiple concurrent thematic chains can ?flow? through the paragraph, and
their length, density, and interaction with discourse markers are used to model coherence.
In other related work, Lin et al. (2011) use discourse relations from Discourse Lexicalized Tree
Adjoining Grammar (D-LTAG) and compile sub-sequences of discourse role transitions to see how the
discourse role of a term varies through the progression of the text. Our work, in contrast, traces how
chains or thematic threads are organized with respect to the discourse. Our approach also differs from
models that measure local coherence between adjacent sentences (Foltz et al., 1998), in that lexical chains
can run though the length of the entire text, and hence the features derived from them are able to capture
aggregate thematic properties of the entire text such as number, distribution and elaboration of topics.
Discourse coherence models have been previously employed for the task of information-ordering in
well-formed texts (e.g., (Soricut and Marcu, 2006; Elsner et al., 2007; Elsner and Charniak, 2008)).
In our tasks, discourse coherence quality is influenced by many factors including, but not limited to,
ordering of information, such as text unity, detailing and organization.
Higgins et al. (2004) implemented a genre-dependent system to predict discourse coherence quality
in essays. Their approach, however, was reliant on organizational structures particular to expository and
persuasive essays, such as thesis statement and conclusion.
5.2 Lexical Chaining and Cohesion
Lexical chaining has been used in a number of applications such as news segmentation (Stokes, 2003),
question-answering (Moldovan and Novischi, 2002), summarization (Barzilay and Elhadad, 1997), de-
tection and correction of malapropisms (Hirst and St-Onge, 1995), topic detection (Hatch et al., 2000),
topic tracking (Carthy and Sherwood-Smith, 2002), and keyword extraction (Ercan and Cicekli, 2007).
In a closely related study, Feng et al. (2009) use lexical chains to measure readability. Lexical chain
features are employed to indicate the number of entities/concepts that a reader must keep in mind while
reading a document, and two of their features (number of chains in the document and average length of
chains) overlap with our LEX-1 features. Our work also differs from systems using cohesion to measure
writing quality (e.g., (Witte and Faigley, 1981; Flor et al., 2013)), in that we focus on predicting the
quality of discourse coherence.
6 Conclusion
In this paper, we investigated the use of lexical chaining for measuring discourse coherence quality.
Based on intuitions about what makes a text coherent, we extracted two sets of features from lexical
chains, one encoding how topical themes and cohesive elements are addressed in the text, and another
958
encoding how the topical themes interact with explicit discourse organizational cues. We performed
detailed experiments which showed that lexical chaining features are useful for predicting discourse
coherence quality. Specifically, when compared to other previously explored discourse-based features,
we found that our lexical chaining features are best performers when used alone. We then experimented
with various feature combinations and showed that top performing systems contain lexical chaining
features. Our analyses also indicated that lexical chaining features can improve performance on various
genres of writing by different populations of writers. Our future work on measuring discourse coherence
quality involves extending chains by using verb information and by exploring finer distinctions within
the chains themselves (e.g., topical and sub-topical chains).
References
Yigal Attali and Jill Burstein. 2006. Automated essay scoring with e-rater v. 2.0. Journal of Technology, Learning,
and Assessment, 4:3.
Regina Barzilay and Michael Elhadad. 1997. Using lexical chains for text summarization. In Proceedings of the
ACL workshop on intelligent scalable text summarization, volume 17, pages 10?17.
Regina Barzilay and Mirella Lapata. 2008. Modeling local coherence: An entity-based approach. Computational
Linguistics, 34(1):1?34.
Taylor Berg-Kirkpatrick, David Burkett, and Dan Klein. 2012. An empirical investigation of statistical signif-
icance in NLP. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language
Processing and Computational Natural Language Learning, pages 995?1005. Association for Computational
Linguistics.
Brent Bridgeman, Catherine Trapani, and Yigal Attali. 2012. Comparison of human and machine scoring of
essays: Differences by gender, ethnicity, and country. Applied Measurement in Education, 25(1):27?40.
Jill Burstein, Karen Kukich, Susanne Wolff, Ji Lu, and Martin Chodorow. 1998. Enriching automated essay
scoring using discourse marking. In Workshop on Discourse Relations and Discourse Marking. ERIC Clear-
inghouse.
Jill Burstein, Jane Shore, John Sabatini, Brad Moulder, Steven Holtzman, and Ted Pedersen. 2012. The language
musesm system: Linguistically focused instructional authoring. Technical report, Educational Testing Services
(ETS).
Jill Burstein, Joel Tetreault, and Martin Chodorow. 2013a. Holistic discourse coherence annotation for noisy essay
writing. Dialogue and Discourse, 4(2):34?52.
Jill Burstein, Joel Tetreault, and Nitin Madnani, 2013b. Handbook of Automated Essay Evaluation: Current
Applications and New Directions, chapter The E-rater Automated Essay Scoring System. Routledge.
Joseph Carthy and Michael Sherwood-Smith. 2002. Lexical chains for topic tracking. In 2002 IEEE International
Conference on Systems, Man and Cybernetics, volume 7. IEEE.
Jacob Cohen. 1968. Weighted kappa: Nominal scale agreement provision for scaled disagreement or partial credit.
Psychological Bulletin, 70(4):213.
Paul Deane. 2013. On the relation between automated essay scoring and modern views of the writing construct.
Assessing Writing, 18(1):7 ? 24. Automated Assessment of Writing.
Micha Elsner and Eugene Charniak. 2008. Coreference-inspired coherence modeling. In Proceedings of the 46th
Annual Meeting of the Association for Computational Linguistics on Human Language Technologies: Short
Papers, pages 41?44. Association for Computational Linguistics.
Micha Elsner, Joseph Austerweil, and Eugene Charniak. 2007. A unified local and global model for discourse
coherence. In Proceedings of NAACL/HLT.
Gonenc Ercan and Ilyas Cicekli. 2007. Using lexical chains for keyword extraction. Information Processing &
Management, 43(6):1705?1714.
Lijun Feng, No?emie Elhadad, and Matt Huenerfauth. 2009. Cognitively motivated features for readability assess-
ment. In Proceedings of the 12th Conference of the European Chapter of the Association for Computational
Linguistics, pages 229?237. Association for Computational Linguistics.
959
Michael Flor, Beata Beigman Klebanov, and Kathleen M. Sheehan. 2013. Lexical tightness and text complexity.
In Proceedings of the Workshop on Natural Language Processing for Improving Textual Accessibility, pages
29?38, Atlanta, Georgia, June. Association for Computational Linguistics.
Peter W Foltz, Walter Kintsch, and Thomas K Landauer. 1998. The measurement of textual coherence with latent
semantic analysis. Discourse processes, 25(2-3):285?307.
Barbara J Grosz, Scott Weinstein, and Aravind K Joshi. 1995. Centering: A framework for modeling the local
coherence of discourse. Computational Linguistics, 21(2):203?225.
Michael AK Halliday and Ruqaiya Hasan. 1976. Cohesion in english. English Language Series. Longman Group
Ltd.
Paula Hatch, Nicola Stokes, and Joe Carthy. 2000. Topic detection, a new application for lexical chaining. In the
proceedings of BCS-IRSG, pages 94?103.
Derrick Higgins, Jill Burstein, Daniel Marcu, and Claudia Gentile. 2004. Evaluating multiple aspects of coherence
in student essays. In HLT-NAACL, pages 185?192.
Graeme Hirst and David St-Onge. 1995. Lexical chains as representations of context for the detection and correc-
tion of malapropisms. WordNet: An electronic lexical database, 305:305?332.
Jerry R Hobbs. 1979. Coherence and coreference. Cognitive Science, 3(1):67?90.
Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2011. Automatically evaluating text coherence using discourse
relations. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human
Language Technologies-Volume 1, pages 997?1006. Association for Computational Linguistics.
Dekang Lin. 1998. Automatic retrieval and clustering of similar words. In Proceedings of the 17th international
conference on Computational linguistics-Volume 2, pages 768?774. Association for Computational Linguistics.
William C Mann and Sandra A Thompson. 1988. Rhetorical structure theory: Toward a functional theory of text
organization. Text, 8(3):243?281.
Daniel Marcu. 2000. The theory and practice of discourse parsing and summarization. MIT Press.
Eleni Miltsakaki and Karen Kukich. 2000. Automated evaluation of coherence in student essays. In Proceedings
of LREC 2000.
Alden J Moe. 1979. Cohesion, coherence, and the comprehension of text. Journal of Reading, 23(1):16?20.
Dan Moldovan and Adrian Novischi. 2002. Lexical chains for question answering. In Proceedings of the 19th
international conference on Computational linguistics-Volume 1, pages 1?7. Association for Computational
Linguistics.
Jane Morris and Graeme Hirst. 1991. Lexical cohesion computed by thesaural relations as an indicator of the
structure of text. Computational linguistics, 17(1):21?48.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss,
V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. 2011. Scikit-
learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825?2830.
Charles A Perfetti and Alan M Lesgold. 1977. Discourse Comprehension and Sources of Individual Differences.
ERIC.
Emily Pitler and Ani Nenkova. 2008. Revisiting readability: A unified framework for predicting text quality.
In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 186?195.
Association for Computational Linguistics.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Miltsakaki, Livio Robaldo, Aravind K Joshi, and Bonnie L Webber.
2008. The Penn Discourse TreeBank 2.0. In LREC.
Vasile Rus and Nobal Niraula. 2012. Automated detection of local coherence in short argumentative essays based
on centering theory. In Computational Linguistics and Intelligent Text Processing, pages 450?461. Springer.
Mark D Shermis and Jill Burstein. 2013. Handbook of automated essay evaluation: Current applications and new
directions. Routledge.
960
Radu Soricut and Daniel Marcu. 2006. Discourse generation using utility-trained coherence models. In Proceed-
ings of the COLING/ACL on Main conference poster sessions, pages 803?810. Association for Computational
Linguistics.
Nicola Stokes. 2003. Spoken and written news story segmentation using lexical chains. In Proceedings of the
2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human
Language Technology: Proceedings of the HLT-NAACL 2003 student research workshop-Volume 3, pages 49?
54. Association for Computational Linguistics.
Paul Van den Broek, Charles R Fletcher, and Kirsten Risden. 1993. Investigations of inferential processes in
reading: A theoretical and methodological integration. Taylor & Francis.
Paul Van den Broek. 2012. Individual and developmental differences in reading comprehension: Assessing
cognitive processes and outcomes. Measuring up: Advances in how we assess reading ability, page 39.
Y Wang, M Harrington, and P White. 2012. Detecting breakdowns in local coherence in the writing of Chinese
English learners. Journal of Computer Assisted Learning, 28(4):396?410.
Stephen P Witte and Lester Faigley. 1981. Coherence, cohesion, and writing quality. College composition and
communication, pages 189?204.
961
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1291?1300,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Exploiting Syntactic and Distributional Information
for Spelling Correction with Web-Scale N-gram Models
Wei Xuc,?Joel Tetreaulta Martin Chodorowb Ralph Grishmanc Le Zhaod
aEducational Testing Service, Princeton, NJ, USA
jtetreault@ets.org
bHunter College of CUNY, New York, NY, USA
martin.chodorow@hunter.cuny.edu
cNew York University, NY, USA
{xuwei,grishman}@cs.nyu.edu
dCarnegie Mellon University, Pittsburgh, PA, USA
lezhao@cs.cmu.edu
Abstract
We propose a novel way of incorporating de-
pendency parse and word co-occurrence in-
formation into a state-of-the-art web-scale n-
gram model for spelling correction. The syn-
tactic and distributional information provides
extra evidence in addition to that provided by a
web-scale n-gram corpus and especially helps
with data sparsity problems. Experimental
results show that introducing syntactic fea-
tures into n-gram based models significantly
reduces errors by up to 12.4% over the current
state-of-the-art. The word co-occurrence in-
formation shows potential but only improves
overall accuracy slightly.
1 Introduction
The function of context-sensitive text correction is
to identify word-choice errors in text (Bergsma et
al., 2009). It can be viewed as a lexical disambigua-
tion task (Lapata and Keller, 2005), where a system
selects from a predefined confusion word set, such
as {affect, effect} or {complement, compliment},
and provides the most appropriate word choice given
the context. Typically, one determines if a word has
been used correctly based on lexical, syntactic and
semantic information from the context of the word.
One of the top performing models of spelling cor-
rection (Bergsma et al, 2010) is based on web-scale
n-gram counts, which reflect both syntax and mean-
ing. However, even with a large-scale n-gram cor-
pus, data sparsity can hurt performance in two ways.
?This work was done when the first author was an intern
for Educational Testing Service.
First, n-gram based methods require exact word and
order matches. If there is a low frequency word in
the context, such as a person?s name, there will be
little, if any, evidence in the n-gram data to sup-
port the usage. Second, if the target confusable word
is rare, there will not be enough n-gram support or
training data to render a confident decision. Because
of the data sparsity problem, language modeling is
not always sufficient to capture the meaning of the
sentence and the correct usage of the word.
Take a sentence from The New York Times
(NYT) for example: ??This fellow?s won a war,? the
dean of the capital?s press corps, David Broder, an-
nounced on ?Meet the Press? after complimenting
the president on the ?great sense of authority and
command? he exhibited in a flight suit.? Unfortu-
nately, neither the phrase ?complementing the pres-
ident? nor ?complimenting the president? exists in
the web-scale Google N-gram corpus (Brants and
Franz, 2006). The n-gram models decide solely
based on the frequency of the bi-grams ?after com-
ple(i)menting? and ?comple(i)menting the?, which
are common usages for both words. The real ques-
tion is whether we are more likely to ?compliment?
or ?complement? a person, the ?president?. Several
clues could help us answer that question. A de-
pendency parser can identify the word ?president?
as the subject of ?compliment? or ?complement?
which also may be the case in some of the train-
ing data. Lexical co-occurrence (Edmonds, 1997)
and semantic word relatedness measurements, such
as Random Indexing (Sahlgren, 2006), could pro-
vide evidence that ?compliment? is more likely to
co-occur with ?president? than ?complement?. Fur-
1291
thermore, some important clues can be quite distant
from the target word, e.g. outside the 9-word context
window Bergsma et al (2010) and Carlson (2007)
used. Consider another sentence in the NYT corpus,
?GM says the addition of OnStar, which includes a
system that automatically notifies an OnStar opera-
tor if the vehicle is involved in a collision, comple-
ments the Vue?s top five-star safety rating for the
driver and front passenger in both front- and side-
impact crash tests.? The dependency parser finds the
object of ?complement? is ?rating?, which is outside
the 9-word window.
We propose enhancing state-of-the-art web-scale
n-gram models for spelling correction with syntac-
tic structures and distributional information. For our
work, we build on a baseline system that combines
n-gram and lexical features (Bergsma et al, 2010).
Specifically, this paper makes the following contri-
butions:
1. We show that the baseline system can be
improved by augmenting it with dependency
parse features.
2. We show that the impact of parse features can
be further augmented when combined with dis-
tributional information, specifically word co-
occurrence information.
In the following section, we describe related
work and how our approach differs from these ap-
proaches. In Sections 3 and 4, we discuss our meth-
ods for using parse features and word co-occurrence
information. In Section 5, we present experimental
results and analysis.
2 Related Work
A variety of approaches have been proposed for
context-sensitive spelling correction ranging from
semantic methods to machine learning classifiers to
large-scale n-gram models.
Some semantics-based systems have been devel-
oped based on an intuitive assumption that the in-
tended word is more likely to be semantically coher-
ent with the context than is a spelling error. Jones
and Martin (1997) made use of the semantic simi-
larity produced by Latent Semantic Analysis. Bu-
danitsky and Hirst (2001) investigated the effective-
ness of predicting words based on different semantic
similarity/distance measures in WordNet. Both sys-
tems report performance that is lower than systems
developed more recently.
A variety of machine-learning methods have been
proposed in spelling correction and preposition and
article error correction fields, such as Bayesian clas-
sifiers (Golding, 1995; Golding and Roth, 1996),
Winnow-based learning (Golding and Roth, 1999),
decision lists (Golding, 1995), transformation-based
learning (Mangu and Brill, 1997), augmented mix-
ture models (Cucerzan and Yarowsky, 2002) and
maximum entropy classifiers (Izumi et al, 2003;
Han et al, 2006; Chodorow et al, 2007; Tetreault
and Chodorow, 2008; Felice and Pulman, 2008).
Despite their differences, these approaches mainly
use contextual features to capture the lexical, seman-
tic and/or syntactic environment of the target word.
The use of distributional similarity measures for
spelling correction has been previously explored in
(Mohammad and Hist, 2006). In our work, distribu-
tional similarity is not the primary contribution but
we show the impact it can have when used in con-
junction with a large scale n-gram model and with
parse features, which allows the system to select
words outside the local window for distributional
similarity. In the prior work, the words for distri-
butional similarity are constrained to the local win-
dow, and positional information of the words is not
encoded.
Recent work (Carlson and Fette, 2007; Gamon
et al, 2008; Bergsma et al, 2009) has demon-
strated that large-scale language modeling is ex-
tremely helpful for contextual spelling correction
and other lexical disambiguation tasks. These sys-
tems make the word choice depending on how fre-
quently each candidate word has been seen in the
given context in web-scale data. As n-gram data has
become more readily available, such as the Google
N-gram Corpus, the likelihood of a word being used
in a certain context can be better estimated.
Bergsma et al (2009; 2010) presented a series
of simple but powerful models which relied heavily
on web-scale n-gram counts. From the Google Web
N-gram Corpus, they retrieve counts of n-grams of
different sizes (2-5) and positions that span the tar-
get word w0 within a window of 9 words. For
example, for the following sentence: ?The system
tried to decide {among, between} the two confus-
1292
able words.?, the method would extract the five 5-
gram patterns, shown below in Figure 2, where w0
can be either word in the confusion set {among, be-
tween} in this particular example. Similarly, there
are four 4-grams, three 3-grams, and two 2-grams,
in total, 14 n-grams for each of the words in the con-
fusion set.
system tried to decide w0
tried to decide w0 the
to decide w0 the two
decide w0 the two confusable
w0 the two confusable words
We briefly describe three of Bergsma et al?s
(2009; 2010) best systems below, which are reported
to achieve state-of-the-art accuracy (NG = n-gram;
LEX = lexical).
1. sumLM: For each candidate word, (Bergsma
et al, 2009) sum the log-counts of all 14 pat-
terns filled with the candidate, and choose the
candidate with the highest total.
2. NG: Bergsma et al (2009) exploit each can-
didate?s 14 log-counts of n-gram patterns as
features in a Support Vector Machine (SVM)
model.
3. NG+LEX: Bergsma et al (2010) augment the
NG model with lexical features (described in
detail in Section 3.1).
Bergsma et al (2009; 2010) restricted their exper-
iments to only five confusion sets where the reported
performance in (Golding and Roth, 1999) was below
90%: {among, between}, {amount, number}, {cite,
sight, site}, {peace, piece} and {raise, rise}. They
reported that the SVM model with NG features out-
performed its unsupervised version, sumLM. How-
ever, the limited confusion word sets they evaluated
may not comprehensively represent the word usage
errors that writers typically make. In this paper, we
test nine additional commonly confused word pairs
to expand the scope of the evaluation. These words
were selected based on their lower frequencies com-
pared to the five pairs in the above work (as shown
later in Table 2).
3 Enhanced N-gram Models with Parse
Features
To our knowledge, only (Elmi and Evans, 1998)
have used parsing for spell correction. They focus
on using a parser as a filter to discriminate between
possible real-world corrections where the part-of-
speech differs. In our work, we show that parse fea-
tures are effective when used directly in the classifi-
cation mode (as opposed to as a final filter) to select
the best correction regardless of whether or not the
part-of-speech of the choices differ.
Statistical parsers have also seen limited use in
the sister tasks of preposition and article error detec-
tion (Hermet et al, 2008; Lee and Knutsson, 2008;
Felice and Pulman, 2009; Tetreault et al, 2010)
and verb sense disambiguation (Dligach and Palmer,
2008). In those instances where parsers have been
used, they have mainly provided shallow analyses
or relations involving specific target words, such as
a preposition or verb. Unlike preposition errors,
spelling errors can occur in any word.
In this paper, we propose a novel way to incor-
porate the parse into spelling correction, applying
the parser to sentences filled by each candidate word
equivalently and extracting salient features. This
overcomes two problem in the existing methods: 1)
the parse trees of the same sentence filled by differ-
ent confusion words can be different. However, in
the test phase, we do not know which word should
be put in the sentences to create parse features for
test examples. Previous studies (Tetreault et al,
2010) failed to discuss this issue. 2) Some existing
work (Whitelaw et al, 2009; Rozovskaya and Roth,
2010) in the text correction field introduced artificial
errors into training data to adapt the system to bet-
ter handle ill-formed text. But this method will en-
counter serious data sparsity problems when facing
rare words.
3.1 Baseline System
We chose one of the leading spelling correction sys-
tems, (Bergsma et al, 2010), as our primary base-
line. As noted earlier, it is an SVM-based system
combining web-scale n-gram counts (NG) and con-
textual words (LEX) as features. To simplify the ex-
planation, throughout the paper, we will only con-
sider the situation with two confusion words. The
1293
problem with more than two words in pre-defined
confusion sets can be solved similarly by using a
one-vs.-all strategy. As we mentioned in Section 2,
NG features include log-counts of 3-to-5-gram pat-
terns for each candidate word with the given context.
LEX features can be broken down into three sub-
categories: 1) bag-of-words (words at all positions
in a 9-word window around the target word), 2) in-
dicators for the words preceding or following the tar-
get word, and 3) indicators for all n-grams and their
positions. For the sentence ?The system tried to de-
cide {among, between} the two confusable words.?,
examples of bag-of-word features would be ?tried?,
?two?, etc., the two positional bigrams would be
?decide? and ?the?, and examples of the n-gram fea-
tures would be right-trigram = ?among the two? and
left-4-gram = ?tried to decide between?.
3.2 Parse Features
The benefit of introducing dependency parse fea-
tures is that 1) parse features capture contextual in-
formation in a larger context window; 2) parse fea-
tures specify which words in the context are salient
to the usage of the target word while purely lexi-
cally based approaches treat all words in the context
equally. We use the Stanford dependency parser (de
Marneffe et al, 2006) to extract six relevant feature
classes.
Parse Features (PAR):
1. relation names (target word as head)
2. complement of the target word
3. combination of 1 and 2
4. relation names (target word as complement)
5. head of the target word
6. combination of 4 and 5
Each of these six classes of PAR features can
contain zero to many values, since the target word
can be involved in none to multiple grammatical
relations and features of different filler words are
merged together. The PAR features, like the LEX
features, are binary. In Table 1, we present the parse
features for an example sentence. The parse fea-
tures here are listed as string values, but are later
converted into binary numbers in the vectors for the
SVM model.
4 Distributional Word Co-occurrence
Though lexical and parse features are complemen-
tary to n-gram models, they are learned from a nor-
mal training corpus and may not have enough cov-
erage due to data sparsity. Take a sentence from the
NYT for example: ?An economist, he began his ca-
reer as a professor ? he is still called ?the professor,?
by friends as a compliment and by foes as an insult ?
and taught at Harvard and Stanford .? If the most in-
dicative word ?friends? does not appear or does not
appear enough times in the local context or depen-
dencies with ?compliment? as compared to ?com-
plement? in the training corpus, then the classifier
may be unable to make the correct selection.
It is impractical and computationally costly to en-
large the training corpus without limit to include
all possible language phenomena. A good compro-
mise is to use word co-occurrence information from
web-scale data. The other option is to make use of
high-order word co-occurrence, which is included in
many semantic word relatedness measures, such as
Latent Semantic Analysis (LSA) (Landauer et al,
1998; Deerwester et al, 1990) or Random Indexing,
both of which can be estimated from a moderate-size
corpus.
Our intuition is to choose the confusion word
which is most relevant to a given context. We define
the salient words in context as a set M=m1, m2, m3,
..., and the relevance between two words as a func-
tion Relevance(w1, w2), which can either be calcu-
lated fromword co-occurrence or Random Indexing.
The score of each candidate word c in the confusion
set given a context with meaningful words M is cal-
culated by the following formula:
Score(c) =
?
m?M
Relevance(c,m)
In this paper, we experiment with first-order word
co-occurrence and Random Indexing as relevance
measures. And we define salient contextual words
as heads or complements in the dependency rela-
tions with the target word. In this way, we use the
parse information to constrain the two distribution
models. Thus the word co-occurrence information
1294
Feature Name PAR Features (compliment) PAR Features (complement)
1. Head Relation Name ccomp appos
2. Head of Relation says collisions
3. Head Combination ccomp says appos collisions
4. Comp Relation Name nsubj dep
5. Comp of Relation addition rating
6. Comp Combination nsub addition dep rating
Table 1: Parse Feature Example for the sentence: ?GM says the addition of OnStar, which includes a system that
automatically notifies an OnStar operator if the vehicle is involved in a collision, complements the Vue?s top five-star
safety rating for the driver and front passenger in both front- and side-impact crash tests.?
considerably overlaps with some values of the PAR
features, but provides extra evidence from web-scale
data rather than a limited amount of training data.
4.1 First-order Word Co-occurrence
The relevance based on first-order word co-
occurrence is calculated from the Google Web 5-
gram Corpus in a fashion similar to how we dealt
with n-gram counts in the previous section. Given
two words, w1 and w2, we consider all 8 possible
patterns that appear in a local context (5-word win-
dow), where we use wildcard (*) to indicate any to-
ken:
w1 w2
w1 * w2
w1 * * w2
w1 * * * w2
w2 w1
w2 * w1
w2 * * w1
w2 * * * w1
The relevance is then calculated by summing the
logarithm of each of the 8 different counts. Finally,
we compare the score of each candidate word and
output the one with higher score.
4.2 Random Indexing
The relevance scores based on Random Indexing
are provided by a tool FRanI (Higgins, 2004) and
a model trained on the Touchstone Applied Science
Associates (TASA) corpus which contains 750k sen-
tences and covers diverse topics (from a diversity of
textbooks up to the college level). Take the sentence
at the beginning of this section for example, where
only the words ?a? and ?friends? are related to the
target word (either ?complement? or ?compliment?)
by either relevance measure. The relevance based
on Random Indexing for (complement, friends) is
0.08, (compliment, friends) is 0.19 and both (com-
pliment, a) and (complement, a) are 0 because ?a?
is in the stop word list. Meanwhile, the relevance
based on first order word co-occurrence for (com-
pliment, friends) is 7.39, (complement, friends) is
5.38, (compliment, a) is 13.25, and (complement, a)
is 13.42. The system with either kind of relevance
outputs ?compliment?.
4.3 System Combination
Since the numeric measurement of word co-
occurrence is not as specific as the PAR features and
less trustworthy, adding word co-occurrence infor-
mation as features into the classifier along with n-
gram counts, lexical and parse features will hurt the
overall performance. It is more practical to combine
the two approaches in the following fashion:
1. When the SVM classifier (using NG, LEX and
PAR features) has high confidence (over a cer-
tain threshold) in the output label, output that
label;
2. Otherwise, output the results of the word
relatedness/co-occurrence-based system.
5 Evaluation
We evaluate the effectiveness of syntactic and dis-
tributional information on spelling correction. The
performance of the system is measured by accu-
racy: the percentage of sentences in the test data
for which the system chooses the correct word. We
compare our results against two baselines: 1) MA-
JOR chooses the most frequent candidate from the
1295
confusion set in the training corpus, and 2) Bergsma
et al?s (2010) best systems, NG+LEX. We include
inflectional variants (?-ing?, ?-ed?, ?-s?, ?-ly?) of
confusion words in the evaluation, such as comple-
menting, complimenting in addition to complement,
compliment, because this better corresponds to the
range of errors that may be encountered in actual
use and thus increases the scope of the system as a
real world application. Also following Bergsma et
al. (2010), we use a linear SVM, more exactly, the
L2-regularized L2-loss dual SVM in LIBLINEAR
(Fan et al, 2008). Unlike Bergsma et al, who used
development data to optimize parameters, we always
use default parameters, since training data is limited
for many of the words we are dealing with.
5.1 Data
Following Bergsma et al (2009; 2010), the test
examples are extracted from The New York Times
(NYT) portion of Gigaword1, but constrained to a
9-month publication time frame from October 2005
to July 2006. Unlike Bergsma et al who use the
same source as training data for the lexical features,
our training data (for both lexical and parse features)
comes from larger and more diverse news sources.
We use the very large database from Sekine?s n-gram
search engine (Sekine, 2008) as training data, which
consists of 1.9B words of newspaper text spanning
89 years from NYT, BBC, WSJ, Xinhua, etc.
We evaluate our systems on 5 confusion sets from
Bergsma et al (2009; 2010) and 9 commonly con-
fused word pairs with moderate frequency in daily
usage (randomly selected from those listed in En-
glish educational resources2). Shown in Table 2,
these 9 sets of words appear much less frequently
than the words selected by Bergsma et al, even
given the fact that we are using a considerably large
training corpus.
For each confusable word pair, sentences that
contain either of the words are extracted to form
training and test data. The word that appears in the
original sentences of the news article is treated as
the gold standard. For frequently occurring confu-
sion word sets used by Bergsma et al, we extract
up to 10k examples for testing, and up to 100k ex-
1Available from the LDC as LDC2003T05
2Such as an English learning blog post at
http://elisaenglish.pixnet.net/blog/post/1335194
Word Confusion Set # in Training Corpus
adverse / averse 13.5k / 1.8k
advice / advise 62.k / 12.9k
allusion / illusion 1.0k / 5.4k
complement / compliment 6.8k / 3.1k
confidant / confident 2.4k / 63.6k
desert / dessert 24.7k / 3.7k
discreet / discrete 0.7k / 2.4k
elicit / illicit 1.9k / 10.0k
stationary / stationery 2.5k/2.3k
wander / wonder 3.3k / 39.5k
Table 2: Training Data Sizes for Common ESL Confused
Words
amples for training. For the 9 less frequent confu-
sion word sets, we extract all the unique examples
for training and testing from the above sources. The
spelling correction system is evaluated by measur-
ing its accuracy in comparison to the gold standard
in test data. The error rate is the complement of ac-
curacy.
Following Carlson et al (2007) and Bergsma
et al (2009; 2010), we obtain the n-gram counts
from the GoogleWeb 1T 5-gram Corpus (Brants and
Franz, 2006).
5.2 Experimental Results
We present the results for each set separately be-
cause each set may behave very differently, depend-
ing upon its frequency, part-of-speech, number of
senses and other differences between the words in
each confusion set. The overall accuracy across con-
fusion sets is also presented to show the effective-
ness of different approaches. The results are tested
for statistical significance using McNemar?s test of
correlated proportions. The performance differences
are marked as significant when p < 0.05.
5.2.1 Effectiveness of Parse Features
We exploit the n-gram counts (NG), lexical fea-
tures (LEX) of Bergsma et al (2010) and our own
parse features (PAR) in linear SVM models.
The first comparison is between the supervised
learning systems with LEX and LEX+PAR. As
shown in Table 3, by exploiting our unique parse
features, for the total 14 confusion sets, the accuracy
increases on 12 sets and decreases on 2 sets. Over-
all, the spelling correction accuracy improves an ab-
1296
solute 1.35% for our 9 confusion sets and 0.60% for
Bergsma et al?s 5 confusion sets.
The second comparison is to see how parse fea-
tures interact with n-gram count features in a su-
pervised classifier. The best system from (Bergsma
et al, 2010) is listed in the table as ?NG+LEX?.
As shown in Table 3, the parse features proved to
be beneficial when augmenting this baseline, except
for the decrease in accuracy on adverse, averse by
only 2 cases out of 368, and among, between by
2 cases out of 10227. For all other confusion sets,
parse features decrease the error rate by as much as
2.74% (absolute) and as much as 38.5% (relative).
Improvements are statistically significant on all con-
fusion sets together, although for each separate set,
improvements are significant on only 5 sets, in part
due to an insufficient number of test cases.
The reason that parse features are occasionally not
helpful is because they sometimes include an un-
common word in dependencies, which happens to
appear once with the wrong word but not with the
correct word in the training data; or they sometimes
include too common words, which bias the classifier
in favor of the more frequent word in the confusion
set. We also noticed that lexical features are not al-
ways helpful when added to n-gram count features,
even for in-domain applications (i.e., with training
data and test data coming from the same domain or
corpus), as marked by underlines. However, lexical
and parse features together show more significant
and constant improvement over n-gram count-based
models, as marked by ?.
Of the six systems, every system that uses parse
features gets the example correct in Section 1, ?com-
plementing the president?; LEX by itself also gets
the example correct, but NG and NG+LEX fail.
In summary, our system NG+LEX+PAR outper-
forms the state-of-the-art system NG+LEX. It re-
duces the error rate by 12.4% across our 9 confusion
sets and by 8.4% across Bergsma et al?s 5 confusion
sets. Both improvements are significant (p < 0.05)
by the McNemar test. In addition, while NG+LEX
is not always better than NG, NG+LEX+PAR is con-
sistently better than NG.
5.2.2 Impact of Word Co-occurrence
The LIBLINEAR tool does not provide probabil-
ity estimates for SVM models but Logistic Regres-
sion can. In this set of experiments, we train a Logis-
tic Regression model with NG+LEX+PAR features
and empirically set the confidence threshold at 0.6,
as described in Section 4, based on the performance
on two word pairs. In the combined system, when
the Logistic Regression model estimates a probabil-
ity higher than the threshold we output its results,
otherwise we output the result of the system based
on word co-occurrence.
Surprisingly, although Random Indexing takes
into account more information than first-order word
co-occurrence, it lowered overall performance sub-
stantially. Thus in Table 4, we only present results
of using first-order word co-occurrence rather than
Random Indexing. For all 12 confusion sets, distri-
butional word co-occurrence information improves
9 sets and hurts 5 sets. Overall, it reduces the er-
ror rate slightly by 0.2% for our 9 sets and 1.5% for
Bergsma et al?s sets.
We believe there are two reasons why Ran-
dom Indexing fared worse than first-order word
co-occurrence: 1) Random Indexing considers co-
occurrence on a document level, while our first-
order word co-occurrence is limited to a 5-word win-
dow context. The latter is more suitable to context-
sensitive spelling correction. 2) The model for Ran-
dom Indexing is trained on a relatively small size
corpus compared to the web-scale data we used to
get n-gram count features for the classifier and thus
is not able to introduce much new evidence besides
the information carried by NG+LEX+PAR features.
Reason 2) also suggests why first-order co-
occurrence helps on some occasions while not on
other occasions. Its impact is limited because the
word co-occurrence information overlaps with some
of the PAR feature values as mentioned earlier. It
improves some cases because it provides some new
evidence from web-scale data to the system based on
NG+LEX+PAR features. It introduces new errors
because it simply favors the word that co-occurred
more often regardless of other factors. Its impact is
also limited because it is only considered when clas-
sifiers with NG+LEX+PAR features are not confi-
dent.
1297
CONFUSION SET # TEST MAJOR LEX LEX+PAR NG NG+LEX NG+LEX+PAR (&)
9 commonly cited ESL confusion pairs
adverse / averse 368 85.87 97.01 96.74 91.03 97.55 97.01 (+22.2%) ?
allusion / illusion 535 76.64 91.22 91.40 91.40 92.52 93.08 (-7.5%) ?
complement / compliment 860 51.51 83.84 85.12 88.49 88.37 89.53 (-10.0%)
confidant / confident 2416 94.41 97.97 98.30 98.51 99.05 99.09 (-4.3%) ?
desert / dessert 2357 70.81 90.71 91.56 87.31 93.68 94.57 (-14.1%) ?*
discreet / discrete 219 79.45 84.48 85.84 85.84 90.41 91.32 (-9.5%) ?
elicit / illicit 563 53.46 82.77 95.56 97.51 97.51 98.22 (-28.6%)
stationary / stationery 182 62.64 87.36 92.31* 93.96 92.86 95.60 (-38.5%)
wander / wonder 6506 86.37 96.42 97.42* 97.56 98.23 98.48 (-13.9%) ?*
Total 13972 81.08 93.94 95.29* 94.82 96.56 96.99 (-12.4%) ?*
5 Original Bergsma pairs
# among / between 10227 57.46 91.89 91.86 88.34 93.60 93.58 (+3.1%) ?
# amount / number 7398 76.44 92.34 93.16* 93.03 93.42 94.08 (-10.1%) ?*
# cite / site 10185 95.71 99.42 99.53 99.16 99.52 99.63 (-22.4%)?
# peace / piece 7330 56.81 95.01 97.01* 95.55 96.74 97.46 (-22.2%)? *
# raise / rise 9464 55.98 96.12 96.64* 94.45 96.68 97.05 (-11.5%) ?
Total 44604 68.92 95.09 95.69* 94.07 96.09 96.42 (-8.4%) ?
Table 3: Spelling correction precision (%), impact of adding parse features
SVM trained on 1G words of news text, tested on 9-months of NYT data.
*: Improvement of (NG+)LEX+PAR vs. (NG+)LEX is statistically significant.
?: Improvement of NG+LEX+PAR vs. NG is statistically significant.
&: Relative increase or decrease of error rate compared to ?NG+LEX?
#: As in Bergsma et al (2009; 2010) no morphological variants of the words are used in evaluation
CONFUSION SET # TEST MAJOR CLASSIFIER COMBINED SYSTEM (&)
9 commonly cited ESL confusion pairs
adverse / averse 368 85.87 97.55 96.74 (+33.3%)
allusion / illusion 535 76.64 92.34 92.34 (- 0.0%)
complement / compliment 860 51.51 89.88 90.81 (-9.2%)
confidant / confident 2416 94.41 99.13 99.05 (+9.5%)
desert / dessert 2357 70.81 93.98 94.23 (-3.7%)
discreet / discrete 219 79.45 90.41 91.78 (-14.3%)
elicit / illicit 563 53.46 98.40 98.76 (-22.2%)
stationary / stationery 182 62.64 93.41 93.96 (-9.1%)
wander / wonder 6506 86.37 98.49 98.36 (+9.2%)
5 Original Bergsma pairs
# among / between 10227 57.46 92.73 92.73 (-0.1%)
# amount / number 7398 76.44 93.44 93.76 (-4.74%)
# cite / site 10185 95.71 99.49 99.47 (+3.8%)
# peace / piece 7330 56.81 96.19 96.38 (-5.0%)
# raise / rise 9464 55.98 96.66 96.59 (+2.2%)
Table 4: Spelling correction accuracy (%), impact of combining word co-occurrence
CLASSIFIER: Logistic Regression trained on 1G words of news text, tested on 9-months NYT data.
COMBINED SYSTEM: CLASSIFER plus system based on first-order word co-occurrence.
&: Relative increase or decrease in error rate compared to CLASSIFIER
#: As in Bergsma et al (2009; 2010), no morphological variants of the words are used in evaluation
1298
6 Conclusions
We propose a novel approach that uses parse
features and lexical features together to improve
the performance of web-scale n-gram models for
spelling correction. This method is especially adap-
tive when less training data are available, which is
the case for confusable words that are not very fre-
quently used. We also investigate the effectiveness
of incorporating web-scale word co-occurrence and
corpus-based semantic word relatedness (Random
Indexing).
For future work, we will investigate using seman-
tic information (e.g. WordNet) to extend n-gram
models. It will be interesting to see if the usage of
the word ?compliment? in ?complimenting the pres-
ident? can be estimated by considering similar us-
ages in the corpus, such as ?complimenting the stu-
dent? or by creating an n-gram database of synset
patterns. We will investigate extending, to other ap-
plications, this general methodology combining dis-
tributional, semantic and syntactic information with
language models.
Acknowledgments
We wish to thank Michael Flor of Educational
Testing Service for his TrendStream tool, which
provides fast access and easy manipulation of the
Google N-gram Corpus. We also thank Derrick Hig-
gins of Educational Testing Service for his Random
Indexing support. We also thank Satoshi Sekine of
New York University, Matthew Snover of City Uni-
versity of New York, and Jing Jiang of Singapore
Management University for their advice.
References
Shane Bergsma, Dekang Lin, and Randy Goebel. 2009.
Web-scale n-gram models for lexical disambiguation.
In IJCAI.
Shane Bergsma, Emily Pitler, and Dekang Lin. 2010.
Creating robust supervised classifiers via web-scale n-
gram data. In ACL.
Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram
version 1. Available at http://www.ldc.upenn.edu.
Alexander Budanitsky and Graeme Hirst. 2001. Seman-
tic distance in wordnet: An experimental, application-
oriented evaluation of five measures. In ACL Work-
shop on WordNet and Other Lexical Resources.
Andrew Carlson and Ian Fette. 2007. Memory-based
context sensitive spelling correction at web scale. In
ICMLA.
Martin Chodorow, Joel Tetreault, and Na-Rae Han. 2007.
Detection of grammatical errors involving preposi-
tions. In Proceedings of the Fourth ACL-SIGSEM
Workshop on Prepositions, pages 25?30.
Silviu Cucerzan and David Yarowsky. 2002. Aug-
mented mixture models for lexical disambigua-tion. In
EMNLP.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed de-
pendency parses from phrase structure parses. In Pro-
ceedings of LREC, Genoa, Italy.
Scott Deerwester, Susan Dumais, George Furmas,
Thomas Landauer, and Richar Harshman. 1990. In-
dexing by latent semantic analysis. The American So-
ciety for Information Science.
Dmitriy Dligach and Martha Palmer. 2008. Novel se-
mantic features for verb sense disambiguation. In
ACL.
Philip Edmonds. 1997. Choosing the word most typical
in context using a lexical co-occurrence network. In
EACL.
Mohammed Ali Elmi and Martha Evans. 1998. Spelling
correction using context. In COLING.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. Liblinear: A library
for large linear classification. Machine Learning Re-
search, 9(1871-1874).
Rachele De Felice and Stephen G. Pulman. 2008.
A classifier-based approach to preposition and deter-
miner error correction in L2 English. In Proceedings
of COLING, Manchester, UK.
Rachele De Felice and Stephen G. Pulman. 2009. Auto-
matic detection of preposition errors in learner writing.
CALICO Journal, 26(3).
Michael Gamon, Jianfeng Gao, Chris Brockett, Alex
Klementiev, William B. Dolan, Dmitriy Belenko, and
Lucy Vanderwende. 2008. Using contextual speller
techniques and language modeling for ESL error cor-
rection. In Proceedings of the International Joint Con-
ference on Natural Language Processing (IJCNLP),
pages 449?456, Hyderabad, India.
Andrew Golding and Dan Roth. 1996. Applying Win-
now to context-sensitive spelling correction. In Pro-
ceedings of the International Conference on Machine
Learning (ICML), pages 182?190.
Andrew Golding and Dan Roth. 1999. A winnow-based
approach to context-sensitive spelling correction. Ma-
chine Learning, 34(1-3):107?130.
Andrew Golding. 1995. A Bayesian hybrid method for
context sensitive spelling correction. In Proceedings
1299
of the Third Workshop on Very Large Corpora (WVLC-
3), pages 39?53.
Na-Rae Han, Martin Chodorow, and Claudia Leacock.
2006. Detecting errors in English article usage by
non-native speakers. Natural Language Engineering,
12(2):115?129.
Matthieu Hermet, Alain De?silets, and Stan Szpakowicz.
2008. Using the web as a linguistic resource to au-
tomatically correct lexico-syntactic errors. In Pro-
ceedings of the Sixth International Conference on Lan-
guage Resources and Evaluation (LREC), pages 390?
396, Marrekech, Morocco.
Emi Izumi, Kiyotaka Uchimoto, Toyomi Saiga, Thepchai
Supnithi, and Hitoshi Isahara. 2003. Automatic er-
ror detection in the Japanese learners? English spoken
data. In Companion Volume to the Proceedings of the
41st Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 145?148.
Michael Jones and James Martin. 1997. Contextual
spelling correction using latent semantic analysis. In
ANLC.
Thomas Landauer, Darrell Laham, and Peter Foltz. 1998.
Learning human-like knowledge by singular value de-
composition: A progress report. Advances in Neural
Information Processing Systems, 10:45?51.
Mirella Lapata and Frank Keller. 2005. Web-based mod-
els for natural language processing. ACM Transac-
tions on Speech and Language Processing, 21:1?31.
John Lee and Ola Knutsson. 2008. The role of pp attach-
ment in preposition generation. In CICLING.
Lidia Mangu and Eric Brill. 1997. Automatic rule acqui-
sition for spelling correction. In ICML.
Saif Mohammad and Graeme Hist. 2006. Distributional
measures of concept distance: A task-oriented evalua-
tion. In EMNLP.
Alla Rozovskaya and Dan Roth. 2010. Training
paradigms for correcting errors in grammar and usage.
In ACL.
Magnus Sahlgren. 2006. The Word-Space Model: us-
ing distributional analysis to represent syntagmatic
and paradigmatic relations between words in high-
dimensional vector spaces. Ph.D. thesis.
Joel Tetreault and Martin Chodorow. 2008. The ups and
downs of prepostion error detection in esl writing. In
COLING.
Joel Tetreault, Jennifer Foster, and Martin Chodorow.
2010. Using parse features for preposition selection
and error detection. In ACL.
Casey Whitelaw, Ben Hutchinson, Grace Y. Chung, and
Gerard Ellis. 2009. Using the web for language inde-
pendent spellchecking and autocorrection. In ACL.
1300
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 20?28,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Identifying High-Level Organizational Elements
in Argumentative Discourse
Nitin Madnani Michael Heilman Joel Tetreault
Educational Testing Service
Princeton, NJ, USA
{nmadnani,mheilman,jtetreault}@ets.org
Martin Chodorow
Hunter College of CUNY
New York, NY, USA
martin.chodorow@hunter.cuny.edu
Abstract
Argumentative discourse contains not only
language expressing claims and evidence, but
also language used to organize these claims
and pieces of evidence. Differentiating be-
tween the two may be useful for many appli-
cations, such as those that focus on the content
(e.g., relation extraction) of arguments and
those that focus on the structure of arguments
(e.g., automated essay scoring). We propose
an automated approach to detecting high-level
organizational elements in argumentative dis-
course that combines a rule-based system and
a probabilistic sequence model in a principled
manner. We present quantitative results on a
dataset of human-annotated persuasive essays,
and qualitative analyses of performance on es-
says and on political debates.
1 Introduction
When presenting an argument, a writer or speaker
usually cannot simply state a list of claims and
pieces of evidence. Instead, the arguer must explic-
itly structure those claims and pieces of evidence, as
well as explain how they relate to an opponent?s ar-
gument. Consider example 1 below, adapted from
an essay rebutting an opponent?s argument that griz-
zly bears lived in a specific region of Canada.
The argument states that based on the
result of the recent research, there proba-
bly were grizzly bears in Labrador. It may
seem reasonable at first glance, but ac-
tually, there are some logical mistakes
in it. . . . There is a possibility that they
were a third kind of bear apart from black
and grizzly bears. Also, the explorer ac-
counts were recorded in the nineteenth
century, which was more than 100 years
ago. . . . In sum, the conclusion of this
argument is not reasonable since the ac-
count and the research are not convinc-
ing enough. . . .
The argument begins by explicitly restating the
opponent?s claim, prefacing the claim with the
phrase ?The argument states that.? Then, the sec-
ond sentence explicitly marks the opponent?s argu-
ment as flawed. Later on, the phrase ?There is a
possibility that? indicates the subsequent clause in-
troduces evidence contrary to the opponent?s claim.
Finally, the sentence ?In sum, . . .? sums up the ar-
guer?s stance in relation to the opponent?s claim.1
As illustrated in the above example, argumenta-
tive discourse can be viewed as consisting of lan-
guage used to express claims and evidence, and
language used to organize them. We believe that
differentiating organizational elements from content
would be useful for analyzing persuasive discourse.
1The word Also signals that additional evidence is about to
be presented and should also be marked as shell. However, it
was not marked in this specific case by our human annotator
(?3.2).
20
We refer to such organizational elements as shell, in-
dicating that they differ from the specific claims and
evidence, or ?meat,? of an argument. In this work,
we develop techniques for detecting shell in texts.
We envision potential applications in political sci-
ence (e.g., to better understand political debates), in-
formation extraction or retrieval (e.g., to help a sys-
tem focus on content rather than organization), and
automated essay scoring (e.g., to analyze the quality
of a test-taker?s argument), though additional work
is needed to determine exactly how to integrate our
approach into such applications.
Detecting organizational elements could also be a
first step in parsing an argument to infer its structure.
We focus on this initial step, leaving the other steps
of categorization of spans (as to whether they evalu-
ate the opponent?s claims, connect one?s own claims,
etc.), and the inference of argumentation structure to
future work.
Before describing our approach to identifying
shell, we begin by defining it. Shell refers to se-
quences of words used to refer to claims and evi-
dence in persuasive writing or speaking, providing
an organizational framework for an argument. It
may be used by the writer or the speaker in the fol-
lowing ways:
? to declare one?s own claims (e.g., ?There is the
possibility that?)
? to restate an opponent?s claims (e.g., ?The argu-
ment states that?)
? to evaluate an opponent?s claims (e.g., ?It may
seem reasonable at first glance, but actually, there
are some logical mistakes in it?)
? to present evidence and relate it to specific claims
(e.g., ?To illustrate my point, I will now give the
example of?)
There are many ways of analyzing discourse. The
most relevant is perhaps rhetorical structure theory
(RST) (Mann and Thompson, 1988). To our knowl-
edge, the RST parser from Marcu (2000) is the only
RST parser readily available for experimentation.
The parser is trained to model the RST corpus (Carl-
son et al, 2001), which treats complete clauses (i.e.,
clauses with their obligatory complements) as the el-
ementary units of analysis. Thus, the parser treats
the first sentence in example 1 as a single unit and
does not differentiate between the main and subordi-
nate clauses. In contrast, our approach distinguishes
the sequence ?The argument states that . . . ? as shell
(which is used here to restate the external claim).
Furthermore, we identify the entire second sentence
as shell (here, used to evaluate the external claim),
whereas the RST parser splits the sentence into two
clauses, ?It may seem . . .? and ?but actually . . .?,
linked by a ?contrast? relationship.2 Finally, our
approach focuses on explicit markers of organiza-
tional structure in arguments, whereas RST covers a
broader range of discourse connections (e.g., elabo-
ration, background information, etc.), including im-
plicit ones. (Note that additional related work is de-
scribed in ?6.)
This work makes the following contributions:
? We describe a principled approach to the task
of detecting high-level organizational elements in
argumentative discourse, combining rules and a
probabilistic sequence model (?2).
? We conduct experiments to validate the approach
on an annotated sample of essays (?3, ?4).
? We qualitatively explore how the approach per-
forms in a new domain: political debate (?5).
2 Detection Methods
In this section, we describe three approaches to the
problem of shell detection: a rule-based system
(?2.1), a supervised probabilistic sequence model
(?2.2), and a simple lexical baseline (?2.3).
2.1 Rule-based system
We begin by describing a knowledge-based ap-
proach to detecting organizational elements in argu-
mentative discourse. This approach uses a set of 25
hand-written regular expression patterns.3
In order to develop these patterns, we created a
sample of 170 annotated essays across 57 distinct
prompts.4 The essays were written by test-takers of
a standardized test for graduate admissions. This
sample of essays was similar in nature to but did
not overlap with those discussed in other sections
2We used the RST parser of Marcu (2000) to analyze the
original essay from which the example was adapted.
3We use the PyParsing toolkit to parse sentences with the
grammar for the rule system.
4Prompts are short texts that present an argument or issue
and ask test takers to respond to it, either by analyzing the given
argument or taking a stance on the given issue.
21
MODAL? do | don?t | can | cannot | will | would | . . .
ADVERB? strongly | totally | fundamentally | vehemently | . . .
AGREEVERB? disagree | agree | concur | . . .
AUTHORNOUN? writer | author | speaker | . . .
SHELL? I [MODAL] [ADVERB] AGREEVERB with the AUTHORNOUN
Figure 1: An example pattern that recognizes shell language describing the author?s position with respect to an oppo-
nent?s, e.g., I totally agree with the author or I will strongly disagree with the speaker.
of the paper (?2.2, ?3.2). The annotations were car-
ried out by individuals experienced in scoring per-
suasive writing. No formal annotation guidelines
were provided. Besides shell language, there were
other annotations relevant to essay scoring. How-
ever, we ignored them for this study because they
are not directly relevant to the task of shell language
detection.
From this sample, we computed lists of n-grams
(n = 1, 2, . . . , 9) that occurred more than once in
essays from at least half of the 57 distinct essay
prompts. We then wrote rules to recognize the shell
language present in the n-gram lists. Additional
rules were added to cover instances of shell that we
observed in the annotated essays but that were not
frequent enough to appear in the n-gram analysis.
We use ?Rules? to refer to this method.
2.2 Supervised Sequence Model
The next approach we describe is a supervised, prob-
abilistic sequence model based on conditional ran-
dom fields (CRFs) (Lafferty et al, 2001), using a
small number of general features based on lexical
frequencies. We assume access to a labeled dataset
of N examples (w,y) indexed by i, containing se-
quences of words w(i) and sequences of labels y(i),
with individual words and labels indexed by j (?3
describes our development and testing sets). y(i) is a
sequence of binary values, indicating whether each
word w(i)j in the sequence is shell (y
(i)
j = 1) or not
(y(i)j = 0). Following Lafferty et al (2001), we find
a parameter vector ? that maximizes the following
log-likelihood objective function:
L(?|w,y) =
N?
i=1
log p
(
y(i) | w(i), ?
)
(1)
=
N?
i=1
(
?>f(w(i), y(i))? logZ(i)
)
The normalization constant Zi is a sum over all
possible label sequences for the ith example, and f
is a feature function that takes pairs of word and la-
bel sequences and returns a vector of feature values,
equal in dimensions to the number of parameters in
?.5
The feature values for the jth word and label pair
are as follows (these are summed over all elements
to compute the values of f for the entire sequence):
? The relative frequency of w(i)j in the British Na-
tional Corpus.
? The relative frequency of w(i)j in a set of 100,000
essays (see below).
? Eight binary features for whether the above fre-
quencies meet or exceed the following thresholds:
10{?6,?5,?4,?3}.
? The proportion of prompts for which w(i)j ap-
peared in at least one essay about that prompt in
the set of 100,000.
? Three binary features for whether the above pro-
portion of prompts meets or exceeds the following
thresholds: {0.25, 0.50, 0.75}.
? A binary feature with value 1 if w(i)j consists only
of letters a-z, and 0 otherwise. This feature dis-
tinguishes punctuation and numbers from other to-
kens.
5We used CRFsuite 0.12 (Okazaki, 2007) to implement the
CRF model.
22
? A binary feature with value 1 if the rule-based sys-
tem predicts that w(i)j is shell, and 0 otherwise.
? A binary feature with value 1 if the rule-based sys-
tem predicts that w(i)j?1 is shell, and 0 otherwise.
? Two binary features for whether or not the current
token was the first or last in the sentence, respec-
tively.
? Four binary features for the possible transitions
between previous and current labels (y(i)j and y
(i)
j?1,
respectively).
To define the features related to essay prompts
and lexical frequencies in essays, we created a set
of 100,000 essays from a larger set of essays written
by test-takers of a standardized test for graduate ad-
missions (the same domain as in ?2.1). The essays
were written in response to 228 different prompts
that asked students to analyze various issues or ar-
guments. We use additional essays sampled from
this source later to acquire annotated training and
test data (?3.2).
We developed the above feature set using cross-
validation on our development set (?3). The intu-
ition behind developing the word frequency features
is that shell language generally consists of chunks of
words that occur frequently in persuasive language
(e.g., ?claims,? ?conclude?) but not necessarily as
frequently in general text (e.g., the BNC). The se-
quence model can also learn to disprefer changes of
state, such that multi-word subsequences are labeled
as shell even though some of the individual words in
the subsequence are stop words, punctuation, etc.
Note there are a relatively small number of pa-
rameters in the model,6 which allows us to estimate
parameters on a relatively small set of labeled data.
We briefly experimented with adding an `2 penalty
on the magnitude of ? in Equation 2, but this did not
seem to improve performance.
When making predictions y?(i) about the label se-
quence for a new sentence, the most common ap-
proach is to find the most likely sequence of labels y
given the words w(i), found with Viterbi decoding:
6There were 42 parameters in our implementation of the full
CRF model. Excluding the four transition features, each of the
19 features had two parameters, one for the positive class and
one for the negative class. Having two parameters for each is
unnecessary, but we are not aware of how to have the crfsuite
toolkit avoid these extra features.
y?(i) = argmax
y
p?(y | w
(i)) (2)
We use ?CRFv? to refer to this approach. We use
the suffix ?+R? to denote models that include the
two rule-based system prediction features, and we
use ?-R? to denote models that exclude these two
features.
In development, we observed that this decoding
approach seemed to very strongly prefer labeling an
entire sentence as shell or not, which is often not
desirable since shell often appears at just the begin-
nings of sentences (e.g., ?The argument states that?).
We therefore test an alternative prediction rule
that works at the word-level, rather than sequence-
level. This approach labels each word as shell if
the sum of the probabilities of all paths in which
the word was labeled as shell?that is, the marginal
probability?exceeds some threshold ?. Words are
labeled as non-shell otherwise. Specifically, an indi-
vidual word w(i)j is labeled as shell (i.e., y?
(i)
j = 1)
according to the following equation, where 1(q) is
an indicator function that returns 1 if its argument q
is true, and 0 otherwise.
y?(i)j = 1
((
?
y
p?(y | w
(i)) yj
)
? ?
)
(3)
We tune ? using the development set, as discussed
in ?3.
We use ?CRFm? to refer to this approach.
2.3 Lexical Baseline
As a simple baseline, we also evaluated a method
that labels words as shell if they appear frequently
in persuasive writing?specifically, in the set of
100,000 unannotated essays described in ?2.2. In
this approach, word tokens are marked as shell
if they belonged to the set of k most frequent
words from the essays. Using the development
set discussed in ?3.2, we tested values of k in
{100, 200, . . . , 1000}. Setting k = 700 led to the
highest F1.
We use ?TopWords? to refer to this method.
23
3 Experiments
In this section, we discuss the design of our exper-
imental evaluation and present results on our devel-
opment set, which we used to select the final meth-
ods to evaluate on the held-out test set.
3.1 Metrics
In our experiments, we evaluated the performance
of the shell detection methods by comparing token-
level system predictions to human labels. Shell lan-
guage typically occurs as fairly long sequences of
words, but identifying the exact span of a sequence
of shell seems less important than in related tag-
ging tasks, such as named entity recognition. There-
fore, rather than evaluating based on spans (either
with exact or a partial credit system), we measured
performance at the word token-level using standard
metrics: precision, recall, and the F1 measure. For
example, for precision, we computed the propor-
tion of tokens predicted as shell by a system that
were also labeled as shell in our human-annotated
datasets.
3.2 Annotated Data
To evaluate the methods described in ?2, we gath-
ered annotations for 200 essays that were not in the
larger, unannotated set discussed in ?2.2. We split
this set of essays into a development set of 150 es-
says (68,601 word tokens) and a held-out test set of
50 essays (21,277 word tokens). An individual with
extensive experience at scoring persuasive writing
and familiarity with shell language annotated all to-
kens in the essays with judgments of whether they
were shell or not (in contrast to ?2.1, this annotation
only involved labeling shell language).
From the first annotator?s judgments on the devel-
opment set, we created a set of annotation guidelines
and trained a second annotator. The second anno-
tator marked the held-out test set so that we could
measure human agreement. Comparing the two an-
notators? test set annotations, we observed agree-
ment of F1 = 0.736 and Cohen?s ? = 0.699 (we
do not use ? in our experiments but report it here
since it is a common measure of human agreement).
Except for measuring agreement, we did not use the
second annotator?s judgments in our experiments.7
7In the version of this paper submitted for review, we mea-
recall
pre
cisi
on
0.0
0.2
0.4
0.6
0.8
1.0
l
0.0 0.2 0.4 0.6 0.8 1.0
linesCRFm?RCRFm+R
points
l CRFm?RCRFm+RCRFv?RCRFv+RRulesTopWords
Figure 2: Precision and recall of the detection methods at
various thresholds, computed through cross-validation on
the development set. Points indicate performance for the
rule-based and baseline system as well as points where
F1 is highest.
3.3 Cross-validation Results
To develop the CRF?s feature set, to tune hyperpa-
rameters, and to select the most promising systems
to evaluate on the test set, we randomly split the sen-
tences from the development set into two halves and
conducted tests with two-fold cross-validation.
We tested thresholds for the CRF at ? =
{0.01, 0.02, . . . , 1.00}.
Figure 2 shows the results on the development set.
For the rule-based system, which did not require la-
beled data, performance is computed on the entire
development set. For the CRF approaches, the pre-
cision and recall were computed after concatenating
predictions on each of the cross-validation folds.
The TopWords baseline performed quite poorly,
with F1 = 0.205. The rule-based system performed
much better, with F1 = 0.382, but still not as well
as the CRF systems. The CRF systems that pre-
dict maximum sequences had F1 = 0.382 without
the rule-based system features (CRFv?R), and F1 =
0.467 with the rule-based features (CRFv+R). The
CRF systems that made predictions from marginal
scores performed best, with F1 = 0.516 without
the rule-based features, and F1 = 0.551 with the
rule-based features. Thus, both the rule-based sys-
sured test set agreement with judgments from a third individ-
ual, who was informally trained by the first, without the formal
guidelines. Agreement was somewhat lower: F1 = 0.668 and
? = 0.613.
24
Method P R F1 Len
TopWords 0.125 0.759 0.214 ? 2.80
Rules 0.561 0.360 0.439 ? 4.99
CRFv?R 0.729 0.268 0.392 ? 15.67
CRFv+R 0.763 0.369 0.498 ? 13.30
CRFm?R 0.586 0.574 0.580 9.00
CRFm+R 0.556 0.670 0.607 9.96
Human 0.685 0.796 0.736 ? 7.91
Table 1: Performance on the held-out test set, in terms of
precision (P), recall (R), F1 measure, and average length
in tokens of sequences of one or more words labeled as
shell (Len). ? indicates F1 scores that are statistically
reliably different from CRFm+R at the p < 0.01 level.
tem features and the marginal prediction approach
led to gains in performance.
From an examination of the predictions from the
CRFm+R and CRFm?R systems, it appears that a
major contribution of the features derived from the
rule-based system is to help the hybrid CRFm+R
system avoid tagging entire sentences as shell when
only parts of them are actually shell. For exam-
ple, consider the sentence ?According to this state-
ment, the speaker asserts that technology can not
only influence but also determine social customs and
ethics? (typographical errors included). CRFm?R
tags everything up to ?determine? as shell, whereas
the rule-based system and CRFm+R correctly stop
after ?asserts that.?
4 Test Set Results
Next, we present results on the held-out test set.
For the CRFm systems, we used the thresholds that
led to the highest F1 scores on the development
set (? = 0.26 for CRFm+R and ? = 0.32 for
CRFm?R). Table 1 presents the results for all sys-
tems, along with results comparing the second anno-
tator?s labels (?Human?) to the gold standard labels
from the first annotator.
The same pattern emerged as on the development
set, with CRFm+R performing the best. The F1
score of 0.607 for the CRFm+R system was rel-
atively close to the F1 score of 0.736 for agree-
ment between human annotators. To test whether
CRFm+R?s relatively high performance was due to
chance, we computed 99% confidence intervals for
the differences in F1 score between CRFm+R and
each of the other methods. We used the bias-
corrected and accelerated (BCa) Bootstrap (Efron
and Tibshirani, 1993) with 10,000 rounds of resam-
pling at the sentence level for each comparison. A
difference is statistically reliable at the ? level (i.e.,
p < ?) if the (1 ? ?)% confidence interval for the
difference does not contain zero, which corresponds
to the null hypothesis. Statistically reliable differ-
ences are indicated in Table 1. The only system that
did not have a reliably lower F1 score than CRFm+R
was CRFm?R, though due to the relatively small
size of our test set, we do not take this as strong ev-
idence against using the rule-based system features
in the CRF.
We note that while the CRFm+R system had lower
precision (0.556) than the CRFv+R system (0.763),
its threshold ? could be tuned to prefer high preci-
sion rather than the best development set F1. Such
tuning could be very important depending on the rel-
ative costs of false positives and false negatives for
a particular application.
We also computed the mean length of sequences
of one or more contiguous words labeled as shell.
Here also, we observed that the CRFm+R approach
provided a close match to human performance. The
mean lengths of shell for the first and second anno-
tators were 8.49 and 7.91 tokens, respectively. For
the CRFm+R approach, the mean length was slightly
higher at 9.96 tokens, but this was much closer to the
means of the human annotators than the mean for
the CRFv+R system, which was 13.30 tokens. For
the rule-based system, the mean length was 4.99 to-
kens, indicating that it captures short sequences such
as ?In addition,? more often than the other systems.
5 Observations about a New Domain
In this section, we apply our system to a corpus of
transcripts of political debates8 in order to under-
stand whether the system can generalize to a new
domain with a somewhat different style of argu-
mentation. Our analyses are primarily qualitative
in nature due to the lack of gold-standard annota-
tions. We chose two historically well-known debates
8The Lincoln?Douglas debates were downloaded from
http://www.bartleby.com/251/. The other debates
were downloaded from http://debates.org/.
25
(Lincoln?Douglas from 1858 and Kennedy?Nixon
from 1960) and two debates that occurred more re-
cently (Gore?Bush from 2000 and Obama?McCain
from 2008). These debates range in length from
38,000 word tokens to 65,000 word tokens.
Political debates are similar to the persuasive es-
says we used above in that debate participants state
their own claims and evidence as well as evaluate
their opponents? claims. They are different from es-
says in that they are spoken rather than written?
meaning that they contain more disfluencies, collo-
quial language, etc.?and that they cover different
social and economic issues. Also, the debates are in
some sense a dialogue between two people.
We tagged all the debates using the CRFm+R sys-
tem, using the same parameters as for the test set
experiments (?4).
First, we observed that a smaller percentage of
tokens were tagged as shell in the debates than in
the essays. For the annotated essay test set (?3.2),
the percentage of tokens tagged as shell was 14.0%
(11.6% were labeled as shell by the first annota-
tor). In contrast, the percentage of tokens tagged
as shell was 4.2% for Lincoln?Douglas, 5.4% for
Kennedy?Nixon, 4.6% for Gore?Bush, and 4.8% for
Obama?McCain. It is not completely clear whether
the smaller percentages tagged as shell are due to a
lack of coverage by the shell detector or more sub-
stantial differences in the domain.
However, it seems that these debates genuinely in-
clude less shell. One potential reason is that many of
the essay prompts asked test-takers to respond to a
particular argument, leading to responses containing
many phrases such as ?The speaker claims that? and
?However, the argument lacks specificity . . . ?.
We analyzed the system?s predictions and ex-
tracted a set of examples, some of which appear in
Table 2, showing true positives, where most of the
tokens appear to be labeled correctly as shell; false
positives, where tokens were incorrectly labeled as
shell; and false negatives, where the system missed
tokens that should have been marked.
Table 2 also provides some examples from our de-
velopment set, for comparison.
We observed many instances of correctly marked
shell, including many that appeared very different
in style than the language used in essays. For ex-
ample, Lincoln demonstrates an aggressive style in
the following: ?Now, I say that there is no charitable
way to look at that statement, except to conclude that
he is actually crazy.? Also, Bush employs a some-
what atypical sentence structure here: ?It?s not what
I think and its not my intentions and not my plan.?
However, the system also incorrectly tagged se-
quences as shell, particularly in short sentences (e.g.,
?Are we as strong as we should be??). It also missed
shell, partially or entirely, such as in the following
example: ?But let?s get back to the core issue here.?
These results suggest that although there is poten-
tial for improvement in adapting to new domains,
our approach to shell detection at least partially gen-
eralizes beyond our initial domain of persuasive es-
say writing.
6 Related Work
There has been much previous work on analyzing
discourse. In this section, we describe similarities
and differences between that work and ours.
Rhetorical structure theory (Mann and Thomp-
son, 1988) is perhaps the most relevant area of work.
See ?1 for a discussion.
In research on intentional structure, Grosz and
Sidner (1986) propose that any discourse is com-
posed of three interacting components: the linguistic
structure defined by the actual utterances, the inten-
tional structure defined by the purposes underlying
the discourse, and an attentional structure defined by
the discourse participants? focus of attention. De-
tecting shell may also be seen as trying to identify
explicit cues of intentional structure in a discourse.
Additionally, the categorization of shell spans as to
whether they evaluate the opponents claims, connect
ones own claims, etc., may be seen as determining
what Grosz and Sidener call ?discourse segment pur-
poses? (i.e., the intentions underlying the segments
containing the shell spans).
We can also view shell detection as the task of
identifying phrases that indicate certain types of
speech acts (Searle, 1975). In particular, we aim to
identify markers of assertive speech acts, which de-
clare that the speaker believes a certain proposition,
and expressive speech acts, which express attitudes
toward propositions.
Shell also overlaps with the concept of discourse
markers (Hutchinson, 2004), such as ?however? or
26
LINCOLN (L) ? DOUGLAS (D) DEBATES
TP L: Now, I say that there is no charitable way to look at that statement, except to conclude that he is
actually crazy.
L: The first thing I see fit to notice is the fact that . . .
FP D: He became noted as the author of the scheme to . . .
D: . . . such amendments were to be made to it as would render it useless and inefficient . . .
FN D: I wish to impress it upon you, that every man who voted for those resolutions . . .
L: That statement he makes, too, in the teeth of the knowledge that I had made the stipulation to
come down here . . .
KENNEDY (K) ? NIXON (N) DEBATES
TP N: I favor that because I believe that?s the best way to aid our schools . . .
N: And in our case, I do believe that our programs will stimulate the creative energies of . . .
FP N: We are for programs, in addition, which will see that our medical care for the aged . . .
K: Are we as strong as we should be?
FN K: I should make it clear that I do not think we?re doing enough . . .
N: Why did Senator Kennedy take that position then? Why do I take it now?
BUSH (B) ? GORE (G) DEBATES
TP B: It?s not what I think and its not my intentions and not my plan.
G: And FEMA has been a major flagship project of our reinventing government efforts. And I agree, it
works extremely well now.
FP B: First of all, most of this is at the state level.
G: And it focuses not only on increasing the supply, which I agree we have to do, but also on . . .
FN B: My opponent thinks the government?the surplus is the government?s money. That?s not what I
think
G: I strongly support local control, so does Governor Bush.
OBAMA (O) ? MCCAIN (M) DEBATES
TP M: But the point is?the point is, we have finally seen Republicans and Democrats sitting down and
negotiating together . . .
O: And one of the things I think we have to do is make sure that college is affordable . . .
FP O: . . . but in the short term there?s an outlay and we may not see that money for a while.
O: We have to do that now, because it will actually make our businesses and our families better off.
FN O: So I think the lesson to be drawn is that we should never hesitate to use military force . . . to keep the
American people safe.
O: But let?s get back to the core issue here.
PERSUASIVE ESSAYS (DEVELOPMENT SET, SPELLING ERRORS INCLUDED)
TP However, the argument lacks specificity and relies on too many questionable assumptions to make a
strong case for adopting an expensive and logistically complicated program.
I believe that both of these claims have been made in hase and other factors need to be considered.
FP Since they are all far from now, the prove is not strong enough to support the conclusion.
As we know that one mind can not think as the other does.
FN History has proven that . . .
The given issue which states that in any field of inquiry . . . is a controversional one.
Table 2: Examples of CRFm+R performance. Underlining marks tokens predicted to be shell, and bold font indicates
shell according to human judgments (our judgments for the debate transcripts, and the annotator?s judgments for the
development set). Examples include true positives (TP), false positives (FP), and false negatives (FN). Note that some
FP and FN examples include partially accurate predictions.
27
?therefore.? Discourse markers, however, are typ-
ically only single words or short phrases that ex-
press a limited number of relationships. On the other
hand, shell can capture longer sequences that ex-
press more complex relationships between the com-
ponents of an argumentative discourse (e.g., ?But
let?s get back to the core issue here? signals that the
following point is more important than the previous
one).
There are also various other approaches to ana-
lyzing arguments. Notably, much recent theoreti-
cal research on argumentation has focused on ar-
gumentation schemes (Walton et al, 2008), which
are high-level strategies for constructing arguments
(e.g., argument from consequences). Recently, Feng
and Hirst (2011) developed automated methods for
classifying texts by argumentation scheme. In sim-
ilar work, Anand et al (2011) use argumentation
schemes to identify tactics in blog posts (e.g., moral
appeal, social generalization, appeals to external au-
thorities etc.). Although shell language can certainly
be found in persuasive writing, it is used to orga-
nize the persuader?s tactics and claims rather than
to express them. For example, consider the follow-
ing sentence: ?It must be the case that this diet
works since it was recommended by someone who
lost 20 pounds on it.? In shell detection, we focus
on the lexico-syntactic level, aiming to identify the
bold words as shell. In contrast, work on argumenta-
tion schemes focuses at a higher level of abstraction,
aiming to classify the sentence as an attempt to per-
suade by appealing to an external authority.
7 Conclusions
In this paper, we described our approach to detect-
ing language used to explicitly structure an arguer?s
claims and pieces of evidence as well as explain
how they relate to an opponent?s argument. We im-
plemented a rule-based system, a supervised proba-
bilistic sequence model, and a principled hybrid ver-
sion of the two. We presented evaluations of these
systems using human-annotated essays, and we ob-
served that the hybrid sequence model system per-
formed the best. We also applied our system to po-
litical debates and found evidence of the potential to
generalize to new domains.
Acknowledgments
We would like to thank the annotators for helping
us create the essay data sets. We would also like
to thank James Carlson, Paul Deane, Yoko Futagi,
Beata Beigman Klebanov, Melissa Lopez, and the
anonymous reviewers for their useful comments on
the paper and annotation scheme.
References
P. Anand, J. King, J. Boyd-Graber, E. Wagner, C. Martell,
D. Oard, and P. Resnik. 2011. Believe me?we can
do this! annotating persuasive acts in blog text. In
Proc. of AAAI Workshop on Computational Models of
Natural Argument.
L. Carlson, D. Marcu, and M. E. Okurowski. 2001.
Building a discourse-tagged corpus in the framework
of rhetorical structure theory. In Proc. of the Second
SIGdial Workshop on Discourse and Dialogue.
B. Efron and R. Tibshirani. 1993. An Introduction to the
Bootstrap. Chapman and Hall/CRC.
V. W. Feng and G. Hirst. 2011. Classifying arguments
by scheme. In Proc. of ACL.
Barbara J. Grosz and Candace L. Sidner. 1986. Atten-
tion, Intentions, and the Structure of Discourse. Com-
put. Linguist., 12(3):175?204.
B. Hutchinson. 2004. Acquiring the meaning of dis-
course markers. In Proc. of ACL.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proc. of ICML.
W. C. Mann and S. A. Thompson. 1988. Rhetorical
structure theory: Toward a functional theory of text
organization. Text, 8(3).
D. Marcu. 2000. The Theory and Practice of Discourse
Parsing and Summarization. MIT Press.
N. Okazaki. 2007. CRFsuite: a fast implementation of
conditional random fields (CRFs).
J. R. Searle. 1975. A classification of illocutionary acts.
Language in Society, 5(1).
D. Walton, C. Reed, and F. Macagno. 2008. Argumenta-
tion Schemes. Cambridge University Press.
28
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 182?190,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Re-examining Machine Translation Metrics for Paraphrase Identification
Nitin Madnani Joel Tetreault
Educational Testing Service
Princeton, NJ, USA
{nmadnani,jtetreault}@ets.org
Martin Chodorow
Hunter College of CUNY
New York, NY, USA
martin.chodorow@hunter.cuny.edu
Abstract
We propose to re-examine the hypothesis that
automated metrics developed for MT evalu-
ation can prove useful for paraphrase iden-
tification in light of the significant work on
the development of new MT metrics over the
last 4 years. We show that a meta-classifier
trained using nothing but recent MT metrics
outperforms all previous paraphrase identifi-
cation approaches on the Microsoft Research
Paraphrase corpus. In addition, we apply our
system to a second corpus developed for the
task of plagiarism detection and obtain ex-
tremely positive results. Finally, we conduct
extensive error analysis and uncover the top
systematic sources of error for a paraphrase
identification approach relying solely on MT
metrics. We release both the new dataset and
the error analysis annotations for use by the
community.
1 Introduction
One of the most important reasons for the recent
advances made in Statistical Machine Translation
(SMT) has been the development of automated met-
rics for evaluation of translation quality. The goal
of any such metric is to assess whether the trans-
lation hypothesis produced by a system is seman-
tically equivalent to the source sentence that was
translated. However, cross-lingual semantic equiv-
alence is even harder to assess than monolingual,
therefore, most MT metrics instead try to measure
whether the hypothesis is semantically equivalent to
a human-authored reference translation of the same
source sentence. Using such automated metrics as
proxies for human judgments can provide a quick as-
sessment of system performance and allow for short
feature and system development cycles, which are
important for evaluating research ideas.
In the last 5 years, several shared tasks and com-
petitions have led to the development of increasingly
sophisticated metrics that go beyond the computa-
tion of n-gram overlaps (BLEU, NIST) or edit dis-
tances (TER, WER, PER etc.). Note that the task
of an MT metric is essentially one of identifying
whether the translation produced by a system is a
paraphrase of the reference translation. Although
the notion of using MT metrics for the task of para-
phrase identification is not novel (Finch et al, 2005;
Wan et al, 2006), it merits a re-examination in the
light of the development of these novel MT metrics
for which we can ask ?How much better, if at all,
do these newer metrics perform for the task of para-
phrase identification??
This paper describes such a re-examination. We
employ 8 different MT metrics for identifying
paraphrases across two different datasets - the
well-known Microsoft Research paraphrase corpus
(MSRP) (Dolan et al, 2004) and the plagiarism
detection corpus (PAN) from the 2010 Uncovering
Plagiarism, Authorship and Social Software Misuse
shared task (Potthast et al, 2010). We include both
MSRP and PAN in our study because they represent
two very different sources of paraphrased text. The
creation of MSRP relied on the massive redundancy
of news articles on the web and extracted senten-
tial paraphrases from different stories written about
the same topic. In the case of PAN, humans con-
sciously paraphrased existing text to generate new,
182
plagiarized text.
In the next section, we discuss previous work on
paraphrase identification. In ?3, we describe our ap-
proach to paraphrase identification using MT met-
rics as features. Our approach yields impressive re-
sults ? the current state of the art for MSRP and ex-
tremely positive for PAN. In the same section, we
examine whether each metric?s purported strength is
demonstrated in our datasets. Next, in ?4 we con-
duct an analysis of our system?s misclassifications
for both datasets and outline a taxonomy of errors
that our system makes. We also look at annotation
errors in the datasets themselves. We discuss the
findings of the error analysis in ?5 and conclude in
?6.
2 Related Work & Our Contributions
Our goal in this paper is to examine the utility of a
paraphrase identification approach that relies solely
on MT evaluation metrics and no other evidence of
semantic equivalence. Given this setup, the most rel-
evant previous work is by Finch et al (2005) which
uses BLEU, NIST, WER and PER as features for
a supervised classification approach using SVMs.
In addition, they also incorporate part-of-speech in-
formation as well as the Jiang-Conrath WordNet-
based lexical relatedness measure (Jiang and Con-
rath, 1997) into their edit distance calculations. In
the first part of our paper, we present classification
experiments with newer MT metrics not available in
2005, a worthwhile exercise in itself. However, we
go much further in our study:
? We apply our approach to two different para-
phrase datasets (MSRP and PAN) that were cre-
ated via different processes.
? We attempt to find evidence of each metric?s
purported strength in both datasets.
? We conduct an extensive error analysis to find
types of errors that a system based solely on
MT metrics is likely to make. In addition, we
also discover interesting paraphrase pairs in the
datasets.
? We release our sentence-level PAN dataset (see
?3.3.2) which contains more realistic exam-
ples of paraphrase and can prove useful to the
community for future evaluations of paraphrase
identification.
BLEU-based features were also employed by
Wan et al (2006) who use them in combination with
several other features based on dependency relations
and tree edit-distance inside an SVM.
There are several other supervised approaches to
paraphrase identification that do not use any features
based on MT metrics. Mihalcea et al (2006) com-
bine pointwise mutual information, latent semantic
analysis and WordNet-based measures of word se-
mantic similarity into an arbitrary text-to-text sim-
ilarity metric. Qiu et al (2006) build a frame-
work that detects dissimilarities between sentences
and makes its paraphrase judgment based on the
significance of such dissimilarities. Kozareva and
Montoyo (2006) use features based on LCS, skip
n-grams and WordNet with a meta-classifier com-
posed of SVM, k-nearest neighbor and maximum
entropy classifiers. Islam and Inkpen (2007) mea-
sure semantic similarity using a corpus-based mea-
sure and a modified version of the Longest Common
Subsequence (LCS) algorithm. Rus et al (2008)
take a graph-based approach originally developed
for recognizing textual entailment and adapt it for
paraphrase identification. Fernando and Stevenson
(2008) construct a matrix of word similarities be-
tween all pairs of words in both sentences instead
of relying only on the maximal similarities. Das and
Smith (2009) use an explicit model of alignment be-
tween the corresponding parts of two paraphrastic
sentences and combine it with a logistic regression
classifier built from n-gram overlap features. Most
recently, Socher et al (2011) employ a joint model
that incorporates the similarities between both sin-
gle word features as well as multi-word phrases ex-
tracted from the parse trees of the two sentences.
We compare our results to those from all the ap-
proaches described in this section later in ?3.4.
3 Classifying with MT Metrics
In this section, we first describe our overall approach
to paraphrase identification that utilizes only MT
metrics. We then discuss the actual MT metrics we
used. Finally, we describe the datasets on which we
evaluated our approach and present our results.
183
MSRP
They had published an advertisement on the Internet on June 10,
offering the cargo for sale, he added.
On June 10, the ship?s owners had published an advertisement on the
Internet, offering the explosives for sale.
Security lights have also been installed and police have swept
the grounds for booby traps.
Security lights have also been installed on a barn near the front gate.
PAN
Dense fogs wrapped the mountains that shut in the little hamlet,
but overhead the stars were shining in the near heaven.
The hamlet is surrounded by mountains which is wrapped with dense
fogs, though above it, near heaven, the stars were shining.
In still other places, the strong winds carry soil over long
distances to be mixed with other soils.
In other places, where strong winds blow with frequent regularity,
sharp soil grains are picked up by the air and hurled against the
rocks, which, under this action, are carved into fantastic forms.
Table 1: Examples of paraphrases and non-paraphrases (in italics) from the MSRP and PAN corpora.
3.1 Classifier
Our best system utilized a classifier combination ap-
proach. We used a simple meta-classifier that uses
the average of the unweighted probability estimates
from the constituent classifiers to make its final de-
cision. We used three constituent classifiers: Logis-
tic regression, the SMO implementation of a support
vector machine (Platt, 1999; Keerthi et al, 2001)
and a lazy, instance-based classifier that extends the
nearest neighbor algorithm (Aha et al, 1991). We
used the WEKA machine learning toolkit to perform
our experiments (Hall et al, 2009). 1
3.2 MT metrics used
1. BLEU (Papineni et al, 2002) is the most com-
monly used metric for MT evaluation. It is
computed as the amount of n-gram overlap?
for different values of n?between the system
output and the reference translation, tempered
by a penalty for translations that might be too
short. BLEU relies on exact matching and has
no concept of synonymy or paraphrasing. We
use BLEU1 through BLEU4 as 4 different fea-
1These constituent classifiers were chosen since they were
the top 3 performers in 5-fold cross-validation experiments
conducted on both MSRP and PAN training sets. The meta-
classifier was chosen similarly once the constituent classifiers
had been chosen.
tures for our classifier (hereafter BLEU(1-4)).
2. NIST (Doddington, 2002) is a variant of BLEU
that uses the arithmetic mean of n-gram over-
laps, rather than the geometric mean. It also
weights each n-gram according to its informa-
tiveness as indicated by its frequency. We use
NIST1 through NIST5 as 5 different features
for our classifier (hereafter NIST(1-5)).
3. TER (Snover et al, 2006) is defined as the
number of edits needed to ?fix? the translation
output so that it matches the reference. TER
differs from WER in that it includes a heuris-
tic algorithm to deal with shifts in addition to
insertions, deletions and substitutions.
4. TERp (TER-Plus) (Snover et al, 2009) builds
upon the core TER algorithm by providing ad-
ditional edit operations based on stemming,
synonymy and paraphrase.
5. METEOR (Denkowski and Lavie, 2010) uses
a combination of both precision and recall un-
like BLEU which focuses on precision. Fur-
thermore, it incorporates stemming, synonymy
(via WordNet) and paraphrase (via a lookup ta-
ble).
6. SEPIA (Habash and El Kholy, 2008) is a
syntactically-aware metric designed to focus on
184
structural n-grams with long surface spans that
cannot be captured efficiently with surface n-
gram metrics. Like BLEU, it is a precision-
based metric and requires a length penalty to
minimize the effects of length.
7. BADGER (Parker, 2008) is a language inde-
pendent metric based on compression and in-
formation theory. It computes a compression
distance between the two sentences that utilizes
the Burrows Wheeler Transformation (BWT).
The BWT enables taking into account common
sentence contexts with no limit on the size of
these contexts.
8. MAXSIM (Chan and Ng, 2008) treats the
problem as one of bipartite graph matching and
maps each word in one sentence to at most one
word in the other sentence. It allows the use of
arbitrary similarity functions between words.2
Our choice of metrics was based on their popular-
ity in the MT community, their performance in open
competitions such as the NIST MetricsMATR chal-
lenge (NIST, 2008) and the WMT shared evaluation
task (Callison-Burch et al, 2010), their availability,
and their relative complementarity.
3.3 Datasets
In this section, we describe the two datasets that we
used to evaluate our approach.
3.3.1 Microsoft Research Paraphrase Corpus
The MSRP corpus was created by mining news
articles on the web for topically similar articles and
then extracting potential sentential paraphrases us-
ing a set of heuristics. Extracted pairs were then
shown to two human judges with disagreements
handled by a third adjudicator. The kappa was re-
ported as 0.62, which indicates moderate to high
agreement. We used the pre-stipulated train-test
splits (4,076 sentence pairs in training and 1,725 in
test) to train and test our classifier.
2We also experimented with TESLA?a variant of
MAXSIM that performs better for MT evaluation?in our pre-
liminary experiments However, both MAXSIM and TESLA
performed almost identically in our cross-validation experi-
ments. Therefore, we only retained MAXSIM in our final ex-
periment since it was significantly faster to run than the version
of TESLA we had.
3.3.2 Plagiarism Detection Corpus (PAN)
We wanted to evaluate our approach on a set of
paraphrases where the semantic similarity was not
simply an accidental by-product of topical similarity
but rather consciously generated. We used the test
collection from the PAN 2010 plagiarism detection
competition. This dataset consists of 41,233 text
documents from Project Gutenberg in which 94,202
cases of plagiarism have been inserted. The pla-
giarism was created either by using an algorithm or
by explicitly asking Turkers to paraphrase passages
from the original text. We focus only on the human-
created plagiarism instances.
Note also that although the original PAN dataset
has been used in plagiarism detection shared tasks,
those tasks are generally formulated differently in
that the goal is to find all potentially plagiarized pas-
sages in a given set of documents along with the cor-
responding source passages from other documents.
In this paper, we wanted to focus on the task of iden-
tifying whether two given sentences can be consid-
ered paraphrases.
To generate a sentence-level PAN dataset, we
wrote a heuristic alignment algorithm to find cor-
responding pairs of sentences within a passage pair
linked by the plagiarism relationship. The align-
ment algorithm utilized only bag-of-words overlap
and length ratios and no MT metrics. For our nega-
tive evidence, we sampled sentences from the same
document and extracted sentence pairs that have at
least 4 content words in common. We then sampled
randomly from both the positive and negative evi-
dence files to create a training set of 10,000 sentence
pairs and a test set of 3,000 sentence pairs.
Table 1 shows examples of paraphrastic and non-
paraphrastic sentence pairs from both the MSRP and
PAN datasets.
3.4 Results
Before presenting the results of experiments that
used multiple metrics as features, we wanted to de-
termine how well each metric performs on its own
when used for paraphrase identification. Table 2
shows the classification results on both the MSRP
and PAN datasets using each metric as the only fea-
ture. Although previously explored metrics such as
BLEU and NIST perform reasonably well, they are
185
MSRP PAN
Metric Acc. F1 Acc. F1
MAXSIM 67.2 79.4 84.7 83.4
BADGER 67.6 79.9 88.5 87.9
SEPIA 68.1 79.8 87.7 86.8
TER 69.9 80.9 85.7 83.8
BLEU(1-4) 72.3 80.9 87.9 87.1
NIST(1-5) 72.8 81.2 88.2 87.3
METEOR 73.1 81.0 89.5 88.9
TERp 74.3 81.8 91.2 90.9
Table 2: Classification results for MSRP and PAN with
individual metrics as features. Entries are sorted by accu-
racies on MSRP.
clearly outperformed by some of the more robust
metrics such as TERp and METEOR.
Table 3 shows the results of our experiments em-
ploying multiple metrics as features, for both MSRP
and PAN. The final row in the table shows the results
of our best system. The remaining rows of this table
show the top performing metrics for both datasets;
we treat BLEU, NIST and TER as our baseline met-
rics since they are not new and are not the primary
focus of our investigation. In terms of novel met-
rics, we find that the top 3 metrics for both datasets
were TERp, METEOR and BADGER respectively
as shown. Combining all 8 metrics led to the best
performance for MSRP but showed no performance
increase for PAN.
MSRP PAN
Features Acc. F1 Acc. F1
Base Metrics 74.1 81.5 88.6 87.8
+ TERp 75.6 82.5 91.5 91.2
+ METEOR 76.6 83.2 92.0 91.8
+ BADGER 77.0 83.7 92.3 92.1
+ Others 77.4 84.1 92.3 92.1
Table 3: The top 3 performing MT metrics for both
MSRP and PAN datasets as identified by ablation stud-
ies. BLEU(1-4), NIST(1-5) and TER were used as the 10
base features in the classifiers.
Our results for the PAN dataset are much better than
those for MSRP since:
(a) It is likely that our negative evidence is too easy
for most MT metrics.
(b) Many plagiarized pairs are linked simply via
lexical synonymy which can be easily captured
by metrics like METEOR and TERp, e.g., the
sentence ?Young?s main contention is that in lit-
erature genius must make rules for itself, and
that imitation is suicidal? is simply plagiarized
as ?Young?s major argument is that in litera-
ture intellect must make rules for itself, and
that replication is dangerous.? However, the
PAN corpus does contains some very challeng-
ing and interesting examples of paraphrases?
even more so than MSRP?which we describe
in ?4.
Finally, Table 4 shows that the results from our
best system are the best ever reported on the MSRP
test set when compared to all previously published
work. Furthermore, the single best performing met-
ric (TERp)?also shown in the table?outperforms,
by itself, many previous approaches utilizing multi-
ple, complex features.
Model Acc. F1
All Paraphrase Baseline 66.5 79.9
(Mihalcea et al, 2006) 70.3 81.3
(Rus et al, 2008) 70.6 80.5
(Qiu et al, 2006) 72.0 81.6
(Islam and Inkpen, 2007) 72.6 81.3
(Fernando and Stevenson, 2008) 74.1 82.4
TERp 74.3 81.8
(Finch et al, 2005) 75.0 82.7
(Wan et al, 2006) 75.6 83.0
(Das and Smith, 2009) 76.1 82.7
(Kozareva and Montoyo, 2006) 76.6 79.6
(Socher et al, 2011) 76.8 83.6
Best MT Metrics 77.4 84.1
Table 4: Comparing the accuracy and F -score for the sin-
gle best performing MT metric TERp (in gray) as well as
the best metric combination system (in gray and bold)
with previously reported results on the MSRP test set
(N = 1, 752). Entries are sorted by accuracy.
3.5 Metric Contributions
In addition to quantitative results, we also wanted to
highlight specific examples from our datasets that
can demonstrate the strength of the new metrics
over simple n-gram overlap and edit-distance based
metrics. Below we present examples for the 4 best
186
metrics across both datasets:
? TERp uses stemming and phrasal paraphrase
recognition to accurately classify the sentence
pair ?For the weekend, the top 12 movies
grossed $157.1 million, up 52 percent from
the same weekend a year earlier.? and ?The
overall box office soared, with the top 12
movies grossing $157.1 million, up 52 percent
from a year ago.? from MSRP as paraphrases.
? METEOR uses synonymy and stemming
to accurately classify the sentence pair ?Her
letters at this time exhibited the two extremes of
feeling in a marked degree.? and ?Her letters
at this time showed two extremes of feelings.?
from PAN as plagiarized.
? BADGER uses unsupervised contextual
similarity detection to accurately classify the
sentence pair ?Otherwise they were false or
mistaken reactions? and ?Otherwise, were false
or wrong responses? from PAN as plagiarized.
? SEPIA uses structural n-grams via dependency
trees to accurately classify the sentence pair
?At his sentencing, Avants had tubes in his
nose and a portable oxygen tank beside him.?
and ?Avants, wearing a light brown jumpsuit,
had tubes in his nose and a portable oxygen
tank beside him.? from MSRP as paraphrases.
4 Error Analysis
In this section, we conduct an analysis of the
misclassifications that our system makes on both
datasets. Our analyses consisted of finding the sen-
tences pairs from the test set for each dataset which
none of our systems (not just the best one) ever clas-
sified correctly and inspecting a random sample of
100 of these. This inspection yields not only the top
sources of error for an approach that relies solely on
MT metrics but also uncovers sources of annotation
errors in both datasets themselves.
4.1 MSRP
In their paper describing the creation of the MSRP
corpus, Dolan et al (2004) clearly state that ?the de-
gree of mismatch allowed before the pair was judged
non-equivalent was left to the discretion of the indi-
vidual rater? and that ?many of the 33% of sentence
pairs judged to be not equivalent still overlap signif-
icantly in information content and even wording?.
We found evidence that the raters were not always
consistent in applying the annotation guidelines. For
example, in some cases the lack of attribution for a
quotation led the raters to label a pair as paraphrastic
whereas in other cases it did not. For example, the
pair ?These are real crimes that hurt a lot of people.?
and ??These are real crimes that disrupt the lives of
real people,? Smith said.? was not marked as para-
phrastic. Furthermore, even though the guidelines
instruct the raters to ?treat anaphors and their full
forms as equivalent, regardless of how great the dis-
parity in length or lexical content between the two
sentences?, we found pairs of sentences marked as
non-paraphrastic which only differed in anaphora.
However, the primary goal of this analysis is to find
sources of errors in an MT-metric driven approach
and below we present the top 5 such sources:
1. Misleading Lexical Overlap. Non-
paraphrastic pairs where there is large
lexical overlap of secondary material between
the two sentences but the primary semantic
content is different. For example, ?Gyorgy
Heizler, head of the local disaster unit, said the
coach had been carrying 38 passengers.?
and ?The head of the local disaster
unit, Gyorgy Heizler, said the coach
driver had failed to heed red stop lights.?.
2. Lack of World Knowledge. Paraphrastic
pairs that require world knowledge. For ex-
ample, ?Security experts are warning that a
new mass-mailing worm is spreading widely
across the Internet, sometimes posing as e-
mail from the Microsoft founder.? and ?A
new worm has been spreading rapidly across
the Internet, sometimes pretending to be
an e-mail from Microsoft Chairman Bill Gates,
antivirus vendors said Monday.?.
3. Tricky Phrasal Paraphrases. Paraphras-
187
tic pairs that contain domain-dependent se-
mantic alternations. For example, ?The
leading actress nod went to energetic new-
comer Marissa Jaret Winokur as Edna?s
daughter Tracy.? and ?Marissa Jaret Winokur,
as Tracy, won for best actress in a musical.?.
4. Date, Time and Currency Differences. Para-
phrastic pairs that contain different temporal
or currency references. These references were
normalized to generic tokens (e.g., $NUMBER)
before being shown to MSRP raters but are re-
tained in the released dataset. For example,
?Expenses are expected to be approximately
$2.3 billion, at the high end of the previous ex-
pectation of $2.2-to-$2.3 billion.? and ?Spend-
ing on research and development is expected to
be $4.4 billion for the year, compared with the
previous expectation of $4.3 billion.?.
5. Anaphoric References. Paraphrastic pairs
wherein one member of the pair contains
anaphora and the other doesn?t (these are con-
sidered paraphrases according to MSRP guide-
lines). For example, ?They certainly reveal a
very close relationship between Boeing and se-
nior Washington officials.? and ?The e-mails
reveal the close relationship between Boeing
and the Air Force.?.
Note that most misclassified sentence pairs can be
categorized into more than one of the above cate-
gories.
4.2 PAN
For the PAN corpus, the only real source of error in
the dataset itself was the sentence alignment algo-
rithm. There were many sentence pairs that were
erroneously linked as paraphrases. Leaving aside
such pairs, the 3 largest sources of error for our MT-
metric based approach were:
1. Complex Sentential Paraphrases. By far,
most of the misclassified pairs were paraphras-
tic pairs that could be categorized as real world
plagiarism, i.e., where the plagiarizer copies
the idea from the source but makes several
complex transformations, e.g., sentence split-
ting, structural paraphrasing etc. so as to ren-
der an MT-metric based approach powerless.
For example, consider the pair ?The school
bears the honored name of one who, in the long
years of the anti-slavery agitation, was known
as an uncompromising friend of human free-
dom.? and ?The school is named after a man
who defended the right of all men and women
to be free, all through the years when people
campaigned against slavery.? Another inter-
esting example is the pair ?The most unpromis-
ing weakly-looking creatures sometimes live to
ninety while strong robust men are carried off
in their prime.? and ?Sometimes the strong per-
sonalities live shorter than those who are unex-
pected.?.
2. Misleading Lexical Overlap. Similar to
MSRP. For example, ?Here was the second pe-
riod of Hebraic influence, an influence wholly
moral and religious.? and ?This was the sec-
ond period of Hellenic influence, an influence
wholly intellectual and artistic.?.
3. Typographical and Spelling Errors. Para-
phrastic pairs where the Turkers creating the
plagiarism also introduced other typos and
spelling errors. For example, ?The boat then
had on board over 1,000 souls in all? and
?1000 people where on board at that tim?.
5 Discussion
The misses due to ?Date, Time, and Currency Dif-
ferences? are really just the result of an artifact in
the testing. It is possible that an MT metrics based
approach could accurately predict these cases if the
references to dates etc. were replaced with generic
tokens as was done for the human raters. In a
similar vein, some of the misses that are due to a
lack of world knowledge might become hits if a
named entity recognizer could discover that ?Mi-
crosoft founder? is the same as ?Microsoft Chair-
man?. Similarly, some of the cases of anaphoric ref-
erence might be recognized with an anaphora res-
olution system. And the problem of misspelling in
PAN could be remedied with automatic spelling cor-
rection. Therefore, it is possible to improve the MT
metrics based approach further by utilizing certain
NLP systems as pre-processing modules for the text.
The only error category in MSRP and PAN
188
that caused false positives was ?Misleading Lexical
Overlap?. Here, the take-away message is that not
every part of a sentence is equally important for rec-
ognizing semantic equivalence or non-equivalence.
In a sentence that describes what someone commu-
nicated, the content of what was said is crucial. For
example, despite lexical matches everywhere else,
the mismatch of ?the coach had been carrying 38
passengers? and ?the driver had failed to heed the
red stop lights? disqualifies the respective sentences
from being paraphrases. Along the same line, dif-
ferences in proper names and their variants should
receive more weight than other words. A sentence
about ?Hebraic influence? on a period in history is
not the same as a sentence which matches in ev-
ery other way but is instead about ?Hellenic influ-
ence?. These sentences represent a bigger chal-
lenge for an approach based solely on MT metrics.
Given enough pairs of ?near-miss? non-paraphrases,
our system might be able to figure this out, but this
would require a large amount of annotated data.
6 Conclusions
In this paper, we re-examined the idea that automatic
metrics used for evaluating translation quality can
perform well explicitly for the task of paraphrase
recognition. The goal of our paper was to deter-
mine whether approaches developed for the related
but different task of MT evaluation can be as com-
petitive as approaches developed specifically for the
task of paraphrase identification. While we do treat
the metrics as black boxes to an extent, we explic-
itly chose metrics that were high performing but also
complementary in nature.
Specifically, our re-examination focused on the
more sophisticated MT metrics of the last few years
that claim to go beyond simple n-gram overlap and
edit distance. We found that a meta-classifier trained
using only MT metrics outperforms all previous ap-
proaches for the MSRP corpus. Unlike previous
studies, we also applied our approach to a new pla-
giarism dataset and obtained extremely positive re-
sults. We examined both datasets not only to find
pairs that demonstrated the strength of each met-
ric but also to conduct an error analysis to discover
the top sources of errors that an MT metric based
approach is susceptible to. Finally, we discovered
that using the TERp metric by itself provides fairly
good performance and can outperform many other
supervised classification approaches utilizing multi-
ple, complex features.
We also have two specific suggestions that we be-
lieve can benefit the community. First, we believe
that binary indicators of semantic equivalence are
not ideal and a continuous value between 0 and 1
indicating the degree to which two pairs are para-
phrastic is more suitable for most approaches. How-
ever, rather than asking annotators to rate pairs on
a scale, a better idea might be to show the sentence
pairs to a large number of Turkers (? 20) on Ama-
zon Mechanical Turk and ask them to classify it as
either a paraphrase or a non-paraphrase. A simple
estimate of the degree of semantic equivalence of
the pair is simply the proportion of the Turkers who
classified the pair as paraphrastic. An example of
such an approach, as applied to the task of grammat-
ical error detection, can be found in (Madnani et al,
2011).3 Second, we believe that the PAN corpus?
with Turker simulated plagiarism?contains much
more realistic examples of paraphrase and should
be incorporated into future evaluations of paraphrase
identification. In order to encourage this, we are re-
leasing our PAN dataset containing 13,000 sentence
pairs.
We are also releasing our error analysis data (100
pairs for MSRP and 100 pairs for PAN) since they
might prove useful to other researchers as well. Note
that the annotations for this analysis were produced
by the authors themselves and, although, they at-
tempted to accurately identify all error categories for
most sentence pairs, it is possible that the errors in
some sentence pairs were not comprehensively iden-
tified.4
Acknowledgments
We would like to thank Aoife Cahill, Michael Heil-
man and the three anonymous reviewers for their
useful comments and suggestions.
3A good approximation is to use an ordinal scale for the
human judgments as in the Semantic Textual Similarity task
of SemEval 2012. See http://www.cs.york.ac.uk/
semeval-2012/task6/ for more details.
4The data is available at http://bit.ly/mt-para.
189
References
D. W. Aha, D. Kibler, and M. K. Albert. 1991. Instance-
based learning algorithms. Mach. Learn., 6:37?66.
C. Callison-Burch, P. Koehn, C. Monz, K. Peterson, and
O. Zaidan, editors. 2010. Proceedings of the Joint
Fifth Workshop on Statistical Machine Translation and
MetricsMATR.
Y. S. Chan and H. T. Ng. 2008. MAXSIM: A maxi-
mum similarity metric for machine translation evalua-
tion. In Proceedings of ACL-HLT, pages 55?62.
D. Das and N.A. Smith. 2009. Paraphrase Identifica-
tion as Probabilistic Quasi-synchronous Recognition.
In Proceedings of ACL-IJCNLP, pages 468?476.
M. Denkowski and M. Lavie. 2010. Extending the
METEOR Machine Translation Metric to the Phrase
Level. In Proceedings of NAACL.
G. Doddington. 2002. Automatic Evaluation of Machine
Translation Quality using N-gram Co-occurrence
Statistics. In Proceedings of HLT, pages 138?145.
B. Dolan, C. Quirk, and C. Brockett. 2004. Unsuper-
vised Construction of Large Paraphrase Corpora: Ex-
ploiting Massively Parallel News Sources. In Proceed-
ings of COLING, pages 350?356, Geneva, Switzer-
land.
S. Fernando and M. Stevenson. 2008. A Semantic Simi-
larity Approach to Paraphrase Detection. In Proceed-
ings of the Computational Linguistics UK (CLUK)
11th Annual Research Colloquium.
A. Finch, Y.S. Hwang, and E. Sumita. 2005. Using Ma-
chine Translation Evaluation Techniques to Determine
Sentence-level Semantic Equivalence. In Proceedings
of the Third International Workshop on Paraphrasing,
pages 17?24.
N. Habash and A. El Kholy. 2008. SEPIA: Surface
Span Extension to Syntactic Dependency Precision-
based MT Evaluation. In Proceedings of the Workshop
on Metrics for Machine Translation at AMTA.
M. Hall, E. Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA Data Mining Software: An Update.
SIGKDD Explorations, 11.
A. Islam and D. Inkpen. 2007. Semantic Similarity of
Short Texts. In Proceedings of RANLP, pages 291?
297.
J. J. Jiang and D. W. Conrath. 1997. Semantic Similar-
ity Based on Corpus Statistics and Lexical Taxonomy.
CoRR, cmp-lg/9709008.
S. S. Keerthi, S. K. Shevade, C. Bhattacharyya, and
K. R. K. Murthy. 2001. Improvements to Platt?s SMO
Algorithm for SVM Classifier Design. Neural Com-
put., 13(3):637?649.
Z. Kozareva and A. Montoyo. 2006. Paraphrase Identi-
fication on the Basis of Supervised Machine Learning
Techniques. In Proceedings of FinTAL, pages 524?
233.
N. Madnani, J. Tetreault, M. Chodorow, and A. Ro-
zovskaya. 2011. They Can Help: Using Crowdsourc-
ing to Improve the Evaluation of Grammatical Error
Detection Systems. In Proceedings of ACL (Short Pa-
pers).
R. Mihalcea, C. Corley, and C. Strapparava. 2006.
Corpus-based and Knowledge-based Measures Of
Text Semantic Similarity. In Proceedings of AAAI,
pages 775?780.
NIST. 2008. NIST MetricsMATR Challenge. Informa-
tion Access Division. http://www.itl.nist.
gov/iad/mig/tests/metricsmatr/.
K. Papineni, S. Roukos, T. Ward, and W. J. Zhu. 2002.
BLEU: A Method for Automatic Evaluation of Ma-
chine Translation. In Proceedings of ACL.
S. Parker. 2008. BADGER: A New Machine Translation
Metric. In Proceedings of the Workshop on Metrics
for Machine Translation at AMTA.
John C. Platt. 1999. Advances in kernel methods. chap-
ter Fast Training of Support Vector Machines using Se-
quential Minimal Optimization, pages 185?208. MIT
Press.
M. Potthast, B. Stein, A. Barro?n-Ceden?o, and P. Rosso.
2010. An Evaluation Framework for Plagiarism De-
tection. In Proceedings of COLING, pages 997?1005.
L. Qiu, M. Y. Kan, and T. S. Chua. 2006. Paraphrase
Recognition via Dissimilarity Significance Classifica-
tion. In Proceedings of the EMNLP, pages 18?26.
V. Rus, P.M. McCarthy, M.C. Lintean, D.S. McNamara,
and A.C. Graesser. 2008. Paraphrase Identification
with Lexico-Syntactic Graph Subsumption. In Pro-
ceedings of FLAIRS, pages 201?206.
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A Study of Translation Edit Rate
with Targeted Human Annotation. In Proceedings of
AMTA.
M. Snover, N. Madnani, B. Dorr, and R. Schwartz. 2009.
TER-Plus: Paraphrase, Semantic, and Alignment En-
hancements to Translation Edit Rate. Machine Trans-
lation, 23(2?3):117?127.
R. Socher, E.H. Huang, J. Pennington, A.Y. Ng, and C.D.
Manning. 2011. Dynamic Pooling and Unfolding
Recursive Autoencoders for Paraphrase Detection. In
Advances in Neural Information Processing Systems
24 (NIPS).
S. Wan, R. Dras, M. Dale, and C. Paris. 2006. Using
Dependency-based Features to Take the ?para-farce?
Out of Paraphrase. In Proceedings of the Australasian
Language Technology Workshop (ALTW), pages 131?
138.
190
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 284?294,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Correcting Comma Errors in Learner Essays, and Restoring Commas in
Newswire Text
Ross Israel
Indiana University
Memorial Hall 322
Bloomington, IN 47405, USA
raisrael@indiana.edu
Joel Tetreault
Educational Testing Service
660 Rosedale Road
Princeton, NJ 08541, USA
jtetreault@ets.org
Martin Chodorow
Hunter College of CUNY
695 Park Avenue
New York, NY 10065, USA
mchodoro@hunter.cuny.edu
Abstract
While the field of grammatical error detection
has progressed over the past few years, one
area of particular difficulty for both native and
non-native learners of English, comma place-
ment, has been largely ignored. We present a
system for comma error correction in English
that achieves an average of 89% precision and
25% recall on two corpora of unedited student
essays. This system also achieves state-of-the-
art performance in the sister task of restor-
ing commas in well-formed text. For both
tasks, we show that the use of novel features
which encode long-distance information im-
proves upon the more lexically-driven features
used in prior work.
1 Introduction
Automatically detecting and correcting grammati-
cal errors in learner language is a growing sub-field
of Natural Language Processing. As the field has
progressed, we have seen research focusing on a
range of grammatical phenomena including English
articles and prepositions (c.f. Tetreault et al, 2010;
De Felice and Pulman, 2008), particles in Korean
and Japanese (c.f. Dickinson et al, 2011; Oyama,
2010), and broad approaches that aim to find mul-
tiple error types (c.f Rozovskaya et al, 2011; Ga-
mon, 2011). However, to the best of our knowledge,
there has not been any research published specifi-
cally on correcting erroneous comma usage in En-
glish (though there have been efforts such as the MS
Word grammar checker, and products like Gram-
marly and White Smoke that include comma check-
ing).
There are a variety of reasons that motivate our
interest in attempting to correct comma errors. First
of all, a review of error typologies in Leacock et al
(2010) reveals that comma usage errors are the
fourth most common error type among non-native
writers in the Cambridge Learner Corpus (Nicholls,
1999), which is composed of millions of words of
text from essays written by learners of English. The
problem of comma usage is not limited to non-
native writers; six of the top twenty error types for
native writers involve misuse of commas (Connors
and Lunsford, 1988). Given these apparent deficits
among both non-native and native speakers, devel-
oping a sound methodology for automatically iden-
tifying comma errors will prove useful in both learn-
ing and automatic assessment environments.
A quick examination of English learner essays re-
veals a variety of errors, with writers both overusing
and underusing commas in certain contexts. Con-
sider examples (1) and (2):
(1) erroneous: If you want to be a master you
should know your subject well.
corrected: If you want to be a master , you
should know your subject well.
(2) erroneous: I suppose , that it is better to spe-
cialize in one specific subject.
corrected: I suppose that it is better to special-
ize in one specific subject.
In example (1), an introductory conditional phrase
begins the sentence, but the learner has not used the
appropriate comma to separate the dependent clause
from the independent clause. The comma in this
case helps the reader to see where one clause ends
284
and another begins. In example (2), the comma after
suppose is unnecessary in American English, and al-
though this error is related more to style than to read-
ability, most native writers would omit the comma
in this context, so it should be avoided by learners as
well.
Another motivating factor for this work is the fact
that sentence internal punctuation contributes to the
overall readability of a sentence (Hill and Murray,
1998). Proper comma placement can lead to faster
reading times and reduce the need to re-read en-
tire sentences. Commas also help remove or reduce
problems arising from difficult ambiguities; the gar-
den path effect can be greatly reduced if commas are
correctly inserted after introductory phrases and re-
duced relative clauses.
This paper makes the following contributions:
? We present the first published comma error cor-
rection system for English, evaluated on essays
written by both native and non-native speakers
of English.
? The same system also achieves state-of-the-art
performance in the task of restoring commas in
well-edited text.
? We describe a novel annotation scheme that al-
lows for robust mark up of comma errors and
use it to annotate two corpora of student essays.
? We show that distance and combination fea-
tures can improve performance for both the er-
ror correction and restoration tasks.
The rest of this paper is organized as follows.
In section 2, we review prior work. Section 3 de-
tails our typology of comma usage. We discuss our
choice of classifier and selection of features in sec-
tion 4. In section 5, we apply our system to the task
of comma restoration. We describe our annotation
scheme and error correction system and evaluation
in sections 6 and 7. Finally, we summarize and out-
line plans for future research in section 8.
2 Previous Work
The only reported research that we are aware
of which specifically deals with comma errors in
learner writing is reported in Hardt (2001) and Ale-
gria et al (2006), two studies that deal with Dan-
ish and Basque, respectively. Hardt (2001) employs
an error driven approach featuring the Brill tagger
(Brill, 1993). The Brill tagger works as it would
for the part-of-speech tagging task for which it was
designed, i.e. it learns rules based on templates by
iterating over a large corpus. This work is also eval-
uated on native text where all existing commas are
considered correct, and additional ?erroneous? com-
mas are added randomly to a sub-corpus, so that the
tagger can learn from the errors. The system is tested
on a distinct subset for the task of correcting exist-
ing comma errors and achieves 91.4% precision and
76.9% recall.
Alegria et al (2006) compare implementations
of Naive Bayes, decision-tree, and support vector
machine (SVM) classifiers and utilize a feature set
based on word-forms, categories, and syntactic in-
formation about each decision point. While the sys-
tem is designed as a possible means for correcting
errors, it is only evaluated on the task of restor-
ing commas in well-formed text produced by native
writers. The system obtains good precision (96%)
and recall (98.3%) for correctly not inserting com-
mas, but performs less well at actually inserting
commas (69.6% precision, 48.6% recall).
It is important to note that the results in both of the
projects are based on constructed errors in an other-
wise native corpus which is free of any other con-
textual errors that might be present in actual learner
data. Moreover, as we will show in section 6, er-
rors of omission (failing to use needed commas) are
much more common than errors of commission (in-
serting commas inappropriately) in the English as
a Foreign Language (EFL) data that we use. Cru-
cially, our error correction efforts described in sec-
tion 7 must be able to account for noise and be able
to insert new commas as well as remove erroneous
ones, as we do evaluate on a set of English learner
essays.
Although we have not found any work published
specifically on correcting comma errors in English,
for language learners or otherwise, there is a fairly
large amount of work that focuses on the task of
comma restoration. Comma restoration refers to
placing commas in a sentence which is presented
with no sentence internal punctuation. This task is
285
mostly attempted in the larger context of Automatic
Speech Recognition (ASR), since there are no ab-
solute cues of where commas should be placed in a
stream of speech. Many of these systems use feature
sets that include prosodic elements that are clearly
not available for text based work (see e.g., Favre
et al, 2009; Huang and Zweig, 2002; Moniz et al,
2009).
There are, however, a few punctuation restora-
tion projects that have used well-formed text-only
data. Shieber and Tao (2003) explore restoring com-
mas to the Wall Street Journal (WSJ) section of
the Penn Treebank (PTB). The authors augment a
HMM trigram-based system with constituency parse
information at each insertion point. Using fully
correct parses directly from the PTB, the authors
achieve an F-score of 74.8% and sentence accuracy
of 57.9%1. However, a shortcoming of this method-
ology is that it dictates that all commas are missing,
but these parses were generated with comma infor-
mation present in the sentence and moreover hand-
corrected by human annotators. Using parses auto-
matically generated with commas removed from the
data, they achieve an F-score of 70.1% and sentence
accuracy of 54.9%.
More recently, Gravano et al (2009), who work
with newswire text, including WSJ, pursue the task
of inserting all punctuation and correcting capital-
ization in a string of text in a single pass, rather
than just comma restoration, but do provide results
based solely on comma insertion. The authors em-
ploy an n-gram language model and experiment with
n-grams from size n = 3 to n = 6, and with different
training data sizes. The result relevant to our work is
their comma F-score on WSJ test data, which is just
over 60% when using 5-grams and 55 billion train-
ing tokens. Baldwin and Joseph (2009) also restore
punctuation and capitalization to newswire texts, us-
ing machine based learning with retagging. Their
results are difficult to compare with our work be-
cause they use a different data set and do not focus
on commas in their evaluation.
Lu and Ng (2010) take an approach that inserts all
1Sentence accuracy is a measure used by some in the field
that counts sentences with 100% correct comma decisions as
correct, and any sentence where a comma is missing or mis-
takenly placed as incorrect. It is motivated by the idea that all
commas are essential to understanding a sentence.
punctuation symbols into text. They use transcribed
English and Chinese speech data and do not provide
specific evaluation for commas, however one im-
portant contribution of their research to our current
task is the finding that Conditional Random Fields
(CRFs) perform better at this task than Hidden Event
Language Models, another algorithm that has been
used for restoration. One reason for this could be
CRFs? better handling of long range dependencies
because they model the entire sequence, rather than
making a singular decision based on information at
each point in the sequence (Liu et al, 2005). CRFs
also do not suffer from the label bias problem that
affects Maximum Entropy classifiers (Lafferty et al,
2001).
3 Comma Usage
One of the challenges present in this research is the
ambiguity as to what constitutes ?correct? comma
usage in American English. For one thing, not
all commas contribute to grammaticality; some are
more tied to stylistic rules and preferences. While
there are certainly rule-based decision points for
comma insertion (Doran, 1998), particularly in the
case of commas that set off significant chunks or
phrases within sentences, there are also some com-
mas that appear to be more prescriptive, as they have
less of an effect on sentence processing (such as in
example (2) in the introduction), and opposing us-
age rules for the same contexts are attested in differ-
ent style manuals. A common example of opposing
rules is the notorious serial or Oxford comma that
refers to the final comma found in a series, which
is required by the Chicago Manual of Style (Univer-
sity of Chicago, 1993), but is considered incorrect
by the New York Times Manual of Style (Siegal and
Connolly, 1999).
As a starting point, we needed to know what kinds
of commas are taught by English language teachers,
as well as what style manuals recommend and/or re-
quire. However, creating a list of comma uses was
a non-trivial part of the process. After consulting
style manuals (University of Chicago, 1993; Siegal
and Connolly, 1999; Strunk and White, 1999) and
popular ESL websites, we compiled a list of over 30
rules for use of commas in English. We took the
most commonly mentioned rules and created a final
286
Rule Example
Elements in a List Paul put the kettle on, Don fetched the teapot, and I made tea.
Initial Word/Phrase Hopefully, this car will last for a while.
Dependent Clause After I brushed the cat, I lint-rollered my clothes.
Independent Clause I have finished painting, but he is still sanding the doors.
Parentheticals My father, a jaded and bitter man, ate the muffin.
Quotations ?Why,? I asked, ?do you always forget to do it??
Adjectives She is a strong, healthy woman.
Conjunctive Adverbs I would be happy, however, to volunteer for the Red Cross.
Contrasting Elements He was merely ignorant, not stupid.
Numbers 345,280,000
Dates She met her husband on December 5, 2003.
Geographical Names I lived in San Francisco, California, for 20 years.
Titles Al Mooney, M.D., is a good doctor
Introducing Words You may be required to bring many items, e.g., spoons, pans, and flashlights.
Other Catch-all rule for any other comma use
Table 1: Common Comma Uses
list of 15 usage rules (the 14 most common plus one
miscellaneous category) for our annotation scheme,
which is discussed in section 6. These rules are
given in Table 1. The 16 rules that were removed
from the list occurred in only one source or were
similar enough to other rules to be conflated. It is
worth noting here that while many of the comma
uses in this table might be best served by some sta-
tistical methodology like the one we describe in sec-
tion 4, one can envision fairly simple heuristic rules
to insert commas and find errors in numbers, dates,
geographical names, titles, and introducing words.
4 Classifier and Features
We use CRFs2 as the basis for our system and treat
the task of comma insertion as a sequence label-
ing task; each space between words is considered
by the classifier, and a comma is either inserted or
not. The feature set incorporates features that have
proven useful in comma restoration and other error
correction tasks, as well as a handful of new features
devised for this specific task (combination and dis-
tance features). The full set of features used in our
final system is given in Figure 1 along with exam-
ples of each feature for the sentence If the teacher
easily gets mad , then the child will always fear go-
ing to school and class. The target insertion point is
after the word mad.
2http://crfpp.sourceforge.net/
Feature Example(s)
Lexical and Syntactic Features
unigram easily, gets, mad, then, the
bigram easily gets, gets mad, mad then, ...
trigram easily gets mad, gets mad then, ...
pos uni RB, VBZ, JJ, RB, DT
pos bi RB VBZ, VBZ JJ, JJ RB, ...
pos tri RB VBZ JJ, VBZ JJ RB, ...
combo easily+RB, gets+VBZ,mad+JJ, ...
first combo If+RB
Distance Features
bos dist 5
eos dist 10
prevCC dist -
nextCC dist 9
Figure 1: CRF Features with examples for:
If the teacher easily gets mad , then the child will always
fear going to school and class.
4.1 Lexical and Syntactic Features
The first six features in Figure 1 refer to simple uni-
grams, bigrams, and trigrams of the words and POS
tags in a sliding 5 word window (target word, +/- 2
words). The lexical items help to encode any id-
iosyncratic relationships between words and com-
mas that might not be exploited through the exami-
nation of more in-depth linguistic features. For ex-
ample, then is a special case of an adverb (RB) that
is often preceded by a comma, even if other adverbs
are not, so POS tags might not capture this relation-
287
ship. The lexical items also provide an approxima-
tion of a language model or hidden event language
model approach, which has proven to be useful in
comma restoration tasks (see e.g. Lu and Ng, 2010).
The POS features abstract away from the words
and avoid the problem of data sparseness by allow-
ing the classifier to focus on the categories of the
words, rather than the lexical items themselves. The
combination (combo) feature is a unigram of the
word+pos for every word in the sliding window. It
reinforces the relationship between the lexical items
and their POS tags, further strengthening the evi-
dence of entries like then RB. All of these features
have been used in previous grammatical error detec-
tion tasks which target particle, article, and prepo-
sition errors (c.f., Dickinson et al, 2011; Gamon,
2010; Tetreault and Chodorow, 2008).
The first combo feature keeps track of the first
combination feature of the sentence so that it can
be referred to by the classifier throughout process-
ing the entire sentence. This feature is helpful when
an introductory phrase is longer than the classifier?s
five word window. Figure 1 provides a good exam-
ple of the utility of this feature, as If the teacher eas-
ily gets mad is so long that by the time the window
has moved to the target position of the space follow-
ing mad, the first word and POS, If RB, which can
often indicate an introductory phrase, is beyond the
scope of the sliding window.
4.2 Distance Features
Next, we encode four distance features. We keep
track of the following distances: from the beginning
of the sentence (bos dist), to the end of the sentence
(eos dist), from the previous coordinating conjunc-
tion (prevCC dist), and to the next coordinating con-
junction (nextCC dist). All of these distance fea-
tures help the classifier by encoding measures for
components of the sentence that can affect the deci-
sion to insert a comma. These features are especially
helpful over long range dependencies, when the in-
formation encoded by the feature is far outside the
scope of the 5-word window the CRF uses. The dis-
tance to the beginning of the sentence helps to en-
code introductory words and phrases, which make
up the bulk of the commas used in essays by learners
of English. The distance to the end of the sentence
is less obviously useful, but it can let the classifier
know the likelihood of a phrase beginning or ending
at a certain point in the sentence. The distances to
and from the nearest CC are useful because many
commas are collocated with coordinating conjunc-
tions. The distance features, as well as first combo,
were designed specifically for the task of comma er-
ror correction, and have not, as far as we know, been
utilized in previous research.
5 Comma Restoration
Before applying our system to the task of error cor-
rection, we tested its utility in restoring commas in
newswire texts. Specifically, we evaluate on section
23 of the WSJ, training on sections 02-22. Here,
the task is straightforward: we remove all commas
from the test data and performance is measured on
the system?s ability to put the commas back in the
right places. After stripping all commas from our
test data, the text is tokenized and POS tagged using
a maximum entropy tagger (Ratnaparkhi, 1996) and
every token is considered by the classifier as either
requiring a following comma or not. Out of 53,640
tokens, 3062 should be followed by a comma. We
provide accuracy, precision, recall, F1-score, and
sentence accuracy (S Acc.) for these tests, along
with results from Gravano et al (2009) and Shieber
and Tao (2003) in Table 2. The first system (LexSyn)
includes only the lexical and syntactic features from
Figure 1; the second (LexSyn+Dist) includes all of
the features.
System Acc. P R F S Acc.
LexSyn 97.4 85.8 64.9 73.9 60.5
LexSyn+Dist 97.5 85.8 66.3 74.8 61.4
Shieber & Tao 97.0 79.7 62.6 70.1 54.9
Gravano et al N.A. 57 67 ?61 N.A.
Table 2: Comma Restoration System Results (%)
As can be seen in Table 2, the full system
(LexSyn+Dist) performs significantly better than
WSJ LexSyn (p < .02, two-tailed), achieving an
F-score of 74.8 on WSJ. This F-score outperforms
Shieber and Tao?s system, which was also tested on
section 23 of the WSJ, by about 4% and our sentence
accuracy of 61.5% is about 7% higher than theirs.
Our F-score is also about 13% higher than that of
Gravano et al (2009), however, they evaluate on the
288
entire WSJ section of the Penn Treebank, so it is not
totally fair to compare results.
6 Annotation
For the comma restoration task, we needed only to
obtain well-formed text and remove the commas to
produce a test set. However, this is not so in the case
of error correction. In order to test a system that
corrects errors in learner essays, we need an anno-
tated test corpus that tells us where the errors are.
Although there are a handful of corpora that include
punctuation errors in their annotation scheme, such
as NUCLE (Dahlmeier and Ng, 2011) and HOO
(Dale and Kilgarriff, 2010), there are none to our
knowledge that focus specifically on commas. Thus,
we designed and implemented our own annotation
scheme on a set of essays to allow us the freedom to
identify the most important aspects of comma usage
for our work.
Our annotation scheme allows the mark-up of a
number of aspects of comma usage. First, each
comma in a text is marked as rejected or accepted
by the annotator. Additionally, any space between
words can be treated as an insertion point for a miss-
ing comma. The annotators also marked all accepted
and inserted commas as either required or optional.
Finally, the annotation also includes the appropriate
usage rule from the set in Table 1.3 In contrast, the
NUCLE and HOO data sets do not have this gran-
ularity of information (the annotation only indicates
whether a comma should be inserted or removed)
and are not exhaustively annotated.
After a one-hour training session on comma us-
age rules, three native English speakers were given a
set of ten learner essays comprising 3,665 tokens to
annotate for comma errors. To assess the difficulty
of the annotation task, we calculated agreement and
kappa. Agreement is a simple measure of how often
the annotators agree, and kappa provides a more ro-
bust measure of agreement since it takes chance into
account (Cohen, 1960). Table 3 provides the results
of these measurements. As can be seen in the table,
the agreement is quite high at either 97 or 98%, and
kappa is a bit lower, ranging from 72 to 81%. The
3The full annotation manual is available at
http://www.cs.rochester.edu/?tetreaul/
comma-manual.pdf
agreement is likely so high due to the great number
of decision points where it is obvious to any native
writer that no comma is needed. To account for this
imbalance, we also provide an adjusted agreement
in the final column of the table that excludes all de-
cisions where both annotators agree that no comma
is necessary.
Annotators Agreement Kappa Adj. agr.
1 & 2 97 74 61
1 & 3 98 72 61
2 & 3 98 81 76
Table 3: Agreement over Annotation Training Set (%)
After completing the training phase, we assigned
one annotator the task of annotating our develop-
ment and test data from two different corpora: es-
says written by English as a foreign language learn-
ers (EFL) and essays written by native speakers of
English (Native). For both data sets we selected 60
essays for development and 60 essays for test. The
annotation was carried out using an annotation tool
developed in-house that gives the annotator an easy
to use interface and outputs standoff annotations in
xml format. (3) is an example of an annotated sen-
tence from an EFL essay, where ? ?? marks a span
for annotation.
(3) The new millenium , 1 the 21st century 2
has dawned upon us 3 and this new century
has brought many positive advancements in our
daily lives .
1) Accept, required, parenthetical
2) Insert, required, parenthetical
3) Insert, required, independent clause
Table 4 provides the comma usage information for
the essays in both sets used in development and test-
ing. The table shows the total number of sentences,
commas in the original text that were accepted by
the annotator, and errors (rejected and missing com-
mas) for the 60 essays in each set.
As can be seen in Table 4, the majority of exist-
ing commas (columns Accept plus Rej) in the texts
were accepted by the annotator; about 84% in the
EFL development set, 87% in the EFL test set, 85%
in the Native development set, and 88% in the Na-
tive test set. The important fact uncovered by these
numbers is that most of the commas that learners do
289
Data Set Sent
Commas
Accept
Errors
Rej Miss
EFL Dev 717 474 49 233
EFL Test 683 427 65 232
Native Dev 970 506 86 363
Native Test 839 377 50 314
Table 4: Comma Usage Statistics
use are correct. However, there are a great number
of commas that the annotator inserted (over 80% of
all errors are missing commas) meaning that these
learners are more prone to underusing than overus-
ing commas. Another interesting fact that can be
gleaned from our annotation is that the top five
comma uses, those listed in the first five rows of Ta-
ble 1, account for more than 80% of all commas in
these essays.
7 Error Correction
With a competitive comma restoration system in
place, we turn to the primary task of correcting er-
rors in learner essays. While the task remains simi-
lar to comma restoration, error correction in student
writing brings a new set of challenges, especially
when the writers are non-native. Newswire texts are
most often well-formed, so the system should not
experience interference from other contextual errors
around the missing commas. Sentences taken from
learner texts, though, often contain multiple errors
that can make it difficult to focus on a single problem
at a time. Spelling errors, for example, can exacer-
bate error correction efforts that use contextual lex-
ical features because well-formed text that is often
used for training data is usually free of such noise.
In these experiments, we use the annotated es-
says described in section 6 for evaluation and train
on 40,000 sentences taken from essays written by
both native and non-native high level college stu-
dents. All of the essays are run through automatic
spelling correction to reduce the noise in the test set
before being tagged with the same tagger used in the
comma restoration experiments.
Because we approach comma error correction as
essentially a comma restoration task, we can we use
largely the same system for error correction as we
did for comma restoration. We still employ CRFs
and label each space between words as requiring
a comma or not, however, there is one significant
change to our methodology for this task. Namely,
we can leave the commas that were present in the
text as provided by the writer as we pre-process
the data for error correction, whereas they were re-
moved in the comma restoration task. For error cor-
rection, the task is really comparing the system?s an-
swer to the annotator?s and the learner?s, as opposed
to simply inserting commas into raw text. Leaving
the learners? commas in the text does introduce some
errors to the POS tagging phase. However, since
over 85% of the existing commas in the development
set were judged as acceptable by our annotator (cf.
section 6) , the number of erroneous commas is not
so great as to contaminate the system. Removing all
of the commas would introduce unnecessary errors
in the pre-processing phase.
We also augment the system with three post-
processing filters that we tuned on the development
set. One requires that the classifier be completely
confident before a change is made to an existing
comma; crf++ will give 100% confidence to a single
class in some cases. This filter is based on the fact
that 85% of the existing commas can be expected
to be correct. A similar filter requires that the clas-
sifier be at least 90% confident in a decision to in-
sert a new comma. The final filter, which overrides
any other information provided by the system, does
not allow commas to be inserted before the word be-
cause. These ensure high precision even though they
may reduce recall.
Table 5 provides the accuracy, precision, recall,
F-score, and number of errors in each set for tests on
our 60 annotated EFL and Native essays, and the re-
sult for the combined corpus. The system performs
quite well on the EFL test set, with scores of 94%
precision, 31.7% recall, and 47.4% F-score for the
LexSyn+Dist system. The results for the Native set
are a bit lower, with 84.9% precision, 20% recall,
and 32.4% F-score for the LexSyn+Dist system.
For both data sets, when the distance features are
added to the model, precision increases by 1%, and
in the EFL set, recall also increases. In keeping with
practices established within the field of grammatical
error correction, the system has been optimized for
high precision even at the cost of recall, to ensure
that feedback systems avoid confusing learners by
290
Data System Acc. P R F n
EFL
LexSyn 98.2 92.9 30.9 46.5 297
LexSyn+Dist 98.3 94.0 31.7 47.4 297
Native
LexSyn 97.8 83.9 20.0 32.3 365
LexSyn+Dist 97.8 84.9 20.0 32.4 365
Combined
LexSyn 98.1 88.7 24.9 38.9 662
LexSyn+Dist 98.1 89.8 25.2 39.4 662
Table 5: Comma Error Correction Results (%)
marking correct comma usage as erroneous. Con-
sidering performance over all of the test data, the
system achieves over 89% precision and 25% recall,
results which are comparable to those in other er-
ror correction tasks. For example, the preposition
error detection system described in Tetreault and
Chodorow (2008) achieved 84% precision, 19% re-
call for prepositions.
It is worth noting that the results in Table 5 in-
clude commas that the annotator had marked as
optional. For these, whatever decision the system
makes is scored as correct. Since the grammatical-
ity/readability of the sentence will not be affected by
the presence or absence of a comma in these cases,
we feel this is the fairest assessment of the system.
7.1 Error Analysis
In order to get a sense of what kinds of construc-
tions are difficult for our system, we randomly ex-
tracted 50 sentences from the output that exhibited
at least one wrong comma decision made by the sys-
tem. The 50 sentences contained a combined total of
62 system errors. Among these cases, the most com-
mon context where the system makes the wrong de-
cision is in introductory words and phrases, which is
not surprising given the frequency with which com-
mas occur in these environments in our development
set (about 40% of all commas in the essays). In (4),
for example, the first word, Here, should be followed
by a comma. Since Here is not a common introduc-
tory word in this type of sentence structure in the
training data, this is a difficult case for the system to
correct.
(4) Here we can get specific knowledge in the sci-
ence that we like the most .
The next most common misclassification involves
comma splices, i.e. conjoining complete sentences
with a comma rather than separating them with a full
stop. In (5), for example, there should be a full stop
between college and I, rather than a comma. This
result is not surprising because the system is not
yet equipped to deal with comma splices. Comma
splices are a different type of phenomenon because
correcting them requires removing the comma and
inserting a full stop, essentially two separate steps
rather than the single reject/accept step that the sys-
tem currently handles.
(5) I entered college, I could learn it and make an
effort to achieve my goal.
The next most common context for system errors
was between clauses that are conjoined with a co-
ordinating conjunction as in (6), where there should
not be a comma. In (6), the second clause is actually
a dependent clause, so no comma should precede
the coordinating conjunction. There are a number
of system errors dealing with commas between two
independent clauses. For example in (7), our annota-
tor recommended a comma between things and but,
however the system did not make the insertion. The
problem with these examples likely stems from the
fact that the rule for comma usage in these contexts
is not clearly stated, even in well-respected manu-
als, and therefore likely not clearly understood, even
by high-level native writers. For example, the NYT
style manual (Siegal and Connolly, 1999) states that
?Commas should be used in compound sentences
before conjunctions... When the clauses are excep-
tionally short, however, the comma may be omit-
ted.? Adding a feature that measures clause length
might help, but even then the classifier must rely on
training data that may have considerable variation
as to what length of clauses requires an intervening
comma.
291
(6) They wants to see their portfolio, and what kind
of skill do they have for company.
(7) I have many things but the best is my parents.
Another facet of the data that consistently chal-
lenges the system is the existence of errors other
than the commas in the sentences. Consider the sen-
tence in (8), where erroneous is the original text
from the essay and corrected is a well-formed in-
terpretation.
(8) erroneous: In the other hand , having just
one specific subject , which represents a great
downfall for many students
corrected: On the other hand, knowing only
one subject is a downfall for many students.
The comma after subject is unnecessary, but so is
the word which. In fact, which would normally sig-
nify the beginning of a non-restrictive clause in this
context, which should be set off with a comma. It
is no surprise then, that the system has trouble re-
moving commas in these types of contexts. At least
11 of the 62 system mistakes that we examined have
grammatical errors in the immediate context of the
comma in question, which makes the classification
more difficult.
8 Summary and Conclusion
We presented a novel comma error correction sys-
tem for English that achieves an average of 89% pre-
cision and 25% recall on essays written by learn-
ers of different levels and language backgrounds,
including native English speakers. The system
achieves state-of-the-art performance on the task of
comma restoration, beating previous systems? F-
score and sentence accuracy by 4% and 7%, respec-
tively. We discovered that augmenting lexical fea-
tures, which have been commonly used in previous
work, with the combination and distance features
can improve F-score by as much as 1% in both the
error correction and comma restoration tasks. We
also developed and implemented a novel comma er-
ror annotation scheme.
Additionally, we are interested in the effect of
correct comma placement on other NLP processes.
Jones (1994) and Briscoe and Carroll (1995) show
that adding punctuation to grammars that utilize
part-of-speech (POS) tags, rather than lexical items,
adds more structure and reduces ambiguity as well
as the number of parses for each sentence. Simi-
larly, Doran (1998) and White and Rajkumar (2008)
found that adding punctuation improved parsing re-
sults in tree-adjoining grammar (TAG) and combi-
natorial categorial grammar (CCG) parsing, respec-
tively. These studies all highlight the importance of
correctly inserted punctuation, especially commas,
for parsing. Given these results, we believe that by
enhancing the quality of the text, comma error cor-
rection will improve not only tagging and parsing,
but also the ability of systems to correct many other
forms of grammatical errors, such as those involv-
ing incorrect word order, number disagreement, and
misuse of prepositions, articles, and collocations.
Acknowledgments
We would like to thank Melissa Lopez for help with
annotating our corpora and Michael Flor for kindly
developing an annotation tool for our purposes. We
also thank Aoife Cahill, Robbie Kantor, Markus
Dickinson, Michael Heilman, Nitin Madnani, and
our anonymous reviewers for insightful comments
and discussion.
References
In?aki Alegria, Bertol Arrieta, Arantza Diaz de Ilar-
raza, Eli Izagirre, and Montse Maritxalar. 2006.
Using machine learning techniques to build a
comma checker for Basque. In Proceedings of the
COLING/ACL main conference poster sessions.
Timothy Baldwin and Manuel Paul Anil Kumar
Joseph. 2009. Restoring punctuation and casing
in English text. In Australasian Conference on
Artificial Intelligence?09.
E. Brill. 1993. A Corpus-Based Approach to Lan-
guage Learning. Ph.D. thesis, The University of
Pennsylvania, Philadelpha, PA.
Ted Briscoe and John Carroll. 1995. Developing and
evaluating a probabilistic LR parser of part-of-
speech and punctuation labels. In Proceedings of
the ACL/SIGPARSE 4th International Workshop
on Parsing Technologies.
Jacob Cohen. 1960. A coefficient of agreement for
292
nominal scales. Educational and Psychological
Measurement, 20(1):37?46.
Robert J. Connors and Andrea A. Lunsford. 1988.
Frequency of formal errors in current college
writing, or Ma and Pa Kettle do research. Col-
lege Composition and Communication, 39(4).
Daniel Dahlmeier and Hwee Tou Ng. 2011. Gram-
matical error correction with alternating structure
optimization. In Proceedings of the 49th An-
nual Meeting of the Association for Computa-
tional Linguistics: Human Language Technolo-
gies - Volume 1. Association for Computational
Linguistics.
Robert Dale and Adam Kilgarriff. 2010. Helping
our own: Text massaging for computational lin-
guistics as a new shared task. In International
Conference on Natural Language Generation.
Rachele De Felice and Stephen Pulman. 2008. A
classifier-based approach to preposition and de-
terminer error correction in L2 English. In Pro-
ceedings of COLING-08. Manchester.
Markus Dickinson, Ross Israel, and Sun-Hee Lee.
2011. Developing methodology for Korean par-
ticle error detection. In Proceedings of the 6th
Workshop on Innovative Use of NLP for Building
Educational Applications. Portland, Oregon.
Christine Doran. 1998. Incorporating Punctuation
into the Sentence Grammar: A Lexicalized Tree-
Adjoining Grammar Perspective. Ph.D. thesis,
University of Pennsylvania.
Benoit Favre, Dilek Hakkani-Tur, and Elizabeth
Shriberg. 2009. Syntactically-informed models
for comma prediction. In Proceedings of the
2009 IEEE International Conference on Acous-
tics, Speech and Signal Processing.
Michael Gamon. 2010. Using mostly native data
to correct errors in learners? writing: A meta-
classifier approach. In Human Language Tech-
nologies: The 2010 Annual Conference of the
North American Chapter of the Association for
Computational Linguistics.
Michael Gamon. 2011. High-order sequence model-
ing for language learner detection high-order se-
quence modeling for language learner error de-
tection. In Proceedings of the 6th Workshop on
Innovative Use of NLP for Building Educational
Applications.
Agustin Gravano, Martin Jansche, and Michiel Bac-
chiani. 2009. Restoring punctuation and capi-
talization in transcribed speech. In Proceedings
of the 2009 IEEE International Conference on
Acoustics, Speech and Signal Processing.
Daniel Hardt. 2001. Comma Checking in Danish. In
Corpus Linguistics.
Robin L. Hill and Wayne S. Murray. 1998. Commas
and spaces: The point of punctuation. In 11th An-
nual CUNY Conference on Human Sentence Pro-
cessing.
Jing Huang and Geoffrey Zweig. 2002. Maximum
entropy model for punctuation annotation from
speech. In Proceedings of ICSLP 2002.
Bernard E. M. Jones. 1994. Exploring the role of
punctuation in parsing natural text. In Proceed-
ings of the 15th conference on Computational lin-
guistics - Volume 1.
John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling
sequence data. In Proceedings of the Eighteenth
International Conference on Machine Learning.
Claudia Leacock, Martin Chodorow, Michael Ga-
mon, and Joel R. Tetreault. 2010. Auto-
mated Grammatical Error Detection for Lan-
guage Learners. Synthesis Lectures on Hu-
man Language Technologies. Morgan & Claypool
Publishers.
Yang Liu, Elizabeth Shriberg, Andreas Stolcke, and
Mary Harper. 2005. Comparing hmm, maximum
entropy, and conditional random fields for disflu-
ency detection. In In Proceeedings of the Euro-
pean Conference on Speech Communication and
Technology.
Wei Lu and Hwee T. Ng. 2010. Better punctua-
tion prediction with dynamic conditional random
fields. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Process-
ing.
Helena Moniz, Fernando Batista, Hugo Meinedo,
and Alberto Abad. 2009. Prosodically-based au-
tomatic segmentation and punctuation. In Pro-
293
ceedings of the 5th International Conference on
Speech Prosody.
Diane Nicholls. 1999. The cambridge learner corpus
- error coding and analysis for writing dictionaries
and other books for english learners. In Summer
Workshop on Learner Corpora. Showa Woman?s
University.
Hiromi Oyama. 2010. Automatic error detection
method for japanese particles. Polyglossia, 18.
Adwait Ratnaparkhi. 1996. A Maximum Entropy
Model for Part-Of-Speech Tagging. In Eric Brill
and Kenneth Church, editors, Proceedings of the
Empirical Methods in Natural Language Process-
ing.
Alla Rozovskaya, Mark Sammons, Joshua Gioja,
and Dan Roth. 2011. University of Illinois sys-
tem in HOO text correction shared task. In Pro-
ceedings of the Generation Challenges Session
at the 13th European Workshop on Natural Lan-
guage Generation, pages 263?266. Association
for Computational Linguistics, Nancy, France.
Stuart M. Shieber and Xiaopeng Tao. 2003. Comma
restoration using constituency information. In
Proceedings of the 2003 Human Language Tech-
nology Conference and Conference of the North
American Chapter of the Association for Compu-
tational Linguistics.
Allan M. Siegal and William G. Connolly. 1999. The
New York Times Manual of Style and Usage : The
Official Style Guide Used by the Writers and Edi-
tors of the World?s Most Authoritative Newspaper.
Crown, rev sub edition.
William Strunk and E. B. White. 1999. The Ele-
ments of Style, Fourth Edition. Longman, fourth
edition.
Joel Tetreault and Martin Chodorow. 2008. The ups
and downs of preposition error detection in ESL
writing. In Proceedings of COLING-08. Manch-
ester.
Joel Tetreault, Jennifer Foster, and Martin
Chodorow. 2010. Using parse features for
preposition selection and error detection. In
Proceedings of the ACL 2010 Conference Short
Papers.
University of Chicago. 1993. The Chicago Manual
of Style. University Of Chicago Press, Chicago,
fourteenth edition.
Michael White and Rajakrishnan Rajkumar. 2008.
A more precise analysis of punctuation for broad-
coverage surface realization with CCG. In Pro-
ceedings of the Workshop on Grammar Engineer-
ing Across Frameworks.
294
Proceedings of the ACL 2010 Conference Short Papers, pages 353?358,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Using Parse Features for Preposition Selection and Error Detection
Joel Tetreault
Educational Testing Service
Princeton
NJ, USA
JTetreault@ets.org
Jennifer Foster
NCLT
Dublin City University
Ireland
jfoster@computing.dcu.ie
Martin Chodorow
Hunter College of CUNY
New York, NY, USA
martin.chodorow
@hunter.cuny.edu
Abstract
We evaluate the effect of adding parse fea-
tures to a leading model of preposition us-
age. Results show a significant improve-
ment in the preposition selection task on
native speaker text and a modest increment
in precision and recall in an ESL error de-
tection task. Analysis of the parser output
indicates that it is robust enough in the face
of noisy non-native writing to extract use-
ful information.
1 Introduction
The task of preposition error detection has re-
ceived a considerable amount of attention in re-
cent years because selecting an appropriate prepo-
sition poses a particularly difficult challenge to
learners of English as a second language (ESL).
It is not only ESL learners that struggle with En-
glish preposition usage ? automatically detecting
preposition errors made by ESL speakers is a chal-
lenging task for NLP systems. Recent state-of-the-
art systems have precision ranging from 50% to
80% and recall as low as 10% to 20%.
To date, the conventional wisdom in the error
detection community has been to avoid the use
of statistical parsers under the belief that a WSJ-
trained parser?s performance would degrade too
much on noisy learner texts and that the tradi-
tionally hard problem of prepositional phrase at-
tachment would be even harder when parsing ESL
writing. However, there has been little substantial
research to support or challenge this view. In this
paper, we investigate the following research ques-
tion: Are parser output features helpful in mod-
eling preposition usage in well-formed text and
learner text?
We recreate a state-of-the-art preposition usage
system (Tetreault and Chodorow (2008), hence-
forth T&C08) originally trained with lexical fea-
tures and augment it with parser output features.
We employ the Stanford parser in our experiments
because it consists of a competitive phrase struc-
ture parser and a constituent-to-dependency con-
version tool (Klein and Manning, 2003a; Klein
and Manning, 2003b; de Marneffe et al, 2006;
de Marneffe and Manning, 2008). We com-
pare the original model with the parser-augmented
model on the tasks of preposition selection in well-
formed text (fluent writers) and preposition error
detection in learner texts (ESL writers).
This paper makes the following contributions:
? We demonstrate that parse features have a
significant impact on preposition selection in
well-formed text. We also show which fea-
tures have the greatest effect on performance.
? We show that, despite the noisiness of learner
text, parse features can actually make small,
albeit non-significant, improvements to the
performance of a state-of-the-art preposition
error detection system.
? We evaluate the accuracy of parsing and
especially preposition attachment in learner
texts.
2 Related Work
T&C08, De Felice and Pulman (2008) and Ga-
mon et al (2008) describe very similar preposi-
tion error detection systems in which a model of
correct prepositional usage is trained from well-
formed text and a writer?s preposition is com-
pared with the predictions of this model. It is
difficult to directly compare these systems since
they are trained and tested on different data sets
353
but they achieve accuracy in a similar range. Of
these systems, only the DAPPER system (De Fe-
lice and Pulman, 2008; De Felice and Pulman,
2009; De Felice, 2009) uses a parser, the C&C
parser (Clark and Curran, 2007)), to determine
the head and complement of the preposition. De
Felice and Pulman (2009) remark that the parser
tends to be misled more by spelling errors than
by grammatical errors. The parser is fundamental
to their system and they do not carry out a com-
parison of the use of a parser to determine the
preposition?s attachments versus the use of shal-
lower techniques. T&C08, on the other hand, re-
ject the use of a parser because of the difficulties
they foresee in applying one to learner data. Her-
met et al (2008) make only limited use of the
Xerox Incremental Parser in their preposition er-
ror detection system. They split the input sentence
into the chunks before and after the preposition,
and parse both chunks separately. Only very shal-
low analyses are extracted from the parser output
because they do not trust the full analyses.
Lee and Knutsson (2008) show that knowl-
edge of the PP attachment site helps in the task
of preposition selection by comparing a classifier
trained on lexical features (the verb before the
preposition, the noun between the verb and the
preposition, if any, and the noun after the preposi-
tion) to a classifier trained on attachment features
which explicitly state whether the preposition is
attached to the preceding noun or verb. They also
argue that a parser which is capable of distinguish-
ing between arguments and adjuncts is useful for
generating the correct preposition.
3 Augmenting a Preposition Model with
Parse Features
To test the effects of adding parse features to
a model of preposition usage, we replicated the
lexical and combination feature model used in
T&C08, training on 2M events extracted from a
corpus of news and high school level reading ma-
terials. Next, we added the parse features to this
model to create a new model ?+Parse?. In 3.1 we
describe the T&C08 system and features, and in
3.2 we describe the parser output features used to
augment the model. We illustrate our features us-
ing the example phrase many local groups around
the country. Fig. 1 shows the phrase structure tree
and dependency triples returned by the Stanford
parser for this phrase.
3.1 Baseline System
The work of Chodorow et al (2007) and T&C08
treat the tasks of preposition selection and er-
ror detection as a classification problem. That
is, given the context around a preposition and a
model of correct usage, a classifier determines
which of the 34 prepositions covered by the model
is most appropriate for the context. A model of
correct preposition usage is constructed by train-
ing a Maximum Entropy classifier (Ratnaparkhi,
1998) on millions of preposition contexts from
well-formed text.
A context is represented by 25 lexical features
and 4 combination features:
Lexical Token and POS n-grams in a 2 word
window around the preposition, plus the head verb
in the preceding verb phrase (PV), the head noun
in the preceding noun phrase (PN) and the head
noun in the following noun phrase (FN) when
available (Chodorow et al, 2007). Note that these
are determined not through full syntactic parsing
but rather through the use of a heuristic chun-
ker. So, for the phrase many local groups around
the country, examples of lexical features for the
preposition around include: FN = country, PN =
groups, left-2-word-sequence = local-groups, and
left-2-POS-sequence = JJ-NNS.
Combination T&C08 expand on the lexical fea-
ture set by combining the PV, PN and FN fea-
tures, resulting in features such as PN-FN and
PV-PN-FN. POS and token versions of these fea-
tures are employed. The intuition behind creat-
ing combination features is that the Maximum En-
tropy classifier does not automatically model the
interactions between individual features. An ex-
ample of the PN-FN feature is groups-country.
3.2 Parse Features
To augment the above model we experimented
with 14 features divided among five main classes.
Table 1 shows the features and their values for
our around example. The Preposition Head and
Complement feature represents the two basic at-
tachment relations of the preposition, i.e. its head
(what it is attached to) and its complement (what
is attached to it). Relation specifies the relation
between the head and complement. The Preposi-
tion Head and Complement Combined features
are similar to the T&C08 Combination features
except that they are extracted from parser output.
354
NP
NP
DT
many
JJ
local
NNS
groups
PP
IN
around
NP
DT
the
NN
country
amod(groups-3, many-1)
amod(groups-3, local-2)
prep(groups-3, around-4)
det(country-6, the-5)
pobj(around-4, country-6)
Figure 1: Phrase structure tree and dependency
triples produced by the Stanford parser for the
phrase many local groups around the country
Prep. Head & Complement
1. head of the preposition: groups
2. POS of the head: NNS
3. complement of the preposition: country
4. POS of the complement: NN
Prep. Head & Complement Relation
5. Prep-Head relation name: prep
6. Prep-Comp relation name: pobj
Prep. Head & Complement Combined
7. Head-Complement tokens: groups-country
8. Head-Complement tags: NNS-NN
Prep. Head & Complement Mixed
9. Head Tag and Comp Token: NNS-country
10. Head Token and Comp Tag: groups-NN
Phrase Structure
11. Preposition Parent: PP
12. Preposition Grandparent: NP
13. Left context of preposition parent: NP
14. Right context of preposition parent: -
Table 1: Parse Features
Model Accuracy
combination only 35.2
parse only 60.6
combination+parse 61.9
lexical only 64.4
combination+lexical (T&C08) 65.2
lexical+parse 68.1
all features (+Parse) 68.5
Table 2: Accuracy on preposition selection task
for various feature combinations
The Preposition Head and Complement Mixed
features are created by taking the first feature in
the previous set and backing-off either the head
or the complement to its POS tag. This mix of
tags and tokens in a word-word dependency has
proven to be an effective feature in sentiment anal-
ysis (Joshi and Penstein-Rose?, 2009). All the fea-
tures described so far are extracted from the set of
dependency triples output by the Stanford parser.
The final set of features (Phrase Structure), how-
ever, is extracted directly from the phrase structure
trees themselves.
4 Evaluation
In Section 4.1, we compare the T&C08 and +Parse
models on the task of preposition selection on
well-formed texts written by native speakers. For
every preposition in the test set, we compare the
system?s top preposition for that context to the
writer?s preposition, and report accuracy rates. In
Section 4.2, we evaluate the two models on ESL
data. The task here is slightly different - if the
most likely preposition according to the model dif-
fers from the likelihood of the writer?s preposition
by a certain threshold amount, a preposition error
is flagged.
4.1 Native Speaker Test Data
Our test set consists of 259K preposition events
from the same source as the original training data.
The T&C08 model performs at 65.2% and when
the parse features are added, the +Parse model im-
proves performance by more than 3% to 68.5%.1
The improvement is statistically significant.
1Prior research has shown preposition selection perfor-
mance accuracy ranging from 65% to nearly 80%. The dif-
ferences are largely due to different test sets and also training
sizes. Given the time required to train large models, we report
here experiments with a relatively small model.
355
Model Accuracy
T&C08 65.2
+Phrase Structure Only 67.1
+Dependency Only 68.2
+Parse 68.5
+head-tag+comp-tag 66.9
+left 66.8
+grandparent 66.6
+head-token+comp-tag 66.6
+head-tag 66.5
+head-token 66.4
+head-tag+comp-token 66.1
Table 3: Which parse features are important? Fea-
ture Addition Experiment
Table 2 shows the effect of various feature class
combinations on prediction accuracy. The results
are clear: a significant performance improvement
is obtained on the preposition selection task when
features from parser output are added. The two
best models in Table 2 contain parse features. The
table also shows that the non-parser-based feature
classes are not entirely subsumed by the parse fea-
tures but rather provide, to varying degrees, com-
plementary information.
Having established the effectiveness of parse
features, we investigate which parse feature
classes contribute the most. To test each contri-
bution, we perform a feature addition experiment,
separately adding features to the T&C08 model
(see Table 3). We make three observations. First,
while there is overlapping information between
the dependency features and the phrase structure
features, the phrase structure features are mak-
ing a contribution. This is interesting because
it suggests that a pure dependency parser might
be less useful than a parser which explicitly pro-
duces both constituent and dependency informa-
tion. Second, using a parser to identify the prepo-
sition head seems to be more useful than using it to
identify the preposition complement.2 Finally, as
was the case for the T&C08 features, the combina-
tion parse features are also important (particularly
the tag-tag or tag/token pairs).
4.2 ESL Test Data
Our test data consists of 5,183 preposition events
extracted from a set of essays written by non-
2De Felice (2009) observes the same for the DAPPER sys-
tem.
Method Precision Recall
T&C08 0.461 0.215
+Parse 0.486 0.225
Table 4: ESL Error Detection Results
native speakers for the Test of English as a Foreign
Language (TOEFL R?). The prepositions were
judged by two trained annotators and checked
by the authors using the preposition annotation
scheme described in Tetreault and Chodorow
(2008b). 4,881 of the prepositions were judged to
be correct and the remaining 302 were judged to
be incorrect.
The writer?s preposition is flagged as an error by
the system if its likelihood according to the model
satisfied a set of criteria (e.g., the difference be-
tween the probability of the system?s choice and
the writer?s preposition is 0.8 or higher). Un-
like the selection task where we use accuracy as
the metric, we use precision and recall with re-
spect to error detection. To date, performance
figures that have been reported in the literature
have been quite low, reflecting the difficulty of the
task. Table 4 shows the performance figures for
the T&C08 and +Parse models. Both precision
and recall are higher for the +Parse model, how-
ever, given the low number of errors in our an-
notated test set, the difference is not statistically
significant.
5 Parser Accuracy on ESL Data
To evaluate parser performance on ESL data,
we manually inspected the phrase structure trees
and dependency graphs produced by the Stanford
parser for 210 ESL sentences, split into 3 groups:
the sentences in the first group are fluent and con-
tain no obvious grammatical errors, those in the
second contain at least one preposition error and
the sentences in the third are clearly ungrammati-
cal with a variety of error types. For each preposi-
tion we note whether the parser was successful in
determining its head and complement. The results
for the three groups are shown in Table 5. The
figures in the first row are for correct prepositions
and those in the second are for incorrect ones.
The parser tends to do a better job of de-
termining the preposition?s complement than its
head which is not surprising given the well-known
problem of PP attachment ambiguity. Given the
preposition, the preceding noun, the preceding
356
OK
Head Comp
Prep Correct 86.7% (104/120) 95.0% (114/120)
Prep Incorrect - -
Preposition Error
Head Comp
Prep Correct 89.0% (65/73) 97.3% (71/73)
Prep Incorrect 87.1% (54/62) 96.8% (60/62)
Ungrammatical
Head Comp
Prep Correct 87.8% (115/131) 89.3% (117/131)
Prep Incorrect 70.8% (17/24) 87.5% (21/24)
Table 5: Parser Accuracy on Prepositions in a
Sample of ESL Sentences
verb and the following noun, Collins (1999) re-
ports an accuracy rate of 84.5% for a PP attach-
ment classifier. When confronted with the same
information, the accuracy of three trained annota-
tors is 88.2%. Assuming 88.2% as an approximate
PP-attachment upper bound, the Stanford parser
appears to be doing a good job. Comparing the
results over the three sentence groups, its ability
to identify the preposition?s head is quite robust to
grammatical noise.
Preposition errors in isolation do not tend to
mislead the parser: in the second group which con-
tains sentences which are largely fluent apart from
preposition errors, there is little difference be-
tween the parser?s accuracy on the correctly used
prepositions and the incorrectly used ones. Exam-
ples are
(S (NP I)
(VP had
(NP (NP a trip)
(PP for (NP Italy))
)
)
)
in which the erroneous preposition for is correctly
attached to the noun trip, and
(S (NP A scientist)
(VP devotes
(NP (NP his prime part)
(PP of (NP his life))
)
(PP in (NP research))
)
)
in which the erroneous preposition in is correctly
attached to the verb devotes.
6 Conclusion
We have shown that the use of a parser can boost
the accuracy of a preposition selection model
tested on well-formed text. In the error detection
task, the improvement is less marked. Neverthe-
less, examination of parser output shows the parse
features can be extracted reliably from ESL data.
For our immediate future work, we plan to carry
out the ESL evaluation on a larger test set to bet-
ter gauge the usefulness of a parser in this context,
to carry out a detailed error analysis to understand
why certain parse features are effective and to ex-
plore a larger set of features.
In the longer term, we hope to compare different
types of parsers in both the preposition selection
and error detection tasks, i.e. a task-based parser
evaluation in the spirit of that carried out by Miyao
et al (2008) on the task of protein pair interaction
extraction. We would like to further investigate
the role of parsing in error detection by looking at
other error types and other text types, e.g. machine
translation output.
Acknowledgments
We would like to thank Rachele De Felice and the
reviewers for their very helpful comments.
References
Martin Chodorow, Joel Tetreault, and Na-Rae Han.
2007. Detection of grammatical errors involv-
ing prepositions. In Proceedings of the 4th ACL-
SIGSEM Workshop on Prepositions, Prague, Czech
Republic, June.
Stephen Clark and James R. Curran. 2007. Wide-
coverage efficient statistical parsing with CCG
and log-linear models. Computational Linguistics,
33(4):493?552.
Michael Collins. 1999. Head-driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania.
Rachele De Felice and Stephen G. Pulman. 2008. A
classifier-based approach to preposition and deter-
miner error correction in L2 english. In Proceedings
of the 22nd COLING, Manchester, United Kingdom.
Rachele De Felice and Stephen Pulman. 2009. Au-
tomatic detection of preposition errors in learning
writing. CALICO Journal, 26(3):512?528.
Rachele De Felice. 2009. Automatic Error Detection
in Non-native English. Ph.D. thesis, Oxford Univer-
sity.
357
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The stanford typed dependencies repre-
sentation. In Proceedings of the COLING08 Work-
shop on Cross-framework and Cross-domain Parser
Evaluation, Manchester, United Kingdom.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of LREC, Genoa, Italy.
Michael Gamon, Jianfeng Gao, Chris Brockett,
Alexandre Klementiev, William B. Dolan, Dmitriy
Belenko, and Lucy Vanderwende. 2008. Using con-
textual speller techniques and language modelling
for ESL error correction. In Proceedings of the In-
ternational Joint Conference on Natural Language
Processing, Hyderabad, India.
Matthieu Hermet, Alain De?silets, and Stan Szpakow-
icz. 2008. Using the web as a linguistic resource
to automatically correct lexico-syntactic errors. In
Proceedings of LREC, Marrekech, Morocco.
Mahesh Joshi and Carolyn Penstein-Rose?. 2009. Gen-
eralizing dependency features for opinion mining.
In Proceedings of the ACL-IJCNLP 2009 Confer-
ence Short Papers, pages 313?316, Singapore.
Dan Klein and Christopher D. Manning. 2003a. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting of the ACL, pages 423?430,
Sapporo, Japan.
Dan Klein and Christopher D. Manning. 2003b. Fast
exact inference with a factored model for exact pars-
ing. In Advances in Neural Information Processing
Systems, pages 3?10. MIT Press, Cambridge, MA.
John Lee and Ola Knutsson. 2008. The role of PP at-
tachment in preposition generation. In Proceedings
of CICling. Springer-Verlag Berlin Heidelberg.
Yusuke Miyao, Rune Saetre, Kenji Sagae, Takuya Mat-
suzaki, and Jun?ichi Tsujii. 2008. Task-oriented
evaluation of syntactic parsers and their representa-
tions. In Proceedings of the 46th Annual Meeting of
the ACL, pages 46?54, Columbus, Ohio.
Adwait Ratnaparkhi. 1998. Maximum Entropy Mod-
els for natural language ambiguity resolution. Ph.D.
thesis, University of Pennsylvania.
Joel Tetreault and Martin Chodorow. 2008. The ups
and downs of preposition error detection in ESL
writing. In Proceedings of the 22nd COLING,
Manchester, United Kingdom.
Joel Tetreault and Martin Chodorow. 2008b. Na-
tive Judgments of non-native usage: Experiments in
preposition error detection. In COLING Workshop
on Human Judgments in Computational Linguistics,
Manchester, United Kingdom.
358
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 508?513,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
They Can Help: Using Crowdsourcing to Improve the Evaluation of
Grammatical Error Detection Systems
Nitin Madnania Joel Tetreaulta Martin Chodorowb Alla Rozovskayac
aEducational Testing Service
Princeton, NJ
{nmadnani,jtetreault}@ets.org
bHunter College of CUNY
martin.chodorow@hunter.cuny.edu
cUniversity of Illinois at Urbana-Champaign
rozovska@illinois.edu
Abstract
Despite the rising interest in developing gram-
matical error detection systems for non-native
speakers of English, progress in the field has
been hampered by a lack of informative met-
rics and an inability to directly compare the
performance of systems developed by differ-
ent researchers. In this paper we address
these problems by presenting two evaluation
methodologies, both based on a novel use of
crowdsourcing.
1 Motivation and Contributions
One of the fastest growing areas in need of NLP
tools is the field of grammatical error detection for
learners of English as a Second Language (ESL).
According to Guo and Beckett (2007), ?over a bil-
lion people speak English as their second or for-
eign language.? This high demand has resulted in
many NLP research papers on the topic, a Synthesis
Series book (Leacock et al, 2010) and a recurring
workshop (Tetreault et al, 2010a), all in the last five
years. In this year?s ACL conference, there are four
long papers devoted to this topic.
Despite the growing interest, two major factors
encumber the growth of this subfield. First, the lack
of consistent and appropriate score reporting is an
issue. Most work reports results in the form of pre-
cision and recall as measured against the judgment
of a single human rater. This is problematic because
most usage errors (such as those in article and prepo-
sition usage) are a matter of degree rather than sim-
ple rule violations such as number agreement. As a
consequence, it is common for two native speakers
to have different judgments of usage. Therefore, an
appropriate evaluation should take this into account
by not only enlisting multiple human judges but also
aggregating these judgments in a graded manner.
Second, systems are hardly ever compared to each
other. In fact, to our knowledge, no two systems
developed by different groups have been compared
directly within the field primarily because there is
no common corpus or shared task?both commonly
found in other NLP areas such as machine transla-
tion.1 For example, Tetreault and Chodorow (2008),
Gamon et al (2008) and Felice and Pulman (2008)
developed preposition error detection systems, but
evaluated on three different corpora using different
evaluation measures.
The goal of this paper is to address the above
issues by using crowdsourcing, which has been
proven effective for collecting multiple, reliable
judgments in other NLP tasks: machine transla-
tion (Callison-Burch, 2009; Zaidan and Callison-
Burch, 2010), speech recognition (Evanini et al,
2010; Novotney and Callison-Burch, 2010), au-
tomated paraphrase generation (Madnani, 2010),
anaphora resolution (Chamberlain et al, 2009),
word sense disambiguation (Akkaya et al, 2010),
lexicon construction for less commonly taught lan-
guages (Irvine and Klementiev, 2010), fact min-
ing (Wang and Callison-Burch, 2010) and named
entity recognition (Finin et al, 2010) among several
others.
In particular, we make a significant contribution
to the field by showing how to leverage crowdsourc-
1There has been a recent proposal for a related shared
task (Dale and Kilgarriff, 2010) that shows promise.
508
ing to both address the lack of appropriate evaluation
metrics and to make system comparison easier. Our
solution is general enough for, in the simplest case,
intrinsically evaluating a single system on a single
dataset and, more realistically, comparing two dif-
ferent systems (from same or different groups).
2 A Case Study: Extraneous Prepositions
We consider the problem of detecting an extraneous
preposition error, i.e., incorrectly using a preposi-
tion where none is licensed. In the sentence ?They
came to outside?, the preposition to is an extrane-
ous error whereas in the sentence ?They arrived
to the town? the preposition to is a confusion er-
ror (cf. arrived in the town). Most work on au-
tomated correction of preposition errors, with the
exception of Gamon (2010), addresses preposition
confusion errors e.g., (Felice and Pulman, 2008;
Tetreault and Chodorow, 2008; Rozovskaya and
Roth, 2010b). One reason is that in addition to the
standard context-based features used to detect con-
fusion errors, identifying extraneous prepositions
also requires actual knowledge of when a preposi-
tion can and cannot be used. Despite this lack of
attention, extraneous prepositions account for a sig-
nificant proportion?as much as 18% in essays by
advanced English learners (Rozovskaya and Roth,
2010a)?of all preposition usage errors.
2.1 Data and Systems
For the experiments in this paper, we chose a propri-
etary corpus of about 500,000 essays written by ESL
students for Test of English as a Foreign Language
(TOEFL R?). Despite being common ESL errors,
preposition errors are still infrequent overall, with
over 90% of prepositions being used correctly (Lea-
cock et al, 2010; Rozovskaya and Roth, 2010a).
Given this fact about error sparsity, we needed an ef-
ficient method to extract a good number of error in-
stances (for statistical reliability) from the large es-
say corpus. We found all trigrams in our essays con-
taining prepositions as the middle word (e.g., marry
with her) and then looked up the counts of each tri-
gram and the corresponding bigram with the prepo-
sition removed (marry her) in the Google Web1T
5-gram Corpus. If the trigram was unattested or had
a count much lower than expected based on the bi-
gram count, then we manually inspected the trigram
to see whether it was actually an error. If it was,
we extracted a sentence from the large essay corpus
containing this erroneous trigram. Once we had ex-
tracted 500 sentences containing extraneous prepo-
sition error instances, we added 500 sentences con-
taining correct instances of preposition usage. This
yielded a corpus of 1000 sentences with a 50% error
rate.
These sentences, with the target preposition high-
lighted, were presented to 3 expert annotators who
are native English speakers. They were asked to
annotate the preposition usage instance as one of
the following: extraneous (Error), not extraneous
(OK) or too hard to decide (Unknown); the last cat-
egory was needed for cases where the context was
too messy to make a decision about the highlighted
preposition. On average, the three experts had an
agreement of 0.87 and a kappa of 0.75. For subse-
quent analysis, we only use the classes Error and
OK since Unknown was used extremely rarely and
never by all 3 experts for the same sentence.
We used two different error detection systems to
illustrate our evaluation methodology:2
? LM: A 4-gram language model trained on
the Google Web1T 5-gram Corpus with
SRILM (Stolcke, 2002).
? PERC: An averaged Perceptron (Freund and
Schapire, 1999) classifier? as implemented in
the Learning by Java toolkit (Rizzolo and Roth,
2007)?trained on 7 million examples and us-
ing the same features employed by Tetreault
and Chodorow (2008).
3 Crowdsourcing
Recently,we showed that Amazon Mechanical Turk
(AMT) is a cheap and effective alternative to expert
raters for annotating preposition errors (Tetreault et
al., 2010b). In other current work, we have extended
this pilot study to show that CrowdFlower, a crowd-
sourcing service that allows for stronger quality con-
trol on untrained human raters (henceforth, Turkers),
is more reliable than AMT on three different error
detection tasks (article errors, confused prepositions
2Any conclusions drawn in this paper pertain only to these
specific instantiations of the two systems.
509
& extraneous prepositions). To impose such quality
control, one has to provide ?gold? instances, i.e., ex-
amples with known correct judgments that are then
used to root out any Turkers with low performance
on these instances. For all three tasks, we obtained
20 Turkers? judgments via CrowdFlower for each in-
stance and found that, on average, only 3 Turkers
were required to match the experts.
More specifically, for the extraneous preposition
error task, we used 75 sentences as gold and ob-
tained judgments for the remaining 923 non-gold
sentences.3 We found that if we used 3 Turker judg-
ments in a majority vote, the agreement with any one
of the three expert raters is, on average, 0.87 with a
kappa of 0.76. This is on par with the inter-expert
agreement and kappa found earlier (0.87 and 0.75
respectively).
The extraneous preposition annotation cost only
$325 (923 judgments ? 20 Turkers) and was com-
pleted in a single day. The only restriction on the
Turkers was that they be physically located in the
USA. For the analysis in subsequent sections, we
use these 923 sentences and the respective 20 judg-
ments obtained via CrowdFlower. The 3 expert
judgments are not used any further in this analysis.
4 Revamping System Evaluation
In this section, we provide details on how crowd-
sourcing can help revamp the evaluation of error de-
tection systems: (a) by providing more informative
measures for the intrinsic evaluation of a single sys-
tem (? 4.1), and (b) by easily enabling system com-
parison (? 4.2).
4.1 Crowd-informed Evaluation Measures
When evaluating the performance of grammatical
error detection systems against human judgments,
the judgments for each instance are generally re-
duced to the single most frequent category: Error
or OK. This reduction is not an accurate reflection
of a complex phenomenon. It discards valuable in-
formation about the acceptability of usage because
it treats all ?bad? uses as equal (and all good ones
as equal), when they are not. Arguably, it would
be fairer to use a continuous scale, such as the pro-
portion of raters who judge an instance as correct or
3We found 2 duplicate sentences and removed them.
incorrect. For example, if 90% of raters agree on a
rating of Error for an instance of preposition usage,
then that is stronger evidence that the usage is an er-
ror than if 56% of Turkers classified it as Error and
44% classified it as OK (the sentence ?In addition
classmates play with some game and enjoy? is an ex-
ample). The regular measures of precision and recall
would be fairer if they reflected this reality. Besides
fairness, another reason to use a continuous scale is
that of stability, particularly with a small number of
instances in the evaluation set (quite common in the
field). By relying on majority judgments, precision
and recall measures tend to be unstable (see below).
We modify the measures of precision and re-
call to incorporate distributions of correctness, ob-
tained via crowdsourcing, in order to make them
fairer and more stable indicators of system perfor-
mance. Given an error detection system that classi-
fies a sentence containing a specific preposition as
Error (class 1) if the preposition is extraneous and
OK (class 0) otherwise, we propose the following
weighted versions of hits (Hw), misses (Mw) and
false positives (FPw):
Hw =
N?
i
(cisys ? p
i
crowd) (1)
Mw =
N?
i
((1? cisys) ? p
i
crowd) (2)
FPw =
N?
i
(cisys ? (1? p
i
crowd)) (3)
In the above equations, N is the total number of
instances, cisys is the class (1 or 0) , and p
i
crowd
indicates the proportion of the crowd that classi-
fied instance i as Error. Note that if we were to
revert to the majority crowd judgment as the sole
judgment for each instance, instead of proportions,
picrowd would always be either 1 or 0 and the above
formulae would simply compute the normal hits,
misses and false positives. Given these definitions,
weighted precision can be defined as Precisionw =
Hw/(Hw + FPw) and weighted recall as Recallw =
Hw/(Hw + Mw).
510
agreement
co
un
t
0
100
200
300
400
500
50 60 70 80 90 100
Figure 1: Histogram of Turker agreements for all 923 in-
stances on whether a preposition is extraneous.
Precision Recall
Unweighted 0.957 0.384
Weighted 0.900 0.371
Table 1: Comparing commonly used (unweighted) and
proposed (weighted) precision/recall measures for LM.
To illustrate the utility of these weighted mea-
sures, we evaluated the LM and PERC systems
on the dataset containing 923 preposition instances,
against all 20 Turker judgments. Figure 1 shows a
histogram of the Turker agreement for the major-
ity rating over the set. Table 1 shows both the un-
weighted (discrete majority judgment) and weighted
(continuous Turker proportion) versions of precision
and recall for this system.
The numbers clearly show that in the unweighted
case, the performance of the system is overesti-
mated simply because the system is getting as much
credit for each contentious case (low agreement)
as for each clear one (high agreement). In the
weighted measure we propose, the contentious cases
are weighted lower and therefore their contribution
to the overall performance is reduced. This is a
fairer representation since the system should not be
expected to perform as well on the less reliable in-
stances as it does on the clear-cut instances. Essen-
tially, if humans cannot consistently decide whether
0.0
0.2
0.4
0.6
0.8
1.0
Pre
cisio
n/Re
call
50?75%[n=93] 75?90%[n=114] 90?100%[n=716]Agreement Bin
LM PrecisionPERC PrecisionLM RecallPERC Recall
Figure 2: Unweighted precision/recall by agreement bins
for LM & PERC.
a case is an error then a system?s output cannot be
considered entirely right or entirely wrong.4
As an added advantage, the weighted measures
are more stable. Consider a contentious instance in
a small dataset where 7 out of 15 Turkers (a minor-
ity) classified it as Error. However, it might easily
have happened that 8 Turkers (a majority) classified
it as Error instead of 7. In that case, the change in
unweighted precision would have been much larger
than is warranted by such a small change in the
data. However, weighted precision is guaranteed to
be more stable. Note that the instability decreases
as the size of the dataset increases but still remains a
problem.
4.2 Enabling System Comparison
In this section, we show how to easily compare dif-
ferent systems both on the same data (in the ideal
case of a shared dataset being available) and, more
realistically, on different datasets. Figure 2 shows
(unweighted) precision and recall of LM and PERC
(computed against the majority Turker judgment)
for three agreement bins, where each bin is defined
as containing only the instances with Turker agree-
ment in a specific range. We chose the bins shown
4The difference between unweighted and weighted mea-
sures can vary depending on the distribution of agreement.
511
since they are sufficiently large and represent a rea-
sonable stratification of the agreement space. Note
that we are not weighting the precision and recall in
this case since we have already used the agreement
proportions to create the bins.
This curve enables us to compare the two sys-
tems easily on different levels of item contentious-
ness and, therefore, conveys much more information
than what is usually reported (a single number for
unweighted precision/recall over the whole corpus).
For example, from this graph, PERC is seen to have
similar performance as LM for the 75-90% agree-
ment bin. In addition, even though LM precision is
perfect (1.0) for the most contentious instances (the
50-75% bin), this turns out to be an artifact of the
LM classifier?s decision process. When it must de-
cide between what it views as two equally likely pos-
sibilities, it defaults to OK. Therefore, even though
LM has higher unweighted precision (0.957) than
PERC (0.813), it is only really better on the most
clear-cut cases (the 90-100% bin). If one were to re-
port unweighted precision and recall without using
any bins?as is the norm?this important qualifica-
tion would have been harder to discover.
While this example uses the same dataset for eval-
uating two systems, the procedure is general enough
to allow two systems to be compared on two dif-
ferent datasets by simply examining the two plots.
However, two potential issues arise in that case. The
first is that the bin sizes will likely vary across the
two plots. However, this should not be a significant
problem as long as the bins are sufficiently large. A
second, more serious, issue is that the error rates (the
proportion of instances that are actually erroneous)
in each bin may be different across the two plots. To
handle this, we recommend that a kappa-agreement
plot be used instead of the precision-agreement plot
shown here.
5 Conclusions
Our goal is to propose best practices to address the
two primary problems in evaluating grammatical er-
ror detection systems and we do so by leveraging
crowdsourcing. For system development, we rec-
ommend that rather than compressing multiple judg-
ments down to the majority, it is better to use agree-
ment proportions to weight precision and recall to
yield fairer and more stable indicators of perfor-
mance.
For system comparison, we argue that the best
solution is to use a shared dataset and present the
precision-agreement plot using a set of agreed-upon
bins (possibly in conjunction with the weighted pre-
cision and recall measures) for a more informative
comparison. However, we recognize that shared
datasets are harder to create in this field (as most of
the data is proprietary). Therefore, we also provide
a way to compare multiple systems across differ-
ent datasets by using kappa-agreement plots. As for
agreement bins, we posit that the agreement values
used to define them depend on the task and, there-
fore, should be determined by the community.
Note that both of these practices can also be im-
plemented by using 20 experts instead of 20 Turkers.
However, we show that crowdsourcing yields judg-
ments that are as good but without the cost. To fa-
cilitate the adoption of these practices, we make all
our evaluation code and data available to the com-
munity.5
Acknowledgments
We would first like to thank our expert annotators
Sarah Ohls and Waverely VanWinkle for their hours
of hard work. We would also like to acknowledge
Lei Chen, Keelan Evanini, Jennifer Foster, Derrick
Higgins and the three anonymous reviewers for their
helpful comments and feedback.
References
Cem Akkaya, Alexander Conrad, Janyce Wiebe, and
Rada Mihalcea. 2010. Amazon Mechanical Turk
for Subjectivity Word Sense Disambiguation. In Pro-
ceedings of the NAACL Workshop on Creating Speech
and Language Data with Amazon?s Mechanical Turk,
pages 195?203.
Chris Callison-Burch. 2009. Fast, Cheap, and Creative:
Evaluating Translation Quality Using Amazon?s Me-
chanical Turk. In Proceedings of EMNLP, pages 286?
295.
Jon Chamberlain, Massimo Poesio, and Udo Kruschwitz.
2009. A Demonstration of Human Computation Us-
ing the Phrase Detectives Annotation Game. In ACM
SIGKDD Workshop on Human Computation, pages
23?24.
5http://bit.ly/crowdgrammar
512
Robert Dale and Adam Kilgarriff. 2010. Helping Our
Own: Text Massaging for Computational Linguistics
as a New Shared Task. In Proceedings of INLG.
Keelan Evanini, Derrick Higgins, and Klaus Zechner.
2010. Using Amazon Mechanical Turk for Transcrip-
tion of Non-Native Speech. In Proceedings of the
NAACL Workshop on Creating Speech and Language
Data with Amazon?s Mechanical Turk, pages 53?56.
Rachele De Felice and Stephen Pulman. 2008. A
Classifier-Based Approach to Preposition and Deter-
miner Error Correction in L2 English. In Proceedings
of COLING, pages 169?176.
Tim Finin, William Murnane, Anand Karandikar,
Nicholas Keller, Justin Martineau, and Mark Dredze.
2010. Annotating Named Entities in Twitter Data with
Crowdsourcing. In Proceedings of the NAACL Work-
shop on Creating Speech and Language Data with
Amazon?s Mechanical Turk, pages 80?88.
Yoav Freund and Robert E. Schapire. 1999. Large Mar-
gin Classification Using the Perceptron Algorithm.
Machine Learning, 37(3):277?296.
Michael Gamon, Jianfeng Gao, Chris Brockett, Alexan-
der Klementiev, William Dolan, Dmitriy Belenko, and
Lucy Vanderwende. 2008. Using Contextual Speller
Techniques and Language Modeling for ESL Error
Correction. In Proceedings of IJCNLP.
Michael Gamon. 2010. Using Mostly Native Data to
Correct Errors in Learners? Writing. In Proceedings
of NAACL, pages 163?171.
Y. Guo and Gulbahar Beckett. 2007. The Hegemony
of English as a Global Language: Reclaiming Local
Knowledge and Culture in China. Convergence: In-
ternational Journal of Adult Education, 1.
Ann Irvine and Alexandre Klementiev. 2010. Using
Mechanical Turk to Annotate Lexicons for Less Com-
monly Used Languages. In Proceedings of the NAACL
Workshop on Creating Speech and Language Data
with Amazon?s Mechanical Turk, pages 108?113.
Claudia Leacock, Martin Chodorow, Michael Gamon,
and Joel Tetreault. 2010. Automated Grammatical
Error Detection for Language Learners. Synthesis
Lectures on Human Language Technologies. Morgan
Claypool.
Nitin Madnani. 2010. The Circle of Meaning: From
Translation to Paraphrasing and Back. Ph.D. thesis,
Department of Computer Science, University of Mary-
land College Park.
Scott Novotney and Chris Callison-Burch. 2010. Cheap,
Fast and Good Enough: Automatic Speech Recogni-
tion with Non-Expert Transcription. In Proceedings
of NAACL, pages 207?215.
Nicholas Rizzolo and Dan Roth. 2007. Modeling
Discriminative Global Inference. In Proceedings of
the First IEEE International Conference on Semantic
Computing (ICSC), pages 597?604, Irvine, California,
September.
Alla Rozovskaya and D. Roth. 2010a. Annotating ESL
errors: Challenges and rewards. In Proceedings of the
NAACLWorkshop on Innovative Use of NLP for Build-
ing Educational Applications.
Alla Rozovskaya and D. Roth. 2010b. Generating Con-
fusion Sets for Context-Sensitive Error Correction. In
Proceedings of EMNLP.
Andreas Stolcke. 2002. SRILM: An Extensible Lan-
guage Modeling Toolkit. In Proceedings of the Inter-
national Conference on Spoken Language Processing,
pages 257?286.
Joel Tetreault and Martin Chodorow. 2008. The Ups and
Downs of Preposition Error Detection in ESL Writing.
In Proceedings of COLING, pages 865?872.
Joel Tetreault, Jill Burstein, and Claudia Leacock, edi-
tors. 2010a. Proceedings of the NAACL Workshop on
Innovative Use of NLP for Building Educational Ap-
plications.
Joel Tetreault, Elena Filatova, and Martin Chodorow.
2010b. Rethinking Grammatical Error Annotation and
Evaluation with the Amazon Mechanical Turk. In Pro-
ceedings of the NAACL Workshop on Innovative Use
of NLP for Building Educational Applications, pages
45?48.
Rui Wang and Chris Callison-Burch. 2010. Cheap Facts
and Counter-Facts. In Proceedings of the NAACL
Workshop on Creating Speech and Language Data
with Amazon?s Mechanical Turk, pages 163?167.
Omar F. Zaidan and Chris Callison-Burch. 2010. Pre-
dicting Human-Targeted Translation Edit Rate via Un-
trained Human Annotators. In Proceedings of NAACL,
pages 369?372.
513
Coling 2008: Proceedings of the workshop on Human Judgements in Computational Linguistics, pages 24?32
Manchester, August 2008
Native Judgments of Non-Native Usage:
Experiments in Preposition Error Detection
Joel R. Tetreault
Educational Testing Service
660 Rosedale Road
Princeton, NJ, USA
JTetreault@ets.org
Martin Chodorow
Hunter College of CUNY
695 Park Avenue
New York, NY, USA
martin.chodorow@hunter.cuny.edu
Abstract
Evaluation and annotation are two of the
greatest challenges in developing NLP in-
structional or diagnostic tools to mark
grammar and usage errors in the writing of
non-native speakers. Past approaches have
commonly used only one rater to annotate
a corpus of learner errors to compare to
system output. In this paper, we show how
using only one rater can skew system eval-
uation and then we present a sampling ap-
proach that makes it possible to evaluate a
system more efficiently.
1 Introduction
In this paper, we present a series of experiments
that explore the reliability of human judgments
in rating preposition usage. While one tends to
think of annotator disagreements about discourse
and semantics as being quite common, our studies
show that judgments of preposition usage, which is
largely lexically driven, can be just as contentious.
As a result, this unreliability poses a serious issue
for the development and evaluation of NLP tools
in the task of automatically detecting preposition
usage errors in the writing of non-native speakers
of English.
To date, single human annotation has typically
been the gold standard for grammatical error de-
tection, such as in the work of (Izumi et al, 2004),
(Han et al, 2006), (Nagata et al, 2006), (Gamon et
al., 2008)
1
. Although there are several learner cor-
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
1
(Eeg-Olofsson and Knuttson, 2003) had a small evalu-
ation of 40 prepositions and it is unclear whether they used
multiple annotators or not.
pora annotated for preposition and determiner er-
rors (such as the Cambridge Learners Corpus
2
and
the Chinese Learner English Corpus
3
), it is unclear
which portions of these, if any, were doubly anno-
tated. This previous work has side-stepped the is-
sue of annotator reliability, which we address here
through the following three contributions:
? Judgments of Native Usage To motivate our
work in non-native usage, we first illustrate
the difficulty of preposition selection with
two experiments: a cloze test and a choice
test, where native speakers judge native texts
(section 4).
? Judgments of Non-Native Usage As stated
earlier, most computational work in the field
of error detection tools for non-native speak-
ers has relied on a single rater to annotate
a gold standard corpus to check a system?s
output. We conduct an extensive double-
annotation evaluation to measure inter-rater
reliability and show that using one rater can
be unreliable and may produce misleading re-
sults in a system test (section 5).
? Sampling ApproachMultiple annotation can
be very costly and time-consuming, which
may explain why previous work employed
only one rater. As an alternative to the
standard exhaustive annotation, we propose
a sampling approach in which estimates of
the rates of hits, false positives, and misses
are derived from random samples of the sys-
tem?s output, and then precision and recall
of the system can be calculated. We show
that estimates of system performance derived
2
http://www.cambridge.org/elt
3
http://langbank.engl.polyu.edu.hk/corpus/clec.html
24
from the sampling approach are comparable
to those derived from an exhaustive annota-
tion, but require only a fraction of the effort
(section 6).
In short, through a battery of experiments we
show how rating preposition usage, in either na-
tive or non-native texts, is a task that has sur-
prisingly low inter-annotator reliability and thus
greatly impacts system evaluation. We then de-
scribe a method for efficiently annotating non-
native texts to make multiple annotation more fea-
sible.
In section 2, we discuss in more depth the mo-
tivation for detecting usage errors in non-native
writing, as well as the complexities of preposition
usage. In section 3, we describe a system that au-
tomatically detects preposition errors involving in-
correct selection and extraneous usage. In sections
4 and 5 respectively, we discuss experiments on the
reliability of judging native and non-native prepo-
sition usage. In section 6, we present results of our
system and results from comparing the sampling
approach with the standard approach of exhaustive
annotation.
2 Motivation
The long-term goal of our work is to develop a
system which detects errors in grammar and us-
age so that appropriate feedback can be given
to non-native English writers, a large and grow-
ing segment of the world?s population. Estimates
are that in China alone as many as 300 million
people are currently studying English as a for-
eign language. Even in predominantly English-
speaking countries, the proportion of non-native
speakers can be very substantial. For example,
the US National Center for Educational Statistics
(2002) reported that nearly 10% of the students in
the US public school population speak a language
other than English and have limited English pro-
ficiency . At the university level in the US, there
are estimated to be more than half a million for-
eign students whose native language is not English
(Burghardt, 2002). Clearly, there is an increasing
demand for tools for instruction in English as a
Second Language (ESL).
Some of the most common types of ESL usage
errors involve prepositions, determiners and col-
locations. In the work discussed here, we target
preposition usage errors, specifically those of in-
correct selection (?we arrived to the station?) and
extraneous use (?he went to outside?)
4
. Preposi-
tion errors account for a substantial proportion of
all ESL usage errors. For example, (Bitchener et
al., 2005) found that preposition errors accounted
for 29% of all the errors made by intermediate to
advanced ESL students. In addition, such errors
are relatively common. In our learner corpora, we
found that 6% of all prepositions were incorrectly
used. Some other estimates are even higher: for
example, (Izumi et al, 2003) reported error rates
that were as high as 10% in a Japanese learner cor-
pus.
At least part of the difficulty in mastering prepo-
sitions seems to be due to the great variety of lin-
guistic functions that they serve. When a prepo-
sition marks the argument of a predicate, such as
a verb, an adjective, or a noun, preposition se-
lection is constrained by the argument role that it
marks, the noun which fills that role, and the par-
ticular predicate. Many English verbs also display
alternations (Levin, 1993) in which an argument
is sometimes marked by a preposition and some-
times not (e.g., ?They loaded the wagon with hay?
/ ?They loaded hay on the wagon?). When prepo-
sitions introduce adjuncts, such as those of time
or manner, selection is constrained by the object
of the preposition (?at length?, ?in time?, ?with
haste?). Finally, the selection of a preposition for
a given context also depends upon the intention of
the writer (?we sat at the beach?, ?on the beach?,
?near the beach?, ?by the beach?).
3 Automatically Detecting Preposition
Usage Errors
In this section, we give a description of our sys-
tem and compare its performance to other sys-
tems. Although the focus of this paper is on hu-
man judgments in the task of error detection, we
describe our system to show that variability in hu-
man judgments can impact the evaluation of a sys-
tem in this task. A full description of our system
and its performance can be found in (Tetreault and
Chodorow, 2008).
3.1 System
Our approach treats preposition error detection as
a classification problem: that is, given a context of
two words before and two words after the writer?s
preposition, what is the best preposition to use?
4
There is a third error type, omission (?we are fond null
beer?), that is a topic for our future research.
25
An error is marked when the system?s sugges-
tion differs from the writer?s by a certain threshold
amount.
We have used a maximum entropy (ME) clas-
sifier (Ratnaparkhi, 1998) to select the most prob-
able preposition for a given context from a set of
34 common English prepositions. One advantage
of using ME is that there are implementations of it
which can handle very large models built frommil-
lions of training events and consisting of hundreds
of thousands of feature-value pairs. To construct
a model, we begin with a training corpus that is
POS-tagged and heuristically chunked into noun
phrases and verb phrases
5
. For each preposition
that occurs in the training corpus, a preprocessing
program extracts a total of 25 features. These con-
sist of words and POS tags in positions adjacent to
the preposition and in the heads of nearby phrases.
In addition, we include combination features that
merge the head features. We also include features
representing only the tags to be able to cover cases
in testing where the words in the context were not
seen in training.
In many NLP tasks (parsing, POS-tagging, pro-
noun resolution), it is easy to acquire training data
that is similar to the testing data. However, in the
case of grammatical error detection, one does not
have that luxury because reliable error-annotated
ESL corpora that are large enough for training a
statistical classifier simply do not exist. To circum-
vent this problem, we have trained our classifier on
examples of prepositions used correctly, as in news
text.
3.2 Evaluation
Before evaluating our system on non-native writ-
ing, we evaluated how well it does on the task of
preposition selection in native text, an area where
there has been relatively little work to date. In this
task, the system predicts the writer?s preposition
based on its context. Its prediction is scored au-
tomatically by comparison to what the writer actu-
ally wrote. Most recently, (Gamon et al, 2008) ad-
dressed preposition selection by developing a sys-
tem that combined a decision tree and a language
model. Besides the difference in algorithms, there
is also a difference in coverage between their sys-
tem, which selects among 13 prepositions plus a
category for Other, and the system presented here,
5
We have avoided parsing because our ultimate test corpus
is non-native writing, text that is difficult to parse due to the
presence of numerous errors in spelling and syntax.
Prep (Gamon et al, 2008) (Tetreault et al, 2008)
in 0.592 0.845
for 0.459 0.698
of 0.759 0.906
on 0.322 0.751
to 0.627 0.775
with 0.361 0.675
at 0.372 0.685
by 0.502 0.747
as 0.699 0.711
from 0.528 0.591
about 0.800 0.654
Table 1: Comparison of F-measures on En-
carta/Reuters Corpus
which selects among 34 prepositions. In their sys-
tem evaluation, they split a corpus of Reuters News
text and Microsoft Encarta into two sets: 70% for
training (3.2M examples), and the remaining 30%
for testing (1.4M examples). For purposes of com-
parison, we used the same corpus and evaluation
method. While (Gamon et al, 2008) do not present
their overall accuracy figures on the Encarta eval-
uation, they do present the precision and recall
scores for each preposition. In Table 3.2, we dis-
play their results in terms of F-measures and show
the performance of our system for each preposi-
tion. Our model outperforms theirs for 9 out of the
10 prepositions that both systems handle. Over-
all accuracy for our system is 77.4% and increases
to 79.0% when 7M more training examples are
added. For comparison purposes, using a major-
ity baseline (always selecting the preposition of) in
this domain results in an accuracy of 27.2%.
(Felice and Pullman, 2007) used perceptron
classifiers for preposition selection in BNC News
Text at 85% accuracy. For each of the five most
frequent prepositions, they used a separate binary
classifier to decide whether that preposition should
be used or not. The classifiers are not combined
into a unified model. When we reconfigured our
system and evaluation to be comparable to (Felice
and Pullman, 2007), our model achieved an accu-
racy of 90% on the same five prepositions when
tested on Wall Street Journal News, which is simi-
lar, though not identical, to BNC News.
While systems can perform at close to 80% ac-
curacy in the task of preposition selection in native
texts, this high performance does not transfer to
the end-task of detecting preposition errors in es-
says by non-native writers. For example, (Izumi et
al., 2003) reported precision and recall as low as
25% and 7% respectively when detecting different
26
grammar errors (one of which was prepositions)
in English essays by non-native writers. (Gamon
et al, 2008) reported precision up to 80% in their
evaluation on the CLEC corpus, but no recall fig-
ure was reported. We have found that our system
(the model which performs at 77.4%), also per-
forms as high as 80% precision, but recall ranged
from 12% to 26% depending on the non-native test
corpus.
While our recall figures may seem low, espe-
cially when compared to other NLP tasks such as
parsing and anaphora resolution, this is really a re-
flection of how difficult the task is. In addition, in
error detection tasks, high precision (and thus low
recall) is favored since one wants to minimize the
number of false positives a student may see. This
is a common practice in grammatical error detec-
tion applications, such as in (Han et al, 2006) and
(Gamon et al, 2008).
4 Human Judgments of Native Usage
4.1 Cloze Test
With so many sources of variation in English
preposition usage, we wondered if the task of se-
lecting a preposition for a given context might
prove challenging even for native speakers. To
investigate this possibility, we randomly selected
200 sentences from Microsoft?s Encarta Encyclo-
pedia, and, in each sentence, we replaced a ran-
domly selected preposition with a blank. We then
asked two native English speakers to perform a
cloze task by filling in the blank with the best
preposition, given the context provided by the rest
of the sentence. In addition, we had our system
predict which preposition should fill each blank as
well. Our results (Table 2) showed only about 76%
agreement between the two raters (bottom row),
and between 74% and 78% when each rater was
compared individually with the original preposi-
tion used in Encarta. Surprisingly, the system
performed just as well as the two native raters,
when compared with Encarta (third row). Al-
though these results seem very promising, it should
be noted that in many cases where the system dis-
agreed with Encarta, its prediction was not a good
fit for the context. But in the cases where the
raters disagreed with Encarta, their prepositions
were also licensed by the context, and thus were
acceptable alternatives to the preposition that was
used in the text.
Our cloze study shows that even with well-
Agreement Kappa
Encarta vs. Rater 1 0.78 0.73
Encarta vs. Rater 2 0.74 0.68
Encarta vs. System 0.75 0.68
Rater 1 vs. Rater 2 0.76 0.70
Table 2: Cloze Experiment on Encarta
formed text, native raters can disagree with each
other by 25% in the task of preposition selec-
tion. We can expect even more disagreement when
the task is preposition error detection in ?noisy?
learner texts.
4.2 Choice Test
The cloze test presented above was scored by au-
tomatically comparing the system?s choice (or the
rater?s choice) with the preposition that was actu-
ally written. But there are many contexts that li-
cense multiple prepositions, and in these cases, re-
quiring an exact match is too stringent a scoring
criterion.
To investigate how the exact match metric might
underestimate system performance, and to further
test the reliability of human judgments in native
text, we conducted a choice test in which two
native English speakers were presented with 200
sentences from Encarta and were asked to select
which of two prepositions better fit the context.
One was the originally written preposition and the
other was the system?s suggestion, displayed in
random order. The human raters were also given
the option of marking both prepositions as equally
good or equally bad. The results indicated that
both Rater 1 and Rater 2 considered the system?s
preposition equal to or better than the writer?s
preposition in 28% of the cases. This suggests
that 28% of the mismatched cases in the automatic
evaluation are not system errors but rather are in-
stances where the context licenses multiple prepo-
sitions. If these mismatches in the automatic eval-
uation are actually cases of correct system perfor-
mance, then the Encarta/Reuters test which per-
forms at 75% accuracy (third row of Table 2), is
more realistically around 82% accuracy (28% of
the 25% mismatch rate is 7%).
5 Annotator Reliability
In this section, we address the central problem of
evaluating NLP error detection tools on learner
data. As stated earlier, most previous work has re-
lied on only one rater to either create an annotated
27
corpus of learner errors, or to check the system?s
output. While some grammatical errors, such as
number disagreement between subject and verb,
no doubt show very high reliability, others, such as
usage errors involving prepositions or determiners
are likely to be much less reliable. In section 5.1,
we describe our efforts in annotating a large cor-
pus of student learner essays for preposition us-
age errors. Unlike previous work such as (Izumi
et al, 2004) which required the rater to check for
almost 40 different error types, we focus on anno-
tating only preposition errors in hopes that having
a single type of target will insure higher reliabil-
ity by reducing the cognitive demands on the rater.
Section 5.2 asks whether, under these conditions,
one rater is acceptable for this task. In section 6,
we describe an approach to efficiently evaluating a
system that does not require the amount of effort
needed in the standard approach to annotation.
5.1 Annotation Scheme
To create a gold-standard corpus of error anno-
tations for system evaluation, and also to deter-
mine whether multiple raters are better than one,
we trained two native English speakers to anno-
tate preposition errors in ESL text. Both annota-
tors had prior experience in NLP annotation and
also in ESL error detection. The training was very
extensive: both raters were trained on 2000 prepo-
sition contexts and the annotation manual was it-
eratively refined as necessary. To our knowledge,
this is the first scheme that specifically targets an-
notating preposition errors
6
.
The two raters were shown sentences randomly
selected from student essays, with each preposi-
tion highlighted in the sentence. The raters were
also shown the sentence which preceded the one
containing the preposition that they rated. The an-
notator was first asked to indicate if there were any
spelling errors within the context of the preposi-
tion (?2-word window and the commanding verb).
Next the annotator noted determiner or plural er-
rors in the context, and then checked if there were
any other grammatical errors (for example, wrong
verb form). The reason for having the annota-
tors check spelling and grammar is that other mod-
ules in a grammatical error detection system would
be responsible for these error types. For an ex-
6
(Gamon et al, 2008) did not have a scheme for annotat-
ing preposition errors to create a gold standard corpus, but did
use a scheme for the similar problem of verifying a system?s
output in preposition error detection.
ample of a sentence with multiple spelling, gram-
matical and collocational errors, consider the fol-
lowing sentence: ?In consion, for some reasons,
museums, particuraly known travel place, get on
many people.? A spelling error follows the prepo-
sition In, and a collocational error surrounds on. If
the contexts are not corrected, it is impossible to
discern if the prepositions are correct. Of course,
there is the chance that by removing these we will
screen out cases where there are multiple interact-
ing errors in the context that involve prepositions.
When comparing human judgments to the perfor-
mance of the preposition module, the latter should
not be penalized for other kinds of errors in the
context.
Finally, the annotator judged the writer?s prepo-
sition with a rating of ?0-extraneous preposition?,
?1-incorrect preposition?, ?2-correct preposition?,
or ?e-equally good prepositions?. If the writer
used an incorrect preposition, the rater supplied the
best preposition(s) given the context. Very often,
when the writer?s preposition was correct, several
other prepositions could also have occurred in the
same context. In these cases, the annotator was in-
structed to use the ?e? category and list the other
equally plausible alternatives. After judging the
use of the preposition and, if applicable, supplying
alternatives, the annotator indicated her confidence
in her judgment on a 2-point scale of ?1-low? and
?2-high?.
5.2 Two Raters vs. One?
Following training, each annotator judged approxi-
mately 18,000 occurrences of preposition use. An-
notation of 500 occurrences took an average of 3 to
4 hours. In order to calculate agreement and kappa
values, we periodically provided identical sets of
100 preposition occurrences for both annotators to
judge (totaling 1800 in all). After removing in-
stances where there were spelling or grammar er-
rors, and after combining categories ?2? and ?e?,
both of which were judgments of correct usage,
we computed the kappa values for the remaining
doubly judged sets. These ranged from 0.411 to
0.786, with an overall combined value of 0.630
7
.
The confusion matrix for the combined set (to-
taling 1336 contexts) is shown in Table 3. The
rows represent Rater 1?s (R1) judgments while the
columns represent Rater 2?s judgments. As one
7
When including spelling and grammar annotations,
kappa ranged from 0.474 to 0.773.
28
would expect given the prior reports of preposition
error rates in non-native writing, the raters? agree-
ment for this task was quite high overall (0.952)
due primarily to the large agreement count where
both annotators rated the usage ?OK? (1213 total
contexts). However there were 42 prepositions that
both raters marked as a ?Wrong Choice? and 17 as
?Extraneous.? It is important to note the disagree-
ments in judging these errors: for example, Rater
1 judged 26 prepositions to be errors that Rater 2
judged to be OK, for a disagreement rate of .302
(26/86). Similarly, Rater 2 judged 37 prepositions
to be errors that Rater 1 judged to be OK, for a
disagreement rate of .381 (37/97).
R1?; R2? Extraneous Wrong-Choice OK
Extraneous 17 0 6
Wrong-Choice 1 42 20
OK 4 33 1213
Table 3: Confusion Matrix
The kappa of 0.630 and the off-diagonal cells
in the confusion matrix both show the difficulty
of this task and also show how two highly trained
raters can produce very different judgments. This
suggests that for certain error annotation tasks,
such as preposition usage, it may not be appropri-
ate to use only one rater and that using two or more
raters to produce an adjudicated gold-standard set
is the more acceptable path.
As a second test, we used a set of 2,000 prepo-
sition contexts from ESL essays (Chodorow et al,
2007) that were doubly annotated by native speak-
ers with a scheme similar to that described above.
We then compared an earlier version of our sys-
tem to both raters? judgments, and found that there
was a 10% difference in precision and a 5% differ-
ence in recall between the two system/rater com-
parisons. That means that if one is using only a
single rater as a gold standard, there is the potential
to over- or under-estimate precision by as much as
10%. Clearly this is problematic when evaluating
a system?s performance. The results are shown in
Table 4.
Precision Recall
System vs. Rater 1 0.78 0.26
System vs. Rater 2 0.68 0.21
Table 4: Rater/System Comparison
6 Sampling Approach
If one uses multiple raters for error annotation,
there is the possibility of creating an adjudicated
set, or at least calculating the variability of sys-
tem evaluation. However, annotation with multiple
raters has its own disadvantages in that it is much
more expensive and time-consuming. Even using
one rater to produce a sizeable evaluation corpus
of preposition errors is extremely costly. For ex-
ample, if we assume that 500 prepositions can be
annotated in 4 hours using our annotation scheme,
and that the error rate for prepositions is 10%, then
it would take at least 80 hours for a rater to find
and mark 1000 errors. In this section, we propose
a more efficient annotation approach to circumvent
this problem.
6.1 Methodology
The sampling procedure outlined here is inspired
by the one described in (Chodorow and Leacock,
2000). The central idea is to skew the annotation
corpus so that it contains a greater proportion of
errors. The result is that an annotator checks more
potential errors since he or she is spending less
time checking prepositions used correctly.
Here are the steps in the procedure. Figure 1 il-
lustrates this procedure with a hypothetical corpus
of 10,000 preposition examples.
1. Process a test corpus of sentences so that each
preposition in the corpus is labeled ?OK? or
?Error? by the system.
2. Divide the processed corpus into two sub-
corpora, one consisting of the system?s ?OK?
prepositions and the other of the system?s
?Error? prepositions. For the hypothetical
data in Figure 1, the ?OK? sub-corpus con-
tains 90% of the prepositions, and the ?Error?
sub-corpus contains the remaining 10%.
3. Randomly sample cases from each sub-
corpus and combine the samples into an an-
notation set that is given to a ?blind? human
rater. We generally use a higher sampling
rate for the ?Error? sub-corpus because we
want to ?enrich? the annotation set with a
larger proportion of errors than is found in the
test corpus as a whole. In Figure 1, 75% of
the ?Error? sub-corpus is sampled while only
16% of the ?OK? sub-corpus is sampled.
29
Figure 1: Sampling Approach (with hypothetical sample calculations)
4. For each case that the human rater judges to
be an error, check to see which sub-corpus it
came from. If it came from the ?OK? sub-
corpus, then the case is a Miss (an error that
the system failed to detect). If it came from
the ?Error? sub-corpus, then the case is a Hit
(an error that the system detected). If the rater
judges a case to be a correct usage and it came
from the ?Error? sub-corpus, then it is a False
Positive (FP).
5. Calculate the proportions of Hits and FPs in
the sample from the ?Error? sub-corpus. For
the hypothetical data in Figure 1, these val-
ues are 600/750 = 0.80 for Hits, and 150/750
= 0.20 for FPs. Calculate the proportion of
Misses in the sample from the ?OK? sub-
corpus. For the hypothetical data, this is
450/1500 = 0.30 for Misses.
6. The values computed in step 5 are conditional
proportions based on the sub-corpora. To cal-
culate the overall proportions in the test cor-
pus, it is necessary to multiply each value
by the relative size of its sub-corpus. This
is shown in Table 5, where the proportion of
Hits in the ?Error? sub-corpus (0.80) is mul-
tiplied by the relative size of the ?Error? sub-
corpus (0.10) to produce an overall Hit rate
(0.08). Overall rates for FPs and Misses are
calculated in a similar manner.
7. Using the values from step 6, calculate Preci-
sion (Hits/(Hits + FP)) and Recall (Hits/(Hits
+ Misses)). These are shown in the last two
rows of Table 5.
Estimated Overall Rates
Sample Proportion * Sub-Corpus Proportion
Hits 0.80 * 0.10 = 0.08
FP 0.20 * 0.10 = 0.02
Misses 0.30 * 0.90 = 0.27
Precision 0.08/(0.08 + 0.02) = 0.80
Recall 0.08/(0.08 + 0.27) = 0.23
Table 5: Sampling Calculations (Hypothetical)
This method is similar in spirit to active learning
((Dagan and Engelson, 1995) and (Engelson and
Dagan, 1996)), which has been used to iteratively
build up an annotated corpus, but it differs from
active learning applications in that there are no it-
erative loops between the system and the human
annotator(s). In addition, while our methodology
is used for evaluating a system, active learning is
commonly used for training a system.
6.2 Application
Next, we tested whether our proposed sampling
approach provides good estimates of a system?s
performance. For this task, we split a large corpus
of ESL essays into two sets: first, a set of 8,269
preposition contexts (standard approach corpus) to
be annotated using the scheme in section 5.1, and
30
second, a set of 22,000 preposition contexts to be
rated using the sampling approach (sampling cor-
pus). We used two non-overlapping sets because
the raters were the same for this test of the two ap-
proaches.
Using the standard approach, the sampling cor-
pus of 22,000 prepositions would normally take
several weeks for two raters to double annotate
and then adjudicate. After this corpus was di-
vided into ?OK? and ?Error? sub-corpora, the two
sub-corpora were proportionally sampled, result-
ing in an annotation set of 750 preposition con-
texts (500 contexts from the ?OK? sub-corpus and
250 contexts from the ?Error? sub-corpus). This
required roughly 6 hours for annotation, which is
substantially more manageable than the standard
approach. We had both raters work together to
make judgments for each preposition context.
The precision and recall scores for both ap-
proaches are shown in Table 6 and are quite simi-
lar, thus suggesting that the sampling approach can
be used as an alternative to exhaustive annotation.
Precision Recall
Standard Approach 0.80 0.12
Sampling Approach 0.79 0.14
Table 6: Sampling Results
6.3 Confidence Intervals
It is important with the sampling approach to use
appropriate sample sizes when drawing from the
sub-corpora, because the accuracy of the estimates
of hits and misses will depend upon the propor-
tion of errors in each sub-corpus as well as on the
sample sizes. The ?OK? sub-corpus is expected
to have even fewer errors than the overall base
rate, so it is especially important to have a rela-
tively large sample from this sub-corpus. The com-
parison study described above used an ?OK? sub-
corpus sample that was twice as large as the Error
sub-corpus sample.
One can compute the 95% confidence interval
(CI) for the estimated rates of hits, misses and false
positives by using the formula:
CI = p? 1.96? ?
p
where p is the proportion and ?
p
is the standard
error of the proportion given by:
?
p
=
?
p(1? p)
N
where N is the sample size.
For the example in Figure 1, the confidence in-
terval for the proportion of Hits from the sample of
the ?Error? sub-corpus is:
CI
hits
= 0.80? 1.96?
?
0.8? (1? 0.80)
750
which yields an interval of 0.077 and 0.083. Using
these values, the confidence interval for precision
is 0.77 to 0.83. The interval for recall can be com-
puted in a similar manner. Of course, a larger sam-
ple size will yield narrower confidence intervals.
6.4 Summary
Table 7 summarizes the advantages and disadvan-
tages of three methods for evaluating error detec-
tion systems. The standard (or exhaustive) ap-
proach refers to the method of annotating the er-
rors in a large corpus. Its advantage is that the an-
notated corpus can be reused to evaluate the same
system or compare multiple systems. However,
it is costly and time-consuming which often pre-
cludes the use of multiple raters. The verification
method (as used in (Gamon et al, 2008)), refers to
the method of simply checking the acceptability of
system output with respect to the writer?s preposi-
tion. Like the sampling method, it has the advan-
tages of efficiency and use of multiple raters (when
compared to the standard method). But the dis-
advantage of verification is that it does not permit
estimation of recall. Both verification and vam-
pling methods require re-annotation for system re-
testing and comparison. In terms of system devel-
opment, sampling (and to a lesser extent, verifica-
tion) allows one to quickly assess system perfor-
mance on a new corpus.
In short, the sampling approach is intended to
alleviate the burden on annotators when faced with
the task of having to rate several thousand errors of
a particular type to produce a sizeable error corpus.
7 Conclusions
In this paper, we showed that the standard ap-
proach to evaluating NLP error detection sys-
tems (comparing the system?s output with a gold-
standard annotation) can greatly skew system re-
sults when the annotation is done by only one rater.
However, one reason why a single rater is com-
monly used is that building a corpus of learner er-
rors can be extremely costly and time-consuming.
To address this efficiency issue, we presented a
31
Approach Advantages Disadvantages
Standard Easy to retest system (no re-annotation required) Costly
Easy to compare systems Time-Consuming
Most reliably estimates precision and recall Difficult to use multiple raters
Sampling Efficient, especially for low-frequency errors Less reliable estimate of recall
Permits estimation of precision and recall Hard to re-test system (re-annotation required)
More easily allows use of multiple raters Hard to compare systems
Verification Efficient, especially for low-frequency errors Does not permit estimation of recall
More easily allows use of multiple raters Hard to re-test system (re-annotation required)
Hard to compare systems
Table 7: Comparison of Evaluation Methods
sampling approach that produces results compa-
rable to exhaustive annotation. This makes using
multiple raters possible since less time is required
to assess the system?s performance. While the
work presented here has focused on prepositions,
the reasons for using multiple raters and a sam-
pling approach apply equally to other error types,
such as determiners and collocations.
It should be noted that the work here uses two
raters. For future work, we plan on annotating
preposition errors with more than two raters to de-
rive a range of judgments. We also plan to look at
the effects of feedback for errors involving prepo-
sitions and determiners, on the quality of ESLwrit-
ing.
The preposition error detection system de-
scribed here was recently integrated into Cri-
terion
SM
Online Writing Evaluation Service
developed by Educational Testing Service.
Acknowledgements We would first like to
thank our two annotators Sarah Ohls and Waverly
VanWinkle for their hours of hard work. We would
also like to acknowledge the three anonymous
reviewers and Derrick Higgins for their helpful
comments and feedback.
References
Bitchener, J., S. Young, and D. Cameron. 2005. The ef-
fect of different types of corrective feedback on ESL
student writing. Journal of Second Language Writ-
ing.
Burghardt, L. 2002. Foreign applications soar at uni-
versities. New York Times, April.
Chodorow, M. and C. Leacock. 2000. An unsupervised
method for detecting grammatical errors. In NAACL.
Chodorow, M., J. Tetreault, and N-R. Han. 2007. De-
tection of grammatical errors involving prepositions.
In Proceedings of the Fourth ACL-SIGSEM Work-
shop on Prepositions.
Dagan, I. and S. Engelson. 1995. Committee-based
sampling for training probabilistic classifiers. In
Proceedings of ICML, pages 150?157.
Eeg-Olofsson, J. and O. Knuttson. 2003. Automatic
grammar checking for second language learners - the
use of prepositions. In Nodalida.
Engelson, S. and I. Dagan. 1996. Minimizing manual
annotation cost in supervised training from corpora.
In Proceedings of ACL, pages 319?326.
Felice, R. De and S. Pullman. 2007. Automatically ac-
quiring models of preposition use. In Proceedings of
the Fourth ACL-SIGSEM Workshop on Prepositions.
Gamon, M., J. Gao, C. Brockett, A. Klementiev, W. B.
Dolan, D. Belenko, and L. Vanderwende. 2008. Us-
ing contextual speller techniques and language mod-
eling for esl error correction. In IJCNLP.
Han, N-R., M. Chodorow, and C. Leacock. 2006. De-
tecting errors in English article usage by non-native
speakers. Natural Language Engineering, 12:115?
129.
Izumi, E., K. Uchimoto, T. Saiga, T. Supnithi, and
H. Isahara. 2003. Automatic error detection in the
Japanese leaners? English spoken data. In ACL.
Izumi, E., K. Uchimoto, and H. Isahara. 2004. The
overview of the sst speech corpus of Japanese learner
English and evaluation through the experiment on
automatic detection of learners? errors. In LREC.
Levin, B. 1993. English verb classes and alternations:
a preliminary investigation. Univ. of Chicago Press.
Nagata, R., A. Kawai, K. Morihiro, and N. Isu. 2006.
A feedback-augmented method for detecting errors
in the writing of learners of English. In Proceedings
of the ACL/COLING.
NCES. 2002. National center for educational statis-
tics: Public school student counts, staff, and graduate
counts by state: School year 2000-2001.
Ratnaparkhi, A. 1998. Maximum Entropy Models for
natural language ambiguity resolution. Ph.D. thesis,
University of Pennsylvania.
Tetreault, J. and M. Chodorow. 2008. The ups and
downs of preposition error detection in ESL writing.
In COLING.
32
Proceedings of the NAACL HLT 2010 Fifth Workshop on Innovative Use of NLP for Building Educational Applications, pages 45?48,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Rethinking Grammatical Error Annotation and Evaluation with the
Amazon Mechanical Turk
Joel R. Tetreault
Educational Testing Service
Princeton, NJ, 08540, USA
JTetreault@ets.org
Elena Filatova
Fordham University
Bronx, NY, 10458, USA
filatova@fordham.edu
Martin Chodorow
Hunter College of CUNY
New York, NY, USA
martin.chodorow@
hunter.cuny.edu
Abstract
In this paper we present results from two pi-
lot studies which show that using the Amazon
Mechanical Turk for preposition error anno-
tation is as effective as using trained raters,
but at a fraction of the time and cost. Based
on these results, we propose a new evaluation
method which makes it feasible to compare
two error detection systems tested on different
learner data sets.
1 Introduction
The last few years have seen an explosion in the de-
velopment of NLP tools to detect and correct errors
made by learners of English as a Second Language
(ESL). While there has been considerable empha-
sis placed on the system development aspect of the
field, with researchers tackling some of the tough-
est ESL errors such as those involving articles (Han
et al, 2006) and prepositions (Gamon et al, 2008),
(Felice and Pullman, 2009), there has been a woeful
lack of attention paid to developing best practices for
annotation and evaluation.
Annotation in the field of ESL error detection has
typically relied on just one trained rater, and that
rater?s judgments then become the gold standard for
evaluating a system. So it is very rare that inter-rater
reliability is reported, although, in other NLP sub-
fields, reporting reliability is the norm. Time and
cost are probably the two most important reasons
why past work has relied on only one rater because
using multiple annotators on the same ESL texts
would obviously increase both considerably. This is
especially problematic for this field of research since
some ESL errors, such as preposition usage, occur at
error rates as low as 10%. This means that to collect
a corpus of 1,000 preposition errors, an annotator
would have to check over 10,000 prepositions.1
(Tetreault and Chodorow, 2008b) challenged the
view that using one rater is adequate by showing
that preposition usage errors actually do not have
high inter-annotator reliability. For example, trained
raters typically annotate preposition errors with a
kappa around 0.60. This low rater reliability has
repercussions for system evaluation: Their experi-
ments showed that system precision could vary as
much as 10% depending on which rater?s judgments
they used as the gold standard. For some grammat-
ical errors such as subject-verb agreement, where
rules are clearly defined, it may be acceptable to
use just one rater. But for usage errors, the rules
are less clearly defined and two native speakers can
have very different judgments of what is acceptable.
One way to address this is by aggregating a multi-
tude of judgments for each preposition and treating
this as the gold standard, however such a tactic has
been impractical due to time and cost limitations.
While annotation is a problem in this field, com-
paring one system to another has also been a major
issue. To date, none of the preposition and article
error detection systems in the literature have been
evaluated on the same corpus. This is mostly due to
the fact that learner corpora are difficult to acquire
(and then annotate), but also to the fact that they are
1(Tetreault and Chodorow, 2008b) report that it would take
80hrs for one of their trained raters to find and mark 1,000
preposition errors.
45
usually proprietary and cannot be shared. Examples
include the Cambridge Learners Corpus2 used in
(Felice and Pullman, 2009), and TOEFL data, used
in (Tetreault and Chodorow, 2008a). This makes it
difficult to compare systems since learner corpora
can be quite different. For example, the ?difficulty?
of a corpus can be affected by the L1 of the writ-
ers, the number of years they have been learning En-
glish, their age, and also where they learn English (in
a native-speaking country or a non-native speaking
country). In essence, learner corpora are not equal,
so a system that performs at 50% precision in one
corpus may actually perform at 80% precision on
a different one. Such an inability to compare sys-
tems makes it difficult for this NLP research area to
progress as quickly as it otherwise might.
In this paper we show that the Amazon Mechani-
cal Turk (AMT), a fast and cheap source of untrained
raters, can be used to alleviate several of the evalua-
tion and annotation issues described above. Specifi-
cally we show:
? In terms of cost and time, AMT is an effec-
tive alternative to trained raters on the tasks of
preposition selection in well-formed text and
preposition error annotation in ESL text.
? With AMT, it is possible to efficiently collect
multiple judgments for a target construction.
Given this, we propose a new method for evalu-
ation that finally allows two systems to be com-
pared to one another even if they are tested on
different corpora.
2 Amazon Mechnical Turk
Amazon provides a service called the Mechani-
cal Turk which allows requesters (companies, re-
searchers, etc.) to post simple tasks (known as Hu-
man Intelligence Tasks, or HITs) to the AMT web-
site for untrained raters to perform for payments as
low as $0.01 in many cases (Sheng et al, 2008).
Recently, AMT has been shown to be an effective
tool for annotation and evalatuation in NLP tasks
ranging from word similarity detection and emotion
detection (Snow et al, 2008) to Machine Transla-
tion quality evaluation (Callison-Burch, 2009). In
these cases, a handful of untrained AMT workers
2http://www.cambridge.org/elt
(or Turkers) were found to be as effective as trained
raters, but with the advantage of being considerably
faster and less expensive. Given the success of us-
ing AMT in other areas of NLP, we test whether we
can leverage it for our work in grammatical error de-
tection, which is the focus of the pilot studies in the
next two sections.
The presence of a gold standard in the above pa-
pers is crucial. In fact, the usability of AMT for text
annotation has been demostrated in those studies by
showing that non-experts? annotation converges to
the gold standard developed by expert annotators.
However, in our work we concentrate on tasks where
there is no single gold standard, either because there
are multiple prepositions that are acceptable in a
given context or because the conventions of preposi-
tion usage simply do not conform to strict rules.
3 Selection Task
0.60
0.65
0.70
0.75
0.80
0.85
0.90
1 2 3 4 5 6 7 8 9 10
Kap
pa
Number of Turkers
Writer vs. AMTRater 1 vs. AMTRater 2 vs. AMT
Figure 1: Error Detection Task: Reliability of AMT as a
function of number of judgments
Typically, an early step in developing a preposi-
tion or article error detection system is to test the
system on well-formed text written by native speak-
ers to see how well the system can predict, or select,
the writer?s preposition given the context around
the preposition. (Tetreault and Chodorow, 2008b)
showed that trained human raters can achieve very
high agreement (78%) on this task. In their work, a
rater was shown a sentence with a target preposition
replaced with a blank, and the rater was asked to se-
lect the preposition that the writer may have used.
We replicate this experiment not with trained raters
but with the AMT to answer two research questions:
1. Can untrained raters be as effective as trained
46
raters? 2. If so, how many raters does it take to
match trained raters?
In the experiment, a Turker was presented with
a sentence from Microsoft?s Encarta encyclopedia,
with one preposition in that sentence replaced with
a blank. There were 194 HITs (sentences) in all, and
we requested 10 Turker judgments per HIT. Some
Turkers did only one HIT, while others completed
more than 100, though none did all 194. The Turk-
ers? performance was analyzed by comparing their
responses to those of two trained annotators and to
the Encarta writer?s preposition, which was consid-
ered the gold standard in this task. Comparing each
trained annotator to the writer yielded a kappa of
0.822 and 0.778, and the two raters had a kappa of
0.742. To determine how many Turker responses
would be required to match or exceed these levels of
reliability, we randomly selected samples of various
sizes from the sets of Turker responses for each sen-
tence. For example, when samples were of size N =
4, four responses were randomly drawn from the set
of ten responses that had been collected. The prepo-
sition that occurred most frequently in the sample
was used as the Turker response for that sentence. In
the case of a tie, a preposition was randomly drawn
from those tied for most frequent. For each sample
size, 100 samples were drawn and the mean values
of agreement and kappa were calculated. The reli-
ability results presented in Table 1 show that, with
just three Turker responses, kappa with the writer
(top line) is comparable to the values obtained from
the trained annotators (around 0.8). Most notable is
that with ten judgments, the reliability measures are
much higher than those of the trained annotators. 3
4 Error Detection Task
While the previous results look quite encouraging,
the task they are based on, preposition selection in
well-formed text, is quite different from, and less
challenging than, the task that a system must per-
form in detecting errors in learner writing. To exam-
ine the reliability of Turker preposition error judg-
ments, we ran another experiment in which Turkers
were presented with a preposition highlighted in a
sentence taken from an ESL corpus, and were in-
3We also experimented with 50 judgments per sentence, but
agreement and kappa improved only negligibly.
structed to judge its usage as either correct, incor-
rect, or the context is too ungrammatical to make
a judgment. The set consisted of 152 prepositions
in total, and we requested 20 judgments per prepo-
sition. Previous work has shown this task to be a
difficult one for trainer raters to attain high reliabil-
ity. For example, (Tetreault and Chodorow, 2008b)
found kappa between two raters averaged 0.630.
Because there is no gold standard for the er-
ror detection task, kappa was used to compare
Turker responses to those of three trained anno-
tators. Among the trained annotators, inter-kappa
agreement ranged from 0.574 to 0.650, for a mean
kappa of 0.606. In Figure 2, kappa is shown for the
comparisons of Turker responses to each annotator
for samples of various sizes ranging from N = 1 to
N = 18. At sample size N = 13, the average kappa is
0.608, virtually identical to the mean found among
the trained annotators.
0.40
0.45
0.50
0.55
0.60
0.65
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18
Kap
pa
Number of Turkers
Rater 1 vs. AMTRater 2 vs. AMTRater 3 vs. AMTMean
Figure 2: Error Detection Task: Reliability of AMT as a
function of number of judgments
5 Rethinking Evaluation
We contend that the Amazon Mechanical Turk can
not only be used as an effective alternative annota-
tion source, but can also be used to revamp evalu-
ation since multiple judgments are now easily ac-
quired. Instead of treating the task of error detection
as a ?black or white? distinction, where a preposi-
tion is either correct or incorrect, cases of prepo-
sition use can now be grouped into bins based on
the level of agreement of the Turkers. For example,
if 90% or more judge a preposition to be an error,
47
Task # of HITs Judgments/HIT Total Judgments Cost Total Cost # of Turkers Total Time
Selection 194 10 1,940 $0.02 $48.50 49 0.5 hours
Error Detection 152 20 3,040 $0.02 $76.00 74 6 hours
Table 1: AMT Experiment Statistics
the high agreement is strong evidence that this is a
clear case of an error. Conversely, agreement lev-
els around 50% would indicate that the use of a par-
ticular preposition is highly contentious, and, most
likely, it should not be flagged by an automated er-
ror detection system.
The current standard method treats all cases of
preposition usage equally, however, some are clearly
harder to annotate than others. By breaking an eval-
uation set into agreement bins, it should be possible
to separate the ?easy? cases from the ?hard? cases
and report precision and recall results for the differ-
ent levels of human agreement represented by differ-
ent bins. This method not only gives a clearer pic-
ture of how a system is faring, but it also ameliorates
the problem of cross-system evaluation when two
systems are evaluated on different corpora. If each
evaluation corpus is annotated by the same number
of Turkers and with the same annotation scheme, it
will now be possible to compare systems by sim-
ply comparing their performance on each respective
bin. The assumption here is that prepositions which
show X% agreement in corpus A are of equivalent
difficulty to those that show X% agreement in cor-
pus B.
6 Discussion
In this paper, we showed that the AMT is an ef-
fective tool for annotating grammatical errors. At
a fraction of the time and cost, it is possible to
acquire high quality judgments from multiple un-
trained raters without sacrificing reliability. A sum-
mary of the cost and time of the two experiments
described here can be seen in Table 1. In the task of
preposition selection, only three Turkers are needed
to match the reliability of two trained raters; in the
more complicated task of error detection, up to 13
Turkers are needed. However, it should be noted
that these numbers can be viewed as upper bounds.
The error annotation scheme that was used is a very
simple one. We intend to experiment with different
guidelines and instructions, and to screen (Callison-
Burch, 2009) and weight Turkers? responses (Snow
et al, 2008), in order to lower the number of Turk-
ers required for this task. Finally, we will look at
other errors, such as articles, to determine howmany
Turkers are necessary for optimal annotation.
Acknowledgments
We thank Sarah Ohls and Waverely VanWinkle for
their annotation work, and Jennifer Foster and the
two reviewers for their comments and feedback.
References
Chris Callison-Burch. 2009. Fast, cheap, and creative:
Evaluating translation quality using Amazon?s Me-
chanical Turk. In EMNLP.
Rachele De Felice and Stephen G. Pullman. 2009. Auto-
matic detection of preposition errors in learner writing.
CALICO Journal, 26(3).
Michael Gamon, Jianfeng Gao, Chris Brockett, Alex
Klementiev, William B. Dolan, Dmitriy Belenko, and
Lucy Vanderwende. 2008. Using contextual speller
techniques and language modeling for esl error cor-
rection. In Proceedings of IJCNLP, Hyderabad, India,
January.
Na-Rae Han, Martin Chodorow, and Claudia Leacock.
2006. Detecting errors in English article usage by
non-native speakers. Natural Language Engineering,
12:115?129.
Victor Sheng, Foster Provost, and Panagiotis Ipeirotis.
2008. Get another label? Improving data quality and
data mining using multiple, noisy labelers. In Pro-
ceeding of ACM SIGKDD, Las Vegas, Nevada, USA.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-
drew Ng. 2008. Cheap and fast ? but is it good?
evaluating non-expert annotations for natural language
tasks. In EMNLP.
Joel R. Tetreault and Martin Chodorow. 2008a. The ups
and downs of preposition error detection in ESL writ-
ing. In COLING.
Joel Tetreault and Martin Chodorow. 2008b. Native
Judgments of non-native usage: Experiments in prepo-
sition error detection. In COLING Workshop on Hu-
man Judgments in Computational Linguistics.
48
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 108?115,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
E-rating Machine Translation
Kristen Parton1 Joel Tetreault2 Nitin Madnani2 Martin Chodorow3
1Columbia University, NY, USA
kristen@cs.columbia.edu
2Educational Testing Service, Princeton, NJ, USA
{jtetreault, nmadnani}@ets.org
3Hunter College of CUNY, New York, NY, USA
martin.chodorow@hunter.cuny.edu
Abstract
We describe our submissions to the WMT11
shared MT evaluation task: MTeRater and
MTeRater-Plus. Both are machine-learned
metrics that use features from e-rater R?, an au-
tomated essay scoring engine designed to as-
sess writing proficiency. Despite using only
features from e-rater and without comparing
to translations, MTeRater achieves a sentence-
level correlation with human rankings equiva-
lent to BLEU. Since MTeRater only assesses
fluency, we build a meta-metric, MTeRater-
Plus, that incorporates adequacy by combin-
ing MTeRater with other MT evaluation met-
rics and heuristics. This meta-metric has a
higher correlation with human rankings than
either MTeRater or individual MT metrics
alone. However, we also find that e-rater fea-
tures may not have significant impact on cor-
relation in every case.
1 Introduction
The evaluation of machine translation (MT) systems
has received significant interest over the last decade
primarily because of the concurrent rising interest in
statistical machine translation. The majority of re-
search on evaluating translation quality has focused
on metrics that compare translation hypotheses to a
set of human-authored reference translations. How-
ever, there has also been some work on methods that
are not dependent on human-authored translations.
One subset of such methods is task-based in that
the methods determine the quality of a translation in
terms of how well it serves the need of an extrin-
sic task. These tasks can either be downstream NLP
tasks such as information extraction (Parton et al,
2009) and information retrieval (Fujii et al, 2009) or
human tasks such as answering questions on a read-
ing comprehension test (Jones et al, 2007).
Besides extrinsic evaluation, there is another set
of methods that attempt to ?learn? what makes a
good translation and then predict the quality of new
translations without comparing to reference trans-
lations. Corston-Oliver et al (2001) proposed the
idea of building a decision tree classifier to sim-
ply distinguish between machine and human transla-
tions using language model (LM) and syntactic fea-
tures. Kulesza and Shieber (2004) attempt the same
task using an support vector machine (SVM) classi-
fier and features derived from reference-based MT
metrics such as WER, PER, BLEU and NIST. They
also claim that the confidence score for the classi-
fier being used, if available, may be taken as an es-
timate of translation quality. Quirk (2004) took a
different approach and examined whether it is pos-
sible to explicitly compute a confidence measure for
each translated sentence by using features derived
from both the source and target language sides. Al-
brecht and Hwa (2007a) expanded on this idea and
conducted a larger scale study to show the viabil-
ity of regression as a sentence-level metric of MT
quality. They used features derived from several
other reference-driven MT metrics. In other work
(Albrecht and Hwa, 2007b), they showed that one
could substitute translations from other MT systems
for human-authored reference translations and de-
rive the regression features from them.
Gamon et al (2005) build a classifier to distin-
guish machine-generated translations from human
108
ones using fluency-based features and show that by
combining the scores of this classifier with LM per-
plexities, they obtain an MT metric that has good
correlation with human judgments but not better
than the baseline BLEU metric.
The fundamental questions that inspired our pro-
posed metrics are as follows:
? Can an operational English-proficiency mea-
surement system, built with absolutely no fore-
thought of using it for evaluation of translation
quality, actually be used for this purpose?
? Obviously, such a system can only assess the
fluency of a translation hypothesis and not the
adequacy. Can the features derived from this
system then be combined with metrics such
as BLEU, METEOR or TERp?measures of
adequacy?to yield a metric that performs bet-
ter?
The first metric we propose (MTeRater) is an
SVM ranking model that uses features derived from
the ETS e-rater R? system to assess fluency of trans-
lation hypotheses. Our second metric (MTeRater-
Plus) is a meta-metric that combines MTeRater fea-
tures with metrics such as BLEU, METEOR and
TERp as well as features inspired by other MT met-
rics.
Although our work is intimately related to some
of the work cited above in that it is a trained regres-
sion model predicting translation quality at the sen-
tence level, there are two important differences:
1. We do not use any human translations ? ref-
erence or otherwise ? for MTeRater, not even
when training the metric. The classifier is
trained using human judgments of translation
quality provided as part of the shared evalua-
tion task.
2. Most of the previous approaches use feature
sets that are designed to capture both transla-
tion adequacy and fluency. However, MTeRater
uses only fluency-based features.
The next section provides some background on
the e-rater system. Section 3 presents a discussion
of the differences between MT errors and learner er-
rors. Section 4 describes how we use e-rater to build
our metrics. Section 5 outlines our experiments and
Section 5 discusses the results of these experiments.
Finally, we conclude in Section 6.
2 E-rater
E-rater is a proprietary automated essay scoring
system developed by Educational Testing Service
(ETS) to assess writing quality.1 The system has
been used operationally for over 10 years in high-
stakes exams such as the GRE and TOEFL given
its speed, reliability and high agreement with human
raters.
E-rater combines 8 main features using linear re-
gression to produce a numerical score for an es-
say. These features are grammar, usage, mechan-
ics, style, organization, development, lexical com-
plexity and vocabulary usage. The grammar feature
covers errors such as sentence fragments, verb form
errors and pronoun errors (Chodorow and Leacock,
2000). The usage feature detects errors related to
articles (Han et al, 2006), prepositions (Tetreault
and Chodorow, 2008) and collocations (Futagi et al,
2008). The mechanics feature checks for spelling,
punctuation and capitalization errors. The style fea-
ture checks for passive constructions and word rep-
etition, among others. Organization and develop-
ment tabulate the presence or absence of discourse
elements and the length of each element. Finally,
the lexical complexity feature details how complex
the writer?s words are based on frequency indices
and writing scales, and the vocabulary feature eval-
uates how appropriate the words are for the given
topic). Since many of the features are essay-specific,
there is certainly some mismatch between what e-
rater was intended for and the genres we are using it
for in this experiment (translated news articles).
In our work, we separate e-rater features into two
classes: sentence level and document level. The
sentence level features consist of all errors marked
by the various features for each sentence alone. In
contrast, the document level features are an aggre-
gation of the sentence level features for the entire
document.
1A detailed description of e-rater is outside the scope of this
paper and the reader is referred to (Attali and Burstein, 2006).
109
3 Learner Errors vs. MT Errors
Since e-rater is trained on human-written text and
designed to look for errors in usage that are com-
mon to humans, one research question is whether it
is even useful for assessing the fluency of machine
translated text. E-rater is unaware of the transla-
tion context, so it does not look for common MT
errors, such as untranslated words, mistranslations
and deleted content words. However, these may get
flagged as other types of learner errors: spelling mis-
takes, confused words, and sentence fragments.
Machine translations do contain learner-like mis-
takes in verb conjugations and word order. In an
error analysis of SMT output, Vilar et al (2006) re-
port that 9.9% - 11.7% of errors made by a Spanish-
English SMT system were incorrect word forms, in-
cluding incorrect tense, person or number. These
error types are also account for roughly 14% of er-
rors made by ESL (English as a Second Language)
writers in the Cambridge Learner Corpus (Leacock
et al, 2010).
On the other hand, some learner mistakes are un-
likely to be made by MT systems. The Spanish-
English SMT system made almost no mistakes in
idioms (Vilar et al, 2006). Idiomatic expressions
are strongly preferred by language models, but may
be difficult for learners to memorize (?kicked a
bucket?). Preposition usage is a common problem
in non-native English text, accounting for 29% of
errors made by intermediate to advanced ESL stu-
dents (Bitchener et al, 2005) but language models
are less likely to prefer local preposition errors e.g.,
?he went to outside?. On the other hand, a language
model will likely not prevent errors in prepositions
(or in other error types) that rely on long-distance
dependencies.
4 E-rating Machine Translation
The MTeRater metric uses only features from e-rater
to score translations. The features are produced di-
rectly from the MT output, with no comparison to
reference translations, unlike most MT evaluation
metrics (such as BLEU, TERp and METEOR).
An obvious deficit of MTeRater is a measure of
adequacy, or how much meaning in the source sen-
tence is expressed in the translation. E-rater was
not developed for assessing translations, and the
MTeRater metric never compares the translation to
the source sentence. To remedy this, we propose
the MTeRater-Plus meta-metric that uses e-rater fea-
tures plus all of the hybrid features described below.
Both metrics were trained on the same data using
the same machine learning model, and differ only in
their feature sets.
4.1 E-rater Features
Each sentence is associated with an e-rater sentence-
level vector and a document-level vector as previ-
ously described and each column in these vectors
was used a feature.
4.2 Features for Hybrid Models
We used existing automatic MT metrics as baselines
in our evaluation, and also as features in our hybrid
metric. The metrics we used were:
1. BLEU (Papineni et al, 2002): Case-insensitive
and case-sensitive BLEU scores were pro-
duced using mteval-v13a.pl, which calculates
smoothed sentence-level scores.
2. TERp (Snover et al, 2009): Translation Edit
Rate plus (TERp) scores were produced using
terp v1. The scores were case-insensitive and
edit costs from Snover et al (2009) were used
to produce scores tuned for fluency and ade-
quacy.
3. METEOR (Lavie and Denkowski, 2009): Me-
teor scores were produced using Meteor-next
v1.2. All types of matches were allowed (ex-
act, stem, synonym and paraphrase) and scores
tuned specifically to rank, HTER and adequacy
were produced using the ?-t? flag in the tool.
We also implemented features closely related to
or inspired by other MT metrics. The set of these
auxiliary features is referred to as ?Aux?.
1. Character-level statistics: Based on the suc-
cess of the i-letter-BLEU and i-letter-recall
metrics from WMT10 (Callison-Burch et al,
2010), we added the harmonic mean of preci-
sion (or recall) for character n-grams (from 1
to 10) as features.
110
2. Raw n-gram matches: We calculated the pre-
cision and precision for word n-grams (up to
n=6) and added each as a separate feature (for
a total of 12). Although these statistics are also
calculated as part of the MT metrics above,
breaking them into separate features gives the
model more information.
3. Length ratios: The ratio between the lengths
of the MT output and the reference translation
was calculated on a character level and a word
level. These ratios were also calculated be-
tween the MT output and the source sentence.
4. OOV heuristic: The percentage of tokens in
the MT that match the source sentence. This
is a low-precision heuristic for counting out of
vocabulary (OOV) words, since it also counts
named entities and words that happen to be the
same in different languages.
4.3 Ranking Model
Following (Duh, 2008), we represent sentence-level
MT evaluation as a ranking problem. For a partic-
ular source sentence, there are N machine transla-
tions and one reference translation. A feature vector
is extracted from each {source, reference, MT} tu-
ple. The training data consists of sets of translations
that have been annotated with relative ranks. Dur-
ing training, all ranked sets are converted to sets of
feature vectors, where the label for each feature vec-
tor is the rank. The ranking model is a linear SVM
that predicts a relative score for each feature vector,
and is implemented by SVM-rank (Joachims, 2006).
When the trained classifier is applied to a set of N
translations for a new source sentence, the transla-
tions can then be ranked by sorting the SVM scores.
5 Experiments
All experiments were run using data from three
years of previous WMT shared tasks (WMT08,
WMT09 and WMT10). In these evaluations, anno-
tators were asked to rank 3-5 translation hypothe-
ses (with ties allowed), given a source sentence and
a reference translation, although they were only re-
quired to be fluent in the target language.
Since e-rater was developed to rate English sen-
tences only, we only evaluated tasks with English
as the target language. All years included source
languages French, Spanish, German and Czech.
WMT08 and WMT09 also included Hungarian and
multisource English. The number of MT systems
was different for each language pair and year, from
as few as 2 systems (WMT08 Hungarian-English) to
as many as 25 systems (WMT10 German-English).
All years had a newswire testset, which was divided
into stories. WMT08 had testsets in two additional
genres, which were not split into documents.
All translations were pre-processed and run
through e-rater. Each document was treated as an es-
say, although news articles are generally longer than
essays. Testsets that were not already divided into
documents were split into pseudo-documents of 20
contiguous sentences or less. Missing end of sen-
tence markers were added so that e-rater would not
merge neighboring sentences.
6 Results
For assessing our metrics prior to WMT11, we
trained on WMT08 and WMT09 and tested on
WMT10. The metrics we submitted to WMT11
were trained on all three years. One criticism of
machine-learned evaluation metrics is that they may
be too closely tuned to a few MT systems, and thus
not generalize well as MT systems evolve or when
judging new sets of systems. In this experiment,
WMT08 has 59 MT systems, WMT09 has 70 dif-
ferent MT systems, and WMT10 has 75 different
systems. Different systems participate each year,
and those that participate for multiple years often
improve from year to year. By training and test-
ing across years rather than within years, we hope
to avoid overfitting.
To evaluate, we measure correlation between each
metric and the human annotated rankings according
to (Callison-Burch et al, 2010): Kendall?s tau is cal-
culated for each language pair and the results are
averaged across language pairs. This is preferable
to averaging across all judgments because the num-
ber of systems and the number of judgments vary
based on the language pair (e.g., there were 7,911
ranked pairs for 14 Spanish-English systems, and
3,575 ranked pairs for 12 Czech-English systems).
It is difficult to calculate the statistical signifi-
cance of Kendall?s tau on these data. Unlike the
111
Source language cz de es fr avg
Individual Metrics & Baselines
MTeRater .32 .31 .19 .23 .26
bleu-case .26 .27 .28 .22 .26
meteor-rank .33 .36 .33 .27 .32
TERp-fluency .30 .36 .28 .28 .30
Meta-Metric & Baseline
BMT+Aux+MTeRater .38 .42 .37 .38 .39
BMT .35 .40 .35 .34 .36
Additional Meta-Metrics
BMT+LM .36 .41 .36 .36 .37
BMT+MTeRater .38 .42 .36 .38 .38
BMT+Aux .38 .41 .38 .37 .39
BMT+Aux+LM .39 .42 .38 .36 .39
Table 1: Kendall?s tau correlation with human rankings.
BMT includes bleu, meteor and TERp; Aux includes aux-
iliary features. BMT+Aux+MTeRater is MTeRater-Plus.
Metrics MATR annotations (Przybocki et al, 2009),
(Peterson and Przybocki, 2010), the WMT judg-
ments do not give a full ranking over all systems for
all judged sentences. Furthermore, the 95% confi-
dence intervals of Kendall?s tau are known to be very
large (Carterette, 2009) ? in Metrics MATR 2010,
the top 7 metrics in the paired-preference single-
reference into-English track were within the same
confidence interval.
To compare metrics, we use McNemar?s test
of paired proportions (Siegel and Castellan, 1988)
which is more powerful than tests of independent
proportions, such as the chi-square test for indepen-
dent samples.2 As in Kendall?s tau, each metric?s
relative ranking of a translation pair is compared to
that of a human. Two metrics, A and B, are com-
pared by counting the number of times both A and B
agree with the human ranking, the number of times
A disagrees but B agrees, the number of times A
agrees but B disagrees, and the number of times both
A and B disagree. These counts can be arranged in
a 2 x 2 contingency table as shown below.
A agrees A disagrees
B agrees a b
B disagrees c d
McNemar?s test determines if the cases of mis-
match in agreement between the metrics (cells b and
c) are symmetric or if there is a significant difference
2See http://faculty.vassar.edu/lowry/propcorr.html for an ex-
cellent description.
in favor of one of the metrics showing more agree-
ment with the human than the other. The two-tailed
probability for McNemar?s test can be calculated us-
ing the binomial distribution over cells b and c.
6.1 Reference-Free Evaluation with MTeRater
The first group of rows in Table 1 shows the
Kendall?s tau correlation with human rankings of
MTeRater and the best-performing version of the
three standard MT metrics. Even though MTeR-
ater is blind to the MT context and does not use the
source or references at all, MTeRater?s correlation
with human judgments is the same as case-sensitive
bleu (bleu-case). This indicates that a metric trained
to assess English proficiency in non-native speakers
is applicable to machine translated text.
6.2 Meta-Metrics
The second group in Table 1 shows the cor-
relations of our second metric, MTeRater-Plus
(BMT+Aux+MTeRater), and a baseline meta-metric
(BMT) that combined BLEU, METEOR and TERp.
MTeRater-Plus performs significantly better than
BMT, according to McNemar?s test.
We also wanted to determine whether the e-
rater features have any significant impact when used
as part of meta-metrics. To this end, we first
created two variants of MTeRater-Plus: one that
removed the MTeRater features (BMT+Aux) and
another that replaced the MTeRater features with
the LM likelihood and perplexity of the sentence
(BMT+Aux+LM).3 Both models perform as well
as MTeRater-Plus, i.e., adding additional fluency
features (either LM scores or MTeRater) to the
BMT+Aux meta-metric has no significant impact.
To determine whether this was generally the case,
we also created two variants of the BMT baseline
meta-metric that added fluency features to it: one in
the form of LM scores (BMT+LM) and another in
the form of the MTeRater score (BMT+MTeRater).
Based on McNemar?s test, both models are sig-
nificantly better than BMT, indicating that these
reference-free fluency features indeed capture an as-
pect of translation quality that is absent from the
standard MT metrics. However, there is no signfi-
cant difference between the two variants of BMT.
3The LM was trained on English Gigaword 3.0, and was
provided by WMT10 organizers.
112
1) Ref: Gordon Brown has discovered yet another hole to fall into; his way out of it remains the same
MT+: Gordon Brown discovered a new hole in which to sink; even if it resigned, the position would not change.
Errors: None marked
MT-: Gordon Brown has discovered a new hole in which could, Even if it demissionnait, the situation does not change not.
Errors: Double negative, spelling, preposition
2) Ref: Jancura announced this in the Twenty Minutes programme on Radiozurnal.
MT+: Jancura said in twenty minutes Radiozurnal. Errors: Spelling
MT-: He said that in twenty minutes. Errors: none marked
Table 2: Translation pairs ranked correctly by MTeRater but not bleu-case (1) and vice versa (2).
6.3 Discussion
Table 2 shows two pairs of ranked translations (MT+
is better than MT-), along with some of the errors de-
tected by e-rater. In pair 1, the lower-ranked trans-
lation has major problems in fluency as detected by
e-rater, but due to n-gram overlap with the reference,
bleu-case ranks it higher. In pair 2, MT- is more
fluent but missing two named entities and bleu-case
correctly ranks it lower.
One disadvantage of machine-learned metrics is
that it is not always clear which features caused one
translation to be ranked higher than another. We
did a feature ablation study for MTeRater which
showed that document-level collocation features sig-
nificantly improve the metric, as do features for
sentence-level preposition errors. Discourse-level
features were harmful to MT evaluation. This is un-
surprising, since MT sentences are judged one at a
time, so any discourse context is lost.
Overall, a metric with only document-level fea-
tures does better than one with only sentence-level
features due to data sparsity ? many sentences have
no errors, and we conjecture that the document-level
features are a proxy for the quality of the MT sys-
tem. Combining both document-level and sentence-
level e-rater features does significantly better than
either alone. Incorporating document-level features
into sentence-level evaluation had one unforeseen
effect: two identical translations can get different
scores depending on how the rest of the document
is translated. While using features that indicate the
relative quality of MT systems can improve overall
correlation, it fails when the sentence-level signal is
not strong enough to overcome the prior belief.
7 Conclusion
We described our submissions to the WMT11 shared
evaluation task: MTeRater and MTeRater-Plus.
MTeRater is a fluency-based metric that uses fea-
tures from ETS?s operational English-proficiency
measurement system (e-rater) to predict the qual-
ity of any translated sentence. MTeRater-Plus is a
meta-metric that combines MTeRater?s fluency-only
features with standard MT evaluation metrics and
heuristics. Both metrics are machine-learned mod-
els trained to rank new translations based on existing
human judgments of translation.
Our experiments showed that MTeRater, by it-
self, achieves a sentence-level correlation as high as
BLEU, despite not using reference translations. In
addition, the meta-metric MTeRater-Plus achieves
higher correlations than MTeRater, BLEU, ME-
TEOR, TERp as well as a baseline meta-metric com-
bining BLEU, METEOR and TERp (BMT). How-
ever, further analysis showed that the MTeRater
component of MTeRater-Plus does not contribute
significantly to this improved correlation. How-
ever, when added to the BMT baseline meta-metric,
MTeRater does make a significant contribution.
Our results, despite being a mixed bag, clearly
show that a system trained to assess English-
language proficiency can be useful in providing an
indication of translation fluency even outside of the
specific WMT11 evaluation task. We hope that this
work will spur further cross-pollination between the
fields of MT evaluation and grammatical error de-
tection. For example, we would like to explore using
MTeRater for confidence estimation in cases where
reference translations are unavailable, such as task-
oriented MT.
Acknowledgments
The authors wish to thank Slava Andreyev at ETS
for his help in running e-rater. This research was
supported by an NSF Graduate Research Fellowship
for the first author.
113
References
Joshua Albrecht and Rebecca Hwa. 2007a. A Re-
examination of Machine Learning Approaches for
Sentence-Level MT Evaluation. In Proceedings of
ACL.
Joshua Albrecht and Rebecca Hwa. 2007b. Regression
for Sentence-Level MT Evaluation with Pseudo Refer-
ences. In Proceedings of ACL.
Yigal Attali and Jill Burstein. 2006. Automated es-
say scoring with e-rater v.2.0. Journal of Technology,
Learning, and Assessment, 4(3).
John Bitchener, Stuart Young, and Denise Cameron.
2005. The effect of different types of corrective feed-
back on esl student writing. Journal of Second Lan-
guage Writing.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, Mark Przybocki, and Omar F. Zaidan.
2010. Findings of the 2010 joint workshop on sta-
tistical machine translation and metrics for machine
translation. In Proceedings of the Joint Fifth Workshop
on Statistical Machine Translation and MetricsMATR,
WMT ?10, pages 17?53, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Ben Carterette. 2009. On rank correlation and the
distance between rankings. In Proceedings of the
32nd international ACM SIGIR conference on Re-
search and development in information retrieval, SI-
GIR ?09, pages 436?443, New York, NY, USA. ACM.
Martin Chodorow and Claudia Leacock. 2000. An unsu-
pervised method for detecting grammatical errors. In
Proceedings of the Conference of the North American
Chapter of the Association of Computational Linguis-
tics (NAACL), pages 140?147.
Simon Corston-Oliver, Michael Gamon, and Chris
Brockett. 2001. A Machine Learning Approach to
the Automatic Evaluation of Machine Translation. In
Proceedings of the 39th Annual Meeting on Associa-
tion for Computational Linguistics, pages 148?155.
Kevin Duh. 2008. Ranking vs. regression in machine
translation evaluation. In Proceedings of the Third
Workshop on Statistical Machine Translation, StatMT
?08, pages 191?194, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Atsushi Fujii, Masao Utiyama, Mikio Yamamoto, and
Takehito Utsuro. 2009. Evaluating Effects of Ma-
chine Translation Accuracy on Cross-lingual Patent
Retrieval. In Proceedings of SIGIR, pages 674?675.
Yoko Futagi, Paul Deane, Martin Chodorow, and Joel
Tetreault. 2008. A computational approach to de-
tecting collocation errors in the writing of non-native
speakers of English. Computer Assisted Language
Learning, 21:353?367.
Michael Gamon, Anthony Aue, and Martine Smets.
2005. Sentence-level MT Evaluation Without Refer-
ence Translations: Beyond Language Modeling. In
Proceedings of the European Association for Machine
Translation (EAMT).
Na-Rae Han, Martin Chodorow, and Claudia Leacock.
2006. Detecting errors in English article usage by
non-native speakers. Natural Language Engineering,
12(2):115?129.
Thorsten Joachims. 2006. Training linear SVMs in linear
time. In ACM SIGKDD International Conference On
Knowledge Discovery and Data Mining (KDD), pages
217?226.
Douglas Jones, Martha Herzog, Hussny Ibrahim, Arvind
Jairam, Wade Shen, Edward Gibson, and Michael
Emonts. 2007. ILR-Based MT Comprehension Test
with Multi-Level Questions. In HLT-NAACL (Short
Papers), pages 77?80.
Alex Kulesza and Stuart M. Shieber. 2004. A Learn-
ing Approach to Improving Sentence-level MT Evalu-
ation. In Proceedings of the 10th International Con-
ference on Theoretical and Methodological Issues in
Machine Translation (TMI).
Alon Lavie and Michael J. Denkowski. 2009. The me-
teor metric for automatic evaluation of machine trans-
lation. Machine Translation, 23:105?115, September.
Claudia Leacock, Martin Chodorow, Michael Gamon,
and Joel Tetreault. 2010. Automated Grammatical
Error Detection for Language Learners. Morgan &
Claypool Publishers.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting on Association for Computa-
tional Linguistics, ACL ?02, pages 311?318, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Kristen Parton, Kathleen R. McKeown, Bob Coyne,
Mona T. Diab, Ralph Grishman, Dilek Hakkani-Tu?r,
Mary Harper, Heng Ji, Wei Yun Ma, Adam Meyers,
Sara Stolbach, Ang Sun, Gokhan Tur, Wei Xu, and
Sibel Yaman. 2009. Who, What, When, Where, Why?
Comparing Multiple Approaches to the Cross-Lingual
5W Task. In Proceedings of the Joint Conference of
the 47th Annual Meeting of the ACL and the 4th Inter-
national Joint Conference on Natural Language Pro-
cessing of the AFNLP, pages 423?431.
Kay Peterson and Mark Przybocki. 2010. Nist
2010 metrics for machine translation evalua-
tion (metricsmatr10) official release of results.
http://www.itl.nist.gov/iad/mig/
tests/metricsmatr/2010/results.
114
Mark Przybocki, Kay Peterson, Se?bastien Bronsart, and
Gregory Sanders. 2009. The nist 2008 metrics for ma-
chine translation challenge?overview, methodology,
metrics, and results. Machine Translation, 23:71?103,
September.
Christopher Quirk. 2004. Training a Sentence-level Ma-
chine Translation Confidence Measure. In Proceed-
ings of LREC.
Sidney Siegel and N. John Castellan. 1988. Nonpara-
metric statistics for the behavioral sciences. McGraw-
Hill, 2 edition.
Matthew Snover, Nitin Madnani, Bonnie J. Dorr, and
Richard Schwartz. 2009. Fluency, adequacy, or
hter?: exploring different human judgments with a tun-
able mt metric. In Proceedings of the Fourth Work-
shop on Statistical Machine Translation, StatMT ?09,
pages 259?268, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Joel Tetreault and Martin Chodorow. 2008. The ups and
downs of preposition error detection in ESL writing.
In Proceedings of the 22nd International Conference
on Computational Linguistics (COLING), pages 865?
872.
David Vilar, Jia Xu, Luis Fernando D?Haro, and Her-
mann Ney. 2006. Error analysis of machine transla-
tion output. In International Conference on Language
Resources and Evaluation, pages 697?702, Genoa,
Italy, May.
115
The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 44?53,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Exploring Grammatical Error Correction with
Not-So-Crummy Machine Translation?
Nitin Madnani Joel Tetreault
Educational Testing Service
Princeton, NJ, USA
{nmadnani,jtetreault}@ets.org
Martin Chodorow
Hunter College of CUNY
New York, NY, USA
martin.chodorow@hunter.cuny.edu
Abstract
To date, most work in grammatical error cor-
rection has focused on targeting specific er-
ror types. We present a probe study into
whether we can use round-trip translations ob-
tained from Google Translate via 8 different
pivot languages for whole-sentence grammat-
ical error correction. We develop a novel
alignment algorithm for combining multiple
round-trip translations into a lattice using the
TERp machine translation metric. We further
implement six different methods for extract-
ing whole-sentence corrections from the lat-
tice. Our preliminary experiments yield fairly
satisfactory results but leave significant room
for improvement. Most importantly, though,
they make it clear the methods we propose
have strong potential and require further study.
1 Introduction
Given the large and growing number of non-native
English speakers around the world, detecting and
correcting grammatical errors in learner text cur-
rently ranks as one of the most popular educational
NLP applications. Previously published work has
explored the effectiveness of using round-trip ma-
chine translation (translating an English sentence
to some foreign language F, called the pivot, and
then translating the F language sentence back to En-
glish) for correcting preposition errors (Hermet and
De?silets, 2009). In this paper, we present a pilot
study that explores the effectiveness of extending
?cf. Good Applications for Crummy Machine Translation.
Ken Church & Ed Hovy. Machine Translation, 8(4). 1993
this approach to whole-sentence grammatical error
correction.
Specifically, we explore whether using the con-
cept of round-trip machine translation via multi-
ple?rather than single?pivot languages has the po-
tential of correcting most, if not all, grammatical
errors present in a sentence. To do so, we de-
velop a round-trip translation framework using the
Google Translate API. Furthermore, we propose a
novel combination algorithm that can combine the
evidence present in multiple round-trip translations
and increase the likelihood of producing a whole-
sentence correction. Details of our methodology are
presented in ?3 and of the dataset we use in ?4. Since
this work is of an exploratory nature, we conduct a
detailed error analysis and present the results in ?5.
Finally, ?6 summarizes the contributions of this pi-
lot study and provides a discussion of possible future
work.
2 Related Work
To date, most work in grammatical error detection
has focused on targeting specific error types (usu-
ally prepositions or article errors) by using rule-
based methods or statistical machine-learning clas-
sification algorithms, or a combination of the two.
Leacock et al (2010) present a survey of the com-
mon approaches. However, targeted errors such as
preposition and determiner errors are just two of the
many types of grammatical errors present in non-
native writing. One of the anonymous reviewers for
this paper makes the point eloquently: ?Given the
frequent complexity of learner errors, less holistic,
error-type specific approaches are often unable to
44
disentangle compounded errors of style and gram-
mar.? Below we discuss related work that uses ma-
chine translation to address targeted errors and some
recent work that also focused on whole-sentence er-
ror correction.
Brockett et al (2006) use information about mass
noun errors from a Chinese learner corpus to engi-
neer a ?parallel? corpus with sentences containing
mass noun errors on one side and their corrected
counterparts on the other. With this parallel corpus,
the authors use standard statistical machine transla-
tion (SMT) framework to learn a translation (correc-
tion) model which can then be applied to unseen sen-
tences containing mass noun errors. This approach
was able to correct almost 62% of the errors found
in a test set of 150 errors. In our approach, we do not
treat correction directly as a translation problem but
instead rely on an MT system to round-trip translate
an English sentence back to English.
Park and Levy (2011) use a noisy channel model
to achieve whole-sentence grammar correction; they
learn a noise model from a dataset of errorful sen-
tences but do not rely on SMT. They show that the
corrections produced by their model generally have
higher n-gram overlap with human-authored refer-
ence corrections than the original errorful sentences.
The previous work that is most directly rele-
vant to our approach is that of Hermet and De?silets
(2009) who focused only on sentences containing
pre-marked preposition errors and generated a sin-
gle round-trip translation for such sentences via a
single pivot language (French). They then simply
posited this round-trip translation as the ?correc-
tion? for the original sentence. In their evaluation
on sentences containing 133 unique preposition er-
rors, their round-trip translation system was able to
correct 66.4% of the cases. However, this was out-
performed by a simple method based on web counts
(68.7%). They also found that combining the round-
trip method with the web counts method into a hy-
brid system yielded higher performance (82.1%).
In contrast, we use multiple pivot languages to
generate several round-trip translations. In addition,
we use a novel alignment algorithm that allows us to
combine different parts of different round-trip trans-
lations and explore a whole new set of corrections
that go beyond the translations themselves. Finally,
we do not restrict our analysis to any single type of
error. In fact, our test sentences contain several dif-
ferent types of grammatical errors.
Outside of the literature on grammatical error de-
tection, our combination approach is directly related
to the research on machine translation system com-
bination wherein translation hypotheses produced
by different SMT systems are combined to allow the
extraction of a better, combined hypothesis (Ban-
galore et al, 2001; Rosti et al, 2007; Feng et al,
2009). However, our combination approach is dif-
ferent in that all the round-trip translations are pro-
duced by a single system but via different pivot lan-
guages.
Finally, the idea of combining multiple surface
renderings with the same meaning has also been ex-
plored in paraphrase generation. Pang et al (2003)
propose an algorithm to align sets of parallel sen-
tences driven entirely by the syntactic representa-
tions of the sentences. The alignment algorithm out-
puts a merged lattice from which lexical, phrasal,
and sentential paraphrases could simply be read off.
Barzilay and Lee (2003) cluster topically related
sentences into slotted word lattices by using mul-
tiple sequence alignment for the purpose of down-
stream paraphrase generation from comparable cor-
pora. More recently, Zhao et al (2010) perform
round-trip translation of English sentences via dif-
ferent pivot languages and different off-the-shelf
SMT systems to generate candidate paraphrases.
However, they do not combine the candidate para-
phrases in any way. A detailed survey of paraphrase
generation techniques can be found in (Androut-
sopoulos and Malakasiotis, 2010) and (Madnani and
Dorr, 2010).
3 Methodology
The basic idea underlying our error correction tech-
nique is quite simple: if we can automatically gen-
erate alternative surface renderings of the meaning
expressed in the original sentence and then pick the
one that is most fluent, we are likely to have picked
a version of the sentence in which the original gram-
matical errors have been fixed.
In this paper, we propose generating such alter-
native formulations using statistical machine trans-
lation. For example, we take the original sentence E
and translate it to Chinese using the Google Trans-
45
Original Both experience and books are very important about living.
Swedish Both experience and books are very important in live.
Italian Both books are very important experience and life.
Russian And the experience, and a very important book about life.
French Both experience and the books are very important in life.
German Both experience and books are very important about life.
Chinese Related to the life experiences and the books are very important.
Spanish Both experience and the books are very important about life.
Arabic Both experience and books are very important for life.
Figure 1: Illustrating the deficiency in using an n-gram language model to select one of the 8 round-trip translations
as the correction for the Original sentence. The grammatical errors in the Original sentence are shown in italics. The
round-trip translation via Russian is chosen by a 5-gram language model trained on the English gigaword corpus even
though it changes the meaning of the original sentence entirely.
late API. We then take the resulting Chinese sen-
tence C and translate it back to English. Since
the translation process is designed to be meaning-
preserving, the resulting round-trip translation E?
can be seen as an alternative formulation of the orig-
inal sentence E. Furthermore, if additional pivot lan-
guages besides Chinese are used, several alterna-
tive formulations of E can be generated. We use 8
different pivot languages: Arabic, Chinese, Span-
ish, French, Italian, German, Swedish, Russian. We
chose these eight languages since they are frequently
used in SMT research and shared translation tasks.
To obtain the eight round-trip translations via each
of these pivot languages, we use the Google Trans-
late research API.1
3.1 Round-Trip Translation Combination
Once the translations are generated, an obvious so-
lution is to pick the most fluent alternative, e.g.,
using an n-gram language model. However, since
the language model has no incentive to preserve the
meaning of the sentence, it is possible that it might
pick a translation that changes the meaning of the
original sentence entirely. For example, consider
the sentence and its round-trip translations shown
in Figure 1. For this sentence, a 5-gram language
model trained on gigaword picks the Russian round-
trip translation simply because it has n-grams that
were seen more frequently in the English gigaword
corpus.
Given the deficiencies in statistical phrase-based
translation, it is also possible that no single round-
1http://research.google.com/university/
translate/
trip translation fixes all of the errors. Again, con-
sider Figure 1. None of the 8 round-trip transla-
tions is error-free itself. Therefore, the task is more
complex than simply selecting the right round-trip
translation. We posit that a better approach will be
to combine the evidence of correction produced by
each independent translation model and increase the
likelihood of producing a final whole-sentence cor-
rection. Additionally, by engineering such a combi-
nation, we increase the likelihood that the final cor-
rection will preserve the meaning of the original sen-
tence.
In order to combine the round-trip translations,
we developed a heuristic alignment algorithm that
uses the TERp machine translation metric (Snover
et al, 2009). The TERp metric takes a pair of sen-
tences and computes the least number of edit opera-
tions that can be employed to turn one sentence into
the other.2 As a by-product of computing the edit
sequence, TERp produces an alignment between the
two sentences where each alignment link is defined
by an edit operation. Figure 2 shows an example of
the alignment produced by TERp between the orig-
inal sentence from Figure 1 and its Russian round-
trip translation. Note that TERp also allows shifting
words and phrases in the second sentence in order
to obtain a smaller edit cost (as indicated by the as-
terisk next to the word book which has shifted from
its original position in the Russian round-trip trans-
lation).
Our algorithm starts by treating the original sen-
tence as the backbone of a lattice. First, it cre-
2Edit operations in TERp include matches, substitutions, in-
sertion, deletions, paraphrase, synonymy and stemming.
46
ates a node for each word in the original sentence
and creates edges between them with a weight of
1. Then, for each of the round-trip translations, it
computes its TERp alignment with the original sen-
tence and aligns it to the backbone based on the edit
operations in the alignment. Specifically, each in-
sertion, substitution, stemming, synonymy and para-
phrase operation lead to creation of new nodes that
essentially provide an alternative formulation for the
aligned substring from the backbone. Any duplicate
nodes are merged. Finally, edges produced by dif-
ferent translations between the same pairs of nodes
are merged and their weights added. Figure 3(a)
shows how our algorithm aligns the Russian round-
trip translation from Figure 1 to the original sentence
using the TERp alignment from Figure 2. Figure
3(b) shows the final lattice produced by our algo-
rithm for the sentence and all the round-trip transla-
tions from Figure 1.
-- and [I]
both -- the [S]
experience -- experience [M]
-- , [I]
and -- and [M]
books -- book [T] [*]
are -- a [S]
very -- very [M]
important -- important [M]
about -- about [M]
living -- life [Y]
. -- . [M]
Figure 2: The alignment produced by TERp between the
original sentence from Figure 1 and its Russian round-
trip translation. The alignment operations are indicated
in square brackets after each alignment link: I=insertion,
M=match, S=substitution, T=stemming and Y=WordNet
synonymy. The asterisk next to the work book denotes
that TERp chose to shift its position before computing an
edit operation for it.
3.2 Correction Generation
For each original sentence, we computed six possi-
ble corrections from the round-trip translations and
the combined lattice:
1. Baseline LM (B). The most fluent round-trip
translation out of the eight as measured by a
5-gram language model trained on the English
gigaword corpus.
2. Greedy (G). A path is extracted from the TERp
lattice using a greedy best-first strategy at each
node, i.e., at each node, the outgoing edge with
the largest weight is followed.
3. 1-Best (1): The shortest path is extracted
from the TERp lattice by using the OpenFST
toolkit.3. This method assumes that, like G, the
combined evidence from the round-trip trans-
lations itself is enough to produce a good final
correction and no external method for measur-
ing fluency is required.4
4. LM Re-ranked (L). An n-best (n=20) list is
extracted from the lattice using the OpenFST
toolkit and re-ranked using the 5-gram giga-
word language model. The 1-best reranked
item is then extracted as the correction. This
method assumes that an external method
of measuring fluency?the 5-gram language
model?can help to bring the most grammati-
cal correction to the top of the n-best list.
5. Product Re-ranked (P). Same as L except the
re-ranking is done based on the product of the
cost of each hypothesis in the n-best list and
the language model score, i.e., both the evi-
dence from the round-trip translations and the
language model is weighted equally.
6. Full LM Composition (C). The edge weights
in the TERp lattice are converted to probabil-
ities. The lattice is then composed with a tri-
gram finite state language model (trained on
a corpus of 100, 000 high-scoring student es-
says).5 The shortest path through the composed
lattice is then extracted as the correction. This
method assumes that using an n-gram language
model during the actual search process is better
than using it as a post-processing tool on an al-
ready extracted n-best list, such as for L and
P.
3http://www.openfst.org/
4Note that the edge weights in the lattice must be converted
into costs for this method (we do so by multiplying the weights
by ?1).
5We adapted the code available at http://www.
ling.ohio-state.edu/?bromberg/ngramcount/
ngramcount.html to perform the LM composition.
47
bo
th
exp
eri
enc
e
1
and
1 ,
1
bo
ok
s
1
bo
ok
1
are
1
ver
y
1
im
po
rta
nt
2
abo
ut
2
liv
ing
1
life
1
.
1
and
the
1
1
1
a
1
1
1
(a
)
bo
th
exp
eri
enc
e
1
and
1,
k
bo
osv
y
bo
os
k
the
m
are
2
uer
l
2
igp
ort
ant
f
abo
.t

in

ie
 or
k
iui
n
k m

k
iue
kk
k1
and
the
k
k
k
a
k
k
m
rea
ted
to
k
the
k
exp
eri
enc
ev
k
k
k
(b
)
O
ri
gi
na
l(
O
)
B
ot
h
ex
pe
ri
en
ce
an
d
bo
ok
s
ar
e
ve
ry
im
po
rt
an
ta
bo
ut
li
vi
ng
.
B
as
el
in
e
L
M
(B
)
A
nd
th
e
ex
pe
ri
en
ce
,a
nd
a
ve
ry
im
po
rt
an
tb
oo
k
ab
ou
tl
if
e.
G
re
ed
y
(G
)
B
ot
h
ex
pe
ri
en
ce
an
d
bo
ok
s
ar
e
ve
ry
im
po
rt
an
ta
bo
ut
li
fe
.
1-
be
st
(1
)
B
ot
h
ex
pe
ri
en
ce
an
d
th
e
bo
ok
s
ar
e
ve
ry
im
po
rt
an
ta
bo
ut
li
fe
.
L
M
R
e-
ra
nk
ed
(L
)
A
nd
th
e
ex
pe
ri
en
ce
an
d
th
e
bo
ok
s
ar
e
ve
ry
im
po
rt
an
ti
n
li
fe
.
P
ro
du
ct
R
e-
ra
nk
ed
(P
)
B
ot
h
ex
pe
ri
en
ce
an
d
bo
ok
s
ar
e
ve
ry
im
po
rt
an
ta
bo
ut
li
fe
.
L
M
C
om
po
si
ti
on
(C
)
B
ot
h
ex
pe
ri
en
ce
an
d
bo
ok
s
ar
e
ve
ry
im
po
rt
an
t
in
lif
e.
(c
)
F
ig
ur
e
3:
(a
)
sh
ow
s
th
e
ou
tp
ut
of
ou
r
al
ig
nm
en
t
al
go
ri
th
m
fo
r
th
e
R
us
si
an
ro
un
d-
tr
ip
tr
an
sl
at
io
n
fr
om
F
ig
ur
e
1.
(b
)
sh
ow
s
th
e
fi
na
l
T
E
R
p
la
tt
ic
e
af
te
r
al
ig
ni
ng
al
l
ei
gh
tr
ou
nd
-t
ri
p
tr
an
sl
at
io
ns
fr
om
F
ig
ur
e
1.
(c
)
sh
ow
s
th
e
co
rr
ec
ti
on
s
fo
r
th
e
or
ig
in
al
se
nt
en
ce
(O
)
pr
od
uc
ed
by
th
e
si
x
te
ch
ni
qu
es
di
sc
us
se
d
in
3.
2.
T
he
co
rr
ec
ti
on
pr
od
uc
ed
by
th
e
F
ul
lL
M
C
om
po
si
ti
on
te
ch
ni
qu
e
(C
)
fi
xe
s
bo
th
th
e
er
ro
rs
in
th
e
or
ig
in
al
se
nt
en
ce
.
48
No. of Errors Sentences Avg. Length
1 61 14.4
2 45 19.9
3 29 24.2
4 14 29.4
> 4 13 38.0
Table 1: The distribution of grammatical errors for the
162 errorful sentences.
Figure 3(c) shows these six corrections as computed
for the sentence from Figure 1.
4 Corpus
To assess our system, we manually selected 200
sentences from a corpus of essays written by non-
native English speakers for a college-level English
proficiency exam. In addition to sentences contain-
ing grammatical errors, we also deliberately sam-
pled sentences that contained no grammatical errors
in order to determine how our techniques perform
in those cases. In total, 162 of the sentences con-
tained at least one error, and the remaining 38 were
perfectly grammatical. For both errorful as well
as grammatical sentences, we sampled sentences of
different lengths (under 10 words, 10-20 words, 20-
30 words, 30-40 words, and over 40 words). The
162 errorful sentences varied in the number and type
of errors present. Table 1 shows the distribution of
the number of errors across these 162 sentences.
Specifically, the error types found in these sen-
tences included prepositions, articles, punctuation,
agreement, collocations, confused words, etc. Some
only contained a handful of straightforward errors,
such as ?In recent day, transportation is one of the
most important thing to support human activity?,
where day and thing should be pluralized. On the
other hand, others were quite garbled to the point
where it was difficult to understand the meaning,
such as ?Sometimes reading a book is give me in-
formation about the knowledge of life so that I can
prevent future happened but who knows that when it
will happen and how fastly can react to that hap-
pen.? During development, we noticed that the
round-trip translation process could not handle mis-
spelled words, so we manually corrected all spelling
mistakes which did not result in a real word.6
6A total of 82 spelling errors were manually corrected.
5 Evaluation
In order to evaluate the six techniques for generating
corrections, we designed an evaluation task where
the annotators would be shown a correction along
with the original sentence for which it was gener-
ated. Since there are 6 corrections for each of the
200 sentences, this yields a total of 1, 200 units for
pairwise preference judgments. We chose two anno-
tators, both native English speakers, each of whom
annotated half of the judgment units.
Given the idiosyncrasies of the statistical machine
translation process underlying our correction tech-
niques, it is quite possible that:
? A correction may fix some, but not all, of the
grammatical errors present in the original sen-
tence, and
? A correction may be more fluent but might
change the meaning of the original sentence.
? A correction may introduce a new disfluency,
even though other errors in the sentence have
been largely corrected. This is especially likely
to be the case for longer sentences.
Therefore, the pairwise preference judgment task
is non-trivial in that it expects the annotators to con-
sider two dimensions: that of grammaticality and of
meaning. To accommodate these considerations, we
designed the evaluation task such that it asked the
annotators to answer the following two questions:
1. Grammaticality. The annotators were asked
to choose between three options: ?Original
sentence sounds better?, ?Correction sounds
better? and ?Both sound about the same?.
2. Meaning. The annotators were asked to choose
between two options: ?Correction preserves
the original meaning? and ?Correction changes
the original meaning?. It should be noted that
determining change in or preservation of mean-
ing was treated as a very strict judgment. Subtle
changes such as the omission of a determiner
were deemed to change the meaning. In some
cases, the original sentences were too garbled
to determine the original meaning itself.
49
C > O C = O C < O
Meaning = 1 S D F
Meaning = 0 F F F
Table 2: A matrix illustrating the Success-Failure-Draw
evaluation criterion for the 162 errorful sentences. The
rows represent the meaning dimension (1 = meaning pre-
served, 0 = meaning changed) and the columns represent
the grammaticality dimension (C > O denotes correc-
tion being more grammatical than the original, C = O
denotes they are about the same and C < O denotes that
the correction is worse). Such a matrix is computed for
each of the six techniques.
5.1 Effectiveness
First, we concentrate our analysis on the original
sentences which contain at least one grammatical er-
ror. We aggregated the results of the pairwise pref-
erence judgments for each of the six specific correc-
tion generation techniques and applied the strictest
evaluation criterion by computing the following, for
each technique:
? Successes. Only those sentences for which
the correction generated by method is not only
more grammatical but also preserves the mean-
ing.
? Failures. All those sentences for which the cor-
rection is either less grammatical or changes
the original meaning.
? Draws. Those sentences for which the correc-
tion preserves the meaning but sounds about
the same as the original.
Table 2 shows a matrix of the six possible com-
binations of grammaticality and meaning for each
method and demonstrates which cells of the matrix
contribute to which of the above three measures:
Successes (S), Failures (F) and Draws (D).
In addition to the six techniques, we also posit an
oracle in order to determine the upper bound on the
performance of our round-trip translation approach.
The oracle picks the most accurate correction gen-
eration method for each individual sentence out of
the 6 that are available. For sentences where none of
the six techniques produce an adequate correction,
the oracle just picks the original sentence. Table 3
shows how the various techniques (including the or-
acle) perform on the 162 errorful sentences as mea-
sured by this criterion. Based on this criterion, the
greedy technique performs the best compared to the
others since it has a higher success rate (36%) and
a lower failure rate (31%). The oracle shows that
60% of the errorful sentences are fixed by at least
one of the six correction generation techniques. We
show examples of success and failure for the greedy
technique in Figure 4.
5.2 Effect of sentence length
From our observations on development data (not
part of the test set), we noticed that Google Trans-
late, like most statistical machine translation sys-
tems, performs significantly better on shorter sen-
tences. Therefore, we wanted to measure whether
the successes for the best method were biased to-
wards shorter sentences and the failures towards
longer ones. To do so, we measured the mean and
standard deviation of lengths of sentences compris-
ing the successes and failures of the greedy tech-
nique. Successful sentences had an average length
of approximately 18 words with a standard devia-
tion of 9.5. Failed sentences had an average length
of 23 words with a standard deviation of 12.31.
These numbers indicate that although the failures
are somewhat correlated with larger sentence length,
there is no evidence of a significant length bias.
5.3 Effect on grammatical sentences
Finally, we also carried out the same Success-
Failure-Draw analysis for the 38 sentences in our
test set that were perfectly grammatical to begin
with. The analysis differs from that of errorful sen-
tences in one key aspect: since the sentences are al-
ready free of any grammatical errors, no correction
can be grammatically better. Therefore, sentences
for which the correction preserves the meaning and
is not grammaticality worse will count as successes
and all other cases will count as failures. There are
no draws. Table 4 illustrates this difference and Ta-
ble 5 presents the success and failure rates for all six
methods. The greedy technique again performs the
best out of all six methods and successfully retains
the meaning and grammaticality for almost 80% of
50
Method Success Draw Failure
Baseline LM (B) 21% (34) 9% (15) 70% (113)
Greedy (G) 36% (59) 33% (52) 31% (51)
1-best (1) 32% (52) 30% (48) 38% (62)
LM Re-ranked (L) 30% (48) 17% (27) 54% (87)
Product Re-ranked (P) 23% (37) 38% (61) 40% (64)
LM Composition (C) 19% (31) 12% (20) 69% (111)
Oracle 60% (97) 40% (65) -
Table 3: The success, draw and failure rates for the six correction generation techniques and the oracle as computed for
the 162 errorful sentences from the test set. The oracle picks the method that produces the most meaning-preserving
and grammatical correction for each sentence. For sentences that have no adequate correction, it picks the original
sentence. Numbers in parentheses represent counts.
Success
That?s why I like to make travel by using my own car.
That?s why I like to travel using my own car.
Having discuss all this I must say that I must rather prefer to be a leader than just a member.
After discussing all this, I must say that I would prefer to be a leader than a member.
Failure
And simply there is fantastic for everyone
All magical and simply there is fantastic for all
I hope that share a room with she can be certainly kindle, because she is likely me
and so will not be problems with she.
I hope that sharing a room with her can be certainly kindle, because it is likely that
I and so there will be no problems with it.
Figure 4: Two examples of success and failure for the Greedy (G) technique. Original sentences are shown first
followed by the corrections in bold. Grammatical errors in the original sentences are in italics.
the grammatical sentences.7
C > O C = O C < O
Meaning = 1 - S F
Meaning = 0 - F F
Table 4: A matrix illustrating the Success-Draw-Failure
evaluation criterion for the 38 grammatical sentences.
There are no draws and sentences for which corrections
preserve meaning and are not grammatically worse count
as successes. The rest are failures.
6 Discussion & Future Work
In this paper, we explored the potential of a novel
technique based on round-trip machine translation
for the more ambitious and realistic task of whole-
sentence grammatical error correction. Although the
idea of round-trip machine translation (via a single
pivot language) has been explored before in the con-
text of just preposition errors, we expanded on it sig-
nificantly by combining multiple round-trip transla-
7An oracle for this setup is uninteresting since it will simply
return the original sentence for every sentence.
Method Success Failure
Baseline LM (B) 26% (10) 74% (28)
Greedy (G) 79% (30) 21% (8)
1-best (1) 61% (23) 39% (15)
LM Re-ranked (L) 34% (13) 66% (25)
Product Re-ranked (P) 42% (16) 58% (22)
LM Composition (C) 29% (11) 71% (25)
Table 5: The success and failure rates for the six correc-
tion generation techniques as computed for the 38 gram-
matical sentences from the test set.
tions and developed several new methods for pro-
ducing whole-sentence error corrections. Our oracle
experiments show that the ideas we explore have the
potential to produce whole-sentence corrections for
a variety of sentences though there is clearly room
for improvement.
An important point needs to be made regard-
ing the motivation for the round-trip translation ap-
proach. We claim that this approach is useful not
just because it can produce alternative renderings of
a given sentence but primarily because each of those
51
renderings is likely to retain at least some of mean-
ing of the original sentence.
Most of the problems with our techniques arise
due to the introduction of new errors by Google
Translate. One could use an error detection sys-
tem (or a human) to explicitly identify spans con-
taining grammatical errors and constrain the SMT
system to translate only these errorful spans while
still retaining the rest of the words in the sentence.
This approach should minimize the introduction of
new errors. Note that Google Translate does not
currently provide a way to perform such selective
translation. However, other open-source SMT sys-
tems such as Moses8 and Joshua9 do. Furthermore,
it might also be useful to exploit n-best translation
outputs instead of just relying on the 1-best as we
currently do.
As an alternative to selective translation, one
could simply extract the identified errorful spans and
round-trip translate each of them individually. For
example, consider the sentence: ?Most of all, luck
is null prep no use without a hard work.? where the
preposition of is omitted and there is an extraneous
article a before ?hard work?. With this approach,
one would simply provide Google Translate with the
two phrasal spans containing the errors, instead of
the entire sentence.
More generally, although we use Google Trans-
late for this pilot study due to its easy availability, it
might be more practical and useful to rely on an in-
house SMT system that trades-off translation quality
for additional features.
We also found that the language-model based
techniques performed quite poorly compared to the
other techniques. We suspect that this is due to the
fact that Google Translate already employs large-
order language models trained on trillions of words.
Using lower-order models trained on much smaller
corpora might simply introduce noise. However, a
detailed analysis is certainly warranted.
In conclusion, we claim that our preliminary ex-
ploration of large-scale round-trip translation based
techniques yielded fairly reasonable results. How-
ever, more importantly, it makes it clear that, with
additional research, these techniques have the poten-
8http://www.statmt.org/moses
9https://github.com/joshua-decoder
tial to be very effective at whole-sentence grammat-
ical error correction.
Acknowledgments
We would like to thank Aoife Cahill, Michael Heil-
man and the three anonymous reviewers for their
useful comments and suggestions. We would also
like to thank Melissa Lopez and Matthew Mulhol-
land for helping with the annotation.
References
Ion Androutsopoulos and Prodromos Malakasiotis.
2010. A Survey of Paraphrasing and Textual Entail-
ment Methods. J. Artif. Int. Res., 38(1):135?187.
Srinivas Bangalore, German Bordel, and Giuseppe Ric-
cardi. 2001. Computing Consensus Translation from
Multiple Machine Translation Systems. In Proceed-
ings of ASRU, pages 351?354.
Regina Barzilay and Lillian Lee. 2003. Learning to Para-
phrase: An Unsupervised Approach Using Multiple-
Sequence Alignment. In Proceedings of HLT-NAACL
2003, pages 16?23.
Chris Brockett, William B. Dolan, and Michael Gamon.
2006. Correcting ESL Errors Using Phrasal SMT
Techniques. In Proceedings of the 21st International
Conference on Computational Linguistics and 44th
Annual Meeting of the Association for Computational
Linguistics, pages 249?256.
Yang Feng, Yang Liu, Haitao Mi, Qun Liu, and Ya-
juan Lu?. 2009. Lattice-based System Combination
for Statistical Machine Translation. In Proceedings of
the 2009 Conference on Empirical Methods in Natu-
ral Language Processing: Volume 3 - Volume 3, pages
1105?1113.
Matthieu Hermet and Alain De?silets. 2009. Using First
and Second Language Models to Correct Preposition
Errors in Second Language Authoring. In Proceedings
of the Fourth Workshop on Innovative Use of NLP for
Building Educational Applications, pages 64?72.
Claudia Leacock, Martin Chodorow, Michael Gamon,
and Joel Tetreault. 2010. Automated Grammatical
Error Detection for Language Learners. Synthesis
Lectures on Human Language Technologies. Morgan
Claypool.
Nitin Madnani and Bonnie J. Dorr. 2010. Generating
Phrasal and Sentential Paraphrases: A Survey of Data-
driven Methods. Computational Linguistics, 36(3).
Bo Pang, Kevin Knight, and Daniel Marcu. 2003.
Syntax-based Alignment of Multiple Translations: Ex-
tracting Paraphrases and Generating New Sentences.
In Proceedings of HLT-NAACL, pages 102?109.
52
Y. Albert Park and Roger Levy. 2011. Automated Whole
Sentence Grammar Correction using a Noisy Channel
Model. In Proceedings of the 49th Annual Meeting
of the Association for Computational Linguistics: Hu-
man Language Technologies - Volume 1, pages 934?
944.
Antti-Veikko Rosti, Necip Fazil Ayan, Bing Xiang, Spy-
ros Matsoukas, Richard Schwartz, and Bonnie Dorr.
2007. Combining Outputs from Multiple Machine
Translation Systems. In Human Language Technolo-
gies 2007: The Conference of the North American
Chapter of the Association for Computational Linguis-
tics; Proceedings of the Main Conference, pages 228?
235.
Matthew Snover, Nitin Madnani, Bonnie Dorr, and
Richard Schwartz. 2009. Fluency, Adequacy, or
HTER? Exploring Different Human Judgments with
a Tunable MT Metric. In Proceedings of the Fourth
Workshop on Statistical Machine Translation at the
12th Meeting of the European Chapter of the Associa-
tion for Computational Linguistics (EACL-2009).
Shiqi Zhao, Haifeng Wang, Xiang Lan, and Ting Liu.
2010. Leveraging Multiple MT Engines for Para-
phrase Generation. In COLING, pages 1326?1334.
53
Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 300?305,
Atlanta, Georgia, June 13 2013. c?2013 Association for Computational Linguistics
Detecting Missing Hyphens in Learner Text
Aoife Cahill?, Martin Chodorow?, Susanne Wolff? and Nitin Madnani?
? Educational Testing Service, 660 Rosedale Road, Princeton, NJ 08541, USA
{acahill, swolff, nmadnani}@ets.org
? Hunter College and the Graduate Center, City University of New York, NY 10065, USA
martin.chodorow@hunter.cuny.edu
Abstract
We present a method for automatically de-
tecting missing hyphens in English text. Our
method goes beyond a purely dictionary-based
approach and also takes context into account.
We evaluate our model on artificially gener-
ated data as well as naturally occurring learner
text. Our best-performing model achieves
high precision and reasonable recall, making
it suitable for inclusion in a system that gives
feedback to language learners.
1 Introduction
While errors of punctuation are not as frequent, nor
often as serious, as some of the other typical mis-
takes that learners make, they are nevertheless an
important consideration for students aiming to im-
prove the overall quality of their writing. In this pa-
per we focus on the error of missing hyphens. The
following example is a typical mistake made by a
student writer:
(1) Schools may have more after school sports.
In this case the tokens after and school should be hy-
phenated as they modify the noun sports. However,
in Example (2) a hyphen between after and school
would be incorrect, since in this instance after func-
tions as as the head of a prepositional phrase modi-
fying went.
(2) I went to the dentist after school today.
These examples illustrate that purely dictionary-
based approaches to detecting missing hyphens are
not likely to be sophisticated enough to differentiate
the contexts in which a hyphen is required. In addi-
tion, learner text frequently contains other grammat-
ical and spelling errors, further complicating auto-
matic error detection. Example (3) contains an error
father like instead of father likes to. This causes dif-
ficulty for automated hyphenation systems because
like is a frequent suffix of hyphenated words and
play can function as a noun.
(3) My father like play basketball with me.
In this paper, we propose a classifier-based approach
to automatically detecting missing hyphen errors.
The goal of our system is to detect missing hyphen
errors and provide feedback to language learners.
Therefore, we place more importance on the preci-
sion of the system than recall. We train our model on
features that take the context of a pair of words into
account, as well as other discriminative features. We
present a number of evaluations on both artificially
generated errors and naturally occurring learner er-
rors and show that our classifiers achieve high preci-
sion and reasonable recall.
2 Related Work
The task of detecting missing hyphens is related to
previous work on detecting punctuation errors. One
of the classes of errors in the Helping Our Own
(HOO) 2011 shared task (Dale and Kilgarriff, 2011)
was punctuation. Comma errors are the most fre-
quent kind of punctuation error made by learners. Is-
rael et al (2012) present a model for detecting these
kinds of errors in learner texts. They train CRF mod-
els on sentences from unedited essays written by
high-level college students and show that they per-
forms well on detecting errors in learner text. As
300
far as we are aware, the HOO 2011 system descrip-
tion of Rozovskaya et al (2011) is the only work to
specifically reference hyphen errors. They use rules
derived from frequencies in the training corpus to
determine whether a hyphen was required between
two words separated by white space.
The task of detecting missing hyphens is related
to the task of inserting punctuation into the output of
unpunctuated text (for example, the output of speech
recognition, automatic generation, machine transla-
tion, etc.). Systems that are built on the output of
speech recognition can obviously take features like
prosody into account. In our case, we are deal-
ing only with written text. Gravano et al (2009)
present an n-gram-based model for automatically
adding punctuation and capitalization to the output
of an ASR system, without taking any of the speech
signal information into account. They conclude that
more training data, rather than wider n-gram con-
texts leads to a greater improvement in accuracy.
3 Baselines
We implement three baseline systems which we will
later compare to our classification approach. The
first baseline is a na??ve heuristic that predicts a miss-
ing hyphen between bigrams that appear hyphenated
in the Collins Dictionary.1 As a somewhat less-
na??ve baseline, we implement a heuristic that pre-
dicts a missing hyphen between bigrams that occur
hyphenated more than 1,000 times in Wikipedia. A
third baseline is a heuristic that predicts a missing
hyphen between bigrams where the probability of
the hyphenated form as estimated from Wikipedia
is greater than 0.66, meaning that the hyphenated
bigram is twice as likely as the non-hyphenated bi-
gram. This baseline is similar to the approach taken
by Rozovskaya et al (2011), except that the proba-
bilities are estimated from a much larger corpus.
4 System Description
Using the features in Table 1, we build a logis-
tic regression model which assigns a probability to
the likelihood of a hyphen occurring between two
words, wi and wi+1. As we are primarily interested
in using this system for giving feedback to language
learners, we require very high precision. Therefore,
1LDC catalog number LDC93T1
Tokens wi?1, wi, wi+1, wi+2
Stems si?1, si, si+1, si+2
Tags ti?1, ti, ti+1, ti+2
Bigrams wi?wi+1, si?si+1, ti?ti+1
Dict Does the hyphenated form appear in
the Collins dictionary?
Prob What is the probability of the word
bigram appearing hyphenated in
Wikipedia?
Distance Distance to following and preced-
ing verb, noun
Verb/Noun Is there a verb/noun preced-
ing/following this bigram
Table 1: Features used in all models. Positive in-
stances are those where there was a hyphen between
wi and wi+1 in the data. Stems are generated using
NLTK?s implementation of the Lancaster Stemmer,
and tags are obtained from the Stanford Parser.
we only predict a missing hyphen error when the
probability of the prediction is >0.99.
We experiment with two different sources of
training data, in addition to their combination. We
first train on well-edited text, using almost 1.8 mil-
lion sentences from the San Jose Mercury News cor-
pus.2 For training, hyphenated words are automati-
cally split (i.e. well-known becomes well known).
The positive examples for the classifier are all bi-
grams where a hyphen was removed. Negative ex-
amples consist of bigrams where there was no hy-
phen in the training data. Since this is over 99% of
the data, we randomly sample 3% of the negative
examples for training. We also restrict the negative
examples to only the most likely contexts, where a
context is defined as a part-of-speech bigram. A list
of possible contexts in which hyphens occur is ex-
tracted from the entire training set. Only contexts
that occur more than 20 times are selected during
training. All contexts are evaluated during testing.
Table 2 lists some of the most frequent contexts with
examples of when they should be hyphenated and
when they should remain unhyphenated.
The second data source for training the model
comes from pairs of revisions from Wikipedia ar-
ticles. Following Cahill et al (2013), we automati-
cally extract a corpus of error annotations for miss-
2LDC catalog number LDC93T3A.
301
Context Hyphenated Unhyphenated
NN NN terrific truck-stop
waitress
a quake insurance
surcharge
CD CD Twenty-two thou-
sand
the 126 million
Americans
JJ NN an early-morning
blaze
an entire practice
session
CD NN a two-year contract about 600 tank cars
NN VBN a court-ordered
program
a letter delivered to-
day
Table 2: Some frequent likely POS contexts for hy-
phenation, with examples from the Brown corpus.
ing hyphens. This is done by extracting the plain
text from every revision to every article and com-
paring adjacent pairs of revisions. For each article,
chains of errors are detected, using the surrounding
text to identify them. When a chain begins and ends
with the same form, it is ignored. Only the first and
last points in an error chain are retained for train-
ing. An example chain is the following: It has been
an ancient {focal point ? location ? focal point
? focal-point} of trade and migration., where we
would extract the correction focal point ? focal-
point. In total, we extract a corpus of 390,298 sen-
tences containing missing hyphen error annotations.
Finally, we combine both data sources.
5 Evaluating on Artificial Data
Since there are large corpora of well-edited text
readily available, it is easy to evaluate on artifi-
cial data. For testing, we take 24,243 sentences
from the Brown corpus and automatically remove
hyphens from the 2,072 hyphenated words (but not
free-standing dashes). Each system makes a predic-
tion for all bigrams about whether a hyphen should
appear between the pair of words. We measure the
performance of each system in terms of precision, P,
(how many of the missing hyphen errors predicted
by the system were true errors), recall, R, (how many
of the artificially removed hyphens the system de-
tected as errors) and f-score, F, (the harmonic mean
of precision and recall). The results are given in
Table 3, and also include the raw number of true
positives, TP, detected by each system. The results
show that the baseline using Wikipedia probabilities
obtains the highest precision, however with low re-
call. The classifiers trained on newswire text and the
TP P R F
Baseline
Collins dict 397 40.5 19.2 26.0
Wiki Counts-1000 359 39.1 17.3 24.0
Wiki Probs-0.66 811 85.5 39.1 53.7
Classifier
SJM-trained 1097 82.0 52.9 64.3
Wiki-revision-trained 1061 72.8 51.2 60.1
Combined 1106 80.9 53.4 64.3
Table 3: Results of evaluating on the Brown Corpus
with hyphens removed
combined news and Wikipedia revision text achieve
the highest overall f-score. Figure (1a) shows the
Precision Recall curves for the Wikipedia baselines
and the three classifiers. The curves mirror the re-
sults in the table, showing that the classifier trained
on the newswire text, and the classifier trained on the
combined data perform best. The Wikipedia counts
baseline performs worst.
6 Evaluating on Learner Text
We carry out two evaluations of our system on
learner text. We first evaluate on the missing hyphen
errors contained in the CLC-FCE (Yannakoudakis et
al., 2011). This corpus contains 1,244 exam scripts
written by learners of English as part of the Cam-
bridge ESOL First Certificate in English. In total,
there are 173 instances of missing hyphen errors.
The results are given in Table 4, and the precision
recall curves are displayed in Figure (1b).
The results show that the classifiers consistently
achieve high precision on this data set. This is as
expected, given the high threshold set. Looking at
the curves, it seems that a slightly lower threshold in
this case may lead to better results. The curves show
that the combined classifier is performing slightly
better than the other two classifiers. The baselines
are clearly not performing as well on this dataset.
While the overall size of the CLC-FCE data set
is quite large, the low frequency of this kind of er-
ror means that the evaluation was carried out on a
relatively small number of examples. For this rea-
son, the reliability of the results may be called into
question. There is, for instance, a striking difference
between the f-scores for the Collins Dictionary base-
302
0.0 0.2 0.4 0.6 0.8
0.0
0.2
0.4
0.6
0.8
1.0
Recall
Prec
ision
combinedsjmwikibaseline?wiki?countsbaseline?wiki?probs
(a) Brown Corpus
0.2 0.4 0.6 0.8 1.00
.0
0.2
0.4
0.6
0.8
1.0
Recall
Prec
ision
combinedsjmwikibaseline?wiki?countsbaseline?wiki?probs
(b) CLC-FCE Corpus
Figure 1: Precision Recall curves for the Wikipedia baselines and the three classifiers.
TP P R F
Baseline
Collins dict 131 64.5 75.7 69.7
Wiki Counts-1000 141 73.1 81.5 77.0
Wiki Probs-0.66 36 92.3 20.8 34.0
Classifier
SJM-trained 60 84.5 34.7 49.2
Wiki-revision-trained 71 98.6 41.0 58.0
Combined 66 98.5 38.2 55.0
Table 4: Results of evaluating on the CLC-FCE
dataset
line on the Brown corpus (26.0) and on the learner
data (69.7). Inspection of the 131 true positives for
the learner data reveal that 87 of these are cases of a
single type, the word ?make-up?, which students of-
ten wrote without a hyphen in response to a prompt
about a fashion and leisure show. Since the hyphen-
ated form was in the Collins Dictionary, the base-
line system was credited with detection of this error.
However, when the 87 occurrences of ?make up? are
removed from the data set, the values of precision,
recall and f-score for the Collins Dictionary baseline
fall to 37.9, 51.2, and 42.9, respectively. This points
to a problem for system evaluation that is more gen-
303
eral than the low frequency of an error type, such
as missing hyphens. The more general problem is
that of non-independence among errors, which oc-
curs when an individual writer contributes multiple
times to an error count or when a particular prompt
gives rise to many occurrences of the same error, as
in the current case of ?make-up?.
Despite the problem of non-independent errors, a
more accurate picture of system performance may
nonetheless emerge with more evidence. Therefore,
we evaluate system precision on a data set of 1,000
student GRE and TOEFL essays written by both na-
tive and nonnative speakers, across a wide range of
proficiency levels and prompts. The essays, drawn
from 295 prompts, ranged in length from 1 to 50
sentences, with an average of 378 words per essay.
We manually inspect a random sample of 100 in-
stances where each system detected a missing hy-
phen. Two native-English speakers judged the cor-
rectness of the predictions using the Chicago Man-
ual of Style as a guide.3 Inter-annotator agreement
on the binary classification task for 600 items was
0.79?, showing high agreement. The results are
given in Table 5.
Total Judge-1 Judge 2
Predictions Precision Precision
Baseline
Collins dict 416 11 8
Wiki Counts 2185 20 21
Wiki Probs 224 54 52
Classifier
SJM-trained 421 62 69
Wiki-revision 577 43 41
Combined 450 60 62
Table 5: Precision results on 1000 student responses,
estimated by randomly sampling 100 hyphen predic-
tions of each system and manually evaluating them.
The results show that the first two baseline sys-
tems do not perform well on this essay data. This
is mainly because they do not take context into ac-
count. Many of the errors made by these systems in-
volved verb + preposition bigrams, as in Examples
(4) and (5). Restricting the detection by probability
clearly improves precision, but at the cost of recall
3http://www.chicagomanualofstyle.org
(only 224 total instances of missing hyphen errors
detected, the lowest of all 6 systems). In the man-
ual evaluation, the system trained on the SJM corpus
achieves the highest precision, though all precision
figures are lower than the previous evaluations. Ex-
ample (6) is a typical example of the kinds of false
positives made by the classifier models.
(4) If these men were required to step-down after a
limited number of years, the damage would be
contained.
(5) These families may even choose to eat at-home
than outside.
(6) The wellness program will save money in the
long-term.
Future work will explore additional features that
may help improve performance. A more thorough
study will also be carried out to fully understand the
differences in performance of the classifiers across
corpora. Another direction to explore in future work
is the related task of identifying extraneous hyphens
in learner text. These are even less frequent than
missing hyphens (87 annotated cases in the CLC-
FCE corpus), but we believe a similar classification
approach could be successful.
7 Conclusion
In this paper we presented a model for automatically
detecting missing hyphen errors in learner text. We
experimented with two kinds of training data, one
well-edited text, and the other an automatically ex-
tracted corpus of error annotations. When evaluat-
ing on artificially generated errors in otherwise well-
edited text, the classifiers generally performed bet-
ter than the baseline systems. When evaluating on
the small number of missing hyphen errors in the
CLC-FCE corpus, the word-based models did well,
though the classifiers also achieved consistently high
precision. A precision-only evaluation on a sample
of learner essays resulted in overall lower scores, but
the classifier trained on well-edited text performed
best. In general, the classifiers outperform the base-
line, especially in terms of precision, showing that
taking context into account when detecting these
kinds of errors is important.
304
References
Aoife Cahill, Nitin Madnani, Joel Tetreault, and Diane
Napolitano. 2013. Robust Systems for Preposition
Error Correction Using Wikipedia Revisions. In Pro-
ceedings of the Conference of the North American
Chapter of the Association for Computational Linguis-
tics: Human Language Technologies, Atlanta, GA.
Robert Dale and Adam Kilgarriff. 2011. Helping Our
Own: The HOO 2011 Pilot Shared Task. In Pro-
ceedings of the Generation Challenges Session at the
13th EuropeanWorkshop on Natural Language Gener-
ation, pages 242?249, Nancy, France, September. As-
sociation for Computational Linguistics.
Agustin Gravano, Martin Jansche, and Michiel Bacchi-
ani. 2009. Restoring punctuation and capitalization in
transcribed speech. In Acoustics, Speech and Signal
Processing, 2009. ICASSP 2009. IEEE International
Conference on, pages 4741?4744. IEEE.
Ross Israel, Joel Tetreault, and Martin Chodorow. 2012.
Correcting Comma Errors in Learner Essays, and
Restoring Commas in Newswire Text. In Proceed-
ings of the 2012 Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, pages 284?
294, Montre?al, Canada, June. Association for Compu-
tational Linguistics.
Alla Rozovskaya, Mark Sammons, Joshua Gioja, and
Dan Roth. 2011. University of Illinois System in
HOO Text Correction Shared Task. In Proceedings
of the Generation Challenges Session at the 13th Eu-
ropean Workshop on Natural Language Generation,
pages 263?266, Nancy, France, September. Associa-
tion for Computational Linguistics.
Helen Yannakoudakis, Ted Briscoe, and Ben Medlock.
2011. A New Dataset and Method for Automati-
cally Grading ESOL Texts. In Proceedings of the
49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies,
pages 180?189, Portland, Oregon, USA, June. Associ-
ation for Computational Linguistics.
305
Proceedings of the Ninth Workshop on Innovative Use of NLP for Building Educational Applications , pages 1?11,
Baltimore, Maryland USA, June 26, 2014.
c?2014 Association for Computational Linguistics
Automated Measures of Specific Vocabulary Knowledge from Constructed
Responses (?Use These Words to Write a Sentence Based on this Picture?)
Swapna Somasundaran
Educational Testing Services
660 Rosedale Road,
Princeton, NJ 08541, USA
ssomasundaran@ets.org
Martin Chodorow
Hunter College and the Graduate Center
City University of New York,
New York, NY 10065, USA
martin.chodorow@hunter.cuny.edu
Abstract
We describe a system for automatically
scoring a vocabulary item type that asks
test-takers to use two specific words in
writing a sentence based on a picture. The
system consists of a rule-based component
and a machine learned statistical model
which uses a variety of construct-relevant
features. Specifically, in constructing the
statistical model, we investigate if gram-
mar, usage, and mechanics features devel-
oped for scoring essays can be applied to
short answers, as in our task. We also ex-
plore new features reflecting the quality of
the collocations in the response, as well as
features measuring the consistency of the
response to the picture. System accuracy
in scoring is 15 percentage points greater
than the majority class baseline and 10
percentage points less than human perfor-
mance.
1 Introduction
It is often said that the best way to see if a per-
son knows the meaning of a word is to have that
person use the word in a sentence. Despite this
widespread view, most vocabulary testing contin-
ues to rely on multiple choice items (e.g. (Law-
less et al., 2012; Lawrence et al., 2012)). In
fact, few assessments use constructed sentence re-
sponses to measure vocabulary knowledge, in part
because of the considerable time and cost required
to score such responses manually. While much
progress has been made in automatically scor-
ing writing quality in essays (Attali and Burstein,
2006; Leacock et al., 2014; Dale et al., 2012),
the essay scoring engines do not measure profi-
ciency in the use of specific words, except perhaps
for some frequently confused homophones (e.g.,
its/it?s, there/their/they?re, affect/effect).
In this paper we present a system for automated
scoring of targeted vocabulary knowledge based
on short constructed responses in a picture de-
scription task. Specifically, we develop a system
for scoring a vocabulary item type that is in op-
erational use in English proficiency tests for non-
native speakers. Each task prompt in this item type
consists of two target key words, for which the vo-
cabulary proficiency is tested, and a picture that
provides the context for the sentence construction.
The task is to generate a single sentence, incorpo-
rating both key words, consistent with the picture.
Presumably, a test-taker with competent knowl-
edge of the key words will be able to use them in a
well-formed grammatical sentence in the context
of the picture.
Picture description tasks have been employed in
a number of areas of study ranging from second
language acquisition to Alzheimer?s disease (El-
lis, 2000; Forbes-McKay and Venneri, 2005). Pic-
tures and picture-based story narration have also
been used to study referring expressions (Lee et
al., 2012) and to analyze child narratives in order
to predict language impairment (Hassanali et al.,
2013). Evanini et al. (2014) employ a series of
pictures and elicit (oral) story narration to test En-
glish language proficiency. In our task, the picture
is used as a constraining factor to limit the type
and content of sentences that can be generated us-
ing the given key words.
In the course of developing our system, we ex-
amined existing features that have been developed
for essay scoring, such as detectors of errors in
grammar, usage and mechanics, as well as col-
location features, to see if they can be re-used
for scoring short responses. We also developed
new features for assessing the quality of sentence
construction using Pointwise Mutual Information
(PMI). As our task requires responses to describe
the prompt pictures, we manually constructed de-
tailed textual descriptions of the pictures, and de-
1
veloped features that measure the overlap between
the content of the responses and the textual de-
scription. Our automated scoring system is partly
based on deterministic scoring criteria and partly
statistical. Overall, it achieves an accuracy of
76%, which is a 15 percentage point improvement
over a simple majority class baseline.
The organization of this paper is as follows:
Section 2 describes the picture description task
and the scoring guide that is used to manually
score the picture description responses opera-
tionally. It also considers which aspects of scor-
ing may be handled best by deterministic proce-
dures and which are more amenable to statistical
modeling. Section 3 details the construction of a
reference corpus of text describing each picture,
and Section 4 presents the features used in scor-
ing. Section 5 describes our system architecture
and presents our experiments and results. Detailed
analysis is presented in Section 6, followed by re-
lated work in Section 7 and a summary with direc-
tions for future research in Section 8.
2 Task Description and Data
The picture description task is an item type that is
in actual operational use as part of a test of En-
glish. It consists of a picture, along with two key
words, one or both of which may be in an inflected
form. Test-takers are required to use the two words
in one sentence to describe the picture. They may
change the inflections of the words as appropriate
to the context of their sentence, but they must use
some form of both words in one sentence. Requir-
ing them to produce a response based on the pic-
ture constrains the variety of sentences and words
that they are likely to generate.
Trained human scorers evaluate the responses
based on appropriate use of grammar and the rel-
evance of the sentence to the picture. The opera-
tional scoring guide is as follows:
score = 3 The response consists of ONE sen-
tence that: (a) has no grammatical errors, (b)
contains forms of both key words used appro-
priately, AND (c) is consistent with the pic-
ture.
score = 2 The response consists of one or
more sentences that: (a) have one or more
grammatical errors that do not obscure the
meaning, (b) contain BOTH key words, (but
they may not be in the same sentence and
the form of the word(s) may not be accurate),
AND (c) are consistent with the picture.
score = 1 The response: (a) has errors that in-
terfere with meaning, (b) omits one or both
key words, OR (c) is not consistent with the
picture.
score = 0 The response is blank, written in
a foreign language, or consists of keystroke
characters.
Our decisions about scoring system design are
based on the scoring guide and its criteria. Some
aspects of the scoring can be handled by simple
pattern matching or lookup, while others require
machine learning. For example, score 0 is as-
signed to responses that are blank or are not in
English. This can be detected and scored in a
straightforward way. On the other hand, the de-
termination of grammaticality for the score points
3, 2 and 1 depends on the presence and severity
of grammatical errors. A wide variety of such er-
rors appear in responses, including errors of punc-
tuation, subject-verb agreement, preposition usage
and article usage. The severity of an error depends
on how problematic the error is, and the system
will have to learn this from the behavior of the
trained human scorer(s), making this aspect of the
scoring more amenable to statistical modeling.
Similarly, statistical modeling is more suitable
for determining the consistency of the response
with respect to the picture. According to the scor-
ing guide, a response gets a score of 0 or 1 if it is
not consistent with the picture, and gets a score of
2 or 3 if it is consistent. Thus, this aspect cannot
solely determine the score of a response ? it influ-
ences the score in conjunction with other language
proficiency factors. Further, measures of how rel-
evant a response is to a picture are likely to fall on
a continuous scale, making a statistical modeling
approach appropriate.
Finally, although there are some aspects of the
scoring guide, such as the number of sentences
and the presence of the key words, that can be
measured trivially, they do not act as sole deter-
minants of the score. For example, having more
than one sentence can result in the response re-
ceiving a score of 2 or 1. The number of sentences
works in conjunction with other factors such as
severity of grammar errors and relevance to the
picture. Hence its contribution to the final score
is best modeled statistically.
2
As a result of the heterogeneous nature of the
problem, our system is made up of a statistical
learning component as well as a non-statistical
component.
2.1 Data
The data set consists of about 58K responses to
434 picture prompts. The mean response length
was 11.26 words with a standard deviation of 5.10.
The data was split into 2 development sets (con-
sisting of a total of about 2K responses) and a fi-
nal train-test set (consisting of the remaining 56K
responses) used for evaluation. All 58K responses
were human scored using the scoring rubric dis-
cussed in Section 2. About 17K responses were
double annotated. The inter-annotator agreement,
using quadratic weighted kappa (QWK), was 0.83.
Score point 3, the most frequent class, was as-
signed to 61% of the responses, followed by score
point 2 (31%), score point 1 (7.6%) and score
point 0 (0.4%).
3 Reference Picture Descriptions
The pictures in our task vary in their complexity.
A typical prompt picture might be a photograph
of an outdoor marketplace, the inside of an airport
terminal, a grocery store, a restaurant or a store
room. Because consistency with respect to the pic-
ture is a crucial component in our task, we needed
a reliable and exhaustive textual representation of
each picture. Therefore, we manually constructed
a reference text corpus for each of our 434 pic-
ture prompts. We chose to use manual creation of
the reference corpus instead of trying automated
image recognition because automated methods of
image recognition are error prone and would result
in a noisy reference corpus. Additionally, auto-
mated approaches would, at best, give us a (noisy)
list of items that are present in the picture, but not
the overall scene or event depicted.
Two annotators employed by a company that
specializes in annotation created the reference cor-
pora of picture descriptions. The protocol used for
creating the reference corpus is shown below:
Part-1: List the items, setting, and events
in the picture.
List, one by one, all the items and events you
see in the picture. These may be animate ob-
jects (e.g. man), inanimate objects (e.g. table)
or events (e.g. dinner). Try to capture both the
overall setting (restaurant), as well as the ob-
jects that make up the picture (e.g. man, table,
food). These are generally (but not necessar-
ily) nouns and noun phrases. Some pictures
can have many items, while some have only a
few. The goal is to list 10-15 items and to cap-
ture as many items as possible, *starting with
the most obvious ones*.
If the picture is too sparse, and you are not
able to list at least 10 items, please indicate
this as a comment.
Part:2 Describe the picture
Describe the scene unfolding in the picture.
The scene in the picture may be greater than
the sum of its parts (many of which you will
list in part-1). For example, the objects in a
picture could be ?shoe? ?man? ?chair?, but the
scene in the picture could be that of a shoe
purchase. The description tries to recreate the
scene (or parts of the scene) depicted in the
picture.
Generate a paragraph of 5-7 sentences de-
scribing the picture. Some of these sentences
will address what is going on, while some may
address relations between items. The propor-
tions of these will differ, based on the picture.
Make sure that you generate at least one sen-
tence containing the two key words.
If the picture is too simple, and you are not
able to generate at least 5 sentences, please
indicate this as a comment.
The human annotator was given the picture and
the two key words. The protocol for creating each
reference corpus asked the annotator to first ex-
haustively list all the items (animate and inani-
mate) in the picture. Then, the annotator was
asked to describe the scene in the picture. We used
this two step process in order to capture, as much
as possible, all objects, relationships between ob-
jects, settings and events depicted in the pictures.
The size of the reference corpus for each prompt
is much larger than the single sentence test-taker
response. This is intentional as the goal is to make
the reference corpus as exhaustive as possible. We
used a single annotator for each prompt. Double
annotation using a secondary annotator was done
in cases where we felt that the coverage of the cor-
pus created by the primary annotator was insuffi-
3
cient
1
.
In order to test coverage, we used a small devel-
opment set of essays from each prompt and com-
pared the coverage of the generated reference cor-
pus over the development essays. If the cover-
age (proportion of content words in the responses
that were found in the reference corpus) was less
than 50% (this was the case for about 20% of
the prompts), we asked the secondary annotator to
create a new reference corpus for the prompt. The
two reference corpora for the prompt were then
simply combined to form a single reference cor-
pus.
4 Features for automated scoring
Because the score points in the scoring guide con-
flate, to some degree, syntactic, semantic, and
other weaknesses in the response, we carried out
a scoring study on a second small development
set (comprising of a total of 80 responses from 4
prompts, picked randomly) to gather insight into
the general problems in English language profi-
ciency exhibited in the responses. For the study,
it was necessary to have test-taker responses re-
scored by an annotator using an analytic scheme
which makes the types and locations of prob-
lems explicit. This exercise revealed that, in ad-
dition to the factors stated explicitly in the scor-
ing guide, there is another factor that results in
low comprehension (readability) of the sentence
and that reflects lower English proficiency. Specif-
ically, the annotator tagged many sentences as be-
ing ?awkward?. This awkwardness was due to
poor choice of words or to poor construction of the
sentence. For example, in the sentence ?The man
is putting some food in bags while he is record-
ing for the payment?, ?recording for the payment?
was marked as an awkward phrase. Based on our
annotation of the scores and on the descriptions in
the scoring guide, we selected features designed to
capture grammar, picture relevance and awkward
usage. We discuss each of our feature sets in the
following subsections.
4.1 Features for Grammatical Error
Detection
Essay scoring engines such as e-rater
R
?
(Attali
and Burstein, 2006) typically use a number of
1
We do not conduct inter-annotator agreement studies as
the goal of the double annotation was to create a diverse de-
scription.
grammar, usage and mechanics features that de-
tect and quantify different types of English usage
errors in essays. Examples of some of these error
types are: Run-on Sentences, Subject Verb Agree-
ment Errors, Pronoun Errors, Missing Posses-
sive Errors, Wrong Article Errors, Missing Arti-
cle Errors, Preposition Errors, Non-standard Verb
or Word Form Errors, Double Negative Errors,
Fragment or Missing Comma Errors, Ill-formed
Verb Errors, Wrong Form of Word Errors, Spelling
Errors, Wrong Part of Speech Errors, and Missing
Punctuation Errors .
In addition to these, essay scoring engines of-
ten also use as features the Number of Sentences
that are Short, the Number of Sentences that are
Long, the Number of Passive Sentences, and other
features that are relevant only for longer texts such
as essays. Accordingly, we selected, from e-rater
113 grammar, word usage, mechanics and lexical
complexity features that could be applied to our
short response task. This forms our grammar fea-
ture set.
4.2 Features for Measuring Content
Relevance
We generated a set of features that measure the
content overlap between a given response and the
corresponding reference corpus for the prompt.
For this, first the keywords and the stop words
were removed from the response and the reference
corpus, and then the proportion of overlap was cal-
culated between the lemmatized content words of
the response and the lemmatized version of the
corresponding reference corpus, as follows:
|Response ? Corpus|
|Response|
It is not always necessary for the test-taker to
use exactly the same words found in the reference
corpus. For example, the annotator might have
referred to a person in the picture as a ?lady?,
while a response may refer to the same person
as a ?woman? or ?girl? or even just ?person?.
Thus, we needed to go beyond simple lexical
match. In order to account for synonyms, we ex-
panded the content words in the reference corpus
by adding their synonyms, as provided in Lin?s
thesaurus (Lin, 1998) and then compared the ex-
panded reference to each response. Along the
same lines, we also used expansions from Word-
Net synonyms, WordNet hypernyms and WordNet
hyponyms. The following is the list of our content
4
relevance features. Each measures the proportion
of overlap as described by the equation above be-
tween the lemmatized response and
1. lemmas: the lemmatized reference corpus.
2. cov-lin: the reference corpus expanded using
Lin?s thesaurus.
3. cov-wn-syns: the reference corpus expanded
using WordNet Synonyms.
4. cov-wn-hyper: the reference corpus ex-
panded using WordNet Hypernyms.
5. cov-wn-hypo: the reference corpus ex-
panded using WordNet Hyponyms.
6. cov-all: the reference corpus expanded using
all of the above methods.
Mean proportions of overlap ranged from 0.65
for lemmas to 0.97 for cov-all.
The 6 features listed above, along with the
prompt id give a total of 7 features that form our
relevance feature set. We use prompt id as a fea-
ture because the extent of overlap can depend on
the prompt. Some pictures are very sparse, so,
the description of the picture in the response will
be short, and will not vary much from the refer-
ence corpus. For these, a good amount of overlap
between the response and reference corpus is ex-
pected. Other pictures are very dense with a large
number of objects and items shown. In this case,
any single response may describe just a small sub-
set of the items and satisfy the consistency criteria,
and consequently, even a small overlap between
the response and the reference corpus may be suf-
ficient.
4.3 Features for Awkward Word Usage
In order to measure awkward word usage, we ex-
plored PMI-based features, and also investigated
whether some features developed for essay scor-
ing can be used effectively for this purpose.
4.3.1 PMI-based ngram features
Non-native writing is often characterized by in-
appropriate combinations of words, indicating the
writer?s lack of knowledge of collocations. For ex-
ample, ?recording for the payment? might be bet-
ter expressed as ?entering the price in the cash reg-
ister?. As ?recording for the payment? is an inap-
propriate construction, it is not likely to be com-
mon, for example, in a large web corpus. We use
this intuition in constructing our PMI-based fea-
tures.
We find the PMI of all adjacent word pairs
(bigrams), as well as all adjacent word triples
(trigrams) in the Google 1T web corpus (Brants
and Franz, 2006) using the TrendStream database
(Flor, 2013).
PMI between word pairs (bigram AB) is defined
as:
log
2
p(AB)
p(A).p(B)
and between word triples (trigram ABC) as
log
2
p(ABC)
p(A).p(B).p(C)
The higher the value of the PMI, the more com-
mon is the collocation for the word pair/triple in
well formed texts. On the other hand, negative
values of PMI indicate that the given word pair (or
triple) is less likely than chance to occur together.
We hypothesized that this would be a good indica-
tor of awkward usage, as suggested in (Chodorow
and Leacock, 2000).
The PMI values for adjacent words obtained
over the entire response are then assigned to bins,
with 8 bins for word pairs and another 8 for word
triples. Each bin represents a range for PMI p tak-
ing real values R as follows:
bin
1
= {p ? R | p > 20}
bin
2
= {p ? R | 10 < p ? 20}
bin
3
= {p ? R | 1 < p ? 10}
bin
4
= {p ? R | 0 < p ? 1}
bin
5
= {p ? R | ? 1 < p ? 0}
bin
6
= {p ? R | ? 10 < p ? ?1}
bin
7
= {p ? R | ? 20 < p ? ?10}
bin
8
= {p ? R | p ? ?20}
Once the PMI values for the adjacent word pairs
in the response are generated, we generate two sets
of features. The first set is based on the counts
of word pairs falling into each bin (for example,
Number of pairs falling into bin
1
, Number of pairs
falling into bin
2
and so on). The second set of fea-
tures are based on percentages (for example Per-
centage of pairs falling into bin
1
, Percentage of
pairs falling into bin
2
etc.). These two sets result
in a total of 16 features. We similarly generate
16 more features for adjacent word triples. We
5
use percentages in addition to raw counts to ac-
count for the length of the response. For example,
it is possible for a long sentence to have phrases
that are awkward as well as well formed, giving
the same counts of phrases in the high-PMI value
bins as that of a short sentence that is entirely well
formed.
In addition to binning, we also encode as fea-
tures the maximum, minimum and median PMI
value obtained over all word pairs. The first two
features capture the best and the worst word col-
locations in a response. The median PMI value
captures the overall general quality of the response
in a single number. For example, if this is a low
number, then the response generally has many bad
phrasal collocations. Finally a null-PMI feature is
used to count the number of pairs that had zero
entries in the database. This feature is an indica-
tor that the given words or word collocations were
not found even once in the database. Given the
size of the underlying database, this usually hap-
pens in cases when words are misspelled, or when
the words never occur together.
All features created for bigrams are also created
for trigrams. We thus have a total of 40 features,
called the pmi feature set.
4.3.2 Features from essay scoring
A number of measures of collocation quality have
been proposed and implemented (e.g. (Futagi et
al., 2008; Dahlmeier and Ng, 2011)). We use e-
rater?s measure of the density of ?good? colloca-
tions found in the response. Another source of
difficulty for non-native writers is the selection of
appropriate prepositions. We use the mean proba-
bility assigned by e-rater to the prepositions in the
response. These two measures, one for the qual-
ity of collocations and the other for the quality of
prepositions, are combined in our colprep feature
set.
4.4 Scoring Rubric-based Features
As seen in Section 2, some of the criteria for scor-
ing are quite straightforward (e.g. ?omits one or
both key words?). While these are not sole deter-
minants of a score, they are certainly strong influ-
ences. Thus, we encode four criteria from the scor-
ing guide. These form our final feature set, rubric,
and are binary values, answering the questions: Is
the first key word from the prompt present in the
response? Is the second key word from the prompt
present in the response? Are both key words from
the prompt present in the response? Is there more
than one sentence in the response?
Table 1 provides a list of feature types and the
corresponding number of features of each type.
Feature set type Number of Features
grammar 113
relevance 7
pmi 40
colprep 2
rubric 4
Table 1: Feature sets and the counts of features in
each set
5 System and Evaluation
Figure 1: System Architecture
As noted earlier, the system is partly rule-based
and partly statistical. Figure 1 illustrates the sys-
tem architecture. The rule-based part captures
the straightforward deterministic scoring criteria
while the machine learning component encodes
features described in Section 4 and learns how to
weight the features for scoring based on human-
scored responses.
As described in Section 2, detection of condi-
tions that result in a score of zero are straight-
forward. Our rule-based scorer (shown as ?For-
eign Language Detector? in Figure 1) assigns a
zero score to a response if it is blank or non-
English. The system determines if the response is
non-English based on the average of PMI bigram
scores over the response. If the average score is
less than a threshold value, the system tags it as
6
a non-English sentence. The threshold was deter-
mined by manually inspecting the PMI values ob-
tained for sentences belonging to English and non-
English news texts. Responses given zero scores
by this module are filtered out and do not go to the
next stage.
Responses that pass the rule-based scorer are
then sent to the statistical scorer. Here, we encode
the features discussed in Section 4. Spell checking
and correction are carried out before features for
content relevance and PMI-based awkward word
usage are computed. This is done in order to pre-
vent misspellings from affecting the reference cor-
pus match or database search. The original text
is sent to the Grammar feature generator as it cre-
ates features based on misspellings and other word
form errors. Finally, we use all the features to train
a Logistic Regression model using sklearn. Note
that the statistical system predicts all 4 scores (0
through 3). This is because the rule-based system
is not perfect; that is, it might miss some responses
that should receive zero scores, and pass them over
to the next stage.
5.1 Metrics
We report our results using overall accuracy,
quadratic weighted kappa (QWK) and score-level
precision, recall and f-measure. The precision P
of the system is calculated for each score point i
as
P
i
=
|S
i
?H
i
|
|S
i
|
where |S
i
| is the number of responses given a
score of i by the system, and |S
i
?H
i
| is the num-
ber of responses given a score of i by the system
as well as the human rater.
Similarly, recall, R is calculated for each score
point i as
R
i
=
|S
i
?H
i
|
|H
i
|
F-measure F
i
is calculated as the harmonic
mean of the precision P
i
and recall R
i
at each
score point i. Accuracy is the ratio of the num-
ber of responses correctly classified over the total
number of responses.
5.2 Results
All of the responses in the train-test set were
passed through the rule-based zero-scorer. A total
of 210 responses had been scored as zero by the
human scorer. The rule-based system scored 222
responses as zeros, of which 184 were correct.
The precision P
rule
of the rule-based system is
calculated as
P
rule
0
=
184
222
= 82.9%
Similarly, Recall is calculated as
R
rule
0
=
184
210
= 87.6%
The corresponding F-measure is 85.2%
The remaining responses pass to the next stage
where machine learning is employed. We per-
formed 10 fold cross-validation experiments us-
ing Logistic Regression as well as Random Forest
learners. As the results are comparable, we only
report those from logistic regression.
Accuracy in % Agreement (QWK)
Baseline 61.00 -
System 76.23 0.63
Human 86.00 0.83
Table 2: Overall system and human accuracy
(in percentage) and agreement (using Quadratic
Weighted Kappa)
Table 2 reports the results. The system achieves
an accuracy of 76.23%, which is more than a 15
percentage point improvement over the majority
class baseline of 61%. The majority class base-
line always predicts a score of 3. Compared to hu-
man performance, system performance is 10 per-
centage points lower (human-human agreement
is 86%). Quadratic weighted kappa for system-
human agreement is also lower (0.63) than for
human-human agreement (0.83).
Table 3 reports the precision, recall and F-
measure of the system for each of the score points.
Score point Precision Recall F-measure
0 84.2 68.3 72.9
1 78.4 67.5 72.6
2 70.6 50.4 58.8
3 77.8 90.5 83.6
Table 3: Overall system performance at each score
point using all features
6 Analysis
In order to understand the usefulness of each fea-
ture set in scoring the responses, we constructed
7
systems using first the individual features alone,
and then using feature combinations. Table 4 re-
ports the accuracy of the learner using individual
features alone. We see that, individually, each fea-
ture set performs much below the performance of
the full system (that has an accuracy of 76.23%),
which is expected, as each feature set represents
a particular aspect of the construct. However, in
general, each of the feature-sets (except colprep)
shows improvement over baseline, indicating that
they contribute towards performance improvement
in the automated system.
Grammar features are the best of the individ-
ual feature sets at 70% accuracy, indicating that
grammatical error features developed for longer
texts can be applied to single sentences. The PMI-
based feature set is the second best performer, in-
dicating its effectiveness in capturing word usage
issues. While colprep and pmi both capture awk-
ward usage, pmi alone shows better performance
(67.44%) than colprep alone (61.26%). Also,
when rubric is used alone, the resulting system
produces a four percentage point improvement
over the baseline, with 65% accuracy, indicating
the presence of responses where the test-takers are
not able to incorporate one or both words in a sin-
gle sentence. The relevance feature set by itself
does not show substantial improvement over the
baseline. This is not surprising, as according to
the scoring guide, a response gets a score of 0 or 1
if it does not describe the picture, and gets a score
of 2 or 3 if it is relevant to the picture. Hence, this
feature cannot solely and accurately determine the
score.
Feature Set Accuracy in %
grammar 70.30
pmi 67.44
rubric 65.00
relevance 62.50
colprep 61.26
Table 4: System performance for individual fea-
tures
Table 5 reports accuracies of systems built us-
ing feature set combinations. The first feature set
combination, grammar + colprep, is a set of all
features obtained from essay scoring. Here we see
that addition of colprep does not improve the per-
formance over that obtained by grammar features
alone. Further, when colprep is combined with
pmi (colprep+pmi, row 2), there is a slight drop
in performance as compared to using pmi-based
features alone. These results indicate that colprep,
while being useful for larger texts, does not trans-
fer well to the simple single sentence responses in
our task.
Further, in Table 5 we see that the system using
a combination of the pmi feature set and the rele-
vance feature set (pmi+relevance) achieves an ac-
curacy of 69%. Thus, this feature combination is
able to improve performance over that using either
feature set alone, indicating that while content rel-
evance features by themselves do not create an im-
pact, they can improve performance when added
to other features. Finally, the feature combination
of all new features developed for this task (pmi +
relevance+ rubric) yields 73% accuracy, which is
again better than each individual feature set?s per-
formance, indicating that they can be synergisti-
cally combined to improve system performance.
Feature Set Accuracy in %
(i) grammar + colprep 70.31
(ii) colprep + pmi 67.42
(iii) pmi + relevance 69.05
(iv) pmi + relevance + rubric 73.21
Table 5: System performance for feature combi-
nations (i) typically used in essay scoring, (ii) that
measure awkwardness, (iii) newly proposed here,
(iv) newly proposed plus rubric-specific criteria
7 Related Work
Most work in automated scoring and learner lan-
guage analysis has focused on detecting grammar
and usage errors (Leacock et al., 2014; Dale et al.,
2012; Dale and Narroway, 2012; Gamon, 2010;
Chodorow et al., 2007; Lu, 2010). This is done
either by means of handcrafted rules or with sta-
tistical classifiers using a variety of information.
In the case of the latter, the emphasis has been on
representing the contexts of function words, such
as articles and prepositions. This work is rele-
vant inasmuch as errors in using content words,
such as nouns and verbs, are often reflected in the
functional elements which accompany them, for
example, articles that indicate the definiteness or
countability of nouns, and prepositions that mark
the cases of the arguments of verbs.
Previous work (Bergsma et al., 2009; Bergsma
et al., 2010; Xu et al., 2011) has shown that mod-
8
els which rely on large web-scale n-gram counts
can be effective for the task of context-sensitive
spelling correction. Measures of ngram associa-
tion such as PMI, log likelihood, chi-square, and
t have a long history of use for detecting colloca-
tions and measuring their quality (see (Manning
and Sch?utze, 1999) and (Leacock et al., 2014)
for reviews). Our application of a large n-gram
database and PMI is to detect inappropriate word
usage.
Our task also differs from work focusing on
evaluating content (e.g. (Meurers et al., 2011;
Sukkarieh and Blackmore, 2009; Leacock and
Chodorow, 2003)) in that, although we are look-
ing for usage of certain content words, we focus
primarily on measuring knowledge of vocabulary.
Recent work on assessment measures of depth
of vocabulary knowledge (Lawless et al., 2012;
Lawrence et al., 2012), has argued that knowl-
edge of specific words can range from superficial
(idiomatic associations built up through word co-
occurrence) to topical (meaning-related associa-
tions between words) to deep (definitional knowl-
edge). Some of our features (e.g. awkward word
usage) capture some of this information (e.g., id-
iomatic associations between words), but assign-
ing the depth of knowledge of the key words is not
the focus of our task.
Work that is closely related to ours is that of
King and Dickinson (2013). They parse picture
descriptions from interactive learner sentences,
classify sentences into syntactic types and extract
the logical subject, verb and object in order to re-
cover simple semantic representations of the de-
scriptions. We do not explicitly model the seman-
tic representations of the pictures, but rather our
goal in this work is to ascertain if a response is
relevant to the picture and to measure other fac-
tors that reflect vocabulary proficiency.
We employ human annotators and use word
similarity measures to obtain alternative forms of
description because the proprietary nature of our
data prevents us from releasing our pictures to
the public. However, crowd sourcing has been
used by other researchers to collect human labels
for images and videos. For example, Rashtchian
et al. (2010) use Amazon Mechanical Turk and
Von Ahn and Dabbish (2004) create games to en-
tice players to correctly label images. Chen and
Dolan (2011) use crowd sourcing to collect multi-
ple paraphrased descriptions of videos to create a
paraphrasing corpus.
In a vast body of related work, automated
methods have been explored for the generation
of descriptions of images (Kulkarni et al., 2013;
Kuznetsova et al., 2012; Li et al., 2011; Yao et
al., 2010; Feng and Lapata, 2010a; Feng and La-
pata, 2010b; Leong et al., 2010; Mitchell et al.,
2012). There is also work in the opposite di-
rection, of finding or generating pictures for a
given narration. Joshi et al. (2006) found the
best set of images from an image database to
match the keywords in a story. Coyne and Sproat
(2001) developed a natural language understand-
ing system which converts English text into three-
dimensional scenes that represent the text. For a
high-stakes assessment, it would be highly unde-
sirable to have any noise in the gold-standard ref-
erence picture descriptions. Hence we chose to use
manual description for creating our reference cor-
pus.
8 Summary and Future Directions
We investigated different types of features for au-
tomatically scoring a vocabulary item type which
requires the test-taker to use two words in writ-
ing a sentence based on a picture. We generated a
corpus of picture descriptions for measuring the
relevance of responses, and as a foundation for
feature development, we performed preliminary
fine-grained annotations of responses. The fea-
tures used in the resulting automated scoring sys-
tem include newly developed statistical measures
of word usage and response relevance, as well as
features that are currently found in essay scoring
engines. System performance shows an overall
accuracy in scoring that is 15 percentage points
above the majority class baseline and 10 percent-
age points below human performance.
There are a number of avenues open for future
exploration. The automated scoring system might
be improved by extending the relevance feature
to include overlap with previously collected high-
scoring responses. The reference corpus could
also be expanded and diversified by using a large
number of annotators, at least some of whom are
speakers of the languages that are most promi-
nently represented in the population of test-takers.
Finally, one particular avenue we would like to ex-
plore is the use of our features to provide feedback
in low stakes practice environments.
9
References
Yigal Attali and Jill Burstein. 2006. Automated essay
scoring with e-rater v. 2.0. Journal of Technology,
Learning, and Assessment, 4:3.
Shane Bergsma, Dekang Lin, and Randy Goebel.
2009. Web-scale n-gram models for lexical disam-
biguation. In IJCAI.
Shane Bergsma, Emily Pitler, and Dekang Lin. 2010.
Creating robust supervised classifiers via web-scale
n-gram data. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics, pages 865?874. Association for Computa-
tional Linguistics.
Thorsten Brants and Alex Franz. 2006. Web 1T 5-
gram Version 1. In Linguistic Data Consortium,
Philadelphia.
David L Chen and William B Dolan. 2011. Collect-
ing highly parallel data for paraphrase evaluation.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies-Volume 1, pages 190?200.
Association for Computational Linguistics.
Martin Chodorow and Claudia Leacock. 2000. An un-
supervised method for detecting grammatical errors.
In Proceedings of the Conference of the North Amer-
ican Chapter of the Association of Computational
Linguistics (NAACL), pages 140?147.
Martin Chodorow, Joel R Tetreault, and Na-Rae Han.
2007. Detection of grammatical errors involving
prepositions. In Proceedings of the fourth ACL-
SIGSEM workshop on prepositions, pages 25?30.
Association for Computational Linguistics.
Bob Coyne and Richard Sproat. 2001. Wordseye: an
automatic text-to-scene conversion system. In Pro-
ceedings of the 28th annual conference on Computer
graphics and interactive techniques, pages 487?496.
ACM.
Daniel Dahlmeier and Hwee Tou Ng. 2011. Cor-
recting semantic collocation errors with L1 induced
paraphrases. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing, EMNLP ?11, pages 107?117, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Robert Dale and George Narroway. 2012. A frame-
work for evaluating text correction. In LREC, pages
3015?3018.
Robert Dale, Ilya Anisimoff, and George Narroway.
2012. HOO 2012: A report on the preposition and
determiner error correction shared task. In Proceed-
ings of the Seventh Workshop on Building Educa-
tional Applications Using NLP, pages 54?62. Asso-
ciation for Computational Linguistics.
Rod Ellis. 2000. Task-based research and language
pedagogy. Language teaching research, 4(3):193?
220.
Keelan Evanini, Michael Heilman, Xinhao Wang, and
Daniel Blanchard. 2014. Automated scoring for
TOEFL Junior comprehensive writing and speaking.
Technical report, ETS, Princeton, NJ.
Yansong Feng and Mirella Lapata. 2010a. How many
words is a picture worth? Automatic caption genera-
tion for news images. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics, ACL ?10, pages 1239?1249, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Yansong Feng and Mirella Lapata. 2010b. Topic
models for image annotation and text illustration.
In Human Language Technologies: The 2010 An-
nual Conference of the North American Chapter of
the Association for Computational Linguistics, HLT
?10, pages 831?839, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Michael Flor. 2013. A fast and flexible architecture for
very large word n-gram datasets. Natural Language
Engineering, 19(1):61?93.
KE Forbes-McKay and Annalena Venneri. 2005. De-
tecting subtle spontaneous language decline in early
Alzheimers disease with a picture description task.
Neurological sciences, 26(4):243?254.
Yoko Futagi, Paul Deane, Martin Chodorow, and Joel
Tetreault. 2008. A computational approach to de-
tecting collocation errors in the writing of non-native
speakers of English. Computer Assisted Language
Learning, 21(4):353?367.
Michael Gamon. 2010. Using mostly native data to
correct errors in learners? writing: A meta-classifier
approach. In Human Language Technologies: The
2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 163?171. Association for Computa-
tional Linguistics.
Khairun-nisa Hassanali, Yang Liu, and Thamar
Solorio. 2013. Using Latent Dirichlet Allocation
for child narrative analysis. ACL 2013, page 111.
Dhiraj Joshi, James Z. Wang, and Jia Li. 2006. The
story picturing engine?a system for automatic text
illustration. ACM Trans. Multimedia Comput. Com-
mun. Appl., 2(1):68?89, February.
Levi King and Markus Dickinson. 2013. Shallow se-
mantic analysis of interactive learner sentences. In
Proceedings of the Eighth Workshop on Innovative
Use of NLP for Building Educational Applications,
pages 11?21, Atlanta, Georgia, June. Association
for Computational Linguistics.
Girish Kulkarni, Visruth Premraj, Vicente Ordonez,
Sagnik Dhar, Siming Li, Yejin Choi, Alexander C.
Berg, and Tamara L. Berg. 2013. Babytalk: Under-
standing and generating simple image descriptions.
IEEE Transactions on Pattern Analysis and Machine
Intelligence, 99(PrePrints):1.
10
Polina Kuznetsova, Vicente Ordonez, Alexander C
Berg, Tamara L Berg, and Yejin Choi. 2012. Col-
lective generation of natural image descriptions. In
Proceedings of the 50th Annual Meeting of the
Association for Computational Linguistics: Long
Papers-Volume 1, pages 359?368. Association for
Computational Linguistics.
Ren?e Lawless, John Sabatini, and Paul Deane. 2012.
Approaches to assessing partial vocabulary knowl-
edge and supporting word learning: Assessing vo-
cabulary depth. In Annual Meeting of the Ameri-
can Educational Research Association, April 13-17,
2012, Vancouver, CA.
Joshua Lawrence, Elizabeth Pare-Blagoev, Ren?e Law-
less, and Chen Deane, Paul andLi. 2012. Gen-
eral vocabulary, academic vocabulary, and vocabu-
lary depth: Examiningpredictors of adolescent read-
ing comprehension. In Annual Meeting of the Amer-
ican Educational Research Association.
Claudia Leacock and Martin Chodorow. 2003. C-rater:
Automated scoring of short-answer questions. Com-
puters and the Humanities, 37(4):389?405.
Claudia Leacock, Martin Chodorow, Michael Gamon,
and Joel Tetreault. 2014. Automated Grammatical
Error Detection for Language Learners. Synthesis
Lectures on Human Language Technologies. Mor-
gan & Claypool.
Choonkyu Lee, Smaranda Muresan, and Karin
Stromswold. 2012. Computational analysis of re-
ferring expressions in narratives of picture books.
NAACL-HLT 2012, page 1.
Chee Wee Leong, Rada Mihalcea, and Samer Hassan.
2010. Text mining for automatic image tagging. In
Proceedings of the 23rd International Conference on
Computational Linguistics: Posters, pages 647?655.
Association for Computational Linguistics.
Siming Li, Girish Kulkarni, Tamara L Berg, Alexan-
der C Berg, and Yejin Choi. 2011. Composing
simple image descriptions using web-scale n-grams.
In Proceedings of the Fifteenth Conference on Com-
putational Natural Language Learning, pages 220?
228. Association for Computational Linguistics.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of the 17th inter-
national conference on Computational linguistics-
Volume 2, pages 768?774. Association for Compu-
tational Linguistics.
Xiaofei Lu. 2010. Automatic analysis of syntactic
complexity in second language writing. Interna-
tional Journal of Corpus Linguistics, 15(4).
Christopher D. Manning and Hinrich Sch?utze. 1999.
Foundations of Statistical Natural Language Pro-
cessing. MIT Press.
Detmar Meurers, Ramon Ziai, Niels Ott, and Stacey M
Bailey. 2011. Integrating parallel analysis mod-
ules to evaluate the meaning of answers to reading
comprehension questions. International Journal of
Continuing Engineering Education and Life Long
Learning, 21(4):355?369.
Margaret Mitchell, Xufeng Han, Jesse Dodge, Alyssa
Mensch, Amit Goyal, Alex Berg, Kota Yamaguchi,
Tamara Berg, Karl Stratos, and Hal Daum?e III.
2012. Midge: Generating image descriptions from
computer vision detections. In Proceedings of the
13th Conference of the European Chapter of the As-
sociation for Computational Linguistics, pages 747?
756. Association for Computational Linguistics.
Cyrus Rashtchian, Peter Young, Micah Hodosh, and
Julia Hockenmaier. 2010. Collecting image annota-
tions using Amazon?s Mechanical Turk. In Proceed-
ings of the NAACL HLT 2010 Workshop on Creating
Speech and Language Data with Amazon?s Mechan-
ical Turk, pages 139?147. Association for Computa-
tional Linguistics.
Jana Zuheir Sukkarieh and John Blackmore. 2009.
C-rater: Automatic content scoring for short con-
structed responses. In FLAIRS Conference.
Luis Von Ahn and Laura Dabbish. 2004. Labeling
images with a computer game. In Proceedings of the
SIGCHI conference on Human factors in computing
systems, pages 319?326. ACM.
Wei Xu, Joel Tetreault, Martin Chodorow, Ralph Gr-
ishman, and Le Zhao. 2011. Exploiting syntactic
and distributional information for spelling correc-
tion with web-scale n-gram models. In Proceedings
of the Conference on Empirical Methods in Natu-
ral Language Processing, pages 1291?1300. Asso-
ciation for Computational Linguistics.
Benjamin Z Yao, Xiong Yang, Liang Lin, Mun Wai
Lee, and Song-Chun Zhu. 2010. I2t: Image pars-
ing to text description. Proceedings of the IEEE,
98(8):1485?1508.
11
LAW VIII - The 8th Linguistic Annotation Workshop, pages 48?53,
Dublin, Ireland, August 23-24 2014.
Finding your ?inner-annotator?: An experiment in annotator 
independence for rating discourse coherence quality in essays  
 
  Jill Burstein              Swapna Somasundaran Martin Chodorow 
Educational Testing Service Educational Testing Service Hunter College, CUNY 
666 Rosedale Road  666 Rosedale Road  695 Park Avenue 
Princeton, NJ 08541  Princeton, NJ 08541  New York, NY  
 
jburstein@ets.org   ssomasundaran@ets.org   martin.chodorow@hunter.cuny.edu 
 
Abstract 
An experimental annotation method is described, showing promise for a subjective labeling task ? 
discourse coherence quality of essays.   Annotators developed personal protocols, reducing front-end 
resources: protocol development and annotator training.  Substantial inter-annotator agreement was 
achieved for a 4-point scale.  Correlational analyses revealed how unique linguistic phenomena were 
considered in annotation. Systems trained with the annotator data demonstrated utility of the data. 
 
1 Introduction1 
  
Systems designed to evaluate discourse coherence quality often use supervised methods, relying on 
human annotation that requires significant front-end resources (time and cost) for protocol 
development and annotator training (Burstein et al., 2013). Crowd-sourcing (e.g., Amazon 
Mechanical Turk) has been used to collect annotation judgments more efficiently than traditional 
means for tasks requiring little domain expertise (Beigman Klebanov et al., 2013; Louis & Nenkova, 
2013). However, proprietary data (test-taker essays) may preclude crowd-sourcing use. In the U.S., 
the need for automated writing evaluation  systems to score proprietary test-taker data is likely to 
increase when Common Core2 assessments are administered to school-age students beginning in 2015 
(Shermis, in press), increasing the need for data annotation. This paper describes an experimental 
method for capturing discourse coherence quality judgments for test-taker essays. Annotators 
developed personal protocols reflecting their intuitions about essay coherence, thus reducing standard 
front-end resources. The paper presents related work (Section 2), the experimental annotation (Section 
3), system evaluations (Section 4), and conclusions (Section 5). 
 
2 Related Work 
 
Even after extensive training, subjective tasks may yield low inter-annotator agreement (Burstein & 
Wolska, 2003; Reidsma & op den Akker, 2008; Burstein et al., 2013).  Front-end annotation activities 
may require significant resources (protocol development and annotator training) (Miltsakaki and 
Kukich, 2000; Higgins, et al., 2004; Wang et al., 2012; Burstein et al., 2013).  Burstein et al (2013) 
reviewed coherence features as discussed in cognitive psychology (Graesser et al., 2004), reading 
research (Van den Broek, 2012), and computational linguistics, and concluded that evaluating text 
coherence is highly personal , relying on a variety of features, including adherence to standard writing 
conventions (e.g., grammar),  and patterns of rhetorical structure and vocabulary usage.  They describe 
an annotation protocol that uses a 3-point coherence quality scale (3 (high), 2 (somewhat,) and 1 (low)) 
applied by 2 annotators to label 1,500 test-taker essays from 6 task types (Table 1).  Protocol 
development took several weeks, and offered extensive descriptions of the 3-point scale, including 
illustrative test-taker responses; rigorous annotator training was also conducted. Burstein et al, 2013 
collapsing the 3-point scale to a 2-point scale (i.e., high (3), low (1,2)). Results for a binary discourse 
coherence quality system (high and low coherence) for essays achieved only borderline modest 
                                                          
1 This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings 
footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 
2
 See http://www.corestandards.org/. 
48
Essay-Writing Item Type Test-Taker Population 
1. K-12 expository  Students3, ages 11-16 
2. Expository  NNES-Univ 
3. Source-based, integrated (reading and listening)  NNES-Univ 
4. Expository  Graduate school applicants 
5. Critical argument  Graduate school applicants 
6. Professional licensing, content/expository  Certification for a business-related profession 
 
Table 1. Six item types & populations in the experimental annotation task. NNES-Univ = non-native 
English speakers, university applicants 
 
performance (?=0.41)4.  Outcomes reported in Burstein et al are consistent with discussions that text 
coherence is a complex and individual process (Graesser et al, 2004; Van den Broek, 2012), motivating 
our experimental method.  In contrast to training annotators to follow an annotation scheme pre-
determined by others, annotators devised their own scoring protocols, capturing their independent 
impressions ? finding their ?inner-annotator.?  The practical outcomes of success of the method would 
be reduced front-end resources in terms of time required to (a) develop the annotation protocol and (b) 
train annotators. As a practical end-goal, another success criterion would be to achieve inter-annotator 
agreement such that classifiers could be trained, yielding substantial annotator-system agreement. 
 
3 Experimental Annotation Study 
 
Annotation scoring protocols from 2 annotators for coherence quality are evaluated and described. 
 
3.1 Human Annotators 
 
Two high school English teachers (employed by a company specializing in annotation) performed the 
annotation.  Annotators never met each other, did not know about each other?s activities, and only 
communicated about the annotation with a facilitator from the company. 
 
3.2 Data 
 
A random sample of 250 essays for 6 different item types (n=1500) and test-taker populations (Table 1) 
was selected. The sample was selected across 20 different prompts (test questions) for each item type in 
order to ensure topic generalizability in the resulting systems. Forty essays were randomly selected for a 
small pilot study; the remaining data (1460 essays) were used for the full annotation study. For the full 
study, 20% of the essays (n=292) had been randomly selected for double annotation to measure inter-
annotator agreement; the remaining 1168 essays were evenly divided, and each annotator labeled half 
(n=584 per annotator). Each annotator labeled a total of 876 essays across the 6 task types. 
 
3.3 Experimental Method Description 
 
A one-week pilot study was conducted. To provide some initial grounding, annotators received a 1-page 
task description that offered a high-level explanation of ?coherence? describing the end-points of a 
potential protocol.  (This description was written in about an hour.) It indicated that high coherence is 
associated with an essay that can be easily understood, and low coherence is associated with an 
incomprehensible essay.  Each annotator developed her own protocol: for each score point she wrote 
descriptive text illustrating a set of defining characteristics for each score point of coherence quality 
(e.g., ?The writer?s point is difficult to understand.?). Annotator 1 (A1) developed a 4-point scale; 
 
                                                          
3 Note that this task type was administered in an instructional setting; all other tasks were completed in high-stakes 
assessment settings. 
4
 Kappa was not reported in the paper, but was accessed through personal communication. 
49
Feature Type A1 (r) A2 (r) 
Grammar errors (e.g., subject verb agreement) 0.42 0.35 
Word usage errors (e.g., determiner errors) 0.46 0.44 
Mechanics errors (e.g.,  spelling, punctuation) 0.58 0.52 
EGT -- best 3 features (out of 112 features): F1, F2, F3 F1. -0.30 
F2. -0.28 
F3.  0.27 
F1. -0.14 
F2. -0.15 
F3.  0.11 
RST features--   best 3 features (out of 100 features): F1, F2, F3 F1. -0.27 
F2.  0.15 
F3.  0.19 
F1. -0.19 
F2.  0.08 
F3.  0.06 
LDSP 0.19 0.06 
Table 2. Pearson r between annotator discourse coherence scores and features. All correlations are 
significant at p < .0001, except for A2?s long-distance sentence-pair similarity at p < .05. 
 
Annotator 2 (A2) developed a 5-point scale. Because the two scales were different, ? could not be used 
to measure agreement, so a Spearman rank-order correlation (rS) was used, yielding a promising value 
(rS=0.82). Annotator protocols were completed at the end of the pilot study. 
 A full experiment was conducted. Each annotator used her protocol to assign a coherence quality 
score to each essay. Annotators assigned a score and wrote brief comments as explanation (drawing from 
the protocol). Comments provided a score supplement that could be used to support analyses beyond 
quantitative measures (Reidsma & Carletta, 2008).  The data were annotated in 12 batches (by task) 
composed of 75 essays (50 unique; 25 for double annotation). A Spearman rank-order correlation was 
computed on the double-scored essays for completed batches. If the correlation fell below 0.70 (which 
was infrequent), one of the authors reviewed the annotator scores and comments to look for 
inconsistencies.  Agreement was re-computed when annotator revisions were completed  to ensure inter-
rater agreement of 0.70. Annotations were completed over approximately 4 weeks to accommodate 
annotator schedules.  While a time log was not strictly maintained, we estimate the total time for 
communication to resolve inconsistency issues was about 4-6 hours. One author communicated score-
comment inconsistencies (e.g., high score with critical comments) to the company?s facilitator (through a 
brief e-mail); the facilitator then relayed the inconsistency information to the annotator(s).  The author?s 
data review and communication e-mail took no longer than 45 minutes for the few rounds where 
agreement fell below 0.70. Communication between the facilitator and the annotator(s) involved a brief 
discussion, essentially reviewing the points made in the e-mail.  
 
3.4 Results: Inter-annotator agreement 
 
Using the Spearman rank-order correlation, inter-rater agreement on the double-annotated data was 
rS=0.71. In order to calculate Kappa statistic, A2?s 5-point scale assignments were then mapped to a 4-
point scale by collapsing the two lowest  categories (1,2) into one (1), since there were very few cases of 
1?s; this is consistent with low frequencies of very low-scoring essays. Using quadratic weighted kappa 
(QWK), post-mapping indicated substantial agreement between the two annotators (?=0.61).  
 
3.5 Correlational Analysis: Which Linguistic Features Did Annotators Consider? 
 
A1 and A2 wrote brief comments explaining their coherence scores. Comments were shorthand notation 
drawn from their protocols (e.g., There are significant grammatical errors...thoughts do not connect.). 
Both annotators included descriptions such as ?word patterns,? ?logical sequencing,? and ?clarity of 
ideas?; however, A2 appeared to have more comments related to grammar and spelling.   Burstein et al., 
(2013)  describe the following features in their binary classification system: (1) grammar, word usage, 
and mechanics errors (GUM), (2) rhetorical parse tree features (Marcu, 2000) (RST), (3) entity-grid 
transition probabilities to capture local ?topic distribution? (Barzilay & Lapata, 2008) (EGT), and (4) a 
long-distance sentence pair similarity measure using latent semantic analysis (Foltz, 1998) to capture 
?long distance, topical distribution. (LDSP).  Annotated data from this study were processed with the 
Burstein et al (2013) system to extract the features above in (1) ? (4).  To quantify the observed 
50
differences in the annotators? comments and potential effects for system score assignment (Section 4), 
we computed Pearson (r) correlations between the system features (on our annotated data set), and the 
discourse coherence scores of A1 and A2 (using the 4-point scale mapping for A2). There are 112 entity-
transition probability features and 100 Rhetorical Structure Theory (RST) features. In Table 2, the 
correlations of the three best predictors from the EGT and RST sets, and the GUM features and the 
LDSP feature are shown. Correlations in Table 2 are significantly correlated between the feature sets and 
annotator coherence scores. However, we observed that the EGT, RST, and LDSP feature correlation 
values for A2 are notably smaller than A1?s. This suggests that A2 may have had a strong reliance on 
GUM features, or that the system feature set did not capture all linguistic phenomena that A2 considered. 
 
4 System Evaluation5 
 
To evaluate the utility of the annotated data, two evaluations were conducted: one built classifiers with 
all system features (Sys_All), and a second with the GUM features (Sys_GUM). Using 10-fold cross-
validation with a gradient boosting regression learner, four classifiers were trained to predict coherence 
quality ratings on a 4-point scale, using the respective annotator data sets: A1 and A2 Sys_All, and A1 
and A2 Sys_GUM systems. 
  
4.1 Results 
 
Sys-All trained with A1 data consistently outperformed Sys-All trained with A2 data. Results are 
reported for averages across the 10-folds, and  showed substantial system-human agreement for A1 (? = 
0.68) and modest system-human agreement for A2 (? = 0.55). When Sys_GUM was trained with A1 
data, system-human agreement dropped to a modest  range (? = 0.60); when Sys_GUM was trained with 
A2 data, however, human agreement was essentially unchanged, staying in the modest  agreement range 
(? = 0.50).  Consistent with the correlational analysis, this finding suggests that A2 has strong reliance 
on GUM features, or the system may have been less successful in capturing A2 features beyond GUM. 
 
5  Discussion and Conclusions 
Our experimental annotation method significantly reduced front-end resources for protocol development 
and annotator training. Analyses reflect one genre: essays from standardized assessments. Minimal time 
was required from the authors or the facilitator (about two hours) for protocol development; the 
annotators developed personal protocols over a week during the pilot; in Burstein et al (2013), this 
process was report to take about one month. Approximately 4-6 hours of additional discussion from one 
author and the facilitator was required during the task; Burstein et al (2013) required two researchers and 
two annotators participated in several 4-hour training sessions, totaling about 64-80 hours of person-time 
across the 4 participants (personal communication). In addition to its efficiency, the experimental 
method was successful per criteria in Section 2. The method captures annotators? subjective judgments 
about coherence quality, yielding substantial inter-annotator agreement (?=0.61) across a 4-point scale.  
Second, classifiers trained with annotator data showed that the systems showed substantial and modest 
agreement (A1 and A2, respectively) ? demonstrating annotation utility, especially for A1. Correlational 
analyses were used to analyze effects of features that annotators may have considered in making their 
decisions. Comment patterns and results from the correlation analysis suggested that A2?s decisions 
were either based on narrower considerations (GUM errors), or not captured by our feature set.  
  The experimental task facilitated the successful collection of subjective coherence judgments with 
substantial inter-annotator agreement on test-taker essays. Consistent with conclusions from Reidsma & 
Carletta (2008), outcomes show that quantitative measures of inter-annotator agreement should not be 
used exclusively.  Descriptive comments were useful for monitoring during annotation, interpreting 
annotator considerations and system evaluations during and after annotation, and informing system 
development. In the future, we would explore strategies to evaluate intra-annotator reliability (Beigman-
Klebanov, Beigman, & Diermeier, 2008) which may have contributed to  lower system performance 
with A2 data. 
                                                          
5
 Many thanks to Binod Gywali for engineering support. 
51
References 
 
Beata Beigman-Klebanov, Nitin Madnani,, and Jill Burstein. 2013.  Using Pivot-Based Paraphrasing and 
Sentiment Profiles to Improve a Subjectivity Lexicon for Essay Data, Transactions of the Association for 
Computational Linguistics, Vol.1: 99-110. 
 
Beata Beigman-Klebanov, Eyal Beigman, and Daniel Diermeier. 2008. Analyzing Disagreements. In 
Proceedings of the workshop on Human Judgments in Computational Linguistics, Manchester: 2-7. 
 
Jill Burstein, Joel Tetreault and Martin Chodorow. 2013. Holistic Annotation of Discourse Coherence Quality 
in Noisy Essay Writing. In the Special issue of Dialogue and Discourse on: Beyond semantics: the challenges 
of annotating pragmatic and discourse phenomena Eds.  S. Dipper, H.  Zinsmeister, and B. Webber. 
Discourse & Dialogue 42, 34-52. 
 
Jill Burstein and Magdalena Wolska..2003. Toward Evaluation of Writing Style: Overly Repetitious Word Use. 
In Proceedings of the 11th Conference of the European Chapter of the Association for Computational 
Linguistics. Budapest, Hungary. 
 
Jacob Cohen.  1960. "A coefficient of agreement for nominal scales". Educational and Psychological 
Measurement 20 1: 37?46.  
 
Joseph Fleiss and Jacob Cohen 1973. "The equivalence of weighted kappa and the intraclass correlation 
coefficient as measures of reliability" in Educational and Psychological Measurement, Vol. 33:613?619. 
 
Peter Foltz, Walter Kintsch, & Thomas Landuaer.  1998. Textual coherence using latent semantic analysis. 
Discourse Processes, 252&3: 285?307.  
 
Arthur Graesser, Danielle McNamara, Max Louwerse. and Zhiqiang  Cai, Z. 2004. Coh-metrix: Analysis of text 
on cohesion and language. Behavior Research Methods, Instruments, & Computers, 36(2), 193-202. 
 
Derrick Higgins, Jill Burstein,  Daniel Marcu &. Claudia Gentile. 2004. Evaluating Multiple Aspects of 
Coherence in Student Essays. In Proceedings of 4th Annual Meeting of the Human Language Technology and 
North American Association for Computation Linguistics:185?192, Boston, MA 
 
 J. Richard Landis,.  & G. Koch. 1977. "The measurement of observer agreement for categorical 
data". Biometrics 33 1: 159?174. 
 
Annie Louis and Ani Nenkova. 2013. A Text Quality Corpus for Science Journalism. In the Special Issue 
of Dialogue and Discourse on: Beyond semantics: the challenges of annotating pragmatic and discourse 
phenomena Eds.  S. Dipper, H.  Zinsmeister, and B. Webber, 42: 87-117. 
 
Eleni Miltsakaki and Karen Kukich. 2000. Automated evaluation of coherence in student essays. In 
Proceedings of the Language Resources and Evaluation Conference, Athens, Greece. 
 
Daniel Marcu. 2000. The Theory and Practice of Discourse Parsing and Summarization. Cambridge, MA: The 
MIT Press. 
 
Dennis Reidsma,  and  Rieks op den Akker. 2008. Exploiting `Subjective' Annotations.  In Proceedings of the 
Workshop on Human Judgments in Computational Linguistics, Coling 2008, 23 August 2008, Manchester, 
UK. 
 
Dennis Reidsma,  and  Jean Carletta. 2008. Reliability measurements without limits. Computational Linguistics, 
343: 319-336. 
 
Mark Shermis. to appear. State-of-the-art automated essay scoring: Competition, results, and future directions 
from a United States demonstration.  Assessing Writing. 
 
Y. Wang, M. Harrington, and P. White. 2012. Detecting Breakdowns in Local Coherence in the Writing  of 
Chinese English Speakers. The Journal of Computer Assisted Learning. 28: 396?410. 
 
52
Paul Van den Broek. 2012. Individual and developmental differences in reading comprehension: Assessing 
cognitive processes and outcomes. In: Sabatini, J.P., Albro, E.R., O'Reilly, T. (Eds.), Measuring up: 
Advances in how we assess reading ability., pp. 39-58. Lanham: Rowman & Littlefield Education. 
 
 
 
53
