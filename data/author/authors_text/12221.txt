Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 128?131,
Queen Mary University of London, September 2009. c?2009 Association for Computational Linguistics
On NoMatchs, NoInputs and BargeIns:
Do Non-Acoustic Features Support Anger Detection?
Alexander Schmitt, Tobias Heinroth
Dialogue Systems Research Group
Institute for Information Technology
Ulm University, Germany
alexander.schmitt@uni-ulm.de
tobias.heinroth@uni-ulm.de
Jackson Liscombe
SpeechCycle, Inc.
Broadway 26
New York City, USA
jackson@speechcycle.com
Abstract
Most studies on speech-based emotion
recognition are based on prosodic and
acoustic features, only employing artifi-
cial acted corpora where the results cannot
be generalized to telephone-based speech
applications. In contrast, we present an
approach based on utterances from 1,911
calls from a deployed telephone-based
speech application, taking advantage of
additional dialogue features, NLU features
and ASR features that are incorporated
into the emotion recognition process. De-
pending on the task, non-acoustic features
add 2.3% in classification accuracy com-
pared to using only acoustic features.
1 Introduction
Certainly, the most relevant employment of
speech-based emotion recognition is that of a
telephone-based Interactive Voice Response Sys-
tem (IVR).
Emotion recognition for IVR differs insofar
to ?traditional? emotion recognition, that it can
be reduced to a binary classification problem,
namely the distinction between angry and non-
angry whereas studies on speech-based emotion
recognition analyze complete and relatively long
sentences covering the full bandwidth of human
emotions. In a way, emotion recognition in the
telephone domain is less challenging since a dis-
tinction between two different emotion classes,
angry and non-angry, is sufficient. We don?t have
to expect callers talking to IVRs in a sad, anxious,
happy, disgusted or bored manner. I.e., even if a
caller is happy, the effect on the dialogue will be
the same as if he is neutral. However, there still
remain challenges for the system developer such
as varying speech quality caused by, e.g., vary-
ing distance to the receiver during the call lead-
ing to loudness variations (which emotion recog-
nizers might mistakenly interpret as anger). But
also bandwidth limitation introduced by the tele-
phone channel and a strongly unbalanced distribu-
tion of non-angry and angry utterances with more
than 80% non-angry utterances make a reliable
distinction of the caller emotion difficult. While
hot anger with studio quality conditions can be de-
termined with over 90% (Pittermann et al, 2009)
studies on IVR anger recognition report lower ac-
curacies due to these limitations. However, there
is one advantage of anger recognition in IVR sys-
tems that can be exploited: additional information
is available from the dialogue context, the speech
recognizer and the natural language parser.
This contribution is organized as follows: first,
we introduce related work and describe our cor-
pus. In Section 4 we outline our employed features
with emphasis on the non-acoustic ones. Experi-
ments are shown in Section 5 where we analyze
the impact of the newly developed features before
we summarize our work in Section 6.
2 Related Work
Speech-based emotion research regarding tele-
phone applications has been increasingly dis-
cussed in the speech community. While in early
studies acted corpora were used, such as in (Ya-
coub et al, 2003), training and testing data in later
studies has been more and more based on real-
life data, see (Burkhardt et al, 2008),(Burkhardt
et al, 2009). Most studies are limited to acous-
tic/prosodic features that have been extracted out
of the audio data. Linguistic information was ad-
ditionaly exploited in (Lee et al, 2002) resulting in
128
a 45.7% accuracy improvement compared to using
only acoustic features. In (Liscombe et al, 2005)
the lexical and prosodic features were additionaly
enriched with dialogue act features leading to an
increase in accuracy of 2.3%.
3 Corpus Description
For our studies we employed a corpus of 1,911
calls from an automated agent helping to resolve
internet-related problems comprising 22,724 utter-
ances. Three labelers divided the corpus into an-
gry, annoyed and non-angry utterances (Cohen?s
? = 0.70 on whole corpus; L1 vs. L2 ? = 0.8,
L1 vs. L3 ? = 0.71, L2 vs. L3 ? = 0.59). The
reason for choosing three emotion classes instead
of a binary classification lies in the hope to find
clearer patterns for strong anger. A distinction be-
tween non-angry and somewhat annoyed callers
is rather difficult even for humans. The final la-
bel was defined based on majority voting resulting
in 90.2% non-angry, 5.1% garbage, 3.4% annoyed
and 0.7% angry utterances. 0.6% of the samples in
the corpus were sorted out since all three raters had
different opinions. The raters were asked to label
?garbage? when the utterance is incomprehensible
or consists of non-speech events. While the num-
ber of angry and annoyed utterances seems very
low, 429 calls (i.e. 22.4%) contained annoyed or
angry utterances.
4 Features
We created two different feature sets: one based
on typical acoustic/prosodic features and another
one to which we will refer as ?non-acoustic? fea-
tures consisting of features from the Automatich
Speech Recognition (ASR), Natural Language
Understanding (NLU), Dialogue Manager (DM)
and Context features.
4.1 Acoustic Features
The acoustic/prosodic features were extracted
with the aid of Praat (Boersma, 2001) and con-
sist of power, mean, rms, mean harmonicity, pitch
(mean, deviation, voiced frames, time step, mean
slope, minimum, maximum, range), voiced pitch
(mean, minimum mean, maximum mean, range),
intensity (mean, maximum, minimum, deviation,
range), jitter points, formants 1-5, MFCC 1-12.
The extraction was performed on the complete
short utterance.
4.2 Non-Acoustic Features
The second, i.e. non-acoustic, feature set is based
on features logged with the aid of the speech plat-
form hosting the IVR application and is presented
here in more detail. They include:
ASR features: raw ASR transcription of
caller?s utterance (Utterance) (unigram bag-of-
words); ASR confidence of returned utterance
transcription, as floating point number between 0
(least confident) and 1 (most confident) (Confi-
dence); names of all grammars active (Grammar-
Name); name of the grammar that returned the
parse (TriggeredGrammarName); did the caller
begin speaking before the prompt completed?
(?yes?, ?no?) (BargedIn); did the caller communi-
cate with speech (?voice?) or keypad (?dtmf?) (In-
putModeName); was the speech recognizer suc-
cessful (?Complete?) or not and if it was not suc-
cessful, an error message is recorded such as
?NoInput? or ?NoMatch? (RecognitionStatus)
NLU-Features: the semantic parse of the caller
utterance as returned by the activated grammar in
the current dialog module (Interpretation); given
caller speech input, we need to try and recognize
the semantic meaning. The first time we try to do
this, this is indicated with a value of ?Initial?. If
we were not returned a parse then we have to re-
prompt (?Retry1? or ?Timeout1?). Similar for if the
caller asks for help or a repetition of the prompt.
Etc. (LoopName)
DM-Features: the text of what the auto-
mated agent said prior to recording the user input
(PromptName); the number of tries to elicit a de-
sired response. Integer values range from 0 (first
try) to 7 (6th try) (RoleIndex); an activity may re-
quest substantive user input (?Collection?) or con-
firm previous substantive input (?Confirmation?)
(RoleName); within a call each event is sequen-
tially organized by these numbers (SequenceID);
the name of the activity (aka dialog module) that
is active (ActivityName); type of activity. Possible
values are: Question, PlatformValue, Announce-
ment, Wait, Escalate (ActivityType)
Context-Features: We further developed addi-
tional cumulative features based on the previous
ones in order to keep track of the NoMatch, NoIn-
puts and similar parameters serving as an indicator
for the call quality: number of non-empty NLU
parses (CumUserTurns); number of statements
and questions by the system (CumSysTurns); num-
ber of questions (CumSysQuestions); number of
129
help requests by the user (CumHelpReq); num-
ber of operator requests (CumOperatorReq); num-
ber of NoInput events (CumNoInputs); number
of NoMatch events (CumNoMatchs) number of
BargeIns (CumBargeIns).
5 Experiments
In order to prevent an adaption of the anger model
to specific callers we seperated the corpus ran-
domly into 75% training and 25% testing material
and ensured that no speaker contained in training
was used for testing. To exclude that we receive a
good classification result by chance, we performed
50 iterations in each test and calculated the per-
formance?s mean and standard deviation over all
iterations.
Note, that our aim in this study is less finding
an optimum classifer, than finding additional fea-
tures that support the distinction between angry
and non-angry callers. Support Vector Machines
and Artificial Neural Networks are thus not con-
sidered, although the best performances are re-
ported with those learning algorithms. A simi-
lar performance, i.e. only slightly poorer, can be
reached with Rule Learners. They enable a thor-
ough study of the features, leading to the decision
for one or the other class, since they produce a
human readable set of if-then-else rules. Our hy-
potheses on a perfect feature set can thus easily be
confirmed or rejected.
We performed experiments with two differ-
ent classes: ?angry? vs. ?non-angry? and ?an-
gry+annoyed? vs. ?non-angry?. Merging angry
and annoyed utterances aims on finding all callers,
where the customer satisfaction is endangered. In
both tasks, we employ a) only acoustic features
b) only ASR/NLU/DM/Context features and c) a
combination of both feature sets. The number of
utterances used for training and testing is shown in
Table 1.
As result we expect acoustic features to per-
form better than non-acoustic features. Among
the relevant non-acoustic features we assume as
an indicator for angry utterances low ASR confi-
dences and high barge-in rates, which we consider
as signal for the caller?s impatience. All tests have
been performed with the machine learning frame-
work RapidMiner (Mierswa et al, 2006) featuring
all common supervised and unsupervised learning
schemes.
Results are listed in Table 2, including preci-
Test A Test B
angry+
annoyed non-a. angry non-a.
Training ? 320 ? 320 ? 80 ? 80
Testing ? 140 ? 140 ? 40 ? 40
Table 1: Number of utterances employed for both
tests per iteration. Since the samples are selected
randomly and the corpus was separated by speak-
ers before training and testing, the numbers may
vary in each iteration.
sion and recall values. As expected, Test B (an-
gry vs. non-angry) has the highest accuracy with
87.23% since the patterns are more clearly sep-
arable compared to Test A (annoyed vs. non-
angry, 72.57%). Obviously, adding non-acoustic
features increases classification accuracy signifi-
cantly, but only where the acoustic features are
not expressive enough. While the additional in-
formation increases the accuracy of the combined
angry+annoyed task by 2.3 % (Test A), it does
not advance the distinction between only angry vs.
non-angry (Test B).
5.1 Emotional History
One could expect, that the probability of
an angry/annoyed turn following another an-
gry/annoyed turn is rather high and that this in-
formation could be exploited. Thus, we further
included two features PrevEmotion and PrevPre-
vEmotion, taking into account the two previous
hand-labeled emotions in the dialogue discourse.
If they would contribute to the recognition pro-
cess, we would replace them by automatically la-
belled ones. All test results, however, did not im-
prove.
5.2 Ruleset Analysis
For a determination of the relevant features in the
non-acoustic feature set, we analyzed the ruleset
generated by the RuleLearner in Test A. Interest-
ingly, a dominant feature in the resulting ruleset is
?AudioDuration?. While shorter utterances were
assigned to non-angry (about <2s), longer utter-
ances tended to be assigned to angry/annoyed. A
following analysis of the utterance length confirms
this rule: utterances labeled as angry averaged
2.07 (+/-0.73) seconds, annoyed utterances lasted
1.82 (+/-0.57) s and non-angry samples were 1.57
(+/- 0.66) s in average. The number of NoMatch
130
Test A: Angry/Annoyed vs. Non-angry only Acoustic only Non-Acoustic both
Accuracy 70.29 (+-2.94) % 61.43 (+-2.75) % 72.57 (+-2.37) %
Precision/Recall Class ?Ang./Ann.? 71.51% / 61.57% 68.35% / 42.57% 73.67% / 70.14%
Precision/Recall Class ?Non-angry? 69.19% / 73.00% 58.30% / 80.29% 71.57% / 75.00%
Test B: Angry vs. Non-angry only Acoustic only Non-Acoustic both
Accuracy 87.06 (+-3.76) % 64.29 (+-1.32) % 87.23 (+-3.72) %
Precision/Recall Class ?Angry? 87.13% / 86.55% 66.0% / 58.9% 86.88% / 87.11%
Precision/Recall Class ?Non-angry? 86.97% / 87.53% 62.9% 69.9% 87.55% / 87.33%
Table 2: Classification results for angry+annoyed vs. non-angry and angry vs. non-angry utterances.
events (CumNoMatch) up to the angry turn played
a less dominant role than expected: only 8 samples
were assigned to angry/annoyed due to reoccur-
ring NoMatch events (>5 NoMatchs). Utterances
that contained ?Operator?, ?Agent? or ?Help? were,
as expected, assigned to angry/annoyed, however,
in combination with high AudioDuration values
(>2s). Non-angry utterances were typically better
recognized: average ASR confidence values are
0.82 (+/-0.288) (non-angry), 0.71 (+/- 0.36) (an-
noyed) and 0.56 (+/- 0.41) (angry).
6 Conclusion and Discussion
In IVR systems, we can take advantage of non-
acoustic information, that comes from the dia-
logue context. As demonstrated in this work,
ASR, NLU, DM and contextual features sup-
port the distinction between angry and non-angry
callers. However, where the samples can be sepa-
rated into clear patterns, such as in Test B, no ben-
efit from the additional feature set can be expected.
In what sense a late fusion of linguistic, dialogue
and context features would improve the classifier,
i.e. by building various subsystems whose opin-
ions are subject to a voting mechanism, will be
evaluated in future work. We will also analyze
why the linguistic features did not have any vis-
ible impact on the classifier. Presumably a combi-
nation of n-grams, bag-of-words and bag of emo-
tional salience will improve classification.
7 Acknowledgements
We would like to take the opportunity to thank the
following colleagues for contributing to the devel-
opment of our emotion recognizer: Ulrich Tschaf-
fon, Shu Ding and Alexey Indiryakov.
References
Paul Boersma. 2001. Praat, a System for Do-
ing Phonetics by Computer. Glot International,
5(9/10):341?345.
Felix Burkhardt, Richard Huber, and Joachim
Stegmann. 2008. Advances in anger detection with
real life data.
Felix Burkhardt, Tim Polzehl, Joachim Stegmann, Flo-
rian Metze, and Richard Huber. 2009. Detecting
real life anger. In Proc. of ICASSP, April.
Chul Min Lee, Shrikanth Narayanan, and Roberto Pier-
accini. 2002. Combining Acoustic and Language
Information for Emotion Recognition. In Interna-
tional Conference on Speech and Language Process-
ing (ICSLP), Denver, USA, October.
Jackson Liscombe, Guiseppe Riccardi, and Dilek
Hakkani-Tu?r. 2005. Using Context to Improve
Emotion Detection in Spoken Dialog Systems. In
International Conference on Speech and Language
Processing (ICSLP), Lisbon, Portugal, September.
Ingo Mierswa, Michael Wurst, Ralf Klinkenberg, Mar-
tin Scholz, and Timm Euler. 2006. Yale: Rapid
prototyping for complex data mining tasks. In KDD
?06: Proceedings of the 12th ACM SIGKDD interna-
tional conference on Knowledge discovery and data
mining, New York, NY, USA, August.
Johannes Pittermann, A. Pittermann, and Wolfgang
Minker. 2009. Handling Emotions in Human-
Computer Dialogues. Text, Speech and Language
Technology. Springer, Dordrecht (The Netherlands).
Sherif Yacoub, Steven Simske, Xiaofan Lin, and John
Burns. 2003. Recognition of emotions in interac-
tive voice response systems. In Proc. Eurospeech,
Geneva, pages 1?4.
131
Proceedings of NAACL-HLT 2013, pages 569?578,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
On Quality Ratings for Spoken Dialogue Systems ? Experts vs. Users
Stefan Ultes, Alexander Schmitt, and Wolfgang Minker
Ulm University
Albert-Einstein-Allee 43
89073 Ulm, Germany
{stefan.ultes,alexander.schmitt,wolfgang.minker}@uni-ulm.de
Abstract
In the field of Intelligent User Interfaces, Spo-
ken Dialogue Systems (SDSs) play a key role
as speech represents a true intuitive means
of human communication. Deriving informa-
tion about its quality can help rendering SDSs
more user-adaptive. Work on automatic esti-
mation of subjective quality usually relies on
statistical models. To create those, manual
data annotation is required, which may be per-
formed by actual users or by experts. Here,
both variants have their advantages and draw-
backs. In this paper, we analyze the relation-
ship between user and expert ratings by in-
vestigating models which combine the advan-
tages of both types of ratings. We explore two
novel approaches using statistical classifica-
tion methods and evaluate those with a pre-
existing corpus providing user and expert rat-
ings. After analyzing the results, we eventu-
ally recommend to use expert ratings instead
of user ratings in general.
1 Introduction and Motivation
In human-machine interaction it is important that
user interfaces can adapt to the specific requirements
of its users. Handicapped persons or angry users, for
example, have specific needs and should be treated
differently than regular users.
Speech is a major component of modern user in-
terfaces as it is the natural means of human com-
munication. Therefore, it seems logical to use Spo-
ken Dialogue Systems (SDS) as part of Intelligent
User Interfaces enabling speech communication of
different complexity reaching from simple spoken
commands up to complex dialogues. Besides the
spoken words, the speech signal also may be used
to acquire information about the user state, e.g.,
about their emotional state (cf., e.g., (Polzehl et
al., 2011))). By additional analysis of the human-
computer-dialogues, even more abstract informa-
tion may be derived, e.g., the quality of the system
(cf., e.g., (Engelbrecht and Mo?ller, 2010)). System
quality information may be used to adapt the sys-
tem?s behavior online during the ongoing dialogue
(cf. (Ultes et al, 2012)).
For determining the quality of Spoken Dialogue
Systems, several aspects are of interest. Mo?ller et
al. (2009) presented a taxonomy of quality criteria.
They describe quality as a bipartite issue consisting
of Quality of Service (QoS) and Quality of Experi-
ence (QoE). Quality of Service describes objective
criteria like dialogue duration or number of turns.
While these are well-defined items that can be de-
termined easily, Quality of Experience, which de-
scribes the user experience with subjective criteria,
is more vague and without a sound definition, e.g.,
User Satisfaction (US).
Subjective aspects like US are either determined
by using questionnaires like SASSI (Hone and Gra-
ham, 2000) or the ITU-standard augmented frame-
work for questionnaires (Mo?ller, 2003), or by us-
ing single-valued ratings, i.e., a rater only applies
one single score. In general, two major categories
of work on determining single-valued User Satisfac-
tion exist. The satisfaction ratings are applied either
? by users during or right after the dialogue or
? by experts by listening to recorded dialogues.
569
In this work, users or user raters are people who
actually perform a dialogue with the system and ap-
ply ratings while doing so. There is no constraint
about their expertise in the field of Human Com-
puter Interaction or Spoken Dialogue Systems: They
may be novices or have a high expertise. With ex-
perts or expert raters, we refer to people who are
not participating in the dialogue thus constituting
a completely different set of people. Expert raters
listen to recorded dialogues after the interactions
and rate them by assuming the point of view of the
actual person performing the dialogue. These ex-
perts are supposed to have some experience with di-
alogue systems. In this work, expert raters were ?ad-
vanced students of computer science and engineer-
ing? (Schmitt et al, 2011a).
For User Satisfaction, ratings applied by the users
seem to be clearly the better choice over ratings ap-
plied by third persons. However, determining true
User Satisfaction is only possible by asking real
users interacting with the system. Ideally, the ratings
are applied by users talking to a system employed in
the field, e.g., commercial systems, as these users
have real concerns.
For such Spoken Dialogue Systems, though, it
is not easy to get users to apply quality ratings
to the dialogue ? especially for each system-user-
exchange. The users would have to rate either by
pressing a button on the phone or by speech, which
would significantly influence the performance of the
dialogue. Longer dialogues imply longer call dura-
tions which cost money. Further, most callers only
want to quickly get some information from the sys-
tem. Therefore, it may be assumed that most users
do not want to engage in dialogues which are ar-
tificially made longer. This also inhabits the risk
that users who participated in long dialogues do
not want to call again. Therefore, collecting rat-
ings applied by users are considered to be expensive.
One possible way of overcoming the problem of rat-
ing input would be to use some special installation
which enables the users to provide ratings more eas-
ily (cf. (Schmitt et al, 2011b)). However, this is also
expensive and the system?s usability would be very
restricted. Further, this setup could most likely only
be used in a lab situation.
Expert raters, on the other hand, are able to simply
listen to the recorded dialogues and to apply ratings,
e.g., by using a specialized rating software. This
process is much easier and does not require the same
amount of effort needed for acquiring user ratings.
Further, as already pointed out, we refer to experts
as people who have some basic understanding of di-
alogue systems but are not required to be high-level
experts in the field. That is why we believe that these
people can be found easily.
As both categories of ratings have their advan-
tages and disadvantages, this contribution aims at
learning about the differences and similarities of
user and expert ratings with the ultimate goal of
either being able to predict user ratings more effi-
ciently or of advocating for replacing the use of user
ratings by using only expert ratings in general.
Therefore, this work analyzes the relation be-
tween quality ratings applied by user and expert
raters by analyzing approaches which take advan-
tage of both categories: Using the less expensive
rating process with expert raters and still predict-
ing real User Satisfaction ratings. Moreover, this
works? goal is to shed light on the question whether
information about one rating (in this case the less
expensive expert ratings) may be used to predict the
other rating (the more expensive user ratings). For
this, we present two approaches applying two differ-
ent statistical classification methods for a showcase
corpus. Results of both methods are compared to a
given baseline.
The remainder of this paper is organized as fol-
lows. First, we give a brief overview of work done
in both categories (user ratings vs. expert ratings) in
Section 2 and present our choice of data the analy-
sis in this paper is based on in Section 3. Further,
evaluation metrics are illustrated in Section 4 and
approaches on facilitating prediction of user rater
scores by expert rater information are presented in
Section 5 followed by an evaluation and discussion
of the results in Section 6.
2 Significant Related Work
Predicting User Satisfaction for SDSs has been in
the focus of research for many years, most famously
the PARADISE framework by Walker et al (1997).
The authors assume a linear dependency between
quantitative parameters derived from the dialogue
and US, modeling this dependency using linear re-
570
gression. Unfortunately, for generating the regres-
sion model, weighting factors have to be computed
for each system anew. This generates high costs
as dialogues have to be performed with real users
where each user further has to complete a question-
naire after completing the dialogue. Moreover, in
the PARADISE framework, only quality measure-
ment for the whole dialogue (or system) is allowed.
However, this is not suitable for using quality infor-
mation for online adaption of the dialogue (cf. (Ultes
et al, 2012)). Furthermore, PARADISE relies on
questionnaires while we focus on work using single-
valued ratings.
Numerous work on predicting User Satisfaction
as a single-valued rating task for each system-user-
exchange has been performed in both categories.
This work is briefly presented in the following.
2.1 Expert Ratings
Higashinaka et al (2010a) proposed a model to pre-
dict turn-wise ratings for human-human dialogues
(transcribed conversation) and human-machine di-
alogues (text from chat system). Ratings ranging
from 1-7 were applied by two expert raters label-
ing ?Smoothness?, ?Closeness?, and ?Willingness?
not achieving a Match Rate per Rating (MR/R)1 of
more than 0.2-0.24. This results are only slightly
above the random baseline of 0.14. Further work
by Higashinaka et al (2010b) uses ratings for over-
all dialogues to predict ratings for each system-
user-exchange. Again, evaluating in three user
satisfaction categories ?Smoothness?, ?Closeness?,
and ?Willingness? with ratings ranging from 1-7
achieved best performance of 0.19 MR/R.
Interaction Quality (IQ) has been introduced by
Schmitt et al (2011a) as an alternative performance
measure to User Satisfaction. In their terminology,
US ratings are only applied by users. As their pre-
sented measure uses ratings applied by expert raters,
a different term is used. Each system-user exchange
was annotated by three different raters using strict
guidelines. The ratings ranging from 1-5 are used
as target variable for statistical classifiers using a set
of automatically derivable interaction parameters as
input. They achieve a MR/R of 0.58.
1MR/R is equal to Unweighted Average Recall (UAR)
which is explained in Section 4.
2.2 User Ratings
An approach presented by Engelbrecht et al (2009)
uses Hidden Markov Models (HMMs) to model the
SDS as a process evolving over time. User Satisfac-
tion was predicted at any point within the dialogue
on a 5 point scale. Evaluation was performed based
on labels the users applied themselves during the di-
alogue.
Hara et al (2010) derived turn level ratings from
an overall score applied by the users after the dia-
logue. Using n-gram models reflecting the dialogue
history, the achieved results for recognizing User
Satisfaction on a 5 point scale showed to be hardly
above chance.
Work by Schmitt et al (2011b) deals with deter-
mining User Satisfaction from ratings applied by the
users themselves during the dialogues. A statistical
classification model was trained using automatically
derived interaction parameter to predict User Satis-
faction for each system-user-exchange on a 5-point
scale achieving an MR/R of 0.49.
3 Corpus
The corpus used by Schmitt et al (2011b) not only
contains user ratings but also expert ratings which
makes it a perfect candidate for our research pre-
sented in this paper. Adopting the terminology by
Schmitt et al, user ratings are described as User Sat-
isfaction (US) whereas expert ratings are referred to
with the term Interaction Quality (IQ) (cf. (Schmitt
et al, 2011a)). The data used for all experiments
of this work was collected by Schmitt et al (2011b)
during a lab user study with 38 users in the domain
of the ?Let?s Go Bus Information? system (Raux et
al., 2006) of the Carnegie Mellon University in Pitts-
burgh. 128 calls were collected consisting of a total
of 2,897 system-user exchanges. Both ratings, IQ
and US, are at a scale from 1 to 5 where 1 stands for
?extremely unsatisfied? and 5 for ?satisfied?. Each
dialogue starts with a rating of 5 as the user is ex-
pected to be satisfied in the beginning because noth-
ing unsatisfying has happened yet.
Further, the corpus also provides interaction pa-
rameters which may be used as input variables
for the IQ and US recognition models. These
parameters have been derived automatically from
three dialogue modules: Automatic Speech Recog-
571
s 1
u 1
s 2
u 2
s 3
u 3
s n
u n
?
e 1
e 2
e 3
e n
e n
?
e n-
1
e n-
2
e 1
e 2
e 3
e n+
1
?
ex
ch
an
ge
 le
ve
l p
ara
me
ter
s
wi
nd
ow
 le
ve
l p
ara
me
ter
s: 
{#
}, {
M
ea
n}
, e
tc.
dia
log
ue
 le
ve
l p
ara
me
ter
s: 
#, 
M
ea
n, 
etc
.
Figure 1: The three different modeling levels representing the interaction at exchange en: The most detailed exchange
level, comprising parameters of the current exchange; the window level, capturing important parameters from the
previous n dialog steps (here n = 3); the dialog level, measuring overall performance values from the entire previous
interaction.
nition, Spoken Language Understanding, and Dia-
logue Management. Furthermore, the parameters
are modeled on three different levels (see Figure 1):
? Exchange level parameters can be derived di-
rectly from the respective dialogue modules,
e.g., ASRConfidence.
? Dialogue level parameters consist of counts (#),
means (Mean), etc. of the exchange level pa-
rameters calculated from all exchanges of the
whole dialogue up to the current exchange, e.g.,
MeanASRConfidence.
? Window level parameters consist of counts
({#}), means ({Mean}), etc. of the exchange
level parameters calculated from the last three
exchanges, e.g., {Mean}ASRConfidence.
4 Evaluation metrics
For measuring the performance of the classification
algorithms, we rely on Unweighted Average Recall
(UAR), Cohen?s Kappa and Spearman?s Rho. The
latter two also represent a measure for similarity of
paired data. All measures will be briefly described
in the following:
Unweighted Average Recall The Unweighted Av-
erage Recall (UAR) is defined as the sum of all
class-wise recalls rc divided by the number of
classes |C|:
UAR =
1
|C|
?
c?C
rc . (1)
Recall rc for class c is defined as
rc =
1
|Rc|
|Rc|?
i=1
?hiri , (2)
where ? is the Kronecker-delta, hi and ri rep-
resent the corresponding hypothesis-reference-
pair of rating i, and |Rc| the total number of
all ratings of class c. In other words, UAR
for multi-class classification problems is the ac-
curacy corrected by the effects of unbalanced
data.
Cohen?s Kappa To measure the relative agreement
between two corresponding sets of ratings, the
number of label agreements corrected by the
chance level of agreement divided by the max-
imum proportion of times the labelers could
agree is computed. ? is defined as
? =
p0 ? pc
1? pc
, (3)
where p0 is the rate of agreement and pc is the
chance agreement (Cohen, 1960). As US and
IQ are on an ordinal scale, a weighting factor w
is introduced reducing the discount of disagree-
ments the smaller the difference is between two
ratings (Cohen, 1968):
w =
|r1 ? r2|
|rmax ? rmin|
. (4)
Here, r1 and r2 denote the rating pair and rmax
and rmin the maximal and minimal rating. This
results inw = 0 for agreement andw = 1 if the
ratings have maximal difference.
Spearman?s Rho The correlation of two variables
describes the degree by that one variable can be
expressed by the other. Spearman?s Rank Cor-
relation Coefficient is a non-parametric method
assuming a monotonic function between the
572
two variables (Spearman, 1904). It is defined
by
? =
?
i(xi ? x?)(yi ? y?)??
i(xi ? x?)
2
?
i(yi ? y?)
2
, (5)
where xi and yi are corresponding ranked rat-
ings and x? and y? the mean ranks. Thus, two
sets of ratings can have total correlation even if
they never agree. This would happen if all rat-
ings are shifted by the same value, for example.
5 Recognition of US Using IQ Information
As discussed in Section 1, automatic recognition of
ratings applied by users as performed by Schmitt et
al. (2011b) for User Satisfaction is time-consuming
and expensive. Therefore, approaches are presented
which facilitate expert ratings, i.e., Interaction Qual-
ity, with the hope of making US recognition more
feasible. IQ an US are strongly related as both met-
rics represent the same quantity applied by differ-
ent rater groups. Results of the Mann-Whitney U
test, which is used to test for significant difference
between Interaction Quality and User Satisfaction,
show their difference (p < 0.05) but values for Co-
hen?s Kappa (Cohen, 1960) and Spearman?s Rank
Correlation Coefficient (Spearman, 1904) empha-
size the that IQ and US are quite similar. Achieving
? = 0.5 can be considered as a moderate agreement
according to Landis and Koch?s Kappa Benchmark
Scale (Landis and Koch, 1977). Furthermore, a cor-
relation of ? = 0.66 (p < 0.01) indicates a strong
relationship between IQ and US (Cohen, 1988).
While it has been shown that user and expert rat-
ings are similar, it is desirable nonetheless to being
able to predict real user ratings. These ratings are the
desired kind of ratings when it comes to subjective
dialogue system assessment. Only users can give a
rating about their satisfaction level, i.e., how they
like the system and the interaction with the system.
However, user ratings are expensive as elaborated in
Section 1. Therefore, we investigate approaches to
recognize US which rely on means of IQ recogni-
tion.
5.1 Belief-Based Sequential Recognition
Methods used for IQ and US recognition by Schmitt
et al (2011b; 2011a) suffer from the fact that the
sequential character of the data is modeled inade-
quately as they assume statistical independence be-
tween the single exchanges (recognition of IQ and
US does not depend on the respective value of the
previous exchange). Hence, we present a Marko-
vian approach overcoming these issues. A probabil-
ity distribution over all US states, called belief state,
is updated after each system-user-exchange taking
also into account the belief state of the previous ex-
change. This belief update2 is equivalent to the For-
ward Algorithm known from Hidden Markov Mod-
els (cf. (Rabiner, 1989)). In doing so, the new US
probabilities also depend on the US values of the
previous exchange. Moreover, a latent variable is
introduced in order to decouple the target variable
US with the variable the observation probability de-
pends on IQ. This results in an indirect approach
for recognizing User Satisfaction that is based on the
more affordable recognition of Interaction Quality
assuming that a universal mapping between IQ and
US exists.
Thus, to determine the probability b(US) of hav-
ing the true User Satisfaction label US after the cur-
rent system-user-exchange, we rely on Interaction
Quality recognition, whose observation probability
is depicted as P (o|IQ). Furthermore, for coupling
both quantities, we introduce a coherence probabil-
ity P (IQ|US). Belief update for estimating the new
values for b?(US?) is as follows:
b?(US?) = ? ?
?
IQ?
P (o?|IQ?) ? P (IQ?|US?)
?
?
US
P (US?|US)b(US) (6)
The observation probability P (o?|IQ?) is modeled
using confidence scores of classifiers applied for IQ
recognition. Further, we compute the sum over all
previous US beliefs b(US) weighted by the transi-
tion probability P (US?|US). Both, transition and
coherence probability have been computed by tak-
ing the frequency of their occurrences in the training
data. The ? factor is used for normalization only.
Since we are aiming at generating an estimate U?S
2Terminology is taken from Partially Observable Markov
Decision Processes, cf. (Kaelbling et al, 1998)
573
at each exchange, it is calculated by
U?S = arg max
US?
b?(US?) (7)
generating a sequence of estimates for each dia-
logue.
As the action of the system a can be expected to
influence the satisfaction level of the user, action-
dependency is added to Equation 6 resulting in
b?(US?) = ? ?
?
IQ?
P (o?|IQ?) ? P (IQ?|US?, a)
?
?
US
P (US?|US, a)b(US). (8)
Hence, each system action a influences coherence
and transition probabilities. It should be noted that
action-dependency can only be introduced as in a
SDS each turn a system action is selected and ex-
ecuted by the dialogue manager.
5.2 Model Exchange
While in Belief-Based Sequential Recognition, prob-
ability models are used for coupling expert and user
ratings explicitly, a simpler approach has also been
examined. A statistical classifier trained on the tar-
get variable IQ is used to evaluate classification of
the target variable US. This seems to be reasonable
as the set of scores and meaning of the scores of both
metrics are equivalent. Furthermore, necessary pre-
requisites are fulfilled: the sample corpus contains
both labels, the labels for US and IQ correspond, and
both recognition approaches are based on the same
feature set.
6 Experiments and Results
For evaluating Belief-Based Sequential Recognition,
not only the absolute performance is of interest but
also how this performance is influenced by the char-
acteristics of the observation probability, i.e., the
performance of the applied statistical classification
approach and the variance of their confidence scores.
In order to obtain different confidence characteris-
tics, multiple classification algorithms, or algorithm
variants respectively, are needed. Hence, five statis-
tical classifiers have been chosen arbitrarily to pro-
duce the observation probabilities for Belief-Based
Sequential Recognition:
? SVM3 with cubic kernel
? SVM with RBF-kernel
? Naive Bayes
? Naive Bayes with kernel
? Rule Induction
In contrast to Schmitt et al (2011b; 2011a), a re-
duced feature set was used consisting of 43 parame-
ters as some textual parameters were removed which
are very specific and take many different values, e.g.,
UTTERANCE (the system utterance) or INTERPRE-
TATION (the interpretation of the speech input).
The resulting feature set consists of the following
parameters (parameter names are in accordance with
the parameter names of the LEGO corpus (Schmitt
et al, 2012)):
Exchange Level ACTIVITY, ACTIVITYTYPE,
UTD, BARGED-IN?, ASRCONFIDENCE,
MEANASRCONFIDENCE, TURNNUMBER,
MODALITY, LOOPNAME, ASRRECOGNI-
TIONSTATUS, ROLEINDEX, ROLENAME,
NOISE?, HELPREQUEST?, REPROMPT?,
WPST, WPUT
Dialogue Level #BARGEINS #ASRSUCCESS,
#HELPREQUESTS, #TIMEOUTS, #TIME-
OUTS ASRREJECTIONS, #ASRREJEC-
TIONS, #REPROMPTS, #SYSTEMQUES-
TIONS, #SYSTEMTURNS, #USERTURNS,
%BARGEINS, %ASRSUCCESS, %HEL-
PREQUESTS, %TIMEOUTS, %TIME-
OUTS ASRREJECTIONS, %ASRREJEC-
TIONS, %REPROMPTS
Window Level {#}TIMEOUTS ASRREJCTIONS,
{#}HELPREQUESTS, {#}ASRREJECTIONS,
{MEAN}ASRCONFIDENCE, {#}TIMEOUTS,
{#}REPROMPTS, {#}SYSTEMQUESTIONS,
{#}ASRSUCCESS, {#}BARGEINS
All results are evaluated with respect to the ref-
erence experiment of direct US recognition (US
recognition using models trained on US). This is
performed in accordance to Schmitt et al (2011b)
using the statistical classification algorithms stated
3Support Vector Machine, cf. (Vapnik, 1995)
574
Table 1: Results (UAR, Cohen?s Kappa, and Spearman?s
Rho) of 10-fold cross-validation for US recognition of US
recognition using models trained on US
Classifier UAR ? ?
SVM (cubic Kernel) 0.39 0.33 0.48
SVM (RBF-Kernel) 0.39 0.42 0.55
Naive Bayes 0.36 0.40 0.55
Naive Bayes (Kernel) 0.42 0.44 0.59
Rule Induction 0.50 0.51 0.61
Table 2: Results (UAR, Cohen?s Kappa, and Spearman?s
Rho) of 10-fold cross-validation for US recognition of the
Model Exchange approach (trained on IQ, evaluated on
US)
Classifier UAR ? ?
SVM (cubic Kernel) 0.34 0.42 0.55
SVM (RBF-Kernel) 0.34 0.42 0.58
Naive Bayes 0.35 0.40 0.57
Naive Bayes (Kernel) 0.34 0.37 0.60
Rule Induction 0.34 0.42 0.59
above. The performance of the reference experiment
is shown in Table 1.
Using the same feature set, these classification al-
gorithms are also applied for the evaluation of the
Model Exchange approach using 10-fold cross val-
idation. Note that the parameters of the classifiers
also remained the same. The data was partitioned
randomly on exchange level, i.e., without regarding
their belonging to a specific dialogue. The measured
results of the Model Exchange approach for the five
classification methods can be seen in Table 2.
While the results are significantly above chance4,
comparing them to the reference experiment reveals
that in terms of UAR the reference experiment out-
performs Model Exchange for all five classifiers.
The achieved ? and ? values show similar scores
for both the reference experiment and the Model Ex-
change approach. However, in the data used for the
experiments, the amount of occurrences of the rat-
ings was not balanced (equal for all classes) which
has been identified as the most likely reason for this
effect.
Experiments for Belief-Based Sequential Recog-
nition have also been performed using 10-fold cross
validation. As complete dialogues and the order
4UAR of 0.2 for five classes
Table 3: Results (UAR, Cohen?s Kappa, and Spearman?s
Rho) of 10-fold cross-validation for US recognition of
action-independent Belief-Based Sequential Recognition
Classifier UAR ? ?
SVM (cubic Kernel) 0.28 0.36 0.48
SVM (RBF-Kernel) 0.30 0.40 0.54
Naive Bayes 0.32 0.39 0.54
Naive Bayes (Kernel) 0.33 0.45 0.61
Rule Induction 0.33 0.47 0.63
Table 4: Results (UAR, Cohen?s Kappa, and Spearman?s
Rho) of 10-fold cross-validation for US recognition of
action-dependent Belief-Based Sequential Recognition
Classifier UAR ? ?
SVM (cubic Kernel) 0.28 0.35 0.48
SVM (RBF-Kernel) 0.29 0.40 0.54
Naive Bayes 0.32 0.40 0.55
Naive Bayes (Kernel) 0.34 0.44 0.60
Rule Induction 0.35 0.47 0.62
of exchanges within the dialogues are important for
this approach, the data was partitioned randomly on
the dialogue level. As previously explained, for the
probability distributions of the observation proba-
bility model, classification results of IQ recognition
with 10-fold cross validation has been used in order
to get good estimates for the whole data set. Re-
sults for the action-independent version can be seen
in Table 3.
For the action-dependent version, four different
basic actions ANNOUNCEMENT, CONFIRMATION,
QUESTION, and WAIT have been used, generat-
ing results presented in Table 4. The results il-
lustrate that neither action-independent nor action-
dependent Belief-Based Sequential Recognition can
outperform the reference experiment (cf. Table 1).
Still, both variants achieve results clearly above
chance. Again, the unbalanced data causes ? and
? to be similar to the reference experiment.
A comparison of the action-independent with the
action-dependent approach shows almost no differ-
ences in their performances. Only a slight tendency
towards better UARs for action-dependency can be
spotted.
Figure 2 displays the performances of both vari-
ants of Belief-Based Sequential Recognition along
with performance of IQ recognition and the vari-
ance ?2 of the corresponding confidence distribu-
575
0.200.300.400.500.60 0.000.10
SVM
?(cub
ic)
SVM
?(RBF
)
Baye
s
Baye
s?(K)
Rule
IQ???
IQ
US?B
elief
US?B
elief
?Acti
on
Figure 2: UAR of IQ recognition and Belief-Based Se-
quential Recognition along with ?2 of confidence distri-
butions of IQ recognition
Table 5: Recognition performance and variance of confi-
dence distributions for IQ recognition
Classifier ?2 UAR ? ?
SVM (cubic Kernel) 0.03 0.38 0.54 0.69
SVM (RBF-Kernel) 0.05 0.48 0.65 0.77
Naive Bayes 0.13 0.49 0.57 0.71
Naive Bayes (Kernel) 0.12 0.52 0.59 0.73
Rule Induction 0.13 0.55 0.68 0.79
tion (cf. Table 5). It can easily be seen that with
rising UAR for IQ recognition, ?2 also rises. This
directly transfers to the performance of the Belief-
Based Sequential Recognition. The more accu-
rate the observation performance, the more accurate
the belief prediction. Furthermore, when compar-
ing the action-dependent to the action-independent
variant of Belief-Based Sequential Recognition, bet-
ter IQ performance and therefore a higher variance
also causes slightly better results for the action-
dependent variant. These differences, however, are
only marginally. Therefore, they do not allow for
drawing a conclusion.
7 Conclusions
For estimating User Satisfaction-like ratings, two
categories exist: work relying on user ratings and
work relying on expert ratings. To learn something
about their differences and similarities, we explored
the possibility of using the information encoded in
the expert ratings to predict user ratings with the
hope to get acceptable user rating prediction results.
Therefore, we investigated if it is possible to de-
termine the preferred true User Satisfaction value
based on less expensive expert ratings. For this, a
corpus containing both kinds of ratings was chosen,
i.e., User Satisfaction (US) and Interaction Qual-
ity (IQ) ratings. Furthermore, interaction parame-
ters were used to create statistical recognition mod-
els for predicting IQ and US, respectively. Two ap-
proaches have been investigated: Belief-Based Se-
quential Recognition, which is based on an HMM-
like structure with IQ as an additional latent variable,
and Model Exchange, which uses statistical models
trained on IQ to recognize US. Unfortunately, nei-
ther Belief-Based Sequential Recognition nor Model
Exchange achieved results with an acceptable UAR.
The high correlation between expert and user rat-
ings, depicted by high values for Cohen?s ? and
Spearman?s ?, already allow the conclusion that ex-
pert ratings can be used as a good replacement for
user ratings. Moreover, the presented recognition re-
sults of the Model Exchange approach being clearly
above chance underpin the strong similarity of IQ
and US. Furthermore, IQ recognition is much more
reliable and accurate than US recognition (shown by
higher UAR, ? and ? values).
While the experiments disproved the hope of get-
ting acceptable user rating prediction results, the ob-
tained results confirmed the similarity between both
kinds of ratings. And as it is not necessary to use
user ratings for most applications, e.g., for using the
quality information to automatically improve the in-
teraction (cf. (Ultes et al, 2012)), we believe that it
suffices to use expert ratings as those can be acquired
easier and less expensively and are similar enough
to user ratings. Prompting the user to apply quality
ratings in everyday situations with real-life systems
will always be annoying to the user while recording
of such interactions are always much easier to rate.
By providing a study for determining quality rat-
ings of dialogues, we hope to encourage other re-
searchers to look into this research for other param-
eters, e.g., emotion recognition.
References
Jacob Cohen. 1960. A coefficient of agreement for nom-
inal scales. In Educational and Psychological Mea-
surement, volume 20, pages 37?46, April.
Jacob Cohen. 1968. Weighted kappa: Nominal scale
576
agreement provision for scaled disagreement or partial
credit. Psychological bulletin, 70(4):213.
Jacob Cohen. 1988. Statistical power analysis for the
behavioral sciences. New Jersey: Lawrence Erlbaum
Associates, July.
Klaus-Peter Engelbrecht and Sebastian Mo?ller. 2010. A
User Model to Predict User Satisfaction with Spoken
Dialog Systems. In Gary Geunbae Lee, Joseph Mari-
ani, Wolfgang Minker, and Satoshi Nakamura, editors,
Spoken Dialogue Systems for Ambient Environments.
2nd Int. Workshop on Spoken Dialogue Systems Tech-
nology, Lecture Notes in Artificial Intelligence, pages
150?155. Springer, October.
Klaus-Peter Engelbrecht, Florian Go?dde, Felix Hartard,
Hamed Ketabdar, and Sebastian Mo?ller. 2009. Mod-
eling user satisfaction with hidden markov model. In
SIGDIAL ?09: Proceedings of the SIGDIAL 2009 Con-
ference, pages 170?177, Morristown, NJ, USA. Asso-
ciation for Computational Linguistics.
Sunao Hara, Norihide Kitaoka, and Kazuya Takeda.
2010. Estimation method of user satisfaction using
n-gram-based dialog history model for spoken dialog
system. In Nicoletta Calzolari (Conference Chair),
Khalid Choukri, Bente Maegaard, Joseph Mariani,
Jan Odijk, Stelios Piperidis, Mike Rosner, and Daniel
Tapias, editors, Proceedings of the Seventh conference
on International Language Resources and Evaluation
(LREC?10), Valletta, Malta, May. European Language
Resources Association (ELRA).
Ryuichiro Higashinaka, Yasuhiro Minami, Kohji
Dohsaka, and Toyomi Meguro. 2010a. Issues in
predicting user satisfaction transitions in dialogues:
Individual differences, evaluation criteria, and predic-
tion models. In Gary Lee, Joseph Mariani, Wolfgang
Minker, and Satoshi Nakamura, editors, Spoken
Dialogue Systems for Ambient Environments, volume
6392 of Lecture Notes in Computer Science, pages
48?60. Springer Berlin / Heidelberg.
Ryuichiro Higashinaka, Yasuhiro Minami, Kohji
Dohsaka, and Toyomi Meguro. 2010b. Modeling
user satisfaction transitions in dialogues from over-
all ratings. In Proceedings of the SIGDIAL 2010
Conference, pages 18?27, Tokyo, Japan, September.
Association for Computational Linguistics.
Kate S. Hone and Robert Graham. 2000. Towards a tool
for the subjective assessment of speech system inter-
faces (sassi). Nat. Lang. Eng., 6(3-4):287?303.
L. P. Kaelbling, M. L. Littman, and A. R. Cassandra.
1998. Planning and acting in partially observable
stochastic domains. Artificial Intelligence, 101(1-
2):99?134.
J. R. Landis and G. G. Koch. 1977. The measurement of
observer agreement for categorical data. Biometrics,
33(1):159?174, March.
Sebastian Mo?ller, Klaus-Peter Engelbrecht, C. Ku?hnel,
I. Wechsung, and B. Weiss. 2009. A taxonomy of
quality of service and quality of experience of multi-
modal human-machine interaction. In Quality of Mul-
timedia Experience, 2009. QoMEx 2009. International
Workshop on, pages 7?12, July.
Sebastian Mo?ller. 2003. Subjective Quality Evalua-
tion of Telephone Services Based on Spoken Dia-
logue Systems. ITU-T Recommendation P.851, Inter-
national Telecommunication Union, Geneva, Switzer-
land, November. Based on ITU-T Contr. COM 12-59
(2003).
Tim Polzehl, Alexander Schmitt, and Florian Metze.
2011. Salient features for anger recognition in german
and english ivr portals. In Wolfgang Minker, Gary Ge-
unbae Lee, Satoshi Nakamura, and Joseph Mariani,
editors, Spoken Dialogue Systems Technology and De-
sign, pages 83?105. Springer New York. 10.1007/978-
1-4419-7934-6 4.
Lawrence R. Rabiner. 1989. A tutorial on hidden Markov
models and selected applications in speech recogni-
tion. Morgan Kaufmann Publishers Inc., San Fran-
cisco, CA, USA.
Antoine Raux, Dan Bohus, Brian Langner, Alan W.
Black, and Maxine Eskenazi. 2006. Doing research
on a deployed spoken dialogue system: One year of
lets go! experience. In Proc. of the International Con-
ference on Speech and Language Processing (ICSLP),
September.
Alexander Schmitt, Benjamin Schatz, and Wolfgang
Minker. 2011a. Modeling and predicting quality in
spoken human-computer interaction. In Proceedings
of the SIGDIAL 2011 Conference, Portland, Oregon,
USA, June. Association for Computational Linguis-
tics.
Alexander Schmitt, Benjamin Schatz, and Wolfgang
Minker. 2011b. A statistical approach for estimat-
ing user satisfaction in spoken human-machine inter-
action. In Proceedings of the IEEE Jordan Confer-
ence on Applied Electrical Engineering and Comput-
ing Technologies (AEECT), Amman, Jordan, Decem-
ber. IEEE.
Alexander Schmitt, Stefan Ultes, and Wolfgang Minker.
2012. A parameterized and annotated corpus of the
cmu let?s go bus information system. In International
Conference on Language Resources and Evaluation
(LREC).
C. Spearman. 1904. The proof and measurement of as-
sociation between two things. American Journal of
Psychology, 15:88?103.
Stefan Ultes, Alexander Schmitt, and Wolfgang Minker.
2012. Towards quality-adaptive spoken dialogue man-
agement. In NAACL-HLT Workshop on Future di-
rections and needs in the Spoken Dialog Commu-
577
nity: Tools and Data (SDCTD 2012), pages 49?52,
Montre?al, Canada, June. Association for Computa-
tional Linguistics.
Vladimir N. Vapnik. 1995. The nature of statistical
learning theory. Springer-Verlag New York, Inc., New
York, NY, USA.
Marilyn Walker, Diane Litman, Candace A. Kamm, and
Alicia Abella. 1997. Paradise: a framework for eval-
uating spoken dialogue agents. In Proceedings of the
eighth conference on European chapter of the Associ-
ation for Computational Linguistics, pages 271?280,
Morristown, NJ, USA. Association for Computational
Linguistics.
578
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 261?264,
The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics
Advances in the Witchcraft Workbench Project
Alexander Schmitt, Wolfgang Minker
Institute for Information Technology
University of Ulm, Germany
alexander.schmitt,
wolfgang.minker@uni-ulm.de
Nada Ahmed Hamed Sharaf
German University in Cairo, Egypt
nada.sharaf@student.guc.edu.eg
Abstract
The Workbench for Intelligent exploraTion
of Human ComputeR conversaTions is
a new platform-independent open-source
workbench designed for the analysis, min-
ing and management of large spoken di-
alogue system corpora. What makes
Witchcraft unique is its ability to visual-
ize the effect of classification and predic-
tion models on ongoing system-user inter-
actions. Witchcraft is now able to handle
predictions from binary and multi-class
discriminative classifiers as well as regres-
sion models. The new XML interface al-
lows a visualization of predictions stem-
ming from any kind of Machine Learning
(ML) framework. We adapted the wide-
spread CMU Let?s Go corpus to demon-
strate Witchcraft.
1 Introduction
Substantial effort has been invested in the past
years in exploring ways to render Spoken Di-
alogue Systems (SDS) more adaptive, natural
and user friendly. Recent studies investigated
the recognition of and adaption to specific user
groups, e.g. the novices and expert users, or
the elderly (Bocklet et al, 2008). Further, there
is a massive effort on recognizing angry users,
differentiate between genders (Burkhardt et al,
2007), spotting dialects, estimating the coopera-
tiveness of users or user satisfaction (Engelbrecht
et al, 2009) and finally, predicting task comple-
tion (Walker et al, 2002). When applied online,
i.e. during the interaction between user and sys-
tem, these models can add valuable information
to the dialogue system which would allow for an
adaption of the dialogue strategy, see Figure 1.
Until now we can report that these models1
1please note that we use the expression recognizer, classi-
Speech Recognition Parsing
Dialogue Manager
Text Generation and Synthesis
ApplicationUse
r Prediction Model
Figure 1: Enhanced SDS: The prediction model
that is used to render the dialogue system more
user-friendly delivers additional information to the
dialogue manager.
work more or less well in batch-test scenarios of-
fline. An anger classifier might deliver 74% accu-
racy when evaluated on utterance level. But which
impact would the deployment of this recognizer
have on specific dialogues when being employed
in a real system? Would it fail or would it suc-
ceed? Similarly, at what point in time would mod-
els predicting gender, speaker age, and expert sta-
tus deliver a reliable statement that can indeed be
used for adapting the dialogue? What we need
prior to deployment is an evaluation of the mod-
els and a statement on how well the models would
work when being shifted on dialogue level. At this
point, the Witchcraft Workbench enters the stage.
2 The Role of Witchcraft
For a more detailed introduction on the Witchcraft
Workbench please refer to (Schmitt et al, 2010a).
In a nutshell, Witchcraft allows managing, mining
and analyzing large dialogue corpora. It brings
logged conversations back to life in such that it
simulates the interaction between user and sys-
tem based on system logs and audio recordings.
Witchcraft is first of all not an annotation or tran-
scription tool in contrast to other workbenches
such as NITE (Bernsen et al, 2002), Transcriber2
fier and prediction model interchanging in this context
2http://trans.sourceforge.net
261
or DialogueView3. Although we also employ it
for annotation, its central purpose is a different
one:Witchcraft contrasts dialogue flows of spe-
cific dialogues which are obtained from a dialogue
corpus with the estimations of arbitrary predic-
tion and classification models. By that it is in-
stantly visible which knowledge the dialogue sys-
tem would have at what point in time in the dia-
logue. Imagine a dialogue system would be en-
dowed with an anger recognizer, a gender recog-
nizer and a recognizer that should predict the out-
come of a dialogue, i.e. task completion. Each of
the three recognizers would be designed to deliver
an estimation at each point in the dialogue. How
likely is the user angry? How likely is he male or
female and how likely will the task be completed
based on what we have seen so far in the dialogue.
To which extent the recognizers deliver a correct
result can be verified within Witchcraft.
3 Handling Models in Witchcraft
Witchcraft had several shortcomings when we first
reported on it in (Schmitt et al, 2010a). It was
only working with a proprietary industrial corpus
and was heavily tailored to our needs. It worked
only with specific models from binary discrimina-
tive classifiers. Since then we have put substantial
effort to generalize the functionality and to make
it available to the community.
To allow an analysis of other recognizers the
system has been extended to further handle pre-
dictions from multiclass discriminative classifica-
tion and regression tasks. Witchcraft does not con-
tain ?intelligence? on its own but makes use of
and manages the predictions of recognizers. We
assume that a recognizer is implemented either
as stand-alone recognizer or with help of a Ma-
chine Learning (ML) framework. We emphasize
that Witchcraft itself does neither perform fea-
ture extraction nor classification. Witchcraft op-
erates on turn level requesting the recognizer to
deliver a prediction based on information avail-
able at the currently processed dialogue turn of
a specific dialogue. Where and how the recog-
nizer accomplishes this is not part of the archi-
tecture. The ML framework of our choice that
was originally supported natively, i.e. directly ac-
cessed by Witchcraft (Schmitt et al, 2010a) was
RapidMiner4, an ML framework that covers a vast
3http://cslu.cse.ogi.edu/DialogueView/
4www.rapid-i.net
majority of supervised and unsupervised machine
learning techniques. The initial plan to interface
other ML frameworks natively (such as MatLab,
the R framework, BoosTexter, Ripper, HTK that
are frequently used in research) turned out not to
be practical. In order to still be able to cover the
broadest possible range of ML tools we introduced
a new generic XML interface. For simplicity we
removed the RapidMiner interface. An overview
of the dependency between Witchcraft and a rec-
ognizer is depicted in Figure 2.
ML Fra
mework Rec
ogn
izer
Witchc
raft
read
s
Dia
log
ue
ID
 78
74
3
Disp
lays
 
disc
ours
e
Chart
 Views
Dial
ogu
e 
View
Dia
log
ue
ID
 93
12
3
???? ???? ???? ???? ???.
Dial
ogu
e Flo
w
Use
r Sa
tisfac
tion
read
s
...
Da
tab
as
e
Log D
ata 
&
Aco
ustic
Featu
res
Dia
log
ue
ID
 67
34
3
Get
 pre
dicti
on
XML Pred
ictio
ns
Per 
turn
 & 
dialo
gue
Pred
ictio
n an
d 
Class
ificat
ion 
Mod
els
Prep
roce
ssin
g
Class
ificat
ion
Featu
re 
Retr
ieval
Di
alo
gu
e C
or
pu
s 
Inter
actio
n Log
s
Aud
io File
s
Aud
io Fe
atur
es
Witchc
raft
Disp
lays
 
Estimat
ionsDisp
lays
 
Dial
ogu
e Flo
w
gen
erat
es
read
s
Figure 2: Dependency of Witchcraft and related
recognizers that are implemented within an ML
framework.
Witchcraft has been extended to support an ar-
bitrary number of models, see Figure 3. They can
now be one of the types ?discriminative bin ry?,
?discriminative multiclass classification? and ?re-
gression?.
Figure 3: Definition of a model within Witchcraft.
External recognizers have to deliver predictions
for the defined models as XML documents.
A recognizer implemented in an ML framework
has to be defined in such a way that it delivers
XML documents that fit the model definition in
Witchcraft. Each XML document represents the
prediction of the recognizer for a specific dialogue
turn of a specific dialogue. It contains for discrimi-
native classification tasks, such as gender, or emo-
tion the number of the turn that has been classified,
262
the actual class label and the confidence scores of
the classifier.
<xml>
<turn>
<number>1</number>
<label>anger</label>
<prediction>non-anger</prediction>
<confidence class=?anger?>0.08</confidence>
<confidence class=?no-ang?>0.92</confidence>
</turn>
</xml>
In regression tasks, such as the prediction of
user satisfaction, retrieving cooperativeness scores
etc., the returned result contains the turn number,
the actual label and the prediction of the classifier:
<xml>
<turn>
<number>1</number>
<label>5</label>
<prediction>3.4</prediction>
</turn>
</xml>
After performing recognition on a number of di-
alogues with the recognizer Witchcraft reads in the
XML files and creates statistics based on the pre-
dictions and calculates dialogue-wise accuracy,
f-score, precision and recall values, root mean
squared error etc. The values give some indica-
tion of how precisely the classifier worked on dia-
logue level. That followed it allows to search for
dialogues with a low overall prediction accuracy,
or e.g. dialogues with high true positive rates, high
or low class-wise f-scores etc. via SQL. Now a de-
tailed analysis of the recognizer?s performance on
dialogue level and possible reasons for the failure
can be spotted.
4 Evaluating Models
In Figure 4 we see prediction series of two rec-
ognizers that have been applied on a specific dia-
logue: a gender recognizer that predicts the gen-
der on turn basis and an emotion recognizer that
predicts the user?s emotional state (angry vs. non-
angry) at the current turn. The red line symbol-
izes the confidence of the recognizers for each of
the predicted classes. For example, in the emotion
model the blue line is the confidence for a non-
angry utterance (0-100%), the red line for an an-
gry one. Exemplary for the two models we take
a closer look at the gender model. It predicts the
gender on turn basis, i.e. it takes the current speech
sample and delivers estimations on the speaker?s
gender. As we can see, there are a number of
misrecognitions in this call. It stems from a fe-
male speaker but the recognizer frequently esti-
Figure 4: Screenshot of charts in Witchcraft based
on turn-wise predictions an anger and a gender
recognizer.
mated a male speaker. The call could be spot-
ted by searching within Witchcraft for calls that
yield a low accuracy for gender. It turned out that
the misrecognized turns originate from the fact
that the user performed off-talk with other persons
in the background which caused the misrecogni-
tion. This finding suggests training the gender
recognizer with non-speech and cross-talk sam-
ples in order to broaden the recognition from two
(male, female) to three (male, female, non-speech)
classes. Further it appears sensitive, to create a
recognizer that would base its recognition on sev-
eral speech samples instead of one, which would
deliver a more robust result.
5 Portability towards other Corpora
Witchcraft has now been extended to cope with
an unlimited number of corpora. An integration
of new corpora is straight-forward. Witchcraft
requires an SQL database containing two tables.
The dialogues table hosts information on the over-
all dialogues (such as the dialogue ID, the cat-
egory, filename of complete recording) and the
exchanges table containing the turn-wise interac-
tions (dialogue ID, turn number, system prompt,
ASR parse, ASR confidence, semantic interpreta-
tion, hand transcription, utterance recording file,
barged in, etc.). Both tables are linked through a
1 : n relationship, i.e. one entry in the dialogues
table relates to n entries in the interactions table,
cf. Figure 5. To demonstrate portability and in
order to create a sample corpus that is deployed
with Witchcraft, we included the CMU Let?s Go
bus information system from 2006 as demo cor-
pus (Raux et al, 2006). It contains 328 dialogues
including full recordings. The Witchcraft project
includes a parser that allows to transform raw log
data from the Let?s Go system into the Witchcraft
table structure.
263
dialogues
PK, FK1 CallerID
CategoryGenderAgeAudioFile...
exchanges
CallerID
SystemPrompt
ASRTranscriptASRConfidenceSemanticInterpretationHandTranscriptRecognitionStatusBargedInAudioFile...
Figure 5: Dialogue and exchanges table with 1:n
relationship. Bold database columns are required,
others are optional.
6 Conclusion and Discussion
Witchcraft turned out to be a valuable framework
in our everyday work when dealing with large di-
alogue corpora. At the current stage several stu-
dents are working with it in multi-user mode to
listen, analyze and annotate dialogues from three
different corpora consisting of up to 100,000 di-
alogues each. Witchcraft allows them to search
for dialogues relevant to the current task. The
SQL-based access allows a powerful and standard-
ized querying and retrieval of dialogues from the
database. Witchcraft provides an overview and
presents decisive information about the dialogue at
one glance and allows to sort and group different
types of dialogue for further research. Moreover,
Witchcraft allows us to test arbitrary recognizers
that provide additional information to the dialogue
manager. Witchcraft tells us at which point in time
a dialogue system would possess which knowl-
edge. Further it allows us to conclude the relia-
bility of this knowledge for further employment
in the dialogue. For an evaluation of recognizers
within Witchcraft please refer to (Schmitt et al,
2010b) where the deployment of an anger recog-
nizer is simulated.
Witchcraft is now freely and publically avail-
able to the community. It is hosted under
GNU General Public License at Sourceforge un-
der witchcraftwb.sourceforge.org. The employed
component architecture allows for the develop-
ment of third-party plug-ins and components for
Witchcraft without the need for getting into detail
of the existing code. This facilitates the extension
of the workbench by other developers. We hope
that Witchcraft will help to foster research on fu-
ture dialogue systems and we encourage the com-
munity to contribute.
Acknowledgements
The research leading to these results has re-
ceived funding from the Transregional Collabora-
tive Research Centre SFB/TRR 62 ?Companion-
Technology for Cognitive Technical Systems?
funded by the German Research Foundation
(DFG). The authors would like to thank the CMU
Let?s Go Lab from Carnegie Mellon University
in Pittsburgh for their permission to deploy the
Let?s Go Bus Information Corpus jointly with
Witchcraft.
References
Niels Ole Bernsen, Laila Dybkjaer, and Mykola Kolod-
nytsky. 2002. The nite workbench - a tool for anno-
tation of natural interactivity and multimodal data.
In Proc. of LREC, pages 43?49, Las Palmas, Spain.
Tobias Bocklet, Andreas Maier, Josef Bauer, Felix
Burkhardt, and Elmar No?th. 2008. Age and gen-
der recognition for telephone applications based on
gmm supervectors and support vector machines. In
Proc. of ICASSP, volume 1, pages 1605?1608.
Felix Burkhardt, Florian Metze, and Joachim
Stegmann, 2007. Speaker Classification for
Next Generation Voice Dialog Systems. Advances
in Digital Speech Transmission. Wiley.
Klaus-Peter Engelbrecht, Florian Go?dde, Felix Har-
tard, Hamed Ketabdar, and Sebastian Mo?ller. 2009.
Modeling user satisfaction with hidden markov
model. In Proc. of SIGDIAL 2009, pages 170?177.
Antoine Raux, Dan Bohus, Brian Langner, Alan W.
Black, and Maxine Eskenazi. 2006. Doing research
on a deployed spoken dialogue system: One year
of lets go! experience. In Proc. of Interspeech,
September.
Alexander Schmitt, Gregor Bertrand, Tobias Heinroth,
and Jackson Liscombe. 2010a. Witchcraft: A work-
bench for intelligent exploration of human computer
conversations. In Proc. of LREC, Valetta, Malta,
May.
Alexander Schmitt, Tim Polzehl, and Wolfgang
Minker. 2010b. Facing reality: Simulating deploy-
ment of anger recognition in ivr systems. In Proc. of
IWSDS, September.
Marilyn Walker, I Langkilde-Geary, H W Hastie,
J Wright, and A Gorin. 2002. Automatically train-
ing a problematic dialogue predictor for a spoken
dialogue system. Journal of Artificial Intelligence
Research, (16):293?319.
264
Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 173?184,
Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational Linguistics
Modeling and Predicting Quality in Spoken Human-Computer Interaction
Alexander Schmitt, Benjamin Schatz and Wolfgang Minker
Dialogue Systems Research Group
Institute for Information Technology
Ulm University, Germany
{alexander.schmitt, benjamin.schatz, wolfgang.minker}@uni-ulm.de
Abstract
In this work we describe the modeling and
prediction of Interaction Quality (IQ) in Spo-
ken Dialogue Systems (SDS) using Support
Vector Machines. The model can be employed
to estimate the quality of the ongoing inter-
action at arbitrary points in a spoken human-
computer interaction. We show that the use
of 52 completely automatic features character-
izing the system-user exchange significantly
outperforms state-of-the-art approaches. The
model is evaluated on publically available data
from the CMU Let?s Go Bus Information sys-
tem. It reaches a performance of 61.6% un-
weighted average recall when discriminating
between 5 classes (good to very poor). It can
be further shown that incorporating knowl-
edge about the user?s emotional state does
hardly improve the performance.
1 Introduction
For years, the research community has been trying
to model quality of Spoken Dialogue Systems (SDS)
with statistical approaches. Most vividly discussed
has been the PARADISE approach which tries to
map objective performance metrics of an SDS to
subjective user ratings (Walker et al, 2000). The
paradigm assumes that task success and dialogue
costs contribute to user satisfaction which is the tar-
get variable in the model. By that, an automatic eval-
uation of an SDS should be enabled. While the in-
tention of PARADISE is to evaluate and compare
SDS or different system versions among each other,
it is not suited to evaluate a spoken dialogue at ar-
bitrary points during an interaction. Such a model
can be helpful for a number of reasons: Firstly,
it allows for a prediction of critical dialogue sit-
uations. These predictions could be employed to
adapt the dialogue strategy or - in telephone appli-
cations with human assistance - escalate to human
operators. Secondly, it could help to uncover poten-
tially weak dialogue design and point out problem-
atic turns that need a re-design. Thirdly, user sat-
isfaction models help understand the satisfaction of
the users. In this study we present such a statisti-
cal model that is trained with a large set of domain-
independent features taken from system logs and use
additional manually created features, such as emo-
tional state and dialogue acts, to create an upper
baseline.
This paper is organized as follows: In Section 2
we present related work and discuss afterwards in
Section 3 further issues that need to be addressed in
this field. There, we also disambiguate the term user
satisfaction from Interaction Quality. After that, we
describe the annotation scheme as well as the rating
process for modeling IQ and present, how we derive
a generic label from the different raters? opinions in
Section 4. The input feature groups along with their
features are presented in Section 5. We anticipate
that the problem is best modeled with Support Vec-
tor Machines (SVM), which is addressed in Section
6. Ensuing, the performance of the model is evalu-
ated. In the first place, we analyze the impact of dif-
ferent feature groups on the SVM classifier in Sec-
tion 7 and secondly, we optimize the model and de-
termine the most relevant features for predicting the
IQ score in Section 8. A linear modeling approach
of IQ by use of multivariate linear regression will be
173
presented and discussed in Section 9 to obtain com-
parability with PARADISE. This study closes with
a conclusion and a discussion in Section 10.
2 Related Work
Models predicting user satisfaction at any point in
an SDS have only been deficiently explored to date.
(Engelbrecht et al, 2009) modeled user satisfac-
tion as process evolving over time with Hidden
Markov Models (HMM). In the experiment, users
were asked to interact with a Wizard-of-Oz restau-
rant information system. Each participant followed
dialogues which have previously been defined fol-
lowing predefined scripts, i.e. specific scenarios.
This resulted in equally long dialogue transcripts
for each scenario. The users were constrained to
rate their satisfaction on a 5-point scale with ?bad?,
?poor?, ?fair?, ?good? and ?excellent? after each di-
alogue step. The interaction was halted while the
user voted.
In a similar spirit, (Higashinaka et al, 2010a)
developed a model for predicting turn-wise rat-
ings, which was evaluated on human-machine and
human-human dialogues. The data employed was
not spoken dialogue but text dialogues from a chat
system and a transcribed conversation between hu-
mans. The labels in the model originated from
two expert raters that listened to the recorded in-
teractions and provided turn-wise scores from 1-7
on smoothness (?Smoothness of the conversation?),
closeness (?Closeness perceived by the user towards
the system?) and willingness (?Willingness to con-
tinue the conversation?). Rater-independent perfor-
mance scores of the model reached about 0.2-0.24
unweighted average recall, which is about 0.1 points
above the baseline of app. 0.14.
(Hara et al, 2010) created n-gram models from
dialogue acts (DA) to predict user satisfaction based
on dialogues from real users interacting with a music
retrieval system. The model is based on overall rat-
ings from the users measuring their satisfaction on a
five point scale after the interaction. The best result
could be achieved with a 3-gram model that reached
34% accuracy in distinguishing between six classes
at any point in the dialogue. It seems that the predic-
tion of turn-level user satisfaction scores given only
one overall dialogue-level score seems hardly possi-
ble and is close to random: The prediction of the five
user satisfaction classes reach an average F-score as
low as 0.252, which is only 0.052 score points above
the baseline of 0.20. A similar result as (Hara et al,
2010) was obtained by (Higashinaka et al, 2010b).
Using HMMs they derived turn-level ratings from
dialogue-wide ratings. The model?s performance
when trained on dialogue-level ratings was closer to
random than when trained on turn-level ratings. The
open issues that arise from the cited work are ad-
dressed in the following.
3 Issues
Our aim is to create a general model that may be
used to predict the quality of the interaction - or
ideally the actual satisfaction of the user - at arbi-
trary system-user exchanges in an SDS. It has be-
come obvious from the cited work that current mod-
els are not suited for deployment due to low predic-
tion accuracy. Crucial for a successful recognition
of user satisfaction is the choice and appropriateness
of the input variables. (Higashinaka et al, 2010a),
(Higashinaka et al, 2010b) and (Hara et al, 2010)
employ a - mostly hand annotated - ?dialogue act?
feature to predict the target variable. Dialogue acts
are frequently highly system-dependent and do not
model the full bandwidth of the interaction. (Engel-
brecht et al, 2009) additionally employed contex-
tual appropriateness, confirmation strategy and task
success, of which many require hand annotation. Yet
it is mandatory for an automatic prediction of user
satisfaction to design and derive completely auto-
matic features that do not require manual interven-
tion. It is further easy to comprehend that the mod-
eling of user satisfaction in ongoing dialogues starts
with a dilemma: tracking user satisfaction from real
users in real environments performing real tasks is
virtually impracticable. Consequently data for de-
riving models can either be obtained under labora-
tory conditions with real users performing fake tasks
in an artificial environment, cf. (Engelbrecht et al,
2009), or by manual annotation of real-life data from
experts that pretend to be the users.
It is thus vital for modeling ?user satisfaction? to
understand the term itself. In the literature there ex-
ists no rigorous definition, however, it seems obvi-
ous that it is the user himself who determines the
174
satisfaction - and not expert annotators. Accord-
ing to (Doll and Torkzadeh, 1991) ?user satisfac-
tion? is the opinion of users about a specific com-
puter application, which they use. Other terms for
?user satisfaction? are common, such as ?user infor-
mation satisfaction?, which is defined as ?the extent
to which users believe the information system avail-
able to them meets their information requirements?
(Ives et al, 1983). User satisfaction and usability are
closely interwoven. (ISO, 1998) subsumes under the
definition ?usability? a compound of efficiency, ef-
fectiveness and satisfaction. Yet satisfaction is often
seen as a by-product of great usability in HCI lit-
erature (Lindgaard and Dudek, 2003). They could
also show that user satisfaction ratings are subject to
large fluctuations among different users and it can be
further assumed that those fluctuations do also occur
within a single dialogue of a user. As a result, gen-
eral prediction models that mirror a universal, un-
biased understanding of satisfaction can presumably
hardly be derived from user?s impressions. Large
influence of subjectivity - and also randomness in
assigning the scores - would prevent such a general
model. Consequently, it seems unavoidable to em-
ploy expert annotations. In the proper meaning of
the word, the scores then do not exactly mirror the
subjective impression of users but the more objec-
tive impression of expert raters.
Thus we decide against the use of the term user
satisfaction in the course of this work in contrast to
(Higashinaka et al, 2010a) and instead opt for the
expression Interaction Quality. It can be assumed
that basic attitudes towards dialogue systems in gen-
eral, opinions about the TTS voice, environmental
factors etc. that would typically influence user satis-
faction scores, and which are not of interest for our
prediction, are not dominant in expert satisfaction
scores in a series of annotated dialogues. Experts
are expected to fade out such system-dependent and
environment-dependent influences and instead focus
on the dialogue behavior (i.e. the Interaction Qual-
ity) only.
As a result, two key issues are addressed in this
work: First of all, the input feature set has to be de-
signed as a generic, domain-independent set that can
be derived from any spoken dialogue system log and
that takes into account a maximum of available in-
formation about the interaction. Secondly, the tar-
get variable, i.e. the IQ score, needs to be deter-
mined in a guided rating process in order to be re-
producible in future work and has to be empirically
derived from several expert annotators that provide
scores for each single system-user turn of an inter-
action.
4 Corpus Annotation
For our study we employ data from the Let?s Go
Bus information system (Raux et al, 2006). Three
raters, advanced students of computer science and
engineering, annotated respectively 200 dialogues
comprising 4885 system-user exchanges from the
2006 corpus. The raters were asked to annotate the
quality of the interaction at each system-user ex-
change with the scores 5 (very good), 4 (good), 3
(fair), 2 (poor) and 1 (very poor). Every dialogue
is initially rated with a score of 5 since every inter-
action at the beginning can be considered as good
until the opposite eventuates. Our model assumes
that users are initially interacting with an SDS with-
out bias, i.e. the basic attitude towards a dialogue
system is positive. Other assumptions would not be
statistically predictable. An example dialogue is de-
picted in Table 5 along with the ratings (cf. Figure 2
in the Appendix). (Higashinaka et al, 2010b) and
(Higashinaka et al, 2010a) report low correlation
among the ratings (Spearman?s ? 0.04-0.32), which
motivated us to develop a set of basic guidelines that
should be used by the raters (cf. Table 6 in the Ap-
pendix). The guidelines have been designed in such
a way that the raters still have sufficient level of free-
dom when choosing the labels but preventing them
from too strong variations among the neighboring
system-user exchanges.
The distribution of the labels provided by the sin-
gle raters is depicted in Figure 3. As expected, the
distribution is skew towards label ?5? since every di-
alogue initially is assumed to have a good IQ.
The inter-rater agreement shows that Interaction
Quality is still a subjective metric, although guide-
lines seem to synchronize the labels to a certain ex-
tent. The overall mean agreement can be reported
with Cohen?s ? = 0.31 and the correlation among
the raters can be reported with Spearman?s ? = 0.72
which depicts a by 0.4 points higher correlation as
reported by (Higashinaka et al, 2010a). Since we
175
aim to model a general opinion on Interaction Qual-
ity, i.e. the model should mirror the IQ score other
raters - and in the last instance users - agree with,
we determine the final label empirically. A major-
ity voting for the distinction of the final label can-
not be used since in 21% of the exchanges all three
raters opted for different scores. Thus we consider
the mean of all rater opinions as possible candidates
for the final class label:
ratingmean = b
(
1
R
R?
r=1
IQr
)
+ 0.5c
where IQ is the Interaction Quality score pro-
vided by rater r. byc denotes the biggest integer
value smaller than y. Every value IQr contributes
equally to the result that is finally rounded half up to
an integer value. Furthermore we consider the me-
dian, which we define as
ratingmedian = select(sort(IQR),
R+ 1
2
)
for an odd number of raters R, where sort is a func-
tion that orders the ratings of all raters ascending and
select(X = [x1, ..., xn], i) chooses the item with in-
dex i from X .
The compliance of the single user ratings with the
final label (calculated on mean and median) is de-
picted in Table 1. As can be seen, the agreement of
the three raters with the median label is significantly
higher than with the mean label. Consequently the
median label represents the most objective measure-
ment of Interaction Quality and commends itself for
creating the model.
5 Input Features
The system-user interaction is modeled on exchange
level. Each system-user exchange consists of a set
of fully automatic features that can be derived from
system logs. We used parameters similar to the ones
described in (Schmitt et al, 2008; Schmitt et al,
2010b). In the first place, we modeled each system-
user exchange with a number of Speech Recognition
(ASR), Spoken Language Understanding (SLU) and
Dialog Manager (DM)-related features:
Mean Label Median Label
Cohen?s ?
Rater1 0.557 0.688
Rater2 0.554 0.679
Rater3 0.402 0.478
Mean 0.504 0.608*
Spearman?s ?
Rater1 0.901 0.900
Rater2 0.911 0.907
Rater3 0.841 0.814
Mean 0.884 0.874
Accuracy
Rater1 0.651 0.755
Rater2 0.647 0.749
Rater3 0.539 0.598
Mean 0.612 0.701*
Table 1: Agreement of single rater opinions to the merged
label when determined by mean and median, measured in
?, ? and accuracy. (*)=significantly higher (? < 0.05)
ASR ASRRECOGNITIONSTATUS: one of ?suc-
cess?, ?reject?, ?timeout?; ASRCONFIDENCE:
confidence of the ASR; BARGED-IN?: did the
user barge-in?, MODALITY: one of ?speech?,
?DTMF?; EXMO: the modality expected from
the system (?speech?, ?DTMF?, ?both?); UN-
EXMO?: did the user employ another modal-
ity than expected?; GRAMMARNAMES: names
of the active grammars; TRIGGEREDGRAM-
MAR: name of grammar that matched; UTTER-
ANCE: raw ASR transcription; WPUT: num-
ber of words per user turn; UTD: utterance turn
duration;
SLU SEMANTICPARSE: semantic interpretation
of caller utterance; HELPREQUEST?: is the
current turn a help request?; OPERATORRE-
QUEST?: is the current turn an operator re-
quest?;
Dialog Manager ACTIVITY: identifier of
the current system action; ACTIVITY-
TYPE: one of ?question?, ?announcement?,
?wait for user feedback?; PROMPT: system
prompt; WPST: number of words per system
turn; REPROMPT?: is the current system turn
a reprompt?; CONFIRMATION?: whether the
176
current system prompt is a confirmation to
elicit common ground between user and system
due to low ASR confidence; TURNNUMBER:
current turn; DD: dialog duration up to this
point in seconds.
To account for the overall history of important
system events we added running tallies, percentages
and mean values for certain features symbolized
with the suffixes ?#?, ?%? and ?MEAN?. They
are: MEANASRCONFIDENCE, the average of
ASR confidence scores from all user utterances
so far in the dialog, and #ASRSUCCESS, the
number of succesfully parsed user utterances so far.
Further we calculate #ASRREJECTIONS, #TIME-
OUTPROMPTS, #BARGEINS, #UNEXMO and the
respective normalized equivalents with the prefix
?%? instead of ?#?. We consider the immediate
context within the previous 3 turns of the current
turn as particularly relevant for the Interaction Qual-
ity. Hence, derived from the basic parameters we
created further parameters that emphasize specific
user behavior prior to the classification point. They
are symbolized with the prefix {#} for a number and
{Mean} for the mean value. A number of successive
barge-ins or recognition problems might indicate a
low IQ. Thus we add {MEAN}ASRCONFIDENCE,
the mean confidence of the ASR within the win-
dow, {#}ASRSUCCESS, {#}ASRREJECTIONS
and {#}TIME-OUTPROMPTS, i.e. the number
of successfully and unsuccessfully parsed ut-
terances within the window and the number
of time-outs. The other counters are calcu-
lated likewise: {#}BARGEINS; {#}UNEXMO,
{#}HELPREQUESTS, {#}OPERATORREQUESTS,
{#}REPROMPT, {#}CONFIRMATIONS,
{#}SYSTEMQUESTIONS.
To provide comparability to previous work (Hi-
gashinaka et al, 2010a), we further introduce a
dialogue act feature group that we create semi-
automatically:
DAct SYSTEMDIALOGUEACT: one of 28 dis-
tinct dialogue acts, such as greeting,offer help,
ask bus, confirm departure, deliver result, etc.
USERDIALOGUEACT: one of 22 distinct DAs,
such as confirm departure, place information,
polite, reject time, request help, etc.
To create an upper baseline of our model we fur-
ther introduce the negative emotional state of the
user that is manually annotated by a human rater
who chooses one of the labels garbage, non-angry,
slightly angry, very angry for each single user turn:
Emo EMOTIONALSTATE: emotional state of the
caller in the current exchange. One of garbage,
non-angry, slightly angry, very angry.
The same annotation scheme as in our previous
work on anger detection has been applied, see e.g.
(Schmitt et al, 2009). From all 4,832 user turns,
68.5% were non-angry, 14.3% slightly angry, 5.0%
very angry and 12.2% contained garbage, i.e. non-
speech events. In total, the number of interaction
parameters servings as input variables for the model
amounts to 52.
6 Non-Linear Modeling with Support
Vector Machines
The IQ scores are classified with Support Vector
Machines (Bennett and Campbell, 2000). In short,
an SVM uses a set of training examples
(x1, y1) . . . (xn, yn)|xi ? X , yi ? {?1, 1}
to create a hyperplane that separates two classes
{?1, 1} in such a manner that the smallest margin
between all training samples is maximized. The hy-
perplane is described by a normal vector w and a
so-called bias b. To classify an unknown sample the
following decision rule is applied:
Y = sgn[wTx+ b > 0] =
{
+1, wTx+ b > 0
?1, wTx+ b ? 0
Depending on the position of the training sample in
relation to the hyperplane, the class 1 or ?1 is as-
signed to the unknown sample. Multi-class prob-
lems are solved by reducing the problem to several
binary classification problems where usually a one-
versus-all decision is applied.
The model is constructed with an SVM with lin-
ear kernel that uses the fast Sequential Minimal Op-
timization (SMO) algorithm (Platt, 1999). Input
variables are features from the described groups, i.e.
x ? {DAct,ASR, SLU,DM,Emo}. The target
variable is the IQ score.
177
7 Feature Group Evaluation
The skew distribution of the five classes requires the
employment of an evaluation metric that weights the
prediction of all classes equally. Hence, a perfor-
mance metric, such as accuracy, would not be a re-
liable measurement. We select the unweighted aver-
age recall (UAR) to assess the model performance.
Although it does not consider the severity of the er-
ror, i.e. predicting ?1? for an IQ of ?5? is considered
as fatal as predicting ?4?, it has been proven to be su-
perior to other evaluation metrics, see (Higashinaka
et al, 2010a), where the UAR is called Match Rate
per Rating (MR/R). It is defined as follows:
MR/R(R,H) =
1
K
K?
r=1
?
i?{i|Ri=r}
match(Ri, Hi)
?
i?{i|Ri=r}
1
,
where K is the number of classes, here ?5?, and
?match? is either ?1? or ?0? depending on whether
the classifier?s hypothesis Hi for the class r matches
the reference label Ri. In the course of this work
we will stick to the expression MR/R by reason of
clearness. We further list Cohen?s ? and Spearman?s
? to make our work comparable to other studies but
will use MR/R as central evaluation criterion and for
feature selection.
We have split all available data into two disjoint
subsets consisting of 60% of the dialogues for train-
ing and testing via 10-fold cross-validation and the
remaining 40% of the dialogues for optimization.
The dialogues have been selected randomly.
In order to assess the performance contribution of
the single feature groups, we trained the SVM re-
spectively with all features from the DAct, ASR, SLU
and DM groups. Further, we subsumed the groups
ASR, SLU and DM as AUTO features since they can
automatically be derived from logs without manual
intervention. In addition, the AUTOEMO group con-
tains all AUTO features plus the emotion label. Fi-
nally, the ALL group contains the AUTOEMO fea-
tures plus the DAct features. For all groups, the sup-
port vector classifier has been trained and evaluated
in 10-fold cross validation with the 3110 exchanges
from the 118 training/testing dialogues. The first
turn of each dialogue has been excluded from the
evaluation since each dialogue starts with a score of
?5?. Results are depicted in the first half of Table 2.
Input Feature Selection MR/R ? ?
Majority Baseline 0.200 0.0 NA
DAct no 0.269 0.136 0.363
ASR no 0.605 0.551 0.753
SLU no 0.250 0.083 0.293
DM no 0.429 0.334 0.653
AUTO no 0.584 0.526 0.776
AUTOEMO no 0.606 0.549 0.785
ALL no 0.619 0.559 0.800
DAct ? - - -
ASR 13/25 0.598 0.545 0.730
SLU 4/5 0.250 0.083 0.293
DM 10/17 0.436 0.338 0.649
AUTO 20/47 0.616 0.563 0.786
AUTOEMO 31/48 0.604 0.545 0.785
ALL 23/52 0.625 0.575 0.795
Table 2: Model performance after 10-fold cross valida-
tion on training/test set. The first half comprises results
when all features of a group are employed. The second
half contains results after feature selection on the opti-
mization set ((x/y)=where x is the number of features
used from all y available features.)
As can be seen, the model reaches a similar
performance as (Higashinaka et al, 2010a) with
MR/R=0.26, when trained with dialogue act features
alone. The slightly higher performance of our model
can potentially be explained by the lower number
of classes (5 vs. 7), a different definition of the
dialogue act set, the employment of Support Vec-
tor Machines instead of Hidden Markov Models or
the difference in the target variable (IQ vs. close-
ness/smoothness/willingness). It can be noted that
the utilization of other features considerably outper-
forms dialogue act features. Particularly the group
of the ASR features alone reaches a performance
of 60.5%. The employment of all AUTO features
delivers 58.4% which is 2.1% below the ASR fea-
tures. Consequently, other variables seem to be
less meaningful for predicting the Interaction Qual-
ity and seem to harm the performance of the SVM.
The knowledge of the emotional state of the user
contributes with merely another 0.1% in compari-
son to the ASR features. It can be assumed that the
emotion feature increases the recognition rate of the
lower IQ scores ?1? and ?2?. However, this could
not be confirmed: even when considering class-wise
178
performance values a significant contribution of the
emotion feature cannot be observed. We also have
to bear in mind that we employed hand-annotated
emotions. Emotion recognition itself is error-prone
and a distinction of the emotional state of the caller
with the employed annotation scheme can be ex-
pected with approximately 70%-80% UAR, see e.g.
(Schmitt et al, 2010a). The influence of emotion
recognition on the IQ distinction can be considered
as limited and is insofar not surprising as the occur-
rence of strong anger in the data is not dominant
(5.0%). The contribution of the single features to
the classification result (across the groups they are
assigned to) is analyzed in the following.
8 Optimizing the Model by Feature
Selection
Since too many (potentially irrelevant) features
might harm the classifier?s performance we perform
feature selection with the optimization set. First,
the features are ordered according to an Informa-
tion Gain Ratio (IGR) ranking. The 10 most rele-
vant features according to IGR for predicting IQ are
depicted in Table 3.
Feature IGR
1 #ASRREJECTIONS 1
2 #TIMEOUT ASRREJ 0.967288
3 #ASRSUCCESS 0.834238
4 #REPROMPTS 0.804752
5 %REPROMPTS 0.800462
6 #TIMEOUTPROMPTS 0.757596
7 #SYSTEMQUESTIONS 0.757596
8 ROLEINDEX 0.699246
9 DD 0.566836
10 #BARGE-INS 0.566836
Table 3: Top 10 features on optimization set according to
IGR.
As can be seen the Interaction Quality is obvi-
ously heavily influenced by the performance of the
ASR. In other words, it can be assumed that the
raters themselves are influenced by the ASR?s per-
formance when assigning the IQ scores. All features
belong to the group AUTO, i.e. they can be deter-
mined automatically during runtime. Furthermore,
nearly all features are related to the overall interac-
tion, i.e. features related to the current exchange,
such as UTTERANCE, ASRSUCCESS? etc. do not
even occur. It can also be noted that the emotional
state and the dialogue acts are not listed as most rel-
evant features. To determine the global maximum of
the classifier, i.e. the best performing feature set, we
incrementally select the k topmost features from the
list and perform 10-fold cross validation on the opti-
mization set. A plot of the iterative feature selection
is depicted in Figure 1.
8.2 Prediction Results 53
Rapidminer GUI
Missing
figure
Figure 8.2: The Graphical User Interface of RapidMiner 4.6
Table 8.1: Baseline perfomances.
Configurations Performance
Corpus Input Target MR/R ? ?
LetsGo DAct IQ-Med - - -AUTO IQ-Med - - -ALL IQ-Med - - -
LetsGoUser DAct IQ - - -AUTO IQ - - -ALL-U IQ - - -DAct US - - -AUTO US - - -ALL-U US - - -
5 10 15 20 25 30 35 40 45
0.45
0.5
0.55
0.6
0.65 max
#Parameters
MR
/R
IGR Parameter Selection
Figure 8.3: Parameter selection performance on the LetsGo Corpus for the optimizationset using set AUTO for prediction of IQ-Med.Figure 1: Performance of the SVM when iteratively in-
creasing the size of the feature vector with the k topmost
features according to IGR.
Several observations can be made: the best per-
forming feature set consists of 20 features with an
absolute performance of 65 % MR/R on the op-
timization set. However, a similar performance
can already be gained with the 7 top-most fea-
tures. All other features obviously neither signif-
icantly decrease nor increase the performance and
can be considered irrelevant for predicting the IQ
score. The impact of feature selection on the model
when evaluated on the single feature groups from
the test/training set using only the most relevant fea-
tures from the optimization set can be seen in the
lower part of Table 2. Again, 10-fold cross vali-
dation has been applied. The AUTO group benefits
from the selection and delivers the highest perfor-
mance with 20 features with an MR/R of 61.6%,
which is an increase of 3.2%. The upper baseline
with hand annotated features (ALL group) amounts
to 62.5%. The fact that the AUTOEMO set underper-
forms with 60.4% - in comparison to the AUTO set
- can be explained due to the potentially too small
size of the optimization set.
The confusion matrix for the AUTO feature set is
depicted in Table 4, along with the class-wise pre-
cision and recall values. The model yields the best
179
performance in predicting the scores at the edge, i.e.
?5? and ?1?. In between, the confusion is slightly
higher and the model performance lower.
Table 4: Confusion matrix including class-wise preci-
sion and recall values after 10-fold cross validation (train-
ing/test set) using the AUTO set. A (weighted average)
accuracy of 67.5% can be derived.
true 5 true 4 true 3 true 2 true 1 prec.
pred. 5 721 154 42 9 5 0.774
pred. 4 89 464 104 44 19 0.644
pred. 3 17 63 231 49 38 0.580
pred. 2 2 15 39 89 33 0.500
pred. 1 4 23 29 27 169 0.670
rec. 0.865 0.645 0.519 0.408 0.640
9 Linear Regression Modeling
Models from the initially mentioned PARADISE ap-
proach presume a linear relationship between input
variables - quantifying the dialogue - and the target
variable US, the user satisfaction. Assuming lin-
earity, such linear models allow inferences such as
?The longer the dialogue duration, the lower the sat-
isfaction?. While linear modeling is descriptive and
easy to read it delivers poor performance when ap-
plied on non-linear problems. Such non-linear prob-
lems reach a better predictability using Support Vec-
tor Machines (SVM). Although we anticipate that a
relationship between IQ and the interaction param-
eters is not given, we list a multivariate linear re-
gression model for comparison reasons with PAR-
ADISE.
The linear regression model of Interaction Quality
is calculated as follows:
IQ =
n?
i=1
wi ? N (pi)
where wi is the weight for the interaction parame-
ters pi, and N the z-score normalization function.
N normalizes the input variables to a mean of zero
and a standard deviation of one. This eliminates the
variying scales of the input variables.
From the CMU Let?s Go dataset we obtained the
following IQ function using the ALL feature set:
IQ = 0.7797 ? N (TURNNUMBER)
+ 0.7797 ? N (#SYSTEMTURNS)
? 0.7386 ? N (#ASRSuccess)
? 0.7175 ? N (#USERTURNS)
? 0.3019 ? N (%RePrompts)
? 0.2371 ? N (EMOTIONALSTATE)
? 0.2224 ? N (#ASRRejections)
? 0.1961 ? N (#TIMEOUTS ASRREJ)
+ 0.1912 ? N (ASRRECOGNITIONSTATUS)
+ 0.1648 ? N (ASRCONFIDENCE)
? 0.1592 ? N (#ASRSUCCESS)
? 0.1466 ? N (ACTIVITY)
+ 0.1388 ? N (ACTIVITYTYPE)
+ 0.1231 ? N (MEANASRCONFIDENCE)
? 0.0981 ? N (#SYSTEMQUESTIONS)
+ 0.0948 ? N (%ASRREJECTIONS)
? 0.0918 ? N (#TIMEOUTS ASRREJ)
+ 0.0835 ? N (#Reprompts)
+ 0.0812 ? N (%BARGE-INS)
? 0.0567 ? N (%TIME-OUTPROMPTS)
? 0.0555 ? N (#TIMEOUTS?ASRREJ)
? 0.0467 ? N (#Time-OutPrompts)
+ 0.0461 ? N (WPST)
+ 0.0432 ? N (HANDTRANSCRIPTION)
? 0.0425 ? N (LOOPNAME)
+ 0.0375 ? N (#SystemQuestions)
+ 0.0374 ? N (SEMANTICPARSE)
? 0.0345 ? N (BARGED-IN?)
+ 0.0338 ? N (RoleIndex)
? 0.0335 ? N (#REPROMPTS)
? 0.0316 ? N (#ASRREJECTIONS)
+ 0.0302 ? N (REPROMPT?)
+ 0.0249 ? N (WPUT)
+ 0.0225 ? N (ROLENAME)
Parameters occurring in the top 10 feature list ac-
cording to IGR (see Table 3) are printed in bold-
face. It is interesting to note that parameters related
to the progress of the dialogue (TURNNUMBER,
#SYSTEMTURNS, #USERTURNS) seem to play the
most important role, which can easily be explained:
the later in the dialogue, the higher the probabil-
ity that the score is low, due to the nature of IQ.
Remember that all dialogues have been annotated
with high IQ scores (?5?) in the beginning (see also
180
Table 5). However, many inconsistencies remain
unexplained, e.g. the negative sign in ??0.7175 ?
N (#USERTURNS)? contradicting the positive sign
in ?+0.7797 ? N (#SYSTEMTURNS)?. The nega-
tive sign in ??0.7386 ? N (#ASRSUCCESS)? would
further imply that the more successful the ASR, the
lower the IQ score. This corroborates our suspicion
that IQ is not a linear problem.
To assess the performance of linear regression for
predicting IQ we employed 10-fold cross validation,
again with all 200 annotated dialogues. We obtained
a root mean squared error of 0.594 and R2 = 0.646.
Mapping the continuous values to discrete score
classes from 1-5, we obtain MR/R = 45.5%
(62.5% using SVM), ? = 0.352 (0.575) and ? =
0.46 (0.795). All values finally suggest that IQ is
better modeled with non-linear classifiers such as
SVMs or Multilayer Perceptrons (MLP).
10 Conclusion and Discussion
In this work we have developed a statistical model
that predicts Interaction Quality, an objective mea-
sure of user satisfaction, at arbitrary points in an
SDS. The model targets on predicting critical situ-
ations on exchange level in ongoing dialogues. The
classifier, an SVM, reaches a performance of 61.6%
MR/R (? = 0.563, ? = 0.786) by use of an opti-
mized feature set that can be automatically derived
during the interaction. It could be further shown
that linear modeling with multivariate linear regres-
sion is not appropriate for predicting IQ and reaches
merely 45.5.% MR/R. Among all five feature groups
comprising altogether 52 features, the ASR and DM
groups contribute the most, while the EMOTIONAL-
STATE underperforms. We could further show that
features requiring manual annotation, such as dia-
logue acts, do not significantly improve the model?s
performance. The model has been evaluated with
200 dialogues from the CMU Let?s Go corpus com-
prising 4885 system-user exchanges where three an-
notators labeled the Interaction Quality on a score
from 1-5. We could show that the median of all three
opinions depicts the most generic view on the IQ and
is thus chosen as final score for the model.
Some issues have to be addressed in future work:
In the current study we assume that Interaction
Quality is an objective metric for determining the
satisfaction of users during the interaction. How
far this assumption is justified has to be empirically
proven in a laboratory study with real users. A cor-
responding study is in preparation and the results
will be published in a follow-up work. Although
the model?s performance can be considered as sat-
isfying, the direct deployment in dialogue systems
cannot be recommended at this stage. For enabling
a dialogue system to react on poor interaction qual-
ity, a correct prediction of the low IQ scores 1,2 and
3 is urgent, i.e. the classifier has to be optimized
to deliver a very high precision on such low scores
rather than towards a high general MR/R value. We
assume that the performance can be further driven
up by extending the feature space to the last n ex-
changes, which would take into account a larger
dialogue history for the decision (cf. (Schmitt et
al., 2010b)). The presented model can principally
be trained on data from any dialogue system; how-
ever, we did not yet deliver the proof that the present
model trained on Let?s Go data is general enough
for cross-domain and cross-system application with-
out significant performance loss. Due to the general
character of the ten most relevant features (cf. Table
3), portability can at least be assumed - without the
need for domain-dependent and system-dependent
re-training.
11 Acknowledgements
We would like to thank Maxine Eskenazi, Alan
Black, Lori Levin, Rita Singh, Antoine Raux and
Brian Langner from the Let?s Go Lab at Carnegie
Mellon University, Pittsburgh, for providing the
Let?s Go Sample Corpus. Further we would like to
thank the reviewers for their constructive advices.
The research leading to these results has re-
ceived funding from the Transregional Collabora-
tive Research Centre SFB/TRR 62 ?Companion-
Technology for Cognitive Technical Systems?
funded by the German Research Foundation (DFG).
181
References
K. P. Bennett and C. Campbell. 2000. Support vector
machines: Hype or hallelujah? Journal of SIGKDD
Explorations, 2(2):1?13.
W. J. Doll and G. Torkzadeh. 1991. The measure-
ment of end-user computing satisfaction: theoretical
and methodological issues. MIS Q., 15:5?10, March.
K.-P. Engelbrecht, F. Go?dde, F. Hartard, H. Ketabdar, and
S. Mo?ller. 2009. Modeling user satisfaction with hid-
den markov model. In Proc. of SIGDIAL 2009 Con-
ference, pages 170?177. ACL.
S. Hara, N. Kitaoka, and K. Takeda. 2010. Estimation
method of user satisfaction using n-gram-based dialog
history model for spoken dialog system. In Proc. of
the Seventh conference on International Language Re-
sources and Evaluation (LREC?10), Valletta, Malta.
R. Higashinaka, Y. Minami, K. Dohsaka, and T. Meguro.
2010a. Issues in predicting user satisfaction transi-
tions in dialogues: Individual differences, evaluation
criteria, and prediction models. In Spoken Dialogue
Systems for Ambient Environments, Lecture Notes in
Computer Science, pages 48?60. Springer Berlin /
Heidelberg.
R. Higashinaka, Y. Minami, K. Dohsaka, and T. Meguro.
2010b. Modeling user satisfaction transitions in dia-
logues from overall ratings. In Proceedings of the SIG-
DIAL 2010 Conference, pages 18?27, Tokyo, Japan,
September. ACL.
ISO. 1998. Ergonomic requirements for office work with
visual display terminals (VDTs), Part 11: Guidance on
usability. International Standardization Organization
(ISO).
B. Ives, M. H. Olson, and J. J. Baroudi. 1983. The mea-
surement of user information satisfaction. Commun.
ACM, 26:785?793.
D. Larcker and V. P. Lessig. 1980. Perceived usefulness
of information: A psychometric examination. Deci-
sion Sciences, pages 121?134, November.
G. Lindgaard and C. Dudek. 2003. What is this evasive
beast we call user satisfaction? Interacting with Com-
puters, 15(3):429?452.
J. C. Platt, 1999. Fast training of support vector ma-
chines using sequential minimal optimization, pages
185?208. MIT Press, Cambridge, MA, USA.
A. Raux, D. Bohus, B. Langner, A. W. Black, and M. Es-
kenazi. 2006. Doing research on a deployed spoken
dialogue system: One year of lets go! experience. In
Proc. of the International Conference on Speech and
Language Processing (ICSLP).
A. Schmitt, C. Hank, and J. Liscombe. 2008. Detecting
problematic calls with automated agents. In 4th IEEE
Tutorial and Research Workshop Perception and Inter-
active Technologies for Speech-Based Systems, Irsee
(Germany), June.
Alexander Schmitt, Tobias Heinroth, and Jackson Lis-
combe. 2009. On nomatchs, noinputs and bargeins:
Do non-acoustic features support anger detection? In
Proceedings of the 10th Annual SIGDIAL Meeting on
Discourse and Dialogue, SigDial Conference 2009,
London (UK), September. Association for Computa-
tional Linguistics.
Alexander Schmitt, Tim Polzehl, and Wolfgang Minker.
2010a. Facing reality: Simulating deployment of
anger recognition in ivr systems. In Second In-
ternational Workshop on Spoken Dialogue Systems
(IWSDS), September.
Alexander Schmitt, Michael Scholz, Wolfgang Minker,
Jackson Liscombe, and David Su?ndermann. 2010b.
Is it possible to predict task completion in automated
troubleshooters? In Proc. of the International Confer-
ence on Speech and Language Processing (ICSLP).
Marilyn Walker, Candace Kamm, and Diane Litman.
2000. Towards developing general models of usabil-
ity with paradise. Nat. Lang. Eng., 6(3-4):363?377.
182
Appendix
1 Welcome to the CMU Let?s Go bus information system.
2 To get help at any time, just say Help or press zero.
3 What can I do for you?
<DTMF-Zero>
4 I am an automated spoken dialogue system that can give you
schedule information for bus routes in Pittsburgh?s East End.
You can ask me about the following buses: 28X, 54C, 56U,
59U, 61A, 61B, 61C, 61D, 61F, 64A, 69A, and 501.
5 What bus schedule information are you looking for?
6 For example, you can say, when is the next 28X from DOWN-
TOWN to THE AIRPORT? or I?d like to go from MCK-
EESPORT to HOMESTEAD tomorrow at 10 a.m..
28X
7 The 56C . Is this correct?
No, 28X
8 What can I do for you?
I want to know whether /uh/ tomorrow the bus is available or
not.
9 Leaving at 11 p.m. . Did I get that right?
No, I just want to know is there any bus service for 28X tomor-
row.
10 Please use shorter answers because I have trouble understand-
ing long sentences.
11 Leaving at 11 p.m. . Is this correct?
On twenty third november.
12 Leaving at 11 p.m. . Is this correct?
No, I want to know the schedule on twenty third november
13 Leaving at 2 a.m. . Did I get that right?
<noise>
14 If you want to leave at 2 a.m. say yes or press one, otherwise
say no or press three.
no <noise>
15 Going to NORTH AND CEDAR. Did I get that right?
28x going to airport at seven a.m.
16 Please use shorter answers because I have trouble understand-
ing long sentences.
17 Going to NORTH AND CEDAR. Is this correct?
No.
18 Leaving from DYNAMO. Is this correct?
No.
19 Leaving from DYNAMO. Is this correct?
Leaving to airport.
20 Leaving from AIRPORT . Did I get that right?
yes
21 Right
22 Where are you going?
To airport.
23 Going to AIRPORT. Did I get that right?
yeah, yep
24 Okay
25 When would you like to travel?
yes
26 When would you like to travel?
Twenty third November.
27 When would you like to travel?
Tomorrow.
28 At what time do you wanna travel?
Seven a.m.
29 Leaving at 7 a.m. . Is this correct?
yep
30 Alright
31 Just a minute. I?ll look that up.
32 AIRPORT and AIRPORT are both the same stop. Please pro-
vide a different start or end point.
33 To ask about a different trip, you can say, start a new query. If
you are finished, you can say goodbye.
Yes.
34 You can say start a new query, or goodbye.
No.
35 To ask about a different trip, you can say, start a new query. If
you are finished, you can say goodbye.
<noise>
36 You can say start a new query, or goodbye.
<noise>
You can say start a new query, or goodbye.
<hangup>
Table 5: Example dialogue (ID: 2061122025) from the CMU Let?s Go System (2006 corpus) with low Interaction
Quality. The user utterances are printed in italic.
5
1
6
11
16
21
26
31
36
uality
Ex
ch
an
ge
s
m
ed
ian
ra
te
r1
1234
Interaction?Q
ra
te
r2
ra
te
r3
1 345
tion?Quality
m
ed
ian
m
od
el?
hy
po
th
es
is
12
Interac
Figure 2: Upper chart: Turn-wise Interaction Quality (IQ) annotation from 3 raters. The final label is the median of
all three opinions. Lower chart: median reference vs. hypothesis of the model trained with AUTO feature set.
183
1 2 3 4 50
500
1,000
1,500 12941220
792
595
983
Rating
Fre
que
ncy
Rater1
1 2 3 4 50
500
1,000
1,500 1417
1043
797
555
1072
Rating
Fre
que
ncy
Rater2
1 2 3 4 50
500
1,000
1,500
1656
1416
927
442443
Rating
Fre
que
ncy
Rater3
Figure 3: Rating distribution for Interaction Quality within the Let?s Go Corpus for each rater.
Table 6: Rater guidelines for annotating Interaction Quality.
Rule Description
1. The rater should try to mirror the users point of view on the interaction as objectively as possible.
2. An exchange consists of the system prompt and the user response. Due to system design, the latter is not always present.
3. The IQ score is defined on a 5-point scale with ?1=bad?, ?2=poor?, ?3=fair?, ?4=good? and ?5=excellent?.
4. The Interaction Quality is to be rated for each exchange in the dialogue. The history of the dialogue should be kept in
mind when assigning the score. For example, a dialogue that has proceeded fairly poor for a long time, should require
some time to recover.
5. A dialogue always starts with an Interaction Quality score of ?5?.
6. The first user input should also be rated with 5, since until this moment, no rateable interaction has taken place.
7. A request for help does not invariably cause a lower Interaction Quality, but can result in it.
8. In general, the score from one exchange to the following exchange is increased or decreased by one point at the most.
9. Exceptions, where the score can be decreased by two points are e.g. hot anger or sudden frustration. The rater?s
perception is decisive here.
10. Also, if the dialogue obviously collapses due to system or user behavior, the score can be set to ?1? immediately. An
example herefore is a reasonable frustrated sudden hang-up.
11. Anger does not need to influence the score, but can. The rater should try to figure out whether anger was caused by the
dialogue behavior or not.
12. In the case a user realizes that he should adapt his dialogue strategy to obtain the desired result or information and
succeeded that way, the Interaction Quality score can be raised up to two points per turn. In other words, the user
realizes that he caused the poor Interaction Quality by himself.
13. If the system does not reply with a bus schedule to a specific user query and prompts that the request is out of scope,
this can nevertheless be considered as ?task completed?. Therefore this does not need to affect the Interaction Quality.
14. If a dialogue consists of several independent queries, each query?s quality is to be rated independently. The former
dialogue history should not be considered when a new query begins. However, the score provided for the first exchange
should be equal to the last label of the previous query.
15. If a dialogue proceeds fairly poor for a long time, the rater should consider to increase the score more slowly if the
dialogue starts to recover. Also, in general, he should observe the remaining dialogue more critical.
16. If a constantly low-quality dialogue finishes with a reasonable result, the Interaction Quality can be increased.
184
NAACL-HLT 2012 Workshop on Future directions and needs in the Spoken Dialog Community: Tools and Data, pages 49?52,
Montre?al, Canada, June 7, 2012. c?2012 Association for Computational Linguistics
Towards Quality-Adaptive Spoken Dialogue Management
Stefan Ultes, Alexander Schmitt, Wolfgang Minker
Dialogue Systems - Ulm University
Albert-Einstein-Allee 43
89081 Ulm, Germany
{stefan.ultes,alexander.schmitt,wolfgang.minker}@uni-ulm.de
Abstract
Information about the quality of a Spoken Di-
alogue System (SDS) is usually used only for
comparing SDSs with each other or manually
improving the dialogue strategy. This infor-
mation, however, provides a means for inher-
ently improving the dialogue performance by
adapting the Dialogue Manager during the in-
teraction accordingly. For a quality metric to
be suitable, it must suffice certain conditions.
Therefore, we address requirements for the
quality metric and, additionally, present ap-
proaches for quality-adaptive dialogue man-
agement.
1 Introduction
For years, research has been focused on enabling
Spoken Dialogue Systems (SDSs) to behave more
adaptively to the user?s expectations and needs.
Mo?ller et al (2009) presented a taxonomy for qual-
ity of human-machine interaction, i.e., Quality of
Service (QoS) and Quality of Experience (QoE). For
QoE, several aspects are identified. They contribute
to good user experience, e.g., interaction quality, us-
ability and acceptability. These aspects can be com-
bined to the term User Satisfaction (US), describ-
ing the degree by which the user is satisfied with the
system?s performance. The dialogue community has
been investigating this aspect for years. Most promi-
nently is the PARADISE framework by Walker et al
(2000) which maps objective performance metrics
of an SDS to subjective user ratings.
Recent work mostly discusses how to evaluate
Spoken Dialogue Systems. However, the issue of
how this information can be useful for improv-
ing dialogue performance remains hardly addressed.
Hence, we focus on exploring techniques for incor-
porating dialogue quality information into the Dia-
logue Manager (DM). This is accompanied by the
problem of defining characteristics of a suitable dia-
logue quality metric.
In Section 2, we present related work both on
measuring dialogue quality and on approaches for
incorporating user state information into the DM.
In Section 3, requirements for a quality metric are
presented along with a suitable example. Section 4
presents our ongoing and future work on incorpo-
rating quality measures into dialogue strategies. Fi-
nally, Section 5 concludes this work.
2 Related Work
In recent years, several studies have been published
on determining the qualitative performance of a
SDS. Engelbrecht et al (2009) predicted User Sat-
isfaction on a five-point scale at any point within the
dialogue using Hidden Markov Models (HMMs).
Evaluation was based on labels the users applied
themselves during a Wizard-of-Oz experiment. To
guarantee for comparable conditions, the dialogue
flow was controlled by predefined scenarios creat-
ing transcripts with equal length for each scenario.
Further work based on HMMs was presented by
Higashinaka et al (2010). The HMM was trained on
US rated at each exchange. These exchange ratings
were derived from ratings for the whole dialogue.
The authors compare their approach with HMMs
trained on manually annotated exchanges achieving
a better performance for the latter.
49
In order to predict US, Hara et al (2010) created
n-gram models from dialogue acts (DA). Based on
dialogues from real users interacting with a music
retrieval system, overall ratings for the whole dia-
logue have been labeled on a five point scale after
the interaction. An accuracy (i.e., rate of correctly
predicted ratings) of 34% by a 3-gram model was
the best performance which could be achieved.
Dealing with true User Satisfaction, Schmitt et al
presented their work about statistical classification
methods for automatic recognition of US (Schmitt
et al, 2011b). The data was collected in a lab
study where the users themselves had to rate the
conversation during the ongoing dialogue. Labels
were applied on a scale from 1 to 5. Perform-
ing automatic classification using a Support Vector
Machine (SVM), they achieved an Unweighted Av-
erage Recall (UAR) of 49.2 (i.e., average rate of
correctly predicted ratings, compensated for unbal-
anced data).
An approach for affective dialogue modeling
based on Partially Observable Markov Decision
Processes (POMDPs) was presented by Bui et al
(2007). Adding stress to the dialogue state enables
the dialogue manager to adapt to the user. To make
belief-update tractable, the authors introduced Dy-
namic Decision Networks as means for reducing
complexity.
Pittermann et al (2007) presented another ap-
proach for adaptive dialogue management. The au-
thors incorporated emotions by modeling the dia-
logue in a semi-stochastic way. Thus, an emotional
dialogue model was created as a combination of a
probabilistic emotional model and probabilistic dia-
logue model defining the current dialogue state.
3 Interaction Quality Metric
In order to enable the Dialogue Manager to be
quality-adaptive, the quality metric must suffice cer-
tain criteria. In this Section, we identify the impor-
tant issues and render the requirements for a suitable
quality metric.
3.1 General Aspects
For adapting the dialogue strategy to the quality of
the dialogue, the quality metric is required to imple-
ment certain characteristics. We identify the follow-
ing items:
? exchange-level quality measurement,
? automatically derivable features,
? domain-independent features,
? consistent labeling process,
? reproducible labels and
? unbiased labels.
The performance of a Spoken Dialogue System
may be evaluated either on the dialogue level or on
the exchange level. As dialogue management is per-
formed after each system-user exchange, dynamic
adaption of the dialogue strategy to the dialogue
performance requires exchange-level performance
measures. Therefor, Dialogue-level approaches are
of no use. Furthermore, previous presented meth-
ods for exchange-level quality measuring could not
achieve satisfying accuracy in predicting dialogue
quality (Engelbrecht et al, 2009; Higashinaka et al,
2010).
Features serving as input variables for a classi-
fication algorithm must be automatically derivable
from the dialogue system modules. This is impor-
tant because other features, e.g., manually annotated
dialogue acts (Higashinaka et al, 2010; Hara et al,
2010), produce high costs and are also not available
immediately during run-time in order to use them as
additional input to the Dialogue Manager. Further-
more, for creating a general quality metric, features
have to be domain-independent, i.e., not depending
on the task domain of the dialogue system.
Another important issue is the consistency of the
labels. Labels applied by the users themselves are
subject to large fluctuations among the different
users (Lindgaard and Dudek, 2003). As this results
in inconsistent labels, which do not suffice for creat-
ing a generally valid quality model, ratings applied
by expert raters yield more consistent labels. The
experts are asked to estimate the user?s satisfaction
following previously established rating guidelines.
Furthermore, expert labelers are also not prone to be
influenced by certain aspects of the SDS, which are
not of interest in this context, e.g., the character of
the synthesized voice. Therefore, they create less bi-
ased labels.
50
3.2 Interaction Quality
As metric, which fulfills all previously addressed
requirements, we present the Interaction Quality
(IQ) metric, see also (2011a). Based on dialogues
from the ?Let?s Go Bus Information System? of the
Carnegie Mellon University in Pittsburgh (Raux et
al., 2006), IQ is labeled on a five point scale. The
labels are (from best (5) to worst (1)) ?satisfied?,
?slightly unsatisfied?, ?unsatisfied?, ?very unsatis-
fied? and ?extremely unsatisfied?. They are applied
by expert raters following rating guidelines, which
have been established to allow consistent and repro-
ducible ratings.
Additionally, domain-independent features used
for IQ recognition have been derived from the di-
alogue system modules automatically for each ex-
change grouped on three levels: the exchange level,
the dialogue level, and the window level. As parame-
ters like ASRCONFIDENCE or UTTERANCE can di-
rectly be acquired from the dialogue modules they
constitute the exchange level. Based on this, counts,
sums, means, and frequencies of exchange level pa-
rameters from multiple exchanges are computed to
constitute the dialogue level (all exchanges up to the
current one) and the window level (the three previous
exchanges).
A corpus containing the labeled data has been
published recently (Schmitt et al, in press) contain-
ing 200 calls annotated by three expert labelers, re-
sulting in a total of 4,885 labeled exchanges. Us-
ing statistical classification of IQ based on SVMs
achieves an Unweighted Average Recall of 0.58
(Schmitt et al, 2011a).
4 Quality-Adaptive Spoken Dialogue
Management
The goal of our work is to enable Dialogue Man-
agers to directly adapt to information about the qual-
ity of the ongoing dialogue. We present two differ-
ent approaches that outline our ongoing and future
work.
4.1 Dialogue Design-Patterns for Quality
Adaption
Rule-based Dialogue Managers are still state-of-the-
art for commercial SDSs. It is hardly arguable that
making the rules quality-dependent is a promising
way for dialogue improvement. However, the num-
ber of possibilities for adapting the dialogue strategy
to the dialogue quality is high. Based on the Speech-
Cycle RPA Dialogue Manager, we are planning on
identifying common dialogue situations in order to
create design-patterns. These patterns can be ap-
plied as a general means of dealing with situations
that arise by introducing quality-adaptiveness to the
dialogue.
4.2 Statistical Quality-Adaptive Dialogue
Management
For the incorporation of Interaction Quality into a
statistical DM, two approaches have been found.
First, based on work on factored Partially Observ-
able Markov Decision Processes by Williams and
Young (2007) and similar to Bui et al (2006), we
presented our own approach for incorporating addi-
tional user state information (Ultes et al, 2011).
In the factored POMDP by Williams and Young
(2007), the state of the underlying process is de-
fined as s = (u, g, h). To incorporate IQ, it is
extended by adding the IQ-state siq, resulting in
s = (u, g, h, siq).
Following the concept of user acts, we further in-
troduce IQ-acts iq that describe the current qual-
ity predicted by the classification algorithm for the
current exchange. Incorporating IQ acts into obser-
vation o results in the two-dimensional observation
space
O = U ? IQ,
where U denotes the set of all user actions and IQ
the set of all possible Interaction Quality values.
Second, for training an optimal policy for ac-
tion selection in POMDPs, a reward function has
to be defined. Common reward functions are task-
oriented and based on task success and dialogue
length. As an example, a considerable positive re-
ward is given for reaching the task goal, a consider-
able negative reward for aborting the dialogue, and a
small negative reward for each exchange in order to
keep the dialogue short. Interaction Quality scores
offer an interesting and promising way of defining a
reward function, e.g., by rewarding improvements in
IQ. By that, strategies that try to keep the quality at
an overall high can be trained allowing for a better
user experience.
51
5 Conclusion
For incorporating information about the dialogue
quality into the Dialogue Manager, we identified
characteristics of a quality metric defining neces-
sary prerequisites for being used during dialogue
management. Further, the Interaction Quality met-
ric has been proposed as measure, which suffices all
requirements. In addition, we presented concrete ap-
proaches of incorporating IQ into the DM outlining
our ongoing and future work.
Acknowledgements
We would like to thank Maxine Eskenazi, Alan
Black, Lori Levin, Rita Singh, Antoine Raux and
Brian Langner from the Lets Go Lab at Carnegie
Mellon University, Pittsburgh, for providing the Lets
Go Sample Corpus. We would further like to thank
Roberto Pieraccini and David Suendermann from
SpeechCycle, Inc., New York, for providing the
SpeechCycle RPA Dialogue Manager.
References
T. H. Bui, J. Zwiers, M. Poel, and A. Nijholt. 2006. To-
ward affective dialogue modeling using partially ob-
servable markov decision processes. In Proceedings
of workshop emotion and computing, 29th annual Ger-
man conference on artificial intelligence.
T. H. Bui, M. Poel, A. Nijholt, and J. Zwiers. 2007.
A tractable ddn-pomdp approach to affective dialogue
modeling for general probabilistic frame-based dia-
logue systems. In Proceedings of the 5th IJCAI Work-
shop on Knowledge and Reasoning in Practical Dia-
logue Systems, pages 34?37.
Klaus-Peter Engelbrecht, Florian Go?dde, Felix Hartard,
Hamed Ketabdar, and Sebastian Mo?ller. 2009. Mod-
eling user satisfaction with hidden markov model. In
SIGDIAL ?09: Proceedings of the SIGDIAL 2009 Con-
ference, pages 170?177. ACL.
Sunao Hara, Norihide Kitaoka, and Kazuya Takeda.
2010. Estimation method of user satisfaction using
n-gram-based dialog history model for spoken dia-
log system. In Proceedings of the Seventh conference
on International Language Resources and Evaluation
(LREC?10), Valletta, Malta, may. ELRA.
Ryuichiro Higashinaka, Yasuhiro Minami, Kohji
Dohsaka, and Toyomi Meguro. 2010. Modeling
user satisfaction transitions in dialogues from over-
all ratings. In Proceedings of the SIGDIAL 2010
Conference, pages 18?27, Tokyo, Japan, September.
Association for Computational Linguistics.
Gitte Lindgaard and Cathy Dudek. 2003. What is this
evasive beast we call user satisfaction? Interacting
with Computers, 15(3):429?452.
Sebastian Mo?ller, Klaus-Peter Engelbrecht, C. Ku?hnel,
I. Wechsung, and B. Weiss. 2009. A taxonomy of
quality of service and quality of experience of multi-
modal human-machine interaction. In Quality of Mul-
timedia Experience, 2009. QoMEx 2009. International
Workshop on, pages 7?12, July.
Johannes Pittermann, A. Pittermann, Hong Meng, and
W. Minker. 2007. Towards an emotion-sensitive
spoken dialogue system - classification and dialogue
modeling. In Intelligent Environments, 2007. IE 07.
3rd IET International Conference on, pages 239 ?246,
September.
Antoine Raux, Dan Bohus, Brian Langner, Alan W.
Black, and Maxine Eskenazi. 2006. Doing research
on a deployed spoken dialogue system: One year of
lets go! experience. In Proc. of the International Con-
ference on Speech and Language Processing (ICSLP),
September.
Alexander Schmitt, Benjamin Schatz, and Wolfgang
Minker. 2011a. Modeling and predicting quality in
spoken human-computer interaction. In Proceedings
of the SIGDIAL 2011 Conference, Portland, Oregon,
USA, June. Association for Computational Linguis-
tics.
Alexander Schmitt, Benjamin Schatz, and Wolfgang
Minker. 2011b. A statistical approach for estimat-
ing user satisfaction in spoken human-machine inter-
action. In Proceedings of the IEEE Jordan Confer-
ence on Applied Electrical Engineering and Comput-
ing Technologies (AEECT), Amman, Jordan, Decem-
ber. IEEE.
Alexander Schmitt, Stefan Ultes, and Wolfgang Minker.
in-press. A parameterized and annotated corpus of the
cmu let?s go bus information system. In International
Conference on Language Resources and Evaluation
(LREC).
Stefan Ultes, Tobias Heinroth, Alexander Schmitt, and
Wolfgang Minker. 2011. A theoretical framework for
a user-centered spoken dialog manager. In Proceed-
ings of the Paralinguistic Information and its Integra-
tion in Spoken Dialogue Systems Workshop, pages 241
? 246. Springer, September.
Marilyn Walker, Candace Kamm, and Diane Litman.
2000. Towards developing general models of usabil-
ity with paradise. Nat. Lang. Eng., 6(3-4):363?377.
Jason D. Williams and Steve J. Young. 2007. Par-
tially observable markov decision processes for spo-
ken dialog systems. Computer Speech and Language,
(21):393?422.
52
