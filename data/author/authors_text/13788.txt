Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1266?1276,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Identifying Functional Relations in Web Text
Thomas Lin, Mausam, Oren Etzioni
Turing Center
University of Washington
Seattle, WA 98195, USA
{tlin,mausam,etzioni}@cs.washington.edu
Abstract
Determining whether a textual phrase denotes
a functional relation (i.e., a relation that maps
each domain element to a unique range el-
ement) is useful for numerous NLP tasks
such as synonym resolution and contradic-
tion detection. Previous work on this prob-
lem has relied on either counting methods or
lexico-syntactic patterns. However, determin-
ing whether a relation is functional, by ana-
lyzing mentions of the relation in a corpus,
is challenging due to ambiguity, synonymy,
anaphora, and other linguistic phenomena.
We present the LEIBNIZ system that over-
comes these challenges by exploiting the syn-
ergy between the Web corpus and freely-
available knowledge resources such as Free-
base. It first computes multiple typed function-
ality scores, representing functionality of the
relation phrase when its arguments are con-
strained to specific types. It then aggregates
these scores to predict the global functionality
for the phrase. LEIBNIZ outperforms previ-
ous work, increasing area under the precision-
recall curve from 0.61 to 0.88. We utilize
LEIBNIZ to generate the first public reposi-
tory of automatically-identified functional re-
lations.
1 Introduction
The paradigm of Open Information Extraction (IE)
(Banko et al, 2007; Banko and Etzioni, 2008) has
scaled extraction technology to the massive set of
relations expressed in Web text. However, additional
work is needed to better understand these relations,
and to place them in richer semantic structures. A
step in that direction is identifying the properties of
these relations, e.g., symmetry, transitivity and our
focus in this paper ? functionality. We refer to this
problem as functionality identification.
A binary relation is functional if, for a given arg1,
there is exactly one unique value for arg2. Exam-
ples of functional relations are father, death date,
birth city, etc. We define a relation phrase to be
functional if all semantic relations commonly ex-
pressed by that phrase are functional. For exam-
ple, we say that the phrase ?was born in? denotes
a functional relation, because the different seman-
tic relations expressed by the phrase (e.g., birth city,
birth year, etc.) are all functional.
Knowing that a relation is functional is helpful
for numerous NLP inference tasks. Previous work
has used functionality for the tasks of contradiction
detection (Ritter et al, 2008), quantifier scope dis-
ambiguation (Srinivasan and Yates, 2009), and syn-
onym resolution (Yates and Etzioni, 2009). It could
also aid in other tasks such as ontology generation
and information extraction. For example, consider
two sentences from a contradiction detection task:
(1) ?George Washington was born in Virginia.? and
(2) ?George Washington was born in Texas.?
As Ritter et al (2008) points out, we can only de-
termine that the two sentences are contradictory if
we know that the semantic relation referred to by
the phrase ?was born in? is functional, and that both
Virginia and Texas are distinct states.
Automatic functionality identification is essential
when dealing with a large number of relations as in
Open IE, or in complex domains where expert help
1266
Distributional
Difference
Assertions
Web
Corpus IE
Functionality prediction for Web relations
Combination Policy
Freebase
Clean 
Lists
type 
listsinstances per relation
functionality scores (typed)
Figure 1: Our system, LEIBNIZ, uses the Web and Free-
base to determine functionality of Web relations.
is scarce or expensive (e.g., biomedical texts). This
paper tackles automatic functionality identification
using Web text. While functionality identification
has been utilized as a module in various NLP sys-
tems, this is the first paper to focus exclusively on
functionality identification as a bona fide NLP infer-
ence task.
It is natural to identify functions based on triples
extracted from text instead of analyzing sentences
directly. Thus, as our input, we utilize tuples ex-
tracted by TEXTRUNNER (Banko and Etzioni, 2008)
when run over a corpus of 500 million webpages.
TEXTRUNNER maps sentences to tuples of the form
<arg1, relation phrase, arg2> and enables our
LEIBNIZ system to focus on the problem of decid-
ing whether the relation phrase is a function.
The naive approach, which classifies a relation
phrase as non-functional if several arg1s have multi-
ple arg2s in our extraction set, fails due to several
reasons: synonymy ? a unique entity may be re-
ferred by multiple strings, polysemy of both entities
and relations ? a unique string may refer to multiple
entities/relations, metaphorical usage, extraction er-
rors and more. These phenomena conspire to make
the functionality determination task inherently sta-
tistical and surprisingly challenging.
In addition, a functional relation phrase may ap-
pear non-functional until we consider the types of its
arguments. In our ?was born in? example, <George
Washington, was born in, 1732> does not contradict
<George Washington, was born in, Virginia> even
though we see two distinct arg2s for the same arg1.
To solve functionality identification, we need to con-
sider typed relations where the relations analyzed
are constrained to have specific argument types.
We develop several approaches to overcome these
challenges. Our first scheme employs approximate
argument merging to overcome the synonymy and
anaphora problems. Our second approach, DIS-
TRDIFF, takes a statistical view of the problem
and learns a separator for the typical count dis-
tributions of functional versus non-functional rela-
tions. Finally, our third and most successful scheme,
CLEANLISTS, identifies and processes a cleaner
subset of the data by intersecting the corpus with en-
tities in a secondary knowledge-base (in our case,
Freebase (Metaweb Technologies, 2009)). Utiliz-
ing pre-defined types, CLEANLISTS first identifies
typed functionality for suitable types for that rela-
tion phrase, and then combines them to output a final
functionality label. LEIBNIZ, a hybrid of CLEAN-
LISTS and DISTRDIFF, returns state-of-the-art re-
sults for our task.
Our work makes the following contributions:
1. We identify several linguistic phenomena that
make the problem of corpus-based functional-
ity identification surprisingly difficult.
2. We designed and implemented three novel
techniques for identifying functionality based
on instance-based counting, distributional dif-
ferences, and use of external knowledge bases.
3. Our best method, LEIBNIZ, outperforms the
existing approaches by wide margins, increas-
ing area under the precision-recall curve from
0.61 to 0.88. It is also capable of distinguishing
functionality of typed relation phrases, when
the arguments are restricted to specific types.
4. Utilizing LEIBNIZ, we created the first public
repository of functional relations.1
2 Related Work
There is a recent surge in large knowledge bases
constructed by human collaboration such as Free-
base (Metaweb Technologies, 2009) and VerbNet
(Kipper-Schuler, 2005). VerbNet annotates its
verbs with several properties but not functionality.
Freebase does annotate some relations with an ?is
unique? property, which is similar to functionality,
but the number of relations in Freebase is still much
1available at http://www.cs.washington.edu/
research/leibniz
1267
George Washington was born in :
Virginia
Westmoreland County
America
a town
a plantation
1732
February
the British colony of Virginia
Rudy Giuliani visited:
Florida
Boca Raton Synagogue
the Florida EvergladesSouth Carolina
Michigan
Republican headquarters
a famous cheesesteak restaurant
Philadelphia
Colonial Beach, Virginia
Figure 2: Sample arg2 values for a non-functional relation (visited) vs. a functional relation (was born in) illustrate
the challenge in discriminating functionality from Web text.
smaller than the hundreds of thousands of relations
existing on the Web, necessitating automatic ap-
proaches to functionality identification.
Discovering functional dependencies has been
recognized as an important database analysis tech-
nique (Huhtala et al, 1999; Yao and Hamilton,
2008), but the database community does not address
any of the linguistic phenomena which make this
a challenging problem in NLP. Three groups of re-
searchers have studied functionality identification in
the context of natural language.
AuContraire (Ritter et al, 2008) is a contradic-
tion detection system that also learns relation func-
tionality. Their approach combines a probabilis-
tic model based on (Downey et al, 2005) with es-
timates on whether each arg1 is ambiguous. The
estimates are used to weight each arg1?s contri-
bution to an overall functionality score for each
relation. Both argument-ambiguity and relation-
functionality are jointly estimated using an EM-like
method. While elegant, AuContraire requires sub-
stantial hand-engineered knowledge, which limits
the scalability of their approach.
Lexico-syntactic patterns: Srinivasan and Yates
(2009) disambiguate a quantifier?s scope by first
making judgments about relation functionality. For
functionality, they look for numeric phrases follow-
ing the relation. For example, the presence of the nu-
meric term ?four? in the sentence ?the fire destroyed
four shops? suggests that destroyed is not functional,
since the same arg1 can destroy multiple things.
The key problem with this approach is that it often
assigns different functionality labels for the present
tense and past tense phrases of the same semantic re-
lation. For example, it will consider ?lived in? to be
non-functional, but ?lives in? to be functional, since
we rarely say ?someone lives in many cities?. Since
both these phrases refer to the same semantic rela-
tion this approach has low precision. Moreover, it
performs poorly for relation phrases that naturally
expect numbers as the target argument (e.g., ?has an
atomic number of?).
While these lexico-syntactic patterns do not per-
form as well for our task, they are well-suited for
identifying whether a verb phrase can take multiple
objects or not. This can be understood as a function-
ality property of the verb phrase within a sentence,
as opposed to functionality of the semantic relation
the phrase represents.
WIE: In a preliminary study, Popescu (2007) ap-
plies an instance based counting approach, but her
relations require manually annotated type restric-
tions, which makes the approach less scalable.
Finally, functionality is just one property of rela-
tions that can be learned from text. A number of
other studies (Guarino and Welty, 2004; Volker et
al., 2005; Culotta et al, 2006) have examined detect-
ing other relation properties from text and applying
them to tasks such as ontology cleaning.
3 Challenges for Functionality Identification
A functional binary relation r is formally defined as
one such that ?x, y1, y2 : r(x, y1)?r(x, y2)? y1 =
y2. We define a relation string to be functional if all
semantic relations commonly expressed by the rela-
tion string are individually functional. Thus, under
our definition, ?was born in? and ?died in? are func-
tional, even though they can take different arg2s for
the same arg1, e.g., year, city, state, country, etc.
The definition of a functional relation suggests a
naive instance-based counting algorithm for identi-
fying functionality. ?Look for the number of arg2s
for each arg1. If all (or most) arg1s have exactly one
arg2, label the relation phrase functional, else, non-
functional.? Unfortunately, this naive algorithm fails
for our task exposing several linguistic phenomena
1268
that make our problem hard (see Figure 2):
Synonymy: Various arg2s for the same arg1 may
refer to the same entity. This makes many func-
tional relations seem non-functional. For instance,
<George Washington, was born in, Virginia> and
<George Washington, was born in, the British
colony of Virginia> are not in conflict. Other
examples of synonyms include ?Windy City? and
?Chicago?; ?3rd March? and ?03/03?, etc.
Anaphora: An entity can be referred to by using
several phrases. For instance,<George Washington,
was born in, a town> does not conflict with his be-
ing born in ?Colonial Beach, Virginia?, since ?town?
is an anaphora for his city of birth. Other examples
include ?The US President? for ?George W. Bush?,
and ?the superpower? to refer to ?United States?. The
effect is similar to that of synonyms ? many relations
incorrectly appear non-functional.
Argument Ambiguity: <George Washington, was
born in, ?Kortrijk, Belgium?> in addition to his be-
ing born in ?Virginia? suggests that ?was born in?
is non-functional. However, the real cause is that
?George Washington? is ambiguous and refers to dif-
ferent people. This ambiguity gets more pronounced
if the person is referred to just by their first (or last
name), e.g., ?Clinton? is commonly used to refer to
both Hillary and Bill Clinton.
Relation Phrase Ambiguity: A relation phrase can
have several senses. For instance ?weighs 80 kilos?
is a different weighs than ?weighs his options?.
Type Restrictions: A closely related problem
is type-variations in the argument. E.g., <George
Washington, was born in, America> vs. <George
Washington, born in, Virginia> both use the same
sense of ?was born in? but refer to different semantic
relations ? one that takes a country in arg2, and the
other that takes a state. Moreover, different argu-
ment types may result in different functionality la-
bels. For example, ?published in? is functional if the
arg2 is a year, but non-functional if it is a language,
since a book could be published in many languages.
We refer to this finer notion of functionality as typed
functionality.
Data Sparsity: There is limited data for more ob-
scure relations instances and non-functional relation
phrases appear functional due to lack of evidence.
Textually Functional Relations: Last but not least,
some relations that are not functional may appear
functional in text. An example is ?collects?. We col-
lect many things, but rarely mention it in text. Usu-
ally, someone?s collection is mentioned in text only
when it makes the news. We name such relations
textually functional. Even though we could build
techniques to reduce the impact of other phenomena,
no instance based counting scheme could overcome
the challenge posed by textually functional relations.
Finally, we note that our functionality predictor
operates over tuples generated by an Open IE sys-
tem. The extractors are not perfect and their errors
can also complicate our analysis.
4 Algorithms
To overcome these challenges, we design three al-
gorithms. Our first algorithm, IBC, applies several
rules to determine whether two arg2s are equal. Our
second algorithm, DISTRDIFF, takes a statistical ap-
proach, and tries to learn a discriminator between
typical count distributions for functional and non-
functional relations. Our final approach, CLEAN-
LISTS, applies counting over a cleaner subset of the
corpus, which is generated based on entities present
in a secondary KB such as Freebase.
From this section onwards, we gloss over the dis-
tinction between a semantic relation and a relation
phrase, since our algorithms do not have access to
relations and operate only at the phrase level. We
use ?relation? to refer to the phrases.
4.1 Instance Based Counting (IBC)
For each relation, IBC computes a global function-
ality score by aggregating local functionality scores
for each arg1. The local functionality for each arg1
computes the fraction of arg2 pairs that refer to the
same entity. To operationalize this computation we
need to identify which arg2s co-refer. Moreover, we
also need to pick an aggregation strategy to combine
local functionality scores.
Data Cleaning: Common nouns in arg1s are of-
ten anaphoras for other entities. For example, <the
company, was headquartered in, ...> refers to dif-
ferent companies in different extractions. To combat
this, IBC restricts arg1s to proper nouns. Secondly,
to counter extraction errors and data bias, it retains
1269
George Washington was born in :
Colonial
Beach
Colonial 
Beach,
Virginia
Westmoreland County, Virginia
February February 1732 1732
Figure 3: IBC judges that Colonial Beach and Westmore-
land County, Virginia refer to the same entity.
an extraction only once per unique sentence. This
reduces the disproportionately large frequencies of
some assertions that are generated from a single ar-
ticle published at multiple websites. Similarly, it al-
lows an extraction only once per website url. More-
over, it filters out any arg1 that does not appear at
least 10 times with that relation.
Equality Checking: This key component judges
if two arg2s refer to the same entity. It first em-
ploys weak typing by disallowing equality checks
across common nouns, proper nouns, dates and
numbers. This mitigates the relation ambiguity
problem, since we never compare ?born in(1732)?
and ?born in(Virginia)?. Within the same category it
judges two arg2s to co-refer if they share a content
word. It also performs a connected component anal-
ysis (Hopcraft and Tarjan, 1973) to take a transitive
closure of arg2s judged equal (see Figure 3).
For example, for the relation ?was named after?
and arg1=?Bluetooth? our corpus has three arg2s:
?Harald Bluetooth?, ?Harald Bluetooth, the King of
Denmark? and ?the King of Denmark?. Our equal-
ity method judges all three as referring to the same
entity. Note that this is a heuristic approach, which
could make mistakes. But for an error, there needs
to be extractions with the same arg1, relation and
similar arg2s. Such cases exist, but are not com-
mon. Our equality checking mitigates the problems
of anaphora, synonymy as well as some typing.
Aggregation: We try several methods to aggre-
gate local functionality scores for each arg1 into a
global score for the relation. These include, a simple
average, a weighted average weighted by frequency
of each arg1, a weighted average weighted by log
of frequency of each arg1, and a Bayesian approach
that estimates the probability that a relation is func-
tional using statistics over a small development set.
Overall, the log-weighting works the best: it assigns
a higher score for popular arguments, but not so high
that it drowns out all the other evidence.
4.2 DISTRDIFF
Our second algorithm, DISTRDIFF, takes a purely
statistical, discriminative view of the problem. It
recognizes that, due to aforementioned reasons,
whether a relation is functional or not, there are
bound to be several arg1s that look locally functional
and several that look locally non-functional. The
difference is in the number of such arg1s ? a func-
tional relation will have more of the former type.
DISTRDIFF studies the count distributions for a
small development set of functional relations (and
similarly for non-functional) and attempts to build
a separator between the two. As an illustration,
Figure 4(a) plots the arg2 counts for various arg1s
for a functional relation (?is headquartered in?).
Each curve represents a unique arg1. For an arg1,
the x-axis represents the rank (based on frequency)
of arg2s and y-axis represents the normalized fre-
quency of the arg2. For example, if an arg1 is found
with just one arg2, then x=1 will match with y=1
(the first point has all the mass) and x=2 will match
with y=0. If, on the other hand, an arg1 is found
with five arg2s, say, appearing ten times each, then
the first five x-points will map to 0.2 and the sixth
point will map to 0.
We illustrate the same plot for a non-functional
relation (?visited?) in Figure 4(b). It is evident from
the two figures that, as one would expect, curves for
most arg1s die early in case of a functional relation,
whereas the lower ranked arg2s are more densely
populated in case of a non-functional relation.
We aggregate this information using slope of the
best-fit line for each arg1 curve. For functional re-
lations, the best-fit lines have steep slopes, whereas
for non-functional the lines are flatter. We bucket the
slopes in integer bins and count the fraction of arg1s
appearing in each bin. This lets us aggregate the
information into a single slope-distribution for each
relation. Bold lines in Figure 4(c) illustrate the aver-
age slope-distributions, averaged over ten sample re-
lations of each kind ? dashed for non-functional and
solid for functional. Most non-functional relations
have a much higher probability of arg1s with low
magnitude slopes, whereas functional relations are
1270
0%
10%
20%
30%
40%
50%
60%
70%
80%
90%
100%
1 2 3 4 5 6 7 8 9 10
Arg
um
ent
 2 M
ass
 
Result Position 
is headquartered in (functional) 
0%
10%
20%
30%
40%
50%
60%
70%
80%
90%
100%
1 2 3 4 5 6 7 8 9 10
Arg
um
ent
 2 M
ass
 
Result Position 
visited (not functional) 
0%
10%
20%
30%
40%
50%
60%
70%
80%
90%
100%
0 1 2 3 4 10
Ma
ss 
Floor(|Slope|) 
Average Slope Distributions 
functional average
nonfunctional average
was born on (sample func)
visited (sample nonfunc)
Figure 4: DISTRDIFF: Arg2 count distributions fall more sharply for (a) a sample functional relation, than (b) a
sample non-functional relation. (c) The distance of aggregated slope-distributions from average slope-distributions
can be used to predict the functionality.
the opposite. Notice that the aggregated curve for
?visited? in the figure is closer to the average curve
for non-functional than to functional and vice-versa
for ?was born on?.
We plot the aggregated slope-distributions for
each relation and use the distance from average dis-
tributions as a means to predict the functionality. We
use KL divergence (Kullback and Leibler, 1951) to
compute the distance between two distributions. We
score a relation?s functionality in three ways using:
(1) KLFUNC, its distance from average functional
slope-distribution Favg, (2) KLDIFF, its distance
from average functional minus its distance from av-
erage non-functional Navg, and (3) average of these
two scores. For a relation with slope distribution R,
the scores are computed as:
KLFUNC =
?
iR(i)ln
R(i)
Favg(i)
KLDIFF = KLFUNC - (
?
iR(i)ln
R(i)
Navg(i)
)
Section 5.2 compares the three scoring functions.
A purely statistical approach is resilient to noisy
data, and does not need to explicitly account for the
various issues we detailed earlier. A disadvantage
is that it cannot handle relation ambiguity and type
restrictions. Moreover, we may need to relearn the
separator if applying DISTRDIFF to a corpus with
very different count distributions.
4.3 CLEANLISTS
Our third algorithm, CLEANLISTS, is based on the
intuition that for identifying functionality we need
not reason over all the data in our corpus; instead,
a small but cleaner subset of the data may work
best. This clean subset should ideally be free of syn-
onyms, ambiguities and anaphora, and be typed.
Several knowledge-bases such as Wordnet,
Wikipedia, and Freebase (Fellbaum, 1998;
Wikipedia, 2004; Metaweb Technologies, 2009),
are readily and freely available and they all provide
clean typed lists of entities. In our experiments
CLEANLISTS employs Freebase as a source of
clean lists, but we could use any of these or other
domain-specific ontologies such as SNOMED
(Price and Spackman, 2000) as well.
CLEANLISTS takes the intersection of Freebase
entities with our corpus to generate a clean subset for
functionality analysis. Freebase currently has over
12 million entities in over 1,000 typed lists. Thus,
this intersection retains significant portions of the
useful data, and gets rid of most of anaphora and
synonymy issues. Moreover, by matching against
typed lists, many relation ambiguities are separated
as well, since ambiguous relations often take dif-
ferent types in the arguments (e.g., ?ran(Distance)?
vs. ?ran(Company)?). To mitigate the effect of argu-
ment ambiguity, we additionally get rid of instances
in which arg1s match multiple names in the Freebase
list of names.
As an example, consider the ?was born in? rela-
tion. CLEANLISTS will remove instances with only
?Clinton? in arg1, since it matches multiple people
in Freebase. It will treat the different types, e.g.,
cities, states, countries, months separately and ana-
lyze the functionality for each of these individually.
1271
By intersecting the relation data with argument lists
for these types, we will be left with a smaller, but
much cleaner, subset of relation data, one for each
type. CLEANLISTS analyzes each subset using sim-
ple, instance based counting and computes a typed
functionality score for each type. Thus, it first com-
putes typed functionality for each relation.
There are two subtleties in applying this algo-
rithm. First, we need to identify the set of types to
consider for each relation. Our algorithm currently
picks the types that occur most in each relation?s
observed data. In the future, we could also use a
selectional preferences system (Ritter et al, 2010;
Kozareva and Hovy, 2010). Note that we remove
Freebase types such as Written Work from consid-
eration for containing many entities whose primary
senses are not that type. For example, both ?Al Gore?
and ?William Clinton? are also names of books, but
references in text to these are rarely a reference to
the written work sense.
Secondly, an argument could belong to multiple
Freebase lists. For example, ?California? is both a
city and a state. We apply a simple heuristic: if a
string appears in multiple lists under consideration,
we assign it to the smallest of the lists (the list of
cities is much larger than states). This simple heuris-
tic usually assigns an argument to its intended type.
On a development set, the error rate of this heuristic
is<4%, though it varies a bit depending on the types
involved.
CLEANLISTS determines the overall functional-
ity of a relation string by aggregating the scores for
each type. It outputs functional if a majority of typed
senses for the relation are functional. For example,
CLEANLISTS judges ?was born in? to be functional,
since all relevant type restrictions are individually
typed functional ? everyone is born in exactly one
country, city, state, month, etc.
CLEANLISTS has a much higher precision due to
the intersection with clean lists, though at some cost
of recall. The reason for lower recall is that the ap-
proach has a bias towards types that are easy to enu-
merate. It does not have different distances (e.g., 50
kms, 20 miles, etc.) in its lists. Moreover, arguments
that do not correspond to a noun cannot be handled.
For example, in the sentence, ?He weighed eating
a cheeseburger against eating a salad?, the arg2 of
?weighed? can?t be matched to a Freebase list. To
increase the recall we back off to DISTRDIFF in the
cases when CLEANLISTS is unable to make a pre-
diction. This combination gives the best balance of
precision and recall for our task. We name our final
system LEIBNIZ.
One current limitation is that using only those
arg2s that exactly match clean lists leaves out some
good data (e.g., a tuple with an arg2 of ?Univ of
Wash? will not match against a list of universities
that spells it as ?University of Washington?). Be-
cause we have access to entity types, using typed
equality checkers (Prager et al, 2007) with the clean
lists would allow us to recapture much of this useful
data. Moreover, the knowledge of functions could
apply to building new type nanotheories and reduce
considerable manual effort. We wish to study this in
the future.
5 Evaluation
In our evaluation, we wish to answer three ques-
tions: (1) How do our three approaches, Instance
Based Counting (IBC), DISTRDIFF, and CLEAN-
LISTS, compare on the functionality identification
task? (2) How does our final system, LEIBNIZ,
compare against the existing state of the art tech-
niques? (3) How well is LEIBNIZ able to identify
typed functionality for different types in the same
relation phrase?
5.1 Dataset
For our experiments we test on the set of 887 re-
lations used by Ritter et al (2008) in their exper-
iments. We use the Open IE corpus generated by
running TEXTRUNNER on 500 million high quality
Webpages (Banko and Etzioni, 2008) as the source
of instance data for these relations. Extractor and
corpus differences lead to some relations not occur-
ring (or not occurring with sufficient frequency to
properly analyze, i.e.,? 5 arg1 with? 10 evidence),
leaving a dataset of 629 relations on which to test.
Two human experts tagged these relations for
functionality. Tagging the functionality of relation
phrases can be a bit subjective, as it requires the
experts to imagine the various senses of a phrase
and judge functionality over all those senses. The
inter-annotator agreement between the experts was
95.5%. We limit ourselves to the subset of the data
1272
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0 0.2 0.4 0.6 0.8 1
Pre
cis
ion
Recall
Distr. Diff. Scoring Functions
KLfunc
KLdiff
Average
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0 0.2 0.4 0.6 0.8 1
Pre
cis
ion
Recall
Internal Methods Comparison
IBC
Distr. Diff.
CleanLists
Figure 5: (a) The best scoring method for DISTRDIFF averages KLFUNC and KLDIFF. (b) CLEANLISTS performs
significantly better than DISTRDIFF, which performs significantly better than IBC.
on which the two experts agreed (a subset of 601
relation phrases).
5.2 Internal Comparisons
First, we compare the three scoring functions for
DISTRDIFF. We vary the score thresholds to gener-
ate the different points on the precision-recall curves
for each of the three. Figure 5(a) plots these curves.
It is evident that the hybrid scoring function, i.e.,
one which is an average of KLFUNC (distance from
average functional) and KLDIFF (distance from av-
erage functional minus distance from average non-
functional) performs the best. We use this scoring in
the further experiments involving DISTRDIFF.
Next, we compare our three algorithms on
the dataset. Figure 5(b) reports the results.
CLEANLISTS outperforms DISTRDIFF by vast mar-
gins, covering a 33.5% additional area under the
precision-recall curve. Overall, CLEANLISTS finds
the very high precision points, because of its use of
clean data. However, it is unable to make 23.1% of
the predictions, primarily because the intersection
between the corpus and Freebase entities results in
very little data for those relations. DISTRDIFF per-
forms better than IBC, due to its statistical nature,
but the issues described in Section 3 plague both
these systems much more than CLEANLISTS.
To increase the recall LEIBNIZ uses a combina-
tion of DISTRDIFF and CLEANLISTS, in which the
algorithm backs off to DISTRDIFF if CLEANLISTS
is unable to output a prediction.
5.3 External Comparisons
We next compare LEIBNIZ against the existing state
of the art approaches. Our competitors are AuCon-
traire and NumericTerms (Ritter et al, 2008; Srini-
vasan and Yates, 2009). Because we use the Au-
Contraire dataset, we report the results from their
best performing system. We reimplement a version
of NumericTerms using their list of numeric quanti-
fiers and extraction patterns that best correspond to
our relation format. We run our implementation of
NumericTerms on a dataset of 100 million English
sentences from a crawl of high quality Webpages to
generate the functionality labels.
Figure 6(a) reports the results of this experiment.
We find that LEIBNIZ outperforms AuContraire by
vast margins covering an additional 44% area in the
precision-recall curve. AuContraire?s AUC is 0.61
whereas LEIBNIZ covers 0.88. A Bootstrap Per-
centile Test (Keller et al, 2005) on F1 score found
the improvement of our techniques over AuCon-
traire to be statistically significant at ? = 0.05. Nu-
mericTerms does not perform well, because it makes
decisions based only on the local evidence in a sen-
tence, and does not integrate the knowledge from
different occurrences of the same relation. It returns
many false positives, such as ?lives in?, which ap-
pear functional to the lexico-syntactic pattern, but
are clearly non-functional, e.g., one could live in
many places over a lifetime.
An example of a LEIBNIZ error is the ?repre-
sented? relation. LEIBNIZ classifies this as func-
tional, because it finds several strongly functional
senses (e.g., when a person represents a country),
1273
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0 0.2 0.4 0.6 0.8 1
Pre
cis
ion
Recall
Typed Functionality
AuContraire
NumericTerms
Leibniz
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0 0.2 0.4 0.6 0.8 1
Pre
cis
ion
Recall
External Comparison
AuContraire
NumericTerms
Leibniz
Figure 6: (a) LEIBNIZ, which is a hybrid of CLEANLISTS and DISTRDIFF, achieves 0.88 AUC and outperforms the
0.61 AUC from AuContraire (Ritter et al, 2008) and the 0.05 AUC from NumericTerms (Srinivasan and Yates, 2009).
(b) LEIBNIZ is able to tease apart different senses of polysemous relations much better than other systems.
but the human experts might have had some non-
functional senses in mind while labeling.
5.4 Typed Functionality
Next, we conduct a study of the typed functional-
ity task. We test on ten common polysemous re-
lations, each having both a functional and a non-
functional sense. An example is the ?was pub-
lished in? relation. If arg2 is a year it is func-
tional, e.g. <Harry Potter 5, was published in,
2003>. However, ?was published in(Language)?
is not functional, e.g. <Harry Potter 5, was pub-
lished in, [French / Spanish / English]>. Simi-
larly, ?will become(Company)? is functional because
when a company is renamed, it transitions away
from the old name exactly once, e.g. <Cingular,
will become, AT&T Wireless>. However, ?will be-
come(government title)? is not functional, because
people can hold different offices in their life, e.g.,
<Obama, will become, [Senator / President]>.
In this experiment, a simple baseline of predict-
ing the same label for the two types of each rela-
tion achieves a precision of 0.5. Figure 6(b) presents
the results of this study. AuContraire achieves a flat
0.5, since it cannot distinguish between types. Nu-
mericTerms can be modified to distinguish between
basic types ? check the word just after the numeric
term to see whether it matches the type name. For
example, the modified NumericTerms will search
the Web for instances of ?was published in [nu-
meric term] years? vs. ?was published in [numeric
term] languages?. This scheme works better when
the type name is simple (e.g., languages) rather than
complex (e.g., government titles).
LEIBNIZ performs the best and is able to tease
apart the functionality of various types very well.
When LEIBNIZ did not work, it was generally be-
cause of textual functionality, which is a larger issue
for typed functionality than general functionality. Of
course, these results are merely suggestive ? we per-
form a larger-scale experiment and generate a repos-
itory of typed functions next.
6 A Repository of Functional Relations
We now report on a repository of typed functional
relations generated automatically by applying LEIB-
NIZ to a large collection of relation phrases. Instead
of starting with the most frequent relations from
TEXTRUNNER, we use OCCAM?s relations (Fader
et al, 2010) because they are more specific. For in-
stance, where TEXTRUNNER outputs an underspec-
ified tuple, <Gold, has, an atomic number of 79>,
OCCAM extracts <Gold, has an atomic number of,
79>. OCCAM enables LEIBNIZ to identify far more
functional relations than TEXTRUNNER.
6.1 Addressing Evidence Sparsity
Scaling up to a large collection of typed relations
requires us to consider the size of our data sets. For
example, consider which relation is more likely to be
functional?a relation with 10 instances all of which
indicate functionality versus a relation with 100 in-
stances where 95 behave functionally.
To address this problem, we adapt the likelihood
ratio approach from Schoenmackers et al (2010).
1274
For a typed relation with n instances, f of which in-
dicate functionality, the G-test (Dunning, 1993), G
= 2*(f*ln(f/k)+(n-f)*ln((n-f)/(n-k))), provides a mea-
sure for the likelihood that the relation is not func-
tional. Here k denotes the evidence indicating func-
tionality for the case where the relation is not func-
tional. Setting k = n*0.25 worked well for us. This
G-score replaces our previous metric for scoring
functional relations.
6.2 Evaluation of the Repository
In CLEANLISTS a factor that affects the quality of
the results is the exact set of lists that is used. If
the lists are not clean, results get noisy. For exam-
ple, Freebase?s list of films contains 73,000 entries,
many of which (e.g., ?Egg?) are not films in their pri-
mary senses. Even with heuristics such as assigning
terms to their smallest lists and disqualifying dictio-
nary words that occur from large type lists, there is
still significant noise left.
Using LEIBNIZ with a set of 35 clean lists on
OCCAM?s extraction corpus, we generated a repos-
itory of 5,520 typed functional relations. To eval-
uate this resource a human expert tagged a random
subset of the top 1,000 relations. Of these relations
22% were either ill-formed or had non-sensical type
constraints. From the well-formed typed relations
the precision was estimated to be 0.8. About half
the errors were due to textual functionality and the
rest were LEIBNIZ errors. Some examples of good
functions found include isTheSequelTo(videogame)
and areTheBirthstoneFor(month). An example of
a textually functional relation found is wasThe-
FounderOf(company).
This is the first public repository of automatically-
identified functional relations. Scaling up our data
set forced us to confront new sources of noise in-
cluding extractor errors, errors due to mismatched
types, and errors due to sparse evidence. Still, our
initial results are encouraging and we hope that our
resource will be valuable as a baseline for future
work.
7 Conclusions
Functionality identification is an important subtask
for Web-scale information extraction and other ma-
chine reading tasks. We study the problem of pre-
dicting the functionality of a relation phrase auto-
matically from Web text. We presented three algo-
rithms for this task: (1) instance-based counting, (2)
DISTRDIFF, which takes a statistical approach and
discriminatively classifies the relations using aver-
age arg-distributions, and (3) CLEANLISTS, which
performs instance based counting on a subset of
clean data generated by intersection of the corpus
with a knowledge-base like Freebase.
Our best approach, LEIBNIZ, is a hybrid of
DISTRDIFF and CLEANLISTS, and outperforms
the existing state-of-the-art approaches by covering
44% more area under the precision-recall curve. We
also observe that an important sub-component of
identifying a functional relation phrase is identifying
typed functionality, i.e., functionality when the ar-
guments of the relation phrase are type-constrained.
Because CLEANLISTS is able to use typed lists, it
can successfully identify typed functionality.
We run our techniques on a large set of relations to
output a first repository of typed functional relations.
We release this list for further use by the research
community.2
Future Work: Functionality is one of the sev-
eral properties a relation can possess. Others in-
clude selectional preferences, transitivity (Schoen-
mackers et al, 2008), mutual exclusion, symme-
try, etc. These properties are very useful in increas-
ing our understanding about these Open IE relation
strings. We believe that the general principles devel-
oped in this work, for example, connecting the Open
IE knowledge with an existing knowledge resource,
will come in very handy in identifying these other
properties.
Acknowledgements
We would like to thank Alan Ritter, Alex Yates and
Anthony Fader for access to their data sets. We
would like to thank Stephen Soderland, Yoav Artzi,
and the anonymous reviewers for helpful comments
on previous drafts. This research was supported
in part by NSF grant IIS-0803481, ONR grant
N00014-08-1-0431, DARPA contract FA8750-09-
C-0179, a NDSEG Fellowship, and carried out at
the University of Washington?s Turing Center.
2available at http://www.cs.washington.edu/
research/leibniz
1275
References
M. Banko and O. Etzioni. 2008. The tradeoffs between
open and traditional relation extraction. In Proceed-
ings of the 46th Annual Meeting of the Association for
Computational Linguistics (ACL).
M. Banko, M. Cafarella, S. Soderland, M. Broadhead,
and O. Etzioni. 2007. Open information extraction
from the web. In Proceedings of the 20th International
Joint Conference on Artificial Intelligence (IJCAI).
A. Culotta, A. McCallum, and J. Betz. 2006. Integrating
probabilistic extraction models and data mining to dis-
cover relations and patterns in text. In Proceedings of
the HLT-NAACL.
D. Downey, O. Etzioni, and S. Soderland. 2005. A prob-
abilistic model of redundancy in information extrac-
tion. In Proceedings of the 19th International Joint
Conference on Artificial Intelligence (IJCAI).
T. Dunning. 1993. Accurate methods for the statistics of
surprise and coincidence. In Computational Linguis-
tics, volume 19.
A. Fader, O. Etzioni, and S. Soderland. 2010. Identi-
fying well-specified relations for open information ex-
traction. (In preparation).
C. Fellbaum, editor. 1998. WordNet: An Electronic Lex-
ical Database. MIT Press, Cambridge, MA.
N. Guarino and C. Welty. 2004. An overview of On-
toClean. In Handbook of Ontologies in Information
Systems, pages 151?172.
J. Hopcraft and R. Tarjan. 1973. Efficient algorithms
for graph manipulation. Communications of the ACM,
16:372?378.
Y. Huhtala, J. Ka?rkka?inen, P. Porkka, and H. Toivonen.
1999. TANE: An efficient algorithm for discover-
ing functional and approximate dependencies. In The
Computer Journal.
M. Keller, S. Bengio, and S.Y. Wong. 2005. Bench-
marking non-parametric statistical tests. In Advances
in Neural Information Processing Systems (NIPS) 18.
K. Kipper-Schuler. 2005. Verbnet: A broad-coverage,
comprehensive verb lexicon. In Ph.D. thesis. Univer-
sity of Pennsylvania.
Z. Kozareva and E. Hovy. 2010. Learning arguments
and supertypes of semantic relations using recursive
patterns. In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics (ACL).
S. Kullback and R.A. Leibler. 1951. On information
and sufficiency. Annals of Mathematical Statistics,
22(1):79?86.
Metaweb Technologies. 2009. Freebase data dumps. In
http://download.freebase.com/datadumps/.
A-M. Popescu. 2007. Information extraction from un-
structured web text. In Ph.D. thesis. University of
Washington.
J. Prager, S. Luger, and J. Chu-Carroll. 2007. Type nan-
otheories: a framework for term comparison. In Pro-
ceedings of the 16th ACM Conference on Information
and Knowledge Management (CIKM).
C. Price and K. Spackman. 2000. SNOMED clincal
terms. In British Journal of Healthcare Computing &
Information Management, volume 17.
A. Ritter, D. Downey, S. Soderland, and O. Etzioni.
2008. It?s a contradiction - no, it?s not: A case study
using functional relations. In Proceedings of the 2008
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP).
A. Ritter, Mausam, and O. Etzioni. 2010. A latent dirich-
let alocation method for selectional preferences. In
Proceedings of the 48th Annual Meeting of the Associ-
ation for Computational Linguistics (ACL).
S. Schoenmackers, O. Etzioni, and D. Weld. 2008. Scal-
ing textual inference to the web. In Proceedings of
the 2008 Conference on Empirical Methods in Natural
Language Processing (EMNLP).
S. Schoenmackers, J. Davis, O. Etzioni, and D. Weld.
2010. Learning first-order horn clauses from web text.
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing (EMNLP).
P. Srinivasan and A. Yates. 2009. Quantifier scope
disambiguation using extracted pragmatic knowledge:
Preliminary results. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP).
J. Volker, D. Vrandecic, and Y. Sure. 2005. Auto-
matic evaluation of ontologies (AEON). In Proceed-
ings of the 4th International Semantic Web Conference
(ISWC).
Wikipedia. 2004. Wikipedia: The free encyclopedia. In
http://www.wikipedia.org. Wikimedia Foundation.
H. Yao and H. Hamilton. 2008. Mining functional de-
pendencies from data. In Data Mining and Knowledge
Discovery.
A. Yates and O. Etzioni. 2009. Unsupervised methods
for determining object and relation synonyms on the
web. In Journal of Artificial Intelligence Research,
volume 34, pages 255?296.
1276
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 893?903, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
No Noun Phrase Left Behind: Detecting and Typing Unlinkable Entities
Thomas Lin, Mausam, Oren Etzioni
Computer Science & Engineering
University of Washington
Seattle, WA 98195, USA
{tlin, mausam, etzioni}@cs.washington.edu
Abstract
Entity linking systems link noun-phrase men-
tions in text to their corresponding Wikipedia
articles. However, NLP applications would
gain from the ability to detect and type all
entities mentioned in text, including the long
tail of entities not prominent enough to have
their own Wikipedia articles. In this paper we
show that once the Wikipedia entities men-
tioned in a corpus of textual assertions are
linked, this can further enable the detection
and fine-grained typing of the unlinkable en-
tities. Our proposed method for detecting un-
linkable entities achieves 24% greater accu-
racy than a Named Entity Recognition base-
line, and our method for fine-grained typing is
able to propagate over 1,000 types from linked
Wikipedia entities to unlinkable entities. De-
tection and typing of unlinkable entities can
increase yield for NLP applications such as
typed question answering.
1 Introduction
A key challenge in machine reading (Etzioni et al
2006) is to identify the entities mentioned in text,
and associate them with appropriate background in-
formation such as their type. Consider the sentence
?Some people think that pineapple juice is good for
vitamin C.? To analyze this sentence, a machine
should know that ?pineapple juice? refers to a bev-
erage, while ?vitamin C? refers to a nutrient.
Entity linking (Bunescu and Pas?ca, 2006;
Cucerzan, 2007) addresses this problem by link-
ing noun phrases within the sentence to entries
in a large, fixed entity catalog (almost always
example noun phrases status
?apple juice? ?orange juice? present
?prune juice? ?wheatgrass juice? absent
?radiation exposure? ?workplace stress? present
?asbestos exposure? ?financial stress? absent
?IJCAI? ?OOPSLA? present
?EMNLP? ?ICAPS? absent
Table 1: Wikipedia has entries for prominent entities,
while missing tail and new entities of the same types.
Wikipedia). Thus, entity linking has a limited and
somewhat arbitrary range. In our example, systems
by (Ferragina and Scaiella, 2010) and (Ratinov et
al., 2011) both link ?vitamin C? correctly, but link
?pineapple juice? to ?pineapple.? ?Pineapple juice?
is not entity linked as a beverage because it is not
prominent enough to have its own Wikipedia entry.
As Table 1 shows, Wikipedia often has prominent
entities, while missing tail and new entities of the
same types.1 (Wang et al2012) notes that there
are more than 900 different active shoe brands, but
only 82 exist in Wikipedia. In scenarios such as in-
telligence analysis and local search, non-Wikipedia
entities are often the most important.
Hence, we introduce the unlinkable noun phrase
problem: Given a noun phrase that does not link
into Wikipedia, return whether it is an entity, as well
its fine-grained semantic types. Deciding if a non-
Wikipedia noun phrase is an entity is challenging
because many of them are not entities (e.g., ?Some
people,? ?an addition? and ?nearly half?). Predict-
1The same problem occurs with Freebase, which is also
missing the same Table 1 entities.
893
ing semantic types is a challenge because of the di-
versity of entity types in general text. In our experi-
ments, we utilized the Freebase type system, which
contains over 1,000 semantic types.
The first part of this paper proposes a novel
method for detecting entities by observing that enti-
ties often have different usage-over-time character-
istics than non-entities. Evaluation shows that our
method achieves 24% relative accuracy gain over
a NER baseline. The second part of this paper
shows how instance-to-instance class propagation
(Kozareva et al2011) can be adapted and scaled to
semantically type general noun-phrase entities using
types from linked entities, by leveraging over one
million different possible textual relations.
Contributions of our research include:
? We motivate and introduce the unlinkable noun
phrase problem, which extends previous work
in entity linking.
? We propose a novel method for discriminating
entities from arbitrary noun phrases, utilizing
features derived from Google Books ngrams.
? We adapt and scale instance-to-instance class
propagation in order to associate types with
non-Wikipedia entities.
? We implement and evaluate our methods, em-
pirically verifying improvement over appropri-
ate baselines.
2 Background
In this section we provide an overview of entity link-
ing, how we entity link our data set, and describe
how our problem and approach differ from related
areas such as NER and Web extraction.
2.1 Entity Linking
Given text, the task of entity linking (Bunescu
and Pas?ca, 2006; Cucerzan, 2007; Milne and Wit-
ten, 2008; Kulkarni et al2009) is to identify the
Wikipedia entities within the text, and mark them
with which Wikipedia entity they correspond to. En-
tity linking elevates us from plain text into mean-
ingful entities that have properties, semantic types,
and relationships with each other. Other entity cata-
logs can be used in place of Wikipedia, especially in
domain-specific contexts, but general purpose link-
ing systems all use Wikipedia because of its broad
general coverage, and to leverage its article texts and
link structure during the linking process.
A problem we observed when using entity link-
ing systems is that despite containing over 3 million
entities, Wikipedia does not cover a significant num-
ber of entities. This occurs with entities that are not
prominent enough to have their own dedicated arti-
cle and with entities that are very new. For exam-
ple, Facebook has over 600 million users, and each
of them could be considered an entity. The REVERB
extractor (Fader et al2011) on the ClueWeb09 Web
corpus found over 1.4 billion noun phrases partic-
ipating in textual relationships, and a sizable por-
tion of these noun phrases are entities. While re-
cent research has used NIL features to determine
whether they are being asked to link an entity not in
Wikipedia (Dredze et al2010; Ploch, 2011), there
has been no research on whether given noun phrases
that are unlinkable (for not being in Wikipedia) are
entities, and how to make them usable if they are.
Our goal is to address this problem of learning
whether non-Wikipedia noun phrases are entities,
and assigning semantic types to them to make them
useful. We begin with a corpus of 15 million ?(noun
phrase subject, textual relation, noun phrase object)?
assertions from the Web that were extracted by RE-
VERB (Fader et al2011).2 REVERB already filters
out relative pronouns, WHO-adverbs, and existential
?there? noun phrases that do not make meaningful
arguments. We then employ standard entity linking
techniques including string matching, prominence
priors (Fader et al2009), and context matching
(Bunescu and Pas?ca, 2006) to link the noun phrase
subjects into Wikipedia.
In this manner, we were able to entity link the
noun phrase subject of 9,699,967 extractions, while
the remaining 5,028,301 extractions had no matches
(mostly due to no close string matches). There were
1,401,713 distinct noun phrase subjects in the 5 mil-
lion extractions that had no matches. These are the
unlinkable noun phrases we will study here.
2.2 Named Entity Recognition
Named Entity Recognition (NER) is the task of
identifying named entities in text. A key difference
between our final goals and NER is that in the con-
2available at http://reverb.cs.washington.edu
894
text of entity linking and Wikipedia, there are many
more entities than just the named entities. For ex-
ample, ?apple juice? and ?television? are Wikipedia
entities (with Wikipedia articles), but are not tradi-
tional named entities. Still, as named entities do
comprise a sizable portion of our unlinkable noun
phrases, we compare against a NER baseline in our
entity detection step.
Fine-grained NER (Sekine and Nobata, 2004; Lee
et al2007) has studied scaling NER to up to 200 se-
mantic types. This differs from our semantic typing
of unlinked entities because our approach assumes
access to corpora-level relationships between a large
set of linked entities (with semantic types) and the
unlinked entities. As a result we are able to propa-
gate 1,339 Freebase semantic types from the linked
entities to the unlinked entities, which is substan-
tially more types than fine-grained NER.
2.3 Extracting Entity Sets
There is a line of research in using Web extraction
(Etzioni et al2005) and entity set expansion (Pantel
et al2009) to extract lists of typed entities from the
Web (e.g., a list of every city). Our problem instead
focuses on determining whether any individual noun
phrase is an entity, and what semantic types it holds.
Given a noun phrase representing a person name, we
return that this is a person entity even if it is not in a
list of people names harvested from the Web.
3 Architecture
Our goal is: given (1) a large set of linked assertions
L and (2) a large set of unlinked assertions U , for
each unlinkable noun phrase subject n ? U , predict:
(1) whether n is an entity, and if so, then (2) the set
of Freebase semantic types for n. For L we use the
9.7 million assertions whose subject argument we
were able to link in Section 2.1, and for U we use
the 5 million assertions that we could not link.
We divide the system into two components. The
first component (described in Section 4) takes any
unlinkable noun phrase and outputs whether it is an
entity. All n ? U classified as entities are placed in
a set E. The second component (described in Sec-
tion 5) uses L and U to predict the semantic types
for each entity e ? E.
Figure 1: Usage over time in Google books for the noun
phrase ?Prices quoted? (e.g., from ?Prices quoted are for
2 adults?) which is not an entity.
Figure 2: Usage over time for the unlinkable noun phrase
?Soluble fibre,? which is an entity. The best fit line has
steeper slope compared to Figure 1.
4 Detecting Unlinkable Entities
This first task takes in any unlinkable noun phrase
and outputs whether it is an entity. There is a long
history of discussion in analytic philosophy litera-
ture on the question of what exists (e.g., (Quine,
1948)). We adopt a more pragmatic view, defin-
ing an ?entity? as a noun phrase that could have a
Wikipedia-style article if there were no notability or
newness considerations, and which would have se-
mantic types. We are interested in entities that could
help populate an entity store. ?EMNLP 2012? is an
example of an entity, while ?The method? and ?12
cats? are examples of noun phrases that are not en-
tities. This is challenging because at a surface level,
many entities and non-entities look similar: ?Sex
and the City? is an entity, while ?John and David?
is not. ?Eminem? is an entity, while ?Youd? (a typo
from ?You?d?) is not.
We address this task by training a classifier with
features primarily derived from a timestamped cor-
pus. An intuition here is that when considering
only unlinkable noun phrases, usage patterns across
895
Figure 3: Plot of R2 vs Slope for the usage over time of a collection of noun phrases selected for illustrative purposes.
Many of the non-entities occur at lower Slope and higher R2, while the entities often have higher slope and/or lower
R2. ?Bluetooth technology? actually has even higher slope, but was adjusted left to fit in this figure.
time often differ for entities and non-entities. Noun
phrase entities that are observed in text going back
hundreds of years (e.g., ?Europe?) almost all have
their own Wikipedia entries, so in unlinkable noun
phrase space, the remaining noun phrases that are
observed in text going back hundreds of years tend
to be all the textual references and expressions that
are not entities. We plan to use this signal to help
separate the entities from the non-entities.
4.1 Classifier Features
We use the Google Books ngram corpus (Michel
et al2010), which contains timestamped usage
of 1-grams through 5-grams in millions of digi-
tized books for each year from 1500 to 2007.3 We
use ngram match count values from case-insensitive
matching. To avoid sparsity anomalies we observed
in years before 1740, we use the data from 1740 on-
ward. While it has not been used for our task before,
this corpus is a rich resource that enables reason-
ing about knowledge (Evans and Foster, 2011) and
3available at http://books.google.com/ngrams/datasets
understanding semantic changes of words over time
(Wijaya and Yeniterzi, 2011). Talukdar et al2012)
recently used it to effectively temporally scope rela-
tional facts.
Our first feature is the slope of the least-squares
best fit line for usage over time. For example, if a
term appears 25 times in books in 1950, 30 times in
1951, ..., 100 times in 2007, then we compute the
straight line that best fits {(1950, 25), (1951, 31), ...,
(2007, 100)}, and examine the slope. We have ob-
served cases of non-entity noun phrases (e.g., Fig-
ure 1) having lower slopes than entity noun phrases
(e.g., Figure 2). Note that we do not normalize
match counts by yearly total frequency, but we do
normalize counts for each term to range from 0 to 1
(setting the max count for each term to 1) to avoid
bias from entity prominence. To capture the current
usage, in cases where there exists a ? 5 year gap in
usage of a term we only use the data after the gap.
Another feature is the R2 fit of the best fit line.
Higher R2 indicates that the data is closer to a
straight line. Figure 3 plots R2 vs Slope values
896
Figure 4: UsageSinceYear of example unlinked terms.
for some sample noun phrases. We observed that
along with their lower Slope, the non-entities often
also had higher R2, indicating that their usage does
not vary wildly from year to year. This contrasts
with certain entities (e.g., ?FY 99? for ?Fiscal Year
1999?) whose usage sometimes varied sharply from
year to year based on their prominence in those spe-
cific years.
A third feature is UsageSinceYear, which finds the
year from when a term last started continually being
used. For example, a UsageSinceYear value of 1920
would indicate that the term was used in books ev-
ery year from 1920 through 2007. Figure 4 shows
examples of where various terms fall along this di-
mension.
From the books ngram corpus, we also calcu-
late features for: PercentProperCaps - the percent-
age of case-insensitive matches for the term where
all words began with a capital letter, PercentExact-
Match - the percentage of case-insensitive matches
for the term that matched the capitalization in the
assertion exactly, and Frequency - the total number
of case-insensitive occurrences of the term in the
book ngrams data, summed across all years, which
reflects prominence. Last, we also include a sim-
ple numeric feature to detect the presence of leading
numeric words (e.g., ?5? in ?5 days? or ?Three? in
?Three choices?).
4.2 Evaluation
From the corpus of 15 million REVERB assertions,
there were 1.4 million unlinked noun phrases includ-
ing 17% unigrams, 51% bigrams, 21% trigrams, and
11% 4-grams or longer. Bigrams comprise over half
the noun phrases and the books bigram data is a self-
contained download that is easier to obtain and store
system correctly classified
Majority class baseline 50.4%
Named Entity Recognition 63.3%
Slope feature only 61.1%
PUF feature combination 69.1%
ALL features 78.4%
Table 2: Our classifier using all features (ALL) outper-
forms majority class and NER baselines.
than the full books ngram corpus, so we focus on
bigrams in our evaluation. In a random sample of
unlinked bigrams, we found that 73% were present
in the books ngram data (65% exact match, 8% case-
insensitive match only), while 27% were not (these
were mostly entities or errors with non-alphabetic
characters). Coverage is a greater issue with longer
ngrams (e.g., there are many more possible 5-grams
than bigrams, so any individual 5-gram is less likely
to reach the minimum threshold to be included in the
books data), but as mentioned earlier, only 11% of
unlinkable noun phrases were 4-grams or longer.
We randomly sampled 250 unlinked bigrams that
had books ngram data, and asked 2 annotators to la-
bel each as ?entity,? ?non-entity,? or ?unclear.? Our
goal is to separate noun phrases that are clearly en-
tities (e.g., ?prune juice?) from those that are clearly
not entities (e.g., ?prices quoted?), rather than to de-
bate phrases that may be in some entity store defi-
nitions but not others, so we asked the annotators to
choose ?unclear? when there was any doubt. There
were 151 bigrams that both annotators believed to
be very clear labels, including 69 that both annota-
tors labeled as entities, 70 that both annotators la-
beled as non-entities, and 12 with label disagree-
ment. Cohen?s kappa was 0.84, indicating excellent
agreement. Our experiment is now to separate the
69 clear entities from the 70 clear non-entities.
For experiment baselines we use the majority
class baseline MAJ, as well as a Named Entity
Recognition baseline NER. For NER we used the
Illinois Named Entity Tagger (Ratinov and Roth,
2009) on the highest setting (that achieved 90.5 F1
score on the CoNLL03 shared task). NER expects a
sentence, so we use the longest assertion in the cor-
pus that the noun phrase was observed in. We eval-
uate several combinations of our features to test dif-
897
ferent aspects of our system: Slope uses only Slope,
PUF uses PercentProperCaps + UsageSinceYear +
Frequency, and ALL uses all features. We evaluate
using the WEKA J48 Decision Tree on default set-
tings, with leave-one-out cross validation.
Table 2 shows the results. MAJ correctly classifies
50.4% of instances, NER correctly classifies 63.3%
and ALL correctly classifies 78.4%.
4.3 Analysis
Overall, 78.4% correctly classified instances is fairly
strong performance on this task. By using the de-
scribed features, our classifier was able to detect and
filter many of the non-entity noun phrases in this
scenario. Compared to the 63.3% of NER, it is an
absolute gain of 15.1%, a relative gain of 24%, and a
reduction in error of 41.1% (from 36.7% to 21.6%).
Student?s t-test at 95% confidence verified that the
difference was significant.
We found that while low Slope (especially with
higher R2) often indicated non-entity, there were nu-
merous cases where higher Slope did not necessarily
indicate entity. For example, the noun phrase ?sev-
eral websites? has fairly sharp slope, but still does
not denote a clear entity. In these cases, the addi-
tion of other features can serve as additional useful
signal. One error from ALL is the term ?Analyst esti-
mates,? which the annotators labeled as a non-entity,
but which occasionally appears in text (especially ti-
tles) as ?Analyst Estimates,? and is a relatively re-
cent phrase. NER misses entities such as ?synthetic
cubism? and ?hunter orange? that occur in our data
but are not traditional named entities. We observed
that while none of our features achieves over 70%
accuracy by themselves, they perform well in con-
junction with each other.
5 Propagating Semantic Types
This second task uses a set of linked assertions L and
set of unlinked assertions U to predict the semantic
types for each entity e ? E. If the previous step
output that ?Sun Microsystems? is likely to be an
entity, then the goal of this step is to further predict
that it has the Freebase types such as organization
and software developer.
From L we use the set of linked entities and the
textual relations they occur with. For example, L
might contain that the entity Microsoft links to a par-
ticular Wikipedia article, and also that it occurs with
textual relations such as ?has already announced?
and ?has released updates for.? For each Wikipedia-
linked entity in L, we further look up its exact set
of Freebase types.4 From U we obtain the set of
textual relations that each e ? E is in the domain
of. We now have a large set of class-labeled in-
stances (all entities in L), a large set of unlabeled
instances (E), and a method to connect the unla-
beled instances with the class-labeled instances (via
any shared textual relations), so we cast this task
as an instance-to-instance class propagation problem
(Kozareva et al2011) for propagating class labels
from labeled to unlabeled instances.
We build on the recent work of Kozareva et al
(2011), and adapt their approach to leverage the
scale and resources of our scenario. While they use
only one type of edge between instances, namely
shared presence in the high precision DAP pattern
(Hovy et al2009), our final system uses 1.3 mil-
lion textual relations from |L ? U |, corresponding
to 1.3 million potential edge types. Their evaluation
involved only 20 semantic classes, while we use all
1,339 Freebase types covered by our entities in L.
There is a rich history of other approaches for
predicting semantic types. (Talukdar et al2008)
and (Talukdar and Pereira, 2010) model relation-
ships between instances and classes, but our un-
linked entities do not come with any class informa-
tion. Pattern-based approaches (Pas?ca, 2004; Pantel
and Ravichandran, 2004) are popular, but (Kozareva
et al2011) notes that they ?are constraint to the in-
formation matched by the pattern and often suffer
from recall,? meaning that they do not cover many
instances. Classifiers have also been trained for fine-
grained semantic typing, but for noticeably fewer
types than we work with. (Rahman and Ng, 2010)
studied hierarchical and collective classification us-
ing 92 types, and FIGER (Ling and Weld, 2012) re-
cently used an adapted perceptron for multi-class
multi-label classification into 112 types.
5.1 Algorithm
Given an entity e, our algorithm involves: (1) find-
ing the textual relations that e is in the domain of, (2)
4data available at http://download.freebase.com/wex
898
Figure 5: This example illustrates the set of Freebase type predictions for the noun phrase ?Sun Microsystems.? We
predict the semantic type of a noun phrase by: (1) finding the textual relations it is in the domain of, (2) finding linked
entities that are also in the domain of those textual relations, and (3) observing their semantic types.
finding linked entities that are also in the domain of
those textual relations, and then (3) predicting types
by observing the types of those linked entities. Fig-
ure 5 illustrates how we would predict the semantic
types of the noun phrase ?Sun Microsystems.?
Find Relations: Obtain the set R of all textual re-
lations in U that e is in the domain of. For example,
if U contains the assertion ?(Sun Microsystems, has
released a minor update to, Java 1.4.2),? then the tex-
tual relation ?has released a minor update to? should
be added to R when typing ?Sun Microsystems.?
Find Similar Entities: Find the linked entities in
L that are in the domain of the most relations in
R. In our example, entities such as ?Microsoft? and
?Apple Inc.? have the greatest overlap in textual re-
lations because they are most often in the domain
of the same textual relations, e.g., (?Microsoft, has
released a minor update to, Windows Live Essen-
tials?). Create a set S of the entities that share the
most textual relations. We found keeping 10 similar
entities (|S| = 10) is generally enough to predict the
original entity?s types in the final step.
Predict Types: Return the most frequent Freebase
types of the entities in S as the prediction. To
avoid penalizing very small types, if there are n in-
stances of semantic class C in S, then we rank C us-
ing a type score T (n,C, S) = max(n/|S|, n/|C|),
which we found to perform better than T (n,C, S) =
avg(n/|S|, n/|C|). For ?Sun Microsystems,? busi-
ness operation was the top predicted type because
all entities in S were observed to include business
operation type.
5.2 Edge Validity
This algorithm will only be effective if entities that
share textual relation strings are more likely to be
of the same semantic types. To verify this, we sam-
pled 30,000 linked entities from L that had at least
30 textual relations each, and associated each with
their 30 most frequent relations. From the 900 mil-
lion possible entity pairs, we then randomly sample
500 entity pairs that shared exactly k out of 30 rela-
tions, for each k from 0 to 15. At each k we then use
our sampled pairs to estimate the probability that any
two entities sharing exactly k relations (out of their
30 possible) will share at least one type.
The results are shown in Figure 6. We found that
entities sharing more textual relations were in fact
more likely to have semantic types in common. Two
entities that shared exactly 0 of 30 textual relations
were only 11% likely to share a semantic type, while
two entities that shared exactly 10 of 30 relations
were 80% likely to share a semantic type. This vali-
dates our use of textual relations as a signal-bearing
edge in instance-to-instance class propagation.
5.3 Weighting Textual Relations
The algorithm as currently described treats all tex-
tual relations equally, when in reality some are
stronger signal to entity type than others. For exam-
ple, two entities in the domain of the ?came with?
relation often will not share semantic types, but two
entities in the domain of the ?autographed? relation
will almost always share a type. To capture this intu-
ition, we define relation weight w(r) as the observed
probability (among the linked entities) that two en-
899
Figure 6: Entities that share more textual relations are
more likely to have semantic types in common.
tities will share a Freebase type if they both occur
in the domain of r. If D(r) = all entities observed
in the domain of relation r and T (e) = all Freebase
types listed for entity e, then weight w(r) of a tex-
tual relation string r is:
w(r) =
?
e1,e2?D(r), e1 6=e2
I(e1, e2)
|D(r)| ? (|D(r)| ? 1)
I(e1, e2) =
{
1, if |T (e1) ? T (e2)| > 0
0, otherwise.
Table 3 shows examples of high weight relations,
and Table 4 shows low weight relations. We now
modify the Find Similar Entities step such that if
a linked entity shares a set of relations Q with the
entity being typed, then it receives a score which
considers all shared relations q ? Q but uses the
high weight relations more. On a development set
we found that a score of
?
q?Q 10
4?w(q) was effec-
tive, as higher weight signifies much stronger signal.
This score then determines which entities to place in
S.
5.4 Evaluation
The goal of the evaluation is to judge how well our
method can predict the Freebase semantic types of
entities in our scenario. Our linked entities cov-
ered 1,339 Freebase types, including many interest-
ing types such as computer operating system, reli-
gious text, airline and baseball team. Human judges
would have trouble manually annotating new enti-
ties with all these types because there are too many
to keep in mind and understand the characteristics
?is a highway in?
?is a university located in?
?became the president of?
?turned down the role of?
?has an embassy in?
Table 3: Example relations found to have high weight.
?comes with?
?is a generic term for?
?works best on?
?can be made from?
?is almost identical to?
Table 4: Example relations found to have low weight.
of. Instead, we automatically generate testing data
by sampling entities from L, and then test on abil-
ity to recover the actual Freebase types (which we
know).
We sample a HEAD set of distinct 500 Freebase
entities (drawn randomly from our set of linked ex-
tractions), and a TAIL set of 500 entities (drawn ran-
domly from our set of linked entities). An entity
that occurs in many extractions is more likely to be
in HEAD than TAIL. Our sampling also picks only
entities that occur with at least 10 relations, which
is appropriate for the Web scenario where more in-
stances can always be queried for.
For baselines we use random baseline BRandom
and a frequency baseline BFrequency which always
returns types in order of their frequency among
all linked entities (e.g., always person, then loca-
tion, etc). We evaluate our system without rela-
tion weighting (SNoWeight) and also with relation
weighting (SWeighted). For SWeighted we leave all
the test set entities out when calculating global re-
lation weights. Our metrics are Precision at 1 and
F1 score. Precision at 1 measures how often the top
returned type is a correct type, and is useful for ap-
plications that want one type per entity. F1 mea-
sures how well the method recovers the full set of
Freebase types (for each test case we graph preci-
sion/recall and take the max F1), and is useful for
applications such as typed question answering.
Table 5 shows the results. BRandom performs
poorly because there are so many semantic types,
and very few of them are correct for each test
case. BFrequency performs slightly better on TAIL
than HEAD because TAIL contains more entities of
the most frequent types. SNoWeight performance
900
HEAD TAIL
Prec@1 F1 Prec@1 F1
BRandom 0.008 0.028 0.004 0.023
BFrequency 0.244 0.302 0.298 0.322
SNoWeight 0.542? 0.465? 0.510? 0.456?
SWeighted 0.610? 0.521? 0.598? 0.522?
Table 5: Evaluation on HEAD and TAIL, 500 elements each. ? indicates statistical significance over BFrequency, and
? over both BFrequency and SNoWeight. Significance is measured using the Student?s t-test at 95% confidence. The
top type predicted by our SWeighted method is correct about 60% of the time, while the top type predicted by the
BFrequency baseline is correct under 30% of the time.
is statistically significant above all baselines, and
SWeighted is statistically significant over SNoWeight
on both test sets and metrics.
5.5 Analysis
SWeighted was successful at recovering the correct
Freebase types of many entities. For example, it
finds that ?Atherosclerosis? is a medical risk fac-
tor by connecting it to ?obesity? and ?diabetes,? that
?Supernatural? is a TV program and a Netflix title
by connecting it to ?House? and ?30 Rock,? and that
?America West? is an aircraft owner and an airline
by connecting it to ?Etihad Airways? and ?China
Eastern Airlines.? While precision at 1 around 60%
may not be high enough yet for certain applications,
it is significantly better than competing approaches,
which are under 30%, and we hope that our values
can serve as a non-trivial baseline on this task for
future systems.
One example where SWeighted made some mis-
takes is fictional characters. Many fictional charac-
ters participate in a textual relations that make them
look like people (e.g., ?was born on?), but predicting
that they belong to people class is incorrect. Some
performance hit was also due to entity linking errors.
From an assertion like ?The Four Seasons is located
in Hamamatsu,? our entity linker (and other entity
linkers we tried) prefer linking ?The Four Seasons?
to Vivaldi?s music composition rather than the hotel
chain. We are then unable to recover music compo-
sition type from relations like ?is located in.? Our
algorithm relies on accurate entity linking in L, but
there is a precision/recall tradeoff to consider here
because it also benefits from higher coverage of en-
tities and relations in L.
As a general reference for performance of
state-of-the-art fine-grained entity classification, the
FIGER system (Ling and Weld, 2012) for classify-
ing into 112 types reported F1 scores ranging from
0.471 to 0.693 in their experiments. It is important to
note that these numbers are not directly comparable
to us because they used different data, different (and
fewer) types, and different evaluation methodology.
6 Discussion
This paper presented an approach for working with
non-Wikipedia entities in text. Consider the follow-
ing possibilities for a noun phrase in a text corpus:
Wikipedia Entity: (e.g., ?Computer Science,?
?South America,? ?apple juice?) - Entity linking
techniques can identify and type these.
Non-Wikipedia, Non-Entity: (e.g., ?strange
things,? ?Early studies,? ?A link?) - Our classifier
from Section 4 is able to filter these.
Non-Wikipedia, Entity: (e.g., ?Safflower oil,?
?prune juice,? ?Amazon UK?) - We identify these
as entities, then propagate semantic types to them.
Our technique finds that ?Safflower oil? occurs with
high weight relations such as ?is sometimes used to
treat? and ?can be substituted for,? making it similar
to linked entities such as ?Phentermine? and ?Dan-
delion,? and then correctly predicts semantic types
including food ingredient and medical treatment.
6.1 Typed Question Answering
From our set of 15 million assertions, we found and
typed many non-Wikipedia entities. In food while
Wikipedia has ?crab meat,? we find it is missing oth-
ers such as ?rabbit meat? and ?goat milk.? In job ti-
tles it has ?scientist? and ?lawyer,? but we find it is
missing ?PhD student,? ?fashion designer,? and oth-
ers. We find many of the people and employers not
901
prominent enough for Wikipedia.
One application of this research is to increase the
yield of applications such as Typed Question An-
swering (Buscaldi and Rosso, 2006). For example,
consider the query ?What computer game is a lot of
fun?? A search for assertions matching ?* is a lot
of fun? in the data yields around 1,000 results such
as ?camping,? ?David Sedaris? and ?Hawaii.? En-
tity linking allows us to identify just the computer
games in Wikipedia that match the query, such as
?Civilization.? However, around 400 query matches
could not be entity linked. Our noun-phrase clas-
sifier filters out non-entities such as ?actual play,?
?Just this? and ?Two kids.? After predicting types
for the matches that did not get filtered, we find ad-
ditional non-Wikipedia computer games that match
the query, including ?Cooking Dash,? ?Delicious
Deluxe? and ?Slingo Supreme.?
7 Future Work
An area we are continuing to improve the system
on is textual ambiguity. For example, an unlinkable
noun phrase might simultaneously be the name of a
film, a car, and a person. Instead of outputting that
the noun phrase holds all of those types, a stronger
output would be to realize that the noun phrase is
ambiguous, determine how many senses it has, and
determine which sense is being referred to in each
instance. We have ideas for how to detect ambiguous
entities using mutual exclusion (Carlson, 2010) and
functional relations. For example, if we predict that
a noun phrase has film and car types but we also
observe in our linked instances that these types are
mutually exclusive, then this is good evidence that
the noun phrase refers to multiple terms.
We also plan to continue improving our tech-
niques, as there is still plenty of room for improve-
ment on both subtasks. For detecting new entities,
we are interested in seeing if timestamped Twitter
data could be analyzed to increase both recall and
precision. For predicting semantic types, (Kozareva
et al2011) proposed additional techniques which
we have not fully explored. Also, we can incor-
porate additional signals such as shared term heads
when they are available, in order to help find terms
that are likely to share types. Last, we would like
to feed back our system output to improve system
performance. For example, non-entity noun phrases
that make it to the typing step might lead to particu-
lar predicted type distributions that indicate an error
occurred earlier in the process.
8 Conclusion
In this paper we showed that while entity linking
cannot link to entities outside of Wikipedia, once a
large text corpus has been entity linked, the presence
and content of the existing links can be leveraged to
help detect and semantically type the non-Wikipedia
entities as well. We designed techniques for de-
tecting whether unlinkable noun phrases are entities,
and if they are, then propagating semantic types to
them from the linked entities. In our evaluations, we
showed that our techniques achieve statistically sig-
nificant improvement over baseline methods.
Our research here takes initial steps toward a fu-
ture where the vast universe of entities that are not
prominent enough to include in manually-authored
knowledge bases is analyzed automatically instead
of being left behind.
Acknowledgements
We thank Stephen Soderland, Xiao Ling, and the
three anonymous reviewers for their helpful feed-
back on earlier drafts. This research was sup-
ported in part by NSF grant IIS-0803481, ONR grant
N00014-08-1-0431, and DARPA contract FA8750-
09-C-0179, and carried out at the University of
Washington?s Turing Center.
References
Razvan Bunescu and Marius Pas?ca. 2006. Using ency-
clopedic knowledge for named entity disambiguation.
In Proceedings of the 11th Conference of the European
Chapter of the Association of Computational Linguis-
tics (EACL).
Davide Buscaldi and Paolo Rosso. 2006. Mining knowl-
edge from wikipedia for the question answering task.
In Proceedings of the 5th International Conference on
Language Resources and Evaluation (LREC).
Andrew Carlson. 2010. Coupled Semi-Supervised
Learning. Ph.D. thesis, Carnegie Mellon University.
Silviu Cucerzan. 2007. Large-scale named entity disam-
biguation based on wikipedia data. In Proceedings of
EMNLP.
902
Mark Dredze, Paul McNamee, Delip Rao, Adam Ger-
ber, and Tim Finin. 2010. Entity disambiguation for
knowledge base population. In Proceedings of COL-
ING.
Oren Etzioni, Michael Cafarella, Doug Downey, Ana-
Maria Popescu, Tal Shaked, Stephen Soderland,
Daniel S. Weld, and Alexander Yates. 2005. Unsu-
pervised named-entity extraction from the Web: An
experimental study. In Artificial Intelligence.
Oren Etzioni, Michele Banko, and Michael J. Cafarella.
2006. Machine Reading. In Proceedings of the 21st
National Conference on Artificial Intelligence (AAAI).
James A. Evans and Jacob G. Foster. 2011. Metaknowl-
edge. In Science.
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2009. Scaling Wikipedia-based named entity disam-
biguation to arbitrary Web text. In IJCAI-09 Workshop
on User-contributed Knowledge and Artificial Intelli-
gence (WikiAI09).
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying relations for open information ex-
traction. In Proceedings of EMNLP.
Paolo Ferragina and Ugo Scaiella. 2010. Tagme:
On-the-fly annotation of short text fragments (by
wikipedia entities). In Proceedings of CIKM.
Eduard Hovy, Zornitsa Kozareva, and Ellen Riloff. 2009.
Toward completeness in concept extraction and classi-
fication. In Proceedings of EMNLP.
Zornitsa Kozareva, Konstantin Voevodski, and Shang-
Hua Teng. 2011. Class label enhancement via related
instances. In Proceedings of EMNLP.
Sayali Kulkarni, Amit Singh, Ganesh Ramakrishnan, and
Soumen Chakrabarti. 2009. Collective annotation of
wikipedia entities in text. In Proceedings of KDD.
Changki Lee, Yi-Gyu Hwang, and Myung-Gil Jang.
2007. Fine-grained named entity recognition and rela-
tion extraction for question answering. In Proceedings
of SIGIR.
Xiao Ling and Daniel S. Weld. 2012. Fine-grained entity
recognition. In Proceedings of the 26th Conference on
Artificial Intelligence (AAAI).
Jean-Baptiste Michel, Yuan Kui Shen, Aviva P. Aiden,
Adrian Veres, Matthew K. Gray, The Google Books
Team, Joseph P. Pickett, Dale Hoiberg, Dan Clancy,
Peter Norvig, Jon Orwant, and Steven Pinker. 2010.
Quantitative analysis of culture using millions of digi-
tized books. In Science.
David Milne and Ian H. Witten. 2008. Learning to link
with wikipedia. In Proceedings of the 17h ACM Inter-
national Conference on Information and Knowledge
Management (CIKM).
Marius Pas?ca. 2004. Acquisition of categorized named
entities for web search. In Proceedings of the ACM
International Conference on Information and Knowl-
edge Management (CIKM).
Patrick Pantel and Deepak Ravichandran. 2004. Auto-
matically labeling semantic classes. In Proceedings of
HLT-NAACL.
Patrick Pantel, Eric Crestan, Arkady Borkovsky, Ana-
Maria Popescu, and Vishnu Vyas. 2009. Web-scale
distributional similarity and entity set expansion. In
Proceedings of EMNLP.
Danuta Ploch. 2011. Exploring entity relations for
named entity disambiguation. In Proceedings of the
Annual Meeting of the Association of Computational
Linguistics (ACL).
Willard Van Orman Quine. 1948. On what there is. In
Review of Metaphysics.
Altaf Rahman and Vincent Ng. 2010. Inducing fine-
grained semantic classes via hierarchical and collec-
tive classification. In Proceedings of COLING.
Lev Ratinov and Dan Roth. 2009. Design challenges
and misconceptions in named entity recognition. In
Proceedings of the Thirteenth Conference on Compu-
tational Natural Language Learning (CoNLL).
Lev Ratinov, Dan Roth, Doug Downey, and Mike Ander-
son. 2011. Local and global algorithms for disam-
biguation to wikipedia. In Proceedings of the Annual
Meeting of the Association of Computational Linguis-
tics (ACL).
Satoshi Sekine and Chikashi Nobata. 2004. Definition,
dictionaries and tagger for extended named entity hier-
archy. In Proceedings of the 4th International Confer-
ence on Language Resources and Evaluation (LREC).
Partha Pratim Talukdar and Fernando Pereira. 2010.
Experiments in graph-based semi-supervised learning
methods for class-instance acquisition. In Proceedings
of the Annual Meeting of the Association of Computa-
tional Linguistics (ACL).
Partha Pratim Talukdar, Joseph Reisinger, Marius Pas?ca,
Deepak Ravichandran, Rahul Bhagat, and Fernando
Pereira. 2008. Weakly supervised acquisition of la-
beled class instances using graph random walks. In
Proceedings of EMNLP.
Partha Pratim Talukdar, Derry Tanti Wijaya, and Tom
Mitchell. 2012. Coupled temporal scoping of rela-
tional facts. In Proceedings of WSDM.
Chi Wang, Kaushik Chakrabarti, Tao Cheng, and Surajit
Chaudhuri. 2012. Targeted disambiguation of ad-hoc,
homogeneous sets of named entities. In Proceedings
of the 21st International World Wide Web Conference
(WWW).
Derry Tanti Wijaya and Reyyan Yeniterzi. 2011. Un-
derstanding semantic changes of words over centuries.
In Workshop on Detecting and Exploiting Cultural Di-
versity on the Social Web.
903
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 563?571,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Mining Entity Types from Query Logs via User Intent Modeling
Patrick Pantel
Microsoft Research
One Microsoft Way
Redmond, WA 98052, USA
ppantel@microsoft.com
Thomas Lin
Computer Science & Engineering
University of Washington
Seattle, WA 98195, USA
tlin@cs.washington.edu
Michael Gamon
Microsoft Research
One Microsoft Way
Redmond, WA 98052, USA
mgamon@microsoft.com
Abstract
We predict entity type distributions in Web
search queries via probabilistic inference in
graphical models that capture how entity-
bearing queries are generated. We jointly
model the interplay between latent user in-
tents that govern queries and unobserved en-
tity types, leveraging observed signals from
query formulations and document clicks. We
apply the models to resolve entity types in new
queries and to assign prior type distributions
over an existing knowledge base. Our mod-
els are efficiently trained using maximum like-
lihood estimation over millions of real-world
Web search queries. We show that modeling
user intent significantly improves entity type
resolution for head queries over the state of the
art, on several metrics, without degradation in
tail query performance.
1 Introduction
Commercial search engines are providing ever-
richer experiences around entities. Querying for a
dish on Google yields recipe filters such as cook
time, calories, and ingredients. Querying for a
movie on Yahoo triggers user ratings, cast, tweets
and showtimes. Bing further allows the movie to
be directly added to the user?s Netflix queue. En-
tity repositories such as Freebase, IMDB, Facebook
Pages, Factual, Pricegrabber, and Wikipedia are in-
creasingly leveraged to enable such experiences.
There are, however, inherent problems in the en-
tity repositories: (a) coverage: although coverage of
head entity types is often reliable, the tail can be
sparse; (b) noise: created by spammers, extraction
errors or errors in crowdsourced content; (c) am-
biguity: multiple types or entity identifiers are of-
ten associated with the same surface string; and (d)
over-expression: many entities have types that are
never used in the context of Web search.
There is an opportunity to automatically tailor
knowledge repositories to the Web search scenario.
Desirable capabilities of such a system include: (a)
determining the prior type distribution in Web search
for each entity in the repository; (b) assigning a type
distribution to new entities; (c) inferring the correct
sense of an entity in a particular query context; and
(d) adapting to a search engine and time period.
In this paper, we build such a system by lever-
aging Web search usage logs with large numbers of
user sessions seeking or transacting on entities. We
cast the task as performing probabilistic inference
in a graphical model that captures how queries are
generated, and then apply the model to contextually
recognize entity types in new queries. We motivate
and design several generative models based on the
theory that search users? (unobserved) intents gov-
ern the types of entities, the query formulations, and
the ultimate clicks on Web documents. We show that
jointly modeling user intent and entity type signifi-
cantly outperforms the current state of the art on the
task of entity type resolution in queries. The major
contributions of our research are:
? We introduce the idea that latent user intents
can be an important factor in modeling type dis-
tributions over entities in Web search.
? We propose generative models and inference
procedures using signals from query context,
click, entity, entity type, and user intent.
563
? We propose an efficient learning technique and
a robust implementation of our models, using
real-world query data, and a realistic large set
of entity types.
? We empirically show that our models outper-
form the state of the art and that modeling latent
intent contributes significantly to these results.
2 Related Work
2.1 Finding Semantic Classes
A closely related problem is that of finding the se-
mantic classes of entities. Automatic techniques for
finding semantic classes include unsupervised clus-
tering (Schu?tze, 1998; Pantel and Lin, 2002), hy-
ponym patterns (Hearst, 1992; Pantel et al, 2004;
Kozareva et al, 2008), extraction patterns (Etzioni
et al, 2005), hidden Markov models (Ritter et al,
2009), classification (Rahman and Ng, 2010) and
many others. These techniques typically lever-
age large corpora, while projects such as WordNet
(Miller et al, 1990) and Freebase (Bollacker et al,
2008) have employed editors to manually enumerate
words and entities with their semantic classes.
The aforementioned methods do not use query
logs or explicitly determine the relative probabilities
of different entity senses. A method might learn that
there is independently a high chance of eBay being a
website and an employer, but does not specify which
usage is more common. This is especially problem-
atic, for example, if one wishes to leverage Freebase
but only needs the most commonly used senses (e.g.,
Al Gore is a US Vice President), rather than
all possible obscure senses (Freebase contains 30+
senses, including ones such as Impersonated
Celebrity and Quotation Subject). In
scenarios such as this, our proposed method can in-
crease the usability of systems that find semantic
classes. We also expand upon text corpora meth-
ods in that the type priors can adapt to Web search
signals.
2.2 Query Log Mining
Query logs have traditionally been mined to improve
search (Baeza-Yates et al, 2004; Zhang and Nas-
raoui, 2006), but they can also be used in place of
(or in addition to) text corpora for learning seman-
tic classes. Query logs can contain billions of en-
tries, they provide an independent signal from text
corpora, their timestamps allow the learning of type
priors at specific points in time, and they can contain
information such as clickthroughs that are not found
in text corpora. Sekine and Suzuki (2007) used fre-
quency features on context words in query logs to
learn semantic classes of entities. Pas?ca (2007) used
extraction techniques to mine instances of semantic
classes from query logs. Ru?d et al (2011) found
that cross-domain generalizations learned from Web
search results are applicable to NLP tasks such as
NER. Alfonseca et al (2010) mined query logs to
find attributes of entity instances. However, these
projects did not learn relative probabilities of differ-
ent senses.
2.3 User Intents in Search
Learning from query logs also allows us to lever-
age the concept of user intents. When users sub-
mit search queries, they often have specific intents in
mind. Broder (2002) introduced 3 top level intents:
Informational (e.g., wanting to learn), Navigational
(wanting to visit a site), and Transactional (e.g.,
wanting to buy/sell). Rose and Levinson (2004) fur-
ther divided these into finer-grained subcategories,
and Yin and Shah (2010) built hierarchical tax-
onomies of search intents. Jansen et al (2007), Hu
et al (2009), and Radlinski et al (2010) examined
how to infer the intent of queries. We are not aware
of any other work that has leveraged user intents to
learn type distributions.
2.4 Topic Modeling on Query Logs
The closest work to ours is Guo et al?s (2009) re-
search on Named Entity Recognition in Queries.
Given an entity-bearing query, they attempt to iden-
tify the entity and determine the type posteriors. Our
work significantly scales up the type posteriors com-
ponent of their work. While they only have four
potential types (Movie, Game, Book, Music) for
each entity, we employ over 70 popular types, allow-
ing much greater coverage of real entities and their
types. Because they only had four types, they were
able to hand label their training data. In contrast,
our system self-labels training examples by search-
ing query logs for high-likelihood entities, and must
handle any errors introduced by this process. Our
models also expand upon theirs by jointly modeling
564
entity type with latent user intents, and by incorpo-
rating click signals.
Other projects have also demonstrated the util-
ity of topic modeling on query logs. Carman et
al. (2010) modeled users and clicked documents to
personalize search results and Gao et al (2011) ap-
plied topic models to query logs in order to improve
document ranking for search.
3 Joint Model of Types and User Intents
We turn our attention now to the task of mining the
type distributions of entities and of resolving the
type of an entity in a particular query context. Our
approach is to probabilistically describe how entity-
bearing queries are generated in Web search. We
theorize that search queries are governed by a latent
user intent, which in turn influences the entity types,
the choice of query words, and the clicked hosts. We
develop inference procedures to infer the prior type
distributions of entities in Web search as well as to
resolve the type of an entity in a query, by maximiz-
ing the probability of observing a large collection of
real-world queries and their clicked hosts.
We represent a query q by a triple {n1, e, n2},
where e represents the entity mentioned in the query,
n1 and n2 are respectively the pre- and post-entity
contexts (possibly empty), referred to as refiners.
Details on how we obtain our corpus are presented
in Section 4.2.
3.1 Intent-based Model (IM)
In this section we describe our main model, IM, il-
lustrated in Figure 1. We derive a learning algorithm
for the model in Section 3.2 and an inference proce-
dure in Section 3.3.
Recall our discussion of intents from Section 2.3.
The unobserved semantic type of an entity e in a
query is strongly correlated with the unobserved
user intent. For example, if a user queries for
?song?, then she is likely looking to ?listen to it?,
?download it?, ?buy it?, or ?find lyrics? for it. Our
model incorporates this user intent as a latent vari-
able.
The choice of the query refiner words, n1 and n2,
is also clearly influenced by the user intent. For
example, refiners such as ?lyrics? and ?words? are
more likely to be used in queries where the intent is
For each query/click pair {q, c}
type t ?Multinomial(?)
intent i ?Multinomial(?t)
entity e ?Multinomial(?t)
switch s1 ? Bernoulli(?i)
switch s2 ? Bernoulli(?i)
if (s1) l-context n1 ?Multinomial(?i)
if (s2) r-context n2 ?Multinomial(?i)
click c ?Multinomial(?i)
Table 1: Model IM: Generative process for entity-
bearing queries.
to ?find lyrics? than in queries where the intent is to
?listen?. The same is true for clicked hosts: clicks on
?lyrics.com? and ?songlyrics.com? are more likely
to occur when the intent is to ?find lyrics?, whereas
clicks on ?pandora.com? and ?last.fm? are more
likely for a ?listen? intent.
Model IM leverages each of these signals: latent
intent, query refiners, and clicked hosts. It generates
entity-bearing queries by first generating an entity
type, from which the user intent and entity is gen-
erated. In turn, the user intent is then used to gen-
erate the query refiners and the clicked host. In our
data analysis, we observed that over 90% of entity-
bearing queries did not contain any refiner words n1
and n2. In order to distribute more probability mass
to non-empty context words, we explicitly represent
the empty context using a switch variable that deter-
mines whether a context will be empty.
The generative process for IM is described in Ta-
ble 1. Consider the query ?ymca lyrics?. Our model
first generates the type song, then given the type
it generates the entity ?ymca? and the intent ?find
lyrics?. The intent is then used to generate the pre-
and post-context words ? and ?lyrics?, respectively,
and a click on a host such as ?lyrics.com?.
For mathematical convenience, we assume that
the user intent is generated independently of the
entity itself. Without this assumption, we would
require learning a parameter for each intent-type-
entity configuration, exploding the number of pa-
rameters. Instead, we choose to include these depen-
dencies at the time of inference, as described later.
Recall that q = {n1, e, n2} and let s = {s1, s2},
where s1 = 1 if n1 is not empty and s2 = 1 if n2 is
not empty, 0 otherwise. The joint probability of the
model is the product of the conditional distributions,
as given by:
565
y  
Q
tt
n
2
e
f f 
T
tt
E
Guo?09
y  
Q
tt
n
2
e
ss  
f f 
Ttt E
T
Model M0
y  
Q
tt
n
2
e
w Tcss  
f f 
Ttt E
T
Model M1 Model IM
tt
Q
tt
n
2
qq
T
iie
w Kcss  
f f 
K
y  
T
K
Figure 1: Graphical models for generating entity-bearing queries. Guo?09 represents the current state of the art (Guo
et al, 2009). Models M0 and M1 add an empty context switch and click information, respectively. Model IM further
constrains the query by the latent user intent.
P (t, i, q, c | ?,?,?, ?,?,?) =
P (t | ?)P (i | t,?)P (e | t,?)P (c | i,?)
2?
j=1
P (nj | i,?)
I[sj=1]P (sj |i, ?)
We now define each of the terms in the joint dis-
tribution. Let T be the number of entity types. The
probability of generating a type t is governed by a
multinomial with probability vector ? :
P (t=t?) =
T?
j=1
? I[j=t?]j , s.t.
T?
j=1
?j = 1
where I is an indicator function set to 1 if its condi-
tion holds, and 0 otherwise.
Let K be the number of latent user intents that
govern our query log, where K is fixed in advance.
Then, the probability of intents i is defined as a
multinomial distribution with probability vector ?t
such that ? = [?1, ?2, ..., ?T ] captures the matrix of
parameters across all T types:
P (i=i? | t=t?) =
K?
j=1
?I[j=i?]
t?,j
, s.t. ?t
K?
j=1
?t,j = 1
LetE be the number of known entities. The prob-
ability of generating an entity e is similarly governed
by a parameter ? across all T types:
P (e=e? | t=t?) =
E?
j=1
?I[j=e?]
t?,j
, s.t. ?t
E?
j=1
?t,j = 1
The probability of generating an empty or non-
empty context s given intent i is given by a Bernoulli
with parameter ?i:
P (s | i=i?) = ?I[s=1]
i?
(1? ?i?)
I[s=0]
Let V be the shared vocabulary size of all query
refiner words n1 and n2. Given an intent, i, the
probability of generating a refiner n is given by a
multinomial distribution with probability vector ?i
such that ? = [?1, ?2, ..., ?K ] represents parame-
ters across intents:
P (n=n? | i=i?) =
V?
v=1
?I[v=n?]
i?,v
, s.t. ?i
V?
v=1
?i,v = 1
Finally, we assume there areH possible click val-
ues, corresponding to H Web hosts. A click on a
host is similarly determined by an intent i and is gov-
erned by parameter ? across all K intents:
P (c=c? | i=i?) =
H?
h=1
?I[h=c?]
i?,h
, s.t. ?i
H?
h=1
?i,h = 1
3.2 Learning
Given a query corpus Q consisting of N inde-
pendently and identically distributed queries qj =
{nj1, e
j , nj2} and their corresponding clicked hosts
cj , we estimate the parameters ? , ?, ?, ?, ?, and
? by maximizing the (log) probability of observing
Q. The logP (Q) can be written as:
logP (Q) =
N?
j=1
?
t,i
P j(t, i | q, c) logP j(q, c, t, i)
In the above equation, P j(t, i | q, c) is the poste-
rior distribution over types and user intents for the
jth query. We use the Expectation-Maximization
(EM) algorithm to estimate the parameters. The
parameter updates are obtained by computing the
derivative of logP (Q) with respect to each parame-
ter, and setting the resultant to 0.
The update for ? is given by the average of the
posterior distributions over the types:
566
?t? =
?N
j=1
?
i P
j(t=t?, i | q, c)
?N
j=1
?
t,i P
j(t, i | q, c)
For a fixed type t, the update for ?t is given by
the weighted average of the latent intents, where the
weights are the posterior distributions over the types,
for each query:
?t?,?i =
?N
j=1 P
j(t=t?, i=i? | q, c)
?N
j=1
?
i P
j(t=t?, i | q, c)
Similarly, we can update ?, the parameters that
govern the distribution over entities for each type:
?t?,e? =
?N
j=1
?
i P
j(t=t?, i | q, c)I[ej=e?]
?N
j=1
?
i P
j(t=t?, i | q, c)
Now, for a fixed user intent i, the update for
?i is given by the weighted average of the clicked
hosts, where the weights are the posterior distribu-
tions over the intents, for each query:
?i?,c? =
?N
j=1
?
t P
j(t, i=i? | q, c)I[cj=c?]
?N
j=1
?
t P
j(t, i=i? | q, c)
Similarly, we can update ? and ?, the parameters
that govern the distribution over query refiners and
empty contexts for each intent, as:
?i?,n?=
?N
j=1
?
t P
j(t,i=i?|q,c)
[
I[nj1=n?]I[s
j
1=1]+I[n
j
2=n?]I[s
j
2=1]
]
?N
j=1
?
t P
j(t,i=i?|q,c)
[
I[sj1=1]+I[s
j
2=1]
]
and
?i? =
?N
j=1
?
t P
j(t, i=i? | q, c)
[
I[s1=1] + I[s2=1]
]
2
?N
j=1
?
t P
j(t, i=i? | q, c)
3.3 Decoding
Given a query/click pair {q, c}, and the learned IM
model, we can apply Bayes? rule to find the poste-
rior distribution, P (t, i | q, c), over the types and
intents, as it is proportional to P (t, i, q, c). We com-
pute this quantity exactly by evaluating the joint for
each combination of t and i, and the observed values
of q and c.
It is important to note that at runtime when a new
query is issued, we have to resolve the entity in the
absence of any observed click. However, we do have
access to historical click probabilities, P (c | q).
We use this information to compute P (t | q) by
marginalizing over i as follows:
P (t | q) =
?
i
H?
j=1
P (t, i | q, cj)P (cj | q) (1)
3.4 Comparative Models
Figure 1 also illustrates the current state-of-the-art
model Guo?09 (Guo et al, 2009), described in Sec-
tion 2.4, which utilizes only query refinement words
to infer entity type distributions. Two extensions to
this model that we further study in this paper are also
shown: Model M0 adds the empty context switch
parameter and Model M1 further adds click infor-
mation. In the interest of space, we omit the update
equations for these models, however they are triv-
ial to adapt from the derivations of Model IM pre-
sented in Sections 3.1 and 3.2.
3.5 Discussion
Full Bayesian Treatment: In the above mod-
els, we learn point estimates for the parameters
(?,?,?, ?,?,?). One can take a Bayesian ap-
proach and treat these parameters as variables (for
instance, with Dirichlet and Beta prior distribu-
tions), and perform Bayesian inference. However,
exact inference will become intractable and we
would need to resort to methods such as variational
inference or sampling. We found this extension un-
necessary, as we had a sufficient amount of training
data to estimate all parameters reliably. In addition,
our approach enabled us to learn (and perform infer-
ence in) the model with large amounts of data with
reasonable computing time.
Fitting to an existing Knowledge Base: Al-
though in general our model decodes type distribu-
tions for arbitrary entities, in many practical cases
it is beneficial to constrain the types to those ad-
missible in a fixed knowledge base (such as Free-
base). As an example, if the entity is ?ymca?,
admissible types may include song, place, and
educational institution. When resolving
types, during inference, one can restrict the search
space to only these admissible types. A desirable
side effect of this strategy is that only valid ambigu-
ities are captured in the posterior distribution.
567
4 Evaluation Methodology
We refer to QL as a set of English Web search
queries issued to a commercial search engine over
a period of several months.
4.1 Entity Inventory
Although our models generalize to any entity reposi-
tory, we experiment in this paper with entities cover-
ing a wide range of web search queries, coming from
73 types in Freebase. We arrived at these types by
grepping for all entities in Freebase within QL, fol-
lowing the procedure described in Section 4.2, and
then choosing the top most frequent types such that
50% of the queries are covered by an entity of one
of these types1.
4.2 Training Data Construction
In order to learn type distributions by jointly mod-
eling user intents and a large number of types, we
require a large set of training examples containing
tagged entities and their potential types. Unlike in
Guo et al (2009), we need a method to automatically
label QL to produce these training cases since man-
ual annotation is impossible for the range of entities
and types that we consider. Reliably recognizing en-
tities in queries is not a solved problem. However,
for training we do not require high coverage of en-
tities in QL, so high precision on a sizeable set of
query instances can be a proper proxy.
To this end, we collect candidate entities in
QL via simple string matching on Freebase entity
strings within our preselected 73 types. To achieve
high precision from this initial (high-recall, low-
precision) candidate set we use a number of heuris-
tics to only retain highly likely entities. The heuris-
tics include retaining only matches on entities that
appear capitalized more than 50% in their occur-
rences in Wikipedia. Also, a standalone score fil-
ter (Jain and Pennacchiotti, 2011) of 0.9 is used,
which is based on the ratio of string occurrence as
1In this process, we omitted any non-core Freebase type
(e.g., /user/* and /base/*), types used for representation
(e.g., /common/* and /type/*), and too general types (e.g.,
/people/person and /location/location) identi-
fied by if a type contains multiple other prominent subtypes.
Finally, we conflated seven of the types that overlapped with
each other into four types (such as /book/written work
and /book/book).
an exact match in queries to how often it occurs as a
partial match.
The resulting queries are further filtered by keep-
ing only those where the pre- and post-entity con-
texts (n1 and n2) were empty or a single word (ac-
counting for a very large fraction of the queries). We
also eliminate entries with clicked hosts that have
been clicked fewer than 100 times over the entire
QL. Finally, for training we filter out any query with
an entity that has more than two potential types2.
This step is performed to reduce recognition er-
rors by limiting the number of potential ambiguous
matches. We experimented with various thresholds
on allowable types and settled on the value two.
The resulting training data consists of several mil-
lion queries, 73 different entity types, and approx-
imately 135K different entities, 100K different re-
finer words, and 40K clicked hosts.
4.3 Test Set Annotation
We sampled two datasets, HEAD and TAIL, each
consisting of 500 queries containing an entity be-
longing to one of the 73 types in our inventory, from
a frequency-weighted random sample and a uniform
random sample of QL, respectively.
We conducted a user study to establish a gold
standard of the correct entity types in each query.
A total of seven different independent and paid pro-
fessional annotators participated in the study. For
each query in our test sets, we displayed the query,
associated clicked host, and entity to the annotator,
along with a list of permissible types from our type
inventory. The annotator is tasked with identifying
all applicable types from that list, or marking the test
case as faulty because of an error in entity identifi-
cation, bad click host (e.g. dead link) or bad query
(e.g. non-English). This resulted in 2,092 test cases
({query, entity, type}-tuples). Each test case was
annotated by two annotators. Inter-annotator agree-
ment as measured by Fleiss? ? was 0.445 (0.498
on HEAD and 0.386 on TAIL), considered moderate
agreement.
From HEAD and TAIL, we eliminated three cat-
egories of queries that did not offer any interesting
type disambiguation opportunities:
? queries that contained entities with only one
2For testing we did not omit any entity or type.
568
HEAD TAIL
nDCG MAP MAPW Prec@1 nDCG MAP MAPW Prec@1
BFB 0.71 0.60 0.45 0.30 0.73 0.64 0.49 0.35
Guo?09 0.79? 0.71? 0.62? 0.51? 0.80? 0.73? 0.66? 0.52?
M0 0.79? 0.72? 0.65? 0.52? 0.82? 0.75? 0.67? 0.57?
M1 0.83? 0.76? 0.72? 0.61? 0.81? 0.74? 0.67? 0.55?
IM 0.87? 0.82? 0.77? 0.73? 0.80? 0.72? 0.66? 0.52?
Table 2: Model analysis on HEAD and TAIL. ? indicates statistical significance over BFB, and ? over both BFB and
Guo?09. Bold indicates statistical significance over all non-bold models in the column. Significance is measured
using the Student?s t-test at 95% confidence.
potential type from our inventory;
? queries where the annotators rated all potential
types as good; and
? queries where judges rated none of the potential
types as good
The final test sets consist of 105 head queries with
359 judged entity types and 98 tail queries with 343
judged entity types.
4.4 Metrics
Our task is a ranking task and therefore the classic
IR metrics nDCG (normalized discounted cumula-
tive gain) and MAP (mean average precision) are
applicable (Manning et al, 2008).
Both nDCG and MAP are sensitive to the rank
position, but not the score (probability of a type) as-
sociated with each rank, S(r). We therefore also
evaluate a weighted mean average precision score
MAPW, which replaces the precision component
of MAP, P (r), for the rth ranked type by:
P (r) =
?r
r?=1 I(r?)S(r?)?r
r?=1 S(r?)
(2)
where I(r) indicates if the type at rank r is judged
correct.
Our fourth metric is Prec@1, i.e. the precision of
only the top-ranked type of each query. This is espe-
cially suitable for applications where a single sense
must be determined.
4.5 Model Settings
We trained all models in Figure 1 using the training
data from Section 4.2 over 100 EM iterations, with
two folds per model. For Model IM, we varied the
number of user intents (K) in intervals from 100 to
400 (see Figure 3), under the assumption that multi-
ple intents would exist per entity type.
We compare our results against two baselines.
The first baseline is an assignment of Freebase types
according to their frequency in our query set BFB,
and the second is Model Guo?09 (Guo et al, 2009)
illustrated in Figure 1.
5 Experimental Results
Table 2 lists the performance of each model on the
HEAD and TAIL sets over each metric defined in
Section 4.4. On head queries, the addition of the
empty context parameter ? and click signal ? to-
gether (Model M1) significantly outperforms both
the baseline and the state-of-the-art model Guo?09.
Further modeling the user intent in Model IM re-
sults in significantly better performance over all
models and across all metrics. Model IM shows
its biggest gains in the first position of its ranking as
evidenced by the Prec@1 metric.
We observe a different behavior on tail queries
where all models significantly outperform the base-
line BFB, but are not significantly different from
each other. In short, the strength of our proposed
model is in improving performance on the head at
no noticeable cost in the tail.
We separately tested the effect of adding the
empty context parameter ?. Figure 2 illustrates the
result on the HEAD data. Across all metrics, ? im-
proved performance over all models3. The more
expressive models benefitted more than the less ex-
pressive ones.
Table 2 reports results for Model IM using K =
200 user intents. This was determined by varying
K and selecting the top-performing value. Figure 3
illustrates the performance of Model IM with dif-
ferent values of K on the HEAD.
3Note that model M0 is just the addition of the ? parameter
over Guo?09.
569
0
0.02
0.04
0.06
0.08
0.1
0.12
0.14
0.16
M0 M1 IMRe
lati
ve 
gai
n o
f sw
itch
 vs.
 no
 sw
itch
 
Effect of Empty Switch Parameter (s) on HEAD 
No switch
nDCG
MAP
MAPW
Prec@1
Figure 2: The switch parameter ? improves performance
of every model and metric.
00.1
0.20.3
0.40.5
0.60.7
0.80.9
1 Varying K (latent intents) -  TAIL  
0.60.65
0.70.75
0.80.85
0.90.95
1
100 150 200 300 400K  
Model IM -  Varying K (latent intents)  
nDCG
MAP
MAPW
Prec@1
Figure 3: Model performance vs. the number of latent
intents (K).
Our models can also assign a prior type distribu-
tion to each entity by further marginalizing Eq. 1
over query contexts n1 and n2. We measured the
quality of our learned type priors using the subset
of queries in our HEAD test set that consisted of
only an entity without any refiners. The results for
Model IM were: nDCG = 0.86, MAP = 0.80,
MAPW = 0.75, and Prec@1 = 0.70. All met-
rics are statistically significantly better than BFB,
Guo?09 and M0, with 95% confidence. Compared
to Model M1, Model IM is statistically signifi-
cantly better on Prec@1 and not significantly dif-
ferent on the other metrics.
Discussion and Error Analysis: Contrary to
our results, we had expected improvements for
both HEAD and TAIL. Inspection of the TAIL
queries revealed that entities were greatly skewed
towards people (e.g., actor, author, and
politician). Analysis of the latent user in-
tent parameter ? in Model IM showed that most
people types had most of their probability mass
assigned to the same three generic and common in-
tents for people types: ?see pictures of?, ?find bio-
graphical information about?, and ?see video of?. In
other words, latent intents in Model IM are over-
expressive and they do not help in differentiating
people types.
The largest class of errors came from queries
bearing an entity with semantically very similar
types where our highest ranked type was not judged
correct by the annotators. For example, for the
query ?philippine daily inquirer? our system ranked
newspaper ahead of periodical but a judge
rejected the former and approved the latter. For
?ikea catalogue?, our system ranked magazine
ahead of periodical, but again a judge rejected
magazine in favor of periodical.
An interesting success case in the TAIL is high-
lighted by two queries involving the entity ?ymca?,
which in our data can either be a song, place,
or educational institution. Our system
learns the following priors: 0.63, 0.29, and 0.08,
respectively. For the query ?jamestown ymca ny?,
IM correctly classified ?ymca? as a place and for
the query ?ymca palomar? it correctly classified it
as an educational institution. We further
issued the query ?ymca lyrics? and the type song
was then highest ranked.
Our method is generalizable to any entity collec-
tion. Since our evaluation focused on the Freebase
collection, it remains an open question how noise
level, coverage, and breadth in a collection will af-
fect our model performance. Finally, although we
do not formally evaluate it, it is clear that training
our model on different time spans of queries should
lead to type distributions adapted to that time period.
6 Conclusion
Jointly modeling the interplay between the under-
lying user intents and entity types in web search
queries shows significant improvements over the
current state of the art on the task of resolving entity
types in head queries. At the same time, no degrada-
tion in tail queries is observed. Our proposed models
can be efficiently trained using an EM algorithm and
can be further used to assign prior type distributions
to entities in an existing knowledge base and to in-
sert new entities into it.
Although this paper leverages latent intents in
search queries, it stops short of understanding the
nature of the intents. It remains an open problem
to characterize and enumerate intents and to iden-
tify the types of queries that benefit most from intent
models.
570
References
Enrique Alfonseca, Marius Pasca, and Enrique Robledo-
Arnuncio. 2010. Acquisition of instance attributes
via labeled and related instances. In Proceedings of
SIGIR-10, pages 58?65, New York, NY, USA.
Ricardo Baeza-Yates, Carlos Hurtado, and Marcelo Men-
doza. 2004. Query recommendation using query logs
in search engines. In EDBT Workshops, Lecture Notes
in Computer Science, pages 588?596. Springer.
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: a collabo-
ratively created graph database for structuring human
knowledge. In Proceedings of SIGMOD ?08, pages
1247?1250, New York, NY, USA.
Andrei Broder. 2002. A taxonomy of web search. SIGIR
Forum, 36:3?10.
Mark James Carman, Fabio Crestani, Morgan Harvey,
and Mark Baillie. 2010. Towards query log based per-
sonalization using topic models. In CIKM?10, pages
1849?1852.
Oren Etzioni, Michael Cafarella, Doug Downey, Ana-
Maria Popescu, Tal Shaked, Stephen Soderland,
Daniel S. Weld, and Alexander Yates. 2005. Unsu-
pervised named-entity extraction from the web: An
experimental study. volume 165, pages 91?134.
Jianfeng Gao, Kristina Toutanova, and Wen-tau Yih.
2011. Clickthrough-based latent semantic models for
web search. In Proceedings of SIGIR ?11, pages 675?
684, New York, NY, USA. ACM.
Jiafeng Guo, Gu Xu, Xueqi Cheng, and Hang Li. 2009.
Named entity recognition in query. In Proceedings
of SIGIR-09, pages 267?274, New York, NY, USA.
ACM.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of
the 14th International Conference on Computational
Linguistics, pages 539?545.
Jian Hu, Gang Wang, Frederick H. Lochovsky, Jian tao
Sun, and Zheng Chen. 2009. Understanding user?s
query intent with wikipedia. In WWW, pages 471?480.
Alpa Jain and Marco Pennacchiotti. 2011. Domain-
independent entity extraction from web search query
logs. In Proceedings of WWW ?11, pages 63?64, New
York, NY, USA. ACM.
Bernard J. Jansen, Danielle L. Booth, and Amanda Spink.
2007. Determining the user intent of web search en-
gine queries. In Proceedings of WWW ?07, pages
1149?1150, New York, NY, USA. ACM.
Zornitsa Kozareva, Ellen Riloff, and Eduard Hovy. 2008.
Semantic class learning from the web with hyponym
pattern linkage graphs. In Proceedings of ACL.
Christopher D. Manning, Prabhakar Raghavan, and Hin-
rich Schu?tze. 2008. Introduction to Information Re-
trieval. Cambridge University Press.
George A. Miller, Richard Beckwith, Christiane Fell-
baum, Derek Gross, and Katherine Miller. 1990.
Wordnet: An on-line lexical database. volume 3,
pages 235?244.
Marius Pas?ca. 2007. Weakly-supervised discovery of
named entities using web search queries. In Proceed-
ings of the sixteenth ACM conference on Conference
on information and knowledge management, CIKM
?07, pages 683?690, New York, NY, USA. ACM.
Patrick Pantel and Dekang Lin. 2002. Discovering word
senses from text. In SIGKDD, pages 613?619, Ed-
monton, Canada.
Patrick Pantel, Deepak Ravichandran, and Eduard Hovy.
2004. Towards terascale knowledge acquisition. In
COLING, pages 771?777.
Filip Radlinski, Martin Szummer, and Nick Craswell.
2010. Inferring query intent from reformulations and
clicks. In Proceedings of the 19th international con-
ference on World wide web, WWW ?10, pages 1171?
1172, New York, NY, USA. ACM.
Altaf Rahman and Vincent Ng. 2010. Inducing fine-
grained semantic classes via hierarchical and collec-
tive classification. In Proceedings of COLING, pages
931?939.
Alan Ritter, Stephen Soderland, and Oren Etzioni. 2009.
What is this, anyway: Automatic hypernym discov-
ery. In Proceedings of AAAI-09 Spring Symposium on
Learning by Reading and Learning to Read, pages 88?
93.
Daniel E. Rose and Danny Levinson. 2004. Under-
standing user goals in web search. In Proceedings of
the 13th international conference on World Wide Web,
WWW ?04, pages 13?19, New York, NY, USA. ACM.
Stefan Ru?d, Massimiliano Ciaramita, Jens Mu?ller, and
Hinrich Schu?tze. 2011. Piggyback: Using search
engines for robust cross-domain named entity recog-
nition. In Proceedings of ACL ?11, pages 965?975,
Portland, Oregon, USA, June. Association for Com-
putational Linguistics.
Hinrich Schu?tze. 1998. Automatic word sense dis-
crimination. Computational Linguistics, 24:97?123,
March.
Satoshi Sekine and Hisami Suzuki. 2007. Acquiring on-
tological knowledge from query logs. In Proceedings
of the 16th international conference on World Wide
Web, WWW ?07, pages 1223?1224, New York, NY,
USA. ACM.
Xiaoxin Yin and Sarthak Shah. 2010. Building taxon-
omy of web search intents for name entity queries. In
WWW, pages 1001?1010.
Z. Zhang and O. Nasraoui. 2006. Mining search en-
gine query logs for query recommendation. In WWW,
pages 1039?1040.
571
Proceedings of the NAACL HLT 2010 First International Workshop on Formalisms and Methodology for Learning by Reading, pages 87?95,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Machine Reading at the University of Washington
Hoifung Poon, Janara Christensen, Pedro Domingos, Oren Etzioni, Raphael Hoffmann,
Chloe Kiddon, Thomas Lin, Xiao Ling, Mausam, Alan Ritter, Stefan Schoenmackers,
Stephen Soderland, Dan Weld, Fei Wu, Congle Zhang
Department of Computer Science & Engineering
University of Washington
Seattle, WA 98195
{hoifung,janara,pedrod,etzioni,raphaelh,chloe,tlin,xiaoling,mausam,
aritter,stef,soderland,weld,wufei,clzhang}@cs.washington.edu
Abstract
Machine reading is a long-standing goal of AI
and NLP. In recent years, tremendous progress
has been made in developing machine learning
approaches for many of its subtasks such as
parsing, information extraction, and question
answering. However, existing end-to-end so-
lutions typically require substantial amount of
human efforts (e.g., labeled data and/or man-
ual engineering), and are not well poised for
Web-scale knowledge acquisition. In this pa-
per, we propose a unifying approach for ma-
chine reading by bootstrapping from the easi-
est extractable knowledge and conquering the
long tail via a self-supervised learning pro-
cess. This self-supervision is powered by joint
inference based on Markov logic, and is made
scalable by leveraging hierarchical structures
and coarse-to-fine inference. Researchers at
the University of Washington have taken the
first steps in this direction. Our existing work
explores the wide spectrum of this vision and
shows its promise.
1 Introduction
Machine reading, or learning by reading, aims to
extract knowledge automatically from unstructured
text and apply the extracted knowledge to end tasks
such as decision making and question answering. It
has been a major goal of AI and NLP since their
early days. With the advent of the Web, the billions
of online text documents contain virtually unlimited
amount of knowledge to extract, further increasing
the importance and urgency of machine reading.
In the past, there has been a lot of progress in
automating many subtasks of machine reading by
machine learning approaches (e.g., components in
the traditional NLP pipeline such as POS tagging
and syntactic parsing). However, end-to-end solu-
tions are still rare, and existing systems typically re-
quire substantial amount of human effort in manual
engineering and/or labeling examples. As a result,
they often target restricted domains and only extract
limited types of knowledge (e.g., a pre-specified re-
lation). Moreover, many machine reading systems
train their knowledge extractors once and do not
leverage further learning opportunities such as ad-
ditional text and interaction with end users.
Ideally, a machine reading system should strive to
satisfy the following desiderata:
End-to-end: the system should input raw text, ex-
tract knowledge, and be able to answer ques-
tions and support other end tasks;
High quality: the system should extract knowledge
with high accuracy;
Large-scale: the system should acquire knowledge
at Web-scale and be open to arbitrary domains,
genres, and languages;
Maximally autonomous: the system should incur
minimal human effort;
Continuous learning from experience: the
system should constantly integrate new infor-
mation sources (e.g., new text documents) and
learn from user questions and feedback (e.g.,
via performing end tasks) to continuously
improve its performance.
These desiderata raise many intriguing and chal-
lenging research questions. Machine reading re-
search at the University of Washington has explored
87
a wide spectrum of solutions to these challenges and
has produced a large number of initial systems that
demonstrated promising performance. During this
expedition, an underlying unifying vision starts to
emerge. It becomes apparent that the key to solving
machine reading is to:
1. Conquer the long tail of textual knowledge via
a self-supervised learning process that lever-
ages data redundancy to bootstrap from the
head and propagates information down the long
tail by joint inference;
2. Scale this process to billions of Web documents
by identifying and leveraging ubiquitous struc-
tures that lead to sparsity.
In Section 2, we present this vision in detail, iden-
tify the major dimensions these initial systems have
explored, and propose a unifying approach that sat-
isfies all five desiderata. In Section 3, we reivew
machine reading research at the University of Wash-
ington and show how they form synergistic effort
towards solving the machine reading problem. We
conclude in Section 4.
2 A Unifying Approach for Machine
Reading
The core challenges to machine reading stem from
the massive scale of the Web and the long-tailed dis-
tribution of textual knowledge. The heterogeneous
Web contains texts that vary substantially in subject
matters (e.g., finance vs. biology) and writing styles
(e.g., blog posts vs. scientific papers). In addition,
natural languages are famous for their myraid vari-
ations in expressing the same meaning. A fact may
be stated in a straightforward way such as ?kale con-
tains calcium?. More often though, it may be stated
in a syntactically and/or lexically different way than
as phrased in an end task (e.g., ?calcium is found in
kale?). Finally, many facts are not even stated ex-
plicitly, and must be inferred from other facts (e.g.,
?kale prevents osteoporosis? may not be stated ex-
plicitly but can be inferred by combining facts such
as ?kale contains calcium? and ?calcium helps pre-
vent osteoporosis?). As a result, machine reading
must not rely on explicit supervision such as manual
rules and labeled examples, which will incur pro-
hibitive cost in the Web scale. Instead, it must be
able to learn from indirect supervision.





	






Figure 1: A unifying vision for machine reading: boot-
strap from the head regime of the power-law distribu-
tion of textual knowledge, and conquer the long tail in
a self-supervised learning process that raises certainty on
sparse extractions by propagating information via joint
inference from frequent extractions.
A key source of indirect supervision is meta
knowledge about the domains. For example, the
TextRunner system (Banko et al, 2007) hinges on
the observation that there exist general, relation-
independent patterns for information extraction. An-
other key source of indirect supervision is data re-
dundancy. While a rare extracted fact or inference
pattern may arise by chance of error, it is much less
likely so for the ones with many repetitions (Downey
et al, 2010). Such highly-redundant knowledge can
be extracted easily and with high confidence, and
can be leveraged for bootstrapping. For knowledge
that resides in the long tail, explicit forms of redun-
dancy (e.g., identical expressions) are rare, but this
can be circumvented by joint inference. For exam-
ple, expressions that are composed with or by sim-
ilar expressions probably have the same meaning;
the fact that kale prevents osteoporosis can be de-
rived by combining the facts that kale contains cal-
cium and that calcium helps prevent osteoporosis via
a transitivity-through inference pattern. In general,
joint inference can take various forms, ranging from
simple voting to shrinkage in a probabilistic ontol-
ogy to sophisticated probabilistic reasoning based
on a joint model. Simple ones tend to scale bet-
ter, but their capability in propagating information
is limited. More sophisticated methods can uncover
implicit redundancy and propagate much more in-
88
formation with higher quality, yet the challenge is
how to make them scale as well as simple ones.
To do machine reading, a self-supervised learning
process, informed by meta knowledege, stipulates
what form of joint inference to use and how. Effec-
tively, it increases certainty on sparse extractions by
propagating information from more frequent ones.
Figure 1 illustrates this unifying vision.
In the past, machine reading research at the Uni-
versity of Washington has explored a variety of so-
lutions that span the key dimensions of this uni-
fying vision: knowledge representation, bootstrap-
ping, self-supervised learning, large-scale joint in-
ference, ontology induction, continuous learning.
See Section 3 for more details. Based on this ex-
perience, one direction seems particularly promising
that we would propose here as our unifying approach
for end-to-end machine reading:
Markov logic is used as the unifying framework for
knowledge representation and joint inference;
Self-supervised learning is governed by a joint
probabilistic model that incorporates a small
amount of heuristic knowledge and large-scale
relational structures to maximize the amount
and quality of information to propagate;
Joint inference is made scalable to the Web by
coarse-to-fine inference.
Probabilistic ontologies are induced from text to
guarantee tractability in coarse-to-fine infer-
ence. This ontology induction and popula-
tion are incorporated into the joint probabilistic
model for self-supervision;
Continuous learning is accomplished by combin-
ing bootstrapping and crowdsourced content
creation to synergistically improve the reading
system from user interaction and feedback.
A distinctive feature of this approach is its empha-
sis on using sophisticated joint inference. Recently,
joint inference has received increasing interest in
AI, machine learning, and NLP, with Markov logic
(Domingos and Lowd, 2009) being one of the lead-
ing unifying frameworks. Past work has shown that
it can substantially improve predictive accuracy in
supervised learning (e.g., (Getoor and Taskar, 2007;
Bakir et al, 2007)). We propose to build on these ad-
vances, but apply joint inference beyond supervised
learning, with labeled examples supplanted by indi-
rect supervision.
Another distinctive feature is that we propose
to use coarse-to-fine inference (Felzenszwalb and
McAllester, 2007; Petrov, 2009) as a unifying
framework to scale inference to the Web. Es-
sentially, coarse-to-fine inference leverages the
sparsity imposed by hierarchical structures that
are ubiquitous in human knowledge (e.g., tax-
onomies/ontologies). At coarse levels (top levels in
a hierarchy), ambiguities are rare (there are few ob-
jects and relations), and inference can be conducted
very efficiently. The result is then used to prune un-
promising refinements at the next level. This process
continues down the hierarchy until decision can be
made. In this way, inference can potentially be sped
up exponentially, analogous to binary tree search.
Finally, we propose a novel form of continuous
learning by leveraging the interaction between the
system and end users to constantly improve the per-
formance. This is straightforward to do in our ap-
proach given the self-supervision process and the
availability of powerful joint inference. Essentially,
when the system output is applied to an end task
(e.g., answering questions), the feedback from user
is collected and incorporated back into the system
as a bootstrap source. The feedback can take the
form of explicit supervision (e.g., via community
content creation or active learning) or indirect sig-
nals (e.g., click data and query logs). In this way,
we can bootstrap an online community by an initial
machine reading system that provides imperfect but
valuable service in end tasks, and continuously im-
prove the quality of system output, which attracts
more users with higher degree of participation, thus
creating a positive feedback loop and raising the ma-
chine reading performance to a high level that is dif-
ficult to attain otherwise.
3 Summary of Progress to Date
The University of Washington has been one of the
leading places for machine reading research and has
produced many cutting-edge systems, e.g., WIEN
(first wrapper induction system for information ex-
traction), Mulder (first fully automatic Web-scale
question answering system), KnowItAll/TextRunner
(first systems to do open-domain information extrac-
89
tion from the Web corpus at large scale), Kylin (first
self-supervised system for Wikipedia-based infor-
mation extraction), UCR (first unsupervised corefer-
ence resolution system that rivals the performance of
supervised systems), Holmes (first Web-scale joint
inference system), USP (first unsupervised system
for semantic parsing).
Figure 2 shows the evolution of the major sys-
tems; dashed lines signify influence in key ideas
(e.g., Mulder inspires KnowItAll), and solid lines
signify dataflow (e.g., Holmes inputs TextRunner tu-
ples). These systems span a wide spectrum in scal-
ability (assessed by speed and quantity in extrac-
tion) and comprehension (assessed by unit yield of
knowledge at a fixed precision level). At one ex-
treme, the TextRunner system is highly scalable, ca-
pable of extracting billions of facts, but it focuses on
shallow extractions from simple sentences. At the
other extreme, the USP and LOFT systems achieve
much higher level of comprehension (e.g., in a task
of extracting knowledge from biomedical papers and
answering questions, USP obtains more than three
times as many correct answers as TextRunner, and
LOFT obtains more than six times as many correct
answers as TextRunner), but are much less scalable
than TextRunner.
In the remainder of the section, we review the
progress made to date and identify key directions for
future work.
3.1 Knowledge Representation and Joint
Inference
Knowledge representations used in these systems
vary widely in expressiveness, ranging from sim-
ple ones like relation triples (<subject, relation,
object>; e.g., in KnowItAll and TextRunner), to
clusters of relation triples or triple components (e.g.,
in SNE, RESOLVER), to arbitrary logical formulas
and their clusters (e.g., in USP, LOFT). Similarly,
a variety forms of joint inference have been used,
ranging from simple voting to heuristic rules to so-
phisticated probabilistic models. All these can be
compactly encoded in Markov logic (Domingos and
Lowd, 2009), which provides a unifying framework
for knowledge representation and joint inference.
Past work at Washington has shown that in su-
pervised learning, joint inference can substantially
improve predictive performance on tasks related to
machine reading (e.g., citation information extrac-
tion (Poon and Domingos, 2007), ontology induc-
tion (Wu and Weld, 2008), temporal information
extraction (Ling and Weld, 2010)). In addition, it
has demonstrated that sophisticated joint inference
can enable effective learning without any labeled
information (UCR, USP, LOFT), and that joint in-
ference can scale to millions of Web documents by
leveraging sparsity in naturally occurring relations
(Holmes, Sherlock), showing the promise of our uni-
fying approach.
Simpler representations limit the expressiveness
in representing knowledge and the degree of sophis-
tication in joint inference, but they currently scale
much better than more expressive ones. A key direc-
tion for future work is to evaluate this tradeoff more
thoroughly, e.g., for each class of end tasks, to what
degree do simple representations limit the effective-
ness in performing the end tasks? Can we automate
the choice of representations to strike the best trade-
off for a specific end task? Can we advance joint
inference algorithms to such a degree that sophisti-
cated inference scales as well as simple ones?
3.2 Bootstrapping
Past work at Washington has identified and lever-
aged a wide range of sources for bootstrapping. Ex-
amples include Wikipedia (Kylin, KOG, IIA, WOE,
WPE), Web lists (KnowItAll, WPE), Web tables
(WebTables), Hearst patterns (KnowItAll), heuristic
rules (TextRunner), semantic role labels (SRL-IE),
etc.
In general, potential bootstrap sources can be
broadly divided into domain knowledge (e.g., pat-
terns and rules) and crowdsourced contents (e.g., lin-
guistic resources, Wikipedia, Amazon Mechanical
Turk, the ESP game).
A key direction for future work is to combine
bootstrapping with crowdsourced content creation
for continuous learning. (Also see Subsection 3.6.)
3.3 Self-Supervised Learning
Although the ways past systems conduct self-
supervision vary widely in detail, they can be di-
vided into two broad categories. One uses heuristic
rules that exploit existing semi-structured resources
to generate noisy training examples for use by su-
pervised learning methods and with cotraining (e.g.,
90

	


		

	
 	