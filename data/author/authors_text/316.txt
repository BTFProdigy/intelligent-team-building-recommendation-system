Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pp. 1112?1118,
Prague, June 2007. c?2007 Association for Computational Linguistics
Multilingual Dependency Parsing and Domain Adaptation using DeSR 
Giuseppe Attardi 
Felice Dell?Orletta 
Maria Simi 
Dipartimento di Informatica 
largo B. Pontecorvo 3 
I-56127 Pisa, Italy 
attardi@di.unipi.it 
felice.dellorletta@
ilc.cnr.it 
simi@di.unipi.it 
Atanas Chanev 
Universit? di Trento 
via Matteo del Ben 5 
I-38068 Rovereto, Italy 
Fondazione Bruno Kessler-irst 
via Sommarive 18 
I-38050 Povo, Italy 
chanev@form.unitn.it 
 
Massimiliano Ciaramita 
Yahoo! Research Barcelona 
Ocata 1 
S-08003 Barcelona, Spain 
massi@yahoo-inc.com 
 
Abstract 
We describe our experiments using the 
DeSR parser in the multilingual and do-
main adaptation tracks of the CoNLL 2007 
shared task. DeSR implements an incre-
mental deterministic Shift/Reduce parsing 
algorithm, using specific rules to handle 
non-projective dependencies. For the multi-
lingual track we adopted a second order 
averaged perceptron and performed feature 
selection to tune a feature model for each 
language. For the domain adaptation track 
we applied a tree revision method which 
learns how to correct the mistakes made by 
the base parser on the adaptation domain. 
1 Introduction 
Classifier-based dependency parsers (Yamada and 
Matsumoto, 2003; Nivre and Scholz, 2004) learn 
from an annotated corpus how to select an 
appropriate sequence of Shift/Reduce actions to 
construct the dependency tree for a sentence. 
Learning is based on techniques such as SVM 
(Vapnik 1998) or Memory Based Learning 
(Daelemans 2003), which provide high accuracy 
but are often computationally expensive. For the 
multilingual track in the CoNLL 2007 Shared 
Task, we employed a Shift/Reduce parser which 
uses a perceptron algorithm with second-order 
feature maps, in order to verify whether a simpler 
and faster algorithm can still achieve comparable 
accuracy. 
For the domain adaptation track we wished to 
explore the use of tree revisions in order to 
incorporate language knowledge from a new 
domain. 
2 Multilingual Track 
The overall parsing algorithm is a deterministic 
classifier-based statistical parser, which extends 
the approach by Yamada and Matsumoto (2003), 
by using different reduction rules that ensure 
deterministic incremental processing of the input 
sentence and by adding specific rules for handling 
non-projective dependencies. The parser also 
performs dependency labeling within a single 
processing step. 
The parser is modular and can use several 
learning algorithms. The submitted runs used a 
second order Average Perceptron, derived from the 
multiclass perceptron of Crammer and Singer 
(2003). 
No additional resources were used. No pre-
processing or post-processing was used, except 
stemming for English, by means of the Snowball 
stemmer (Porter 2001). 
3 Deterministic Classifier-based Parsing 
DeSR (Attardi, 2006) is an incremental determinis-
tic classifier-based parser. The parser constructs 
dependency trees employing a deterministic bot-
tom-up algorithm which performs Shift/Reduce 
actions while analyzing input sentences in left-to-
right order. 
Using a notation similar to (Nivre and Scholz, 
2003), the state of the parser is represented by a 
1112
quadruple ?S, I, T, A?, where S is the stack of past 
tokens, I is the list of (remaining) input tokens, T is 
a stack of temporary tokens and A is the arc rela-
tion for the dependency graph. 
Given an input string W, the parser is initialized 
to ?(), W, (), ()?, and terminates when it reaches a 
configuration ?S, (), (), A?. 
The three basic parsing rule schemas are as fol-
lows: ?S, n|I, T, A? Shift ?n|S, I, T, A? ?s|S, n|I, T, A? Rightd ?S, n|I, T, A?{(s, d, n)}? ?s|S, n|I, T, A? Leftd ?S, s|I, T, A?{(n, d, s)}? 
The schemas for the Left and Right rules are in-
stantiated for each dependency type d ? D, for a 
total of 2|D| + 1 rules. These rules perform both 
attachment and labeling. 
At each step the parser uses classifiers trained 
on a treebank corpus in order to predict which ac-
tion to perform and which dependency label to as-
sign given the current configuration. 
4 Non-Projective Relations 
For handling non-projective relations, Nivre and 
Nilsson (2005) suggested applying a pre-
processing step to a dependency parser, which con-
sists in lifting non-projective arcs to their head re-
peatedly, until the tree becomes pseudo-projective. 
A post-processing step is then required to restore 
the arcs to the proper heads. 
In DeSR non-projective dependencies are han-
dled in a single step by means of the following ad-
ditional parsing rules, slightly different from those 
in (Attardi, 2006): 
 ?s1|s2|S, n|I, T, A? Right2d ? S, s1|n|I, T, A?{(s2, d, n)}? ?s1|s2|S, n|I, T, A? Left2d ?s2|S, s1|I, T, A?{(n, d, s2)}? ?s1|s2|s3|S, n|I, T, A? Right3d ? S, s1|s2|n|I, T, A?{(s3, d, n)}? ?s1|s2|s3|S, n|I, T, A? Left3d ?s2|s3|S, s1|I, T, A?{(n, d, s3)}? 
 ?s1|s2|S, n|I, T, A? Extract ?n|s1|S, I, s2|T, A? ?S, I, s1|T, A? Insert ?s1|S, I, T, A? 
Left2, Right2 are similar to Left and Right, except 
that they create links crossing one intermediate 
node, while Left3 and Right3 cross two intermedi-
ate nodes. Notice that the RightX actions put back 
on the input the intervening tokens, allowing the 
parser to complete the linking of tokens whose 
processing had been delayed. Extract/Insert gener-
alize the previous rules by moving one token to the 
stack T and reinserting the top of T into S. 
5 Perceptron Learning and 2nd-Order 
Feature Maps 
The software architecture of the DeSR parser is 
modular. Several learning algorithms are available, 
including SVM, Maximum Entropy, Memory-
Based Learning, Logistic Regression and a few 
variants of the perceptron algorithm. 
We obtained the best accuracy with a multiclass 
averaged perceptron classifier based on the 
ultraconservative formulation of Crammer and 
Singer (2003) with uniform negative updates. The 
classifier function is: { }xxF k
k
?= ?maxarg)(  
where each parsing action k is associated with a 
weight vector ?k. To regularize the model the final 
weight vectors are computed as the average of all 
weight vectors posited during training. The number 
of learning iterations over the training data, which 
is the only adjustable parameter of the algorithm, 
was determined by cross-validation.  
In order to overcome the limitations of a linear 
perceptron, we introduce a feature map ?: IRd ? 
IRd(d+1)/2 that maps a feature vector x into a higher 
dimensional feature space consisting of all un-
ordered feature pairs: ?(x) = ?xixj | i = 1, ?, d, j = i, ?, d? 
In other words we expand the original 
representation in the input space with a feature 
map that generates all second-order feature 
combinations from each observation. We call this 
the 2nd-order model, where the inner products are 
computed as ?k ? ?(x), with ?k a vector of dimen-
sion d(d+1)/2. Applying a linear perceptron to this 
feature space corresponds to simulating a polyno-
mial kernel of degree two.  
A polynomial kernel of degree two for SVM 
was also used by Yamada and Matsumoto (2003). 
However, training SVMs on large data sets like 
those arising from a big training corpus was too 
1113
computationally expensive, forcing them to resort 
to partitioning the training data (by POS) and to 
learn several models. 
Our implementation of the perceptron algorithm 
uses sparse data structures (hash maps) so that it 
can handle efficiently even large feature spaces in 
a single model. For example the feature space for 
the 2nd-order model for English contains over 21 
million. Parsing unseen data can be performed at 
tens of sentences per second. More details on such 
aspects of the DeSR parser can be found in (Ci-
aramita and Attardi 2007). 
6 Tuning 
The base parser was tuned on several parameters to 
optimize its accuracy as follows. 
6.1 Feature Selection 
Given the different characteristics of languages and 
corpus annotations, it is worth while to select a 
different set of features for each language. For ex-
ample, certain corpora do not contain lemmas or 
morphological information so lexical information 
will be useful. Vice versa, when lemmas are pre-
sent, lexical information might be avoided, reduc-
ing the size of the feature set. 
We performed a series of feature selection ex-
periments on each language, starting from a fairly 
comprehensive set of 43 features and trying all 
variants obtained by dropping a single feature. The 
best of these alternatives feature models was cho-
sen and the process iterated until no further gains 
were achieved. The score for the alternatives was 
computed on a development set of approximately 
5000 tokens, extracted from a split of the original 
training corpus. 
Despite the process is not guaranteed to produce 
a global optimum, we noticed LAS improvements 
of up to 4 percentage points on some languages. 
The set of features to be used by DeSR is con-
trolled by a number of parameters supplied through 
a parameter file. Each parameter describes a fea-
ture and from which tokens to extract it. Tokens 
are referred through positive numbers for input 
tokens and negative numbers for tokens on the 
stack. For example 
PosFeatures -2 -1 0 1 2 3 
means to use the POS tag of the first two tokens on 
the stack and of the first four tokens on the input. 
The parameter PosPrev refers to the POS of the 
preceding token in the original sentence, PosLeftChild refers to the POS of the left chil-
dren of a token, PastActions tells how many 
previous actions to include as features. 
The selection process was started from the fol-
lowing base feature model: 
LexFeatures -1 0 1 LemmaFeatures -2 -1 0 1 2 3 LemmaPrev  -1 0 LemmaSucc  -1 0 LemmaLeftChild -1 0 LemmaRightChild -1 MorphoFeatures -1 0 1 2 PosFeatures -2 -1 0 1 2 3 PosNext  -1 0 PosPrev  -1 0 PosLeftChild -1 0 PosRightChild -1 0 CPosFeatures -1 0 1 DepFeatures -1 0 DepLeftChild -1 0 DepRightChild -1 PastActions 1 
The selection process produced different variants 
for each language, sometimes suggesting dropping 
certain intermediate features, like the lemma of the 
third next input token in the case of Catalan: 
LemmaFeatures -2 -1 0 1 3 LemmaPrev  0 LemmaSucc  -1 LemmaLeftChild 0 LemmaRightChild -1 PosFeatures -2 -1 0 1 2 3 PosPrev  0 PosSucc  -1 PosLeftChild -1 0 PosRightChild -1 0 CPosFeatures -1 0 1 MorphoFeatures 0 1 DepLeftChild -1 0 DepRightChild -1 
For Italian, instead, we ran a series of tests in par-
allel using a set of manually prepared feature mod-
els. The best of these models achieved a LAS of 
80.95%. The final run used this model with the 
addition of the morphological agreement feature 
discussed below. 
 
English was the only language for which no feature 
selection was done and for which lexical features 
1114
were used. English is also the language where the 
official score is significantly lower than what we 
had been getting on our development set (90.01% 
UAS). 
6.2 Prepositional Attachment 
Certain languages, such as Catalan, use detailed 
dependency labeling, that for instance distinguish 
between adverbials of location and time. We ex-
ploited this information by introducing a feature 
that captures the entity type of a child of the top 
word on the stack or in the input. During training a 
list of nouns occurring in the corpus as dependent 
on prepositions with label CCL (meaning ?com-
plement of location? for Catalan) was created and 
similarly for CCT (complement of time). The en-
tity type TIME is extracted as a feature depending 
on whether the noun occurs in the time list more 
than ? times than in the location list, and similarly 
for the feature LOCATION. ? was set to 1.5 in our 
experiments. 
6.3 Morphological Agreement 
Certain languages require gender and number 
agreement between head and dependent. The fea-
ture MorphoAgreement is computed for such lan-
guages and provided noticeable accuracy 
improvements. 
For example, for Italian, the improvement was 
from: 
  LAS: 80.95%,  UAS: 85.03% 
to: 
  LAS: 81.34%,  UAS: 85.54% 
For Catalan, adding this feature we obtained an 
unofficial score of: 
  LAS: 87.64%,  UAS: 92.20% 
with respect to the official run: 
  LAS: 86.86%,  UAS: 91.41% 
7 Accuracy 
Table 1 reports the accuracy scores in the multilin-
gual track. They are all considerably above the 
average and within 2% from the best for Catalan, 
3% for Chinese, Greek, Italian and Turkish. 
8 Performance 
The experiments were performed on a 2.4 Ghz 
AMD Opteron machine with 32 GB RAM. Train-
ing the parser using the 2nd-order perceptron on the 
English corpus required less than 3 GB of memory 
and about one hour for each iteration over the 
whole dataset. Parsing the English test set required 
39.97 sec. For comparison, we tested the MST 
parser version 0.4.3 (Mstparser, 2007), configured 
for second-order, on the same data: training took 
73.9 minutes to perform 10 iterations and parsing 
took 97.5 sec. MST parser achieved: 
LAS: 89.01%, UAS: 90.17% 
9 Error Analysis on Catalan 
The parser achieved its best score on Catalan, so 
we performed an analysis on its output for this lan-
guage. 
Among the 42 dependency relations that the 
parser had to assign to a sentence, the largest num-
ber of errors occurred assigning CC (124), SP (33), CD (27), SUJ (26), CONJUNCT (22), SN (23). 
The submitted run for Catalan did not use the 
entity feature discussed earlier and indeed 67 er-
rors were due to assigning CCT or CCL instead of 
CC (generic complement of circumstance). How-
ever over half of these appear as underspecified 
annotation errors in the corpus rather than parser 
errors. 
By adding the ChildEntityType feature, 
which distinguishes better between CCT and CCL, 
the UAS improved, while the LAS dropped 
slightly, due to the effect of underspecified annota-
tions in the corpus: 
   LAS: 87.22%,    UAS: 91.71% 
Table 1. Multilingual track official scores. 
LAS UAS 
Task 
1st DeSR Avg 1st DeSR Avg 
Arabic  76.52  72.66 68.34  86.09  82.53 78.84  
Basque  76.92  69.48 68.06  82.80  76.86 75.15  
Catalan  88.70  86.86 79.85  93.40  91.41 87.98  
Chinese  84.69  81.50 76.59  88.94  86.73 81.98  
Czech  80.19  77.37 70.12  86.28  83.40 77.56  
English  89.61  85.85 80.95  90.63  86.99 82.67  
Greek  76.31  73.92 70.22  84.08  80.75 77.78  
Hungarian  80.27  76.81 71.49  83.55  81.81 76.34  
Italian  84.40  81.34 78.06  87.91  85.54 82.45  
Turkish  79.81  76.87 73.19  86.22  83.56 80.33  
1115
A peculiar aspect of the original Catalan corpus 
was the use of a large number (195) of dependency 
labels. These labels were reduced to 42 in the ver-
sion used for CoNNL 2007, in order to make it 
comparable to other corpora. However, performing 
some preliminary experiments using the original 
Catalan collection with all 195 dependency labels, 
the DeSR parser achieved a significantly better 
score: 
LAS: 88.80%, UAS: 91.43% 
while with the modified one, the score dropped to: 
LAS: 84.55%, UAS: 89.38% 
This suggests that accuracy might improve for 
other languages as well if the training corpus was 
labeled with more precise dependencies. 
10 Adaptation Track 
The adaptation track originally covered two do-
mains, the CHILDES and the Chemistry domain.  
The CHILDES (Brown, 1973; MacWhinney, 
2000) consists of transcriptions of dialogues with 
children, typically short sentences of the kind: 
Would you like more grape juice ? 
That 's a nice box of books . 
Phrases are short, half of them are questions. The 
only difficulty that appeared from looking at the 
unlabeled collection supplied for training in the 
domain was the presence of truncated terms like goin (for going), d (for did), etc. However none 
of these unusually spelled words appeared in the 
test set, so a normal English parser performed rea-
sonably well on this task. Because of certain in-
consistencies in the annotation guidelines, the 
organizers decided to make this task optional and 
hence we submitted just the parse produced by the 
parser trained for English. 
For the second adaptation task we were given a 
large collection of unlabeled data in the chemistry 
domain (Kulick et al 2004) as well as a test set of 
5000 tokens (200 sentences) to parse (eng-lish_pchemtbtb_test.conll). 
There were three sets of unlabeled documents: 
we chose the smallest (unlab1) consisting of over 
300,000 tokens (11663 sentences). unlab1 was 
tokenized, POS and lemmas were added using our 
version of TreeTagger (Schmid, 1994), and lem-
mas replaced with stems, which had turned out to 
be more effective than lemmas. We call this set pchemtb_unlab1.conll. 
We trained the DeSR parser on English using english_ptb_train.conll, the WSJ PTB col-
lection provided for CoNLL 2007. This consists of 
WSJ sections 02-11, half of the usual set 02-23, for 
a total of 460,000 tokens with dependencies gener-
ated with the converter by Johansson and Nugues 
(2007). 
We added stems and produced a parser called DeSRwsj. By parsing eng-lish_pchem_test.conll with DeSRwsj we 
obtained pchemtb_test_base.desr, our base-
line for the task. 
By visual inspection using DgAnnotator 
(DgAnnotator, 2006), the parses looked generally 
correct. Most of the errors seemed due to improper 
handling of conjunctions and disjunctions. The 
collection in fact contains several phrases like: 
Specific antibodies raised against 
P450IIB1 , P450 IA1 or IA2 , 
P450IIE1 , and P450IIIA2 inhibited 
the activation in liver microsomes 
from rats pretreated with PB , BNF , 
INH and DEX respectively 
The parser did not seem to have much of a problem 
with terminology, possibly because the supplied 
gold POS were adequate. 
For the adaptation we proceeded as follows. We 
parsed pchemtb_unlab1.conll using DeSRwsj 
obtaining pchemtb_unlab1.desr. 
We then extracted a set of 12,500 sentences 
from ptb_train.conll and 7,500 sentences 
from pchemtb_unlab1.desr, creating a corpus 
of 20,000 sentences called combined.conll. In 
both cases the selection criteria was to choose sen-
tences shorter than 30 tokens. 
We then trained a low accuracy parser (called DesrCombined) on combined.conll, by using 
a 1st-order averaged perceptron. DesrCombined 
was used to parse english_ptb_train.conll, 
the original training corpus for English. By com-
paring this parse with the original, one can detect 
where such parser makes mistakes. The rationale 
for using an inaccurate parser is to obtain parses 
with many errors so that they form a suitably large 
training set for the next step: parser revision. 
We then used a parsing revision technique (At-
tardi and Ciaramita, 2007) to learn how to correct 
these errors, producing a parse reviser called DesrReviser. The revision technique consists of 
comparing the parse trees produced by the parser 
with the gold standard parse trees, from the 
annotated corpus. Where a difference is noted, a 
1116
revision rule is determined to correct the mistake. 
Such rules consist in movements of a single link to 
a different head. Learning how to revise a parse 
tree consists in training a classifier on a set of 
training examples consisting of pairs ?(wi, d, wj), 
ti?, i.e. the link to be modified and the 
transformation rule to apply. Attardi and Ciaramita 
(2007) showed that 80% of the corrections can be 
typically dealt with just 20 tree revision rules. For 
the adaptation track we limited the training to 
errors recurring at least 20 times and to 30 rules. DesrReviser was then applied to pchemtb_test_base.desr producing pchemtb_test_rev.desr, our final submission. 
Many conjunction errors were corrected, in par-
ticular by moving the head of the sentence from a 
coordinate verb to the conjunction ?and? linking 
two coordinate phrases. 
The revision step produced an improvement of 
0.42% LAS over the score achieved by using just 
the base DeSRwsj parser. 
Table 2 reports the official accuracy scores on 
the closed adaptation track. DeSR achieved a close 
second best UAS on the ptchemtb test set and 
third best on CHILDES. The results are quite en-
couraging, particularly considering that the revi-
sion step does not yet correct the dependency 
labels and that our base English parser had a lower 
rank in the multilingual track. 
 
LAS UAS 
Task 
1st DeSR Avg 1st DeSR Avg 
CHILDES     61.37 58.67 57.89 
Pchemtb  81.06 80.40 73.03 83.42  83.08 76.42 
Table 2. Closed adaptation track scores. 
Notice that the adaptation process could be iter-
ated. Since the combination DeSRwsj+DesrReviser is a more accurate parser 
than DeSRwsj, we could use it again to parse pchemtb_unlab1.conll and so on. 
11 Conclusions 
For performing multilingual parsing in the CoNLL 
2007 shared task we employed DeSR, a classifier-
based Shift/Reduce parser. We used a second order 
averaged perceptron as classifier and achieved ac-
curacy scores quite above the average in all lan-
guages. For proper comparison with other 
approaches, one should take into account that the 
parser is incremental and deterministic; hence it is 
typically faster than other non linear algorithms. 
For the adaptation track we used a novel ap-
proach, based on the technique of tree revision, 
applied to a parser trained on a corpus combining 
sentences from both the training and the adaptation 
domain. The technique achieved quite promising 
results and it also offers the interesting possibility 
of being iterated, allowing the parser to incorporate 
language knowledge from additional domains. 
Since the technique is applicable to any parser, 
we plan to test it also with more accurate English 
parsers. 
Acknowledgments.  The following treebanks 
were used for training the parser: (Aduriz et al, 
2003; B?hmov? et al, 2003; Chen et al, 2003; Ha-
ji? et al, 2004; Marcus et al, 1993; Mart? et al, 
2002; Montemagni et al 2003; Oflazer et al, 2003; 
Prokopidis et al, 2005; Csendes et al, 2005). 
Ryan McDonald and Jason Baldridge made avail-
able mstparser and helped us using it. We grate-
fully acknowledge Hugo Zaragoza and Ricardo 
Baeza-Yates for supporting the first author during 
a sabbatical at Yahoo! Research Barcelona. 
References 
A. Abeill?, editor. 2003. Treebanks: Building and Using 
Parsed Corpora. Kluwer. 
I. Aduriz, M. J. Aranzabe, J. M. Arriola, A. Atutxa, A. 
Diaz de Ilarraza, A. Garmendia and M. Oronoz. 
2003. Construction of a Basque Dependency Tree-
bank. In Proc. of the 2nd Workshop on Treebanks 
and Linguistic Theories (TLT), 201?204. 
G. Attardi. 2006. Experiments with a Multilanguage 
non-projective dependency parser. In Proc. of the 
Tenth CoNLL, 2006. 
G. Attardi, M. Ciaramita. 2007. Tree Revision Learning 
for Dependency Parsing. In Proc. of NAACL/HLTC 
2007. 
A. B?hmov?, J. Hajic, E. Hajicov? and B. Hladk?. 2003. 
The PDT: a 3-level annotation scenario. In Abeill? 
(2003), chapter 7, 103?127. 
R. Brown. 1973. A First Language: The Early Stages. 
Harvard University Press. 
K. Chen, C. Luo, M. Chang, F. Chen, C. Chen, C. 
Huang and Z. Gao. 2003. Sinica Treebank: Design 
Criteria, Representational Issues and Implementation. 
In Abeill? (2003), chapter 13, 231?248. 
1117
M. Ciaramita, G. Attardi. 2007. Dependency Parsing 
with Second-Order Feature Maps and Annotated Se-
mantic Information. Proc. of the 12th International 
Workshop on Parsing Technologies (IWPT), 2007. 
K. Crammer, Y. Singer. 2003. Ultraconservative Online 
Algorithms for Multiclass Problems. Journ. of Ma-
chine Learning Research. 
D. Csendes, J. Csirik, T. Gyim?thy, and A. Kocsor. 
2005. The Szeged Treebank. Springer.  
DgAnnotator. 2006. 
http://medialab.di.unipi.it/Project/Parser/DgAnnotato
r/. 
J. Hajic, O. Smrz, P. Zem?nek, J. Snaidauf and E. 
Beska. 2004. Prague Arabic Dependency Treebank: 
Development in Data and Tools. In Proc. of the 
NEMLAR Intern. Conf. on Arabic Language Re-
sources and Tools, 110?117. 
R. Johansson and P. Nugues. 2007. Extended 
constituent-to-dependency conversion for English. In 
Proc. of the 16th Nordic Conference on 
Computational Linguistics (NODALIDA).  
S. Kulick, A. Bies, M. Liberman, M. Mandel, R. Mc- 
Donald, M. Palmer, A. Schein, and L. Ungar. 2004. 
Integrated annotation for biomedical information ex- 
traction. In Proc. of the Human Language 
Technology Conference and the Annual Meeting of 
the North American Chapter of the Association for 
Computational Linguistics (HLT/NAACL).  
B. MacWhinney. 2000. The CHILDES Project: Tools 
for Analyzing Talk. Lawrence Erlbaum. 
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993. 
Building a large annotated corpus of English: the 
Penn Treebank. Computational Linguistics, 
19(2):313?330. 
M. A. Mart?, M. Taul?, L. M?rquez and M. Bertran. 
2007. CESS-ECE: A Multilingual and Multilevel 
Annotated Corpus. Available for download from: 
http://www.lsi.upc.edu/~mbertran/cess-ece/. 
R. McDonald, et al 2005. Non-projective Dependency 
Parsing using Spanning Tree Algorithms. In Proc. of 
HLT-EMNLP. 
B. MacWhinney. 2000. The CHILDES Project: Tools 
for Analyzing Talk. Lawrence Erlbaum. 
S. Montemagni, F. Barsotti, M. Battista, N. Calzolari, 
O. Corazzari, A. Lenci, A. Zampolli, F. Fanciulli, M. 
Massetani, R. Raffaelli, R. Basili, M. T. Pazienza, D. 
Saracino, F. Zanzotto, N. Nana, F. Pianesi, and R. 
Delmonte. 2003. Building the Italian Syntactic-
Semantic Treebank. In Abeill? (2003), chapter 11, 
189?210.  
Mstparser 0.4.3. 2007.  
http://sourceforge.net/projects/mstparser/ 
J. Nivre, et al 2004. Memory-based Dependency Pars-
ing. In Proc.s of the Eighth CoNLL, ed. H. T. Ng and 
E. Riloff, Boston, Massachusetts, 49?56. 
J. Nivre and J. Nilsson. 2005. Pseudo-Projective De-
pendency Parsing. In Proc. of the 43rd Annual Meet-
ing of the ACL, 99?106. 
J. Nivre and M. Scholz. 2004. Deterministic Depend-
ency Parsing of English Text. In Proc. of COLING 
2004, Geneva, Switzerland, 64?70. 
J. Nivre, J. Hall, S. K?bler, R. McDonald, J. Nilsson, S. 
Riedel, and D. Yuret. 2007. The CoNLL 2007 shared 
task on dependency parsing. In Proc. of the CoNLL 
2007 Shared Task. Joint Conf. on Empirical Methods 
in Natural Language Processing and Computational 
Natural Language Learning (EMNLP-CoNLL). 
K. Oflazer, B. Say, D. Zeynep Hakkani-T?r, and G. T?r. 
2003. Building a Turkish treebank. In Abeill? (2003), 
chapter 15, 261?277.  
M.F. Porter. 2001. Snowball Stemmer.  
http://www.snowball.tartarus.org/ 
P. Prokopidis, E. Desypri, M. Koutsombogera, H. 
Papageorgiou, and S. Piperidis. 2005. Theoretical 
and practical issues in the construction of a Greek 
depen- dency treebank. In Proc. of the 4th Workshop 
on Treebanks and Linguistic Theories (TLT), pages 
149?160. 
H. Schmid. 1994. Probabilistic Part-of-Speech Tagging 
Using Decision Trees. In Proc. of International Con-
ference on New Methods in Language Processing. 
V. N. Vapnik. 1998. The Statistical Learning Theory. 
Springer. 
H. Yamada and Y. Matsumoto. 2003. Statistical De-
pendency Analysis with Support Vector Machines. In 
Proc. of the 8th International Workshop on Parsing 
Technologies (IWPT), 195?206. 
 
1118
Proceedings of NAACL HLT 2007, pages 388?395,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Tree Revision Learning for Dependency Parsing
Giuseppe Attardi
Dipartimento di Informatica
Universita` di Pisa
Pisa, Italy
attardi@di.unipi.it
Massimiliano Ciaramita
Yahoo! Research Barcelona
Barcelona, Spain
massi@yahoo-inc.com
Abstract
We present a revision learning model for
improving the accuracy of a dependency
parser. The revision stage corrects the out-
put of the base parser by means of revi-
sion rules learned from the mistakes of
the base parser itself. Revision learning
is performed with a discriminative classi-
fier. The revision stage has linear com-
plexity and preserves the efficiency of the
base parser. We present empirical evalu-
ations on the treebanks of two languages,
which show effectiveness in relative error
reduction and state of the art accuracy.
1 Introduction
A dependency parse tree encodes useful semantic in-
formation for several language processing tasks. De-
pendency parsing is a simpler task than constituent
parsing, since dependency trees do not have ex-
tra non-terminal nodes and there is no need for a
grammar to generate them. Approaches to depen-
dency parsing either generate such trees by consid-
ering all possible spanning trees (McDonald et al,
2005), or build a single tree on the fly by means of
shift-reduce parsing actions (Yamada & Matsumoto,
2003). In particular, Nivre and Scholz (2004) and
Attardi (2006) have developed deterministic depen-
dency parsers with linear complexity, suitable for
processing large amounts of text, as required, for ex-
ample, in information retrieval applications.
We investigate a novel revision approach to
dependency parsing related to re-ranking and
transformation-based methods (Brill, 1993; Brill,
1995; Collins, 2000; Charniak & Johnson, 2005;
Collins & Koo, 2006). Similarly to re-ranking, the
second stage attempts to improve the output of a
base parser. Instead of re-ranking n-best candi-
date parses, our method works by revising a sin-
gle parse tree, either the first-best or the one con-
structed by a deterministic shift-reduce parser, as in
transformation-based learning. Parse trees are re-
vised by applying rules which replace incorrect with
correct dependencies. These rules are learned by
comparing correct parse trees with incorrect trees
produced by the base parser on a training corpus.
We use the same training corpus on which the base
parser was trained, but this need not be the case.
Hence, we define a new learning task whose output
space is a set of revision rules and whose input is
a set of features extracted at each node in the parse
trees produced by the parser on the training corpus.
A statistical classifier is trained to solve this task.
The approach is more suitable for dependency
parsing since trees do not have non-terminal nodes,
therefore revisions do not require adding/removing
nodes. However, the method applies to any parser
since it only analyzes output trees. An intuitive mo-
tivation for this method is the observation that a
dependency parser correctly identifies most of the
dependencies in a tree, and only local corrections
might be necessary to produce a correct tree. Per-
forming several parses in order to generate multiple
trees would often just repeat the same steps. This
could be avoided by focusing on the points where at-
tachments are incorrect. In the experiments reported
below, on average, the revision stage performs 4.28
388
corrections per sentence, or one every 6.25 tokens.
In our implementation we adopt a shift-reduce
parser which minimizes computational costs. The
resulting two-stage parser has complexity O(n), lin-
ear in the length of the sentence. We evaluated our
model on the treebanks of English and Swedish. The
experimental results show a relative error reduction
of, respectively, 16% and 11% with respect to the
base parser, achieving state of accuracy on Swedish.
2 Dependency parsing
Detection of dependency relations can be useful
in tasks such as information extraction (Culotta &
Sorensen, 2004), lexical acquisition (Snow et al,
2005), ontology learning (Ciaramita et al, 2005),
and machine translation (Ding & Palmer, 2005).
A dependency parser is trained on a corpus an-
notated with lexical dependencies, which are eas-
ier to produce by annotators without deep linguis-
tic knowledge and are becoming available in many
languages (Buchholz & Marsi, 2006). Recent de-
velopments in dependency parsing show that deter-
ministic parsers can achieve good accuracy (Nivre &
Scholz, 2004), and high performance, in the range of
hundreds of sentences per second (Attardi, 2006).
A dependency parser takes as input a sentence
s and returns a dependency graph G. Let D =
{d1, d2, ..., dm} be the set of permissible depen-
dency types. A dependency graph for a sentence
s = ?s1, s2, ..., sn? is a labeled directed graph G =
(s,A), such that:
(a) s is the set of nodes, corresponding to the to-
kens in the input string;
(b) A is a set of labeled arcs (wi, d, wj), wi,j ? s,
d ? D; wj is called the head, wi the modifier
and d the dependency label;
(c) ?wi ? s there is at most one arc a ? A, such
that a = (wi, d, wj);
(d) there are no cycles;
In statistical parsing a generator (e.g. a
PCFG) is used to produce a number of candidate
trees (Collins, 2000) with associated scores. This
approach has been used also for dependency parsing,
generating spanning trees as candidates and comput-
ing the maximum spanning tree using discriminative
learning algorithms (McDonald et al, 2005).
Shift ?S,n|I,T,A??n|S,I,T,A? (1)
Right ?s|S,n|I,T,A??S,n|I,T,A?{(s,r,n)}? (2)
Left ?s|S,n|I,T,A??S,s|I,T,A?{(n,r,s)}? (3)
Right2
?s1|s2|S,n|I,T,A?
?s1|S,n|I,T,A?{(s2,r,n)}?
(4)
Left2
?s1|s2|S,n|I,T,A?
?s2|S,s1|I,T,A?{(n,r,s2)}?
(5)
Right3
?s1|s2|s3|S,n|I,T,A?
?s1|s2|S,n|I,T,A?{(s3,r,n)}?
(6)
Left3
?s1|s2|s3|S,n|I,T,A?
?s2|s3|S,s1|I,T,A?{(n,r,s3)}?
(7)
Extract ?s1|s2|S,n|I,T,A??n|s1|S,I,s2|T,A? (8)
Insert ?S,I,s1|T,A??s1|S,I,T,A? (9)
Table 1. The set of parsing rules of the base parser.
Yamada and Matsumoto (2003) have proposed an
alternative approach, based on deterministic bottom-
up parsing. Instead of learning directly which tree
to assign to a sentence, the parser learns which
Shift/Reduce actions to use for building the tree.
Parsing is cast as a classification problem: at each
step the parser applies a classifier to the features rep-
resenting its current state to predict the next action to
perform. Nivre and Scholz (2004) proposed a vari-
ant of the model of Yamada and Matsumoto that re-
duces the complexity from the worst case quadratic
to linear. Attardi (2006) proposed a variant of the
rules that allows deterministic single-pass parsing
and as well as handling non-projective relations.
Several approaches to dependency parsing on multi-
ple languages have been evaluated in the CoNLL-X
Shared Task (Buchholz & Marsi, 2006).
3 A shift-reduce dependency parser
As a base parser we use DeSR, a shift-reduce
parser described in (Attardi, 2006). The parser
constructs dependency trees by scanning input sen-
tences in a single left-to-right pass and performing
Shift/Reduce parsing actions. The parsing algorithm
is fully deterministic and has linear complexity. Its
behavior can be described as repeatedly selecting
and applying some parsing rules to transform its
state.
The state of the parser is represented by a quadru-
389
ple ?S, I, T,A?: S is the stack, I is the list of (re-
maining) input tokens, T is a stack of saved to-
kens and A is the arc relation for the dependency
graph, consisting of a set of labeled arcs (wi, r, wj),
wi, wj ? W (the set of tokens), and d ? D (the
set of dependencies). Given an input sentence s,
the parser is initialized to ??, s, ?, ??, and terminates
when it reaches the configuration ?s, ?, ?, A?.
Table 1 lists all parsing rules. The Shift rule
advances on the input, while the various Left,
Right variants create links between the next in-
put token and some previous token on the stack.
Extract/Insert generalize the previous rules by
respectively moving one token to the stack T and
reinserting the top of T into S. An essential differ-
ence with respect to the rules of Yamada and Mat-
sumoto (2003) is that the Right rules move back to
the input the top of the stack, allowing some further
processing on it, which would otherwise require a
second pass. The extra Left and Right rules (4-
7, Table 1), and the ExtractInsert rules (8 and
9, Table 1), are new rules added for handling non-
projective trees. The algorithm works as follows:
Algorithm 1: DeSR
input: s = w1, w2, ..., wn
begin
S ? ??
I ? ?w1, w2, ..., wn?
T ? ??
A? ??
while I 6= ?? do
x? getContext(S, I, T,A)
y ? estimateAction(w,x)
performAction(y, S, I, T,A)
end
The function getContext() extracts a vector x
of contextual features around the current token, i.e.,
from a subset of I and S. estimateAction() pre-
dicts a parsing action y given a trained modelw and
x. In the experiments presented below, we used as
features the lemma, Part-of-Speech, and dependency
type of the following items:
? 2 top items from S;
? 4 items from I;
Step Description
r Up to root node
u Up one parent
?n Left to the n-th token
+n Right to the n-th token
[ Head of previous constituent
] Head of following constituent
> First token of previous constituent
< First token of following constituent
d?? Down to the leftmost child
d + + Down to the rightmost child
d? 1 Down to the first left child
d + 1 Down to the first right child
dP Down to token with POS P
Table 2. Description of the atomic movements allowed on
the graph relatively to a token w.
? 2 leftmost and 2 rightmost children from the
top of S and I .
4 Revising parse trees
The base parser is fairly accurate and even when
there are mistakes most sentence chunks are correct.
The full correct parse tree can often be recovered by
performing just a small number of revisions on the
base parse. We propose to learn these revisions and
to apply them to the single best tree output by the
base parser. Such an approach preserves the deter-
ministic nature of the parser, since revising the tree
requires a second sequential step over the whole sen-
tence. The second step may also improve accuracy
by incorporating additional evidence, gathered from
the analysis of the tree which is not available during
the first stage of parsing.
Our approach introduces a second learning task
in which a model is trained to revise parse trees.
Several questions needs to be addressed: which tree
transformations to use in revising the parse tree,
how to determine which transformation to apply, in
which order, and which features to use for learning.
4.1 Basic graph movements
We define a revision as a combination of atomic
moves on a graph; e.g., moving a link to the follow-
ing or preceding token in the sentence, up or down
the graph following the directed edges. Table 2 sum-
marizes the set of atomic steps we used.
390
Figure 1. An incorrect dependency tree: the dashed arrow from ?sale? to ?by? should be replaced with the one from
?offered? to ?by?.
4.2 Revision rules
A revision rule is a sequence of atomic steps on the
graph which identifies the head of a modifier. As an
example, Figure 1 depicts a tree in which the mod-
ifier ?by? is incorrectly attached to the head ?sale?
(dashed arrow), rather than to the correct head ?of-
fered? (continuous arrow)1. There are several possi-
ble revision rules for this case: ?uu?, move up two
nodes; ?3, three tokens to the left, etc. To bound
the complexity of feature extraction the maximum
length of a sequence is bound to 4. A revision for
a dependency relation is a link re-direction, which
moves a single link in a tree to a different head. This
is an elementary transformation which preserves the
number of nodes in the tree.
A possible problem with these rules is that they
are not tree-preserving, i.e. a tree may become a
cyclic graph. For instance, rules that create a link
to a descendant introduce cycles, unless the appli-
cation of another rule will link one of the nodes in
the path to the descendant to a node outside the cy-
cle. To address these issues we apply the following
heuristics in selecting the proper combination: rules
that redirect to child nodes are chosen only when
no other rule is applicable (upwards rule are safe),
and shorter rules are preferred over longer ones. In
our experiments we never observed the production
of any cycles.
On Wall Street Journal Penn Treebank section 22
we found that the 20 most frequent rules are suffi-
cient to correct 80% of the errors, see Table 3. This
confirms that the atomic movements produce simple
and effective revision rules.
1Arrows go from head to modifier as agreed among the par-
ticipants to the CoNLL-X shared task.
COUNTS RULE TARGET LOCATION
983 uu Up twice
685 -1 Token to the left
469 +1 Token to the right
265 [ Head of previous constituent
215 uuu Up 3 times
197 +1u Right, up
194 r To root
174 -1u Left, up
116 >u Token after constituent, up
103 ud?? Up down to leftmost child
90 V To 1st child with POS verb
83 d+1 Down to first right child
82 uuuu Up 4 times
74 < Token before constituent
73 ud+1 Up down to 1st right child
71 uV Up, down to 1st verb
61 ud-1 Up, down to last left child
56 ud+1d+1 Up, down to 1st right child twice
55 d+1d+1 Down to 1st right child twice
48 d?? Down to leftmost child
Table 3. 20 most frequent revision rules in wsj22.
4.3 Tree revision problem
The tree revision problem can be formalized as fol-
lows. Let G = (s,A) be a dependency tree for
sentence s = ?w1, w2, ..., wn?. A revision rule is
a mapping r : A ? A which, when applied to an
arc a = (wi, d, wj), returns an arc a? = (wi, d, ws).
A revised parse tree is defined as r(G) = (s,A?)
such that A? = {r(a) : a ? A}.
This definition corresponds to applying the revi-
sions to the original tree in a batch, as in (Brill,
1993). Alternatively, one could choose to apply the
transformations incrementally, applying each one to
the tree resulting from previous applications. We
chose the first alternative, since the intermediate
trees created during the transformation process may
not be well-formed dependency graphs, and analyz-
ing them in order to determine features for classifi-
391
cation might incur problems. For instance, the graph
might have abnormal properties that differ from
those of any other graph produced by the parser.
Moreover, there might not be enough cases of such
graphs to form a sufficiently large training set.
5 Learning a revision model
We frame the problem of revising a tree as a super-
vised classification task. Given a training set S =
(xi, yi)Ni=1, such that xi ? IR
d and yi ? Y , our goal
is to learn a classifier, i.e., a function F : X ? Y .
The output space represents the revision rules, in
particular we denote with y1 the identity revision
rule. Features represents syntactic and morphologi-
cal properties of the dependency being examined in
its context on the graph.
5.1 Multiclass perceptron
The classifier used in revision is based on the per-
ceptron algorithm (Rosemblatt, 1958), implemented
as a multiclass classifier (Crammer & Singer, 2003).
One introduces a weight vector ?i ? IRd for each
yi ? Y , in which ?i,j represents the weight associ-
ated with feature j in class i, and learn ? with the
perceptron from the training data using a winner-
take-all discriminant function:
F (x) = argmax
y?Y
?x, ?y? (10)
The only adjustable parameter in this model is the
number of instances T to use for training. We chose
T by means of validation on the development data,
typically with a value around 10 times the size of the
training data. For regularization purposes we adopt
an average perceptron (Collins, 2002) which returns
for each y, ?y = 1T
?T
t=1 ?
t
y, the average of all
weight vectors ?ty posited during training. The per-
ceptron was chosen because outperformed other al-
gorithms we experimented with (MaxEnt, MBL and
SVM), particularly when including feature pairs, as
discussed later.
5.2 Features
We used as features for the revision phase the same
type of features used for training the parser (de-
scribed in Section 3). This does not have to be the
case in general. In fact, one might want to introduce
features that are specific for this task. For example,
global features of the full tree which might be not
possible to represent or extract while parsing, as in
statistical parse re-ranking (Collins & Koo, 2006).
The features used are lemma, Part-of-Speech, and
dependency type of the following items: the current
node, its parent, grandparent, great-grandparent, of
the children thereof and, in addition, the previous
and next tokens of the node. We also add as features
all feature pairs that occurred more than 10 times,
to reduce the size of the feature space. In alternative
one could use a polynomial kernel. We preferred this
option because, given the large size of the training
data, a dual model is often impractical.
5.3 Revision model
Given a dependency graph G = (s,A), for a sen-
tence s = ?w1, ..., wn?, the revised tree is R(G) =
(s,A?), where each dependency a?i is equal to F (ai).
In other words, the head in ai has been changed, or
not, according to the rule predicted by the classifier.
In particular, we assume that revisions are indepen-
dent of each other and perform a revision of a tree
from left to right. As Table 3 suggests, there are
many revision rules with low frequency. Rather than
learning a huge classifier, for rules with little train-
ing data, we limit the number of classes to a value
k. We experimented with values between 30 and
50, accounting for 98-99% of all rules, and even-
tually used 50, by experimenting with the develop-
ment portion of the data. All rules that fall outside
the threshold are collected in a single class y0 of ?un-
resolved? cases. If predicted, y0, similarly to y1, has
no effect on the dependency.
Occasionally, in 59 sentences out of 2416 on
section 23 of the Wall Street Journal Penn Tree-
bank (Marcus et al, 1993), the shift-reduce parser
fails to attach a node to a head, producing a dis-
connected graph. The disconnected node will ap-
pear as a root, having no head. The problem occurs
most often on punctuations (66/84 on WSJ section
23), so it affects only marginally the accuracy scores
(UAS, LAS) as computed in the CoNLL-X evalua-
tion (Buchholz & Marsi, 2006). A final step of the
revision deals with multiple roots, using a heuristic
rule it selects one of the disconnected sub-trees as
root, a verb, and attaches all sub-trees to it.
392
Figure 2. Frequency of the 30 most frequent rules ob-
tained with different parsers on wsj22 and wsj2-21.
5.4 Algorithm complexity
The base dependency parser is deterministic and per-
forms a single scan over the sentence. For each word
it performs feature extraction and invokes the classi-
fier to predict the parsing action. If prediction time is
bound by a constant, as in linear classifiers, parsing
has linear complexity. The revision pass is deter-
ministic and performs similar feature extraction and
prediction on each token. Hence, the complexity of
the overall parser is O(n). In comparison, the com-
plexity of McDonald?s parser (2006) is cubic, while
the parser of Yamada and Matsumoto (2003) has a
worst case quadratic complexity.
6 Experiments
6.1 Data and setup
We evaluated our method on English using the stan-
dard partitions of the Wall Street Journal Penn Tree-
bank: sections 2-21 for training, section 22 for
development, and section 23 for evaluation. The
constituent trees were transformed into dependency
trees by means of a script implementing rules pro-
posed by Collins and Yamada2. In a second eval-
uation we used the Swedish Treebank (Nilsson et
al., 2005) from CoNLL-X, approximately 11,000
sentences; for development purposes we performed
cross-validation on the training data.
We trained two base parsers on the Penn Tree-
bank: one with our own implementation of Maxi-
2http://w3.msi.vxu.se/%7enivre/research/Penn2Malt.html
Parser UAS LAS
DeSR-ME 84.96 83.53
DeSR-MBL 88.41 86.85
Revision-MBL 89.11 86.39
Revision-ME 90.27 86.44
N&S 87.3 -
Y&M 90.3 -
MST-2 91.5 -
Table 4. Results on the Wall Street Journal Penn Tree-
bank.
mum Entropy, one with the TiMBL library for Mem-
ory Based Learning (MBL, (Timbl, 2003)). We
parsed sections 2 to 21 with each parser and pro-
duced two datasets for training the revision model:
?wsj2-21.mbl? and ?wsj2-21.me?. Each depen-
dency is represented as a feature vector (cf. Sec-
tion 5.2), the prediction is a revision rule (cf. Sec-
tion 4.2). For the smaller Swedish data we trained
one base parser with MaxEnt and one with the SVM
implementation in libSVM (Chang & Lin, 2001) us-
ing a polynomial kernel with degree 2.
6.2 Results
On the Penn Treebank, the base parser trained with
MBL (DeSR-MBL) achieves higher accuracy, 88.41
unlabeled accuracy score (UAS), than the same
parser trained with MaxEnt (DeSR-ME), 84.96
UAS. The revision model trained on ?wsj2-21.me?
(Revision-ME) increases the accuracy of DeSR-ME
to 88.01 UAS (+3%). The revision model trained
on ?wsj2-21.mbl? (DeSR-MBL) improves the accu-
racy of DeSR-MBL from 88.42 to 89.11 (+0.7%).
The difference is mainly due to the fact that DeSR-
MBL is quite accurate on the training data, almost
99%, hence ?wsj2-21.mbl? contains less errors on
which to train the revision parser. This is typi-
cal of the memory-based learning algorithm used
in DeSR-MBL. Conversely, DeSR-ME achieves a
score of of 85% on the training data, which is
closer to the actual accuracy of the parser on unseen
data. As an illustration, Figure 2 plots the distri-
butions of revision rules in ?wsj2-21.mbl? (DeSR-
MBL), ?wsj2-21.me? (DeSR-ME), and ?wsj22.mbl?
(DeSR-MBL) which represents the distribution of
correct revision rules on the output of DeSR-MBL
on the development set. The distributions of ?wsj2-
393
Parser UAS LAS
DeSR-SVM 88.41 83.31
Revision-ME 89.76 83.13
Corston-Oliver& Aue 89.54 82.33
Nivre 89.50 84.58
Table 5. Results on the Swedish Treebank.
21.me? and ?wsj22.mbl? are visibly similar, while
?wsj2-21.mbl? is significantly more skewed towards
not revising. Hence, the less accurate parser DeSR-
ME might be more suitable for producing revision
training data. Applying the revision model trained
on ?wsj2-21.me? (Revision-ME) to the output of
DeSR-MBL the result is 90.27% UAS. A relative
error reduction of 16.05% from the previous 88.41
UAS of DeSR-MBL. This finding suggests that it
may be worth while experimenting with all possi-
ble revision-model/base-parser pairs as well as ex-
ploring alternative ways for generating data for the
revision model; e.g., by cross-validation.
Table 4 summarizes the results on the Penn Tree-
bank. Revision models are evaluated on the output
of DeSR-MBL. The table also reports the scores ob-
tained on the same data set by by the shift reduce
parsers of Nivre and Scholz?s (2004) and Yamada
andMatsumoto (2003), andMcDonald and Pereira?s
second-order maximum spanning tree parser (Mc-
Donald & Pereira, 2006). However the scores are
not directly comparable, since in our experiments
we used the settings of the CoNLL-X Shared Task,
which provide correct POS tags to the parser.
On the Swedish Treebank collection we trained
a revision model (Revision-ME) on the output of
the MaxEnt base parser. We parsed the evalua-
tion data with the SVM base parser (DeSR-SVM)
which achieves 88.41 UAS. The revision model
achieves 89.76 UAS, with a relative error reduc-
tion of 11.64%. Here we can compare directly with
the best systems for this dataset in CoNLL-X. The
best system (Corston-Oliver & Aue, 2006), a vari-
ant of the MST algorithm, obtained 89.54 UAS,
while the second system (Nivre, 2006) obtained
89.50; cf. Table 5. Parsing the Swedish evalua-
tion set (about 6,000 words) DeSR-SVM processes
1.7 words per second on a Xeon 2.8Ghz machine,
DeSR-ME parses more than one thousand w/sec. In
the revision step Revision-ME processes 61 w/sec.
7 Related work
Several authors have proposed to improve parsing
via re-ranking (Collins, 2000; Charniak & Johnson,
2005; Collins & Koo, 2006). The base parser pro-
duces a list of n-best parse trees for a sentence. The
re-ranker is trained on the output trees, using addi-
tional global features, with a discriminative model.
These approaches achieve error reductions up to
13% (Collins & Koo, 2006). In transformation-
based learning (Brill, 1993; Brill, 1995; Satta &
Brill, 1995) the learning algorithm starts with a
baseline assignment, e.g., the most frequent Part-of-
Speech for a word, then repeatedly applies rewriting
rules. Similarly to re-ranking our method aims at
improving the accuracy of the base parser with an
additional learner. However, as in transformation-
based learning, it avoids generating multiple parses
and applies revisions to arcs in the tree which it con-
siders incorrect. This is consistent with the architec-
ture of our base parser, which is deterministic and
builds a single tree, rather than evaluating the best
outcome of a generator.
With respect to transformation-based methods,
our method does not attempt to build a tree but only
to revise it. That is, it defines a different output space
from the base parser?s: the possible revisions on the
graph. The revision model of Nakagawa et al (2002)
applies a second classifier for deciding whether the
predictions of a base learner are accurate. However,
the model only makes a binary decision, which is
suitable for the simpler problem of POS tagging.
The work of Hall and Novak (Hall & Novak, 2005)
is the closest to ours. Hall and Novak develop a cor-
rective model for constituency parsing in order to
recover non-projective dependencies, which a stan-
dard constituent parser does not handle. The tech-
nique is applied to parsing Czech.
8 Conclusion
We presented a novel approach for improving the
accuracy of a dependency parser by applying re-
vision transformations to its parse trees. Experi-
mental results prove that the approach is viable and
promising. The proposed method achieves good ac-
curacy and excellent performance using a determin-
istic shift-reduce base parser. As an issue for further
investigation, we mention that in this framework, as
394
in re-ranking, it is possible to exploit global features
in the revision phase; e.g., semantic features such as
those produced by named-entity detection systems.
Acknowledgments
We would like to thank Jordi Atserias and Brian
Roark for useful discussions and comments.
References
G. Attardi. 2006. Experiments with a Multilanguage
Non-Projective Dependency Parser. In Proceedings of
CoNNL-X 2006.
S. Buchholz and E. Marsi. 2006. Introduction to
CoNNL-X Shared Task on Multilingual Dependency
Parsing. In Proceedings of CoNNL-X 2006.
E. Brill. 1993. Automatic Grammar Induction and Pars-
ing free Text: A Transformation-Based Approach. In
Proceedings of ACL 1993.
E. Brill. 1995. Transformation-Based Error-Driven
Learning and Natural Language Processing. Compu-
tational Linguistics 21(4): pp.543-565.
E. Charniak and M. Johnson. 2005. Coarse-to-Fine n-
Best Parsing and MaxEnt Discriminative Reranking.
In Proceedings of ACL 2005.
C. Chang and C. Lin. 2001. LIBSVM: A Library
for Support Vector Machines. Software available at
http://www.csie.ntu.edu.tw/ cjlin/libsvm.
M. Ciaramita, A. Gangemi, E. Ratsch, J. Saric? and I. Ro-
jas. 2005. Unsupervised Learning of Semantic Rela-
tions between Concepts of a Molecular Biology Ontol-
ogy. In Proceedings of IJCAI 2005.
M. Collins. 2000. Discriminative Reranking for Natural
Language Parsing. In Proceedings of ICML 2000.
M. Collins. 2002. Discriminative Training Meth-
ods for Hidden Markov Models: Theory and Experi-
ments with Perceptron Algorithms. In Proceedings of
EMNLP 2002.
M. Collins and T. Koo. 2006. Discriminative Reranking
for Natural Language Parsing. Computational Lin-
guistics 31(1): pp.25-69.
K. Crammer and Y. Singer. 2003. Ultraconservative On-
line Algorithms for Multiclass Problems. Journal of
Machine Learning Research 3: pp.951-991.
S. Corston-Oliver and A. Aue. 2006. Dependency Pars-
ing with Reference to Slovene, Spanish and Swedish.
In Proceedings of CoNLL-X.
A. Culotta and J. Sorensen. 2004. Dependency Tree Ker-
nels for Relation Extraction. In Proceedings of ACL
2004.
W. Daelemans, J. Zavrel, K. van der Sloot, and
A. van den Bosch. 2003. Timbl: Tilburg memory
based learner, version 5.0, reference guide. Technical
Report ILK 03-10, Tilburg University, ILK.
Y. Ding and M. Palmer. 2005. Machine Translation us-
ing Probabilistic Synchronous Dependency Insertion
Grammars. In Proceedings of ACL 2005.
K. Hall and V. Novak. 2005. Corrective Modeling for
Non-Projective Dependency Parsing. In Proceedings
of the 9th International Workshop on Parsing Tech-
nologies.
M. Marcus, B. Santorini and M. Marcinkiewicz. 1993.
Building a Large Annotated Corpus of English: The
Penn Treebank. Computational Linguistics, 19(2): pp.
313-330.
R. McDonald, F. Pereira, K. Ribarov and J. Hajic?. 2005.
Non-projective Dependency Parsing using Spanning
Tree Algorithms. In Proceedings of HLT-EMNLP
2005.
R. McDonald and F. Pereira. 2006. Online Learning
of Approximate Dependency Parsing Algorithms. In
Proceedings of EACL 2006.
T. Nakagawa, T. Kudo and Y. Matsumoto. 2002. Revi-
sion Learning and its Applications to Part-of-Speech
Tagging. In Proceedings of ACL 2002.
J. Nilsson, J. Hall and J. Nivre. 2005. MAMBA Meets
TIGER: Reconstructing a Swedish Treebank from An-
tiquity. In Proceedings of the NODALIDA.
J. Nivre and M. Scholz. 2004. Deterministic Depen-
dency Parsing of English Text. In Proceedings of
COLING 2004.
J. Nivre. 2006. Labeled Pseudo-Projective Dependency
Parsing with Support Vector Machines. In Proceed-
ings of CoNLL-X.
F. Rosemblatt. 1958. The Perceptron: A Probabilistic
Model for Information Storage and Organization in the
Brain. Psych. Rev., 68: pp. 386-407.
G. Satta and E. Brill. 1995, Efficient Transformation-
Based Parsing. In Proceedings of ACL 1996.
R. Snow, D. Jurafsky and Y. Ng 2005. Learning Syn-
tactic Patterns for Automatic Hypernym Discovery. In
Proceedings of NIPS 17.
H. Yamada and Y. Matsumoto. 2003. Statistical De-
pendency Analysis with Support Vector Machines.
In Proceedings of the 9th International Workshop on
Parsing Technologies.
395
Proceedings of NAACL HLT 2009: Short Papers, pages 261?264,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Reverse Revision and Linear Tree Combination for Dependency Parsing
Giuseppe Attardi
Dipartimento di Informatica
Universita` di Pisa
Pisa, Italy
attardi@di.unipi.it
Felice Dell?Orletta
Dipartimento di Informatica
Universita` di Pisa
Pisa, Italy
felice.dellorletta@di.unipi.it
1 Introduction
Deterministic transition-based Shift/Reduce depen-
dency parsers make often mistakes in the analysis of
long span dependencies (McDonald & Nivre, 2007).
Titov and Henderson (2007) address this accuracy
drop by using a beam search instead of a greedy al-
gorithm for predicting the next parser transition.
We propose a parsing method that allows reduc-
ing several of these errors, although maintaining a
quasi linear complexity. The method consists in two
steps: first the sentence is parsed by a determinis-
tic Shift/Reduce parser, then a second deterministic
Shift/Reduce parser analyzes the sentence in reverse
using additional features extracted from the parse
trees produced by the first parser.
Right-to-left parsing has been used as part of
ensemble-based parsers (Sagae & Lavie, 2006; Hall
et al, 2007). Nivre and McDonald (2008) instead
use hints from one parse as features in a second
parse, exploiting the complementary properties of
graph-based parsers (Eisner, 1996; McDonald et al,
2005) and transition-based dependency parsers (Ya-
mada & Matsumoto, 2003; Nivre & Scholz, 2004).
Also our method uses input from a previous parser
but only uses parsers of a single type, determin-
istic transition-based Shift/Reduce, maintaining an
overall linear complexity. In fact both the en-
semble parsers and the stacking solution of Nivre-
McDonald involve the computation of the maximum
spanning tree (MST) of a graph, which require algo-
rithms of quadratic time complexity (e.g. (Chu &
Liu, 1965; Edmonds, 1967)).
We introduce an alternative linear combination
method. The algorithm is greedy and works by com-
bining the trees top down. We tested it on the de-
pendency trees produced by three parsers, a Left-
to-Right (LR ), a Right-to-Left (RL ) and a stacked
Right-to-Left parser, or Reverse Revision parser
(Rev2 ). 1 The experiments show that in practice
its output often outperforms the results produced by
calculating the MST.
2 Experiments
In the reported experiments we used DeSR (Attardi
at al., 2007), a freely available implementation of
a transition-based parser. The parser processes in-
put tokens advancing on the input with Shift actions
and accumulates processed tokens on a stack with
Reduce actions. The parsing algorithm is fully de-
terministic and linear.
For the LR parser and the Rev2 parser we em-
ployed an SVM classifier while aMaximum Entropy
classifier, with lower accuracy, was used to create
the training set for the Rev2 parser. The reason for
this appears to be that the output of a low accuracy
parser with many errors provides a better source of
learning to the stacked parser.
The Rev2 parser exploits the same basic set of
features as in the LR parser plus the additional fea-
tures extracted from the output of the LR parser
listed in Table 1, where: PHLEMMA is the lemma
of the predicted head, PHPOS is the Part of Speech
of the predicted head, PDEP is the predicted depen-
dency label of a token to its predicted head, PHDIST
indicates whether a token is located before or after
1The stacked Left-to-Right parser produced slightly worse
results than Rev2.
261
Feature Tokens
PHHLEMMA w0 w1
PHDEP w0 w1
PHPOS s0 w0 w1
PHLEMMA s0 w0 w1
PDEP s0 w0 w1
PHDIST s0 w0 w1
Table 1: Additional features used in training the Revision
parser.
its predicted head, PHHLEMMA is the lemma of
the predicted grandparent and PHDEP is the pre-
dicted dependency label of the predicted head of a
token to the predicted grandparent. s0 refers to a to-
ken on top of the stack, wi refers to word at the i-th
relative position with respect to the current word and
parsing direction. This feature model was used for
all languages in our tests.
We present experiments and comparative error
analysis on three representative languages from the
CoNLL 2007 shared task (Nivre at al., 2007): Ital-
ian, Czech and English. We also report an evaluation
on all thirteen languages of the CoNLL-X shared
task (Buchholz &Marsi, 2006), for comparison with
the results by Nivre and McDonald (2008).
Table 2 shows the Labeled Attachment Score
(LAS), for the Left-to-right parser (LR ), Right-to-
Left (RL ), Reverse Revision parser (Rev2 ), linear
parser combination (Comb) and MST parser combi-
nation (CombMST).
Figure 1 and 2 present the accuracies of the LR
and Rev2 parsers for English relative to the depen-
dency length and the length of sentences, respec-
tively. For Czech and Italian the RL parser achieves
higher accuracy than the LR parser and the Rev2
parser even higher. The error analysis for Czech
showed that the Rev2 parser improves over the LR
parser everywhere except in the Recall for depen-
dencies of length between 10 and 14. Such an im-
provement has positive impact on the analysis of
sentences longer than 10 tokens, like for Italian.
2.1 CoNLL-X Results
For direct comparison with the approach by Nivre
and McDonald (2008), we present the results on the
CoNLL-X corpora (Table 3): MST and MSTMalt
are the results achieved by the MST parser and the
MST parser using hints from Maltparser, Malt and
 
0.4
 
0.5
 
0.6
 
0.7
 
0.8
 
0.9 1  0
 
5
 
10
 
15
 
20
 
25
 
30
 
35
F-Measure
Depen
dency
 Lengt
h
Left-to
-Right
_DeSR
Revis
ion_D
eSR
Figure 1: English. F-Measure relative to dependency
length.
 
80
 
82
 
84
 
86
 
88
 
90
 
92
 
94
 
96  10
 
20
 
30
 
40
 
50
 
60
Dependency Accuracy
Sente
nce Le
ngthLeft-to
-Right
_DeSR
Revis
ion_D
eSR
Figure 2: English. Accuracy relative to sentence length.
MaltMST the results of the opposite stacking.
2.2 Remarks
The Rev2 parser, informed with data from the LR
parser, achieves better accuracy in twelwe cases, sta-
tistically significantly better in eight.
The error analysis confirms that indeed the Rev2
parser is able to reduce the number of errors made on
long dependency links, which are a major weakness
of a deterministic Shift/Reduce parser. The accuracy
of the Rev2 parser might be further improved by
more sophisticated feature selection, choosing fea-
tures that better represent hints to the second parsing
stage.
3 Linear Voting Combination
Our final improvements arise by combining the out-
puts of the three parser models: the LR parser, the
262
Language LR RL Rev2 Comb CombMST CoNLL
2007 Best
Czech 77.12 78.20 79.95 80.57 80.25 80.19
English 86.94 87.44 88.34 89.00 88.79 89.61
Italian 81.40 82.89 83.52 84.56 84.28 84.40
Table 2: LAS for selected CoNLL 2007 languages.
Language LR RL Rev2 Comb CombMST Conll-X
Best
MST MSTMalt Malt MaltMST
arabic 67.27 66.05 67.54 68.38 68.50 66.91 66.91 68.64 66.71 67.80
bulgarian 86.83 87.13 87.41 88.11 87.85 87.57 87.57 89.05 87.41 88.59
chinese 87.44 85.77 87.51 87.77 87.75 89.96 85.90 88.43 86.92 87.44
czech 79.84 79.46 81.78 82.22 82.22 80.18 80.18 82.26 78.42 81.18
danish 83.89 83.63 84.85 85.47 85.25 84.79 84.79 86.67 84.77 85.43
dutch 75.71 77.27 78.77 79.55 80.19 79.19 79.19 81.63 78.59 79.91
german 85.34 85.20 86.50 87.40 87.38 87.34 87.34 88.46 85.82 87.66
japanese 90.03 90.63 90.87 91.67 91.59 91.65 90.71 91.43 91.65 92.20
portuguese 86.84 87.00 87.86 88.14 88.20 87.60 86.82 87.50 87.60 88.64
slovene 73.64 74.40 75.32 75.72 75.48 73.44 73.44 75.94 70.30 74.24
spanish 81.63 81.61 81.85 83.33 83.13 82.25 82.25 83.99 81.29 82.41
swedish 82.95 81.62 82.91 83.69 83.69 84.58 82.55 84.66 84.58 84.31
turkish 64.91 61.92 63.33 65.27 65.23 65.68 63.19 64.29 65.58 66.28
Average 80.49 80.13 81.27 82.05 82.03 81.63 80.83 82.53 80.74 82.01
Table 3: Labeled attachment scores for CoNLL-X corpora.
RL parser and the Rev2 parser.
Instead of using a general algorithm for calcu-
lating the MST of a graph, we exploit the fact that
we are combining trees and hence we developed an
approximate algorithm that has O(kn) complexity,
where n is the number of nodes in a tree and k is the
number of trees being combined.
The algorithm builds the combined tree T incre-
mentally, starting from the empty tree. We will ar-
gue that an invariant of the algorithm is that the par-
tial result T is always a tree.
The algorithm exploits the notion of fringe F , i.e.
the set of arcs whose parent is in T and that can be
added to T without affecting the invariant. Initially
F consists of the roots of all trees to be combined.
The weight of each arc a in the fringe is the number
of parsers that predicted a.
At each step, the algorithm selects from F an arc
a = (h, d, r) among those with maximum weight,
having h ? T . Then it:
1. adds a to T
2. removes from F all arcs whose child is d
3. adds to F all arcs (h?, d?, r?) in the original trees
where h? ? T and d? /? T .
Step 3 guarantees that no cycles are present in T .
The final T is connected because each added node
is connected to a node in T . T is a local maximum
because if there were another tree with higher score
including arc (h, n, r), either it is present in T or its
weight is smaller than the weight for node (h?, n, r?)
in T , as chosen by the algorithm.
The algorithm hasO(kn) complexity. A sketch of
the proof can be given as follows. Step 3 guarantees
that the algorithm is iterated n times, where n is the
number of nodes in a component tree. Using appro-
priate data structures to represent the fringe F , in-
sert or delete operations take constant time. At each
iteration of the algorithm the maximum number of
removals from F (step 2) is constant and it is equal
to k, hence the overall cost is O(nk).
Table 2 shows the results for the three languages
from CoNLL 2007. With respect to the best results
at the CoNLL 2007 Shared Task, the linear parser
combination achieves the best LAS for Czech and
Italian, the second best for English.
The results for the CoNLL-X languages (Table 3)
show also improvements: the Rev2 parser is more
263
accurate than MST, except for Bulgarian, Dutch,
German, and Spanish, where the difference is within
1%, and it is often better than the MaltMST stacking.
The improvements of the Rev2 over the LR parser
range from 0.38% for Chinese to 3.84% for Dutch.
The column CombMST shows the results of com-
bining parsers using the Chu-Liu-Edmonds MST al-
gorithm and the same weighting scheme of Lin-
ear Combination algorithm. For most languages
the Linear Combination algorithm leads to a bet-
ter accuracy than the MST algorithm. The some-
what surprising result might be due indeed to the top
down processing of the algorithm: since the algo-
rithm chooses the best among the connections that
are higher in the parse tree, this leads to a prefer-
ence to long spanning links over shorter links even
if these contribute higher weights to the MST.
Finally, the run time of the linear combination al-
gorithm on the whole CoNLL-X test set is 11.2 sec,
while the MST combination requires 92.5 sec.
We also tested weights based on the accuracy
score of each parser for the POS of an arc head, but
this produced less accurate results.
4 Conclusions
We presented a method for improving the accuracy
of a dependency parser by using a parser that ana-
lyzes a sentence in reverse using hints from the trees
produced by a forward parser.
We also introduced a new linear algorithm to per-
form parser combination.
Experiments on the corpora of languages from
the CoNLL-X and the CoNLL 2007 shared tasks
show that reverse revision parsing improves the ac-
curacy over a transition-based dependency parser in
all the tested languages. Further improvements are
obtained by using a linear parser combination algo-
rithm on the outputs of three parsers: a LR parser, a
RL parser and a Rev2 parser.
The combination parser achieves accuracies that
are best or second best with respect to the results
of the CoNLL 2007 shared task. Since all the indi-
vidual parsers as well as the combination algorithm
is linear, the combined parser maintains an overall
linear computational time. On the languages from
the CoNLL-X shared task the combination parser
achieves often the best accuracy in ten out of thirteen
languages but falls short of the accuracy achieved
by integrating a graph-based with a transition based
parser.
We expect that further tuning of the method might
help reduce these differences.
References
G. Attardi, F. Dell?Orletta, M. Simi, A. Chanev and
M. Ciaramita. 2007. Multilingual Dependency Parsing
and Domain Adaptation using DeSR. In Proc. of the
CoNLL Shared Task Session of EMNLP-CoNLL 2007.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared
task on multilingual dependency parsing. In Proc. of
CoNLL, 149?164.
Y. J. Chu and T. H. Liu. 1965. On the shortest arbores-
cence of a directed graph. Science Sinica(14), 1396?
1400.
J. Edmonds. 1967. Optimum branchings. Journal of Re-
search of the National Bureau of Standards (71B),
233?240.
J. M. Eisner. 1996. Three new probabilistic models for
dependency parsing: An exploration. In Proc. of COL-
ING 1996, 340?345.
J. Hall, et al 2007. Single Malt or Blended? A Study
in Multilingual Parser Optimization. In Proc. of the
CoNLL Shared Task Session of EMNLP-CoNLL 2007.
R. McDonald and J. Nivre. 2007. Characterizing the Er-
rors of Data-Driven Dependency Parsing Models In
Proc. of EMNLP-CoNLL 2007.
R. McDonald, F. Pereira, K. Ribarov and J. Hajic?. 2005.
Non-projective Dependency Parsing using Spanning
Tree Algorithms. In Proc. of HLT-EMNLP 2005.
R. McDonald and F. Pereira. 2006. Online Learning
of Approximate Dependency Parsing Algorithms. In
Proc. of EACL 2006.
J. Nivre, et al 2007. The CoNLL 2007 Shared Task on
Dependency Parsing. In Proc. of the CoNLL Shared
Task Session of EMNLP/CoNLL-2007.
J. Nivre and R. McDonald. 2008. Integrating Graph-
Based and Transition-Based Dependency Parsers. In
Proc. of ACL 2008.
J. Nivre and M. Scholz. 2004. Deterministic Dependency
Parsing of English Text. In Proc. of COLING 2004.
K. Sagae and A. Lavie. 2006. Parser Combination by
Reparsing. In Proc. of HLT-NAACL 2006.
I. Titov and J. Henderson. 2007. Fast and Robust Multi-
lingual Dependency Parsing with a Generative Latent
Variable Model In Proc. of the CoNLL Shared Task
Session of EMNLP/CoNNL-2007.
H. Yamada and Y. Matsumoto. 2003. Statistical depen-
dency analysis using support vector machines. In Proc.
of the 8th IWPT. Nancy, France.
264
Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X),
pages 166?170, New York City, June 2006. c?2006 Association for Computational Linguistics
Experiments with a Multilanguage Non-Projective  
Dependency Parser 
 
Giuseppe Attardi 
Dipartimento di Informatica 
largo B. Pontecorvo, 3 
I-56127 Pisa, Italy 
attardi@di.unipi.it 
 
  
1 Introduction 
Parsing natural language is an essential step in 
several applications that involve document 
analysis, e.g. knowledge extraction, question 
answering, summarization, filtering. The best 
performing systems at the TREC Question 
Answering track employ parsing for analyzing 
sentences in order to identify the query focus, to 
extract relations and to disambiguate meanings of 
words. 
These are often demanding applications, which 
need to handle large collections and to provide 
results in a fraction of a second. Dependency 
parsers are promising for these applications since a 
dependency tree provides predicate-argument 
relations which are convenient for use in the later 
stages. Recently statistical dependency parsing 
techniques have been proposed which are 
deterministic and/or linear (Yamada and 
Matsumoto, 2003; Nivre and Scholz, 2004). These 
parsers are based on learning the correct sequence 
of Shift/Reduce actions used to construct the 
dependency tree. Learning is based on techniques 
like SVM (Vapnik 1998) or Memory Based 
Learning (Daelemans 2003), which provide high 
accuracy but are often computationally expensive. 
Kudo and Matsumoto (2002) report a two week 
learning time on a Japanese corpus of about 8000 
sentences with SVM. Using Maximum Entropy 
(Berger, et al 1996) classifiers I built a parser that 
achieves a throughput of over 200 sentences per 
second, with a small loss in accuracy of about 2-
3 %. 
The efficiency of Maximum Entropy classifiers 
seems to leave a large margin that can be exploited 
to regain accuracy by other means. I performed a 
series of experiments to determine whether 
increasing the number of features or combining 
several classifiers could allow regaining the best 
accuracy. An experiment cycle in our setting 
requires less than 15 minutes for a treebank of 
moderate size like the Portuguese treebank 
(Afonso et al, 2002) and this allows evaluating the 
effectiveness of adding/removing features that 
hopefully might apply also when using other 
learning techniques. 
I extended the Yamada-Matsumoto parser to 
handle labeled dependencies: I tried two 
approaches: using a single classifier to predict 
pairs of actions and labels and using two separate 
classifiers, one for actions and one for labels. 
Finally, I extended the repertoire of actions used 
by the parser, in order to handle non-projective 
relations. Tests on the PDT (B?hmov? et al, 2003) 
show that the added actions are sufficient to handle 
all cases of non-projectivity. However, since the 
cases of non-projectivity are quite rare in the 
corpus, the general learner is not supplied enough 
of them to learn how to classify them accurately, 
hence it may be worthwhile to exploit a second 
classifier trained specifically in handling non-
projective situations. 
1. Summary of the approach 
The overall parsing algorithm is an inductive 
statistical parser, which extends the approach by 
Yamada and Matsumoto (2003), by adding six new 
reduce actions for handling non-projective 
relations and also performs dependency labeling. 
Parsing is deterministic and proceeds bottom-up. 
Labeling is integrated within a single processing 
step. 
166
The parser is modular: it can use several 
learning algorithms: Maximum Entropy, SVM, 
Winnow, Voted Perceptron, Memory Based 
Learning, as well as combinations thereof. The 
submitted runs used Maximum Entropy and I 
present accuracy and performance comparisons 
with other learning algorithms. 
No additional resources are used. 
No pre-processing or post-processing is used, 
except stemming for Danish, German and Swedish. 
2 Features 
Columns from input data were used as follows. 
LEMMA was used in features whenever 
available, otherwise the FORM was used. For 
Danish, German and Swedish the Snowball 
stemmer (Porter 2001) was used to generate a 
value for LEMMA. This use of stemming slightly 
improved both accuracy and performance. 
Only CPOSTAG were used. PHEAD/PDEPREL 
were not used. 
FEATS were used to extract a single token 
combining gender, number, person and case, 
through a language specific algorithm. 
The selection of features to be used in the parser 
is controlled by a number of parameters. For ex-
ample, the parameter PosFeatures determines 
for which tokens the POS tag will be included in 
the context, PosLeftChildren determines how 
many left outermost children of a token to con-
sider, PastActions tells how many previous ac-
tions to include as features. 
The settings used in the submitted runs are listed 
below and configure the parser for not using any 
word forms. Positive numbers refer to input to-
kens, negative ones to token on the stack. 
LemmaFeatures         -2 -1 0 1 2 3 
PosFeatures           -2 -1 0 1 2 3 
MorphoFeatures        -1 0 1 2 
DepFeatures           -1 0 
PosLeftChildren       2 
PosLeftChild          -1 0 
DepLeftChild          -1 0 
PosRightChildren      2 
PosRightChild         -1 0 
DepRightChild         -1 
PastActions           1 
The context for POS tags consisted of 1 token left 
and 3 tokens to the right of the focus words, except 
for Czech and Chinese were 2 tokens to the left 
and 4 tokens to the right were used. These values 
were chosen by performing experiments on the 
training data, using 10% of the sentences as held-
out data for development. 
3 Inductive Deterministic Parsing 
The parser constructs dependency trees employing 
a deterministic bottom-up algorithm which per-
forms Shift/Reduce actions while analyzing input 
sentences in left-to-right order. 
Using a notation similar to (Nivre and Scholz, 
2003), the state of the parser is represented by a 
quadruple ?S, I, T, A?, where S is the stack, I is the 
list of (remaining) input tokens, T is a stack of 
temporary tokens and A is the arc relation for the 
dependency graph. 
Given an input string W, the parser is initialized 
to ?(), W, (), ()?, and terminates when it reaches a 
configuration ?S, (), (), A?. 
The parser by Yamada and Matsumoto (2003) 
used the following actions: 
Shift in a configuration ?S, n|I, T, A?, pushes 
n to the stack, producing the configura-
tion ?n|S, I, T, A?. 
Right1 in a configuration ?s1|S, n|I, T, A?, adds 
an arc from s1 to n and pops s1 from the 
stack, producing the configuration ?S, 
n|I, T, A?{(s1, r, n)}?. 
Left in a configuration ?s1|S, n|I, T, A?, adds 
an arc from n to s1, pops n from input, 
pops s1 from the stack and moves it 
back to I, producing the configuration 
?S, s1|I, T, A?{(n, r, s1)}?. 
At each step the parser uses classifiers trained on 
treebank data in order to predict which action to 
perform and which dependency label to assign 
given the current configuration. 
4 Non-Projective Relations 
For handling non-projective relations, Nivre and 
Nilsson (2005) suggested applying a pre-
processing step to a dependency parser, which con-
sists in lifting non-projective arcs to their head re-
peatedly, until the tree becomes pseudo-projective. 
A post-processing step is then required to restore 
the arcs to the proper heads. 
                                                          
1
 Nivre and Scholz reverse the direction, while I follow here 
the terminology in Yamada and Matsumoto (2003). 
167
I adopted a novel approach, which consists in 
adding six new parsing actions: 
Right2 in a configuration ?s1|s2|S, n|I, T, A?, 
adds an arc from s2 to n and removes s2 
from the stack, producing the configu-
ration ?s1|S, n|I, T, A?{(s2, r, n)}?. 
Left2 in a configuration ?s1|s2|S, n|I, T, A?, 
adds an arc from n to s2, pops n from 
input, pops s1 from the stack and moves 
it back to I, producing the configuration 
?s2|S, s1|I, T, A?{(n, r, s2)}?. 
Right3 in a configuration ?s1|s2|s3|S, n|I, T, A?, 
adds an arc from s3 to n and removes s3 
from the stack, producing the configu-
ration ?s1|s2|S, n|I, T, A?{(s3, r, n)}?. 
Left3 in a configuration ?s1|s2|s3|S, n|I, T, A?, 
adds an arc from n to s3, pops n from 
input, pops s1 from the stack and moves 
it back to I, producing the configuration 
?s2|s3|S, s1|I, T, A?{(n, r, s3)}?. 
Extract in a configuration ?s1|s2|S, n|I, T, A?, 
move s2 from the stack to the temporary 
stack, then Shift, producing the con-
figuration ?n|s1|S, I, s2|T, A?. 
Insert in a configuration ?S, I, s1|T, A?, pops s1 
from T and pushes it to the stack, pro-
ducing the configuration ?s1|S, I, T, A?. 
The actions Right2 and Left2 are sufficient to 
handle almost all cases of non-projectivity: for in-
stance the training data for Czech contain 28081 
non-projective relations, of which 26346 can be 
handled by Left2/Right2, 1683 by 
Left3/Right3 and just 52 require Ex-
tract/Insert. 
Here is an example of non-projectivity that can 
be handled with Right2 (nejen ? ale) and Left3 
(fax ? V?t?inu): 
V?t?inu t?chto p??stroj? lze take pou??vat nejen jako fax, 
ale sou?asn? ? 
 
The remaining cases are handled with the last two 
actions: Extract is used to postpone the creation 
of a link, by saving the token in a temporary stack; 
Insert restores the token from the temporary 
stack and resumes normal processing. 
 
This fragment in Dutch is dealt by performing an 
Extract in configuration ?moeten|gemaakt|zou, 
worden|in, A? followed immediately by an In-
sert, leading to the following configuration, 
which can be handled by normal Shift/Reduce 
actions: 
 
Another linguistic phenomenon is the anticipation 
of pronouns, like in this Portuguese fragment: 
Tudo ? possivel encontrar em o IX 
Sal?o de Antiguidades, desde objectos 
de ouro e prata, moedas, ? 
The problem here is due to the pronoun Tudo 
(Anything), which is the object of encontrar 
(find), but which is also the head of desde (from) 
and its preceding comma. In order to be able to 
properly link desde to Tudo, it is necessary to 
postpone its processing; hence it is saved with Ex-
tract to the temporary stack and put back later in 
front of the comma with Insert. In fact the pair 
Extract/Insert behaves like a generalized 
Rightn/Leftn, when n is not known. As in the 
example, except for the case where n=2, it is diffi-
cult to predict the value of n, since there can be an 
arbitrary long sequence of tokens before reaching 
the position where the link can be inserted. 
5 Performance 
I used my own C++ implementation of Maximum 
Entropy, which is very fast both in learning and 
classification. On a 2.8 MHz Pentium Xeon PC, 
the learning time is about 15 minutes for Portu-
guese and 4 hours for Czech. Parsing is also very 
fast, with an average throughput of 200 sentences 
per second: Table 1 reports parse time for parsing 
each whole test set. Using Memory Based Learn-
ing increases considerably the parsing time, while 
as expected learning time is quite shorter. On the 
other hand MBL achieves an improvement up to 
5% in accuracy, as shown in detail in Table 1. 
zou moeten worden gemaakt in 
zou gemaakt moeten worden in 
V?t?inu t?chto p??stroj? lze take pou??vat nejen jako fax  ,  ale 
168
Language Maximum Entropy MBL 
LAS 
% 
Cor-
rected 
LAS 
UAS 
% 
LA 
% 
Train 
time 
sec 
Parse 
time 
sec 
LAS 
% 
UAS 
% 
LA 
% 
Train 
time 
sec 
Parse 
time 
sec 
Arabic 53.81 54.15 69.50 72.97 181 2.6 59.70 74.69 75.49 24 950 
Bulgarian 72.89 72.90 85.24 77.68 452 1.5 79.17 85.92 83.22 88 353 
Chinese 54.89 70.00 81.33 58.75 1156 1.8 72.17 83.08 75.55 540 478 
Czech 59.76 62.10 73.44 69.84 13800 12.8 69.20 80.22 77.72 496 13500 
Danish 66.35 71.72 78.84 74.65 386 3.2 76.13 83.65 82.06 52 627 
Dutch 58.24 63.71 68.93 66.47 679 3.3 68.97 74.73 75.93 132 923 
German 69.77 75.88 80.25 78.39 9315 4.3 79.79 84.31 86.88 1399 3756 
Japanese 65.38 78.01 82.05 73.68 129 0.8 83.39 86.73 89.95 44 97 
Portuguese 75.36 79.40 85.03 80.79 1044 4.9 80.97 86.78 85.27 160 670 
Slovene 57.19 60.63 72.14 69.36 98 3.0 62.67 76.60 72.72 16 547 
Spanish 67.44 70.33 74.25 82.19 204 2.4 74.37 79.70 85.23 54 769 
Swedish 68.77 75.20 83.03 72.42 1424 2.9 74.85 83.73 77.81 96 1177 
Turkish 37.80 48.83 65.25 49.81 177 2.3 47.58 65.25 59.65 43 727 
Table 1. Results for the CoNLL-X Shared task (official values in italics). 
For details on the CoNLL-X shared task and the 
measurements see (Buchholz, et al 2006). 
6 Experiments 
I performed several experiments to tune the parser. 
I also tried alternative machine learning algo-
rithms, including SVM, Winnow, Voted Percep-
tron. 
The use of SVM turned out quite impractical 
since the technique does not scale to the size of 
training data involved: training an SVM with such 
a large number of features was impossible for any 
of the larger corpora. For smaller ones, e.g. Portu-
guese, training required over 4 days but produced a 
bad model which could not be used (I tried both 
the TinySVM (Kudo 2002) and the LIBSVM 
(Chang and Lin 2001) implementations). 
Given the speed of the Maximum Entropy clas-
sifier, I explored whether increasing the number of 
features could improve accuracy. I experimented 
adding various features controlled by the parame-
ters above: none appeared to be effective, except 
the addition of the previous action. 
The classifier returns both the action and the la-
bel to be assigned. Some experiments were carried 
out splitting the task among several specialized 
classifiers. I experimented with: 
1. three classifiers: one to decide between 
Shift/Reduce, one to decide which Reduce 
action and a third one to choose the depend-
ency in case of Left/Right action 
2. two classifiers: one to decide which action to 
perform and a second one to choose the de-
pendency in case of Left/Right action 
None of these variants produced improvements in 
precision. Only a small improvement in labeled 
attachment score was noticed using the full, non-
specialized classifier to decide the action but dis-
carding its suggestion for label and using a special-
ized classifier for labeling. However this was 
combined with a slight decrease in unlabeled at-
tachment score, hence it was not considered worth 
the effort. 
7 Error Analysis 
The parser does not attempt to assign a dependency 
relation to the root. A simple correction of assign-
ing a default value for each language gave an im-
provement in the LAS as shown in Table 1. 
7.1 Portuguese 
Out of the 45 dependency relations that the parser 
had to assign to a sentence, the largest number of 
169
errors occurred assigning N<PRED (62), ACC (46), 
PIV (43), CJT (40), N< (34), P< (30). 
The highest number of head error occurred at 
the CPOS tags PRP with 193 and V with 176. In 
particular just four prepositions (em, de, a, para) 
accounted for 120 head errors. 
Most of the errors occur near punctuations. Of-
ten this is due to the fact that commas introduce 
relative phrases or parenthetical phrases (e.g. ?o 
suspeito, de 38 anos, que trabalha?), 
that produce diversions in the flow. Since the 
parser makes decisions analyzing only a window 
of tokens of a limited size, it gets confused in cre-
ating attachments. I tried to add some global con-
text features, to be able to distinguish these cases, 
in particular, a count of the number of punctuation 
marks seen so far, whether punctuation is present 
between the focus words. None of them helped 
improving precision and were not used in the sub-
mitted runs. 
7.2 Czech 
Most current parsers for Czech do not perform well 
on Apos (apposition), Coord (coordination) and 
ExD (ellipses), but they are not very frequent. The 
largest number of errors occur on Obj (166), Adv 
(155), Sb (113), Atr (98). There is also often con-
fusion among these: 33 times Obj instead of Adv, 
32 Sb instead of Obj, 28 Atr instead of Adv. 
The high error rate of J (adjective) is expected, 
mainly due to coordination problems. The error of 
R (preposition) is also relatively high. Prepositions 
are problematic, but their error rate is higher than 
expected since they are, in terms of surface order, 
rather regular and close to the noun. It could be 
that the decision by the PDT to hang them as heads 
instead of children, causes a problem in attaching 
them. It seems that a post-processing may correct a 
significant portion of these errors. 
The labels ending with _Co, _Ap or _Pa are 
nodes who are members of the Coordination, Ap-
position or the Parenthetical relation, so it may be 
worth while omitting these suffixes in learning and 
restore them by post-processing. 
An experiment using as training corpus a subset 
consisting of just sentences which include non-
projective relations achieved a LAS of 65.28 % 
and UAS of 76.20 %, using MBL. 
Acknowledgments. Kiril Ribarov provided in-
sightful comments on the results for Czech.  
The following treebanks were used for training the 
parser: (Afonso et al, 2002; Atalay et al, 2003; 
B?hmov? et al, 2003; Brants et al, 2002; Chen et 
al., 2003; Civit Torruella and Mart? Anton?n, 2002; 
D?eroski et al, 2006; Haji? et al, 2004; Kawata 
and Bartels, 2000; Kromann, 2003; Nilsson et al, 
2005; Oflazer et al, 2003; Simov et al, 2005; van 
der Beek et al, 2002). 
References 
A. Berger, S. Della Pietra, and M. Della Pietra. 1996. A 
Maximum Entropy Approach to Natural Language 
Processing. Computational Linguistics, 22(1). 
S. Buchholz, et al 2006. CoNLL-X Shared Task on 
Multilingual Dependency Parsing. In Proc. of the 
Tenth CoNLL. 
C.-C. Chang, C.-J. Lin. 2001.  LIBSVM: a library for 
support vector machines.  
http://www.csie.ntu.edu.tw/~cjlin/libsvm/ 
W. Daelemans, J. Zavrel, K. van der Sloot, and A. van 
den Bosch. 2003. Timbl: Tilburg memory based 
learner, version 5.0, reference guide. Technical Re-
port ILK 03-10, Tilburg University, ILK. 
T. Kudo. 2002. tinySVM.  
http://www.chasen.org/~taku/software/TinySVM/ 
T. Kudo, Y. Matsumoto. 2002. Japanese Dependency 
Analysis using Cascaded Chunking. In Proc. of the 
Sixth CoNLL. 
R. McDonald, et al 2005. Non-projective Dependency 
Parsing using Spanning Tree Algorithms. In Proc. of 
HLT-EMNLP. 
J. Nivre, et al 2004. Memory-based Dependency Pars-
ing. In Proc.s of the Eighth CoNLL, ed. H. T. Ng and 
E. Riloff, Boston, Massachusetts, pp. 49?56. 
J. Nivre and M. Scholz. 2004. Deterministic Depend-
ency Parsing of English Text. In Proc. of COLING 
2004, Geneva, Switzerland, pp. 64?70. 
J. Nivre and J. Nilsson, 2005. Pseudo-Projective De-
pendency Parsing. In Proc. of the 43rd Annual Meet-
ing of the ACL, pp. 99-106. 
M.F. Porter. 2001. Snowball Stemmer.  
http://www.snowball.tartarus.org/ 
V. N. Vapnik. 1998. The Statistical Learning Theory. 
Springer. 
H. Yamada and Y. Matsumoto. 2003. Statistical De-
pendency Analysis with Support Vector Machines. In 
Proc. of the 8th International Workshop on Parsing 
Technologies (IWPT), pp. 195?206. 
170
Proceedings of the 10th Conference on Parsing Technologies, pages 133?143,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Dependency Parsing with Second-Order Feature Maps and Annotated
Semantic Information
Massimiliano Ciaramita
Yahoo! Research
Ocata 1, S-08003
Barcelona, Spain
massi@yahoo-inc.com
Giuseppe Attardi
Dipartimento di Informatica
Universita` di Pisa
L. B. Pontecorvo 3, I-56127
Pisa, Italy
attardi@di.unipi.it
Abstract
This paper investigates new design options
for the feature space of a dependency parser.
We focus on one of the simplest and most
efficient architectures, based on a determin-
istic shift-reduce algorithm, trained with the
perceptron. By adopting second-order fea-
ture maps, the primal form of the perceptron
produces models with comparable accuracy
to more complex architectures, with no need
for approximations. Further gains in accu-
racy are obtained by designing features for
parsing extracted from semantic annotations
generated by a tagger. We provide experi-
mental evaluations on the Penn Treebank.
1 Introduction
A dependency tree represents a sentence as a labeled
directed graph encoding syntactic and semantic in-
formation. The labels on the arcs can represent ba-
sic grammatical relations such as ?subject? and ?ob-
ject?. Dependency trees capture grammatical struc-
tures that can be useful in several language process-
ing tasks such as information extraction (Culotta &
Sorensen, 2004) and machine translation (Ding &
Palmer, 2005). Dependency treebanks are becoming
available in many languages, and several approaches
to dependency parsing on multiple languages have
been evaluated in the CoNLL 2006 and 2007 shared
tasks (Buchholz & Marsi, 2006; Nivre et al, 2007).
Dependency parsing is simpler than constituency
parsing, since dependency trees do not have extra
non-terminal nodes and there is no need for a gram-
mar to generate them. Approaches to dependency
parsing either generate such trees by considering all
possible spanning trees (McDonald et al, 2005), or
build a single tree by means of shift-reduce parsing
actions (Yamada & Matsumoto, 2003). Determinis-
tic dependency parsers which run in linear time have
also been developed (Nivre & Scholz, 2004; Attardi,
2006). These parsers process the sentence sequen-
tially, hence their efficiency makes them suitable for
processing large amounts of text, as required, for ex-
ample, in information retrieval applications.
Recent work on dependency parsing has high-
lighted the benefits of using rich feature sets
and high-order modeling. Yamada and Mat-
sumoto (2003) showed that learning an SVM model
in the dual space with higher-degree polynomial ker-
nel functions improves significantly the parser?s ac-
curacy. McDonald and Pereira (2006) have shown
that incorporating second order features relating to
adjacent edge pairs improves the accuracy of max-
imum spanning tree parsers (MST). In the SVM-
based approach, if the training data is large, it is not
feasible to train a single model. Rather, Yamada and
Matsumoto (see also (Hall et al, 2006)) partition the
training data in different sets, on the basis of Part-
of-Speech, then train one dual SVM model per set.
While this approach simplifies the learning task it
makes the parser more sensitive to the error rate of
the POS tagger. The second-order MST algorithm
has cubic time complexity. For non-projective lan-
guages the algorithm is NP-hard and McDonald and
Pereira (2006) introduce an approximate algorithm
to handle such cases.
In this paper we extend shift reduce parsing with
second-order feature maps which explicitly repre-
133
sent all feature pairs. Also the augmented fea-
ture sets impose additional computational costs.
However, excellent efficiency/accuracy trade-off is
achieved by using the perceptron algorithm, with-
out the need to resort to approximations, producing
high-accuracy classifiers based on a single model.
We also evaluate a novel set of features for pars-
ing. Recently various forms of shallow semantic
processing have been investigated such as named-
entity recognition (NER), semantic role labeling
(SRL) and relation extraction. Syntactic parsing can
provide useful features for these tasks; e.g., Pun-
yakanok et al (2005) show that full parsing is effec-
tive for semantic role labeling (see also related ap-
proaches evaluated within the CoNNL 2005 shared
task (Carreras et al, 2005)). However, no evidence
has been provided so far that annotated semantic
information can be leveraged for improving parser
performance. We report experiments showing that
adding features extracted by an entity tagger im-
proves the accuracy of a dependency parser.
2 Dependency parsing
A dependency parser takes as input a sentence s and
returns a dependency graph d. Figure 1 shows a de-
pendency tree for the sentence ?Last week CBS Inc.
canceled ?The People Next Door?.?1. Dependencies
are represented as labeled arrows from the head of
the relation to the modifier word; thus, in the exam-
ple, ?Inc.? is the modifier of a dependency labeled
?SUB? (subject) to the main verb, the head, ?can-
celed?.
In statistical syntactic parsing a generator (e.g.,
a PCFG) is used to produce a number of candi-
date trees (Collins, 2000) with associated proba-
bility scores. This approach has been used also
for dependency parsing, generating spanning trees
as candidates and computing the maximum span-
ning tree (MST) using discriminative learning algo-
rithms (McDonald et al, 2005). Second-order MST
dependency parsers currently represent the state of
the art in terms of accuracy. Yamada and Mat-
sumoto (2003) proposed a deterministic classifier-
based parser. Instead of learning directly which
tree to assign to a sentence, the parser learns which
1The figure also contains entity annotations which will be
explained below in Section 4.1.
Shift/Reduce actions to use in building the tree. Pars-
ing is cast as a classification problem: at each step
the parser applies a classifier to the features rep-
resenting its current state to predict which action
to perform on the tree. Similar deterministic ap-
proaches to parsing have been investigated also in
the context of constituent parsing (Wong & Wu,
1999; Kalt, 2004).
Nivre and Scholz (2004) proposed a variant of the
model of Yamada and Matsumoto that reduces the
complexity, from the worst case quadratic to linear.
Attardi (2006) proposed a variant of the rules that
handle non-projective relations while parsing deter-
ministically in a single pass. Shift-reduce algorithms
are simple and efficient, yet competitive in terms
of accuracy: in the CoNLL-X shared task, for sev-
eral languages, there was no statistically significant
difference between second-order MST parsers and
shift-reduce parsers.
3 A shift-reduce parser
We build upon DeSR, the shift-reduce parser de-
scribed in (Attardi, 2006). This and Nivre and
Scholz?s (2004) provide among the simplest and
most efficient methods. This parser constructs de-
pendency trees by scanning input sentences in a
single left-to-right pass and performing shift/reduce
parsing actions. The parsing algorithm is fully de-
terministic and has linear complexity. The parser?s
behavior can be described as repeatedly selecting
and applying a parsing rule to transform its state,
while advancing through the sentence. Each to-
ken is analyzed once and a decision is made lo-
cally concerning the action to take, that is, without
considering global properties of the tree being built.
Nivre (2004) investigated the issue of (strict) incre-
mentality for this type of parsers; i.e., if at any point
of the analysis the processed input forms one con-
nected structure. Nivre found that strict incremen-
tality is not guaranteed within this parsing frame-
work, although for correctly parsed trees the prop-
erty holds in almost 90% of the cases.
3.1 Parsing algorithm
The state of the parser is represented by a triple
?S, I,A?, where S is the stack, I is the list of input
tokens that remain to be processed and A is the arc
134
Figure 1. A dependency tree from the Penn Treebank, with additional entity annotation from the BBN corpus.
relation for the dependency graph, which consists of
a set of labeled arcs (wi, r, wj), where wi, wj ? W
(the set of tokens), d ? D (the set of dependencies).
Given an input sentence s, the parser is initialized
to ??, s, ??, and terminates at configuration ?s, ?, A?.
There are three parsing schemata:
Shift ?S,n|I,A??n|S,I,A?(1)
Rightr
?s|S,n|I,A?
?S,n|I,A?{(s,r,n)}?(2)
Leftr
?s|S,n|I,A?
?S,s|I,A?{(n,r,s)}?(3)
The Shift rule advances on the input; each Leftr and
Rightr rule creates a link r between the next input
token n and the top token on the stack s. For produc-
ing labeled dependencies the rules Leftr and Rightr
are instantiated several times once for each depen-
dency label.
Additional parsing actions (cf. (Attardi, 2006))
have been introduced for handling non-projective
dependency trees: i.e., trees that cannot be drawn
in the plane without crossing edges. However, they
are not needed in the experiments reported here,
because in the Penn Treebank used in our experi-
ments dependencies are extracted without consider-
ing empty nodes and the resulting trees are all pro-
jective2.
The pseudo code in Algorithm 1 reproduces
schematically the parsing process.
The function getContext() extracts a vector of
features x relative to the structure built up to that
point from the context of the current token, i.e., from
a subset of I , S and A. The step estimateAction()
predicts a parsing action y, given a trained model ?
2Instead, the version of the Penn Treebank used for the
CoNLL 2007 shared task includes also non-projective represen-
tations.
Algorithm 1: DeSR: Dependency Shift Reduce
parser.
input: s = w1, w2, ..., wn
begin
S ? ??
I ? ?w1, w2, ..., wn?
A? ??
while I 6= ?? do
x? getContext(S, I,A)
y ? estimateAction(x, ?)
performAction(y, S, I, A)
end
and x. The final step performAction() updates the
state according to the predicted parsing rule.
3.2 Features
The set of features used in this paper were chosen
with a few simple experiments on the development
data as a variant of a generic model. The only fea-
tures of the tokens used are ?Lemma?, ?Pos? and
?Dep?: ?Lemma? refers to the morphologically sim-
plified form of the token, ?Pos? is the Part-of-Speech
and ?Dep? is the label on a dependency. ?Child?
refers to the child of a node (right or left): up to
two furthest children of a node are considered. Ta-
ble 1 lists which feature is extracted for which to-
ken: negative numbers refer to tokens on the stack,
positive numbers refer to input tokens. As an exam-
ple, POS(-1) is the Part-of-Speech of the token on
the top of the stack, while Lemma(0) is the lemma
of the next token in the input, PosLeftChild(-1) ex-
tracts the Part-of-Speech of the leftmost child of the
token on the top of the stack, etc.
135
TOKEN
FEATURES Stack Input
Lemma -2 -1 0 1 2 3
Pos -2 -1 0 1 2 3
LemmaLeftChild -1 0
PosLeftChild -1 0
DepLeftChild -1 0
LemmaRightChild -1 0
PosRightChild -1 0
DepRightChild -1
LemmaPrev 0
PosSucc -1
Table 1. Configuration of the feature parameters used in
the experiments.
3.3 Learning a parsing model with the
perceptron
The problem of learning a parsing model can be
framed as a classification task where each class
yi ? Y represents one of k possible parsing actions.
Each of such actions is associated with a weight vec-
tor ?k ? IR
d. Given a datapoint x ? X , a d-
dimensional vector of binary features in the input
space X , a parsing action is chosen with a winner-
take-all discriminant function:
estimateAction(x, ?) = argmax
k
f(x, ?k) (4)
when using a linear classifier, such as the perceptron
or SVM, f(u,v) = ?u,v? is the inner product be-
tween vectors u and v.
We learn the parameters ? from the training data
with the perceptron (Rosemblatt, 1958), in the on-
line multiclass formulation of the algorithm (Cram-
mer & Singer, 2003) with uniform negative updates.
The perceptron has been used in previous work on
dependency parsing by Carreras et al (2006), with
a parser based on Eisner?s algorithm (Eisner, 2000),
and also on incremental constituent parsing (Collins
& Roark, 2006). Also the MST parser of McDonald
uses a variant of the perceptron algorithm (McDon-
ald, 2006). The choice is motivated by the simplicity
and performance of perceptrons, which have proved
competitive on a number of tasks; e.g., in shallow
parsing, where perceptron?s performance is com-
parable to that of Conditional Random Field mod-
els (Sha & Pereira, 2003).
The only adjustable parameter of the model is the
number of instances T to use for training. We fixed
T using the development portion of the data. In
our experiments, the best value is between 20 and
30 times the size of the training data. To regularize
the model we take as the final model the average of
all weight vectors posited during training (Collins,
2002). Algorithm 2 illustrates the perceptron learn-
ing procedure. The final average model can be com-
puted efficiently during training without storing the
individual ? vectors (e.g., see (Ciaramita & Johnson,
2003)).
Algorithm 2: Average multiclass perceptron
input : S = (xi, yi)N ;?0k = ~0, ?k ? Y
for t = 1 to T do
choose j
Et = {r ? Y : ?xj , ?tr? ? ?xj , ?
t
yj ?}
if |Et| > 0 then
?t+1r = ?
t
r ?
xj
|Et| , ?r ? E
t
?t+1yj = ?
t
yj + xj
output: ?k = 1T
?
t ?
t
k, ?k ? Y
3.4 Higher-order feature spaces
Yamada and Matsumoto (2003) and McDonald and
Pereira (2006) have shown that higher-order fea-
ture representations and modeling can improve pars-
ing accuracy, although at significant computational
costs. To make SVM training feasible in the dual
model with polynomial kernels, Yamada and Mat-
sumoto split the training data into several sets, based
on POS tags, and train a parsing model for each
set. McDonald and Pereira?s second-order MST
parser has O(n3) complexity, while for handling
non-projective trees, otherwise an NP-hard problem,
the parser resorts to an approximate algorithm. Here
we discuss how the feature representation can be
enriched to improve parsing while maintaining the
simplicity of the shift-reduce architecture, and per-
forming discriminative learning without partitioning
the training data.
The linear classifier (see Equation 4) learned with
the perceptron is inherently limited in the types of
solutions it can learn. As originally pointed out by
Minsky and Papert (1969), there are problems which
require non-linear solutions that cannot be learned
by such models. A simple workaround this limi-
tation relies on feature maps ? : IRd ? IRh that
136
map the input vectors x ? X into some higher h-
dimensional representation ?(X ) ? IRh, the fea-
ture space. The feature space can represent, for ex-
ample, all combinations of individual features in the
input space. We define a feature map which ex-
tracts all second order features of the form xixj ;
i.e., ?(x) = (xi, xj |i = 1, ..., d, j = i, ..., d). The
linear perceptron working in ?(X ) effectively im-
plements a non-linear classifier in the original in-
put space X . One shortcoming of this approach is
that it inflates considerably the feature representa-
tion and might not scale. In general, the number of
features of degree g over an input space of dimen-
sion d is
(d+g?1
g
)
. In practice, a second-order fea-
ture map can be handled with reasonable efficiency
by the perceptron. We call this the 2nd-order model,
which uses a modified scoring function:
g(x, ?k) = f(?(x), ?k) (5)
where also ?k is h-dimensional. The proposed fea-
ture map is equivalent to a polynomial kernel func-
tion of degree two. Yamada and Matsumoto (2003)
have shown that the degree two polynomial ker-
nel has superior accuracy than the linear model and
polynomial kernels of higher degrees. However, us-
ing the dual model is not always practical for depen-
dency parsing. The discriminant function of the dual
model is defined as:
f ?(x, ?) = argmax
k
N?
i=1
?k,i?x,xi?
g (6)
where the weights ? are associated with class-
instance pairs rather than class-feature pairs. With
respect to the discriminant function of equation (4)
there is an additional summation. In principle, the
inner products can be cached in a Kernel matrix to
speed up training.
There are two shortcomings to using such a model
in dependency parsing. First, if the amount of train-
ing data is large it might not be feasible to store the
Kernel matrix; which for a dataset of sizeN requires
O(N3) computations and O(N2) space. As an ex-
ample, the number of training instances N in the
Penn Treebank is over 1.8 million, caching the Ker-
nel matrix would require several Terabytes of space.
The second shortcoming is independent of training.
In predicting a tree for unseen sentences the model
will have to recompute the inner products between
the observation and all the support vectors; i.e., all
class-instance pairs with ?k,i > 0. The second-order
feature map with the perceptron is more efficient and
allows faster training and prediction. Training a sin-
gle parsing model avoids a potential loss of accuracy
that occurs when using the technique of partitioning
the training data according to the POS. Inaccurate
predictions of the POS can affect significantly the
accuracy of the actions predicted, while the single
model is more robust, since the POS is just one of
the many features used in prediction.
4 Semantic features
Semantic information is used implicitly in parsing.
For example, conditioning on lexical heads pro-
vides a source of semantic information. There have
been a few attempts at using semantic information
more explicitly. Charniak?s 1997 parser (1997), de-
fined probability estimates backed off to word clus-
ters. Collins and Koo (Collins & Koo, 2005) in-
troduced an improved reranking model for parsing
which includes a hidden layer of semantic features.
Yi and Palmer (2005) retrained a constituent parser
in which phrases were annotated with argument in-
formation to improve SRL, however this didn?t im-
prove over the output of the basic parser.
In recent years there has been a significant
amount of work on semantic annotation tasks such
as named-entity recognition, semantic role labeling
and relation extraction. There is evidence that de-
pendency and constituent parsing can be helpful in
these and other tasks; e.g., by means of tree ker-
nels in question classification and semantic role la-
beling (Zhang & Lee, 2003; Moschitti, 2006).
It is natural to ask if also the opposite holds:
whether semantic annotations can be used to im-
prove parsing. In particular, it would be interesting
to know if entity-like tags can be used for this pur-
pose. One reason for this is that entity tagging is ef-
ficient and does not seem to need parsing for achiev-
ing top performance. Beyond improving traditional
parsing, independently learned semantic tags might
be helpful in adapting a parser to a new domain. To
the best of our knowledge, no evidence has been pro-
duced yet that annotated semantic information can
improve parsing. In the following we investigate
137
adding entity tags as features of our parser.
4.1 BBN Entity corpus
The BBN corpus (BBN, 2005) supplements the Wall
Street Journal Penn Treebank with annotation of a
large set of entity types. The corpus includes an-
notation of 12 named entity types (Person, Facility,
Organization, GPE, Location, Nationality, Product,
Event, Work of Art, Law, Language, and Contact-
Info), nine nominal entity types (Person, Facility,
Organization, GPE, Product, Plant, Animal, Sub-
stance, Disease and Game), and seven numeric types
(Date, Time, Percent, Money, Quantity, Ordinal and
Cardinal). Several of these types are further divided
into subtypes3. This corpus provides adequate sup-
port for experimenting semantic features for parsing.
Figure 1 illustrates the annotation layer provided
by the BBN corpus4. It is interesting to notice one
apparent property of the combination of semantic
tags and dependencies. When we consider segments
composed of several words there is exactly one de-
pendency connecting a token outside the segment
with a token inside the segment; e.g., ?CBS Inc.? is
connected outside only through the token ?Inc.?, the
subject of the main verb. With respect to the rest of
the tree, segments tend to form units, with their own
internal structure. Intuitively, this information seems
relevant for parsing. This locally-structured patterns
could help particularly simple algorithms like ours,
which have limited knowledge of the global struc-
ture being built.
Table 2 lists the 40 most frequent categories in
sections 2 to 21 of the BBN corpus, and the per-
centage of all entities they represent ? together more
than 97%. Sections 2-21 are comprised of 949,853
tokens, 23.5% of the tokens have a non-null BBN
entity tag, on average there is one tagged token every
four. The total number of entities is 139,029, 70.5%
of which are named entities and nominal concepts,
17% are numerical types and the remaining 12.5%
describe time entities.
We designed three new features which extract
simple properties of entities from the semantic an-
notation information:
3BBN Corpus documentation.
4The full label for ?ORG? is ?ORG:Corporation?, and
?WOA? stands for ?WorkOfArt:Other?.
TOKEN
FEATURES Stack Input
AS-0 = EOS+BIO+TAG 0
AS-1 = EOS+BIO+TAG -1 0 1
AS-2 = EOS+BIO+TAG -2 -1 0 1 2
EOS -2 -1 0 1 2
BIO -2 -1 0 1 2
TAG -2 -1 0 1 2
Table 3. Additional configurations for the models with
BBN entity features.
? EOS: Distance to the end of the segment; e.g.,
EOS(?Last?) = 1, EOS(?canceled?) = 0;
? BIO: The first character of the BBN label
for a token; e.g., BIO(?CBS?) = ?B?, and
BIO(?canceled?) = 0;
? TAG: Full BBN tag for the token; e.g.,
TAG(?CBS?) = ?B-ORG:Corporation?,
TAG(?week?) = ?I-DATE?.
The feature EOS provides information about the rel-
ative position of the token within a segment with re-
spect to the end of the segment. The feature BIO dis-
criminates tokens with no semantic annotation as-
sociated, from tokens within a segment and token
which start a segment. Finally the feature TAG iden-
tifies the full semantic tag associated with the token.
With respect to the former two features this bears
the most fine-grained semantics. Table 3 summa-
rizes six additional models we implemented. The
first three use all additional features together, ap-
plied to different sets of tokens, while the last three
apply only one feature, on top of the base model,
relative to the next token in the input, the following
two tokens in the input, and the previous two tokens
on the stack.
4.2 Corpus pre-processing
The original BBN corpus has its own tokeniza-
tion which often does not reflect the Penn Tree-
bank tokenization; e.g., when an entity intersects
an hyphenated compound, thus ?third-highest? be-
comes ?thirdORDINAL - highest?. This is problem-
atic for combining entity annotation and dependency
trees. Since our main focus is parsing we re-aligned
the BBN Corpus with the Treebank tokenization.
Thus, for example, when an entity splits a Tree-
bank token we extend the entity boundary to contain
138
WSJ-BBN Corpus Categories
Tag % Tag % Tag % Tag %
PER DESC 15.5 ORG:CORP 13.7 DATE:DATE 9.2 ORG DESC:CORP 8.9
PERSON 8.13 MONEY 6.5 CARDINAL 6.0 PERCENT 3.5
GPE:CITY 3.12 GPE:COUNTRY 2.9 ORG:GOV 2.6 NORP:NATION-TY 1.9
DATE:DURATION 1.8 GPE:PROVINCE 1.5 ORG DESC:GOV 1.4 FAC DESC:BLDG 1.1
ORG:OTHER 0.7 PROD DESC:VEHICLE 0.7 ORG DESC:OTHER 0.6 ORDINAL 0.6
TIME 0.5 GPE DESC:COUNTRY 0.5 SUBST:OTHER 0.5 SUBST:FOOD 0.5
DATE:OTHER 0.4 NORP:POLITICAL 0.4 DATE:AGE 0.4 LOC:REGION 0.3
SUBST:CHEM 0.3 WOA:OTHER 0.3 FAC DESC:OTHER 0.3 SUBST:DRUG 0.3
ANIMAL 0.3 GPE DESC:PROVINCE 0.2 PROD:VEHICLE 0.2 GPE DESC:CITY 0.2
PRODUCT:OTHER 0.2 LAW 0.2 ORG:POLITICAL 0.2 ORG:EDU 0.2
Table 2. The 40 most frequent labels in sections 2 to 21 of the Wall Street Journal BBN Corpus and the percentage of
tags occurrences.
the whole original Treebank token, thus obtaining
?third-highestORDINAL? in the example above.
4.3 Semantic tagger
We treated semantic tags as POS tags. A tagger
was trained on the BBN gold standard annotation
and used it to annotate development and evaluation
data. We briefly describe the tagger (see (Ciaramita
& Altun, 2006) for more details), a Hidden Markov
Model trained with the perceptron algorithm intro-
duced in (Collins, 2002). The tagger uses Viterbi
decoding. Label to label dependencies are limited to
the previous tag (first order HMM). A generic fea-
ture set for NER based on words, lemmas, POS tags,
and word shape features was used.
The tagger is trained on sections 2-21 of the BBN
corpus. As before, section 22 of the BBN corpus
is used for choosing the perceptron?s parameter T .
The tagger?s model is regularized as described for
Algorithm 2. The full BBN tagset is comprised
of 105 classes organized hierarchically, we ignored
the hierarchical organization and treated each tag as
an independent class in the standard BIO encoding.
The tagger evaluated on section 23 achieves an F-
score of 86.8%. The part of speech for the evalua-
tion/development sections was produced with Tree-
Tagger. As a final remark we notice that the tagger?s
complexity, linear in the length of the sentence, pre-
serves the parser?s complexity.
5 Parsing experiments
5.1 Data and setup
We used the standard partitions of the Wall Street
Journal Penn Treebank (Marcus et al, 1993); i.e.,
sections 2-21 for training, section 22 for develop-
ment and section 23 for evaluation. The constituent
trees were transformed into dependency trees by
means of a program created by Joakim Nivre that
implements the rules proposed by Yamada and Mat-
sumoto, which in turn are based on the head rules
of Collins? parser (Collins, 1999)5. The lemma for
each token was produced using the ?morph? func-
tion of the WordNet (Fellbaum, 1998) library6. The
data in the WSJ sections 22 and 23, both for the
parser and for the semantic tagger, was POS-tagged
using TreeTagger7, which has an accuracy of 97.0%
on section 23.
Training a parsing model on the Wall Street Jour-
nal requires a set of 22 classes: 10 of the 11 labels
in the dependency corpus generated from the Penn
Treebank (e.g., subj, obj, sbar, vmod, nmod, root,
etc.) are paired with both a Left and Right actions.
In addition, there is in one rule for the ?root? label
and one for the Shift action. The total number of
features found in training ranges from two hundred
thousand for the 1st-order model to approximately
20 million of the 2nd-order models.
We evaluated several models, each trained with
1st-order and 2nd-order features. The base model
(BASE) only uses the traditional set of features (cf.
Table 1). Models EOS, BIO and TAG each use only
one type of semantic feature with the configuration
described in Table 3. Models AS-0, AS-1, and AS-2
use all three semantic features for the token on the
stack in AS-0, plus the previous token on the stack
and the new token in the input in AS-1, plus an addi-
5The script is available from
http://w3.msi.vxu.se/%7enivre/research/Penn2Malt.html
6http://wordnet.princeton.edu
7TreeTagger is available from http://www.ims.uni-
stuttgart.de/projekte/corplex/TreeTagger/
139
1st-order scores 2nd-order scores
DeSR MODEL LAS UAS Imp LAC LAS UAS Imp LAC
BASE 84.01 85.56 - 88.24 89.20 90.55 - 92.22
EOS 84.89 86.37 +5.6 88.94 89.36 90.64 +1.0 92.37
BIO 84.95 86.37 +6.6 89.06 89.63 90.89 +3.6 92.55
TAG 84.76 86.26 +4.8 88.80 89.54 90.81 +2.8 92.55
AS-0 84.40 85.95 +2.7 88.38 89.41 90.72 +1.8 92.38
AS-1 85.13 86.52 +6.6 89.11 89.57 90.77 +2.3 92.49
AS-2 85.32 86.71 +8.0 89.25 89.87 91.10 +5.8 92.68
Table 4. Results of the different models on WSJ section 23 using the CoNLL scores Labeled attachment score (LAS),
Unlabeled attachment score (UAS), and Label accuracy score (LAC). The column labeled ?Imp? reports the improve-
ment in terms of relative error reduction with respect to the BASE model for the UAS score. In bold the best results.
tional token from the stack and an additional token
from the input for AS-2 (cf. Table 3).
5.2 Results of 2nd-order models
Table 4 summarizes the results of all experiments.
We report the following scores, obtained with the
CoNLL-X scoring script: labeled attachment score
(LAS), unlabeled attachment score (UAS) and label
accuracy score (LAC). For the UAS score, the most
frequently reported, we include the improvement in
relative error reduction.
The 2nd-order base model improves on all mea-
sures over the 1st-order model by approximately
5%. The UAS score is 90.55%, with an improve-
ment of 4.9%. The magnitude of the improve-
ment is remarkable and reflects the 4.6% improve-
ment that Yamada and Matsumoto (Yamada & Mat-
sumoto, 2003) report going from the linear SVM to
the polynomial of degree two. Our base model?s ac-
curacy (90.55% UAS) compares well with the ac-
curacy of the parsers based on the polynomial ker-
nel trained with SVM of Yamada and Matsumoto
(UAS 90.3%), and Hall et al (2006) (UAS 89.4%).
We notice in particular that, given the lack of non-
projective cases/rules, the parser of Hall et al (2006)
is almost identical to our parser, hence the differ-
ence in accuracy (+1.1%) might effectively be due
to a better classifier. Yamada & Matsumoto?s parser
is slightly more complex than our parser, and has
quadratic worst-case complexity. Overall, the accu-
racy of the 2nd-order parser is comparable to that of
the 1st-order MST parser (90.7%).
There is no direct evidence that our perceptron
produces better classifiers than SVM. Rather, the
pattern of results produced by the perceptron seems
comparable to that of SVM (Yamada & Matsumoto,
2003). This is a useful finding in itself, given that
the former is more efficient: perceptron?s update is
linear while SVM solves a quadratic problem at each
update. However, one major difference between the
two approaches lies in the fact that learning with the
primal model does not require splitting the model
by Part-of-Speech, or other means. As a conse-
quence, beyond the greater simplicity, our method
might benefit from not depending so strongly on the
quality of POS tagging. POS information is encoded
as a feature and contributes its weight to the selec-
tion of the parsing action, together with all addi-
tionally available information. In the SVM-trained
methods the model that makes the prediction for the
parsing rule is essentially chosen by an oracle, the
prediction of the POS tagger. Furthermore, it might
be argued that learning a single model makes a bet-
ter use of the training data by exploiting the cor-
relations between all datapoints, while in the dual
split-training case the interaction is limited to dat-
apoints in the same partition. In any case, second-
order feature maps could be used also with SVM or
other classifiers. The advantage of using the per-
ceptron lies in the unchallenged accuracy/efficiency
trade-off. Finally, we recall that training in the pri-
mal model can be performed fully on-line without
affecting the resulting model nor the complexity of
the algorithm.
5.3 Results of models with semantic features
All models based on semantic features improve over
the base model on all measures. The best configura-
140
Parser UAS
Hall et al ?06 89.4
Yamada & Matsumoto ?03 90.3
DeSR 90.55
McDonald & Pereira 1st-order MST 90.7
DeSR AS-2 91.1
McDonald & Pereira 2nd-order MST 91.5
Sagae & Lavie ?06 92.7
Table 5. Comparison of main results on the Penn Tree-
bank dataset.
tion is that of model AS-2 which extracts all seman-
tic features from the widest context. In the 1st-order
AS-2 model the improvement, 86.71% UAS (+8%
relative error reduction) is more marked than in the
2nd-order AS-2 model, 91.1% UAS (+5.8% error
reduction). A possible simple exaplanation is that
some information captured by the semantic features
is correlated with other higher-order features which
do not occur in the 1st-order encoding. Overall the
accuracy of the DeSR parser with semantic informa-
tion is slightly inferior to that of the second-order
MST parser (McDonald & Pereira, 2006) (91.5%
UAS). The best result on this dataset to date (92.7%
UAS) is that of Sagae and Lavie (Sagae & Lavie,
2006) who use a parser which combines the predic-
tions of several pre-existing parsers, including Mc-
Donald?s and Nivre?s parsers. Table 5 lists the main
results to date on the version of the Penn Treebank
for dependency parsing task used in this paper.
In Table 4 we also evaluate the gain obtained by
adding one semantic feature type at a time (cf. rows
EOS/BIO/TAG). These results show that all seman-
tic features provide some improvement (with the du-
bious case of EOS in the 2nd-order model). The
BIO encoding seems to produce the most accurate
features. This could be promising because it sug-
gests that the benefit does not depend only on the
specific tags, but that the segmentation in itself is
important. Hence tagging could improve the adapta-
tion of parsers to new domains even if only generic
tagging methods are available.
5.4 Remarks on efficiency
All experiments were performed on a 2.4GHz AMD
Opteron CPU machine with 32GB RAM. The 2nd-
order parser uses almost 3GB of memory. While
Parsing time/sec
Parser English Chinese
MST 2n-order 97.52 59.05
MST 1st-order 76.62 49.13
DeSR 36.90 21.22
Table 6. Parsing times for the CoNNL 2007 English and
Chinese datasets for MST and DeSR.
it is several times slower and larger than the 1st-
order model8 the 2nd-order model performance is
still competitive. It takes 3 minutes (user time) to
parse section 23, POS tagging included. In train-
ing, the model takes about 1 hour to process the full
dataset once. As a comparison, Hall et al (2006)
reports 1.5 hours for training the partitioned SVM
model and 10 minutes for parsing the evaluation set
on the same Penn Treebank data. We also compared
directly the parsing time of our parser with that of
the MST parser using the version 0.4.3 of MST-
Parser9. For these experiments we used two datasets
from the CoNLL 2007 shared task for English and
Chinese. Table 6 reports the times, in seconds, to
parse the test sets for these languages on a 3.3GHz
Xeon machine with 4 GB Ram, of the MST 1st and
2nd-order parser and DeSR parser (without semantic
features).
The architecture of the model presented here of-
fers several options for optimization. For exam-
ple, implementing the ? models with full vectors
rather than hash tables speeds up parsing by a factor
of three, at the expense of memory. Alternatively,
memory load in training can be reduced, at the ex-
pense of time, by using on-line training. However,
the most valuable option for space need reduction
might be to filter out low-frequency second-order
features. Since the frequency of such features seems
to follow a power law distribution, this reduces sig-
nificantly the feature space size even for low thresh-
olds at small accuracy expense. In this paper how-
ever we focused on the full model, no approxima-
tions were required to run the experiments.
8The 1st-order parser takes 7 seconds (user time) to process
Section 23.
9Available from sourceforge.net.
141
6 Conclusion
We explored the design space of a dependency
parser by modeling and extending the feature repre-
sentation, while adopting one of the simplest parsing
architecture: a single-pass deterministic shift-reduce
algorithm trained with a regularized multiclass per-
ceptron. We showed that with the perceptron it is
possible to adopt higher-order feature maps equiva-
lent to polynomial kernels without need of approx-
imating the model (although this remains an option
for optimization). The resulting models achieve ac-
curacies comparable (or better) to more complex ar-
chitectures based on dual SVM training, and faster
parsing on unseen data. With respect to learning, it is
possible that more sophisticated formulations of the
perceptron (e.g. MIRA (Crammer & Singer, 2003))
could provide further gains in accuracy, as shown
with the MST parser (McDonald et al, 2005).
We also experimented with novel types of se-
mantic features, extracted from the annotations pro-
duced by an entity tagger trained on the BBN cor-
pus. This model further improves over the standard
model yielding an additional 5.8% relative error re-
duction. Although the magnitude of the improve-
ment is not striking, to the best of our knowledge
this is the first encouraging evidence that annotated
semantic information can improve parsing and sug-
gests several options for further research. For exam-
ple, this finding might indicate that this type of ap-
proach, which combines semantic tagging and pars-
ing, is viable for the adaptation of parsing to new
domains for which semantic taggers exist. Seman-
tic features could be also easily included in other
types of dependency parsing algorithms, e.g., MST,
and in current methods for constituent parse rerank-
ing (Collins, 2000; Charniak & Johnson, 2005).
For future research several issues concerning the
semantic features could be tackled. We notice that
more complex semantic features can be designed
and evaluated. For example, it might be useful to
guess the ?head? of segments with simple heuris-
tics, i.e., the guess the node which is more likely to
connect the segment with the rest of the tree, which
all internal components of the entity depend upon.
It would be also interesting to extract semantic fea-
tures from taggers trained on different datasets and
based on different tagsets.
Acknowledgments
The first author would like to thank Thomas Hof-
mann for useful inputs concerning the presentation
of the issue of higher-order feature representations
of Section 3.4. We would also like to thank Brian
Roark and the anonymous reviewers for useful com-
ments and pointers to related work.
References
G. Attardi. 2006. Experiments with a Multilanguage
Non-Projective Dependency Parser. In Proceedings of
CoNNL-X 2006.
BBN. 2005. BBN Pronoun Coreference and Entity Type
Corpus. Linguistic Data Consortium (LDC) catalog
number LDC2005T33.
S. Buchholz and E. Marsi. 2006. Introduction to
CoNNL-X Shared Task on Multilingual Dependency
Parsing. In Proceedings of CoNNL-X 2006.
X. Carreras and L. Ma`rquez. 2005. Introduction to the
CoNLL-2005 Shared Task: Semantic Role Labeling.
In Proceedings of CoNLL 2005.
X. Carreras, M. Surdeanu, and L. Ma`rquez. 2006 Pro-
jective Dependency Parsing with Perceptron. In Pro-
ceedings of CoNLL-X.
E. Charniak. 1997. Statistical Parsing with a Context-
Free Grammar and Word Statistics. In Proceedings of
the Fourteenth National Conference on Artificial Intel-
ligence AAAI.
E. Charniak and M. Johnson. 2005. Coarse-to-Fine n-
Best Parsing and MaxEnt Discriminative Reranking.
In Proceedings of ACL 2005.
M. Ciaramita and Y. Altun. 2006. Broad-Coverage Sense
Disambiguation and Information Extraction with a Su-
persense Sequence Tagger. In Proceedings of EMNLP
2006.
M. Ciaramita and M. Johnson. 2003. Supersense Tag-
ging of Unknown Nouns in WordNet. In Proceedings
of EMNLP 2003.
M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. Thesis, University
of Pennsylvania.
M. Collins. 2000. Discriminative Reranking for Natural
Language Parsing. In Proceedings of ICML 2000.
M. Collins. 2002. Discriminative Training Meth-
ods for Hidden Markov Models: Theory and Experi-
ments with Perceptron Algorithms. In Proceedings of
EMNLP 2002.
142
M. Collins and T. Koo. 2005. Hidden-Variable Mod-
els for Discriminative Reranking. In Proceedings of
EMNLP 2005.
M. Collins and B. Roark. 2004. Incremental Parsing
with the Perceptron Algorithm. In Proceedings of ACL
2004.
K. Crammer and Y. Singer. 2003. Ultraconservative On-
line Algorithms for Multiclass Problems. Journal of
Machine Learning Research 3: pp.951-991.
A. Culotta and J. Sorensen. 2004. Dependency Tree Ker-
nels for Relation Extraction. In Proceedings of ACL
2004.
Y. Ding and M. Palmer. 2005. Machine Translation us-
ing Probabilistic Synchronous Dependency Insertion
Grammars. In Proceedings of ACL 2005.
J. Eisner. 2000. Bilexical Grammars and their Cubic-
Time Parsing Algorithms. In H.C. Bunt and A. Ni-
jholt, eds. New Developments in Natural Language
Parsing, pp. 29-62. Kluwer Academic Publishers.
C. Fellbaum. 1998. WordNet: An Electronic Lexical
Database MIT Press, Cambridge, MA. 1969.
J. Hall, J. Nivre and J. Nilsson. 2006. Discriminative
Classifiers for Deterministic Dependency Parsing. In
Proceedings of the COLING/ACL 2006.
T. Kalt. 2004. Induction of Greedy Controllers for Deter-
ministic Treebank Parsers. In Proceedings of EMNLP
2004.
M. Marcus, B. Santorini and M. Marcinkiewicz. 1993.
Building a Large Annotated Corpus of English: The
Penn Treebank. Computational Linguistics, 19(2): pp.
313-330.
R. McDonald. 2006. Discriminative Training and Span-
ning Tree Algorithms for Dependency Parsing. Ph.D.
Thesis, University of Pennsylvania.
R. McDonald, F. Pereira, K. Ribarov and J. Hajic?. 2005.
Non-projective Dependency Parsing using Spanning
Tree Algorithms. In Proceedings of HLT-EMNLP
2005.
R. McDonald and F. Pereira. 2006. Online Learning
of Approximate Dependency Parsing Algorithms. In
Proceedings of EACL 2006.
M.L. Minsky and S.A. Papert. 1969. Perceptrons: An
Introduction to Computational Geometry. MIT Press,
Cambridge, MA. 1969.
A. Moschitti. 2006. Efficient Convolution Kernels for
Dependency and Constituent Syntactic Trees. In Pro-
ceedings of ECML 2006.
J. Nivre. 2004. Incrementality in Deterministic Depen-
dency Parsing. In Incremental Parsing: Bringing En-
gineering and Cognition Together. Workshop at ACL-
2004.Spain, 50-57.
J. Nivre, J. Hall, S. Ku?bler, R. McDonald, J. Nilsson,
S. Riedel and D. Yuret. 2007. The CoNLL 2007
Shared Task on Dependency Parsing, In Proceedings
of EMNLP-CoNLL 2007.
J. Nivre and M. Scholz. 2004. Deterministic Depen-
dency Parsing of English Text. In Proceedings of
COLING 2004.
V. Punyakanok, D. Roth, and W. Yih. 2005. The Neces-
sity of Syntactic Parsing for Semantic Role Labeling.
In Proceedings of IJCAI 2005.
F. Rosemblatt. 1958. The Perceptron: A Probabilistic
Model for Information Storage and Organization in the
Brain. Psych. Rev., 68: pp. 386-407.
K. Sagae and A. Lavie. 2005. Parser Combination by
Reparsing. In Proceedings of HLT-NAACL 2006.
F. Sha and F. Pereira. 2003. Shallow Parsing with Condi-
tional Random Fields. In Proceedings of HLT-NAACL
2003.
H. Yamada and Y. Matsumoto. 2003. Statistical De-
pendency Analysis with Support Vector Machines. In
Proceedings of the Eighth International Workshop on
Parsing Technologies. Nancy, France.
S. Yi and M. Palmer. 2005. The Integration of Syntactic
Parsing and Semantic Role Labeling. In Proceedings
of CoNLL 2005.
A. Wong and D. Wu. 1999. Learning a Lightweight De-
terministic Parser. In Proceedings of EUROSPEECH
1999.
D. Zhang and W.S. Less. 2003. Question Classification
using Support Vector Machines. In Proceedings of SI-
GIR 2003.
143
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 108?111,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
TANL-1: Coreference Resolution by Parse Analysis and 
Similarity Clustering 
 
Giuseppe Attardi 
Dipartimento di Informatica 
Universit? di Pisa 
Largo B. Pontecorvo, 3 
attardi@di.unipi.it 
Stefano Dei Rossi 
Dipartimento di Informatica 
Universit? di Pisa 
Largo B. Pontecorvo, 3 
deirossi@di.unipi.it 
Maria Simi 
Dipartimento di Informatica 
Universit? di Pisa 
Largo B. Pontecorvo, 3 
simi@di.unipi.it 
 
  
 
Abstract 
Our submission to the Semeval 2010 task 
on coreference resolution in multiple lan-
guages is based on parse analysis and si-
milarity clustering. The system uses a bi-
nary classifier, based on Maximum En-
tropy, to decide whether or not there is a 
relationship between each pair of men-
tions extracted from a textual document. 
Mention detection is based on the analy-
sis of the dependency parse tree. 
1 Overview 
Coreference resolution can be described as the 
problem of clustering noun phrases (NP), also 
called mentions, into sets referring to the same 
discourse entity.  
The ?Coreference Resolution in Multiple Lan-
guages task? at SemEval-2010 is meant to assess 
different machine learning techniques in a multi-
lingual context, and by means of different 
evaluation metrics. Two different scenarios are 
considered: a gold standard scenario (only avail-
able for Catalan and Spanish), where correct 
mention boundaries are provided to the partici-
pants, and a regular scenario, where mention 
boundaries are to be inferred from other linguis-
tic annotations provided in the input data. In par-
ticular the linguistic annotations provided for 
each token in a sentence are: position in sentence 
(ID), word (TOKEN), lemma and predicted 
lemma (LEMMA and PLEMMA), morpho-
syntactic information, both gold and/or predicted 
(POS and PPOS, FEAT and PFEAT), depend-
ency parsing annotations (HEAD and PHEAD, 
DEPREL and PDEPREL), named entities (NE 
and PNE), and semantic roles (PRED, PPRED, 
and corresponding roles in the following col-
umns). In the gold scenario, mention boundaries 
annotations (in column COREF) can also be used 
as input. 
Our approach to the task was to split corefer-
ence resolution into two sub-problems: mention 
identification and creation of entities. Mention 
recognition was based on the analysis of parse 
trees produced from input data, which were pro-
duced by manual annotation or state-of-the-art 
dependency parsers. Once the mentions are iden-
tified, coreference resolution involves partition-
ing them into subsets corresponding to the same 
entity. This problem is cast into the binary classi-
fication problem of deciding whether two given 
mentions are coreferent. A Maximum Entropy 
classifier is trained to predict how likely two 
mentions refer to the same entity. This is fol-
lowed by a greedy procedure whose purpose is to 
cluster mentions into entities.  
According to Ng (2005), most learning based 
coreference systems can be defined by four ele-
ments: the learning algorithm used to train the 
coreference classifier, the method of creating 
training instances for the learner, the feature set 
used to represent a training or test instance, and 
the clustering algorithm used to coordinate the 
coreference classification decisions. In the fol-
lowing we will detail our approach by making 
explicit the strategies used in each of above men-
tioned components. 
The data model used by our system is based 
on the concepts of entity and mention. The col-
lection of mentions referring to the same object 
in a document forms an entity. A mention is an 
instance referring to an object: it is represented 
by the start and end positions in a sentence, a 
type and a sequence number. For convenience it 
also contains a frequency count and a reference 
to the containing sentence. 
108
2 Mention detection 
The first stage of the coreference resolution 
process tries to identify the occurrence of men-
tions in documents. 
In the training phase mentions are obtained 
from the NE (or PNE) column of the corpus and 
are partitioned into entities using the information 
provided in the COREF column. 
In the regular setting, we used an algorithm for 
predicting boundaries that relies on the parse tree 
of the sentence produced from the gold annota-
tions in columns HEAD and DEP, if available, or 
else from columns PHEAD and PDEP, the out-
put of a dependency parser provided as input da-
ta.  
This analysis relied on minimal language 
knowledge, in order to determine possible heads 
of sub-trees counting as mentions, i.e. noun 
phrases or adverbial phrases referring to quanti-
ties, times and locations. POS tags and morpho-
logical features, when available, were mostly 
taken into account in determining mention heads. 
The leaves of the sub-trees of each detected head 
were collected as possible mentions. 
The mentions identified by the NE column 
were then added to this set, discarding duplicates 
or partial overlaps. Partial overlaps in principle 
should not occur, but were present occasionally 
in the data. When this occurred, we applied a 
strategy to split them into a pair of mentions.  
The same mention detection strategy was used 
also in the gold task, where we could have just 
returned the boundaries present in the data, scor-
ing 100% in accuracy. This explains the small 
loss in accuracy we achieved in mention identifi-
cation in the gold setting. 
Relying on parse trees turned out to be quite 
effective, especially for languages where gold 
parses where available. For some other languag-
es, the strategy was less effective. This was due 
to different annotation policies across different 
languages, and, in part, to inconsistencies in the 
data. For example in the Italian data set, named 
entities may include prepositions, which are typ-
ically the head of the noun phrase, while our 
strategy of looking for noun heads leaves the 
preposition out of the mention boundaries. 
Moreover this strategy obviously fails when 
mentions span across sentences as was the case, 
again, for Italian. 
3 Determining coreference 
For determining which mentions belong to the 
same entity, we applied a machine learning tech-
nique. We trained a Maximum Entropy classifier 
written in Python (Le, 2004) to determine 
whether two mentions refer to the same entity.  
We did do not make any effort to optimize the 
number of training instances for the pair-wise 
learner: a positive instance is created for each 
anaphoric NP, paired with each of its antecedents 
with the same number, and a negative instance is 
created by pairing each NP with each of its pre-
ceding non-coreferent noun phrases.  
The classifier is trained using the following 
features, extracted for each pair of mentions. 
Lexical features 
 Same: whether two mentions are equal; 
 Prefix: whether one mention is a prefix of 
the other;  
 Suffix: whether one mention is a suffix of 
the other; 
 Acronym: whether one mention is the 
acronym of the other. 
 Edit distance: quantized editing distance 
between two mentions. 
 Distance features 
 Sentence distance: quantized distance be-
tween the sentences containing the two 
mentions; 
 Token distance: quantized distance be-
tween the start tokens of the two mentions; 
 Mention distance: quantized number of 
other mentions between two mentions. 
Syntax features 
 Head: whether the heads of two mentions 
have the same POS; 
 Head POS: pairs of POS of the two men-
tions heads; 
Count features 
 Count: pairs of quantized numbers, each 
counting how many times a mention oc-
curs. 
Type features 
 Type: whether two mentions have the 
same associated NE (Named Entity) type. 
Pronoun features 
109
When the most recent mention is a pronominal 
anaphora, the following features are extracted: 
 Gender: pair of attributes {female, male or 
undetermined}; 
 Number: pair of attributes {singular, plur-
al, undetermined}; 
 Pronoun type: this feature is language de-
pendent and represents the type of prono-
minal mention, i.e. whether the pronoun is 
reflexive, possessive, relative, ? 
In the submitted run we used the GIS (Genera-
lized Iterative Scaling) algorithm for parameter 
estimation, with 600 iterations, which appeared 
to provide better results than using L-BFGS (a 
limited-memory algorithm for unconstrained op-
timization). Training times ranged from one 
minute for German to 8 minutes for Italian, 
hence the slower speed of GIS was not an issue. 
3.1 Entity creation 
The mentions detected in the first phase were 
clustered, according to the output of the classifi-
er, using a greedy clustering algorithm.  
Each mention is compared to all previous 
mentions, which are collected in a global men-
tions table. If the pair-wise classifier assigns a 
probability greater than a given threshold to the 
fact that a new mention belongs to a previously 
identified entity, it is assigned to that entity. In 
case more than one entity has a probability great-
er than the threshold, the mention is assigned to 
the one with highest probability. This strategy 
has been described as best-first clustering by Ng 
(2005). 
In principle the process is not optimal since, 
once a mention is assigned to an entity, it cannot 
be later assigned to another entity to which it 
more likely refers. Luo et al (2004) propose an 
approach based on the Bell tree to address this 
problem. Despite this potential limitation, our 
system performed quite well. 
4 Data preparation 
We used the data as supplied by the task organ-
izers for all languages except Italian. A modified 
version of the Hunpos tagger (Hal?csy, Kornai & 
Oravecz, 2007; Attardi et al, 2009) was used to 
add to the Italian training and development cor-
pora more accurate POS tags than those supplied, 
as well as missing information about morphol-
ogy. The POS tagger we used, in fact is capable 
of tagging sentences with detailed POS tags, 
which include morphological information; this 
was added to column PFEATS in the data. Just 
for this reason our submission for Italian is to be 
considered an open task submission. 
The Italian training corpus appears to contain 
several errors related to mention boundaries. In 
particular there are cases of entities starting in a 
sentence and ending in the following one. This 
appears to be due to sentence splitting (for in-
stance at semicolons) performed after named ent-
ities had been tagged. As explained in section 2, 
our system was not prepared to deal with these 
situations. 
Other errors in the annotations of entities oc-
curred in the Italian test data, in particular incor-
rect balancing of openings and closings named 
entities, which caused problems to our submis-
sion. We could only complete the run after the 
deadline, so we could only report unofficial re-
sults for Italian. 
5 Results 
We submitted results to the gold and regular 
challenges for the following languages: Catalan, 
English, German and Spanish. 
Table 1 summarizes the performance of our 
system, according to the different accuracy 
scores for the gold task, Table 2 for the regular 
task. We have outlined in bold the cases where 
we achieved the best scores among the partici-
pating systems. 
 
 Mention CEAF MUC B
3 BLANC 
Catalan 98.4 64.9 26.5 76.2 54.4 
German 100 77.7 25.9 85.9 57.4 
English 89.8 67.6 24.0 73.4 52.1 
Spanish 98.4 65.8 25.7 76.8 54.1 
Table 1. Gold task, Accuracy scores. 
 
 Mention CEAF MUC B
3 BLANC 
Catalan 82.7 57.1 22.9 64.6 51.0 
German 59.2 49.5 15.4 50.7 44.7 
English 73.9 57.3 24.6 61.3 49.3 
Spanish 83.1 59.3 21.7 66.0 51.4 
Table 2. Regular task. Accuracy scores. 
6 Error analysis 
We performed some preliminary error analysis. 
The goal was to identify systematic errors and 
possible corrections for improving the perfor-
mance of our system. 
We limited our analysis to the mention boun-
daries detection for the regular tasks. A similar 
110
analysis for coreference detection, would require 
the availability of gold test data.  
7 Mention detection errors 
As described above, the strategy used for the ex-
traction of mentions boundaries is based on de-
pendency parse trees and named entities. This 
proved to be a good strategy in some languages 
such as Catalan (F1 score: 82.7) and Spanish (F1 
score: 83.1) in which the dependency data avail-
able in the corpora were very accurate and con-
sistent with the annotation of named entities. In-
stead, there have been unexpected problems in 
other languages like English or German, where 
the dependencies information were annotated 
using a different approach. 
For German, while we achieved the best B3 
accuracy on coreference analysis in the gold set-
tings, we had a quite low accuracy in mention 
detection (F1: 59.2), which was responsible of a 
significant drop in coreference accuracy for the 
regular task. This degradation in performance 
was mainly due to punctuations, which in Ger-
man are linked to the sub-tree containing the 
noun phrase rather than to the root of the sen-
tence or tokens outside the noun phrase, as it 
happens in Catalan and Spanish. This misled our 
mention detection algorithm to create many men-
tions with wrong boundaries, just because punc-
tuation marks were included. 
In the English corpus different conventions 
were apparently used for dependency parsing and 
named entity annotations (Table 3), which pro-
duced discrepancies between the boundaries of 
the named entities present in the data and those 
predicted by our algorithm. This in turn affected 
negatively the coreference detection algorithm 
that uses both types of information. 
 
ID TOKEN HEAD DEPREL NE COREF 
1 Defense 2 NAME (org) (25 
2 Secretary 4 NMOD _ _ 
3 William 4 NAME (person _ 
4 Cohen 5 SBJ person) 25) 
Table 3. Example of different conventions for NE and 
COREF in the English corpus. 
 
Error analysis also has shown that further im-
provements could be obtained, for all languages, 
by using more accurate language specific extrac-
tion rules. For example, we missed to consider a 
number of specific POS tags as possible identifi-
ers for the head of noun phrases. By some simple 
tuning of the algorithm we obtained some im-
provements. 
8 Conclusions 
We reported our experiments on coreference res-
olution in multiple languages. We applied an ap-
proach based on analyzing the parse trees in or-
der to detect mention boundaries and a Maxi-
mum Entropy classifier to cluster mentions into 
entities. 
Despite a very simplistic approach, the results 
were satisfactory and further improvements are 
possible by tuning the parameters of the algo-
rithms. 
References 
G. Attardi et al, 2009. Tanl (Text Analytics and Natu-
ral Language Processing). SemaWiki project: 
http://medialab.di.unipi.it/wiki/SemaWiki. 
P. Hal?csy, A. Kornai, and C. Oravecz, 2007. Hun-
Pos: an open source trigram tagger. Proceedings of 
the ACL 2007, Prague. 
Z. Le, Maximum Entropy Modeling Toolkit for Pytho 
and C++, Reference Manual. 
X. Luo, A. Ittycheriah, H. Jing, N. Kambhatla & S. 
Roukos. 2004. A Mention-Synchronous Corefer-
ence Resolution Algorithm Based on the Bell Tree. 
Proceedings of the ACL 2004, Barcelona. 
V. Ng, Machine Learning for Coreference Resolution: 
From Local Classification to Global Ranking, Pro-
ceedings of the 43rd Annual Meeting of the Associ-
ation for Computational Linguistics (ACL), Ann 
Arbor, MI, June 2005, pp. 157-164. 
M. Recasens, L. M?rquez,  E. Sapena, M. A. Mart?,  
M. Taul?, V. Hoste, M. Poesio and Y. Versley, 
SemEval-2010 Task 1: Coreference resolution in 
multiple languages, in Proceedings of the 5th In-
ternational Workshop on Semantic Evaluations 
(SemEval-2010), Uppsala, Sweden, 2010. 
 
 
111
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 754?760,
Dublin, Ireland, August 23-24, 2014.
UniPi: Recognition of Mentions of Disorders in Clinical Text  
Giuseppe Attardi, Vittoria Cozza, Daniele Sartiano 
Dipartimento di Informatica 
Universit? di Pisa 
Largo B. Pontecorvo, 3 
I-56127 Pisa, Italy 
{attardi, cozza, sartiano}@di.unipi.it 
 
Abstract 
The paper describes our experiments ad-
dressing the SemEval 2014 task on the 
Analysis of Clinical text. Our approach 
consists in extending the techniques of 
NE recognition, based on sequence label-
ling, to address the special issues of this 
task, i.e. the presence of overlapping and 
discontiguous mentions and the require-
ment to map the mentions to unique iden-
tifiers. We explored using supervised 
methods in combination with word em-
beddings generated from unannotated da-
ta.  
1 Introduction 
Clinical records provide detailed information on 
examination and findings of a patient consulta-
tion expressed in a narrative style. Such records 
abound in mentions of clinical conditions, ana-
tomical sites, medications, and procedures, 
whose accurate identification is crucial for any 
further activity of text mining. Many different 
surface forms are used to represent the same 
concept and the mentions are interleaved with 
modifiers, e.g. adjectives, verb or adverbs, or are 
abbreviated involving implicit terms. 
For example, in 
Abdomen is soft, nontender, 
nondistended, negative bruits 
the mention occurrences are ?Abdomen 
nontender? and ?Abdomen bruits?, which 
refer to the disorders: ?nontender abdomen? 
and ?abdomininal bruit?, with only the sec-
ond having a corresponding UMLS Concept 
Unique Identifier (CUI). In this case the two 
mentions overlap and both are interleaved with 
other terms, not part of the mentions. 
Secondly, mentions can be nested, as in this 
example: 
left pleural and parenchymal 
calcifications 
where the mention calcifications is nested 
within pleural calcifications. 
Mentions of this kind are a considerable de-
parture from those dealt in typical Named Entity 
recognition, which are contiguous and non-
overlapping, and therefore they represents a new 
challenge for text analysis. 
The analysis of clinical records poses addi-
tional difficulties with respect to other biomedi-
cal NER tasks, which use corpora from the med-
ical literature. Clinical records are entered by 
medical personnel on the fly and so they contain 
misspellings and inconsistent use of capitaliza-
tion. 
The task 7 at SemEval 2014, Analysis of 
Clinical Text, addresses the problem of recogni-
tion of mentions of disorders and is divided in 
two parts: 
A. recognition of mentions of bio-medical 
concepts that belong to the UMLS se-
mantic group disorders;  
B. mapping of each disorder mention to a 
unique UMLS CUI (Concept Unique 
Identifiers). 
The challenge organizers provided the following 
resources: 
? A training corpus of clinical notes from 
MIMIC II database manually annotated 
for disorder mentions and normalized to 
an UMLS CUI, consisting of 9432 sen-
tences, with 5816 annotations. 
? A collection of unannotated notes, consist-
ing of 1,611,080 sentences. 
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:   
http://creativecommons.org/licenses/by/4.0/ 
754
We also had access to the UMLS ontology (Bo-
denreider, 2004). 
Our approach to portion A of the task was to 
adapt a sequence labeller, which provides good 
accuracy in Named Entity recognition in the 
newswire domain, to handle the peculiarities of 
the clinical domain. 
We performed mention recognition in two 
steps: 
1. identifying contiguous portions of a men-
tion; 
2. combining separated portions of mentions 
into a full mention. 
In order to use a traditional sequence tagger for 
the first step, we had to convert the input data 
into a suitable format, in particular, we dealt with 
nested mentions by transforming them into non-
overlapping sequences, through replication. 
For recombining discontiguous mentions, we 
employed a classifier, trained to recognize 
whether pairs of mentions belong to the same 
entity. The classifier was trained using also fea-
tures extracted from the dependency tree of a 
sentence, in particular the distance of terms along 
the tree path. Terms related by a dependency 
have distance 1 and terms having a common 
head have distance 2. By limiting the pairs1 to be 
considered for combination to those within dis-
tance 3, we both ensure that only plausible com-
binations are performed and reduce the cost of 
the algorithm. 
For dealing with portion B of the task, we ap-
ply fuzzy matching (Fraiser, 2011) between the 
extracted mentions and the textual description of 
entities present in selected sections of UMLS 
disorders. The CUI from the match with highest 
score is chosen. 
In the following sections, we describe how we 
carried out the experiments, starting with the pre-
processing of the data, then with the training of 
several versions of NE recognizer, the training of 
the classifier for mention combination. We then 
report on the results and discuss some error anal-
ysis on the results. 
2 Preprocessing of the annotated data 
The training data was pre-processed, in order to 
obtain corpora in a suitable format for: 
1. training a sequence tagger 
2. training the classifier for mention com-
bination. 
                                                 
1 Not implemented in the submitted runs. 
Annotations in the training data adopt a pipe-
delimited stand-off character-offset format. The 
example in the introduction has these annota-
tions: 
00098-016139-
DISCHARGE_SUMMARY.txt || Dis-
ease_Disorder || C0221755 || 
1141 || 1148 || 1192 || 1198 
00098-016139-
DISCHARGE_SUMMARY.txt || Dis-
ease_Disorder || CUI-less || 
1141 || 1148 || 1158 || 1167 
The first annotation marks Disease_Disorder 
as annotation type, C0221755 as CUI, while the 
remaining pairs of numbers represent character 
offsets within the original text that correspond to 
spans of texts containing the mention, i.e. Abdo-
men nondistended. The second annotation is 
similar and refers to Abdomen bruits. 
In order to prepare the training corpus for a 
NE tagger, the data had to be transformed and 
converted into IOB2 notation. However a stand-
ard IOB notation does not convey information 
about overlapping or discontiguous mentions. 
In order to deal with overlapping mentions, as 
is the case for word ?Abdomen? in our earlier 
example, multiple copies of the sentence are pro-
duced, each one annotated with disjoint men-
tions. If two mentions overlap, two versions are 
generated, one annotated with just the first men-
tion and one with the second. If several overlap-
ping mentions are present in a sentence, copies 
are generated for all possible combinations of 
non-overlapping mentions. 
For dealing with discontiguous mentions, each 
annotated entity is assigned an id, uniquely iden-
tifying the mention within the sentence. This id 
is added as an extra attribute to each token, rep-
resented as an extra column in the tab separated 
IOB file format for the NE tagger. 
We processed with the Tanl pipeline (Attardi 
et al., 2009; Attardi et al., 2010). We first ex-
tracted the text from the training corpus in XML 
format and added the mentions annotations as 
tags enclosing them, with spans and mentions id 
as attributes. We then applied sentence splitting, 
tokenization, PoS tagging and dependency pars-
ing using DeSR (Attardi, 2006). 
The tags were converted to IOB format.  
Here are two sample tokens in the resulting 
annotation, with attributes id, form, pos, head, 
deprel, entity, entity id: 
                                                 
2 http://en.wikipedia.org/wiki/Inside_Outside_Beginning 
755
1 Abdomen NNP 2 SBJ B-DISO 1 
? 
5 nontender NN 10 NMOD B-DISO 1 
3 Named Entity Tagging 
The core of our approach relies on an initial 
stage of Named Entity recognition. We per-
formed several experiments, using different NE 
taggers in different configurations and using both 
features from the training corpus and features 
obtained from the unannotated data. 
3.1 Tanl NER 
We performed several experiments using the 
Tanl NE Tagger (Attardi et al., 2009), a generic, 
customizable statistical sequence labeller, suita-
ble for many tasks of sequence labelling, such as 
POS tagging or Named Entity Recognition. 
The tagger implements a Conditional Markov 
Model and can be configured to use different 
classification algorithms and to specify feature 
templates for extracting features. In our experi-
ments we used a linear SVM classification algo-
rithm. 
We experimented with several configurations, 
all including a set of word shape features, as in 
(Attardi et al., 2009): (1) the previous word is 
capitalized; (2) the following word is capitalized; 
(3) the current word is in upper case; (4) the cur-
rent word is in mixed case; (5) the current word 
is a single uppercase character; (6) the current 
word is a uppercase character and a dot; (7) the 
current word contains digits; (8) the current word 
is two digits; (9) the current word is four digits; 
(10) the current word is made of digits and ?/?; 
(11) the current word contains ?$?; (12) the cur-
rent word contains ?%?; (13) the current word 
contains an apostrophe; (14) the current word is 
made of digits and dots. 
A number of dictionary features were also 
used, including prefix and suffix dictionaries, 
bigrams, last words, first word and frequent 
words, all extracted from the training corpus. 
Additionally, a dictionary of disease terms was 
used, consisting of about 22,000 terms extracted 
from the preferred terms for CUIs belonging to 
the UMLS semantic type ?Disease or Syn-
drome?. 
The first character of the POS tag was also 
used as feature, extracted from a window of to-
kens before and after the current token.  
Finally attribute features are extracted from 
attributes (Form, PoS, Lemma, NE, Disease) of 
surrounding tokens, denoted by their relative po-
sition to the current token. The best combination 
of Attribute features obtained with runs on the 
development set was the following: 
Feature Tokens 
POS[0] wi-2 wi?1 wi wi+1 
DISEASE wi wi+1 wi+2 
Table 1. Attribute features used in the runs. 
3.2 Word Embeddings 
We explored ways to use the unannotated data in 
NE recognition by exploiting word embeddings 
(Collobert et al, 2011). In a paper published after 
our submission, Tang et al. (2014) show that 
word embeddings are beneficial to Biomedical 
NER. 
We used the word embeddings for 100,000 
terms created through deep learning on the Eng-
lish Wikipedia by Al-Rfou et al. (2013). We then 
built, with the same procedure, embedding for 
terms from the supplied unlabelled data. The 
corpus was split, tokenized and normalized and a 
vocabulary was created with the most frequent 
words not already present among the Wikipedia 
word embeddings. Four versions of the embed-
dings were created, varying the size of the vo-
cabulary and the size of the context window, as 
described in Table 1. 
 
 Run1 Run2 Run3 Run4 
Vocabulary size 50,000 50,000 30,000 30,000 
Context     5 2 5 2 
Hidden Layers 32 32 32 32 
Learning Rate 0.1 0.1 0.1 0.1 
Embedding size 64 64 64 64 
Table 2. Word Embedding Parameters. 
We developed and trained a Deep Learning NE 
tagger (nlpnet, 2014) based on the SENNA archi-
tecture (SENNA, 2011) using these word em-
beddings. 
As an alternative to using the embeddings di-
rectly as features, we created clusters of word 
embeddings using the Dbscan algorithm (Ester et 
al., 1996) implemented in the sklearn library. We 
carried out several experiments, varying the pa-
rameters of the algorithm. The configuration that 
produced the largest number of clusters had 572 
clusters. The clusters turned out not to be much 
significant, since a single cluster had about 
29,000 words, another had 5,000 words, and the 
others had few, unusual words. 
We added the clusters as a dictionary feature 
to our NE tagger. Unfortunately, most of the 
756
terms fell within 4 clusters, so the feature turned 
out to be little discriminative. 
3.3 Stanford NER 
We performed experiments also with a tagger 
based on a different statistical approach: the 
Stanford Named Entity Recognizer. This tagger 
is based on the Conditional Random Fields 
(CRF) statistical model and uses Gibbs sampling 
instead of other dynamic programming tech-
niques for inference on sequence models (Finkel 
et al., 2005). This tagger normally works well 
enough using just the form of tokens as feature 
and we applied it so. 
3.4 NER accuracy 
We report the accuracy of the various NE taggers 
we tested on the development set, using the scor-
er from the CoNLL Shared Task 2003 (Tjong 
Kim Sang and De Meulder, 2003). 
We include here also the results with 
CRFsuite, the CRF tagger used in (Tang et al., 
2014). 
 
NER Precision Recall F- score 
Tanl 80.41 65.08 71.94 
Tanl+clusters     80.43 64.48 71.58 
nlpnet 80.29 62.51 70.29 
Stanford 80.30 64.89 71.78 
CRFsuite 79.69 61.97 69.72 
Table 3. Accuracy of various NE taggers on the 
development set. 
Based on these results we chose the Tanl tagger 
and the Stanford NER for our submitted runs. 
All these taggers are known to be capable of 
achieving state of the art performance or close to 
it (89.57 F1) in the CoNLL 2003 shared task on 
the WSJ Penn Treebank. 
The accuracy on the current benchmark is 
much lower, despite the fact that there is only 
one category and the terminology for disorders is 
drawn from a restricted vocabulary. 
It has been noted by Dingare et al. (2005) that 
NER over biomedical texts achieves lower accu-
racy compared to other domains, quite within the 
range of the above results. Indeed, compared 
with the newswire domain or other domains, the 
entities in the biomedical domain tend to be more 
complex, without the distinctive shape features 
of the newswire categories. 
4 Discontiguous mentions 
Discontiguous mention detection can be formu-
lated as a problem of deciding whether two con-
tiguous mentions belong to the same mention. As 
such, it can be cast into a classification problem. 
A similar approach was used successfully for the 
coreference resolution task at SemEval 2010 (At-
tardi, Dei Rossi et al., 2010) 
 
4.1 Mentions  detection 
We trained a Maximum Entropy classifier 
(Ratnaparkhi, 1996) to recognize whether two 
terms belong to the same mention. 
The training instances for the pair-wise learner 
consist of each pair of terms within a sentence 
annotated as disorders. A positive instance is 
created if the terms belong to the same mention, 
negative otherwise. 
The classifier was trained using the following 
features, extracted for each pair of words for dis-
eases. 
Distance features 
? Token distance: quantized distance be-
tween the two words; 
? Ancestor distance: quantized distance be-
tween the words in the parse tree if one is 
the ancestor of the other 
Syntax features 
? Head: whether the two words have the 
same head; 
? DepPath: concatenation of the dependen-
cy relations of the two words to their 
common parent  
Dictionary features 
? UMLS: whether the two words are both 
present in an UMLS definition 
The last feature is motivated by the fact that, ac-
cording to the task description, most of the dis-
order mentions correspond to diseases in the 
SNOMED terminology. 
4.2 Merging of mentions 
The mentions detected in the first phase are 
merged using the following process. Sentence 
are parsed and then for each pair of words that 
are tagged as disorder, features are extracted and 
passed to the classifier. 
If the classifier assigns a probability greater 
than a given threshold the two words are com-
bined into a larger mention. The process is then 
repeated trying to further extend each mention 
757
with additional terms by combining mentions 
that share a word. 
 
5 Mapping entities to CUIs 
Task B requires mapping each recognized entity 
to a concept in the SNOMED-CT terminology, 
assigning to it a unique UMLS CUI, if possible, 
or else marking it as CUI-less. The CUIs are 
limited to those corresponding to SNOMED 
codes and belonging to the following UMLS se-
mantic types: ?Acquired Abnormality" or ?Con-
genital Abnormality", ?Injury or Poisoning", 
"Pathologic Function", "Disease or Syndrome", 
"Mental or Behavioral Dysfunction", "Cell or 
Molecular Dysfunction", "Experimental Model 
of Disease" or "Anatomical Abnormality", ?Ne-
oplastic Process" or "Sign or Symptom". 
In order to speed up search, we created two 
indices: an inverted index from words in the def-
inition of a CUI to the corresponding CUI and a 
forward index from a CUI to its definition. 
For assigning a CUI to a mention, we search 
in the dictionary of CUI preferred terms, first for 
an exact match, then for a normalized  mention 
and finally for a fuzzy match (Fraiser, 2011). 
Normalization entails dropping punctuation and 
stop words. Fuzzy matching is sometimes too 
liberal, for example it matches ?chronic ob-
structive pulmonary? with ?chronic ob-
structive lung disease?; so we also put a 
ceiling on the edit distance between the phrases. 
The effectiveness of the process is summa-
rized in these results on the development set: 
Exact 
matches 
Normalized 
matches 
Fuzzy 
matches 
No 
matches 
1352 868 304 5488 
Table 4. CUI identifications on the devel set. 
6 Experiments 
The training corpus for the submission consisted 
of the merge of the train and development sets. 
We submitted three runs, using different or 
differently configured NE tagger. 
Two runs were submitted using the Tanl tag-
ger using the features listed in Table 5, where 
DISEASE and CLUSTER meaning is explained 
earlier. 
Feature UniPI_run0 UniPI_ run1 
POS[0] wi-2 wi?1 wi wi+1 wi-2 wi?1 wi wi+1 
CLUSTER wi wi+1 wi wi+1 
DISEASE wi wi+1 wi+2  
Table 5. Attribute features used in the runs. 
Since the clustering produced few large clusters, 
the inclusion of this feature did not affect sub-
stantially the results. 
 
A third run (UniPi_run_2) was performed us-
ing the Stanford NER with default settings. 
7 Results 
The results obtained in the three submitted runs, 
are summarized in Table 6, in terms of accuracy, 
precision, recall and F-score. For comparison, 
also the results obtained by the best performing 
systems are included. 
Run Precision Recall F- score 
Task A 
Unipi_run0 0.539 0.684 0.602 
Unipi_run1     0.659 0.612 0.635 
Unipi_run2 0.712 0.601 0.652 
SemEval best 0.843 0.786 0813 
Task A relaxed 
Unipi_run0 0.778 0.885 0.828 
Unipi_run1 0.902 0.775 0.834 
Unipi_run2 0.897 0.766 0.826 
SemEval best 0.936 0.866 0.900 
Table 6. UniPI Task A results, compared to the 
best submission. 
Run Accuracy 
Task B 
Unipi_run0 0.467 
Unipi_run1     0.428 
Unipi_run2 0.417 
SemEval best 0.741 
Task B relaxed 
Unipi_run0 0.683 
Unipi_run1 0.699 
Unipi_run2 0.693 
SemEval best 0.873 
Table 7. UniPI Task B results, compared to the 
best submission. 
8 Error analysis 
Since the core step of our approach is the NE 
recognition, we tried to analyze possible causes 
of its errors. 
Some errors might be due to mistakes by the 
POS tagger. For example, often some words oc-
cur in full upper case, leading to classify adjec-
tives like ABDOMINAL as NNP instead of JJ. 
Training our POS tagger on the GENIA corpus 
or using the GENIA POS tagger might have 
helped a little. Spelling errors like abdominla 
758
instead of abdominal could also have been cor-
rected. 
Another choice that might have affected the 
NER accuracy was our decision to duplicate the 
sentences in order to remove mention overlaps. 
An alternative solution might have been to use 
two categories in the IOB annotation: one cate-
gory for full contiguous disorder mentions and 
another for partial disorder mentions. This might 
have reduced the confusion in the tagger, since 
isolated words like abdomen get tagged as dis-
order, having been so annotated in the training 
set. Distinguishing the two cases, abdomen 
would become a disorder mention in the step of 
mention merging. Counting the errors in the de-
velopment set we found that 939 out of the 1757 
errors were indeed individual words incorrectly 
identified as disorders. 
8.1 After submission experiments 
After the submission, we changed the algorithm 
for merging mentions, in order to avoid nested 
spans, retaining only the larger one. Tests on the 
development set show that this change leads to a 
small improvement in the strict evaluation: 
Run Precision Recall F- score 
Task A 
devel_run1 0.596 0.653 0.624 
devel run1_after 0.668 0.637 0.652 
Task A relaxed 
devel_run1 0.865 0.850 0.858 
devel run1_after 0.864 0.831 0.847 
Table 8. UniPI Task A post submission results. 
 
9 Conclusions 
We reported our participation to SemEval 2014 
on the Analysis of Clinical Text. Our approach is 
based on using a NER, for identifying contiguous 
mentions and on a Maximum Entropy classifier 
for merging discontiguous ones. 
The training data was transformed into a for-
mat suitable for a standard NE tagger, that does 
not accept discontiguous or nested mentions. Our 
measurements on the development set showed 
that different NE tagger reach a similar accuracy. 
We explored using word embeddings as fea-
tures, generated from the unsupervised data pro-
vided, but they did not improve the accuracy of 
the NE tagger. 
Acknowledgements 
Partial support for this work was provided by 
project RIS (POR RIS of the Regione Toscana, 
CUP n? 6408.30122011.026000160). 
References 
Rami Al-Rfou?, Bryan Perozzi, and Steven Skiena. 
2013. Polyglot: Distributed Word Representations 
for Multilingual NLP. In Proceedings of Confer-
ence on Computational Natural Language Learn-
ing, CoNLL ?13,  pages 183-192, Sofia, Bulgaria. 
Giuseppe Attardi. 2006. Experiments with a Mul-
tilanguage Non-Projective Dependency Parser. In 
Proceedings of the Tenth Conference on Natural 
Language Learning, CoNLL ?06, pages 166-170, 
New York, NY. 
Giuseppe Attardi et al., 2009. Tanl  (Text Analytics 
and Natural Language Processing). SemaWiki pro-
ject: http://medialab.di.unipi.it/wiki/SemaWiki. 
Giuseppe Attardi, Stefano Dei Rossi, Felice Dell'Or-
letta and Eva Maria Vecchi. 2009. The Tanl 
Named Entity Recognizer at Evalita 2009. In Pro-
ceedings of Workshop Evalita?09 - Evaluation of 
NLP and Speech Tools for Italian, Reggio Emilia, 
ISBN 978-88-903581-1-1. 
Giuseppe Attardi, Felice Dell'Orletta, Maria Simi and 
Joseph Turian. 2009. Accurate Dependency Pars-
ing with a Stacked Multilayer Perceptron. In Pro-
ceedings of Workshop Evalita?09 - Evaluation of 
NLP and Speech Tools for Italian, Reggio Emilia, 
ISBN 978-88-903581-1-1.  
Giuseppe Attardi, Stefano Dei Rossi and Maria Simi.  
2010. The Tanl Pipeline. In Proceedings of LREC 
Workshop on Web Services and Processing Pipe-
lines in HLT, WSPP, La Valletta, Malta, pages 14-
21 
Giuseppe Attardi, Stefano Dei Rossi and Maria Simi. 
2010. TANL-1: Coreference Resolution by Parse 
Analysis and Similarity Clustering. In Proceedings 
of the 5th International Workshop on Semantic 
Evaluation, SemEval 2010, Uppsala, Sweden, pag-
es 108-111 
Olivier Bodenreider. 2004. The Unified Medical Lan-
guage System (UMLS): integrating biomedical 
terminology. Nucleic Acids Research, vol. 32, no. 
supplement 1, pages D267?D270. 
Ronan Collobert et al. 2011. Natural Language Pro-
cessing (Almost) from Scratch. Journal of Machine 
Learning Research, 12, pages 2461?2505. 
Shipra Dingare, Malvina Nissim, Jenny Finkel, Chris-
topher Manning and Claire Grover. 2005. A Sys-
tem for Identifying Named Entities in Biomedical 
Text: how Results From two Evaluations Reflect 
on Both the System and the Evaluations. Comp 
Funct Genomics. Feb-Mar; 6(1-2): pages 77?85. 
759
Martin Ester, et al. 1996. A density-based algorithm 
for discovering clusters in large spatial databases 
with noise. In Proceedings of 2nd International 
Conference on Knowledge Discovery and Data Mi-
ing, KDD 96, pages 226?231. 
Jenny Rose Finkel, Trond Grenager and Christopher 
Manning 2005. Incorporating Non-local Infor-
mation into Information Extraction Systems by 
Gibbs Sampling. In Proceedings of the 43nd Annu-
al Meeting of the Association for Computational 
Linguistics, 2005, pages 363?370. 
Neil Fraser. 2011. Diff, Match and Patch libraries for 
Plain Text. (Based on Myer's diff algorithm). 
Adwait Ratnaparkhi. 1996. A Maximum Entropy 
Part-Of-Speech Tagger. In Proceedings of the Em-
pirical Methods in Natural Language Processing 
Conference, EMNLP ?96, pages 17-18. 
Buzhou Tang, Hongxin Cao, Xiaolong Wang, Qingcai 
Chen, and Hua Xu. 2014. Evaluating Word Repre-
sentation Features in Biomedical Named Entity 
Recognition Tasks. BioMed Research Internation-
al, Volume 2014, Article ID 240403. 
Erik F. Tjong Kim Sang and Fien De Meulder 2003. 
Introduction to the CoNLL ?03 Shared Task: Lan-
guage-Independent Named Entity Recognition. In: 
Proceedings of CoNLL ?03, Edmonton, Canada, 
pages 142-147. 
SENNA. 2011. http://ml.nec-labs.com/senna/ 
nlpnet. 2014. https://github.com/attardi/nlpnet 
760
CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 258?262
Manchester, August 2008
DeSRL: A Linear-Time Semantic Role Labeling System
Massimiliano Ciaramita
??
massi@yahoo-inc.com
Felice Dell?Orletta
?
dellorle@di.unipi.it
?: Yahoo! Research Barcelona, Ocata 1, 08003, Barcelona, Catalunya, Spain
?: Dipartimento di Informatica, Universit`a di Pisa, L. B. Pontecorvo 3, I-56127, Pisa, Italy
?: Barcelona Media Innovation Center, Ocata 1, 08003, Barcelona, Catalunya, Spain
Giuseppe Attardi
?
attardi@di.unipi.it
Mihai Surdeanu
?,?
mihai.surdeanu@barcelonamedia.org
Abstract
This paper describes the DeSRL sys-
tem, a joined effort of Yahoo! Research
Barcelona and Universit`a di Pisa for the
CoNLL-2008 Shared Task (Surdeanu et
al., 2008). The system is characterized by
an efficient pipeline of linear complexity
components, each carrying out a different
sub-task. Classifier errors and ambigui-
ties are addressed with several strategies:
revision models, voting, and reranking.
The system participated in the closed chal-
lenge ranking third in the complete prob-
lem evaluation with the following scores:
82.06 labeled macro F1 for the overall task,
86.6 labeled attachment for syntactic de-
pendencies, and 77.5 labeled F1 for se-
mantic dependencies.
1 System description
DeSRL is implemented as a sequence of compo-
nents of linear complexity relative to the sentence
length. We decompose the problem into three sub-
tasks: parsing, predicate identification and clas-
sification (PIC), and argument identification and
classification (AIC). We address each of these sub-
tasks with separate components without backward
feedback between sub-tasks. However, the use of
multiple parsers at the beginning of the process,
and re-ranking at the end, contribute beneficial
stochastic aspects to the system. Figure 1 summa-
rizes the system architecture. We detail the parsing
?
All authors contributed equally to this work.
?
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
sub-task in Section 2 and the semantic sub-tasks
(PIC and AIC) in Section 3.
2 Parsing
In the parsing sub-task we use a combination strat-
egy on top of three individual parsing models,
two developed in-house ?DeSR
left?to?right
and
DeSR
revision
right?to?left
? and a third using an off-the-
shelf parser, Malt 1.0.0
1
.
2.1 DeSR
left?to?right
This model is a version of DeSR (Attardi, 2006),
a deterministic classifier-based Shift/Reduce
parser. The parser processes input tokens advanc-
ing on the input from left to right with Shift ac-
tions and accumulates processed tokens on a stack
with Reduce actions. The parser has been adapted
for this year?s shared task and extended with addi-
tional classifiers, e.g., Multi Layer Perceptron and
multiple SVMs.
2
The parser uses the following features:
1. SPLIT LEMMA: from tokens ?1, 0, 1, prev(0),
leftChild(0), rightChild(0)
2. PPOSS: from ?2, ?1, 0, 1, 2, 3, prev(0), next(?1),
leftChild(?1), leftChild(0), rightChild(?1),
rightChild(0)
3. DEPREL: from leftChild(?1), leftChild(0),
rightChild(?1)
4. HDIST: from ?1, 0
In the above list negative numbers refer to tokens
on the stack, positive numbers to tokens in the in-
put queue. We use the following path operators:
leftChild(x) refers to the leftmost child of token
x, rightChild(x) to the rightmost child of token
x, prev(x) and next(x) respectively to the token
preceding or following x in the sentence.
1
http://w3.msi.vxu.se/
?
nivre/research/
MaltParser.html
2
This parser is available for download at: http://
sourceforge.net/projects/desr/.
258
Voting PIC AIC Reranking
Argument
Frame
DeSR left?to?right
Malt
DeSR right?to?leftrevision OutputInput
Figure 1: DeSRL system architecture.
The first three types of features are directly ex-
tracted from the attributes of tokens present in the
training corpus. The fourth feature represents the
distance of the token to the head of the noun phrase
to which it belongs, or ?O? if it does not belong to
a noun phrase. This distance is computed with a
simple heuristic, based on a pattern of POS tags.
Attardi and Dell?Orletta (2008) have shown that
this feature improves the accuracy of a shift/reduce
dependency parser by providing approximate in-
formation about NP chunks in the sentence. In fact
no token besides the head of a noun phrase can
have a head referring to a token outside the noun
phrase. Hence the parser can learn to avoid creat-
ing such links. The addition of this feature yields
an increase of 0.80% in Labeled Accuracy on the
development set.
2.2 Revision Parser: DeSR
revision
right?to?left
Our second individual parsing model implements
an alternative to the method of revising parse trees
of Attardi and Ciaramita (2007) (see also (Hall &
Novak, 2005)). The original approach consisted in
training a classifier to revise the errors of a base-
line parser. The approach assumed that only lo-
cal revisions to the parse tree would be needed,
since the dependency parser mostly gets individual
phrases correctly. The experiments showed that in-
deed most of the corrections can be expressed by
a small set of (about 20) complex movement rules.
Furthermore, there was evidence that one could get
higher improvements from the tree revision classi-
fier if this was trained on the output of a lower ac-
curacy parser. The reason for this is that the num-
ber of errors is higher and this provides a larger
amount of training data.
For the CoNLL 2008 shared task, we refined this
idea, but instead of using an independent classi-
fier for the revision, we use the parser itself. The
second parser is trained on the original corpus ex-
tended with dependency information predicted by
a lower accuracy parser. To obtain the base parser
we use DeSR trained on half the training corpus
using a Maximum Entropy (ME) classifier. The
ME classifier is considerably faster to train but has
a lower accuracy: this model achieved an LAS of
76.49% on the development set. Using the out-
put of the ME-based parser we extend the original
corpus with four additional columns: the lemma
of the predicted head (PHLEMMA), the PPOSS of
the predicted head (PHPPOSS), the dependency of
the predicted head (PHDEPREL), and the indica-
tion of whether a token appears before or after its
predicted head. A second parser is trained on this
corpus, scanning sentences from right to left and
using the following additional features:
1. PHPPOSS: from ?1, 0
2. PHLEMMA: from ?1, 0
3. PHDEPREL: from ?1, 0
4. PHHDIST: from 0
Performing parsing in reverse order helps reduce
several of the errors that a deterministic parser
makes when dependency links span a long distance
in the input sequence. Experiments on the CoNLL
2007 corpora (Dell?Orletta, 2008) have shown that
this indeed occurs, especially for distances in the
range from 6 to 23. In particular, the most signifi-
cant improvements are for dependencies with label
COORD (+ 6%) and P (+ 8%).
The revision parser achieves an LAS of 85.81%
on the development set. Note that the extra fea-
tures from the forward parser are indeed use-
ful, since a simple backward parser only achieves
82.56% LAS on the development set.
2.3 Parser Combination
The final step consists in combining the out-
puts of the three individual models a simple
voting scheme: for each token we use major-
ity voting to select its head and dependency la-
bel. In case of ties, we chose the dependency
predicted by our overall best individual model
(DeSR
revision
right?to?left
).
3
Note that typical approaches to parser
combination combine the outputs of inde-
pendent parsers, while in our case one base
model (DeSR
revision
right?to?left
) is trained with
3
We tried several voting strategies but none performed bet-
ter.
259
information predicted by another individual
model(DeSR
left?to?right
). To the best of our
knowledge, combining individual parsing models
that are inter-dependent is novel.
3 Semantic Role Labeling
We implement the Semantic Role Labeling (SRL)
problem using three components: PIC, AIC, and
reranking of predicted argument frames.
3.1 Predicate Identification and Classification
The PIC component carries out the identification
of predicates, as well as their partial disambigua-
tion, and it is implemented as a multiclass average
Perceptron classifier (Crammer & Singer, 2003).
For each token i we extract the following features
(?, ? stands for token combination):
1. SPLIT LEMMA: from ?i?1, i?, i?1, i, i+1, ?i, i+1?
2. SPLIT FORM: from i? 2, i? 1, i, i+ 1.i+ 2
3. PPOSS: from ?i?2, i?1?, ?i?1, i?, i?1, i, i+1, ?i, i+
1?, ?i+ 1, i+ 2?
4. WORD SHAPE: e.g., ?Xx*? for ?Brazil?, from ?i?2, i?
1, i?, ?i? 1, i?, i? 1, i, i+1, ?i, i+1?, ?i, i+1, i+2?
5. Number of children of node i
6. For each children j of i: split lemma
j
, pposs
j
,
deprel
i,j
, ?split lemma
i
, split lemma
j
?, ?pposs
i
,
pposs
j
?
7. Difference of positions: j ? i, for each child j of i.
The PIC component uses one single classifier map-
ping tokens to one of 8 classes corresponding to
the rolesets suffixes 1 to 6, the 6 most frequent
types, plus a class grouping all other rolesets, and
a class for non predicates; i.e., Y = {0, 1, 2, .., 7}.
Each token classified as y
7
is mapped by default to
the first sense y
1
. This approach is capable of dis-
tinguishing between different predicates based on
features 1 and 2, but it can also exploit information
that is shared between predicates due to similar
frame structures. The latter property is intuitively
useful especially for low-frequency predicates.
The classifier has an accuracy in the multiclass
problem, considering also the mistakes due to the
non-predicted classes, of 96.2%, and an F-score of
92.7% with respect to the binary predicate iden-
tification problem. To extract features from trees
(5-7) we use our parser?s output on training, devel-
opment and evaluation data.
3.2 Argument Identification and
Classification
Algorithm 1 describes our AIC framework. The al-
gorithm receives as input a sentence S where pred-
icates have been identified and classified using the
Algorithm 1: AIC
input : sentence S; inference strategy I; model w
foreach predicate p in S do
set frame F
in
= {}
foreach token i in S do
if validCandidate(i) then
?
y = arg max
y?Y
score(?(p, i),w, y)
if
?
y 6= nil then
add argument (i,
?
y) to F
in
F
out
= inference(F
in
, I)
output: set of all frames F
out
PIC component, an inference strategy I is used
to guarantee that the generated best frames satisfy
the domain constraints, plus an AIC classification
model w. We learn w using a multiclass Percep-
tron, using as output label setY all argument labels
that appear more than 10 times in training plus a nil
label assigned to all other tokens.
During both training and evaluation we se-
lect only the candidate tokens that pass the
validCandidate filter. This function requires that
the length of the dependency path between pred-
icate and candidate argument be less than 6, the
length of the dependency path between argument
and the first common ancestor be less than 3, and
the length of the dependency path between the
predicate and the first common ancestor be less
than 5. This heuristic covers over 98% of the ar-
guments in training.
In the worst case, Algorithm 1 has quadratic
complexity in the sentence size. But, on average,
the algorithm has linear time complexity because
the number of predicates per sentence is small (av-
eraging less than five for sentences of 25 words).
The function ? generates the feature vector for
a given predicate-argument tuple. ? extracts the
following features from a given tuple of a predicate
p and argument a:
1. token(a)
4
, token(modifier of a) if a is the
head of a prepositional phrase, and token(p).
2. Patterns of PPOSS tags and DEPREL labels
for: (a) the predicate children, (b) the children
of the predicate ancestor across VC and IM
dependencies, and (c) the siblings of the same
ancestor. In all paths we mark the position of
p, a and any of their ancestors.
3. The dependency path between p and a. We
add three versions of this feature: just the
4
token extracts the split lemma, split form, and PPOSS
tag of a given token.
260
path, and the path prefixed with p and a?s
PPOSS tags or split lemmas.
4. Length of the dependency path.
5. Distance in tokens between p and a.
6. Position of a relative to p: before or after.
We implemented two inference strategies:
greedy and reranking. The greedy strategy sorts
all arguments in a frame F
in
in descending order
of their scores and iteratively adds each argument
to the output frame F
out
only if it respects the do-
main constraints with the other arguments already
selected. The only domain constraint we use is that
core arguments cannot repeat.
3.3 Reranking of Argument Frames
The reranking inference strategy adapts the ap-
proach of Toutanova et al (2005) to the depen-
dency representation with notable changes in can-
didate selection, feature set, and learning model.
For candidate selection we modify Algorithm 1:
instead of storing only y? for each argument in F
in
we store the top k best labels. Then, from the ar-
guments in F
in
, we generate the top k frames with
the highest score, where the score of a frame is the
product of all its argument probabilities, computed
as the softmax function on the output of the Per-
ceptron. In this set of candidate frames we mark
the frame with the highest F
1
score as the positive
example and all others as negative examples.
From each frame we extract these features:
1. Position of the frame in the set ordered by
frame scores. Hence, smaller positions in-
dicate candidate frames that the local model
considered better (Marquez et al, 2007).
2. The complete sequence of arguments and
predicate for this frame (Toutanova, 2005).
We add four variants of this feature: just the
sequence and sequence expanded with: (a)
predicate voice, (b) predicate split lemma,
and (c) combination of voice and split lemma.
3. The complete sequence of arguments and
predicate for this frame combined with their
PPOSS tags. Same as above, we add four
variants of this feature.
4. Overlap with the PropBank or NomBank
frame for the same predicate lemma and
sense. We add the precision, recall, and F
1
score of the overlap as features (Marquez et
al., 2007).
5. For each frame argument, we add the features
from the local AIC model prefixed with the
WSJ + Brown WSJ Brown
Labeled macro F
1
82.69 83.83 73.51
LAS 87.37 88.21 80.60
Labeled F
1
78.00 79.43 66.41
Table 1: DeSRL results in the closed challenge,
for the overall task, syntactic dependencies, and
semantic dependencies.
Devel WSJ Brown
DeSR
left?to?right
85.61 86.54 79.74
DeSR
revision
right?to?left
85.81 86.19 78.91
MaltParser 84.10 85.50 77.06
Voting 87.37 88.21 80.60
Table 2: LAS of individual and combined parsers.
corresponding argument label in the current
frame (Toutanova, 2005).
The reranking classifier is implemented as multi-
layer perceptron with one hidden layer of 5 units,
trained to solve a regression problem with a least
square criterion function. Previously we experi-
mented, unsuccessfully, with a multiclass Percep-
tron and a ranking Perceptron. The limited number
of hidden units guarantees a small computational
overhead with respect to a linear model.
4 Results and Analysis
Table 1 shows the overall results of our system
in the closed challenge. Note that these scores
are higher than those of our submitted run mainly
due to improved parsing models (discussed be-
low) whose training ended after the deadline. The
score of the submitted system is the third best
for the complete task. The system throughput in
our best configuration is 28 words/second, or 30
words/second without reranking. In exploratory
experiments on feature selection for the re-ranking
model we found that several features classes do
not contribute anything and could be filtered out
speeding up significantly this last SRL step. Note
however that currently over 90% of the runtime is
occupied by the syntactic parsers? SVM classifiers.
We estimate that we can increase throughput one
order of magnitude simply by switching to a faster,
multiclass classifier in parsing.
4.1 Analysis of Parsing
Table 2 lists the labeled attachment scores (LAS)
achieved by each parser and by their combination
on the development set, the WSJ and Brown test
sets. The results are improved with respect to the
official run, by using a revision parser trained on
the output of the lower accuracy ME parser, as
261
Labeled F
1
Unlabeled F
1
Syntax PIC Inference Devel WSJ Brown Devel WSJ Brown
gold gold greedy 88.95 90.21 84.95 93.71 94.34 93.29
predicted gold greedy 85.96 86.70 78.68 90.60 90.98 88.02
predicted predicted greedy 79.88 79.27 66.41 86.07 85.33 80.14
predicted predicted reranking 80.13 79.43 66.41 86.33 85.62 80.41
Table 3: Scores of the SRL component under various configurations.
Devel WSJ Brown
Unlabeled F
1
92.69 90.88 86.96
Labeled F
1
(PIC) 87.29 84.87 71.99
Labeled F
1
(Sense 1) 79.62 78.94 70.11
Table 4: Scores of the PIC component.
mentioned earlier. These results show that vot-
ing helps significantly (+1.56% over the best single
parser) even though inter-dependent models were
used. However, our simple voting scheme does
not guarantee that a well-formed tree is generated,
leaving room for further improvements; e.g., as
in (Sagae & Lavie, 2006).
4.2 Analysis of SRL
Table 3 shows the labeled and unlabeled F
1
scores
of our SRL component as we move from gold to
predicted information for syntax and PIC. For the
shared task setting ?predicted syntax and predicted
PIC? we show results for the two inference strate-
gies implemented: greedy and reranking. The first
line in the table indicates that the performance of
the SRL component when using gold syntax and
gold PIC is good: the labeled F
1
is 90 points for the
in-domain corpus and approximately 85 points for
the out-of-domain corpus. Argument classification
suffers the most on out-of-domain input: there is
a difference of 5 points between the labeled scores
on WSJ and Brown, even though the correspond-
ing unlabeled scores are comparable.
The second line in the table replicates the setup
of the 2005 CoNLL shared task: predicted syntax
but gold PIC. This yields a moderate drop of 3 la-
beled F
1
points on in-domain data and a larger drop
of 6 points for out-of-domain data.
We see larger drops when switching to predicted
PIC (line 3): 5-6 labeled F
1
points in domain and
12 points out of domain. This drop is caused by the
PIC component, e.g., if a predicate is missed the
whole frame is lost. Table 4 lists the scores of our
PIC component, which we compare with a base-
line system that assigns sense 1 to all identified
predicates. The table indicates that, even though
our disambiguation component improves signifi-
cantly over the baseline, it performs poorly, espe-
cially on out-of-domain data. Same as SRL, the
classification sub-task suffers the most out of do-
main (there is a difference of 15 points between
unlabeled and labeled F
1
scores on Brown).
Finally, the reranking inference strategy yields
only modest improvements (last line in Table 3).
We attribute these results to the fact that, unlike
Toutanova et al (2005), we use only one tree to
generate frame candidates, hence the variation in
the candidate frames is small. Considering that the
processing overhead of reranking is already large
(it quadruples the runtime of our AIC component),
we do not consider reranking a practical extension
to a SRL system when processing speed is a dom-
inant requirement.
References
G. Attardi. 2006. Experiments with a Multilan-
guage Non-Projective Dependency Parser. In Proc.
of CoNNL-X 2006.
G. Attardi and M. Ciaramita. 2007. Tree Revi-
sion Learning for Dependency Parsing. In Proc. of
NAACL/HLTC 2007.
G. Attardi, F. Dell?Orletta. 2008. Chunking and De-
pendency Parsing. In Proc. of Workshop on Partial
Parsing.
K. Crammer and Y. Singer. 2003. Ultraconservative
Online Algorithms for Multiclass Problems. Journal
of Machine Learning Research 3: pp.951-991.
F. Dell?Orletta. 2008. Improving the Accuracy of De-
pendency Parsing. PhD Thesis. Dipartimento di In-
formatica, Universit`a di Pisa, forthcoming.
K. Hall and V. Novak. 2005. Corrective Modeling
for Non-Projective Dependency Parsing. In Proc. of
IWPT.
L. Marquez, L. Padro, M. Surdeanu, and L. Villarejo.
2007. UPC: Experiments with Joint Learning within
SemEval Task 9. In Proc. of SemEval 2007.
K. Sagae and A. Lavie. 2006. Parser Combination by
reparsing. In Proc. of HLT/NAACL.
M. Surdeanu, R. Johansson, A. Meyers, L. M`arquez
and J. Nivre. 2008. The CoNLL-2008 Shared Task
on Joint Parsing of Syntactic and Semantic Depen-
dencies. In Proc. of CoNLL-2008.
K. Toutanova, A. Haghighi, and C. Manning. 2005.
Joint Learning Improves Semantic Role Labeling. In
Proc. of ACL.
262
Proceedings of SSST-5, Fifth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 79?87,
ACL HLT 2011, Portland, Oregon, USA, June 2011. c?2011 Association for Computational Linguistics
A Dependency Based Statistical Translation Model 
 
Giuseppe Attardi 
 
Universit? di Pisa 
Dipartimento di Informatica 
attardi@di.unipi.it 
Atanas Chanev 
 
Universit? di Pisa 
Dipartimento di Informatica 
chanev@di.unipi.it 
Antonio Valerio Miceli Barone 
 
Universit? di Pisa 
Dipartimento di Informatica 
miceli@di.unipi.it 
 
 
Abstract 
We present a translation model based on 
dependency trees. The model adopts a tree-
to-string approach and extends Phrase-
Based translation (PBT) by using the de-
pendency tree of the source sentence for 
selecting translation options and for reor-
dering them. Decoding is done by translat-
ing each node in the tree and combining its 
translations with those of its head in alter-
native orders with respect to its siblings. 
Reordering of the siblings exploits a heu-
ristic based on the syntactic information 
from the parse tree which is learned from 
the corpus. The decoder uses the same 
phrase tables produced by a PBT system 
for looking up translations of single words 
or of partial sub-trees. A mathematical 
model is presented and experimental re-
sults are discussed.  
1 Introduction 
Several efforts are being made to incorporate syn-
tactic analysis into phrase-base statistical transla-
tion (PBT) (Och 2002; Koehn et. al. 2003), which 
represents the state of the art in terms of robust-
ness in modeling local word reordering and effi-
ciency in decoding. Syntactic analysis is meant to 
improve some of the pitfalls of PBT: 
 Translation options selection: candidate phrases 
for translation are selected as consecutive n-
grams. This may miss to consider certain syn-
tactic phrases if their component words are far 
apart. 
 Phrase reordering: especially for languages 
with different word order, e.g. subject-verb-
object (SVO) and subject-object-verb (SVO) 
languages, long distance reordering is a prob-
lem. This has been addressed with a distance 
based distortion model (Och 2002; Koehn et al 
2003), lexicalized phrase reordering (Tillmann, 
2004; Koehn, et.al., 2005; Al-Onaizan and Pa-
pineni, 2006), by hierarchical phrase reordering 
model (Galley and Manning, 2008) or by reor-
dering the nodes in a dependency tree (Xu et 
al., 2009) 
 Movement of translations of fertile words: a 
word with fertility higher than one can be trans-
lated into several words that do not occur con-
secutively. For example, the Italian sentence 
?Lui partir? domani? translates into German as 
?Er wird morgen abreisen?. The Italian word 
?partir?? (meaning ?will leave?) translates into 
?wird gehen? in German, but the infinite ?ab-
reisen? goes to the end of the sentence with a 
movement that might be quite long. 
Reordering of phrases is necessary because of dif-
ferent word order typologies of languages: consti-
tuent word order like SOV for Hindi vs. SVO for 
English; order of modifiers like noun?adjective for 
French, Italian vs. adjective-noun in English. Xu et 
al. (2009) tackle this issue by introducing a reor-
dering approach based on manual rules that are 
applied to the parse tree produced by a dependen-
cy parser. 
However the splitting phenomenon mentioned 
above requires more elaborate solutions than sim-
ple reordering grammatical rules. 
Several schemes have been proposed for im-
proving PBMT systems based on dependency 
trees.  Our approach extends basic PBT as de-
79
scribed in (Koehn et. al., 2003) with the following 
differences: 
 we perform tree-to-string translation. The de-
pendency tree of the source language sentence 
allows identifying syntactically meaningful 
phrases as translation options, instead of n-
grams. However these phrases are then still 
looked up in a Phrase Translation Table (PT) 
quite similarly to PBT. Thus we avoid the 
sparseness problem that other methods based 
on treelets suffer (Quirk et al, 2005). 
 reordering of phrases is carried out traversing 
the dependency tree and selecting as options 
phrases that are children of each head. Hence a 
far away but logically connected portion of a 
phrase can be included in the reordering. 
 phrase combination is performed by combining 
the translations of a node with those of its head. 
Hence only phrases that have a syntactic rela-
tion are connected. The Language Model (LM) 
is still consulted to ensure that the combination 
is proper, and the overall score of each transla-
tion is carried along.  
 when all the links in the parse tree have been 
reduced, the root node contains candidate trans-
lations for the whole sentences 
 alternative visit orderings of the tree may pro-
duce different translations so the final transla-
tion is the one with the highest score. 
Some of the benefits of our approach include: 
1) reordering is based on syntactic phrases rather 
than arbitrary chunks 
2) computing the future cost estimation can be 
avoided, since the risk of choosing an easier n-
gram is mitigated by the fact that phrases are 
chosen according to the dependency tree 
3) since we are translating from tree to string, we 
can directly exploit the standard phrase tables 
produced by PBT tools such as giza++ (Och 
and Ney, 2000) and Moses (Koehn, 2007) 
4) integration with the parser: decoding can be 
performed incrementally while a dependency 
Shift/Reduce parser builds the parse tree (At-
tardi, 2006). 
2 The  Dependency Based Decoder 
We describe in more detail the approach by pre-
senting a simple example. 
The translation of an input sentence is generated 
by reducing the dependency tree one link at a time, 
i.e. merging one node with its parent and combin-
ing their translations, until a single node remains. 
Links must be chosen in an order that preserves 
the connectivity of the dependency tree. Since 
there is a one-to-one correspondence between 
links and nodes (i.e. the link between a node and 
its head), we can use any ordering that corres-
ponds to a topological ordering of the nodes of the 
tree. 
A sentence is a sequence of words (w1, ? , wn), 
so we can use their index to identify words and 
hence each ordering is a permutation of those in-
dexes. 
Consider for example the dependency tree for 
the Italian sentence: Il ragazzo alto (?The tall 
boy?). 
 
There are only two possible topological orderings 
for this tree: 1-3-2 and 3-1-2.  
In principle the decoding process should ex-
plore all possible topological orderings for gene-
rating translations, but their number is too big, 
being proportional to the factorial of the number of 
words, so we will introduce later a criterion for 
selecting a subset of these, which conform best 
with the rules of the languages. 
Given a permutation we obtain a translation by 
merging in that order each node with its parent. 
The initialization step of the decoder creates 
nodes corresponding to the parse tree and collects 
translations for each individual word from the PT. 
 
ragazzo 
boy 
 
alto 
tall 
high 
Il 
The 
Il   ragazzo   alto 
80
Case 1: Permutation 1-3-2 
The first merge step is applied to the nodes for w1 
and its head w2, performing the concatenation of 
the translations of nodes il (the) and ragazzo (boy), 
both in normal and reverse order. Hence expansion 
of this hypothesis reduces the tree to the follow-
ing, where we show also the partial translations 
associated to each node. Each translation has asso-
ciated weights (i.e. the LM weight, the translation 
model weight, etc.) and a cumulative score. The 
score is the dot product of the weights for the sen-
tence and the vector of tuning parameters for the 
model. The score is used to rank the sentences and 
also to limit how many of them are kept according 
to the beam size parameter of the algorithm. 
 
The second step merges the node for word w3 (?al-
to?) with that of its head w2 (?ragazzo?) producing 
a single node with four translations: ?the boy tall?, 
?boy the tall?, ?tall the boy? and ?tall boy the?. 
 
Case 2: Permutation 3-1-2 
The first merge between w3 and w2 generates two 
translation fragments: ?boy tall? and ?tall boy?. 
The second one creates four translations: ?the boy 
tall?, ?boy tall the?, ?the tall boy?, ?tall boy the?. 
 
When the tree has been reduced to a single root 
node and the results of both permutations are col-
lected, the node will contain all eight alternative 
translations ranked according to the language 
model, so that the best one, possibly ?the tall boy?, 
can be selected as overall sentence translation. 
3 Node Merge 
The operation of node merge consists of taking all 
possible translations for the two nodes and conca-
tenating them in either sequential or reverse order, 
adding them to the translation of the parent node 
and dropping the child. 
In certain cases though, for example idiomatic 
phrases, the best translation is not obtained by 
combining the individual translations of each 
word, but instead a proper translation might be 
found in the Phrase Translation Table (PT). Hence 
besides performing combination of translations, 
we also consider the sub-tree rooted at the head 
node hri of node ri. We consider the phrase corres-
ponding to the leaves of the sub-tree rooted at hri 
and all children already merged into it, including 
ri: if this phrase is present in the PT, then its trans-
lations are also added to the node. 
This is sometimes useful, since it allows the de-
coder to exploit phrases that only correspond to 
partial sub-trees that it will otherwise miss. 
4 Reordering Rules 
In order to restrict the number of permutations to 
consider, we introduce a reordering step based on 
rules that examine the dependency tree of the 
source sentence. 
The rules are dependent on the language pair 
and they can be learned automatically from the 
corpus. 
We report first a simple set of hand crafted rules 
devised for the pair Italian-English that we used as 
a baseline. 
The default ordering is to start numbering the 
left children of a node backwards, i.e. the node 
closer to the head comes first, then continuing 
with the right children in sequential order. 
Special rules handle these cases: 
1) The head is a verb: move an adverb child to 
first position.  This lets a sequence of VA VM 
V R be turned into VA VM R V, where VA is 
the POS for auxiliary verbs, VM for modals, 
V for main verb and R for adverbs. 
2) The head is a noun: move adjectives or prepo-
sitions immediately following the head to the 
beginning. 
Il ragazzo alto 
the boy tall 
boy the tall 
tall the boy 
tall boy the 
Il ragazzo 
the boy 
boy the 
alto 
tall 
high 
81
4.1 Learning Reordering Rules 
In order to learn the reordering rules we created a 
word-aligned parallel corpus from 1.3 million 
source sentences selected from the parallel corpus. 
The corpus is parsed and each parse tree is ana-
lyzed using the giza++ word alignments of its 
translation to figure out node movements. 
For each source-language word, we estimate a 
unique alignment to a target-language word. If the 
source word is aligned to more than one target 
word we select the first one appearing in the 
alignment file. If a source word is not aligned to 
any word, we choose the first alignment in its des-
cendants in the dependency tree. If no alignment 
can be found in the descendants, we assume that 
the word stays in its original position. 
We reorder the source sentence according to 
this alignment, putting it in target-language order. 
We produce a training event consisting of a pair 
(context, offset) for each non-root word. The con-
text of the event consists of a set of features (the 
POS tag of a word, its dependency tag and the 
POS of its head) extracted for the word and its 
children. The outcome of the event is the offset of 
the word relative to its parent (negative for words 
that appear on the left of their parent in target-
language order, positive otherwise). 
We calculate the relative frequency of each 
event conditioned on the context, deriving rules of 
the form: 
(context, offset, Pr[Offset = offset | Context = 
context]). 
During decoding, we compute a reordering posi-
tion for each source word by adding to the word 
position to the offset predicted by the most likely 
reordering rule matching the word context (or 0 if 
no matching context is found). 
The reordering position drives the children 
combination procedure in the decoder. 
Our reordering rules are similar to those pro-
posed by Xu at al. (2009), except that we derive 
them automatically from the training set, rather 
than being hand-coded. 
4.2 Beam Search 
Search through the space of hypotheses generated 
is performed using beam search that keeps in each 
node the list of the top best translations for the 
node. The score for the translation is computed 
using the weights of the individual phrases that 
make up the translation and the overall LM proba-
bility of the combination. 
The scores are computed querying the standard 
Moses Phrase Table and the LM for the target lan-
guage; other weights uses by moses such as the 
reordering weights or the future cost estimates are 
discarded or not computed. 
5 The Model 
A mathematical model of the dependency based 
translation process can be formulated as follows. 
Consider the parse of a sentence f of length n. 
Let R denote all topological ordering of the nodes 
according to the dependency tree. 
Let fr denote the parse tree along with a consis-
tent node ordering r. Each ordering gives rise to 
several different translations. Let Er denote the set 
of translations corresponding to fr. We assign to 
each translation er  Er a probability according to 
the formula below. The final translation is the best 
result obtained through combinations over all or-
derings. 
Error! Objects cannot be created from editing field 
codes. 
Where er denotes any of the translations of f ob-
tained when nodes are combined according to 
node ordering r.  
The probability of a translation er corresponding 
to a node ordering r for a phrase f, p(er | f ) is de-
fined as: 
Error! Objects cannot be created from editing field 
codes. 
where 
Error! Objects cannot be created from editing 
field codes. andError! Objects cannot be 
created from editing field codes.denote the leaf 
words from node ri and those of its head node hri,  
respectively. 
Error! Objects cannot be created from edit-
ing field codes.is either Error! Objects cannot 
82
be created from editing field codes.or Error! 
Objects cannot be created from editing field 
codes. 
p(f, e) = pPT(str(f), e) if str(f)  PT 
str(f) is the sentence at the leaves of node ri 
pLM is the Language Model probability 
pPT is the Phrase Table probability 
6 Related Work 
Yamada and Knight (2001) introduced a syntax-
based translation model that incorporated source-
language syntactic knowledge within statistical 
translation. Many similar approaches are based on 
constituent grammars, among which we mention 
(Chiang, 2005) who introduced hierarchical trans-
lation models. 
The earliest approach based on dependency 
grammars is the work by Ashlawi et al (2000), 
who developed a tree-to-tree translation model, 
based on middle-out string transduction capable of 
phrase reordering. It translated transcribed spoken 
utterances from English to Spanish and from Eng-
lish to Japanese. Improvements were reported over 
a word-for-word baseline. 
Ambati (2008) presents a survey of other ap-
proaches based on dependency trees. 
Quirk et. al. (2005) explore a tree-to-tree ap-
proach, called treelet translation, that extracts tree-
lets, i.e. sub-trees, from both source and target 
language by means of a dependency parser. A 
word aligner is used to align the parallel corpus. 
The source dependency is projected onto the target 
language sentence in order to extract treelet trans-
lation pairs. Given a foreign input sentence, their 
system first generates its dependency tree made of 
treelets. These treelets are translated into treelets 
of the target language, according to the dependen-
cy treelet translation model. Translated treelets are 
then reordered according to a reorder model. 
The ordering model is trained on the parallel 
corpus. Treelet translation pairs are used for de-
coding. The reordering is done at the treelet level 
where all the child nodes of a node are allowed all 
possible orders. The results show marginal im-
provements in the BLEU score (40.66) in compar-
ison with Pharaoh and MSR-MT.  But the treelet 
translation algorithm is more than an order of 
magnitude slower. 
Shen et. al. (2008) present a hierarchical ma-
chine translation method from string to trees. The 
scheme uses the dependency structure of the target 
language to use transfer rules while generating a 
translation. The scheme uses well-formed depen-
dency structure which involves fixed and floating 
type structures. The floating structures allow the 
translation scheme to perform different concatena-
tion, adjoining and unification operations still be-
ing within the definition of well-formed structures. 
While decoding the scheme uses the probability of 
a word being the root, and also the left-side, right-
side generative probabilities. The number of rules 
used varies from 27 M (for a string to dependency 
system) to 140 M (baseline system). The perfor-
mance reached 37.25% for the system with 3-
grams, 39.47% for 5-grams. 
Marcu and Wong (2002) propose a joint- prob-
ability model. The model establishes a correspon-
dence between a source phrase and a target phrase 
through some concept. The reordering is inte-
grated into the joint probability model with the 
help of: 
3) Phrase translation probabilities Error! Ob-
jects cannot be created from editing field 
codes. denoting the probability that concept ci 
generates the translation Error! Objects can-
not be created from editing field codes. for 
the English and Error! Objects cannot be 
created from editing field codes. for the for-
eign language inputs. 
4) Distortion probabilities based on absolute po-
sitions of the phrases.  
Decoding uses a hill-climbing algorithm.  Perfor-
mance wise the approach records an average 
BLEU score of 23.25%, with about 2% of im-
provement over the baseline IBM system. 
Zhang et. al. (2007) present a reordering model 
that uses linguistic knowledge to guide both 
phrase reordering and translation between linguis-
tically correct phrases by means of rules. Rules are 
encoded in the form of weighted synchronous 
grammar and express transformations on the parse 
trees. They experiment also mixing constituency 
and dependency trees achieving some improve-
83
ments in BLEU score (27.37%) over a baseline 
system (26.16%). 
Cherry (2008) introduces a cohesion feature in-
to a traditional phrase based decoder. It is imple-
mented as a soft constraint which is based on the 
dependency syntax of the source language. He 
reports a BLEU score improvement on French-
English translation. 
The work by Xu et al (2009) is the closest to 
our approach. They perform preprocessing of the 
foreign sentences by parsing them with a depen-
dency parser and applying a set of hand written 
rules to reorder the children of certain nodes. The 
preprocessing is applied to both the training cor-
pus and to the sentences to translate, hence after 
reordering a regular hierarchical system can be 
applied. Translation experiments between English 
and five non SVO Asian languages show signifi-
cant improvements in accuracy in 4 out of 5 lan-
guages. With respect to our approach the solution 
by Xu et al does not require any intervention on 
the translation tools, since the sentences are rewrit-
ten before being passed to the processing chain: on 
the other hand the whole collection has to undergo 
full parsing with higher performance costs and 
higher dependency on the accuracy of the parser. 
Dyer and Resnik (2010) introduce a translation 
model based on a Synchronous Context Free 
Grammar (SCFG). In their model, translation 
examples are stored as a context-free forest. The 
process of translation comprise two steps: tree-
based reordering and phrase transduction. While 
reordering is modeled with the context-free forest, 
the reordered source is transduced into the target 
language by a Finite State Transducer (FST). The 
implemented model is trained on those portions of 
the data which it is able to generate. An increase 
of BLEU score is achieved for Chinese-English 
when compared to the phrase based baseline. 
Our approach is a true tree-to-string model and 
differs from (Xu et al, 2009), which uses trees 
only as an intermediate representation to rearrange 
the original sentences. We perform parsing and 
reordering only on the phrases to be translated. 
The training collection is kept in the original form, 
and this has two benefits: training is not subject to 
parsing errors and our system can share the same 
model of a regular hierarchical system. 
Another difference is in the selection of transla-
tion options: our method exploits the parse tree to 
select grammatical phrases as translation options. 
7 Implementation 
The prototype decoder consists of the following 
components: 
1) A specialized table lookup server, providing 
an XML-RPC interface for querying both the 
phrase table and the LM 
2) A parser engine based on DeSR (DeSR, 2009) 
3) A reordering algorithm that adds ordering 
numbers to the output produced by DeSR in 
CoNLL-X format. Before reordering, this step 
also performs a restructuring of the parse tree, 
converting from the conventions of the Italian 
Tanl Treebank to a structure that helps the 
analysis. In particular it converts conjunctions, 
which are represented as chains, where each 
conjunct connects to the previous, to a tree 
where they are all dependent of the same head 
word. Compound verbs are also revised: in the 
dependency tree each auxiliary of a verb is a 
direct child of the main verb. For example in 
?avrebbe potuto vedere?, both the auxiliary 
?avrebbe? and the modal ?potuto? depend on 
the verb ?vedere?.  This steps groups all aux-
iliaries of a verb under the first one, i.e. ?potu-
to?. This helps so that the full auxiliary can be 
looked up separately from the verb in the 
phrase table. 
4) A decoder that uses the output produced by 
the reordering algorithm, queries the phrase 
table and performs a beam search on the hypo-
theses produced according to the suggested 
reordering. 
8 Experimental Setup and Results 
Moses (Koehn et al, 2007) is used as a baseline 
phrase-based SMT system. The following tools 
and data were used in our experiments:  
1) the IRSTLM toolkit (Marcello and Cettolo, 
2007) is used to train a 5-gram language mod-
84
el with Kneser-Ney smoothing on a set of 4.5 
million sentences from the Italian Wikipedia. 
2) the Europarl version 6 corpus, consisting of 
1,703,886 sentence pairs, is used for training. 
A tuning set of 2000 sentences from ACL 
WMT 2007 is used to tune the parameters.  
3) the model is trained with lexical reordering. 
4) the model is tuned with mert (Bertoldi, et al ) 
5) the official test set from ACL WMT 2008 
(Callison-Burch et al, 2008), consisting of 
2000 sentences, is used as test set. 
6) the open-source parser DeSR (DeSR, 2009) is 
used to parse Italian sentences, trained on the 
Evalita 2009 corpus (Bosco et al, 2009). Pars-
er domain adaptation is obtained by adding to 
this corpus a set of 1200 sentences from the 
ACL WMT 2005 test set, parsed by DeSR and 
then corrected by hand. 
Both the training corpora and the test set had to be 
cleaned in order to normalize tokens: for example 
the English versions contained possessives split 
like this ?Florence' s?. We applied the same toke-
nizer used by the parser which conforms to the 
PTB standard. 
DeSR achieved a Labeled Accuracy Score of 
88.67% at Evalita 2009, but for the purpose of 
translation, just the Unlabeled Accuracy is rele-
vant, which was 92.72%. 
The table below shows the results of our decod-
er (Desrt) in the translation from Italian to English, 
compared to a baseline Moses system trained on 
the same corpora and to the online version of 
Google translate. 
Desrt was run with a beam size of 10, since ex-
periments showed no improvements with a larger 
beam size. 
We show two versions of Desrt, one with parse 
trees as obtained by the parser and one (Desrt 
gold) where the trees were corrected by hand. The 
difference is minor and this confirms that the de-
coder is robust and not much affected by parsing 
errors. 
System BLEU NIST 
Moses 29.43 7.22 
Moses tree phrases 28.55 7.10 
Desrt gold 26.26 6.88 
Desrt 26.08 6.86 
Google Translate 24.96 6.86 
Desrt learned 24.37 6.76 
Table 1. Results of the experiments. 
Since we used the same phrase table produced by 
Moses also for Desrt, Moses has an advantage, 
because it can look up n-grams that do not corres-
pond to grammatical phrases, which Desrt never 
considers. In order to determine how this affects 
the results, we tested Moses restricting its choice 
to phrases corresponding to treelets form the parse 
tree. The result is shown in the row in the table 
labeled as ?Moses tree phrases?. The score is low-
er, as expected, but this confirms that Desrt makes 
quite good use of the portion of the phrase table it 
uses. 
Since the version of the reordering algorithm we 
used produces a single reordering, the Desrt de-
coder has linear complexity on the length of the 
sentence. Indeed, despite being written in Python 
and having to query the PT as a network service, it 
is quite faster than Moses.  
9 Error Analysis 
Despite that fact that Desrt is driven by the parse 
tree, it is capable of selecting fairly good and even 
long sentences for look up in the phrase table. 
How close is the Desrt translation from those of 
the Moses baseline can be seen from this table: 
 1-gram 2-gram 3-gram 4-gram 5-gram 
NIST 7.28 3.05 1.0 0.27 0.09 
BLEU 84.73 67.69 56.94 48.59 41.78 
Sometimes Desrt fails to select a better translation 
for a verb, since it looks up prepositional phrases 
separately from the verb, while Moses often con-
nects the preposition to the verb. 
This could be improved by performing a check 
and scoring higher translations which include the 
translation of the preposition dependent on the 
verb. 
Another improvement could come from creating 
phrase tables limited to treelet phrases, i.e. phrases 
corresponding to treelets from the parser. 
85
10 Enhancements 
The current algorithm needs to be improved to 
fully deal with certain aspects of long distance 
dependencies. Consider for example the sentence 
?The grass around the house is wet?. The depen-
dency tree of the sentence contains the non-
contiguous phrases ?The grass? and ?wet?, whose 
Italian translation must obey a morphological 
gender agreement between the subject ?grass? 
(?erba?, feminine), and the adjective ?wet? (?bag-
nata?). 
However, the current combination algorithm 
does not exploit this dependence, because the last 
phases of node merge will occur when the tree has 
been reduced to this: 
The PT however could tell us that ?erba bagnata? 
is more likely than ?erba bagnato? and allow us to 
score the former higher. 
11 Conclusions 
We have described a decoding algorithm guided 
by the dependency tree of the source sentence. By 
exploiting the dependency tree and deterministic 
reordering rules among the children of a node, the 
decoder is fast and can be kept simple by avoiding 
to consider multiple reorderings, to use reordering 
weights and to estimate future costs. 
There is still potential for improving the algo-
rithm exploiting information implicit in the PT in 
terms of morphological constraints, while main-
taining a simple decoding algorithm that does not 
involve complex grammatical transformation 
rules. 
The experiments show encouraging results with 
respect to state of the art PBT systems. We plan to 
test the system on other language pairs to see how 
it generalizes to other situations where phrase 
reordering is relevant. 
Acknowledgments 
Zauhrul Islam helped setting up our baseline sys-
tem and Niladri Chatterjie participated in the early 
design of the model.  
References 
G. Attardi. 2006. Experiments with a Multilanguage 
Non-Projective Dependency Parser. Proc. of the 
Tenth Conference on Natural Language Learning, 
New York, (NY). 
H. Alshawi, S. Douglas and S. Bangalore. 2000. 
Learning Dependency Translation Models as 
Collections of Finite State Head Transducers. 
Computational Linguistics 26(1), 45?60. 
N. Bertoldi, B. Haddow, J-B. Fouet. 2009. Improved 
Minimum Error Rate Training in Moses. In Proc. of 
3rd MT Marathon, Prague, Czech Republic. 
V. Ambati. 2008. Dependency Structure Trees in Syn-
tax Based Machine Translation. Adv. MT Seminar 
Course Report. 
C. Bosco, S. Montemagni, A. Mazzei, V. Lombardo, F. 
Dell?Orletta and A. Lenci. 2009. Evalita?09 Parsing 
Task: comparing dependency parsers and treebanks. 
Proc. of Evalita 2009. 
P. F. Brown, V. J. Della Pietra, S. A. and  R. L. Mercer. 
1993. The Mathematics of Statistical Machine Trans-
lation: Parameter Estimation. Computational Lin-
guistics, 19(2), 263?311. 
Callison-Burch et al 2008. Further Meta-Evaluation of 
Machine Translation. Proc. of ACL WMT 2008. 
C. Cherry. 2008. Cohesive phrase-based decoding for 
statistical machine translation. Proc. of ACL 2008: 
HLT. 
D. Chiang. 2005. A hierarchical phrase-based model for 
statistical machine translation. In Proc. of ACL 2005.  
DeSR. Dependency Shift Reduce parser. 
http://sourceforge.net/projects/desr/ 
Y. Ding, and M. Palmer. 2005.  Machine Translation 
using Probabilistic Synchronous Dependency Inser-
tion Grammar.  Proc. of ACL?05, 541?548. 
C. Dyer and P. Resnik. 2010. Context-free reordering, 
finite-state translation. Proc. of HLT: The 2010 
Annual Conference of the North American Chapter 
of the ACL, 858?866. 
grass 
L? erba intorno alla casa 
is 
? 
wet 
bagnata 
bagnato 
 
 
86
F. Marcello, M. Cettolo. 2007. Efficient Handling of N-
gram Language Models for Statistical Machine 
Translation. Workshop on Statistical Machine Trans-
lation 2007. 
M. Galley and C. D. Manning. 2008. A Simple and 
Effective Hierarchical Phrase Reordering Model. In 
Proc. of EMNLP 2008. 
R. Hwa, P. Resnik, A. Weinberg, C. Cabezas and O. 
Kolak, 2005.  Bootstrapping Parsers via Syntactic 
Projection across Parallel texts. Natural Language 
Engineering 11(3), 311-325. 
P. Koehn, F. J. Och and D. Marcu.  2003. Statistical 
Phrase-Based Translation. Proc. of Human Lan-
guage Technology and North American Association 
for Computational Linguistics Conference 
(HLT/NAACL), 127?133. 
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. 
Federico, N. Bertoldi, B. Cowan, W. Shen, C. Mo-
ran, R. Zens, C. Dyer, O. Bojar, A. Constantin, and 
E. Herbst. 2007. Moses: Open source toolkit for sta-
tistical machine translation. In Proc. of the 45th An-
nual Meeting of the ACL, demonstration session, 
177?180, Prague, Czech Republic. 
P. Koehn. 2009. Statistical Machine Translation.  
Cambridge University Press. 
Y. Liu, Q. Liu and S. Lin. 2006. Tree-to-string Align-
ment Template for Statistical Machine Translation, 
In Proc. of COLING-ACL. 
D. Marcu and W. Wong. 2002. A Phrase-Based Joint 
Probability Model for Statistical Machine Transla-
tion. Proc. Empirical Methods in Natural Language 
Processing (EMNLP), 133?139. 
C. Quirk, A. Menzes and C. Cherry. 2005. Dependency 
Treelet Translation: Syntactically Informed Phrasal 
SMT. Proc. 43rd Annual Meeting of the ACL, 217?
279. 
S. Libin, J. Xu and R. Weischedel. 2008. A New String-
to-Dependency Machine Translation Algorithm with 
a Target Dependency Language Model. Proc. ACL-
08, 577?585. 
F. J. Och 2002. Statistical Machine Translation: From 
Single Word Models to Alignment Template. Ph.D. 
Thesis, RWTH Aachen, Germany. 
F.J. Och, H. Ney. 2000. Improved Statistical Alignment 
Models. Proc. of the 38th Annual Meeting of the 
ACL.  Hong Kong, China. 440-447. 
K. Yamada and K. Knight. 2001. A Syntax-Based Sta-
tistical Translation Model. Proc. 39th Annual Meet-
ing of ACL (ACL-01), 6?11. 
P. Xu, J. Kang, M. Ringgaard and F. Och. 2009. Using 
a Dependency Parser to Improve SMT for Subject-
Object-Verb Languages. Proc. of NAACL 2009, 245?
253, Boulder, Colorado. 
D. Zhang, Mu Li, Chi-Ho Li and M. Zhou.  2007. 
Phrase Reordering Model Integrating Syntactic 
Knowledge for SMT.  Proc. Joint Conference on 
Empirical Methods in Natural Language Processing 
and Computational  Natural Language Processing: 
533?540. 
 
 
87
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 55?59,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Dependency Parsing domain adaptation using transductive SVM
Antonio Valerio Miceli-Barone
University of Pisa, Italy /
Largo B. Pontecorvo, 3, Pisa, Italy
miceli@di.unipi.it
Giuseppe Attardi
University of Pisa, Italy /
Largo B. Pontecorvo, 3, Pisa, Italy
attardi@di.unipi.it
Abstract
Dependency Parsing domain adaptation
involves adapting a dependency parser,
trained on an annotated corpus from a given
domain (e.g., newspaper articles), to work
on a different target domain (e.g., legal doc-
uments), given only an unannotated corpus
from the target domain.
We present a shift/reduce dependency
parser that can handle unlabeled sentences
in its training set using a transductive SVM
as its action selection classifier.
We illustrate the the experiments we per-
formed with this parser on a domain adap-
tation task for the Italian language.
1 Introduction
Dependency parsing is the task of identifying syn-
tactic relationships between words of a sentence
and labeling them according to their type. Typ-
ically, the dependency relationships are not de-
fined by an explicit grammar, rather implicitly
through a human-annotated corpus which is then
processed by a machine learning procedure, yield-
ing a parser trained on that corpus.
Shift-reduce parsers (Yamada and Matsumoto,
2003; Nivre and Scholz, 2004; Attardi, 2006) are
an accurate and efficient (linear complexity) ap-
proach to this task: They scan the words of a sen-
tence while updating an internal state by means of
shift-reduce actions selected by a classifier trained
on the annotated corpus.
Since the training corpora are made by human an-
notators, they are expensive to produce and are
typically only available for few domains that don?t
adequately cover the whole spectrum of the lan-
guage. Parsers typically lose significant accuracy
when applied on text from domains not covered
by their training corpus. Several techniques have
been proposed to adapt a parser to a new domain,
even when only unannotated samples from it are
available (Attardi et al, 2007a; Sagae and Tsujii,
2007).
In this work we present a domain adaptation based
on the semi-supervised training of the classifier of
a shift-reduce parser. We implement the classifier
as a multi-class SVM and train it with a transduc-
tive SVM algorithm that handles both labeled ex-
amples (generated from the source-domain anno-
tated corpus) and unlabeled examples (generated
from the the target-domain unannotated corpus).
2 Background
2.1 Shift-Reduce Parsing
A shift-reduce dependency parser is essentially a
pushdown automaton that scans the sentence one
token at a time in a fixed direction, while updat-
ing a stack of tokens and also updating a set of
directed, labeled edges that is eventually returned
as the dependency parse graph of the sentence.
Let T be the set of input token instances of
the sentence and D be the set of dependency
labels. The state of the parser is defined by
the tuple ?s, q, p?, where s ? T ? is the stack,
q ? T ? is the current token sequence and p ?
{
E|E ? 2T?T?D, E is a forest
}
is the current
parse graph.
The parser starts in the state ?[], q0, {}?, where q0
is the input sentence, and terminates whenever it
reaches a state in the form ?s, [], p?. At each step,
55
it performs one of the following actions:
shift :
?s, [t|q], p?
?[t|s], q, p?
rightreduced :
?[u|s], [t|q], p?
?s, [t|q], p ? {(u, t, d)}?
leftreduced :
?[u|s], [t|q], p?
?s, [u|q], p ? {(t, u, d)}?
note that there are rightreduced and
leftreduced actions for each label d ? D.
Action selection is done by the combination
of two functions f ? c : a feature extraction
function f : States ? Rn that computes a
(typically sparse) vector of numeric features of
the current state and the multi-class classifier
c : R ? Actions. Alternatively, the classifier
could score each available action, allowing a
search procedure such as best-first (Sagae and
Tsujii, 2007) or beam search to be used.
In our experiments we used an extension of
this approach that has an additional stack and
additional actions to handle non-projective de-
pendency relationships (Attardi, 2006). Training
is performed by computing, for each sentence
in the annotated training corpus, a sequence of
states and actions that generates its correct parse,
yielding, for each transition, a training example
(x, y) ? Rn ?Actions for the classifier.
Various classification algorithms have been
successfully used, including maximum entropy,
multi-layer perceptron, averaged perceptron,
SVM, etc. In our approach, the classifier is
always a multi-class SVM composed of multiple
(one-per-parsing-action) two-class SVMs in
one-versus-all configuration.
2.2 Parse Graph Revision
Attardi and Ciaramita (2007b) developed a
method for improving parsing accuracy using
parse graph revision: the output of the parser is
fed to a procedure that scans the parsed sentence
in a fixed direction and, at each step, possibly re-
vises the current node (rerouting or relabeling its
unique outgoing edge) based on the classifier?s
output.
Training is performed by parsing the training cor-
pus and comparing the outcome against the anno-
tation: for each sentence, a sequence of actions
necessary to transform the machine-generated
parse into the reference parse is computed and it
is used to train the classifier. (Usually, a lower-
quality parser is used during training, assuming
that it will generate more errors and hence more
revision opportunities).
This method tends to produce robust parsers: er-
rors in the first stage have the opportunity to be
corrected in the revision stage, thus, even if it
does not learn from unlabeled data, it neverthe-
less performs well in domain adaptation tasks (At-
tardi et al, 2007a). In our experiments we used
parse graph revision both as a baseline for accu-
racy comparison, and in conjunction with our ap-
proach (using a transductive SVM classifier in the
revision stage).
2.3 Transductive SVM
Transductive SVM (Vapnik, 1998) is a framework
for the semi-supervised training of SVM classi-
fiers.
Consider the inductive (completely supervised)
two-class SVM training problem: given a training
set {(xi, yi) |xi ? Rn, yi ? {?1, 1}}
L
i=1, find
the maximum margin separation hypersurface w ?
? (x) + b = 0 by solving the following optimiza-
tion problem:
arg min
w, b, ?
1
2
?w?22 + C
L?
i=1
?i (1)
?i : yiw ? ? (x) + b ? 1? ?i
?i : ?i ? 0
w ? Rm, b ? R
where C ? 0 is a regularization parameter and
?(?) is defined such that k (x, x?) ? ?(x) ? ? (x?)
is the SVM kernel function. This is a convex
quadratic programming problem that can be
solved efficiently by specialized algorithms.
Including an unlabeled example set
{
x?j |x
?
j ? R
n
}L?
j=1
we obtain the transduc-
tive SVM training problem:
arg min
w, b, ?, y?, xi?
1
2
?w?22 + C
L?
i=1
?i + C
?
L??
j=1
??j
(2)
56
?i : yiw ? ? (xi) + b ? 1? ?i
?j : y?j w ? ?
(
x?j
)
+ b ? 1? ??j
?i : ?i ? 0
?j : ??j ? 0
?j : y?j ? {?1, 1}
w ? Rm, b ? R
This formulation essentially models the unlabeled
examples the same way the labeled examples
are modeled, with the key difference that the
y?j (the unknown labels of the unlabeled exam-
ples) are optimization variables rather than pa-
rameters. Optimizing over these discrete variables
makes the problem non-convex and in fact NP-
hard. Nevertheless, algorithms that feasibly find
a local minimum that is typically good enough
for practical purposes do exist. In our exper-
iments we used the iterative transductive SVM
algorithm implemented in the SvmLight library
(Joachims, 1999). This algorithm tends to be-
come impractical when the number of unlabeled
examples is greater than a few thousands, hence
we were forced to use only a small portion on the
available target domain corpus. We also tried the
concave-convex procedure (CCCP) TSVM algo-
rithm (Collobert et al, 2006) as implemented by
the the Universvm package, and the multi-switch
and deterministic annealing algorithms for linear
TSVM (Sindhwani and Keerthi, 2007) as imple-
mented by the Svmlin package. These methods
are considerably faster but appear to be substan-
tially less accurate than SvmLight on our training
data.
3 Proposed approach
We present a semi-supervised training procedure
for shift/reduce SVM parsers that allows to in-
clude unannotated sentences in the training cor-
pus.
We randomly sample a small number (approx.
100) of sentences from the unannotated corpus
(the target domain corpus in a domain adaptation
task). For each of these sentences, we generate a
sequence of states that the parser may encounter
while scanning the sentence. For each state we
extract the features to generate an unlabeled train-
ing example for the SVM classifier which is in-
cluded in the training set alng with the labeled
examples generated from the annotated corpus.
There is a caveat here: the parser state at any given
point during the parsing of a sentence generally
depends on the actions taken before, but when we
are training on an unannotated sentence, we have
no way of knowing what actions the parser should
have taken, and thus the state we generate can be
generally incorrect. For this reason we evaluated
pre-parsing the unannotated sentences with a non-
transductively trained parser in order to generate
plausible state transitions while still adding unla-
beled examples. However, it turned out that this
pre-parsing does not seem to improve accuracy.
We conjecture that, because the classifier does not
see actual states but only features derived from
them, and many of these features are independent
of previous states and actions (features such as the
lemma and POS tag of the current token and its
neighbors have this property), these features con-
tain enough information to perform parsing.
The classifier is trained using the SvmLight trans-
ductive algorithm. Since SvmLight supports only
two-class SVMs while our classifier is multi-class
(one class for each possible parsing action), we
implement it in terms of two-class classifiers. We
chose the one-versus-all strategy:
We train a number of sub-classifiers equal to the
number of original classes. Each labeled training
example (x, y) is converted to the example (x, 1)
for the sub-classifier number y and to the example
(x, ?1) for the rest of sub-classifiers. Unlabeled
examples are just replicated to all sub-classifiers.
During classification the input example is eval-
uated by all the sub-classifiers and the one re-
turning the maximum SVM score determines the
class.
Our approach has been also applied to the second
stage of the revision parser, by presenting the fea-
tures of the unannotated sentences to the revision
classifier as unlabeled training examples.
4 Experiments
4.1 Experimental setup
We performed our experiments using the DeSR
parser (Attardi, 2006) on the data sets for the
Evalita 2011 dependency parsing domain adapta-
tion task for the Italian language (Evalita, 2011).
The data set consists in an annotated source-
domain corpus (newspaper articles) and an unan-
notated target-domain corpus (legal documents),
57
plus a small annotated development corpus also
from the target domain, which we used to evalu-
ate the performance.
We performed a number of runs of the DeSR
parser in various configurations, which differed
in the number and type of features extracted, the
sentence scanning direction, and whether or not
parse tree revision was enabled. The SVM clas-
sifiers always used a quadratic kernel. In order to
keep the running time of transductive SVM train-
ing acceptable, we limited the number of unanno-
tated sentences to one hundred, which resulted in
about 3200 unlabeled training examples fed to the
classifiers. The annotated sentences were 3275.
We performed one run with 500 unannotated sen-
tences and, at the cost of a greatly increased run-
ning time, the accuracy improvement was about
1%. We conjecture that a faster semi-supervised
training algorithm could allow greater perfor-
mance improvements by increasing the size of the
unannotated corpus that can be processed. All
the experiments were performed on a machine
equipped with an quad-core Intel Xeon X3440
processor (8M Cache, 2.53 GHz) and 12 Giga-
bytes of RAM.
4.2 Discussion
As it is evidenced from the table in figure
1, our approach typically outperforms the non-
transductive parser by about 1% of all the three
score measures we considered. While the im-
provement is small, it is consistent with differ-
ent configurations of the parser that don?t use
parse tree revision. Accuracy remained essen-
tially equal or became slightly worse in the two
configurations that use parse tree revision. This is
possibly due to the fact that the first stage parser of
the revision configurations uses a maximum en-
tropy classifier during training that does not learn
from the unlabeled examples.
These results suggest that unlabeled examples
contain information that can exploited to improve
the parser accuracy on a domain different than the
labeled set domain. However, the computational
cost of transductive learning algorithm we used
limits the amount of unlabeled data we can ex-
ploit.
This is consistent with the results obtained by
the self-training approaches, where a first parser
is trained on a the labeled set, which is used to
parse the unlabeled set which is then included into
the training set of a second parser. (In fact, self-
training is performed in the first step of the Svm-
Light TSVM algorithm).
Despite earlier negative results, (Sagae, 2010)
showed that even naive self-training can provide
accuracy benefits (about 2%) in domain adapta-
tion, although these results are not directly com-
parable to ours because they refer to constituency
parsing rather than dependency parsing. (Mc-
Closky et al, 2006) obtain even better results (5%
f-score gain) using a more sophisticated form of
self-training, involving n-best generative parsing
and discriminative reranking. (Sagae and Tsujii,
2007) obtain similar gains (about 3 %) for de-
pendency parsing domain adaptation, using self-
training on a subset of the target-domain instances
selected on the basis of agreement between two
different parsers. (the results are not directly com-
parable to ours because they were obtained on a
different corpus in a different language).
5 Conclusions and future work
We presented a semi-supervised training ap-
proach for shift/reduce SVM parsers and we illus-
trated an application to domain adaptation, with
small but mostly consistent accuracy gains. While
these gains may not be worthy enough to justify
the extra computational cost of the transductive
SVM algorithm (at least in the SvmLight imple-
mentation), they do point out that there exist a
significant amount of information in an unanno-
tated corpus that can be exploited for increasing
parser accuracy and performing domain adapta-
tion. We plan to further investigate this method by
exploring classifier algorithms other than trans-
ductive SVM and combinations with other semi-
supervised parsing approaches. We also plan to
test our method on standardized English-language
corpora to obtain results that are directly compa-
rable to those in the literature.
References
H. Yamada and Y. Matsumoto. 2003. Statistical De-
pendency Analysis with Support Vector Machines.
Proceedings of the 9th International Workshop on
Parsing Technologies.
J. Nivre and M. Scholz. 2004. Deterministic De-
pendency Parsing of English Text. Proceedings of
COLING 2004.
G. Attardi. 2006. Experiments with a Multilanguage
58
Figure 1: Experimental results
Accuracy (-R: right-to-left, -rev: left-to-right with revision, -rev2: right-to-left with revision):
Transductive Normal
Parser configuration LAS UAS Label only LAS UAS Label only
6 74.3 77.0 87.5 73.1 75.5 86.7
6-R 75.7 78.6 88.7 74.6 77.6 87.8
6-rev 75.2 78.2 88.6 75.1 78.0 88.3
6-rev2 75.0 77.8 88.7 75.8 78.6 88.7
8 74.3 77.0 87.3 73.4 76.0 85.9
8-R 75.7 78.6 88.7 75.3 78.3 88.1
2 74.7 77.4 87.4 73.1 75.8 86.5
Figure 2: Typical features (configuration 6).
Numbers denote offsets.
?FEATS? denotes rich morphological features (grammatical number, gender, etc).
LEMMA -2 -1 0 1 2 3 prev(0) leftChild(-1) leftChild(0) rightChild(-1) rightChild(0)
POSTAG -2 -1 0 1 2 3 next(-1) leftChild(-1) leftChild(0) rightChild(-1) rightChild(0)
CPOSTAG -1 0 1
FEATS -1 0 1
DEPREL leftChild(-1) leftChild(0) rightChild(-1)
Non-Projective Dependency Parser. Proceedings of
CoNNL-X 2006.
G. Attardi, A. Chanev, M. Ciaramita, F. Dell?Orletta
and M. Simi. 2007. Multilingual Dependency Pars-
ing and domain adaptation using DeSR. Proceed-
ings the CoNLL Shared Task Session of EMNLP-
CoNLL 2007, Prague, 2007.
Kenji Sagae and Jun?ichi Tsujii. 2007. Dependency
parsing and domain adaptation with LR models and
parser ensembles. CoNLL Shared Task.
G. Attardi, M. Ciaramita. 2007. Tree Revision Learn-
ing for Dependency Parsing. Proc. of the Human
Language Technology Conference 2007.
V. Vapnik. 1998. Statistical Learning Theory. Wiley.
Ronan Collobert and Fabian Sinz and Jason Weston
and Lon Bottou and Thorsten Joachims. 2006.
Large Scale Transductive SVMs. Journal of Ma-
chine Learning Research
Thorsten Joachims. 1999. Transductive Infer-
ence for Text Classification using Support Vector
Machines. International Conference on Machine
Learning (ICML), 1999.
Vikas Sindhwani and S. Sathiya Keerthi 2007. New-
ton Methods for Fast Solution of Semisupervised
Linear SVMs. Large Scale Kernel Machines. MIT
Press (Book Chapter), 2007
Kenji Sagae 2010. Self-Training without Reranking
for Parser Domain Adaptation and Its Impact on
Semantic Role Labeling. Proceedings of the 2010
Workshop on Domain Adaptation for Natural Lan-
guage Processing. Uppsala, Sweden: Association
for Computational Linguistics. p. 37-44
David McClosky, Eugene Charniak and Mark John-
son 2006. Reranking and self-training for parser
adaptation. Proceeding ACL-44. Proceedings of
the 21st International Conference on Computational
Linguistics and the 44th annual meeting of the As-
sociation for Computational Linguistics
Evalita. 2011. Domain Adaptation for Dependency
Parsing. .
59
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 164?169,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
Pre-reordering for machine translation using transition-based walks on
dependency parse trees
Antonio Valerio Miceli-Barone
Dipartimento di Informatica
Largo B. Pontecorvo, 3
56127 Pisa, Italy
miceli@di.unipi.it
Giuseppe Attardi
Dipartimento di Informatica
Largo B. Pontecorvo, 3
56127 Pisa, Italy
attardi@di.unipi.it
Abstract
We propose a pre-reordering scheme to
improve the quality of machine translation
by permuting the words of a source sen-
tence to a target-like order. This is accom-
plished as a transition-based system that
walks on the dependency parse tree of the
sentence and emits words in target-like or-
der, driven by a classifier trained on a par-
allel corpus. Our system is capable of gen-
erating arbitrary permutations up to flexi-
ble constraints determined by the choice of
the classifier algorithm and input features.
1 Introduction
The dominant paradigm in statistical machine
translation consists mainly of phrase-based sys-
tem such as Moses (Koehn et.al.,2007). Differ-
ent languages, however, often express the same
concepts in different idiomatic word orders, and
while phrase-based system can deal to some ex-
tent with short-distance word swaps that are cap-
tured by short segments, they typically perform
poorly on long-distance (more than four or five
words apart) reordering. In fact, according to
(Birch et.al., 2008), the amount of reordering be-
tween two languages is the most predictive feature
of phrase-based translation accuracy.
A number of approaches to deal with long-
distance reordering have been proposed. Since an
extuasive search of the permutation space is un-
feasible, these approaches typically constrain the
search space by leveraging syntactical structure of
natural languages.
In this work we consider approaches which in-
volve reordering the words of a source sentence
in a target-like order as a preprocessing step, be-
fore feeding it to a phrase-based decoder which
has itself been trained with a reordered training
set. These methods also try to leverage syntax,
typically by applying hand-coded or automatically
induced reordering rules to a constituency or de-
pendency parse of the source sentence. (Gal-
ley and Manning, 2008; Xu et.al., 2009; Genzel,
2010; Isozaki et.al., 2010) or by treating reorder-
ing as a global optimization problem (Tromble and
Eisner, 2009; Visweswariah et.al., 2011). In or-
der to keep the training and execution processes
tractable, these methods impose hard constrains
on the class of permutations they can generate.
We propose a pre-reordering method based on
a walk on the dependency parse tree of the source
sentence driven by a classifier trained on a parallel
corpus.
In principle, our system is capable of generat-
ing arbitrary permutations of the source sentence.
Practical implementations will necessarily limit
the available permutations, but these constraints
are not intrinsic to the model, rather they depend
on the specific choice of the classifier algorithm,
its hyper-parameters and input features.
2 Reordering as a walk on a dependency
tree
2.1 Dependency parse trees
Let a sentence be a list of words s ?
(w1, w2, . . . , wn) and its dependency parse tree
be a rooted tree whose nodes are the words of the
sentence. An edge of the tree represents a syntac-
tical dependency relation between a head (parent)
word and a modifier (child) word. Typical depen-
dency relations include verb-subject, verb-object,
noun-adjective, and so on.
We assume that in addition to its head hi and
dependency relation type di each word is also an-
notated with a part-of-speech pi and optionally a
lemma li and a morphology mi (e.g. grammatical
case, gender, number, tense).
Some definitions require dependency parse
trees to be projective, meaning that any complete
164
subtree must correspond to a contiguous span of
words in the sentence, however, we don?t place
such a requirement. In practice, languages with a
substantially strict word ordering like English typ-
ically have largely projective dependencies, while
languages with a more free word ordering like
Czech can have substantial non-projectivity.
2.2 Reordering model
Given a sentence s ? S with its dependency parse
tree and additional annotations, we incrementally
construct a reordered sentence s? by emitting its
words in a sequence of steps. We model the re-
ordering process as a non-deterministic transition
system which traverses the parse tree:
Let the state of the system be a tuple x ?
(i, r, a, , . . . ) containing at least the index of the
current node i (initialized at the root), the list of
emitted nodes r (initialized as empty) and the last
transition action a (initialized as null). Additional
information can be included in the state x, such as
the list of the last K nodes that have been visited,
the last K actions and a visit count for each node.
At each step we choose one of the following ac-
tions:
? EMIT : emit the current node. Enabled onlyif the current node hasn?t already been emit-ted
i /? r
(i, r, a, , . . . ) EMIT? (i, (r | i) , EMIT, , . . . )
? UP : move to the parent of the current node
hi 6= null, ?j a 6= DOWNj
(i, r, a, , . . . ) UP? (hi, r, UP, , . . . )
? DOWNj : move to the child j of the currentnode. Enabled if the subtree of j (including
j) contains nodes that have not been emittedyet.
hj = i, a 6= UP, ?k ? subtree(i) : k /? r
(i, r, a, , . . . ) DOWNj? (j, r, DOWNj , , . . . )
The pre-conditions on the UP and DOWN actions
prevent them from canceling each other, ensuring
that progress is made at each step. The additional
precondition on DOWN actions ensures that the
process always halts at a final state where all the
nodes have been emitted.
Let T (s) be the set of legal traces of the transi-
tion system for sentence s. Each trace ? ? T (s)
defines a permutation s? of s as the list of emitted
nodes r of its final state.
We define the reordering problem as finding the
trace ?? that maximizes a scoring function ?
?? ? arg max
??T (s)
? (s, ?) (1)
Note that since the parse tree is connected, in
principle any arbitrary permutation can be gen-
erated for a suitable choice of ?, though the
maximization problem (1) is NP-hard and APX-
complete in the general case, by trivial reduction
from the traveling salesman problem.
The intuition behind this model is to leverage
the syntactical information provided by the de-
pendency parse tree, as successfully done by (Xu
et.al., 2009; Genzel, 2010; Isozaki et.al., 2010)
without being strictly constrained by a specific
type reordering rules.
2.3 Trace scores
We wish to design a scoring function ? that cap-
tures good reorderings for machine translation and
admits an efficient optimization scheme.
We chose a function that additively decomposes
into local scoring functions, each depending only
on a single state of the trace and the following tran-
sition action
? (s, ?) ?
|? |?1?
t=1
? (s, x (?, t) , xa (?, t+ 1))
(2)
We further restrict our choice to a function
which is linear w.r.t. a set of elementary local fea-
ture functions {fk}
? (s, x, a) ?
|F |?
k=1
vkfk (s, x, a) (3)
where {vk} ? R|F | is a vector of parameters
derived from a training procedure.
While in principle each feature function could
depend on the whole sentence and the whole se-
quence of nodes emitted so far, in practice we re-
strict the dependence to a fixed neighborhood of
the current node and the last few emitted nodes.
This reduces the space of possible permutations.
2.4 Classifier-driven action selection
Even when the permutation space has been re-
stricted by an appropriate choice of the feature
functions, computing an exact solution of the opti-
mization problem (1) remains non-trivial, because
165
at each step of the reordering generation process,
the set of enabled actions depends in general on
nodes emitted at any previous step, and this pre-
vents us from applying typical dynamic program-
ming techniques. Therefore, we need to apply an
heuristic procedure.
In our experiments, we apply a simple greedy
procedure: at each step we choose an action ac-
cording to the output a two-stage classifier:
1. A three-class one-vs-all logistic classifier
chooses an action among EMIT, UP or
DOWN based on a vector of features ex-
tracted from a fixed neighborhood of the cur-
rent node i, the last emitted nodes and addi-
tional content of the state.
2. If a DOWN action was chosen, then a one-
vs-one voting scheme is used to choose
which child to descend to: For each pair
(j, j?) : j < j? of children of i, a binary lo-
gistic classifier assigns a vote either to j or
j?. The child that receives most votes is cho-
sen. This is similar to the max-wins approach
used in packages such as LIBSVM (Chang
and Lin, 2011) to construct a M -class clas-
sifier from M (M ? 1) /2 binary classifiers,
except that we use a single binary classifier
acting on a vector of features extracted from
the pair of children (j, j?) and the node i,
with their respective neighborhoods.
We also experimented with different classification
schemes, but we found that this one yields the best
performance.
Note that we are not strictly maximizing a
global linear scoring function as as defined by
equations (2) and (3), although this approach is
closely related to that framework.
This approach is related to transition-based de-
pendency parsing such as (Nivre and Scholz,
2004; Attardi, 2006) or dependency tree revi-
sion(Attardi and Ciaramita, 2007).
3 Training
3.1 Dataset preparation
Following (Al-Onaizan and Papineni, 2006;
Tromble and Eisner, 2009; Visweswariah et.al.,
2011), we generate a source-side reference re-
ordering of a parallel training corpus. For each
sentence pair, we generate a bidirectional word
alignment using GIZA++ (Och and Ney, 2000)
and the ?grow-diag-final-and? heuristic imple-
mented in Moses (Koehn et.al.,2007), then we as-
sign to each source-side word a integer index cor-
responding to the position of the leftmost target-
side word it is aligned to (attaching unaligned
words to the following aligned word) and finally
we perform a stable sort of source-side words ac-
cording to this index.
On language pairs where GIZA++ produces
substantially accurate alignments (generally all
European languages) this scheme generates a
target-like reference reordering of the corpus.
In order to tune the parameters of the down-
stream phrase-based translation system and to test
the overall translation accuracy, we need two addi-
tional small parallel corpora. We don?t need a ref-
erence reordering for the tuning corpus since it is
not used for training the reordering system, how-
ever we generate a reference reordering for the test
corpus in order to evaluate the accuracy of the re-
ordering system in isolation. We obtain an align-
ment of this corpus by appending it to the train-
ing corpus, and processing it with GIZA++ and
the heuristic described above.
3.2 Reference traces generation and classifier
training
For each source sentence s in the training set
and its reference reordering s?, we generate a
minimum-length trace ? of the reordering transi-
tion system, and for each state and action pair in it
we generate the following training examples:
? For the first-stage classifier we generate a sin-
gle training examples mapping the local fea-
tures to an EMIT, UP or DOWN action label
? For the second-stage classifier, if the action is
DOWNj , for each pair of children (k, k?) :
k < k? of the current node i, we generate a
positive example if j = k or a negative ex-
ample if j = k?.
Both classifiers are trained with the LIBLIN-
EAR package (Fan et.al., 2008), using the L2-
regularized logistic regression method. The reg-
ularization parameter C is chosen by two-fold
cross-validation. In practice, subsampling of the
training set might be required in order to keep
memory usage and training time manageable.
3.3 Translation system training and testing
Once the classifiers have been trained, we run
the reordering system on the source side of the
166
whole (non-subsampled) training corpus and the
tuning corpus. For instance, if the parallel cor-
pora are German-to-English, after the reorder-
ing step we obtain German?-to-English corpora,
where German? is German in an English-like
word order. These reordered corpora are used to
train a standard phrase-based translation system.
Finally, the reordering system is applied to source
side of the test corpus, which is then translated
with the downstream phrase-based system and the
resulting translation is compared to the reference
translation in order to obtain an accuracy measure.
We also evaluate the ?monolingual? reordering ac-
curacy of upstream reordering system by compar-
ing its output on the source side of the test cor-
pus to the reference reordering obtained from the
alignment.
4 Experiments
We performed German-to-English and Italian-to-
English reordering and translation experiments.
4.1 Data
The German-to-English corpus is Europarl v7
(Koehn, 2005). We split it in a 1,881,531 sentence
pairs training set, a 2,000 sentence pairs develop-
ment set (used for tuning) and a 2,000 sentence
pairs test set. We also used a 3,000 sentence pairs
?challenge? set of newspaper articles provided by
the WMT 2013 translation task organizers.
The Italian-to-English corpus has been assem-
bled by merging Europarl v7, JRC-ACQUIS v2.2
(Steinberger et.al., 2006) and bilingual newspaper
articles crawled from news websites such as Cor-
riere.it and Asianews.it. It consists of a 3,075,777
sentence pairs training set, a 3,923 sentence pairs
development set and a 2,000 sentence pairs test
set.
The source sides of these corpora have been
parsed with Desr (Attardi, 2006). For both lan-
guage pairs, we trained a baseline Moses phrase-
based translation system with the default configu-
ration (including lexicalized reordering).
In order to keep the memory requirements and
duration of classifier training manageable, we sub-
sampled each training set to 40,000 sentences,
while both the baseline and reordered Moses sys-
tem are trained on the full training sets.
4.2 Features
After various experiments with feature selection,
we settled for the following configuration for both
German-to-English and Italian-to-English:
? First stage classifier: current node i state-
ful features (emitted?, left/right subtree emit-
ted?, visit count), curent node lexical and
syntactical features (surface form wi, lemma
li, POS pi, morphology mi, DEPREL di, and
pairwise combinations between lemma, POS
and DEPREL), last two actions, last two vis-
ited nodes POS, DEPREL and visit count,
last two emitted nodes POS and DEPREL, bi-
gram and syntactical trigram features for the
last two emitted nodes and the current node,
all lexical, syntactical and stateful features
for the neighborhood of the current node
(left, right, parent, parent-left, parent-right,
grandparent, left-child, right-child) and pair-
wise combination between syntactical fea-
tures of these nodes.
? Second stage classifier: stateful features for
the current node i and the the children pair
(j, j?), lexical and syntactical features for
each of the children and pairwise combina-
tions of these features, visit count differences
and signed distances between the two chil-
dren and the current node, syntactical trigram
features between all combinations of the two
children, the current node, the parent hi and
the two last emitted nodes and the two last
visited nodes, lexical and syntactical features
for the two children left and right neighbors.
All features are encoded as binary one-of-n indi-
cator functions.
4.3 Results
For both German-to-English and Italian-to-
English experiments, we prepared the data as
described above and we trained the classifiers on
their subsampled training sets. In order to evaluate
the classifiers accuracy in isolation from the rest
of the system, we performed two-fold cross vali-
dation on the same training sets, which revealed
an high accuracy: The first stage classifier obtains
approximately 92% accuracy on both German and
Italian, while the second stage classifier obtains
approximately 89% accuracy on German and 92%
on Italian.
167
BLEU NIST
German 57.35 13.2553
Italian 68.78 15.3441
Table 1: Monolingual reordering scores
BLEU NIST
de-en baseline 33.78 7.9664
de-en reordered 32.42 7.8202
it-en baseline 29.17 7.1352
it-en reordered 28.84 7.1443
Table 2: Translation scores
We applied the reordering preprocessing system
to the source side of the corpora and evaluated the
monolingual BLEU and NIST score of the test sets
(extracted from Europarl) against their reference
reordering computed from the alignment
To evaluate translation performance, we trained
a Moses phrase-based system on the reordered
training and tuning corpora, and evaluated the
BLEU and NIST of the (Europarl) test sets. As
a baseline, we also trained and evaluated Moses
system on the original unreordered corpora.
We also applied our baseline and reordered
German-to-English systems to the WMT2013
translation task dataset.
5 Discussion
Unfortunately we were generally unable to im-
prove the translation scores over the baseline, even
though our monolingual BLEU for German-to-
English reordering is higher than the score re-
ported by (Tromble and Eisner, 2009) for a com-
parable dataset.
Accuracy on the WMT 2013 set is very low. We
attribute this to the fact that it comes form a differ-
ent domain than the training set.
Since classifier training set cross-validation ac-
curacy is high, we speculate that the main problem
lies with the training example generation process:
training examples are generated only from opti-
mal reordering traces. This means that once the
classifiers produce an error and the system strays
away from an optimal trace, it may enter in a fea-
ture space that is not well-represented in the train-
ing set, and thus suffer from unrecoverable per-
formance degradation. Moreover, errors occurring
on nodes high in the parse tree may cause incor-
rect placement of whole spans of words, yielding
a poor BLEU score (although a cursory exami-
nation of the reordered sentences doesn?t reveal
this problem to be prevalent). Both these issues
could be possibly addressed by switching from
a classifier-based system to a structured predic-
tion system, such as averaged structured percep-
tron (Collins, 2002) or MIRA (Crammer, 2003;
McDonald et.al., 2005).
Another possible cause of error is the purely
greedy action selection policy. This could be ad-
dressed using a search approach such as beam
search.
We reserve to investigate these approaches in
future work.
References
Roy Tromble and Jason Eisner. 2009. Learning linear
ordering problems for better translation. In Proceed-
ings of the 2009 Conference on Empirical Methods
in Natural Language Processing: Volume 2 - Vol-
ume 2 (EMNLP ?09), Vol. 2. Association for Com-
putational Linguistics, Stroudsburg, PA, USA, 1007-
1016.
G. Attardi, M. Ciaramita. 2007. Tree Revision Learn-
ing for Dependency Parsing. In Proc. of the Human
Language Technology Conference 2007.
Peng Xu, Jaeho Kang, Michael Ringgaard, and Franz
Och. 2009. Using a dependency parser to improve
SMT for subject-object-verb languages. In Proceed-
ings of Human Language Technologies: The 2009
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics
(NAACL ?09). Association for Computational Lin-
guistics, Stroudsburg, PA, USA, 245-253.
Dmitriy Genzel. 2010. Automatically learning source-
side reordering rules for large scale machine trans-
lation. In Proceedings of the 23rd International
Conference on Computational Linguistics (COL-
ING ?10). Association for Computational Linguis-
tics, Stroudsburg, PA, USA, 376-384.
Yaser Al-Onaizan and Kishore Papineni. 2006. Dis-
tortion models for statistical machine translation. In
Proceedings of the 21st International Conference
on Computational Linguistics and the 44th annual
meeting of the Association for Computational Lin-
guistics (ACL-44). Association for Computational
Linguistics, Stroudsburg, PA, USA, 529-536.
Alexandra , Miles Osborne, and Philipp Koehn. 2008.
Predicting success in machine translation. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing (EMNLP ?08). Asso-
ciation for Computational Linguistics, Stroudsburg,
PA, USA, 745-754.
168
BLEU BLEU (11b) BLEU-cased BLEU-cased (11b) TER
de-en baseline 18.8 18.8 17.8 17.8 0.722
de-en reordered 18.1 18.1 17.3 17.3 0.739
Table 3: WMT2013 de-en translation scores
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondr?ej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions
(ACL ?07). Association for Computational Linguis-
tics, Stroudsburg, PA, USA, 177-180.
Hideki Isozaki, Katsuhito Sudoh, Hajime Tsukada, and
Kevin Duh. 2010. Head finalization: a simple re-
ordering rule for SOV languages. In Proceedings
of the Joint Fifth Workshop on Statistical Machine
Translation and MetricsMATR (WMT ?10). Asso-
ciation for Computational Linguistics, Stroudsburg,
PA, USA, 244-251.
Michel Galley and Christopher D. Manning. 2008.
A simple and effective hierarchical phrase reorder-
ing model. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP ?08). Association for Computational Lin-
guistics, Stroudsburg, PA, USA, 848-856.
Karthik Visweswariah, Rajakrishnan Rajkumar, Ankur
Gandhe, Ananthakrishnan Ramanathan, and Jiri
Navratil. 2011. A word reordering model for im-
proved machine translation. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP ?11). Association for
Computational Linguistics, Stroudsburg, PA, USA,
486-496.
Giuseppe Attardi. 2006. Experiments with a multi-
language non-projective dependency parser. In Pro-
ceedings of the Tenth Conference on Computational
Natural Language Learning (CoNLL-X ?06). Asso-
ciation for Computational Linguistics, Stroudsburg,
PA, USA, 166-170.
Joakim Nivre and Mario Scholz. 2004. Determinis-
tic dependency parsing of English text. In Proceed-
ings of the 20th international conference on Compu-
tational Linguistics (COLING ?04). Association for
Computational Linguistics, Stroudsburg, PA, USA, ,
Article 64 .
Franz Josef Och and Hermann Ney. 2000. Improved
statistical alignment models. In Proceedings of the
38th Annual Meeting on Association for Computa-
tional Linguistics (ACL ?00). Association for Com-
putational Linguistics, Stroudsburg, PA, USA, 440-
447.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM: A library for support vector machines. ACM
Trans. Intell. Syst. Technol. 2, 3, Article 27 (May
2011), 27 pages.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A Library for Large Linear Classification. J. Mach.
Learn. Res. 9 (June 2008), 1871-1874.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. MT Summit 2005.
Ralf Steinberger, Bruno Pouliquen, Anna Widiger,
Camelia Ignat, Toma Erjavec, Dan Tufis?. 2006. The
JRC-Acquis: A multilingual aligned parallel corpus
with 20+ languages. In Proceedings of the 5th In-
ternational Conference on Language Resources and
Evaluation (LREC?2006).
Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models: theory and experi-
ments with perceptron algorithms. In Proceedings
of the ACL-02 conference on Empirical methods in
natural language processing - Volume 10 (EMNLP
?02), Vol. 10. Association for Computational Lin-
guistics, Stroudsburg, PA, USA, 1-8.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
pendency parsers. In Proceedings of the 43rd An-
nual Meeting on Association for Computational Lin-
guistics (ACL ?05). Association for Computational
Linguistics, Stroudsburg, PA, USA, 91-98.
Koby Crammer and Yoram Singer. 2003. Ultraconser-
vative online algorithms for multiclass problems. J.
Mach. Learn. Res. 3 (March 2003), 951-991.
169
