Proceedings of the ACL-08: HLT Student Research Workshop (Companion Volume), pages 13?18,
Columbus, June 2008. c?2008 Association for Computational Linguistics
Inferring Activity Time in News through Event Modeling
Vladimir Eidelman
Department of Computer Science
Columbia University
New York, NY 10027
vae2101@columbia.edu
Abstract
Many applications in NLP, such as question-
answering and summarization, either require
or would greatly benefit from the knowledge
of when an event occurred. Creating an ef-
fective algorithm for identifying the activ-
ity time of an event in news is difficult in
part because of the sparsity of explicit tem-
poral expressions. This paper describes a
domain-independent machine-learning based
approach to assign activity times to events
in news. We demonstrate that by applying
topic models to text, we are able to cluster
sentences that describe the same event, and
utilize the temporal information within these
event clusters to infer activity times for all sen-
tences. Experimental evidence suggests that
this is a promising approach, given evaluations
performed on three distinct news article sets
against the baseline of assigning the publica-
tion date. Our approach achieves 90%, 88.7%,
and 68.7% accuracy, respectively, outperform-
ing the baseline twice.
1 Introduction
Many practical applications in NLP either require
or would greatly benefit from the use of temporal
information. For instance, question-answering and
summarization systems demand accurate process-
ing of temporal information in order to be useful
for answering ?when? questions and creating coher-
ent summaries by temporally ordering information.
Proper processing is especially relevant in news,
where multiple disparate events may be described
within one news article, and it is necessary to iden-
tify the separate timepoints of each event.
Event descriptions may be confined to one sen-
tence, which we establish as our text unit, or be
spread over many, thus forcing us to assign all sen-
tences an activity time. However, only 20%-30%
of sentences contain an explicit temporal expres-
sion, thus leaving the vast majority of sentences
without temporal information. A similar proportion
is reported in Mani et al (2003), with only 25%
of clauses containing explicit temporal expressions.
The sparsity of these expressions poses a real chal-
lenge. Therefore, a method for efficiently and accu-
rately utilizing temporal expressions to infer activity
times for the remaining 70%-80% of sentences with
no temporal information is necessary.
This paper proposes a domain-independent
machine-learning based approach to assign activity
times to events in news without deferring to the pub-
lication date. Posing the problem in an informa-
tion retrieval framework, we model events by ap-
plying topic models to news, providing a way to
automatically distribute temporal information to all
sentences. The result is prototype system which
achieves promising results.
In the following section, we discuss related work
in temporal information processing. Next we moti-
vate the use of topic models for our task, and present
our methods for distributing temporal information.
We conclude by presenting and discussing our re-
sults.
2 Related Work
Mani and Wilson (2000) worked on news and in-
troduced an annotation scheme for temporal ex-
pressions, and a method for using explicit tempo-
13
Sentence Order Event Temporal Expression
1 Event X None
2 Event Y January 10, 2007
3 Event X None
4 Event X November 16, 1967
5 Event Y None
6 Event Y January 10, 2007
7 Event X None
Table 1: Problematic Example
ral expressions to assign activity times to the en-
tirety of an article. Their preliminary work on in-
ferring activity times suggested a baseline method
which spread time values of temporal expressions
to neighboring events based on proximity. Fila-
tova and Hovy (2001) also process explicit tempo-
ral expressions within a text and apply this informa-
tion throughout the whole article, assigning activity
times to all clauses.
More recent work has tried to temporally anchor
and order events in news by looking at clauses (Mani
et al, 2003). Due to the sparsity of temporal ex-
pressions, they computed a reference time for each
clause. The reference time is inferred using a num-
ber of linguistic features if no explicit reference is
present, but the algorithm defaults to assigning the
most recent time when all else fails.
A severe limitation of previous work is the depen-
dence on article structure. Mani and Wilson (2000)
attribute over half the errors of their baseline method
to propagation of an incorrect event time to neigh-
boring events. Filatova and Hovy (2001) infer time
values based on the most recently assigned date or
the date of the article. The previous approaches will
all perform unfavorably in the example presented in
Table 1, where a second historical event is referred
to between references to a current event. This kind
of example is quite common.
3 Modeling News
To address the aforementioned issues of sparsity
while relieving dependence on article structure, we
treat event discovery as a clustering problem. Clus-
tering methods have previously been used for event
identification (Hatzivassiloglou et al, 2000; Sid-
dharthan et al, 2004). After a topic model of news
text is created, sentences are clustered into topics -
where each topic represents a specific event. This
allows us to utilize all available temporal informa-
tion in each cluster to distribute to all the sentences
within that cluster, thus allowing for assigning of ac-
tivity times to sentences without explicit temporal
expressions. Our key assumption is that similar sen-
tences describe the same event.
Our approach is based on information retrieval
techniques, so we subsequently use the standard lan-
guage of text collections. We may refer to sentences,
or clusters of sentences created from a topic model
as ?documents?, and a collection of sentences, or col-
lection of clusters of sentences from one or more
news articles as a ?corpus?. We use Latent Dirich-
let Allocation (LDA) (Blei et al, 2003), a genera-
tive model for describing collections of text corpora,
which represents each document as a mixture over a
set of topics, where each topic has associated with it
a distribution over words. Topics are shared by all
documents in the corpus, but the topic distribution is
assumed to come from a Dirichlet distribution. LDA
allows documents to be composed of multiple topics
with varying proportions, thus capturing multiple la-
tent patterns.
Depending on the words present in each docu-
ment, we associate it with one ofN topics, whereN
is the number of latent topics in the model. We as-
sign each document to the topic which has the high-
est probability of having generated that document.
We expect document similarity in a cluster to be
fairly high, as evidenced by document modeling per-
formance in Blei et al (2003). Since each cluster is
a collection of similar documents, with our assump-
tion that similar documents describe the same event,
we conclude that each cluster represents a specific
event. Thus, if at least one sentence in an event clus-
ter contains an explicit temporal expression, we can
distribute that activity time to other sentences in the
cluster using an inference algorithm we explain in
the next section. More than one event cluster may
represent the same event, as in Table 3, where both
topics describe a different perspective on the same
event: the administrative reaction to the incident at
Duke.
Creating a cluster of similar documents which
represent an event can be powerful. First, we are no
longer restricted by article structure. To refer back to
14
Table 1, our approach will assign the correct activ-
ity time for all event X sentences, even though they
are separated in the article and only one contains an
explicit temporal expression, by utilizing an event
cluster which contains the four sentences describing
event X to distribute the temporal information1.
Second, we are not restricted to using only one
article to assign activity times to sentences. In fact,
one of the major strengths of this approach is the
ability to take a collection of articles and treat them
all as one corpus, allowing the model to use all
explicit temporal expressions on event X present
throughout all of the articles to distribute activity
times. This is especially helpful in multidocument
summarization, where we have multiple articles on
the same event.
Additionally, using LDA as a method for event
identification may be advantageous over other clus-
tering methods. For one, Siddharthan et al (2004)
reported that removing relative clauses and appos-
itives, which provide background or discourse re-
lated information, improves clustering. LDA allows
us to discover the presence of multiple events within
a sentence, and future work will focus on exploiting
this to improve clustering.
3.1 Corpus
We obtained 22 news articles, which can be divided
into three distinct sets: Duke Rape Case (DR), Ter-
rorist Bombings in Mumbai (MB), Israeli-Lebanese
conflict (IC) (Table 2). All articles come from En-
glish Newswire text, and each sentence was manu-
ally annotated with an activity time by people out-
side of the project. The Mumbai Bombing articles
all occur within a several day span, as do the Israeli-
Conflict articles. The Duke Rape case articles are
an exception, since they are comprised of multi-
ple events which happened over the course of sev-
eral months: Thus these articles contain many cases
such as ?The report said...on March 14...?, where
the report is actually in May, yet speaks of events
in March. For the purposes of this experiment we
took the union of the possible dates mentioned in a
sentence as acceptable activity times, thus both the
report statement date and the date mentioned in the
1Analogously, our approach will assign correct activity time
to all event Y sentences
Article Set # of Articles # of Sentences
Duke Rape Case 5 151
Mumbai Bombing 8 284
Israeli Conflict 9 300
Table 2: Article and Sentence distribution
report are correct activity times for the sentence. Fu-
ture work will investigate whether we can discrimi-
nate between these two dates.
Our approach relies on prior automatic linguistic
processing of the articles by the Proteus system (Gr-
ishman et al, 2005). The articles are annotated with
time expression tags, which assign values to both
absolute ?July 16, 2006? and relative ?now? tem-
poral expressions. Although present, our approach
does not currently use activity time ranges, such as
?past 2 weeks? or ?recent days?. The articles are
also given entity identification tags, which assigns a
unique intra-article id to entities of the types speci-
fied in the ACE 2005 evaluation. For example, both
?they? - an anaphoric reference - and ?police offi-
cers? are recognized as referring to the same real-
world entity.
3.2 Feature Extraction
From this point on unless otherwise noted, refer-
ence to news articles indicates one of the three sets
of news articles, not the complete set. We begin
by breaking news articles into their constituent sen-
tences, which are our ?documents?, the collection
of them being our ?corpus?, and indexing the doc-
uments.
We use the bag-of-words assumption to represent
each document as an unordered collection of words.
This allows the representation of each document as
a word vector. Additionally, we add any entity iden-
tification information and explicit temporal expres-
sions present in the document to the feature vector
representation of each document.
3.3 Intra-Article Event Representation
To represent events within one news article, we con-
struct a topic model for each article separately. The
Intra-Article (IAA) model constructed for an article
allows us to group sentences within that article to-
gether according to event. This allows the forma-
tion of new ?documents?, which consist not of single
15
The administrators did not know of the racial dimension until March 24, the report said.
The report did say that Brodhead was hampered by the administration?s lack of diversity.
He said administrators would be reviewed on their performance on the normal schedule
and he had no immediate plans to make personnel changes.
Administrators allowed the team to keep practicing; Athletics Director Joe Alleva called
the players ?wonderful young men.?
Yet even Duke faculty members, many of them from the ?60s and ?70s generations that
pushed college administrators to ease their controlling ways, now are urging the university
to require greater social as well as scholastic discipline from students.
Duke professors, in fact, are offering to help draft new behavior codes for the school.
With years of experience and academic success to their credit, faculty members ought to
be listened to.
For the moment, five study committees appointed by Brodhead seem to mean business,
which is encouraging.
Table 3: Two topics representing a different perspective
on the same event
sentences, but a cluster of sentences representing an
event. Accordingly, we combine the feature vector
representations of the single sentences in an event
cluster into one feature vector, forming an aggregate
of all their features. Although at this stage we have
everything we need to infer activity times, our ap-
proach allows incorporating information from mul-
tiple articles.
3.4 Inter-Article Event Representation
To represent events over multiple articles, we sug-
gest two methods for Inter-Article (IRA) topic mod-
eling. The first, IRA.1, is to combine the articles
and treat them as one large article. This allows pro-
cessing as described in IAA, with the exception that
event clusters may contain sentences from multiple
articles. The second, IRA.2, builds on IAA mod-
els of single articles and uses them to construct an
IRA model. The IRA.2 model is constructed over
a corpus of documents containing event clusters, al-
lowing a grouping of event clusters from multiple
articles. Event clusters may now be composed of
sentences describing the same event from multiple
articles, thus increasing our pool of explicit tempo-
ral expressions available for inference.
3.5 Activity Time Assignment
To accurately infer activity times of all sentences, it
is crucial to properly utilize the available temporal
expressions in the event clusters formed in the IRA
or IAA models. Our proposed inference algorithm
is a starting point for further work. We use the most
frequent activity time present in an event cluster as
the value to assign all the sentences in that event
cluster. In phase one of the algorithm we process
each event cluster separately. If the majority of sen-
tences with temporal expressions have the same ac-
tivity time, then this activity time is distributed to the
other sentences. If there is a tie between the num-
ber of occurrences of two activity times, both these
times are distributed as the activity time to the other
sentences. If there is no majority time and no tie
in the event cluster, then each of the sentences with
a temporal expression retains its activity time, but
no information is distributed to the other sentences.
Phase two of the inference algorithm reassembles
the sentences back into their original articles, with
most sentences now having activity times tags as-
signed from phase one. Sentences that remain un-
marked, indicating that they were in event clusters
with no majority and no tie, are assigned the ma-
jority activity time appearing in their reassembled
article.
4 Empirical Evaluation
In evaluating our approach, we wanted to compare
different methods of modeling events prior to per-
forming inference.
? Method (1) IAA then IRA.2 - Creating IAA
models with 20 topics for each news article,
and IRA.2 models for each of the three sets of
IAA models with 20, 50, and 100 topic.
? Method (2) IAA only - Creating an IAA model
with 20 topics for each article
? Method (3) IRA.1 only - Creating IRA.1 model
with 20 and 50 topics for each of the three sets
of articles.
4.1 Results
Table 4 presents results for the three sets of articles
on the six different experiments performed. Since
our approach assigns activity times to all sentences,
overall accuracy is measured as the total number of
correct activity time assignments made out of the
total number of sentences. The baseline accuracy
is computed by assigning each sentence the article
publication date, and because news generally de-
scribes current events, this achieves remarkably high
performance.
16
The overall accuracy measures performance of
the complete inference algorithm, while the rest of
the metrics measure the performance of phase one
only, where we process each event cluster separately.
Assessing the performance of phase one allows us to
indirectly evaluate the event clusters which we cre-
ate using LDA. M1 accuracy represents the number
of sentences that were assigned the correct activity
time in phase one out of the total number of activ-
ity time inferences made in phase one. Thus, this
does not take into account any assignments made by
phase two, and allows us to examine our assump-
tions about event representation expressed earlier. A
large denominator in M1 indicates that many sen-
tences were assigned in phase one, while a low one
indicates the presence of event clusters which were
unable to distribute temporal information.
M2 looks at how well the algorithm performs on
the difficult cases where the activity time is not the
same as the publication date. M3 looks at how well
the algorithm performs on the majority of sentences
which have no temporal expressions.
For the IC and DR sets, results show that Method
(1), where IAA is performed prior to IRA.2 achieves
the best performance, with accuracy of 88.7% and
90%, respectively, giving credence to the claim that
representing events within an article before combin-
ing multiple articles improves inference.
The MB set somewhat counteracts this claim, as
the best performance was achieved by Method (3),
where IRA.1 is performed. This may be due to the
fact that MB differs from DR and IC sets in that it
contains several regurgitated news articles. Regurgi-
tated news articles are comprised almost entirely of
statements made at a previous time in other news ar-
ticles. Method (3) combines similar sentences from
all the articles right away, placing sentences from re-
gurgitated articles in an event cluster with the orig-
inal sentences. This allows our approach to outper-
form the baseline system by 4.3%, with and accu-
racy of 68.7%.
5 Discussion
There are limitations to our approach which need
to be addressed. Foremost, evidence suggests that
event clusters are not perfect, as error analysis has
shown event clusters which represent two or more
Set Setup Accur. M1 M2 M3
DR Base 135/151
89.4%
DR (1) 20 121/151
80.1%
55/83
66.2%
5/12
41.6%
27/43
62.7%
DR (1) 50 136/151
90.0%
91/105
86.6%
4/13
30.7%
60/66
90.9%
DR (1)100 128/151
84.7%
87/109
79.8%
4/13
30.7%
58/70
82.8%
DR (2) 20 106/151
70.2%
45/68
66.2%
4/11
36.4%
20/33
60.6%
DR (3) 20 111/151
73.5%
82/110
74.7%
8/14
57.1%
49/71
69.0%
DR (3) 50 99/151
65.5%
92/135
68.1%
6/14
42.9%
63/95
66.3%
Set Setup Accur. M1 M2 M3
MB Base 183/284
64.4%
MB (1) 20 166/284
58.5%
116/187
62.0%
41/68
60.2%
60/104
57.7%
MB (1) 50 152/284
53.5%
121/206
58.7%
41/72
56.9%
66/120
55.0%
MB (1)100 139/284
48.9%
112/204
54.9%
41/81
50.6%
60/124
48.4%
MB (2) 20 143/284
50.3%
103/161
63.9%
40/63
63.5%
49/85
57.3%
MB (3) 20 146/284
51.4%
99/160
61.9%
45/64
70.3%
47/81
58.0%
MB (3) 50 195/284
68.7%
123/184
66.8%
32/67
47.8%
74/103
71.8%
Set Setup Accur. M1 M2 M3
IC Base 272/300
90.7%
IC (1) 20 250/300
83.3%
158/205
77.1%
12/22
54.5%
118/151
78.1%
IC (1) 50 263/300
87.7%
168/192
87.5%
12/19
63.2%
127/139
91.4%
IC (1)100 266/300
88.7%
173/202
85.6%
11/20
55.0%
130/149
87.2%
IC (2) 20 250/300
83.3%
156/181
86.2%
11/18
61.1%
117/130
90.0%
IC (3) 20 225/300
75.0%
112/145
77.2%
14/21
66.7%
75/95
78.9%
IC (3) 50 134/300
44.7%
115/262
43.9%
14/25
56.0%
76/206
36.9%
Table 4: Results : Sentence Breakdown
17
events. Event clusters which contain sentences de-
scribing several events pose a real challenge, as
they are primarily responsible for inhibiting perfor-
mance. This limitation is not endemic to our ap-
proach for event discovery, as Xu et al (2006) stated
that event extraction is still considered as one of the
most challenging tasks, because an event mention
can be expressed by several sentences and different
linguistic expressions.
One of the major strengths of our approach is the
ability to combine all temporal information on an
event from multiple articles. However, due the im-
perfect event clusters, combining temporal informa-
tion from different articles within an event cluster
has not yet yielded satisfactory results.
Although sentences from the same article in IRA
event clusters usually represent the same event, other
sentences from different articles may not. We mod-
ified the inference algorithm to reflect this, and only
consider sentences from the same news article when
distributing temporal information, even though sen-
tences from other articles may be present in the event
cluster. Therefore, further work to construct event
clusters which more closely represent events is ex-
pected to yield improvements in performance. Fu-
ture work will explore a richer feature set, including
such features as cross-document entity identification
information, linguistic features, and outside seman-
tic knowledge to increase robustness of the feature
vectors. Finally, the optimal model parameters are
currently selected by an oracle, however, we hope to
further evaluate our approach on a larger dataset in
order to determine how to automatically select the
optimal parameters.
6 Conclusion
This paper presented a novel approach for inferring
activity times for all sentences in a text. We demon-
strate we can produce reasonable event representa-
tions in an unsupervised fashion using LDA, pos-
ing event discovery as a clustering problem, and that
event clusters can further be used to distribute tem-
poral information to the sentences which lack ex-
plicit temporal expressions. Our approach achieves
90%, 88.7%, and 68.7% accuracy, outperforming
the baseline set forth in two cases. Although differ-
ences prevent a direct comparison, Mani and Wil-
son (2000) achieved an accuracy of 59.4% on 694
verb occurrences using their baseline method, Fi-
latova and Hovy (2001) achieved 82% accuracy on
time-stamping clauses for a single type of event on
172 clauses, and Mani et al (2003) achieved 59%
accuracy in their algorithm for computing a refer-
ence time for 2069 clauses. Future work will im-
prove upon the majority criteria used in the inference
algorithm, on creating more accurate event represen-
tations, and on determining optimal model parame-
ters automatically.
Acknowledgements
We wish to thank Kathleen McKeown and Barry
Schiffman for invaluable discussions and comments.
References
David M. Blei, Andrew Y. Ng and Michael I. Jordan.
2003. Latent Dirichlet Allocation. Journal of Ma-
chine Learning Research, vol. 3, pp.993?1022
Elena Filatova and Eduard Hovy. 2001. Assigning Time-
Stamps to Event-Clauses. Workshop on Temporal and
Spatial Information Processing, ACL?2001 88-95.
Ralph Grishman, David Westbrook, and Adam Meyers.
2005. NYU?s English ACE 2005 system description.
In ACE 05 Evaluation Workshop.
Vasileios Hatzivassiloglou, Luis Gravano, and Ankineedu
Maganti. 2000. An Investigation of Linguistic Fea-
tures and Clustering Algorithms for Topical Document
Clustering. In Proceedings of the 23rd ACM SIGIR,
pages 224-231.
Inderjeet Mani, Barry Schiffman and Jianping Zhang.
2003. Inferring Temporal Ordering of Events in News.
Proceedings of the Human Language Technology Con-
ference.
Inderjeet Mani and George Wilson. 2000. Robust Tem-
poral Processing of News. Proceedings of the 38th
Annual Meeting of the Association for Computational
Linguistics, 69-76. Hong Kong.
Advaith Siddharthan, Ani Nenkova, and Kathleen McK-
eown. 2004. Syntactic simplification for improving
content selection in multi-document summarization.
In 20th International Conference on Computational
Linguistics .
Feiyu Xu, Hans Uszkoreit, and Hong Li. 2006. Auto-
matic event and relation detection with seeds of vary-
ing complexity. In Proceedings of the AAAI Workshop
Event Extraction and Synthesis, pages 1217, Boston.
18
Proceedings of the ACL-08: HLT Demo Session (Companion Volume), pages 9?12,
Columbus, June 2008. c?2008 Association for Computational Linguistics
BART: A Modular Toolkit for Coreference Resolution
Yannick Versley
University of Tu?bingen
versley@sfs.uni-tuebingen.de
Simone Paolo Ponzetto
EML Research gGmbH
ponzetto@eml-research.de
Massimo Poesio
University of Essex
poesio@essex.ac.uk
Vladimir Eidelman
Columbia University
vae2101@columbia.edu
Alan Jern
UCLA
ajern@ucla.edu
Jason Smith
Johns Hopkins University
jsmith@jhu.edu
Xiaofeng Yang
Inst. for Infocomm Research
xiaofengy@i2r.a-star.edu.sg
Alessandro Moschitti
University of Trento
moschitti@dit.unitn.it
Abstract
Developing a full coreference system able
to run all the way from raw text to seman-
tic interpretation is a considerable engineer-
ing effort, yet there is very limited avail-
ability of off-the shelf tools for researchers
whose interests are not in coreference, or for
researchers who want to concentrate on a
specific aspect of the problem. We present
BART, a highly modular toolkit for de-
veloping coreference applications. In the
Johns Hopkins workshop on using lexical
and encyclopedic knowledge for entity dis-
ambiguation, the toolkit was used to ex-
tend a reimplementation of the Soon et al
(2001) proposal with a variety of additional
syntactic and knowledge-based features, and
experiment with alternative resolution pro-
cesses, preprocessing tools, and classifiers.
1 Introduction
Coreference resolution refers to the task of identify-
ing noun phrases that refer to the same extralinguis-
tic entity in a text. Using coreference information
has been shown to be beneficial in a number of other
tasks, including information extraction (McCarthy
and Lehnert, 1995), question answering (Morton,
2000) and summarization (Steinberger et al, 2007).
Developing a full coreference system, however, is
a considerable engineering effort, which is why a
large body of research concerned with feature en-
gineering or learning methods (e.g. Culotta et al
2007; Denis and Baldridge 2007) uses a simpler but
non-realistic setting, using pre-identified mentions,
and the use of coreference information in summa-
rization or question answering techniques is not as
widespread as it could be. We believe that the avail-
ability of a modular toolkit for coreference will sig-
nificantly lower the entrance barrier for researchers
interested in coreference resolution, as well as pro-
vide a component that can be easily integrated into
other NLP applications.
A number of systems that perform coreference
resolution are publicly available, such as GUITAR
(Steinberger et al, 2007), which handles the full
coreference task, and JAVARAP (Qiu et al, 2004),
which only resolves pronouns. However, literature
on coreference resolution, if providing a baseline,
usually uses the algorithm and feature set of Soon
et al (2001) for this purpose.
Using the built-in maximum entropy learner
with feature combination, BART reaches 65.8%
F-measure on MUC6 and 62.9% F-measure on
MUC7 using Soon et al?s features, outperforming
JAVARAP on pronoun resolution, as well as the
Soon et al reimplementation of Uryupina (2006).
Using a specialized tagger for ACE mentions and
an extended feature set including syntactic features
(e.g. using tree kernels to represent the syntactic
relation between anaphor and antecedent, cf. Yang
et al 2006), as well as features based on knowledge
extracted from Wikipedia (cf. Ponzetto and Smith, in
preparation), BART reaches state-of-the-art results
on ACE-2. Table 1 compares our results, obtained
using this extended feature set, with results from
Ng (2007). Pronoun resolution using the extended
feature set gives 73.4% recall, coming near special-
ized pronoun resolution systems such as (Denis and
Baldridge, 2007).
9
Figure 1: Results analysis in MMAX2
2 System Architecture
The BART toolkit has been developed as a tool to
explore the integration of knowledge-rich features
into a coreference system at the Johns Hopkins Sum-
mer Workshop 2007. It is based on code and ideas
from the system of Ponzetto and Strube (2006), but
also includes some ideas from GUITAR (Steinberger
et al, 2007) and other coreference systems (Versley,
2006; Yang et al, 2006). 1
The goal of bringing together state-of-the-art ap-
proaches to different aspects of coreference res-
olution, including specialized preprocessing and
syntax-based features has led to a design that is very
modular. This design provides effective separation
of concerns across several several tasks/roles, in-
cluding engineering new features that exploit dif-
ferent sources of knowledge, designing improved or
specialized preprocessing methods, and improving
the way that coreference resolution is mapped to a
machine learning problem.
Preprocessing To store results of preprocessing
components, BART uses the standoff format of the
MMAX2 annotation tool (Mu?ller and Strube, 2006)
with MiniDiscourse, a library that efficiently imple-
ments a subset of MMAX2?s functions. Using a
generic format for standoff annotation allows the use
of the coreference resolution as part of a larger sys-
tem, but also performing qualitative error analysis
using integrated MMAX2 functionality (annotation
1An open source version of BART is available from
http://www.sfs.uni-tuebingen.de/?versley/BART/.
diff, visual display).
Preprocessing consists in marking up noun
chunks and named entities, as well as additional in-
formation such as part-of-speech tags and merging
these information into markables that are the start-
ing point for the mentions used by the coreference
resolution proper.
Starting out with a chunking pipeline, which
uses a classical combination of tagger and chun-
ker, with the Stanford POS tagger (Toutanova et al,
2003), the YamCha chunker (Kudoh and Mat-
sumoto, 2000) and the Stanford Named Entity Rec-
ognizer (Finkel et al, 2005), the desire to use richer
syntactic representations led to the development of
a parsing pipeline, which uses Charniak and John-
son?s reranking parser (Charniak and Johnson, 2005)
to assign POS tags and uses base NPs as chunk
equivalents, while also providing syntactic trees that
can be used by feature extractors. BART also sup-
ports using the Berkeley parser (Petrov et al, 2006),
yielding an easy-to-use Java-only solution.
To provide a better starting point for mention de-
tection on the ACE corpora, the Carafe pipeline
uses an ACE mention tagger provided by MITRE
(Wellner and Vilain, 2006). A specialized merger
then discards any base NP that was not detected to
be an ACE mention.
To perform coreference resolution proper, the
mention-building module uses the markables cre-
ated by the pipeline to create mention objects, which
provide an interface more appropriate for corefer-
ence resolution than the MiniDiscourse markables.
These objects are grouped into equivalence classes
by the resolution process and a coreference layer is
written into the document, which can be used for de-
tailed error analysis.
Feature Extraction BART?s default resolver goes
through all mentions and looks for possible an-
tecedents in previous mentions as described by Soon
et al (2001). Each pair of anaphor and candi-
date is represented as a PairInstance object,
which is enriched with classification features by fea-
ture extractors, and then handed over to a machine
learning-based classifier that decides, given the fea-
tures, whether anaphor and candidate are corefer-
ent or not. Feature extractors are realized as sepa-
rate classes, allowing for their independent develop-
10
Figure 2: Example system configuration
ment. The set of feature extractors that the system
uses is set in an XML description file, which allows
for straightforward prototyping and experimentation
with different feature sets.
Learning BART provides a generic abstraction
layer that maps application-internal representations
to a suitable format for several machine learning
toolkits: One module exposes the functionality of
the the WEKA machine learning toolkit (Witten
and Frank, 2005), while others interface to special-
ized state-of-the art learners. SVMLight (Joachims,
1999), in the SVMLight/TK (Moschitti, 2006) vari-
ant, allows to use tree-valued features. SVM Classi-
fication uses a Java Native Interface-based wrapper
replacing SVMLight/TK?s svm classify pro-
gram to improve the classification speed. Also in-
cluded is a Maximum entropy classifier that is
based upon Robert Dodier?s translation of Liu and
Nocedal?s (1989) L-BFGS optimization code, with
a function for programmatic feature combination.2
Training/Testing The training and testing phases
slightly differ from each other. In the training phase,
the pairs that are to be used as training examples
have to be selected in a process of sample selection,
whereas in the testing phase, it has to be decided
which pairs are to be given to the decision function
and how to group mentions into equivalence rela-
tions given the classifier decisions.
This functionality is factored out into the en-
2see http://riso.sourceforge.net
coder/decoder component, which is separate from
feature extraction and machine learning itself. It
is possible to completely change the basic behav-
ior of the coreference system by providing new
encoders/decoders, and still rely on the surround-
ing infrastructure for feature extraction and machine
learning components.
3 Using BART
Although BART is primarily meant as a platform for
experimentation, it can be used simply as a corefer-
ence resolver, with a performance close to state of
the art. It is possible to import raw text, perform
preprocessing and coreference resolution, and either
work on the MMAX2-format files, or export the re-
sults to arbitrary inline XML formats using XSL
stylesheets.
Adapting BART to a new coreferentially anno-
tated corpus (which may have different rules for
mention extraction ? witness the differences be-
tween the annotation guidelines of MUC and ACE
corpora) usually involves fine-tuning of mention cre-
ation (using pipeline and MentionFactory settings),
as well as the selection and fine-tuning of classi-
fier and features. While it is possible to make rad-
ical changes in the preprocessing by re-engineering
complete pipeline components, it is usually possi-
ble to achieve the bulk of the task by simply mix-
ing and matching existing components for prepro-
cessing and feature extraction, which is possible by
modifying only configuration settings and an XML-
11
BNews NPaper NWire
Recl Prec F Recl Prec F Recl Prec F
basic feature set 0.594 0.522 0.556 0.663 0.526 0.586 0.608 0.474 0.533
extended feature set 0.607 0.654 0.630 0.641 0.677 0.658 0.604 0.652 0.627
Ng 2007? 0.561 0.763 0.647 0.544 0.797 0.646 0.535 0.775 0.633
?: ?expanded feature set? in Ng 2007; Ng trains on the entire ACE training corpus.
Table 1: Performance on ACE-2 corpora, basic vs. extended feature set
based description of the feature set and learner(s)
used.
Several research groups focusing on coreference
resolution, including two not involved in the ini-
tial creation of BART, are using it as a platform
for research including the use of new information
sources (which can be easily incorporated into the
coreference resolution process as features), different
resolution algorithms that aim at enhancing global
coherence of coreference chains, and also adapting
BART to different corpora. Through the availability
of BART as open source, as well as its modularity
and adaptability, we hope to create a larger com-
munity that allows both to push the state of the art
further and to make these improvements available to
users of coreference resolution.
Acknowledgements We thank the CLSP at Johns
Hopkins, NSF and the Department of Defense for
ensuring funding for the workshop and to EML
Research, MITRE, the Center for Excellence in
HLT, and FBK-IRST, that provided partial support.
Yannick Versley was supported by the Deutsche
Forschungsgesellschaft as part of SFB 441 ?Lin-
guistic Data Structures?; Simone Paolo Ponzetto has
been supported by the Klaus Tschira Foundation
(grant 09.003.2004).
References
Charniak, E. and Johnson, M. (2005). Coarse-to-fine n-best
parsing and maxent discriminative reranking. In Proc. ACL
2005.
Culotta, A., Wick, M., and McCallum, A. (2007). First-order
probabilistic models for coreference resolution. In Proc.
HLT/NAACL 2007.
Denis, P. and Baldridge, J. (2007). A ranking approach to pro-
noun resolution. In Proc. IJCAI 2007.
Finkel, J. R., Grenager, T., and Manning, C. (2005). Incorpo-
rating non-local information into information extraction sys-
tems by Gibbs sampling. In Proc. ACL 2005, pages 363?370.
Joachims, T. (1999). Making large-scale SVM learning prac-
tical. In Scho?lkopf, B., Burges, C., and Smola, A., editors,
Advances in Kernel Methods - Support Vector Learning.
Kudoh, T. and Matsumoto, Y. (2000). Use of Support Vector
Machines for chunk identification. In Proc. CoNLL 2000.
Liu, D. C. and Nocedal, J. (1989). On the limited memory
method for large scale optimization. Mathematical Program-
ming B, 45(3):503?528.
McCarthy, J. F. and Lehnert, W. G. (1995). Using decision trees
for coreference resolution. In Proc. IJCAI 1995.
Morton, T. S. (2000). Coreference for NLP applications. In
Proc. ACL 2000.
Moschitti, A. (2006). Making tree kernels practical for natural
language learning. In Proc. EACL 2006.
Mu?ller, C. and Strube, M. (2006). Multi-level annotation of
linguistic data with MMAX2. In Braun, S., Kohn, K., and
Mukherjee, J., editors, Corpus Technology and Language
Pedagogy: New Resources, New Tools, New Methods. Peter
Lang, Frankfurt a.M., Germany.
Ng, V. (2007). Shallow semantics for coreference resolution. In
Proc. IJCAI 2007.
Petrov, S., Barett, L., Thibaux, R., and Klein, D. (2006). Learn-
ing accurate, compact, and interpretable tree annotation. In
COLING-ACL 2006.
Ponzetto, S. P. and Strube, M. (2006). Exploiting semantic role
labeling, WordNet and Wikipedia for coreference resolution.
In Proc. HLT/NAACL 2006.
Qiu, L., Kan, M.-Y., and Chua, T.-S. (2004). A public reference
implementation of the RAP anaphora resolution algorithm.
In Proc. LREC 2004.
Soon, W. M., Ng, H. T., and Lim, D. C. Y. (2001). A machine
learning approach to coreference resolution of noun phrases.
Computational Linguistics, 27(4):521?544.
Steinberger, J., Poesio, M., Kabadjov, M., and Jezek, K. (2007).
Two uses of anaphora resolution in summarization. Informa-
tion Processing and Management, 43:1663?1680. Special
issue on Summarization.
Toutanova, K., Klein, D., Manning, C. D., and Singer, Y.
(2003). Feature-rich part-of-speech tagging with a cyclic de-
pendency network. In Proc. NAACL 2003, pages 252?259.
Uryupina, O. (2006). Coreference resolution with and without
linguistic knowledge. In Proc. LREC 2006.
Versley, Y. (2006). A constraint-based approach to noun phrase
coreference resolution in German newspaper text. In Kon-
ferenz zur Verarbeitung Natu?rlicher Sprache (KONVENS
2006).
Wellner, B. and Vilain, M. (2006). Leveraging machine read-
able dictionaries in discriminative sequence models. In Proc.
LREC 2006.
Witten, I. and Frank, E. (2005). Data Mining: Practical ma-
chine learning tools and techniques. Morgan Kaufmann.
Yang, X., Su, J., and Tan, C. L. (2006). Kernel-based pronoun
resolution with structured syntactic knowledge. In Proc.
CoLing/ACL-2006.
12
Proceedings of NAACL HLT 2009: Short Papers, pages 213?216,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Improving A Simple Bigram HMM Part-of-Speech Tagger by Latent
Annotation and Self-Training
Zhongqiang Huang?, Vladimir Eidelman?, Mary Harper??
?Laboratory for Computational Linguistics and Information Processing
Institute for Advanced Computer Studies
University of Maryland, College Park
?Human Language Technology Center of Excellence
Johns Hopkins University
{zqhuang,vlad,mharper}@umiacs.umd.edu
Abstract
In this paper, we describe and evaluate a bi-
gram part-of-speech (POS) tagger that uses
latent annotations and then investigate using
additional genre-matched unlabeled data for
self-training the tagger. The use of latent
annotations substantially improves the per-
formance of a baseline HMM bigram tag-
ger, outperforming a trigram HMM tagger
with sophisticated smoothing. The perfor-
mance of the latent tagger is further enhanced
by self-training with a large set of unlabeled
data, even in situations where standard bigram
or trigram taggers do not benefit from self-
training when trained on greater amounts of
labeled training data. Our best model obtains
a state-of-the-art Chinese tagging accuracy of
94.78% when evaluated on a representative
test set of the Penn Chinese Treebank 6.0.
1 Introduction
Part-of-speech (POS) tagging, the process of as-
signing every word in a sentence with a POS tag
(e.g., NN (normal noun) or JJ (adjective)), is pre-
requisite for many advanced natural language pro-
cessing tasks. Building upon the large body of re-
search to improve tagging performance for various
languages using various models (e.g., (Thede and
Harper, 1999; Brants, 2000; Tseng et al, 2005b;
Huang et al, 2007)) and the recent work on PCFG
grammars with latent annotations (Matsuzaki et al,
2005; Petrov et al, 2006), we will investigate the use
of fine-grained latent annotations for Chinese POS
tagging. While state-of-the-art tagging systems have
achieved accuracies above 97% in English, Chinese
POS tagging (Tseng et al, 2005b; Huang et al,
2007) has proven to be more challenging, and it is
the focus of this study.
The value of the latent variable approach for tag-
ging is that it can learn more fine grained tags to bet-
ter model the training data. Liang and Klein (2008)
analyzed the errors of unsupervised learning using
EM and found that both estimation and optimiza-
tion errors decrease as the amount of unlabeled data
increases. In our case, the learning of latent anno-
tations through EM may also benefit from a large
set of automatically labeled data to improve tagging
performance. Semi-supervised, self-labeled data has
been effectively used to train acoustic models for
speech recognition (Ma and Schwartz, 2008); how-
ever, early investigations of self-training on POS
tagging have mixed outcomes. Clark et al (2003)
reported positive results with little labeled training
data but negative results when the amount of labeled
training data increases. Wang et al (2007) reported
that self-training improves a trigram tagger?s accu-
racy, but this tagger was trained with only a small
amount of in-domain labeled data.
In this paper, we will investigate whether the
performance of a simple bigram HMM tagger can
be improved by introducing latent annotations and
whether self-training can further improve its perfor-
mance. To the best of our knowledge, this is the first
attempt to use latent annotations with self-training
to enhance the performance of a POS tagger.
2 Model
POS tagging using a hidden Markov model can be
considered as an instance of Bayesian inference,
213
wherein we observe a sequence of words and need
to assign them the most likely sequence of POS tags.
If ti1 denotes the tag sequence t1, ? ? ? , ti, and wi1denotes the word sequence w1, ? ? ? , wi, given the
first-order Markov assumption of a bigram tagger,
the best tag sequence ?(wn1 ) for sentence wn1 can be
computed efficiently as1:
?(wn1 ) = argmaxtn1 p(tn1 |wn1 )
? argmaxtn1
?
i
p(ti|ti?1)p(wi|ti)
with a set of transition parameters {p(b|a)}, for tran-
siting to tag b from tag a, and a set of emission
parameters {p(w|a)}, for generating word w from
tag a. A simple HMM tagger is trained by pulling
counts from labeled data and normalizing to get the
conditional probabilities.
It is well know that the independence assumption
of a bigram tagger is too strong in many cases. A
common practice for weakening the independence
assumption is to use a second-order Markov as-
sumption, i.e., a trigram tagger. This is similar to
explicitly annotating each POS tag with the preced-
ing tag. Rather than explicit annotation, we could
use latent annotations to split the POS tags, sim-
ilarly to the introduction of latent annotations to
PCFG grammars (Matsuzaki et al, 2005; Petrov
et al, 2006). For example, the NR tag may be
split into NR-1 and NR-2, and correspondingly the
POS tag sequence of ?Mr./NR Smith/NR saw/VV
Ms./NR Smith/NR? could be refined as: ?Mr./NR-2
Smith/NR-1 saw/VV-2 Ms./NR-2 Smith/NR-1?.
The objective of training a bigram tagger with la-
tent annotations is to find the transition and emission
probabilities associated with the latent tags such that
the likelihood of the training data is maximized. Un-
like training a standard bigram tagger where the POS
tags are observed, in the latent case, the latent tags
are not observable, and so a variant of EM algorithm
is used to estimate the parameters.
Given a sentence wn1 and its tag sequence tn1 , con-sider the i-th word wi and its latent tag ax ? a = ti
(which means ax is a latent tag of tag a, the i-th tag
in the sequence) and the (i + 1)-th word wi+1 and
its latent tag by ? b = ti+1, the forward, ?i+1(by) =
p(wi+11 , by), and backward, ?i(ax) = p(wni+1|ax),probabilities can be computed recursively:
?i+1(by) =
?
x
?i(ax)p(by|ax)p(wi+1|by)
1We assume that symbols exist implicitly for boundary con-
ditions.
?i(ax) =
?
y
p(by|ax)p(wi+1|by)?j+1(by)
In the E step, the posterior probabilities of co-
occurrence events can be computed as:
p(ax, by|w) ? ?i(ax)p(by|ax)?i+1(by)
p(ax, wi|w) ? ?i(ax)?i(ax)
In the M step, the above posterior probabilities are
used as weighted observations to update the transi-
tion and emission probabilities2:
p(by|ax) = c(ax, by)/
?
by
c(ax, by)
p(w|ax) = c(ax, w)/
?
w
c(ax, w)
A hierarchical split-and-merge method, similar
to (Petrov et al, 2006), is used to gradually increase
the number of latent annotations while allocating
them adaptively to places where they would pro-
duce the greatest increase in training likelihood (e.g.,
we observe heavy splitting in categories such as NN
(normal noun) and VV (verb), that cover a wide vari-
ety of words, but only minimal splitting in categories
like IJ (interjection) and ON (onomatopoeia)).
Whereas tag transition occurrences are frequent,
allowing extensive optimization using EM, word-tag
co-occurrences are sparser and more likely to suf-
fer from over-fitting. To handle this problem, we
map all words with frequency less than threshold3
? to symbol unk and for each latent tag accumu-
late the word tag statistics of these rare words to
cr(ax, unk) = ?w:c(w)<? c(ax, w). These statistics
are redistributed among the rare words (w : c(w) <
?) to compute their emission probabilities:
c(ax, w) = cr(ax, unk) ? c(a,w)/cr(a, unk)
p(w|ax) = c(ax, w)/
?
w
c(ax, w)
The impact of this rare word handling method will
be investigated in Section 3.
A character-based unknown word model, similar
to the one described in (Huang et al, 2007), is used
to handle unknown Chinese words during tagging.
A decoding method similar to the max-rule-product
method in (Petrov and Klein, 2007) is used to tag
sentences using our model.
3 Experiments
The Penn Chinese Treebank 6.0 (CTB6) (Xue et al,
2005) is used as the labeled data in our study. CTB6
2c(?) represents the count of the event.
3The value of ? is tuned on the development set.
214
contains news articles, which are used as the primary
source of labeled data in our experiments, as well as
broadcast news transcriptions. Since the news ar-
ticles were collected during different time periods
from different sources with a diversity of topics, in
order to obtain a representative split of train-test-
development sets, we divide them into blocks of 10
files in sorted order and for each block use the first
file for development, the second for test, and the re-
maining for training. The broadcast news data ex-
hibits many of the characteristics of newswire text
(it contains many nonverbal expressions, e.g., num-
bers and symbols, and is fully punctuated) and so is
also included in the training data set. We also uti-
lize a greater number of unlabeled sentences in the
self-training experiments. They are selected from
similar sources to the newswire articles, and are
normalized (Zhang and Kahn, 2008) and word seg-
mented (Tseng et al, 2005a). See Table 1 for a sum-
mary of the data used.
Train Dev Test Unlabeled
sentences 24,416 1904 1975 210,000
words 678,811 51,229 52,861 6,254,947
Table 1: The number of sentences and words in the data.
50 100 150 200 250 300 350 40091.5
92
92.5
93
93.5
94
94.5
Number of latent annotations
Tok
en 
acc
ura
cy (%
)
Bigram+LA:1
Bigram+LA:2
Trigram
Figure 1: The learning curves of the bigram tagger with
latent annotations on the development set.
Figure 1 plots the learning curves of two bigram
taggers with latent annotations (Bigram+LA:2 has
the special handling of rare words as described in
Section 2 while Bigram+LA:1 does not) and com-
pares its performance with a state-of-the-art trigram
HMM tagger (Huang et al, 2007) that uses trigram
transition and emission models together with bidi-
rectional decoding. Both bigram taggers initially
have much lower tagging accuracy than the trigram
tagger, due to its strong but invalid independence as-
sumption. As the number of latent annotations in-
creases, the bigram taggers are able to learn more
from the context based on the latent annotations,
and their performance improves significantly, out-
performing the trigram tagger. The performance
gap between the two bigram taggers suggests that
over-fitting occurs in the word emission model when
more latent annotations are available for optimiza-
tion; sharing the statistics among rare words alle-
viates some of the sparseness while supporting the
modeling of deeper dependencies among more fre-
quent events. In the later experiments, we use Bi-
gram+LA to denote the Bigram+LA:2 tagger.
Figure 2 compares the self-training capability of
three models (the bigram tagger w/ or w/o latent
annotations, and the aforementioned trigram tagger)
using different sizes of labeled training data and the
full set of unlabeled data. For each model, a tag-
ger is first trained on the allocated labeled training
data and is then used to tag the unlabeled data. A
new tagger is then trained on the combination4 of
the allocated labeled training data and the newly au-
tomatically labeled data.
0.1 0.2 0.4 0.6 0.8 189
90
91
92
93
94
95
Fraction of CTB6 training data
Tok
en 
acc
ura
cy (%
)
Bigram+LA+STBigram+LATrigram+STTrigramBigram+STBigram
Figure 2: The performance of three taggers evaluated on
the development set, before and after self-training with
different sizes of labeled training data.
There are two interesting observations that distin-
guish the bigram tagger with latent annotations from
the other two taggers. First, although all of the tag-
gers improve as more labeled training data is avail-
able, the performance gap between the bigram tag-
ger with latent annotations and the other two taggers
also increases. This is because more latent annota-
tions can be used to take advantage of the additional
training data to learn deeper dependencies.
Second, the bigram tagger with latent annotations
benefits much more from self-training, although it
4We always balance the size of manually and automatically
labeled data through duplication (for the trigram tagger) or pos-
terior weighting (for the bigram tagger w/ or w/o latent annota-
tions), as this provides superior performance.
215
already has the highest performance among the three
taggers before self-training. The bigram tagger
without latent annotations benefits little from self-
training. Except for a slight improvement when
there is a small amount of labeled training, self-
training slightly hurts tagging performance as the
amount of labeled data increases. The trigram tag-
ger benefits from self-training initially but eventu-
ally has a similar pattern to the bigram tagger when
trained on the full labeled set. The performance
of the latent bigram tagger improves consistently
with self-training. Although the gain decreases for
models trained on larger training sets, since stronger
models are harder to improve, self-training still con-
tributes significantly to model accuracy.
The final tagging performance on the test set is
reported in Table 2. All of the improvements are
statistically significant (p < 0.005).
Tagger Token Accuracy (%)
Bigram 92.25
Trigram 93.99
Bigram+LA 94.53
Bigram+LA+ST 94.78
Table 2: The performance of the taggers on the test set.
It is worth mentioning that we initially added la-
tent annotations to a trigram tagger, rather than a bi-
gram tagger, to build from a stronger starting point;
however, this did not work well. A trigram tagger re-
quires sophisticated smoothing to handle data spar-
sity, and introducing latent annotations exacerbates
the sparsity problem, especially for trigram word
emissions. The uniform extension of a bigram tag-
ger to a trigram tagger ignores whether the use of ad-
ditional context is helpful and supported by enough
data, nor is it able to use a longer context. In con-
trast, the bigram tagger with latent annotations is
able to learn different granularities for tags based on
the training data.
4 Conclusion
In this paper, we showed that the accuracy of a sim-
ple bigram HMM tagger can be substantially im-
proved by introducing latent annotations together
with proper handling of rare words. We also showed
that this tagger is able to benefit from self-training,
despite the fact that other models, such as bigram or
trigram HMM taggers, do not.
In the future work, we will investigate automatic
data selection methods to choose materials that are
most suitable for self-training and evaluate the effect
of the amount of automatically labeled data.
Acknowledgments
This work was supported by NSF IIS-0703859
and DARPA HR0011-06-C-0023 and HR0011-06-
2-001. Any opinions, findings and/or recommenda-
tions expressed in this paper are those of the authors
and do not necessarily reflect the views of the fund-
ing agencies.
References
T. Brants. 2000. TnT a statistical part-of-speech tagger.
In ANLP.
S. Clark, J. R. Curran, and M. Osborne. 2003. Bootstrap-
ping pos taggers using unlabelled data. In CoNLL.
Z. Huang, M. Harper, and W. Wang. 2007. Mandarin
part-of-speech tagging and discriminative reranking.
EMNLP.
P. Liang and D. Klein. 2008. Analyzing the errors of
unsupervised learning. In ACL.
J. Ma and R. Schwartz. 2008. Factors that affect unsu-
pervised training of acoustic models. In Interspeech.
T. Matsuzaki, Y. Miyao, and J. Tsujii. 2005. Probabilis-
tic CFG with latent annotations. In ACL. Association
for Computational Linguistics.
S. Petrov and D. Klein. 2007. Improved inference for
unlexicalized parsing. In HLT-NAACL.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning accurate, compact, and interpretable tree an-
notation. In ACL.
S. M. Thede and M. P. Harper. 1999. A second-order
hidden markov model for part-of-speech tagging. In
ACL.
H. Tseng, P. Chang, G. Andrew, D. Jurafsky, and C. Man-
ning. 2005a. A conditional random field word seg-
menter. In SIGHAN Workshop on Chinese Language
Processing.
H. Tseng, D. Jurafsky, and C. Manning. 2005b. Morpho-
logical features help pos tagging of unknown words
across language varieties. In SIGHAN Workshop on
Chinese Language Processing.
W. Wang, Z. Huang, and M. Harper. 2007. Semi-
supervised learning for part-of-speech tagging of Man-
darin transcribed speech. In ICASSP.
N. Xue, F. Xia, F. Chiou, and M. Palmer. 2005. The
penn chinese treebank: Phrase structure annotation of
a large corpus. Natural Language Engineering.
B. Zhang and J. G. Kahn. 2008. Evaluation of decatur
text normalizer for language model training. Technical
report, University of Washington.
216
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 821?831,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Lessons Learned in Part-of-Speech Tagging of Conversational Speech
Vladimir Eidelman?, Zhongqiang Huang?, and Mary Harper??
?Laboratory for Computational Linguistics and Information Processing
Institute for Advanced Computer Studies
University of Maryland, College Park, MD
?Human Language Technology Center of Excellence
Johns Hopkins University, Baltimore, MD
{vlad,zhuang,mharper}@umiacs.umd.edu
Abstract
This paper examines tagging models for spon-
taneous English speech transcripts. We ana-
lyze the performance of state-of-the-art tag-
ging models, either generative or discrimi-
native, left-to-right or bidirectional, with or
without latent annotations, together with the
use of ToBI break indexes and several meth-
ods for segmenting the speech transcripts (i.e.,
conversation side, speaker turn, or human-
annotated sentence). Based on these studies,
we observe that: (1) bidirectional models tend
to achieve better accuracy levels than left-to-
right models, (2) generative models seem to
perform somewhat better than discriminative
models on this task, and (3) prosody improves
tagging performance of models on conversa-
tion sides, but has much less impact on smaller
segments. We conclude that, although the use
of break indexes can indeed significantly im-
prove performance over baseline models with-
out them on conversation sides, tagging ac-
curacy improves more by using smaller seg-
ments, for which the impact of the break in-
dexes is marginal.
1 Introduction
Natural language processing technologies, such as
parsing and tagging, often require reconfiguration
when they are applied to challenging domains that
differ significantly from newswire, e.g., blogs, twit-
ter text (Foster, 2010), or speech. In contrast to
text, conversational speech represents a significant
challenge because the transcripts are not segmented
into sentences. Furthermore, the transcripts are of-
ten disfluent and lack punctuation and case informa-
tion. On the other hand, speech provides additional
information, beyond simply the sequence of words,
which could be exploited to more accurately assign
each word in the transcript a part-of-speech (POS)
tag. One potentially beneficial type of information
is prosody (Cutler et al, 1997).
Prosody provides cues for lexical disambigua-
tion, sentence segmentation and classification,
phrase structure and attachment, discourse struc-
ture, speaker affect, etc. Prosody has been found
to play an important role in speech synthesis sys-
tems (Batliner et al, 2001; Taylor and Black, 1998),
as well as in speech recognition (Gallwitz et al,
2002; Hasegawa-Johnson et al, 2005; Ostendorf et
al., 2003). Additionally, prosodic features such as
pause length, duration of words and phones, pitch
contours, energy contours, and their normalized val-
ues have been used for speech processing tasks like
sentence boundary detection (Liu et al, 2005).
Linguistic encoding schemes like ToBI (Silver-
man et al, 1992) have also been used for sentence
boundary detection (Roark et al, 2006; Harper et al,
2005), as well as for parsing (Dreyer and Shafran,
2007; Gregory et al, 2004; Kahn et al, 2005). In
the ToBI scheme, aspects of prosody such as tone,
prominence, and degree of juncture between words
are represented symbolically. For instance, Dreyer
and Shafran (2007) use three classes of automati-
cally detected ToBI break indexes, indicating major
intonational breaks with a 4, hesitation with a p, and
all other breaks with a 1.
Recently, Huang and Harper (2010) found that
they could effectively integrate prosodic informa-
821
tion in the form of this simplified three class ToBI
encoding when parsing spontaneous speech by us-
ing a prosodically enriched PCFG model with latent
annotations (PCFG-LA) (Matsuzaki et al, 2005;
Petrov and Klein, 2007) to rescore n-best parses
produced by a baseline PCFG-LA model without
prosodic enrichment. However, the prosodically en-
riched models by themselves did not perform sig-
nificantly better than the baseline PCFG-LA model
without enrichment, due to the negative effect that
misalignments between automatic prosodic breaks
and true phrase boundaries have on the model.
This paper investigates methods for using state-
of-the-art taggers on conversational speech tran-
scriptions and the effect that prosody has on tagging
accuracy. Improving POS tagging performance of
speech transcriptions has implications for improving
downstream applications that rely on accurate POS
tags, including sentence boundary detection (Liu
et al, 2005), automatic punctuation (Hillard et al,
2006), information extraction from speech, parsing,
and syntactic language modeling (Heeman, 1999;
Filimonov and Harper, 2009). While there have
been several attempts to integrate prosodic informa-
tion to improve parse accuracy of speech transcripts,
to the best of our knowledge there has been little
work on using this type of information for POS tag-
ging. Furthermore, most of the parsing work has
involved generative models and rescoring/reranking
of hypotheses from the generative models. In this
work, we will analyze several factors related to ef-
fective POS tagging of conversational speech:
? discriminative versus generative POS tagging
models (Section 2)
? prosodic features in the form of simplified ToBI
break indexes (Section 4)
? type of speech segmentation (Section 5)
2 Models
In order to fully evaluate the difficulties inherent in
tagging conversational speech, as well as the possi-
ble benefits of prosodic information, we conducted
experiments with six different POS tagging mod-
els. The models can be broadly separated into two
classes: generative and discriminative. As the first
of our generative models, we used a Hidden Markov
Model (HMM) trigram tagger (Thede and Harper,
1999), which serves to establish a baseline and to
gauge the difficulty of the task at hand. Our sec-
ond model, HMM-LA, was the latent variable bi-
gram HMM tagger of Huang et al (2009), which
achieved state-of-the-art tagging performance by in-
troducing latent tags to weaken the stringent Markov
independence assumptions that generally hinder tag-
ging performance in generative models.
For the third model, we implemented a bidirec-
tional variant of the HMM-LA (HMM-LA-Bidir)
that combines evidence from two HMM-LA tag-
gers, one trained left-to-right and the other right-to-
left. For decoding, we use a product model (Petrov,
2010). The intuition is that the context information
from the left and the right of the current position
is complementary for predicting the current tag and
thus, the combination should serve to improve per-
formance over the HMM-LA tagger.
Since prior work on parsing speech with prosody
has relied on generative models, it was necessary
to modify equations of the model in order to incor-
porate the prosodic information, and then perform
rescoring in order to achieve gains. However, it is
far simpler to directly integrate prosody as features
into the model by using a discriminative approach.
Hence, we also investigate several log-linear mod-
els, which allow us to easily include an arbitrary
number and varying kinds of possibly overlapping
and non-independent features.
First, we implemented a Conditional Random
Field (CRF) tagger, which is an attractive choice due
to its ability to learn the globally optimal labeling
for a sequence and proven excellent performance on
sequence labeling tasks (Lafferty et al, 2001). In
contrast to an HMM which optimizes the joint like-
lihood of the word sequence and tags, a CRF opti-
mizes the conditional likelihood, given by:
p?(t|w) =
exp
?
j ?jFj(t, w)
?
t exp
?
j ?jFj(t, w)
(1)
where the ??s are the parameters of the model to es-
timate and F indicates the feature functions used.
The denominator in (1) is Z?(x), the normalization
factor, with:
Fj(t, w) =
?
i
fj(t, w, i)
822
Class Model Name Latent Variable Bidirectional N-best-Extraction Markov Order
Generative
Trigram HMM
?
2nd
HMM-LA
? ?
1st
HMM-LA-Bidir
? ?
1st
Discriminative
Stanford Bidir
?
2nd
Stanford Left5 2nd
CRF 2nd
Table 1: Description of tagging models
The objective we need to maximize then becomes :
L =
?
n
?
?
?
j
?jFj(tn, wn)? logZ?(xn)
?
??
???2
2?2
where we use a spherical Gaussian prior to pre-
vent overfitting of the model (Chen and Rosen-
feld, 1999) and the wide-spread quasi-Newtonian
L-BFGS method to optimize the model parame-
ters (Liu and Nocedal, 1989). Decoding is per-
formed with the Viterbi algorithm.
We also evaluate state-of-the-art Maximum En-
tropy taggers: the Stanford Left5 tagger (Toutanova
and Manning, 2000) and the Stanford bidirectional
tagger (Toutanova et al, 2003), with the former us-
ing only left context and the latter bidirectional de-
pendencies.
Table 1 summarizes the major differences be-
tween the models along several dimensions: (1) gen-
erative versus discriminative, (2) directionality of
decoding, (3) the presence or absence of latent anno-
tations, (4) the availability of n-best extraction, and
(5) the model order.
In order to assess the quality of our models, we
evaluate them on the section 23 test set of the stan-
dard newswire WSJ tagging task after training all
models on sections 0-22. Results appear in Ta-
ble 2. Clearly, all the models have high accuracy
on newswire data, but the Stanford bidirectional tag-
ger significantly outperforms the other models with
the exception of the HMM-LA-Bidir model on this
task.1
1Statistically significant improvements are calculated using
the sign test (p < 0.05).
Model Accuracy
Trigram HMM 96.58
HMM-LA 97.05
HMM-LA-Bidir 97.16
Stanford Bidir 97.28
Stanford Left5 97.07
CRF 96.81
Table 2: Tagging accuracy on WSJ
3 Experimental Setup
In the rest of this paper, we evaluate the tag-
ging models described in Section 2 on conver-
sational speech. We chose to utilize the Penn
Switchboard (Godfrey et al, 1992) and Fisher tree-
banks (Harper et al, 2005; Bies et al, 2006) because
they provide gold standard tags for conversational
speech and we have access to corresponding auto-
matically generated ToBI break indexes provided by
(Dreyer and Shafran, 2007; Harper et al, 2005)2.
We utilized the Fisher dev1 and dev2 sets contain-
ing 16,519 sentences (112,717 words) as the primary
training data and the entire Penn Switchboard tree-
bank containing 110,504 sentences (837,863 words)
as an additional training source3. The treebanks
were preprocessed as follows: the tags of auxiliary
verbs were replaced with the AUX tag, empty nodes
2A small fraction of words in the Switchboard treebank do
not align with the break indexes because they were produced
based on a later refinement of the transcripts used to produce
the treebank. For these cases, we heuristically added break *1*
to words in the middle of a sentence and *4* to words that end
a sentence.
3Preliminary experiments evaluating the effect of training
data size on performance indicated using the additional Switch-
board data leads to more accurate models, and so we use the
combined training set.
823
and function tags were removed, words were down-
cased, punctuation was deleted, and the words and
their tags were extracted. Because the Fisher tree-
bank was developed using the lessons learned when
developing Switchboard, we chose to use its eval
portion for development (the first 1,020 tagged sen-
tences containing 7,184 words) and evaluation (the
remaining 3,917 sentences with 29,173 words).
We utilize the development set differently for the
generative and discriminative models. Since the EM
algorithm used for estimating the parameters in the
latent variable models introduces a lot of variabil-
ity, we train five models with a different seed and
then choose the best one based on dev set perfor-
mance. For the discriminative models, we tuned
their respective regularization parameters on the dev
set. All results reported in the rest of this paper are
on the test set.
4 Integration of Prosodic Information
In this work, we use three classes of automatically
generated ToBI break indexes to represent prosodic
information (Kahn et al, 2005; Dreyer and Shafran,
2007; Huang and Harper, 2010): 4, 1, and p.
Consider the following speech transcription exam-
ple, which is enriched with ToBI break indexes in
parentheses and tags: i(1)/PRP did(1)/VBD
n?t(1)/RB you(1)/PRP know(4)/VBP
i(1)/PRP did(1)/AUX n?t(1)/RB...
The speaker begins an utterance, and then restarts
the utterance. The automatically predicted break 4
associated with know in the utterance compellingly
indicates an intonational phrase boundary and could
provide useful information for tagging if we can
model it appropriately.
To integrate prosody into our generative models,
we utilize the method from (Dreyer and Shafran,
2007) to add prosodic breaks. As Figure 1 shows,
ToBI breaks provide a secondary sequence of ob-
servations that is parallel to the sequence of words
that comprise the sentence. Each break bi in the sec-
ondary sequence is generated by the same tag ti as
that which generates the corresponding wordwi, and
so it is conditionally independent of its correspond-
ing word given the tag:
P (w, b|t) = P (w|t)P (b|t)
PRP
i
1
VBD
did
1
RB
n?t
1
VBP
know
4
Figure 1: Parallel generation of words and breaks for the
HMM models
The HMM-LA taggers are then able to split tags to
capture implicit higher order interactions among the
sequence of tags, words, and breaks.
The discriminative models are able to utilize
prosodic features directly, enabling the use of con-
textual interactions with other features to further im-
prove tagging accuracy. Specifically, in addition to
the standard set of features used in the tagging lit-
erature, we use the feature templates presented in
Table 3, where each feature associates the break bi,
word wi, or some combination of the two with the
current tag ti4.
Break and/or word values Tag value
bi=B ti = T
bi=B & bi?1=C ti = T
wi=W & bi=B ti = T
wi+1=W & bi=B ti = T
wi+2=W & bi=B ti = T
wi?1=W & bi=B ti = T
wi?2=W & bi=B ti = T
wi=W & bi=B & bi?1=C ti = T
Table 3: Prosodic feature templates
5 Experiments
5.1 Conversation side segmentation
When working with raw speech transcripts, we ini-
tially have a long stream of unpunctuated words,
which is called a conversation side. As the average
length of conversation side segments in our data is
approximately 630 words, it poses quite a challeng-
ing tagging task. Thus, we hypothesize that it is on
these large segments that we should achieve the most
4We modified the Stanford taggers to handle these prosodic
features.
824
93
93.3
93.6
93.9
94.2
94.5
HMM-LA HMM-LA Bidir Stanford Bidir Stanford Left5 CRF
Baseline Prosody OracleBreak OracleBreak+Sent OracleSent OracleBreak-Sent Rescoring
Figure 2: Tagging accuracy on conversation sides
improvement from the addition of prosodic informa-
tion.
In fact, as the baseline results in Figure 2 show,
the accuracies achieved on this task are much lower
than those on the newswire task. The trigram HMM
tagger accuracy drops to 92.43%, while all the other
models fall to within the range of 93.3%-94.12%,
a significant departure from the 96-97.3% range on
newswire sentences. Note that the Stanford bidi-
rectional and HMM-LA tagger perform very simi-
larly, although the HMM-LA-Bidir tagger performs
significantly better than both. In contrast to the
newswire task on which the Stanford bidirectional
tagger performed the best, on this genre, it is slightly
worse than the HMM-LA tagger, albeit the differ-
ence is not statistically significant.
With the direct integration of prosody into the
generative models (see Figure 2), there is a slight but
statistically insignificant shift in performance. How-
ever, integrating prosody directly into the discrimi-
native models leads to significant improvements in
the CRF and Stanford Left5 taggers. The gain in
the Stanford bidirectional tagger is not statistically
significant, however, which suggests that the left-
to-right models benefit more from the addition of
prosody than bidirectional models.
5.2 Human-annotated sentences
Given the lack-luster performance of the tagging
models on conversation side segments, even with the
direct addition of prosody, we chose to determine the
performance levels that could be achieved on this
task using human-annotated sentences, which we
will refer to as sentence segmentation. Figure 3 re-
ports the baseline tagging accuracy on sentence seg-
ments, and we see significant improvements across
all models. The HMM Trigram tagger performance
increases to 93.00%, while the increase in accuracy
for the other models ranges from around 0.2-0.3%.
The HMM-LA taggers once again achieve the best
performance, with the Stanford bidirectional close
behind. Although the addition of prosody has very
little impact on either the generative or discrimina-
tive models when applied to sentences, the base-
line tagging models (i.e., not prosodically enriched)
significantly outperform all of the prosodically en-
riched models operating on conversation sides.
At this point, it would be apt to suggest us-
ing automatic sentence boundary detection to cre-
ate shorter segments. Table 4 presents the results
of using baseline models without prosodic enrich-
ment trained on the human-annotated sentences to
tag automatically segmented speech5. As can be
seen, the results are quite similar to the conversation
side segmentation performances, and thus signifi-
cantly lower than when tagging human-annotated
sentences. A caveat to consider here is that we break
the standard assumption that the training and test set
be drawn from the same distribution, since the train-
ing data is human-annotated and the test is automat-
ically segmented. However, it can be quite challeng-
ing to create a corpus to train on that represents the
biases of the systems that perform automatic sen-
tence segmentation. Instead, we will examine an-
5We used the Baseline Structural Metadata System de-
scribed in Harper et al (2005) to predict sentence boundaries.
825
93
93.3
93.6
93.9
94.2
94.5
HMM-LA HMM-LA Bidir Stanford Bidir Stanford Left5 CRF
Baseline Prosody OracleBreak Rescoring
Figure 3: Tagging accuracy on human-annotated segments
other segmentation method to shorten the segments
automatically, i.e., by training and testing on speaker
turns, which preserves the train-test match, in Sec-
tion 5.5.
Model Accuracy
HMM-LA 93.95
HMM-LA-Bidir 94.07
Stanford Bidir 93.77
Stanford Left5 93.35
CRF 93.29
Table 4: Baseline tagging accuracy on automatically de-
tected sentence boundaries
5.3 Oracle Break Insertion
As we believe one of the major roles that prosodic
cues serve for tagging conversation sides is as a
proxy for sentence boundaries, perhaps the efficacy
of the prosodic breaks can, at least partially, be at-
tributed to errors in the automatically induced break
indexes themselves, as they can misalign with syn-
tactic phrase boundaries, as discussed in Huang and
Harper (2010). This may degrade the performance
of our models more than the improvement achieved
from correctly placed breaks. Hence, we conduct
a series of experiments in which we systematically
eliminate noisy phrase and disfluency breaks and
show that under these improved conditions, prosodi-
cally enriched models can indeed be more effective.
To investigate to what extent noisy breaks are im-
peding the possible improvements from prosodically
enriched models, we replaced all 4 and p breaks in
the training and evaluation sets that did not align
to the correct phrase boundaries as indicated by the
treebank with break 1 for both the conversation sides
and human-annotated sentences. The results from
using Oracle Breaks on conversation sides can be
seen in Figure 2. All models except Stanford Left5
and HMM-LA-Bidir significantly improve in accu-
racy when trained and tested on the Oracle Break
modified data. On human-annotated sentences, Fig-
ure 3 shows improvements in accuracies across all
models, however, they are statistically insignificant.
To further analyze why prosodically enriched
models achieve more improvement on conversation
sides than on sentences, we conducted three more
Oracle experiments on conversation sides. For the
first, OracleBreak-Sent, we further modified the data
such that all breaks corresponding to a sentence
ending in the human-annotated segments were con-
verted to break 1, thus effectively only leaving in-
side sentence phrasal boundaries. This modification
results in a significant drop in performance, as can
be seen in Figure 2.
For the second, OracleSent, we converted all
the breaks corresponding to a sentence end in the
human-annotated segmentations to break 4, and all
the others to break 1, thus effectively only leaving
sentence boundary breaks. This performed largely
on par with OracleBreak, suggesting that the phrase-
aligned prosodic breaks seem to be a stand-in for
sentence boundaries.
Finally, in the last condition, OracleBreak+Sent,
we modified the OracleBreak data such that all
breaks corresponding to a sentence ending in the
human-annotated sentences were converted to break
826
93
93.3
93.6
93.9
94.2
94.5
HMM-LA HMM-LA Bidir Stanford Bidir Stanford Left5 CRF
Baseline Prosody Rescoring
Figure 4: Tagging accuracy on speaker turns
4 (essentially combining OracleBreak and Oracle-
Sent). As Figure 2 indicates, this modification re-
sults in the best tagging accuracies for all the mod-
els. All models were able to match or even improve
upon the baseline accuracies achieved on the human
segmented data. This suggests that when we have
breaks that align with phrasal and sentence bound-
aries, prosodically enriched models are highly effec-
tive.
5.4 N-best Rescoring
Based on the findings in the previous section and the
findings of (Huang and Harper, 2010), we next ap-
ply a rescoring strategy in which the search space
of the prosodically enriched generative models is re-
stricted to the n-best list generated from the base-
line model (without prosodic enrichment). In this
manner, the prosodically enriched model can avoid
poor tag sequences produced due to the misaligned
break indexes. As Figure 2 shows, using the base-
line conversation side model to produce an n-best
list for the prosodically enriched model to rescore
results in significant improvements in performance
for the HMM-LA model, similar to the parsing re-
sults of (Huang and Harper, 2010). The size of the
n-best list directly impacts performance, as reducing
to n = 1 is akin to tagging with the baseline model,
and increasing n ? ? amounts to tagging with the
prosodically enriched model. We experimented with
a number of different sizes for n and chose the best
one using the dev set. Figure 3 presents the results
for this method applied to human-annotated sen-
tences, where it produces only marginal improve-
ments6.
5.5 Speaker turn segmentation
The results presented thus far indicate that if we
have access to close to perfect break indexes, we
can use them effectively, but this is not likely to be
true in practice. We have also observed that tagging
accuracy on shorter conversation sides is greater
than longer conversation sides, suggesting that post-
processing the conversation sides to produce shorter
segments would be desirable.
We thus devised a scheme by which we could
automatically extract shorter speaker turn segments
from conversation sides. For this study, speaker
turns, which effectively indicate speaker alterna-
tions, were obtained by using the metadata in the
treebank to split the sentences into chunks based on
speaker change. Every time a speaker begins talk-
ing after the other speaker was talking, we start a
new segment for that speaker. In practice, this would
need to be done based on audio cues and automatic
transcriptions, so these results represent an upper
bound.
Figure 4 presents tagging results on speaker turn
segments. For most models, the difference in accu-
racy achieved on these segments and that of human-
annotated sentences is statistically insignificant. The
only exception is the Stanford bidirectional tagger,
6Rescoring using the CRF model was also performed, but
led to a performance degradation. We believe this is due to
the fact that the prosodically enriched CRF model was able to
directly use the break index information, and so restricting it to
the baseline CRF model search space limits the performance to
that of the baseline model.
827
0100
200
300
400
NNP RP AUX JJ PRP RB WDT VBP VBZ UH XX VB NN DT VBD IN
Number
 o
f Er
ro
rs
 Conv Baseline Conv Rescore Conv OracleBreak Sent Baseline
(a) Number of errors by part of speech category for the HMM-LA model with and without prosody
0
100
200
300
400
NNP RP AUX JJ PRP RB WDT VBP VBZ UH XX VB NN DT VBD IN
Number
 o
f Er
ro
rs
 Conv Baseline Conv Prosody Conv OracleBreak Sent Baseline
(b) Number of errors by part of speech category for the CRF model with and without prosody
Figure 5: Error reduction for prosodically enriched HMM-LA (a) and CRF (b) models
which performs worse on these slightly longer seg-
ments. With the addition of break indexes, we see
marginal changes in most of the models; only the
CRF tagger receives a significant boost. Thus, mod-
els achieve performance gains from tagging shorter
segments, but at the cost of limited usefulness of the
prosodic breaks. Overall, speaker turn segmenta-
tion is an attractive compromise between the original
conversation sides and human-annotated sentences.
6 Discussion
Across the different models, we have found that tag-
gers applied to shorter segments, either sentences or
speaker turns, do not tend to benefit significantly
from prosodic enrichment, in contrast to conversa-
tion sides. To analyze this further we broke down
the results by part of speech for the two models
for which break indexes improved performance the
most: the CRF and HMM-LA rescoring models,
which achieved an overall error reduction of 2.8%
and 2.1%, respectively. We present those categories
that obtained the greatest benefit from prosody in
Figure 5 (a) and (b). For both models, the UH cate-
gory had a dramatic improvement from the addition
of prosody, achieving up to a 10% reduction in error.
For the CRF model, other categories that saw im-
pressive error reductions were NN and VB, with
10% and 5%, respectively. Table 5 lists the prosodic
features that received the highest weight in the CRF
model. These are quite intuitive, as they seem to rep-
resent places where the prosody indicates sentence
or clausal boundaries. For the HMM-LA model,
the VB and DT tags had major reductions in error
of 13% and 10%, respectively. For almost all cat-
egories, the number of errors is reduced by the ad-
dition of breaks, and further reduced by using the
OracleBreak processing described above.
Weight Feature
2.2212 wi=um & bi=4 & t=UH
1.9464 wi=uh & bi=4 & t=UH
1.7965 wi=yes & bi=4 & t=UH
1.7751 wi=and & bi=4 & t=CC
1.7554 wi=so & bi=4 & t=RB
1.7373 wi=but & bi=4 & t=CC
Table 5: Top break 4 prosody features in CRF prosody
model
To determine more precisely the effect that the
segment size has on tagging accuracy, we extracted
the oracle tag sequences from the HMM-LA and
CRF baseline and prosodically enriched models
across conversation sides, sentences, and speaker
turn segments. As the plot in Figure 6 shows, as
we increase the n-best list size to 500, the ora-
cle accuracy of the models trained on sentences in-
828
92
94
96
98
100
2 5 10 20 50 100 200 500
Ac
cur
ac
y
 
N -Best size  
Sentences 
Speaker  tuns  
Conversation sides  
Figure 6: Oracle comparison: solid lines for sentences,
dashed lines for speaker turns, and dotted lines for con-
versation sides
creases rapidly to 99%; whereas, the oracle accu-
racy of models on conversation sides grow slowly
to between 94% and 95%. The speaker turn trained
models, however, behave closely to those using sen-
tences, climbing rapidly to accuracies of around
98%. This difference is directly attributable to the
length of the segments. As can be seen in Table 6,
the speaker turn segments are more comparable in
length to sentences.
Train Eval
Conv 627.87 ? 281.57 502.98 ? 151.22
Sent 7.52? 7.86 7.45 ? 8.29
Speaker 15.60? 29.66 15.27? 21.01
Table 6: Length statistics of different data segmentations
Next, we return to the large performance degrada-
tion when tagging speech rather than newswire text
to examine the major differences among the mod-
els. Using two of our best performing models, the
Stanford bidirectional and HMM-LA, in Figure 7
we present the categories for which performance
degradation was the greatest when comparing per-
formance of a tagger trained on WSJ to a tagger
trained on spoken sentences and conversation sides.
The performance decrease is quite similar across
both models, with the greatest degradation on the
NNP, RP, VBN, and RBS categories.
Unsurprisingly, both the discriminative and gen-
erative bidirectional models achieve the most im-
pressive results. However, the generative HMM-
LA and HMM-LA-Bidir models achieved the best
results across all three segmentations, and the best
overall result, of 94.35%, on prosodically enriched
sentence-segmented data. Since the Stanford bidi-
rectional model incorporates all of the features that
produced its state-of-the-art performance on WSJ,
we believe the fact that the HMM-LA outperforms
it, despite the discriminative model?s more expres-
sive feature set, is indicative of the HMM-LA?s abil-
ity to more effectively adapt to novel domains during
training. Another challenge for the discriminative
models is the need for regularization tuning, requir-
ing additional time and effort to train several mod-
els and select the most appropriate parameter each
time the domain changes. Whereas for the HMM-
LA models, although we also train several models,
they can be combined into a product model, such as
that described by Petrov (2010), in order to further
improve performance.
Since the prosodic breaks are noisier features than
the others incorporated in the discriminative models,
it may be useful to set their regularization param-
eter separately from the rest of the features, how-
ever, we have not explored this alternative. Our ex-
periments used human transcriptions of the conver-
sational speech; however, realistically our models
would be applied to speech recognition transcripts.
In such a case, word error will introduce noise in ad-
dition to the prosodic breaks. In future work, we will
evaluate the use of break indexes for tagging when
there is lexical error. We would also apply the n-
best rescoring method to exploit break indexes in the
HMM-LA bidirectional model, as this would likely
produce further improvements.
7 Conclusion
In this work, we have evaluated factors that are im-
portant for developing accurate tagging models for
speech. Given that prosodic breaks were effective
knowledge sources for parsing, an important goal
of this work was to evaluate their impact on vari-
ous tagging model configurations. Specifically, we
have examined the use of prosodic information for
tagging conversational speech with several different
discriminative and generative models across three
different speech transcript segmentations. Our find-
829
0%
10%
20%
30%
40%
50%
NNP VBN WP CD RP EX WRB WDT JJR POS JJS RBS
Err
or
 R
at
e
 
WSJ (Stanford-Bidir) WSJ (HMM-LA)
Sent (Stanford-Bidir) Sent (HMM-LA)
Conv (Stanford-Bidir) Conv (HMM-LA)
Figure 7: Comparison of error rates between the Standford Bidir and HMM-LA models trained on WSJ, sentences,
and conversation sides
ings suggest that generative models with latent an-
notations achieve the best performance in this chal-
lenging domain. In terms of transcript segmenta-
tion, if sentences are available, it is preferable to use
them. In the case that no such annotation is avail-
able, then using automatic sentence boundary detec-
tion does not serve as an appropriate replacement,
but if automatic speaker turn segments can be ob-
tained, then this is a good alternative, despite the fact
that prosodic enrichment is less effective.
Our investigation also shows that in the event that
conversation sides must be used, prosodic enrich-
ment of the discriminative and generative models
produces significant improvements in tagging accu-
racy (by direct integration of prosody features for
the former and by restricting the search space and
rescoring with the latter). For tagging, the most im-
portant role of the break indexes appears to be as a
stand in for sentence boundaries. The oracle break
experiments suggest that if the accuracy of the au-
tomatically induced break indexes can be improved,
then the prosodically enriched models will perform
as well, or even better, than their human-annotated
sentence counterparts.
8 Acknowledgments
This research was supported in part by NSF IIS-
0703859 and the GALE program of the Defense
Advanced Research Projects Agency, Contract No.
HR0011-06-2-001. Any opinions, findings, and rec-
ommendations expressed in this paper are those of
the authors and do not necessarily reflect the views
of the funding agency or the institutions where the
work was completed.
References
Anton Batliner, Bernd Mo?bius, Gregor Mo?hler, Antje
Schweitzer, and Elmar No?th. 2001. Prosodic models,
automatic speech understanding, and speech synthesis:
toward the common ground. In Eurospeech.
Ann Bies, Stephanie Strassel, Haejoong Lee, Kazuaki
Maeda, Seth Kulick, Yang Liu, Mary Harper, and
Matthew Lease. 2006. Linguistic resources for speech
parsing. In LREC.
Stanley F. Chen and Ronald Rosenfeld. 1999. A Gaus-
sian prior for smoothing maximum entropy models.
Technical report, Technical Report CMU-CS-99-108,
Carnegie Mellon University.
Anne Cutler, Delphine Dahan, and Wilma v an Donselaar.
1997. Prosody in comprehension of spoken language:
A literature review. Language and Speech.
Markus Dreyer and Izhak Shafran. 2007. Exploiting
prosody for PCFGs with latent annotations. In Inter-
speech.
Denis Filimonov and Mary Harper. 2009. A joint
language model with fine-grain syntactic tags. In
EMNLP.
Jennifer Foster. 2010. ?cba to check the spelling?: Inves-
tigating parser performance on discussion forum posts.
In NAACL-HLT.
Florian Gallwitz, Heinrich Niemann, Elmar No?th, and
Volker Warnke. 2002. Integrated recognition of words
and prosodic phrase boundaries. Speech Communica-
tion.
John J. Godfrey, Edward C. Holliman, and Jane Mc-
Daniel. 1992. SWITCHBOARD: Telephone speech
corpus for research and development. In ICASSP.
Michelle L. Gregory, Mark Johnson, and Eugene Char-
niak. 2004. Sentence-internal prosody does not help
parsing the way punctuation does. In NAACL.
Mary P. Harper, Bonnie J. Dorr, John Hale, Brian Roark,
Izhak Shafran, Matthew Lease, Yang Liu, Matthew
Snover, Lisa Yung, Anna Krasnyanskaya, and Robin
Stewart. 2005. 2005 Johns Hopkins Summer Work-
shop Final Report on Parsing and Spoken Structural
830
Event Detection. Technical report, Johns Hopkins
University.
Mark Hasegawa-Johnson, Ken Chen, Jennifer Cole,
Sarah Borys, Sung suk Kim, Aaron Cohen, Tong
Zhang, Jeung yoon Choi, Heejin Kim, Taejin Yoon,
and Ra Chavarria. 2005. Simultaneous recognition
of words and prosody in the boston university radio
speech corpus. speech communication. Speech Com-
munication.
Peter A. Heeman. 1999. POS tags and decision trees for
language modeling. In EMNLP.
Dustin Hillard, Zhongqiang Huang, Heng Ji, Ralph Gr-
ishman, Dilek Hakkani-Tur, Mary Harper, Mari Os-
tendorf, and Wen Wang. 2006. Impact of automatic
comma prediction on POS/name tagging of speech. In
ICASSP.
Zhongqiang Huang and Mary Harper. 2010. Appropri-
ately handled prosodic breaks help PCFG parsing. In
NAACL.
Zhongqiang Huang, Vladimir Eidelman, and Mary
Harper. 2009. Improving a simple bigram hmm part-
of-speech tagger by latent annotation and self-training.
In NAACL-HLT.
Jeremy G. Kahn, Matthew Lease, Eugene Charniak,
Mark Johnson, and Mari Ostendorf. 2005. Effective
use of prosody in parsing conversational speech. In
EMNLP-HLT.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic models
for segmenting and labeling sequence data. In ICML.
D. C. Liu and Jorge Nocedal. 1989. On the limited mem-
ory BFGS method for large scale optimization. Math-
ematical Programming.
Yang Liu, Andreas Stolcke, Elizabeth Shriberg, and Mary
Harper. 2005. Using conditional random fields for
sentence boundary detection in speech. In ACL.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic CFG with latent annotations. In
ACL.
Mari Ostendorf, Izhak Shafran, and Rebecca Bates.
2003. Prosody models for conversational speech
recognition. In Plenary Meeting and Symposium on
Prosody and Speech Processing.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In HLT-NAACL.
Slav Petrov. 2010. Products of random latent variable
grammars. In HLT-NAACL.
Brian Roark, Yang Liu, Mary Harper, Robin Stewart,
Matthew Lease, Matthew Snover, Izhak Shafran, Bon-
nie Dorr, John Hale, Anna Krasnyanskaya, and Lisa
Yung. 2006. Reranking for sentence boundary detec-
tion in conversational speech. In ICASSP.
Kim Silverman, Mary Beckman, John Pitrelli, Mari Os-
tendorf, Colin Wightman, Patti Price, Janet Pierrehum-
bert, and Julia Hirshberg. 1992. ToBI: A standard for
labeling English prosody. In ICSLP.
Paul Taylor and Alan W. Black. 1998. Assigning
phrase breaks from part-of-speech sequences. Com-
puter Speech and Language.
Scott M. Thede and Mary P. Harper. 1999. A second-
order hidden markov model for part-of-speech tag-
ging. In ACL.
Kristina Toutanova and Christopher D. Manning. 2000.
Enriching the knowledge sources used in a maximum
entropy part-of-speech tagger. In EMNLP.
Kristina Toutanova, Dan Klein, Christopher D. Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In NAACL.
831
Proceedings of the ACL 2010 System Demonstrations, pages 7?12,
Uppsala, Sweden, 13 July 2010. c?2010 Association for Computational Linguistics
cdec: A Decoder, Alignment, and Learning Framework for
Finite-State and Context-Free Translation Models
Chris Dyer
University of Maryland
redpony@umd.edu
Adam Lopez
University of Edinburgh
alopez@inf.ed.ac.uk
Juri Ganitkevitch
Johns Hopkins University
juri@cs.jhu.edu
Jonathan Weese
Johns Hopkins University
jweese@cs.jhu.edu
Ferhan Ture
University of Maryland
fture@cs.umd.edu
Phil Blunsom
Oxford University
pblunsom@comlab.ox.ac.uk
Hendra Setiawan
University of Maryland
hendra@umiacs.umd.edu
Vladimir Eidelman
University of Maryland
vlad@umiacs.umd.edu
Philip Resnik
University of Maryland
resnik@umiacs.umd.edu
Abstract
We present cdec, an open source frame-
work for decoding, aligning with, and
training a number of statistical machine
translation models, including word-based
models, phrase-based models, and models
based on synchronous context-free gram-
mars. Using a single unified internal
representation for translation forests, the
decoder strictly separates model-specific
translation logic from general rescoring,
pruning, and inference algorithms. From
this unified representation, the decoder can
extract not only the 1- or k-best transla-
tions, but also alignments to a reference,
or the quantities necessary to drive dis-
criminative training using gradient-based
or gradient-free optimization techniques.
Its efficient C++ implementation means
that memory use and runtime performance
are significantly better than comparable
decoders.
1 Introduction
The dominant models used in machine transla-
tion and sequence tagging are formally based
on either weighted finite-state transducers (FSTs)
or weighted synchronous context-free grammars
(SCFGs) (Lopez, 2008). Phrase-based models
(Koehn et al, 2003), lexical translation models
(Brown et al, 1993), and finite-state conditional
random fields (Sha and Pereira, 2003) exemplify
the former, and hierarchical phrase-based models
the latter (Chiang, 2007). We introduce a soft-
ware package called cdec that manipulates both
classes in a unified way.1
Although open source decoders for both phrase-
based and hierarchical translation models have
been available for several years (Koehn et al,
2007; Li et al, 2009), their extensibility to new
models and algorithms is limited by two sig-
nificant design flaws that we have avoided with
cdec. First, their implementations tightly couple
the translation, language model integration (which
we call rescoring), and pruning algorithms. This
makes it difficult to explore alternative transla-
tion models without also re-implementing rescor-
ing and pruning logic. In cdec, model-specific
code is only required to construct a translation for-
est (?3). General rescoring (with language models
or other models), pruning, inference, and align-
ment algorithms then apply to the unified data
structure (?4). Hence all model types benefit im-
mediately from new algorithms (for rescoring, in-
ference, etc.); new models can be more easily pro-
totyped; and controlled comparison of models is
made easier.
Second, existing open source decoders were de-
signed with the traditional phrase-based parame-
terization using a very small number of dense fea-
tures (typically less than 10). cdec has been de-
signed from the ground up to support any parame-
terization, from those with a handful of dense fea-
tures up to models with millions of sparse features
(Blunsom et al, 2008; Chiang et al, 2009). Since
the inference algorithms necessary to compute a
training objective (e.g. conditional likelihood or
expected BLEU) and its gradient operate on the
unified data structure (?5), any model type can be
trained using with any of the supported training
1The software is released under the Apache License, ver-
sion 2.0, and is available from http://cdec-decoder.org/ .
7
criteria. The software package includes general
function optimization utilities that can be used for
discriminative training (?6).
These features are implemented without com-
promising on performance. We show experimen-
tally that cdec uses less memory and time than
comparable decoders on a controlled translation
task (?7).
2 Decoder workflow
The decoding pipeline consists of two phases. The
first (Figure 1) transforms input, which may be
represented as a source language sentence, lattice
(Dyer et al, 2008), or context-free forest (Dyer
and Resnik, 2010), into a translation forest that has
been rescored with all applicable models.
In cdec, the only model-specific logic is con-
fined to the first step in the process where an
input string (or lattice, etc.) is transduced into
the unified hypergraph representation. Since the
model-specific code need not worry about integra-
tion with rescoring models, it can be made quite
simple and efficient. Furthermore, prior to lan-
guage model integration (and distortion model in-
tegration, in the case of phrase based translation),
pruning is unnecessary for most kinds of mod-
els, further simplifying the model-specific code.
Once this unscored translation forest has been
generated, any non-coaccessible states (i.e., states
that are not reachable from the goal node) are re-
moved and the resulting structure is rescored with
language models using a user-specified intersec-
tion/pruning strategy (?4) resulting in a rescored
translation forest and completing phase 1.
The second phase of the decoding pipeline (de-
picted in Figure 2) computes a value from the
rescored forest: 1- or k-best derivations, feature
expectations, or intersection with a target language
reference (sentence or lattice). The last option
generates an alignment forest, from which a word
alignment or feature expectations can be extracted.
Most of these values are computed in a time com-
plexity that is linear in the number of edges and
nodes in the translation hypergraph using cdec?s
semiring framework (?5).
2.1 Alignment forests and alignment
Alignment is the process of determining if and
how a translation model generates a ?source, tar-
get? string pair. To compute an alignment under
a translation model, the phase 1 translation hyper-
graph is reinterpreted as a synchronous context-
free grammar and then used to parse the target
sentence.2 This results in an alignment forest,
which is a compact representation of all the deriva-
tions of the sentence pair under the translation
model. From this forest, the Viterbi or maximum a
posteriori word alignment can be generated. This
alignment algorithm is explored in depth by Dyer
(2010). Note that if the phase 1 forest has been
pruned in some way, or the grammar does not de-
rive the sentence pair, the target intersection parse
may fail, meaning that an alignment will not be
recoverable.
3 Translation hypergraphs
Recent research has proposed a unified repre-
sentation for the various translation and tagging
formalisms that is based on weighted logic pro-
gramming (Lopez, 2009). In this view, trans-
lation (or tagging) deductions have the structure
of a context-free forest, or directed hypergraph,
where edges have a single head and 0 or more tail
nodes (Nederhof, 2003). Once a forest has been
constructed representing the possible translations,
general inference algorithms can be applied.
In cdec?s translation hypergraph, a node rep-
resents a contiguous sequence of target language
words. For SCFG models and sequential tag-
ging models, a node also corresponds to a source
span and non-terminal type, but for word-based
and phrase-based models, the relationship to the
source string (or lattice) may be more compli-
cated. In a phrase-based translation hypergraph,
the node will correspond to a source coverage vec-
tor (Koehn et al, 2003). In word-based models, a
single node may derive multiple different source
language coverages since word based models im-
pose no requirements on covering all words in the
input. Figure 3 illustrates two example hyper-
graphs, one generated using a SCFG model and
other from a phrase-based model.
Edges are associated with exactly one syn-
chronous production in the source and target lan-
guage, and alternative translation possibilities are
expressed as alternative edges. Edges are further
annotated with feature values, and are annotated
with the source span vector the edge corresponds
to. An edge?s output label may contain mixtures
of terminal symbol yields and positions indicating
where a child node?s yield should be substituted.
2The parser is smart enough to detect the left-branching
grammars generated by lexical translation and tagging mod-
els, and use a more efficient intersection algorithm.
8
SCFG parser
FST transducer
Tagger
Lexical transducer
Phrase-based 
transducer
Source CFG
Source 
sentence
Source lattice
Unscored 
hypergraph
Input Transducers
Cube pruning
Full intersection
FST rescoring
Translation 
hypergraph
Output
Cube growing
No rescoring
Figure 1: Forest generation workflow (first half of decoding pipeline). The decoder?s configuration
specifies what path is taken from the input (one of the bold ovals) to a unified translation hypergraph.
The highlighted path is the workflow used in the test reported in ?7.
Translation 
hypergraph
Target 
reference
Viterbi extraction
k-best extraction
max-translation 
extraction
feature 
expectations
intersection by 
parsing
Alignment 
hypergraph
feature 
expectations
max posterior 
alignment
Viterbi alignment
Translation outputs Alignment outputs
Figure 2: Output generation workflow (second half of decoding pipeline). Possible output types are
designated with a double box.
In the case of SCFG grammars, the edges corre-
spond simply to rules in the synchronous gram-
mar. For non-SCFG translation models, there are
two kinds of edges. The first have zero tail nodes
(i.e., an arity of 0), and correspond to word or
phrase translation pairs (with all translation op-
tions existing on edges deriving the same head
node), or glue rules that glue phrases together.
For tagging, word-based, and phrase-based mod-
els, these are strictly arranged in a monotone, left-
branching structure.
4 Rescoring with weighted FSTs
The design of cdec separates the creation of a
translation forest from its rescoring with a lan-
guage models or similar models.3 Since the struc-
ture of the unified search space is context free (?3),
we use the logic for language model rescoring de-
scribed by Chiang (2007), although any weighted
intersection algorithm can be applied. The rescor-
3Other rescoring models that depend on sequential con-
text include distance-based reordering models or Markov fea-
tures in tagging models.
ing models need not be explicitly represented as
FSTs?the state space can be inferred.
Although intersection using the Chiang algo-
rithm runs in polynomial time and space, the re-
sulting rescored forest may still be too large to rep-
resent completely. cdec therefore supports three
pruning strategies that can be used during intersec-
tion: full unpruned intersection (useful for tagging
models to incorporate, e.g., Markov features, but
not generally practical for translation), cube prun-
ing, and cube growing (Huang and Chiang, 2007).
5 Semiring framework
Semirings are a useful mathematical abstraction
for dealing with translation forests since many
useful quantities can be computed using a single
linear-time algorithm but with different semirings.
A semiring is a 5-tuple (K,?,?, 0, 1) that indi-
cates the set from which the values will be drawn,
K, a generic addition and multiplication operation,
? and ?, and their identities 0 and 1. Multipli-
cation and addition must be associative. Multi-
plication must distribute over addition, and v ? 0
9
Goal
JJ NN
1 2
a
s
m
a
l
l
l
i
t
t
l
e
h
o
u
s
e
s
h
e
l
l
Goal
010
100 101
110
a
s
m
a
l
l
l
i
t
t
l
e
1
a
1
house
1
shell
1
little
1
small
1
house
1
shell
1
little
1
small
Figure 3: Example unrescored translation hypergraphs generated for the German input ein (a) kleines
(small/little) Haus (house/shell) using a SCFG-based model (left) and phrase-based model with a distor-
tion limit of 1 (right).
must equal 0. Values that can be computed using
the semirings include the number of derivations,
the expected translation length, the entropy of the
translation posterior distribution, and the expected
values of feature functions (Li and Eisner, 2009).
Since semirings are such a useful abstraction,
cdec has been designed to facilitate implementa-
tion of new semirings. Table 1 shows the C++ rep-
resentation used for semirings. Note that because
of our representation, built-in types like double,
int, and bool (together with their default op-
erators) are semirings. Beyond these, the type
prob t is provided which stores the logarithm of
the value it represents, which helps avoid under-
flow and overflow problems that may otherwise
be encountered. A generic first-order expectation
semiring is also provided (Li and Eisner, 2009).
Table 1: Semiring representation. T is a C++ type
name.
Element C++ representation
K T
? T::operator+=
? T::operator*=
0 T()
1 T(1)
Three standard algorithms parameterized with
semirings are provided: INSIDE, OUTSIDE, and
INSIDEOUTSIDE, and the semiring is specified us-
ing C++ generics (templates). Additionally, each
algorithm takes a weight function that maps from
hypergraph edges to a value in K, making it possi-
ble to use many different semirings without alter-
ing the underlying hypergraph.
5.1 Viterbi and k-best extraction
Although Viterbi and k-best extraction algorithms
are often expressed as INSIDE algorithms with
the tropical semiring, cdec provides a separate
derivation extraction framework that makes use of
a < operator (Huang and Chiang, 2005). Thus,
many of the semiring types define not only the el-
ements shown in Table 1 but T::operator< as
well. The k-best extraction algorithm is also pa-
rameterized by an optional predicate that can filter
out derivations at each node, enabling extraction
of only derivations that yield different strings as in
Huang et al (2006).
6 Model training
Two training pipelines are provided with cdec.
The first, called Viterbi envelope semiring train-
ing, VEST, implements the minimum error rate
training (MERT) algorithm, a gradient-free opti-
mization technique capable of maximizing arbi-
trary loss functions (Och, 2003).
6.1 VEST
Rather than computing an error surface using k-
best approximations of the decoder search space,
cdec?s implementation performs inference over
the full hypergraph structure (Kumar et al, 2009).
In particular, by defining a semiring whose values
are sets of line segments, having an addition op-
eration equivalent to union, and a multiplication
operation equivalent to a linear transformation of
the line segments, Och?s line search can be com-
puted simply using the INSIDE algorithm. Since
the translation hypergraphs generated by cdec
may be quite large making inference expensive,
the logic for constructing error surfaces is fac-
tored according to the MapReduce programming
paradigm (Dean and Ghemawat, 2004), enabling
parallelization across a cluster of machines. Im-
plementations of the BLEU and TER loss functions
are provided (Papineni et al, 2002; Snover et al,
2006).
10
6.2 Large-scale discriminative training
In addition to the widely used MERT algo-
rithm, cdec also provides a training pipeline for
discriminatively trained probabilistic translation
models (Blunsom et al, 2008; Blunsom and Os-
borne, 2008). In these models, the translation
model is trained to maximize conditional log like-
lihood of the training data under a specified gram-
mar. Since log likelihood is differentiable with
respect to the feature weights in an exponential
model, it is possible to use gradient-based opti-
mization techniques to train the system, enabling
the parameterization of the model using millions
of sparse features. While this training approach
was originally proposed for SCFG-based transla-
tion models, it can be used to train any model
type in cdec. When used with sequential tagging
models, this pipeline is identical to traditional se-
quential CRF training (Sha and Pereira, 2003).
Both the objective (conditional log likelihood)
and its gradient have the form of a difference in
two quantities: each has one term that is com-
puted over the translation hypergraph which is
subtracted from the result of the same computa-
tion over the alignment hypergraph (refer to Fig-
ures 1 and 2). The conditional log likelihood is
the difference in the log partition of the translation
and alignment hypergraph, and is computed using
the INSIDE algorithm. The gradient with respect
to a particular feature is the difference in this fea-
ture?s expected value in the translation and align-
ment hypergraphs, and can be computed using ei-
ther INSIDEOUTSIDE or the expectation semiring
and INSIDE. Since a translation forest is generated
as an intermediate step in generating an alignment
forest (?2) this computation is straightforward.
Since gradient-based optimization techniques
may require thousands of evaluations to converge,
the batch training pipeline is split into map and
reduce components, facilitating distribution over
very large clusters. Briefly, the cdec is run as the
map function, and sentence pairs are mapped over.
The reduce function aggregates the results and per-
forms the optimization using standard algorithms,
including LBFGS (Liu et al, 1989), RPROP (Ried-
miller and Braun, 1993), and stochastic gradient
descent.
7 Experiments
Table 2 compares the performance of cdec, Hi-
ero, and Joshua 1.3 (running with 1 or 8 threads)
decoding using a hierarchical phrase-based trans-
lation grammar and identical pruning settings.4
Figure 4 shows the cdec configuration and
weights file used for this test.
The workstation used has two 2GHz quad-core
Intel Xenon processors, 32GB RAM, is running
Linux kernel version 2.6.18 and gcc version 4.1.2.
All decoders use SRI?s language model toolkit,
version 1.5.9 (Stolcke, 2002). Joshua was run on
the Sun HotSpot JVM, version 1.6.0 12. A hierar-
chical phrase-based translation grammar was ex-
tracted for the NIST MT03 Chinese-English trans-
lation using a suffix array rule extractor (Lopez,
2007). A non-terminal span limit of 15 was used,
and all decoders were configured to use cube prun-
ing with a limit of 30 candidates at each node and
no further pruning. All decoders produced a BLEU
score between 31.4 and 31.6 (small differences are
accounted for by different tie-breaking behavior
and OOV handling).
Table 2: Memory usage and average per-sentence
running time, in seconds, for decoding a Chinese-
English test set.
Decoder Lang. Time (s) Memory
cdec C++ 0.37 1.0Gb
Joshua (1?) Java 0.98 1.5Gb
Joshua (8?) Java 0.35 2.5Gb
Hiero Python 4.04 1.1Gb
formalism=scfg
grammar=grammar.mt03.scfg.gz
add pass through rules=true
scfg max span limit=15
feature function=LanguageModel \
en.3gram.pruned.lm.gz -o 3
feature function=WordPenalty
intersection strategy=cube pruning
cubepruning pop limit=30
LanguageModel 1.12
WordPenalty -4.26
PhraseModel 0 0.963
PhraseModel 1 0.654
PhraseModel 2 0.773
PassThroughRule -20
Figure 4: Configuration file (above) and feature
weights file (below) used for the decoding test de-
scribed in ?7.
4http://sourceforge.net/projects/joshua/
11
8 Future work
cdec continues to be under active development.
We are taking advantage of its modular design to
study alternative algorithms for language model
integration. Further training pipelines are un-
der development, including minimum risk train-
ing using a linearly decomposable approximation
of BLEU (Li and Eisner, 2009), and MIRA train-
ing (Chiang et al, 2009). All of these will be
made publicly available as the projects progress.
We are also improving support for parallel training
using Hadoop (an open-source implementation of
MapReduce).
Acknowledgements
This work was partially supported by the GALE
program of the Defense Advanced Research
Projects Agency, Contract No. HR0011-06-2-001.
Any opinions, findings, conclusions or recommen-
dations expressed in this paper are those of the au-
thors and do not necessarily reflect the views of the
sponsors. Further support was provided the Euro-
Matrix project funded by the European Commis-
sion (7th Framework Programme). Discussions
with Philipp Koehn, Chris Callison-Burch, Zhifei
Li, Lane Schwarz, and Jimmy Lin were likewise
crucial to the successful execution of this project.
References
P. Blunsom and M. Osborne. 2008. Probalistic inference for
machine translation. In Proc. of EMNLP.
P. Blunsom, T. Cohn, and M. Osborne. 2008. A discrimina-
tive latent variable model for statistical machine transla-
tion. In Proc. of ACL-HLT.
P. F. Brown, V. J. Della Pietra, S. A. Della Pietra, and R. L.
Mercer. 1993. The mathematics of statistical machine
translation: parameter estimation. Computational Lin-
guistics, 19(2):263?311.
D. Chiang, K. Knight, and W. Wang. 2009. 11,001 new
features for statistical machine translation. In Proc. of
NAACL, pages 218?226.
D. Chiang. 2007. Hierarchical phrase-based translation.
Comp. Ling., 33(2):201?228.
J. Dean and S. Ghemawat. 2004. MapReduce: Simplified
data processing on large clusters. In Proc. of the 6th Sym-
posium on Operating System Design and Implementation
(OSDI 2004), pages 137?150.
C. Dyer and P. Resnik. 2010. Context-free reordering, finite-
state translation. In Proc. of HLT-NAACL.
C. Dyer, S. Muresan, and P. Resnik. 2008. Generalizing
word lattice translation. In Proc. of HLT-ACL.
C. Dyer. 2010. Two monolingual parses are better than one
(synchronous parse). In Proc. of HLT-NAACL.
L. Huang and D. Chiang. 2005. Better k-best parsing. In In
Proc. of IWPT, pages 53?64.
L. Huang and D. Chiang. 2007. Forest rescoring: Faster
decoding with integrated language models. In Proc. ACL.
L. Huang, K. Knight, and A. Joshi. 2006. A syntax-directed
translator with extended domain of locality. In Proc. of
AMTA.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical phrase-
based translation. In Proc. of HLT/NAACL, pages 48?54.
P. Koehn, H. Hoang, A. B. Mayne, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran,
R. Zens, C. Dyer, O. Bojar, A. Constantin, and E. Herbst.
2007. Moses: Open source toolkit for statistical ma-
chine translation. In Proc. of ACL, Demonstration Ses-
sion, pages 177?180, June.
S. Kumar, W. Macherey, C. Dyer, and F. Och. 2009. Efficient
minimum error rate training and minimum B ayes-risk de-
coding for translation hypergraphs and lattices. In Proc.
of ACL, pages 163?171.
Z. Li and J. Eisner. 2009. First- and second-order expectation
semirings with applications to minimum-risk training on
translation forests. In Proc. of EMNLP, pages 40?51.
Z. Li, C. Callison-Burch, C. Dyer, J. Ganitkevitch, S. Khu-
danpur, L. Schwartz, W. N. G. Thornton, J. Weese, and
O. F. Zaidan. 2009. Joshua: an open source toolkit for
parsing-based machine translation. In Proc. of the Fourth
Workshop on Stat. Machine Translation, pages 135?139.
D. C. Liu, J. Nocedal, D. C. Liu, and J. Nocedal. 1989. On
the limited memory BFGS method for large scale opti-
mization. Mathematical Programming B, 45(3):503?528.
A. Lopez. 2007. Hierarchical phrase-based translation with
suffix arrays. In Proc. of EMNLP, pages 976?985.
A. Lopez. 2008. Statistical machine translation. ACM Com-
puting Surveys, 40(3), Aug.
A. Lopez. 2009. Translation as weighted deduction. In Proc.
of EACL, pages 532?540.
M.-J. Nederhof. 2003. Weighted deductive parsing and
Knuth?s algorithm. Comp. Ling., 29(1):135?143, Mar.
F. Och. 2003. Minimum error rate training in statistical ma-
chine translation. In Proc. of ACL, pages 160?167.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. In Proc. of ACL, pages 311?318.
M. Riedmiller and H. Braun. 1993. A direct adaptive method
for faster backpropagation learning: The RPROP algo-
rithm. In Proc. of the IEEE international conference on
neural networks, pages 586?591.
F. Sha and F. Pereira. 2003. Shallow parsing with conditional
random fields. In Proc. of NAACL, pages 134?141.
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A study of translation edit rate with
targeted human annotation. In Proc. AMTA.
A. Stolcke. 2002. SRILM ? an extensible language modeling
toolkit. In Intl. Conf. on Spoken Language Processing.
12
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 115?119,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Topic Models for Dynamic Translation Model Adaptation
Vladimir Eidelman
Computer Science
and UMIACS
University of Maryland
College Park, MD
vlad@umiacs.umd.edu
Jordan Boyd-Graber
iSchool
and UMIACS
University of Maryland
College Park, MD
jbg@umiacs.umd.edu
Philip Resnik
Linguistics
and UMIACS
University of Maryland
College Park, MD
resnik@umd.edu
Abstract
We propose an approach that biases machine
translation systems toward relevant transla-
tions based on topic-specific contexts, where
topics are induced in an unsupervised way
using topic models; this can be thought of
as inducing subcorpora for adaptation with-
out any human annotation. We use these topic
distributions to compute topic-dependent lex-
ical weighting probabilities and directly in-
corporate them into our translation model as
features. Conditioning lexical probabilities
on the topic biases translations toward topic-
relevant output, resulting in significant im-
provements of up to 1 BLEU and 3 TER on
Chinese to English translation over a strong
baseline.
1 Introduction
The performance of a statistical machine translation
(SMT) system on a translation task depends largely
on the suitability of the available parallel training
data. Domains (e.g., newswire vs. blogs) may vary
widely in their lexical choices and stylistic prefer-
ences, and what may be preferable in a general set-
ting, or in one domain, is not necessarily preferable
in another domain. Indeed, sometimes the domain
can change the meaning of a phrase entirely.
In a food related context, the Chinese sentence
????? ? (?fe?nsi? he?nduo??) would mean ?They
have a lot of vermicelli?; however, in an informal In-
ternet conversation, this sentence would mean ?They
have a lot of fans?. Without the broader context, it
is impossible to determine the correct translation in
otherwise identical sentences.
This problem has led to a substantial amount of
recent work in trying to bias, or adapt, the transla-
tion model (TM) toward particular domains of inter-
est (Axelrod et al, 2011; Foster et al, 2010; Snover
et al, 2008).1 The intuition behind TM adapta-
tion is to increase the likelihood of selecting rele-
vant phrases for translation. Matsoukas et al (2009)
introduced assigning a pair of binary features to
each training sentence, indicating sentences? genre
and collection as a way to capture domains. They
then learn a mapping from these features to sen-
tence weights, use the sentence weights to bias the
model probability estimates and subsequently learn
the model weights. As sentence weights were found
to be most beneficial for lexical weighting, Chiang
et al (2011) extends the same notion of condition-
ing on provenance (i.e., the origin of the text) by re-
moving the separate mapping step, directly optimiz-
ing the weight of the genre and collection features
by computing a separate word translation table for
each feature, estimated from only those sentences
that comprise that genre or collection.
The common thread throughout prior work is the
concept of a domain. A domain is typically a hard
constraint that is externally imposed and hand la-
beled, such as genre or corpus collection. For ex-
ample, a sentence either comes from newswire, or
weblog, but not both. However, this poses sev-
eral problems. First, since a sentence contributes its
counts only to the translation table for the source it
came from, many word pairs will be unobserved for
a given table. This sparsity requires smoothing. Sec-
ond, we may not know the (sub)corpora our training
1Language model adaptation is also prevalent but is not the
focus of this work.
115
data come from; and even if we do, ?subcorpus? may
not be the most useful notion of domain for better
translations.
We take a finer-grained, flexible, unsupervised ap-
proach for lexical weighting by domain. We induce
unsupervised domains from large corpora, and we
incorporate soft, probabilistic domain membership
into a translation model. Unsupervised modeling of
the training data produces naturally occurring sub-
corpora, generalizing beyond corpus and genre. De-
pending on the model used to select subcorpora, we
can bias our translation toward any arbitrary distinc-
tion. This reduces the problem to identifying what
automatically defined subsets of the training corpus
may be beneficial for translation.
In this work, we consider the underlying latent
topics of the documents (Blei et al, 2003). Topic
modeling has received some use in SMT, for in-
stance Bilingual LSA adaptation (Tam et al, 2007),
and the BiTAM model (Zhao and Xing, 2006),
which uses a bilingual topic model for learning
alignment. In our case, by building a topic distri-
bution for the source side of the training data, we
abstract the notion of domain to include automati-
cally derived subcorpora with probabilistic member-
ship. This topic model infers the topic distribution
of a test set and biases sentence translations to ap-
propriate topics. We accomplish this by introduc-
ing topic dependent lexical probabilities directly as
features in the translation model, and interpolating
them log-linearly with our other features, thus allow-
ing us to discriminatively optimize their weights on
an arbitrary objective function. Incorporating these
features into our hierarchical phrase-based transla-
tion system significantly improved translation per-
formance, by up to 1 BLEU and 3 TER over a strong
Chinese to English baseline.
2 Model Description
Lexical Weighting Lexical weighting features es-
timate the quality of a phrase pair by combining
the lexical translation probabilities of the words in
the phrase2 (Koehn et al, 2003). Lexical condi-
tional probabilities p(e|f) are obtained with maxi-
mum likelihood estimates from relative frequencies
2For hierarchical systems, these correspond to translation
rules.
c(f, e)/
?
e c(f, e) . Phrase pair probabilities p(e|f)
are computed from these as described in Koehn et
al. (2003).
Chiang et al (2011) showed that is it benefi-
cial to condition the lexical weighting features on
provenance by assigning each sentence pair a set
of features, fs(e|f), one for each domain s, which
compute a new word translation table ps(e|f) esti-
mated from only those sentences which belong to s:
cs(f, e)/
?
e cs(f, e) , where cs(?) is the number of
occurrences of the word pair in s.
Topic Modeling for MT We extend provenance
to cover a set of automatically generated topics zn.
Given a parallel training corpus T composed of doc-
uments di, we build a source side topic model over
T , which provides a topic distribution p(zn|di) for
zn = {1, . . . ,K} over each document, using Latent
Dirichlet Allocation (LDA) (Blei et al, 2003). Then,
we assign p(zn|di) to be the topic distribution for
every sentence xj ? di, thus enforcing topic sharing
across sentence pairs in the same document instead
of treating them as unrelated. Computing the topic
distribution over a document and assigning it to the
sentences serves to tie the sentences together in the
document context.
To obtain the lexical probability conditioned on
topic distribution, we first compute the expected
count ezn(e, f) of a word pair under topic zn:
ezn(e, f) =
?
di?T
p(zn|di)
?
xj?di
cj(e, f) (1)
where cj(?) denotes the number of occurrences of
the word pair in sentence xj , and then compute:
pzn(e|f) =
ezn(e, f)?
e ezn(e, f)
(2)
Thus, we will introduce 2?K new word trans-
lation tables, one for each pzn(e|f) and pzn(f |e),
and as many new corresponding features fzn(e|f),
fzn(f |e). The actual feature values we compute will
depend on the topic distribution of the document we
are translating. For a test document V , we infer
topic assignments on V , p(zn|V ), keeping the topics
found from T fixed. The feature value then becomes
fzn(e|f) = ? log
{
pzn(e|f) ? p(zn|V )
}
, a combi-
nation of the topic dependent lexical weight and the
116
topic distribution of the sentence from which we are
extracting the phrase. To optimize the weights of
these features we combine them in our linear model
with the other features when computing the model
score for each phrase pair3:
?
p
?php(e, f)
? ?? ?
unadapted features
+
?
zn
?znfzn(e|f)
? ?? ?
adapted features
(3)
Combining the topic conditioned word translation
table pzn(e|f) computed from the training corpus
with the topic distribution p(zn|V ) of the test sen-
tence being translated provides a probability on how
relevant that translation table is to the sentence. This
allows us to bias the translation toward the topic of
the sentence. For example, if topic k is dominant in
T , pk(e|f) may be quite large, but if p(k|V ) is very
small, then we should steer away from this phrase
pair and select a competing phrase pair which may
have a lower probability in T , but which is more rel-
evant to the test sentence at hand.
In many cases, document delineations may not be
readily available for the training corpus. Further-
more, a document may be too broad, covering too
many disparate topics, to effectively bias the weights
on a phrase level. For this case, we also propose a
local LDA model (LTM), which treats each sentence
as a separate document.
While Chiang et al (2011) has to explicitly
smooth the resulting ps(e|f), since many word pairs
will be unseen for a given domain s, we are already
performing an implicit form of smoothing (when
computing the expected counts), since each docu-
ment has a distribution over all topics, and therefore
we have some probability of observing each word
pair in every topic.
Feature Representation After obtaining the topic
conditional features, there are two ways to present
them to the model. They could answer the question
F1: What is the probability under topic 1, topic 2,
etc., or F2: What is the probability under the most
probable topic, second most, etc.
A model using F1 learns whether a specific topic
is useful for translation, i.e., feature f1 would be
f1 := pz=1(e|f) ? p(z = 1|V ). With F2, we
3The unadapted lexical weight p(e|f) is included in the
model features.
are learning how useful knowledge of the topic dis-
tribution is, i.e., f1 := p(argmaxzn (p(zn|V ))(e|f) ?
p(argmaxzn(p(zn|V ))|V ).
Using F1, if we restrict our topics to have a one-
to-one mapping with genre/collection4 we see that
our method fully recovers Chiang (2011).
F1 is appropriate for cross-domain adaptation
when we have advance knowledge that the distribu-
tion of the tuning data will match the test data, as in
Chiang (2011), where they tune and test on web. In
general, we may not know what our data will be, so
this will overfit the tuning set.
F2, however, is intuitively what we want, since
we do not want to bias our system toward a spe-
cific distribution, but rather learn to utilize informa-
tion from any topic distribution if it helps us cre-
ate topic relevant translations. F2 is useful for dy-
namic adaptation, where the adapted feature weight
changes based on the source sentence.
Thus, F2 is the approach we use in our work,
which allows us to tune our system weights toward
having topic information be useful, not toward a spe-
cific distribution.
3 Experiments
Setup To evaluate our approach, we performed ex-
periments on Chinese to English MT in two set-
tings. First, we use the FBIS corpus as our training
bitext. Since FBIS has document delineations, we
compare local topic modeling (LTM) with model-
ing at the document level (GTM). The second setting
uses the non-UN and non-HK Hansards portions of
the NIST training corpora with LTM only. Table 1
summarizes the data statistics. For both settings,
the data were lowercased, tokenized and aligned us-
ing GIZA++ (Och and Ney, 2003) to obtain bidi-
rectional alignments, which were symmetrized us-
ing the grow-diag-final-and method (Koehn
et al, 2003). The Chinese data were segmented us-
ing the Stanford segmenter. We trained a trigram
LM on the English side of the corpus with an addi-
tional 150M words randomly selected from the non-
NYT and non-LAT portions of the Gigaword v4 cor-
pus using modified Kneser-Ney smoothing (Chen
and Goodman, 1996). We used cdec (Dyer et al,
4By having as many topics as genres/collections and setting
p(zn|di) to 1 for every sentence in the collection and 0 to ev-
erything else.
117
Corpus Sentences Tokens
En Zh
FBIS 269K 10.3M 7.9M
NIST 1.6M 44.4M 40.4M
Table 1: Corpus statistics
2010) as our decoder, and tuned the parameters of
the system to optimize BLEU (Papineni et al, 2002)
on the NIST MT06 tuning corpus using the Mar-
gin Infused Relaxed Algorithm (MIRA) (Crammer
et al, 2006; Eidelman, 2012). Topic modeling was
performed with Mallet (Mccallum, 2002), a stan-
dard implementation of LDA, using a Chinese sto-
plist and setting the per-document Dirichlet parame-
ter ? = 0.01. This setting of was chosen to encour-
age sparse topic assignments, which make induced
subdomains consistent within a document.
Results Results for both settings are shown in Ta-
ble 2. GTM models the latent topics at the document
level, while LTM models each sentence as a separate
document. To evaluate the effect topic granularity
would have on translation, we varied the number of
latent topics in each model to be 5, 10, and 20. On
FBIS, we can see that both models achieve moderate
but consistent gains over the baseline on both BLEU
and TER. The best model, LTM-10, achieves a gain
of about 0.5 and 0.6 BLEU and 2 TER. Although the
performance on BLEU for both the 20 topic models
LTM-20 and GTM-20 is suboptimal, the TER im-
provement is better. Interestingly, the difference in
translation quality between capturing document co-
herence in GTM and modeling purely on the sen-
tence level is not substantial.5 In fact, the opposite
is true, with the LTM models achieving better per-
formance.6
On the NIST corpus, LTM-10 again achieves the
best gain of approximately 1 BLEU and up to 3 TER.
LTM performs on par with or better than GTM, and
provides significant gains even in the NIST data set-
ting, showing that this method can be effectively ap-
plied directly on the sentence level to large training
5An avenue of future work would condition the sentence
topic distribution on a document distribution over topics (Teh
et al, 2006).
6As an empirical validation of our earlier intuition regarding
feature representation, presenting the features in the form of F1
caused the performance to remain virtually unchanged from the
baseline model.
Model MT03 MT05
?BLEU ?TER ?BLEU ?TER
BL 28.72 65.96 27.71 67.58
GTM-5 28.95ns 65.45 27.98ns 67.38ns
GTM-10 29.22 64.47 28.19 66.15
GTM-20 29.19 63.41 28.00ns 64.89
LTM-5 29.23 64.57 28.19 66.30
LTM-10 29.29 63.98 28.18 65.56
LTM-20 29.09ns 63.57 27.90ns 65.17
Model MT03 MT05
?BLEU ?TER ?BLEU ?TER
BL 34.31 61.14 30.63 65.10
MERT 34.60 60.66 30.53 64.56
LTM-5 35.21 59.48 31.47 62.34
LTM-10 35.32 59.16 31.56 62.01
LTM-20 33.90ns 60.89ns 30.12ns 63.87
Table 2: Performance using FBIS training corpus (top)
and NIST corpus (bottom). Improvements are significant
at the p <0.05 level, except where indicated (ns).
corpora which have no document markings. De-
pending on the diversity of training corpus, a vary-
ing number of underlying topics may be appropriate.
However, in both settings, 10 topics performed best.
4 Discussion and Conclusion
Applying SMT to new domains requires techniques
to inform our algorithms how best to adapt. This pa-
per extended the usual notion of domains to finer-
grained topic distributions induced in an unsuper-
vised fashion. We show that incorporating lexi-
cal weighting features conditioned on soft domain
membership directly into our model is an effective
strategy for dynamically biasing SMT towards rele-
vant translations, as evidenced by significant perfor-
mance gains. This method presents several advan-
tages over existing approaches. We can construct
a topic model once on the training data, and use
it infer topics on any test set to adapt the transla-
tion model. We can also incorporate large quanti-
ties of additional data (whether parallel or not) in
the source language to infer better topics without re-
lying on collection or genre annotations. Multilin-
gual topic models (Boyd-Graber and Resnik, 2010)
would provide a technique to use data from multiple
languages to ensure consistent topics.
118
Acknowledgments
Vladimir Eidelman is supported by a National De-
fense Science and Engineering Graduate Fellow-
ship. This work was also supported in part by
NSF grant #1018625, ARL Cooperative Agree-
ment W911NF-09-2-0072, and by the BOLT and
GALE programs of the Defense Advanced Research
Projects Agency, Contracts HR0011-12-C-0015 and
HR0011-06-2-001, respectively. Any opinions, find-
ings, conclusions, or recommendations expressed
are the authors? and do not necessarily reflect those
of the sponsors.
References
Amittai Axelrod, Xiaodong He, and Jianfeng Gao. 2011.
Domain adaptation via pseudo in-domain data selec-
tion. In Proceedings of Emperical Methods in Natural
Language Processing.
David M. Blei, Andrew Y. Ng, Michael I. Jordan, and
John Lafferty. 2003. Latent Dirichlet Allocation.
Journal of Machine Learning Research, 3:2003.
Jordan Boyd-Graber and Philip Resnik. 2010. Holistic
sentiment analysis across languages: Multilingual su-
pervised latent Dirichlet alocation. In Proceedings of
Emperical Methods in Natural Language Processing.
Stanley F. Chen and Joshua Goodman. 1996. An empir-
ical study of smoothing techniques for language mod-
eling. In Proceedings of the 34th Annual Meeting of
the Association for Computational Linguistics, pages
310?318.
David Chiang, Steve DeNeefe, and Michael Pust. 2011.
Two easy improvements to lexical weighting. In Pro-
ceedings of the Human Language Technology Confer-
ence.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. Journal of Machine Learning
Research, 7:551?585.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Jonathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Setiawan,
Vladimir Eidelman, and Philip Resnik. 2010. cdec: A
decoder, alignment, and learning framework for finite-
state and context-free translation models. In Proceed-
ings of ACL System Demonstrations.
Vladimir Eidelman. 2012. Optimization strategies for
online large-margin learning in machine translation.
In Proceedings of the Seventh Workshop on Statistical
Machine Translation.
George Foster, Cyril Goutte, and Roland Kuhn. 2010.
Discriminative instance weighting for domain adapta-
tion in statistical machine translation. In Proceedings
of Emperical Methods in Natural Language Process-
ing.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the 2003 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics on Human Language Technology - Volume 1,
NAACL ?03, Stroudsburg, PA, USA.
Spyros Matsoukas, Antti-Veikko I. Rosti, and Bing
Zhang. 2009. Discriminative corpus weight estima-
tion for machine translation. In Proceedings of Em-
perical Methods in Natural Language Processing.
A. K. Mccallum. 2002. MALLET: A Machine Learning
for Language Toolkit.
Franz Och and Hermann Ney. 2003. A systematic com-
parison of various statistical alignment models. In
Computational Linguistics, volume 29(21), pages 19?
51.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic evalu-
ation of machine translation. In Proceedings of the As-
sociation for Computational Linguistics, pages 311?
318.
Matthew Snover, Bonnie Dorr, and Richard Schwartz.
2008. Language and translation model adaptation us-
ing comparable corpora. In Proceedings of Emperical
Methods in Natural Language Processing.
Yik-Cheung Tam, Ian Lane, and Tanja Schultz. 2007.
Bilingual LSA-based adaptation for statistical machine
translation. Machine Translation, 21(4):187?207.
Yee Whye Teh, Michael I. Jordan, Matthew J. Beal, and
David M. Blei. 2006. Hierarchical Dirichlet pro-
cesses. Journal of the American Statistical Associa-
tion, 101(476):1566?1581.
Bing Zhao and Eric P. Xing. 2006. BiTAM: Bilingual
topic admixture models for word alignment. In Pro-
ceedings of the Association for Computational Lin-
guistics.
119
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1116?1126,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Online Relative Margin Maximization for Statistical Machine Translation
Vladimir Eidelman
Computer Science
and UMIACS
University of Maryland
College Park, MD
vlad@umiacs.umd.edu
Yuval Marton
Microsoft
City Center Plaza
Bellevue, WA
yuvalmarton@gmail.com
Philip Resnik
Linguistics
and UMIACS
University of Maryland
College Park, MD
resnik@umd.edu
Abstract
Recent advances in large-margin learning
have shown that better generalization can
be achieved by incorporating higher order
information into the optimization, such as
the spread of the data. However, these so-
lutions are impractical in complex struc-
tured prediction problems such as statis-
tical machine translation. We present an
online gradient-based algorithm for rela-
tive margin maximization, which bounds
the spread of the projected data while max-
imizing the margin. We evaluate our op-
timizer on Chinese-English and Arabic-
English translation tasks, each with small
and large feature sets, and show that our
learner is able to achieve significant im-
provements of 1.2-2 BLEU and 1.7-4.3
TER on average over state-of-the-art opti-
mizers with the large feature set.
1 Introduction
The desire to incorporate high-dimensional sparse
feature representations into statistical machine
translation (SMT) models has driven recent re-
search away from Minimum Error Rate Training
(MERT) (Och, 2003), and toward other discrim-
inative methods that can optimize more features.
Examples include minimum risk (Smith and Eis-
ner, 2006), pairwise ranking (PRO) (Hopkins and
May, 2011), RAMPION (Gimpel and Smith, 2012),
and variations of the margin-infused relaxation al-
gorithm (MIRA) (Watanabe et al, 2007; Chiang et
al., 2008; Cherry and Foster, 2012). While the ob-
jective function and optimization method vary for
each optimizer, they can all be broadly described
as learning a linear model, or parameter vector w,
which is used to score alternative translation hy-
potheses.
In every SMT system, and in machine learn-
ing in general, the goal of learning is to find a
model that generalizes well, i.e. one that will yield
good translations for previously unseen sentences.
However, as the dimension of the feature space in-
creases, generalization becomes increasingly diffi-
cult. Since only a small portion of all (sparse) fea-
tures may be observed in a relatively small fixed
set of instances during tuning, we are prone to
overfit the training data. An alternative approach
for solving this problem is estimating discrimina-
tive feature weights directly on the training bi-
text (Tillmann and Zhang, 2006; Blunsom et al,
2008; Simianer et al, 2012), which is usually sub-
stantially larger than the tuning set, but this is com-
plementary to our goal here of better generaliza-
tion given a fixed size tuning set.
In order to achieve that goal, we need to care-
fully choose what objective to optimize, and how
to perform parameter estimation of w for this ob-
jective. We focus on large-margin methods such
as SVM (Joachims, 1998) and passive-aggressive
algorithms such as MIRA. Intuitively these seek
a w such that the separating distance in geomet-
ric space of two hypotheses is at least as large as
the cost incurred by selecting the incorrect one.
This criterion performs well in practice at find-
ing a linear separator in high-dimensional feature
spaces (Tsochantaridis et al, 2004; Crammer et
al., 2006).
Now, recent advances in machine learning have
shown that the generalization ability of these
learners can be improved by utilizing second or-
der information, as in the Second Order Percep-
tron (Cesa-Bianchi et al, 2005), Gaussian Margin
Machines (Crammer et al, 2009b), confidence-
weighted learning (Dredze and Crammer, 2008),
AROW (Crammer et al, 2009a; Chiang, 2012)
and Relative Margin Machines (RMM) (Shiv-
aswamy and Jebara, 2009b). The latter, RMM,
was introduced as an effective and less computa-
tionally expensive way to incorporate the spread
of the data ? second order information about the
1116
distance between hypotheses when projected onto
the line defined by the weight vector w.
Unfortunately, not all advances in machine
learning are easy to apply to structured prediction
problems such as SMT; the latter often involve la-
tent variables and surrogate references, resulting
in loss functions that have not been well explored
in machine learning (Mcallester and Keshet, 2011;
Gimpel and Smith, 2012). Although Shivaswamy
and Jebara extended RMM to handle sequen-
tial structured prediction (Shivaswamy and Jebara,
2009a), their batch approach to quadratic opti-
mization, using existing off-the-shelf QP solvers,
does not provide a practical solution: as Taskar et
al. (2006) observe, ?off-the-shelf QP solvers tend
to scale poorly with problem and training sam-
ple size? for structured prediction problems.. This
motivates an online gradient-based optimization
approach?an approach that is particularly attrac-
tive because its simple update is well suited for ef-
ficiently processing structured objects with sparse
features (Crammer et al, 2012).
The contributions of this paper include (1) in-
troduction of a loss function for structured RMM
in the SMT setting, with surrogate reference trans-
lations and latent variables; (2) an online gradient-
based solver, RM, with a closed-form parameter
update to optimize the relative margin loss; and
(3) an efficient implementation that integrates well
with the open source cdec SMT system (Dyer et
al., 2010).1 In addition, (4) as our solution is not
dependent on any specific QP solver, it can be
easily incorporated into practically any gradient-
based learning algorithm.
After background discussion on learning in
SMT (?2), we introduce a novel online learning al-
gorithm for relative margin maximization suitable
for SMT (?3). First, we introduce RMM (?3.1) and
propose a latent structured relative margin objec-
tive which incorporates cost-augmented hypothe-
sis selection and latent variables. Then, we de-
rive a simple closed-form online update necessary
to create a large margin solution while simulta-
neously bounding the spread of the projection of
the data (?3.2). Chinese-English translation exper-
iments show that our algorithm, RM, significantly
outperforms strong state-of-the-art optimizers, in
both a basic feature setting and high-dimensional
(sparse) feature space (?4). Additional Arabic-
English experiments further validate these results,
1https://github.com/veidel/cdec
even where previously MERT was shown to be ad-
vantageous (?5). Finally, we discuss the spread
and other key issues of RM (?6), and conclude
with discussion of future work (?7).
2 Learning in SMT
Given an input sentence in the source language
x ? X , we want to produce a translation y ? Y(x)
using a linear model parameterized by a weight
vector w:
(y?, d?) = arg max
(y,d)?Y(x),D(x)
w>f(x, y, d)
where w>f(x, y, d) is the weighted feature scor-
ing function, hereafter s(x, y, d), and Y(x) is the
space of possible translations of x. While many
derivations d ? D(x) can produce a given transla-
tion, we are only able to observe y; thus we model
d as a latent variable. Although our models are
actually defined over derivations, they are always
paired with translations, so our feature function
f(x, y, d) is defined over derivation?translation
pairs.2 The learning goal is then to estimate w.
The instability of MERT in larger feature
sets (Foster and Kuhn, 2009; Hopkins and May,
2011), has motivated many alternative tuning
methods for SMT. These include strategies based
on batch log-linear models (Tillmann and Zhang,
2006; Blunsom et al, 2008), as well as the in-
troduction of online linear models (Liang et al,
2006a; Arun and Koehn, 2007).
Recent batch optimizers, PRO and RAMPION,
and Batch-MIRA (Cherry and Foster, 2012), have
been partly motivated by existing MT infrastruc-
tures, as they iterate between decoding the entire
tuning set and optimizing the parameters. PRO
considers tuning a classification problem and em-
ploys a binary classifier to rank pairs of outputs.
RAMPION aims to address the disconnect between
MT and machine learning by optimizing a struc-
tured ramp loss with a concave-convex procedure.
2.1 Large-Margin Learning
Online large-margin algorithms, such as MIRA,
have also gained prominence in SMT, thanks to
their ability to learn models in high-dimensional
feature spaces (Watanabe et al, 2007; Chiang et
al., 2009). The usual presentation of MIRA?s opti-
mization problem is given as a quadratic program:
2We may omit d in some equations for clarity.
1117
wt+1 = arg min
w
1
2 ||w ?wt||
2 + C?i
s.t. s(xi, yi, d)? s(xi, y?, d) ? ?i(y?)? ?i
(1)
where y? is the single most violated constraint, the
cost ?i(y) is computed using an external measure
of quality, such as 1-BLEU(yi, y), and a slack vari-
able ?i is introduced to allow for non-separable
instances. C acts as a regularization parameter,
trading off between margin maximization and con-
straint violations.
While solving the optimization problem relies
on computing the margin between the correct out-
put yi, and y?, in SMT our decoder is often inca-
pable of producing the reference translation, i.e.
yi /? Y(xi). We must instead resort to selecting a
surrogate reference, y+ ? Y(xi). This issue has
recently received considerable attention (Liang
et al, 2006a; Eidelman, 2012; Chiang, 2012),
with preference given to surrogate references ob-
tained through cost-diminished hypothesis selec-
tion. Thus, y+ is selected based on a combination
of model score and error metric from the k-best
list produced by our current model. A similar se-
lection is made for the cost-augmented hypothesis
y? ? Y(xi):
(y+, d+)? arg max
(y,d)?Y(xi),D(xi)
s(xi, y, d)??i(y)
(y?, d?)? arg max
(y,d)?Y(xi),D(xi)
s(xi, y, d) + ?i(y)
In this setting, the optimization problem be-
comes:
wt+1 = arg min
w
1
2 ||w ?wt||
2 + C?i
s.t. ?s(xi, y+, y?) ? ?i(y?)??i(y+)? ?i
(2)
where ?s(xi, y+, y?)=s(xi, y+, d+)-s(xi, y?, d?)
This leads to a variant of the structured ramp
loss to be optimized:
` =
? max
(y+,d+)?Y(xi),D(xi)
(s(xi, y+, d+)??i(y+)
)
+ max
(y?,d?)?Y(xi),D(xi)
(s(xi, y?, d?) + ?i(y?)
)
(3)
The passive-aggressive update (Crammer et al,
2006), which is used to solve this problem, up-
dates w on each round such that the score of the
correct hypothesis y+ is greater than the score of
the incorrect y? by a margin at least as large as the
cost incurred by predicting the incorrect hypothe-
sis, while keeping the change to w small.
 
(a)
 
(b)
Figure 1: (a) RM and large margin solution comparison and
(b) the spread of the projections given by each. RM and large
margin solutions are shown with a darker dotted line and a
darker solid line, respectively.
3 The Relative Margin Machine in SMT
3.1 Relative Margin Machine
The margin, the distance between the correct
hypothesis and incorrect one, is defined by
s(xi, y+, d+) and s(xi, y?, d?). It is maxi-
mized by minimizing the norm in SVM, or
analogously, the proximity constraint in MIRA:
arg minw 12 ||w ?wt||2. However, theoretical re-sults supporting large-margin learning, such as the
VC-dimension (Vapnik, 1995) or the Rademacher
bound (Bartlett and Mendelson, 2003) consider
measures of complexity, in addition to the empir-
ical performance, when describing future predic-
tive ability. The measures of complexity usually
take the form of some value on the radius of the
data, such as the ratio of the radius of the data to
the margin (Shivaswamy and Jebara, 2009a). As
radius is a way of measuring spread in any pro-
jection direction, here we will specifically be in-
terested in the the spread of the data as measured
after the projection defined by the learned model
w.
More formally, the spread is the dis-
tance between y+, and the worst candidate
(yw, dw)? arg min(y,d)?Y(xi),D(xi) s(xi, y, d),after projecting both onto the line defined by the
weight vector w. For each y?, this projection is
conveniently given by s(xi, y?, d), thus the spread
is calculated as ?s(xi, y+, yw).
RMM was introduced as a generalization over
SVM that incorporates both the margin constraint
1118
and information regarding the spread of the data.
The relative margin is the ratio of the absolute,
or maximum margin, to the spread of the pro-
jected data. Thus, the RMM learns a large mar-
gin solution relative to the spread of the data, or
in other words, creates a max margin while si-
multaneously bounding the spread of the projected
data. As a concrete example, consider the plot
shown in Figure 1(a), with hypotheses represented
by two-dimensional feature vectors. The point
marked with a circle in the upper right represents
f(xi, y+), while all other squares represent alter-
native incorrect hypotheses f(xi, y?). The large
margin decision boundary is shown with a darker
solid line, while the relative margin solution is
shown with a darker dotted line. The lighter lines
parallel to each define the margins, with the square
at the intersection being f(xi, y?). The bottom
portion of Figure 1(b) presents an alternative view
of each solution, showing the projections of the
hypotheses given the learned model of each. No-
tice that with a large margin solution, although the
distance between y+ and y? is greater, the points
are highly spread, extending far to the left of the
decision boundary.
In contrast, with a relative margin, although
we have a smaller absolute margin, the spread is
smaller, all points being within a smaller distance 
of the decision boundary. The higher the spread of
the projection, the higher the variance of the pro-
jected points, and the greater the likelihood that
we will mislabel a new instance, since the high
variance projections may cross the learned deci-
sion boundary. In higher dimensions, accounting
for the spread becomes even more crucial, as will
be discussed in Section 6.3
Although RMM is theoretically well-founded
and improves practical performance over large-
margin learning in the settings where it was intro-
duced, it is unsuitable for most complex structured
prediction in NLP. Nonetheless, since structured
RMM is a generalization of Structured SVM,
which shares its underlying objective with MIRA,
our intuition is that SMT should be able to benefit
as well. But to take advantage of the second-order
information RMM utilizes for increased general-
izability in SMT, we need a computationally effi-
3The motivation of confidence-weighted estima-
tion (Dredze and Crammer, 2008) and AROW (Crammer
et al, 2009a) is related in spirit. They use second-order
information in the form of a distribution over weights to
change the maximum margin solution.
cient optimization procedure that does not require
batch training or an off-the-shelf QP solver.
3.2 RM Algorithm
We address the above-mentioned limitations by in-
troducing a novel online learning algorithm for
relative margin maximization, RM. The relative
margin solution is obtained by maximizing the
same margin as Equation (2), but now with re-
spect to the distance between y+, and the worst
candidate yw. Thus, the relative margin dictates
trading-off between a large margin as before, and
a small spread of the projection, in other words,
bounding the distance between y+ and yw. The
additional computation required, namely, obtain-
ing yw, is efficient to perform, and has likely al-
ready happened while obtaining the k-best deriva-
tions necessary for the margin update. The online
latent structured soft relative margin optimization
problem is then:
wt+1 = arg min
w
1
2 ||w ?wt||
2 + C?i + D?i
s.t.: ?s(xi, y+, y?) ? ?i(y?)??i(y+)? ?i
?B ? ?i ? ?s(xi, y+, yw) ? B + ?i
(4)
where additional bounding constraints are added
to the usual margin constraints in order to contain
the spread by bounding the difference in projec-
tions. B is an additional parameter; it controls
the spread, trading off between margin maximiza-
tion and spread minimization. Notice that when
B ? ?, the bounding constraints disappear, and
we are left with the original problem in Equa-
tion (2). D, which plays an analogous role to C,
allows penalized violations of the bounding con-
straints.
The dual of Equation (4) can be derived as:
max
?,?,??
L =
?
y?Y(xi)
?y ?B
?
y?Y(xi)
?y ?B
?
y?Y(xi)
??y
?12
? ?
y?Y(xi)
?y?i(y+, y)?
?
y?Y(xi)
?y?i(y+, y)
+
?
y?Y(xi)
??y?i(y+, y),
?
y??Y(xj)
?y??j(y+, y?)?
?
y??Y(xj)
?y??j(y+, y?)
+
?
y??Y(xj)
??y??j(y+, y?)
?
(5)
where the ? Lagrange multiplier corresponds
to the standard margin constraint, while ? and
1119
?? each correspond to a bounding constraint,
and ?i(y+, y?) corresponds to the difference of
f(xi, y+, d+) and f(xi, y?, d?). The weight up-
date can then be obtained from the dual variables:
?
?y?i(y+, y)?
?
?y?i(y+, y) +
?
??y?i(y+, y)
(6)
The dual in Equation (5) can be optimized us-
ing a cutting plane algorithm, an effective method
for solving a relaxed optimization problem in
the dual, used in Structured SVM, MIRA, and
RMM (Tsochantaridis et al, 2004; Chiang, 2012;
Shivaswamy and Jebara, 2009a). The cutting
plane presented in Alg. 1 decomposes the overall
problem into subproblems which are solved inde-
pendently by creating working sets Sji , which cor-
respond to the largest violations of either the mar-
gin constraint, or bounding constraints, and itera-
tively satisfying the constraints in each set.
The cutting plane in Alg. 1 makes use of the
the closed-form gradient-based updates we de-
rived for RM presented in Alg. 2. The updates
amount to performing a subgradient descent step
to update w in accordance with the constraints.
Since the constraint matrix of the dual program is
not strictly decomposable across constraint types,
we are in effect solving an approximation of the
original problem.
Algorithm 1 RM Cutting Plane Algorithm
(adapted from (Shivaswamy and Jebara, 2009a))
Require: ith training example (xi, yi), weight w, margin
reg. C, bound B, bound reg. D, , B
1: S1i ?
{
y+
}, S2i ?
{
y+
}, S3i ?
{
y+
}
2: repeat
3: H(y) := ?i(y)??i(y+)? ?s(xi, y+, y)
4: y1 ? arg maxy?Y(xi) H(y)
5: y2 ? arg maxy?Y(xi) G(y) := ?s(xi, y+, y)6: y3 ? arg miny?Y(xi)?G(y)7: ? ? max {0,maxy?Si H(y)}8: V1 ? H(y1)? ? ? 
9: V2 ? G(y2)?B ? B
10: V3 ? ?G(y3)?B ? B
11: j ? argmaxj??{1,2,3} Vj?
12: if Vj > 0 then
13: Sji ? Sji ? {yj}
14: OPTIMIZE(w, S1i , S2i , S3i , C,B) . see Alg. 2
15: end if
16: until S1i , S2i , S3i do not change
Alternatively, we could utilize a passive-
aggressive updating strategy (Crammer et al,
2006), which would simply bypass the cutting
plane and select the most violated constraint for
Algorithm 2 RM update with ?, ?, ??
1: procedure OPTIMIZE(w, S1i , S2i , S3i , C,B)
2: whilew changes do
3: if ??S1i
?? > 1 then
4: UPDATEMARGIN(w, S1i , C)
5: end if
6: if ??S2i
?? > 1 then
7: UPDATEUPPERBOUND(w, S2i , B)
8: end if
9: if ??S3i
?? > 1 then
10: UPDATELOWERBOUND(w, S3i , B)
11: end if
12: end while
13: end procedure
14: procedure UPDATEMARGIN(w, S1i , C)
15: ?y ? 0 for all y ? S1i
16: ?y+i ? C17: for n? 1...MaxIter do
18: Select two constraints y, y? from S1i
19: ?? ? ?i(y?)??i(y)??s(xi, y, y?)||?(y,y?)||2
20: ?? ? max(??y,min(?y? , ??))
21: ?y ? ?y + ?? ; ??y ? ??y ? ??
22: w? w + ??(?(y, y?))
23: end for
24: end procedure
25: procedure UPDATEUPPERBOUND(w, S2i , B)
26: ?y ? 0 for all y ? S2i
27: for n? 1...MaxIter do
28: Select one constraint y from S2i
29: ?? ? max(0, B??s(xi ,y+ ,y)||?(y+,y)||2 )
30: ?y ? ?y + ??
31: w? w ? ??(?(y+, y))
32: end for
33: end procedure
34: procedure UPDATELOWERBOUND(w, S3i , B)
35: ??y ? 0 for all y ? S3i
36: for n? 1...MaxIter do
37: Select one constraint y from S3i
38: ??? ? max(0, ?B??s(xi ,y+ ,y)||?(y+,y)||2 )
39: ??y ? ??y + ???
40: w? w + ???(?(y+, y))
41: end for
42: end procedure
each set, if there is one, and perform the corre-
sponding parameter updates in Alg. 2. We re-
fer to the resulting passive-aggressive algorithm as
RM-PA, and the cutting plane version as RM-CP.
Preliminary experiments showed that RM-PA per-
forms on par with RM-CP, thus RM-PA is the one
used in the empirical evaluation below.
A graphical depiction of the passive-aggressive
RM update is presented in Figure 2. The upper
right circle represents y+, while all other squares
represent alternative hypotheses y?. As in the stan-
dard MIRA solution, we select the maximum mar-
gin constraint violator, y?, shown as the triangle,
and update such that the margin is greater than the
cost. Additionally, we select the maximum bound-
1120
 
Bounding Constraint 
 
dist 
 
cost > margin 
 
B
LE
U
  
S
co
re
 
Margin Constraint 
cost 
 
margin 
 
Model Score 
dist > B 
 
 B 
 
Figure 2: RM update with margin and bounding con-
straints. The diagonal dotted line depicts cost?margin equi-
librium. The vertical gray dotted line depicts the bound B.
White arrows indicate updates triggered by constraint viola-
tions. Squares are data points in the k-best list not selected
for update in this round.
task Corpus Sentences Tokens
En Zh/Ar
Zh-En
training 1.6M 44.4M 40.4M
tune (MT06) 1664 48k 39k
MT03 919 28k 24k
MT05 1082 35k 33k
Ar-En
training 1M 23.7M 22.8M
tune (MT06) 1797 55k 49k
MT05 1056 36k 33k
MT08 1360 51k 45k
4-gram LM 24M 600M ?
Table 1: Corpus statistics
ing constraint violator, yw, shown as the upside-
down triangle, and update so the distance from y+
is no greater than B.
4 Experiments
4.1 Setup
To evaluate the advantage of explicitly accounting
for the spread of the data, we conducted several
experiments on two Chinese-English translation
test sets, using two different feature sets in each.
For training we used the non-UN and non-HK
Hansards portions of the NIST training corpora,
which was segmented using the Stanford seg-
menter (Tseng et al, 2005). The data statistics are
summarized in the top half of Table 1. The English
data was lowercased, tokenized and aligned using
GIZA++ (Och and Ney, 2003) to obtain bidirec-
tional alignments, which were symmetrized using
the grow-diag-final-and method (Koehn
et al, 2003). We trained a 4-gram LM on the
English side of the corpus with additional words
from non-NYT and non-LAT, randomly selected
portions of the Gigaword v4 corpus, using modi-
fied Kneser-Ney smoothing (Chen and Goodman,
1996). We used cdec (Dyer et al, 2010) as our
hierarchical phrase-based decoder, and tuned the
parameters of the system to optimize BLEU (Pap-
ineni et al, 2002) on the NIST MT06 corpus.
We applied several competitive optimizers as
baselines: hypergraph-based MERT (Kumar et al,
2009), k-best variants of MIRA (Crammer et al,
2006; Chiang et al, 2009), PRO (Hopkins and
May, 2011), and RAMPION (Gimpel and Smith,
2012). The size of the k-best list was set to 500
for RAMPION, MIRA and RM, and 1500 for PRO,
with both PRO and RAMPION utilizing k-best ag-
gregation across iterations. RAMPION settings
were as described in (Gimpel and Smith, 2012),
and PRO settings as described in (Hopkins and
May, 2011), with PRO requiring regularization
tuning in order to be competitive with the other op-
timizers. MIRA and RM were run with 15 paral-
lel learners using iterative parameter mixing (Mc-
Donald et al, 2010). All optimizers were imple-
mented in cdec and use the same system config-
uration, thus the only independent variable is the
optimizer itself. We set C to 0.01, and MaxIter
to 100. We selected the bound step size D, based
on performance on a held-out dev set, to be 0.01
for the basic feature set and 0.1 for the sparse fea-
ture set. The bound constraintB was set to 1.4 The
approximate sentence-level BLEU cost ?i is com-
puted in a manner similar to (Chiang et al, 2009),
namely, in the context of previous 1-best transla-
tions of the tuning set. All results are averaged
over 3 runs.
4.2 Feature Sets
We experimented with a small (basic) feature set,
and a large (sparse) feature set. For the small
feature set, we use 14 features, including a lan-
guage model, 5 translation model features, penal-
ties for unknown words, the glue rule, and rule
arity. For experiments with a larger feature set,
we introduced additional lexical and non-lexical
sparse Boolean features of the form commonly
found in the literature (Chiang et al, 2009; Watan-
4We also conducted an investigation into the setting of the
B parameter. We explored alternative values for B, as well
as scaling it by the current candidate?s cost, and found that
the optimizer is fairly insensitive to these changes, resulting
in only minor differences in BLEU.
1121
Optimizer Zh Ar
MIRA 35k 37k
PRO 95k 115k
RAMPION 22k 24k
RM 30k 32k
Active+Inactive 3.4M 4.9M
Table 2: Active sparse feature templates
abe et al, 2007; Simianer et al, 2012).
Non-lexical features include structural distor-
tion, which captures the dependence between re-
ordering and the size of a filler, and rule shape,
which bins grammar rules by their sequence of
terminals and nonterminals (Chiang et al, 2008).
Lexical features on rules include rule ID, which
fires on a specific grammar rule. We also in-
troduce context-dependent lexical features for the
300 most frequent aligned word pairs (f ,e) in the
training corpus, which fire on triples (f ,e,f+1) and
(f ,e,f?1), capturing when we see f aligned to e,
with f+1 and f?1 occurring to the right or left of f ,
respectively. All other words fall into the default
?unk? feature bin. In addition, we have insertion
and deletion features for the 150 most frequently
unaligned target and source words. These feature
templates resulted in a total of 3.4 million possible
features, of which only a fraction were active for
the respective tuning set and optimizer, as shown
in Table 2.
4.3 Results
As can be seen from the results in Table 3, our
RM method was the best performer in all Chinese-
English tests according to all measures ? up to 1.9
BLEU and 6.6 TER over MIRA ? even though we
only optimized for BLEU.5 Surprisingly, it seems
that MIRA did not benefit as much from the sparse
features as RM. The results are especially notable
for the basic feature setting ? up to 1.2 BLEU and
4.6 TER improvement over MERT ? since MERT
has been shown to be competitive with small num-
bers of features compared to high-dimensional op-
timizers such as MIRA (Chiang et al, 2008).
For the tuning set, the decoder performance was
consistently the lowest with RM, compared to the
5In the small feature set RAMPION yielded similar best
BLEU scores, but worse TER. In preliminary experiments
with a smaller trigram LM, our RM method consistently
yielded the highest scores in all Chinese-English tests ? up
to 1.6 BLEU and 6.4 TER from MIRA, the second best per-
former.
other optimizers. We believe this is due to the
RM bounding constraint being more resistant to
overfitting the training data, and thus allowing for
improved generalization. Conversely, while PRO
had the second lowest tuning scores, it seemed to
display signs of underfitting in the basic and large
feature settings.
5 Additional Experiments
In order to explore the applicability of our ap-
proach to a wider range of languages, we also eval-
uated its performance on Arabic-English transla-
tion. All experimental details were the same as
above, except those noted below.
For training, we used the non-UN portion of the
NIST training corpora, which was segmented us-
ing an HMM segmenter (Lee et al, 2003). Dataset
statistics are given in the bottom part of Table 1.
The sparse feature templates resulted here in a to-
tal of 4.9 million possible features, of which again
only a fraction were active, as shown in Table 2.
As can be seen in Table 4, in the smaller feature
set, RM and MERT were the best performers, with
the exception that on MT08, MIRA yielded some-
what better (+0.7) BLEU but a somewhat worse
(-0.9) TER score than RM.
On the large feature set, RM is again the best
performer, except, perhaps, a tied BLEU score
with MIRA on MT08, but with a clear 1.8 TER
gain. In both Arabic-English feature sets, MIRA
seems to take the second place, while RAMPION
lags behind, unlike in Chinese-English (?4).6
Interestingly, RM achieved substantially higher
BLEU precision scores in all tests for both lan-
guage pairs. However, this was also usually cou-
pled had a higher brevity penalty (BP) thanMIRA,
with the BP increasing slightly when moving to
the sparse setting.
6 Discussion
The trend of the results, summarized as RM gain
over other optimizers averaged over all test sets, is
presented in Table 5. RM shows clear advantage
in both basic and sparse feature sets, over all other
state-of-the-art optimizers. The RM gains are no-
tably higher in the large feature set, which we take
6In our preliminary experiments with the smaller trigram
LM, MERT did better on MT05 in the smaller feature set, and
MIRA had a small advantage in two cases. RAMPION per-
formed similarly to RM on the smaller feature set. RM?s loss
was only up to 0.8 BLEU (0.7 TER) from MERT or MIRA,
while its gains were up to 1.7 BLEU and 2.1 TER over MIRA.
1122
Small (basic) feature set Large (sparse) feature set
Optimizer Tune MT03 MT05 Tune MT03 MT05
?BLEU ?BLEU ?TER ?BLEU ?TER ?BLEU ?BLEU ?TER ?BLEU ?TER
MERT 35.4 35.8 60.8 32.4 63.9 - - - - -
MIRA 35.5 35.8 61.1 32.1 64.6 36.6 35.9 60.6 32.1 64.1
PRO 34.1 36.0 60.2 31.7 63.4 35.7 34.8 56.1 31.4 59.1
RAMPION 35.1 36.5 58.6 33.0 61.3 36.7 36.9 57.7 33.3 60.6
RM 31.3 36.5 56.4 33.6 59.3 33.2 37.5 54.6 34.0 57.5
Table 3: Performance on Zh-En with basic (left) and sparse (right) feature sets on MT03 and MT05.
Small (basic) feature set Large (sparse) feature set
Optimizer Tune MT05 MT08 Tune MT05 MT08
?BLEU ?BLEU ?TER ?BLEU ?TER ?BLEU ?BLEU ?TER ?BLEU ?TER
MERT 43.8 53.3 40.2 41.0 50.7 - - - - -
MIRA 43.0 52.8 40.8 41.3 50.6 44.4 53.4 40.1 41.8 50.2
PRO 41.5 51.3 41.5 39.4 51.5 46.8 53.2 40.0 41.4 49.7
RAMPION 42.4 52.0 40.8 40.0 50.8 44.6 52.9 40.4 41.0 50.4
RM 38.5 53.3 39.8 40.6 49.7 43.0 55.3 37.5 41.8 48.4
Table 4: Performance on Ar-En with basic (left) and sparse (right) feature sets on MT05 and MT08.
Small set Large set
Optimizer BLEU TER BLEU TER
MERT 0.4 2.6 - -
MIRA 0.5 3.0 1.4 4.3
PRO 1.4 2.9 2.0 1.7
RAMPION 0.6 1.6 1.2 2.8
Table 5: RM gain over other optimizers averaged
over all test sets.
as an indication for the importance of bounding
the spread.
Spread analysis: For RM, the average spread
of the projected data in the Chinese-English small
feature set was 0.9?3.6 for all tuning iterations,
and 0.7?2.9 for the iteration with the highest de-
coder performance. In comparison, the spread of
the data for MIRA was 5.9?20.5 for the best it-
eration. In the sparse setting, RM had an aver-
age spread of 0.9?2.4 for the best iteration, while
MIRA had a spread of 14.0?31.1. Similarly,
on Arabic-English, RM had a spread of 0.7?2.4
in the small setting, and 0.82?1.4 in the sparse
setting, while MIRA?s spread was 9.4?26.8 and
11.4?22.1, for the small and sparse settings, re-
spectively. Notice that the average spread for RM
stays about the same when moving to higher di-
mensions, with the variance decreasing in both
cases. For MIRA, however, the average spread
increases in both cases, with the variance being
much higher than RM. For instance, observe that
the spread of MIRA on Chinese grows from 5.9 to
14.0 in the sparse feature setting. While bounding
the spread is useful in the low-dimensional setting
(0.7-1.5 BLEU gain with RM over MIRA as shown
in Table 3), accounting for the spread is even more
crucial with sparse features, where MIRA gains
only up to 0.1 BLEU, while RM gains 1 BLEU.
These results support the claim that our imposed
bound B indeed helps decrease the spread, and
that, in turn, lower spread yields better general-
ization performance.
Error Analysis: The inconclusive advantage
of RM over MIRA (in BLEU vs. TER scores)
on Arabic-English MT08 calls for a closer look.
Therefore we conducted a coarse error analysis
on 15 randomly selected sentences from MERT,
RMM and MIRA, with basic and sparse feature
settings for the latter two. This sample yielded
450 data points for analysis: output of the 5 con-
ditions on 15 sentences scored in 6 violation cate-
gories. The categories were: function word drop,
content word drop, syntactic error (with a reason-
able meaning), semantic error (regardless of syn-
tax), word order issues, and function word mis-
translation and ?hallucination?. The purpose of
this analysis was to get a qualitative feel for the
output of each model, and a better idea as to why
we obtained performance improvements. RM no-
1123
ticeably had more word order and excess/wrong
function word issues in the basic feature setting
than any optimizer. However, RM seemed to ben-
efit the most from the sparse features, as its bad
word order rate dropped close toMIRA, and its ex-
cess/wrong function word rate dropped below that
of MIRA with sparse features (MIRA?s rate actu-
ally doubled from its basic feature set). We con-
jecture both these issues will be ameliorated with
syntactic features such as those in Chiang et al
(2008). This correlates with our observation that
RM?s overall BLEU score is negatively impacted
by the BP, as the BLEU precision scores are no-
ticeably higher.
K-best: RM is potentially more sensitive to the
size and order of the k-best list. While MIRA is
only concerned with the margin between y+ and
y?, RM also accounts for the distance between y+
and yw. It might be the case that a larger k-best, or
revisiting previous strategies for y+ and y? selec-
tion, such as bold updating, local updating (Liang
et al, 2006b), or max-BLEU updating (Tillmann
and Zhang, 2006) might have a greater impact.
Also, we only explored several settings of B, and
there remains a continuum of RM solutions that
trade off between margin and spread in different
ways.
Active features: Perhaps contrary to expecta-
tion, we did not see evidence of a correlation be-
tween the number of active features and optimizer
performance. RAMPION, with the fewest features,
is the closest performer to RM in Chinese, while
MIRA, with a greater number, is the closest on
Arabic. We also notice that while PRO had the
lowest BLEU scores in Chinese, it was competi-
tive in Arabic with the highest number of features.
7 Conclusions and Future Work
We have introduced RM, a novel online margin-
based algorithm designed for optimizing high-
dimensional feature spaces, which introduces con-
straints into a large-margin optimizer that bound
the spread of the projection of the data while max-
imizing the margin. The closed-form online up-
date for our relative margin solution accounts for
surrogate references and latent variables.
Experimentation in statistical MT yielded sig-
nificant improvements over several other state-
of-the-art optimizers, especially in a high-
dimensional feature space (up to 2 BLEU and 4.3
TER on average). Overall, RM achieves the best or
comparable performance according to two scoring
methods in two language pairs, with two test sets
each, in small and large feature settings. More-
over, across conditions, RM always yielded the
best combined TER-BLEU score.7
These improvements are achieved using stan-
dard, relatively small tuning sets, contrasted with
improvements involving sparse features obtained
using much larger tuning sets, on the order of
hundreds of thousands of sentences (Liang et al,
2006a; Tillmann and Zhang, 2006; Blunsom et al,
2008; Simianer et al, 2012). Since our approach
is complementary to scaling up the tuning data, in
future work we intend to combine these two meth-
ods. In future work we also intend to explore using
additional sparse features that are known to be use-
ful in translation, e.g. syntactic features explored
by Chiang et al (2008).
Finally, although motivated by statistical ma-
chine translation, RM is a gradient-based method
that can easily be applied to other problems. We
plan to investigate its utility elsewhere in NLP
(e.g. for parsing) as well as in other domains in-
volving high-dimensional structured prediction.
Acknowledgments
We would like to thank Pannaga Shivaswamy for
valuable discussions, and the anonymous review-
ers for their comments. Vladimir Eidelman is sup-
ported by a National Defense Science and Engi-
neering Graduate Fellowship. This work was also
supported in part by the BOLT program of the De-
fense Advanced Research Projects Agency, Con-
tract HR0011-12-C-0015.
References
Abishek Arun and Philipp Koehn. 2007. Online learn-
ing methods for discriminative training of phrase
based statistical machine translation. In MT Summit
XI.
Peter L. Bartlett and Shahar Mendelson. 2003.
Rademacher and gaussian complexities: risk bounds
and structural results. J. Mach. Learn. Res., 3:463?
482, March.
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
A discriminative latent variable model for statisti-
cal machine translation. In Proceedings of ACL-08:
HLT, Columbus, Ohio, June.
7We and other researchers often use 12 (TER?BLEU) as acombined SMT quality metric.
1124
Nicolo` Cesa-Bianchi, Alex Conconi, and Claudio Gen-
tile. 2005. A second-order perceptron algorithm.
SIAM J. Comput., 34(3):640?668, March.
Stanley F. Chen and Joshua Goodman. 1996. An em-
pirical study of smoothing techniques for language
modeling. In Proceedings of the 34th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 310?318.
Colin Cherry and George Foster. 2012. Batch tuning
strategies for statistical machine translation. In Pro-
ceedings of NAACL.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), Waikiki, Honolulu, Hawaii.
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 new features for statistical machine trans-
lation. In Proceedings of Human Language Tech-
nologies: The 2009 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, NAACL ?09, pages 218?226.
David Chiang. 2012. Hope and fear for discriminative
training of statistical translation models. J. Machine
Learning Research.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. 2006. On-
line passive-aggressive algorithms. J. Mach. Learn.
Res., 7:551?585.
Koby Crammer, Alex Kulesza, and Mark Dredze.
2009a. Adaptive regularization of weight vectors.
In Advances in Neural Information Processing Sys-
tems 22, pages 414?422.
Koby Crammer, Mehryar Mohri, and Fernando Pereira.
2009b. Gaussian margin machines. Journal of
Machine Learning Research - Proceedings Track,
5:105?112.
Koby Crammer, Mark Dredze, and Fernando Pereira.
2012. Confidence-weighted linear classification
for text categorization. J. Mach. Learn. Res.,
98888:1891?1926, June.
Mark Dredze and Koby Crammer. 2008. Confidence-
weighted linear classification. In In ICML 08: Pro-
ceedings of the 25th international conference on
Machine learning, pages 264?271. ACM.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Jonathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Seti-
awan, Vladimir Eidelman, and Philip Resnik. 2010.
cdec: A decoder, alignment, and learning framework
for finite-state and context-free translation models.
In Proceedings of ACL System Demonstrations.
Vladimir Eidelman. 2012. Optimization strategies for
online large-margin learning in machine translation.
In Proceedings of the Seventh Workshop on Statisti-
cal Machine Translation.
George Foster and Roland Kuhn. 2009. Stabilizing
minimum error rate training. In Proceedings of the
Fourth Workshop on Statistical Machine Transla-
tion, pages 242?249, Athens, Greece, March. As-
sociation for Computational Linguistics.
Kevin Gimpel and Noah A. Smith. 2012. Structured
ramp loss minimization for machine translation. In
Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics.
Mark Hopkins and Jonathan May. 2011. Tuning as
ranking. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1352?1362, Edinburgh, Scotland, UK.,
July. Association for Computational Linguistics.
Thorsten Joachims. 1998. Text Categorization with
Support Vector Machines: Learning with Many Rel-
evant Features. In Claire Ne?dellec and Ce?line Rou-
veirol, editors, European Conference on Machine
Learning, pages 137?142, Berlin. Springer.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the 2003 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics on Human Language Technology - Vol-
ume 1, NAACL ?03, Stroudsburg, PA, USA.
Shankar Kumar, Wolfgang Macherey, Chris Dyer, and
Franz Och. 2009. Efficient minimum error rate
training and minimum bayes-risk decoding for trans-
lation hypergraphs and lattices. In Proceedings of
the Joint Conference of the 47th Annual Meeting of
the ACL and the 4th International Joint Conference
on Natural Language Processing of the AFNLP,
pages 163?171.
Young-Suk Lee, Kishore Papineni, Salim Roukos, Os-
sama Emam, and Hany Hassan. 2003. Language
model based Arabic word segmentation. In Pro-
ceedings of the 41st Annual Meeting on Associa-
tion for Computational Linguistics - Volume 1, pages
399?406.
Percy Liang, Alexandre Bouchard-Co?te?, Dan Klein,
and Ben Taskar. 2006a. An end-to-end discrimi-
native approach to machine translation. In Proceed-
ings of the 21st International Conference on Com-
putational Linguistics and the 44th annual meeting
of the Association for Computational Linguistics,
ACL-44, pages 761?768.
Percy Liang, Alexandre Bouchard-Co?te?, Dan Klein,
and Ben Taskar. 2006b. An end-to-end discrimi-
native approach to machine translation. In Proceed-
ings of the 2006 International Conference on Com-
putational Linguistics (COLING) - the Association
for Computational Linguistics (ACL).
David Mcallester and Joseph Keshet. 2011. Gener-
alization bounds and consistency for latent struc-
tural probit and ramp loss. In J. Shawe-Taylor,
1125
R.S. Zemel, P. Bartlett, F.C.N. Pereira, and K.Q.
Weinberger, editors, Advances in Neural Informa-
tion Processing Systems 24, pages 2205?2212.
Ryan McDonald, Keith Hall, and Gideon Mann. 2010.
Distributed training strategies for the structured per-
ceptron. In Human Language Technologies: The
2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 456?464, Los Angeles, California.
Franz Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
In Computational Linguistics, volume 29(21), pages
19?51.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Annual Meeting of the Association for Com-
putational Linguistics, pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311?318.
Pannagadatta Shivaswamy and Tony Jebara. 2009a.
Structured prediction with relative margin. In In
International Conference on Machine Learning and
Applications.
Pannagadatta K Shivaswamy and Tony Jebara. 2009b.
Relative margin machines. In In Advances in Neural
Information Processing Systems 21. MIT Press.
Patrick Simianer, Stefan Riezler, and Chris Dyer. 2012.
Joint feature selection in distributed stochastic learn-
ing for large-scale discriminative training in smt. In
Proceedings of the 50th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), Jeju Island, Korea, July.
David A. Smith and Jason Eisner. 2006. Minimum risk
annealing for training log-linear models. In Pro-
ceedings of the COLING/ACL 2006 Main Confer-
ence Poster Sessions, Sydney, Australia, July. Asso-
ciation for Computational Linguistics.
Ben Taskar, Simon Lacoste-Julien, and Michael I. Jor-
dan. 2006. Structured prediction, dual extragradi-
ent and bregman projections. J. Mach. Learn. Res.,
7:1627?1653, December.
Christoph Tillmann and Tong Zhang. 2006. A discrim-
inative global training algorithm for statistical MT.
In Proceedings of the 2006 International Conference
on Computational Linguistics (COLING) - the Asso-
ciation for Computational Linguistics (ACL).
Huihsin Tseng, Pi-Chuan Chang, Galen Andrew,
Daniel Jurafsky, and Christopher Manning. 2005. A
conditional random field word segmenter. In Fourth
SIGHAN Workshop on Chinese Language Process-
ing.
Ioannis Tsochantaridis, Thomas Hofmann, Thorsten
Joachims, and Yasemin Altun. 2004. Support vector
machine learning for interdependent and structured
output spaces. In Proceedings of the twenty-first in-
ternational conference on Machine learning, ICML
?04.
Vladimir N. Vapnik. 1995. The nature of statistical
learning theory. Springer-Verlag New York, Inc.,
New York, NY, USA.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and
Hideki Isozaki. 2007. Online large-margin train-
ing for statistical machine translation. In Proceed-
ings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), Prague, Czech Republic, June. Association
for Computational Linguistics.
1126
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 199?204,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Mr. MIRA: Open-Source Large-Margin Structured Learning on
MapReduce
Vladimir Eidelman1, Ke Wu1, Ferhan Ture1, Philip Resnik2, Jimmy Lin3
1 Dept. of Computer Science 2 Dept. of Linguistics 3 The iSchool
Institute for Advanced Computer Studies
University of Maryland
{eidelman,wuke,fture,resnik,jimmylin}@umd.edu
Abstract
We present an open-source framework
for large-scale online structured learning.
Developed with the flexibility to handle
cost-augmented inference problems such
as statistical machine translation (SMT),
our large-margin learner can be used with
any decoder. Integration with MapReduce
using Hadoop streaming allows efficient
scaling with increasing size of training
data. Although designed with a focus
on SMT, the decoder-agnostic design of
our learner allows easy future extension to
other structured learning problems such as
sequence labeling and parsing.
1 Introduction
Structured learning problems such as sequence la-
beling or parsing, where the output has a rich in-
ternal structure, commonly arise in NLP. While
batch learning algorithms adapted for structured
learning such as CRFs (Lafferty et al, 2001)
and structural SVMs (Joachims, 1998) have re-
ceived much attention, online methods such as
the structured perceptron (Collins, 2002) and a
family of Passive-Aggressive algorithms (Cram-
mer et al, 2006) have recently gained promi-
nence across many tasks, including part-of-speech
tagging (Shen, 2007), parsing (McDonald et
al., 2005) and statistical machine translation
(SMT) (Chiang, 2012), due to their ability to deal
with large training sets and high-dimensional in-
put representations.
Unlike batch learners, which must consider all
examples when optimizing the objective, online
learners operate in rounds, optimizing using one
example or a handful of examples at a time. This
online nature offers several attractive properties,
facilitating scaling to large training sets while re-
maining simple and offering fast convergence.
Mr. MIRA, the open source system1 de-
scribed in this paper, implements an online large-
margin structured learning algorithm based on
MIRA (?2.1), for cost-augmented online large-
scale training in high-dimensional feature spaces.
Our contribution lies in providing the first pub-
lished decoder-agnostic parallelization of MIRA
with Hadoop for structured learning.
While the current demonstrated application fo-
cuses on large-scale discriminative training for
machine translation, the learning algorithm is gen-
eral with respect to the inference algorithm em-
ployed. We are able to decouple our learner en-
tirely from the MT decoder, allowing users to
specify their own inference procedure through a
simple text communication protocol (?2.2). The
learner only requires k-best output with feature
vectors, as well as the specification of a cost func-
tion. Standard automatic evaluation metrics for
MT, such as BLEU (Papineni et al, 2002) and TER
(Snover et al, 2006), have already been imple-
mented. Furthermore, our system can be extended
to other structured learning problems with a min-
imal amount of effort, simply by implementing a
task-specific cost function and specifying an ap-
propriate decoder.
Through Hadoop streaming, our system can
take advantage of commodity clusters to handle
large-scale training (?3), while also being capable
of running in environments ranging from a single
machine to a PBS-managed batch cluster. Experi-
mental results (?4) show that it scales linearly and
makes fast parameter tuning on large tuning sets
for SMT practical.
2 Learning and Inference
2.1 Online Large-Margin Learning
MIRA is a popular online large-margin structured
learning method for NLP tasks (McDonald et al,
2005; Chiang et al, 2009; Chiang, 2012). The
1https://github.com/kho/mr-mira
199
main intuition is that we want our model to enforce
a margin between the correct and incorrect out-
puts of a sentence that agrees with our cost func-
tion. This is done by making the smallest update
we can to our parameters, w, on every sentence,
that will ensure that the difference in model scores
?fi(y?) = w>(f(xi, y+) ? f(xi, y?)) between the
correct output y+ and incorrect output y? is at least
as large as the cost, ?i(y?), incurred by predicting
the incorrect output:2
wt+1 = arg minw
1
2 ||w ?wt||
2 + C?i
s.t. ?y? ? Y(xi), ?fi(y?) ? ?i(y?)? ?i
where Y(xi) is the space of possible structured
outputs we are able to produce from xi, and
C is a regularization parameter that controls the
size of the update. In practice, we can de-
fine Y(xi) to be the k-best output. With a
passive-aggressive (PA) update, the ?y? constraint
above can be approximated by selecting the sin-
gle most violated constraint, which maximizes
y? ? arg maxy?Y(xi) w>f(xi, y) + ?i(y). This
optimization problem is attractive because it re-
duces to a simple analytical solution, essentially
performing a subgradient descent step with the
step size adjusted based on each example:
?? min
(
C, ?i(y
?)? ?fi(y?)
?f(xi, y+)? f(xi, y?)?2
)
w? w + ??
(
f(xi, y+)? f(xi, y?)
)
The user-defined cost function is a task-specific
external measure of quality that relays how bad se-
lecting y? truly is on the task we care about. The
cost can take any form as long as it decomposes
across the local parts of the structure, just as the
feature functions. For instance, it could be the
Hamming loss for sequence labeling, F-score for
parsing, or an approximate BLEU score for SMT.
Cost-augmented Inference For most struc-
tured prediction problems in machine learning,
yi ? Y(xi), that is, the model is able to produce,
and thus score, the correct output structure, mean-
ing y+ = yi. However, for certain NLP prob-
lems this may not be the case. For instance in
SMT, our model may not be able to produce or
reach the correct reference translation, which pro-
hibits our model from scoring it. This problem
2For a more formal description we refer the reader
to (Crammer et al, 2006; Chiang, 2012).
necessitates cost-augmented inference, where we
select y+ ? arg maxy?Y(xi) w>f(xi, y)??i(y)
from the space of structures our model can pro-
duce, to stand in for the correct output in optimiza-
tion. Our system was developed to handle both
cases, with the decoder providing the k-best list
to the learner, specifying whether to perform cost-
augmented selection.
Sparse Features While utilizing sparse features
is a primary motivation for performing large-scale
discriminative training, which features to use and
how to learn their weights can have a large im-
pact on the potential benefit. To this end, we in-
corporate `1/`2 regularization for joint feature se-
lection in order to improve efficiency and counter
overfitting effects (Simianer et al, 2012). Further-
more, the PA update has a single learning rate ?
for all features, which specifies how much the fea-
ture weights can change at each update. How-
ever, since dense features (e.g., language model)
are observed far more frequently than sparse fea-
tures (e.g., rule id), we may instead want to use
a per-feature learning rate that allows larger steps
for features that do not have much support. Thus,
we allow setting an adaptive per-feature learning
rate (Green et al, 2013; Crammer et al, 2009;
Duchi et al, 2011).
2.2 Learner/Decoder Communication
Training requires communication between the de-
coder and the learner. The decoder needs to re-
ceive weight updates and the input sentence from
the learner; and the learner needs to receive k-best
output with feature vectors from the decoder. This
is essentially all the required communication be-
tween the learner and the decoder. Below, we de-
scribe a simple line-based text protocol.
Input sentence and weight updates Follow-
ing common practice in machine translation, the
learner encodes each input sentence as a single-
line SGML entry named seg and sends it to the
decoder. The first line of Figure 1 is an exam-
ple sentence in this format. In addition to the
required sentence ID (useful in parallel process-
ing), an optional delta field is used to encode
the weight updates, as a sparse vector indexed
by feature names. First, for each name and up-
date pair, a binary record consisting of a null-
terminated string (name) and a double-precision
floating point number in native byte order (up-
date) is created. Then, all binary records are con-
200
<seg id="123" delta="TE0AexSuR+F6hD8="> das ist ein kleine haus </seg>
<seg id="124"> ein kleine haus </seg>\tein kleine ||| a small\thaus ||| house
Figure 1: Example decoder input in SGML
5
123 ||| 5 ||| this is a small house ||| TE0AAAAA... <base64> ||| 120.3
123 ||| 5 ||| this is the small house ||| <base64> ||| 118.4
123 ||| 5 ||| this was small house ||| <base64> ||| 110.5
<empty>
<empty>
Figure 2: Example k-best output
catenated and encoded in base64. In the example
above, the value of delta is the base64 encod-
ing of 0x4c 0x4d 0x00 0x7b 0x14 0xae 0x47
0xe1 0x7a 0x84 0x3f. The first 3 bytes store the
feature name (LM) and the next 8 bytes is its update
(0.01), to be added to the decoder?s current value
of the corresponding feature weight.
The learner also allows the user to pass any ad-
ditional information to the decoder, as long as it
can be encoded as a single-line text string. Such
information, if given, is appended after the seg en-
try, with a leading tab character as the delimiter.
For example, the second line of Figure 1 passes
two phrase translation rules to the decoder.
k-best output The decoder reads from standard
input and outputs the k-best output for one input
sentence before consuming the next line. For the
k-best output, the decoder first outputs to standard
output a line consisting of a single integerN . Next
the decoder outputs N lines where each line can
be either empty or an actual hypothesis. When the
line is an actual hypothesis, it consists of the fol-
lowing parts:
SID ||| LEN ||| TOK ||| FEAT [ REST ]
SID is the sentence ID of the corresponding input;
LEN is the length of source sentence;3 TOK contains
the tokens of the hypothesis sentence separated by
spaces; FEAT is the feature vector, encoded in the
same way as the weight updates, delimited by a
whitespace. Everything after FEAT until the end of
the line is discarded. See Figure 2 for an example
of k-best output. Note the scores after the last |||
are discarded by the learner.
Overall workflow The learner reads lines from
standard input in the following tab-delimited for-
mat:
3This is used in computing the smoothed cost. Usually
this is identical for all hypotheses if the input is a plain sen-
tence. But in applications such as lattice-based translation,
each hypothesis can be produced from different source sen-
tences, resulting in different lengths.
SRC<tab>REF<tab>REST
SRC is the actual input sentence as a seg entry; REF
is the gold output for the input sentence, for ex-
ample, reference translations in MT;4 REST is the
additional information that will be appended after
the seg entry and passed to the decoder.
The learner creates a sub-process for the de-
coder and connects to the sub-process? standard
input and output with pipes. Then it processes the
input lines one by one. For each line, it first sends
a composed input message to the decoder, combin-
ing the input sentence, weight updates, and user-
supplied information. Next it collects the k-best
output from the decoder, solves the QP problem to
obtain weight updates and repeats.
The learner produces two types of output. First,
the 1-best hypothesis for each input sentence, in
the following format:
SID<tab>TOK
Second, when there are no more input lines, the
learner outputs final weights and the number of
lines processed, in the following format:
-1<tab>NUM ||| WEIGHTS
The 1-best hypotheses can be scored against ref-
erences to obtain an estimate of cost. The final
weights are stored in a way convenient for averag-
ing in a parallel setting, as we shall discuss next.
3 Large-Scale Discriminative Training
3.1 MapReduce
With large amounts of data available today,
distributed computations have become essen-
tial. MapReduce (Dean and Ghemawat, 2004)
has emerged as a popular distributed process-
ing framework for commodity clusters that has
gained widespread adoption in both industry and
academia, thanks to its simplicity and the avail-
ability of the Hadoop open-source implementa-
tion. MapReduce provides a higher level of
4There can be multiple references, separated by |||.
201
abstraction for designing distributed algorithms
compared to, say, MPI or pthreads, by hiding
system-level details (e.g., deadlock, race condi-
tions, machine failures) from the developer.
A single MapReduce program begins with a
map phase, where mapper processes input key-
value pairs to produce an arbitrary number of in-
termediate key-value pairs. The mappers execute
in parallel, consuming data splits independently.
Following the map phase, all key-value pairs emit-
ted by the mappers are sorted by key and dis-
tributed to the reducers, such that all pairs shar-
ing the same key are guaranteed to arrive at the
same reducer. Finally, in the reduce phase, each
reducer processes the intermediate key-value pairs
it receives and emits final output key-value pairs.
3.2 System Architecture
Algorithm design We use Hadoop streaming to
parallelize the training process. Hadoop stream-
ing allows any arbitrary executable to serve as the
mapper or reducer, as long as it handles key-value
pairs properly.5 One iteration of training is im-
plemented as a single Hadoop streaming job. In
the map step, our learner can be directly used as
the mapper. Each mapper loads the same initial
weights, processes a single split of data and pro-
duces key-value pairs: the one-best hypothesis of
each sentence is output with the sentence ID as
the key (non-negative); the final weights with re-
spect to the split are output with a special negative
key. In the reduce step, a single reducer collects all
key-value pairs, grouped and sorted by keys. The
one-best hypotheses are output to disk in the or-
der they are received, so that the order matches the
reference translation set. The reducer also com-
putes the feature selection and weighted average
of final weights received from all of the mappers.
Assuming mapper i produces the final weights wi
after processing ni sentences, the weighted aver-
aged is defined as w? =
?
iwi?ni?
i ni
. Although aver-
aging yields different result from running a single
learner over the entire data, we have found the dif-
ference to be quite small in terms of convergence
and quality of tuned weights in practice.
After the reducer finishes, the averaged weights
are extracted and used as the initial weights for the
next iteration; the emitted hypotheses are scored
5By default, each line is treated as a key-value pair en-
coded in text, where the key and the value are separated by a
<tab>.
against the references, which allows us to track the
learning curve and the progress of convergence.
Scalability In an application such as SMT, the
decoder requires access to the translation gram-
mar and language model to produce translation hy-
potheses. For small tuning sets, which have been
typical in MT research, having these files trans-
ferred across the network to individual servers
(which then load the data into memory) is not
a problem. However, for even modest input on
the order of tens of thousands of sentences, this
creates a challenge. For example, distributing
thousands of per-sentence grammar files to all the
workers in a Hadoop cluster is time-consuming,
especially when this needs to be performed prior
to every iteration.
To benefit from MapReduce, it is essential to
avoid dependencies on ?side data? as much as
possible, due to the challenges explained above
with data transfer. To address this issue, we ap-
pend the per-sentence translation grammar as user-
supplied additional information to each input sen-
tence. This results in a large input file (e.g., 75 gi-
gabytes for 50,000 sentences), but this is not an is-
sue since the data reside on the Hadoop distributed
file system and MapReduce optimizes for data lo-
cality when scheduling mappers.
Unfortunately, it is much more difficult to ob-
tain per-sentence language models that are small
enough to handle in this same manner. Currently,
the best solution we have found is to use Hadoop?s
distributed cache to ship the single large language
model to each worker.
4 Evaluation
We evaluated online learning in Hadoop Map-
Reduce by applying it to German-English ma-
chine translation, using our hierarchical phrase-
based translation system with cdec as the de-
coder (Dyer et al, 2010). The parallel training
data consist of the Europarl and News Commen-
tary corpora from the WMT12 translation task,6
containing 2.08M sentences. A 5-gram language
model was trained on the English side of the bi-
text along with 750M words of news using SRILM
with modified Kneser-Ney smoothing (Chen and
Goodman, 1996).
We experimented with two feature sets: (1) a
small set with standard MT features, including
6http://www.statmt.org/wmt12/translation-task.html
202
Tuning set size Time/iteration # splits # features Tuning BLEU Test
(corpus) (on disk, GB) (in seconds) BLEU TER
dev 3.3 119 120 16 22.38 22.69 60.61
5k 7.8 289 120 16 32.60 22.14 59.60
10k 15.2 432 120 16 33.16 22.06 59.43
25k 37.2 942 300 16 32.48 22.21 59.54
50k 74.5 1802 600 16 32.21 22.21 59.39
dev 3.3 232 120 85k 23.08 23.00 60.19
5k 7.8 610 120 159k 33.70 22.26 59.26
10k 15.2 1136 120 200k 34.00 22.12 59.24
25k 37.2 2395 300 200k 32.96 22.35 59.29
50k 74.5 4465 600 200k 32.86 22.40 59.15
Table 1: Evaluation of our Hadoop implementation of MIRA, showing running time as well as BLEU
and TER values for tuning and testing data.
dev test 5k 10k 25k 50k
Sentences 3003 3003 5000 10000 25000 50000
Tokens en 75k 74k 132k 255k 634k 1258k
Tokens de 74k 73k 133k 256k 639k 1272k
Table 2: Corpus statistics
phrase and lexical translation probabilities in both
directions, word and arity penalties, and language
model scores; and (2) a large set containing the top
200k sparse features that might be useful to train
on large numbers of instances: rule id and shape,
target bigrams, insertions and deletions, and struc-
tural distortion features.
All experiments were conducted on a Hadoop
cluster (running Cloudera?s distribution, CDH
4.2.1) with 16 nodes, each with two quad-core 2.2
GHz Intel Nehalem Processors, 24 GB RAM, and
three 2 TB drives. In total, the cluster is configured
for a capacity of 128 parallel workers, although
we do not have direct control over the number
of simultaneous mappers, which depends on the
number of input splits. If the number of splits is
smaller than 128, then the cluster is under-utilized.
To note this, we report the number of splits for
each setting in our experimental results (Table 1).
We ran MIRA on a number of tuning sets, de-
scribed in Table 2, in order to test the effective-
ness and scalability of our system. First, we used
the standard development set from WMT12, con-
sisting of 3,003 sentences from news domain. In
order to show the scaling characteristics of our ap-
proach, we then used larger portions of the train-
ing bitext directly to tune parameters. In order to
avoid overfitting, we used a jackknifing method
to split the training data into n = 10 folds, and
built a translation system on n ? 1 folds, while
adjusting the sampling rate to sample sentences
from the other fold to obtain tuning sets ranging
from 5,000 sentences to 50,000 sentences. Table 1
shows details of experimental results for each set-
ting. The second column shows the space each
tuning set takes up on disk when we include refer-
ence translations and grammar files along with the
sentences. The reported tuning BLEU is from the
iteration with best performance, and running times
are reported from the top-scoring iteration as well.
Even though our focus in this evaluation is to
show the scalability of our implementation to large
input and feature sets, it is also worthwhile to men-
tion the effectiveness aspect. As we increase the
tuning set size by sampling sentences from the
training data, we see very little improvement in
BLEU and TER with the smaller feature set. This
is not surprising, since sparse features are more
likely to gain from additional tuning instances. In-
deed, tuning scores for all sets improve substan-
tially with sparse features, accompanied by small
increases on test.
While tuning on dev data results in better BLEU
on test data than when tuning on the larger sets, it
is important to note that although we are able to
tune more features on the larger bitext tuning sets,
they are not composed of the same genre as the
dev and test sets, resulting in a domain mismatch.
203
Therefore, we are actually comparing a smaller in-
domain tuning set with a larger out-of-domain set.
While this domain adaptation is problematic (Had-
dow and Koehn, 2012), the ability to discrimina-
tively tune on larger sets remains highly desirable.
In terms of running time, we observe that the al-
gorithm scales linearly with respect to input size,
regardless of the feature set. With more features,
running time increases due to a more complex
translation model, as well as larger intermediate
output (i.e., amount of information passed from
mappers to reducers). The scaling characteristics
point out the strength of our system: our scalable
MIRA implementation allows one to tackle learn-
ing problems where there are many parameters,
but also many training instances.
Comparing the wall clock time of paralleliza-
tion with Hadoop to the standard mode of 10?20
learner parallelization (Haddow et al, 2011; Chi-
ang et al, 2009), for the small 25k feature set-
ting, after one iteration, which takes 4625 sec-
onds using 15 learners on our PBS cluster, the tun-
ing score is 19.5 BLEU, while in approximately
the same time, we can perform five iterations
with Hadoop and obtain 30.98 BLEU. While this
is not a completely fair comparison, as the two
clusters utilize different resources and the num-
ber of learners, it suggests the practical benefits
that Hadoop can provide. Although increasing the
number of learners on our PBS cluster to the num-
ber of mappers used in Hadoop would result in
roughly equivalent performance, arbitrarily scal-
ing out learners on the PBS cluster to handle larger
training sets can be challenging since we?d have to
manually coordinate the parallel processes in an
ad-hoc manner. In contrast, Hadoop provides scal-
able parallelization in a manageable framework,
providing data distribution, synchronization, fault
tolerance, as well as other features, ?for free?.
5 Conclusion
In this paper, we presented an open-source
framework that allows seamlessly scaling struc-
tured learning to large feature-rich problems with
Hadoop, which lets us take advantage of large
amounts of data as well as sparse features. The
development of Mr. MIRA has been motivated pri-
marily by application to SMT, but we are planning
to extend our system to other structured prediction
tasks in NLP such as parsing, as well as to facili-
tate its use in other domains.
Acknowledgments
This research was supported in part by the DARPA
BOLT program, Contract No. HR0011-12-C-
0015; NSF under awards IIS-0916043 and IIS-
1144034. Vladimir Eidelman is supported by a
NDSEG Fellowship. Any opinions, findings, con-
clusions, or recommendations expressed are those
of the authors and do not necessarily reflect views
of the sponsors.
References
S. Chen and J. Goodman. 1996. An empirical study of
smoothing techniques for language modeling. In ACL.
D. Chiang, K. Knight, and W. Wang. 2009. 11,001 new fea-
tures for statistical machine translation. In NAACL-HLT.
D. Chiang. 2012. Hope and fear for discriminative training
of statistical translation models. JMLR, 13:1159?1187.
M. Collins. 2002. Ranking algorithms for named-entity ex-
traction: boosting and the voted perceptron. In ACL.
K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz, and
Y. Singer. 2006. Online passive-aggressive algorithms.
JMLR, 7:551?585.
K. Crammer, A. Kulesza, and M. Dredze. 2009. Adaptive
regularization of weight vectors. In NIPS.
J. Dean and S. Ghemawat. 2004. MapReduce: Simplified
data processing on large clusters. In OSDI.
J. Duchi, E. Hazan, and Y. Singer. 2011. Adaptive subgra-
dient methods for online learning and stochastic optimiza-
tion. JMLR, 12:2121?2159.
C. Dyer, A. Lopez, J. Ganitkevitch, J. Weese, F. Ture, P. Blun-
som, H. Setiawan, V. Eidelman, and P. Resnik. 2010.
cdec: A decoder, alignment, and learning framework for
finite-state and context-free translation models. In ACL
System Demonstrations.
S. Green, S. Wang, D. Cer, and C. Manning. 2013. Fast and
adaptive online training of feature-rich translation models.
In ACL.
B. Haddow and P. Koehn. 2012. Analysing the effect of out-
of-domain data on smt systems. In WMT.
B. Haddow, A. Arun, and P. Koehn. 2011. SampleRank
training for phrase-based machine translation. In WMT.
T. Joachims. 1998. Text categorization with support vec-
tor machines: Learning with many relevant features. In
ECML.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional
random fields: Probabilistic models for segmenting and
labeling sequence data. In ICML.
R. McDonald, K. Crammer, and F. Pereira. 2005. Online
large-margin training of dependency parsers. In ACL.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. In ACL.
L. Shen. 2007. Guided learning for bidirectional sequence
classification. In ACL.
P. Simianer, S. Riezler, and C. Dyer. 2012. Joint feature
selection in distributed stochastic learning for large-scale
discriminative training in SMT. In ACL.
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A study of translation edit rate with
targeted human annotation. In AMTA.
204
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1166?1176,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Polylingual Tree-Based Topic Models for Translation Domain Adaptation
Yuening Hu
?
Computer Science
University of Maryland
ynhu@cs.umd.edu
Ke Zhai
?
Computer Science
University of Maryland
zhaike@cs.umd.edu
Vladimir Eidelman
FiscalNote Inc.
Washington DC
vlad@fiscalnote.com
Jordan Boyd-Graber
iSchool and UMIACS
University of Maryland
jbg@umiacs.umd.edu
Abstract
Topic models, an unsupervised technique
for inferring translation domains improve
machine translation quality. However, pre-
vious work uses only the source language
and completely ignores the target language,
which can disambiguate domains. We pro-
pose new polylingual tree-based topic mod-
els to extract domain knowledge that con-
siders both source and target languages and
derive three different inference schemes.
We evaluate our model on a Chinese to En-
glish translation task and obtain up to 1.2
BLEU improvement over strong baselines.
1 Introduction
Probabilistic topic models (Blei and Lafferty,
2009), exemplified by latent Dirichlet aloca-
tion (Blei et al, 2003, LDA), are one of the most
popular statistical frameworks for navigating large
unannotated document collections. Topic models
discover?without any supervision?the primary
themes presented in a dataset: the namesake topics.
Topic models have two primary applications: to
aid human exploration of corpora (Chang et al,
2009) or serve as a low-dimensional representa-
tion for downstream applications. We focus on
the second application, which has been fruitful for
computer vision (Li Fei-Fei and Perona, 2005),
computational biology (Perina et al, 2010), and
information retrieval (Kataria et al, 2011).
In particular, we use topic models to aid statisti-
cal machine translation (Koehn, 2009, SMT). Mod-
ern machine translation systems use millions of
examples of translations to learn translation rules.
These systems work best when the training corpus
has consistent genre, register, and topic. Systems
that are robust to systematic variation in the train-
ing set are said to exhibit domain adaptation.
? indicates equal contributions.
As we review in Section 2, topic models are
a promising solution for automatically discover-
ing domains in machine translation corpora. How-
ever, past work either relies solely on monolingual
source-side models (Eidelman et al, 2012; Hasler
et al, 2012; Su et al, 2012), or limited modeling
of the target side (Xiao et al, 2012). In contrast,
machine translation uses inherently multilingual
data: an SMT system must translate a phrase or sen-
tence from a source language to a different target
language, so existing applications of topic mod-
els (Eidelman et al, 2012) are wilfully ignoring
available information on the target side that could
aid domain discovery.
This is not for a lack of multilingual topic mod-
els. Topic models bridge the chasm between lan-
guages using document connections (Mimno et
al., 2009), dictionaries (Boyd-Graber and Resnik,
2010), and word alignments (Zhao and Xing, 2006).
In Section 2, we review these models for discover-
ing topics in multilingual datasets and discuss how
they can improve SMT.
However, no models combine multiple bridges
between languages. In Section 3, we create a
model?the polylingual tree-based topic models
(ptLDA)?that uses information from both external
dictionaries and document alignments simultane-
ously. In Section 4, we derive both MCMC and
variational inference for this new topic model.
In Section 5, we evaluate our model on the task
of SMT using aligned datasets. We show that ptLDA
offers better domain adaptation than other topic
models for machine translation. Finally, in Sec-
tion 6, we show how these topic models improve
SMT with detailed examples.
2 Topic Models for Machine Translation
Before considering past approaches using topic
models to improve SMT, we briefly review lexical
weighting and domain adaptation for SMT.
1166
2.1 Statistical Machine Translation
Statistical machine translation casts machine trans-
lation as a probabilistic process (Koehn, 2009). For
a parallel corpus of aligned source and target sen-
tences (F , E), a phrase
?
f ? F is translated to a
phrase e? ? E according to a distribution p
w
(e?|
?
f).
One popular method to estimate the probability
p
w
(e?|
?
f) is via lexical weighting features.
Lexical Weighting In phrase-based SMT, lexi-
cal weighting features estimate the phrase pair
quality by combining lexical translation probabil-
ities of words in a phrase (Koehn et al, 2003).
Lexical conditional probabilities p(e|f) are maxi-
mum likelihood estimates from relative lexical fre-
quencies c(f, e)/
?
e
c(f, e) , where c(f, e) is the
count of observing lexical pair (f, e) in the train-
ing dataset. The phrase pair probabilities p
w
(e?|
?
f)
are the normalized product of lexical probabili-
ties of the aligned word pairs within that phrase
pair (Koehn et al, 2003). In Section 2.2, we create
topic-specific lexical weighting features.
Cross-Domain SMT A SMT system is usu-
ally trained on documents with the same genre
(e.g., sports, business) from a similar style (e.g.,
newswire, blog-posts). These are called domains.
Translations within one domain are better than
translations across domains since they vary dra-
matically in their word choices and style. A correct
translation in one domain may be inappropriate in
another domain. For example, ???? in a newspa-
per usually means ?underwater diving?. On social
media, it means a non-contributing ?lurker?.
Domain Adaptation for SMT Training a SMT
system using diverse data requires domain adap-
tation. Early efforts focus on building separate
models (Foster and Kuhn, 2007) and adding fea-
tures (Matsoukas et al, 2009) to model domain
information. Chiang et al (2011) combine these
approaches by directly optimizing genre and col-
lection features by computing separate translation
tables for each domain.
However, these approaches treat domains as
hand-labeled, constant, and known a priori. This
setup is at best expensive and at worst infeasible for
large data. Topic models provide a solution where
domains can be automatically induced from raw
data: treat each topic as a domain.
1
1
Henceforth we will use the term ?topic? and ?domain?
interchangeably: ?topic? to refer to the concept in topic models
and ?domain? to refer to SMT corpora.
2.2 Inducing Domains with Topic Models
Topic models take the number of topics K and a
collection of documents as input, where each docu-
ment is a bag of words. They output two distribu-
tions: a distribution over topics for each document
d; and a distribution over words for each topic. If
each topic defines a SMT domain, the document?s
topic distribution is a soft domain assignment for
that document.
Given the soft domain assignments, Eidelman et
al. (2012) extract lexical weighting features condi-
tioned on the topics, optimizing feature weights us-
ing the Margin Infused Relaxed Algorithm (Cram-
mer et al, 2006, MIRA). The topics come from
source documents only and create topic-specific
lexical weights from the per-document topic distri-
bution p(k | d). The lexical probability conditioned
on the topic is expected count e
k
(e, f) of a word
translation pair under topic k,
c?
k
(e, f) =
?
d
p(k|d)c
d
(e, f), (1)
where c
d
(?) is the number of occurrences of the
word pair in document d. The lexical probability
conditioned on topic k is the unsmoothed probabil-
ity estimate of those expected counts
p
w
(e|f ; k) =
c?
k
(e,f)?
e
c?
k
(e,f)
, (2)
from which we can compute the phrase pair proba-
bilities p
w
(e?|
?
f ; k) by multiplying the lexical prob-
abilities and normalizing as in Koehn et al (2003).
For a test document d, the document topic dis-
tribution p(k | d) is inferred based on the topics
learned from training data. The feature value of a
phrase pair (e?,
?
f) is
f
k
(e?|
?
f) = ? log
{
p
w
(e?|
?
f ; k) ? p(k|d)
}
, (3)
a combination of the topic dependent lexical weight
and the topic distribution of the document, from
which we extract the phrase. Eidelman et al (2012)
compute the resulting model score by combining
these features in a linear model with other standard
SMT features and optimizing the weights.
Conceptually, this approach is just reweighting
examples. The probability of a topic given a docu-
ment is never zero. Every translation observed in
the training set will contribute to p
k
(e|f); many of
the expected counts, however, will be less than one.
This obviates the explicit smoothing used in other
domain adaptation systems (Chiang et al, 2011).
1167
We adopt this framework in its entirety. Our
contribution are topics that capture multilingual
information and thus better capture the domains in
the parallel corpus.
2.3 Beyond Vanilla Topic Models
Eidelman et al (2012) ignore a wealth of infor-
mation that could improve topic models and help
machine translation. Namely, they only use mono-
lingual data from the source language, ignoring all
target-language data and available lexical semantic
resources between source and target languages.
Different complement each other to reduce ambi-
guity. For example, ???? in a Chinese document
can be either ?hobbyhorse? in a children?s topic,
or ?Trojan virus? in a technology topic. A short
Chinese context obscures the true topic. However,
these terms are unambiguous in English, revealing
the true topic.
While vanilla topic models (LDA) can only be
applied to monolingual data, there are a number
of topic models for parallel corpora: Zhao and
Xing (2006) assume aligned word pairs share same
topics; Mimno et al (2009) connect different lan-
guages through comparable documents. These
models take advantage of word or document align-
ment information and infer more robust topics from
the aligned dataset.
On the other hand, lexical information can in-
duce topics from multilingual corpora. For in-
stance, orthographic similarity connects words with
the same meaning in related languages (Boyd-
Graber and Blei, 2009), and dictionaries are a
more general source of information on which words
share meaning (Boyd-Graber and Resnik, 2010).
These two approaches are not mutually exclu-
sive, however; they reveal different connections
across languages. In the next section, we combine
these two approaches into a polylingual tree-based
topic model.
3 Polylingual Tree-based Topic Models
In this section, we bring existing tree-based topic
models (Boyd-Graber et al, 2007, tLDA) and
polylingual topic models (Mimno et al, 2009,
pLDA) together and create the polylingual tree-
based topic model (ptLDA) that incorporates both
word-level correlations and document-level align-
ment information.
Word-level Correlations Tree-based topic mod-
els incorporate the correlations between words by
encouraging words that appear together in a con-
cept to have similar probabilities given a topic.
These concepts can come from WordNet (Boyd-
Graber and Resnik, 2010), domain experts (An-
drzejewski et al, 2009), or user constrains (Hu et
al., 2013). When we gather concepts from bilin-
gual resources, these concepts can connect different
languages. For example, if a bilingual dictionary
defines ???? as ?computer?, we combine these
words in a concept.
We organize the vocabulary in a tree structure
based on these concepts (Figure 1): words in the
same concept share a common parent node, and
then that concept becomes one of many children of
the root node. Words that are not in any concept?
uncorrelated words?are directly connected to
the root node. We call this structure the tree prior.
When this tree serves as a prior for topic models,
words in the same concept are correlated in topics.
For example, if ???? has high probability in a
topic, so will ?computer?, since they share the same
parent node. With the tree priors, each topic is no
longer a distribution over word types, instead, it is a
distribution over paths, and each path is associated
with a word type. The same word could appear in
multiple paths, and each path represents a unique
sense of this word.
Document-level Alignments Lexical resources
connect languages and help guide the topics. How-
ever, these resources are sometimes brittle and may
not cover the whole vocabulary. Aligned document
pairs provide a more corpus-specific, flexible asso-
ciation across languages.
Polylingual topic models (Mimno et al, 2009)
assume that the aligned documents in different lan-
guages share the same topic distribution and each
language has a unique topic distribution over its
word types. This level of connection between lan-
guages is flexible: instead of requiring the exact
matching on words and sentences, only a coarse
document alignment is necessary, as long as the
documents discuss the same topics.
Combine Words and Documents We propose
polylingual tree-based topic models (ptLDA),
which connect information across different lan-
guages by incorporating both word correlation (as
in tLDA) and document alignment information (as
in pLDA). We initially assume a given tree struc-
ture, deferring the tree?s provenance to the end of
this section.
1168
Generative Process As in LDA, each word to-
ken is associated with a topic. However, tree-based
topic models introduce an additional step of select-
ing a concept in a topic responsible for generating
each word token. This is represented by a path y
d,n
through the topic?s tree.
The probability of a path in a topic depends on
the transition probabilities in a topic. Each concept
i in topic k has a distribution over its children nodes
is governed by a Dirichlet prior: pi
k,i
? Dir(?
i
).
Each path ends in a word (i.e., a leaf node) and
the probability of a path is the product of all of
the transitions between topics it traverses. Topics
have correlations over words because the Dirichlet
parameters can encode positive or negative correla-
tions (Andrzejewski et al, 2009).
With these correlated in topics in hand, the gen-
eration of documents are very similar to LDA. For
every document d, we first sample a distribution
over topics ?
d
from a Dirichlet prior Dir(?). For
every token in the documents, we first sample a
topic z
dn
from the multinomial distribution ?
d
, and
then sample a path y
dn
along the tree according to
the transition distributions specified by topic z
dn
.
Because every path y
dn
leads to a word w
dn
in lan-
guage l
dn
, we append the sampled word w
dn
to
document d
l
dn
. Aligned documents have words in
both languages; monolingual documents only have
words in a single language.
The full generative process is:
1: for topic k ? 1, ? ? ? ,K do
2: for each internal node n
i
do
3: draw a distribution pi
ki
? Dir(?
i
)
4: for document set d ? 1, ? ? ? , D do
5: draw a distribution ?
d
? Dir(?)
6: for each word in documents d do
7: choose a topic z
dn
? Mult(?
d
)
8: sample a path y
dn
with probability
?
(i,j)?y
dn
pi
z
dn
,i,j
9: y
dn
leads to word w
dn
in language l
dn
10: append token w
dn
to document d
l
dn
If we use a flat symmetric Dirichlet prior instead
of the tree prior, we recover pLDA; and if all docu-
ments are monolingual (i.e., with distinct distribu-
tions over topics ?), we recover tLDA. ptLDA con-
nects different languages on both the word level (us-
ing the word correlations) and the document level
(using the document alignments). We compare
these models? machine translation performance in
Section 5.
computer, 
market, ?
government, ??
science, ??
Dictionary: Vocabulary: English (0), Chinese (1)
computer

market ?
government
??
science
??
??scientific policy
0    scientific
0    policy
1    
1    ?
0    computer  
0    market
0    government
0    science
1    ??
1    ??
1    ??
Prior Tree:
 0  1
Figure 1: An example of constructing a prior tree
from a bilingual dictionary: word pairs with the
same meaning but in different languages are con-
cepts; we create a common parent node to group
words in a concept, and then connect to the root; un-
correlated words are connected to the root directly.
Each topic uses this tree structure as a prior.
Build Prior Tree Structures One remaining
question is the source of the word-level connections
across languages for the tree prior. We consider
two resources to build trees that correlate words
across languages. The first are a multilingual dic-
tionaries (dict), which match words with the same
meaning in different languages together. These re-
lations between words are used as the concepts in
the prior tree (Figure 1).
In addition, we extract the word alignments from
aligned sentences in a parallel corpus. The word
pairs define concepts for the prior tree (align). We
use both resources for our models (denoted as
ptLDA-dict and ptLDA-align) in our experiments
(Section 5) and show that they yield comparable
performance in SMT.
4 Inference
Inference of probabilistic models discovers the pos-
terior distribution over latent variables. For a col-
lection of D documents, each of which contains
N
d
number of words, the latent variables of ptLDA
are: transition distributions pi
ki
for every topic k
and internal node i in the prior tree structure; multi-
nomial distributions over topics ?
d
for every docu-
ment d; topic assignments z
dn
and path y
dn
for the
n
th
word w
dn
in document d. The joint distribution
of polylingual tree-based topic models is
p(w, z,y,?,pi;?, ?) =
?
k
?
i
p(pi
ki
|?
i
) (4)
?
?
d
p(?
d
|?) ?
?
d
?
n
p(z
dn
|?
d
)
?
?
d
?
n
(
p(y
dn
|z
dn
,pi)p(w
dn
|y
dn
)
)
.
Exact inference is intractable, so we turn to ap-
1169
proximate posterior inference to discover the latent
variables that best explain our data. Two widely
used approximation approaches are Markov chain
Monte Carlo (Neal, 2000, MCMC) and variational
Bayesian inference (Blei et al, 2003, VB). Both
frameworks produce good approximations of the
posterior mode (Asuncion et al, 2009). In addition,
Mimno et al (2012) propose hybrid inference that
takes advantage of parallelizable variational infer-
ence for global variables (Wolfe et al, 2008) while
enjoying the sparse, efficient updates for local vari-
ables (Neal, 1993). In the rest of this section, we
discuss all three methods in turn.
We explore multiple inference schemes because
while all of these methods optimize likelihood be-
cause they might give different results on the trans-
lation task.
4.1 Markov Chain Monte Carlo Inference
We use a collapsed Gibbs sampler for tree-based
topic models to sample the path y
dn
and topic as-
signment z
dn
for word w
dn
,
p(z
dn
= k, y
dn
= s|?z
dn
,?y
dn
,w;?,?)
? I [?(s) = w
dn
] ?
N
k|d
+?
?
k
?
(N
k
?
|d
+?)
?
?
i?j?s
N
i?j|k
+?
i?j?
j
?
(N
i?j
?
|k
+?
i?j
?
)
,
where ?(s) represents the word that path s leads
to, N
k|d
is the number of tokens assigned to topic k
in document d and N
i?j|k
is the number of times
edge i? j in the tree assigned to topic k, exclud-
ing the topic assignment z
dn
and its path y
dn
of
current token w
dn
. In practice, we sample the la-
tent variables using efficient sparse updates (Yao et
al., 2009; Hu and Boyd-Graber, 2012).
4.2 Variational Bayesian Inference
Variational Bayesian inference approximates the
posterior distribution with a simplified variational
distribution q over the latent variables: document
topic proportions ?, transition probabilities pi, topic
assignments z, and path assignments y.
Variational distributions typically assume a
mean-field distribution over these latent variables,
removing all dependencies between the latent vari-
ables. We follow this assumption for the transi-
tion probabilities q(pi |?) and the document topic
proportions q(? |?); both are variational Dirichlet
distributions. However, due to the tight coupling
between the path and topic variables, we must
model this joint distribution as one multinomial,
q(z,y |?). If word token w
dn
has K topics and
S paths, it has a K ? S length variational multino-
mial ?
dnks
, which represents the probability that
the word takes path s in topic k. The complete
variational distribution is
q(?,pi, z,y|?,?,?) =
?
d
q(?
d
|?
d
)? (5)
?
k
?
i
q(pi
ki
|?
ki
) ?
?
d
?
n
q(z
dn
, y
dn
|?
dn
).
Our goal is to find the variational distribution q
that is closest to the true posterior, as measured by
the Kullback-Leibler (KL) divergence between the
true posterior p and variational distribution q. This
induces an ?evidence lower bound? (ELBO, L) as a
function of a variational distribution q: L =
E
q
[log p(w, z,y,?,pi)]? E
q
[log q(?,pi, z,y)]
=
?
k
?
i
E
q
[log p(pi
ki
|?
i
)]
+
?
d
E
q
[log p(?
d
|?)]
+
?
d
?
n
E
q
[log p(z
dn
, y
dn
|?
d
,pi)p(w
dn
|y
dn
)]
+ H[q(?)] + H[q(pi)] + H[q(z,y)], (6)
where H[?] represents the entropy of a distribution.
Optimizing L using coordinate descent provides
the following updates:
?
dnkt
? exp{?(?
dk
)??(
?
k
?
dk
) (7)
+
?
i?j?s
(
?(?
k,i?j
)??(
?
j
?
?
k,i?j
?
)
)
};
?
dk
= ?
k
+
?
n
?
s??
?1
(w
dn
)
?
dnkt
; (8)
?
k,i?j
= ?
i?j
(9)
+
?
d
?
n
?
s??
?
(w
dn
)
?
dnkt
I [i? j ? s] ;
where ?
?
(w
dn
) is the set of all paths that lead to
wordw
dn
in the tree, and t represents one particular
path in this set. I [i? j ? s] is the indicator of
whether path s contains an edge from node i to j.
4.3 Hybrid Stochastic Inference
Given the complementary strengths of MCMC and
VB, and following hybrid inference proposed by
Mimno et al (2012), we also derive hybrid infer-
ence for ptLDA.
The transition distributions pi are treated identi-
cally as in variational inference. We posit a varia-
tional Dirichlet distribution ? and choose the one
that minimizes the KL divergence between the true
posterior and the variational distribution.
For topic z and path y, instead of variational
updates, we use a Gibbs sampler within a document.
We sample z
dn
and y
dn
conditioned on the topic
1170
and path assignments of all other document tokens,
based on the variational expectation of pi,
q(z
dn
= k, y
dn
= s|?z
dn
,?y
dn
;w) ? (10)
(?+
?
m 6=n
I [z
dm
= k])
? exp{E
q
[log p(y
dn
|z
dn
,pi)p(w
dn
|y
dn
)]}.
This equation embodies how this is a hybrid algo-
rithm: the first term resembles the Gibbs sampling
term encoding how much a document prefers a
topic, while the second term encodes the expecta-
tion under the variational distribution of how much
a path is preferred by this topic,
E
q
[log p(y
dn
|z
dn
,pi)p(w
dn
|y
dn
)] = I
[?(y
dn
)=w
dn
]
?
?
i?j?y
dn
E
q
[log ?
z
dn
,i?j
].
For every document, we sweep over all its to-
kens and resample their topic z
dn
and path y
dn
conditioned on all the other tokens? topic and path
assignments ?z
dn
and ?y
dn
. To avoid bias, we
discard the first B burn-in sweeps and take the
following M samples. We then use the empirical
average of these samples update the global varia-
tional parameter q(pi|?) based on how many times
we sampled these paths
?
k,i?j
=
1
M
?
d
?
n
?
s??
?1
(w
dn
)
(
I [i? j ? s]
? I [z
dn
= k, y
dn
= s]
)
+ ?
i?j
. (11)
For our experiments, we use the recommended set-
tingsB = 5 andM = 5 from Mimno et al (2012).
5 Experiments
We evaluate our new topic model, ptLDA, and exist-
ing topic models?LDA, pLDA, and tLDA?on their
ability to induce domains for machine translation
and the resulting performance of the translations
on standard machine translation metrics.
Dataset and SMT Pipeline We use the NIST MT
Chinese-English parallel corpus (NIST), excluding
non-UN and non-HK Hansards portions as our train-
ing dataset. It contains 1.6M sentence pairs, with
40.4M Chinese tokens and 44.4M English tokens.
We replicate the SMT pipeline of Eidelman et al
(2012): word segmentation (Tseng et al, 2005),
align (Och and Ney, 2003), and symmetrize (Koehn
et al, 2003) the data. We train a modified Kneser-
Ney trigram language model on English (Chen and
Goodman, 1996). We use CDEC (Dyer et al, 2010)
for decoding, and MIRA (Crammer et al, 2006)
for parameter training. To optimize SMT system,
we tune the parameters on NIST MT06, and report
results on three test sets: MT02, MT03 and MT05.
2
Topic Models Configuration We compare our
polylingual tree-based topic model (ptLDA) against
tree-based topic models (tLDA), polylingual topic
models (pLDA) and vanilla topic models (LDA).
3
We also examine different inference algorithms?
Gibbs sampling (gibbs), variational inference
(variational) and hybrid approach (variational-
hybrid)?on the effects of SMT performance. In
all experiments, we set the per-document Dirichlet
parameter ? = 0.01 and the number of topics to
10, as used in Eidelman et al (2012).
Resources for Prior Tree To build the tree for
tLDA and ptLDA, we extract the word correla-
tions from a Chinese-English bilingual dictio-
nary (Denisowski, 1997).
4
We filter the dictionary
using the NIST vocabulary, and keep entries map-
ping single Chinese and single English words. The
prior tree has about 1000 word pairs (dict).
We also extract the bidirectional word align-
ments between Chinese and English using
GIZA++ (Och and Ney, 2003). We then remove
the word pairs appearing more than 50K times or
fewer than 500 times and construct a second prior
tree with about 2500 word pairs (align).
We apply both trees to tLDA and ptLDA, denoted
as tLDA-dict, tLDA-align, ptLDA-dict, and ptLDA-
align. However, tLDA-align and ptLDA-align do
worse than tLDA-dict and ptLDA-dict, so we omit
tLDA-align in the results.
Domain Adaptation using Topic Models We
examine the effectiveness of using topic models
for domain adaptation on standard SMT evalua-
tion metrics?BLEU (Papineni et al, 2002) and
TER (Snover et al, 2006). We report the results
on three different test sets (Figure 2), and all SMT
results are averaged over five runs.
We refer to the SMT model without domain adap-
tation as baseline.
5
LDA marginally improves ma-
chine translation (less than half a BLEU point).
2
The NIST datasets contain 878, 919, 1082 and 1664 sen-
tences for MT02, MT03, MT05 and MT06 respectively.
3
For Gibbs sampling, we use implementations available in
Hu and Boyd-Graber (2012) for tLDA; and Mallet (McCallum,
2002) for LDA and pLDA.
4
This is a two-level tree structure. However, one could
build a more sophisticated tree prior with a hierarchical dictio-
nary such as multilingual WordNet.
5
Our replication of Eidelman et al (2012) yields slightly
higher baseline performance, but the trend is consistent.
1171
gibbs variational variational?hybrid
34.8 +0.3 +0.6 +0.4
+1.2 +0.5
35.1 +0.1 +0.3 +0.2 +0.7 +0.4
31.4 +0.4 +0.7 +0.4 +1 +0.4
34.8 +0.4 +0.5 +0.4 +0.8 +0.5
35.1
?0.1 +0.2 ?0.1 +0.2 +0.2
31.4 +0.3 +0.5 +0.3 +0.8 +0.4
34.8 +0.2 +0.4 +0.2 +0.7 +0.4
35.1
?0.1 ?0.1 ?0.1 +0.2 +0.2
31.4 +0.3 +0.3 +0.1 +0.6 +0.3
3132
3334
3536
37
3132
3334
3536
37
3132
3334
3536
37
mt02
mt03
mt05
BLE
U S
cor
e
model baseline LDA pLDA ptLDA?align ptLDA?dict tLDA?dict
gibbs variational variational?hybrid
61.9 ?0.1
?1 ?1.2
?2.5 ?1.1
60.1
?0.3
?0.9 ?0.8
?1.9 ?0.9
63.3
?0.9
?1.3 ?1.2
?2.6 ?1.1
61.9
?0.4
?1 ?0.6
?1.6 ?1.3
60.1
?0.2
?0.5 ?0.1
?1 ?0.7
63.3
?0.5
?1 ?0.4
?1.5 ?1.2
61.9
?0.3
?0.7 ?0.1
?1.6 ?0.9
60.1 0 ?0.2 +0.2
?1.1 ?0.5
63.3
?0.4
?0.7 ?0.1
?1.6 ?0.8
5658
6062
6466
5658
6062
6466
5658
6062
6466
mt02
mt03
mt05
TE
R S
cor
e
model baseline LDA pLDA ptLDA?align ptLDA?dict tLDA?dict
Figure 2: Machine translation performance for different models and inference algorithms against the
baseline, on BLEU (top, higher the better) and TER (bottom, lower the better) scores. Our proposed ptLDA
performs best. Results are averaged over 5 random runs. For model ptLDA-dict with different inference
schemes, the BLEU improvement on three test sets is mostly significant with p = 0.01, except the results
on MT03 using variational and variational-hybrid inferences.
Polylingual topic models pLDA and tree-based
topic models tLDA-dict are consistently better than
LDA, suggesting that incorporating additional bilin-
gual knowledge improves topic models. These im-
provements are not redundant: our new ptLDA-dict
model, which has aspects of both models yields the
best performance among these approaches?up to a
1.2 BLEU point gain (higher is better), and -2.6 TER
improvement (lower is better). The BLEU improve-
ment is significant (Koehn, 2004) at p = 0.01,
6
except on MT03 with variational and variational-
hybrid inference.
While ptLDA-align performs better than base-
line SMT and LDA, it is worse than ptLDA-dict,
possibly because of errors in the word alignments,
making the tree priors less effective.
Scalability While gibbs has better translation
scores than variational and variational-hybrid, it
is less scalable to larger datasets. With 1.6M NIST
6
Because we have multiple runs of each topic model (and
thus different translation models), we select the run closest to
the average BLEU for the translation significance test.
training sentences, gibbs takes nearly a week to
run 1000 iterations. In contrast, the parallelized
variational and variational-hybrid approaches,
which we implement in MapReduce (Dean and
Ghemawat, 2004; Wolfe et al, 2008; Zhai et al,
2012), take less than a day to converge.
6 Discussion
In this section, we qualitatively analyze the trans-
lation results and investigate how ptLDA and its
cousins improve SMT. We also discuss other ap-
proaches to improve unsupervised domain adapta-
tion for SMT.
6.1 How do Topic Models Help SMT?
We present two examples of how topic models can
improve SMT. The first example shows both LDA
and ptLDA improve the baseline. The second exam-
ple shows how LDA introduce biases that mislead
SMT and how ptLDA?s bilingual constraints correct
these mistakes.
Figure 3 shows a sentence about a company
1172
source ???????????
 , ????
reference
sony has already sold about 570,000 units of narrowband connection 
kits in north america at the price of about 39 us dollars and some 20 
compatible games .
baseline
LDA
ptLDA
? internet links set ...
? internet links kit ? 
? internet links kit ?  
? with about 20 of the game .
? , there are about 20 compatible games .
? , there are about 20 compatible games .
source ?  ... ? ???

LDA-Topic 0 (business)
ptLDA-Topic 0 (business)
reference
? connection kits ... ? some 20 compatible games .

	, ???

??(company), ??(China), ?(service), ?
(market), ?(technology), ?(industry), ??
(provide), (develop), ?(year), 
(product), 
?, ??(coorporate), ?, ??(manage), ?
(invest), (economy), ?(international), ?
(system), (bank)
??(company), ?(service), ?(market), ?
(technology), china, ?(industry), 

(product), market, company, technology, services, 
?(system), year, industry, products, business, 
(economy), information, ??(manage), ?
(invest), percent, ?	(internet), companies, world, 
system, ??(information), ?(increase), 
(device), service, (service)
Figure 3: Better SMT result using topic models for domain adaptation. Top row: the source sentence and
its reference translation. Middle row: the highlighted translations from different approaches. Bottom row:
the change of relevant translation probabilities after incorporating the domain knowledge from LDA and
ptLDA. Right: most-probable words of the topic the source sentence is assigned to under LDA (top) and
ptLDA (bottom). The Chinese translations are in parenthesis.
introducing new technology gadgets where both
LDA and ptLDA improve translations. The base-
line translates ???? to ?set? (red), and ???? to
?with? (blue), which do not capture the reference
meaning of a add-on device that works with com-
patible games. Both LDA and ptLDA assign this
sentence to a business domain, which makes the
translations probabilities shift toward correct trans-
lations: the probability of translating ???? to
?compatible? and the probability of translating ??
?? to ?kit? in the business domain are both signif-
icantly larger than without the domain knowledge;
and the probabilities of translating ???? to ?with?
and the probability of translating ?set? to ????
in the business domain decrease.
The second example (Figure 4) illustrates how
ptLDA offers further improvements over LDA. The
source sentence discusses foreign affairs. The
baseline correctly translates the word ???? to
?affect?. However, LDA?which only takes mono-
lingual information from the source language?
assigns this sentence to economic development.
This misleads SMT to lower the probability for
the correct translation ?affect?; it chooses ?impact?
instead. In contrast, ptLDA?which incorporates
bilingual constraints?successfully labels this sen-
tence as foreign affairs and produces a softer, more
nuanced translation that better matches the refer-
ence. The translation of ???? is very similar,
except in this case, both the baseline and LDA
produce the incorrect translation ?the commitment
of?. This is possible because the probabilities of
translating ???? to ?promised to? and translat-
ing ?promised to? to ???? (the correct transla-
tion, in both directions) increase when conditioned
on ptLDA?s correct topic but decrease when condi-
tioned on LDA?s incorrect topic.
6.2 Other Approaches
Other approaches have used topic models for ma-
chine translation. Xiao et al (2012) present a topic
similarity model based on LDA that produces a fea-
ture that weights grammar rules based on topic
compatibility. They also model the source and tar-
get side of rules and compare the target similarity
during decoding by projecting the target distribu-
tion into the source space. Hasler et al (2012)
use the source-side topic assignments from hidden
topic Markov models (Gruber et al, 2007, HTMM)
which models documents as a Markov chain and
assign one topic to the whole sentence, instead of
a mixture of topics. Su et al (2012) also apply
HTMM to monolingual data and apply the results to
machine translation. To our knowledge, however,
this is the first work to use multilingual topic mod-
els for domain adaptation in machine translation.
6.3 Improving Language Models
Topic models capture document-level properties
of language, but a critical component of machine
translation systems is the language model, which
provides local constraints and preferences. Do-
main adaptation for language models (Bellegarda,
2004; Wood and Teh, 2009) is an important avenue
for improving machine translation. Models that si-
multaneously discover global document themes as
well as local, contextual domain-specific informa-
1173
source
????, ?????????, ???????????Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 72?76,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
The University of Maryland Statistical Machine Translation System for
the Fifth Workshop on Machine Translation
Vladimir Eidelman?, Chris Dyer??, and Philip Resnik??
?UMIACS Laboratory for Computational Linguistics and Information Processing
?Department of Linguistics
University of Maryland, College Park
{vlad,redpony,resnik}@umiacs.umd.edu
Abstract
This paper describes the system we devel-
oped to improve German-English transla-
tion of News text for the shared task of
the Fifth Workshop on Statistical Machine
Translation. Working within cdec, an
open source modular framework for ma-
chine translation, we explore the benefits
of several modifications to our hierarchical
phrase-based model, including segmenta-
tion lattices, minimum Bayes Risk de-
coding, grammar extraction methods, and
varying language models. Furthermore,
we analyze decoder speed and memory
performance across our set of models and
show there is an important trade-off that
needs to be made.
1 Introduction
For the shared translation task of the Fifth Work-
shop on Machine Translation (WMT10), we par-
ticipated in German to English translation under
the constraint setting. We were especially inter-
ested in translating from German due to set of
challenges it poses for translation. Namely, Ger-
man possesses a rich inflectional morphology, pro-
ductive compounding, and significant word re-
ordering with respect to English. Therefore, we
directed our system design and experimentation
toward addressing these complications and mini-
mizing their negative impact on translation qual-
ity.
The rest of this paper is structured as follows.
After a brief description of the baseline system
in Section 2, we detail the steps taken to improve
upon it in Section 3, followed by experimental re-
sults and analysis of decoder performance metrics.
2 Baseline system
As our baseline system, we employ a hierarchical
phrase-based translation model, which is formally
based on the notion of a synchronous context-free
grammar (SCFG) (Chiang, 2007). These gram-
mars contain pairs of CFG rules with aligned non-
terminals, and by introducing these nonterminals
into the grammar, such a system is able to uti-
lize both word and phrase level reordering to cap-
ture the hierarchical structure of language. SCFG
translation models have been shown to be well
suited for German-English translation, as they are
able to both exploit lexical information for and ef-
ficiently compute all possible reorderings using a
CKY-based decoder (Dyer et al, 2009).
Our system is implemented within cdec, an ef-
ficient and modular open source framework for
aligning, training, and decoding with a num-
ber of different translation models, including
SCFGs (Dyer et al, 2010).1 cdec?s modular
framework facilitates seamless integration of a
translation model with different language models,
pruning strategies and inference algorithms. As
input, cdec expects a string, lattice, or context-free
forest, and uses it to generate a hypergraph repre-
sentation, which represents the full translation for-
est without any pruning. The forest can now be
rescored, by intersecting it with a language model
for instance, to obtain output translations. The
above capabilities of cdec allow us to perform the
experiments described below, which would other-
wise be quite cumbersome to carry out in another
system.
The set of features used in our model were the
rule translation relative frequency P (e|f), a target
n-gram language model P (e), a ?pass-through?
penalty when passing a source language word
to the target side without translating it, lexical
translation probabilities Plex(e|f) and Plex(f |e),
1http://cdec-decoder.org
72
a count of the number of times that arity-0,1, or 2
SCFG rules were used, a count of the total num-
ber of rules used, a source word penalty, a target
word penalty, the segmentation model cost, and a
count of the number of times the glue rule is used.
The number of non-terminals allowed in a syn-
chronous grammar rule was restricted to two, and
the non-terminal span limit was 12 for non-glue
grammars. The hierarchical phrase-base transla-
tion grammar was extracted using a suffix array
rule extractor (Lopez, 2007).
2.1 Data preparation
In order to extract the translation grammar nec-
essary for our model, we used the provided Eu-
roparl and News Commentary parallel training
data. The lowercased and tokenized training data
was then filtered for length and aligned using the
GIZA++ implementation of IBM Model 4 (Och
and Ney, 2003) to obtain one-to-many alignments
in both directions and symmetrized by combining
both into a single alignment using the grow-diag-
final-and method (Koehn et al, 2003). We con-
structed a 5-gram language model using the SRI
language modeling toolkit (Stolcke, 2002) from
the provided English monolingual training data
and the non-Europarl portions of the parallel data
with modified Kneser-Ney smoothing (Chen and
Goodman, 1996). Since the beginnings and ends
of sentences often display unique characteristics
that are not easily captured within the context of
the model, and have previously been demonstrated
to significantly improve performance (Dyer et al,
2009), we explicitly annotate beginning and end
of sentence markers as part of our translation
process. We used the 2525 sentences in news-
test2009 as our dev set on which we tuned the fea-
ture weights, and report results on the 2489 sen-
tences of the news-test2010 test set.
2.2 Viterbi envelope semiring training
To optimize the feature weights for our model,
we use Viterbi envelope semiring training (VEST),
which is an implementation of the minimum er-
ror rate training (MERT) algorithm (Dyer et al,
2010; Och, 2003) for training with an arbitrary
loss function. VEST reinterprets MERT within
a semiring framework, which is a useful mathe-
matical abstraction for defining two general oper-
ations, addition (?) and multiplication (?) over
a set of values. Formally, a semiring is a 5-tuple
(K,?,?, 0, 1), where addition must be commu-
nicative and associative, multiplication must be as-
sociative and must distribute over addition, and an
identity element exists for both. For VEST, hav-
ing K be the set of line segments, ? be the union
of them, and? be Minkowski addition of the lines
represented as points in the dual plane, allows us
to compute the necessary MERT line search with
the INSIDE algorithm.2 The error function we use
is BLEU (Papineni et al, 2002), and the decoder is
configured to use cube pruning (Huang and Chi-
ang, 2007) with a limit of 100 candidates at each
node. During decoding of the test set, we raise
the cube pruning limit to 1000 candidates at each
node.
2.3 Compound segmentation lattices
To deal with the aforementioned problem in Ger-
man of productive compounding, where words
are formed by the concatenation of several mor-
phemes and the orthography does not delineate the
morpheme boundaries, we utilize word segmen-
tation lattices. These lattices serve to encode al-
ternative ways of segmenting compound words,
and as such, when presented as the input to the
system allow the decoder to automatically choose
which segmentation is best for translation, leading
to markedly improved results (Dyer, 2009).
In order to construct diverse and accurate seg-
mentation lattices, we built a maximum entropy
model of compound word splitting which makes
use of a small number of dense features, such
as frequency of hypothesized morphemes as sep-
arate units in a monolingual corpus, number of
predicted morphemes, and number of letters in
a predicted morpheme. The feature weights are
tuned to maximize conditional log-likelihood us-
ing a small amount of manually created reference
lattices which encode linguistically plausible seg-
mentations for a selected set of compound words.3
To create lattices for the dev and test sets, a lat-
tice consisting of all possible segmentations for
every word consisting of more than 6 letters was
created, and the paths were weighted by the pos-
terior probability assigned by the segmentation
model. Then, max-marginals were computed us-
ing the forward-backward algorithm and used to
prune out paths that were greater than a factor of
2.3 from the best path, as recommended by Dyer
2This algorithm is equivalent to the hypergraph MERT al-
gorithm described by Kumar et al (2009).
3The reference segmentation lattices used for training are
available in the cdec distribution.
73
(2009).4 To create the translation model for lattice
input, we segmented the training data using the
1-best segmentation predicted by the segmenta-
tion model, and word aligned this with the English
side. This version of the parallel corpus was con-
catenated with the original training parallel cor-
pus.
3 Experimental variation
This section describes the experiments we per-
formed in attempting to assess the challenges
posed by current methods and our exploration of
new ones.
3.1 Bloom filter language model
Language models play a crucial role in transla-
tion performance, both in terms of quality, and in
terms of practical aspects such as decoder memory
usage and speed. Unfortunately, these two con-
cerns tend to trade-off one another, as increasing
to a higher-order more complex language model
improves performance, but comes at the cost of
increased size and difficulty in deployment. Ide-
ally, the language model will be loaded into mem-
ory locally by the decoder, but given memory con-
straints, it is entirely possible that the only option
is to resort to a remote language model server that
needs to be queried, thus introducing significant
decoding speed delays.
One possible alternative is a randomized lan-
guage model (RandLM) (Talbot and Osborne,
2007). Using Bloom filters, which are a ran-
domized data structure for set representation, we
can construct language models which signifi-
cantly decrease space requirements, thus becom-
ing amenable to being stored locally in memory,
while only introducing a quantifiable number of
false positives. In order to assess what the im-
pact on translation quality would be, we trained
a system identical to the one described above, ex-
cept using a RandLM. Conveniently, it is possi-
ble to construct a RandLM directly from an exist-
ing SRILM, which is the route we followed in us-
ing the SRILM described in Section 2.1 to create
our RandLM.5 Table 1 shows the comparison of
SRILM and RandLM with respect to performance
on BLEU and TER (Snover et al, 2006) on the test
set.
4While normally the forward-backward algorithm com-
putes sum-marginals, by changing the addition operator to
max, we can obtain max-marginals.
5Default settings were used for constructing the RandLM.
Language Model BLEU TER
RandLM 22.4 69.1
SRILM 23.1 68.0
Table 1: Impact of language model on translation
3.2 Minimum Bayes risk decoding
During minimum error rate training, the decoder
employs a maximum derivation decision rule.
However, upon exploration of alternative strate-
gies, we have found benefits to using a mini-
mum risk decision rule (Kumar and Byrne, 2004),
wherein we want the translation E of the input F
that has the least expected loss, again as measured
by some loss function L:
E? = argmin
E?
EP (E|F )[L(E,E
?)]
= argmin
E?
?
E
P (E|F )L(E,E?)
Using our system, we generate a unique 500-
best list of translations to approximate the poste-
rior distribution P (E|F ) and the set of possible
translations. Assuming H(E,F ) is the weight of
the decoder?s current path, this can be written as:
P (E|F ) ? exp?H(E,F )
where ? is a free parameter which depends on
the models feature functions and weights as well
as pruning method employed, and thus needs to
be separately empirically optimized on a held out
development set. For this submission, we used
? = 0.5 and BLEU as the loss function. Table 2
shows the results on the test set for MBR decod-
ing.
Language Model Decoder BLEU TER
RandLM
Max-D 22.4 69.1
MBR 22.7 68.8
SRILM
Max-D 23.1 68.0
MBR 23.4 67.7
Table 2: Comparison of maximum derivation ver-
sus MBR decoding
3.3 Grammar extraction
Although the grammars employed in a SCFG
model allow increased expressivity and translation
quality, they do so at the cost of having a large
74
Language Model Grammar Decoder Memory (GB) Decoder time (Sec/Sentence)
Local SRILM corpus 14.293 ? 1.228 5.254 ? 3.768
Local SRILM sentence 10.964 ? .964 5.517 ? 3.884
Remote SRILM corpus 3.771 ? .235 15.252 ? 10.878
Remote SRILM sentence .443 ? .235 14.751 ? 10.370
RandLM corpus 7.901 ? .721 9.398 ? 6.965
RandLM sentence 4.612 ? .699 9.561 ? 7.149
Table 3: Decoding memory and speed requirements for language model and grammar extraction varia-
tions
number of rules, thus efficiently storing and ac-
cessing grammar rules can become a major prob-
lem. Since a grammar consists of the set of rules
extracted from a parallel corpus containing tens of
millions of words, the resulting number of rules
can be in the millions. Besides storing the whole
grammar locally in memory, other approaches
have been developed, such as suffix arrays, which
lookup and extract rules on the fly from the phrase
table (Lopez, 2007). Thus, the memory require-
ments for decoding have either been for the gram-
mar, when extracted beforehand, or the corpus, for
suffix arrays. In cdec, however, loading grammars
for single sentences from a disk is very fast relative
to decoding time, thus we explore the additional
possibility of having sentence-specific grammars
extracted and loaded on an as-needed basis by the
decoder. This strategy is shown to massively re-
duce the memory footprint of the decoder, while
having no observable impact on decoding speed,
introducing the possibility of more computational
resources for translation. Thus, in addition to the
large corpus grammar extracted in Section 2.1,
we extract sentence-specific grammars for each of
the test sentences. We measure the performance
across using both grammar extraction mechanisms
and the three different language model configu-
rations: local SRILM, remote SRILM, and Ran-
dLM.
As Table 3 shows, there is a marked trade-
off between memory usage and decoding speed.
Using a local SRILM regardless of grammar in-
creases decoding speed by a factor of 3 compared
to the remote SRILM, and approximately a fac-
tor of 2 against the RandLM. However, this speed
comes at the cost of its memory footprint. With a
corpus grammar, the memory footprint of the lo-
cal SRILM is twice as large as the RandLM, and
almost 4 times as large as the remote SRILM. Us-
ing sentence-specific grammars, the difference be-
comes increasingly glaring, as the remote SRILM
memory footprint drops to ?450MB, a factor of
nearly 24 compared to the local SRILM and a fac-
tor of 10 compared to the process size with the
RandLM. Thus, using the remote SRILM reduces
the memory footprint substantially but at the cost
of significantly slower decoding speed, and con-
versely, using the local SRILM produces increased
decoder speed but introduces a substantial mem-
ory overhead. The RandLM provides a median
between the two extremes: reduced memory and
(relatively) fast decoding at the price of somewhat
decreased translation quality. Since we are using
a relatively large beam of 1000 candidates for de-
coding, the time presented in Table 3 does not rep-
resent an accurate basis for comparison of cdec to
other decoders, which should be done using the
results presented in Dyer et al (2010).
We also tried one other grammar extraction
configuration, which was with so-called ?loose?
phrase extraction heuristics, which permit un-
aligned words at the edges of phrases (Ayan and
Dorr, 2006). When decoded using the SRILM and
MBR, this achieved the best performance for our
system, with a BLEU score of 23.6 and TER of
67.7.
4 Conclusion
We presented the University of Maryland hier-
archical phrase-based system for the WMT2010
shared translation task. Using cdec, we experi-
mented with a number of methods that are shown
above to lead to improved German-to-English
translation quality over our baseline according to
BLEU and TER evaluation. These include methods
to directly address German morphological com-
plexity, such as appropriate feature functions, seg-
mentation lattices, and a model for automatically
constructing the lattices, as well as alternative de-
coding strategies, such asMBR.We also presented
75
several language model configuration alternatives,
as well as grammar extraction methods, and em-
phasized the trade-off that must be made between
decoding time, memory overhead, and translation
quality in current statistical machine translation
systems.
5 Acknowledgments
The authors gratefully acknowledge partial sup-
port from the GALE program of the Defense Ad-
vanced Research Projects Agency, Contract No.
HR0011-06-2-001 and NSF award IIS0838801.
Any opinions, findings, conclusions or recommen-
dations expressed in this paper are those of the au-
thors and do not necessarily reflect the view of the
sponsors.
References
Necip Fazil Ayan and Bonnie J. Dorr. 2006. Going
beyond AER: An extensive analysis of word align-
ments and their impact on MT. In Proceedings
of the Joint Conference of the International Com-
mittee on Computational Linguistics and the As-
sociation for Computational Linguistics (COLING-
ACL?2006), pages 9?16, Sydney.
Stanley F. Chen and Joshua Goodman. 1996. An em-
pirical study of smoothing techniques for language
modeling. In Proceedings of the 34th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 310?318.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. In Computational Linguistics, volume 33(2),
pages 201?228.
Chris Dyer, Hendra Setiawan, Yuval Marton, and
P. Resnik. 2009. The University of Maryland sta-
tistical machine translation system for the Fourth
Workshop on Machine Translation. In Proceedings
of the EACL-2009 Workshop on Statistical Machine
Translation.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Jonathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Seti-
awan, Vladimir Eidelman, and Philip Resnik. 2010.
cdec: A decoder, alignment, and learning framework
for finite-state and context-free translation models.
In Proceedings of ACL System Demonstrations.
Chris Dyer. 2009. Using a maximum entropy model to
build segmentation lattices for mt. In Proceedings
of NAACL-HLT.
Liang Huang and David Chiang. 2007. Forest rescor-
ing: Faster decoding with integrated language mod-
els. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
144?151.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
NAACL ?03: Proceedings of the 2003 Conference
of the North American Chapter of the Association
for Computational Linguistics on Human Language
Technology, pages 48?54.
Shankar Kumar and William Byrne. 2004. Minimum
bayes-risk decoding for statistical machine transla-
tion. In HLT-NAACL 2004: Main Proceedings.
Shankar Kumar, Wolfgang Macherey, Chris Dyer, and
Franz Och. 2009. Efficient minimum error rate
training and minimum bayes-risk decoding for trans-
lation hypergraphs and lattices. In Proceedings of
the Joint Conference of the 47th Annual Meeting of
the ACL and the 4th International Joint Conference
on Natural Language Processing of the AFNLP,
pages 163?171.
Adam Lopez. 2007. Hierarchical phrase-based trans-
lation with suffix arrays. In Proceedings of EMNLP,
pages 976?985.
Franz Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
In Computational Linguistics, volume 29(21), pages
19?51.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Annual Meeting of the Association for Com-
putational Linguistics, pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311?318.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study
of translation edit rate with targeted human annota-
tion. In In Proceedings of Association for Machine
Translation in the Americas, pages 223?231.
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In Intl. Conf. on Spoken
Language Processing.
David Talbot and Miles Osborne. 2007. Randomised
language modelling for statistical machine transla-
tion. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, June.
76
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 344?350,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
Noisy SMS Machine Translation in Low-Density Languages
Vladimir Eidelman?, Kristy Hollingshead?, and Philip Resnik??
?UMIACS Laboratory for Computational Linguistics and Information Processing
?Department of Linguistics
University of Maryland, College Park
{vlad,hollingk,resnik}@umiacs.umd.edu
Abstract
This paper presents the system we developed
for the 2011 WMT Haitian Creole?English
SMS featured translation task. Applying stan-
dard statistical machine translation methods to
noisy real-world SMS data in a low-density
language setting such as Haitian Creole poses
a unique set of challenges, which we attempt
to address in this work. Along with techniques
to better exploit the limited available train-
ing data, we explore the benefits of several
methods for alleviating the additional noise
inherent in the SMS and transforming it to
better suite the assumptions of our hierarchi-
cal phrase-based model system. We show
that these methods lead to significant improve-
ments in BLEU score over the baseline.
1 Introduction
For the featured translation task of the Sixth Work-
shop on Statistical Machine Translation, we devel-
oped a system for translating Haitian Creole Emer-
gency SMS messages. Given the nature of the task,
translating text messages that were sent during the
January 2010 earthquake in Haiti to an emergency
response service called Mission 4636, we were not
only faced with the problem of dealing with a low-
density language, but additionally, with noisy, real-
world data in a domain which has thus far received
relatively little attention in statistical machine trans-
lation. We were especially interested in this task be-
cause of the unique set of challenges that it poses
for existing translation systems. We focused our re-
search effort on techniques to better utilize the lim-
ited available training resources, as well as ways in
which we could automatically alleviate and trans-
form the noisy data to our advantage through the
use of automatic punctuation prediction, finite-state
raw-to-clean transduction, and grammar extraction.
All these techniques contributed to improving trans-
lation quality as measured by BLEU score over our
baseline system.
The rest of this paper is structured as follows.
First, we provide a brief overview of our baseline
system in Section 2, followed by an examination of
issues posed by this task and the steps we have taken
to address them in Section 3, and finally we con-
clude with experimental results and additional anal-
ysis.
2 System Overview
Our baseline system is based on a hierarchical
phrase-based translation model, which can formally
be described as a synchronous context-free gram-
mar (SCFG) (Chiang, 2007). Our system is imple-
mented in cdec, an open source framework for align-
ing, training, and decoding with a number of differ-
ent translation models, including SCFGs. (Dyer et
al., 2010). 1 SCFG grammars contain pairs of CFG
rules with aligned nonterminals, where by introduc-
ing these nonterminals into the grammar, such a sys-
tem is able to utilize both word and phrase level re-
ordering to capture the hierarchical structure of lan-
guage. SCFG translation models have been shown
to produce state-of-the-art translation for most lan-
guage pairs, as they are capable of both exploit-
ing lexical information for and efficiently comput-
ing all possible reorderings using a CKY-based de-
coder (Dyer et al, 2009).
1http://cdec-decoder.org
344
One benefit of cdec is the flexibility allowed with
regard to the input format, as it expects either a
string, lattice, or context-free forest, and subse-
quently generates a hypergraph representing the full
translation forest without any pruning. This forest
can now be rescored, by intersecting it with a lan-
guage model for instance, to obtain output transla-
tions. These capabilities of cdec allow us to perform
the experiments described below, which may have
otherwise proven to be quite impractical to carry out
in another system.
The set of features used in our model were the
rule translation relative frequency P (e|f), a target
n-gram language model P (e), lexical translation
probabilities Plex(e|f) and Plex(f |e), a count of the
total number of rules used, a target word penalty,
and a count of the number of times the glue rule
is used. The number of non-terminals allowed in
a synchronous grammar rule was restricted to two,
and the non-terminal span limit was 12 for non-glue
grammars. The hierarchical phrase-based transla-
tion grammar was extracted using a suffix array rule
extractor (Lopez, 2007).
To optimize the feature weights for our model, we
used an implementation of the hypergraph minimum
error rate training (MERT) algorithm (Dyer et al,
2010; Och, 2003) for training with an arbitrary loss
function. The error function we used was BLEU (Pa-
pineni et al, 2002), and the decoder was configured
to use cube pruning (Huang and Chiang, 2007) with
a limit of 100 candidates at each node.
2.1 Data Preparation
The SMS messages were originally translated by
English speaking volunteers for the purpose of pro-
viding first responders with information and loca-
tions requiring their assistance. As such, in order to
create a suitable parallel training corpus from which
to extract a translation grammar, a number of steps
had to be taken in addition to lowercasing and tok-
enizing both sides of training data. Many of the En-
glish translations had additional notes sections that
were added by the translator to the messages with
either personal notes or further informative remarks.
As these sections do not correspond to any text on
the source side, and would therefore degrade the
alignment process, these had to be identified and re-
moved. Furthermore, the anonymization of the data
resulted in tokens such as firstname and phonenum-
ber which were prevalent and had to be preserved
as they were. Since the total amount of Haitian-
English parallel data provided is quite limited, we
found additional data and augmented the available
set with data gathered by the CrisisCommons group
and made it available to other WMT participants.
The combined training corpus from which we ex-
tracted our grammar consisted of 123,609 sentence
pairs, which was then filtered for length and aligned
using the GIZA++ implementation of IBM Model
4 (Och and Ney, 2003) to obtain one-to-many align-
ments in either direction and symmetrized using the
grow-diag-final-and method (Koehn et al, 2003).
We trained a 5-gram language model using the
SRI language modeling toolkit (Stolcke, 2002) from
the English monolingual News Commentary and
News Crawl language modeling training data pro-
vided for the shared task and the English portion of
the parallel data with modified Kneser-Ney smooth-
ing (Chen and Goodman, 1996). We have previ-
ously found that since the beginnings and ends of
sentences often display unique characteristics that
are not easily captured within the context of the
model, explicitly annotating beginning and end of
sentence markers as part of our translation process
leads to significantly improved performance (Dyer
et al, 2009).
A further difficulty of the task stems from the fact
that there are two versions of the SMS test set, a raw
version, which contains the original messages, and a
clean version which was post-edited by humans. As
the evaluation of the task will consist of translating
these two versions of the test set, our baseline sys-
tem consisted of two systems, one built on the clean
data using the 900 sentences in SMS dev clean to
tune our feature weights, and evaluated using SMS
devtest clean, and one built analogously for the raw
data tuned on the 900 sentences in SMS dev raw and
evaluated on SMS devtest raw. We report results on
these sets as well as the 1274 sentences in the SMS
test set.
3 Experimental Variation
The results produced by the baseline systems are
presented in Table 1. As can be seen, the clean ver-
sion performs on par with the French-English trans-
345
BASELINE
Version Set BLEU TER
clean
dev 30.36 56.04
devtest 28.15 57.45
test 27.97 59.19
raw
dev 25.62 63.27
devtest 24.09 63.82
test 23.33 65.93
Table 1: Baseline system BLEU and TER scores
lation quality in the 2011 WMT shared translation
task,2 and significantly outperforms the raw version,
despite the content of the messages being identical.
This serves to underscore the importance of proper
post-processing of the raw data in order to attempt to
close the performance gap between the two versions.
Through analysis of the raw and clean data we iden-
tified several factors which we believe greatly con-
tribute to the difference in translation output. We
examine punctuation in Section 3.2, grammar post-
processing in Section 3.3, and morphological differ-
ences in Sections 3.4 and 3.5.
3.1 Automatic Resource Confidence Weighting
A practical technique when working with a low-
density language with limited resources is to du-
plicate the same trusted resource multiple times in
the parallel training corpus in order for the transla-
tion probabilities of the duplicated items to be aug-
mented. For instance, if we have confidence in the
entries of the glossary and dictionary, we can dupli-
cate them 10 times in our training data to increase
the associated probabilities. The aim of this strat-
egy is to take advantage of the limited resources and
exploit the reliable ones.
However, what happens if some resources are
more reliable than others? Looking at the provided
resources, we saw that in the Haitisurf dictionary,
the entry for paske is matched with for, while in
glossary-all-fix, paske is matched with because. If
we then consider the training data, we see that in
most cases, paske is in fact translated as because.
Motivated by this type of phenomenon, we em-
ployed an alternative strategy to simple duplication
which allows us to further exploit our prior knowl-
edge.
2http://matrix.statmt.org/matrix
First, we take the previously word-aligned base-
line training corpus and for each sentence pair and
word ei compute the alignment link count c(ei, fj)
over the positions j that ei is aligned with, repeating
for c(fi, ej) in the other direction. Then, we pro-
cess each resource we are considering duplicating,
and augment its score by c(ei, fj) for every pair of
words which was observed in the training data and
is present in the resource. This score is then normal-
ized by the size of the resource, and averaged over
both directions. The outcome of this process is a
score for each resource. Taking these scores on a
log scale and pinning the top score to associate with
20 duplications, the result is a decreasing number of
duplications for each subsequent resources, based on
our confidence in its entries. Thus, every entry in the
resource receives credit, as long as there is evidence
that the entries we have observed are reliable. On
our set of resources, the process produces a score of
17 for the Haitisurf dictionary and 183 for the glos-
sary, which is in line with what we would expect.
It may be that the resources may have entries which
occur in the test set but not in the training data, and
thus we may inadvertently skew our distribution in
a way which negatively impacts our performance,
however, overall we believe it is a sound assumption
that we should bias ourselves toward the more com-
mon occurrences based on the training data, as this
should provide us with a higher translation probabil-
ity from the good resources since the entries are re-
peated more often. Once we obtain a proper weight-
ing scheme for the resources, we construct a new
training corpus, and proceed forward from the align-
ment process.
Table 2 presents the BLEU and TER results of the
standard strategy of duplication against the confi-
dence weighting scheme outlined above. As can be
CONF. WT. X10
Version Set BLEU TER BLEU TER
clean
dev 30.79 55.71 30.61 55.31
devtest 27.92 57.66 28.22 57.06
test 27.97 59.65 27.74 59.34
raw
dev 26.11 62.64 25.72 62.99
devtest 24.16 63.71 24.18 63.71
test 23.66 65.69 23.06 66.78
Table 2: Confidence weighting versus x10 duplication
346
seen, the confidence weighting scheme substantially
outperforms the duplication for the dev set of both
versions, but these improvements do not carry over
to the clean devtest set. Therefore, for the rest of the
experiments presented in the paper, we will use the
confidence weighting scheme for the raw version,
and the standard duplication for the clean version.
3.2 Automatic Punctuation Prediction
Punctuation does not usually cause a problem in
text-based machine translation, but this changes
when venturing into the domain of SMS. Punctua-
tion is very informative to the translation process,
providing essential contextual information, much
as the aforementioned sentence boundary markers.
When this information is lacking, mistakes which
would have otherwise been avoided can be made.
Examining the data, we see there is substantially
more punctuation in the clean set than in the raw.
For example, there are 50% more comma?s in the
clean dev set than in the raw. A problem of lack of
punctuation has been studied in the context of spo-
ken language translation, where punctuation predic-
tion on the source language prior to translation has
been shown to improve performance (Dyer, 2007).
We take an analogous approach here, and train a hid-
den 5-gram model using SRILM on the punctuated
portion of the Haitian side of the parallel data. We
then applied the model to punctuate the raw dev set,
and tuned a system on this punctuated set. How-
ever, the translation performance did not improve.
This may have been do to several factors, including
the limited size of the training set, and the lack of
in-domain punctuated training data. Thus, we ap-
plied a self-training approach. We applied the punc-
tuation model to the SMS training data, which is
only available in the raw format. Once punctuated,
we re-trained our punctuation prediction model, now
including the automatically punctuated SMS data
AUTO-PUNC
Version Set BLEU TER
raw
dev 26.09 62.84
devtest 24.38 64.26
test 23.59 65.91
Table 3: Automatic punctuation prediction results
as part of the punctuation language model training
data. We use this second punctuation prediction
model to predict punctuation for the tuning and eval-
uation sets. We continue by creating a new parallel
training corpus which substitutes the original SMS
training data with the punctuated version, and build
a new translation system from it. The results from
using the self-trained punctuation method are pre-
sented in Table 3. Future experiments on the raw
version are performed using this punctuation.
3.3 Grammar Filtering
Although the grammars of a SCFG model per-
mit high-quality translation, the grammar extraction
procedure extracts many rules which are formally li-
censed by the model, but are otherwise incapable of
helping us produce a good translation. For example,
in this task we know that the token firstname must al-
ways translate as firstname, and never as phonenum-
ber. This refreshing lack of ambiguity allows us to
filter the grammar after extracting it from the train-
ing corpus, removing any grammar rule where these
conditions are not met, prior to decoding. Filtering
removed approximately 5% of the grammar rules.3
Table 4 shows the results of applying grammar fil-
tering to the raw and clean version.
GRAMMAR
Version Set BLEU TER
clean
dev 30.88 54.53
devtest 28.69 56.21
test 28.29 58.78
raw
dev 26.41 62.47
devtest 24.47 63.26
test 23.96 65.82
Table 4: Results of filtering the grammar in a post-
processing step before decoding
3.4 Raw-Clean Segmentation Lattice
As noted above, a major cause of the performance
degradation from the clean to the raw version is re-
lated to the morphological errors in the messages.
Figure 1 presents a segmentation lattice with two
versions of the same sentence; the first being from
3We experimented with more aggressive filtering based
on punctuation and numbers, but translation quality degraded
rapidly.
347
the raw version, and the second from the clean. We
can see that that Ilavach has been broken into two
segments, while ki sou has been combined into one.
Since we do not necessarily know in advance
which segmentation is the correct one for a better
quality translation, it may be of use to be able to
utilize both segmentations and allow the decoder to
learn the appropriate one. In previous work, word
segmentation lattices have been used to address the
problem of productive compounding in morphologi-
cally rich languages, such as German, where mor-
phemes are combined to make words but the or-
thography does not delineate the morpheme bound-
aries. These lattices encode alternative ways of seg-
menting compound words, and allow the decoder
to automatically choose which segmentation is best
for translation, leading to significantly improved re-
sults (Dyer, 2009). As opposed to building word
segmentation lattices from a linguistic morphologi-
cal analysis of a compound word, we propose to uti-
lize the lattice to encode all alternative ways of seg-
menting a word as presented to us in either the clean
or raw versions of a sentence. As the task requires
us to produce separate clean and raw output on the
test set, we tune one system on a lattice built from
the clean and raw dev set, and use the single system
to decode both the clean and raw test set separately.
Table 5 presents the results of using segmentation
lattices.
3.5 Raw-to-Clean Transformation Lattice
As can be seen in Tables 1, 2, and 3, system per-
formance on clean text greatly outperforms system
performance on raw text, with a difference of almost
5 BLEU points. Thus, we explored the possibility of
automatically transforming raw text into clean text,
based on the ?parallel? raw and clean texts that were
provided as part of the task.
One standard approach might have been to train
SEG-LATTICE
Version Set BLEU TER
raw
dev 26.17 61.88
devtest 24.64 62.53
test 23.89 65.27
Table 5: Raw-Clean segmentation lattice tuning results
FST-LATTICE
Version Set BLEU TER
raw
dev 26.20 62.15
devtest 24.21 63.45
test 22.56 67.79
Table 6: Raw-to-clean transformation lattice results
a Haitian-to-Haitian MT system to ?translate? from
raw text to clean text. However, since the training set
was only available as raw text, and only the dev and
devtest datasets had been cleaned, we clearly did not
have enough data to train a raw-to-clean translation
system. Thus, we created a finite-state transducer
(FST) by aligning the raw dev text to the clean dev
text, on a sentence-by-sentence basis. These raw-to-
clean alignments were created using a simple mini-
mum edit distance algorithm; substitution costs were
calculated according to orthographic match.
One option would be to use the resulting raw-to-
clean transducer to greedily replace each word (or
phrase) in the raw input with the predicted transfor-
mation into clean text. However, such a destructive
replacement method could easily introduce cascad-
ing errors by removing text that might have been
translated correctly. Fortunately, as mentioned in
Section 2, and utilized in the previous section, the
cdec decoder accepts lattices as input. Rather than
replacing raw text with the predicted transformation
into ?clean? text, we add a path to the input lat-
tice for each possible transform, for each word and
phrase in the input. We tune a system on a lattice
built from this approach on the dev set, and use the
FST developed from the dev set in order to create
lattices for decoding the devtest and test sets. An
example is shown in Figure 3.4. Note that in this
example, the transformation technique correctly in-
serted new paths for ilavach and ki sou, correctly
retained the single path for zile, but overgenerated
many (incorrect) options for nan. Note, though, that
the original path for nan remains in the lattice, de-
laying the ambiguity resolution until later in the de-
coding process. Results from creating raw-to-clean
transformation lattices are presented in Table 6.
By comparing the results in Table 6 to those in
Table 5, we can see that the noise introduced by the
finite-state transformation process outweighed the
348
1 2Eske?ske 3
nou ap kite nou mouri nan zile
4ila
5Ilavach
vach
6la 7
kisou
9
ki 8okay 10lasou
Figure 1: Partial segmentation lattice combining the raw and clean versions of the sentence:
Are you going to let us die on Ile a` Vaches which is located close the city of Les Cayes.
15
16
a
m
pa
te
l
17
an
nan
lan
nan
18
nanak
19
zile
20
tant
21
ila
22ilavach
nan vach
23
e
24
sa
la
lanan 25
ki
26kisou
sou
Figure 2: Partial input lattice for sentence in Figure 3.4, generated using the raw-to-clean transform technique
described in Section 3.5.
gains of adding new phrases for tuning.
4 System Comparison
Table 7 shows the performance on the devtest set
of each of the system variations that we have pre-
sented in this paper. From this table, we can see
that our best-performing system on clean data was
the GRAMMAR system, where the training data was
multiplied by ten as described in Section 3.1, then
the grammar was filtered as described in Section 3.3.
Our performance on clean test data, using this sys-
tem, was 28.29 BLEU and 58.78 TER. Table 7 also
demonstrates that our best-performing system on
raw data was the SEG-LATTICE system, where the
training data was confidence-weighted (Section 3.1),
the grammar was filtered (Section 3.3), punctuation
was automatically added to the raw data as described
in Section 3.2, and the system was tuned on a lattice
created from the raw and clean dev dataset. Our per-
formance on raw test data, using this system, was
23.89 BLEU and 65.27 TER.
5 Conclusion
In this paper we presented our system for the 2011
WMT featured Haitian Creole?English translation
task. In order to improve translation quality of low-
density noisy SMS data, we experimented with a
number of methods that improve performance on
both the clean and raw versions of the data, and help
clean raw
System BLEU TER BLEU TER
BASELINE 28.15 57.45 24.09 63.82
CONF. WT. 27.92 57.66 24.16 63.71
X10 28.22 57.06 24.18 63.71
GRAMMAR 28.69 56.21 24.47 63.26
AUTO-PUNC ? ? 24.38 64.26
SEG-LATTICE ? ? 24.64 62.53
FST-LATTICE ? ? 24.21 63.45
Table 7: Comparison of all systems? performance on
devtest set
close the gap between the post-edited and real-world
data according to BLEU and TER evaluation. The
methods employed were developed to specifically
address shortcomings we observed in the data, such
as segmentation lattices for morphological ambigu-
ity, confidence weighting for resource utilization,
and punctuation prediction for lack thereof. Overall,
this work emphasizes the feasibility of adapting ex-
isting translation technology to as-yet underexplored
domains, as well as the shortcomings that need to be
addressed in future work in real-world data.
6 Acknowledgments
The authors gratefully acknowledge partial support
from the DARPA GALE program, No. HR0011-06-
2-001. In addition, the first author was supported by
the NDSEG Fellowship. Any opinions or findings
do not necessarily reflect the view of the sponsors.
349
References
Stanley F. Chen and Joshua Goodman. 1996. An empir-
ical study of smoothing techniques for language mod-
eling. In Proceedings of the 34th Annual Meeting of
the Association for Computational Linguistics, pages
310?318.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. In Computational Linguistics, volume 33(2),
pages 201?228.
Chris Dyer, Hendra Setiawan, Yuval Marton, and Philip
Resnik. 2009. The University of Maryland statistical
machine translation system for the Fourth Workshop
on Machine Translation. In Proceedings of the EACL-
2009 Workshop on Statistical Machine Translation.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Jonathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Setiawan,
Vladimir Eidelman, and Philip Resnik. 2010. cdec: A
decoder, alignment, and learning framework for finite-
state and context-free translation models. In Proceed-
ings of ACL System Demonstrations.
Chris Dyer. 2007. The University of Maryland Trans-
lation system for IWSLT 2007. In Proceedings of
IWSLT.
Chris Dyer. 2009. Using a maximum entropy model to
build segmentation lattices for MT. In Proceedings of
NAACL-HLT.
Liang Huang and David Chiang. 2007. Forest rescoring:
Faster decoding with integrated language models. In
Proceedings of the 45th Annual Meeting of the Associ-
ation of Computational Linguistics, pages 144?151.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In NAACL
?03: Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology,
pages 48?54.
Adam Lopez. 2007. Hierarchical phrase-based trans-
lation with suffix arrays. In Proceedings of EMNLP,
pages 976?985.
Franz Och and Hermann Ney. 2003. A systematic com-
parison of various statistical alignment models. In
Computational Linguistics, volume 29(21), pages 19?
51.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting of the Association for Computa-
tional Linguistics, pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of 40th
Annual Meeting of the Association for Computational
Linguistics, pages 311?318.
Andreas Stolcke. 2002. SRILM - an extensible language
modeling toolkit. In Intl. Conf. on Spoken Language
Processing.
350
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 399?404,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
The Value of Monolingual Crowdsourcing in a Real-World Translation
Scenario: Simulation using Haitian Creole Emergency SMS Messages
Chang Hu?, Philip Resnik??, Yakov Kronrod?
Vladimir Eidelman?, Olivia Buzek??, Benjamin B. Bederson?
?UMIACS and Department of Linguistics
?UMIACS and Department of Computer Science
University of Maryland, College Park
{changhu,bederson}@cs.umd.edu
{resnik,vlad,buzek}@umiacs.umd.edu
yakov@umd.edu
Abstract
MonoTrans2 is a translation system that com-
bines machine translation (MT) with human
computation using two crowds of monolin-
gual source (Haitian Creole) and target (En-
glish) speakers. We report on its use in the
WMT 2011 Haitian Creole to English trans-
lation task, showing that MonoTrans2 trans-
lated 38% of the sentences well compared to
Google Translate?s 25%.
1 Introduction
One of the most remarkable success stories to come
out of the January 2010 earthquake in Haiti in-
volved translation (Munro, 2010). While other
forms of emergency response and communication
channels were failing, text messages were still get-
ting through, so a number of people came together to
create a free phone number for emergency text mes-
sages, which allowed earthquake victims to report
those who were trapped or in need of medical atten-
tion. The problem, of course, was that most people
were texting in Haitian Creole (Kreyol), a language
not many of the emergency responders understood,
and few, if any, professional translators were avail-
able. The availability of usable translations literally
became a matter of life and death.
In response to this need, Stanford University grad-
uate student Rob Munro coordinated the rapid cre-
ation of a crowdsourcing framework, which allowed
volunteers ? including, for example, Haitian expa-
triates and French speakers ? to translate messages,
providing responders with usable information in as
little as ten minutes. Translations may not have been
perfect, but to a woman in labor, it had to have made
a big difference for English-speaking responders to
see Undergoing children delivery Delmas 31 instead
of Fanm gen tranche pou fe` yon pitit nan Delmas 31.
What about a scenario, though, in which even am-
ateur bilingual volunteers are hard to find, or too
few in number? What about a scenario, e.g. the
March 2011 earthquake and tsunami in Japan, in
which there are many people worldwide who wish
to help but are not fluent in both the source and tar-
get languages?
For the last few years, we have been exploring the
idea of monolingual crowdsourcing for translation
? that is, technology-assisted collaborative transla-
tion involving crowds of participants who know only
the source or target language (Buzek et al, 2010;
Hu, 2009; Hu et al, 2010; Hu et al, 2011; Resnik
et al, 2010). Our MonoTrans2 framework has pre-
viously shown very promising results on children?s
books: on a test set where Google Translate pro-
duced correct translations for only 10% of the input
sentences, monolingual German and Spanish speak-
ers using our framework produced translations that
were fully correct (as judged by two independent
bilinguals) nearly 70% of the time (Hu et al, 2011).
We used the same framework in the WMT 2011
Haitian-English translation task. For this experi-
ment, we hired Haitian Creole speakers located in
Haiti, and recruited English speakers located in the
U.S., to serve as the monolingual crowds.
2 System
MonoTrans2 is a translation system that combines
machine translation (MT) with human computation
(Quinn et al, 2011) using two ?crowds? of mono-
lingual source (Haitian Creole) and target (English)
399
speakers.1 We summarize its operation here; see Hu
et al (2011) for details.
The Haitian Creole sentence is first automatically
translated into English and presented to the English
speakers. The English speakers then can take any of
the following actions for candidate translations:
? Mark a phrase in the candidate as an error
? Suggest a new translation candidate
? Vote candidates up or down
Identifying likely errors and voting for candidates
are things monolinguals can do reasonably well:
even without knowing the intended interpretation,
you can often identify when some part of a sentence
doesn?t make sense, or when one sentence seems
more fluent or plausible than another. Sometimes
rather than identifying errors, it is easier to suggest
an entirely new translation candidate based on the
information available on the target side, a variant
of monolingual post-editing (Callison-Burch et al,
2004).
Any new translation candidates are then back-
translated into Haitian Creole, and any spans marked
as translation errors are projected back to identify
the corresponding spans in the source sentence, us-
ing word alignments as the bridge (cf. Hwa et al
(2002), Yarowsky et al (2001)).2 The Haitian Cre-
ole speakers can then:
? Rephrase the entire source sentence (cf.
(Morita and Ishida, 2009))
? ?Explain? spans marked as errors
? Vote candidates up or down (based on the back-
translation)
Source speakers can ?explain? error spans by of-
fering a different way of phrasing that piece of the
source sentence (Resnik et al, 2010), in order to
produce a new source sentence, or by annotating the
spans with images (e.g. via Google image search)
or Web links (e.g. to Wikipedia). The protocol then
continues: new source sentences created via partial-
1For the work reported here, we used Google Translate as
the MT component via the Google Translate Research API.
2The Google Translate Research API provides alignments
with its hypotheses.
or full-sentence paraphrase pass back through MT
to the English side, and any explanatory annota-
tions are projected back to the corresponding spans
in the English candidate translations (where the er-
ror spans had been identified). The process is asyn-
chronous: participants on the Haitian Creole and
English sides can work independently on whatever
is available to them at any time. At any point, the
voting-based scores can be used to extract a 1-best
translation.
In summary, the MonoTrans2 framework uses
noisy MT to cross the language barrier, and supports
monolingual participants in doing small tasks that
gain leverage from redundant information, the hu-
man capacity for linguistic and real-world inference,
and the wisdom of the crowd.
3 Experiment
We recruited 26 English speakers and 4 Haitian Cre-
ole speakers. The Haitian Creole speakers were re-
cruited from Haiti and do not speak English. Five of
the 26 English speakers were paid UMD undergrad-
uates; the other 21 were volunteer researchers, grad-
uate students, and staff unrelated to this research. 3
Over a 13 day period, Haitian Creole and English
speaker efforts totaled 15 and 29 hours, respectively.
4 Data Sets
Our original goal of fully processing the entire SMS
clean test and devtest sets could not be realized in the
available time, owing to unanticipated reshuffling of
the data by the shared task organizers and logistical
challenges working with participants in Haiti. Ta-
ble 1 summarizes the data set sizes before and after
reshuffling. We put 1,224 sentences from the pre-
before after
test 1,224 1,274
devtest 925 900
Table 1: SMS clean data sets before and after reshuffling
reshuffling test set, interspersed with 123 of the 925
sentences from the pre-reshuffling devtest set, into
the system ? 1,347 sentences in total. We report
3These, obviously, did not include any of the authors.
400
results on the union of pre- and post-reshuffling de-
vtest sentences (Set A, |A| = 1516), and the post-
reshuffling test set (Set B, |B| = 1274 ).
5 Evaluation
Of the 1,347 sentences available for processing in
MonoTrans2, we define three subsets:
? Touched: Sentences that were processed by at
least one person (657 sentences)
? Each-side: Sentences that were processed by at
least one English speaker followed by at least
one Haitian Creole speaker (431 sentences)
? Full: Sentences that have at least three trans-
lation candidates, of which the most voted-for
one received at least three votes (207 sentences)
We intersect these three sets with sets A and B in or-
der to evaluate MonoTrans2 output against the pro-
vided references (Table 2).4
Set S |S| |S ?A| |S ?B|
Touched 657 162 168
Each-side 431 127 97
Full 207 76 60
Table 2: Data sets for evaluation and their sizes
Tables 3 and 4 report two automatic scoring met-
rics, uncased BLEU and TER, comparing Mono-
Trans2 (M2) against Google Translate (GT) as a
baseline.
Set Condition BLEU TER
Touched ?A
GT 21.75 56.99
M2 23.25 57.27
Each-side ?A
GT 21.44 57.51
M2 21.47 58.98
Full ?A
GT 25.05 54.15
M2 27.59 52.78
Table 3: BLEU and TER results for different levels of com-
pletion on the devtest set A
Since the number of sentences in each evaluated
set is different (Table 2), we cannot directly compare
4Note that according to these definitions, Touched contains
both Each-side and Full, but Each-side does not contain Full.
Set Condition BLEU TER
Touched ?B
GT 19.78 59.88
M2 24.09 58.15
Each-side ?B
GT 21.15 56.88
M2 23.80 57.19
Full ?B
GT 22.51 54.51
M2 28.90 52.22
Table 4: BLEU and TER results for different levels of com-
pletion on the test set B
scores between the sets. However, Table 4 shows
that when the MonoTrans2 process is run on test
items ?to completion?, in the sense defined by ?Full?
(i.e. Full?B), we see a dramatic BLEU gain of 6.39,
and a drop in TER of 2.29 points. Moreover, even
when only target-side or only source-side monolin-
gual participation is available we see a gain of 4.31
BLEU and a drop of 1.73 TER points (Touched?B).
By contrast, the results on the devtest data are en-
couraging, but arguably mixed (Table 3). In order to
step away from the vagaries of single-reference au-
tomatic evaluations, therefore, we also conducted an
evaluation based on human judgments. Two native
English speakers unfamiliar with the project were
recruited and paid for fluency and adequacy judg-
ments: for each target translation paired with its cor-
responding reference, each evaluator rated the tar-
get sentence?s fluency and adequacy on a 5-point
scale, where fluency of 5 indicates complete fluency
and adequacy of 5 indicates complete preservation
of meaning (Dabbadie et al, 2002).5
Sentences N Google MonoTrans2
Full ?A 76 18 (24%) 30 (39%)
Full ?B 60 15 (25%) 23 (38%)
Table 5: Number of sentences with maximum possible
adequacy (5) in Full ?A and Full ?B, respectively.
Similar to Hu et al (2011), we adopt the very con-
servative criterion that a translation output is consid-
ered correct only if both evaluators independently
give it a rating of 5. Unlike Hu et al (2011), for
whom children?s book translation requires both flu-
ency and adequacy, we make this a requirement only
5Presentation order was randomized.
401
for adequacy, since in this scenario what matters to
aid organizations is not whether a translation is fully
fluent, but whether it is correct. On this criterion,
the Google Translate baseline of around 25% cor-
rect improves to around 40% for Monotrans, con-
sistently for both the devtest and test data (Table 5).
Nonetheless, Figures 1 and 2 make it clear that the
improvements in fluency are if anything more strik-
ing.
5.1 Statistical Analysis
Variable Adequacy Fluency
Positive
mostSingleCandidateVote ** ***
candidateCount ** **
numOfAnswers * NS
Negative
roundTrips *** ***
voteCount * .
Table 6: Effects of independent variables in linear regres-
sion for 330 touched sentences
(Signif. codes: ?***? 0.001, ?**? 0.01, ?*? 0.05, ?.? 0.1)
In addition to the main evaluation, we investi-
gated the relationship between tasks performed in
the MonoTrans2 system and human judgments us-
ing linear regression and an analysis of variance.
We evaluate the set of all 330 touched sentences in
Touched?A and Touched?B in order to under-
stand which properties of the MonoTrans2 process
correlate with better translation outcomes.
Our analysis focused on improvement over the
Google Translate baseline, looking specifically at
the improvement based on the human evaluators? av-
eraged fluency and adequacy scores.
Table 6 summarizes the positive and negative
effects for five of six variables we considered that
came out significant for at least one of the measures.
6
The positive results were as expected. Having
more votes for the winning candidate (mostSingle-
CandidateVote) made it more successful, since this
means that more people felt it was a good represen-
tative translation. Having more candidates to choose
6A sixth, numOfVoters, was not significant in the linear re-
gression for either adequacy or fluency.
from (candidateCount) meant that more people had
taken the time to generate alternatives, reflecting at-
tention paid to the sentence. Also, the amount of
attention paid to target speakers? requests for clarifi-
cation (numOfAnswers) is as expected related to the
adequacy of the final translation, and perhaps as ex-
pected does not correlate with fluency of the output
since it helps with meaning and not actual target-side
wording.
We were, however, confused at first by the neg-
ative influence of the roundTrips measure and vote-
Count measures. We conjecture that the first effect
arises due to a correlation between roundTrips and
translation difficulty; much harder sentences would
have led to many more paraphrase requests, and
hence to more round trips. We attempted to inves-
tigate this hypothesis by testing correlation with a
naive measure of sentence difficulty, length, but this
was not fruitful. We suspect that inspecting use of
abbreviations, proper nouns, source-side mistakes,
and syntactic complexity would give us more insight
into this issue.
As for voteCount, the negative correlation is un-
derstandable when considered side by side with
the other vote-based measure, mostSingleCandidat-
eVote. Having a higher number of votes for the win-
ning candidate leads to improvement (strongly sig-
nificant for both adequacy and fluency), so a higher
general vote count means that people were also vot-
ing more times for other candidates. Hence, once the
positive winning vote count is taken into account,
the remaining votes actually represent disagreement
on the candidates, hence correlating negatively with
overall improvement over baseline.
It is important to note that when these measures
are all considered together, they show that there is a
clear correlation between the MonoTrans2 system?s
human processing and the eventual increase in both
quality and fluency of the sentences. As people give
more attention to sentences, these sentences show
better performance, as judged by increase over base-
line.
6 Discussion
Our experiment did not address acquisition of, and
incentives for, monolingual participants. In fact, get-
ting time from Haitian Creole speakers, even for pay,
402
0	 ?
10	 ?
20	 ?
30	 ?
40	 ?
50	 ?
60	 ?
1	 ? 2	 ? 3	 ? 4	 ? 5	 ?
#	 ?of	 ?s
enten
ces	 ?
Google	 ?MonoTrans2	 ?
(a) Fluency Distribution
0	 ?
10	 ?
20	 ?
30	 ?
40	 ?
50	 ?
1	 ? 2	 ? 3	 ? 4	 ? 5	 ?
#	 ?of	 ?S
enten
ces	 ?
Google	 ?MonoTrans2	 ?
(b) Adequacy Distribution
Figure 1: Human judgments for fluency and adequacy in fully processed devtest items (Full ?A)
0	 ?
10	 ?
20	 ?
30	 ?
40	 ?
50	 ?
1	 ? 2	 ? 3	 ? 4	 ? 5	 ?
#	 ?of	 ?s
enten
ces	 ?
Google	 ?MonoTrans2	 ?
(a) Fluency Distribution
0	 ?
10	 ?
20	 ?
30	 ?
40	 ?
50	 ?
1	 ? 2	 ? 3	 ? 4	 ? 5	 ?
#	 ?of	 ?S
enten
ces	 ?
Google	 ?MonoTrans2	 ?
(b) Adequacy Distribution
Figure 2: Human judgments for fluency and adequacy in fully processed test items (Full ?B)
created a large number of logistical challenges, and
was a contributing factor as to why we did not obtain
translations for the entire test set. However, avail-
ability of monolingual participants is not the issue
being addressed in this experiment: we are confi-
dent that in a real-world scenario like the Haitian
or Japanese earthquakes, large numbers of monolin-
gual volunteers would be eager to help, certainly in
larger total numbers than bilingual volunteers. What
matters here, therefore, is not how much of the test
set was translated in total, but how much the trans-
lations improved for the sentences where monolin-
gual crowdsourcing was involved, compared to the
MT baseline, and what throughput might be like in
a real-world scenario.
We also were interested in throughput, particu-
larly in comparison to bilingual translators. In previ-
ous experimentation (Hu et al, 2011), throughput in
MonoTrans2 extrapolated to roughly 800 words per
day, a factor of 2.5 slower than professional trans-
lators? typical speed of 2000 words per day. In
this experiment, overall translation speed averaged
about 300 words per day, a factor of more than 6
times slower. However, this is an extremely pes-
simistic estimate, for several reasons. First, our pre-
vious experiment had more than 20 users per side,
while here our Haitian crowd consisted of only four
people. Second, we discovered after beginning the
experiment that the translation of our instructions
into Haitian Creole had been done somewhat slop-
pily. And, third, we encountered a range of tech-
nical and logistical problems with our Haitian par-
ticipants, ranging from finding a location with In-
ternet access to do the work (ultimately an Internet
Cafe? turned out to be the best option), to slow and
sporadic connections (even in an Internet Cafe?), to
relative lack of motivation for part-time rather than
full-time work. It is fair to assume that in a real-
world scenario, some unanticipated problems like
these might crop up, but it also seems fair to assume
that many would not; for example, most people from
the Haitian Creole and French-speaking communi-
ties who volunteered using Munro et al?s system
in January 2010 were not themselves located in the
403
third world.
Finally, regarding quality, the results here are
promising, albeit not as striking as those Hu et al
(2011) obtained for Spanish-German translation of
children?s books. The nature of SMS messages
themselves may have been a contributing factor to
the lower translation adequacy: even in clean form,
these are sometimes written using shorthand (e.g.
?SVP?), and are sometimes not syntactically correct.
The text messages are seldom related to each other,
unlike sentences in larger bodies of text where even
partially translated sentences can be related to each
other to provide context, as is the case for children?s
books. One should also keep in mind that the under-
lying machine translation engine, Google Translate
between Haitian Creole and English, is still in an al-
pha phase.
Those considerations notwithstanding, it is en-
couraging to see a set of machine translations get
better without the use of any human bilingual exper-
tise. We are optimistic that with further refinements
and research, monolingual translation crowdsourc-
ing will make it possible to harness the vast num-
ber of technologically connected people who want
to help in some way when disaster strikes.
7 Acknowledgments
This research is supported by NSF contract
#BCS0941455 and by a Google Research Award.
References
Olivia Buzek, Philip Resnik, and Benjamin B. Bederson.
2010. Error driven paraphrase annotation using me-
chanical turk. In NAACL 2010 Workshop on Creating
Speech and Text Language Data With Amazon?s Me-
chanical Turk.
Chris Callison-Burch, Colin Bannard, , and Josh
Schroeder. 2004. Improving statistical translation
through editing. In Workshop of the European Asso-
ciation for Machine Translation.
Marianne Dabbadie, Anthony Hartley, Margaret King,
Keith J. Miller, Widad Mustafa El Hadi, Andrei
Popescu-Belis, Florence Reeder, and Michelle Vanni.
2002. A hands-on study of the reliability and coher-
ence of evaluation metrics. In Workshop at the LREC
2002 Conference, page 8. Citeseer.
Chang Hu, Benjamin B. Bederson, and Philip Resnik.
2010. Translation by iterative collaboration between
monolingual users. In Proceedings of Graphics Inter-
face 2010 on Proceedings of Graphics Interface 2010,
pages 39?46, Ottawa, Ontario, Canada. Canadian In-
formation Processing Society.
Chang Hu, Ben Bederson, Philip Resnik, and Yakov Kro-
nrod. 2011. Monotrans2: A new human computation
system to support monolingual translation. In Human
Factors in Computing Systems (CHI 2011), Vancou-
ver, Canada, May. ACM, ACM.
Chang Hu. 2009. Collaborative translation by monolin-
gual users. In Proceedings of the 27th international
conference extended abstracts on Human factors in
computing systems, pages 3105?3108, Boston, MA,
USA. ACM.
Rebecca Hwa, Philip Resnik, Amy Weinberg, and Okan
Kolak. 2002. Evaluating translational correspon-
dence using annotation projection. In Proceedings of
the 40th Annual Meeting on Association for Computa-
tional Linguistics, pages 392?399, Philadelphia, Penn-
sylvania. Association for Computational Linguistics.
Daisuke Morita and Toru Ishida. 2009. Designing pro-
tocols for collaborative translation. In PRIMA ?09:
Proceedings of the 12th International Conference on
Principles of Practice in Multi-Agent Systems, pages
17?32, Berlin, Heidelberg. Springer-Verlag.
Robert Munro. 2010. Crowdsourced translation for
emergency response in haiti: the global collaboration
of local knowledge. In AMTA Workshop on Collabo-
rative Crowdsourcing for Translation. Keynote.
Alexander J. Quinn, Bederson, and Benjamin B. Beder-
son. 2011. Human computation: A survey and tax-
onomy of a growing field. In Human Factors in Com-
puting Systems (CHI 2011), Vancouver, Canada, May.
ACM, ACM.
Philip Resnik, Olivia Buzek, Chang Hu, Yakov Kronrod,
Alexander J. Quinn, and Benjamin B. Bederson. 2010.
Improving translation via targeted paraphrasing. In
EMNLP.
David Yarowsky, Grace Ngai, and Richard Wicentowski.
2001. Inducing multilingual text analysis tools via
robust projection across aligned corpora. In HLT
?01: Proceedings of the first international conference
on Human language technology research, pages 1?8,
Morristown, NJ, USA. Association for Computational
Linguistics.
404
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 480?489,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Optimization Strategies for Online Large-Margin Learning in Machine
Translation
Vladimir Eidelman
UMIACS Laboratory for Computational Linguistics and Information Processing
Department of Computer Science
University of Maryland, College Park, MD
vlad@umiacs.umd.edu
Abstract
The introduction of large-margin based dis-
criminative methods for optimizing statistical
machine translation systems in recent years
has allowed exploration into many new types
of features for the translation process. By
removing the limitation on the number of
parameters which can be optimized, these
methods have allowed integrating millions of
sparse features. However, these methods have
not yet met with wide-spread adoption. This
may be partly due to the perceived complex-
ity of implementation, and partly due to the
lack of standard methodology for applying
these methods to MT. This papers aims to shed
light on large-margin learning for MT, explic-
itly presenting the simple passive-aggressive
algorithm which underlies many previous ap-
proaches, with direct application to MT, and
empirically comparing several widespread op-
timization strategies.
1 Introduction
Statistical machine translation (SMT) systems rep-
resent knowledge sources in the form of features,
and rely on parameters, or weights, on each feature,
to score alternative translations. As in all statistical
models, these parameters need to be learned from
the data. In recent years, there has been a growing
trend of moving away from discriminative training
using batch log-linear optimization, with Minimum-
Error Rate Training (MERT) (Och, 2003) being the
principle method, to online linear optimization (Chi-
ang et al, 2008; Watanabe et al, 2007; Arun and
Koehn, 2007). The major motivation for this has
been that while MERT is able to efficiently optimize
a small number of parameters directly toward an ex-
ternal evaluation metric, such as BLEU (Papineni et
al., 2002), it has been shown that its performance
can be erratic, and it is unable to scale to a large
set of features (Foster and Kuhn, 2009; Hopkins and
May, 2011). Furthermore, it is designed for batch
learning, which may be prohibitive or undesirable
in certain scenarios, for instance if we have a large
tuning set. One or both of these limitations have
led to recent introduction of alternative optimization
strategies, such as minimum-risk (Smith and Eis-
ner, 2006), PRO (Hopkins and May, 2011), Struc-
tured SVM (Cherry and Foster, 2012), and RAM-
PION (Gimpel and Smith, 2012), which are batch
learners, and online large-margin structured learn-
ing (Chiang et al, 2009; Watanabe et al, 2007;
Watanabe, 2012).
A popular method of large-margin optimiza-
tion is the margin-infused relaxed algorithm
(MIRA) (Crammer et al, 2006), which has been
shown to perform well for machine translation, as
well as other structured prediction tasks, such as
parsing. (McDonald et al, 2005). This is an at-
tractive method because we have a simple analytical
solution for the optimization problem at each step,
which reduces to dual coordinate descent when us-
ing 1-best MIRA. It is also quite easy to implement,
as will be shown below.
Despite the proven success of MIRA-based large-
margin optimization for both small and large num-
bers of features, these methods have not yielded
wide adoption in the community. Part of the rea-
son for this is a perception that these methods are
complicated to implement, which has been cited as
motivation for other work (Hopkins and May, 2011;
Gimpel and Smith, 2012). Furthermore, there is a di-
480
vergence between the standard application of these
methods in machine learning, and our application
in machine translation (Gimpel and Smith, 2012),
where in machine learning there are usually clear
correct outputs and no latent structures. As a con-
sequence of the above, there is a lack of standard
practices for large-margin learning for MT, which
has resulted in numerous different implementations
of MIRA-based optimizers, which further add to the
confusion.
This paper aims to shed light on practical con-
cerns with online large margin training. Specif-
ically, our contribution is first, to present the
MIRA passive-aggressive update, which underlies
all MIRA-based training, with an eye to applica-
tion in MT. Then, we empirically compare several
widespread as well as novel optimization strategies
for large-margin training on Czech-to-English (cs-
en) and French-to-English (fr-en) translation. Ana-
lyzing the findings, we recommend an optimization
strategy which should ensure convergence and sta-
bility.
2 Large-Margin Learning
2.1 Description
MIRA is an online large-margin learner, and be-
longs to a class of passive-aggressive (PA) algo-
rithms (Crammer et al, 2006). Although the exact
procedure it employs is different from other subgra-
dient optimizers, in essence it is performing a sub-
gradient descent step, where the step size is adjusted
based on each example. The underlying objective
of MIRA is the same as that of the margin rescaled
Structural SVM (Tsochantaridis et al, 2004; Mar-
tins et al, 2010), where we want to predict the cor-
rect output over the incorrect one by a margin at least
as large as the cost incurred by predicting the in-
correct output. However, the norm constraint from
SVM is replaced with a proximity constraint, indi-
cating we want to update our parameters, but keep
them as close as possible to the previous parame-
ter estimates. In the original formulation for sepa-
rable classification (Crammer and Singer, 2003), if
no constraints are violated, no update occurs. How-
ever, when there is a loss, the algorithm updates the
parameters to satisfy the constraints. To allow for
noise in the data, i.e. nonseparable instances, a slack
variable ?i is introduced for each example, and we
optimize a soft-margin. The usual presentation of
MIRA is then given as:
wt+1 = argmin
w
1
2
||w ?wt||2 + C?i
s.t. w>f(xi, yi)?w>f(xi, y?) ? cost(yi, y?)? ?i
(1)
where f(xi, yi) is a vector of feature functions1, w
is a vector of corresponding parameters, y? ? Y(xi),
where Y(xi) is the space of possible translations we
are able to produce from x,2 and cost(yi, ?) is com-
puted using an external measure of quality, such as
BLEU.
The underlying structured hinge loss objective
function can be rewritten as:
`h = ?w
>f(xi, yi)+
max
y??Y(xi)
(
w>f(xi, y?) + cost(yi, y?)
) (2)
2.2 Hypothesis Selection
Our training corpus T = (xi, yi)
T
i=1 for selecting the
parameters w that optimize this objective consists of
input sentences xi in the source language paired with
reference translations yi in the target language. No-
tice that `h depends on computing the margin be-
tween y? ? Y(xi) and the correct output, yi. How-
ever, there is no guarantee that yi ? Y(xi) since
our decoder is often incapable of producing the ref-
erence translation yi. Since we need to have some
notion of the correct output in order to compute its
feature vector for the margin, in practice we revert to
using surrogate references in place of yi. These are
often referred to as oracles, y+, which are selected
from the hypothesis space Y(xi) of the decoder.
We are also faced with the problem of how best
to select the most appropriate y? to shy away from,
which we will refer to as y?. Since optimization will
proceed by setting parameters to increase the score
of y+, and decrease the score of y?, the selection
of these two hypotheses is crucial to success. The
range of possibilities is presented in Eq. 3 below.
1More appropriately, since we only observe translations
yi, which may have many possible derivations dj , we model
the derivations as a latent variable, and our feature functions
are actually computed over derivation and translation pairs
f(xi, yi, dj). We omit dj for clarity.
2The entire hypergraph in hierarchical translation or lattice
in phrase based translation.
481
`r = ? max
y+?Y(xi)
(
?+w>f(xi, y+)? ?+cost(yi, y+)
)
+ max
y??Y(xi)
(
??w>f(xi, y?) + ??cost(yi, y?)
)
(3)
Although this formulation has commonly been
referred to as the hinge loss in previous litera-
ture, Gimpel and Smith (2012) have recently pointed
out that we are in fact optimizing losses that are
closer to different variants of the structured ramp
loss. The difference in definition between the two is
subtle, in that for the ramp loss, yi is replaced with
y+. Each setting of ?? and ?? corresponds to opti-
mizing a different loss function. Several definitions
of `r have been explored in the literature, and we
discuss them below with corresponding settings of
?? and ??.
In selecting y+, we vary the settings of ?+ and
?+. Assuming our cost function is based on BLEU,
in setting ?+ ? 1 and ?+ ? 0, if Y(xi) is taken
to be the entire space of possible translations, we
are selecting the hypothesis with the highest BLEU
overall. This is referred to in past work as max-
BLEU (Tillmann and Zhang, 2006) (MB). If we ap-
proximate the search space by restricting Y(xi) to
a k-best list, we have the local-update (Liang et
al., 2006), where we select the highest BLEU can-
didate from those hypotheses that the model consid-
ers good (LU). With increasing k-best size, the max-
BLEU and local-update strategies begin to converge.
Setting both ?+ ? 1 and ?+ ? 1, we ob-
tain the cost-diminished hypothesis, which consid-
ers both the model and the cost, and corresponds to
the ?hope? hypothesis in Chiang et al (2008) (M-
C). This can be computed over the entire space of
hypotheses or a k-best list. In a sense, this is the
intuition that local-updating is after, but expressed
more directly.
The alternatives for selecting y? are quite sim-
ilar. Setting ?? ? 1 and ?? ? 0, we select
the hypothesis with the highest cost (MC). Setting
?? ? 0 and ?? ? 1, we have the highest scor-
ing hypothesis according to the model, which cor-
responds to prediction-based selection (Crammer et
al., 2006) (PB). Setting both to 1, we have the cost-
augmented hypothesis, which is referred to as the
?fear? (Chiang et al, 2008), and max-loss (Cram-
mer et al, 2006) (M+C). This hypothesis is consid-
ered the most dangerous because it has a high model
score along with a high cost.
Considering the settings for both parts of Eq. 3,
?+, ?+ and ??, ??, assigning all ?? and ?? to 1
corresponds to the most commonly used loss func-
tion in MT (Gimpel and Smith, 2012; Chiang et
al., 2009). This is the ?hope?/?fear? pairing, where
we use the cost-diminished hypothesis y+ and cost-
augmented hypothesis y?. Other loss functions have
also been explored, such as ?? ? 1, ?+ ? 1,
?? ? 0 (Liang et al, 2006), and something ap-
proximating ?? ? 1, ?+ ? 0, ?? ? 1 (Cherry
and Foster, 2012), which is closer to the usual loss
used for max-margin in machine learing. To our best
knowledge, other loss functions explored below are
novel to this work.
Since our external metric, BLEU, is a gain, we can
think of the first term in Eq. 3 as the model score plus
the BLEU score, and the second term as the model
minus the BLEU score. That is, with all ?? and ??
set to 1, we want y+ to be the hypothesis with a
high model score, as well as being close to the refer-
ence translation, as indicated by a high BLEU score.
While for y?, we want a high model score, but it
should be far away from the reference, as indicated
by a low BLEU score. The motivation for choosing
y? in this fashion is grounded in the fact that since
we are penalized by this term in the ramp loss ob-
jective, we should try to optimize on it directly. In
practice, we can compute the cost for both terms as
(1-BLEU(y,yi)), or use that as the cost of the first
term, and after selecting y+, compute the cost of y?
by taking the difference between BLEU(y+,yi) and
BLEU(y,yi).
The ramp loss objectives are non-convex, and by
separately computing the max for both y+ and y?,
we are theoretically prohibited from online learning
since we are no longer guaranteed to be optimizing
the desired loss. This is one motivation for the batch
learner, RAMPION (Gimpel and Smith, 2012). How-
ever, as with many non-convex optimization prob-
lems in NLP, such as those involving latent vari-
ables, in practice online learning in this setting be-
haves quite well.
482
2.3 Parameter Update
The major practical concern with these methods for
SMT is that oftentimes the implementation aspect
is unclear, a problem which is further exacerbated
by the apparent difficulty of implementation. This
is further compounded with a lack of standard prac-
tices; both theoretical, such as the objective to op-
timize, and practical, such as efficient paralleliza-
tion. The former is a result of the disconnect be-
tween the standard machine learning setting, which
posits reachable references and lack of latent vari-
ables, and our own application. The latter is an
active engineering problem. Both of these aspects
have been receiving recent attention (McAllester et
al., 2010; Mcallester and Keshet, 2011; Gimpel and
Smith, 2012; McDonald et al, 2010), and although
certain questions remain as to the exact loss being
optimized, we now have a better understanding of
the theoretical underpinnings of this method of opti-
mization.
The first adaptations of MIRA-based learning for
structured prediction in NLP utilized a set of k con-
straints, either for y+, y?, or both. This complicated
the optimization by creating a QP problem with a set
of linear constraints which needed to be solved with
either Hildreth?s algorithm or SMO style optimiza-
tion, thereby precluding the possibility of a sim-
ple analytical solution. Later, Chiang (2012) intro-
duced a cutting-plane algorithm, like that of Struc-
tural SVM?s (Tsochantaridis et al, 2004), which op-
timizes on a small set of active constraints.
While these methods of dealing with structured
prediction may perform better empirically, they
come with a higher computational cost. Crammer
et al (2006) shows that satisfying the single most
violated margin constraint, commonly referred to
as 1-best MIRA, is amenable to a simple analyt-
ical solution for the optimization problem at each
step. Furthermore, the 1-best MIRA update is con-
ceptually and practically much simpler, while retain-
ing most of the optimization power of the more ad-
vanced methods. Thus, this is the method we present
below.
Since the MIRA optimization problem is an in-
stance of a general structured problem with an `2
norm, the update at each step reduces to dual co-
ordinate descent (Smith, 2011). In our soft-margin
Algorithm 1 MIRA Training
Require: : Training set T = (xi, yi)
T
i=1, w, C
1: for j ? 1 to N do
2: for i? 1 to T do
3: Y(xi)?Decode(xi,w)
4: y+ ? FindOracle(Y(xi))
5: y? ? FindPrediction(Y(xi))
6: margin? w>f(xi, y?)?w>f(xi, y+)
7: cost? BLEU(yi, y+)? BLEU(yi, y?)
8: loss = margin + cost
9: if loss > 0 then
10: ? ? min
(
C, loss
?f(xi,y+)?f(xi,y?)?
2
)
11: w? w+ ? (f(xi, y+)? f(xi, y?))
12: end if
13: end for
14: end for
15: return w
Algorithm 2 FindOracle
Require: : Y(xi)
1: if ?+=0 and ?+=1 then
2: y+ ? argmaxy?Y(xi)?cost(yi, y)
3: else if ?+ = ?+ = 1 then
4: y+ ? argmaxy?Y(xi)w
>f(xi, y) ?
cost(yi, y)
5: end if
6: return y+
setting, this is analogous to the PA-I update of Cram-
mer et al (2006). In fact, this update remains largely
intact as the inner core within k-best constraint or
cutting plane optimization. Algorithm 1 presents the
entire training regime necessary for 1-best MIRA
training of a machine translation system. As can be
seen, the parameter update at step 11 depends on the
difference between the features of y+ and y?, where
? is the step size, which is controlled by the regular-
ization parameter C; indicating how far we are will-
ing to move at each step. Y(xi) may be a k-best list
or the entire space of hypotheses.3
3For a more in depth examination and derivation of large-
margin learning in MT, see (Chiang, 2012).
483
Algorithm 3 FindPrediction
Require: : Y(xi)
1: if ??=0 and ??=1 then
2: y? ? argmaxy?Y(xi) cost(yi, y)
3: else if ??=1 and ??=0 then
4: y? ? argmaxy?Y(xi)w
>f(xi, y)
5: else if ?? = ?? = 1 then
6: y? ? argmaxy?Y(xi)w
>f(xi, y) +
cost(yi, y)
7: end if
8: return y?
3 Experiments
3.1 Setup
To empirically analyze which loss, and thereby
which strategy, for selecting y+ and y? is most
appropriate for machine translation, we conducted
a series of experiments on Czech-to-English and
French-to-English translation. The parallel corpora
are taken from the WMT2012 shared translation
task, and consist of Europarl data along with the
News Commentary corpus. All data were tokenized
and lowercased, then filtered for length and aligned
using the GIZA++ implementation of IBM Model
4 (Och and Ney, 2003) to obtain bidirectional align-
ments, which were symmetrized using the grow-
diag-final-and method (Koehn et al, 2003). Gram-
mars were extracted from the resulting parallel text
and used in our hierarchical phrase-based system us-
ing cdec (Dyer et al, 2010) as the decoder. We con-
structed a 5-gram language model from the provided
English News monolingual training data as well as
the English side of the parallel corpus using the SRI
language modeling toolkit with modified Kneser-
Ney smoothing (Chen and Goodman, 1996). This
was used to create a KenLM (Heafield, 2011).
As the tuning set for both language pairs, we used
the 2051 sentences in news-test2008 (NT08), and re-
port results on the 2525 sentences of news-test2009
(NT09) and 2489 of news-test2010 (NT10).
Corpus Sentences Tokens
en *
cs-en 764K 20.5M 17.5M
fr-en 2M 57M 63M
Table 1: Corpus statistics
pair 1 500 50k 100k
cs-en 17.9 24.9 29.4 29.7
fr-en 20.25 29.9 33.8 34.1
Table 2: Oracle score for model 1-best (baseline) and for
k-best of size 500, 50k, and 100k on NT08
We approximate cost-augmented decoding by ob-
taining a k-best list with k=500 unique best from our
decoder at each iteration, and selecting the respec-
tive hypotheses for optimization from it. To approx-
imate max-BLEU decoding using a k-best list, we set
k=50k unique best hypotheses.4 As can be seen in
Table 2, we found this size was sufficient for our pur-
poses as increasing size led to small improvements
in oracle BLEU score. C is set to 0.01.
For comparison with MERT, we create a base-
line model which uses a small standard set of fea-
tures found in translation systems: language model
probability, phrase translation probabilities, lexi-
cal weighting probabilities, and source word, pass-
through, and word penalties.
While BLEU is usually calculated at the corpus
level, we need to approximate the metric at the sen-
tence level. In this, we mostly follow previous ap-
proaches, where in the first iteration through the cor-
pus we use a smoothed sentence level BLEU approx-
imation, similar to Lin and Och (2004), and in sub-
sequently iterations, the BLEU score is calculated in
the context of the previous set of 1-best translations
of the entire tuning set.
To make parameter estimation more efficient,
some form of parallelization is preferred. While ear-
lier versions of MIRA training had complex paral-
lelization procedures which necessitated passing in-
formation between learners, performing iterative pa-
rameter mixing (McDonald et al, 2010) has been
shown to be just as effective (Chiang, 2012). We
use a simple implementation of this regime, where
we divide the tuning set into n shards and distribute
them amongst n learners, along with the parameter
vectorw. Each learner decodes and updates parame-
4We are able to theoretically extract more constraints from
a large list, in the spirit of k-constraints or a cutting plane,
but Chiang (2012) showed that cutting plane performance is
approximately 0.2-0.4 BLEU better than a single constraint, so
although there is a trade off between the simplicity of a single
constraint and performance, it is not substantial.
484
cs-en NT09 NT10
LU M-C LU M-C
PB 16.4 18.3 17 19.3
MC 18.5 16 19.1 17.5
M+C 17.8 18.7 18.4 19.6
Table 3: Results with different strategies on cs-en transla-
tion. MERT baseline is 18.4 for NT09 and 19.7 for NT10
ters on its shard of the tuning set, and once all learn-
ers are finished, these n parameter vectors are aver-
aged to form the initial parameter vector for the next
iteration. In our experiments, n=20.
3.2 Results
The results of using different optimization strategies
for cs-en and fr-en are presented in Tables 3 and 4
below. For all experiments, all settings are kept ex-
actly the same, with the only variation being the se-
lection of the oracle y+ and prediction y?. The first
column in each table indicates the method for se-
lecting the prediction, y?. PB indicates prediction-
based, MC is the hypothesis with the highest cost,
and M+C is cost-augmented selection. Analogously,
the headings across the table indicate oracle selec-
tion strategies, with LU indicating local updating,
and M-C being cost-diminished selection.
From the cs-en results in Table 3, we can see that
two settings fair the best: LU oracle selection paired
with MC prediction selection (LU/MC), and M-C
oracle selection paired with M+C prediction selec-
tion (M?C). On both sets, (M?C) performs better,
but the results are comparable. Pairing M-C with
PB is also a viable strategy, while no other pairing is
successful for LU.
When comparing with MERT, note that we use
a hypergraph based MERT (Kumar et al, 2009),
while the MIRA updates are computed from a k-best
list. For max-BLEU oracle selection paired with MC,
the performance decreases substantially, to 15.4 and
16.6 BLEU on NT09 and NT10, respectively. Using
the augmented k-best list did not significantly affect
performance for M-C oracle selection.
For fr-en, we see much the same behavior as in
cs-en. However, here LU/MC slightly outperforms
M?C. From both tasks, we can see that LU is more
sensitive to prediction selection, and can only op-
fr-en NT09 NT10
LU M-C LU M-C
PB 20.5 23.1 22.2 25
MC 23.9 23 25.8 24.8
M+C 22.2 23.6 24 25.4
Table 4: Results with different strategies on fr-en transla-
tion. MERT baseline is 24.2 for NT09 and 26 for NT10
timize effectively when paired with MC. M-C on
the other hand, is more forgiving, and can make
progress with PB and MC, albeit not as effectively
as with M+C.
3.3 Large Feature Set
Since one of the primary motivations for large-
margin learning is the ability to effectively handle
large quantities of features, we further evaluate the
ability of the strategies by introducing a large num-
ber of sparse features into our model. We introduce
sparse binary indicator features of the form com-
monly found in MT research (Chiang et al, 2009;
Watanabe et al, 2007). Specifically, we introduce
two types of features based on word alignment from
hierarchical phrase pairs and a target bigram fea-
ture. The first type, a word pair feature, fires for
every word pair (ei, fj) observed in the phrase pair.
The second, insertion features, account for spurious
words on the target side of a phrase pair by firing for
unaligned target words, associating them with ev-
ery source word, i.e. (ei, fj), (ei, fj+1), etc.. The
target bigram feature fires for every pair of consec-
utive words on the target side (ei, ei+1). In all, we
introduce 650k features for cs-en, and 1.1M for fr-
en. Taking the two best performing strategies from
the baseline model, LU/MC and M?C, we compare
their performance with the larger feature set in Ta-
ble 5.
Although integrating these features does not sig-
nificantly alter the performance on either task, our
purpose was to establish once again that the large-
margin learning framework is capable of effectively
optimizing parameters for a large number of sparse
features in the MT setting.
485
0.07 
0.12 
0.17 
0.22 
0.27 
0.32 
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 
BLE
U 
Iteration 
Figure 1: Comparison of performance on development set
for cs-en when using LU/MC and M?C selection.
0.07 
0.12 
0.17 
0.22 
0.27 
0.32 
0.37 
0.42 
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 
BLE
U 
Iteration 
Figure 2: Comparison of performance on development set
for fr-en when using LU/MC and M?C selection.
fr-en cs-en
NT09 NT10 NT09 NT10
LU/MC 23.9 25.7 18.5 19.6
M?C 23.8 25.4 18.6 19.6
Table 5: Results on cs-en and fr-en with extended feature
set.
4 Discussion
Although the performance of the two strategies is
competitive on the evaluation sets, this does not re-
lay the entire story. For a more complete view of
the differences between optimization strategies, we
turn to Figures 1-6. Figure 1 and 2 present the
comparison of performance on the NT08 develop-
ment set for cs-en and fr-en, respectively, when us-
ing LU/MC to select the oracle and prediction ver-
sus M?C selection. M?C is indicated with a solid
black line, while LU/MC is a dotted red line. The
corpus-level oracle and prediction BLEU scores at
each iteration are indicated with error bars around
each point, using solid lines for M?C and dotted
lines for LU/MC. As can be seen in Figure 1, while
optimizing with M?C is stable and smooth, where
we converge on our optimum after several iterations,
optimizing with LU/MC is highly unstable. This is
at least in part due to the wide range in BLEU scores
for the oracle and prediction, which are in the range
of 10 BLEU points higher or lower than the current
model best. On the contrary, the range of BLEU
scores for the M?C optimizer is on the order of 2
BLEU points, leading to more gradual changes.
We see a similar, albeit slightly less pronounced
behavior on fr-en in Figure 2. M?C optimization
is once again smooth, and converges quickly, with
a small range for the oracle and prediction scores
around the model best. LU/MC remains unstable,
oscillating up to 2 BLEU points between iterations.
Figures 3-6 compare the different optimization
strategies further. In Figures 3 and 5, we use M-C
as the oracle, and show performance on the develop-
ment set while using the three prediction selection
strategies, M+C with a solid blue line, PB with a
dotted green line, and MC with a dashed red line.
Error bars indicate the oracle and prediction BLEU
scores for each pairing as before. In all three cases,
the oracle BLEU score is in about the same range,
as expected, since all are using the same oracle se-
lection strategy. We can immediately observe that
PB has no error bars going down, indicating that the
PB method for selecting the prediction keeps pace
with the model best at each iteration. On the other
hand, MC selection also stands out, since it is the
only one with a large drop in prediction BLEU score.
Crucially, all learners are stable, and move toward
convergence smoothly, which serves to validate our
earlier observation that M-C oracle selection can be
paired with any prediction selection strategy and op-
timize effectively. In both cs-en and fr-en, we can
observe that M?C performs the best.
In Figures 4 and 6, we use LU as the oracle, and
show performance using the three prediction selec-
tion strategies, with each line representing the same
strategy as described above. The major difference,
which is immediately evident, is that the optimizers
are highly unstable. The only pairing which shows
some stability is LU/MC, with both the other predic-
486
0.05 
0.07 
0.09 
0.11 
0.13 
0.15 
0.17 
0.19 
0.21 
0.23 
0.25 
1 2 3 4 5 6 7 8 9 10 
BLE
U 
Iteration 
Figure 3: Comparison of performance on development set
for cs-en of the three prediction selection strategies when
using M-C selection as oracle.
0.05 
0.1 
0.15 
0.2 
0.25 
0.3 
0.35 
1 2 3 4 5 6 7 8 9 10 
BLE
U 
Iteration 
Figure 4: Comparison of performance on development set
for cs-en of the three prediction selection strategies when
using LU selection as oracle.
0.05 
0.1 
0.15 
0.2 
0.25 
0.3 
1 2 3 4 5 6 7 8 9 10 
BLE
U 
Iteration 
Figure 5: Comparison of performance on development set
for fr-en of the three prediction selection strategies when
using M-C selection as oracle.
0.05 
0.1 
0.15 
0.2 
0.25 
0.3 
0.35 
0.4 
1 2 3 4 5 6 7 8 9 10 
BLE
U 
Iteration 
Figure 6: Comparison of performance on development set
for fr-en of the three prediction selection strategies when
using LU selection as oracle.
tion selection methods, PB and M+C significantly
underperforming it.
Given that the translation performance of optimiz-
ing the loss functions represented by LU/MC and
M?C selection is comparable on the evaluation sets
for fr-en and cs-en, it may be premature to make
a general recommendation for one over the other.
However, taking the unstable nature of LU/MC into
account, the extent of which may depend on the tun-
ing set, as well as other factors which need to be
further examined, the current more prudent alterna-
tive is selecting the oracle and prediction pair based
on M?C.
5 Conclusion
In this paper, we strove to elucidate aspects of large-
margin structured learning with concrete application
to the MT setting. Towards this goal, we presented
the MIRA passive-aggressive algorithm, which can
be used directly to effectively tune a statistical MT
system with millions of parameters, in the hope that
some confusion surrounding MIRA-based methods
may be cleared, and more MT researchers can adopt
it for their own use. We then used the presented al-
gorithm to empirically compare several widespread
loss functions and strategies for selecting hypothe-
ses for optimization. We showed that although there
are two competing strategies with comparable per-
formance, one is an unstable learner, and before we
understand more regarding the nature of the insta-
bility, the preferred alternative is to use M?C as the
hypothesis pair in optimization.
Acknowledgments
We would like to thank the anonymous reviewers
for their comments. The author is supported by
the Department of Defense through the National
Defense Science and Engineering Graduate Fellow-
487
ship. Any opinions, findings, conclusions, or rec-
ommendations expressed are the author?s and do not
necessarily reflect those of the sponsors.
References
Abishek Arun and Philipp Koehn. 2007. Online learning
methods for discriminative training of phrase based
statistical machine translation. In MT Summit XI.
Stanley F. Chen and Joshua Goodman. 1996. An empir-
ical study of smoothing techniques for language mod-
eling. In Proceedings of the 34th Annual Meeting of
the Association for Computational Linguistics, pages
310?318.
Colin Cherry and George Foster. 2012. Batch tuning
strategies for statistical machine translation. In Pro-
ceedings of NAACL.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In Proceedings of the 2008
Conference on Empirical Methods in Natural Lan-
guage Processing, Honolulu, Hawaii, October.
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 new features for statistical machine transla-
tion. In Proceedings of Human Language Technolo-
gies: The 2009 Annual Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics, NAACL ?09, pages 218?226.
David Chiang. 2012. Hope and fear for discriminative
training of statistical translation models. To appear in
J. Machine Learning Research.
Koby Crammer and Yoram Singer. 2003. Ultraconser-
vative online algorithms for multiclass problems. J.
Mach. Learn. Res., 3:951?991, March.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. J. Mach. Learn. Res., 7:551?
585.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Jonathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Setiawan,
Vladimir Eidelman, and Philip Resnik. 2010. cdec: A
decoder, alignment, and learning framework for finite-
state and context-free translation models. In Proceed-
ings of ACL System Demonstrations.
George Foster and Roland Kuhn. 2009. Stabilizing
minimum error rate training. In Proceedings of the
Fourth Workshop on Statistical Machine Translation,
pages 242?249, Athens, Greece, March. Association
for Computational Linguistics.
Kevin Gimpel and Noah A. Smith. 2012. Structured
ramp loss minimization for machine translation. In
Proceedings of NAACL.
Kenneth Heafield. 2011. Kenlm: faster and smaller
language model queries. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, WMT
?11, pages 187?197.
Mark Hopkins and Jonathan May. 2011. Tuning as rank-
ing. In Proceedings of the 2011 Conference on Empir-
ical Methods in Natural Language Processing, pages
1352?1362, Edinburgh, Scotland, UK., July. Associa-
tion for Computational Linguistics.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the 2003 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics on Human Language Technology - Volume 1,
NAACL ?03, Stroudsburg, PA, USA.
Shankar Kumar, Wolfgang Macherey, Chris Dyer, and
Franz Och. 2009. Efficient minimum error rate train-
ing and minimum bayes-risk decoding for translation
hypergraphs and lattices. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP, pages 163?171.
Percy Liang, Alexandre Bouchard-Co?te?, Dan Klein, and
Ben Taskar. 2006. An end-to-end discriminative ap-
proach to machine translation. In Proceedings of the
21st International Conference on Computational Lin-
guistics and the 44th annual meeting of the Associ-
ation for Computational Linguistics, ACL-44, pages
761?768.
Chin-Yew Lin and Franz Josef Och. 2004. Orange: a
method for evaluating automatic evaluation metrics for
machine translation. In Proceedings of the 20th inter-
national conference on Computational Linguistics.
A. F. T. Martins, K. Gimpel, N. A. Smith, E. P.
Xing, P. M. Q. Aguiar, and M. A. T. Figueiredo.
2010. Learning structured classifiers with dual coor-
dinate descent. Technical Report CMU-ML-10-109,
Carnegie Mellon University.
David Mcallester and Joseph Keshet. 2011. Generaliza-
tion bounds and consistency for latent structural pro-
bit and ramp loss. In J. Shawe-Taylor, R.S. Zemel,
P. Bartlett, F.C.N. Pereira, and K.Q. Weinberger, edi-
tors, Advances in Neural Information Processing Sys-
tems 24, pages 2205?2212.
David McAllester, Tamir Hazan, and Joseph Keshet.
2010. Direct loss minimization for structured predic-
tion. In J. Lafferty, C. K. I. Williams, J. Shawe-Taylor,
R.S. Zemel, and A. Culotta, editors, Advances in Neu-
ral Information Processing Systems 23, pages 1594?
1602.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. In Proceedings of the 43rd Annual Meeting on
488
Association for Computational Linguistics, ACL ?05.
Association for Computational Linguistics.
Ryan McDonald, Keith Hall, and Gideon Mann. 2010.
Distributed training strategies for the structured per-
ceptron. In Human Language Technologies: The 2010
Annual Conference of the North American Chapter of
the Association for Computational Linguistics, pages
456?464, Los Angeles, California.
Franz Och and Hermann Ney. 2003. A systematic com-
parison of various statistical alignment models. In
Computational Linguistics, volume 29(21), pages 19?
51.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting of the Association for Computa-
tional Linguistics, pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of 40th
Annual Meeting of the Association for Computational
Linguistics, pages 311?318.
David A. Smith and Jason Eisner. 2006. Minimum
risk annealing for training log-linear models. In Pro-
ceedings of the COLING/ACL 2006 Main Conference
Poster Sessions, Sydney, Australia, July. Association
for Computational Linguistics.
Noah A. Smith. 2011. Linguistic Structure Predic-
tion. Synthesis Lectures on Human Language Tech-
nologies. Morgan and Claypool, May.
Christoph Tillmann and Tong Zhang. 2006. A discrim-
inative global training algorithm for statistical mt. In
Proceedings of the 21st International Conference on
Computational Linguistics and the 44th annual meet-
ing of the Association for Computational Linguistics,
ACL-44, pages 721?728.
Ioannis Tsochantaridis, Thomas Hofmann, Thorsten
Joachims, and Yasemin Altun. 2004. Support vec-
tor machine learning for interdependent and structured
output spaces. In Proceedings of the twenty-first inter-
national conference on Machine learning, ICML ?04.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki
Isozaki. 2007. Online large-margin training for sta-
tistical machine translation. In Proceedings of the
2007 Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL), Prague, Czech
Republic, June. Association for Computational Lin-
guistics.
Taro Watanabe. 2012. Optimized online rank learning
for machine translation. In Proceedings of NAACL.
489
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 128?133,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
Towards Efficient Large-Scale Feature-Rich Statistical Machine
Translation
Vladimir Eidelman1, Ke Wu1, Ferhan Ture1, Philip Resnik2, Jimmy Lin3
1 Dept. of Computer Science 2 Dept. of Linguistics 3 The iSchool
Institute for Advanced Computer Studies
University of Maryland
{vlad,wuke,fture,resnik,jimmylin}@umiacs.umd.edu
Abstract
We present the system we developed to
provide efficient large-scale feature-rich
discriminative training for machine trans-
lation. We describe how we integrate with
MapReduce using Hadoop streaming to
allow arbitrarily scaling the tuning set and
utilizing a sparse feature set. We report our
findings on German-English and Russian-
English translation, and discuss benefits,
as well as obstacles, to tuning on larger
development sets drawn from the parallel
training data.
1 Introduction
The adoption of discriminative learning methods
for SMT that scale easily to handle sparse and lex-
icalized features has been increasing in the last
several years (Chiang, 2012; Hopkins and May,
2011). However, relatively few systems take full
advantage of the opportunity. With some excep-
tions (Simianer et al, 2012), most still rely on
tuning a handful of common dense features, along
with at most a few thousand others, on a relatively
small development set (Cherry and Foster, 2012;
Chiang et al, 2009). While more features tuned
on more data usually results in better performance
for other NLP tasks, this has not necessarily been
the case for SMT.
Thus, our main focus in this paper is to improve
understanding into the effective use of sparse fea-
tures, and understand the benefits and shortcom-
ings of large-scale discriminative training. To
this end, we conducted experiments for the shared
translation task of the 2013 Workshop on Statis-
tical Machine Translation for the German-English
and Russian-English language pairs.
2 Baseline system
We use a hierarchical phrase-based decoder im-
plemented in the open source translation system
cdec1 (Dyer et al, 2010). For tuning, we use
Mr. MIRA2 (Eidelman et al, 2013), an open
source decoder agnostic implementation of online
large-margin learning in Hadoop MapReduce. Mr.
MIRA separates learning from the decoder, allow-
ing the flexibility to specify the desired inference
procedure through a simple text communication
protocol. The decoder receives input sentences
and weight updates from the learner, while the
learner receives k-best output with feature vectors
from the decoder.
Hadoop MapReduce (Dean and Ghemawat,
2004) is a popular distributed processing frame-
work that has gained widespread adoption, with
the advantage of providing scalable parallelization
in a manageable framework, taking care of data
distribution, synchronization, fault tolerance, as
well as other features. Thus, while we could oth-
erwise achieve the same level of parallelization, it
would be in a more ad-hoc manner.
The advantage of online methods lies in their
ability to deal with large training sets and high-
dimensional input representations while remain-
ing simple and offering fast convergence. With
Hadoop streaming, our system can take advantage
of commodity clusters to handle parallel large-
scale training while also being capable of running
on a single machine or PBS-managed batch clus-
ter.
System design To efficiently encode the infor-
mation that the learner and decoder require (source
sentence, reference translation, grammar rules) in
a manner amenable to MapReduce, i.e. avoiding
dependencies on ?side data? and large transfers
across the network, we append the reference and
1http://cdec-decoder.org
2https://github.com/kho/mr-mira
128
per-sentence grammar to each input source sen-
tence. Although this file?s size is substantial, it is
not a problem since after the initial transfer, it re-
sides on Hadoop distributed file system, and Map-
Reduce optimizes for data locality when schedul-
ing mappers.
A single iteration of training is performed as
a Hadoop streaming job. Each begins with a
map phase, with every parallel mapper loading the
same initial weights and decoding and updating
parameters on a shard of the data. This is followed
by a reduce phase, with a single reducer collect-
ing final weights from all mappers and computing
a weighted average to distribute as initial weights
for the next iteration.
Parameter Settings We tune our system toward
approximate sentence-level BLEU (Papineni et al,
2002),3 and the decoder is configured to use cube
pruning (Huang and Chiang, 2007) with a limit
of 200 candidates at each node. For optimiza-
tion, we use a learning rate of ?=1, regularization
strength of C=0.01, and a 500-best list for hope
and fear selection (Chiang, 2012) with a single
passive-aggressive update for each sentence (Ei-
delman, 2012).
Baseline Features We used a set of 16 stan-
dard baseline features: rule translation relative
frequency P (e|f), lexical translation probabilities
Plex(e|f) and Plex(f |e), target n-gram language
model P (e), penalties for source and target words,
passing an untranslated source word to the tar-
get side, singleton rule and source side, as well
as counts for arity-0,1, or 2 SCFG rules, the total
number of rules used, and the number of times the
glue rule is used.
2.1 Data preparation
For both languages, we used the provided Eu-
roparl and News Commentary parallel training
data to create the translation grammar neces-
sary for our model. For Russian, we addi-
tionally used the Common Crawl and Yandex
data. The data were lowercased and tokenized,
then filtered for length and aligned using the
GIZA++ implementation of IBM Model 4 (Och
and Ney, 2003) to obtain one-to-many align-
ments in both directions and symmetrized sing the
grow-diag-final-and method (Koehn et al, 2003).
3We approximate corpus BLEU by scoring sentences us-
ing a pseudo-document of previous 1-best translations (Chi-
ang et al, 2009).
We constructed a 5-gram language model us-
ing SRILM (Stolcke, 2002) from the provided
English monolingual training data and parallel
data with modified Kneser-Ney smoothing (Chen
and Goodman, 1996), which was binarized using
KenLM (Heafield, 2011). The sentence-specific
translation grammars were extracted using a suffix
array rule extractor (Lopez, 2007).
For German, we used the 3,003 sentences in
newstest2011 as our Dev set, and report results
on the 3,003 sentences of the newstest2012 Test
set using BLEU and TER (Snover et al, 2006).
For Russian, we took the first 2,000 sentences of
newstest2012 for Dev, and report results on the re-
maining 1,003. For both languages, we selected
1,000 sentences from the bitext to be used as an
additional testing set (Test2).
Compound segmentation lattices As German
is a morphologically rich language with produc-
tive compounding, we use word segmentation lat-
tices as input for the German translation task.
These lattices encode alternative segmentations of
compound words, allowing the decoder to auto-
matically choose which segmentation is best. We
use a maximum entropy model with recommended
settings to create lattices for the dev and test sets,
as well as for obtaining the 1-best segmentation of
the training data (Dyer, 2009).
3 Evaluation
This section describes the experiments we con-
ducted in moving towards a better understanding
of the benefits and challenges posed by large-scale
high-dimensional discriminative tuning.
3.1 Sparse Features
The ability to incorporate sparse features is the pri-
mary reason for the recent move away from Min-
imum Error Rate Training (Och, 2003), as well as
for performing large-scale discriminative training.
We include the following sparse Boolean feature
templates in our system in addition to the afore-
mentioned baseline features: rule identity (for ev-
ery unique rule in the grammar), rule shape (map-
ping rules to sequences of terminals and nontermi-
nals), target bigrams, lexical insertions and dele-
tions (for the top 150 unaligned words from the
training data), context-dependent word pairs (for
the top 300 word pairs in the training data), and
structural distortion (Chiang et al, 2008).
129
Dev Test Test2 5k 10k 25k 50k
en 75k 74k 27k 132k 255k 634k 1258k
de 74k 73k 26k 133k 256k 639k 1272k
Table 1: Corpus statistics in tokens for German.
Dev Test Test2 15k
ru 46k 24k 24k 350k
en 50k 27k 25k 371k
Table 2: Corpus statistics in tokens for
Russian.
Set # features Tune Test
?BLEU ?BLEU ?TER
de-en 16 22.38 22.69 60.61
+sparse 108k 23.86 23.01 59.89
ru-en 16 30.18 29.89 49.05
+sparse 77k 32.40 30.81 48.40
Table 3: Results with the addition of sparse fea-
tures for German and Russian.
All of these features are generated from the
translation rules on the fly, and thus do not have
to be stored as part of the grammar. To allow for
memory efficiency while scaling the training data,
we hash all the lexical features from their string
representation into a 64-bit integer.
Altogether, these templates result in millions of
potential features, thus how to select appropriate
features, and how to properly learn their weights
can have a large impact on the potential benefit.
3.2 Adaptive Learning Rate
The passive-aggressive update used in MIRA has a
single learning rate ? for all features, which along
with ? limits the amount each feature weight can
change at each update. However, since the typical
dense features (e.g., language model) are observed
far more frequently than sparse features (e.g., rule
identity), it has been shown to be advantageous
to use an adaptive per-feature learning rate that
allows larger steps for features that do not have
much support (Green et al, 2013; Duchi et al,
2011). Essentially, instead of having a single pa-
rameter ?,
?? min
(
C, cost(y
?)?w>(f(y+)? f(y?))
?f(y+)? f(y?)?2
)
w? w + ??
(
f(y+)? f(y?)
)
we instead have a vector ? with one entry for each
feature weight:
??1 ? ??1 + ?diag
(
ww>
)
w? w + ??1/2
(
f(y+)? f(y?)
)
?=1 
?=0.01 
?=0.1 
22.2 
22.4 
22.6 
22.8 
23 
23.2 
23.4 
23.6 
23.8 
24 
BLE
U 
Iteration 
Figure 1: Learning curves for tuning when using
a single step size (?) versus different per-feature
learning rates.
In practice, this update is very similar to that of
AROW (Crammer et al, 2009; Chiang, 2012).
Figure 1 shows learning curves for sparse mod-
els with a single learning rate, and adaptive learn-
ing with ?=0.01 and ?=0.1, with associated re-
sults on Test in Table 4.4 As can be seen, using
a single ? produces almost no gain on Dev. How-
ever, while both settings using an adaptive rate fare
better, the proper setting of ? is important. With
?=0.01 we observe 0.5 BLEU gain over ?=0.1 in
tuning, which translates to a small gain on Test.
Henceforth, we use an adaptive learning rate with
?=0.01 for all experiments.
Table 3 presents baseline results for both lan-
guages. With the addition of sparse features, tun-
ing scores increase by 1.5 BLEU for German, lead-
ing to a 0.3 BLEU increase on Test, and 2.2 BLEU
for Russian, with 1 BLEU increase on Test. The
majority of active features for both languages are
rule id (74%), followed by target bigrams (14%)
and context-dependent word pairs (11%).
3.3 Feature Selection
As the tuning set size increases, so do the num-
ber of active features. This may cause practi-
cal problems, such as reduced speed of computa-
tion and memory issues. Furthermore, while some
4All sparse models are initialized with the same tuned
baseline weights. Learning rates are local to each mapper.
130
Adaptive # feat. Tune Test
?BLEU ?BLEU ?TER
none 74k 22.75 22.87 60.19
?=0.01 108k 23.86 23.01 59.89
?=0.1 62k 23.32 22.92 60.09
Table 4: Results with different ? settings for using a per-feature learning rate with sparse features.
Set # feat. Tune Test
?BLEU ?BLEU ?TER
all 510k 32.99 22.36 59.26
top 200k 200k 32.96 22.35 59.29
all 373k 34.26 28.84 49.29
top 200k 200k 34.45 28.98 49.30
Table 5: Comparison of using all features versus
top k selection.
sparse features will generalize well, others may
not, thereby incurring practical costs with no per-
formance benefit. Simianer et al (2012) recently
explored `1/`2 regularization for joint feature se-
lection for SMT in order to improve efficiency and
counter overfitting effects. When performing par-
allel learning, this allows for selecting a reduced
set of the top k features at each iteration that are
effective across all learners.
Table 5 compares selecting the top 200k fea-
tures versus no selection for a larger German and
Russian tuning set (?3.4). As can be seen, we
achieve the same performance with the top 200k
features as we do when using double that amount,
while the latter becomes increasing cumbersome
to manage. Therefore, we use a top 200k selection
for the remainder of this work.
3.4 Large-Scale Training
In the previous section, we saw that learning
sparse features on the small development set leads
to substantial gains in performance. Next, we
wanted to evaluate if we can obtain further gains
by scaling the tuning data to learn parameters di-
rectly on a portion of the training bitext. Since the
bitext is used to learn rules for translation, using
the same parallel sentences for grammar extrac-
tion as well as for tuning feature weights can lead
to severe overfitting (Flanigan et al, 2013). To
avoid this issue, we used a jackknifing method to
split the training data into n = 10 folds, and built
a translation system on n?1 folds, while sampling
sentences from the News Commentary portion of
the held-out fold to obtain tuning sets from 5,000
to 50,000 sentences for German, and 15,000 sen-
tences for Russian.
Results for large-scale training for German are
presented in Table 6. Although we cannot com-
pare the tuning scores across different size sets,
we can see that tuning scores for all sets improve
substantially with sparse features. Unfortunately,
with increasing tuning set size, we see very little
improvement in Test BLEU and TER with either
feature set. Similar findings for Russian are pre-
sented in Table 7. Introducing sparse features im-
proves performance on each set, respectively, but
Dev always performs better on Test.
While tuning on Dev data results in better BLEU
on Test than when tuning on the larger sets, it is
important to note that although we are able to tune
more features on the larger bitext tuning sets, they
are not composed of the same genre as the Tune
and Test sets, resulting in a domain mismatch.
This phenomenon is further evident in German
when testing each model on Test2, which is se-
lected from the bitext, and is thus closer matched
to the larger tuning sets, but is separate from both
the parallel data used to build the translation model
and the tuning sets. Results on Test2 clearly show
significant improvement using any of the larger
tuning sets versus Dev for both the baseline and
sparse features. The 50k sparse setting achieves
almost 1 BLEU and 2 TER improvement, showing
that there are significant differences between the
Dev/Test sets and sets drawn from the bitext.
For Russian, we amplified the effects by select-
ing Test2 from the portion of the bitext that is sepa-
rate from the tuning set, but is among the sentences
used to create the translation model. The effects of
overfitting are markedly more visible here, as there
is almost a 7 BLEU difference between tuning on
Dev and the 15k set with sparse features. Further-
more, it is interesting to note when looking at Dev
that using sparse features has a significant nega-
tive impact, as the baseline tuned Dev performs
131
Tuning Test
?BLEU ?TER
5k 22.81 59.90
10k 22.77 59.78
25k 22.88 59.77
50k 22.86 59.76
Table 8: Results for German with 2 iterations of
tuning on Dev after tuning on larger set.
reasonably well, while the introduction of sparse
features leads to overfitting the specificities of the
Dev/Test genre, which are not present in the bitext.
We attempted two strategies to mitigate this
problem: combining the Dev set with the larger
bitext tuning set from the beginning, and tuning
on a larger set to completion, and then running 2
additional iterations of tuning on the Dev set using
the learned model. Results for tuning on Dev and a
larger set together are presented in Table 7 for Rus-
sian and Table 6 for German. As can be seen, the
resulting model improves somewhat on the other
genre and strikes a middle ground, although it is
worse on Test than Dev.
Table 8 presents results for tuning several ad-
ditional iterations after learning a model on the
larger sets. Although this leads to gains of around
0.5 BLEU on Test, none of the models outperform
simply tuning on Dev. Thus, neither of these two
strategies seem to help. In future work, we plan
to forgo randomly sampling the tuning set from
the bitext, and instead actively select the tuning
set based on similarity to the test set.
4 Conclusion
We explored strategies for scaling learning for
SMT to large tuning sets with sparse features.
While incorporating an adaptive per-feature learn-
ing rate and feature selection, we were able to
use Hadoop to efficiently take advantage of large
amounts of data. Although discriminative training
on larger sets still remains problematic, having the
capability to do so remains highly desirable, and
we plan to continue exploring methods by which
to leverage the power of the bitext effectively.
Acknowledgments
This research was supported in part by the DARPA
BOLT program, Contract No. HR0011-12-C-
0015; NSF under awards IIS-0916043 and IIS-
1144034. Vladimir Eidelman is supported by a
NDSEG Fellowship.
References
S. Chen and J. Goodman. 1996. An empirical study
of smoothing techniques for language modeling. In
ACL.
Colin Cherry and George Foster. 2012. Batch tun-
ing strategies for statistical machine translation. In
NAACL.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In EMNLP.
D. Chiang, K. Knight, and W. Wang. 2009. 11,001
new features for statistical machine translation. In
NAACL-HLT.
D. Chiang. 2012. Hope and fear for discriminative
training of statistical translation models. JMLR,
13:1159?1187.
K. Crammer, A. Kulesza, and M. Dredze. 2009. Adap-
tive regularization of weight vectors. In NIPS.
J. Dean and S. Ghemawat. 2004. MapReduce: Simpli-
fied data processing on large clusters. In OSDI.
J. Duchi, E. Hazan, and Y. Singer. 2011. Adaptive sub-
gradient methods for online learning and stochastic
optimization. JMLR, 12:2121?2159.
C. Dyer, A. Lopez, J. Ganitkevitch, J. Weese,
F. Ture, P. Blunsom, H. Setiawan, V. Eidelman, and
P. Resnik. 2010. cdec: A decoder, alignment, and
learning framework for finite-state and context-free
translation models. In ACL System Demonstrations.
Chris Dyer. 2009. Using a maximum entropy model to
build segmentation lattices for mt. In Proceedings
of NAACL-HLT.
Vladimir Eidelman, Ke Wu, Ferhan Ture, Philip
Resnik, and Jimmy Lin. 2013. Mr. MIRA: Open-
source large-margin structured learning on map-
reduce. In ACL System Demonstrations.
Vladimir Eidelman. 2012. Optimization strategies for
online large-margin learning in machine translation.
In WMT.
Jeffrey Flanigan, Chris Dyer, and Jaime Carbonell.
2013. Large-scale discriminative training for statis-
tical machine translation using held-out line search.
In NAACL.
S. Green, S. Wang, D. Cer, and C. Manning. 2013.
Fast and adaptive online training of feature-rich
translation models. In ACL.
Kenneth Heafield. 2011. KenLM: faster and smaller
language model queries. In WMT.
Mark Hopkins and Jonathan May. 2011. Tuning as
ranking. In EMNLP.
132
Tuning # mappers # features Tune Test Test2
?BLEU ?BLEU ?TER ?BLEU ?TER
Dev 120 16 22.38 22.69 60.61 29.31 54.26
5k 120 16 32.60 22.14 59.60 29.69 52.96
10k 120 16 33.16 22.06 59.43 29.93 52.37
Dev+10k 120 16 19.40 22.32 59.37 30.17 52.45
25k 300 16 32.48 22.21 59.54 30.03 51.71
50k 600 16 32.21 22.21 59.39 29.94 52.55
Dev 120 108k 23.86 23.01 59.89 29.65 53.86
5k 120 159k 33.70 22.26 59.26 30.53 51.84
10k 120 200k 34.00 22.12 59.24 30.51 51.71
Dev+10k 120 200k 19.62 22.42 59.17 30.26 52.21
25k 300 200k 32.96 22.35 59.29 30.39 52.14
50k 600 200k 32.86 22.40 59.15 30.54 51.88
Table 6: German evaluation with large-scale tuning, showing numbers of mappers employed, number of
active features for best model, and test scores on Test and bitext Test2 domains.
Tuning # mappers # features Tune Test Test2
?BLEU ?BLEU ?TER ?BLEU ?TER
Dev 120 16 30.18 29.89 49.05 57.14 32.56
15k 200 16 34.65 28.60 49.63 59.64 30.65
Dev+15k 200 16 33.97 28.88 49.37 58.24 31.81
Dev 120 77k 32.40 30.81 48.40 52.90 36.85
15k 200 200k 35.05 28.34 49.69 59.81 30.59
Dev+15k 200 200k 34.45 28.98 49.30 57.61 32.71
Table 7: Russian evaluation with large-scale tuning, showing numbers of mappers employed, number of
active features for best model, and test scores on Test and bitext Test2 domains.
Liang Huang and David Chiang. 2007. Forest rescor-
ing: Faster decoding with integrated language mod-
els. In ACL.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
NAACL.
Adam Lopez. 2007. Hierarchical phrase-based trans-
lation with suffix arrays. In EMNLP.
Franz Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
In Computational Linguistics.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In ACL.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
BLEU: a method for automatic evaluation of ma-
chine translation. In ACL.
P. Simianer, S. Riezler, and C. Dyer. 2012. Joint fea-
ture selection in distributed stochastic learning for
large-scale discriminative training in SMT. In ACL.
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A study of translation edit rate
with targeted human annotation. In AMTA.
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In ICSLP.
133
