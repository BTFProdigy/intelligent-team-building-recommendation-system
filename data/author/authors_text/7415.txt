Unsupervised Discovery of Persian Morphemes  
Mohsen Arabsorkhi 
Computer Science and Engineering Dept., 
Shiraz University, 
Shiraz, Iran 
marabsorkhi@cse.shirazu.ac.ir
Mehrnoush Shamsfard 
Electrical and Computer Engineering Dept., 
Shahid Beheshti University, 
Tehran, Iran 
m-shams@sbu.ac.ir
Abstract 
This paper reports the present results of a 
research on unsupervised Persian mor-
pheme discovery. In this paper we pre-
sent a method for discovering the mor-
phemes of Persian language through 
automatic analysis of corpora. We util-
ized a Minimum Description Length 
(MDL) based algorithm with some im-
provements and applied it to Persian cor-
pus. Our improvements include enhanc-
ing the cost function using some heuris-
tics, preventing the split of high fre-
quency chunks, exploiting penalty for 
first and last letters and distinguishing 
pre-parts and post-parts. Our improved 
approach has raised the precision, recall 
and f-measure of discovery by respec-
tively %32, %17 and %23. 
1 Introduction 
According to linguistic theory, morphemes are 
considered to be the smallest meaning-bearing 
elements of a language. However, no adequate 
language-independent definition of the word as a 
unit has been agreed upon. If effective methods 
can be devised for the unsupervised discovery of 
morphemes, they could aid the formulation of a 
linguistic theory of morphology for a new lan-
guage. The utilization of morphemes as basic 
representational units in a statistical language 
model instead of words seems a promising 
course [Creutz, 2004].  
Many natural language processing tasks, includ-
ing parsing, semantic modeling, information re-
trieval, and machine translation, frequently re-
quire a morphological analysis of the language at 
hand. The task of a morphological analyzer is to 
identify the lexeme, citation form, or inflection 
class of surface word forms in a language. It 
seems that even approximate automated morpho-
logical analysis would be beneficial for many NL
applications dealing with large vocabularies (e.g. 
text retrieval applications). On the other hand, 
the construction of a comprehensive
morphological analyzer for a language based on
linguistic theory requires a considerable amount
of work by experts. This is both slow and 
expensive and therefore not applicable to all 
languages. Consequently, it is important to 
develop methods that are able to discover and
induce morphology for a language based on 
unsupervised analysis of large amounts of data. 
Persian is the most-spoken of the modern Iranian 
languages, which, according to traditional classi-
fication, with the Indo-Aryan language constitute 
the Indo-Iranian group within the Satem branch 
of the Indo-European family. Persian is written 
right-to-left in the Arabic alphabet with a few 
modifications. Three of 32 Persian letters do 
double duty in representing both consonant and 
vowels: /h/, /v/, /y/, doubling, as /e/ (word fi-
nally), /u/, and /I/ respectively [Mahootian 97]. 
Persian morphology is an affixal system consist-
ing mainly of suffixes and a few prefixes. The 
nominal paradigm consists of a relatively small 
number of affixes [Megerdoomian 2000]. The 
verbal inflectional system is quite regular and 
can be obtained by the combination of prefixes, 
stems, inflections and auxiliaries. Persian mor-
phologically is a powerful language and there are 
a lot of morphological rules in it. For example 
we can derive more than 200 words from the 
stem of the verb ?raftan? (to go). Table 1 shows 
some morphological rules and table 2 illustrates 
some inflections and derivations as examples.  
There is no morphological irregularity in Persian 
and all of the words are stems or derived words, 
except some imported foreign words, that are not 
compatible with Persian rules (such as irregular 
Arabic plural forms imported to Persian.) 
simple past verb past stem + identifier 
continuous present verb Mi+present stem+identifier 
Noun present stem +  (y)e? 
Table 1. Some Persian morphological rules. 
175
POS Persian Translation 
Verb Infinitive Nega?t?n to write 
Present Verb Stem Negar Write 
Past Verb Stem Nega?t wrote 
Continuous Present verb mi-negar-?m I am writing 
Simple Past verb nega?t-?m I wrote 
Noun from verb Neg?re? Writing 
Table 2. Some example words. 
2 Related Works 
There are several approaches for inducing mor-
phemes from text. Some of them are supervised 
and use some information about words such as 
part of speech (POS) tags, morphological rules, 
suffix list, lexicon, etc. Other approaches are un-
supervised and use only raw corpus to extract 
morphemes. In this section we concentrate on 
some unsupervised methods as related works. 
[Monson 2004] presents a framework for unsu-
pervised induction of natural language morphol-
ogy, wherein candidate suffixes are grouped into 
candidate inflection classes, which are then 
placed in a lattice structure. With similar ar-
ranged inflection classes placed near one candi-
date in the lattice, it proposes this structure to be 
an ideal search space in which to isolate the true 
inflection classes of a language. [Schone and Ju-
rafsky 2000] presents an unsupervised model in 
which knowledge-free distributional cues are 
combined orthography-based with information 
automatically extracted from semantic word co-
occurrence patterns in the input corpus.
Word induction from natural language text 
without word boundaries is also studied in 
[Deligne and Bimtol 1997], where MDL- based 
model optimization measures are used. Viterbi or 
the forward- backward algorithm (an EM algo-
rithm) is used for improving the segmentation of 
the corpus. Some of the approaches remove 
spaces from text and try to identify word bounda-
ries utilizing e.g. entropy- based measures, as in 
[Zellig and Harris, 1967; Redlich, 1993].  
[Brent, 1999] presents a general, modular prob-
abilistic model structure for word discovery. He 
uses a minimum representation length criterion 
for model optimization and applies an incre-
mental, greedy search algorithm which is suit-
able for on- line learning such that children 
might employ. 
[Baroni, et al 2002] proposes an algorithm 
that takes an unannotated corpus as its input, and 
a ranked list of probable returning related pairs 
as its output. It discovers related pairs by looking 
morphologically for pairs that are both ortho-
graphically and semantically similar. 
[Goldsmith 2001] concentrates on stem+suffix-
languages, in particular Indo-European lan-
guages, and produces output that would match as 
closely as possible with the analysis given by a 
human morphologist. He further assumes that 
stems form groups that he calls signatures, and 
each signature shares a set of possible affixes. He 
applies an MDL criterion for model optimiza-
tion.  
3 Inducing Persian Morphemes 
Our task is to find the correct segmentation of 
the source text into morphemes while we don?t 
have any information about words or any struc-
tural rules to make them. So we use an algorithm 
that works based on minimization of some heu-
ristic cost function. Our approach is based on a 
variation of MDL model and contains some 
modifications to adopt it for Persian and improve 
the results especially for this language.  
Minimum Description Length (MDL) analysis is 
based on information theory [Rissanen 1989]. 
Given a corpus, an MDL model defines a de-
scription length of the corpus. Given a probabil-
istic model of the corpus, the description length 
is the sum of the most compact statement of the 
model expressible in some universal language of 
algorithms, plus the length of the optimal com-
pression of the corpus, when we use the prob-
abilistic model to compress the data. The length 
of the optimal compression of the corpus is the 
base 2 logarithm of the reciprocal of the prob-
ability assigned to the corpus by the model. 
Since we are concerned with morphological 
analysis, we will henceforth use the more spe-
cific term the morphology rather than model. 
(1)
)|(log)(log
),(
22 MCpMp
MModelCCorpusnLengthDescriptio

 
MDL analysis proposes that the morphology M 
which minimizes the objective function in (1) is 
the best morphology of the corpus. Intuitively, 
the first term (the length of the model, in bits) 
expresses the conciseness of the morphology, 
giving us strong motivation to find the simplest 
possible morphology, while the second term ex-
presses how well the model describes the corpus 
in question.  
The method proposed at [Creutz 2002; 2004] is a 
derivation of MDL algorithm which we use as 
the basis of our approach. In this algorithm, each 
time a new word token is read from the input, 
different ways of segmenting it into morphs are 
evaluated, and the one with minimum cost is se-
lected. First, the word as a whole is considered to 
176
be a morph and added to the morph list. Then, 
every possible splits of the word into two parts 
are evaluated. The algorithm selects the split (or 
no split) that yields the minimum total cost. In 
case of no split, the processing of the word is 
finished and the next word is read from input. 
Otherwise, the search for a split is performed 
recursively on the two segments. The order of 
splits can be represented as a binary tree for each 
word, where the leaves represent the morphs 
making up the word, and the tree structure de-
scribes the ordering of the splits.  
During model search, an overall hierarchical data 
structure is used for keeping track of the current 
segmentation of every word type encountered so 
far. There is an occurrence counter field for each 
morph in morph list. The occurrence counts from 
segments flow down through the hierarchical 
structure, so that the count of a child always 
equals the sum of the counts of its parents. The 
occurrence counts of the leaf nodes are used for 
computing the relative frequencies of the 
morphs. To find out the morph sequence that a 
word consists of, we look up the chunk that is 
identical to the word, and trace the split indices 
recursively until we reach the leaves, which are 
the morphs. This algorithm was applied on Per-
sian corpus and results were not satisfiable. So 
we gradually, applied some heuristic functions to 
get better results. Our approach contains (1) Util-
izing a heuristic function to compute cost more 
precisely, (2) Using Threshold to prevent split-
ting high frequency chunks, (3) Exerting Penalty 
for first and last letters and (4) Distinguishing 
Pre-parts and post-parts. 
After analyzing the results of the initial algo-
rithm, we observed that the algorithm tries to 
split words into some morphemes to keep the 
cost minimum based on current morph list so 
recognized morphemes may prevent extracting 
new correct morphemes. Therefore we applied a 
new reward function to find the best splitting 
with respect to the next words. In fact our func-
tion (equation (2)) rewards to the morphemes 
that are used in next words frequently.  
(2)  }/)1)((*)({ WNLPlenLPfreqRF
          CWNRPlenRPfreq *}/)1)((*)({ 
In which LP is the left part of word, RP is the 
right part of it, Len (p) is the length of part P 
(number of characters), freq(p) is the frequency 
of part P in corpus, WN is the number of words 
(corpus size) and C is a constant number. 
In this cost function freq(LP)/WN can be inter-
preted as the probability of LP being a morph in 
the corpus. We use len(P) to increase the reward 
for long segments that are frequent and it is de-
creased by 1 to avoid mono-letter splitting. We 
found the parameter C empirically. Figure 1 
shows the results of the algorithm for various 
amounts of C.  
40
50
60
70
1 2 3 4 5 6 7 8 9 10
Recall
Precision
f-measure
Figure 1. Algorithm results for various Cs. 
Our experiments showed that the best value for C 
is 8. It means that RP is 8 times more important 
that LP. This may be because of the fact that Per-
sian is written right-to-left and moreover most of 
affixes are suffixes. 
The final cost function in our algorithm is shown 
in equation (3). 
(3) RFEF ' 
In which E is the description length, calculated in 
equation (1) and RF the cost function described 
in equation (2). Since RF values are in a limited 
range, they are large numbers (in comparison 
with other function values) in the first iterations, 
but after processing some words, cost function 
values will become large so that the RF is not 
significant any more. So we used the difference 
of cost function in two sequential processes (two 
iterations) instead of the cost function itself. In 
other words in our algorithm the cost function 
(E) is re-evaluated and replaced with its changes 
(?E). This improvement causes better splitting in 
some words such as the words shown in table 3. 
(Each word is shown by its written form in Eng-
lish alphabet : its pronunciation (its translation)).    
word Initial alg.  Improved alg. 
?n: ?en (sand) ?n ?n
?nva: ?en?va 
(that can hear)
?n + va ?nv (hear) + 
 a (subjective 
adjective sign)
mi-?nvm: 
mi-?en?v?m 
(I hear)
mi (continuous 
tense sign) + 
?n + v + m
mi + ?nv + m 
(first person 
pronoun)
Table 3. Comparing the results of the initial and  
               improved algorithm. 
We also used a frequency threshold T to avoid 
splitting words that are observed as a substring in 
other words. It means that in the current algo-
rithm, for each word we first compute its fre-
quency and it will be splitted just when it is used 
177
less than the threshold. Based on our experi-
ments, the best value for T is 4.One of the most 
wrong splitting is mono-letter splitting which 
means that we split just the first or the last letter 
to be a morpheme. Our experiments show that 
the first letter splitting occurs more than the last 
letter. So we apply a penalty factor on splitting in 
these positions to avoid creating mono-letter 
morphemes.  
Another improvement is that we distinguished 
between pre-part and post-part. So splitting 
based on observed morphemes will become more 
precise. In this process each morpheme that is 
observed at the left corner of a word, in the first 
splitting phase, is post-part and each of them at 
the right corner of a word is pre-part. Other mor-
phemes are added to both pre and post-part lists. 
4 Experimental Results 
We applied improved algorithm on Persian cor-
pus and observed significant improvements on 
our results. Our corpus contains about 4000 
words from which 100 are selected randomly for 
tests. We split selected words to their morphemes 
both manually and automatically and computed 
precision and recall factors. For computing recall 
and precision, we numerated splitting positions 
and compared with the gold data. Precision is the 
number of correct splits divided to the total num-
ber of splits done and recall is the number of cor-
rect splits divided by total number of gold splits. 
Our experiments showed that our approach re-
sults in increasing the recall measure from 45.53 
to 53.19, the precision from 48.24 to 63.29 and f-
measure from 46.91 to 57.80. Precision im-
provement is significantly more than recall. This 
has been predictable as we make algorithm to 
prevent unsure splitting. So usually done splits 
are correct whereas there are some necessary 
splitting that have not been done. 
5 Conclusion 
In this paper we proposed an improved approach 
for morpheme discovery from Persian texts. Our 
algorithm is an improvement of an existing algo-
rithm based on MDL model. The improvements 
are done by adding some heuristic functions to 
the split procedure and also introducing new cost 
and reward functions. Experiments showed very 
good results obtained by our improvements. 
The main problems for our experiments were the 
lack of good, safe and large corpora and also 
handling the foreign words which do not obey 
the morphological rules of Persian.
Our proposed improvements are rarely language-
dependent (such as right-to-left feature of Per-
sian) and could be applied to other languages 
with a little customization. To extend the project 
we suppose to work on some probabilistic distri-
bution functions which help to split words cor-
rectly. Moreover we plan to test our algorithm on  
large Persian and also English corpora. 
References 
Marco Baroni, Johannes Matiasek, Harald Trost 2002. 
Unsupervised discovery of morphologically related 
words based on orthographic and semantic similar-
ity, ACL Workshop on Morphological and 
Phonological Learning.
Michael R. Brent. 1999. An efficient, probabilistically 
sound algorithm for segmentation and word dis-
covery, Machine Learning, 34:71?105. 
Mathias Creutz, Krista Lagus, 2002. Unsupervised 
discovery of morphemes. Workshop on Morpho-
logical and Phonological Learning of ACL?02, 
Philadelphia, Pennsylvania, USA, 21?30. 
Mathias Creutz, Krista Lagus, 2004. Induction of a 
simple morphology for highly inflecting languages. 
Proceedings of 7th Meeting of SIGPHON, Bar-
celona. 43?51 
S. Deligne and F. Bimbot. 1997. Inference of vari-
able-length linguistic and acoustic units by multi-
grams. Speech Communication, 23:223?241. 
John Goldsmith, 2001. Unsupervised learning of the 
morphology of a natural language, Computational 
Linguistics, 27(2): 153?198 
Zellig. Harris, 1967. Morpheme Boundaries within 
Words: Report on a Computer Test. Transforma-
tions and Discourse Analysis Papers, 73.
Shahrzad Mahootian, 1997. Persian, Routledge. 
Karine Megerdoomian, 2000 Persian Computational 
Morphology: A unification-based approach, 
NMSU, CLR, MCCS Report. 
Christian Monson. 2004. A Framework for Unsuper-
vised Natural Language Morphology Induction, 
The Student Workshop at ACL-04. 
A. Norman Redlich. 1993. Redundancy reduction as a 
strategy for unsupervised learning. Neural Com-
putation, 5:289?304. 
Jorma Rissanen 1989, Stochastic Complexity in 
Statistical Inquiry, World Scientific. 
P. Schone and D. Jurafsky. 2000. Knowldedge-free 
induction of morphology using latent semantic 
analysis, Proceedings of the Conference on 
Computational Natural Language Learning.
178
A Bottom up Approach to Persian Stemming
Amir Azim Sharifloo
NLP Research Lab,
Department of Electrical &
Computer Engineering,
Shahid Beheshti University,
Tehran, Iran
a.sharifloo@mail.sbu.ac.ir
Mehrnoush Shamsfard
NLP Research Lab,
Department of Electrical &
Computer Engineering,
Shahid Beheshti University,
Tehran, Iran
m-shams@sbu.ac.ir
Abstract
Stemmers have many applications in natu-
ral language processing and some fields
such as information retrieval. Many algo-
rithms have been proposed for stemming.
In this paper, we propose a new algorithm
for Persian language. Our algorithm is a
bottom up algorithm that is capable to re-
organize without changing the implementa-
tion. Our experiments show that the pro-
posed algorithm has a suitable result in
stemming and flexibility.
1 Introduction
In linguistics, stem is a form that unifies the ele-
ments in a set of morphologically similar words
(Frakes and Yates, 1992), therefore stemming is
the operation which determines the stem of a given
word. In other words, the goal of a stemming algo-
rithm is to reduce variant word forms to a common
morphological root, caled ?stem? (Bacchin et al,
2002).
There are three common approaches that are used
in stemming: affix stripping, lookup table and sta-
tistical methods (Bento et al, 2005). Affix strip-
ping dependends on the morphological structure of
the language. The stem is obtained by removing
some morphemes from the one or both sides of the
word. Porter algorithm (Porter, 1980) is an exam-
ple of this kind of algorithms. This stemmer is
made up of five steps, during which certain rules
are applied to the words and the most common suf-
fixes are removed.
In lookup table approach, each word and its re-
lated stem are stored in some kind of structured
form. Consequently, for each stored word, we find
its stem. However, the approach needs more space.
Also, for each new word, table must be updated
manually.
In statistical methods, through a process of in-
ference and based on a corpus, rules are formulated
regarding word formation. Some of the method-
ologies adopted are: frequency counts, n-gram
(Mayfield and McNamee, 2003), link analysis
(Bacchin et al, 2002), and Hidden Markov Models
(Melucci and Orio, 2003). This approach does not
require any linguistic knowledge whatsoever, be-
ing totally independent of the morphological struc-
ture of the target language.
In this paper, we propose a new algorithm for
stemming in Persian. Our algorithm is rule based
and in contrast with affix stripping approach, it is a
stem based approach. That means, at first we find
possible stems in the word, after that we check
which stems are matched with rules.
Our algorithm is bottom up while affix stripping
methods are top down. In other words, we try to
generate the word using candidate stems of the
word which we call cores of the word. If the word
is generated, the stem is correct. On the other hand,
affix stripping approaches try to removing affixes
until reaching to any stem in the word.
Some stemming methods have been presented
for Persian (Taghva et al, 2005) which use affix
stripping approach. Our proposed method tries to
reach better precision rather than previous methods.
Also, this method tokenizes the word to mor-
phemes which could employ in other morphologi-
cal methods.
The paper is organized as follows: section 2 pre-
sents a brief review of Persian from morphological
perspective; in section 3, we describe the proposed
583
algorithm in details; section 4 is about our experi-
ments.
2 Persian from a Morphological Perspec-
tive
Persian is an Indo-European language, spoken and
written primarily in Iran, Afghanistan, and a part of
Tajikistan. It is written from right to left in the
Arabic-like alphabet.
In Persian, verbs involve tense, number and
person. For example1, the verb ??? ??????(mi-x?n-
am: I read) is a present tense verb consisting of
three morphemes. ??? (am) is a suffix denoting first
single person ?????? (x?n) is the present tense root
of the verb and ???? (mi) is a prefix that expresses
continuity.
If a verb has any object pronoun, it can be at-
tached to the end of the verb such as ? ???????? ? 
(mi-x?n-am-a?: I read it) in which ??? (a?: it) is an
object pronoun. Also, negative form of verbs is
produced with adding ??? (ne) to the first of them.
For example, ? ???????? ? (ne-mi-x?n-am - I don?t 
read) is the negative form of the verb ? ??????? ?
(mix?nam - I read). We have gathered 43 rules for
verbs, some of them are shown in Table .1.
Table 1. Some rules for verbs in Persian
Rule example
 ????? ?????+?? ????? +??
(present person identifier +
present root + mi)
?? ?????
(mi-x?n-am)
(I read)
 ????? ????+??? +? +?? ????
(past person identifier + bud
+eh + past root)
???? ????
(raft-e bud-am)
(I had gone)
 ?? ?????+?
(present root + b)
????
(be-gozar)
(Pass)
??+? +? ?? ???
(shod + h + past root)
?????? ??
(xand-e ?od)
(it was read)
Nouns are more challengeable than others in
Persian. We have gathered many rules for nouns
that in following, we describe one of them. The
plural forms of nouns are formed by adding the
suffixes ( ??, ??, ??, ?? ,??). ????(h?) is used for all
1 Through the paper, we show Persian examples by their
writen form in Persian alphabet between ?? followed by
(their pronunciation: translation).
words. ???? (?n) is used for humans, animals and
every thing that is alive. Also, ???, ?? ,??? (?t ,un ,
in) is used for some words borrowed from Arabic
and some Persian words. We have another kind of
plural form in Persian that is called Mokassar
which is a derivational plural form (irregulars in
Persian). Some examples of plural form are shown
in Table 2.
Also, there are some orthographic rules which
show the effects of joining affixes to the word. For
example, consider that we have two parts of a word:
A and B for joining as BA (Consider, Persian is
written right to left). If the last letter of A and the
first leter of B are ???(?), one leter ???(y) is
added between them. Assume A is ??????(d?n? -
wise) and B is ????(?n), the joining result is ?????????
(d?n?-y?n: wise people).
Table 2. Some kinds of plural form in Persian
Joining Result noun
 ??+????
(h? + ke?var)
(h? + country)
??????
(ke?var-h?)
(countries)
 ??+????
(h? + deraxt)
(h? + tree)
??????
(deraxt-?n)
(trees)
(Mokassar form)???
(kotob)
(books)
???
(kotob)
(books)
 ??+? +???
(?n + y + ?gh?)
(?n + y + mister)
??????
(?gh?-y?n)
(men)
3 The Proposed Algorithm
Our algorithm is rule based and bottom up. At first,
it tries to find substrings of the word that are stems
or morphemes which are derived from any stem,
we call them cores. After that, it joins each of
cores with other elements of word for generating
that word according to available rules. Finally,
each core with at least one correct generation is a
correct core and its stem is correct stem of the
word. The algorithm includes three phases: 1. Sub-
string tagging 2. Rule matching 3. Anti rule match-
ing (Figure 1).
584
In substring tagging phase, we extract morpho-
logical information for all possible substrings of
the word. At the end of this phase, we know which
substrings of the word are morphemes and which
ones are not. Also, we know clusters that each
morpheme is their member. We use clusters for
rule matching phase. Accordingly, we know cores
in the word before beginning the second phase. We
describe substring tagging details in section 3.1.
Figure1. Three phases of the proposed algorithm.
In rule matching phase, for each core that has been
known in previous phase, we extract related rules.
For example, ??????(x?n) is one core of the word
? ??????? ? (mi-x?n-am: I read) and ??? ?????? (bone
moz?re: present root) is one of clusters that ?????? 
(x?n) is its member. Also,??? (am) is a member of
cluster ?????? ??????(?enase moz?re: present per-
son identifier)and ???? (mi) is a member of cluster
????(mi). We have a rule in rules repository as:
(  ????? ?????+?? ????? +?? )
(present person identifier + present root + mi)
where it is matched with the word ? ??????? ?(mi-
x?n-am: I read). Therefore, we find a matched rule
for ??????(x?n). At the end of second phase, each
core that has extracted any possible rule for the
word, remains in cores list and other cores are re-
moved from it.
In anti-rule matching phase, we extract anti rules
from anti rules repository for each core in the list.
Each core which has any matched anti rule with
the word morphemes, is removed from the cores
list. At the end of the third phase, each stem of any
core in the cores list is the correct stem for the
word.
3.1 Substring Tagging
Every word with length N has N*(N+1)/2 sub-
strings. Therefore, we need N*(N+1)/2 string
matching for finding them in morphemes reposi-
tory. We employ a Trie tree for storing morphemes
and present an algorithm for retrieving morpho-
logical information from it that reduces the number
of string matching. This algorithm needs N(N+1)/2
character matching (instead of string matching) at
most. A simplified part of tree is shown in Figure 2.
Figure 2. A simplified part of Trie tree that is used
for storing morphological information.
The algorithm is described in the following:
We initiate N pointers (N is the word length)
that they point to the tree root. Also, we use a
counter C that is an index on the word. At first, C?s 
value is one that means its related letter is first let-
ter of the word. At the end of each step, C is in-
creased by one. Therefore, in each step, C points to
one letter of the word that we call this letter L.
xu
x
x?
x?n
m ????
mi
clusters
1.present person
identifier.
2. past person
identifier.
?/ m
tree root
clusters
1.verb suffix mi
2.noun
clusters
1.present root
2.noun
cluster
1.noun
cluster
---
cluster
---
?/ x
?/ i ?/ u
?/ ?
?/ n
Substring Tagging
Rule Matching
Anti Rule Matching
Stems
Cores List
Cores List
Input Word
585
At first step, first pointer P1 finds one edge be-
tween root edges that its letter is equal with L. P1
goes down on tree by that edge. Here, P1 extract
morphological information from its new position (a
node of the tree) and fills morphological informa-
tion for substring (1, 2).
At the second step, L is the second letter of the
word, second pointer P2 finds one edge between
root edges that its letter is equal with L. P2 goes
down on tree by that edge, extract morphological
information from its new position (a node of the
tree) and fills morphological information for sub-
string (2, 3). Also, P1 goes down on tree by an
edge contained L, from its position that it is one of
root children and fills morphological information
for substring (1, 3).At third step, L is third letter of
the word. Third pointer P3 starts from root and
goes down on tree by an edge that its letter is equal
with L and fills morphological information for sub-
string (3, 4). P1, P2 repeat this work from their
positions and fill morphological information for
substring (1, 4) and (2, 4) respectively.
Next steps are done like these steps. Finally, we
have obtained morphological information for all
substrings of the word. Also, if one pointer could
not find any edge with value L, it is blocked until
the end of algorithm. Figure 3 shows pseudo code
of this algorithm.
Figure 3. The used algorithm for obtaining mor-
phological information from Trie tree.
3.2 Rule Matching
We use many rules to generate correct words by
morphemes in Persian. We store these rules in
rules repository. Some gathered rules are shown in
Table 3.
Table 3. Some gathered rules that we use.
Rule
 ????? ???? +?? ???? ????? ????
(past person identifier + past root? sim-
ple past)
 ?? ????? +? ????
(present root + b? imperative)
 ? ?+??? ???? ??? 
(h? + noun? plural noun)
 ??+??? ?????? ???? ?????? ??? 
(?n + alive noun? alive plural noun)
Each rule is a sequence of clusters. A cluster
represents a set of morphemes that affects role of
them in the word. In other words, each morpheme
could be applied as one or more roles for generat-
ing words. So, each role can be a cluster member-
ship. For example, in English, ?book? is a verb and 
a noun. But, As a noun, it has a plural form (books)
and as a verb, it has a past form (booked).
Similarly, in Persian, the word????? (mord: die)
is a verb root and ??????? (mord-and: They died) is
a verb, too. Also, ????? (mard: man) is a noun and
??????? (mard-h?: men) is one of its plural forms. In
consequence, we put ?????in both of cluster ????? 
(esm: noun) and ??? ?????(bone m?zi: past root).
We create a new cluster when a rule needs it and
that cluster is not in clusters repository.
As we discussed about it, in Persian, we have
several suffixes for plural form that every one is
used for a set of nouns. The suffix ???? (h?) is used
for every noun and the suffix ????(?n) is special
for everything that is alive. Other suffixes are ap-
Word: string;
P: array of pointer with word.length size;
for C = 1 to word.length do
{
for i = 1 to C do
{
If (P[i] <> Blocked)
{
edge = find_edge( P[i] , L );
// find_edge finds a edge from its position
// in tree that its letter is equal with L.
if ( edge <> null )
{
GoDown(P[i],edge);
substring(i, C).mInfo = P[i]-> mInfo;
// mInfo is morphological Information
}
else P[i] = Blocked;
}
}
}
586
plied for some words borrowed from Arabic and
some Persian words. A noun such as ????? (pesar:
boy)has several plural forms (e.g. ???????/pesar-h?,
???????/pesar-?n). Therefore, we employ clusters
for organizing this situation. For example, we put
the morpheme ?????(pesar: boy) in cluster ?????
(esm: noun) and ???????? (j?nd?r: alive). Also, we
have two rules in rules repository:
?????+????
(h? + noun)
and
????????+????
(?n + alive)
The morpheme ?????(pesar: boy) is a member of
both clusters ????? (esm: noun) and???????? (j?nd?r:
alive). Accordingly, these words ??????? (pesar-h?:
boys) and??????? (pesar-?n: boys) are correct form
and their stem is ????? (pesar: boy). But about the
morpheme ??????(ket?b: book), it is a noun and a
member of cluster ?????(esm: noun) but it is not a
member of cluster ???????? (j?nd?r: alive). So,
???????? (ket?b-h?: books) is a correct form and its
stem is ??????(ket?b: book). In contrast, ???????? 
(ket?b-?n) is a wrong form and??????(ket?b: book)
is not its stem. Also, we organize suffixes in simi-
lar cluster such as cluster ?????? ?????? (?enase 
moz?re: present person identifier), ???? ??? ???? 
(harfe nafye fel). Table 4 shows some clusters.
Table 4. Some clusters that we use.
Cluster Cluster
????? ?????
(present person identifier)
?? ?????
(present root)
????? ??? ??
(plural suffix h?)
?? ????
(past root)
????? ??? ??
(plural suffix ?n)
???
(noun)
At the end of this phase, each core must have a
rule that it can generate the word. Otherwise, it is
removed from cores list.
3.3 Anti Rule Matching
This phase is similar previous phase with a small
difference. Like previous phase, we have a rules
repository, but these rules are not applied in Per-
sian. In fact, these rules are exceptions of previous
phase rules. For example, we have a rule in rules
repository:
( ??+ ????????? )
(?n + alive noun)
On the other hand, there is an exception for this
rule. Every noun with the final leter ??? (he) can
not use this rule. For example, ??????? (parand-e:
bird) is a kind of animals with the final leter ???
(he) and theword ???????? ? (parand-e-?n) is a wrong
word in Persian. We call these exceptional rules
?Anti rules?.
The details of this phase: Each core from cores
list retrieves the anti rules that they involve it. Af-
ter that, each retrieved anti rule is checked with the
morphemes in the word for possibility of word
generation. Until now, all things were similar pre-
vious phase, but the difference is here. If there is
any anti rule related to a rule of any core, that rule
is removed from candidate rule list of that core. At
the end of this phase, each core must have at least
one rule that it can generate the word. Otherwise, it
is removed from cores list. Finally, remained cores
in cores list have correct stems of the word.
We have gathered a set of anti rules in a reposi-
tory that each anti rule is related to a rule in rule
repository. Some of these anti rules are shown in
Table 5.
Table 5. Some gathered anti rules that we use.
Anti Rule
( ??+????? ?? ???? ?????? )
(an + alive noun ended with h)
( ??+? ? ??? ????? ?? ?? ?? ? )
(at + noun ended with ?,u,h, y)
4 Experiments and Results
The most primitive method for assessing the per-
formance of a stemmer is to examine its behavior
when applied to samples of words - especially
words which have already been arranged into 'con-
flation groups'. This way, specific errors (e.g., fail-
587
ing to merge "maintained" with "maintenance", or
wrongly merging "experiment" with "experience")
can be identified, and the rules adjusted accord-
ingly.
We evaluated the proposed algorithm with a
limited corpus of Hamshahri newspaper. We
started with 252 rules and 20 anti rules. The algo-
rithm retrieved 90.1 % of word stems correctly.
The failed words are related to absence of some
rules in rule repository or stems in Trie tree. Some
of words in the corpus are shown in Table 6.
Table 6. Some of words in Hamshahri newspa-
per corpus
Stem Word
?????
(m?jar?)
(event)
??????
(m?jar?-ye)
(event of)
???
(rasm)
(custom)
?????
(rasm-h?)
(customs)
?????
(padide)
(phenomenon)
????? ???
(padid-e-h?-ye)
(phenomenons of)
????
(bud)
(to be)
?????
(bud-and)
(They were)
????
(s?at)
(watch)
??????????
(s?at-h?-ye-??n)
(watch)
?????
(ke?idan)
(to draw)
?????
(be-ke?a-and)
???
(?xar)
(end)
?????
(?xar-in)
(last)
????
(raftan)
(going)
????? ?????
(na-raft-e budand)
(They had not gone)
???
(s?l)
(year)
?????
(em-s?l)
(this year)
??????
(mot?le'e)
(study)
???????
(mot?le-at)
(studies)
?????
(mantaghe)
(area)
?????
(man?tegh)
(areas)
One of words could not be handle with our algo-
rithm is ???????(j?-be-j? - exchange). We discov-
ered related rule for that and added it to rules re-
pository. Therefore, if we evaluate the algorithm,
the result will be better. Rules repository evolves
and the algorithm result will be better without any
change of program and code compilation.
5 Conclusion
In this paper, we proposed a bottom up method to
stem Persian words. The main purpose of this
method is high precision stemming based on mor-
phological rules. The experiments show that it has
suitable results in stemming and presents possibil-
ity of evolution easily.
References
Bento, Cardoso and Dias. 2005. Progress in Artificial
Intellegence, 12th Portuguese Conference on Artifi-
cial Intelligence, pages 693?701.
Chris Paice. 1996. Method for Evaluation of Stemming
Algorithms Based on Error Counting. JASIS , pages
632-649.
Frakes and Yates. 1992. Information Retrieval: Data
Structures and Algorithms. Prentice Hall, NJ.
Mayfield and McNamee. 2003. Single N-gram Stem-
ming. In Proceedings of the 26th annual international
ACM SIGIR conference on Research and develop-
ment in information. retrieval, pages 415-416.
Melucci and Orio. 2003. A Novel Method for Stemmer
Generation Based on Hidden Markov Models. In
Proceedings of Conference on Information and
Knowledge Management (CIKM03), pages 131-138.
Michela Bacchin, Nicola Ferro, and Massimo Melucci.
2002. Experiments to evaluate a statistical stemming
algorithm. Working Notes for CLEF 2002, pages
161-168.
Michela Bacchin, Nicola Ferro, and Massimo Melucci.
2002. The Effectiveness of a Graph-Based Algorithm
for Stemming. Springer-Verlag Berlin Heidelberg.
pages 117?128.
Porter. An Algorithm for Suffix Stripping. 1980. Pro-
gram. pages 130-137.
Taghva, Beckley and Sadeh. 2005. A stemming algo-
rithm for the Farsi language. IEEE ITCC 2005,
pages 158 - 162.
588
