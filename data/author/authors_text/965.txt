A Representation for Complex and Evolving Data Dependencies 
in Generation 
C Me l l i sh  $, R Evans  t, L Cah i l l  t, C Doran  t, D Pa iva  t, M Reape $, D Scot t  t, N T ipper  t 
t Information Technology Research Institute, University of Brighton, Lewes Rd, Brighton, UK 
SDivision of Informatics, University of Edinburgh, 80 South Bridge, Edinburgh, UK 
rags@itri, brighton, ac. uk 
http :/www. itri. brighton, ac. uk/proj ect s/rags 
Abst rac t  
This paper introduces an approach to represent- 
ing the kinds of information that components 
in a natural language generation (NLG) sys- 
tem will need to communicate to one another. 
This information may be partial, may involve 
more than one level of analysis and may need 
to include information about the history of a 
derivation. We present a general representation 
scheme capable of handling these cases. In ad- 
dition, we make a proposal for organising inter- 
module communication i an NLG system by 
having a central server for this information. We 
have validated the approach by a reanalysis of 
an existing NLG system and through a full im- 
plementation of a runnable specification. 
1 In t roduct ion  
One of the distinctive properties of natural an- 
guage generation when compared with other 
language ngineering applications i that it has 
to take seriously the full range of linguistic rep- 
resentation, from concepts to morphology, or 
even phonetics. Any processing system is only 
as sophisticated as its input allows, so while a 
natural language understanding system might 
be judged primarily by its syntactic prowess, 
even if its attention to semantics, pragmatics 
and underlying conceptual analysis is minimal, 
a generation system is only as good as its deep- 
est linguistic representations. Moreover, any at- 
tempt to abstract away from individual gener- 
ation systems to a more generic architectural 
specification faces an even greater challenge: 
not only are complex linguistic representations 
required, able to support the dynamic evolu- 
tionary development of data during the gener- 
* Now at the MITRE Corporation, Bedford, MA, USA, 
cdoran@mitre, org. 
ation process, but they must do so in a generic 
and flexible fashion. 
This paper describes a representation devel- 
oped to meet these requirements. It offers a 
formally well-defined eclarative representation 
language, which provides a framework for ex- 
pressing the complex and dynamic data require- 
ments of NLG systems. The approach supports 
different levels of representation, mixed repre- 
sentations that cut across levels, partial and 
shared structures and 'canned' representations, 
as well as dynamic relationships between data 
at different stages in processing. We are using 
the approach to develop a high level data model 
for NLG systems as part of a generic generation 
architecture called RAGS 1. 
The framework has been implemented in the 
form of a database server for modular genera- 
tion systems. As proof of concept of the frame- 
work, we have reimplemented an existing NLG 
system. The system we chose was the Caption 
Generation System (CGS) (Mittal et al, 1995; 
Mittal et al, 1998). The reimplementation in- 
volved defining the interfaces to the modules of 
CGS in terms of the RAGS representations and 
then implementing modules that had the requi- 
site input and output representations. 
Generation systems, especially end-to-end, 
applied generation systems, have, unsurpris- 
ingly, many things in common. Reiter (1994) 
proposed an analysis of such systems in terms 
of a simple three stage pipeline. More recently, 
the RAGS project attempted to repeat he anal- 
1This work is supported by ESPRC grants 
GR/L77041 (Edinburgh) and GR/L77102 (Brighton), 
RAGS: Reference Architecture for Generation Systems. 
We would also like to acknowledge the contribution of 
Jo Calder to the ideas and formalisation described in 
this paper. In particular, parts of this paper are based 
on (Calder et al, 1999). 
119 
ysis (Cahill et al, 1999a), but found that while 
most systems did implement a pipeline, they 
did not implement the same pipeline - different 
functionalities occurred in different places and 
different orders in different systems. In order 
to accommodate his result, we sought to de- 
velop an architecture that is more general than 
a simple pipeline, and thus supports the range 
of pipelines observed, as well as other more com- 
plex control regimes (see (Cahill et al, 1999a; 
Cahill et al, 1999b)). In this paper, we argue 
that supporting such an architecture requires 
careful consideration of the way data represen- 
tations interact and develop. Any formal frame- 
work for expressing the architecture must take 
account of this. 
2 The  representat iona l  requ i rements  
o f  generat ion  sys tems 
We noted in the introduction that generation 
systems have to deal with a range of linguis- 
tic information. It is natural, especially in the 
context of a generic architecture proposal, to 
model this breadth in terms of discrete layers 
of representation: (1999a) introduce layers such 
as conceptual, semantic, rhetorical, syntactic 
and document structure, but the precise demar- 
cation is not as important here as the princi- 
ple. The different kinds of information are typi- 
cally represented differently, and built up sepa- 
rately. However the layers are far from indepen- 
dent: objects at one layer are directly related to 
those at others, forming chains of dependency 
from conceptual through rhetorical and seman- 
tic structure to final syntactic and document re- 
alisation. This means that data resources, such 
as grammars and lexicons, and processing mod- 
ules in the system, are often defined in terms of 
mixed  data: structures that include informa- 
tion in more than one representation layer. So 
the ability to represent such mixed structures 
in a single formal framework is an important 
property of a generic data proposal. 
In addition, it is largely standard in gener- 
ation as elsewhere in language applications, to 
make extensive use of par t ia l  representations, 
often using a type system to capture grades of 
underspecification. An immediate corollary of 
providing support for partial structures is the 
notion that they may become further specified 
over time, that data structures evolve. If the 
framework seeks to avoid over-commitment to 
particular processing strategies it needs to pro- 
vide a way of representing such evolution ex- 
plicitly if required, rather than relying on de- 
structive modification of a structure. Related 
to this, it should provide explicit support for 
representing a l te rnat ive  specifications at any 
point. Finally, to fully support efficient pro- 
cessing across the range of applications, from 
the simple to the most complex, the represen- 
tation must allow for compact sharing of infor- 
mation in tang led  structures (two structures 
which share components). 
In addition to these direct requirements of the 
generation task itself, additional requirements 
arise from more general methodological consid- 
erations: we desire a representation that is for- 
mally well  def ined,  allows for theoretical rea-  
son ing about the data and performance of sys- 
tems, and supports control regimes from simple 
deterministic pipelines to complex parallel ar- 
chitectures. 
3 The  Representat ion  Scheme 
In this section, we present our proposal for a 
general representation scheme capable of cover- 
ing the above requirements. Our formulation is 
layered: the foundation is a simple, flexible, rig- 
orously defined graph representation formalism, 
on top of which we introduce notions of com- 
plex types and larger data structures and rela- 
tionships between them. This much is sufficient 
to capture the requirements just discussed. We 
suppose a yet higher level of specification could 
capture a more constraining data model but 
make no specific proposals about this here, how- 
ever the following sections use examples that do 
conform to such a higher level data model. 
The lowest level of the representation scheme 
is: 
? re lat iona l :  the basic data entity is x -~ y, 
an ar row representing a relation from ob- 
ject x to object y; 
? typed:  objects and arrows have an asso- 
ciated type system, so it is possible to de- 
fine classes and subclasses of objects and 
arrows. 
At the most fundamental level, this is more or 
less the whole definition. There is no commit- 
ment to what object or arrow types there are or 
120 
how they relate to each other. So a representa- 
tion allowed by the scheme consists of: 
? a set of objects, organised into types; 
? a set of binary relations, organised into 
types; 
? a set of arrows, each indicating that a rela- 
tion holds between one object and another 
object. 
Sets,  sequences  and  funct ions  
For the next level, we introduce more struc- 
ture in the type system to support sets, se- 
quences and functions. Objects are always 
atomic (though they can be of type set, se- 
quence or function) - it is not possible to make 
an object which actually is a set of two other 
objects (as you might with data structures in a 
computer program). To create a set, we intro- 
duce a set type for the object, and a set mem- 
bership arrow type (el), that links the set's el- 
ements to the set. Similarly, for a sequence, we 
introduce a sequence type and sequence mem- 
ber arrow types (1-el, 2-el, 3-el, . . .  ), and for a 
function, we have a complex type which spec- 
ifies the types of the arrows that make up the 
domain and the range of the function. 
SemRep 
~ fun(Role.SemRep) 
7 V show SemRep SemRep 
Figure 1: The partial semantic representation 
of "The second chart shows the number of days 
on the market" 
As an example, consider Figure 1, which 
shows a semantic representation (SemRep) from 
the CGS reimplementation. Here, the tree 
nodes correspond to objects, each labelled with 
its type. The root node is of type SemRep, and 
although it is not an explicit sequence type, we 
can see that it is a triple, as it has three sequence 
member arrows (with types 1-el, 2-el and 3-el). 
Its first arrow's target is an object of type DR 
(Discourse Referent). Its second represents a set 
of SemPred (Semantic Predicate) objects, and in 
this case there's just one, of type show. Its third 
element is a (partial) function, from Role arrow 
types (agent and affected are both subtypes of 
Role) to SemReps. (In this case, the SemReps 
have not yet been fully specified.) 
Local  and  non- loca l  a r rows  
The second extension to the basic representa- 
tion scheme is to distinguish two different ab- 
stract kinds of arrows - local and non-local. 
Fundamentally we are representing just a homo- 
geneous network of objects and relationships. In 
the example above we saw a network of arrows 
that we might want to view as a single data 
structure, and other major data types might 
similarly appear as networks. Additionally, we 
want to be able to express relationships between 
these larger 'structures' - between structures 
of the same type (alternative solutions, or re- 
vised versions) or of different ypes (semantic 
and syntactic for example). To capture these 
distinctions among arrows, we classify our ar- 
row types as local or non-local (we could do 
this in the type system itself, or leave it as an 
informal distinction). Local arrows are used to 
build up networks that we think of as single 
data structures. Non-local arrows express rela- 
tionships between such data structures. 
All the arrow types we saw above were local. 
Examples of non-local arrows might include: 
real ises These arro~vs link something more ab- 
stract to something less abstract hat re- 
alises it. Chains of realises arrows might 
lead from the original conceptual input to 
the generator through rhetorical, seman- 
tic and syntactic structures to the actual 
words that express the input. 
revises These arrows link a structure to an- 
other one of the same type, which is con- 
sidered to be a 'better' solution - perhaps 
because it is more instantiated. It is impor- 
tant to note that parts of larger structures 
can be revised without revising the entire 
structure. 
coreference These arrows link structures 
which are somehow "parallel" and which 
perhaps hare some substructure, i.e., tan- 
gled structures. For instance, document 
representations may be linked to rhetorical 
representations, either as whole isomorphic 
structures or at the level of individual con- 
stituents. 
121 
Notice that the representation scheme does 
not enforce any kind of well-formedness with 
respect o local and non-local arrows. In fact, 
although it is natural to think of a 'structure' as 
being a maximal network of local arrows with 
a single root object, there's no reason why this 
should be so - networks with multiple roots rep- 
resent tangled structures (structures that share 
content), networks that include non-local links 
might be mixed representations, containing in- 
formation of more than one sort. Such tech- 
niques might be useful for improving generator 
efficiency, or representing canned text or tem- 
plates, cf. (Calder et al, 1999). 
Par t ia l  and  Opaque s t ruc tures  
Partial structures are essential when a module 
needs to produce a skeleton of a representa- 
tion that it does not have the competence to 
completely fill out. For instance, lexical choice 
brings with it certain syntactic commitments, 
but in most NLG systems lexical choice occurs 
some time before a grammar is consulted to 
flesh out syntactic structure in detail. 
Figure 2: A partial structure 
By simply leaving out local arrows, we can 
represent a range of partial structures. Con- 
sider Fig. 2, where the triangles represent local 
structure, representing a sentence object and its 
component verb phrase. There is a link to a sub- 
ject noun phrase object, but none of the local 
arrows of the actual noun phrase are present. In 
subsequent processing this local structure might 
be filled in. This is possible as long as the noun 
phrase object has been declared to be of the 
right type. 
An opaque structure is one which has an in- 
complete derivational history - for example part 
of a syntactic structure without any correspond- 
ing semantic structure. Three possible reasons 
for having such structures are (a) to allow struc- 
ture to be introduced that the generator is not 
capable of producing directly, (b) to prevent he 
generator from interfering with the structure 
thus built (for example, by trying to modify an 
idiom in an inappropriate way), or (c) to im- 
prove generator efficiency by hiding detail that 
may lead to wasteful processing. An opaque 
structure is represented simply by the failure 
to include a rea l i ses  arrow to that structure. 
Such structures provide the basis for a gener- 
alised approach to "canning". 
4 Imp lementat ion  
There are many ways that modules in an 
NLG system could communicate information 
using the representation scheme just outlined. 
Here we describe a particularly general model 
of inter-module communication, based around 
modules communicating with a single cen- 
tralised repository of data called the whiteboard 
(Calder et al, 1999). A whiteboard is a cumu- 
lative typed relational blackboard: 
? t yped  and  re lat iona l :  because it is based 
on using the above representation scheme; 
? a b lackboard :  a control architec- 
ture and data store shared between 
processing modules; typically, modules 
add/change/remove objects in the data 
store, examine its contents, and/or ask to 
be notified of changes; 
? cumulat ive :  unlike standard blackboards, 
once data is added, it can't be changed or 
removed. So a structure is built incremen- 
tally by making successive copies of it (or of 
constituents of it) linked by rev ises  links 
(although actually, there's no constraint on 
the order in which they are built). 
A whiteboard allows modules to add ar- 
rows (typically forming networks through ar- 
rows sharing source or target objects), to in- 
spect the set of arrows looking for particular 
configurations of types, or to be informed when 
a particular type of arrow (or group of arrows) 
is added. 
The whiteboard is an active database server. 
This means that it runs as an independent pro- 
cess that other modules connect o by appropri- 
ate means. There are essentially three kinds of 
interaction that a module might have with the 
whiteboard server: 
? pub l i sh  - add an arrow or arrows to the 
whiteboard; 
122 
? query  - look for an arrow or arrows in the 
whiteboard; 
? wa i t  - register interest in an arrow or ar- 
rows appearing in the whiteboard. 
In both query and wait ,  arrows are specified 
by type, and with a hierarchical type system on 
objects and relations, this amounts to a pattern 
that matches arrows of subtypes as well. The 
wait  function allows the whiteboard to take the 
initiative in processing - if a module wai ts  on a 
query then the whiteboard waits until the query 
is satisfied, and then tells the module about it. 
So the module does not have to continuously 
scan the whiteboard for work to do, but can 
let the whiteboard tell it as soon as anything 
interesting happens. 
Typically a module will start up and regis- 
ter interest in the kind of arrow that represents 
the module's input data. It will then wait for 
the whiteboard to notify it of instances of that 
data (produced by other modules), and when- 
ever anything turns up, it processes it, adding 
its own results to the whiteboard. All the mod- 
ules do this asynchronously, and processing con- 
tinues until no module has any more work to 
do. This may sound like a recipe for confusion, 
but more standard pipelined behaviour is not 
much different. In fact, pipelining is exactly a 
data-based constraint - the second module in a 
pipeline does not start until the first one pro- 
duces its output. 
However, to be a strict pipeline, the first mod- 
ule must produce all of its output before the sec- 
ond one starts. This can be achieved simply by 
making the first module produce all its output 
at once, but sometimes that is not ideal - for ex- 
ample if the module is recursive and wishes to 
react to its own output. Alternative strategies 
include the use of markers in the whiteboard, 
so that modules can tell each other that they've 
finished processing (by adding a marker), or 
extending the whiteboard architecture itself so 
that modules can tell the whiteboard that they 
have finished processing, and other modules can 
wait for that to occur. 
5 Reconst ruct ion  o f  the  Capt ion  
Generat ion  System 
In order to prove this representation scheme 
in practice, we have implemented the white- 
board in Sicstus Prolog and used it to support 
data communications between modules in a re- 
construction of the Caption Generation System 
(Mittal et al, 1995). CGS is a system developed 
at the University of Pittsburgh, which takes in- 
put from the SAGE graphics presentation sys- 
tem (Roth et al, 1994) and generates captions 
for the graphics SAGE produces. We selected it 
for this effort because it appeared to be a fairly 
simple pipelined system, with modules perform- 
ing clearly defined linguistic tasks. As such, we 
thought it would be a good test case for our 
whiteboard specification. 
Although the CGS is organised as a pipeline, 
shown in Figure 3, the representations commu- 
nicated between the modules do not correspond 
to complete, separate instances of RAGS data- 
type representations. Instead, the representa- 
tions at the various levels accumulate along the 
pipeline or are revised in a way that does not 
correspond exactly to module boundaries. Fig- 
ure 3 gives a simple picture of how the different 
levels of representation build up. The labels for 
the RAGS representations refer to the following: 
? I = conceptual; 
? II -- semantic; 
? I I I  = rhetorical; 
? IV = document; 
? V = syntactic. 
For instance, some semantic (II) information is 
produced by the Text Planning module, and 
more work is done on this by Aggregation, but 
the semantic level of representation is not com- 
plete and final until the Referring Expression 
module has run. Also, for instance, at the 
point where the Ordering module has run, there 
are partially finished versions of three different 
types of representation. It is clear from this that 
the interfaces between the modules are more 
complex than could be accounted for by just re- 
ferring to the individual evels of representation 
of RAGS. The ability to express combinations of 
structures and partial structures was fundamen- 
tal to the reimplementation of CGS. We high- 
light below a few of the interesting places where 
these features were used. 
123 
AbsSemRep 
I-el ~ ~  .................................... SemRep 
--(~------~_set{KBPredl ~ fun(Role,set(KBId)) I-el ~3-e l  
. . . .  /X  . . . . . . . .  
el agent affected . . . .  DR fun(Role,set(SemRep)) ~i/  ~ ..... ~ el?set(SemPredi~t A ~ . ? 
nresent set(KSld) 0 . . . . . .  v ? ~--"- ................. / agen, /  \a\] Jec,ea 
el / \ el . . . . .  " . . . . . . . . . .  ~ ?J / "k~ present S~mRep SemRep 
chart1 chart2 
Figure 4: Combined Abstract Semantic Representation a d Concrete Semantic Representation for 
the output: "These two charts present information about house sales from data-set ts-1740" 
CG$ aroh i ta ,~ lu 'e  RAGS representat/on$ 
II I l l  IV ~' SAGE 
- -  . . . . . . . . . .  
tuning II 
- . . . . . . . . . .  
I1 I11 iV  
--' . . . . . . . . . .  
I\[ I11 IV 
. . . . . . . . . .  I ;11@ 
11 III I v  v 
. . . . . . . . .  
II I11 IV V 
. . . . . . . . .  III1  
II 111 IV V 
l - -  . . . . . . . . . .  I I I I I  
FUF 
Figure 3: A RAGS view of the CGS system 
5.1 Referr ing Express ion Generat ion  
In many NLG systems, (nominal) referring ex- 
pression generation is an operation that is in- 
voked at a relatively late stage, after the struc- 
ture of individual sentences i  fairly well speci- 
fied (at least semantically). However, referring 
expression generation eeds to go right back to 
the original world model/knowledge base to se- 
lect appropriate semantic ontent o realise a 
particular conceptual item as an NP (whereas 
all other content has been determined much ear- 
lier). In fact, there seems to be no place to 
put referring expression generation i a pipeline 
without there being some resulting awkward- 
ness. 
In RAGS, pointers to conceptual items can 
be included inside the first, "abstract", level of 
semantic representation (AbsSemRep), which is 
intended to correspond to an initial bundling of 
conceptual material under semantic predicates. 
On the other hand, the final, "concrete", level 
of semantic representation (SemRep) is more 
like a fully-fledged logical form and it is no 
longer appropriate for conceptual material to 
be included there. In the CGS reimplementa- 
tion, it is necessary for the Aggregation mod- 
ule to reason about the final high-level semantic 
representation f sentences, which means that 
this module must have access to "concrete" se- 
mantic representations. The Referring Expres- 
sion generation module does not run until later, 
which means that these representations cannot 
be complete. 
Our way around this was to ensure that the 
initial computation of concrete semantics from 
abstract semantics (done as part of Aggrega- 
tion here) left a record of the relationship by 
including realises arrows between correspond- 
ing structures. That computation could not be 
completed whenever it reached conceptual ma- 
terial - at that point it left a "hole" (an ob- 
ject with no further specification) in the con- 
crete semantic representation li ked back to the 
conceptual material. When referring expression 
was later invoked, by following the arrows in the 
124 
resulting mixed structure, it could tell exactly 
which conceptual entity needed to be referred 
to and where in the semantic structure the re- 
sulting semantic expression should be placed. 
Figure 4 shows the resulting arrangement for 
one example CGS sentence. The dashed lines 
indicate realises, i.e. non-local, arrows. 
5.2 Handling Centering Information 
The CGS Centering module reasons about the 
entities that will be referred to in each sentence 
and produces a representation which records the 
forward and backward-looking centers (Grosz et 
al., 1995). This representation is later used by 
the Referring Expression generation module in 
making pronominalisation decisions. This in- 
formation could potentially also be used in the 
Realisation module. 
Since Centering is not directly producing re- 
ferring expressions, its results have to sit around 
until they can actually be used. This posed 
a possible problem for us, because the RAGS 
framework does not provide a specific level of 
representation for Centering information and 
therefore seems on first sight unable to account 
for this information being communicated be- 
tween modules. The solution to the problem 
came when we realised that Centering informa- 
tion is in fact a kind of abstract syntactic in- 
formation. Although one might not expect ab- 
stract syntactic structure to be determined until 
the Realisation module (or perhaps lightly ear- 
lier), the CGS system starts this computation i
the Centering module. 
Thus in the reimplementation, the Centering 
module computes (very partial) abstract syn- 
tactic representations for the entities that will 
eventually be realised as NPs. These represen- 
tations basically just indicate the relevant Cen- 
tering statuses using syntactic features. Figure 
5 shows an example of the semantics for a typi- 
cal output sentence and the two partial abstract 
syntactic representations computed by the Cen- 
tering module for what will be the two NPs in 
that sentence 2. As before, dashed lines indicate 
realises arrows. Of course, given the discussion 
of the last section, the semantic representation 
objects that are the source of these arrows are in 
fact themselves linked back to conceptual enti- 
ties by being the destination of realises arrows 
2FVM = Feature Value Matrix. 
from them. 
When the Referring Expression generation 
module runs, it can recover the Centering infor- 
mation by inspecting the partial syntactic rep- 
resentations for the phrases it is supposed to 
generate. These partial representations are then 
further instantiated by, e.g., Lexical Choice at 
later stages of the pipeline. 
6 Conc lus ion  
The representation scheme we have proposed 
here is designed specifically to support he re- 
quirements of the current state-of-the-art NLG 
systems, and our pilot implementation demon- 
strates the practical applicability of the pro- 
posal. Tangled, partial and mixed structures 
are of obvious utility to any system with a flex- 
ible control strategy and we have shown here 
how the proposed representation scheme sup- 
ports them. By recording the derivational his- 
tory of computations, it also supports decisions 
which partly depend on earlier stages of the 
generation process (e.g., possibly, lexical choice) 
and revision-based architectures which typically 
make use of such information. We have shown 
how the representation scheme might be the ba- 
sis for an inter-module communication model, 
the whiteboard, which supports a wide range of 
processing strategies that require the represen- 
tation of complex and evolving data dependem 
cies. The fact that the whiteboard is cumula- 
tive, or monotonic in a logical sense, means that 
the whiteboard also supports reasoning about 
the behaviour of NLG systems implemented in 
terms of it. This is something that we would 
like to exploit directly in the future. 
The reimplementation f the CGS system 
in the RAGS framework was a challenge to 
the framework because it was a system that 
had already been developed completely inde- 
pendently. Even though we did not always un- 
derstand the detailed motivation for the struc- 
ture of CGS being as it was, within a short time 
we reconstructed a working system with mod- 
ules that corresponded closely to the original 
CGS modules. The representation scheme we 
have proposed here was a key ingredient in giv- 
ing us the flexibility to achieve the particular 
processing scheme used by CGS whilst remain- 
ing faithful to the (relatively simple) RAGS 
data model. 
125 
SemRep 
fun(Role,setlSemRep)) 
sl S " ' .  
t t ~ .  
2 AbsSynRep "~ AbsSynRep _(:5 ~ ,  
, , / \ \ 
ckward-looking-cemer ckward.looking-cenler 
+ + 
Figure 5: Arrangement of centering information for the output sentence above 
The representation scheme is useful in situa- 
tions where modules need to be defined and im- 
plemented to work with other modules, possibly 
developed by different people. In such cases, the 
representation scheme we propose permits pre- 
cise definition of the interfaces of the modules, 
even where they are not restricted to a single 
'level' of representation. Even though the con- 
trol structure of CGS is quite simple, we found 
that the use of a centralised whiteboard was use- 
ful in helping us to agree on interfaces and on 
the exact contribution that each module should 
be making. Ultimately, it is hoped that the use 
of a scheme of this type will permit much more 
widespread 'plug-and-play' among members of 
the NLG community. 
Re ferences  
Lynne Cahill, Christy Doran, Roger Evans, Chris 
Mellish, Daniel Paiva, Mike Reape, Donia Scott, 
and Neil Tipper. 1999a. In Search of a Reference 
Architecture for NLG Systems. In Proceedings of 
the 7th European Workshop on Natural Language 
Generation, pages 77-85, Toulouse. 
Lynne Cahill, Christy Doran, Roger Evans, Chris 
Mellish, Daniel Paiva, Mike Reape, Donia Scott, 
and Neil Tipper. 1999b. Towards a Reference 
Architecture for Natural Language Genera- 
tion Systems. Technical Report ITRI-99-14, 
Information Technology Research Institute 
(ITRI), University of Brighton. Available at 
http://www, i t r i  .brighton. ac. uk/proj ects/rags.  
Jo Calder, Roger Evans, Chris Mellish, and Mike 
Reape. 1999. "Free choice" and templates: how 
to get both at the same time. In "May I speak 
freely?" Between templates and free choice in nat- 
ural language generation, number D-99-01, pages 
19-24. Saarbriicken. 
B.J. Grosz, A.K. Joshi, and S. Weinstein. 1995. 
Centering: a framework for modelling the local co- 
herence of discourse. Computational Linguistics, 
21 (2):203-226. 
V. O. Mittal, S. Roth, J. D. Moore, J. Mattis, and 
G. Carenini. 1995. Generating explanatory cap- 
tions for information graphics. In Proceedings of 
the 15th International Joint Conference on Ar- 
tificial Intelligence (IJCAI'95), pages 1276-1283, 
Montreal, Canada, August. 
V. O. Mittal, J. D. Moore, G. Carenini, and S. Roth. 
1998. Describing complex charts in natural lan- 
guage: A caption generation system. Computa- 
tional Linguistics, 24(3):431-468. 
Ehud Reiter. 1994. Has a consensus NL generation 
architecture appeared and is it psycholinguisti- 
cally plausible? In Proceedings of the Seventh In- 
ternational Workshop on Natural Language Gen- 
eration, pages 163-170, Kennebunkport, Maine. 
Steven F. Roth, John Kolojejchick, Joe Mattis, and 
Jade Goldstein. 1994. Interactive graphic design 
using automatic presentation knowledge. In Pro- 
ceedings of CHI'9~: Human Factors in Computing 
Systems, Boston, MA. 
126 
Incorporating Metaphonemes in a Multi l ingual Lexicon 
Carole Tiberius and Lynne Cahill 
Information Technology Research Institute 
University of Brighton 
Brighton, UK 
{Carole.Tiberius, Lynne.Cahil l}@itri .brighton. ac .uk 
Abstract English 
bed 
This paper describes a framework for multilingual /bEd/ 
inheritance-based l xical representation which al- rib 
lows sharing of information across languages at /rib/ 
all levels of linguistic description. The paper fo- hand 
cuses on phonology. It explores the possibility /h{nd/ 
of establishing a phoneme inventory for a group cat 
of languages in which language-specific phonemes /k{t/ 
function as "allophones" of newly defined recta- 
phonemes. Dutch, English, and German were taken 
as a test bed and their vowel phoneme inventories 
were studied. The results of the cross-linguistic 
analysis are presented in this paper. The paper con- 
cludes by showing how these metaphonelnes can be 
incorporated in a multilingual lexicon. 
1 Introduction 
This paper describes a framework for multilingual 
inheritance-based lexical representation which aP 
lows sharing of information across (related) hm- 
guages at all levels of linguistic description. Most 
work on multilingual lexicons up to now has as- 
sumed mouolingual lexicons linked only at the level 
of semantics (MUI_TILEX 1993; Copestake t al. 
1992). Cahill and Gazdar (1999) show that this 
approach might be appropriate for unrelated lan- 
guages, as for example English and Japanese, but 
that it makes it impossible to capture useful gener- 
alisations about related languages - such as English 
and German. Related languages share many linguis- 
tic characteristics at all levels of description - syn- 
tax, morphology, phonology, etc. - not just seman- 
tics. For instance, words which come fl'om a single 
root have very similar orthographic and phonologi- 
cal forms. Compare English, Dutch, and German1: 
IThe lranscriptions are taken from CELEX (Baayen et al 
1995) and use tile SAMPA phonetic alphabet (Wells 1989). 
Dutch Gernmn 
bed Bett 
/bEt/ /bEt/ 
rib Rippe 
/rip/ /rip@/ 
hand ltand 
/hAnt/ /hant/ 
kat Katze 
/kAt/ /kats @/ 
Most differences can be attributed to dil'ferent 
orthographic conventions and regular phonological 
changes (e.g. final devoicing in Dutch and German). 
The English/{I, the Dutch/AI, and the German/a/ 
in the last two exmnples, are even virtually the same. 
They have slightly different realisations but they are 
phonologically non-distinctive, i.e. if the Dutch/A/ 
were substituted by the English/{/in Dutch, the re- 
sult would not be a different word, but it would sim- 
ply sound like a different accent. 
Cahill and Gazdar (I 999) describe an architecture 
for nmltilingual lexicons which aims to encode and 
exploit lexical similarities between closely related 
languages. This architecture has been successfully 
applied in the PolyLex project 2 to define a trilingual 
lexicon for Dutch, English, and German sharing 
morphological, phonological, and lnorphophono- 
logical information between these languages. 
in this paper, we will take the Polykex fiame- 
work as our basis. We will focus on the phono- 
logical similarities between related hmguages and 
we will extend the PolyLex approach by capturing 
cross-linguistic phoneme correspondences, such as 
the/{/-/A/-/a/correspondence mentioned above 3. 
First, we will discuss how a phoneme inventory 
can be defined for a group of languages - l)utch, 
2http://www.cogs. susx. ac.uk/ lab/nlp/polylex/ 
3We believe the approach would be even more beneficial if 
exlended to a featural evel, but for tile present purposes we 
conline ourselves to the segmental level. 
1126 
English, and German. Then, we will explain tile 
multilingual architecture used in PolyLex. Finally, 
we will explore how these cross-linguistic phoneme 
correspoudences can be integrated into tile multilin- 
gual frmnework. 
2 A Metaphoneme Inventory 
In this section we describe how a phoneme inven- 
tory can be defined for a group of languages in 
which language-specific phonemes flmction its "al- 
lophones" of newly defined metaphonemes. We will 
restrict ourselves to the vowel phonemes of l)utch, 
English, and Gerlnan. If we know, for example, 
that words which are realised with an /{I in En- 
glish are usually realised with an/A/ in I)utch, and 
an/a / in  German (as in hand/h{nd/ versus/hAnt/ 
w:rsus/hant/, cal/k{t/versus/kArl versus/kats(@/, 
elc.), we might be able to generalise over these three 
hmguage-specific phonemes and introduce a meta- 
phoneme, e.g. I{Aa\], which captures this generali- 
sation. 
To give an impression of the distribution of the 
different vowel phonemes across l)utch, English, 
and German, their vowel charts (K6nig and van der 
Auwera 1994; Wells 1989) were merged into one 
big vowel chart containing all the vowel phonemes 
of these three hmguages. 4, The resulting char| is 
given iu tigure 15: 
I ; ronl  I:hlck 
2: \ 
e \ 9 .~: 
:\ \ v 
I ..... a:  \____ _ ) AA: 
a: A 
I1." 
IJ 
i f ;  
O: 
o: i - l)ulch 
\[~: i - English 
i -Gerrnan 
0 
(2 
Figure 1: Vowel phonemes in Dutch, English, and 
German 
This figure shows which vowel phonemes are re- 
atised in which language (e.g./{/occurs in English, 
but not in l)utch and German), but it does not tell us 
4phonemes that only occur in loanwords were not i~lcluded 
a.'; languages adapt loanwords to different degrees to their own 
phonetic syslem. 
5The w)wels are described along the three dimensions of 
w)wel quality: \[high\], \[back\], and \[round\]. The rounded w~wels 
are/y,y:,Y,Y,2:,2:,9,O,O,O, O:,o:,o:,u,u:,tr:,U, I. 
anything about cross-linguistic phoneme correspon- 
deuces. Knowing that Dutch and German both have 
a phoneme/o:/, does not mean that they are cross- 
linguistically non-distinctive. 
qb find cross-linguistic phoneme correspon- 
deuces, we followed O'Connor's (1973) strategy 
for establishing phonelne conespondences between 
difl'erent accents, identifying phonemes of one ac- 
cent with those el' another: 
"How are we to decide whether to equate 
phoneme X with phoneme A or with 
phoneme D? We can do so only on the 
basis el' the words in which they occur: 
if X and A both occur in a large number 
of words common to both accents we link 
them together as representing the same 
point on the pattern, if, on the other hand, 
X shares more words with D than with A, 
we l inkXandD.  \[...\] Even so, i fXand  
D occur in a very similar word-set and X 
and A do not, then it is much more reveal- 
ing to equate X and D than X and A." 
(O'Connor 1973, p. 186) 
We extended O'Connor's trategy and applied it 
to a group of (closely) related hmguages haring 
a co lnmou word  stock - in our case  a s l lbset  o f  
the West Gmmanic languages haring worcls with 
a common Germanic origin. We compiled a list 
of g00 (mono- and disyllabic) Germanic ognates, 
looked up the transcriptions in the CELEX database 
(Baayen el al. 1995), and then mapped words con- 
tainiug a palticular vowel in one hmguage onto its 
cognates in the other two hmguages to see how this 
particular vowel was realised in tile other two lan- 
guages. This process was repeated for all the vow- 
els, for all three languages. 
A few examples of tile results we obtained for En- 
glish vowels are included below c'. 
As can be seen fl'om these v, tlaere is some vari- 
ation in the closeness of the correspondences. The 
vowel set /{/ - /A/ - /a/ ,  as we anticipated at the out- 
set, does turn out to be a wflid correspondence. The 
set associated with English/i:/, on the other hand, 
is less clearcut, as there are several possible cor- 
r'The remaining correspondence tables are available at 
http://www, itri .bton.ac.uk/~Carole.'i~iberius/ 
mphon, html 
7Note that the total number o1' words is not always exactly 
the same in all lhree hmguages. This is because for some words 
the con'esponding phonemic transcription was not found. 
1127 
Engl ish  
{ 37 
Dutch  Gernmn 
A 27 a 22 
a: 3 a: 3 
E 2 E 3 
} 2 I 2 
o: 2 e: 1 
u: 1 O 1 
o: 1 
u: 1 
l: l 
total 37 total 35 
Table 1: Correspondences for English/{/ 
in hand/h{nd/vs/hAnt/vs/hant/. 
words as 
Engl i sh  
i: 65 
Dutch  German 
a: 14 a: 12 
o: 11 i: 8 
e: 9 ai 7 
i: 8 e: 5 
u: 7 y: 5 
I 5 au 5 
E 4 I 5 
EI 3 o: 4 
l: 2 a 3 
/I i E 3 
A 1 u: 3 
O 2 
E: 1 
Y 1 
I: \] 
total 65 total 65 
Table 2: Colxespondences for English/i : /words as 
in meal/mi:l/vs/ma:l/vs/ma:l/and deep/di:p/vs 
/di:p/vs/ti:ff. 
responding vowel phonemes in the other two lan- 
guages. If we consider the correspondences from 
the starting point of one of the other languages, the 
results are slightly different. For instance, English 
/A:/ corresponds trongly to Dutch/A/,  but Dutch 
/A/ corresponds ahnost equally to Eng l i sh / ( /and  
/A:/. Further investigation is required to ascertain 
how many of these cases can be further generalised 
by recourse to phonological or phonotactic proper- 
ties of the words in question. Currently the mapping 
from metaphoneme to (language-specific) phoneme 
requires reference only to the language. For a more 
Engl ish  
A: 31 
Dutch German 
A 19 a 15 
a: 4 a: 5 
E 4 E 5 
O 2 e: 2 
e: 1 E: 1 
El 1 U 1 
Y 1 
ai 1 
total 31 total 31 
Table 3: Correspondences for English/A:/words as 
in heart/hA:T/vs/hArt/vs/hart/. 
Dutch 
A 77 
Engl ish  German 
{ 25 a 53 
A: 17 a: 9 
ell 10 E 6 
O: 8 I 3 
Q 4 ai 1 
@U 4 e: 1 
u: 2 
E 2 
3: 2 
i: 1 
I 1 
aI 1 
total 77 total 73 
Table 4: Correspondences for Dutch/A/words as in 
hand (hand) and hart (hem't). 
sophisticated analysis, phonological and phonotac- 
tic information would need to be considered as well. 
Howcvel; even at the present level of analysis, the 
metaphoneme principle can be helpful in the mul- 
tilingual lexical structure proposed, as we now dis- 
CUSS. 
3 The  mul t i l i ngua l  inher i tance  l ex icon  
In this section, we will explore the sharing of phono- 
logical information in the lexical entries of a mul- 
tilingual inheritance-based lexicon. We focus on 
phonology rather than orthography as phonology is 
nearer to primary language use (i.e. spoken lan- 
guage), it can be used as input for hyphenation rules, 
spelling correction, and it is essential as the level of 
symbolic representation for speech synthesis (MUD 
TILEX 1993). 
1128 
We will take the multilingual architecture of 
PolyLex as our starting point. First, we will describe 
the PolyLex arclaitecture. Then, we will show how 
phonological information can be shared in the lexi- 
cal entries. 
PolyLex detines a multilingual inheritance-based 
lexicon for l)utch, English and German. It is 
implemented in DATR, an inheritance-based lexi- 
cal knowledge representation formalism (Evans and 
Gazdar 1996). The rationale of inheritance-based 
lexicons requires information to be pushed as far up 
the hierarchy as it can go, generalising as much as 
possible. In a multilingual exicon, this means that 
information which is common to several anguages 
is stated at higher points in the hierarchy than that 
which is unique to just one of the languages. In 
addition, Polykex makes use of orthogonal multiple 
inheritance which allows a node in the hierarchy to 
inherit different kinds of information (e.g. seman- 
tics, morphology, phonology, syntax) fi'om different 
parent nodes. In this papen we are just interested in 
the phonological hierarchy. 
Polykex assumes a contemporary phonological 
fralnework in which all lexical entries are detined 
as having a phonological structure consisting of a 
sequence of structured syllables, a syllable consist- 
ing o1' an onset (the initial consonant cluster, which 
might be split up into onset 1, onset 2, etc.) and a 
rhylne. The rhyme consists of a peak (the vowel) 
and a coda (the final consonant cluster, which might 
bc split up into coda 1, coda 2, etc.). This struc- 
ture is defined at the top el' the hierarchy, and ap- 
plies by default o all words. Only the relevant val- 
ues for onset, peal<, and coda have to be defined at 
the individual exical entries (see Cahill and Gazdar 
1!)97). Following PolyLex we will concentrate on a 
segmental phonelnic representation. An example of 
the lexical entry gram as it would be represented in
PolyLex, is shown in figure 2. 
The multilingual phonological entry for gram, is 
delined by sharing identical segments occnrring in 
the majority of the language-specific entries (/gr{m/ 
- /xrAm/-/gram/).  That is, onset 1 is/g/, onset 2 is 
/1"/, and coda is/m/. 
English and German can inherit all the informa- 
tion fiom the common part except for the value of 
their peak, which is respectively /{/ and /a/. In 
Dutch, the value of the peak has to be specified as 
being/A/, plus we will have to override the wdue 
for the first onset o get \[xrAm\]. 
This example misses the generalisation that the 
Peak = { 
Coll ln'lon 
MGram:  
Onset 1 = g 
Onset 2 = r 
Coda = Ill 
Eng l i sh  Dutch  German 
Onset I = x 
l'cak = A Peak = a 
Syllable 
Onset Rhyme 
? /X 
Peak Coda 
? ? 
Figure 2: A multilingual inheritance lexicon with- 
out metaphonemes 
English/{/, the Dutch/A/, and the German/a/are 
phonologically non-distinctive. For each lexical ca- 
try where English uses/{/, l)utch/A/, and German 
/a/, the value for peak has to be specitied in the 
language-specific parts. By using the metaphoneme 
I{Aal instead, this information needs to be speci- 
fied only once. The resulting multilingual phonemic 
representation for gram is given in ligure 3. 
M_,;, ...... Coma,on l l~  
()I1SCl 2 =I" ~ 
Peak = {Aa ~ f~ 
Coda = ~ ; ~  
English l)ulch German 
()llSCI "- X 
Figure 3: A multilingual inheritance lexicon with 
iYletaphonelneS 
A l l  the information has now been pushed up as 
far as it can go, capturing as many generalisations 
as possible. The information that \]{Aa\] results ill 
an/{/in English, an/A/in Dutch, and an/a/in Ger- 
man is specified only at the top level. The language- 
specitic boxes are almost empty, except for the value 
of the first onset in Dutch. The reason for this is 
that as yet we have only defined cross-linguistic 
phoneme correspondences for vowels, not for con- 
sonants. We do, howevm, suspect that the Dutch/x/ 
is phonologically non-distinctive fi'om the German 
and English /g/. Further research defining cross- 
linguistic phoneme correspondences forconsonants 
1129 
will have to confirm this. 
It is a fundamental feature of this account that 
the inherited information is only default informa- 
tion which can be overridden. Thus, it is not re- 
quired that metaphoneme correspondences are com- 
plete and we may choose to use a metaphoneme 
even if one of the languages uses a different vowel 
in some words. The definitions can be overridden 
in exactly the same way as the onset definition in 
Dutch in the example above. So if we consider the 
vowel correspondences in table 1, we can see that 
of the 35 words which have cognates in all three 
languages, 27 can be defined as having the meta- 
phoneme \[{Aa I in the common lexical entry (those 
for which both English and l)utch have the corre- 
sponding vowels). Five of these will require a sep- 
arate vowel defined for Gerlnan, while the remain- 
der will need separate vowel definitions for all three 
languages. 
Given this, we can see that economy of rep- 
resentation can be achieved even in cases where 
the vowel correspondences are far from conclusive. 
Even if only half or fewer of the Dutch words, for 
example, have the same vowel in cognates for which 
the English words have the same vowel, this still 
means that those half can be defined without the 
need for the language-specific vowel to be defined. 
Another feature of the metaphoneme principle 
that differentiates it from the phonemic principle 
is that there is no requirement for biuniqueness. 
A phoneme in a language can be a realisation of 
morn than one metaphoneme. This means that we 
can define a metaphoneme I{Aa\[ as well as another, 
IA:Aal. Each of these will then be used in different 
common lexical entries. This can be used as an al- 
ternative to phonological/phonotactic conditioning 
or in addition to it, for just those cases where there 
is more than one correspondence but no obvious 
phonologicai/phonotactic onditioning for the deci- 
sion between phonemes. 
4 Conclusion 
In this paper, we have discussed the concept of 
metaphonemes. Metaphonemes are cross-linguistic 
phoneme correspondences such as the English/{/, 
the Dutch/A/,  and the German/a/correspondence 
inentioned above. At the lnultilingual level, the 
realisation of the metaphoneme is conditioned by 
the choice of language. At the lower monolingual 
level its realisation as an allophone of a particular 
phoneme is conditioned by the phonological envi- 
romnent. As such, a metaphoneme is a generalisa- 
tion of a generalisation. 
We have shown how a metaphoneme inventory 
can be defined for a group of languages and that 
incorporating these cross-linguistic phoneme corre- 
spondences in a multilingual inheritance lexicon in- 
creases the number of generalisations that can be 
captured. Calculations on the syllable inventories of 
Dutch, English, and German in the CELEX database 
show that the introduction of metaphonemes in- 
creases the amount of sharing at the syllable level 
by about 25%. 
Another benefit of introducing metaphonemes is 
improved robustness in NLP systems. Knowledge 
about cross-linguistic ommonalities can help to 
provide grounds for making an "intelligent" guess 
when a lexical item for a particular language is not 
present. 
Tiffs research as concentrated on cross-linguistic 
vowel phoneme correspondences. Similar research 
will be done for consonants. 
References 
Baayen, 1t., R. Piepenbrock and It. van Rijn. 1995. The CELEX 
Lexical Database, Release 2 (CD-ROM). Linguistic l)ata 
Consortium, University of Pennsylvania, Philadelphia, PA. 
Cahill, L. and G, Gazdar. 1997. "The inllectional phonology of 
Gerlnan adjectives, determiners and pronouns", In Linguis- 
tics, 35.2, pp.211-245. 
Cahill, L. and G. Gazdar. 1999. "The Polykex architecture: 
multilingual lexicons for related languages", In 7)witement 
Automatique des l~mgues, 40:2, pp.5-23. 
Copestake, A., B. Jones, A. Sanfilippo, H. Rodriguez, P. 
Vossen, S. Montemagni, and E. Marinai. 1992. "Multi- 
lingual Lexical Representation". ESPRIT BRA-3030 AC- 
QUILEX Working Paper N ? 043. 
Evans, R. and G. Gazdar. 1996. "DATR: A Language for Lexi- 
cal Knowledge Representation", I  Contlmtational Linguis- 
tics, Vol. 22-2, pp.167-216. 
Kfnig, E. and J. van der Auwera (eds.) 1994. The Germanic 
lJtnguages, Routledge, London. 
MULTILEX, 1993. "MLEX,I Standards for a Multi functional 
Lexicon", Final Report, CAP GEMINI INNOVATION for 
tt~e MULTILEX Consortium, Paris. 
O'Connor, J.l). 1973. Phonetics, Pelican Books, Great Britain. 
Wells, J. 1989. "Computer-coded phonemic notation of indi- 
vidual anguages of lhe European Community", In Journal 
qf the International Phonetic Association, 19:1, pp.31-54. 
1130 
Cross-linguistic phoneme correspondences
Lynne Cahill and Carole Tiberius
Information Technology Research Institute Surrey Morphology Group
University of Brighton University of Surrey
Brighton Guildford
UK UK
Lynne.Cahill@itri.brighton.ac.uk c.tiberius@surrey.ac.uk
Abstract
Cross-linguistic phoneme correspondences, or meta-
phonemes1, can be defined across languages which are
relatively closely related in exactly the same way as cor-
respondences can be defined for dialects, or accents, of a
single language (e.g. O?Connor, 1973; Fitt, 2001). In this
paper we present the theory of metaphonemes, compar-
ing them with traditional archi- and morphophonemes as
well as with similar work using ?keysymbols? done for
accents of English. We describe the metaphoneme inven-
tory defined for Dutch, English and German, comparing
the results for vowels and consonants. We also describe
some of the unexpected information that arose from the
analysis of cognate forms we undertook to find the meta-
phoneme correspondences.
1 Introduction
Tiberius and Cahill (2000) presented the theory of
cross-linguistic phoneme correspondences (meta-
phonemes) with an example pilot study of the vow-
els of Dutch, English and German. The aim of this
work is to allow the type of generalisation that is
permitted by the use of phonemes with allophonic
variation to be taken one level higher, i.e. above the
level of the single language. The idea behind it is to
represent the near-identities that closely related lan-
guages such as Dutch, English and German so often
share. For example, the Dutch word ?kat? /kAt/ has
the English equivalent ?cat? /k
 
t/, and the German
?Katze? /kats@/2. While the consonants are largely
identical (/k/?/k/?/k/ and /t/?/t/?/ts/), the vowels are
subtly different. However, they are not distinctive
? i.e. if the /
 
/ in English were replaced with /a/ it
would not sound like a different word, but rather it
would sound like a different accent. Thus our aim
is not to construct a universal phoneme set repre-
senting all phonemes occurring in a particular set
1This work was supported by ESRC grant no R000223681.
2The transcriptions are taken from CELEX (Baayen et al,
1995) and use the SAMPA phonetic alphabet (Wells, 1989).
of languages, but we aim to capture phoneme corre-
spondences between languages such as the /
 
/ ? /a/ ?
/A/ correspondence mentioned above. Our work is,
therefore, different from proposals put forward by
Deng (1997) who defines a set of universal phono-
logical features to be used for multilingual speech
recognition.
The three language-specific vowels discussed
above can be grouped together into the meta-
phoneme 
 
Aa  , which will be realised as an /
 
/
in English, an /A/ in Dutch, and an /a/ in Ger-
man3. Tiberius and Cahill (2000) described the
vowel metaphonemes for these three languages. In
this paper we describe a similar experiment that
looked at the consonants of the three languages. The
consonants are interestingly different from the vow-
els for a number of reasons:
 the consonant space is more discrete than the
vowel space, so there is less scope for small
and non-meaning-bearing distinctions within
the consonants;
 the phoneme inventories of the three languages
show that, while they have significantly differ-
ent vowel inventories, their consonant invento-
ries overlap greatly;
 while vowels were considered to occur one per
syllable (i.e. long vowels and diphthongs were
treated as single vowels), consonants can oc-
cur in clusters at either the beginning or end of
syllables;
 unlike vowels, consonants can be lost alto-
gether, thus leading to synchronic alternations
3We use the notation  xyz  to denote the metaphoneme
where x, y and z are normally the sounds for the three languages
in the order English, Dutch, and German. However, this is in-
tended only as a mnemonic and does not necessarily imply that
these three sounds always occur. Metaphonemes may involve
quite complex definitions that are dependent on phonological
context as well as just the language in question.
between zero and other segments.
In this paper we present the results of our ex-
periment, first discussing the exact nature of meta-
phonemes, comparing them with archiphonemes
and morphophonemes as used in traditional ap-
proaches to morphology as well as the keysymbols
used by Fitt (2001) to define cross-accent differ-
ences. We then go on to describe the methodology
used and the results obtained. Finally we discuss
the implications of our findings both on the intended
application, i.e. multilingual lexicons, and for other
fields such as historical linguistics.
2 Metaphonemes, archiphonemes and
keysymbols
The phonemic principle, which has been with us
since the end of the nineteenth century, proposes
that sets of similar sounds, which can be distin-
guished by the phonological context in which they
occur, can be grouped together to form a single ab-
stract phoneme. The distinct sounds or phones have
been defined as allophones of the phoneme, one and
only one allophone being permitted to appear in any
particular phonological context. The metaphoneme
principle states that sets of distinct phonemes that
appear in different (but related) languages may be
grouped together in a similar way as an abstract
metaphoneme, where the conditioning factor is the
language in question rather than the phonological
context. This is the simplest case, but we also al-
low phonological conditioning to play a part in the
definition of metaphonemes. For example, we may
want to say that where English has /s/, German has
/S/ if it is in the onset and appears immediately be-
fore a /t/ but has /s/ otherwise4 .
Archiphonemes (Trubetzkoy, 1939) are used to
generalise over phonemes within a language to rep-
resent cases where neutralisations arise in certain
contexts. For example, for stops that immediately
follow /s/ in English, there is no voicing distinc-
tion (?skin?, for example, cannot be contrasted with
?sgin?)5 . Trubetzkoy proposed that in such cases we
use a different symbol to denote the underspecified
or neutralised sound. Similarly, morphophonemes
(or systematic phonemes) have been proposed by
4This is still very simplified. See below for a more detailed
discussion of this particular metaphoneme.
5In fact the realisation of such consonants is somewhere
between the voiced and voiceless forms, with minimal actual
voicing, but no aspiration that is usually associated with voice-
less stops.
generative grammarians (Chomsky, 1964) to repre-
sent situations where distinctions are neutralised in
certain morphological contexts. For example, the
voicing of the final consonant of the stem in ?knife?
and ?knives? is determined entirely by the presence
or absence of the plural suffix.
Although there is a superficial similarity between
archi- and morphophonemes and metaphonemes,
there are a number of crucial differences. We should
note first that both archi- and morphophonemes
were introduced as an answer to a problem that we
do not actually face ? namely the problems of vio-
lation of the phonemic principle. It is only if one
needs to insist on biuniqueness, invariance and lin-
earity that a solution to the potential problem is
needed. In the overall approach to phonology and
morphology advocated in the present work these are
simply not necessary. We allow lexical entries (or
definitions of lexical classes) to specify phonolog-
ical and morphophonological alternations without
being restricted to the phonemic principle. Thus, a
phoneme in a language can be a realisation of more
than one metaphoneme. The other most obvious
difference is that archiphonemes are defined only
within a single language, whereas metaphonemes
are defined across languages. In terms of the overall
theory of morphology, phonology and the lexicon
into which metaphonemes were designed to fit, the
generalisations represented by metaphonemes come
at a different level from archiphonemes.
The keysymbols proposed by Fitt (2001) are
much closer to our metaphonemes. The most obvi-
ous difference here is that metaphonemes range over
languages, while keysymbols are defined across dif-
ferent accents of a single language. However, this
apparently significant difference is only sustainable
if we maintain that there is a solid definition of
what is a language and what is a dialect (or ac-
cent). We would maintain that the type of lexicon
which represents related languages according to a
hierarchical definition of their similarities can be ex-
tended very simply to represent distinct dialects of
a single language in exactly the same way. How-
ever, there are practical differences in the way Fitt?s
keysymbols and our metaphonemes are employed.
Fitt assumes a text-to-speech application in which
the same words are to be pronounced, but in differ-
ent accents. We assume a more general lexicon sys-
tem, in which we may want to represent differences
in whole dialects, not just accents, so that not only
the pronunciation will be different. Fitt?s system al-
lows the definition of a single lexicon which out-
puts ambiguous strings, including keysymbols, to a
speech synthesiser which interprets the keysymbols
and disambiguates the pronunciation to get that de-
sired. In the case of metaphonemes, we anticipate
a lexical structure which allows lexical entries to be
ambiguous as to their pronunciation, but the output
of the lexicon as a whole is unambiguous, the meta-
phonemes being expanded out to their realisation in
the different languages (or dialects) as part of the
output process from the lexicon.
3 The metaphonemes of Dutch, English
and German
In order to define the metaphonemes, we con-
structed a database of around 800 cognate words
from the three languages. The database began
with orthographic forms, to which we automatically
added the phonological forms from CELEX6. We
then slightly massaged the database so that lead-
ing or trailing schwa syllables were ignored and for
most cases just the core root was left for each lan-
guage. Finally, we analysed the forms into syllabic
structures and collated the onsets, peaks and codas
for each language7 .
With this information we did two things: first we
looked at the absolute correspondences, for clus-
ters and for single consonants, and their frequencies.
That is, we considered each grouping of correspon-
dences, such as:8
st+st+St
str+str+str
nd+nd+nt
m+m+m
k+k+k
This gave us both some idea of the likely corre-
spondences and some suggestions as to how phono-
logical context might affect them. We did this for
onsets only, codas only and for the two combined.
6Where there were homographs with different pronuncia-
tions, the choices between them were made manually.
7It should be noted that this process is entirely automatic,
and could be applied equally to databases of other cognate lan-
guages (e.g. French, Italian, Spanish). Indeed, it would also
be possible to construct a database that included for English
the cognates from other languages (e.g. French). There will
inevitably be gaps in the cognate mappings for any set of lan-
guages, a database that maps some English words to one lan-
guage and other words to another language would be just as
acceptable as the database we have worked with to date.
8Note that we use the ordering English, Dutch, German
throughout.
Secondly, we extracted all of the individual conso-
nant correspondences. This had to be done semi-
manually as we wanted to ensure that, in cases
such as sk+sx+S the correspondences came out
as s+s+S, and k+x+0 (e.g. ?school?, ?school?,
?Schule?). From this we derived a set of tables9
which give, for each consonant in each language,
the consonants it can correspond to in the other two
languages and how often it does so in our cognate
database.
As we expected, there were many cases where the
consonants in question were almost always the same
across the languages (e.g. m+m+m). Also as we ex-
pected, the most interesting areas were where one
or more languages have different phonological con-
straints (e.g. /St/ in German onsets vs /st/ in Dutch
and English onsets) or where one or more languages
have a phoneme that the other(s) do not (e.g. /pf/ in
German, /G/ in Dutch).
3.1 Analysis of results
The tables themselves give us a great deal of in-
formation, but the whole story can only be gleaned
from both sets of data taken together. Let us now
consider in detail one small area of the analysis,
that covering the consonants /s/, /S/ and /z/. The
sounds are obviously related phonologically. /s/, /S/
and /z/ are the only sibilants that occur in all three
languages. Figure 1 shows the relevant tables for
these sounds starting from English. Just looking at
these tables tells us that for /S/ in English, there is
just a single metaphoneme worth defining, namely
 SsS  , i.e. English /S/ maps to Dutch /s/ and Ger-
man /S/. The table for /s/, however, shows us rather
more interesting things. For English /s/, Dutch has
two clear possibilities, /s/ or /z/, while German has
three, /S/, /z/ or /s/. To determine how these are re-
lated we need to look at the original correspondence
database so that we can see if there are any patterns
for the possible correspondences. The relevant en-
tries10 from the first data set for onset only are:
31 s+z+z
15 st+st+St
14 S+sx+S
6 str+str+Str
5 sw+zw+Sv
5 sp+sp+Sp
4 s+s+z
3 s+z+0
9The full tables for vowels and consonants are available at
http://www.itri.bton.ac.uk/projects/metaphon.
10This is a reduced set for simplicity.
English Dutch German
s 131 s 83 S 39
z 42 z 37
t 3 s 35
w 1 0 9
l 1 ts 5
0 1 l 2
v 1
x 1
r 1
g 1
total 131 total 131 total 131
English Dutch German
z 9 s 5 z 6
z 4 r 2
S 1
total 9 total 9 total 9
English Dutch German
S 26 s 21 S 21
k 1 0 3
l 1 k 1
z 1 z 1
0 1
d 1
total 26 total 26 total 26
Figure 1: Tables for /s/, /z/ and /S/ in English
2 st+st+0
2 spr+spr+Spr
2 sl+sl+Sl
2 sk+sx+S
for coda only they are:
21 t+t+s
16 st+st+st
11 s+s+s
5 S+s+S
4 z+s+z
3 st+st+0
3 ks+s+ks
2 0+s+s
We can see that in German, whereas /st/ appears
in coda position (corresponding strongly with /st/ in
both Dutch and English), in onset position /St/ ap-
pears corresponding with /st/ in Dutch and English.
Indeed, /s/ followed by a consonant in English and
Dutch onsets tends to correspond to /S/ followed by
that consonant in German onsets. We could spec-
ulate on many possible implications of the clus-
tering of consonants, but in the majority of cases,
the absolute correspondences across the languages
are so strong that we gain very little by consider-
ing phonological context. However, this is clearly a
case where phonological context is useful. The ta-
bles themselves suggest six possible metaphonemes
for English /s/:  ssS  ,  ssz  ,  sss  ,  szS  ,  szz  and
 szs  . The third of these we can eliminate as it is
simply the default case where all languages have the
same segment. From the data above, we can see that
the metaphoneme  ssS  is likely to be a very useful
one, as it occurs in many onset clusters.
The data above, however, allow us to say even
more. When we look at the distribution of the /s/
and /S/ in German, it is evident that a metaphoneme
that specified that English and Dutch both have /s/
in all contexts while German has /s/ in the coda and
/S/ in the onset would capture a much wider gener-
alisation, and cover 74 of the 131 English /s/ cases.
This then leaves us with the alternations that involve
/z/ in Dutch and German. We therefore propose a
metaphoneme  szz  , which is clearly evidenced by
the 31 cases of this simple correspondence for on-
sets above. However, looking more closely again,
we can see that this correspondence does not occur
at all in the coda, where English /s/ (on its own) cor-
responds to /s/ in both Dutch and German. This is
clearly a result of final consonant devoicing in these
two languages, and can be captured by making the
metaphoneme defined above phonologically condi-
tioned. Thus, English /s/ corresponds to /z/ in the
other two languages in the onset, and to /s/ in the
coda.
4 Implications of the results
The intended application of metaphonemes is hier-
archically organised multilingual lexicons that per-
mit the sharing of information at all levels (Cahill
and Gazdar, 1999), potentially useable for speech
recognition or synthesis. The use of metaphonemes
allows us to greatly increase the amount of shar-
ing of phonological information across related lan-
guages in such a multilingual lexicon. As Tiberius
and Cahill (2000) described, using metaphonemes
for the vowels alone increased the amount of phono-
logical definitions that could be shared by around
25%. While the use of consonant metaphonemes
does not lead to such significant increases in shar-
ing, we estimate that the combined figure rises to
around 40%.
Introducing metaphonemes may also be bene-
ficial with respect to the robustness of NLP sys-
tems. Knowledge about cross-linguistic common-
alities can help to provide grounds for making ?in-
telligent guesses? when lexical items for a particular
language are not present. For example, consider the
lexical entry for English ?plough?. We hypothesise a
metaphoneme  pppf  (/p/ in English and Dutch, /pf/
in German) as well as  aUu:u:  (/u:/ in Dutch and
German, /aU/ in English) and  0xg  (/0/ in English,
/x/ in Dutch and /g/ in German)11. If we know that
the English word ?plough? has the form /plaU/ and
that the corresponding Dutch word ?ploeg? has the
form /plu:x/, we may predict that the German form
would be /pflu:g/. In fact, the German ?Pflug? has
the form /pflu:k/, due to the pervasive final conso-
nant devoicing. Thus we can see that in such a case,
metaphonemes may help us to predict a form, al-
though the result will not necessarily be fully cor-
rect. This example also illustrates the usefulness of
phonological conditioning, as we would surely want
ultimately to define all consonant correspondences
in German and Dutch to take account of the final
consonant devoicing process.
Another potential use for metaphonemes is in the
field of second language learning, where the typical
errors made by learners of a language may be deter-
mined by unconscious use of corresponding sounds
from their own language.
As well as giving a good indication of possi-
ble candidate metaphonemes, the analysis we per-
formed also gave us other information about the
three languages which is potentially of interest
to historical linguists. The analysis we did in-
volved matching the corresponding segments in
forms which are originally from identical roots.
Thus we might expect that the data can give us
clues about how the languages have changed and
diverged. For example, a zero in a possible con-
sonant position in one language suggests that that
language has lost a segment where (at least one of)
the other languages still have one. Looking at which
segments are found in such positions gives us a clue
as to which segments are most likely to be lost in
language change (at least in these languages). In-
11All of these metaphonemes are predictable from the full
correspondence tables.
deed, it transpires that the highest ranked segments
in these positions are, as one would expect, mostly
approximants, liquids and glides (/r/, /w/, /l/ etc.).
Also interesting is that of the stop consonants, the
most likely to be lost in all three languages are the
velar consonants /k/ and /g/. Another interesting re-
sult from this examination is that Dutch is appar-
ently less likely to have zeros than German, while
English is much more likely to have zeros than ei-
ther of the other two languages. (41 for Dutch com-
pared to 147 in German and 268 in English).
5 Conclusions
In this paper, we have presented the theory of meta-
phonemes. We have illustrated our theory with the
definition of cross-linguistic phoneme correspon-
dences for English, Dutch, and German. We believe
that our work has interesting benefits for speech ap-
plications. The information that can be specified in
metaphonemes can be used to tune speech applica-
tions to closely related languages in a similar way
that Fitt?s (2001) keysymbols are used to model dif-
ferent accents. The next steps in this research will
involve fully integrating the metaphonemes into a
multilingual lexicon to enable testing on a speech
synthesis system.
References
Baayen, H., R. Piepenbrock and H. van Rijn. 1995. The CELEX
Lexical Database, Release 2 (CD-ROM). Linguistic Data
Consortium, University of Pennsylvania, Philadelphia, PA.
Cahill, L. and G. Gazdar. 1999. ?The PolyLex architecture:
multilingual lexicons for related languages?, In Traitement
Automatique des Langues, 40:2, pp.5-23.
Chomsky, N. 1964. Current Issues in Linguistic Theory, Mou-
ton, The Hague.
Deng, L. 1997. ?Integrated-Multilingual Speech Recognition
using Universal Phonological Features in a Functional
Speech Production Model?, In Proceedings of the 1997
IEEE International Conference on Acoustics, Speech, and
Signal Processing, Vol II Speech Processing, Munich. Ger-
many. pp.1007-1010.
Fitt, S. 2001. ?Morphological Approaches for an English Pro-
nunciation Lexicon.? In Proceedings of Eurospeech 2001.
Aalborg, Denmark.
O?Connor, J.D. 1973. Phonetics, Pelican Books, Great Britain.
Tiberius, C. and L.J. Cahill. 2000. ?Incorporating Meta-
phonemes in a Multilingual Lexicon.? In Proceedings of the
18th International Conference on Computational Linguis-
tics (COLING 2000), Saarbruecken, Germany, pp. 1126-
1130.
Trubetzkoy, N. 1939. Grundzuge der Phonologie, Vanden-
hoeck and Ruprecht, Gottingen.
Wells, J. 1989. ?Computer-coded phonemic notation of indi-
vidual languages of the European Community?, In Journal
of the International Phonetic Association, 19:1, pp.31-54.
From RAGS to RICHES: exploiting the potential of a flexible generation
architecture  
Lynne Cahill  , John Carroll  , Roger Evans  , Daniel Paiva  ,
Richard Power

, Donia Scott  and Kees van Deemter 

ITRI, University of Brighton
Brighton, BN2 4GJ, UK
Firstname.Lastname@itri.bton.ac.uk
 School of Cognitive and Computing Sciences, University of Sussex
Brighton, BN1 9QH, UK
johnca@cogs.susx.ac.uk
Abstract
The RAGS proposals for generic speci-
fication of NLG systems includes a de-
tailed account of data representation,
but only an outline view of processing
aspects. In this paper we introduce a
modular processing architecture with a
concrete implementation which aims to
meet the RAGS goals of transparency
and reusability. We illustrate the model
with the RICHES system ? a generation
system built from simple linguistically-
motivated modules.
1 Introduction
As part of the RAGS (Reference Architecture for
Generation Systems) project, Mellish et al(2000)
introduces a framework for the representation of
data in NLG systems, the RAGS ?data model?.
This model offers a formally well-defined declar-
ative representation language, which supports the
complex and dynamic data requirements of gen-
eration systems, e.g. different levels of repre-
sentation (conceptual to syntax), mixed represen-
tations that cut across levels, partial and shared
structures and ?canned? representations. However

We would like to acknowledge the financial support of
the EPSRC (RAGS ? Reference Architecture for Generation
Systems: grant GR/L77102 to Donia Scott), as well as the
intellectual contribution of our partners at Edinburgh (Chris
Mellish and Mike Reape: grant GR/L77041 to Mellish) and
other colleagues at the ITRI, especially Nedjet Bouayad-
Agha. We would also like to acknowledge the contribution
of colleagues who worked on the RICHES system previ-
ously: Neil Tipper and Rodger Kibble. We are grateful to
our anonymous referees for their helpful comments.
RAGS, as described in that paper, says very little
about the functional structure of an NLG system,
or the issues arising from more complex process-
ing regimes (see for example Robin (1994), Inuie
et al, (1992) for further discussion).
NLG systems, especially end-to-end, applied
NLG systems, have many functionalities in com-
mon. Reiter (1994) proposed an analysis of such
systems in terms of a simple three stage pipeline.
More recently Cahill et al(1999) attempted to re-
peat the analysis, but found that while most sys-
tems did implement a pipeline, they did not im-
plement the same pipeline ? different functional-
ities occurred in different ways and different or-
ders in different systems. But this survey did
identify a number of core functionalities which
seem to occur during the execution of most sys-
tems. In order to accommodate this result, a ?pro-
cess model? was sketched which aimed to support
both pipelines and more complex control regimes
in a flexible but structured way (see (Cahill et al,
1999),(RAGS, 2000)). In this paper, we describe
our attempts to test these ideas in a simple NLG
application that is based on a concrete realisation
of such an architecture1 .
The RAGS data model aims to promote com-
parability and re-usability in the NLG research
community, as well as insight into the organisa-
tion and processing of linguistic data in NLG. The
present work has similar goals for the processing
aspects: to propose a general approach to organis-
ing whole NLG systems in a way which promotes
1More details about the RAGS project, the
RICHES implementation and the OASYS subsys-
tem can be found at the RAGS project web site:
http://www.itri.bton.ac.uk/projects/rags.
the same ideals. In addition, we aim to test the
claims that the RAGS data model approach sup-
ports the flexible processing of information in an
NLG setting.
2 The RAGS data model
The starting point for our work here is the RAGS
data model as presented in Mellish et al(2000).
This model distinguishes the following five levels
of data representation that underpin the genera-
tion process:
Rhetorical representations (RhetReps) define how propo-
sitions within a text are related. For example, the sen-
tence ?Blow your nose, so that it is clear? can be con-
sidered to consist of two propositions: BLOW YOUR
NOSE and YOUR NOSE IS CLEAR, connected by a re-
lation like MOTIVATION.
Document representations (DocReps) encode information
about the physical layout of a document, such as tex-
tual level (paragraph, orthographic sentence, etc.),
layout (indentation, bullet lists etc.) and their relative
positions.
Semantic representations (SemReps) specify information
about the meaning of individual propositions. For
each proposition, this includes the predicate and its
arguments, as well as links to underlying domain ob-
jects and scoping information.
Syntactic representations (SynReps) define ?abstract?
syntactic information such as lexical features (FORM,
ROOT etc.) and syntactic arguments and adjuncts
(SUBJECT, OBJECT etc.).
Quote representations These are used to represent literal
unanalysed content used by a generator, such as
canned text, pictures or tables.
The representations aim to cover the core com-
mon requirements of NLG systems, while avoid-
ing over-commitment on less clearly agreed is-
sues relating to conceptual representation on the
one hand and concrete syntax and document ren-
dering on the other. When one considers process-
ing aspects, however, the picture tends to be a lot
less tidy: typical modules in real NLG systems
often manipulate data at several levels at once,
building structures incrementally, and often work-
ing with ?mixed? structures, which include infor-
mation from more than one level. Furthermore
this characteristic remains even when one consid-
ers more purely functionally-motivated ?abstract?
NLG modules. For example, Referring Expres-
sion Generation, commonly viewed as a single
task, needs to have access to at least rhetorical and
document information as well as referencing and
adding to the syntactic information.
To accommodate this, the RAGS data model in-
cludes a more concrete representational proposal,
called the ?whiteboard? (Calder et al, 1999), in
which all the data levels can be represented in
a common framework consisting of networks of
typed ?objects? connected by typed ?arrows?. This
lingua franca allows NLG modules to manipulate
data flexibly and consistently. It also facilitates
modular design of NLG systems, and reusability
of modules and data sets. However, it does not in
itself say anything about how modules in such a
system might interact.
This paper describes a concrete realisation of
the RAGS object and arrows model, OASYS,
as applied to a simple but flexible NLG system
called RICHES. This is not the first such re-
alisation: Cahill et al, (2000) describes a par-
tial re-implementation of the ?Caption Generation
System? (Mittal et al, 1999) which includes an
objects and arrows ?whiteboard?. The OASYS
system includes more specific proposals for pro-
cessing and inter-module communication, and
RICHES demonstrates how this can be used to
support a modular architecture based on small
scale functionally-motivated units.
3 OASYS
OASYS (Objects and Arrows SYStem) is a soft-
ware library which provides:
  an implementation of the RAGS Object and
Arrows (O/A) data representation,
  support for representing the five-layer RAGS
data model in O/A terms,
  an event-driven active database server for
O/A representations.
Together these components provide a central core
for RAGS-style NLG applications, allowing sepa-
rate parts of NLG functionality to be specified in
independent modules, which communicate exclu-
sively via the OASYS server.
The O/A data representation is a simple
typed network representation language. An O/A
database consists of a collection of objects, each
of which has a unique identifier and a type, and
arrows, each of which has a unique identifier,
a type, and source and target objects. Such a
database can be viewed as a (possibly discon-
nected) directed network representation: the fig-
ures in section 5 give examples of such networks.
OASYS pre-defines object and arrow types re-
quired to support the RAGS data model. Two ar-
row types, el (element) and el(<integer>),
are used to build up basic network structures ?
el identifies its target as a member of the set rep-
resented by its source, el(3), identifies its tar-
get as the third element of the tuple represented
by its source. Arrow type realised by re-
lates structures at different levels of representa-
tion. for example, indicating that this SemRep
object is realised by this SynRep object. Arrow
type revised to provides for support for non-
destructive modification of a structure, mapping
from an object to another of the same type that
can be viewed as a revision of it. Arrow type
refers to allows an object at one level to indi-
rectly refer to an object at a different level. Object
types correspond to the types of the RAGS data
model, and are either atomic, tuples, sets or se-
quences. For example, document structures are
built out of DocRep (a 2-tuple), DocAttr (a set
of DocFeatAtoms ? feature-value pairs), DocRe-
pSeq (a sequence of DocReps or DocLeafs) and
DocLeafs.
The active database server supports multiple
independent O/A databases. Individual modules
of an application publish and retrieve objects and
arrows on databases, incrementally building the
?higher level?, data structures. Modules com-
municate by accessing a shared database. Flow
of control in the application is event-based: the
OASYS module has the central thread of execu-
tion, calls to OASYS generate ?events?, and mod-
ules are implemented as event handlers. A mod-
ule registers interest in particular kinds of events,
and when those events occur, the module?s hander
is called to deal with them, which typically will
involve inspecting the database and adding more
structure (which generates further events).
OASYS supports three kinds of events: pub-
lish events occur whenever an object or arrow is
published in a database, module lifecycle events
occur whenever a new module starts up or termi-
nates, and synthetic events ? arbitrary messages
passed between the modules, but not interpreted
by OASYS itself ? may be generated by mod-
ules at any time. An application starts up by ini-
tialising all its modules. This generates initialise
events, which at least one module must respond
to, generating further events which other modules
may respond to, and so on, until no new events
are generated, at which point OASYS generates
finalise events for all the modules and terminates
them.
This framework supports a wide range of archi-
tectural possibilities. Publish events can be used
to make a module wake up whenever data of a
particular sort becomes available for processing.
Lifecycle events provide, among other things, an
easy way to do pipelining: the second module in a
pipeline waits for the finalise event of the first and
then starts processing, the third waits similarly
for the second to finalise etc. Synthetic events
allow modules to tell each other more explicitly
that some data is ready for processing, in situa-
tion where simple publication of an object is not
enough.
RICHES includes examples of all three
regimes: the first three modules are pipelined us-
ing lifecycle events; LC and RE, FLO and REND
interact using synthetic events; while SF watches
the database specifically for publication events.
4 RICHES
The RICHES system is a simple generation sys-
tem that takes as input rhetorical plans and pro-
duces patient advice texts. The texts are intended
to resemble those found at the PharmWeb site
(http://www.pharmweb.net). These are
simple instructional texts telling patients how to
use certain types of medicines, such as nosedrops,
eye drops, suppositories etc.. An example text
from PharmWeb is shown in figure 1, alongside
the corresponding text produced by RICHES.
The main aim of RICHES is to demonstrate
the feasibility of a system based on both the RAGS
data model and the OASYS server model. The
modules collectively construct and access the data
representations in a shared blackboard space and
this allows the modules to be defined in terms of
their functional role, rather than say, the kind of
data they manipulate or their position in a pro-
cessing pipeline. Each of the modules in the sys-
 How to Use Nose Drops
1. Blow your nose gently, so that it is clear. 
  
2. Wash your hands. 
  
3. Unscrew the top of the bottle and draw some liquid into the dropper. 
  
4. Tilt your head back. 
  
5. Hold the dropper just above your nose and put the correct number of drops into your nostril. 
  
6. DO NOT let the dropper touch the inside of your nose. 
  
7. Keep your head tilted back for two to three minutes to help the drops run to the back of your nose. 
  
8. Replace the top on the bottle. 
 
KEEP ALL MEDICINES OUT OF THE REACH OF CHILDREN 
PharmWeb - Copyright?1994-2001. All rights reserved
  
  
Blow your nose so that it is clear. 
Wash your hands
Unscrew the top. Then draw the liquid into the dropper. 
Tilt your head back
Hold the dropper above your nose. Then put the drops into your nostril.
The dropper must not touch the inside.
Keep your head tilted back for two to three minutes so that the drops run to the back.
Replace the top on the bottle
Generated by RICHES version 1.0 (9/5/2001) on 9/5/2001 
?2001, ITRI, University of Brighton 
Figure 1: An example text from PharmWeb, together with the corresponding text generated by RICHES
tem is in itself very simple ? our primary interest
here is in the way they interact.
Figure 2 shows the structure of the system2.
The functionality of the individual modules is
briefly described below.
Rhetorical Oracle (RO) The input to the sys-
tem is a RhetRep of the document to be gen-
erated: a tree with internal nodes labelled with
(RST-style) rhetorical relations and RhetLeaves
referring to semantic proposition representations
(SemReps). RO simply accesses such a represen-
tation from a data file and initialises the OASYS
database.
Media Selection (MS) RICHES produces doc-
uments that may include pictures as well as text.
As soon as the RhetRep becomes available, this
module examines it and decides what can be il-
lustrated and what picture should illustrate it. Pic-
2The dashed lines indicate flow of information, solid ar-
rows indicate approximately flow of control between mod-
ules, double boxes indicate a completely reused module
(from another system), while a double box with a dashed
outer indicates a module partially reused. Ellipses indicate
information sources, as opposed to processing modules.
tures, annotated with their SemReps, are part of
the picture library, and Media Selection builds
small pieces of DocRep referencing the pictures.
Document Planner (DP) The Document Plan-
ner, based on the ICONOCLAST text planner
(Power, 2000) takes the input RhetRep and pro-
duces a document structure (DocRep). This
specifies aspects such as the text-level (e.g.,
paragraph, sentence) and the relative or-
dering of propositions in the DocRep. Its
leaves refer to SynReps corresponding to syntac-
tic phrases. This module is pipelined after MS,
to make sure that it takes account of any pictures
that have been included in the document.
Lexical Choice (LC) Lexical choice happens in
two stages. In the first stage, LC chooses the lex-
ical items for the predicate of each SynRep. This
fixes the basic syntactic structure of the proposi-
tion, and the valency mapping between semantic
and syntactic arguments. At this point the ba-
sic document structure is complete, and the LC
advises REND and SF that they can start pro-
cessing. LC then goes into a second phase, in-
TEXT
SENTENCE
RHETORICAL 
ORACLE
LEXICAL
FINALISER
RENDERER
LINGO
PICTURE
LIBRARY
SELECTION
MEDIUM FLO
LEXICON
CHOICE
OASYS
REFERRING
EXPRESSIONS
DOCUMENT
PLANNER
Figure 2: The structure of the RICHES system
terleaved with RE and FLO: for each sentence,
RE determines the referring expressions for each
noun phrase, LC then lexicalises them, and when
the sentence is complete FLO invokes LinGO to
realise them.
Referring Expressions (RE) The Referring
Expression module adapts the SynReps to add in-
formation about the form of a noun phrase. It de-
cides whether it should be a pronoun, a definite
noun phrase or an indefinite noun phrase.
Sentence Finaliser (SF) The Sentence Fi-
naliser carries out high level sentential organisa-
tion. LC and RE together build individual syntac-
tic phrases, but do not combine them into whole
sentences. SF uses rhetorical and document struc-
ture information to decide how to complete the
syntactic representations, for example, combin-
ing main and subordinate clauses. In addition, SF
decides whether a sentence should be imperative,
depending on who the reader of the document is
(an input parameter to the system).
Finalise Lexical Output (FLO) RICHES uses
an external sentence realiser component with its
own non-RAGS input specification. FLO provides
the interface to this realiser, extracting (mostly
syntactic) information from OASYS and convert-
ing it to the appropriate form for the realiser. Cur-
rently, FLO supports the LinGO realiser (Carroll
et al, 1999), but we are also looking at FLO mod-
ules for RealPro (Lavoie and Rambow, 1997) and
FUF/SURGE (Elhadad et al, 1997).
Renderer (REND) The Renderer is the module
that puts the concrete document together. Guided
by the document structure, it produces HTML for-
matting for the text and positions and references
the pictures. Individual sentences are produced
for it by LinGO, via the FLO interface. FLO actu-
ally processes sentences independently of REND,
so when REND makes a request, either the sen-
tence is there already, or the request is queued,
and serviced when it becomes available.
LinGO The LinGO realiser uses a wide-
coverage grammar of English in the LKB HPSG
framework, (Copestake and Flickinger, 2000).
The tactical generation component accepts in-
put in the Minimal Recursion Semantics formal-
ism and produces the target text using a chart-
driven algorithm with an optimised treatment of
modification (Carroll et al, 1999). No domain-
specific tuning of the grammar was required for
the RICHES system, only a few additions to the
lexicon were necessary.
5 An example: generation in RICHES
In this section we show how RICHES generates
the first sentence of the example text, Blow your
nose so that it is clear and the picture that accom-
panies the text.
The system starts with a rhetorical represen-
tation (RhetRep) provided by the RO (see Fig-
ure 3)3. The first active module to run is MS
3In the figures, labels indicate object types and the sub-
script numbers are identifiers provided by OASYS for each
which traverses the RhetRep looking at the se-
mantic propositions labelling the RhetRep leaves,
to see if any can be illustrated by pictures in the
picture library. Each picture in the library is en-
coded with a semantic representation. Matching
between propositions and pictures is based on the
algorithm presented in Van Deemter (1999) which
selects the most informative picture whose repre-
sentation contains nothing that is not contained in
the proposition. For each picture that will be in-
cluded, a leaf node of document representation is
created and a realised by arrow is added to it
from the semantic proposition object (see Figure
4).
  	


  



el(1) el(2)
  		
(motivation)
  	ffReinterpretation of an existing?NLG system in a Generic Generation 
Architecture 
L. Cahill, C. Doran~ R. Evans, C. Meilish, D. Paiva,:M. Reape, D. Scott,, N. Tipper 
.Universities of Brighton and Edinburgh. 
Email rags@itri, brighton, ac. uk 
Abstract 
The RAGS project aims to define a reference ar- 
chitecture for Natural Language Generation (NLG) 
systems. Currently the major part of this archi- 
tecture consists of a set of datatype definitions for 
specifying the input and output formats for mod- 
ules within NLG systems. In this paper we describe 
our efforts to reinterpret an existing NLG system in 
terms of these definitions. The system chosen was 
the Caption Generation System. 
2. Which aspects of the RAGS repertoire would 
: . . . .  . . . .  - .... -,.= ., ~,~,aemaltybe'requireti~ftrr~strch~a-~reinterpretation; 
which would be unnecessary and which addi- 
tions to the RAGS repertoire would be moti- 
vated. 
1 Introduction 
The RAGS project ~ aims to define a reference ar- 
chitecture for natural anguage generation systems. 
Currently the major part of this architecture consists 
of a set of datatype definitions for specifying the 
input and output formats for modules within NLG 
systems. The intention is that such representations 
can be used to assist in reusability of components 
of NLG systems. System components that adhere 
to these representations, or use a format hat can be 
translated into such representations relatively eas- 
ily, can then, in principle, be substituted into other 
systems. Also, individual components could be de- 
veloped without the need for a complete system if 
datasets, based on the representations, were made 
available. 
In this paper we describe an attempt to reinterpret 
an existing NLG system in terms of the RAGS data 
definitions. The point of this exercise was to lem-n: 
1. Whether these data structures were sufficient 
to describe the input and output functionality 
of an existing, independently developed, ap- 
3. Whether studying the system would generate 
good ideas about possible reusable generation 
modules that could be developed. 
In this exercise it was important o choose a sys- 
tem that had been developed by people outside the 
RAGS project. Equally, it was important o have 
sufficient clear information about the system in the 
available literature, and/or by means of personal 
contact with the developers. The system chosen was 
the Caption Generation System (Mittal et al, 1995; 
Mittal et al, 1998) 3. This system was chosen be- 
cause, as well as fulfilling the criteria above, it ap- 
peared to be a relatively simple pipeline, thus avoid- 
ing complex control issues, with individual modules 
performing the varied linguistic tasks that the RAGS 
data structures had been designed to handle. 
The reinterpretation exercise took the form of 
coming up with an account of how the interfaces 
to the CGS modules corresponded to the RAGS 
model and reimplementing a working version of 
each module (apart from Text Planning and Realisa- 
tion) which was tested to ensure that, given appro- 
priate input, its output was correct (i.e. conforming 
to the global account) on key examples. Naturally, 
given the scope of this exercise, we had to gloss over 
some interesting implementational issues. The aim 
was not to produce a complete system or a system 
as good as CGS, but merely to demonstrate hat the 
broad functionality of the system could be repro- 
plied 2 NLG system. 
? Now at the MITRE Corporation, Bedford, MA, USA, 
cdoran.?mitre, org. 
tThis work was supported by ESPRC grants GR/L77041 
(Edinburgh) and GR/L77102 (Brighton), RAGS: Reference Ar- 
chitecture for Generation Systems. 
-'See (Paiva, 1998) for a definition of applied in this specific 
context. 
" . -ducedwithin:the RAGS .structures. 
In this paper we first describe the RAGS data 
structures. We then describe the CGS system 
3In addition to these published sources, we were greatly 
helped by the developers of the system who gave us the ben- 
efit of their own expertise as well as access to the original code 
of the system and a technical report hat included implementa- 
tional details such as system traces. 
69 
followed by our reinterpretation of the system in Abstract Rhetorical Abstract Rhetorical Repre- 
RAGS terms. Finally we discuss,, the :implications:. :. -._..sentations ,are--tree-structures with,rhetorical .rela- 
for RAGS of this exercise, tions at the internal nodes and Abstract Rhetorical 
2 The RAGS datatypes  
The RAGS project initially set out to develop a ref- 
erence architecture based on the three-stage pipeline 
suggested by Reiter (Reiter, 1994). However, a 
trees or Abstract Semantic Representations at the 
leaves. 
Rhetorical Abstract Rhetorical Representations 
are viewed as descriptions of sets of possible 
Rhetorical Representations. Each one may be trans- 
detailed analysis of existing applied NLG systems formed into some subset of the possible Rhetori- 
(Cahill and Reape~_~ l:998}:suggested~,that~ttch.an~ ar -~: ~<.eaLReprese, ntations by,,means ~ofa,set..o_f~.petmitted 
chitecture was not specific enough and not closely transformations, e.g. reversing the order of nucleus 
enough adhered to by the majority of the systems 
surveyed for this to be used as the basis of the archi- 
tecture. 
The abstract functionality of a generation system 
can be specified without specific reference to pro- 
cessing. The RAGS approach to this is to develop a 
data model, that is, to define the functional modules 
entirely in terms of the datatypes they manipulate 
and the operations they can perform on them. On 
top of such a model, more specific process models 
can be created in terms of constraints on the order 
and level of instantiation of different ypes of data in 
the data model. A 'rational reconstnaction' of some 
pipeline model might then be produced, but other 
process models would also be possible. 
The RAGS levels of representation are as fol- 
lows4: 
Conceptual The conceptual level of representa- 
tion is defined only indirectly through an API via 
which a knowledge base (providing the content 
from which generation takes place) can be viewed 
as if it were defined in a simple KL-ONE (Brach- 
man and Schmolze, 1985) like system. 
Abstract Semantic Abstract semantic representa- 
tions are the first level at which semantic predicates 
are associated with arguments. At this level, seman- 
tic predicates and roles are those used in the API to 
query the knowledge base and arguments are knowl- 
edge base entities. 
Semantic (Concrete) semantic representations 
provide a complete notation for "logical forms" 
where there is no longer any reference to ,the knowl- 
edge base. The representations are based on sys- 
tems such as SPL (Kasper, 1989) and DRT (Kamp 
and Reyle, 1993). 
4More details can be found in (Cahill et 
al., 1999) and at the RAGS project web site: 
ht tp  : / /www.  i t r i  . b r ighton ,  ac. uk/rags.  
and satellite or changing the rhetorical relation to 
one within a permitted set. 
Abstract Document Document structure defines 
the linear ordering of the constituents of the Rhetor- 
ical Representation with a POSITION feature, as 
well as two other features, TEXT-LEVEL, which 
takes values such as paragraph or sentence; and 
LAYOUT, which takes values such as wrapped-text 
and vertical list. It takes the form of a tree, usu- 
ally, but not necessarily, isomorphic to the Rhetor- 
ical Representation a d linked to it, but with these 
three features at the nodes instead of rhetorical rela- 
tions. 
Abstract Syntactic Abstract Syntactic Represen- 
tations capture high-level aspects of syntactic struc- 
ture in terms of notions such as lexical head, speci- 
fiers, modifiers and complements. This level of rep- 
resentation is compatible with approaches such as 
LFG f-structure, HPSG and Meteer's Text Structure. 
3 Partial and Mixed Representations 
For all of the RAGS levels partial representations 
are possible. Without this, it is not possible for a 
module to pass any result to another until that re- 
sult is completely determined, and this would im- 
pose an unwanted bias towards simple pipeline ar- 
chitectures into the model. There are many cases 
in NLG where a representation is built collabora- 
tively by several modules. For instance, many sys- 
tems have a referring expression generation module 
whose task is to complete a semantic representation 
which lacks those structures which will be realised 
as NPs. Such a functionality cannot be described 
unless partially complete semantic representations 
can be communicated. 
In addition, mixed representations are possible, 
where (possibly partial) representations at several 
levels are combined with explicit links between the 
elements. Many NLG modules have to be sensi- 
70 
tive to a number of levels at once (consider, for 
.......... instance, -aggregatiomxeferring,expmssion.,genera- 
tion and lexicalisation, all of which need to take 
into account rhetorical, semantic and syntactic on- 
straints). The input to most reusable realisation sys- 
tems is also best viewed as a mixture of semantic 
and abstract syntactic information. 
The extra flexibility of having partial and mixed 
representations turned out to be vital in the recon- 
struction of the CGS system. (Mellish et al, 2000). 
4 The CGS system 
The Caption Generation System (CGS) generates 
explanatory captions of graphical presentations (2- 
D charts and graphs). Its architecture is a pipeline 
with several modules, shown in the left hand part of 
Figure 1. An example of a diagram and its accom- 
panying text are given in Figure 2. The propositions 
are numbered for ease of reference throughout the 
paper. 
The input to CGS is a picture representation 
(graphical elements and its mapping from the data 
set) generated by SAGE plus its complexity metric. 
The text planning module (Moore and Paris (1993)) 
plans an explanation i  terms of high level discourse 
goals. The output of the planner is a partially or- 
dered plan with speech-acts as leaves. 
The ordering module receives as input the dis- 
course plan with links specifying the ordering re- 
lations between sub-trees and specifies an order for 
them based on heuristics uch as that the description 
should be done from left to right in the visual space. 
The aggregation module "only conjoins pairs of 
contiguous propositions about the same grapheme 
type 5 in the same space" (Mittai et al, 1999) and 
inserts cue phrases compatible with the propositions 
e o ( .=., "whereas" for contrastive ones). The internal 
order of the sentence constituents i determined by 
the centering module using an extension of the cen- 
tering theory of Grosz and colleagues (Grosz et al, 
1995). 
The referring expression module uses Date and 
Reiter's (Dale and Reiter, 1995) algorithm to con- 
struct the set of attributes that can uniquely identify 
a referent. There are'two, situations where the text 
planning module helps specifically in the generation 
of referring expressions: (1) when the complexity 
for expressing a graphic demands an example and 
5"Graphemes are the basic building blocks for constructing 
pictures. Marks, text, lines and bars are some of the different 
grapheme classes available in SAGE." (IVlittal et al, 1999). 
CGS architecture 
SAGE 
RAGS representations 
I I I  I I I  IV  V 
I II HI  IV  V 
? I I I  I I I  IV  " V . 
I I I  In  IV  v 
l -  .......... I /11  
1 It  I I I  IV V 
l -  .......... 
I 11 11I IV V 
.......... III1  
I I I  HI  IV  V 
; "  .......... I I I I I  
FUF 
Figure 1: A RAGS view of the CGS system. The 
labels for the RAGS representations refer to the fol- 
lowing: I = conceptual; II = semantic; III = rhetori- 
cal; IV = document; V = syntactic. 
it signals this both to SAGE (for highlighting the 
corresponding grapheme) and to the rest of the text 
generation modules; and (2) when in a specific sit- 
uation the referring algorithm would need several 
interactions for detecting that an entity is unique in 
? a certain visual space and.the planning could detect 
it in the construction of the description of this space. 
When this occurs, the text planner "circumvents he 
problem for the:.referring ,expression :module at the 
planning stage itself, processing the speech-acts ap- 
propriately to avoid this situation completely". 
After lexicalisation, which adds lexeme and ma- 
jor category information, the resulting functional 
descriptions are passed to the FUF/SURGE realiser 
that generates texts like the caption of Figure 2. 
71 
\ [ \ ]  
O 
te l  
O 
IZl 
ZS:3 
I21 ,:7-. ,,S . . . .  ; . . . .  .' ? 
O ~ ~Ipc~ q~L~ 
\] 
I 
\] 
=::::::;=a___.,____.__,_______~ 
. ,  , : ,  ; .  . ,  
Figure 2: (1) These two charts present information about house sales from data-set ts-1740. (2) In the two 
charts, the y-axis indicates the houses. (7) In the first chart, the left edge of the bar shows the house's elling 
price whereas (8) the right edge shows the asking price. (3) The horizontal position of the mark shows the 
agency estimate. (4) The color shows the neighbourhood and (5) shape shows the listing agency. (6) Size 
shows the number of rooms. (9) The second chart shows the number of days on the market. 
5 Reinterpretat ion f  CGS in RAGS 
Our reinterpretation f the CGS system defines the 
interfaces between the modules of CGS in terms 
of the RAGS data structures discussed above. In 
this section we discuss the input and output inter- 
faces for each CGS module in turn as well as any 
problems we encountered in mapping the structures 
into RAGS structures. Figure 1 shows the incre- 
mental build-up of the RAGS data levels across 
the pipeline. Here we have collapsed the Abstract 
Rhetorical and Rhetorical and the Abstract Seman- 
tic and Semantic. It is-interesting to note that the 
build up of levels of representation does not tend to 
correspond exactly with module boundaries. 
One of the major issues we faced in' our reinter- 
pretation was where to produce representations (or
partial representations) whose emergence was not 
defined clearly in the descriptions of CGS. For in- 
stance, many decisions about document structure 
are made only implicitly by the system. In most 
cases we have opted to produce all types of repre- 
sentations at the earliest point where they can con- 
ceivably have any content. This means, for instance, 
that our reimplementation assumes an (unimple- 
mented) text planner which produces an Abstract 
Rhetorical Representation with Abstract Semantic 
leaves and an Abstract Document Representation. 
Text Planner The input to the Longbow text plan- 
ner discussed in section 4 above is a representation 
of a picture in SAGE format (which has been an- 
notated to indicate the types of complexity of each 
grapheme) together with a goal, which can typi- 
cally be interpreted as "describe". It outputs an es- 
sentially fiat sequence of plan operators, each of 
which corresponds in the output? text .to .a.speech 
act. In our reinterpretation, we have assumed that 
this fiat structure needs to be translated into an Ab- 
stract Rhetorical Representation with (at least) min- 
imal structure. Such a structure is implicit in the 
plan steps, and our interpretation f the rhetorical 
structure for the example text corresponds closely to 
that of the post-processing trace produced by CGS. 
72 
I .AYOI  FII" * 'upped tel l  
" IU  ,I.EVIZL. p J t l~aph 
~ f ~ I O N :  2 
POSlllON I 
I.AYOtr'I+: -~pped tell 
TEX"T.L~ VEL 
(1) 
POSITION: I POSITION: 2 
LAYOUT: *T~,pl~n.l teat 
"IEXT-LEVEL: + 
(2) 
Po$ : I POSITION: 1 
LAYOUT: -mtpFcd te~t 
. TE.ICr-t.EVEL~ ?
0OSFI-K~N. 2 PosmoN: i 
POSIllON: I PosrnoN: I POsmoN. ~ FoSmON: 4 POSt'nON I PosrnoN: 2 
LAYOUT: ~pp~d lesl LAYOU'T. ~ppe,.f ~xt LAYO\[rF. ~apped lesl LAYOUT: ~+r~pS~d I?xt LAYOUT. ~'?~l~,Od ~est LAYOUT: ~Tappe~ text 
TEXT,LEVEL  7 "II~XT,LEVEI.: ~ "II~XT-LEVEL ? "I I~XT-LEVEL: ? TEXT-LEVEL  "+ TIE~XT-L.EVI:I.: ? 
(3) (4) (5) (6) (7) (8) 
Figure 3: Initial Document Structure 
. . .Z., 
However, we are still not entirely sure 
exactly CGS creates this structure, so 
posed it at the very beginning, onto the 
text planner. 
Already at this stage it is necessary 
about where 
we have im- 
output of the 
to make use 
of mixed RAGS representations. As well as this 
Abstract Rhetorical Representation, the text planner 
has to produce an Abstract Document Representa- 
tion, linked to the Abstract Rhetorical Representa- 
tion. This is already partially ordered - although the 
exact value of POSITION features cannot be speci- 
fied at this stage, the document tree is constructed 
so that propositions are already grouped together. 
In addition, we make explicit certain default infor- 
mation that the CGS leaves implicit at this stage, 
namely, that the LAYOUT feature is always wrapped 
text and that the TEXT-LEVEL feature of the top 
node is always paragraph. 
Ordering The ordering module takes the Abstract 
Document Representation a d the Abstract Rhetor- 
ical Representation as input and outputs an Abstract 
Document Representation with the POSITION fea- 
ture 's  value filled,for all :the nodes, .That is, it fixes. ? 
the linear order of the final output of the speech acts. 
In our example, the ordering is changed so that steps 
7 and 8 are promoted to appear before 3, 4, 5 and 6. 
The resulting structure is shown in figure 36 . 
6In this and the.following diagrams, objects are represented 
by circles with (labelled) arrows indicating the relations be-- 
Aggregation Although aggregation might seem 
like a self-contained process within NLG, in prac- 
tice it can make changes at a number of levels of 
representation a d indeed it may be the last opera- 
tion that has an effect on several levels. The aggre- 
gation module in our reinterpretation thus has the fi- 
nal responsibility to convert an Abstract Rhetorical 
Representation with Abstract Semantic Represen- 
tation leaves into a Rhetorical Representation with 
Semantic Representation leaves. The new Rhetori- 
cal Representation may be different from before as 
a result of speech acts being aggregated but whether 
different or not, it can now be considered final as 
it will no longer be changed by the system. The 
resulting Semantic Representations are no longer 
Abstract because further structure may have been 
determined for arguments to predicates. On the 
other hand, referring expressions have not yet been 
generated and so the (Concrete) Semantic Repre- 
sentations cannot be complete. The reconstruc- 
,.tion createspartia.i Semantic Representations with 
"holes" where the referring expressions (Semantic 
Representations) will be inserted. These "holes" are 
linked back to the knowledge base entities tfiat they 
correspond to. 
Because Aggregation affects text levels, it also af- 
fects the Abstract Document Representation, which 
has its TEXT-LEVEL feature's values all filled at this 
tween them. Dashed arrows indicate links between different 
levels of representation. 
73 
SemRep 
fun(Role,SemRep) 
DR preS  , . mRep 
? AbsSynRep ~ AbsSynRe~ 
/ " \  Y^.Z(" 
FVM (~ ~ ,un(Funs,~gS~c) (,~ ~M (~) lun(Funs.ArgSpec) 
0 ~ 0 0 
? + 
Adjs 
. . - ; . .  
Figure 4: Syntactic representations constructed by Centering 
point. It may also need to change the structure 
of the Abstract Document Representation, for in- 
stance, adding in a node for a sentence above two, 
now aggregated, clause nodes. 
Centering Because Centering comes before Re- 
ferring Expression generation and Realisation, all it 
can do is establish constraints that must be heeded 
by the later modules. At one stage, it seemed as if 
this required communicating a kind of information 
that was not covered by the RAGS datatypes. How- 
ever, the fact that an NP corresponds (or not) to a 
center of some kind can be regarded as a kind of 
abstract syntactic information. The reconstruction 
therefore has the centering module building a partial 
(unconnected) Abstract Syntactic representation for 
each Semantic Representation that will be realised 
as an NP, inserting a feature that specifies whether 
it constitutes a forward- or backward-facing cen- 
ter, approximately following Grosz et al(Grosz et 
al., 1995). This information is used to determine 
whether active or passive voice will be used. An 
example of such a partial Abstract Syntactic Repre- 
sentation is given in Figure 4. 
Referring Expression In our reconstruction of 
the CGS system, we have deviated from reproduc- 
ing the exact functionality for the referring expres- 
sion module and part of the lexical choice module. 
In the CGS system, the referring expression module 
computes association lists which can be used by the 
lexical choice module to construct referring expres- 
sions suitable for realisation. In our reconstruction, 
however, the referring expression module directly 
computes the Semantic Representations of referring 
expressions. 
We believe that this is a good example of a 
case where developing a system with the RAGS 
data structures in mind simplifies the task. There 
are undoubtedly many different ways in which the 
same results could be achieved, and there are many 
(linguistic, engineering etc.) reasons for choosing 
one rather than another. Our particular choice is 
driven by the desire for conceptual simplicity, rather 
than any strictly linguistic or computational motiva- 
tions. We considered for each module which RAGS 
level(s) it contributed to and then implemented it to 
manipulate that (or those) level(s). In this case, that 
meant a much more conceptually simple module 
which just adds information to the Semantic Rep- 
resentations. 
Lexical Choice In CGS, this module performs a 
range of tasks, including what we might call the 
later.stages of_referring expression generation and 
lexical choice, before converting the plan leaves 
into FDs (Functional Descriptions), which serve as 
the input to the FUF/SURGE module. In the re- 
construction, on the other hand, referring expres- 
sions have already been computed and the Rhetor- 
ical Representation, with its now complete Seman- 
tic Representations, needs to be "lexicalised" and 
74 
' ,t ~1  
" .  set 
Figure 5: Combined Semantic and Abstract Syntactic Representation 
translated into FUF/SURGE format. Lexicalisa- 
tion in our terms involves adding the lexeme and 
major category information to the Abstract Syntac- 
tic Representations for the semantic predicates in 
each Semantic Representation. The FUF/SURGE 
input format was regarded as a combination of Se- 
mantic and Abstract Syntactic information, and this 
can easily be produced from the RAGS representa- 
tions. The combined Semantic and Abstract Syn- 
tactic Representations for the plan step "These two 
charts present information about house sales from 
data set ts-1740" is shown in Figure 5. The boxes 
indicate suppressed subgraphs of the lexemes cor- 
responding to the word in the boxes and triangles 
indicate suppressed subgraphs of the two adjuncts. 
6 Conclusions 
The reconstruction of CGS has taken the form of 
working out in detail the RAGS representations 
passed between modules at each stage for a set 
of key examples and reimplementing the modules 
(apart from the Planner and Realiser) in a way that 
correctly reproduces these representations. The ac- 
tual implementation used an incrementally growing 
data store for the RAGS representations which the 
modules accessed in turn, though the passing of data 
could also have been achieved in other ways. 
The fact that the reconstruction has been success- 
ful indicates that the RAGS architecture is broadly 
adequate to redescribe this NLG system: 
? No changes to the existing levels of represen- 
tation were needed, though it was necessary to 
make extensive use of partial and mixed repre- 
sentations. 
o No new levels of representation needed to be 
introduced to capture the inter-module com- 
munication of the system. 
o All of the levels of representation_apart from 
the Conceptual level were used significantly in
the reconstruction. 
In some ways, i t  is unfortunate that none of the 
inter-module interfaces of CGS turned out to use a 
single level of RAGS representation. Given the mo- 
tivation for partial and mixed representations above, 
however, this did not really come as a surprise. It 
may well be that any really useful reusable modules 
for NLG will have to have this complexity. 
75 
In spite of the successful testing of the RAGS data 
model, somedifficulties were encountered: 
* It was difficult to determine the exact nature 
of the representations produced by the Planner, 
though in the end we were able to develop a 
system to automatically translate these into a 
format we could deal with. 
o Although the theoretical model o f  CGS has a 
simple modular structure, in practice the mod- 
ules are very tightly inte-gr~ifed and making-the " 
exact interfaces explicit was not always easy. 
? Referring expression generation requires fur- 
ther access to the "knowledge base" holding 
information about he graphic to be produced. 
This knowledge was only available via interac- 
tions with SAGE, and so it was not possible to 
determine whether the RAGS view of Concep- 
tual Representations was applicable. Our own 
implementation f referring expression gener- 
ation had to work around this problem in a non- 
portable way. 
? It became clear that there are many housekeep- 
ing tasks that an NLG system must perform 
following Lexical Choice in order for the final 
Semantic and Abstract Syntactic Representa- 
tions to be appropriate for direct input to a re- 
alisation system such as FUF. 
o The fact that the system was driving 
FUF/SURGE seems to have had a signif- 
icant effect on the internal representations 
used by CGS. The reconstruction echoed this 
and as a result may not be as general as could 
be desired. 
? Even though CGS only performs imple types 
of Aggregation, it is clear that this is a critical 
module for determining the final form of sev- 
eral levels of representation. 
The division of CGS into modules is different from 
that used in any NLG systems we have previously 
worked on and so has been a useful stimulus to think 
about ways in which reusable modules can be de- 
signed. We envisage reusmgat  least,the reimple- 
mentation of the Centering module in our further 
work. 
References 
R. Brachman and J. Schmolze. 1985. An overview of the KL- 
ONE knowledge representation system. Cognitive Science, 
9:171-216. 
Lynne Cahill and Mike Reape. 1998. Component asks 
in applied NLG .systems . . . .  Technical Report ITR!- 
99-05, ITRI, University of Brighton. obtainable at 
http:/lwww.itri.brighton.ac.uk/projects/rags/. 
Lynne Cahill, Christy Doran, Roger Evans, Chris Mellish, 
Daniel Paiva, Mike Reape, Donia Scott, and Neil Tipper. 
1999. In Search of a Reference Architecture for NLG Sys- 
tems. In Proceedings of the 7th European Workshop on Nat- 
ural Language Generation, pages 77-85, Toulouse. 
Robert Dale and Ehud Reiter. 1995. Computational interpre- 
tations of the Gricean maxims in the generation ofreferring 
expressions. Cognitive Science, 18:233-263. 
B J .  Grosz, A/K.J6shil-and S.Weinstein. 1995~ Centering: a 
framework for modelling the local coherence of discourse. 
Computational Linguistics, 21 (2):203-226. 
H. Kamp and U. Reyle. 1993. From discourse to logic: Intro- 
duction to model theoretic semantics of natural language, 
formal logic and discourse representation theory. Kluwer, 
Dordrecht; London. 
R. T. Kasper. 1989. A flexible interface for linking applica- 
tions to penman's sentence generator. In Proceedings of the 
DARPA Speech and Natural Language Workshop, Philadel- 
phia. 
C. Mellish, R. Evans, L. Cahill, C. Doran, D. Paiva, M. Reape, 
D. Scott, and N. Tipper. 2000. A representation forcomplex 
and evolving data dependencies in generation. In Proceed- 
ings of the Applied Natural Language Processing (ANLP- 
NAACL2000) Conference, Seattle. 
V. O. Mittal, S. Roth, J. D. Moore, J. Mattis, and G. Carenini. 
1995. Generating explanatory captions for information 
graphics. In Proceedings of the 15th International Joint 
Conference on Artificial Intelligence (IJCAI'95), pages 
1276-1283, Montreal, Canada, August. 
V. O. Mittal, J. D. Moore, G. Carenini, and S. Roth. 1998. 
Describing complex charts in natural anguage: A caption 
generation system. Computational Linguistics, 24(3):431- 
468. 
Daniel Paiva. 1998. A survey of applied natural lan- 
guage generation systems. Technical Report ITRI- 
98-03, Information Technology Research Insti- 
tute (ITRI), University of Brighton. Available at 
http://www.itri.brighton.ac.uk/techreports. 
Ehud Reiter. 1994. Has a consensus NL generation architec- 
ture appeared and is it psycholinguistically p ausible? In 
Proceedings of the Seventh International Workshop on Nat- 
ural Language Generation, pages 163-170, Kennebunkport, 
Maine. 
Acknowledgements 
We would like to thank the numerous people who have 
helped us in this work. The developers of CGS, especially 
Giuseppe Carenini and Vibhu Mittal; the RAGS consultants 
and other colleagues at Brighton and Edinburgh, who have con- 
tributed greatly to our development ofthe representations; and 
finally to the anonymous reviewers of this paper. 
76 
Proceedings of the Twelfth Meeting of the Special Interest Group on Computational Morphology and Phonology (SIGMORPHON2012), pages 35?41,
Montre?al, Canada, June 7, 2012. c?2012 Association for Computational Linguistics
A rule-based approach to unknown word recognition in Arabic 
 
 
Lynne Cahill 
NLTG, University of Brighton
Lewes Rd, Brighton 
BN2 4GJ, UK 
L.Cahill@brighton.ac.uk 
 
 
 
Abstract 
This paper describes a small experiment to 
test a rule-based approach to unknown word 
recognition in Arabic. The morphological 
complexity of Arabic presents its challenges 
to a variety of NLP applications, but it can al-
so be viewed as an advantage, if we can tap 
into the complex linguistic knowledge associ-
ated with these complex forms. In particular, 
the derived forms of verbs can be analysed 
and an educated guess at the likely meaning of 
a derived form can be predicted, based on the 
meaning of a known form and the relationship 
between the known form and the unknown 
one. The performance of the approach is test-
ed on the NEMLAR Written Arabic Corpus. 
1 Introduction 
The Semitic languages, especially Arabic, are lin-
guistically interesting for a number of reasons, and 
are attracting more and more attention for both 
linguistic and socio-political reasons. One of the 
aspects of Arabic that makes it particularly inter-
esting to linguists, namely the morphological com-
plexity, is at once both appealing and the source of 
potential practical problems. It is appealing to lin-
guists, for whom it offers interesting challenges in 
their descriptive frameworks, but for builders of 
NLP applications, it represents a significant chal-
lenge. In this paper, we are particularly interested 
in the derivational aspects of the morphology, 
whereby verb stems are derived from triliteral 
roots in well defined formal ways, and with vary-
ing degrees of regularity in the meanings of those 
derived forms.  
Another aspect of the Arabic language that makes 
it both interesting and challenging is the fact that it 
is not actually a single language. There are many 
varieties of Arabic, with rather different status. 
Classical Arabic (CA) is the language of the Ko-
ran, and the historical ancestor of the other varie-
ties. Modern Standard Arabic (MSA) is the modern 
version of CA and is, broadly speaking, the univer-
sal (i.e. not regional) standard variety of Arabic. 
Until recently, CA and MSA were the only varie-
ties that were written ? other, regional, varieties 
were only spoken. The situation is rapidly chang-
ing, with electronic communication increasingly 
involving written versions of the regional varieties. 
Even in traditional written forms, such as news 
reports, the vocabulary used in different geograph-
ical regions is different. For example, Khoja 
(2001) found that the percentage of out of vocabu-
lary items in news reports from Egypt and Qatar 
was around double that found in Saudi news re-
ports, Saudi Arabic being much closer to MSA 
than the other two regional varieties. Ways in 
which the present approach may assist in this prob-
lem will be discussed later. 
The approach we describe here depends on a hier-
archically organised lexicon, based on the DATR 
lexical representation language (Evans and Gazdar, 
1996). The PolyLex lexical framework (Cahill and 
Gazdar, 1999) was developed originally with lan-
guages like English, German and Dutch in mind, 
but has been shown to lend itself to the description 
of Arabic templatic morphology (Cahill, 2007, 
2010). The inheritance of information by default in 
this framework is fundamental to the approach we 
describe. 
The problem to which we seek a solution is not one 
unique to Arabic. Any NLP system which wants to 
35
process naturally occurring text will always have 
to deal to some degree with the problem of un-
known or out of vocabulary (OOV) items. Whether 
these items are neologisms, errors or names, they 
need to be handled in some way. Solutions to this 
particular problem are unlikely to have a large sta-
tistical impact on the success rates of the pro-
cessing applications, but that does not mean that 
they are not worth finding. While it is undoubtedly 
the case that many applications will work perfectly 
well with a word recognition rate of, say, 95%, 
supported by statistical approaches  which provide 
syntactic information, there are other applications 
for which full semantic interpretation is desirable, 
if not necessary. It is such applications that the cur-
rent paper addresses. We are only addressing a part 
of the problem, as this approach does not help rec-
ognise names or errors. 
The particular approach described in this paper is 
based on the observation that a native speaker who 
encounters a word they have not seen before may, 
if that word is related to others that they do know, 
be able to make an educated guess at not only the 
syntactic category, but also the meaning of that 
word. To a large degree, that guesswork involves 
the specific context that the word occurs in, but 
native speakers will also have more abstract struc-
tural knowledge about their language which allows 
them to make guesses about words on the basis of 
their internal structure. For example, if an English 
speaker knows the word ?confuse? and hears the 
word ?confuser?, even though they have most like-
ly never before come across the latter, they will be 
able to at least guess that it means ?someone/thing 
that confuses?. Of course, with derivation the 
meaning relationship is not always transparent. So 
a person encountering the word ?decider? for the 
first time may be surprised to find that it does not 
mean ?one who decides? but rather a deciding 
match/game etc.. Such issues and other limitations 
of this approach will be discussed later. 
2 Previous approaches 
There has been a lot of work on how to handle 
OOV items, largely based on statistical approach-
es. Some are language independent (see e.g. Attia 
et al(2010), Adler et al(2008)) while others focus 
on specific languages (see e.g. Habash and 
Rambow (2005, 2007) and Marsi et al(2005) on 
Arabic and Adler and Elhadad (2006) on Hebrew, 
another Semitic language with similar morphologi-
cal structure). The work by Habash and Rambow, 
for example, employs a form of morphological 
expansion to handle OOV items, but only makes 
use of the inflectional morphology of Arabic, not 
the derivational morphology as in the current ap-
proach. 
 
Other approaches to morphological analysis in Ar-
abic include methods to deal with OOV items. For 
example, Beesley and Karttunen (2003), describe a 
two-level approach which includes a general meth-
od for guessing OOV words which could certainly 
apply to some degree to Arabic, but it would not be 
able to take into account the linguistic (specifically 
semantic) information which is at the heart of the 
present approach. 
3 PolyLex/PolyOrth 
The PolyLex project (Cahill and Gazdar, 1999) 
developed multilingual lexicons of the morphology 
and phonology of English, German and Dutch, im-
plemented in the lexical representation language 
DATR (Evans and Gazdar, 1996) which allows for 
default inheritance. Therefore, aspects of these 
languages that were shared could be inherited by 
default by each language.  
In addition to the aspects of inter- and intra-
language default inheritance, the other aspect of 
the PolyLex framework which contributes to the 
unknown word processing proposed here is the use 
of phonological structures, specifically syllables, to 
define morphological structures and relationships. 
Thus, in PolyLex, the lexical entries consist of 
specifications of the phonological forms of the syl-
lable constituents (onset, peak and coda). These 
can be determined by morpho-syntactic features. 
For example, the English word man has default 
values for the onset (/m/), peak (/?/) and coda 
(/n/), but a further value for the peak in the plural 
(/?/).  This is represented in DATR as1: 
<phn syl1 onset> == m 
<phn syl1 peak> == { 
<phn syl1 coda> == n 
<phn syl1 peak plur> == E. 
The PolyOrth project (Cahill et al 2006) further 
developed the representation so that orthographic 
                                                          
1 In the DATR code, the SAMPA machine readable alphabet 
(Wells, 1989) is used. 
36
forms are derived by means of a combination of 
phoneme-grapheme mappings and spelling rules. 
Both types of information include phonological 
and morphological determinants, so that, for ex-
ample, the default mapping for any particular pho-
neme will depend on both its phonological position 
(is it in the onset or coda?) and on its morphologi-
cal position (is it in a stem or an affix?). Both types 
of information are defined by means of Finite State 
Transducers (FSTs) 2 . This framework has been 
implemented and tested on English, German and 
Dutch, and now extended to Arabic (Cahill, 2010). 
The Arabic lexicon allows for forms to be defined 
in Arabic script, Roman transliteration or phono-
logical representation. 
4 Arabic verbal morphology 
The Arabic languages have around 280 million 
speakers. They belong to the Semitic language 
family, and share many linguistic features with 
other Semitic languages, such as Hebrew and Mal-
tese. Much work in both theoretical and computa-
tional linguistics has focused on the so-called 
templatic morphology of the Semitic languages.  
The key area of Arabic morphology addressed in 
this paper is the verbal derivation. Verbs in Arabic 
are typically based on a tri-literal root, consisting 
of three consonants. Inflectional variation involves 
interdigitating these consonants with vowels which 
indicate the tense, aspect and mood. In addition, 
the three consonants can be differently arranged 
(doubled, swapped etc.) to form distinct Forms (or 
measures, also known as binyanim 3 , especially 
when applied to Hebrew). These are essentially 
derivations and form distinct verbs with different 
meanings. For example, the tri-literal root k-t-b has 
the core meaning ?write?. The forms katabtu and 
aktubtu, represent the active perfective and active 
imperfective first person singular forms of ?write?, 
namely, ?I wrote? and ?I write?. The second Form 
or measure verb k-tt-b also has the inflectional var-
iations, but has the meaning ?cause to write?, thus 
the two actual forms kattabtu and akttabtu have the 
                                                          
2 The PolyOrth project was inspired by Herring (2006). How-
ever, while Herring uses one-stage FSTs, the PolyOrth project 
used two levels of FST, including a separate treatment of 
?post-lexical? spelling rules.  
3 We will use the term ?Form?, capitalised to avoid confusion 
with the more usual use of ?form?. 
meanings ?I caused (someone) to write? and ?I 
cause (someone) to write? respectively. 
There are fifteen different Forms in CA, but fewer 
in the modern varieties. In MSA there are ten that 
are commonly found, although two more are found 
rarely. The regional varieties all make use of few-
er. While some of the Forms have clear transparent 
meanings, others have far less clear or apparently 
random meaning relations.  
The following descriptions of the meanings of the 
ten Forms is adapted from Scheindlin (2007): 
I. The basic Form ? all verbs have this form. 
May be transitive or intransitive. 
II. Almost always transitive. If a verb exists 
in both Form I and II then I will often be 
intransitive and II transitive (write (I) ? 
cause to write (II)). If I is transitive then II 
may be ditransitive. II may also involve an 
intensifying of the meaning on I, e.g. kill 
(I) ? massacre (II). 
III. May involve reciprocity, e.g. follow (I) ? 
alternate (III).  
IV. Like II, mostly transitive, and often 
matched with intransitive in I. 
V. Often involves a reflexive element, e.g. 
know (I) ? teach (II) ? learn (V). 
VI. Like III, often involves reciprocity, e.g. 
fight (I) ? fight each other (VI). 
VII. Mostly reflexive, resultative or passive. 
Roots that are transitive in I are intransi-
tive in VII. E.g. break (I) ? be broken 
(VII). 
VIII. Often reflexive for verbs that are transitive 
in I, e.g. divide (I) ? part (VIII). 
IX. Very restricted in application, only apply-
ing to verbs indicating colours and de-
fects, e.g. turn yellow. 
X. Often associated with asking for some-
thing associated with the Form I verb, e.g. 
pardon (I) ? apologise (ask for pardon) 
(X). 
As is clear from these descriptions, the meaning 
relationships are not fully predictable, but they can 
give some hints as to the likely meaning of an un-
known verb. As the framework relies on default 
inheritance, the assumption that any definitions 
may be overridden by more specific information 
means that even very approximate definitions are 
still valuable. 
37
5 Arabic in syllable-based morphology 
A small sample lexicon of Arabic in the PolyLex 
framework is presented in Cahill (2007). What 
makes this account different from most accounts of 
the morphology of the Semitic languages is that it 
requires no special apparatus to allow for the defi-
nition of so-called ?templatic? morphology, but 
makes use of the same kind of equations as are 
required for ablaut and consonant devoicing, for 
example, that are found in English, German and 
Dutch.  
5.1 The default, Form I root 
The main part of the account addresses a single 
verb root, namely k.t.b, ?write?, and generates all 
possible Form stems for perfective, imperfective 
and participle, active and passive. The approach is 
based on defining the leaves of syllable-structure 
trees, with the consonants of the triliteral stems 
occupying the onset and coda positions, and the 
vowels (syllable peaks) being defined according to 
the morphosyntactic specification, as in the exam-
ple of man above. To illustrate this, the figure be-
low shows the default structure for a triliteral root, 
with no vowels specified. The default structure is a 
disyllabic root, with the first consonant occupying 
the onset of the first syllable, the second consonant 
occupying the onset of the second syllable and the 
third consonant occupying the coda of the second 
syllable4. 
 
 
 
Figure 1: the structure of /katab/ 
                                                          
4 The syllable position is indicated by simple numbering. Syl-
lables can be counted from either right of left. For languages 
which largely use suffixation, it makes more sense to count 
from the right, as for Arabic here. 
5.2 The other Form stems 
As described in Cahill (2007), the remaining nine 
forms have their default structure defined in simi-
lar terms. Figure 2 depicts the inheritance of forms 
from each other. This inheritance is for the syllable 
structure definitions, so the Form II structure is the 
same as the Form I structure except that the first 
coda has the value of the second root consonant, 
the same as the onset of the second syllable. The 
definitions are all incremental, so that each Form 
specification only supplies one or two pieces of 
information. 
5.3 Meanings 
The original lexicon was designed to demonstrate 
that the complex relationships between phonologi-
cal, morphological and orthographic forms in Ara-
bic could be captured in the PolyLex/PolyOrth 
architecture. There was no semantic information in 
the lexicons at all. For the present experiment, we 
have added very basic semantic information for the 
100 verbs we have included. Most of these are 
Form I verbs, but there are some Form II, Form IV 
and Form V verbs. Where possible, we have repre-
sented the meanings of the verbs of Forms other 
than I in terms that can be generalised. For exam-
ple, the verb apologise has the meaning expressed 
as ASK FOR PARDON5.  
 
The lexical hierarchy, in addition, defines a default 
meaning expression for each Form. For Form VIII, 
for example, this is: 
 
<meaning> == ask for ?<formI meaning>? 
 
which says that the meaning is simply the string 
?ask for? followed by the meaning for Form I for 
the root6.  
 
 
 
5.4 The full lexicon 
                                                          
5 For this small experiment, the exact representation of the 
meanings is not important. It is assumed that in a genuine 
application will have its representations which would be in-
cluded in the lexicon, or for which a mapping can be defined. 
6 The quotes around the path <form1 meaning> indicate that it 
is to be evaluated at the original query node, i.e. the root node 
in DATR.  
root
syl2 syl1
t
b
k
38
As stated above, the lexicon we are working from 
has only 100 verbs. There are no verb roots for 
which we have more than one Form. This is a very 
small number, but for each verb in the lexicon 
there are a theoretically possible further nine verbs 
which may be derived from the same root. The 
lexicon will recognise any theoretically possible 
verb from the roots it knows about, although it 
does not have semantic information explicitly pro-
vided for a large proportion of these verbs.  
6 Using the lexicon for word recognition 
The highly structured, hierarchical lexicons are not 
designed to be used as they are within NLP appli-
cations. The information in them is cached in a 
lookup table which can be used for either genera-
tion or comprehension, with entries which look 
like this:  
 
??? k-t-
b     
katab stem p, 
a 
k-
t-
b 
I write 
 
???? 
k-
tt-b   
kattab stem p, 
p 
k-
t-
b 
II [cause 
to 
write] 
 
The first column is the form in Arabic script, the 
second is the transliteration, the third is one possi-
ble full (vowelised) form, the fourth and fifth give 
the morphological analysis, the sixth is the triliteral 
root it is derived from, the seventh is the Form and 
the last is the translation. The first row, which has 
the Form I entry, has a translation which was pro-
vided explicitly in the lexicon but the second gets 
its meaning by default. This is indicated by the 
square brackets.  In use in an application, these 
meanings would be used more cautiously, possibly 
in conjunction with other methods, especially mak-
ing use of context.  
 
The lookup table often provides more than one 
possible entry for a single form, especially when 
the form is unvowelised.  
 
6.1 Testing 
In order to test the approach, we tested the recogni-
tion of all verbs in the NEMLAR written corpus 
(Attiyya et al, 2005). The corpus provides versions 
with POS tagging, which enabled us to extract 
each verb. There were a total of just over 40,000 
forms tagged as verbs, approximately 11,000 of 
them unique forms. Initial tests only took those 
forms which were tagged as having neither prefix 
nor suffix, a total of 1274 verb forms7. These in-
cluded forms which were inflectionally distinct, 
and once these forms were collapsed, the total 
number of verb forms is 577. Of these, 32 occurred 
in our initial lexicon of 100 verbs.  
 
These tests showed that of the remaining 545 un-
known verbs, 84 could apparently be analysed as 
derived forms of one of our existing verbs. This 
                                                          
7 The decision to use only those forms without prefix of suffix 
was simply made to make the testing process simpler and to 
ensure that the results were not skewed by the presence of 
consonants in prefixes or suffixes. 
Figure 2: The inheritance of Forms 
Verb/Form I 
Form II Form III Form IV Form V 
Form VI 
Form VII Form VIII 
Form IX Form X 
39
was determined by checking the main entries in an 
online Arabic dictionary and comparing the mean-
ings given to those generated by the lexicon. This 
was a very promising figure, given the very small 
size of the lexicon.8 
 
In the next testing phase we looked more closely at 
these forms. There are two ways in which the anal-
yses may not be appropriate. The analysis might 
not be an appropriate (or at least not the most ap-
propriate) one. This is not a major problem since 
we are dealing with a situation in which we fre-
quently have multiple possible analyses for a word, 
so generating a number of possibilities from which 
an application must choose is exactly what is re-
quired. The second issue is the question of whether 
the meanings generated are useful. In order to 
check this we manually compared the generated 
meanings against the actual meanings for a sample 
of the verbs in question. We found that just over 
half of the verbs we checked had meanings which 
were at least clearly connected to the generated 
meaning. For example, the stem  ???? (teach) is 
clearly related to the stem ??? (know), and turns out 
to be the second Form (?cause to X?) of the root 
for which know is the first Form. 
6.2 Analysis of results 
The verbs for which meanings were generated fit 
into three broad categories. First there are verbs for 
which the derived Form appears in dictionaries 
with the same meaning as that for Form I, possibly 
as one of its meanings. Thus, for example, the 
Form VIII verb ktatab had the meaning ?wrote?, 
the same as the Form I katab. There were 23 verbs 
in our set of 84 for which this was the case. 
 
The second category consists of verbs for which 
the meaning is related in the way suggested by our 
earlier analysis. 22 of the verbs came into this cat-
egory.9  
 
Finally, the last category consists of verbs whose 
meaning is not related in the way suggested. This 
is the most problematic class, and unfortunately the 
largest in the small test set we are working with. 
                                                          
8 There were some difficulties with transliteration which mean 
that these figures may not be fully accurate. 
9 This is clearly a case of subjective judgement, and from a 
non-native speaker these judgements may not be accurate.  
However, in most, indeed nearly all, of these cases, 
the generated meaning was not wildly different 
from that in the dictionary. Closer inspection sug-
gests that simply improving the meaning relations, 
and allowing more than one additional possible 
lexicon entry for some Forms would improve the 
performance significantly. 
 
 
7 Discussion and conclusion 
This paper has described a small experiment to test 
a novel rule-based approach to unknown word 
recognition in Arabic. Although testing is at an 
early stage, the initial results are promising.  
 
The experiment described is intended to address a 
small part of the overall problem of unknown 
words. In some respects it can be viewed as more 
of a technique for extending an existing lexicon 
than for dealing with OOV items at runtime. How-
ever, it would be possible to enable an application 
to have access to the default lexical information at 
runtime, to allow this.  
 
Another area in which the above technique may 
prove particularly useful is in the processing of 
regional varieties of Arabic. As stated above, 
Khoja (2001) found that even texts apparently 
written in MSA were twice as likely to have un-
known words in texts from Egypt and Qatar than 
from Saudi Arabia. This suggests some variation in 
the vocabulary, most likely involving ?leakage? of 
vocabulary items from Egyptian and Qatari Arabic 
into the MSA used by those speakers. As the mor-
phological patterns of derived verbs are different in 
the different regional varieties, taking these pat-
terns into account will provide further possible in-
terpretations. The PolyLex structure allows the 
definition of similarities and differences between 
the lexicons of languages and dialects that are 
closely related.  
7.1 Limitations and future work 
The experiment described here is a very small 
scale one, and the lexicon is extremely small. The 
representation of meaning is also extremely simpli-
fied. It is possible that the approach described 
simply could not be scaled up to a size useful for 
an application. However, there is a range of ways 
40
of representing meaning, including linking to an 
external ontology, which could also be implement-
ed in the lexicon described. 
 
The next phase of work is to fully evaluate the re-
sults of the initial tests, followed by further more 
extensive testing. It is envisaged that an iterated 
cycle of testing and extension of the lexicon could 
lead to a lexicon large enough to be useful and ro-
bust enough to handle significant (if still small) 
numbers of OOV items. 
 
Subsequently, and further down the line, develop-
ment of a lexicon (or lexicons) for the vocabulary 
of regional varieties, linked to the MSA lexicon in 
the PolyLex framework will help to exploit the 
similarities. That is, the lexicon for, say, Egyptian 
Arabic assumes that, by default, words are the 
same as in MSA, with only those words (mor-
phemes, phonemes etc.) which differ requiring 
specification.  
Acknowledgements 
 
The work described here was partly supported by 
the ESRC (Economic and Social Research Coun-
cil, UK) as part of the project: RES-000-22-3868 
Orthography, phonology and morphology in the 
Arabic lexicon. We are grateful to the anonymous 
reviewers for their helpful comments. 
References 
 
Adler, Meni and Michael Elhadad. () An Unsupervised 
Morpheme-Based HMM for Hebrew Morphologi-
cal Disambiguation. COLING-ACL 2006, pp. 665-
672. 
Adler, Meni, Yoav Goldberg, David Gabay and Michael 
Elhadad. (2008) Unsupervised Lexicon-Based Re-
solution of Unknown Words for Full Morphologi-
cal Analysis. ACL-08 : HLT, pp. 728-36. 
Atiyya, Muhammed, Khalid Choukri and Mustafa Ya-
seen. (2005) The NMELAR Written Corpus ELDA. 
Attia, Mohammed, Jennifer Foster, Deirdre Hogan, Jo-
seph Le Roux, Lamia Tounsi and Josef van Gena-
bith. (2010) Handling Unknown Words in 
Statistical Latent-Variable Parsing Models for 
Arabic, English and French. NAACL HLT 
Workshop on Statistical Parsing of Morphologi-
cally Rich Languages. pp. 67-75. 
Beesley, Kenneth and Lauri Karttunen. (2003) Finite 
State Morphology Chicago : CSLI. 
Cahill, Lynne. (2010) A Syllable-based Approach to 
verbal Morphology in Arabic. Workshop on Semi-
tic Languages,  LREC2010, Malta, 2010. 
Cahill, Lynne. (2007) A Syllable-based Account of 
Arabic Morphology. In Abdelhadi Soudi, Antal 
van der Bosch and G?nther Neumann (eds.) Ara-
bic Computational Morphology Dordrecht : Sprin-
ger. pp. 45-66. 
Cahill, Lynne, Jon Herring and Carole Tiberius, 
?PolyOrth: Adding Orthography to a Phonological 
Inheritance Lexicon?,  Fifth International Work-
shop on Writing Systems, Nijmegen, Netherlands, 
October 2006 (available at 
http://www.nltg.brighton.ac.uk/projects/polyorth). 
Cahill, Lynne and Gazdar, Gerald. (1999) The PolyLex 
architecture : multilingual lexicons for related lan-
guages. Traitement Automatique des Langues, 
40 :2, pp. 5-23. 
Evans, Roger and Gazdar, Gerald. (1996) DATR : a 
language for lexical knowledge representation. 
Computational Linguistics, 22 :2, pp. 167-216.  
Habash, Nizar and Owen Rambow. (2007) Arabic 
Diacritization through Full Morphological Tag-
ging. NAACL HLT 2007pp. 53-56. 
Habash, Nizar and Owen Rambow. (2005) Arabic To-
kenization, Part-of-Speech Tagging and Morpho-
logical Disambiguation in One Fell Swoop. ACL 
2005, pp. 573-80. 
Herring, J. (2006) Orthography and the lexicon, PhD 
dissertation, University of Brighton. 
Khoja, Shereen. (2001) APT: Arabic Part-of-speech 
Tagger. Proceedings of the Student Workshop at 
the Second Meeting of the North American Chap-
ter of the Association for Computational Linguis-
tics (NAACL2001). 
Marsi, Erwin, Antal van den Bosch and Abdelhadi 
Soudi. (2005) Memory-based morphological anal-
ysis, generation and part-of-speech tagging of Ar-
abic. ACL Workshop on Computational 
Approaches to Semitic Languages. pp. 1-8. 
Scheindlin, Raymond P. (2007) 501 Arabic verbs 
Haupage: Barron. 
Wells, John. (1989) Computer-coded phonemic notation 
of individual languages of the European Community. 
Journal of the International Phonetic Association, 
19 :1, pp. 31-54. 
 
41
