Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1169?1179,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
A Random Walk Approach to Selectional Preferences Based on
Preference Ranking and Propagation?
Zhenhua Tian?, Hengheng Xiang, Ziqi Liu, Qinghua Zheng?
Ministry of Education Key Lab for Intelligent Networks and Network Security
Department of Computer Science and Technology
Xi?an Jiaotong University
Xi?an, Shaanxi 710049, China
{zhhtian?,qhzheng?}@mail.xjtu.edu.cn
Abstract
This paper presents an unsupervised ran-
dom walk approach to alleviate data spar-
sity for selectional preferences. Based on
the measure of preferences between predi-
cates and arguments, the model aggregates
all the transitions from a given predicate to
its nearby predicates, and propagates their
argument preferences as the given predi-
cate?s smoothed preferences. Experimen-
tal results show that this approach out-
performs several state-of-the-art method-
s on the pseudo-disambiguation task, and
it better correlates with human plausibility
judgements.
1 Introduction
Selectional preferences (SP) or selectional restric-
tions capture the plausibility of predicates and
their arguments for a given relation. Kaze and
Fodor (1963) describe that predicates and their
arguments have strict boolean restrictions, either
satisfied or violated. Sentences are semantically
anomalous and not consistent in reading if they
violated the restrictions. Wilks (1973) argues that
?rejecting utterances is just what humans do not.
They try to understand them.? He further states s-
electional restrictions as preferences between the
predicates and arguments, where the violation can
be less preferred, but not fatal. For instance, given
the predicate word eat, word food is likely to be
its object, iPhone is likely to be implausible for it,
and tiger is less preferred but not curious.
SP have been proven to help many natural lan-
guage processing tasks that involve attachment de-
?Partial of this work was done when the first author vis-
iting at Language Technologies Institute of Carnegie Mellon
University sponsored by the China Scholarship Council.
cisions, such as semantic role labeling (Resnik,
1993; Gildea and Jurafsky, 2002), word sense dis-
ambiguation (Resnik, 1997), human plausibility
judgements (Spasic? and Ananiadou, 2004), syn-
tactic disambiguation (Toutanova et al, 2005),
word compositionality (McCarthy et al, 2007),
textual entailment (Pantel et al, 2007) and pro-
noun resolution (Bergsma et al, 2008) etc.
A direct approach to acquire SP is to extract
triples (q, r, a) of predicates, relations, and argu-
ments from a syntactically analyzed corpus, and
then conduct maximum likelihood estimation (M-
LE) on the data. However, this strategy is infea-
sible for many plausible triples due to data spar-
sity. For example, given the relation <verb-dobj-
noun> in a corpus, we may see plausible triples:
eat - {food, cake, apple, banana, candy...}
But we may not see plausible and implausible
triples such as:
eat - {watermelon, ziti, escarole, iPhone...}
Then how to use a smooth model to alleviate
data sparsity for SP?
Random walk models have been successful-
ly applied to alleviate the data sparsity issue on
collaborative filtering in recommender systems.
Many online businesses, such as Netflix, Ama-
zon.com, and Facebook, have used recommender
systems to provide personalized suggestions on
the movies, books, or friends that the users may
prefer and interested in (Liben-Nowell and Klein-
berg, 2007; Yildirim and Krishnamoorthy, 2008).
In this paper, we present an extension of using
the random walk model to alleviate data sparsi-
ty for SP. The main intuition is to aggregate all
the transitions from a given predicate to its near-
by predicates, and propagate their preferences on
arguments as the given predicate?s smoothed argu-
1169
ment preferences. Our work and contributions are
summarized as follows:
? We present a framework of random walk ap-
proach to SP. It contains four components with
flexible configurations. Each component is cor-
responding to a specific functional operation on
the bipartite and monopartite graphs which rep-
resenting the SP data;
? We propose an adjusted preference ranking
method to measure SP based on the popularity
and association of predicate-argument pairs. It
better correlates with human plausibility judge-
ments. It also helps to discover similar predi-
cates more precisely;
? We introduce a probability function for random
walk based on the predicate distances. It con-
trols the influence of nearby and distant predi-
cates to achieve more accurate results;
? We find out that propagate the measured prefer-
ences of predicate-argument pairs is more prop-
er and natural for SP smooth. It helps to im-
prove the final performance significantly.
We conduct experiments using two sections of
the LDC English gigaword corpora as the general-
ization data. For the pseudo-disambiguation task,
we evaluate it on the Penn TreeBank-3 data. Re-
sults show that our model outperforms several pre-
vious methods. We further investigate the correla-
tions of smoothed scores with human plausibili-
ty judgements. Again our method achieves better
correlations on two third party data.
The remainder of the paper is organized as fol-
lows: Section 2 introduces related work. Section 3
briefly formulates the overall framework of our
method. Section 4 describes the detailed model
configurations, with discussions on their roles and
implications. Section 5 provides experiments on
both the pseudo-disambiguation task and human
plausibility judgements. Finally, Section 6 sum-
marizes the conclusions and future work.
2 Related Work
2.1 WordNet-based Approach
Resnik (1996) conducts the pioneer work on
corpus-driven SP induction. For a given predi-
cate q, the system firstly computes its distribution
of argument semantic classes based on WordNet.
Then for a given argument a, the system collects
the set of candidate semantic classes which con-
tain the argument a, and ensures they are seen in
q. Finally the system picks a semantic class from
the candidates with the maximal selectional asso-
ciation score, and defines the score as smoothed
score of (q, a).
Many researchers have followed the so-called
WordNet-based approach to SP. One of the key
issues is to induce the set of argument semantic
classes that are acceptable by the given predicate.
Li and Abe (1998) propose a tree cut model based
on minimal description length (MDL) principle
for the induction of semantic classes. Clark and
Weir (2002) suggest a hypothesis testing method
by ascending the noun hierarchy of WordNet. Cia-
ramita and Johnson (2000) model WordNet as a
Bayesian network to solve the ?explain away? am-
biguity. Beyond induction on argument classes on-
ly, Agirre and Martinez (2001) propose a class-to-
class model that simultaneously learns SP on both
the predicate and argument classes.
WordNet-based approach produces human in-
terpretable output, but suffers the poor lexical cov-
erage problem. Gildea and Jurafsky (2002) show
that clustering-based approach has better cover-
age than WordNet-based approach. Brockman-
n and Lapata (2003) find out that sophisticated
WordNet-based methods do not always outperfor-
m simple frequency-based methods.
2.2 Distributional Models without WordNet
Alternatively, Rooth et al (1999) propose an EM-
based clustering smooth for SP. The key idea is to
use the latent clusterings to take the place ofWord-
Net semantic classes. Where the latent clusterings
are automatically derived from distributional da-
ta based on EM algorithm. Recently, more so-
phisticated methods are innovated for SP based on
topic models, where the latent variables (topics)
take the place of semantic classes and distribution-
al clusterings (Se?aghdha, 2010; Ritter et al, 2010).
Without introducing semantic classes and laten-
t variables, Keller and Lapata (2003) use the web
to obtain frequencies for unseen bigrams smooth.
Pantel et al (2007) apply a collection of rules to
filter out incorrect inferences for SP. Specifically,
Dagan et al (1999) introduce a general similarity-
based model for word co-occurrence probabilities,
which can be interpreted for SP. Similarly, Erk et
al. propose an argument-oriented similarity model
based on semantic or syntactic vector spaces (Erk,
1170
2007; Erk et al, 2010). They compare several sim-
ilarity functions and weighting functions in their
model. Furthermore, instead of employing various
similarity functions, Bergsma et al (2008) pro-
pose a discriminative approach to learn the weight-
s between the predicates, based on the verb-noun
co-occurrences and other kinds of features.
Random walk model falls into the non-class
based distributional approach. Previous literatures
have fully studied the selection of distance or sim-
ilarity functions to find out similar predicates and
arguments (Dagan et al, 1999; Erk et al, 2010), or
learn the weights between the predicates (Bergsma
et al, 2008). Instead, we put effort in following
issues: 1) how to measure SP; 2) how to trans-
fer between predicates using random walk; 3) how
to propagate the preferences for smooth. Experi-
ments show these issues are important for SP and
they should be addressed properly to achieve bet-
ter results.
3 RSP: A Random Walk Model for SP
In this section, we briefly introduce how to address
SP using random walk. We propose a framework
of RSP with four components (functions). Each of
them are flexible to be configured. In summary,
Algorithm 1 describes the overall process.
Algorithm 1 RSP: Random walk model for SP
Require: Init bipartite graph G with raw counts
1: // Ranking on the bipartite graph G;
2: R = ?(G); // ranking function
3: // Project R to monopartite graph D
4: D = ?(R); // distance function
5: // Transform D to stochastic matrix P
6: P = ?(D); // probability function
7: // Get the convergence P?
8: P? =??t=1
(dP )t
|(dP )t| = dP (I ? dP )?1;
9: return Smoothed bipartite graph R?
10: R? = P? ?R; // propagation function
Bipartite Graph Construction: For a giv-
en relation r, the observed predicate-argument
pairs can be represented by a bipartite graph
G=(X,Y,E). Where X={q1 , q2 , ..., qm} are the
m predicates, and Y ={a1 , a2 , ..., an} are the n ar-
guments. We initiate the links E with the raw
co-occurrence counts of seen predicate-argument
pairs in a given generalization data. We represent
the graph by an adjacency matrix with rows repre-
senting predicates and columns as arguments. For
convenience, we use indices i, j to represent pred-
icates qi , qj , and k, l for arguments ak , al .
We employ a preference ranking function ? to
measure the SP between the predicates and argu-
ments. It transforms G to a corresponding bipar-
tite graph R, with links representing the strength
of SP. Each row of the adjacency matrix R denotes
the predicate vector q?i or q?j . We discuss the selec-
tion of ? in section 4.1.
? := G 7? R (1)
Argument 
Nodes
Predicate
Nodes
can
fish
food
crop
flower
soil
fruit
eat
cook
harvest
cultivate
irrigateconsume
harvestconsumecook eat cultivate irrigate
chicken
cropfood fruit flowercanchickenfish
Predicate Projection Argument  Projection
soil
Figure 1: Illustration of (R) the bipartite
graph of the verb-dobj-noun relation, (Q) the
predicate-projection monopartite graph, and (A)
the argument-projection monopartite graph.
Monopartite Graph Projection: In order to
conduct random walk on the graph, we project
the bipartite graph R onto a monopartite graph
Q=(X,E) between the predicates, or A=(Y,E)
between the arguments (Zhou et al, 2007). Fig-
ure 1 illustrates the intuition of the projection. The
links in Q represent the indirect connects between
the predicates in R. Two predicates are connected
in Q if they share at least one common neighbor
argument in R. The weight of the links in Q could
be set by arbitrary distance measures. We refer D
as an instance of the projection Q by a given dis-
tance function ?.
? := R 7? D (2)
Stochastic Walking Strategy: We introduce a
probability function ? to transform the predicate
distances D into transition probabilities P . Where
P is a stochastic matrix, with each element pij
represents the transition probability from predicate
qi to qj . Generally speaking, nearby predicates
gain higher probabilities to be visited, while dis-
tant predicates will be penalized.
? := D 7? P (3)
1171
Follow Equation 4, we aggregate over all orders
of the transition probabilities P as the final sta-
tionary probabilities P? . According to the Perron-
Frobenius theory, one can verify that it converges
to dP (I ? dP )?1 when P is non-negative and
regular matrix (Li et al, 2009). Where t repre-
sents the orders: the length of the path between
two nodes in terms of edges. The damp factor
d ? (0, 1), and its value mainly depends on the da-
ta sparsity level. Typically d prefers small values
such as 0.005. It means higher order transitions
are much less reliable than lower orders (Liben-
Nowell and Kleinberg, 2007).
P? =
??
t=1
(dP )t
|(dP )t| = dP (I ? dP )
?1 (4)
Preference Propagation: in Equation 5, we
combine the converged transition probabilities P?
with the measured preferences R as the propa-
gation function: 1) for a given predicate, firstly
it transfers to all nearby predicates with designed
probabilities; 2) then it sums over the arguments
preferred by these predicates with quantified s-
cores to get smoothed R?. We further describe it-
s configuration details in Section 4.4 and Equa-
tion 12 with two propagation modes.
R? = P? ?R (5)
4 Model Configurations
4.1 Preference Ranking: Measure the
Selectional Preferences
In collaborative filtering, usually there are explic-
it and scaled user ratings on their item prefer-
ences. For instance, a user ratings a movie with
a score?[0,10] on IMDB site. But in SP, the pref-
erences between the predicates and arguments are
implicit: their co-occurrence counts follow the
power law distribution and vary greatly.
Therefore, we employ a ranking function ? to
measure the SP of the seen predicate-argument
pairs. We suppose this could bring at least two
benefits: 1) a proper measure on the preferences
can make the discovering of nearby predicates
with similar preferences to be more accurate; 2)
while propagation, we propagate the scored pref-
erences, rather than the raw counts or condition-
al probabilities, which could be more proper and
agree with the nature of SP smooth. We denote
SelPref(q, a) as Pr(q, a) for short.
SelPref(q, a) = ?(q, a) (6)
Previous literatures have well studied on various
smooth models for SP. However, they vary great-
ly on the measure of preferences. It is still not
clear how to do this best. Lapata et al investigate
the correlations between the co-occurrence counts
(CT) c(q, a), or smoothed counts with the human
plausibility judgements (Lapata et al, 1999; Lap-
ata et al, 2001). Some introduce conditional prob-
ability (CP) p(a|q) for the decision of preference
judgements (Chambers and Jurafsky, 2010; Erk et
al., 2010; Se?aghdha, 2010). Meanwhile, the point-
wise mutual information (MI) is also employed
by many researchers to filter out incorrect infer-
ences (Pantel et al, 2007; Bergsma et al, 2008).
?CT = c(q, a) ?MI = log
p(q, a)
p(q)p(a)
?CP =
c(q, a)
c(q, ?) ?TD = c(q, a)log(
m
|a|)
(7)
In this paper, we present an adjusted ranking
function (AR) in Equation 8 to measure the SP of
seen predicate-argument pairs. Intuitively, it mea-
sures the preferences by combining both the pop-
ularity and association, with parameters control
the uncertainty of the trade-off between the two.
We define the popularity as the joint probability
p(q, a) based on MLE, and the association as MI.
This is potentially similar to the process of human
plausibility judgements. One may judge the plau-
sibility of a predicate-argument collocation from
two sides: 1) if it has enough evidences and com-
monly to be seen; 2) if it has strong association
according to the cognition based on kinds of back-
ground knowledge. This metric is also similar to
the TF-IDF (TD) used in information retrieval.
?AR(q, a) = p(q, a)?1
( p(q, a)
p(q)p(a)
)?2
s.t. ?1 , ?2 ? [0, 1]
(8)
We verify if a metric is better by two tasks:
1) how well it correlates with human plausibility
judgements; 2) how well it helps with the smooth
inference to disambiguate plausible and implausi-
ble instances. We conduct empirical experiments
on these issues in Section 5.3 and Section 5.4.
4.2 Distance Function: Projection of the
Monopartite Graph
In Equation 9, the distance function ? is used to
discover nearby predicates with distance dij . It
weights the links on the monopartite graph Q. It
1172
guides the walker to transfer between predicates.
We calculate ? based on the vectors q?i, q?j repre-
sented by the measured preferences in R.
dij = ?(q?i, q?j) (9)
Where ? can be distance functions such as Eu-
clidean (norm) distance or Kullback-Leibler diver-
gence (KL) etc., or one minus the similarity func-
tions such as Jaccard and Cosine etc. The selection
of distributional functions has been fully studied
by previous work (Lee, 1999; Erk et al, 2010). In
this paper, we do not focus on this issue due to
page limits. We simply use the Cosine function:
?cosine(q?i, q?j) = 1 ?
q?i ? q?j
?q?i??q?j?
(10)
4.3 Probability Function: the Walk Strategy
We define the probability function ? as Equa-
tion 11. Where the transition probability p(qj |qi)
in P is defined as a function of the distance dij
with a parameter ?. Intuitively, it means in a given
walk step, a predicate qj which is far away from
qi will get much less probability to be visited, and
qi has high probabilities to start walk from itself
and its nearby predicates to pursue good precision.
Once we get the transition matrix P , we can com-
pute P? according to Equation 4.
p(qj |qi) = ?(dij) =
(1 ? dij)?
Z(qi)
s.t. ? ? 0, dij ? [0, 1]
(11)
Where the parameter ? is used to control the bal-
ance of nearby and distant predicates. Z(qi) is the
normalize factor. Typically, ? around 2 can pro-
duce good enough results in most cases. We verify
the settings of ? in section 5.3.2.
4.4 Propagation Function
The propagation function in Equation 5 is repre-
sented by the matrix form. It can be expanded and
rewritten as Equation 12. Where p?(qj |qi) is the
converged transition probability from predicate qi
to qj . Pr(ak, qj) is the measured preference of
predicate qj with argument ak.
P?r(ak, qi) =
m?
j=1
p?(qj |qi) ? Pr(ak, qj) (12)
We employ two propagation modes (PropMode)
for the preference propagation function. One is
?CP? mode. In this mode, we always set Pr(q, a)
as the conditional probability p(a|q) for the prop-
agation function, despite what ? is used for the
distance function. This mode is similar to previ-
ous methods (Dagan et al, 1999; Keller and Lap-
ata, 2003; Bergsma et al, 2008). The other is ?PP?
mode. We set ranking function ?=Pr(q, a) always
to be the same in both the distance function and the
propagation function. That means what we propa-
gated is the designed and scored preferences. This
could be more proper and agree with the nature
of SP smooth. We show the improvement of this
extension in section 5.3.1.
5 Experiments
5.1 Data Set
Generalization Data: We parsed the Agence
France-Presse (AFP) and New York Times (NYT)
sections of the LDC English Gigaword corpo-
ra (Parker et al, 2011), each from year 2001-2010.
The parser is provided by the Stanford CoreNLP
package1. We filter out all tokens containing
non-alphabetic characters, collect the <verb-dobj-
noun > triples from the syntactically analyzed da-
ta. Predicates (verbs) whose frequency lower than
30 and arguments (noun headwords) whose fre-
quency less than 5 are excluded out. No other fil-
ters have been done. The resulting data consist of:
? AFP: 26, 118, 892 verb-dobj-noun observa-
tions with 1, 918, 275 distinct triples, totally
4, 771 predicates and 44, 777 arguments.
? NYT: 29, 149, 574 verb-dobj-noun observa-
tions with 3, 281, 391 distinct triples, totally
5, 782 predicates and 57, 480 arguments.
Test Data: For pseudo-disambiguation, we em-
ploy Penn TreeBank-3 (PTB) as the test data (Mar-
cus et al, 1999)2. We collect the 36, 400 manu-
ally annotated verb-dobj-noun dependencies (with
23, 553 distinct ones) from PTB. We keep depen-
dencies whose predicates and arguments are seen
in the generalization data. We randomly selec-
t 20% of these dependencies as the test set. We
split the test set equally into two parts: one as the
development set and the other as the final test set.
Human Plausibility Judgements Data: We
employ two human plausibility judgements data
1http://nlp.stanford.edu/software/corenlp.shtml
2PTB includes 2, 499 stories from the Wall Street Journal
(WSJ). It is different with our two generalization data.
1173
for the correlation evaluation. In each they col-
lect a set of predicate-argument pairs, and anno-
tate with two kinds of human ratings: one for an
argument takes the role as the patient of a predi-
cate, and the other for the argument as the agent.
The rating values are between 1 and 7: e.g. they
assign hunter-subj-shoot with a rating 6.9 but 2.8
for shoot-dobj-hunter.
? PBP: Pado? et al (2007) develop a set of hu-
man plausibility ratings on the basis of the
Penn TreeBank and FrameNet respectively.
We refer PBP as their 212 patient ratings
from the Penn TreeBank.
? MRP: This data are originally contributed by
McRae et al (1998). We use all their 723
patient-nn ratings.
Without explicit explanation, we remove all the
selected PTB tests and human plausibility pairs
from AFP and NYT to treat them unseen.
5.2 Comparison Methods
Since RSP falls into the unsupervised distribu-
tional approach, we compare it with previous
similarity-based methods and unsupervised gener-
ative topic model 3.
Erk et al (Erk, 2007; Erk et al, 2010) are
the pioneers to address SP using similarity-based
method. For a given (q, a) in relation r, the mod-
el sums over the similarities between a and the
seen headwords a? ? Seen(q, r). They investi-
gated several similarity functions sim(a, a?) such
as Jaccard, Cosine, Lin, and nGCM etc., and dif-
ferent weighting functions wtq,r(a?).
S(q, r, a) =
?
a?
wtq,r(a?)
Zq,r
? sim(a, a?) (13)
For comparison, we suppose the primary cor-
pus and generalization corpus in their model to be
the same. We set the similarity function of their
model as nGCM, use both the FREQ and DISCR
weighting functions. The vector space is in SYN-
PRIMARY setting with 2, 000 basis elements.
Dagan et al (1999) propose state-of-the-art
similarity based model for word co-occurrence
probabilities. Though it is not intended for SP, but
it can be interpreted and rewritten for SP as:
Pr(a|q) =
?
q??Simset(q)
sim(q, q?)
Z(q) p(a|q
?) (14)
3The implementation of RSP and listed previous methods
are available at https://github.com/ZhenhuaTian/RSP
They use the k-closest nearbys as Simset(q),
with a parameter ? to revise the similarity func-
tion. For comparison, we use the Jensen-Shannon
divergence (Lin, 1991) which shows the best per-
formance in their work as sim(q, q?), and optimize
the settings of k and ? in our experiments.
LDA-SP: Another kind of sophisticated unsu-
pervised approaches for SP are latent variable
models based on Latent Dirichlet Allocation (L-
DA). O? Se?aghdha (2010) applies topic models
for the SP induction with three variations: LDA,
Rooth-LDA, and Dual-LDA; Ritter et al (2010)
focus on inferring latent topics and their distribu-
tions over multiple arguments and relations (e.g.,
the subject and direct object of a verb).
In this work, we compare with O? Se?aghdha?s
original LDA approach to SP. We use the Mat-
lab Topic Modeling Toolbox4 for the inference
of latent topics. The hyper parameters are set as
suggested ?=50/T and ?=200/n, where T is the
number of topics and n is the number of argu-
ments. We test T=100, 200, 300, each with 1, 000
iterations of Gibbs sampling.
5.3 Pseudo-Disambiguation
Pseudo-disambiguation has been used for SP e-
valuation by many researchers (Rooth et al, 1999;
Erk, 2007; Bergsma et al, 2008; Chambers and
Jurafsky, 2010; Ritter et al, 2010). First the sys-
tem removes a portion of seen predicate-argument
pairs from the generalization data to treat them as
unseen positive tests (q, a+). Then it introduces
confounder selection to create a pseudo negative
test (q, a?) for each positive (q, a+). Finally it
evaluates a SP model by how well the model dis-
ambiguates these positive and negative tests.
Confounder Selection: for a given (q, a+), the
system selects an argument a? from the argumen-
t vocabulary. Then by ensure (q, a?) is unseen in
the generalization data, it treats a? as pseudo a?.
This process guarantees that (q, a?) to be negative
in real case with very high probability. Previous
work have made advances on confounder selec-
tion with random, bucket and nearest confounder-
s. Random confounder (RND) most closes to the
realistic case; While nearest confounder (NER) is
reproducible and it avoids frequency bias (Cham-
bers and Jurafsky, 2010).
In this work, we employ both RND and NER
confounders: 1) for RND, we randomly select
4psiexp.ss.uci.edu/research/programs data/toolbox.htm
1174
confounders according to the occurrence probabil-
ity of arguments. We sample confounders on both
the development and final test data with 100 itera-
tions. 2) for NER, firstly we sort the arguments by
their frequency. Then we select the nearest con-
founders with two iterations. One iteration selects
the confounder whose frequency is more than or
equal to a+, and the other iteration with frequency
lower than or equal to a+.
Evaluation Metric: we evaluate performance
on both the pairwise and pointwise settings:
1) On pairwise setting, we combine correspond-
ing (q, a+, a?) together as test instances. The per-
formance is evaluated based on the accuracy (AC-
C) metric. It computes the portion of test instances
(q, a+, a?) which correctly predicted by the s-
mooth model with score(q, a+) > score(q, a?).
We weight each instance equally for macroACC,
and weight each by the frequency of the positive
pair (q, a+) for microACC.
2) On pointwise setting, we use each positive
test (q, a+) or negative test (q, a?) as test in-
stances independently. We treat it as a binary
classification task, and evaluate using the standard
area-under-the-curve (AUC) metric. This metric
is firstly employed for the SP evaluation by Ritter
et al(2010). For macroAUC, we weight each in-
stance equally; for microAUC, we weight each by
its argument frequency (Bergsma et al, 2008).
Parameters Tuning: The parameters are tuned
on the PTB development set, using AFP as the
generalization data. We report the overall perfor-
mance on the final test set. While using NYT as
the generalization data, we hold the same parame-
ter settings as AFP to ensure the results are robust.
Note that indeed the parameter settings would vary
among different generalization and test data.
5.3.1 Verify Ranking Function and
Propagation Method
This experiment is conducted on the PTB devel-
opment set with RND confounders. We use AFP
and NYT as the generalization data. For compari-
son, we set the distance function ? as Cosine, with
default d=0.005, and ?=1.
In Table 1, the evaluation metric is Accuracy.
The first 4 rows are the results of ?CP? PropMode,
and the latter 3 rows are the ?PP? PropMode. With
respect to the ranking function ?, CP performs
the worst as it considers only the popularity rather
than association. The heavy bias on frequent pred-
icates and arguments has two major drawbacks: a)
The computation of predicate distances would re-
ly much more on frequent arguments, rather than
those arguments they preferred; b) While propaga-
tion, it may bias more on frequent arguments, too.
Even these frequent arguments are less preferred
and not proper to be propagated.
Crit. AFP NYTmacro micro macro micro
?CP 71.7 76.7 78.2 81.2
?MI 70.9 75.8 79.1 81.8
?TD 73.4 78.2 80.9 83.4
?AR 72.9 77.8 81.0 83.5
?MI 76.8 80.6 81.9 83.8
?TD 74.4 79.1 81.8 84.2
?AR 82.5 85.2 87.7 88.6
Table 1: Comparing different ranking functions.
For MI, it biases infrequent arguments with
strong association, without regarding to the popu-
lar arguments with more evidences. Furthermore,
the generalization data is automatically parsed and
kind of noisy, especially on infrequent predicates
and arguments. The noises could yield unreliable
estimations and decrease the performance. For T-
D, it outperforms MI method on ?CP? PropMode,
but it not always outperforms MI on ?PP? Prop-
Mode. It is no surprise to find out the adjusted
ranking AR achieves better results on both AFP
and NYT data, with ?1=0.2 and ?2=0.6. Finally,
it shows the ?PP? mode, which propagating the de-
signed preference scores, gains significantly better
performance as discussed in Section 4.4.
5.3.2 Verify ? of the Probability Function
This experiment is conducted on the PTB develop-
ment tests with both RND and NER confounders.
The generalization data is AFP.
0 0.5 1 1.5 2 2.5 3 3.5 4 4.576
78
80
82
84
86
88
90
acc
ura
cy (%
)
delta 
 
RND macro accuracyRND micro accuracyNER macro accuracyNER micro accuracy
Figure 2: Performance variation on different ?.
1175
Criterion
AFP NYT
RND NER RND NER
macro micro macro micro macro micro macro micro
Erk et al FREQ 73.7 73.6 73.9 73.6 68.3 68.4 63.8 63.0
Erk et alDISCR 76.0 78.3 79.1 78.1 83.3 84.2 82.4 82.6
Dagan et al 80.6 82.8 84.7 85.0 87.0 87.6 86.9 87.3
LDA-SP 82.0 83.5 83.7 82.9 89.1 89.0 87.9 87.8
RSPnaive 72.6 76.4 79.4 81.1 78.5 80.4 74.8 78.0
+Rank 74.0 77.7 83.5 85.2 81.4 83.1 84.5 86.9
+Rank+PP 83.5 85.2 87.2 87.0 88.2 88.2 88.0 88.3
+Rank+PP+Delta 86.2 87.3 88.4 88.1 90.6 90.1 91.1 89.3
Table 2: Pseudo-disambiguation results of different smooth models. Macro and micro Accuracy.
0 0.2 0.4 0.6 0.8 10
0.2
0.4
0.6
0.8
1
False Positive (FP)
True
 Pos
itive
 (TP)
 
 
Erk et al      macroAUC=0.72Dagan et al macroAUC=0.80LDA?SP       macroAUC=0.77RSP?ALL     macroAUC=0.84
0 0.2 0.4 0.6 0.8 10
0.2
0.4
0.6
0.8
1
False Positive (FP)
True
 Pos
itive
 (TP)
 
 
Erk et al      microAUC=0.62Dagan et al microAUC=0.83LDA?SP       microAUC=0.73RSP?ALL     microAUC=0.89
Figure 3: Marco and micro ROC curves of different smooth models.
We set the ranking function ? as AR (with
tuned ?1=0.2 and ?2=0.6), the distance function
? as Cosine, default d=0.005, and we restrict ? ?
[0.5, 4]. Figure 2 shows ? has significant impact
on the performance. Starting from ?=0.5, the sys-
tem gains better performance while ? increasing.
It achieves good results around ?=2. This mean-
s for a given predicate, the penalty on its distant
predicates helps to get more accurate smooth. The
performance will drop if ? becomes too big. This
means closest predicates are useful for smooth. It
it not better to penalize them heavily.
5.3.3 Overall Performance
Finally we compare the overall performance of d-
ifferent models. We report the results on the PTB
final test set, with RND and NER confounders.
Table 2 shows the overall performance on Accu-
racy metric. Among previous methods in the first
4 rows, LDA-SP performs the best in most cas-
es. In the last 4 rows, RSPnaive means both the
ranking function and PropMode are set as ?CP?
and ?=1. This configuration yields poor perfor-
mance. Iteratively, by employing the adjusted
ranking function, smoothing with preference prop-
agation method, and revising the probability func-
tion with the parameter ?, RSP outperforms all
previous methods. The parameter settings of RSP-
All are ?1=0.2, ?2=0.6, ?=1.75 and d=0.005.
Figure 3 show the macro (left) and micro (right)
receiver-operating-characteristic (ROC) curves of
different models, using AFP as the generalization
data and RND confounders. For each kind of
previous methods, we show the best AUC they
achieved. RASP-All still performs the best on
the terms of AUC metric, achieving macroAUC
at 84% and microAUC at 89%. We also verified
the AUC metric using NYT as the generalization
data. The results are similar to the AFP data. It
is also interesting to find out that the ACC met-
ric is not always bring into correspondence with
the AUC metric. The difference mainly raise on
the pointwise and pairwise test settings of pseudo-
disambiguation.
5.4 Human Plausibility Judgements
We conduct empirical studies on the correla-
tions between different preference ranking func-
1176
Criterion
AFP NYT
Spearman?s ? Kendall?s ? Spearman?s ? Kendall?s ?
PBP MRP PBP MRP PBP MRP PBP MRP
CT 0.49 0.36 0.37 0.28 0.54 0.44 0.41 0.34
CP 0.47 0.39 0.35 0.30 0.51 0.48 0.39 0.37
MI 0.56 0.39 0.43 0.31 0.54 0.49 0.41 0.38
TD 0.53 0.36 0.39 0.28 0.56 0.45 0.42 0.34
AR 0.58 0.40 0.44 0.31 0.58 0.50 0.44 0.39
Erk et al FREQ 0.30 0.08 0.22 0.06 0.25 0.09 0.18 0.06
Erk et alDISCR 0.06 0.21 0.04 0.15 0.16 0.23 0.11 0.16
Dagan et al 0.32 0.24 0.24 0.18 0.46 0.29 0.34 0.21
LDA-SP 0.31 0.32 0.23 0.23 0.38 0.38 0.28 0.28
LDA-SP+Bayes 0.39 0.25 0.30 0.18 0.40 0.32 0.30 0.23
RSP-All 0.46 0.31 0.34 0.23 0.53 0.38 0.40 0.28
Table 3: Correlation results on the human plausibility judgements data.
tions and human ratings. Follow Lapata et
al. (2001), we first collect the co-occurrence
counts of predicate-argument pairs in the human
plausibility data from AFP and NYT (before re-
moving them as unseen pairs). Then we score
them with different ranking functions (described
in Section 4.1) based on MLE. Inspired by Erk et
al. (2010), we do not suppose linear correlations
between the estimated scores and human ratings.
We use the Spearman?s ? and Kendal?s ? rank
correlation coefficient.
We also compare the correlations between the
smoothed scores of different models with human
ratings. With respect to upper bounds, Pado? et
al. (2007) suggest that the typical agreement of
human participants is around a correlation of 0.7
on their plausibility data. We hold that automatic
models of plausibility can not be expected to sur-
pass this upper bound.
In Table 3, all coefficients are verified at signif-
icant level p<0.01. The first 5 rows are the corre-
lations between the preference ranking function-
s and human ratings based on MLE. On both the
PBP and MRP data, the proposed AR metric better
correlates with human ratings than others, with ?2
>0.5 and ?1 around [0.2, 0.35]. The latter 6 rows
are the results of smooth models. It shows LDA-
SP performs good correlation with human ratings,
where LDA-SP+Bayes refers to the Bayes predic-
tion method of Ritter et al (2010). RSP model
gains the best correlation on the two plausibility
data in most cases, where the parameter settings
are the same as pseudo-disambiguation.
6 Conclusions and Future Work
In this work we present an random walk approach
to SP. Experiments show it is efficient and effec-
tive to address data sparsity for SP. It is also flex-
ible to be applied to new data. We find out that a
proper measure on SP between the predicates and
arguments is important for SP. It helps with the
discovering of nearby predicates and it makes the
preference propagation to be more accurate. An-
other issue is that it is not good enough to direct-
ly applies the similarity or distance functions for
smooth. Potential future work including but not
limited to follows: investigate argument-oriented
and personalized random walk, extend the model
in heterogenous network with multiple link types,
discover soft clusters using random walk for se-
mantic induction, and combine it with discrimina-
tive learning approach etc.
Acknowledgments
The research is supported in part by the Na-
tional High Technology Research and Devel-
opment Program 863 of China under Grant
No.2012AA011003; Key Projects in the Nation-
al Science and Technology Pillar Program under
Grant No.2011BAK08B02; Chinese Government
Graduate Student Overseas Study Program spon-
sored by the China Scholarship Council (CSC).
We also gratefully acknowledge the anonymous
reviewers for their helpful comments.
1177
References
Eneko Agirre and David Martinez. 2001. Learning
class-to-class selectional preferences. In Proceed-
ings of the 2001 workshop on Computational Natu-
ral Language Learning.
Shane Bergsma, Dekang Lin, and Randy Goebel.
2008. Discriminative learning of selectional pref-
erence from unlabeled text. In EMNLP.
Carsten Brockmann and Mirella Lapata. 2003. Evalu-
ating and combining approaches to selectional pref-
erence acquisition. In EACL.
Nathanael Chambers and Dan Jurafsky. 2010. Improv-
ing the use of pseudo-words for evaluating selection-
al preferences. In ACL.
Massimiliano Ciaramita and Mark Johnson. 2000. Ex-
plaining away ambiguity: Learning verb selectional
preference with bayesian networks. In COLING.
Stephen Clark and David J. Weir. 2002. Class-based
probability estimation using a semantic hierarchy.
Computational Linguistics, 28(2):187?206.
Ido Dagan, Lillian Lee, and Fernando C. N. Pereira.
1999. Similarity-Based Models of Word Cooccur-
rence Probabilities. Machine Learning, 34:43?69.
Katrin Erk, Sebastian Pado?, and Ulrike Pado?. 2010. A
flexible, corpus-driven model of regular and inverse
selectional preferences. Computational Linguistics,
36(4):723?763.
Katrin Erk. 2007. A simple, similarity-based model
for selectional preferences. In ACL.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational Linguis-
tics, 28(3):245?288.
Jerrold J. Katz and Jerry A. Fodor. 1963. The structure
of a semantic theory. Language, 39(2):170?210.
Frank Keller and Mirella Lapata. 2003. Using the web
to obtain frequencies for unseen bigrams. Computa-
tional Linguistics, 29(3):459?484.
Maria Lapata, Scott McDonald, and Frank Keller.
1999. Determinants of adjective-noun plausibility.
In EACL, pages 30?36. Association for Computa-
tional Linguistics.
Maria Lapata, Frank Keller, and Scott McDonald.
2001. Evaluating smoothing algorithms against
plausibility judgements. In ACL, pages 354?361.
Association for Computational Linguistics.
Lillian Lee. 1999. Measures of distributional similar-
ity. In ACL, pages 25?32, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Hang Li and Naoki Abe. 1998. Generalizing case
frames using a thesaurus and the mdl principle.
Computational linguistics, 24(2):217?244.
Ming Li, Benjamin M Dias, Ian Jarman, Wael El-
Deredy, and Paulo JG Lisboa. 2009. Grocery shop-
ping recommendations based on basket-sensitive
random walk. In SIGKDD, pages 1215?1224.
ACM.
David Liben-Nowell and Jon Kleinberg. 2007. The
link-prediction problem for social networks. Jour-
nal of the American society for information science
and technology, 58(7):1019?1031.
Jianhua Lin. 1991. Divergence measures based on the
shannon entropy. IEEE Transactions on Information
Theory, 37(1):145?151.
Mitchell P. Marcus, Beatrice Santorini, Mary Ann
Marcinkiewicz, and Ann Taylor. 1999. Treebank-
3.
Diana McCarthy, Sriram Venkatapathy, and Aravind K.
Joshi. 2007. Detecting compositionality of verb-
object combinations using selectional preferences.
In EMNLP-CoNLL.
Ken McRae, Michael J. Spivey-Knowltonb, and
Michael K. Tanenhausc. 1998. Modeling the influ-
ence of thematic fit (and other constraints) in on-line
sentence comprehension. Journal of Memory and
Language, 38(3):283?312.
Sebastian Pado?, Ulrike Pado?, and Katrin Erk. 2007.
Flexible, corpus-based modelling of human plausi-
bility judgements. In EMNLP/CoNLL, volume 7.
Patrick Pantel, Rahul Bhagat, Bonaventura Coppola,
Timothy Chklovski, and Eduard Hovy. 2007. Is-
p: Learning inferential selectional preferences. In
NAACL-HLT.
Robert Parker, David Graff, Junbo Kong, Ke Chen, and
Kazuaki Maeda. 2011. English gigaword fifth edi-
tion.
Philip Resnik. 1993. Selection and information: a
class-based approach to lexical relationships. IRCS
Technical Reports Series.
Philip Resnik. 1996. Selectional constraints: An
information-theoretic model and its computational
realization. Cognition, 61(1):127?159.
Philip Resnik. 1997. Selectional preference and sense
disambiguation. In Proceedings of the ACL SIGLEX
Workshop on Tagging Text with Lexical Semantics:
Why, What, and How. Washington, DC.
Alan Ritter, Mausam, and Oren Etzioni. 2010. A la-
tent dirichlet alocation method for selectional pref-
erences. In ACL.
Mats Rooth, Stefan Riezler, Detlef Prescher, Glenn
Carroll, and Franz Beil. 1999. Inducing a semanti-
cally annotated lexicon via em-based clustering. In
ACL.
Diarmuid O? Se?aghdha. 2010. Latent variable models
of selectional preference. In ACL.
1178
Irena Spasic? and Sophia Ananiadou. 2004. Us-
ing automatically learnt verb selectional preferences
for classification of biomedical terms. Journal of
Biomedical Informatics, 37(6):483?497.
Kristina Toutanova, Christopher D. Manning, Dan
Flickinger, and Stephan Oepen. 2005. Stochas-
tic hpsg parse disambiguation using the redwood-
s corpus. Research on Language & Computation,
3(1):83?105.
Yorick Wilks. 1973. Preference semantics. Technical
report, DTIC Document.
Hilmi Yildirim and Mukkai S. Krishnamoorthy. 2008.
A random walk method for alleviating the sparsity
problem in collaborative filtering. In Proceedings of
the 2008 ACM conference on Recommender system-
s, pages 131?138. ACM.
Tao Zhou, Jie Renan, Matu?s? Medo, and Yi-Cheng
Zhang. 2007. Bipartite network projection and
personal recommendation. Physical Review E,
76(4):046115.
1179
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 572?581,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Omni-word Feature and Soft Constraint
for Chinese Relation Extraction
Yanping Chen
?
Qinghua Zheng
?
?
MOEKLINNS Lab, Department of Computer Science and Technology
Xi?an Jiaotong University, China
ypench@gmail.com, qhzheng@mail.xjtu.edu.cn
?
Amazon.com, Inc.
wzhan@amazon.com
Wei Zhang
?
Abstract
Chinese is an ancient hieroglyphic. It is inat-
tentive to structure. Therefore, segmenting
and parsing Chinese are more difficult and less
accurate. In this paper, we propose an Omni-
word feature and a soft constraint method for
Chinese relation extraction. The Omni-word
feature uses every potential word in a sentence
as lexicon feature, reducing errors caused by
word segmentation. In order to utilize the
structure information of a relation instance, we
discuss how soft constraint can be used to cap-
ture the local dependency. Both Omni-word
feature and soft constraint make a better use
of sentence information and minimize the in-
fluences caused by Chinese word segmenta-
tion and parsing. We test these methods on
the ACE 2005 RDC Chinese corpus. The re-
sults show a significant improvement in Chi-
nese relation extraction, outperforming other
methods in F-score by 10% in 6 relation types
and 15% in 18 relation subtypes.
1 Introduction
Information Extraction (IE) aims at extracting
syntactic or semantic units with concrete concepts
or linguistic functions (Grishman, 2012; McCal-
lum, 2005). Instead of dealing with the whole doc-
uments, focusing on designated information, most
of the IE systems extract named entities, relations,
quantifiers or events from sentences.
The relation recognition task is to find the rela-
tionships between two entities. Successful recog-
nition of relation implies correctly detecting both
the relation arguments and relation type. Although
this task has received extensive research. The per-
formance of relation extraction is still unsatisfac-
tory with a F-score of 67.5% for English (23 sub-
types) (Zhou et al, 2010). Chinese relation extrac-
tion also faces a weak performance having F-score
about 66.6% in 18 subtypes (Dandan et al, 2012).
The difficulty of Chinese IE is that Chinese
words are written next to each other without de-
limiter in between. Lacking of orthographic word
makes Chinese word segmentation difficult. In
Chinese, a single sentence often has several seg-
mentation paths leading to the segmentation ambi-
guity problem (Liang, 1984). The lack of delimiter
also causes the Out-of-Vocabulary problem (OOV,
also known as new word detection) (Huang and
Zhao, 2007). These problems are worsened by the
fact that Chinese has a large number of characters
and words. Currently, the state-of-the-art Chinese
OOV recognition system has performance about
75% in recall (Zhong et al, 2012). The errors
caused by segmentation and OOV will accumulate
and propagate to subsequent processing (e.g. part-
of-speech (POS) tagging or parsing).
Therefore, the Chinese relation extraction is
more difficult. According to our survey, com-
pared to the same work in English, the Chinese re-
lation extraction researches make less significant
progress.
Based on the characteristics of Chinese, in this
paper, an Omni-word feature and a soft constraint
method are proposed for Chinese relation extrac-
tion. We apply these approaches in a maximum
entropy based system to extract relations from the
ACE 2005 corpus. Experimental results show that
our method has made a significant improvement.
The contributions of this paper include
1. Propose a novel Omni-word feature for Chi-
nese relation extraction. Unlike the tradi-
tional segmentation based method, which is a
partition of the sentence, the Omni-word fea-
ture uses every potential word in a sentence
as lexicon feature.
2. Aiming at the Chinese inattentive structure,
we utilize the soft constraint to capture the
local dependency in a relation instance. Four
constraint conditions are proposed to gener-
572
ate combined features to capture the local de-
pendency and maximize the classification de-
termination.
The rest of this paper is organized as follows.
Section 2 introduces the related work. The Omni-
word feature and soft constrain are proposed in
Section 3. We give the experimental results in Sec-
tion 3.2 and analyze the performance in Section 4.
Conclusions are given in Section 5.
2 Related Work
There are two paradigms extracting the relation-
ship between two entities: the Open Relation Ex-
traction (ORE) and the Traditional Relation Ex-
traction (TRE) (Banko et al, 2008).
Based on massive and heterogeneous corpora,
the ORE systems deal with millions or billions
of documents. Even strict filtrations or constrains
are employed to filter the redundancy information,
they often generate tens of thousands of relations
dynamically (Hoffmann et al, 2010). The practi-
cability of ORE systems depends on the adequate-
ness of information in a big corpus (Brin, 1999).
Most of the ORE systems utilize weak supervi-
sion knowledge to guide the extracting process,
such as: Databases (Craven and Kumlien, 1999),
Wikipedia (Wu and Weld, 2007; Hoffmann et al,
2010), Regular expression (Brin, 1999; Agichtein
and Gravano, 2000), Ontology (Carlson et al,
2010; Mohamed et al, 2011) or Knowledge Base
extracted automatically from Internet (Mintz et al,
2009; Takamatsu et al, 2012). However, when
iteratively coping with large heterogeneous data,
the ORE systems suffer from the ?semantic drift?
problem, caused by error accumulation (Curran
et al, 2007). Agichtein, Carlson and Fader et
al. (2010; 2011; 2000) propose syntactic and se-
mantic constraints to prevent this deficiency. The
soft constraints, proposed in this paper, are com-
bined features like these syntactic or semantic con-
straints, which will be discussed in Section 3.2.
The TRE paradigm takes hand-tagged ex-
amples as input, extracting predefined relation
types (Banko et al, 2008). The TRE systems
use techniques such as: Rules (Regulars, Pat-
terns and Propositions) (Miller et al, 1998), Ker-
nel method (Zhang et al, 2006b; Zelenko et al,
2003), Belief network (Roth and Yih, 2002), Lin-
ear programming (Roth and Yih, 2007), Maximum
entropy (Kambhatla, 2004) or SVM (GuoDong et
al., 2005). Compared to the ORE systems, the
TRE systems have a robust performance. Disad-
vantages of the TRE systems are that the manu-
ally annotated corpus is required, which is time-
consuming and costly in human labor. And mi-
grating between different applications is difficult.
However, the TRE systems are evaluable and com-
parable. Different systems running on the same
corpus can be evaluated appropriately.
In the field of Chinese relation extraction, Liu
et al (2012) proposed a convolution tree ker-
nel. Combining with external semantic resources,
a better performance was achieved. Che et
al. (2005) introduced a feature based method,
which utilized lexicon information around entities
and was evaluated on Winnow and SVM classi-
fiers. Li and Zhang et al (2008; 2008) explored
the position feature between two entities. For each
type of these relations, a SVM was trained and
tested independently. Based on Deep Belief Net-
work, Chen et al (2010) proposed a model han-
dling the high dimensional feature space. In addi-
tion, there are mixed models. For example, Lin et
al. (2010) employed a model, combining both the
feature based and the tree kernel based methods.
Despite the popularity of kernel based method,
Huang et al (2008) experimented with different
kernel methods and inferred that simply migrating
from English kernel methods can result in a bad
performance in Chinese relation extraction. Chen
and Li et al (2008; 2010) also pointed out that,
due to the inaccuracy of Chinese word segmenta-
tion and parsing, the tree kernel based approach
is inappropriate for Chinese relation extraction.
The reason of the tree kernel based approach not
achieve the same level of accuracy as that from En-
glish may be that segmenting and parsing Chinese
are more difficult and less accurate than process-
ing English.
In our research, we proposed an Omni-word
feature and a soft constraint method. Both ap-
proaches are based on the Chinese characteristics.
Therefore, better performance is expected. In the
following, we introduce the feature construction,
which discusses the proposed two approaches.
3 Feature Construction
In this section, the employed candidate features
are discussed. And four constraint conditions
are proposed to transform the candidate features
into combined features. The soft constraint is the
573
method to generate the combine features
1
.
3.1 Candidate Feature Set
In the ACE corpus, an entity is an object or set of
objects in the world. An entity mention is a ref-
erence to an entity. The entity mention is anno-
tated with its full extent and its head, referred to as
the extend mention and the head mention respec-
tively. The extent mention includes both the head
and its modifiers. Each relation has two entities as
arguments: Arg-1 and Arg-2, referred to as E1 and
E2. A relation mention (or instance) is the embod-
iment of a relation. It is referred by the sentence
(or clause) in which the relation is located in. In
our work, we focus on the detection and recogni-
tion of relation mention.
Relation identification is handled as a classifi-
cation problem. Entity-related information (e.g.
head noun, entity type, subtype, CLASS, LDC-
TYPE, etc.) are supposed to be known and pro-
vided by the corpus. In our experiment, the entity
type, subtype and the head noun are used.
All the employed features are simply classi-
fied into five categories: Entity Type and Subtype,
Head Noun, Position Feature, POS Tag and Omni-
word Feature. The first four are widely used. The
last one is proposed in this paper and is discussed
in detail.
Entity Type and Subtype: In ACE 2005 RDC
Chinese corpus, there are 7 entity types (Person,
Organization, GPE, Location, Facility, Weapon
and Vehicle) and 44 subtypes (e.g. Group, Gov-
ernment, Continent, etc.).
Head Noun: The head noun (or head mention)
of entity mention is manually annotated. This fea-
ture is useful and widely used.
Position Feature: The position structure be-
tween two entity mentions (extend mentions). Be-
cause the entity mentions can be nested, two en-
tity mentions may have four coarse structures: ?E1
is before E2?, ?E1 is after E2?, ?E1 nests in E2?
and ?E2 nests in E1?, encoded as: ?E1_B_E2?,
?E1_A_E2?, ?E1_N_E2? and ?E2_N_E1?.
POS Tag: In our model, we use only the ad-
jacent entity POS tags, which lie in two sides of
the entity mention. These POS tags are labelled
by the ICTCLAS package
2
. The POS tags are not
used independently. It is encoded by combining
1
If without ambiguity, we also use the terminology of
?soft constraint? denoting features generated by the em-
ployed constraint conditions.
2
http://ictclas.org/
the POS tag with the adjacent entity mention in-
formation. For example ?E1_Right_n? means
that the right side of the first entity is a noun (?n?).
Omni-word Feature: The notion of ?word?
in Chinese is vague and has never played a role
in the Chinese philological tradition (Sproat et
al., 1996). Some Chinese segmentation perfor-
mance has been reported precision scores above
95% (Peng et al, 2004; Xue, 2003; Zhang et
al., 2003). However, for the same sentence, even
native peoples in China often disagree on word
boundaries (Hoosain, 1992; Yan et al, 2010).
Sproat et al (1996) has showed that there is a con-
sistence of 75% on the segmentation among differ-
ent native Chinese speakers. The word-formation
of Chinese also implies that the meanings of a
compound word are made up, usually, by the
meanings of words that contained in it (Hu and
Du, 2012). So, fragments of phrase are also infor-
mative.
Because high precision can be received by using
simple lexical features (Kambhatla, 2004; Li et al,
2008). Making better use of such information is
beneficial. In consideration of the Chinese char-
acteristics, we use every potential word in a rela-
tion mention as the lexical features. For example,
relation mention ?????????? (Taipei
Daan Forest Park) has a ?PART-WHOLE? relation
type. The traditional segmentation method may
generate four lexical features {????, ????, ??
??, ????}, which is a partition of the relation
mention. On the other hand, the Omni-word fea-
ture denoting all the possible words in the relation
mention may generate features as:
{???, ???, ???, ???, ???, ???, ???, ???,
????, ????, ????, ????, ??????,
????????}3
Most of these features are nested or overlapped
mutually. So, the traditional character-based or
word-based feature is only a subset of the Omni-
word feature. To extract the Omni-word feature,
only a lexicon is required, then scan the sentence
to collect every word.
Because the number of lexicon entry determines
the dimension of the feature space, performance
of Omni-word feature is influenced by the lexicon
being employed. In this paper, we generate the
lexicon by merging two lexicons. The first lexicon
3
The generated Omni-word features dependent on the em-
ployed lexicon.
574
is obtained by segmenting every relation instance
using the ICTCLAS package, collecting very word
produced by ICTCLAS. Because the ICTCLAS
package was trained on annotated corpus contain-
ing many meaningful lexicon entries. We expect
this lexicon to improve the performance. The sec-
ond lexicon is the Lexicon Common Words in Con-
temporary Chinese
4
.
Despite the Omni-word can be seen as a sub-
set of n-Gram feature. It is not the same as the
n-Gram feature. N-Gram features are more frag-
mented. In most of the instances, the n-Gram fea-
tures have no semantic meanings attached to them,
thus have varied distributions. Furthermore, for
a single Chinese word, occurrences of 4 charac-
ters are frequent. Even 7 or more characters are
not rare. Because Chinese has plenty of char-
acters
5
, when the corpus becoming larger, the n-
Gram (n?4) method is difficult to be adopted. On
the other hand, the Omni-word can avoid these
problems and take advantages of Chinese charac-
teristics (the word-formation and the ambiguity of
word segmentation).
3.2 Soft Constraint
The structure information (or dependent informa-
tion) of relation instance is critical for recognition.
However, even in English, ?deeper? analysis (e.g.
logical syntactic relations or predicate-argument
structure) may suffer from a worse performance
caused by inaccurate chunking or parsing. Hence,
the local dependency contexts around the rela-
tion arguments are more helpful (Zhao and Gr-
ishman, 2005). Zhang et al (2006a) also showed
that Path-enclosed Tree (PT) achieves the best per-
formance in the kernel based relation extraction.
In this field, the tree kernel based method com-
monly uses the parse tree to capture the struc-
ture information (Zelenko et al, 2003; Culotta and
Sorensen, 2004). On the other hand, the feature
based method usually uses the combined feature
to capture such structure information (GuoDong
et al, 2005; Kambhatla, 2004).
In the open relation extraction domain, syntac-
tic and semantic constraints are widely employed
to prevent the ?semantic drift? problem. Such con-
straints can also be seen as structural constraint.
4
Published by Ministry of Education of the People?s Re-
public of China in 2008, containing 56,008 entries.
5
Currently, at least 13000 characters are used by na-
tive Chinese people. Modern Chinese Dictionary: http:
//www.cp.com.cn/
Most of these constraints are hard constraints. Any
relation instance violating these constraints (or be-
low a predefined threshold) will be abandoned.
For example, Agichtein and Gravano (2000) gen-
erates patterns according to a confidence threshold
(?
t
). Fader et al (2011) utilizes a confidence func-
tion. And Carlson et al (2010) filters candidate
instances and patterns using the number of times
they co-occurs.
Deleting of relation instances is acceptable for
open relation extraction because it always deals
with a big data set. But it?s not suitable for tra-
ditional relation extraction, and will result in a
low recall. Utilizing the notion of combined fea-
ture (GuoDong et al, 2005; Kambhatla, 2004), we
replace the hard constraint by the soft constraint.
Each soft constraint (combined feature) has a pa-
rameter trained by the classifier indicating the dis-
crimination ability it has. No subjective or priori
judgement is adopted to delete any potential de-
terminative constraint (except for the reason of di-
mensionality reduction).
Most of the researches make use of the com-
bined feature, but rarely analyze the influence of
the approaches we combine them. In this paper,
we use the soft constraint to model the local de-
pendency. It is a subset of the combined feature,
generated by four constraint conditions: singleton,
position sensitive, bin sensitive and semantic pair
. For every employed candidate feature, an appro-
priate constraint condition is selected to combine
them with additional information to maximize the
classification determination.
Singleton: A feature is employed as a single-
ton feature when it is used without combining with
any information. In our experiments, only the po-
sition feature is used as singleton feature.
Position Sensitive: A position sensitive feature
has a label indicating which entity mention it de-
pends on. In our experiment, the Head noun and
POS Tag are utilized as position sensitive features,
which has been introduced in Section 3.1. For ex-
ample, ???_E1? means that the head noun ??
?? depend on the first entity mention.
Semantic Pair: Semantic pair is generated by
combining two semantic units. Two kinds of
semantic pair are employed. Those are gener-
ated by combining two entity types or two en-
tity subtypes into a semantic pair. For example,
?Person_Location? denotes that the type of
the first relation argument is a ?Person? (entity
575
type) and the second is a ?Location? (entity type).
Semantic pair can capture both the semantic and
structure information in a relation mention.
Bin Sensitive: In our study, Omni-word feature
is not added as ?bag of words?. To use the Omni-
word feature, we segment each relation mention
by two entity mentions. Together with the two en-
tity mentions, we get five parts: ?FIRST?, ?MID-
DLE?, ?END?, ?E1? and ?E2? (or less, if the two
entity mentions are nested). Each part is taken
as an independent bin. A flag is used to distin-
guish them. For example, ???_Bin_F?, ??
?_Bin_E1? and ???_Bin_E? mean that the
lexicon entry ???? appears in three bins: the
FIRST bin, the first entity mention (E1) bin and
the END bin. They will be used as three indepen-
dent features.
To sum up, among the five candidate feature
sets, the position feature is used as a singleton fea-
ture. Both head noun and POS tag are position
sensitive. Entity types and subtypes are employed
as semantic pair. Only Omni-word feature is bin
sensitive. In the following experiments, focusing
on Chinese relation extraction, we will analyze the
performance of candidate feature sets and study
the influence of the constraint conditions.
sectionExperiments
In this section, methodologies of the Omni-
word feature and the soft constraint are tested.
Then they are compared with the state-of-the-art
methods.
3.3 Settings and Results
We use the ACE 2005 RDC Chinese corpus, which
was collected from newswires, broadcasts and we-
blogs, containing 633 documents with 6 major re-
lation types and 18 subtypes. There are 8,023 rela-
tions and 9,317 relation mentions. After deleting
5 documents containing wrong annotations
6
, we
keep 9,244 relation mentions as positive instances.
To get the negative instances, each document is
segmented into sentences
7
. Those sentences that
do not contain any entity mention pair are deleted.
For each of the remained sentences, we iteratively
extract every entity mention pair as the arguments
of relation instances for predicting. For example,
suppose a sentence has three entity mentions: A,B
6
DAVYZW {20041230.1024, 20050110.1403,
20050111.1514, 20050127.1720, 20050201.1538}.
7
The five punctuations are used as sentence boundaries:
Period (?), Question mark (?), Exclamatory mark (?),
Semicolon (?) and Comma (?).
and C. Because the relation arguments are order
sensitive, six entity mention pairs can be gener-
ated: [A,B], [A,C], [B,C], [B,A], [C,A] and [C,B].
After discarding the entity mention pairs that were
used as positive instances, we generated 93,283
negative relation instances labelled as ?OTHER?.
Then, we have 7 relation types and 19 subtypes.
A maximum entropy multi-class classifier is
trained and tested on the generated relation in-
stances. We adopt the five-fold cross validation
for training and testing. Because we are interested
in the 6 annotated major relation types and the 18
subtypes, we average the results of five runs on the
6 positive relation types (and 18 subtypes) as the
final performance. F-score is computed by
2? (Precision?Recall)
Precision+Recall
To implement the maximum entropy model, the
toolkit provided by Le (2004) is employed. The
iteration is set to 30.
Five candidate feature sets are employed to gen-
erate the combined features. The entity type and
subtype, head noun, position feature are referred
to as F
thp
8
. The POS tags are referred to as F
pos
.
The Omni-word feature set is denoted by F
ow
.
Table 1 gives the performance of our system on
the 6 types and 18 subtypes. Note that, in this pa-
per, bare numbers and numbers in the parentheses
represent the results of the 6 types and the 18 sub-
types respectively.
Table 1: Performance on Type (Subtype)
Features P R F
F
thp
61.51 48.85 54.46
(52.92) (36.92) (43.49)
F
ow
80.16 75.45 77.74
(66.98) (54.85) (60.31)
F
thp
? F
pos
83.93 77.81 80.76
(69.83) (61.63) (65.47)
F
thp
? F
ow
92.40 88.37 90.34
(81.94) (70.69) (75.90)
F
thp
? F
pos
? F
ow
92.26 88.51 90.35
(80.52) (70.96) (75.44)
In Row 1, because F
thp
are features directly ob-
tained from annotated corpus, we take this per-
8
?thp? is an acronym of ?type, head, position?. Features
in F
thp
are the candidate features combined with the corre-
sponding constraint conditions. The following F
pos
and F
ow
are the same.
576
formance as our referential performance. In Row
2, with only the F
ow
feature, the F-score already
reaches 77.74% in 6 types and 60.31% in 18 sub-
types. The last row shows that adding the F
pos
al-
most has no effect on the performance when both
the F
thp
and F
ow
are in use. The results show that
F
ow
is effective for Chinese relation extraction.
The superiorities of Owni-word feature depend
on three reasons. First, the specificity of Chi-
nese word-formation indicates that the subphrases
of Chinese word (or phrase) are also informative.
Second, most of relation instances have limited
context. The Owni-word feature, utilizing every
possible word in them, is a better way to capture
more information. Third, the entity mentions are
manually annotated. They can precisely segment
the relation instance into corresponding bins. Seg-
mentation of bins bears the sentence structure in-
formation. Therefore, the Owni-word feature with
bin information can make a better use of both the
syntactic information and the local dependency.
3.4 Comparison
Various systems were proposed for Chinese re-
lation extraction. We mainly focus on systems
trained and tested on the ACE corpus. Table 2 lists
three systems.
Table 2: Survey of Other Systems
System P R F
Che et al (2005) 76.13 70.18 73.27
Zhang et al (2011)
80.71 62.48 70.43
(77.75) (60.20) (67.86)
Liu et al (2012)
81.1 61.0 69.0
(79.1) (57.5) (66.6)
Che et al (2005) was implemented on the ACE
2004 corpus, with 2/3 data for training and 1/3 for
testing. The performance was reported on 7 re-
lation types: 6 major relation types and the none
relation (or negative instance). Zhang et al (2011)
was based on the ACE 2005 corpus with 75% data
for training and 25% for testing. Performances
about the 7 types and 19 subtypes were given.
Both of them are feature based methods. Liu et
al. (2012) is a kernel based method evaluated on
the ACE 2005 corpus. The five-fold cross valida-
tion was used and declared the performances on 6
relation types and 18 subtypes.
The data preprocessing makes differences from
our experiments to others. In order to give a bet-
ter comparison with the state-of-the-art methods,
based on our experiment settings and data, we im-
plement the two feature based methods proposed
by Che et al (2005) and Zhang et al (2011) in Ta-
ble 2. The results are shown in Table 3.
In Table 3, Ei (i ? 1, 2) represents entity men-
tion. ?Order? in Che et al (2005) denotes the posi-
tion structure of entity mention pair. Four types of
order are employed (the same as ours). Word
Ei
+
?
k
and POS
Ei
+
?
k
are the words and POS of Ei, ?
+
?
k?
means that it is the kth word (of POS) after (+)
or before (-) the corresponding entity mention. In
this paper, k = 1 and k = 2 were set.
In Row 2, the ?Uni-Gram? represents the Uni-
gram features of internal and external character
sequences. Internal character sequences are the
four entity extend and head mentions. Five kinds
of external character sequences are used: one In-
Between character sequence between E1 and E2
and four character sequences around E1 and E2 in
a given window size w s. The w s is set to 4. The
?Bi-Gram? is the 2-gram feature of internal and
external character sequences. Instead of the 4 po-
sition structures, the 9 position structures are used.
Please refer to Zhang et al (2011) for the details
of these 9 position structures.
In Table 3, it is shown that our system outper-
forms other systems, in F-score, by 10% on 6 re-
lation types and by 15% on 18 subtypes.
For researchers who are interested in our work,
the source code of our system and our imple-
mentations of Che et al (2005) and Zhang et
al. (2011) are available at https://github.
com/YPench/CRDC.
4 Discussion
In this section, we analyze the influences of em-
ployed feature sets and constraint conditions on
the performances.
Most papers in relation extraction try to aug-
ment the number of employed features. In our ex-
periment, we found that this does not always guar-
antee the best performance, despite the classifier
being adopted is claimed to control these features
independently. Because features may interact mu-
tually in an indirect way, even with the same fea-
ture set, different constraint conditions can have
significant influences on the final performance.
In Section 3, we introduced five candidate fea-
ture sets. Instead of using them as independent
features, we combined them with additional in-
577
Table 3: Comparing With the State-of-the-Art Methods
System Feature Set P R F
(Che et al, 2005)
Ei.Type, Ei.Subtype, Order, Word
Ei
+
?
1
,
Word
Ei
+
?
2
, POS
Ei
+
?
1
, POS
Ei
+
?
2
84.81 75.69 79.99
(64.89) (52.99) (58.34)
(Zhang et al, 2011)
Ei.Type, Ei.Subtype, 9 Position Feature,
Uni-Gram, Bi-Gram
79.56 72.99 76.13
(66.78) (54.56) (60.06)
Ours F
thp
? F
pos
? F
ow
92.26 88.51 90.35
(80.52) (70.96) (75.44)
formation. We proposed four constraint condi-
tions to generate the soft constraint features. In
Table 4, the performances of candidate features
are compared when different constraint conditions
was employed.
In Column 3 of Table 4 (Constraint Condi-
tion), (1), (2), (3), (4) and (5) stand for the referen-
tial feature sets
9
in Table 1. Symbol ?/? means that
the corresponding candidate features in the refer-
ential feature set are substituted by the new con-
straint condition. Par in Column 4 is the num-
ber of parameters in the trained maximum entropy
model, which indicate the model complexity. I in
Column 5 is the influence on performance. ?-? and
?+? mean that the performance is decreased or in-
creased.
The first observation is that the combined fea-
tures are more powerful than used as singletons.
Model parameters are increased by the combined
features. Increasing of parameters projects the
relation extraction problem into a higher dimen-
sional space, making the decision boundaries be-
come more flexible.
The named entities in the ACE corpus are also
annotated with the CLASS and LDCTYPE labels.
Zhou et al (2010) has shown that these labels can
result in a weaker performance. Row 1, 2 and 3
show that, no matter how they are used, the perfor-
mances decrease obviously. The reason of the per-
formance degradation may be caused by the prob-
lem of over-fitting or data sparseness.
At most of the time, increase of model param-
eters can result in a better performance. Except
in Row 8 and Row 11, when two head nouns
of entity pair were combined as semantic pair
and when POS tag were combined with the en-
tity type, the performances are decreased. There
are 7356 head nouns in the training set. Combin-
ing two head nouns may increase the feature space
9
(1), (2), (3), (4) and (5) denote F
thp
, F
ow
, F
thp
?F
pos
,
F
thp
? F
ow
and F
thp
? F
pos
? F
ow
respectively.
by 7356? (7356? 1). Such a large feature space
makes the occurrence of features close to a random
distribution, leading to a worse data sparseness.
In Row 4, 10 and 13, these features are used as
singleton, the performance degrades considerably.
This means that, the missing of sentence structure
information on the employed features can lead to
a bad performance.
Row 9 and 12 show an interesting result. Com-
paring the reference set (5) with the reference set
(3), the Head noun and adjacent entity POS tag
get a better performance when used as singletons.
These results reflect the interactions between dif-
ferent features. Discussion of this issue is be-
yond this paper?s scope. In this paper, for a better
demonstration of the constraint condition, we still
use the Position Sensitive as the default setting to
use the Head noun and the adjacent entity POS
tag.
Row 13 and 14 compare the Omni-word fea-
ture (By-Omni-word) with the traditional seg-
mentation based feature (By-Segmentation). By-
Segmentation denotes the traditional segmentation
based feature set generated by a segmentation tool,
collecting every output of relation mention. In this
place, the ICTCLAS package is adopted too.
Conventionally, if a sentence is perfectly seg-
mented, By-Segmentation is straightforward and
effective. But, our experiment shows different ob-
servations. Row 13 and 14 show that the Omni-
word method outperforms the traditional method.
Especially, when the bin information is used (Row
15), the performance of Omni-word feature in-
creases considerably.
Row 14 shows that, compared with the tradi-
tional method, the Omni-word feature improves
the performance by about 8.79% in 6 relation
types and 11.83% in 18 subtypes in F-core. Such
improvement may reside in the three reasons dis-
cussed in Section 3.3.
In short, from Table 4 we have seen that the en-
578
Table 4: Influence of Feature Set
No. Feature Constraint Condition Par P R F I
1
entity
CLASS and
LDCTYPE
(1)/as singleton
21,112 60.29 42.82 50.07 -4.39
21,910 (41.70) (25.18) (31.40) -12.09
2
(1)/combined with
positional Info
21,159 63.02 44.47 52.15 -2.31
22,013 (41.61) (26.31) (32.24) -11.25
3 (1)/as semantic pair
21,207 63.35 47.67 54.40 -0.06
22,068 (42.98) (31.34) (36.25) -7.24
4
Type,
Subtype
semantic
pair
(1)/as singleton
19,390 51.37 29.16 37.20 -17.26
147,435 (32.8) (18.97) (24.06) -19.43
5
(1)/combined with
positional info
19,524 61.77 43.67 51.17 -3.29
20,297 (41.13) (26.83) (32.47) -11.02
6 (5)/as singleton
105,865 91.39 87.92 89.62 -0.73
121,218 (79.32) (68.73) (73.65) -1.79
7
head noun
(3)/as singleton
21,450 85.66 75.74 80.40 -0.36
22,409 (64.38) (57.14) (60.55) -0.34
8 (3)/as semantic pair
77,333 83.05 73.14 77.78 -2.54
77,947 (59.70) (51.70) (55.41) -5.48
9 (5)/as singleton
100,963 92.50 88.90 90.66 +0.31
115,499 (82.63) (71.67) (76.76) +1.32
10
adjacent
entity POS
tag
(3)/as singleton
21,450 72.66 61.16 66.41 -13.91
22,409 (62.42) (45.69) (52.76) -8.13
11
(3)/combined with
entity type
22,151 80.66 71.67 75.90 -4.42
23,357 (63.41) (53.16) (57.83) -3.06
12 (5)/as singleton
106,931 92.50 88.66 90.54 +0.19
121,194 (82.04) (71.36) (76.33) +0.89
13
Omni-word
feature
(2)/By-Segmentation as
singleton
36,916 67.19 60.12 63.46 -14.28
41,652 (55.85) (44.50) (49.54) -10.77
14
(2)/By-Segmentation
with bins
79,430 71.12 66.90 68.95 -8.79
84,715 (54.76) (43.50) (48.48) -11.83
15
(2)/By-Omni-word as
singleton
47,428 69.67 63.77 66.59 -11.15
57,702 (54.85) (48.84) (51.67) -8.64
16 (5)/as singleton
57,321 91.43 86.37 88.83 -1.52
67,722 (76.43) (69.57) (72.84) -2.60
tity type and subtype maximize the performance
when used as semantic pair. Head noun and
adjacent entity POS tag are employed to com-
bine with positional information. Omni-word fea-
ture with bins information can increase the perfor-
mance considerably. Our model (in Section 3.3)
uses these settings. This insures that the perfor-
mances of the candidate features are optimized.
5 Conclusion
In this paper, We proposed a novel Omni-word
feature taking advantages of Chinese sub-phrases.
We also introduced the soft constraint method for
Chinese relation recognition. The soft constraint
utilizes four constraint conditions to catch the
structure information in a relation instance. Both
the Omni-word feature and soft constrain make
better use of information a sentence has, and min-
imize the deficiency caused by Chinese segmenta-
tion and parsing.
The size of the employed lexicon determines the
dimension of the feature space. The first impres-
sion is that more lexicon entries result in more
power. However, more lexicon entries also in-
crease the computational complexity and bring in
noises. In our future work, we will study this issue.
The notion of soft constraints can also be extended
to include more patterns, rules, regexes or syntac-
579
tic constraints that have been used for information
extraction. The usability of these strategies is also
left for future work.
Acknowledgments
The research was supported in part by NSF of
China (91118005, 91218301, 61221063); 863
Program of China (2012AA011003); Cheung
Kong Scholar?s Program; Pillar Program of
NST (2012BAH16F02); Ministry of Education
of China Humanities and Social Sciences Project
(12YJC880117); The Ministry of Education Inno-
vation Research Team (IRT13035).
References
Eugene Agichtein and Luis Gravano. 2000. Snow-
ball: Extracting relations from large plain-text col-
lections. In Proceedings of DL ?00, pages 85?94.
ACM.
Michele Banko, Oren Etzioni, and Turing Center.
2008. The tradeoffs between open and traditional
relation extraction. Proceedings of ACL-HLT ?08,
pages 28?36.
Sergey Brin. 1999. Extracting patterns and relations
from the world wide web. The World Wide Web and
Databases, pages 172?183.
Andrew Carlson, Justin Betteridge, Richard C. Wang,
Estevam R. Hruschka Jr, and Tom M. Mitchell.
2010. Coupled semi-supervised learning for infor-
mation extraction. In Proceedings of WSDM ?10,
pages 101?110.
Wanxiang Che, Ting Liu, and Sheng Li. 2005. Auto-
matic entity relation extraction. Journal of Chinese
Information Processing, 19(2):1?6.
Yu Chen, Wenjie Li, Yan Liu, Dequan Zheng, and
Tiejun Zhao. 2010. Exploring deep belief network
for chinese relation extraction. In Proceedings of
CLP ?10, pages 28?29.
Mark Craven and Johan Kumlien. 1999. Constructing
biological knowledge bases by extracting informa-
tion from text sources. In Proceedings of ISMB ?99,
pages 77?86.
Aron Culotta and Jeffrey Sorensen. 2004. Dependency
tree kernels for relation extraction. In Proceedings
of ACL ?04, page 423.
James R. Curran, Tara Murphy, and Bernhard Scholz.
2007. Minimising semantic drift with mutual ex-
clusion bootstrapping. In Proceedings of PACLING
?07, pages 172?180.
Liu Dandan, Hu Yanan, and Qian Longhua. 2012.
Exploiting lexical semantic resource for tree kernel-
based chinese relation extraction. Natural Language
Processing and Chinese Computing, pages 213?224.
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying relations for open information
extraction. In Proceedings of EMNLP ?11, pages
1535?1545.
Ralph Grishman. 2012. Information extraction: Capa-
bilities and challenges. Notes prepared for the.
Zhou GuoDong, Su Jian, Zhang Jie, and Zhang Min.
2005. Exploring various knowledge in relation ex-
traction. In Proceedings of ACL ?05, pages 427?434.
Raphael Hoffmann, Congle Zhang, and Daniel S.
Weld. 2010. Learning 5000 relational extractors.
In Proceedings of ACL ?10, volume 10, pages 286?
295.
Rumjahn Hoosain. 1992. Psychological reality of the
word in chinese. Advances in psychology, 90:111?
130.
He Hu and Xiaoyong Du. 2012. Radical features for
chinese text classification. In Proceedings of FSKD
?12, pages 720?724.
Changning Huang and Hai Zhao. 2007. Chinese word
segmentation : A decade review. Journal of Chinese
Information Processing, 21(3):8?19.
Ruihong Huang, Le Sun, and Yuanyong Feng. 2008.
Study of kernel-based methods for chinese relation
extraction. Information Retrieval Technology, pages
598?604.
Nanda Kambhatla. 2004. Combining lexical, syntac-
tic, and semantic features with maximum entropy
models for extracting relations. In Proceedings of
ACL-demo ?04, page 22.
Zhang Le. 2004. Maximum entropy modeling toolkit
for python and c++. Natural Language Processing
Lab, Northeastern University, China.
Wenjie Li, Peng Zhang, Furu Wei, Yuexian Hou, and
Qin Lu. 2008. A novel feature-based approach to
chinese entity relation extraction. In Proceedings of
HLT-Short ?08, pages 89?92.
Nanyuan Liang. 1984. Written chinese word segmen-
tation system-cdws. Journal of Beijing Institute of
Aeronautics and Astronautics, 4.
Ruqi Lin, Jinxiu Chen, Xiaofang Yang, and Honglei
Xu. 2010. Research on mixed model-based chinese
relation extraction. In Proceedings of ICCSIT ?10,
volume 1, pages 687?691.
Andrew McCallum. 2005. Information extraction:
Distilling structured data from unstructured text.
Queue, 3(9):48?57.
580
Scott Miller, Michael Crystal, Heidi Fox, Lance
Ramshaw, Richard Schwartz, Rebecca Stone, and
Ralph Weischedel. 1998. Algorithms that learn to
extract information: Bbn: Tipster phase iii. In Pro-
ceedings of TIPSTER ?98, pages 75?89.
Mike Mintz, Steven Bills, Rion Snow, and Dan Juraf-
sky. 2009. Distant supervision for relation extrac-
tion without labeled data. In Proceedings of ACL
?09, pages 1003?1011.
Thahir P Mohamed, Estevam R Hruschka Jr., and
Tom M Mitchell. 2011. Discovering relations be-
tween noun categories. In Proceedings of EMNLP
?11, pages 1447?1455.
Fuchun Peng, Fangfang Feng, and Andrew McCallum.
2004. Chinese segmentation and new word detec-
tion using conditional random fields. In Proceedings
of COLING ?04.
Dan Roth and Wen-tau Yih. 2002. Probabilistic rea-
soning for entity & relation recognition. In Proceed-
ings of COLING ?02, pages 1?7.
Dan Roth and Wen-tau Yih. 2007. Global inference
for entity and relation identification via a linear pro-
gramming formulation. Introduction to Statistical
Relational Learning, pages 553?580.
Richard Sproat, William Gale, Chilin Shih, and Nancy
Chang. 1996. A stochastic finite-state word-
segmentation algorithm for chinese. Computational
linguistics, 22(3):377?404.
Shingo Takamatsu, Issei Sato, and Hiroshi Nakagawa.
2012. Reducing wrong labels in distant supervision
for relation extraction. In Proceedings of ACL ?12,
pages 721?729.
Fei Wu and Daniel S. Weld. 2007. Autonomously se-
mantifying wikipedia. In Proceedings of CIKM ?07,
pages 41?50.
Nianwen Xue. 2003. Chinese word segmentation as
character tagging. Computational Linguistics and
Chinese Language Processing, 8(1):29?48.
Ming Yan, Reinhold Kliegl, Eike Richter, Antje Nuth-
mann, and Hua Shu. 2010. Flexible saccade-target
selection in chinese reading. The Quarterly Journal
of Experimental Psychology, 63(4):705?725.
Dmitry Zelenko, Chinatsu Aone, and Anthony
Richardella. 2003. Kernel methods for relation
extraction. The Journal of Machine Learning Re-
search, 3:1083?1106.
Hua-Ping Zhang, Hong-Kui Yu, De-Yi Xiong, and Qun
Liu. 2003. Hhmm-based chinese lexical analyzer
ictclas. In Proceedings of SIGHAN ?03, pages 184?
187.
Min Zhang, Jie Zhang, and Jian Su. 2006a. Exploring
syntactic features for relation extraction using a con-
volution tree kernel. In Proceedings of HLT-NAACL
?06, pages 288?295.
Min Zhang, Jie Zhang, Jian Su, and Guodong Zhou.
2006b. A composite kernel to extract relations be-
tween entities with both flat and structured features.
In Proceedings of ACL ?06, pages 825?832.
Peng Zhang, Wenjie Li, Furu Wei, Qin Lu, and Yuexian
Hou. 2008. Exploiting the role of position feature in
chinese relation extraction. In Proceedings of LREC
?08.
Peng Zhang, Wenjie Li, Yuexian Hou, and Dawei Song.
2011. Developing position structure-based frame-
work for chinese entity relation extraction. ACM
Transactions on Asian Language Information Pro-
cessing (TALIP), 10(3):14.
Shubin Zhao and Ralph Grishman. 2005. Extracting
relations with integrated information using kernel
methods. In Proceedings of ACL ?05, pages 419?
426.
Ming Zhong, Sheng Wang, and Ming Wu. 2012. Re-
vising word lattice using support vector machine for
chinese word segmentation. In Proceedings of II-
WAS ?12, pages 352?355.
Guodong Zhou, Longhua Qian, and Jianxi Fan. 2010.
Tree kernel-based semantic relation extraction with
rich syntactic and semantic information. Informa-
tion Sciences, 180(8):1313?1325.
581
