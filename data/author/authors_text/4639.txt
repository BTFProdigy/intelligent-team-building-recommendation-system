Mostly-Unsupervised Statistical Segmentation of Japanese: 
Applications to Kanji 
Rie Kubota Ando and Lillian Lee 
Department of Computer Science 
Cornell University 
Ithaca, NY 14853-7501 
{kubotar, llee} @cs.cornell.edu 
Abstract 
Given the lack of word delimiters in written 
Japanese, word segmentation is generally consid- 
ered a crucial first step in processing Japanese texts. 
Typical Japanese segmentation algorithms rely ei- 
ther on a lexicon and grammar or on pre-segmented 
data. In contrast, we introduce a novel statistical 
method utilizing unsegmented training data, with 
performance on kanji sequences comparable to and 
sometimes surpassing that of morphological nalyz- 
ers over a variety of error metrics. 
I Introduction 
Because Japanese is written without delimiters be- 
tween words) accurate word segmentation to re- 
cover the lexical items is a key step in Japanese text 
processing. Proposed applications of segmentation 
technology include extracting new technical terms, 
indexing documents for information retrieval, and 
correcting optical character recognition (OCR) er- 
rors (Wu and Tseng, 1993; Nagao and Mori, 1994; 
Nagata, 1996a; Nagata, 1996b; Sproat et al, 1996; 
Fung, 1998). 
Typically, Japanese word segmentation is per- 
formed by morphological nalysis based on lexical 
and grammatical knowledge. This analysis is aided 
by the fact that there are three types of Japanese 
characters, kanji, hiragana, and katakana: changes 
in character type often indicate word boundaries, al- 
though using this heuristic alone achieves less than 
60% accuracy (Nagata, 1997). 
Character sequences consisting solely of kanji 
pose a challenge to morphologically-based seg- 
reenters for several reasons. First and most 
importantly, kanji sequences often contain domain 
terms and proper nouns: Fung (1998) notes that 
50-85% of the terms in various technical dictio- 
~The analogous situation i English would be if words were 
written without spaces between them. 
Sequence l ngth # of characters % of corpus 
1 - 3 kanji 20,405,486 25.6 
4 - 6 kanji 12,743,177 16.1 
more than 6 kanji 3,966,408 5.1 
Total 37,115,071 46.8 
Figure 1: Statistics from 1993 Japanese newswire 
(NIKKEI), 79,326,406 characters total. 
naries are composed at least partly of kanji. Such 
words tend to be missing from general-purpose 
lexicons, causing an unknown word problem for 
morphological nalyzers; yet, these terms are quite 
important for information retrieval, information 
extraction, and text summarization, making correct 
segmentation f these terms critical. Second, kanji 
sequences often consist of compound nouns, so 
grammatical constraints are not applicable. For 
instance, the sequence sha-chohlkenlgyoh-mulbu- 
choh (presidentlandlbusinesslgeneral m nager 
= "a president as well as a general manager of 
business") could be incorrectly segmented as: sha- 
chohlken-gyohlmulbu-choh (presidentl subsidiary 
business\[Tsutomu \[a name\]\[general manager); 
since both alternatives are four-noun sequences, 
they cannot be distinguished by part-of-speech 
information alone. Finally, heuristics based on 
changes in character type obviously do not apply to 
kanji-only sequences. 
Although kanji sequences are difficult to seg- 
ment, they can comprise a significant portion of 
Japanese text, as shown in Figure 1. Since se- 
quences of more than 3 kanji generally consist of 
more than one word, at least 21.2% of 1993 Nikkei 
newswire consists of kanji sequences requiring seg- 
mentation. Thus, accuracy on kanji sequences i  an 
important aspect of the total segmentation process. 
As an alternative to lexico-grammatical and su- 
pervised approaches, we propose a simple, effi- 
241 
cient segmentation method which learns mostly 
from very large amounts of unsegmented training 
data, thus avoiding the costs of building a lexicon 
or grammar or hand-segmenting large amounts of 
training data. Some key advantages of this method 
are: 
? No  Japanese-specific rules are employed, en- 
hancing portability to other languages. 
? A very small number of pre-segmented train- 
ing examples (as few as 5 in our experiments) 
are needed for good performance, as long as 
large amounts of unsegmented data are avail- 
able. 
? For long kanji strings, the method produces re- 
sults rivalling those produced by Juman 3.61 
(Kurohashi and Nagao, 1998) and Chasen 1.0 
(Matsumoto et al, 1997), two morphological 
analyzers in widespread use. For instance, we 
achieve 5% higher word precision and 6% bet- 
ter morpheme r call. 
2 A lgor i thm 
Our algorithm employs counts of character n-grams 
in an unsegmented corpus to make segmentation de- 
cisions. We illustrate its use with an example (see 
Figure 2). 
Let "A B C D W X Y Z" represent an eight-kanji 
sequence. To decide whether there should be a word 
boundary between D and W, we check whether n- 
grams that are adjacent to the proposed boundary, 
such as the 4-grams l ="A B C D" and 82 ="W 
X Y Z", tend to be more frequent than n-grams that 
straddle it, such as the 4-gram tl ---- "B C D W". If 
so, we have evidence of a word boundary between 
D and W, since there seems to be relatively little 
cohesion between the characters on opposite sides 
of this gap. 
The n-gram orders used as evidence in the seg- 
mentation decision are specified by the set N. For 
instance, if N = {4} in our example, then we pose 
the six questions of the form, "Is #(s~) > #(t j)?",  
where #(x)  denotes the number of occurrences of 
x in the (unsegmented) training corpus. If N = 
{2,4}, then two more questions (Is "#(C D) > 
#(D W)?" and "Is #(W X) > #(O W)?") are 
added. 
More formally, let s~ and 8~ be the non- 
straddling n-grams just to the left and right of lo- 
cation k, respectively, and let t~ be the straddling 
n-gram with j characters to the right of location k. 
s, ? 
I 
i ABCb{WXYZ 
t, 
% ? 
/, 
Figure 2: Collecting evidence for a word boundary 
- are the non-straddling n-grams 81 and 82 more 
frequent than the straddling n-grams tl, t2, and t3? 
Let I> (y, z) be an indicator function that is 1 when 
y > z, and 0 otherwise, 2 In order to compensate for 
the fact that there are more n-gram questions than 
(n - 1)-gram questions, we calculate the fraction of 
affirmative answers eparately for each n in N: 
2 n--1 1 
vn(k) = 2(n -  1) x>(#(87), 
~=1 j=l 
Then, we average the contributions of each n-gram 
order: 
1 
VN(k) = INI ~ vn(k) 
nEN 
After vN(k) is computed for every location, bound- 
aries are placed at all locations ~ such that either: 
? VN(g) > VN(e -- 1) and VN(g) > VN(e + 1) 
(that is, e is a local maximum), or 
? VN (2) > t, a threshold parameter. 
The second condition is necessary to allow for 
single-character words (see Figure 3). Note that it 
also controls the granularity of the segmentation: 
low thresholds encourage shorter segments. 
Both the count acquisition and the testing phase 
are efficient. Computing n-gram statistics for all 
possible values of n simultaneously can be done in 
O(m log m) time using suffix arrays, where m is 
the training corpus size (Manber and Myers, 1993; 
Nagao and Mori, 1994). However, if the set N of 
n-gram orders is known in advance, conceptually 
simpler algorithms uffice. Memory allocation for 
:Note that we do not take into account he magnitude of 
the difference between the two frequencies; see section 5 for 
discussion. 
242 
v~(k) 
A B \[C DI W X IY\] Z 
Figure 3: Determining word boundaries. The X- Y 
boundary is created by the threshold criterion, the 
other three by the local maximum condition. 
count tables can be significantly reduced by omit- 
ting n-grams occurring only once and assuming the 
count of unseen n-grams to be one. In the applica- 
tion phase, the algorithm is clearly linear in the test 
corpus size if \[NI is treated as a constant. 
Finally, we note that some pre-segmented data is 
necessary in order to set the parameters N and t. 
However, as described below, very little such data 
was required to get good performance; we therefore 
deem our algorithm to be "mostly unsupervised". 
3 Experimental Framework 
Our experimental data was drawn from 150 
megabytes of 1993 Nikkei newswire (see Figure 
1). Five 500-sequence held-out subsets were ob- 
tained from this corpus, the rest of the data serv- 
ing as the unsegmented corpus from which to derive 
character n-gram counts. Each held-out subset was 
hand-segmented and then split into a 50-sequence 
parameter-training set and a 450-sequence t st set. 
Finally, any sequences occurring in both a test set 
and its corresponding parameter-training set were 
discarded from the parameter-training set, so that 
these sets were disjoint. (Typically no more than 
five sequences were removed.) 
3.1 Held-out set annotation 
Each held-out set contained 500 randomly-extracted 
kanji sequences at least ten characters long (about 
twelve on average), lengthy sequences being the 
most difficult to segment (Takeda and Fujisaki, 
1987). To obtain the gold-standard annotations, we 
segmented the sequences by hand, using an observa- 
tion of Takeda and Fujisaki (1987) that many kanji 
compound words consist of two-character stem 
words together with one-character p efixes and suf- 
fixes. Using this terminology, our two-level bracket- 
ing annotation may be summarized as follows. 3 At 
3A complete description of the annotation policy, including 
the treatment of numeric expressions, may be found in a tech- 
nical report (Ando and Lee, 1999). 
the word level, a stem and its affixes are bracketed 
together as a single unit. At the morpheme level, 
stems are divided from their affixes. For example, 
although both naga-no (Nagano) and shi (city) can 
appear as individual words, naga-no-shi (Nagano 
city) is bracketed as \[\[naga-no\]\[shi\]\], since here shi 
serves as a suffix. Loosely speaking, word-level 
bracketing demarcates discourse entities, whereas 
morpheme-level brackets enclose strings that cannot 
be further segmented without loss of meaning. 4 For 
instance, if one segments naga-no in naga-no-shi 
into naga (long) and no (field), the intended mean- 
ing disappears. Here is an example sequence from 
our datasets: 
Three native Japanese speakers participated in 
the annotation: one segmented all the held-out data 
based on the above rules, and the other two reviewed 
350 sequences in total. The percentage of agree- 
ment with the first person's bracketing was 98.42%: 
only 62 out of 3927 locations were contested by a 
verifier. Interestingly, all disagreement was at the 
morpheme l vel. 
3.2 Baseline algorithms 
We evaluated our segmentation method by com- 
paring its performance against Chasen 1.05 (Mat- 
sumoto et al, 1997) and Juman 3.61, 6 (Kurohashi 
and Nagao, 1998), two state-of-the-art, publically- 
available, user-extensible morphological analyzers. 
In both cases, the grammars were used as distributed 
without modification. The sizes of Chasen's and Ju- 
man's default lexicons are approximately 115,000 
and 231,000 words, respectively. 
Comparison issues An important question that 
arose in designing our experiments was how to en- 
able morphological analyzers to make use of the 
parameter-training data, since they do not have pa- 
rameters to tune. The only significant way that they 
can be updated is by changing their grammars or 
lexicons, which is quite tedious (for instance, we 
had to add part-of-speech information ' to new en- 
tries by hand). We took what we felt to be a rea- 
sonable, but not too time-consuming, course of cre- 
ating new lexical entries for all the bracketed words 
in the parameter-training data. Evidence that this 
4This level of segmentation is consistent with Wu's (1998) 
Monotonicity Principle for segmentation. 
5http://cactus.aist-nara.ac.jp/lab/nlt/chasen.html 
6http:/Ipine.kuee.kyoto-u.ac.jplnl-resourceljuman.e.html 
243 
90 
85 
8o 
75 
70 
Word accuracy 
CHASEN JUMAN ~otimize optimize recaU optindze F 
~'ecision 
Figure 4: Word accuracy. The three rightmost 
groups represent our algorithm with parameters 
tuned for different optimization criteria. 
was appropriate comes from the fact that these ad- 
ditions never degraded test set performance, and in- 
deed improved itby one percent in some cases (only 
small improvements are to be expected because the 
parameter-training sets were fairly small). 
It is important to note that in the end, we are com- 
paring algorithms with access to different sources 
of knowledge. Juman and Chasen use lexicons and 
grammars developed by human experts. Our al- 
gorithm, not having access to such pre-compiled 
knowledge bases, must of necessity draw on other 
information sources (in this case, a very large un- 
segmented corpus and a few pre-segmented xam- 
ples) to compensate for this lack. Since we are in- 
terested in whether using simple statistics can match 
the performance of labor-intensive methods, we do 
not view these information sources as conveying 
an unfair advantage, specially since the annotated 
training sets were small, available to the morpho- 
logical analyzers, and disjoint from the test sets. 
4 Results 
We report he average r sults over the five test sets 
using the optimal parameter settings for the corre- 
sponding training sets (we tried all nonempty sub- 
sets of {2, 3, 4, 5, 6} for the set of n-gram orders N 
and all values in {.05, .1, .15, . . . ,  1} for the thresh- 
old t) 7. In all performance graphs, the "error bars" 
represent one standard eviation. The results for 
Chasen and Juman reflect he lexicon additions de- 
7For simplicity, ties were deterministically broken by pre- 
ferring smaller sizes of N, shorter n-grams in N, and larger 
threshold values, in that order. 
scribed in section 3.2. 
Word and morpheme accuracy The standard 
metrics in word segmentation are word precision 
and recall. Treating a proposed segmentation as a 
non-nested bracketing (e.g., "lAB ICI" corresponds 
to the bracketing "[AB][C]"), word precision (P) is 
defined as the percentage of proposed brackets that 
exactly match word-level brackets in the annotation; 
word recall (R) is the percentage of word-level an- 
notation brackets that are proposed by the algorithm 
in question; and word F combines precision and re- 
call: F = 2PR/(P + R). 
One problem with using word metrics is that 
morphological analyzers are designed to produce 
morpheme-level segments. To compensate, we al- 
tered the segmentations produced by Juman and 
Chasen by concatenating stems and affixes, as iden- 
tified by the part-of-speech information the analyz- 
ers provided. (We also measured morpheme accu- 
racy, as described below.) 
Figures 4 and 8 show word accuracy for Chasen, 
Juman, and our algorithm for parameter settings 
optimizing word precision, recall, and F-measure 
rates. Our algorithm achieves 5.27% higher preci- 
sion and 0.25% better F-measure accuracy than Ju- 
man, and does even better (8.8% and 4.22%, respec- 
tively) with respect o Chasen. The recall perfor- 
mance falls (barely) between that of Juman and that 
of Chasen. 
As noted above, Juman and Chasen were de- 
signed to produce morpheme-level segmentations. 
We therefore also measured morpheme precision, 
recall, and F measure, all defined analogously to 
their word counterparts. 
Figure 5 shows our morpheme accuracy results. 
We see that our algorithm can achieve better ecall 
(by 6.51%) and F-measure (by 1.38%) than Juman, 
and does better than Chasen by an even wider mar- 
gin (11.18% and 5.39%, respectively). Precision 
was generally worse than the morphological nalyz- 
ers. 
Compatible Brackets Although word-level accu- 
racy is a standard performance metric, it is clearly 
very sensitive to the test annotation. Morpheme ac- 
curacy suffers the same problem. Indeed, the au- 
thors of Juman and Chasen may well have con- 
structed their standard ictionaries using different 
notions of word and morpheme than the definitions 
we used in annotating the data. We therefore devel- 
oped two new, more robust metrics to measure the 
number of proposed brackets that would be incor- 
244 
[ [data] [base] ] [system] (annotation brackets  
Proposedsegmentadon wo~ morpheme 
e~o~ e~o~ 
[data][base] [system] 2 0 
[data][basesystem] 2 I 
[database] [sys][tem] 2 3 
compatible-bracket errors 
crossing morpheme-dividing 
0 0 
1 0 
0 2 
Figure 6: Examples of word, morpheme, and compatible-bracket errors. The sequence "data base" has been 
annotated as "[[data] [base]]" because "data base" and "database" are interchangeable. 
8O 
,?:: 
~- 75 
Morpheme accuracy 
85 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
o 
CHASEN JUMAN optimize optimize recall optir#.ze F 
Ixacir,~ 
Figure 5: Morpheme accuracy. 
rect with respect to any reasonable annotation. 
Our novel metrics account for two types of er- 
rors. The first, a crossing bracket, is a proposed 
bracket hat overlaps but is not contained within an 
annotation bracket (Grishman et al, 1992). Cross- 
ing brackets cannot coexist with annotation brack- 
ets, and it is unlikely that another human would 
create such brackets. The second type of er- 
ror, a morpheme-dividing bracket, subdivides a 
morpheme-level annotation bracket; by definition, 
such a bracket results in a loss of meaning. See Fig- 
ure 6 for some examples. 
We define a compatible bracket as a proposed 
bracket that is neither crossing nor morpheme- 
dividing. The compatible brackets rate is simply the 
compatible brackets precision. Note that this met- 
ric accounts for different levels of segmentation si-
multaneously, which is beneficial because the gran- 
ularity of Chasen and Juman's segmentation varies 
from morpheme l vel to compound word level (by 
our definition). For instance, well-known university 
names are treated assingle segments by virtue of be- 
ing in the default lexicon, whereas other university 
names are divided into the name and the word "uni- 
versity". Using the compatible brackets rate, both 
segmentations can be counted as correct. 
We also use the all-compatible brackets rate, 
which is the fraction of sequences for which all 
the proposed brackets are compatible. Intuitively, 
this function measures the ease with which a human 
could correct he output of the segmentation algo- 
rithm: if the all-compatible brackets rate is high, 
then the errors are concentrated in relatively few 
sequences; if it is low, then a human doing post- 
processing would have to correct many sequences. 
Figure 7 depicts the compatible brackets and all- 
compatible brackets rates. Our algorithm does bet- 
ter on both metrics (for instance, when F-measure 
is optimized, by 2.16% and 1.9%, respectively, in
comparison to Chasen, and by 3.15% and 4.96%, 
respectively, in comparison to Juman), regardless of 
training optimization function (word precision, re- 
call, or F - -  we cannot directly optimize the com- 
patible brackets rate because "perfect" performance 
is possible simply by making the entire sequence a 
single segment). 
Compatible and all-compatible brackets rates 
lOOT 
I 
g5 t 
85 . . . . . .  
80  . . . .  
75  
CI'~SEN JUMAN 0ptimize precision 00timize recall optimize F 
10 compatib4e brackets rates ? all-compatible brackets rates, 
Figure 7: Compatible brackets and all-compatible 
bracket rates when word accuracy is optimized. 
245 
precision 
recall 
F-measure 
Juman5 vs. Juman50 Our50 vs Juman50 Our5 vs. Juman5 
- 1.04 +5.27 +6.18 
-0.63 -4.39 -3.73 
-0.84 +0.26 +1.14 
Our5 vs. Juman50 
+5.14 
-4.36 
+0.30 
Figure 8: Relative word accuracy as a function of training set size. "5" and "50" denote training set size 
before discarding overlaps with the test sets. 
4.1 Discussion 
Minimal human effort is needed. In contrast 
to our mostly-unsupervised method, morphological 
analyzers need a lexicon and grammar ules built 
using human expertise. The workload in creating 
dictionaries on the order of hundreds of thousands 
of words (the size of Chasen's and Juman's de- 
fault lexicons) is clearly much larger than annotat- 
ing the small parameter-training sets for our algo- 
rithm. We also avoid the need to segment a large 
amount of parameter-training data because our al- 
gorithm draws almost all its information from an 
unsegmented corpus. Indeed, the only human effort 
involved in our algorithm is pre-segmenting the five 
50-sequence parameter training sets, which took 
only 42 minutes. In contrast, previously proposed 
supervised approaches have used segmented train- 
ing sets ranging from 1000-5000 sentences (Kash- 
ioka et al, 1998) to 190,000 sentences (Nagata, 
1996a). 
To test how much annotated training data is actu- 
ally necessary, we experimented with using minis- 
cule parameter-training sets: five sets of only five 
strings each (from which any sequences repeated in
the test data were discarded). It took only 4 minutes 
to perform the hand segmentation in this case. As 
shown in Figure 8, relative word performance was 
not degraded and sometimes even slightly better. In 
fact, from the last column of Figure 8 we see that 
even if our algorithm has access to only five anno- 
tated sequences when Juman has access to ten times 
as many, we still achieve better precision and better 
F measure. 
Both the local maximum and threshold condi- 
tions contribute. In our algorithm, a location k 
is deemed a word boundary if VN(k) is either (1) a 
local maximum or (2) at least as big as the thresh- 
old t. It is natural to ask whether we really need two 
conditions, or whether just one would suffice. 
We therefore studied whether optimal perfor- 
mance could be achieved using only one of the con- 
ditions. Figure 9 shows that in fact both contribute 
to producing ood segmentations. Indeed, in some 
cases, both are needed to achieve the best perfor- 
mance; also, each condition when used in isolation 
yields suboptimal performance with respect to some 
performance metrics. 
accuracy optimize optimize optimize 
precision recall F-measure 
word M M & T M 
morpheme M & T T T 
Figure 9: Entries indicate whether best performance 
is achieved using the local maximum condition (M), 
the threshold condition (T), or both. 
5 Related Work  
Japanese Many previously proposed segmenta- 
tion methods for Japanese text make use of either 
a pre-existing lexicon (Yamron et al, 1993; Mat- 
sumoto and Nagao, 1994; Takeuchi and Matsumoto, 
1995; Nagata, 1997; Fuchi and Takagi, 1998) or 
pre-segmented training data (Nagata, 1994; Papa: 
georgiou, 1994; Nagata, 1996a; Kashioka et al, 
1998; Mori and Nagao, 1998). Other approaches 
bootstrap from an initial segmentation provided by 
a baseline algorithm such as Juman (Matsukawa et 
al., 1993; Yamamoto, 1996). 
Unsupervised, non-lexicon-based methods for 
Japanese segmentation doexist, but they often have 
limited applicability. Both Tomokiyo and Ries 
(1997) and Teller and Batchelder (1994) explicitly 
avoid working with kanji charactes. Takeda and 
Fujisaki (1987) propose the short unit model, a 
type of Hidden Markov Model with linguistically- 
determined topology, to segment kanji compound 
words. However, their method does not handle 
three-character stem words or single-character stem 
words with affixes, both of which often occur in 
proper nouns. In our five test datasets, we found 
that 13.56% of the kanji sequences contain words 
that cannot be handled by the short unit model. 
Nagao and Mori (1994) propose using the heuris- 
246 
tic that high-frequency haracter n-grams may rep- 
resent (portions of) new collocations and terms, 
but the results are not experimentally evaluated, 
nor is a general segmentation algorithm proposed. 
The work of Ito and Kohda (1995) similarly relies 
on high-frequency haracter n-grams, but again, is 
more concerned with using these frequent n-grams 
as pseudo-lexicon entries; a standard segmentation 
algorithm is then used on the basis of the induced 
lexicon. Our algorithm, on the hand, is fundamen- 
tally different in that it incorporates no explicit no- 
tion of word, but only "sees" locations between 
characters. 
Chinese According to Sproat et al (1996), most 
prior work in Chinese segmentation has exploited 
lexical knowledge bases; indeed, the authors assert 
that they were aware of only one previously pub- 
lished instance (the mutual-information method of 
Sproat and Shih (1990)) of a purely statistical ap- 
proach. In a later paper, Palmer (1997) presents 
a transformation-based algorithm, which requires 
pre-segmented training data. 
To our knowledge, the Chinese segmenter most 
similar to ours is that of Sun et al (1998). They 
also avoid using a lexicon, determining whether a
given location constitutes a word boundary in part 
by deciding whether the two characters on either 
side tend to occur together; also, they use thresholds 
and several types of local minima and maxima to 
make segmentation decisions. However, the statis- 
tics they use (mutual information and t-score) are 
more complex than the simple n-gram counts that 
we employ. 
Our preliminary reimplementation of their 
method shows that it does not perform as well as 
the morphological analyzers on our datasets, al- 
though we do not want to draw definite conclusions 
because some aspects of Sun et als method seem 
incomparable to ours. We do note, however, that 
their method incorporates numerical differences 
between statistics, whereas we only use indicator 
functions; for example, once we know that one 
trigram is more common than another, we do not 
take into account he difference between the two 
frequencies. We conjecture that using absolute 
differences may have an adverse effect on rare 
sequences. 
6 Conclusion 
In this paper, we have presented a simple, mostly- 
unsupervised algorithm that segments Japanese se- 
quences into words based on statistics drawn from 
a large unsegmented corpus. We evaluated per- 
formance on kanji with respect o several metrics, 
including the novel compatible brackets and all- 
compatible brackets rates, and found that our al- 
gorithm could yield performances rivaling that of 
lexicon-based morphological nalyzers. 
In future work, we plan to experiment on 
Japanese sentences with mixtures of character 
types, possibly in combination with morphologi- 
cal analyzers in order to balance the strengths and 
weaknesses of the two types of methods. Since 
our method does not use any Japanese-dependent 
heuristics, we also hope to test it on Chinese or other 
languages as well. 
Acknowledgments 
We thank Minoru Shindoh and Takashi Ando for 
reviewing the annotations, and the anonymous re- 
viewers for their comments. This material was sup- 
ported in part by a grant from the GE Foundation. 
References 
Rie Ando and Lillian Lee. 1999. Unsupervised sta- 
tistical segmentation of Japanese kanji strings. 
Technical Report TR99-1756, Cornell University. 
Takeshi Fuchi and Shinichiro Takagi. 1998. 
Japanese morphological nalyzer using word co- 
occurrence - JTAG. In Proc. of COLING-ACL 
'98, pages 409-413. 
Pascale Fung. 1998. Extracting key terms from 
Chinese and Japanese texts. Computer Process- 
ing of Oriental Languages, 12(1 ). 
Ralph Grishman, Catherine Macleod, and John 
Sterling. 1992. Evaluating parsing strategies us- 
ing standardized parse files. In Proc. of the 3rd 
ANLP, pages 156--161. 
Akinori Ito and Kasaki Kohda. 1995. Language 
modeling by string pattern N-gram for Japanese 
speech recognition. In Proc. oflCASSP. 
Hideki Kashioka, Yasuhiro Kawata, Yumiko Kinjo, 
Andrew Finch, and Ezra W. Black. 1998. Use 
of mutual information based character clus- 
ters in dictionary-less morphological nalysis of 
Japanese. In Proc. of COL1NG-ACL '98, pages 
658-662. 
Sadao Kurohashi and Makoto Nagao. 1998. 
Japanese morphological nalysis ystem JUMAN 
version 3.6 manual. In Japanese. 
Udi Manber and Gene Myers. 1993. Suffix arrays: 
247 
A new method for on-line string searches. SIAM 
Journal on Computing, 22(5):935-948. 
T. Matsukawa, Scott Miller, and Ralph Weischedel. 
1993. Example-based correction of word seg- 
mentation and part of speech labelling. In Proc. 
of the HLT Workshop, ages 227-32. 
Yuji Matsumoto and Makoto Nagao. 1994. Im- 
provements of Japanese morphological nalyzer 
JUMAN. In Proc. of the International Workshop 
on Sharable Natural Language Resources, pages 
22-28. 
Yuji Matsumoto, Akira Kitauchi, Tatsuo Yamashita, 
Yoshitaka Hirano, Osamu Imaichi, and Tomoaki 
Imamura. 1997. Japanese morphological nal- 
ysis system ChaSen manual. Technical Report 
NAIST-IS-TR97007, Nara Institute of Science 
and Technology. In Japanese. 
Shinsuke Mori and Makoto Nagao. 1998. Un- 
known word extraction from corpora using n- 
gram statistics. Journal of the Information Pro- 
cessing Society of Japan, 39(7):2093-2100. In 
Japanese. 
Makoto Nagao and Shinsuke Mori. 1994. A new 
method of N-gram statistics for large number of 
n and automatic extraction of words and phrases 
from large text data of Japanese. In Proc. of the 
15th COLING, pages 611-615. 
Masaaki Nagata. 1994. A stochastic Japanese 
morphological analyzer using a forward-DP 
backward-A* n-best search algorithm. In Proc. 
of the 15th COLING, pages 201-207. 
Masaaki Nagata. 1996a. Automatic extraction of 
new words from Japanese texts using generalized 
forward-backward search. In Proc. of the Confer- 
ence on Empirical Methods in Natural Language 
Processing, pages 48-59. 
Masaaki Nagata. 1996b. Context-based spelling 
correction for Japanese OCR. In Proc. of the 16th 
COLING, pages 806-811. 
Masaaki Nagata. 1997. A self-organizing Japanese 
word segmenter using heuristic word identifica- 
tion and re-estimation. In Proc. of the 5th Work- 
shop on Very Large Corpora, pages 203-215. 
David Palmer. 1997. A trainable rule-based algo- 
rithm for word segmentation. I  Proc. of the 35th 
ACL/8th EACL, pages 321-328. 
Constantine P.Papageorgiou. 1994. Japanese word 
segmentation by hidden Markov model. In Proc. 
of the HLT Workshop, ages 283-288. 
Richard Sproat and Chilin Shih. 1990. A statistical 
method for finding word boundaries in Chinese 
text. Computer Processing of Chinese and Ori- 
ental Languages, 4:336-351. 
Richard Sproat, Chilin Shih, William Gale, and 
Nancy Chang. 1996. A stochastic finite-sate 
word-segmentation algorithm for Chinese. Com- 
putational Linguistics, 22(3). 
Maosong Sun, Dayang Shen, and Benjamin K. 
Tsou. 1998. Chinese word segmentation without 
using lexicon and hand-crafted training data. In 
Proc. of COLING-ACL '98, pages 1265-1271. 
Koichi Takeda and Tetsunosuke Fujisaki. 1987. 
Automatic decomposition of kanji compound 
words using stochastic estimation. Journal of 
the Information Processing Society of Japan, 
28(9):952-961. In Japanese. 
Kouichi Takeuchi and Yuji Matsumoto. 1995. 
HMM parameter learning for Japanese morpho- 
logical analyzer. In Proc, of the lOth Pacific Asia 
Conference on Language, Information and Com- 
putation (PACLING), pages 163-172. 
Virginia Teller and Eleanor Olds Batchelder. 1994. 
A probabilistic algorithm for segmenting non- 
kanji Japanese strings. In Proc. of the 12th AAAI, 
pages 742-747. 
Laura Mayfield Tomokiyo and Klaus Ries. 1997. 
What makes a word: learning base units in 
Japanese for speech recognition. In Proc. of the 
ACL Special Interest Group in Natural Language 
Learning (CoNLL97), pages 60-69. 
Zimin Wu and Gwyneth Tseng. 1993. Chinese text 
segmentation for text retrieval: Achievements 
and problems. Journal of the American Society 
for Information Science, 44(9):532-542. 
Dekai Wu. 1998. A position statement on Chinese 
segmentation, http://www.cs.ust.hk/-,~dekai/- 
papers/segmentation.html. Presented at the 
Chinese Language Processing Workshop, 
University of Pennsylvania. 
Mikio Yamamoto. 1996. A re-estimation method 
for stochastic language modeling from ambigu- 
ous observations. In Proc. of the 4th Workshop 
on Very Large Corpora, pages 155-167. 
J. Yamron, J. Baker, P. Bamberg, H. Chevalier, 
T. Dietzel, J. Elder, F. Kampmann, M. Mandel, 
L. Manganaro, T. Margolis, and E. Steele. 1993. 
LINGSTAT: An interactive, machine-aided trans- 
lation system. In Proc. of the HLT Workshop, 
pages 191-195. 
248 
Coling 2008: Companion volume ? Posters and Demonstrations, pages 15?18
Manchester, August 2008
The power of negative thinking:
Exploiting label disagreement in the min-cut classification framework
Mohit Bansal
Dept. of Computer Science & Engineering
Indian Institute of Technology Kanpur
mbansal47@gmail.com
Claire Cardie and Lillian Lee
Dept. of Computer Science
Cornell University
{cardie,llee}@cs.cornell.edu
Abstract
Treating classification as seeking minimum
cuts in the appropriate graph has proven ef-
fective in a number of applications. The
power of this approach lies in its abil-
ity to incorporate label-agreement prefer-
ences among pairs of instances in a prov-
ably tractable way. Label disagreement
preferences are another potentially rich
source of information, but prior NLP work
within the minimum-cut paradigm has not
explicitly incorporated it. Here, we re-
port on work in progress that examines
several novel heuristics for incorporating
such information. Our results, produced
within the context of a politically-oriented
sentiment-classification task, demonstrate
that these heuristics allow for the addition
of label-disagreement information in a way
that improves classification accuracy while
preserving the efficiency guarantees of the
minimum-cut framework.
1 Introduction
Classification algorithms based on formulating the
classification task as one of finding minimum s-t
cuts in edge-weighted graphs ? henceforth min-
imum cuts or min cuts ? have been successfully
employed in vision, computational biology, and
natural language processing. Within NLP, appli-
cations include sentiment-analysis problems (Pang
and Lee, 2004; Agarwal and Bhattacharyya, 2005;
Thomas et al, 2006) and content selection for text
generation (Barzilay and Lapata, 2005).
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
As a classification framework, the minimum-
cut approach is quite attractive. First, it provides
a principled, yet flexible mechanism for allowing
problem-specific relational information ? includ-
ing several types of both hard and soft constraints
? to influence a collection of classification deci-
sions. Second, in many important cases, such as
when all the edge weights are non-negative, find-
ing a minimum cut can be done in a provably effi-
cient manner.
To date, however, researchers have restricted the
semantics of the constraints mentioned above to
encode pair-wise ?agreement? information only.
There is a computational reason for this restriction:
?agreement? and ?disagreement? information are
arguably most naturally expressed via positive and
negative edge weights, respectively; but in general,
the inclusion of even a relatively small number of
negative edge weights makes finding a minimum
cut NP-hard (McCormick et al, 2003).
To avoid this computational issue, we propose
several heuristics that encode disagreement infor-
mation with non-negative edge weights. We in-
stantiate our approach on a sentiment-polarity clas-
sification task ? determining whether individual
conversational turns in U.S. Congressional floor
debates support or oppose some given legislation.
Our preliminary results demonstrate promising im-
provements over the prior work of Thomas et al
(2006), who considered only the use of agreement
information in this domain.
2 Method
2.1 Min-cut classification framework
Binary classification problems are usually ap-
proached by considering each classification deci-
sion in isolation. More formally, let X
test
=
15
{x
1
, x
2
, . . . , x
n
} be the test instances, drawn from
some universe X , and let C = {c
1
, c
2
} be
the two possible classes. Then, the usual ap-
proach can often be framed as labeling each x
i
according to some individual-preference function
Ind:X ? C?<, such as the signed distance to
the dividing hyperplane according to an SVM or
the posterior class probability assigned by a Naive
Bayes classifier.
But when it is difficult to accurately classify a
particular x
i
in isolation, there is a key insight
that can help: knowing that x
i
has the same la-
bel as an easily-categorized x
j
makes labeling x
i
easy. Thus, suppose we also have an association-
preference function Assoc:X ?X?< express-
ing a reward for placing two items in the same
class; an example might be the output of an agree-
ment classifier or a similarity function. Then, we
can search for a classification function c(x
i
|X
test
)
? note that all of X
test
can affect an instance?s la-
bel ? that minimizes the total ?pining? of the test
items for the class they were not assigned to due to
either their individual or associational preferences:
?
i
Ind(x
i
, c(x
i
|X
test
)) +
??
?
i,j:c(x
i
|X
test
)=c(x
j
|X
test
)
Assoc(x
i
, x
j
),
where c(x
i
|X
test
) is the class ?opposite? to
c(x
i
|X
test
), and the free parameter ? regulates
the emphasis on agreement information. Solutions
to the above minimization problem correspond to
minimum s-t cuts in a certain graph, and if both
Ind and Assoc are non-negative functions, then,
surprisingly, minimum cuts can be found in poly-
nomial time; see Kleinberg and Tardos (2006, Sec-
tion 7.10) for details. But, as mentioned above,
allowing negative values makes finding a solution
intractable in the general case.
2.2 Prior work discards some negative values
The starting point for our work is Thomas et
al. (2006) (henceforth TPL). The reason for this
choice is that TPL used minimum-cut-based classi-
fication wherein signed distances to dividing SVM
hyperplanes were employed to define Ind(x, c)
and Assoc(x, x?). It was natural to use SVMs,
since association was determined by classification
rather than similarity ? specifically, categorizing
references by one congressperson to another as re-
flecting agreement or not ? but as a result, neg-
ative association-preferences (e.g., negative dis-
tance to a hyperplane) had to be accounted for.
We formalize TPL?s approach at a high
level as follows. Let Ind?:X ? C?< and
Assoc
?
:X ?X?< be initial individual- and
association-preference functions, such as the
signed distances mentioned above. TPL create two
non-negative conversion functions f :<? [0, 1]
and g:<? [0, 1], and then define
Ind(x
i
, c) := f(Ind
?
(x
i
, c))
Assoc(x
i
, x
j
) := g(Assoc
?
(x
i
, x
j
))
so that an optimal classification can be found in
polynomial time, as discussed above. We omit the
exact definitions of f and g in order to focus on
what is important here: roughly speaking, f and
g normalize values and handle outliers1, with the
following crucial exception. While negative initial
individual preferences for one class can be trans-
lated into positive individual preferences for the
other, there is no such mechanism for negative val-
ues of Assoc?; so TPL resort to defining g to be
0 for negative arguments. They thus discard po-
tentially key information regarding the strength of
label disagreement preferences.
2.3 Encoding negative associations
Instead of discarding the potentially crucial label-
disagreement information represented by negative
Assoc
? values, we propose heuristics that seek to
incorporate this valuable information, but that keep
Ind and Assoc non-negative (by piggy-backing off
of TPL?s pre-existing conversion-function strat-
egy2) to preserve the min-cut-classification effi-
ciency guarantees.
We illustrate our heuristics with a running
example. Consider a simplified setting with only
two instances x
1
and x
2
; f(z) = z; g(z) = 0 if
z < 0, 1 otherwise; and Ind? values (numbers
labeling left or right arrows in the diagrams below,
e.g., Ind?(x
1
, c
1
) = .7) and Assoc? value (the -2
labeling the up-and-down arrow) as depicted here:
? [.7]? x
1
?[.3]?
c
1
m [?2] c
2
? [.6]? x
2
?[.4]?
Then, the resulting TPL Ind and Assoc values are
1Thus, strictly speaking, f and g also depend on Ind?,
Assoc
?
, and X
test
, but we suppress this dependence for nota-
tional compactness.
2Our approach also applies to definitions of f and g dif-
ferent from TPL?s.
16
? [.7]? x
1
?[.3]?
c
1
m [0] c
2
? [.6]? x
2
?[.4]?
Note that since the initial -2 association value is
ignored, c(x
1
|X
test
) = c(x
2
|X
test
) = c
1
appears
to be a good classification according to TPL.
The Scale all up heuristic Rather than discard
disagreement information, a simple strategy is to
just scale up all initial association preferences by a
sufficiently large positive constant N :
Ind(x
i
, c) := f(Ind
?
(x
i
, c))
Assoc(x
i
, x
j
) := g(Assoc
?
(x
i
, x
j
) + N)
For N = 3 in our example, we get
? [.7]? x
1
?[.3]?
c
1
m [1] c
2
? [.6]? x
2
?[.4]?
This heuristic ensures that the more negative the
Assoc
? value, the lower the cost of separating the
relevant item pair (whereas TPL don?t distinguish
between negative Assoc? values). The heuristic
below tries to be more proactive, forcing such
pairs to receive different labels.
The SetTo heuristic We proceed through
x
1
, x
2
, . . . in order. Each time we encounter
an x
i
where Assoc?(x
i
, x
j
) < 0 for some
j > i, we try to force x
i
and x
j
into dif-
ferent classes by altering the four relevant
individual-preferences affecting this pair of in-
stances, namely, f(Ind?(x
i
, c
1
)), f(Ind
?
(x
i
, c
2
)),
f(Ind
?
(x
j
, c
1
)), and f(Ind?(x
j
, c
2
)). Assume
without loss of generality that the largest of
these values is the first one. If we respect
that preference to put x
i
in c
1
, then according
to the association-preference information, it
follows that we should put x
j
in c
2
. We can
instantiate this chain of reasoning by setting
Ind(x
i
, c
1
) := max(?, f(Ind
?
(x
i
, c
1
)))
Ind(x
i
, c
2
) := min(1? ?, f(Ind
?
(x
i
, c
2
)))
Ind(x
j
, c
1
) := min(1? ?, f(Ind
?
(x
j
, c
1
)))
Ind(x
j
, c
2
) := max(?, f(Ind
?
(x
j
, c
2
)))
for some constant ? ? (.5, 1], and making no
change to TPL?s definition of Assoc. For ? = .8
in our example, we get
? [.8]? x
1
?[.2]?
c
1
m [0] c
2
? [.2]? x
2
?[.8]?
Note that as we proceed through x
1
, x
2
, . . . in
order, some earlier changes may be undone.
The IncBy heuristic A more conservative ver-
sion of the above heuristic is to increment and
decrement the individual-preference values so that
they are somewhat preserved, rather than com-
pletely replace them with fixed constants:
Ind(x
i
, c
1
) := min(1, f(Ind
?
(x
i
, c
1
)) + ?)
Ind(x
i
, c
2
) := max(0, f(Ind
?
(x
i
, c
2
))? ?)
Ind(x
j
, c
1
) := max(0, f(Ind
?
(x
j
, c
1
))? ?)
Ind(x
j
, c
2
) := min(1, f(Ind
?
(x
j
, c
2
)) + ?)
For ? = .1, our example becomes
? [.8]? x
1
?[.2]?
c
1
m [0] c
2
? [.5]? x
2
?[.5]?
3 Evaluation
For evaluation, we adopt the sentiment-
classification problem tackled by TPL: clas-
sifying speech segments (individual conversational
turns) in a U.S. Congressional floor debate as
to whether they support or oppose the legis-
lation under discussion. TPL describe many
reasons why this is an important problem. For
our purposes, this task is also very convenient
because all of TPL?s computed raw and converted
Ind
? and Assoc? data are publicly available at
www.cs.cornell.edu/home/llee/data/convote.html.
Thus, we used their calculated values to imple-
ment our algorithms as well as to reproduce their
original results.3
One issue of note is that TPL actually in-
ferred association preferences between speakers,
not speech segments. We do the same when ap-
plying SetTo or IncBy to a pair {x
i
, x
j
} by con-
sidering the average of f(Ind?(x
k
, c
1
)) over all
x
k
uttered by the speaker of x
i
, instead of just
f(Ind
?
(x
i
, c
1
)). The other three relevant individ-
ual values are treated similarly. We also make
appropriate modifications (according to SetTo and
IncBy) to the individual preferences of all such x
k
simultaneously, not just x
i
, and similarly for x
j
.
A related issue is that TPL assume that all
speech segments by the same speaker should have
the same label. To make experimental compar-
isons meaningful, we follow TPL in considering
two different instantiations of this assumption. In
segment-based classification, Assoc(x
i
, x
j
) is set
to an arbitrarily high positive constant if the same
speaker uttered both x
i
and x
j
. In speaker-based
classification, Ind?(x
i
, c) is produced by running
3For brevity, we omit TPL?s ?high-threshold? variants.
17
 60
 62
 64
 66
 68
 70
 72
 74
 76
 78
SetTo(.6)SVM SetTo(1) IncBy(.25)IncBy(.15)TPL IncBy(.05)Scale all up SetTo(.8)
pe
rc
en
t c
or
re
ct
ALGORITHMS
Test-set classification accuracies, using held-out parameter estimation
segment-based, test
speaker-based, test
best TPL, test
Figure 1: Experimental results. ?SVM?: classification using only individual-preference information.
Values of ? are indicated in parentheses next to the relevant algorithm names.
an SVM on the concatenation of all speeches ut-
tered by x
i
?s speaker.
Space limits preclude inclusion of further de-
tails; please see TPL for more information.
3.1 Results and future plans
The association-emphasis parameter ? was trained
on held-out data, with ties broken in favor of the
largest ? in order to emphasize association in-
formation. We used Andrew Goldberg?s HIPR
code (http://avglab.com/andrew/soft.html) to com-
pute minimum cuts. The resultant test-set classifi-
cation accuracies are presented in Figure 1.
We see that Scale all up performs worse
than TPL, but the more proactive heuristics
(SetTo, IncBy) almost always outperform TPL on
segment-based classification, sometimes substan-
tially so, and outperform TPL on speaker-based
classification for half of the variations. We there-
fore conclude that label disagreement informa-
tion is indeed valuable; and that incorporating la-
bel disagreement information on top of the (posi-
tive) label agreement information that TPL lever-
aged can be achieved using simple heuristics; and
that good performance enhancements result with-
out any concomitant significant loss of efficiency.
These results are preliminary, and the diver-
gence in behaviors between different heuristics
in different settings requires investigation. Ad-
ditional future work includes investigating more
sophisticated (but often therefore less tractable)
formalisms for joint classification; and looking
at whether approximation algorithms for finding
minimum cuts in graphs with negative edge capac-
ities can be effective.
Acknowledgments We thank Jon Kleinberg and the
reviewers for helpful comments. Portions of this work
were done while the first author was visiting Cornell Uni-
versity. This paper is based upon work supported in part
by the National Science Foundation under grant nos. IIS-
0329064, BCS-0624277, and IIS-0535099, a Cornell Univer-
sity Provost?s Award for Distinguished Scholarship, a Yahoo!
Research Alliance gift, an Alfred P. Sloan Research Fellow-
ship, and by DHS grant N0014-07-1-0152. Any opinions,
findings, and conclusions or recommendations expressed are
those of the authors and do not necessarily reflect the views
or official policies, either expressed or implied, of any spon-
soring institutions, the U.S. government, or any other entity.
References
A. Agarwal, P. Bhattacharyya. 2005. Sentiment analysis: A
new approach for effective use of linguistic knowledge and
exploiting similarities in a set of documents to be classi-
fied. ICON.
R. Barzilay, M. Lapata. 2005. Collective content selection for
concept-to-text generation. HLT/EMNLP, pp. 331?338.
J. Kleinberg, ?E. Tardos. 2006. Algorithm Design. Addison
Wesley.
S. T. McCormick, M. R. Rao, G. Rinaldi. 2003. Easy and dif-
ficult objective functions for max cut. Mathematical Pro-
gramming, Series B(94):459?466.
B. Pang, L. Lee. 2004. A sentimental education: Sentiment
analysis using subjectivity summarization based on mini-
mum cuts. ACL, pp. 271?278.
M. Thomas, B. Pang, L. Lee. 2006. Get out the vote: De-
termining support or opposition from Congressional floor-
debate transcripts. EMNLP, pp. 327?335.
18
Coling 2008: Companion volume ? Posters and Demonstrations, pages 75?78
Manchester, August 2008
Using very simple statistics for review search: An exploration
Bo Pang
Yahoo! Research
bopang@yahoo-inc.com
Lillian Lee
Computer Science Department, Cornell University
llee@cs.cornell.edu
Abstract
We report on work in progress on using
very simple statistics in an unsupervised
fashion to re-rank search engine results
when review-oriented queries are issued;
the goal is to bring opinionated or subjec-
tive results to the top of the results list. We
find that our proposed technique performs
comparably to methods that rely on sophis-
ticated pre-encoded linguistic knowledge,
and that both substantially improve the ini-
tial results produced by the Yahoo! search
engine.
1 Introduction
One important information need shared by many
people is to find out about opinions and perspec-
tives on a particular topic (Mishne and de Rijke,
2006; Pang and Lee, 2008). In fact, locating rel-
evant subjective texts was a core task in the 2006
and 2007 TREC Blog tracks (Ounis et al, 2006;
Ounis et al, 2008). Most participants considered a
two-phase re-ranking approach, where first topic-
based relevancy search was employed, and then
some sort of filtering for subjectivity was applied;
these filters were based on trained classifiers or
subjectivity lexicons.
We propose an alternative approach to review
search, one that is unsupervised and that does
not rely on pre-existing dictionaries. Rather, it
in essence simply re-ranks the top k topic-based
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
search results by placing those that have the least
idiosyncratic term distributions, with respect to the
statistics of the top k results, at the head of the list.
The fact that it is the least, not the most, rare terms
with respect to the search results that are most in-
dicative of subjectivity may at first seem rather
counterintuitive; indeed, previous work has found
rare terms to be important subjectivity cues (Wiebe
et al, 2004). However, reviews within a given set
of search results may tend to resemble each other
because they tend to all discuss salient attributes of
the topic in question.
2 Algorithm
Define a search set as the top n webpages returned
in response to a review- or opinion-oriented query
by a high-quality initial search engine, in our case,
the top 20 returned by Yahoo!. As a question of
both pragmatic and scientific value, we consider
how much information can be gleaned simply from
the items in the search set itself; in particular, we
ask whether the subjective texts in the search set
can be ranked above the objective ones solely from
examination of the patterns of term occurrences
across the search-set documents.
The idea we pursue is based in part on the as-
sumption that the initial search engine is of rel-
atively high quality, so that many of the search-
set documents probably are, in fact, subjective.
Therefore, re-ordering the top-ranked documents
by how much they resemble the other search-set
documents in aggregate may be a good way to
identify the reviews. Indeed, perhaps the reviews
will be similar to one another because they all tend
to discuss salient features of the topic in question.
75
Suppose we have defined a search-set rarity
function Rarity
ss
(t) (see Section 2.1 below) that
varies inversely with the number of documents in
the search set that contain the term t. Then, we
define the idiosyncrasy score of a document d as
the average search-set rarity of the most common
terms it contains:
I(d, k) =
1
k
?
t?k-commonest-terms(d)
Rarity
ss
(t) ,
(1)
where k-commonest-terms(d) is the k common-
est terms in the search set that also occur in d. For
example, when we set k to be the size of the vo-
cabulary of d, the idiosyncrasy score is the aver-
age search-set rarity of all the terms d contains.
Then, to instantiate the similarity intuition outlined
above, we simply rank by decreasing idiosyncrasy.
The reason we look at just the top most com-
mon terms is that the rarer terms might be noise.
For example, terms that occur in just a few of
the search-set documents might represent page- or
site-specific information that is irrelevant to the
query; but the presence of such terms does not nec-
essarily indicate that the document in question is
objective.
One potential problem with the approach out-
lined above is the presence of stopwords, since
all documents, subjective or objective, can be ex-
pected to contain many of them. Therefore, stop-
word removal is indicated as an important pre-
processing step. As it turns out, the commonly-
used InQuery stopword list (Allan et al, 2000)
contains terms like ?less? that, while uninforma-
tive for topic-based information retrieval, may be
important for subjectivity detection. Therefore, we
used a 102-item list1 based solely on frequencies in
the British National Corpus.
2.1 Defining search-set rarity
There are various ways to define a search-set rar-
ity function on terms. Inspired by the efficacy of
the inverse document frequency (IDF) in informa-
tion retrieval, we consider several definitions for
Rarity
ss
(t). Let n
ss
(t) be the number of docu-
ments in the search set (not the entire corpus) that
contain the term t. Due to space constraints, we
1www.eecs.umich.edu/?qstout/586/bncfreq.html
only report results for:
Rarity
ss
(t)
def
=
1
n
ss
(t)
,
which is linearly increasing in 1/n
ss
(t), (as befits
a measure of ?idiosyncrasy?). The other defini-
tions we considered were logarithmic or polyno-
mial in 1/n
ss
(t), and performed similarly to the
linear function.
2.2 Comparison algorithms
OpinionFinder is a state-of-the-art publicly avail-
able software package for sentiment analysis that
can be applied to determining sentence-level sub-
jectivity (Riloff and Wiebe, 2003; Wiebe and
Riloff, 2005). It employs a number of
pre-processing steps, including sentence splitting,
part-of-speech tagging, stemming, and shallow
parsing. Shallow parsing is needed to identify the
extraction patterns that the sentence classifiers in-
corporate.
We used OpinionFinder?s sentence-level output2
to perform document-level subjectivity re-ranking
as follows. The result of running OpinionFinder?s
sentence classifier is that each valid sentence3 is
annotated with one of three labels: ?subj?, ?obj?,
or ?unknown?. First, discard the sentences labeled
?unknown?. Then, rank the documents by de-
creasing percentage of subjective sentences among
those sentences that are left. In the case of ties, we
use the ranking produced by the initial search en-
gine.
We also considered a more lightweight way
to incorporate linguistic knowledge: score each
document according the percentage of adjectives
within the set of tokens it contains. The motiva-
tion is previous work suggesting that the presence
of adjectives is a strong indicator of the subjectiv-
ity of the enclosing sentence (Hatzivassiloglou and
Wiebe, 2000; Wiebe et al, 2004).
2There are actually two versions. We used the accuracy-
optimized version, as it outperformed the precision-optimized
version.
3OpinionFinder will only process documents in which all
strings identified as sentences by the system contain fewer
than 1000 words. For the 31 documents in our dataset that
failed this criterion, we set their score to 0.
76
p@1 p@2 p@3 p@4 p@5 p@10 p@S MAP
Search-engine baseline .536 .543 .541 .554 .554 .528 .538 .612
OpinionFinder (accuracy version) .754 .717 .729 .725 .733 .675 .690 .768
% of adjectives (type-based) .710 .703 .696 .681 .678 .625 .633 .715
idiosyncrasy(linear), k = 50 .797 .783 .739 .717 .696 .613 .640 .729
idiosyncrasy(linear), k = 100 .754 .783 .768 .739 .716 .630 .665 .743
idiosyncrasy(linear), k = 200 .768 .761 .744 .746 .716 .623 .653 .731
idiosyncrasy(linear), k = 300 .754 .761 .749 .736 .704 .614 .641 .724
Table 1: Average search-set subjective-document precision results. ?S?: number of subjective docu-
ments. Bold and underlining: best and second-best performance per column, respectively.
3 Evaluation
Our focus is on the quality of the documents placed
at the very top ranks, since users often look only
at the first page or first half of the first page of
results (Joachims et al, 2005). Hence, we report
the precision of the top 1-5 and 10 documents, as
well as precision at the number of subjective doc-
uments and mean average precision (MAP) for the
subjective documents. All performance numbers
are averages over the 69 search sets in our data,
described next.
Data Here, we sketch the data acquisition and
labeling process. In order to get real user queries
targeted at reviews, we began with a randomly se-
lected set of queries containing the word ?review?
or ?reviews?4 from the the query log available at
http://www.sigkdd.org/kdd2005/kddcup/
KDDCUPData.zip . We created a search set for
each query by taking the top 20 webpages returned
by the Yahoo! search engine and applying some
postprocessing. Over a dozen volunteer annotators
then labeled the documents as to whether they
were subjective or objective according to a set
of detailed instructions. The end result was
over 1300 hand-labeled documents distributed
across 69 search sets, varying widely with re-
spect to query topic. Our dataset download site
is http://www.cs.cornell.edu/home/llee/
data/search-subj.html .
For almost every annotator, at least two of his
or her search sets were labeled by another person
as well, so that we could measure pair-wise agree-
4Subsequent manual filtering discarded some non-
opinion-oriented queries, such as ?alternative medicine re-
view volume5 numer1 pages 28 38 2000?.
ment with respect to multiple queries. On average,
there was agreement on 88.2% of the documents
per search set, with the average Kappa coefficient
(?) being an acceptable 0.73, reflecting in part the
difficulty of the judgment.5 The lowest ? occurs
on a search set with a 75% agreement rate.
Results A natural and key baseline is the ranking
provided by the Yahoo! search engine, which is a
high-quality, industrial-strength system. We con-
sider this to be a crucial point of comparison. The
results are shown in the top line in Table 1.
OpinionFinder clearly outperforms the initial
search engine by a substantial margin, indicating
that there are ample textual cues that can help
achieve better subjectivity re-ranking.
The adjective-percentage baseline is also far su-
perior to that of the search-engine baseline at all
ranks, but does not quite match OpinionFinder.
(Note that to achieve these results, we first dis-
carded all terms contained in three or fewer of the
search-set documents, since including such terms
decreased performance.) Still, it is interesting to
see that it appears that a good proportion of the
improvements provided by OpinionFinder can be
achieved using just adjective counts alone.
We now turn to subjectivity re-ranking based on
term-distribution (idiosyncrasy) information. For
5One source of disagreement that stems from the specifics
of our design is that we instructed annotators to mark ?sales
pitch? documents as non-reviews, on the premise that al-
though such texts are subjective, they are not valuable to a
user searching for unbiased reviews. (Note that this pol-
icy presumably makes the dataset more challenging for au-
tomated algorithms.) There are several cases where only one
annotator identified this type of bias, which is not surprising
since the authors of sales pitches may actively try to fool read-
ers into believing the text to be unbiased.
77
consistency with the adjective-based method just
described, we first discarded all terms contained in
three or fewer of the search-set documents.
As shown in Table 1, the idiosyncrasy-based al-
gorithm posts results that are overall strongly su-
perior to those of the initial, high-quality search
engine algorithm and also generally better than the
adjective-percentage algorithm. Note that these
phenomena hold for a range of values of k. The
overall performance is also on par with Opin-
ionFinder; for instance, according to the paired
t-test, the only statistically significant perfor-
mance difference (.05 level) between the accuracy-
emphasizing version of OpinionFinder and the
idiosyncrasy-based algorithm for k = 100 is for
precision at 10. In some sense, this is a striking re-
sult: just looking at within-search-set frequencies
yields performance comparable to that of a method
that utilizes rich linguistic knowledge and external
resources regarding subjectivity indicators.
Another interesting observation is that term-
distribution information seems to be more effective
for achieving high precision at the very top ranks
(precision at 1, 2, 3, and 4), whereas in contrast,
relatively deep NLP seems to be more effective at
achieving high precision at the ?lower? top ranks,
as demonstrated by the results for precision at 5,
10, and the number of subjective documents, and
for MAP. These results suggest that a combination
of the two methods could produce even greater im-
provements.
4 Concluding remarks
We considered the task of document-level sub-
jectivity re-ranking of search sets, a task mod-
eling a scenario in which a search engine is
queried to find reviews. We found that our pro-
posed term-distributional, idiosyncrasy-based al-
gorithm yielded the best precision for the very top
ranks, whereas the more linguistically-oriented,
knowledge-rich approach exemplified by Opinion-
Finder gave the best results for precision at lower
ranks. It therefore seems that both types of infor-
mation can be very valuable for the subjectivity
re-ranking task, since they have somewhat com-
plementary performance behaviors and both out-
perform the initial search engine and an adjective-
based approach.
Our motivation that within a search set, reviews
tend to resemble one another rather than differ
is reminiscent of intuitions underlying the use of
pseudo relevance feedback (PF) in IR (Ruthven
and Lalmas, 2003, Section 3.5). Future work in-
cludes comparison against PF methods and inves-
tigation of ways to select the value of k.
Acknowledgments We thank Eli Barzilay, Rich Caru-
ana, Thorsten Joachims, Jon Kleinberg, Ravi Kumar, and the
reviewers for their very useful help. We are also very grateful
to our annotators, Mohit Bansal, Eric Breck, Yejin Choi, Matt
Connelly, Tom Finley, Effi Georgala, Asif-ul Haque, Kersing
Huang, Evie Kleinberg, Art Munson, Ben Pu, Ari Rabkin,
Benyah Shaparenko, Ves Stoyanov, and Yisong Yue. This
paper is based upon work supported in part by the NSF un-
der grant no. IIS-0329064, a Yahoo! Research Alliance gift,
Google Anita Borg Memorial Scholarship funds, a Cornell
Provost?s Award for Distinguished Research, and an Alfred
P. Sloan Research Fellowship. Any opinions, findings, and
conclusions or recommendations expressed are those of the
authors and do not necessarily reflect the views or official
policies, either expressed or implied, of any sponsoring in-
stitutions, the U.S. government, or any other entity.
References
Allan, James, Margaret E. Connell, W. Bruce Croft, Fang-
Fang Feng, David Fisher, and Xiaoyan Li. 2000. IN-
QUERY and TREC-9. In Proceedings of TREC, pages
551?562. NIST Special Publication 500-249.
Hatzivassiloglou, Vasileios and Janyce Wiebe. 2000. Effects
of adjective orientation and gradability on sentence subjec-
tivity. In Proceedings of COLING.
Joachims, Thorsten, Laura Granka, Bing Pan, Helene Hem-
brooke, and Geri Gay. 2005. Accurately interpreting
clickthrough data as implicit feedback. In Proceedings of
SIGIR, pages 154?161.
Mishne, Gilad and Maarten de Rijke. 2006. A study of blog
search. In Proceedings of ECIR.
Ounis, Iadh, Maarten de Rijke, Craig Macdonald, Gilad
Mishne, and Ian Soboroff. 2006. Overview of the TREC-
2006 Blog Track. In Proceedings of TREC.
Ounis, Iadh, Craig Macdonald, and Ian Soboroff. 2008. On
the TREC Blog Track. In Proceedings of ICWSM.
Pang, Bo and Lillian Lee. 2008. Opinion Mining and Sen-
timent Analysis. Foundations and Trends in Information
Retrieval series. Now publishers.
Riloff, Ellen and Janyce Wiebe. 2003. Learning extrac-
tion patterns for subjective expressions. In Proceedings
of EMNLP.
Ruthven, Ian and Mounia Lalmas. 2003. A survey on the
use of relevance feedback for information access systems.
Knowledge Engineering Review, 18(2):95?145.
Wiebe, Janyce M. and Ellen Riloff. 2005. Creating subjective
and objective sentence classifiers from unannotated texts.
In Proceedings of CICLing, number 3406 in LNCS, pages
486?497.
Wiebe, Janyce M., Theresa Wilson, Rebecca Bruce, Matthew
Bell, and Melanie Martin. 2004. Learning subjec-
tive language. Computational Linguistics, 30(3):277?308,
September.
78
Learning to Paraphrase: An Unsupervised Approach Using
Multiple-Sequence Alignment
Regina Barzilay and Lillian Lee
Department of Computer Science
Cornell University
Ithaca, NY 14853-7501
 
regina,llee  @cs.cornell.edu
Abstract
We address the text-to-text generation prob-
lem of sentence-level paraphrasing ? a phe-
nomenon distinct from and more difficult than
word- or phrase-level paraphrasing. Our ap-
proach applies multiple-sequence alignment to
sentences gathered from unannotated compara-
ble corpora: it learns a set of paraphrasing pat-
terns represented by word lattice pairs and au-
tomatically determines how to apply these pat-
terns to rewrite new sentences. The results of
our evaluation experiments show that the sys-
tem derives accurate paraphrases, outperform-
ing baseline systems.
1 Introduction
This is a late parrot! It?s a stiff! Bereft of life,
it rests in peace! If you hadn?t nailed him to
the perch he would be pushing up the daisies!
Its metabolical processes are of interest only to
historians! It?s hopped the twig! It?s shuffled
off this mortal coil! It?s rung down the curtain
and joined the choir invisible! This is an EX-
PARROT! ? Monty Python, ?Pet Shop?
A mechanism for automatically generating multiple
paraphrases of a given sentence would be of signif-
icant practical import for text-to-text generation sys-
tems. Applications include summarization (Knight and
Marcu, 2000) and rewriting (Chandrasekar and Banga-
lore, 1997): both could employ such a mechanism to
produce candidate sentence paraphrases that other system
components would filter for length, sophistication level,
and so forth.1 Not surprisingly, therefore, paraphrasing
has been a focus of generation research for quite some
1Another interesting application, somewhat tangential to
generation, would be to expand existing corpora by providing
time (McKeown, 1979; Meteer and Shaked, 1988; Dras,
1999).
One might initially suppose that sentence-level para-
phrasing is simply the result of word-for-word or phrase-
by-phrase substitution applied in a domain- and context-
independent fashion. However, in studies of para-
phrases across several domains (Iordanskaja et al, 1991;
Robin, 1994; McKeown et al, 1994), this was gen-
erally not the case. For instance, consider the fol-
lowing two sentences (similar to examples found in
Smadja and McKeown (1991)):
After the latest Fed rate cut, stocks rose across the board.
Winners strongly outpaced losers after Greenspan cut in-
terest rates again.
Observe that ?Fed? (Federal Reserve) and ?Greenspan?
are interchangeable only in the domain of US financial
matters. Also, note that one cannot draw one-to-one cor-
respondences between single words or phrases. For in-
stance, nothing in the second sentence is really equiva-
lent to ?across the board?; we can only say that the en-
tire clauses ?stocks rose across the board? and ?winners
strongly outpaced losers? are paraphrases. This evidence
suggests two consequences: (1) we cannot rely solely on
generic domain-independent lexical resources for the task
of paraphrasing, and (2) sentence-level paraphrasing is an
important problem extending beyond that of paraphrasing
smaller lexical units.
Our work presents a novel knowledge-lean algorithm
that uses multiple-sequence alignment (MSA) to learn
to generate sentence-level paraphrases essentially from
unannotated corpus data alone. In contrast to previ-
ous work using MSA for generation (Barzilay and Lee,
several versions of their component sentences. This could, for
example, aid machine-translation evaluation, where it has be-
come common to evaluate systems by comparing their output
against a bank of several reference translations for the same sen-
tences (Papineni et al, 2002). See Bangalore et al (2002) and
Barzilay and Lee (2002) for other uses of such data.
                                                               Edmonton, May-June 2003
                                                               Main Papers , pp. 16-23
                                                         Proceedings of HLT-NAACL 2003
2002), we need neither parallel data nor explicit infor-
mation about sentence semantics. Rather, we use two
comparable corpora, in our case, collections of articles
produced by two different newswire agencies about the
same events. The use of related corpora is key: we can
capture paraphrases that on the surface bear little resem-
blance but that, by the nature of the data, must be descrip-
tions of the same information. Note that we also acquire
paraphrases from each of the individual corpora; but the
lack of clues as to sentence equivalence in single corpora
means that we must be more conservative, only selecting
as paraphrases items that are structurally very similar.
Our approach has three main steps. First, working on
each of the comparable corpora separately, we compute
lattices ? compact graph-based representations ? to
find commonalities within (automatically derived) groups
of structurally similar sentences. Next, we identify pairs
of lattices from the two different corpora that are para-
phrases of each other; the identification process checks
whether the lattices take similar arguments. Finally, given
an input sentence to be paraphrased, we match it to a lat-
tice and use a paraphrase from the matched lattice?s mate
to generate an output sentence. The key features of this
approach are:
Focus on paraphrase generation. In contrast to earlier
work, we not only extract paraphrasing rules, but also au-
tomatically determine which of the potentially relevant
rules to apply to an input sentence and produce a revised
form using them.
Flexible paraphrase types. Previous approaches to
paraphrase acquisition focused on certain rigid types of
paraphrases, for instance, limiting the number of argu-
ments. In contrast, our method is not limited to a set of a
priori-specified paraphrase types.
Use of comparable corpora and minimal use of knowl-
edge resources. In addition to the advantages mentioned
above, comparable corpora can be easily obtained for
many domains, whereas previous approaches to para-
phrase acquisition (and the related problem of phrase-
based machine translation (Wang, 1998; Och et al, 1999;
Vogel and Ney, 2000)) required parallel corpora. We
point out that one such approach, recently proposed by
Pang et al (2003), also represents paraphrases by lat-
tices, similarly to our method, although their lattices are
derived using parse information.
Moreover, our algorithm does not employ knowledge
resources such as parsers or lexical databases, which may
not be available or appropriate for all domains ? a key
issue since paraphrasing is typically domain-dependent.
Nonetheless, our algorithm achieves good performance.
2 Related work
Previous work on automated paraphrasing has con-
sidered different levels of paraphrase granularity.
Learning synonyms via distributional similarity has
been well-studied (Pereira et al, 1993; Grefen-
stette, 1994; Lin, 1998). Jacquemin (1999) and
Barzilay and McKeown (2001) identify phrase-
level paraphrases, while Lin and Pantel (2001) and
Shinyama et al (2002) acquire structural paraphrases
encoded as templates. These latter are the most closely
related to the sentence-level paraphrases we desire,
and so we focus in this section on template-induction
approaches.
Lin and Pantel (2001) extract inference rules, which
are related to paraphrases (for example, X wrote Y im-
plies X is the author of Y), to improve question an-
swering. They assume that paths in dependency trees
that take similar arguments (leaves) are close in mean-
ing. However, only two-argument templates are consid-
ered. Shinyama et al (2002) also use dependency-tree
information to extract templates of a limited form (in their
case, determined by the underlying information extrac-
tion application). Like us (and unlike Lin and Pantel, who
employ a single large corpus), they use articles written
about the same event in different newspapers as data.
Our approach shares two characteristics with the two
methods just described: pattern comparison by analysis
of the patterns? respective arguments, and use of non-
parallel corpora as a data source. However, extraction
methods are not easily extended to generation methods.
One problem is that their templates often only match
small fragments of a sentence. While this is appropriate
for other applications, deciding whether to use a given
template to generate a paraphrase requires information
about the surrounding context provided by the entire sen-
tence.
3 Algorithm
Overview We first sketch the algorithm?s broad out-
lines. The subsequent subsections provide more detailed
descriptions of the individual steps.
The major goals of our algorithm are to learn:
  recurring patterns in the data, such as X (in-
jured/wounded) Y people, Z seriously, where the
capital letters represent variables;
  pairings between such patterns that represent para-
phrases, for example, between the pattern X (in-
jured/wounded) Y people, Z of them seriously
and the pattern Y were (wounded/hurt) by X,
among them Z were in serious condition.
Figure 1 illustrates the main stages of our approach.
During training, pattern induction is first applied inde-
pendently to the two datasets making up a pair of compa-
rable corpora. Individual patterns are learned by applying
paraphrase
corpus 1 corpus 2
lattice 1
lattice 1
lattice 2
lattice 3
lattice a
lattice b
lattice c
lattice b
new sentence
Training
Figure 1: System architecture.
(1) A Palestinian suicide bomber blew himself up in
a southern city Wednesday, killing two other people and
wounding 27.
(2) A suicide bomber blew himself up in the settlement of
Efrat, on Sunday, killing himself and injuring seven peo-
ple.
(3) A suicide bomber blew himself up in the coastal re-
sort of Netanya on Monday, killing three other people and
wounding dozens more.
(4) A Palestinian suicide bomber blew himself up in a gar-
den cafe on Saturday, killing 10 people and wounding 54.
(5) A suicide bomber blew himself up in the centre of Ne-
tanya on Sunday, killing three people as well as himself and
injuring 40.
Figure 2: Five sentences (without date, number, and
name substitution) from a cluster of 49, similarities em-
phasized.
multiple-sequence alignment to clusters of sentences de-
scribing approximately similar events; these patterns are
represented compactly by lattices (see Figure 3). We then
check for lattices from the two different corpora that tend
to take the same arguments; these lattice pairs are taken
to be paraphrase patterns.
Once training is done, we can generate paraphrases as
follows: given the sentence ?The surprise bombing in-
jured twenty people, five of them seriously?, we match
it to the lattice X (injured/wounded) Y people, Z
of them seriously which can be rewritten as Y were
(wounded/hurt) by X, among them Z were in serious
condition, and so by substituting arguments we can gen-
erate ?Twenty were wounded by the surprise bombing,
among them five were in serious condition? or ?Twenty
were hurt by the surprise bombing, among them five were
in serious condition?.
3.1 Sentence clustering
Our first step is to cluster sentences into groups from
which to learn useful patterns; for the multiple-sequence
techniques we will use, this means that the sentences
within clusters should describe similar events and have
similar structure, as in the sentences of Figure 2. This
is accomplished by applying hierarchical complete-link
clustering to the sentences using a similarity metric based
on word n-gram overlap (   
	 ). The only sub-
tlety is that we do not want mismatches on sentence de-
tails (e.g., the location of a raid) causing sentences de-
scribing the same type of occurrence (e.g., a raid) from
being separated, as this might yield clusters too frag-
mented for effective learning to take place. (Moreover,
variability in the arguments of the sentences in a cluster
is needed for our learning algorithm to succeed; see be-
low.) We therefore first replace all appearances of dates,
numbers, and proper names2 with generic tokens. Clus-
ters with fewer than ten sentences are discarded.
3.2 Inducing patterns
In order to learn patterns, we first compute a multiple-
sequence alignment (MSA) of the sentences in a given
cluster. Pairwise MSA takes two sentences and a scor-
ing function giving the similarity between words; it de-
termines the highest-scoring way to perform insertions,
deletions, and changes to transform one of the sentences
into the other. Pairwise MSA can be extended efficiently
to multiple sequences via the iterative pairwise align-
ment, a polynomial-time method commonly used in com-
putational biology (Durbin et al, 1998).3 The results
can be represented in an intuitive form via a word lat-
tice (see Figure 3), which compactly represents (n-gram)
structural similarities between the cluster?s sentences.
To transform lattices into generation-suitable patterns
requires some understanding of the possible varieties of
lattice structures. The most important part of the transfor-
mation is to determine which words are actually instances
of arguments, and so should be replaced by slots (repre-
senting variables). The key intuition is that because the
sentences in the cluster represent the same type of event,
such as a bombing, but generally refer to different in-
stances of said event (e.g. a bombing in Jerusalem versus
in Gaza), areas of large variability in the lattice should
correspond to arguments.
To quantify this notion of variability, we first formal-
ize its opposite: commonality. We define backbone nodes
2Our crude proper-name identification method was to flag
every phrase (extracted by a noun-phrase chunker) appearing
capitalized in a non-sentence-initial position sufficiently often.
3Scoring function: aligning two identical words scores
1; inserting a word scores -0.01, and aligning two dif-
ferent words scores -0.5 (parameter values taken from
Barzilay and Lee (2002)).
start
Palestinian
suicide bomber blew himself up in SLOT1 on SLOT2 killing SLOT3
other people and
wounding
injuring SLOT4 end
start
Palestinian
suicide bomber blew himself up in
centre
southern
settlement
coastal
garden
of
city
DATENAME on
resort
cafe
killing NUM
himself
other people and
as well as
wounding
injuring
NUM
people
more
end
Figure 3: Lattice and slotted lattice for the five sentences from Figure 2. Punctuation and articles removed for clarity.
as those shared by more than 50% of the cluster?s sen-
tences. The choice of 50% is not arbitrary ? it can
be proved using the pigeonhole principle that our strict-
majority criterion imposes a unique linear ordering of the
backbone nodes that respects the word ordering within
the sentences, thus guaranteeing at least a degree of well-
formedness and avoiding the problem of how to order
backbone nodes occurring on parallel ?branches? of the
lattice.
Once we have identified the backbone nodes as points
of strong commonality, the next step is to identify the re-
gions of variability (or, in lattice terms, many parallel dis-
joint paths) between them as (probably) corresponding to
the arguments of the propositions that the sentences rep-
resent. For example, in the top of Figure 3, the words
?southern city, ?settlement of NAME?,?coastal resort of
NAME?, etc. all correspond to the location of an event
and could be replaced by a single slot. Figure 3 shows
an example of a lattice and the derived slotted lattice; we
give the details of the slot-induction process in the Ap-
pendix.
3.3 Matching lattices
Now, if we were using a parallel corpus, we could em-
ploy sentence-alignment information to determine which
lattices correspond to paraphrases. Since we do not have
this information, we essentially approximate the parallel-
corpus situation by correlating information from descrip-
tions of (what we hope are) the same event occurring in
the two different corpora.
Our method works as follows. Once lattices for each
corpus in our comparable-corpus pair are computed, we
identify lattice paraphrase pairs, using the idea that para-
phrases will tend to take the same values as arguments
(Shinyama et al, 2002; Lin and Pantel, 2001). More
specifically, we take a pair of lattices from different cor-
pora, look back at the sentence clusters from which the
two lattices were derived, and compare the slot values
of those cross-corpus sentence pairs that appear in arti-
cles written on the same day on the same topic; we pair
the lattices if the degree of matching is over a threshold
tuned on held-out data. For example, suppose we have
two (linearized) lattices slot1 bombed slot2 and slot3
was bombed by slot4 drawn from different corpora.
If in the first lattice?s sentence cluster we have the sen-
tence ?the plane bombed the town?, and in the second lat-
tice?s sentence cluster we have a sentence written on the
same day reading ?the town was bombed by the plane?,
then the corresponding lattices may well be paraphrases,
where slot1 is identified with slot4 and slot2 with slot3.
To compare the set of argument values of two lattices,
we simply count their word overlap, giving double weight
to proper names and numbers and discarding auxiliaries
(we purposely ignore order because paraphrases can con-
sist of word re-orderings).
3.4 Generating paraphrase sentences
Given a sentence to paraphrase, we first need to iden-
tify which, if any, of our previously-computed sentence
clusters the new sentence belongs most strongly to. We
do this by finding the best alignment of the sentence to
the existing lattices.4 If a matching lattice is found, we
choose one of its comparable-corpus paraphrase lattices
to rewrite the sentence, substituting in the argument val-
ues of the original sentence. This yields as many para-
phrases as there are lattice paths.
4 Evaluation
All evaluations involved judgments by native speakers of
English who were not familiar with the paraphrasing sys-
tems under consideration.
We implemented our system on a pair of comparable
corpora consisting of articles produced between Septem-
ber 2000 and August 2002 by the Agence France-Presse
(AFP) and Reuters news agencies. Given our interest in
domain-dependent paraphrasing, we limited attention to
9MB of articles, collected using a TDT-style document
clustering system, concerning individual acts of violence
in Israel and army raids on the Palestinian territories.
From this data (after removing 120 articles as a held-
4To facilitate this process, we add ?insert? nodes between
backbone nodes; these nodes can match any word sequence and
thus account for new words in the input sentence. Then, we per-
form multiple-sequence alignment where insertions score -0.1
and all other node alignments receive a score of unity.
=76%
=74%
=68%
=96%
(cf. 74%)
(cf. 79%)
(cf. 77%)
(cf. 98%)
=32%
=40%
=42%
=56%
(cf. 31%)
(cf. 43%)
(cf. 37%)
(cf. 61%)
% of common instances deemed valid
% of common + individual instances
deemed valid
judge 1
judge 2
judge 3
judge 4
judge 1
judge 2
judge 3
judge 4
DIRT (Lin and Pantel)
"X1 forced out of X2"; "X1 ousted from X2"
MSA
The 50 common template pairs, sorted by judges? perceived validity
 "palestinian suicide bomber blew himself up at X1 in X2 DATE, killing NUM1 and wounding NUM2  police said."
"DATE: NUM1 are killed and around NUM2 injured when suicide bomber blows up his explosive?packed belt at X1 in X2.";
"X1 stormed into X2"; "X1 thrusted into X2"
"palestinian suicide bomber blew himself up at X1 in X2 on DATE, killing NUM1 and wounding NUM2, police said."
"? DATE: bombing at X1 in X2 kills NUM1 israelis and leaves NUM2.";
"X1?s candidacy for X2"; "X2 expressed X1?s condemnation"
The 50 common template pairs, sorted by judges? perceived validity
"latest violence bring to NUM1 number of people killed as a direct result of the palestinian [sic], including NUM2 palestinians and NUM3 israelis."; 
 "At least NUM1 palestinians and NUM2 israelis have been killed since palestinian uprising against israeli occupation began in September 2000 after peace talks stalled."
Figure 4: Correctness and agreement results. Columns = instances; each grey box represents a judgment of ?valid?
for the instance. For each method, a good, middling, and poor instance is shown. (Results separated by algorithm for
clarity; the blind evaluation presented instances from the two algorithms in random order.)
out parameter-training set), we extracted 43 slotted lat-
tices from the AFP corpus and 32 slotted lattices from
the Reuters corpus, and found 25 cross-corpus matching
pairs; since lattices contain multiple paths, these yielded
6,534 template pairs.5
4.1 Template Quality Evaluation
Before evaluating the quality of the rewritings produced
by our templates and lattices, we first tested the qual-
ity of a random sample of just the template pairs. In
our instructions to the judges, we defined two text units
(such as sentences or snippets) to be paraphrases if one
of them can generally be substituted for the other without
great loss of information (but not necessarily vice versa).
6 Given a pair of templates produced by a system, the
judges marked them as paraphrases if for many instanti-
ations of the templates? variables, the resulting text units
were paraphrases. (Several labelled examples were pro-
vided to supply further guidance).
To put the evaluation results into context, we wanted
to compare against another system, but we are not aware
5The extracted paraphrases are available at
http://www.cs.cornell.edu/Info/Projects/
NLP/statpar.html
6We switched to this ?one-sided? definition because in ini-
tial tests judges found it excruciating to decide on equivalence.
Also, in applications such as summarization some information
loss is acceptable.
of any previous work creating templates precisely for
the task of generating paraphrases. Instead, we made
a good-faith effort to adapt the DIRT system (Lin and
Pantel, 2001) to the problem, selecting the 6,534 highest-
scoring templates it produced when run on our datasets.
(The system of Shinyama et al (2002) was unsuitable for
evaluation purposes because their paraphrase extraction
component is too tightly coupled to the underlying in-
formation extraction system.) It is important to note
some important caveats in making this comparison, the
most prominent being that DIRT was not designed with
sentence-paraphrase generation in mind ? its templates
are much shorter than ours, which may have affected
the evaluators? judgments ? and was originally imple-
mented on much larger data sets.7 The point of this eval-
uation is simply to determine whether another corpus-
based paraphrase-focused approach could easily achieve
the same performance level.
In brief, the DIRT system works as follows. Depen-
dency trees are constructed from parsing a large corpus.
Leaf-to-leaf paths are extracted from these dependency
7To cope with the corpus-size issue, DIRT was trained on
an 84MB corpus of Middle-East news articles, a strict superset
of the 9MB we used. Other issues include the fact that DIRT?s
output needed to be converted into English: it produces paths
like ?N:of:N   tide  N:nn:N?, which we transformed into ?Y tide
of X? so that its output format would be the same as ours.
trees, with the leaves serving as slots. Then, pairs of
paths in which the slots tend to be filled by similar val-
ues, where the similarity measure is based on the mutual
information between the value and the slot, are deemed
to be paraphrases.
We randomly extracted 500 pairs from the two algo-
rithms? output sets. Of these, 100 paraphrases (50 per
system) made up a ?common? set evaluated by all four
judges, allowing us to compute agreement rates; in addi-
tion, each judge also evaluated another ?individual? set,
seen only by him- or herself, consisting of another 100
pairs (50 per system). The ?individual? sets allowed us to
broaden our sample?s coverage of the corpus.8 The pairs
were presented in random order, and the judges were not
told which system produced a given pair.
As Figure 4 shows, our system outperforms the DIRT
system, with a consistent performance gap for all the
judges of about 38%, although the absolute scores vary
(for example, Judge 4 seems lenient). The judges? as-
sessment of correctness was fairly constant between the
full 100-instance set and just the 50-instance common set
alone.
In terms of agreement, the Kappa value (measuring
pairwise agreement discounting chance occurrences9) on
the common set was 0.54, which corresponds to moder-
ate agreement (Landis and Koch, 1977). Multiway agree-
ment is depicted in Figure 4 ? there, we see that in 86
of 100 cases, at least three of the judges gave the same
correctness assessment, and in 60 cases all four judges
concurred.
4.2 Evaluation of the generated paraphrases
Finally, we evaluated the quality of the paraphrase sen-
tences generated by our system, thus (indirectly) testing
all the system components: pattern selection, paraphrase
acquisition, and generation. We are not aware of another
system generating sentence-level paraphrases. Therefore,
we used as a baseline a simple paraphrasing system that
just replaces words with one of their randomly-chosen
WordNet synonyms (using the most frequent sense of the
word that WordNet listed synonyms for). The number of
substitutions was set proportional to the number of words
our method replaced in the same sentence. The point of
this comparison is to check whether simple synonym sub-
stitution yields results comparable to those of our algo-
8Each judge took several hours at the task, making it infea-
sible to expand the sample size further.
9One issue is that the Kappa statistic doesn?t account for
varying difficulty among instances. For this reason, we actu-
ally asked judges to indicate for each instance whether making
the validity decision was difficult. However, the judges gen-
erally did not agree on difficulty. Post hoc analysis indicates
that perception of difficulty depends on each judge?s individual
?threshold of similarity?, not just the instance itself.
rithm. 10
For this experiment, we randomly selected 20 AFP ar-
ticles about violence in the Middle East published later
than the articles in our training corpus. Out of 484 sen-
tences in this set, our system was able to paraphrase 59
(12.2%). (We chose parameters that optimized precision
rather than recall on our small held-out set.) We found
that after proper name substitution, only seven sentences
in the test set appeared in the training set,11 which im-
plies that lattices boost the generalization power of our
method significantly: from seven to 59 sentences. Inter-
estingly, the coverage of the system varied significantly
with article length. For the eight articles of ten or fewer
sentences, we paraphrased 60.8% of the sentences per ar-
ticle on average, but for longer articles only 9.3% of the
sentences per article on average were paraphrased. Our
analysis revealed that long articles tend to include large
portions that are unique to the article, such as personal
stories of the event participants, which explains why our
algorithm had a lower paraphrasing rate for such articles.
All 118 instances (59 per system) were presented in
random order to two judges, who were asked to indi-
cate whether the meaning had been preserved. Of the
paraphrases generated by our system, the two evalua-
tors deemed 81.4% and 78%, respectively, to be valid,
whereas for the baseline system, the correctness results
were 69.5% and 66.1%, respectively. Agreement accord-
ing to the Kappa statistic was 0.6. Note that judging full
sentences is inherently easier than judging templates, be-
cause template comparison requires considering a variety
of possible slot values, while sentences are self-contained
units.
Figure 5 shows two example sentences, one where
our MSA-based paraphrase was deemed correct by both
judges, and one where both judges deemed the MSA-
generated paraphrase incorrect. Examination of the re-
sults indicates that the two systems make essentially or-
thogonal types of errors. The baseline system?s relatively
poor performance supports our claim that whole-sentence
paraphrasing is a hard task even when accurate word-
level paraphrases are given.
5 Conclusions
We presented an approach for generating sentence level
paraphrases, a task not addressed previously. Our method
learns structurally similar patterns of expression from
data and identifies paraphrasing pairs among them using
a comparable corpus. A flexible pattern-matching pro-
cedure allows us to paraphrase an unseen sentence by
10We chose not to employ a language model to re-rank either
system?s output because such an addition would make it hard to
isolate the contribution of the paraphrasing component itself.
11Since we are doing unsupervised paraphrase acquisition,
train-test overlap is allowed.
Original (1) The caller identified the bomber as Yussef Attala, 20, from the Balata refugee camp near Nablus.
MSA The caller named the bomber as 20-year old Yussef Attala from the Balata refugee camp near Nablus.
Baseline The company placed the bomber as Yussef Attala, 20, from the Balata refugee camp near Nablus.
Original (2) A spokesman for the group claimed responsibility for the attack in a phone call to AFP in this northern West Bank town.
MSA The attack in a phone call to AFP in this northern West Bank town was claimed by a spokesman of the group.
Baseline A spokesman for the grouping laid claim responsibility for the onslaught in a phone call to AFP in this northern West
Bank town.
Figure 5: Example sentences and generated paraphrases. Both judges felt MSA preserved the meaning of (1) but not
(2), and that neither baseline paraphrase was meaning-preserving.
matching it to one of the induced patterns. Our approach
generates both lexical and structural paraphrases.
Another contribution is the induction of MSA lat-
tices from non-parallel data. Lattices have proven ad-
vantageous in a number of NLP contexts (Mangu et
al., 2000; Bangalore et al, 2002; Barzilay and Lee,
2002; Pang et al, 2003), but were usually produced from
(multi-)parallel data, which may not be readily available
for many applications. We showed that word lattices can
be induced from a type of corpus that can be easily ob-
tained for many domains, broadening the applicability of
this useful representation.
Acknowledgments
We are grateful to many people for helping us in this work.
We thank Stuart Allen, Itai Balaban, Hubie Chen, Tom Hey-
erman, Evelyn Kleinberg, Carl Sable, and Alex Zubatov for
acting as judges. Eric Breck helped us with translating the out-
put of the DIRT system. We had numerous very useful con-
versations with all those mentioned above and with Eli Barzi-
lay, Noemie Elhadad, Jon Kleinberg (who made the ?pigeon-
hole? observation), Mirella Lapata, Smaranda Muresan and Bo
Pang. We are very grateful to Dekang Lin for providing us
with DIRT?s output. We thank the Cornell NLP group, espe-
cially Eric Breck, Claire Cardie, Amanda Holland-Minkley, and
Bo Pang, for helpful comments on previous drafts. This paper
is based upon work supported in part by the National Science
Foundation under ITR/IM grant IIS-0081334 and a Sloan Re-
search Fellowship. Any opinions, findings, and conclusions
or recommendations expressed above are those of the authors
and do not necessarily reflect the views of the National Science
Foundation or the Sloan Foundation.
References
Srinivas Bangalore, Vanessa Murdock, and Giuseppe Riccardi.
2002. Bootstrapping bilingual data using consensus transla-
tion for a multilingual instant messaging system. In Proc. of
COLING.
Regina Barzilay and Lillian Lee. 2002. Bootstrapping lex-
ical choice via multiple-sequence alignment. In Proc. of
EMNLP, pages 164?171.
Regina Barzilay and Kathleen McKeown. 2001. Extract-
ing paraphrases from a parallel corpus. In Proc. of the
ACL/EACL, pages 50?57.
Raman Chandrasekar and Srinivas Bangalore. 1997. Auto-
matic induction of rules for text simplification. Knowledge-
Based Systems, 10(3):183?190.
Mark Dras. 1999. Tree Adjoining Grammar and the Reluctant
Paraphrasing of Text. Ph.D. thesis, Macquarie University.
Richard Durbin, Sean Eddy, Anders Krogh, and Graeme
Mitchison. 1998. Biological Sequence Analysis. Cambridge
University Press, Cambridge, UK.
Gregory Grefenstette. 1994. Explorations in Automatic The-
saurus Discovery, volume 278. Kluwer.
L. Iordanskaja, R. Kittredge, and A. Polguere. 1991. Lex-
ical selection and paraphrase in a meaning-text generation
model. In C. Paris, W. Swartout, and W. Mann, editors,
Natural Language Generation in Artificial Intelligence and
Computational Linguistics, chapter 11. Kluwer.
Christian Jacquemin. 1999. Syntagmatic and paradigmatic rep-
resentations of term variations. In Proc. of the ACL, pages
341?349.
Kevin Knight and Daniel Marcu. 2000. Statistics-based sum-
marization ? Step one: Sentence compression. In Proc. of
AAAI.
J. Richard Landis and Gary G. Koch. 1977. The measure-
ment of observer agreement for categorical data. Biometrics,
33:159?174.
Dekang Lin and Patrick Pantel. 2001. Discovery of inference
rules for question-answering. Natural Language Engineer-
ing, 7(4):343?360.
Dekang Lin. 1998. Automatic retrieval and clustering of simi-
lar words. In Proc. of ACL/COLING, pages 768?774.
Lidia Mangu, Eric Brill, and Andreas Stolcke. 2000. Finding
consensus in speech recognition: Word error minimization
and other applications of confusion networks. Computer,
Speech and Language, 14(4):373?400.
Kathleen R. McKeown, Karen Kukich, and James Shaw. 1994.
Practical issues in automatic documentation generation. In
Proc. of ANLP, pages 7?14.
Kathleen R. McKeown. 1979. Paraphrasing using given and
new information in a question-answer system. In Proc. of
the ACL, pages 67?72.
Marie Meteer and Varda Shaked. 1988. Strategies for effective
paraphrasing. In Proc. of COLING, pages 431?436.
Franz Josef Och, Christoph Tillman, and Hermann Ney. 1999.
Improved alignment models for statistical machine transla-
tion. In Proc. of EMNLP, pages 20?28.
Bo Pang, Kevin Knight, and Daniel Marcu. 2003. Syntax-
based alignment of multiple translations: Extracting para-
phrases and generating new sentences. In Proceedings of
HLT/NAACL.
Kishore A. Papineni, Salim Roukos, Todd Ward, and Wei-Jing
Zhu. 2002. Bleu: A method for automatic evaluation of
machine translation. In Proc. of the ACL, pages 311?318.
Fernando Pereira, Naftali Tishby, and Lillian Lee. 1993. Dis-
tributional clustering of English words. In Proc. of the ACL,
pages 183?190.
Jacques Robin. 1994. Revision-Based Generation of Natu-
ral Language Summaries Providing Historical Background:
Corpus-Based Analysis, Design, Implementation, and Eval-
uation. Ph.D. thesis, Columbia University.
Yusuke Shinyama, Satoshi Sekine, Kiyoshi Sudo, and Ralph
Grishman. 2002. Automatic paraphrase acquisition from
news articles. In Proc. of HLT, pages 40?46.
Frank Smadja and Kathleen McKeown. 1991. Using colloca-
tions for language generation. Computational Intelligence,
7(4). Special issue on natural language generation.
Stephan Vogel and Hermann Ney. 2000. Construction of a
hierarchical translation memory. In Proc. of COLING, pages
1131?1135.
Ye-Yi Wang. 1998. Grammar Inference and Statistical Ma-
chine Translation. Ph.D. thesis, CMU.
Appendix
In this appendix, we describe how we insert slots into
lattices to form slotted lattices.
Recall that the backbone nodes in our lattices represent
words appearing in many of the sentences from which
the lattice was built. As mentioned above, the intuition
is that areas of high variability between backbone nodes
may correspond to arguments, or slots. But the key thing
to note is that there are actually two different phenomena
giving rise to multiple parallel paths: argument variabil-
ity, described above, and synonym variability. For exam-
ple, Figure 6(b) contains parallel paths corresponding to
the synonyms ?injured? and ?wounded?. Note that we
want to remove argument variability so that we can gen-
erate paraphrases of sentences with arbitrary arguments;
but we want to preserve synonym variability in order to
generate a variety of sentence rewritings.
To distinguish these two situations, we analyze the split
level of backbone nodes that begin regions with multi-
ple paths. The basic intuition is that there is probably
more variability associated with arguments than with syn-
onymy: for example, as datasets increase, the number of
locations mentioned rises faster than the number of syn-
onyms appearing. We make use of a synonymy threshold
  (set by held-out parameter-tuning to 30), as follows.
  If no more than   % of all the edges out of a back-
bone node lead to the same next node, we have high
enough variability to warrant inserting a slot node.
  Otherwise, we incorporate reliable synonyms12 into
the backbone structure by preserving all nodes that
are reached by at least   % of the sentences passing
through the two neighboring backbone nodes.
Furthermore, all backbone nodes labelled with our spe-
cial generic tokens are also replaced with slot nodes,
since they, too, probably represent arguments (we con-
dense adjacent slots into one). Nodes with in-degree
lower than the synonymy threshold are removed un-
der the assumption that they probably represent idiosyn-
crasies of individual sentences. See Figure 6 for exam-
ples.
Figure 3 shows an example of a lattice and the slotted
lattice derived via the process just described.
arrested
of the sentences lead here
only 1 out of 7 (14%)
Delete:
lead to these nodes
of the sentences
3 out of 7 (43%)
Preserve both nodes:
nearwere
(b) Synonym variability
near
(a) Argument variability
in
to the same node
Replace with a slot:
no more than 2 out of 7 (28%)
of the sentences lead
store
restaurant
grocery
station
cafe
wounded
injured
Figure 6: Simple seven-sentence examples of two types
of variability. The double-boxed nodes are backbone
nodes; edges show consecutive words in some sentence.
The synonymy threshold (set to 30% in this example) de-
termines the type of variability.
12While our original implementation, evaluated in Section 4,
identified only single-word synonyms, phrase-level synonyms
can similarly be acquired by considering chains of nodes con-
necting backbone nodes.
Catching the Drift: Probabilistic Content Models, with Applications to
Generation and Summarization
Regina Barzilay
Computer Science and AI Lab
MIT
regina@csail.mit.edu
Lillian Lee
Department of Computer Science
Cornell University
llee@cs.cornell.edu
Abstract
We consider the problem of modeling the con-
tent structure of texts within a specific do-
main, in terms of the topics the texts address
and the order in which these topics appear.
We first present an effective knowledge-lean
method for learning content models from un-
annotated documents, utilizing a novel adap-
tation of algorithms for Hidden Markov Mod-
els. We then apply our method to two com-
plementary tasks: information ordering and ex-
tractive summarization. Our experiments show
that incorporating content models in these ap-
plications yields substantial improvement over
previously-proposed methods.
1 Introduction
The development and application of computational mod-
els of text structure is a central concern in natural lan-
guage processing. Document-level analysis of text struc-
ture is an important instance of such work. Previous
research has sought to characterize texts in terms of
domain-independent rhetorical elements, such as schema
items (McKeown, 1985) or rhetorical relations (Mann
and Thompson, 1988; Marcu, 1997). The focus of our
work, however, is on an equally fundamental but domain-
dependent dimension of the structure of text: content.
Our use of the term ?content? corresponds roughly
to the notions of topic and topic change. We desire
models that can specify, for example, that articles about
earthquakes typically contain information about quake
strength, location, and casualties, and that descriptions
of casualties usually precede those of rescue efforts. But
rather than manually determine the topics for a given
domain, we take a distributional view, learning them
directly from un-annotated texts via analysis of word
distribution patterns. This idea dates back at least to
Harris (1982), who claimed that ?various types of [word]
recurrence patterns seem to characterize various types of
discourse?. Advantages of a distributional perspective in-
clude both drastic reduction in human effort and recogni-
tion of ?topics? that might not occur to a human expert
and yet, when explicitly modeled, aid in applications.
Of course, the success of the distributional approach
depends on the existence of recurrent patterns. In arbi-
trary document collections, such patterns might be too
variable to be easily detected by statistical means. How-
ever, research has shown that texts from the same domain
tend to exhibit high similarity (Wray, 2002). Cognitive
psychologists have long posited that this similarity is not
accidental, arguing that formulaic text structure facilitates
readers? comprehension and recall (Bartlett, 1932).1
In this paper, we investigate the utility of domain-
specific content models for representing topics and
topic shifts. Content models are Hidden Markov
Models (HMMs) wherein states correspond to types
of information characteristic to the domain of in-
terest (e.g., earthquake magnitude or previous earth-
quake occurrences), and state transitions capture possible
information-presentation orderings within that domain.
We first describe an efficient, knowledge-lean method
for learning both a set of topics and the relations be-
tween topics directly from un-annotated documents. Our
technique incorporates a novel adaptation of the standard
HMM induction algorithm that is tailored to the task of
modeling content.
Then, we apply techniques based on content models to
two complex text-processing tasks. First, we consider in-
formation ordering, that is, choosing a sequence in which
to present a pre-selected set of items; this is an essen-
tial step in concept-to-text generation, multi-document
summarization, and other text-synthesis problems. In our
experiments, content models outperform Lapata?s (2003)
state-of-the-art ordering method by a wide margin ? for
one domain and performance metric, the gap was 78 per-
centage points. Second, we consider extractive summa-
1But ?formulaic? is not necessarily equivalent to ?simple?,
so automated approaches still offer advantages over manual
techniques, especially if one needs to model several domains.
rization: the compression of a document by choosing
a subsequence of its sentences. For this task, we de-
velop a new content-model-based learning algorithm for
sentence selection. The resulting summaries yield 88%
match with human-written output, which compares fa-
vorably to the 69% achieved by the standard ?leading  
sentences? baseline.
The success of content models in these two comple-
mentary tasks demonstrates their flexibility and effective-
ness, and indicates that they are sufficiently expressive to
represent important text properties. These observations,
taken together with the fact that content models are con-
ceptually intuitive and efficiently learnable from raw doc-
ument collections, suggest that the formalism can prove
useful in an even broader range of applications than we
have considered here; exploring the options is an appeal-
ing line of future research.
2 Related Work
Knowledge-rich methods Models employing manual
crafting of (typically complex) representations of content
have generally captured one of three types of knowledge
(Rambow, 1990; Kittredge et al, 1991): domain knowl-
edge [e.g., that earthquakes have magnitudes], domain-
independent communication knowledge [e.g., that de-
scribing an event usually entails specifying its location];
and domain communication knowledge [e.g., that Reuters
earthquake reports often conclude by listing previous
quakes2]. Formalisms exemplifying each of these knowl-
edge types are DeJong?s (1982) scripts, McKeown?s
(1985) schemas, and Rambow?s (1990) domain-specific
schemas, respectively.
In contrast, because our models are based on a dis-
tributional view of content, they will freely incorporate
information from all three categories as long as such in-
formation is manifested as a recurrent pattern. Also, in
comparison to the formalisms mentioned above, content
models constitute a relatively impoverished representa-
tion; but this actually contributes to the ease with which
they can be learned, and our empirical results show that
they are quite effective despite their simplicity.
In recent work, Duboue and McKeown (2003) propose
a method for learning a content planner from a collec-
tion of texts together with a domain-specific knowledge
base, but our method applies to domains in which no such
knowledge base has been supplied.
Knowledge-lean approaches Distributional models of
content have appeared with some frequency in research
on text segmentation and topic-based language modeling
(Hearst, 1994; Beeferman et al, 1997; Chen et al, 1998;
Florian and Yarowsky, 1999; Gildea and Hofmann, 1999;
2This does not qualify as domain knowledge because it is
not about earthquakes per se.
Iyer and Ostendorf, 1996; Wu and Khudanpur, 2002). In
fact, the methods we employ for learning content models
are quite closely related to techniques proposed in that
literature (see Section 3 for more details).
However, language-modeling research ? whose goal
is to predict text probabilities ? tends to treat topic as a
useful auxiliary variable rather than a central concern; for
example, topic-based distributional information is gener-
ally interpolated with standard, non-topic-based   -gram
models to improve probability estimates. Our work, in
contrast, treats content as a primary entity. In particular,
our induction algorithms are designed with the explicit
goal of modeling document content, which is why they
differ from the standard Baum-Welch (or EM) algorithm
for learning Hidden Markov Models even though content
models are instances of HMMs.
3 Model Construction
We employ an iterative re-estimation procedure that al-
ternates between (1) creating clusters of text spans with
similar word distributions to serve as representatives of
within-document topics, and (2) computing models of
word distributions and topic changes from the clusters so
derived.3
Formalism preliminaries We treat texts as sequences
of pre-defined text spans, each presumed to convey infor-
mation about a single topic. Specifying text-span length
thus defines the granularity of the induced topics. For
concreteness, in what follows we will refer to ?sentences?
rather than ?text spans? since that is what we used in our
experiments, but paragraphs or clauses could potentially
have been employed instead.
Our working assumption is that all texts from a given
domain are generated by a single content model. A con-
tent model is an HMM in which each state  corresponds
to a distinct topic and generates sentences relevant to that
topic according to a state-specific language model  ?
note that standard   -gram language models can there-
fore be considered to be degenerate (single-state) content
models. State transition probabilities give the probability
of changing from a given topic to another, thereby cap-
turing constraints on topic shifts. We can use the forward
algorithm to efficiently compute the generation probabil-
ity assigned to a document by a content model and the
Viterbi algorithm to quickly find the most likely content-
model state sequence to have generated a given docu-
ment; see Rabiner (1989) for details.
In our implementation, we use bigram language mod-
els, so that the probability of an   -word sentence 

	 being generated by a state  is ffHuman Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 137?145,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Without a ?doubt??
Unsupervised discovery of downward-entailing operators
Cristian Danescu-Niculescu-Mizil, Lillian Lee, and Richard Ducott
Department of Computer Science
Cornell University
Ithaca, NY 14853-7501
cristian@cs.cornell.edu, llee@cs.cornell.edu, rad47@cornell.edu
Abstract
An important part of textual inference is mak-
ing deductions involving monotonicity, that
is, determining whether a given assertion en-
tails restrictions or relaxations of that asser-
tion. For instance, the statement ?We know the
epidemic spread quickly? does not entail ?We
know the epidemic spread quickly via fleas?,
but ?We doubt the epidemic spread quickly?
entails ?We doubt the epidemic spread quickly
via fleas?. Here, we present the first algorithm
for the challenging lexical-semantics prob-
lem of learning linguistic constructions that,
like ?doubt?, are downward entailing (DE).
Our algorithm is unsupervised, resource-lean,
and effective, accurately recovering many DE
operators that are missing from the hand-
constructed lists that textual-inference sys-
tems currently use.
1 Introduction
Making inferences based on natural-language state-
ments is a crucial part of true natural-language un-
derstanding, and thus has many important applica-
tions. As the field of NLP has matured, there has
been a resurgence of interest in creating systems ca-
pable of making such inferences, as evidenced by
the activity surrounding the ongoing sequence of
?Recognizing Textual Entailment? (RTE) competi-
tions (Dagan, Glickman, and Magnini, 2006; Bar-
Haim, Dagan, Dolan, Ferro, Giampiccolo, Magnini,
and Szpektor, 2006; Giampiccolo, Magnini, Dagan,
and Dolan, 2007) and the AQUAINT knowledge-
based evaluation project (Crouch, Saur??, and Fowler,
2005).
The following two examples help illustrate the
particular type of inference that is the focus of this
paper.
1. ?We know the epidemic spread quickly?
2. ?We doubt the epidemic spread quickly?
A relaxation of ?spread quickly? is ?spread?; a re-
striction of it is ?spread quickly via fleas?. From
statement 1, we can infer the relaxed version, ?We
know the epidemic spread?, whereas the restricted
version, ?We know the epidemic spread quickly via
fleas?, does not follow. But the reverse holds for
statement 2: it entails the restricted version ?We
doubt the epidemic spread quickly via fleas?, but not
the relaxed version. The reason is that ?doubt? is a
downward-entailing operator;1 in other words, it al-
lows one to, in a sense, ?reason from sets to subsets?
(van der Wouden, 1997, pg. 90).
Downward-entailing operators are not restricted
to assertions about belief or to verbs. For example,
the preposition ?without? is also downward entail-
ing: from ?The applicants came without payment or
waivers? we can infer that all the applicants came
without payment. (Contrast this with ?with?, which,
like ?know?, is upward entailing.) In fact, there are
many downward-entailing operators, encompassing
many syntactic types; these include explicit nega-
tions like ?no? and ?never?, but also many other
terms, such as ?refuse (to)?, ?preventing?, ?nothing?,
?rarely?, and ?too [adjective] to?.
1Synonyms for ?downward entailing? include downward-
monotonic and monotone decreasing. Related concepts include
anti-additivity, veridicality, and one-way implicatives.
137
As the prevalence of these operators indicates and
as van der Wouden (1997, pg. 92) states, downward
entailment ?plays an extremely important role in
natural language? (van Benthem, 1986; Hoeksema,
1986; Sa?nchez Valencia, 1991; Dowty, 1994; Mac-
Cartney and Manning, 2007). Yet to date, only a few
systems attempt to handle the phenomenon in a gen-
eral way, i.e., to consider more than simple direct
negation (Nairn, Condoravdi, and Karttunen, 2006;
MacCartney and Manning, 2008; Christodoulopou-
los, 2008; Bar-Haim, Berant, Dagan, Greental,
Mirkin, Shnarch, and Szpektor, 2008). These sys-
tems rely on lists of items annotated with respect to
their behavior in ?polar? (positive or negative) envi-
ronments. The lists contain a relatively small num-
ber of downward-entailing operators, at least in part
because they were constructed mainly by manual
inspection of verb lists (although a few non-verbs
are sometimes also included). We therefore propose
to automatically learn downward-entailing opera-
tors2 ? henceforth DE operators for short ? from
data; deriving more comprehensive lists of DE op-
erators in this manner promises to substantially en-
hance the ability of textual-inference systems to han-
dle monotonicity-related phenomena.
Summary of our approach There are a num-
ber of significant challenges to applying a learning-
based approach. First, to our knowledge there do
not exist DE-operator-annotated corpora, and more-
over, relevant types of semantic information are ?not
available in or deducible from any public lexical
database? (Nairn et al, 2006). Also, it seems there
is no simple test one can apply to all possible candi-
dates; van der Wouden (1997, pg. 110) remarks, ?As
a rule of thumb, assume that everything that feels
negative, and everything that [satisfies a condition
described below], is monotone decreasing. This rule
of thumb will be shown to be wrong as it stands; but
2We include superlatives (?tallest?), comparatives (?taller?),
and conditionals (?if?) in this category because they have non-
default (i.e., non-upward entailing) properties ? for instance,
?he is the tallest father? does not entail ?he is the tallest man?.
Thus, they also require special treatment when considering en-
tailment relations. In fact, there have been some attempts
to unify these various types of non-upward entailing opera-
tors (von Fintel, 1999). We use the term downward entailing
(narrowly-defined) (DE(ND)) when we wish to specifically ex-
clude superlatives, comparatives, and conditionals.
it sort of works, like any rule of thumb.?
Our first insight into how to overcome these chal-
lenges is to leverage a finding from the linguistics lit-
erature, Ladusaw?s (1980) hypothesis, which can be
treated as a cue regarding the distribution of DE op-
erators: it asserts that a certain class of lexical con-
structions known as negative polarity items (NPIs)
can only appear in the scope of DE operators. Note
that this hypothesis suggests that one can develop
an unsupervised algorithm based simply on check-
ing for co-occurrence with known NPIs.
But there are significant problems with apply-
ing this idea in practice, including: (a) there is no
agreed-upon list of NPIs; (b) terms can be ambigu-
ous with respect to NPI-hood; and (c) many non-DE
operators tend to co-occur with NPIs as well. To
cope with these issues, we develop a novel unsuper-
vised distillation algorithm that helps filter out the
noise introduced by these problems. This algorithm
is very effective: it is accurate and derives many DE
operators that do not appear on pre-existing lists.
Contributions Our project draws a connection be-
tween the creation of textual entailment systems and
linguistic inquiry regarding DE operators and NPIs,
and thus relates to both language-engineering and
linguistic concerns.
To our knowledge, this work represents the first
attempt to aid in the process of discovering DE oper-
ators, a task whose importance we have highlighted
above. At the very least, our method can be used
to provide high-quality raw materials to help human
annotators create more extensive DE operator lists.
In fact, while previous manual-classification efforts
have mainly focused on verbs, we retrieve DE oper-
ators across multiple parts of speech. Also, although
we discover many items (including verbs) that are
not on pre-existing manually-constructed lists, the
items we find occur frequently ? they are not some-
how peculiar or rare.
Our algorithm is surprisingly accurate given that it
is quite resource- and knowledge-lean. Specifically,
it relies only on Ladusaw?s hypothesis as initial in-
spiration, a relatively short and arguably noisy list
of NPIs, and a large unannotated corpus. It does
not use other linguistic information ? for exam-
ple, we do not use parse information, even though
c-command relations have been asserted to play a
138
key role in the licensing of NPIs (van der Wouden,
1997).
2 Method
We mentioned in the introduction some significant
challenges to developing a machine-learning ap-
proach to discovering DE operators. The key insight
we apply to surmount these challenges is that in the
linguistics literature, it has been hypothesized that
there is a strong connection between DE operators
and negative polarity items (NPIs), which are terms
that tend to occur in ?negative environments?. An
example NPI is ?anymore?: one can say ?We don?t
have those anymore? but not ?We have those any-
more?.
Specifically, we propose to take advantage of the
seminal hypothesis of Ladusaw (1980, influenced by
Fauconnier (1975), inter alia):
(Ladusaw) NPIs only appear within the
scope of downward-entailing operators.
This hypothesis has been actively discussed, up-
dated, and contested by multiple parties (Linebarger,
1987; von Fintel, 1999; Giannakidou, 2002, inter
alia). It is not our intent to comment (directly) on its
overall validity. Rather, we simply view it as a very
useful starting point for developing computational
tools to find DE operators? indeed, even detractors
of the theory have called it ?impressively algorith-
mic? (Linebarger, 1987, pg. 361).
First, a word about scope. For Ladusaw?s hypoth-
esis, scope should arguably be defined in terms of c-
command, immediate scope, and so on (von Fintel,
1999, pg. 100). But for simplicity and to make our
approach as resource-lean as possible, we simply as-
sume that potential DE operators occur to the left of
NPIs,3 except that we ignore text to the left of any
preceding commas or semi-colons as a way to en-
force a degree of locality. For example, in both ?By
the way, we don?t have plants anymoreNPI because
they died? and ?we don?t have plants anymoreNPI?,
we look for DE operators within the sequence of
words ?we don?t have plants?. We refer to such se-
quences in which we seek DE operators as NPI con-
texts.
3There are a few exceptions, such as with the NPI ?for the
life of me? (Hoeksema, 1993).
Now, Ladusaw?s hypothesis suggests that we can
find DE operators by looking for words that tend to
occur more often in NPI contexts than they occur
overall. We formulate this as follows:
Assumption: For any DE operator d,
FbyNPIpdq ? F pdq.
Here, FbyNPIpdq is the number of occurrences of d
in NPI contexts4 divided by the number of words
in NPI contexts, and F pxq refers to the number of
occurrences of x relative to the number of words in
the corpus.
An additional consideration is that we would like
to focus on the discovery of novel or non-obvious
DE operators. Therefore, for a given candidate DE
operator c, we compute pFbyNPIpcq: the value of
FbyNPIpcq that results if we discard all NPI con-
texts containing a DE operator on a list of 10 well-
known instances, namely, ?not?, ?n?t?, ?no?, ?none?,
?neither?, ?nor?, ?few?, ?each?, ?every?, and ?without?.
(This list is based on the list of DE operators used by
the RTE system presented in MacCartney and Man-
ning (2008).) This yields the following scoring func-
tion:
Spcq : pFbyNPIpcqF pcq . (1)
Distillation There are certain terms that are not
DE operators, but nonetheless co-occur with NPIs as
a side-effect of co-occurring with true DE operators
themselves. For instance, the proper noun ?Milken?
(referring to Michael Milken, the so-called ?junk-
bond king?) occurs relatively frequently with the DE
operator ?denies?, and ?vigorously? occurs frequently
with DE operators like ?deny? and ?oppose?. We re-
fer to terms like ?milken? and ?vigorously? as ?pig-
gybackers?, and address the piggybackers problem
by leveraging the following intuition: in general, we
do not expect to have two DE operators in the same
NPI context.5 One way to implement this would be
to re-score the candidates in a winner-takes-all fash-
ion: for each NPI context, reward only the candidate
4Even if d occurs multiple times in a single NPI context we
only count it once; this way we ?dampen the signal? of func-
tion words that can potentially occur multiple times in a single
sentence.
5One reason is that if two DE operators are composed, they
ordinarily create a positive context, which would not license
NPIs (although this is not always the case (Dowty, 1994)).
139
with the highest score S. However, such a method
is too aggressive because it would force us to pick
a single candidate even when there are several with
relatively close scores? and we know our score S is
imperfect. Instead, we propose the following ?soft?
mechanism. Each sentence distributes a ?budget? of
total score 1 among the candidates it contains ac-
cording to the relative scores of those candidates;
this works out to yield the following new distilled
scoring function
Sdpcq 
?
NPIcontexts p
Spcq
nppq
Npcq , (2)
where nppq  ?cP p Spcq is an NPI-context normal-
izing factor and Npcq is the number of NPI con-
texts containing the candidate c. This way, plausi-
ble candidates that have high S scores relative to the
other candidates in the sentence receive enhanced Sd
scores. To put it another way: apparently plausible
candidates that often appear in sentences with mul-
tiple good candidates (i.e., piggybackers) receive a
low distilled score, despite a high initial score.
Our general claim is that the higher the distilled
score of a candidate, the better its chances of being
a DE operator.
Choice of NPIs Our proposed method requires ac-
cess to a set of NPIs. However, there does not ap-
pear to be universal agreement on such a set. Lichte
and Soehn (2007) mention some doubts regarding
approximately 200 (!) of the items on a roughly 350-
item list of German NPIs (Ku?rschner, 1983). For
English, the ?moderately complete?6 Lawler (2005)
list contains two to three dozen items; however,
there is also a list of English NPIs that is several
times longer (von Bergen and von Bergen, 1993,
written in German), and Hoeksema (1997) asserts
that English should have hundreds of NPIs, similarly
to French and Dutch.
We choose to focus on the items on these lists
that seem most likely to be effective cues for our
task. Specifically, we select a subset of the Lawler
NPIs, focusing mostly on those that do not have
a relatively frequent non-NPI sense. An example
discard is ?much?, whose NPI-hood depends on
6www-personal.umich.edu/jlawler/aue/
npi.html
what it modifies and perhaps on whether there
are degree adverbs pre-modifying it (Hoeksema,
1997). There are some ambiguous NPIs that we
do retain due to their frequency. For example,
?any? occurs both in a non-NPI ?free choice?
variant, as in ?any idiot can do that?, and in an
NPI version. Although it is ambiguous with re-
spect to NPI-hood, ?any? is also a very valuable
cue due to its frequency.7 Here is our NPI list:
any in weeks/ages/years budge yet
at all drink a drop red cent ever
give a damn last/be/take long but what bother to
do a thing arrive/leave until give a shit lift a finger
bat an eye would care/mind eat a bite to speak of
3 Experiments
Our main set of evaluations focuses on the precision
of our method at discovering new DE operators. We
then briefly discuss evaluation of other dimensions.
3.1 Setup
We applied our method to the entirety of the BLLIP
(Brown Laboratory for Linguistic Information Pro-
cessing) 1987?89 WSJ Corpus Release 1, available
from the LDC (LDC2000T43). The 1,796,379 sen-
tences in the corpus comprise 53,064 NPI contexts;
after discarding the ones containing the 10 well-
known DE operators, 30,889 NPI contexts were left.
To avoid sparse data problems, we did not consider
candidates with very low frequency in the corpus
(?150 occurrences) or in the NPI contexts (?10 oc-
currences).
Methodology for eliciting judgments The obvi-
ous way to evaluate the precision of our algorithm is
to have human annotators judge each output item as
to whether it is a DE operator or not. However, there
are some methodological issues that arise.
First, if the judges know that every term they are
rating comes from our system and that we are hoping
that the algorithm extracts DE operators, they may
be biased towards calling every item ?DE? regard-
less of whether it actually is. We deal with this prob-
lem by introducing distractors ? items that are not
produced by our algorithm, but are similar enough
to not be easily identifiable as ?fakes?. Specifically,
7It is by far the most frequent NPI, appearing in 36,554 of
the sentences in the BLLIP corpus (see Section 3).
140
for each possible part of speech of each of our sys-
tem?s outputs c that exists in WordNet, we choose a
distractor that is either in a ?sibling? synset (a hy-
ponym of c?s hypernym) or an antonym. Thus, the
distractors are highly related to the candidates. Note
that they may in fact also be DE operators.
The judges were made aware of the presence of
a substantial number of distractors (about 70 for the
set of top 150 outputs). This design choice did seem
to help ensure that the judges carefully evaluated
each item.
The second issue is that, as mentioned in the in-
troduction, there does not seem to be a uniform test
that judges can apply to all items to ascertain their
DE-ness; but we do not want the judges to impro-
vise excessively, since that can introduce undesir-
able randomness into their decisions. We therefore
encouraged the judges to try to construct sentences
wherein the arguments for candidate DE operators
were drawn from a set of phrases and restricted
replacements we specified (example: ?singing? vs
?singing loudly?). However, improvisation was still
required in a number of cases; for example, the can-
didate ?act?, as either a noun or a verb, cannot take
?singing? as an argument.
The labels that the judges could apply were
?DE(ND)? (downward entailing (narrowly-
defined)), ?superlative?, ?comparative?, ?condi-
tional?, ?hard to tell?, and ?not-DE? (= none of the
above). We chose this fine-grained sub-division
because the second through fourth categories are
all known to co-occur with NPIs. There is some
debate in the linguistics literature as to whether
they can be considered to be downward entailing,
narrowly construed, or not (von Fintel, 1999,
inter alia), but nonetheless, such operators call for
special reasoning quite distinct from that required
when dealing with upward entailing operators ?
hence, we consider it a success when our algorithm
identifies them.
Since monotonicity phenomena can be rather sub-
tle, the judges engaged in a collaborative process.
Judge A (the second author) annotated all items, but
worked in batches of around 10 items. At the end of
each batch, Judge B (the first author) reviewed Judge
A?s decisions, and the two consulted to resolve dis-
agreements as far as possible.
One final remark regarding the annotation: some
decisions still seem uncertain, since various factors
such as context, Gricean maxims, what should be
presupposed8 and so on come into play. However,
we take comfort in a comment by Eugene Charniak
(personal communication) to the effect that if a word
causes a native speaker to pause, that word is inter-
esting enough to be included. And indeed, it seems
reasonable that if a native speaker thinks there might
be a sense in which a word can be considered down-
ward entailing, then our system should flag it as a
word that an RTE system should at least perhaps
pass to a different subsystem for further analysis.
3.2 Precision Results
We now examine the 150 items that were most
highly ranked by our system, which were sub-
sequently annotated as just described. (For
full system output that includes the unannotated
items, see http://www.cs.cornell.edu/
cristian. We would welcome external anno-
tation help.) As shown in Figure 1a, which depicts
precision at k for various values of k, our system
performs very well. In fact, 100% of the first 60 out-
puts are DE, broadly construed. It is also interesting
to note the increasing presence of instances that the
judges found hard to categorize as we move further
down the ranking.
Of our 73 distractors, 46% were judged to be
members of one of our goal categories. The fact that
this percentage is substantially lower than our algo-
rithm?s precision at both 73 and 150 (the largest k we
considered) confirms that our judges were not mak-
ing random decisions. (We expect the percentage
of DE operators among the distractors to be much
higher than 0 because they were chosen to be simi-
lar to our system?s outputs, and so can be expected
to also be DE operators some fraction of the time.)
Table 1 shows the lemmas of just the DE(ND) op-
erators that our algorithm placed in its top 150 out-
puts.9 Most of these lemmas are new discoveries, in
the sense of not appearing in Ladusaw?s (1980) (im-
plicit) enumeration of DE operators. Moreover, the
8For example, ?X doubts the epidemic spread quickly? might
be said to entail ?X would doubt the epidemic spreads via fleas,
presupposing that X thinks about the flea issue?.
9By listing lemmas, we omit variants of the same word, such
as ?doubting? and ?doubted?, to enhance readability. We omit
superlatives, comparatives, and conditionals for brevity.
141
10 20 30 40 50 60 70 80 90 100 110 120 130 140 1500
10
20
30
40
50
60
70
80
90
100
k
Prec
ision
 at k
 
 DE(ND)S/C/CHard
10 20 30 40 50 60 70 80 90 100 110 120 130 140 1500
10
20
30
40
50
60
70
80
90
100
k
Prec
ision
 at k
 
 DE(ND)S/C/CHard
(a) (b)
Figure 1: (a) Precision at k for k divisible by 10 up to k  150. The bar divisions are, from the x-axis up,
DE(ND) (blue, the largest); Superlatives/Conditionals/Comparatives (green, 2nd largest); and Hard (red, sometimes
non-existent). For example, all of the first 10 outputs were judged to be either downward entailing (narrowly-defined)
(8 of 10, or 80%) or in one of the related categories (20%). (b) Precision at k when the distillation step is omitted.
not-DE Hard
almost firmly one-day approve
ambitious fined signal cautioned
considers liable remove dismissed
detect notify vowed fend
Table 3: Examples of words judged to be either not in
one of our monotonicity categories of interest (not-DE)
or hard to evaluate (Hard).
lists of DE(ND) operators that are used by textual-
entailment systems are significantly smaller than
that depicted in Table 1; for example, MacCartney
and Manning (2008) use only about a dozen (per-
sonal communication).
Table 3 shows examples of the words in our sys-
tem?s top 150 outputs that are either clear mistakes
or hard to evaluate. Some of these are due to id-
iosyncrasies of newswire text. For instance, we of-
ten see phrases like ?biggest one-day drop in ...?,
where ?one-day? piggybacks on superlatives, and
?vowed? piggybacks on the DE operator ?veto?, as
in the phrase ?vowed to veto?.
Effect of distillation In order to evaluate the im-
portance of the distillation process, we study how
the results change when distillation is omitted (thus
using as score function S from Equation 1 rather
than Sd). When comparing the results (summarized
in Figure 1b) with those of the complete system
(Figure 1a) we observe that the distillation indeed
has the desired effect: the number of highly ranked
words that are annotated as not-DE decreases after
distillation. This results in an increase of the preci-
sion at k ranging from 5% to 10% (depending on k),
as can be observed by comparing the height of the
composite bars in the two figures.10
Importantly, this improvement does indeed seem
to stem at least in part from the distillation process
handling the piggybacking problem. To give just a
few examples: ?vigorously? is pushed down from
rank 48 (undistilled scoring) to rank 126 (distilled
scoring), ?one-day? from 25th to 65th, ?vowed? from
45th to 75th, and ?Milken? from 121st to 350th.
3.3 Other Results
It is natural to ask whether the (expected) decrease
in precision at k is due to the algorithm assigning
relatively low scores to DE operators, so that they
do not appear in the top 150, or due to there be-
ing no more more true DE operators to rank. We
cannot directly evaluate our method?s recall because
no comprehensive list of DE operators exists. How-
ever, to get a rough impression, we can check how
our system ranks the items in the largest list we are
aware of, namely, the Ladusaw (implicit) list men-
tioned above. Of the 31 DE operator lemmas on this
list (not including the 10 well-known DE operators),
only 7 of those frequent enough to be considered by
our algorithm are not in its top 150 outputs, and only
10The words annotated ?hard? do not affect this increase in
precision.
142
absence of
absent from
anxious about 
to avoid (L)
to bar
barely
to block
cannot (L)
compensate for 
to decline
to defer
to deny (L)
to deter
to discourage
to dismiss
to doubt (L)
to eliminate
essential for 
to exclude
to fail (L)
hardly (L)
to lack
innocent of 
to minimize 
never (L)
nobody
nothing
to oppose
to postpone 
to preclude
premature to
to prevent
to prohibit
rarely (L)
to refrain from
to refuse (L)
regardless 
to reject
reluctant to (L)
to resist
to rule out
skeptical 
to suspend
to thwart
unable to
unaware of
unclear on
unlike
unlikely (L)
unwilling to
to veto
wary of
warned that (L)
whenever
withstand
Table 1: The 55 lemmas for the 90 downward entailing (narrowly-defined) operators among our algorithm?s top 150
outputs. (L) marks instances from Ladusaw?s list.  marks some of the more interesting cases. We have added
function words (e.g., ?to?, ?for?) to indicate parts of speech or subcategorization; our algorithm does not discover
multi-word phrases.
Original ? Restriction
Dan is unlikely to sing. ??
??{
Dan is unlikely to sing loudly.
Olivia compensates for eating by exercising. ??
??{
Olivia compensates for eating late by exercising.
Ursula refused to sing or dance. ??
??{
Ursula refused to sing.
Bob would postpone singing. ??
??{
Bob would postpone singing loudly.
Talent is essential for singing. ??
??{
Talent is essential for singing a ballad.
She will finish regardless of threats. ??
??{
She will finish regardless of threats to my career.
Table 2: Example demonstrations that the underlined expressions (selected from Table 1) are DE operators: the
sentences on the left entail those on the right. We also have provided??{ indicators because the reader might find it
helpful to reason in the opposite direction and see that these expressions are not upward entailing.
5 are not in the top 300. Remember that we only an-
notated the top 150 outputs; so, there may be many
other DE operators between positions 150 and 300.
Another way of evaluating our method would be
to assess the effect of our newly discovered DE op-
erators on downstream RTE system performance.
There are two factors to take into account. First, the
DE operators we discovered are quite prevalent in
naturally occurring text11 : the 90 DE(ND) operators
appearing in our algorithm?s top 150 outputs occur
in 111,456 sentences in the BLLIP corpus (i.e., in
6% of its sentences). Second, as previously men-
tioned, systems do already account for monotonic-
ity to some extent ? but they are limited by the fact
that their DE operator lexicons are restricted mostly
to well-known instances; to take a concrete example
with a publicly available RTE system: Nutcracker
(Bos and Markert, 2006) correctly infers that ?We
did not know the disease spread? entails ?We did not
know the disease spread quickly? but it fails to in-
11However, RTE competitions do not happen to currently
stress inferences involving monotonicity. The reasons why are
beyond the scope of this paper.
fer that ?We doubt the disease spread? entails ?We
doubt the disease spread quickly?. So, systems can
use monotonicity information but currently do not
have enough of it; our method can provide themwith
this information, enabling them to handle a greater
fraction of the large number of naturally occurring
instances of this phenomenon than ever before.
4 Related work not already discussed
Magnini (2008), in describing modular approaches
to textual entailment, hints that NPIs may be used
within a negation-detection sub-component.
There is a substantial body of work in the linguis-
tics literature regarding the definition and nature of
polarity items (Polarity Items Bibliography). How-
ever, very little of this work is computational. There
has been passing speculation that one might want
to learn polarity-inverting verbs (Christodoulopou-
los, 2008, pg. 47). There have also been a few
projects on the discovery of NPIs, which is the con-
verse of the problem we consider. Hoeksema (1997)
discusses some of the difficulties with corpus-based
determination of NPIs, including ?rampant? poly-
143
semy and the problem of ?how to determine inde-
pendently which predicates should count as nega-
tive?? a problemwhich our work addresses. Lichte
and Soehn (Lichte, 2005; Lichte and Soehn, 2007)
consider finding German NPIs using a method con-
ceptually similar in some respects to our own, al-
though again, their objective is the reverse of ours.
Their discovery statistic for single-word NPIs is the
ratio of within-licenser-clause occurrences to total
occurrences, where, to enhance precision, the list of
licensers was filtered down to a set of fairly unam-
biguous, easily-identified items. They do not con-
sider distillation, which we found to be an impor-
tant component of our DE-operator-detection algo-
rithm. Their evaluation scheme, unlike ours, did not
employ a bias-compensation mechanism. They did
employ a collocation-detection technique to extend
their list to multi-word NPIs, but our independent
experiments with a similar technique (not reported
here) did not yield good results.
5 Conclusions and future work
To our knowledge, this work represents the first at-
tempt to discover downward entailing operators. We
introduced a unsupervised algorithm that is moti-
vated by research in linguistics but employs simple
distributional statistics in a novel fashion. Our algo-
rithm is highly accurate and discovers many reason-
able DE operators that are missing from pre-existing
manually-built lists.
Since the algorithm is resource-lean ? requiring
no parser or tagger but only a list of NPIs? it can be
immediately applied to languages where such lists
exist, such as German and Romanian (Trawin?ski and
Soehn, 2008). On the other hand, although the re-
sults are already quite good for English, it would
be interesting to see what improvements could be
gained by using more sophisticated syntactic infor-
mation.
For languages where NPI lists are not extensive,
one could envision applying an iterative co-learning
approach: use the newly-derived DE operators to in-
fer new NPIs, and then discover even more new DE
operators given the new NPI list. (For English, our
initial attempts at bootstrapping from our initial NPI
list on the BLLIP corpus did not lead to substantially
improved results.)
In practice, subcategorization is an important fea-
ture to capture. In Table 1, we indicate which sub-
categorizations are DE. An interesting extension of
our work would be to try to automatically distin-
guish particular DE subcategorizations that are lex-
ically apparent, e.g., ?innocent? (not DE) vs. ?inno-
cent of? (as in ?innocent of burglary?, DE).
Our project provides a connection (among many)
between the creation of textual entailment systems
(the domain of language engineers) and the char-
acterization of DE operators (the subject of study
and debate among linguists). The prospect that our
method might potentially eventually be refined in
such a way so as to shed at least a little light on lin-
guistic questions is a very appealing one, although
we cannot be certain that any progress will be made
on that front.
Acknowledgments We thank Roy Bar-Haim, Cleo Con-
doravdi, and Bill MacCartney for sharing their systems? lists
and information about their work with us; Mats Rooth for
helpful conversations; Alex Niculescu-Mizil for technical as-
sistance; and Eugene Charniak for reassuring remarks. We also
thank Marisa Ferrara Boston, Claire Cardie, Zhong Chen, Yejin
Choi, Effi Georgala, Myle Ott, Stephen Purpura, and Ainur
Yessenalina at Cornell University, the UT-Austin NLP group,
Roy Bar-Haim, Bill MacCartney, and the anonymous review-
ers for for their comments on this paper. This paper is based
upon work supported in part by DHS grant N0014-07-1-0152,
National Science Foundation grant No. BCS-0537606, a Ya-
hoo! Research Alliance gift, a CU Provost?s Award for Distin-
guished Scholarship, and a CU Institute for the Social Sciences
Faculty Fellowship. Any opinions, findings, and conclusions or
recommendations expressed are those of the authors and do not
necessarily reflect the views or official policies, either expressed
or implied, of any sponsoring institutions, the U.S. government,
or any other entity.
References
Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo
Giampiccolo, Bernardo Magnini, and Idan Szpektor.
The second PASCAL Recognising Textual Entailment
challenge. In Proceedings of the Second PASCAL
Challenges Workshop on Recognising Textual Entail-
ment, 2006.
Roy Bar-Haim, Jonathan Berant, Ido Dagan, Iddo Green-
tal, Shachar Mirkin, Eyal Shnarch, and Idan Szpektor.
Efficient semantic deduction and approximate match-
ing over compact parse forests. In Proceedings of TAC,
2008.
Johan Bos and Katja Markert. Recognising textual en-
tailment with robust logical inference. In Quin?onero
Candela, Dagan, Magnini, and d?Alche? Buc (2006),
pages 404?426.
Christos Christodoulopoulos. Creating a natural logic in-
ference system with combinatory categorial grammar.
Master?s thesis, University of Edinburgh, 2008.
144
Dick Crouch, Roser Saur??, and Abraham Fowler.
AQUAINT pilot knowledge-based evalua-
tion: Annotation guidelines. http://www2.
parc.com/istl/groups/nltt/papers/
aquaint kb pilot evaluation guide.pdf,
2005.
Ido Dagan, Oren Glickman, and Bernardo Magnini. The
PASCAL Recognising Textual Entailment challenge.
In Quin?onero Candela et al (2006), pages 177?190.
David Dowty. The role of negative polarity and con-
cord marking in natural language reasoning. In Mandy
Harvey and Lynn Santelmann, editors, Proceedings of
SALT IV, pages 114?144, Ithaca, New York, 1994.
Cornell University.
Gilles Fauconnier. Polarity and the scale principle. In
Proceedings of the Chicago Linguistic Society (CLS),
pages 188?199, 1975. Reprinted in Javier Gutierrez-
Rexach (ed.), Semantics: Critical Concepts in Linguis-
tics, 2003.
Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and
Bill Dolan. The third PASCAL Recognizing Textual
Entailment challenge. In Proceedings of the ACL-
PASCAL Workshop on Textual Entailment and Para-
phrasing, pages 1?9, 2007. URL http://www.
aclweb.org/anthology/W/W07/W07-1401.
Anastasia Giannakidou. Licensing and sensitivity in po-
larity items: from downward entailment to nonveridi-
cality. In Proceedings of the Chicago Linguistic Soci-
ety (CLS), 2002.
Jack Hoeksema. Monotonicity phenomena in natural lan-
guage. Linguistic Analysis, 16:25?40, 1986.
Jack Hoeksema. As (of) yet. Appears in Language and
Cognition 3, the 1992 yearbook of the research group
for theoretical and experimental linguistics of the Uni-
versity of Groningen, 1993. http://www.let.
rug.nl/hoeksema/asofyet.pdf.
Jack Hoeksema. Corpus study of negative polar-
ity items. IV-V Jornades de corpus linguistics
1996-1997, 1997. http://odur.let.rug.nl/
hoeksema/docs/barcelona.html.
Wilfried Ku?rschner. Studien zur Negation im Deutschen.
Narr, 1983.
William A. Ladusaw. Polarity Sensitivity as Inherent
Scope Relations. Garland Press, New York, 1980.
Ph.D. thesis date 1979.
John Lawler. Negation and NPIs. http://www.
umich.edu/jlawler/NPIs.pdf, 2005. Ver-
sion of 10/29/2005.
Timm Lichte. Corpus-based acquisition of complex neg-
ative polarity items. In ESSLLI Student Session, 2005.
Timm Lichte and Jan-Philipp Soehn. The retrieval and
classification of Negative Polarity Items using statisti-
cal profiles. In Sam Featherston and Wolfgang Sterne-
feld, editors, Roots: Linguistics in Search of its Ev-
idential Base, pages 249?266. Mouton de Gruyter,
2007.
Marcia Linebarger. Negative polarity and grammatical
representation. Linguistics and philosophy, 10:325?
387, 1987.
Bill MacCartney and Christopher D. Manning. Natural
logic for textual inference. In Proceedings of the ACL-
PASCAL Workshop on Textual Entailment and Para-
phrasing, pages 193?200, 2007.
Bill MacCartney and Christopher D. Manning. Mod-
eling semantic containment and exclusion in natu-
ral language inference. In Proceedings of the 22nd
International Conference on Computational Linguis-
tics (Coling 2008), pages 521?528, Manchester, UK,
August 2008. Coling 2008 Organizing Committee.
URL http://www.aclweb.org/anthology/
C08-1066.
Bernardo Magnini. Slides for a presentation entitled ?Se-
mantic Knowledge for Textual Entailment?. Sympo-
sium on Semantic Knowledge Discovery, Organiza-
tion and Use, New York University, November 14 and
15, 2008.
Rowan Nairn, Cleo Condoravdi, and Lauri Karttunen.
Computing relative polarity for textual inference. In
Proceedings of Inference in Computational Semantics
(ICoS), 2006.
Polarity Items Bibliography. The polarity items
bibliography. http://www.sfb441.
uni-tuebingen.de/a5/pib/XML2HTML/
list.html, 2008. Maintenance guaranteed only
through December 2008.
Joaquin Quin?onero Candela, Ido Dagan, Bernardo
Magnini, and Florence d?Alche? Buc, editors. Ma-
chine Learning Challenges, Evaluating Predictive Un-
certainty, Visual Object Classification and Recogniz-
ing Textual Entailment, First PASCAL Machine Learn-
ing Challenges Workshop, MLCW 2005, Southamp-
ton, UK, April 11-13, 2005, Revised Selected Papers,
volume 3944 of Lecture Notes in Computer Science
(LNCS), 2006. Springer.
V??ctor Sa?nchez Valencia. Studies on natural logic and
categorial grammar. PhD thesis, University of Ams-
terdam, 1991.
Beata Trawin?ski and Jan-Philipp Soehn. A Multilingual
Database of Polarity Items. In Proceedings of LREC
2008, May 28?30, Marrakech, Morocco, 2008.
Johan van Benthem. Essays in Logical Semantics. Reidel,
Dordrecht, 1986.
Ton van der Wouden. Negative contexts: Collocation,
polarity and multiple negation. Routledge, 1997.
Anke von Bergen and Karl von Bergen. Negative
Polarita?t im Englischen. Gunter Narr, 1993. List
extracted and compiled by Manfred Sailer, 2008,
http://www.sfs.uni-tuebingen.de/fr/
esslli/08/byday/english-npi.pdf.
Kai von Fintel. NPI licensing, Strawson entailment, and
context dependency. Journal of Semantics, 16:97?148,
1999.
145
A Sentimental Education: Sentiment Analysis Using Subjectivity
Summarization Based on Minimum Cuts
Bo Pang and Lillian Lee
Department of Computer Science
Cornell University
Ithaca, NY 14853-7501
{pabo,llee}@cs.cornell.edu
Abstract
Sentiment analysis seeks to identify the view-
point(s) underlying a text span; an example appli-
cation is classifying a movie review as ?thumbs up?
or ?thumbs down?. To determine this sentiment po-
larity, we propose a novel machine-learning method
that applies text-categorization techniques to just
the subjective portions of the document. Extracting
these portions can be implemented using efficient
techniques for finding minimum cuts in graphs; this
greatly facilitates incorporation of cross-sentence
contextual constraints.
1 Introduction
The computational treatment of opinion, sentiment,
and subjectivity has recently attracted a great deal
of attention (see references), in part because of its
potential applications. For instance, information-
extraction and question-answering systems could
flag statements and queries regarding opinions
rather than facts (Cardie et al, 2003). Also, it
has proven useful for companies, recommender sys-
tems, and editorial sites to create summaries of peo-
ple?s experiences and opinions that consist of sub-
jective expressions extracted from reviews (as is
commonly done in movie ads) or even just a re-
view?s polarity ? positive (?thumbs up?) or neg-
ative (?thumbs down?).
Document polarity classification poses a signifi-
cant challenge to data-driven methods, resisting tra-
ditional text-categorization techniques (Pang, Lee,
and Vaithyanathan, 2002). Previous approaches fo-
cused on selecting indicative lexical features (e.g.,
the word ?good?), classifying a document accord-
ing to the number of such features that occur any-
where within it. In contrast, we propose the follow-
ing process: (1) label the sentences in the document
as either subjective or objective, discarding the lat-
ter; and then (2) apply a standard machine-learning
classifier to the resulting extract. This can prevent
the polarity classifier from considering irrelevant or
even potentially misleading text: for example, al-
though the sentence ?The protagonist tries to pro-
tect her good name? contains the word ?good?, it
tells us nothing about the author?s opinion and in
fact could well be embedded in a negative movie
review. Also, as mentioned above, subjectivity ex-
tracts can be provided to users as a summary of the
sentiment-oriented content of the document.
Our results show that the subjectivity extracts
we create accurately represent the sentiment in-
formation of the originating documents in a much
more compact form: depending on choice of down-
stream polarity classifier, we can achieve highly sta-
tistically significant improvement (from 82.8% to
86.4%) or maintain the same level of performance
for the polarity classification task while retaining
only 60% of the reviews? words. Also, we ex-
plore extraction methods based on a minimum cut
formulation, which provides an efficient, intuitive,
and effective means for integrating inter-sentence-
level contextual information with traditional bag-of-
words features.
2 Method
2.1 Architecture
One can consider document-level polarity classi-
fication to be just a special (more difficult) case
of text categorization with sentiment- rather than
topic-based categories. Hence, standard machine-
learning classification techniques, such as support
vector machines (SVMs), can be applied to the en-
tire documents themselves, as was done by Pang,
Lee, and Vaithyanathan (2002). We refer to such
classification techniques as default polarity classi-
fiers.
However, as noted above, we may be able to im-
prove polarity classification by removing objective
sentences (such as plot summaries in a movie re-
view). We therefore propose, as depicted in Figure
1, to first employ a subjectivity detector that deter-
mines whether each sentence is subjective or not:
discarding the objective ones creates an extract that
should better represent a review?s subjective content
to a default polarity classifier.
s1
s2
s3
s4
s_n
+/?s4
s1
subje
ctivity
dete
ctor
yes
no
no
yes
n?sentence review subjectivesentence? m?sentence extract(m<=n) review?positive or negative
defa
ult
class
ifier
pola
rity
subjectivity extraction
Figure 1: Polarity classification via subjectivity detec-
tion.
To our knowledge, previous work has not in-
tegrated sentence-level subjectivity detection with
document-level sentiment polarity. Yu and Hatzi-
vassiloglou (2003) provide methods for sentence-
level analysis and for determining whether a doc-
ument is subjective or not, but do not combine these
two types of algorithms or consider document polar-
ity classification. The motivation behind the single-
sentence selection method of Beineke et al (2004)
is to reveal a document?s sentiment polarity, but they
do not evaluate the polarity-classification accuracy
that results.
2.2 Context and Subjectivity Detection
As with document-level polarity classification, we
could perform subjectivity detection on individual
sentences by applying a standard classification algo-
rithm on each sentence in isolation. However, mod-
eling proximity relationships between sentences
would enable us to leverage coherence: text spans
occurring near each other (within discourse bound-
aries) may share the same subjectivity status, other
things being equal (Wiebe, 1994).
We would therefore like to supply our algorithms
with pair-wise interaction information, e.g., to spec-
ify that two particular sentences should ideally re-
ceive the same subjectivity label but not state which
label this should be. Incorporating such informa-
tion is somewhat unnatural for classifiers whose in-
put consists simply of individual feature vectors,
such as Naive Bayes or SVMs, precisely because
such classifiers label each test item in isolation.
One could define synthetic features or feature vec-
tors to attempt to overcome this obstacle. However,
we propose an alternative that avoids the need for
such feature engineering: we use an efficient and
intuitive graph-based formulation relying on find-
ing minimum cuts. Our approach is inspired by
Blum and Chawla (2001), although they focused on
similarity between items (the motivation being to
combine labeled and unlabeled data), whereas we
are concerned with physical proximity between the
items to be classified; indeed, in computer vision,
modeling proximity information via graph cuts has
led to very effective classification (Boykov, Veksler,
and Zabih, 1999).
2.3 Cut-based classification
Figure 2 shows a worked example of the concepts
in this section.
Suppose we have n items x1, . . . , xn to divide
into two classes C1 and C2, and we have access to
two types of information:
? Individual scores indj(xi): non-negative esti-
mates of each xi?s preference for being in Cj based
on just the features of xi alone; and
? Association scores assoc(xi, xk): non-negative
estimates of how important it is that xi and xk be in
the same class.1
We would like to maximize each item?s ?net hap-
piness?: its individual score for the class it is as-
signed to, minus its individual score for the other
class. But, we also want to penalize putting tightly-
associated items into different classes. Thus, after
some algebra, we arrive at the following optimiza-
tion problem: assign the xis to C1 and C2 so as to
minimize the partition cost
?
x?C1
ind2(x)+
?
x?C2
ind1(x)+
?
xi?C1,
xk?C2
assoc(xi, xk).
The problem appears intractable, since there are
2n possible binary partitions of the xi?s. How-
ever, suppose we represent the situation in the fol-
lowing manner. Build an undirected graph G with
vertices {v1, . . . , vn, s, t}; the last two are, respec-
tively, the source and sink. Add n edges (s, vi), each
with weight ind1(xi), and n edges (vi, t), each with
weight ind2(xi). Finally, add
(n
2
)
edges (vi, vk),
each with weight assoc(xi, xk). Then, cuts in G
are defined as follows:
Definition 1 A cut (S, T ) of G is a partition of its
nodes into sets S = {s} ? S? and T = {t} ? T ?,
where s 6? S?, t 6? T ?. Its cost cost(S, T ) is the sum
of the weights of all edges crossing from S to T . A
minimum cut of G is one of minimum cost.
1Asymmetry is allowed, but we used symmetric scores.
[]
s t
Y
M
N
2ind (Y) [.2]1ind (Y) [.8]
2ind (M) [.5]1ind (M) [.5]
[.1]assoc(Y,N)
2ind (N) [.9]1ind (N)
assoc(M,N)
assoc(Y,M)
[.2]
[1.0]
[.1]
C1 Individual Association Cost
penalties penalties
{Y,M} .2 + .5 + .1 .1 + .2 1.1
(none) .8 + .5 + .1 0 1.4
{Y,M,N} .2 + .5 + .9 0 1.6
{Y} .2 + .5 + .1 1.0 + .1 1.9
{N} .8 + .5 + .9 .1 + .2 2.5
{M} .8 + .5 + .1 1.0 + .2 2.6
{Y,N} .2 + .5 + .9 1.0 + .2 2.8
{M,N} .8 + .5 + .9 1.0 + .1 3.3
Figure 2: Graph for classifying three items. Brackets enclose example values; here, the individual scores happen to
be probabilities. Based on individual scores alone, we would put Y (?yes?) in C1, N (?no?) in C2, and be undecided
about M (?maybe?). But the association scores favor cuts that put Y and M in the same class, as shown in the table.
Thus, the minimum cut, indicated by the dashed line, places M together with Y in C1.
Observe that every cut corresponds to a partition of
the items and has cost equal to the partition cost.
Thus, our optimization problem reduces to finding
minimum cuts.
Practical advantages As we have noted, formulat-
ing our subjectivity-detection problem in terms of
graphs allows us to model item-specific and pair-
wise information independently. Note that this is
a very flexible paradigm. For instance, it is per-
fectly legitimate to use knowledge-rich algorithms
employing deep linguistic knowledge about sen-
timent indicators to derive the individual scores.
And we could also simultaneously use knowledge-
lean methods to assign the association scores. In-
terestingly, Yu and Hatzivassiloglou (2003) com-
pared an individual-preference classifier against a
relationship-based method, but didn?t combine the
two; the ability to coordinate such algorithms is
precisely one of the strengths of our approach.
But a crucial advantage specific to the utilization
of a minimum-cut-based approach is that we can use
maximum-flow algorithms with polynomial asymp-
totic running times ? and near-linear running times
in practice ? to exactly compute the minimum-
cost cut(s), despite the apparent intractability of
the optimization problem (Cormen, Leiserson, and
Rivest, 1990; Ahuja, Magnanti, and Orlin, 1993).2
In contrast, other graph-partitioning problems that
have been previously used to formulate NLP clas-
sification problems3 are NP-complete (Hatzivassi-
loglou and McKeown, 1997; Agrawal et al, 2003;
Joachims, 2003).
2Code available at http://www.avglab.com/andrew/soft.html.
3Graph-based approaches to general clustering problems
are too numerous to mention here.
3 Evaluation Framework
Our experiments involve classifying movie reviews
as either positive or negative, an appealing task for
several reasons. First, as mentioned in the intro-
duction, providing polarity information about re-
views is a useful service: witness the popularity of
www.rottentomatoes.com. Second, movie reviews
are apparently harder to classify than reviews of
other products (Turney, 2002; Dave, Lawrence, and
Pennock, 2003). Third, the correct label can be ex-
tracted automatically from rating information (e.g.,
number of stars). Our data4 contains 1000 positive
and 1000 negative reviews all written before 2002,
with a cap of 20 reviews per author (312 authors
total) per category. We refer to this corpus as the
polarity dataset.
Default polarity classifiers We tested support vec-
tor machines (SVMs) and Naive Bayes (NB). Fol-
lowing Pang et al (2002), we use unigram-presence
features: the ith coordinate of a feature vector is
1 if the corresponding unigram occurs in the input
text, 0 otherwise. (For SVMs, the feature vectors
are length-normalized). Each default document-
level polarity classifier is trained and tested on the
extracts formed by applying one of the sentence-
level subjectivity detectors to reviews in the polarity
dataset.
Subjectivity dataset To train our detectors, we
need a collection of labeled sentences. Riloff and
Wiebe (2003) state that ?It is [very hard] to ob-
tain collections of individual sentences that can be
easily identified as subjective or objective?; the
polarity-dataset sentences, for example, have not
4Available at www.cs.cornell.edu/people/pabo/movie-
review-data/ (review corpus version 2.0).
been so annotated.5 Fortunately, we were able
to mine the Web to create a large, automatically-
labeled sentence corpus6. To gather subjective
sentences (or phrases), we collected 5000 movie-
review snippets (e.g., ?bold, imaginative, and im-
possible to resist?) from www.rottentomatoes.com.
To obtain (mostly) objective data, we took 5000 sen-
tences from plot summaries available from the In-
ternet Movie Database (www.imdb.com). We only
selected sentences or snippets at least ten words
long and drawn from reviews or plot summaries of
movies released post-2001, which prevents overlap
with the polarity dataset.
Subjectivity detectors As noted above, we can use
our default polarity classifiers as ?basic? sentence-
level subjectivity detectors (after retraining on the
subjectivity dataset) to produce extracts of the orig-
inal reviews. We also create a family of cut-based
subjectivity detectors; these take as input the set of
sentences appearing in a single document and de-
termine the subjectivity status of all the sentences
simultaneously using per-item and pairwise rela-
tionship information. Specifically, for a given doc-
ument, we use the construction in Section 2.2 to
build a graph wherein the source s and sink t cor-
respond to the class of subjective and objective sen-
tences, respectively, and each internal node vi cor-
responds to the document?s ith sentence si. We can
set the individual scores ind1(si) to PrNBsub (si) and
ind2(si) to 1 ? PrNBsub (si), as shown in Figure 3,
where PrNBsub (s) denotes Naive Bayes? estimate of
the probability that sentence s is subjective; or, we
can use the weights produced by the SVM classi-
fier instead.7 If we set al the association scores
to zero, then the minimum-cut classification of the
sentences is the same as that of the basic subjectiv-
ity detector. Alternatively, we incorporate the de-
gree of proximity between pairs of sentences, con-
trolled by three parameters. The threshold T spec-
ifies the maximum distance two sentences can be
separated by and still be considered proximal. The
5We therefore could not directly evaluate sentence-
classification accuracy on the polarity dataset.
6Available at www.cs.cornell.edu/people/pabo/movie-
review-data/ , sentence corpus version 1.0.
7We converted SVM output di, which is a signed distance
(negative=objective) from the separating hyperplane, to non-
negative numbers by
ind1(si)
def
=
{
1 di > 2;
(2 + di)/4 ?2 ? di ? 2;
0 di < ?2.
and ind2(si) = 1 ? ind1(si). Note that scaling is employed
only for consistency; the algorithm itself does not require prob-
abilities for individual scores.
non-increasing function f(d) specifies how the in-
fluence of proximal sentences decays with respect to
distance d; in our experiments, we tried f(d) = 1,
e1?d, and 1/d2. The constant c controls the relative
influence of the association scores: a larger c makes
the minimum-cut algorithm more loath to put prox-
imal sentences in different classes. With these in
hand8, we set (for j > i)
assoc(si, sj)
def
=
{
f(j ? i) ? c if (j ? i) ? T ;
0 otherwise.
4 Experimental Results
Below, we report average accuracies computed by
ten-fold cross-validation over the polarity dataset.
Section 4.1 examines our basic subjectivity extrac-
tion algorithms, which are based on individual-
sentence predictions alone. Section 4.2 evaluates
the more sophisticated form of subjectivity extrac-
tion that incorporates context information via the
minimum-cut paradigm.
As we will see, the use of subjectivity extracts
can in the best case provide satisfying improve-
ment in polarity classification, and otherwise can
at least yield polarity-classification accuracies indis-
tinguishable from employing the full review. At the
same time, the extracts we create are both smaller
on average than the original document and more
effective as input to a default polarity classifier
than the same-length counterparts produced by stan-
dard summarization tactics (e.g., first- or last-N sen-
tences). We therefore conclude that subjectivity ex-
traction produces effective summaries of document
sentiment.
4.1 Basic subjectivity extraction
As noted in Section 3, both Naive Bayes and SVMs
can be trained on our subjectivity dataset and then
used as a basic subjectivity detector. The former has
somewhat better average ten-fold cross-validation
performance on the subjectivity dataset (92% vs.
90%), and so for space reasons, our initial discus-
sions will focus on the results attained via NB sub-
jectivity detection.
Employing Naive Bayes as a subjectivity detec-
tor (ExtractNB) in conjunction with a Naive Bayes
document-level polarity classifier achieves 86.4%
accuracy.9 This is a clear improvement over the
82.8% that results when no extraction is applied
8Parameter training is driven by optimizing the performance
of the downstream polarity classifier rather than the detector
itself because the subjectivity dataset?s sentences come from
different reviews, and so are never proximal.
9This result and others are depicted in Figure 5; for now,
consider only the y-axis in those plots.
.
.
.
.
.
.
subsub
NB NBs1
s2
s3
s4
s_n
     
    
constructgraph computemin. cut extractcreate s1
s4
m?sentence extract(m<=n)


 
 


 
 
			
			

 


 

n?sentence review
v1
v2
s
v3
edge crossing the cut
v2
v3
v1
ts
v n
t
v n
proximity link
individual subjectivity?probability linkPr
1?Pr   (s1)Pr   (s1)

    
Figure 3: Graph-cut-based creation of subjective extracts.
(Full review); indeed, the difference is highly sta-
tistically significant (p < 0.01, paired t-test). With
SVMs as the polarity classifier instead, the Full re-
view performance rises to 87.15%, but comparison
via the paired t-test reveals that this is statistically
indistinguishable from the 86.4% that is achieved by
running the SVM polarity classifier on ExtractNB
input. (More improvements to extraction perfor-
mance are reported later in this section.)
These findings indicate10 that the extracts pre-
serve (and, in the NB polarity-classifier case, appar-
ently clarify) the sentiment information in the orig-
inating documents, and thus are good summaries
from the polarity-classification point of view. Fur-
ther support comes from a ?flipping? experiment:
if we give as input to the default polarity classifier
an extract consisting of the sentences labeled ob-
jective, accuracy drops dramatically to 71% for NB
and 67% for SVMs. This confirms our hypothesis
that sentences discarded by the subjectivity extrac-
tion process are indeed much less indicative of sen-
timent polarity.
Moreover, the subjectivity extracts are much
more compact than the original documents (an im-
portant feature for a summary to have): they contain
on average only about 60% of the source reviews?
words. (This word preservation rate is plotted along
the x-axis in the graphs in Figure 5.) This prompts
us to study how much reduction of the original doc-
uments subjectivity detectors can perform and still
accurately represent the texts? sentiment informa-
tion.
We can create subjectivity extracts of varying
lengths by taking just the N most subjective sen-
tences11 from the originating review. As one base-
10Recall that direct evidence is not available because the po-
larity dataset?s sentences lack subjectivity labels.
11These are the N sentences assigned the highest probability
by the basic NB detector, regardless of whether their probabil-
line to compare against, we take the canonical sum-
marization standard of extracting the first N sen-
tences ? in general settings, authors often be-
gin documents with an overview. We also con-
sider the last N sentences: in many documents,
concluding material may be a good summary, and
www.rottentomatoes.com tends to select ?snippets?
from the end of movie reviews (Beineke et al,
2004). Finally, as a sanity check, we include results
from the N least subjective sentences according to
Naive Bayes.
Figure 4 shows the polarity classifier results as
N ranges between 1 and 40. Our first observation
is that the NB detector provides very good ?bang
for the buck?: with subjectivity extracts containing
as few as 15 sentences, accuracy is quite close to
what one gets if the entire review is used. In fact,
for the NB polarity classifier, just using the 5 most
subjective sentences is almost as informative as the
Full review while containing on average only about
22% of the source reviews? words.
Also, it so happens that at N = 30, performance
is actually slightly better than (but statistically in-
distinguishable from) Full review even when the
SVM default polarity classifier is used (87.2% vs.
87.15%).12 This suggests potentially effective ex-
traction alternatives other than using a fixed proba-
bility threshold (which resulted in the lower accu-
racy of 86.4% reported above).
Furthermore, we see in Figure 4 that the N most-
subjective-sentences method generally outperforms
the other baseline summarization methods (which
perhaps suggests that sentiment summarization can-
not be treated the same as topic-based summariza-
ities exceed 50% and so would actually be classified as subjec-
tive by Naive Bayes. For reviews with fewer than N sentences,
the entire review will be returned.
12Note that roughly half of the documents in the polarity
dataset contain more than 30 sentences (average=32.3, standard
deviation 15).
 55
 60
 65
 70
 75
 80
 85
 90
 1  5  10  15  20  25  30  35  40
Av
era
ge
 ac
cur
acy
N
Accuracy for N-sentence abstracts (def =  NB)
most subjective N sentenceslast N sentencesfirst N sentencesleast subjective N sentencesFull review
 55
 60
 65
 70
 75
 80
 85
 90
 1  5  10  15  20  25  30  35  40
Av
era
ge
 ac
cur
acy
N
Accuracy for N-sentence abstracts (def = SVM)
most subjective N sentenceslast N sentencesfirst N sentencesleast subjective N sentencesFull review
Figure 4: Accuracies using N-sentence extracts for NB (left) and SVM (right) default polarity classifiers.
 83
 83.5
 84
 84.5
 85
 85.5
 86
 86.5
 87
 0.6  0.7  0.8  0.9  1  1.1
Av
era
ge
 ac
cur
acy
% of words extracted
Accuracy for subjective abstracts (def = NB)
difference in accuracy 
ExtractSVM+Prox
ExtractNB+ProxExtractNB
ExtractSVM
not statistically significant
Full Review
indicates statistically significant
improvement in accuracy
 83
 83.5
 84
 84.5
 85
 85.5
 86
 86.5
 87
 0.6  0.7  0.8  0.9  1  1.1
Av
era
ge
 ac
cur
acy
% of words extracted
Accuracy for subjective abstracts (def = SVM)
difference in accuracy ExtractNB+Prox
ExtractSVM+Prox
ExtractSVM
ExtractNB not statistically significant
Full Review
improvement in accuracy
indicates statistically significant
Figure 5: Word preservation rate vs. accuracy, NB (left) and SVMs (right) as default polarity classifiers.
Also indicated are results for some statistical significance tests.
tion, although this conjecture would need to be veri-
fied on other domains and data). It?s also interesting
to observe how much better the last N sentences are
than the first N sentences; this may reflect a (hardly
surprising) tendency for movie-review authors to
place plot descriptions at the beginning rather than
the end of the text and conclude with overtly opin-
ionated statements.
4.2 Incorporating context information
The previous section demonstrated the value of
subjectivity detection. We now examine whether
context information, particularly regarding sentence
proximity, can further improve subjectivity extrac-
tion. As discussed in Section 2.2 and 3, con-
textual constraints are easily incorporated via the
minimum-cut formalism but are not natural inputs
for standard Naive Bayes and SVMs.
Figure 5 shows the effect of adding in
proximity information. ExtractNB+Prox and
ExtractSVM+Prox are the graph-based subjectivity
detectors using Naive Bayes and SVMs, respec-
tively, for the individual scores; we depict the
best performance achieved by a single setting of
the three proximity-related edge-weight parameters
over all ten data folds13 (parameter selection was
not a focus of the current work). The two compar-
isons we are most interested in are ExtractNB+Prox
versus ExtractNB and ExtractSVM+Prox versus
ExtractSVM.
We see that the context-aware graph-based sub-
jectivity detectors tend to create extracts that are
more informative (statistically significant so (paired
t-test) for SVM subjectivity detectors only), al-
though these extracts are longer than their context-
blind counterparts. We note that the performance
13Parameters are chosen from T ? {1, 2, 3}, f(d) ?
{1, e1?d, 1/d2}, and c ? [0, 1] at intervals of 0.1.
enhancements cannot be attributed entirely to the
mere inclusion of more sentences regardless of
whether they are subjective or not ? one counter-
argument is that Full review yielded substantially
worse results for the NB default polarity classifier?
and at any rate, the graph-derived extracts are still
substantially more concise than the full texts.
Now, while incorporating a bias for assigning
nearby sentences to the same category into NB and
SVM subjectivity detectors seems to require some
non-obvious feature engineering, we also wish
to investigate whether our graph-based paradigm
makes better use of contextual constraints that can
be (more or less) easily encoded into the input of
standard classifiers. For illustrative purposes, we
consider paragraph-boundary information, looking
only at SVM subjectivity detection for simplicity?s
sake.
It seems intuitively plausible that paragraph
boundaries (an approximation to discourse bound-
aries) loosen coherence constraints between nearby
sentences. To capture this notion for minimum-cut-
based classification, we can simply reduce the as-
sociation scores for all pairs of sentences that oc-
cur in different paragraphs by multiplying them by
a cross-paragraph-boundary weight w ? [0, 1]. For
standard classifiers, we can employ the trick of hav-
ing the detector treat paragraphs, rather than sen-
tences, as the basic unit to be labeled. This en-
ables the standard classifier to utilize coherence be-
tween sentences in the same paragraph; on the other
hand, it also (probably unavoidably) poses a hard
constraint that all of a paragraph?s sentences get the
same label, which increases noise sensitivity.14 Our
experiments reveal the graph-cut formulation to be
the better approach: for both default polarity clas-
sifiers (NB and SVM), some choice of parameters
(including w) for ExtractSVM+Prox yields statisti-
cally significant improvement over its paragraph-
unit non-graph counterpart (NB: 86.4% vs. 85.2%;
SVM: 86.15% vs. 85.45%).
5 Conclusions
We examined the relation between subjectivity de-
tection and polarity classification, showing that sub-
jectivity detection can compress reviews into much
shorter extracts that still retain polarity information
at a level comparable to that of the full review. In
fact, for the Naive Bayes polarity classifier, the sub-
jectivity extracts are shown to be more effective in-
put than the originating document, which suggests
14For example, in the data we used, boundaries may have
been missed due to malformed html.
that they are not only shorter, but also ?cleaner? rep-
resentations of the intended polarity.
We have also shown that employing the
minimum-cut framework results in the develop-
ment of efficient algorithms for sentiment analy-
sis. Utilizing contextual information via this frame-
work can lead to statistically significant improve-
ment in polarity-classification accuracy. Directions
for future research include developing parameter-
selection techniques, incorporating other sources of
contextual cues besides sentence proximity, and in-
vestigating other means for modeling such informa-
tion.
Acknowledgments
We thank Eric Breck, Claire Cardie, Rich Caruana,
Yejin Choi, Shimon Edelman, Thorsten Joachims,
Jon Kleinberg, Oren Kurland, Art Munson, Vincent
Ng, Fernando Pereira, Ves Stoyanov, Ramin Zabih,
and the anonymous reviewers for helpful comments.
This paper is based upon work supported in part
by the National Science Foundation under grants
ITR/IM IIS-0081334 and IIS-0329064, a Cornell
Graduate Fellowship in Cognitive Studies, and by
an Alfred P. Sloan Research Fellowship. Any opin-
ions, findings, and conclusions or recommendations
expressed above are those of the authors and do not
necessarily reflect the views of the National Science
Foundation or Sloan Foundation.
References
Agrawal, Rakesh, Sridhar Rajagopalan, Ramakrish-
nan Srikant, and Yirong Xu. 2003. Mining news-
groups using networks arising from social behav-
ior. In WWW, pages 529?535.
Ahuja, Ravindra, Thomas L. Magnanti, and
James B. Orlin. 1993. Network Flows: Theory,
Algorithms, and Applications. Prentice Hall.
Beineke, Philip, Trevor Hastie, Christopher Man-
ning, and Shivakumar Vaithyanathan. 2004.
Exploring sentiment summarization. In AAAI
Spring Symposium on Exploring Attitude and Af-
fect in Text: Theories and Applications (AAAI
tech report SS-04-07).
Blum, Avrim and Shuchi Chawla. 2001. Learning
from labeled and unlabeled data using graph min-
cuts. In Intl. Conf. on Machine Learning (ICML),
pages 19?26.
Boykov, Yuri, Olga Veksler, and Ramin Zabih.
1999. Fast approximate energy minimization via
graph cuts. In Intl. Conf. on Computer Vision
(ICCV), pages 377?384. Journal version in IEEE
Trans. Pattern Analysis and Machine Intelligence
(PAMI) 23(11):1222?1239, 2001.
Cardie, Claire, Janyce Wiebe, Theresa Wilson, and
Diane Litman. 2003. Combining low-level and
summary representations of opinions for multi-
perspective question answering. In AAAI Spring
Symposium on New Directions in Question An-
swering, pages 20?27.
Cormen, Thomas H., Charles E. Leiserson, and
Ronald L. Rivest. 1990. Introduction to Algo-
rithms. MIT Press.
Das, Sanjiv and Mike Chen. 2001. Yahoo! for
Amazon: Extracting market sentiment from stock
message boards. In Asia Pacific Finance Associ-
ation Annual Conf. (APFA).
Dave, Kushal, Steve Lawrence, and David M. Pen-
nock. 2003. Mining the peanut gallery: Opinion
extraction and semantic classification of product
reviews. In WWW, pages 519?528.
Dini, Luca and Giampaolo Mazzini. 2002. Opin-
ion classification through information extraction.
In Intl. Conf. on Data Mining Methods and
Databases for Engineering, Finance and Other
Fields, pages 299?310.
Durbin, Stephen D., J. Neal Richter, and Doug
Warner. 2003. A system for affective rating of
texts. In KDD Wksp. on Operational Text Classi-
fication Systems (OTC-3).
Hatzivassiloglou, Vasileios and Kathleen Mc-
Keown. 1997. Predicting the semantic orienta-
tion of adjectives. In 35th ACL/8th EACL, pages
174?181.
Joachims, Thorsten. 2003. Transductive learning
via spectral graph partitioning. In Intl. Conf. on
Machine Learning (ICML).
Liu, Hugo, Henry Lieberman, and Ted Selker.
2003. A model of textual affect sensing using
real-world knowledge. In Intelligent User Inter-
faces (IUI), pages 125?132.
Montes-y-Go?mez, Manuel, Aurelio Lo?pez-Lo?pez,
and Alexander Gelbukh. 1999. Text mining as a
social thermometer. In IJCAI Wksp. on Text Min-
ing, pages 103?107.
Morinaga, Satoshi, Kenji Yamanishi, Kenji Tateishi,
and Toshikazu Fukushima. 2002. Mining prod-
uct reputations on the web. In KDD, pages 341?
349. Industry track.
Pang, Bo, Lillian Lee, and Shivakumar
Vaithyanathan. 2002. Thumbs up? Senti-
ment classification using machine learning
techniques. In EMNLP, pages 79?86.
Qu, Yan, James Shanahan, and Janyce Wiebe, edi-
tors. 2004. AAAI Spring Symposium on Explor-
ing Attitude and Affect in Text: Theories and Ap-
plications. AAAI technical report SS-04-07.
Riloff, Ellen and Janyce Wiebe. 2003. Learning
extraction patterns for subjective expressions. In
EMNLP.
Riloff, Ellen, Janyce Wiebe, and Theresa Wilson.
2003. Learning subjective nouns using extraction
pattern bootstrapping. In Conf. on Natural Lan-
guage Learning (CoNLL), pages 25?32.
Subasic, Pero and Alison Huettner. 2001. Af-
fect analysis of text using fuzzy semantic typing.
IEEE Trans. Fuzzy Systems, 9(4):483?496.
Tong, Richard M. 2001. An operational system for
detecting and tracking opinions in on-line discus-
sion. SIGIR Wksp. on Operational Text Classifi-
cation.
Turney, Peter. 2002. Thumbs up or thumbs down?
Semantic orientation applied to unsupervised
classification of reviews. In ACL, pages 417?424.
Wiebe, Janyce M. 1994. Tracking point of view in
narrative. Computational Linguistics, 20(2):233?
287.
Yi, Jeonghee, Tetsuya Nasukawa, Razvan Bunescu,
and Wayne Niblack. 2003. Sentiment analyzer:
Extracting sentiments about a given topic using
natural language processing techniques. In IEEE
Intl. Conf. on Data Mining (ICDM).
Yu, Hong and Vasileios Hatzivassiloglou. 2003.
Towards answering opinion questions: Separat-
ing facts from opinions and identifying the polar-
ity of opinion sentences. In EMNLP.
A non-programming introduction to computer science via
NLP, IR, and AI
Lillian Lee
Department of Computer Science
Cornell University
Ithaca, NY, USA, 14853-7501
llee@cs.cornell.edu
Abstract
This paper describes a new Cornell
University course serving as a non-
programming introduction to com-
puter science, with natural language
processing and information retrieval
forming a crucial part of the syllabus.
Material was drawn from a wide vari-
ety of topics (such as theories of dis-
course structure and random graph
models of the World Wide Web) and
presented at some technical depth, but
was massaged to make it suitable for
a freshman-level course. Student feed-
back from the first running of the class
was overall quite positive, and a grant
from the GE Fund has been awarded
to further support the course?s devel-
opment and goals.
1 Introduction
Algorithmic concepts and programming tech-
niques from computer science are very useful to
researchers in natural language processing. To
ensure the continued strength of our field, then,
it is important to encourage undergraduates in-
terested in NLP to take courses conveying com-
puter science content. This is especially true for
students not intending to become computer sci-
ence majors.
Usually, one points beginning students inter-
ested in NLP towards the first programming
course (henceforth ?CS101?). However, at many
institutions, CS101 is mandatory for a large por-
tion of the undergraduates (e.g., all engineering
students) and is designed primarily to trans-
mit specific programming skills. Experience
suggests that a significant fraction of students
find CS101?s emphasis on skills rather than con-
cepts unstimulating, and therefore decide not to
take further computer science courses. Unfortu-
nately, down the road this results in fewer en-
tering NLP graduate students having been ed-
ucated in important advanced computer-science
concepts. Furthermore, fewer students are intro-
duced to NLP at all, since the subject is often
presented only in upper-level computer-science
classes.
In an attempt to combat these problems, I cre-
ated a new freshman-level course, Computation,
Information, and Intelligence1, designed to in-
troduce entering undergraduates to some of the
ideas and goals of AI (and hence computer sci-
ence). The premise was that if freshmen first
learned something of what artificial intelligence
is about, what the technical issues are, what
has been accomplished, and what remains to be
done, then they would be much more motivated
when taking CS101, because they would under-
stand what they are learning programming for.
Three major design decisions were made at
the outset:
? No programming: Teaching elementary pro-
gramming would be a needless reduplication of
effort, since programming pedagogy is already
well-honed in CS101 and other such classes.
Moreover, it was desirable to attract students
having little or no programming experience: the
new course would offer them an opportunity for
1http://www.cs.cornell.edu/courses/cs172/2001fa
                     July 2002, pp. 33-38.  Association for Computational Linguistics.
              Natural Language Processing and Computational Linguistics, Philadelphia,
         Proceedings of the Workshop on Effective Tools and Methodologies for Teaching
initial exploration at a conceptual level. Indeed,
for the first edition of the class, students with
programming experience were actively discour-
aged from enrolling, in order to ensure a more
level playing field for those without such back-
ground.2
? Emphasis on technically challenging material:
Although no programming would be involved,
the course would nevertheless bring students
face-to-face with substantial technical material
requiring mathematical and abstract reasoning
(indeed, topics from graduate courses in NLP
and machine learning were incorporated). To
achieve this aim, the main coursework would
involve challenging pencil-and-paper problems
with significant mathematical content.3
Of course, one had to be mindful that the in-
tended audience was college freshmen, and thus
one could only assume basic calculus as a pre-
requisite. Even working within this constraint,
though, it was possible to craft problem sets
and exams in which students explored concepts
in some depth; the typical homework problem
asked them not just to demonstrate comprehen-
sion of lecture material but to investigate alter-
native proposals. Sample questions are included
in the appendix.
? Substantial NLP and IR content:4 Because
many students have a lot of experience with
search engines, and, of course, all students have
a great deal of experience with language, NLP
and IR are topics that freshmen can easily relate
to without being introduced to a lot of back-
ground first.
2Students who have programmed previously are more
likely to happily enroll in further computer science
courses, and thus are already well-served by the standard
curriculum.
3An alternative to a technically- and mathematically-
oriented course would have been a ?computers and the
humanities? class, but Cornell already offers classes on
the history of computing, the philosophy of AI, and the
social implications of living in an information society.
One of the goals for Computation, Information, and In-
telligence was that students learn what ?doing AI? is re-
ally like.
4In this class, I treated information retrieval as a spe-
cial type of NLP for simplicity?s sake.
2 Course content
The course title, Computation, Information,
and Intelligence, reflects its organization, which
was inspired by Herb Simon?s (1977) statement
that ?Knowledge without appropriate proce-
dures for its use is dumb, and procedure without
suitable knowledge is blind.? More specifically,
the first 15 lectures were mainly concerned with
algorithms and computation (game-tree search,
perceptron learning, nearest-neighbor learning,
Turing machines, and the halting problem). For
the purposes of this workshop, though, this pa-
per focuses on the remaining 19 lectures, which
were devoted to information, and in particular,
to IR and NLP. As mentioned above, sample
homework problems for each of the units listed
below can be found in the appendix.
We now outline the major topics of the last
22 lectures. Observe that IR was presented be-
fore NLP, because the former was treated as a
special, simpler case of the latter; that is, we
first treated documents as bags of words before
considering relations between words.
Document retrieval [3 lectures]. Students
were first introduced to the Boolean query re-
trieval model, and hence to the concepts of index
data structures (arrays and B-trees) and binary
search. We then moved on to the vector space
model5, and considered simple term-weighting
schemes like tf-idf.
The Web [4 lectures]. After noting how Van-
nevar Bush?s (1945) famous ?Memex? article an-
ticipated the development of the Web, we stud-
ied the Web?s global topology, briefly consider-
ing the implications of its so-called ?bow-tie?
structure (Broder et al, 2000) for web crawlers
? students were thus introduced to graph-
theoretic notions of strongly-connected compo-
nents and node degrees. Then, we investigated
Kleinberg?s (1998) hubs and authorities algo-
rithm as an alternative to mere in-link counting:
5This does require some linear algebra background in
that one needs to compute inner products, but this was
covered in the section of the course on perceptrons. Since
trigonometry is actually relatively fresh in the minds of
first-year students, their geometric intuitions tended to
serve them fairly well.
fortunately, the method is simple enough that
students could engage in hand simulations. Fi-
nally, we looked at the suitability of various ran-
dom graph generation models (e.g., the ?rich-
get-richer? (Baraba?si et al, 1999) and ?copy-
ing? models (Kumar et al, 2000)) for capturing
the local structure of the Web, such as the phe-
nomenon of in-degree distributions following a
power law ? conveniently, these concepts could
be presented in such a way that only required
the students to have intuitive notions of proba-
bility and the ability to take derivatives.
Language structure [7 lectures]. Relying on
students? instincts about and experience with
language, we considered evidence for the exis-
tence of hidden language structure; such clues
included possible and impossible syntactic and
discourse ambiguities, and movement, prosody
and pause cues for constituents. To describe
this structure, we formally defined context-free
grammars. We then showed how (a tiny frag-
ment of) X-bar theory can be modeled by a
context-free grammar and, using its structural
assignments and the notion of heads of con-
stituents, accounted for some of the ambiguities
and non-ambiguities in the linguistic examples
we previously examined.
The discussion of context-free grammars nat-
urally led us to pushdown automata (which pro-
vided a nice contrast to the Turing machines
we studied earlier in the course). And, hav-
ing thus introduced stacks, we then investigated
the Grosz and Sidner (1986) stack-based theory
of discourse structure, showing that language
structures exist at granularities beyond the sen-
tence level.
Statistical language processing [6 lectures]
We began this unit by considering word fre-
quency distributions, and in particular, Zipf?s
law ? note that our having studied power-law
distributions in the Web unit greatly facilitated
this discussion. In fact, because we had pre-
viously investigated generative models for the
Web, it was natural to consider Miller?s (1957)
?monkeys? model which demonstrates that very
simple generative models can account for Zipf?s
law. Next, we looked at methods taking advan-
tage of statistical regularities, including the IBM
Candide statistical machine translation system,
following Knight?s (1999) tutorial and treating
probabilities as weights. It was interesting to
point out parallels with the hubs and authorities
algorithm ? both are iterative update proce-
dures with auxiliary information (alignments in
one case, hubs in the other). We also discussed
an intuitive algorithm for Japanese segmenta-
tion drawn from one of my own recent research
collaborations (Ando and Lee, 2000), and how
word statistics were applied to determining the
authorship of the Federalist Papers (Mosteller
and Wallace, 1984). We concluded with an ex-
amination of human statistical learning, focus-
ing on recent evidence indicating that human
infants can use statistics when learning to seg-
ment continuous speech into words (Saffran et
al., 1996).
The Turing test [2 lectures] Finally, we
ended the course with a consideration of intel-
ligence in the large. In particular, we focused
on Turing?s (1950) proposal of the ?imitation
game?, which can be interpreted as one of the
first appearances of the claim that natural lan-
guage processing is ?AI-complete?, and Searle?s
(1980) ?Chinese Room? rebuttal that fluent lan-
guage behavior is not a sufficient indication of
intelligence. Then, we concluded with an exam-
ination of the first running of the Restricted Tur-
ing Test (Shieber, 1994), which served as an ob-
ject lesson as to the importance of careful eval-
uation in NLP, or indeed any science.
3 Experience
Twenty-three students enrolled, with only one-
third initially expressing interest in majoring in
computer science. By the end, I was approached
by four students asking if there were research op-
portunities available in the topics we had cov-
ered; interestingly, one of these students had
originally intended to major in electrical engi-
neering. Furthermore, because of the class?s
promise in drawing students into further com-
puter science study, the GE Fund awarded a
grant for the purpose of bringing in a senior out-
side speaker and supporting teaching assistants
in future years.
One issue that remains to be resolved is the
lack, to my knowledge, of a textbook or text-
books that would both cover the syllabus topics
and employ a level of presentation suitable for
freshmen. For first-year students to learn effec-
tively, some sort of reference seems crucial, but
a significant portion of the course material was
drawn from research papers that would proba-
bly be too difficult. In the next edition of the
course, I plan to write up and distributeformal
lecture notes.
Overall, although Computation, Information,
and Intelligence proved quite challenging for the
students, for the most part they felt that they
had learned a lot from the experience, and based
on this evidence and the points outlined in the
previous paragraph, I believe that the course did
make definite progress towards its goal of inter-
esting students in taking further computer sci-
ence courses, especially in AI, IR, and NLP.
Acknowledgments
I thank my chair Charles Van Loan for en-
couraging me to develop the course described
in this paper, for discussing many aspects of
the class with me, and for contacting the GE
Fund, which I thank for supplying a grant sup-
porting the future development of the class.
Thanks to Jon Kleinberg for many helpful dis-
cussions, especially regarding curriculum con-
tent, and to the anonymous reviewers for their
feedback. Finally, I am very grateful to my
teaching assistants, Amanda Holland-Minkley,
Milo Polte, and Neeta Rattan, who helped im-
mensely in making the first outing of the course
run smoothly.
References
Rie Kubota Ando and Lillian Lee. 2000. Mostly-
unsupervised statistical segmentation of Japanese.
In First Conference of the North American Chap-
ter of the Association for Computational Linguis-
tics (NAACL), pages 241?248.
Albert-La?szlo? Baraba?si, Re?ka Albert, and Hawoong
Jeong. 1999. Mean-field theory for scale-free ran-
dom networks. Physica, 272:173?187.
Andrei Broder, Ravi Kumar, Farzin Maghoul, Prab-
hakar Raghavan, Sridhar Rajagopalan, Raymie
Stata, Andrew Tomkins, and Janet Wiener. 2000.
Graph structure in the web. In Proceedings of the
Ninth International World Wide Web Conference,
pages 309?430.
Vannevar Bush. 1945. As we may think. The At-
lantic Monthly, 176(1):101?108.
Ralph Grishman. 1986. Computational Linguistics:
An Introduction. Studies in Natural Language
Processing. Cambridge.
Barbara J. Grosz and Candace L. Sidner. 1986. At-
tention, intentions, and the structure of discourse.
Computational Linguistics, 12(3):175?204.
Jon Kleinberg. 1998. Authoritative sources in a hy-
perlinked environment. In Proceedings of the 9th
ACM-SIAM Symposium on Discrete Algorithms
(SODA), pages 668?677.
Kevin Knight. 1999. A statistical MT tutorial work-
book. http://www.isi.edu/natural-language/-
mt/wkbk.rtf, August.
Ravi Kumar, Prabhakar Raghavan, Sridhar Ra-
jagopolan, D. Sivakumar, Andrew Tomkins, and
Eli Upfal. 2000. Stochastic models for the web
graph. In Proceedings of the 41st IEEE Sympo-
sium on the Foundations of Computer Science,
pages 57?65.
George A. Miller. 1957. Some effects of intermittent
silence. American Journal of Psychology, 70:311?
313.
Frederick Mosteller and David L. Wallace. 1984. Ap-
plied Bayesian and Classical Inference: The Case
of the Federalist Papers. Springer-Verlag.
Jenny R. Saffran, Richard N. Aslin, and Elissa L.
Newport. 1996. Statistical learning by 8-month-
old infants. Science, 274(5294):1926?1928, De-
cember.
John R. Searle. 1980. Minds, brains, and programs.
Behavioral and Brain Sciences, 3(3):417?457.
Stuart M. Shieber. 1994. Lessons from a re-
stricted Turing test. Communications of the
ACM, 37(6):70?78.
Herb A. Simon. 1977. Artificial intelligence systems
that understand. In Proceedings of the Fifth In-
ternational Joint Conference on Artificial Intelli-
gence, volume 2, pages 1059?1073.
Alan M. Turing. 1950. Computing machinery and
intelligence. Mind, LIX:433?60.
Appendix: sample homework
problems
IR unit
For simplicity, in this question, let the document
vector entries be the term frequencies normal-
ized by vector length.
Suppose someone proposes to you to inte-
grate negative information by converting a query
q = ?x1, x2, . . . , xj ,?y1,?y2, . . . ,?yk? to an m-
dimensional query vector ??q as follows: the ith
entry qi of ??q is:
qi =
?
?
?
0, wi not in the query
1, wi is a positive query term
?1, wi is a negative query term
They claim the -1 entries in the query vector will
prevent documents that contain negative query
terms from being ranked highly.
Show that this claim is incorrect, as follows.
Let the entire three-word vocabulary be w1 =
alligator, w2 = bat, and w3 = cat, and let q =
?alligator, bat, ?cat?. Give two documents d1
and d2 such that
? d1 and d2 both contain exactly 8 words (ob-
viously, some will be repetitions);
? d1 does not contain the word ?cat?;
? d2 does contain the word ?cat?; and yet,
? d2 is ranked more highly with respect to q
than d1 is.
Explain your answer; remember to show the
document vectors corresponding to d1 and d2,
the query vector, and how you computed them.
Make sure the documents you choose and corre-
sponding document vectors satisfy all the con-
straints of this problem, including how the doc-
uments get transformed into document vectors.
Web unit
In this question, we engage in some preliminary
explorations as to how many ?colluding? web
pages might be needed to ?spam? the hubs-and-
authorities algorithm (henceforth HA).
Let m and n be two whole numbers bigger
than 1 (m and n need not be equal, although
they could be). Consider the following set of
web pages (all presumably on the same topic):
Q
Y
Y
Y
Z
Z
Z
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
1
2
P
P
P
m
1
2
n
1
2
n
That is, all of the m Pi pages point to Q, and all
of the n Yj pages point to all of the n Zk pages.
(a) Let m = 5 and n = 3 (thus, m = 2n ? 1,
and in particularm > n), and suppose HA is run
for two iterations. What are the best hub and
authority pages? Explain your answers, show-
ing your computations of the hub and authority
scores of every web page (using the tabular for-
mat from class is fine).
(b) Now, let n be some whole number greater
than 1, and let m = n2. Suppose HA is run
for two iterations in this situation. What are
the best hub and authority pages? Explain your
answers, showing your computations of the hub
and authority scores of every web page. (Note:
in this problem, you don?t get to choose n; we?re
trying to see what happens in general if there are
a quadratic number of colluding web pages. So
treat n as an unknown but fixed constant.)
Language structure unit
Recall the Grishman (1986) ?next train to
Boston? dialog:
(1) A: Do you know when the next
train to Boston leaves?
(2) B: Yes.
(3) A: I want to know when the
train to Boston leaves.
(4) B: I understand.
(a) Using the Grosz/Sidner model, analyze
the discourse structure of the entire conversa-
tion from the point of view of speaker A. That
is, give the discourse segments (i.e., ?DS1 con-
sists of sentences 1 and 3, and DS2 consists of
sentences 2 and 4?), the corresponding discourse
segment purposes, and the intentional structure
of the conversation. Then, show what the focus
stack is after each sentence is uttered. Explain
how you determined your answers.
(b) Repeat the above subproblem, but from
the point of view of speaker B.
(c) Would your answers to the previous sub-
problem change if sentence (4) had been ?Why
are you telling me these things?? Does the
Grosz/Sidner model adequately account for this
case? Explain.
Statistical language processing unit
In this problem, we explicitly derive a type of
power-law behavior in the ?Miller?s monkeys?
model (henceforth MMM) from class. First, a
useful fact: for any fixed integers n > 1 and
k > 0,
k
?
i=1
ni = n
k+1 ? n
n? 1 .
In each of the following subproblems, we rank
the ?words? (remember that ?zz? is a ?word?
in the MMM) by their probability, rather than
by corpus frequency. Also, j refers to some fixed
but unknown integer greater than 1; hence, your
answers should generally be functions of j.
(a) Show mathematically that the number of
words that are shorter than j letters long is
26
25
(
26j?1 ? 1
)
.
(b) Compute the maximum possible rank for a
word that is j letters long; explain your answer.
(c) Using your answers to the previous sub-
problems, find the function AR(j), the aver-
age rank of a word that is j letters long, show-
ing your work. (For example, you might say
?AR(j) = 4? j?.)
(d) The probability of a word of length j is
P (j) = 127?
( 1
27
)j (that we aren?t combining like
terms is meant to be helpful ...). Show mathe-
matically that the AR(j) function you computed
above and the probability function P (j) have a
particularly simple power-law relationship:
AR(j) ? ?? 1P (j)
for some constant ? that doesn?t depend on j.
You may make some reasonable approximations,
for example, saying that n+1n+2 is close enough to
1 that we can replace it by 1 for argument?s
sake; but please make all such approximations
explicit.
Thumbs up? Sentiment Classification using Machine Learning
Techniques
Bo Pang and Lillian Lee
Department of Computer Science
Cornell University
Ithaca, NY 14853 USA
{pabo,llee}@cs.cornell.edu
Shivakumar Vaithyanathan
IBM Almaden Research Center
650 Harry Rd.
San Jose, CA 95120 USA
shiv@almaden.ibm.com
Abstract
We consider the problem of classifying doc-
uments not by topic, but by overall senti-
ment, e.g., determining whether a review
is positive or negative. Using movie re-
views as data, we find that standard ma-
chine learning techniques definitively out-
perform human-produced baselines. How-
ever, the three machine learning methods
we employed (Naive Bayes, maximum en-
tropy classification, and support vector ma-
chines) do not perform as well on sentiment
classification as on traditional topic-based
categorization. We conclude by examining
factors that make the sentiment classifica-
tion problem more challenging.
1 Introduction
Today, very large amounts of information are avail-
able in on-line documents. As part of the effort to
better organize this information for users, researchers
have been actively investigating the problem of au-
tomatic text categorization.
The bulk of such work has focused on topical cat-
egorization, attempting to sort documents accord-
ing to their subject matter (e.g., sports vs. poli-
tics). However, recent years have seen rapid growth
in on-line discussion groups and review sites (e.g.,
the New York Times? Books web page) where a cru-
cial characteristic of the posted articles is their senti-
ment, or overall opinion towards the subject matter
? for example, whether a product review is pos-
itive or negative. Labeling these articles with their
sentiment would provide succinct summaries to read-
ers; indeed, these labels are part of the appeal and
value-add of such sites as www.rottentomatoes.com,
which both labels movie reviews that do not con-
tain explicit rating indicators and normalizes the
different rating schemes that individual reviewers
use. Sentiment classification would also be helpful in
business intelligence applications (e.g. MindfulEye?s
Lexant system1) and recommender systems (e.g.,
Terveen et al (1997), Tatemura (2000)), where user
input and feedback could be quickly summarized; in-
deed, in general, free-form survey responses given in
natural language format could be processed using
sentiment categorization. Moreover, there are also
potential applications to message filtering; for exam-
ple, one might be able to use sentiment information
to recognize and discard ?flames?(Spertus, 1997).
In this paper, we examine the effectiveness of ap-
plying machine learning techniques to the sentiment
classification problem. A challenging aspect of this
problem that seems to distinguish it from traditional
topic-based classification is that while topics are of-
ten identifiable by keywords alone, sentiment can be
expressed in a more subtle manner. For example, the
sentence ?How could anyone sit through this movie??
contains no single word that is obviously negative.
(See Section 7 for more examples). Thus, sentiment
seems to require more understanding than the usual
topic-based classification. So, apart from presenting
our results obtained via machine learning techniques,
we also analyze the problem to gain a better under-
standing of how difficult it is.
2 Previous Work
This section briefly surveys previous work on non-
topic-based text categorization.
One area of research concentrates on classifying
documents according to their source or source style,
with statistically-detected stylistic variation (Biber,
1988) serving as an important cue. Examples in-
clude author, publisher (e.g., the New York Times vs.
The Daily News), native-language background, and
?brow? (e.g., high-brow vs. ?popular?, or low-brow)
(Mosteller and Wallace, 1984; Argamon-Engelson et
1http://www.mindfuleye.com/about/lexant.htm
                                            Association for Computational Linguistics.
                      Language Processing (EMNLP), Philadelphia, July 2002, pp. 79-86.
                         Proceedings of the Conference on Empirical Methods in Natural
al., 1998; Tomokiyo and Jones, 2001; Kessler et al,
1997).
Another, more related area of research is that of
determining the genre of texts; subjective genres,
such as ?editorial?, are often one of the possible
categories (Karlgren and Cutting, 1994; Kessler et
al., 1997; Finn et al, 2002). Other work explicitly
attempts to find features indicating that subjective
language is being used (Hatzivassiloglou and Wiebe,
2000; Wiebe et al, 2001). But, while techniques for
genre categorization and subjectivity detection can
help us recognize documents that express an opin-
ion, they do not address our specific classification
task of determining what that opinion actually is.
Most previous research on sentiment-based classi-
fication has been at least partially knowledge-based.
Some of this work focuses on classifying the semantic
orientation of individual words or phrases, using lin-
guistic heuristics or a pre-selected set of seed words
(Hatzivassiloglou and McKeown, 1997; Turney and
Littman, 2002). Past work on sentiment-based cat-
egorization of entire documents has often involved
either the use of models inspired by cognitive lin-
guistics (Hearst, 1992; Sack, 1994) or the manual or
semi-manual construction of discriminant-word lex-
icons (Huettner and Subasic, 2000; Das and Chen,
2001; Tong, 2001). Interestingly, our baseline exper-
iments, described in Section 4, show that humans
may not always have the best intuition for choosing
discriminating words.
Turney?s (2002) work on classification of reviews
is perhaps the closest to ours.2 He applied a spe-
cific unsupervised learning technique based on the
mutual information between document phrases and
the words ?excellent? and ?poor?, where the mu-
tual information is computed using statistics gath-
ered by a search engine. In contrast, we utilize sev-
eral completely prior-knowledge-free supervised ma-
chine learning methods, with the goal of understand-
ing the inherent difficulty of the task.
3 The Movie-Review Domain
For our experiments, we chose to work with movie
reviews. This domain is experimentally convenient
because there are large on-line collections of such re-
views, and because reviewers often summarize their
overall sentiment with a machine-extractable rat-
ing indicator, such as a number of stars; hence, we
did not need to hand-label the data for supervised
learning or evaluation purposes. We also note that
Turney (2002) found movie reviews to be the most
2Indeed, although our choice of title was completely
independent of his, our selections were eerily similar.
difficult of several domains for sentiment classifica-
tion, reporting an accuracy of 65.83% on a 120-
document set (random-choice performance: 50%).
But we stress that the machine learning methods and
features we use are not specific to movie reviews, and
should be easily applicable to other domains as long
as sufficient training data exists.
Our data source was the Internet Movie Database
(IMDb) archive of the rec.arts.movies.reviews
newsgroup.3 We selected only reviews where the au-
thor rating was expressed either with stars or some
numerical value (other conventions varied too widely
to allow for automatic processing). Ratings were
automatically extracted and converted into one of
three categories: positive, negative, or neutral. For
the work described in this paper, we concentrated
only on discriminating between positive and nega-
tive sentiment. To avoid domination of the corpus
by a small number of prolific reviewers, we imposed
a limit of fewer than 20 reviews per author per sen-
timent category, yielding a corpus of 752 negative
and 1301 positive reviews, with a total of 144 re-
viewers represented. This dataset will be available
on-line at http://www.cs.cornell.edu/people/pabo/-
movie-review-data/ (the URL contains hyphens only
around the word ?review?).
4 A Closer Look At the Problem
Intuitions seem to differ as to the difficulty of the sen-
timent detection problem. An expert on using ma-
chine learning for text categorization predicted rela-
tively low performance for automatic methods. On
the other hand, it seems that distinguishing positive
from negative reviews is relatively easy for humans,
especially in comparison to the standard text catego-
rization problem, where topics can be closely related.
One might also suspect that there are certain words
people tend to use to express strong sentiments, so
that it might suffice to simply produce a list of such
words by introspection and rely on them alone to
classify the texts.
To test this latter hypothesis, we asked two gradu-
ate students in computer science to (independently)
choose good indicator words for positive and nega-
tive sentiments in movie reviews. Their selections,
shown in Figure 1, seem intuitively plausible. We
then converted their responses into simple decision
procedures that essentially count the number of the
proposed positive and negative words in a given doc-
ument. We applied these procedures to uniformly-
distributed data, so that the random-choice baseline
result would be 50%. As shown in Figure 1, the
3http://reviews.imdb.com/Reviews/
Proposed word lists Accuracy Ties
Human 1 positive: dazzling, brilliant, phenomenal, excellent, fantastic 58% 75%
negative: suck, terrible, awful, unwatchable, hideous
Human 2 positive: gripping, mesmerizing, riveting, spectacular, cool, 64% 39%
awesome, thrilling, badass, excellent, moving, exciting
negative: bad, cliched, sucks, boring, stupid, slow
Figure 1: Baseline results for human word lists. Data: 700 positive and 700 negative reviews.
Proposed word lists Accuracy Ties
Human 3 + stats positive: love, wonderful, best, great, superb, still, beautiful 69% 16%
negative: bad, worst, stupid, waste, boring, ?, !
Figure 2: Results for baseline using introspection and simple statistics of the data (including test data).
accuracy ? percentage of documents classified cor-
rectly ? for the human-based classifiers were 58%
and 64%, respectively.4 Note that the tie rates ?
percentage of documents where the two sentiments
were rated equally likely ? are quite high5 (we chose
a tie breaking policy that maximized the accuracy of
the baselines).
While the tie rates suggest that the brevity of
the human-produced lists is a factor in the relatively
poor performance results, it is not the case that size
alone necessarily limits accuracy. Based on a very
preliminary examination of frequency counts in the
entire corpus (including test data) plus introspection,
we created a list of seven positive and seven negative
words (including punctuation), shown in Figure 2.
As that figure indicates, using these words raised the
accuracy to 69%. Also, although this third list is of
comparable length to the other two, it has a much
lower tie rate of 16%. We further observe that some
of the items in this third list, such as ??? or ?still?,
would probably not have been proposed as possible
candidates merely through introspection, although
upon reflection one sees their merit (the question
mark tends to occur in sentences like ?What was the
director thinking??; ?still? appears in sentences like
?Still, though, it was worth seeing?).
We conclude from these preliminary experiments
that it is worthwhile to explore corpus-based tech-
niques, rather than relying on prior intuitions, to se-
lect good indicator features and to perform sentiment
classification in general. These experiments also pro-
vide us with baselines for experimental comparison;
in particular, the third baseline of 69% might actu-
ally be considered somewhat difficult to beat, since
it was achieved by examination of the test data (al-
though our examination was rather cursory; we do
4Later experiments using these words as features for
machine learning methods did not yield better results.
5This is largely due to 0-0 ties.
not claim that our list was the optimal set of four-
teen words).
5 Machine Learning Methods
Our aim in this work was to examine whether it suf-
fices to treat sentiment classification simply as a spe-
cial case of topic-based categorization (with the two
?topics? being positive sentiment and negative sen-
timent), or whether special sentiment-categorization
methods need to be developed. We experimented
with three standard algorithms: Naive Bayes clas-
sification, maximum entropy classification, and sup-
port vector machines. The philosophies behind these
three algorithms are quite different, but each has
been shown to be effective in previous text catego-
rization studies.
To implement these machine learning algorithms
on our document data, we used the following stan-
dard bag-of-features framework. Let {f1, . . . , fm} be
a predefined set of m features that can appear in
a document; examples include the word ?still? or
the bigram ?really stinks?. Let ni(d) be the num-
ber of times fi occurs in document d. Then, each
document d is represented by the document vector
~d := (n1(d), n2(d), . . . , nm(d)).
5.1 Naive Bayes
One approach to text classification is to assign to a
given document d the class c? = argmaxc P (c | d).
We derive the Naive Bayes (NB) classifier by first
observing that by Bayes? rule,
P (c | d) = P (c)P (d | c)P (d) ,
where P (d) plays no role in selecting c?. To estimate
the term P (d | c), Naive Bayes decomposes it by as-
suming the fi?s are conditionally independent given
d?s class:
PNB(c | d) :=
P (c)
(
?m
i=1 P (fi | c)ni(d)
)
P (d) .
Our training method consists of relative-frequency
estimation of P (c) and P (fi | c), using add-one
smoothing.
Despite its simplicity and the fact that its con-
ditional independence assumption clearly does not
hold in real-world situations, Naive Bayes-based text
categorization still tends to perform surprisingly well
(Lewis, 1998); indeed, Domingos and Pazzani (1997)
show that Naive Bayes is optimal for certain problem
classes with highly dependent features. On the other
hand, more sophisticated algorithms might (and of-
ten do) yield better results; we examine two such
algorithms next.
5.2 Maximum Entropy
Maximum entropy classification (MaxEnt, or ME,
for short) is an alternative technique which has
proven effective in a number of natural lan-
guage processing applications (Berger et al, 1996).
Nigam et al (1999) show that it sometimes, but not
always, outperforms Naive Bayes at standard text
classification. Its estimate of P (c | d) takes the fol-
lowing exponential form:
PME(c | d) :=
1
Z(d) exp
(
?
i
?i,cFi,c(d, c)
)
,
where Z(d) is a normalization function. Fi,c is a fea-
ture/class function for feature fi and class c, defined
as follows:6
Fi,c(d, c?) :=
{ 1, ni(d) > 0 and c? = c
0 otherwise .
For instance, a particular feature/class function
might fire if and only if the bigram ?still hate? ap-
pears and the document?s sentiment is hypothesized
to be negative.7 Importantly, unlike Naive Bayes,
MaxEnt makes no assumptions about the relation-
ships between features, and so might potentially per-
form better when conditional independence assump-
tions are not met.
The ?i,c?s are feature-weight parameters; inspec-
tion of the definition of PME shows that a large ?i,c
means that fi is considered a strong indicator for
6We use a restricted definition of feature/class func-
tions so that MaxEnt relies on the same sort of feature
information as Naive Bayes.
7The dependence on class is necessary for parameter
induction. See Nigam et al (1999) for additional moti-
vation.
class c. The parameter values are set so as to max-
imize the entropy of the induced distribution (hence
the classifier?s name) subject to the constraint that
the expected values of the feature/class functions
with respect to the model are equal to their expected
values with respect to the training data: the under-
lying philosophy is that we should choose the model
making the fewest assumptions about the data while
still remaining consistent with it, which makes intu-
itive sense. We use ten iterations of the improved
iterative scaling algorithm (Della Pietra et al, 1997)
for parameter training (this was a sufficient num-
ber of iterations for convergence of training-data ac-
curacy), together with a Gaussian prior to prevent
overfitting (Chen and Rosenfeld, 2000).
5.3 Support Vector Machines
Support vector machines (SVMs) have been shown to
be highly effective at traditional text categorization,
generally outperforming Naive Bayes (Joachims,
1998). They are large-margin, rather than proba-
bilistic, classifiers, in contrast to Naive Bayes and
MaxEnt. In the two-category case, the basic idea be-
hind the training procedure is to find a hyperplane,
represented by vector ~w, that not only separates
the document vectors in one class from those in the
other, but for which the separation, or margin, is as
large as possible. This search corresponds to a con-
strained optimization problem; letting cj ? {1,?1}
(corresponding to positive and negative) be the cor-
rect class of document dj , the solution can be written
as
~w :=
?
j
?jcj ~dj , ?j ? 0,
where the ?j ?s are obtained by solving a dual opti-
mization problem. Those ~dj such that ?j is greater
than zero are called support vectors, since they are
the only document vectors contributing to ~w. Clas-
sification of test instances consists simply of deter-
mining which side of ~w?s hyperplane they fall on.
We used Joachim?s (1999) SVM light package8 for
training and testing, with all parameters set to their
default values, after first length-normalizing the doc-
ument vectors, as is standard (neglecting to normal-
ize generally hurt performance slightly).
6 Evaluation
6.1 Experimental Set-up
We used documents from the movie-review corpus
described in Section 3. To create a data set with uni-
form class distribution (studying the effect of skewed
8http://svmlight.joachims.org
Features # of frequency or NB ME SVM
features presence?
(1) unigrams 16165 freq. 78.7 N/A 72.8
(2) unigrams ? pres. 81.0 80.4 82.9
(3) unigrams+bigrams 32330 pres. 80.6 80.8 82.7
(4) bigrams 16165 pres. 77.3 77.4 77.1
(5) unigrams+POS 16695 pres. 81.5 80.4 81.9
(6) adjectives 2633 pres. 77.0 77.7 75.1
(7) top 2633 unigrams 2633 pres. 80.3 81.0 81.4
(8) unigrams+position 22430 pres. 81.0 80.1 81.6
Figure 3: Average three-fold cross-validation accuracies, in percent. Boldface: best performance for a given
setting (row). Recall that our baseline results ranged from 50% to 69%.
class distributions was out of the scope of this study),
we randomly selected 700 positive-sentiment and 700
negative-sentiment documents. We then divided this
data into three equal-sized folds, maintaining bal-
anced class distributions in each fold. (We did not
use a larger number of folds due to the slowness of
the MaxEnt training procedure.) All results reported
below, as well as the baseline results from Section 4,
are the average three-fold cross-validation results on
this data (of course, the baseline algorithms had no
parameters to tune).
To prepare the documents, we automatically re-
moved the rating indicators and extracted the tex-
tual information from the original HTML docu-
ment format, treating punctuation as separate lex-
ical items. No stemming or stoplists were used.
One unconventional step we took was to attempt
to model the potentially important contextual effect
of negation: clearly ?good? and ?not very good? in-
dicate opposite sentiment orientations. Adapting a
technique of Das and Chen (2001), we added the tag
NOT to every word between a negation word (?not?,
?isn?t?, ?didn?t?, etc.) and the first punctuation
mark following the negation word. (Preliminary ex-
periments indicate that removing the negation tag
had a negligible, but on average slightly harmful, ef-
fect on performance.)
For this study, we focused on features based on
unigrams (with negation tagging) and bigrams. Be-
cause training MaxEnt is expensive in the number of
features, we limited consideration to (1) the 16165
unigrams appearing at least four times in our 1400-
document corpus (lower count cutoffs did not yield
significantly different results), and (2) the 16165 bi-
grams occurring most often in the same data (the
selected bigrams all occurred at least seven times).
Note that we did not add negation tags to the bi-
grams, since we consider bigrams (and n-grams in
general) to be an orthogonal way to incorporate con-
text.
6.2 Results
Initial unigram results The classification accu-
racies resulting from using only unigrams as fea-
tures are shown in line (1) of Figure 3. As a whole,
the machine learning algorithms clearly surpass the
random-choice baseline of 50%. They also hand-
ily beat our two human-selected-unigram baselines
of 58% and 64%, and, furthermore, perform well in
comparison to the 69% baseline achieved via limited
access to the test-data statistics, although the im-
provement in the case of SVMs is not so large.
On the other hand, in topic-based classification,
all three classifiers have been reported to use bag-
of-unigram features to achieve accuracies of 90%
and above for particular categories (Joachims, 1998;
Nigam et al, 1999)9 ? and such results are for set-
tings with more than two classes. This provides
suggestive evidence that sentiment categorization is
more difficult than topic classification, which cor-
responds to the intuitions of the text categoriza-
tion expert mentioned above.10 Nonetheless, we still
wanted to investigate ways to improve our senti-
ment categorization results; these experiments are
reported below.
Feature frequency vs. presence Recall that we
represent each document d by a feature-count vector
(n1(d), . . . , nm(d)). However, the definition of the
9Joachims (1998) used stemming and stoplists; in
some of their experiments, Nigam et al (1999), like us,
did not.
10We could not perform the natural experiment of at-
tempting topic-based categorization on our data because
the only obvious topics would be the film being reviewed;
unfortunately, in our data, the maximum number of re-
views per movie is 27, too small for meaningful results.
MaxEnt feature/class functions Fi,c only reflects the
presence or absence of a feature, rather than directly
incorporating feature frequency. In order to investi-
gate whether reliance on frequency information could
account for the higher accuracies of Naive Bayes and
SVMs, we binarized the document vectors, setting
ni(d) to 1 if and only feature fi appears in d, and
reran Naive Bayes and SVM light on these new vec-
tors.11
As can be seen from line (2) of Figure 3,
better performance (much better performance for
SVMs) is achieved by accounting only for fea-
ture presence, not feature frequency. Interestingly,
this is in direct opposition to the observations of
McCallum and Nigam (1998) with respect to Naive
Bayes topic classification. We speculate that this in-
dicates a difference between sentiment and topic cat-
egorization ? perhaps due to topic being conveyed
mostly by particular content words that tend to be
repeated ? but this remains to be verified. In any
event, as a result of this finding, we did not incor-
porate frequency information into Naive Bayes and
SVMs in any of the following experiments.
Bigrams In addition to looking specifically for
negation words in the context of a word, we also
studied the use of bigrams to capture more context
in general. Note that bigrams and unigrams are
surely not conditionally independent, meaning that
the feature set they comprise violates Naive Bayes?
conditional-independence assumptions; on the other
hand, recall that this does not imply that Naive
Bayes will necessarily do poorly (Domingos and Paz-
zani, 1997).
Line (3) of the results table shows that bigram
information does not improve performance beyond
that of unigram presence, although adding in the bi-
grams does not seriously impact the results, even for
Naive Bayes. This would not rule out the possibility
that bigram presence is as equally useful a feature
as unigram presence; in fact, Pedersen (2001) found
that bigrams alone can be effective features for word
sense disambiguation. However, comparing line (4)
to line (2) shows that relying just on bigrams causes
accuracy to decline by as much as 5.8 percentage
points. Hence, if context is in fact important, as our
intuitions suggest, bigrams are not effective at cap-
turing it in our setting.
11Alternatively, we could have tried integrating fre-
quency information into MaxEnt. However, feature/class
functions are traditionally defined as binary (Berger et
al., 1996); hence, explicitly incorporating frequencies
would require different functions for each count (or count
bin), making training impractical. But cf. (Nigam et al,
1999).
Parts of speech We also experimented with ap-
pending POS tags to every word via Oliver Mason?s
Qtag program.12 This serves as a crude form of word
sense disambiguation (Wilks and Stevenson, 1998):
for example, it would distinguish the different usages
of ?love? in ?I love this movie? (indicating sentiment
orientation) versus ?This is a love story? (neutral
with respect to sentiment). However, the effect of
this information seems to be a wash: as depicted in
line (5) of Figure 3, the accuracy improves slightly
for Naive Bayes but declines for SVMs, and the per-
formance of MaxEnt is unchanged.
Since adjectives have been a focus of previous work
in sentiment detection (Hatzivassiloglou and Wiebe,
2000; Turney, 2002)13, we looked at the performance
of using adjectives alone. Intuitively, we might ex-
pect that adjectives carry a great deal of informa-
tion regarding a document?s sentiment; indeed, the
human-produced lists from Section 4 contain almost
no other parts of speech. Yet, the results, shown in
line (6) of Figure 3, are relatively poor: the 2633
adjectives provide less useful information than uni-
gram presence. Indeed, line (7) shows that simply
using the 2633 most frequent unigrams is a better
choice, yielding performance comparable to that of
using (the presence of) all 16165 (line (2)). This may
imply that applying explicit feature-selection algo-
rithms on unigrams could improve performance.
Position An additional intuition we had was that
the position of a word in the text might make a dif-
ference: movie reviews, in particular, might begin
with an overall sentiment statement, proceed with
a plot discussion, and conclude by summarizing the
author?s views. As a rough approximation to deter-
mining this kind of structure, we tagged each word
according to whether it appeared in the first quar-
ter, last quarter, or middle half of the document14.
The results (line (8)) didn?t differ greatly from using
unigrams alone, but more refined notions of position
might be more successful.
7 Discussion
The results produced via machine learning tech-
niques are quite good in comparison to the human-
generated baselines discussed in Section 4. In terms
of relative performance, Naive Bayes tends to do the
worst and SVMs tend to do the best, although the
12http://www.english.bham.ac.uk/staff/oliver/soft-
ware/tagger/index.htm
13Turney?s (2002) unsupervised algorithm uses bi-
grams containing an adjective or an adverb.
14We tried a few other settings, e.g., first third vs. last
third vs middle third, and found them to be less effective.
differences aren?t very large.
On the other hand, we were not able to achieve ac-
curacies on the sentiment classification problem com-
parable to those reported for standard topic-based
categorization, despite the several different types of
features we tried. Unigram presence information
turned out to be the most effective; in fact, none of
the alternative features we employed provided consis-
tently better performance once unigram presence was
incorporated. Interestingly, though, the superiority
of presence information in comparison to frequency
information in our setting contradicts previous obser-
vations made in topic-classification work (McCallum
and Nigam, 1998).
What accounts for these two differences ? dif-
ficulty and types of information proving useful ?
between topic and sentiment classification, and how
might we improve the latter? To answer these ques-
tions, we examined the data further. (All examples
below are drawn from the full 2053-document cor-
pus.)
As it turns out, a common phenomenon in the doc-
uments was a kind of ?thwarted expectations? narra-
tive, where the author sets up a deliberate contrast
to earlier discussion: for example, ?This film should
be brilliant. It sounds like a great plot, the actors are
first grade, and the supporting cast is good as well, and
Stallone is attempting to deliver a good performance.
However, it can?t hold up? or ?I hate the Spice Girls.
...[3 things the author hates about them]... Why I saw
this movie is a really, really, really long story, but I
did, and one would think I?d despise every minute of
it. But... Okay, I?m really ashamed of it, but I enjoyed
it. I mean, I admit it?s a really awful movie ...the ninth
floor of hell...The plot is such a mess that it?s terrible.
But I loved it.? 15
In these examples, a human would easily detect
the true sentiment of the review, but bag-of-features
classifiers would presumably find these instances dif-
ficult, since there are many words indicative of the
opposite sentiment to that of the entire review. Fun-
damentally, it seems that some form of discourse
analysis is necessary (using more sophisticated tech-
15This phenomenon is related to another common
theme, that of ?a good actor trapped in a bad movie?:
?AN AMERICAN WEREWOLF IN PARIS is a failed at-
tempt... Julie Delpy is far too good for this movie. She im-
bues Serafine with spirit, spunk, and humanity. This isn?t
necessarily a good thing, since it prevents us from relax-
ing and enjoying AN AMERICAN WEREWOLF IN PARIS
as a completely mindless, campy entertainment experience.
Delpy?s injection of class into an otherwise classless produc-
tion raises the specter of what this film could have been
with a better script and a better cast ... She was radiant,
charismatic, and effective ....?
niques than our positional feature mentioned above),
or at least some way of determining the focus of each
sentence, so that one can decide when the author is
talking about the film itself. (Turney (2002) makes
a similar point, noting that for reviews, ?the whole
is not necessarily the sum of the parts?.) Further-
more, it seems likely that this thwarted-expectations
rhetorical device will appear in many types of texts
(e.g., editorials) devoted to expressing an overall
opinion about some topic. Hence, we believe that an
important next step is the identification of features
indicating whether sentences are on-topic (which is
a kind of co-reference problem); we look forward to
addressing this challenge in future work.
Acknowledgments
We thank Joshua Goodman, Thorsten Joachims, Jon
Kleinberg, Vikas Krishna, John Lafferty, Jussi Myl-
lymaki, Phoebe Sengers, Richard Tong, Peter Tur-
ney, and the anonymous reviewers for many valuable
comments and helpful suggestions, and Hubie Chen
and Tony Faradjian for participating in our baseline
experiments. Portions of this work were done while
the first author was visiting IBM Almaden. This pa-
per is based upon work supported in part by the Na-
tional Science Foundation under ITR/IM grant IIS-
0081334. Any opinions, findings, and conclusions or
recommendations expressed above are those of the
authors and do not necessarily reflect the views of
the National Science Foundation.
References
Shlomo Argamon-Engelson, Moshe Koppel, and
Galit Avneri. 1998. Style-based text categoriza-
tion: What newspaper am I reading? In Proc. of
the AAAI Workshop on Text Categorization, pages
1?4.
Adam L. Berger, Stephen A. Della Pietra, and Vin-
cent J. Della Pietra. 1996. A maximum entropy
approach to natural language processing. Compu-
tational Linguistics, 22(1):39?71.
Douglas Biber. 1988. Variation across Speech and
Writing. Cambridge University Press.
Stanley Chen and Ronald Rosenfeld. 2000. A survey
of smoothing techniques for ME models. IEEE
Trans. Speech and Audio Processing, 8(1):37?50.
Sanjiv Das and Mike Chen. 2001. Yahoo! for
Amazon: Extracting market sentiment from stock
message boards. In Proc. of the 8th Asia Pacific
Finance Association Annual Conference (APFA
2001).
Stephen Della Pietra, Vincent Della Pietra, and John
Lafferty. 1997. Inducing features of random fields.
IEEE Transactions on Pattern Analysis and Ma-
chine Intelligence, 19(4):380?393.
Pedro Domingos and Michael J. Pazzani. 1997. On
the optimality of the simple Bayesian classifier un-
der zero-one loss. Machine Learning, 29(2-3):103?
130.
Aidan Finn, Nicholas Kushmerick, and Barry Smyth.
2002. Genre classification and domain transfer
for information filtering. In Proc. of the Eu-
ropean Colloquium on Information Retrieval Re-
search, pages 353?362, Glasgow.
Vasileios Hatzivassiloglou and Kathleen McKeown.
1997. Predicting the semantic orientation of adjec-
tives. In Proc. of the 35th ACL/8th EACL, pages
174?181.
Vasileios Hatzivassiloglou and Janyce Wiebe. 2000.
Effects of adjective orientation and gradability on
sentence subjectivity. In Proc. of COLING.
Marti Hearst. 1992. Direction-based text interpre-
tation as an information access refinement. In
Paul Jacobs, editor, Text-Based Intelligent Sys-
tems. Lawrence Erlbaum Associates.
Alison Huettner and Pero Subasic. 2000. Fuzzy
typing for document management. In ACL
2000 Companion Volume: Tutorial Abstracts and
Demonstration Notes, pages 26?27.
Thorsten Joachims. 1998. Text categorization with
support vector machines: Learning with many rel-
evant features. In Proc. of the European Confer-
ence on Machine Learning (ECML), pages 137?
142.
Thorsten Joachims. 1999. Making large-scale SVM
learning practical. In Bernhard Scho?lkopf and
Alexander Smola, editors, Advances in Kernel
Methods - Support Vector Learning, pages 44?56.
MIT Press.
Jussi Karlgren and Douglass Cutting. 1994. Recog-
nizing text genres with simple metrics using dis-
criminant analysis. In Proc. of COLING.
Brett Kessler, Geoffrey Nunberg, and Hinrich
Schu?tze. 1997. Automatic detection of text genre.
In Proc. of the 35th ACL/8th EACL, pages 32?38.
David D. Lewis. 1998. Naive (Bayes) at forty: The
independence assumption in information retrieval.
In Proc. of the European Conference on Machine
Learning (ECML), pages 4?15. Invited talk.
Andrew McCallum and Kamal Nigam. 1998. A com-
parison of event models for Naive Bayes text clas-
sification. In Proc. of the AAAI-98 Workshop on
Learning for Text Categorization, pages 41?48.
Frederick Mosteller and David L. Wallace. 1984. Ap-
plied Bayesian and Classical Inference: The Case
of the Federalist Papers. Springer-Verlag.
Kamal Nigam, John Lafferty, and Andrew McCal-
lum. 1999. Using maximum entropy for text clas-
sification. In Proc. of the IJCAI-99 Workshop on
Machine Learning for Information Filtering, pages
61?67.
Ted Pedersen. 2001. A decision tree of bigrams is an
accurate predictor of word sense. In Proc. of the
Second NAACL, pages 79?86.
Warren Sack. 1994. On the computation of point of
view. In Proc. of the Twelfth AAAI, page 1488.
Student abstract.
Ellen Spertus. 1997. Smokey: Automatic recog-
nition of hostile messages. In Proc. of Innova-
tive Applications of Artificial Intelligence (IAAI),
pages 1058?1065.
Junichi Tatemura. 2000. Virtual reviewers for col-
laborative exploration of movie reviews. In Proc.
of the 5th International Conference on Intelligent
User Interfaces, pages 272?275.
Loren Terveen, Will Hill, Brian Amento, David Mc-
Donald, and Josh Creter. 1997. PHOAKS: A sys-
tem for sharing recommendations. Communica-
tions of the ACM, 40(3):59?62.
Laura Mayfield Tomokiyo and Rosie Jones. 2001.
You?re not from round here, are you? Naive Bayes
detection of non-native utterance text. In Proc. of
the Second NAACL, pages 239?246.
Richard M. Tong. 2001. An operational system for
detecting and tracking opinions in on-line discus-
sion. Workshop note, SIGIR 2001 Workshop on
Operational Text Classification.
Peter D. Turney and Michael L. Littman. 2002. Un-
supervised learning of semantic orientation from
a hundred-billion-word corpus. Technical Report
EGB-1094, National Research Council Canada.
Peter Turney. 2002. Thumbs up or thumbs down?
Semantic orientation applied to unsupervised clas-
sification of reviews. In Proc. of the ACL.
Janyce M. Wiebe, Theresa Wilson, and Matthew
Bell. 2001. Identifying collocations for recognizing
opinions. In Proc. of the ACL/EACL Workshop
on Collocation.
Yorick Wilks and Mark Stevenson. 1998. The gram-
mar of sense: Using part-of-speech tags as a first
step in semantic disambiguation. Journal of Nat-
ural Language Engineering, 4(2):135?144.
Bootstrapping Lexical Choice via Multiple-Sequence Alignment
Regina Barzilay
Department of Computer Science
Columbia University
New York, NY 10027 USA
regina@cs.columbia.edu
Lillian Lee
Department of Computer Science
Cornell University
Ithaca, NY 14853 USA
llee@cs.cornell.edu
Abstract
An important component of any generation
system is the mapping dictionary, a lexicon
of elementary semantic expressions and cor-
responding natural language realizations.
Typically, labor-intensive knowledge-based
methods are used to construct the dictio-
nary. We instead propose to acquire it
automatically via a novel multiple-pass al-
gorithm employing multiple-sequence align-
ment, a technique commonly used in bioin-
formatics. Crucially, our method lever-
ages latent information contained in multi-
parallel corpora ? datasets that supply
several verbalizations of the corresponding
semantics rather than just one.
We used our techniques to generate natural
language versions of computer-generated
mathematical proofs, with good results on
both a per-component and overall-output
basis. For example, in evaluations involv-
ing a dozen human judges, our system pro-
duced output whose readability and faith-
fulness to the semantic input rivaled that of
a traditional generation system.
1 Introduction
One or two homologous sequences whisper . . . a full
multiple alignment shouts out loud (Hubbard et al,
1996).
Today?s natural language generation systems
typically employ a lexical chooser that translates
complex semantic concepts into words. The lex-
ical chooser relies on a mapping dictionary that
lists possible realizations of elementary seman-
tic concepts; sample entries might be [Parent
[sex:female]] ? mother or love(x,y )?
{x loves y, x is in love with y}.1
To date, creating these dictionaries has involved
human analysis of a domain-relevant corpus com-
prised of semantic representations and correspond-
ing human verbalizations (Reiter and Dale, 2000).
The corpus analysis and knowledge engineering work
required in such an approach is substantial, pro-
hibitively so in large domains. But, since corpus data
is already used in building lexical choosers by hand,
an appealing alternative is to have the system learn a
mapping dictionary directly from the data. Clearly,
this would greatly reduce the human effort involved
and ease porting the system to new domains. Hence,
we address the following problem: given a parallel
(but unaligned) corpus consisting of both complex
semantic input and corresponding natural language
verbalizations, learn a semantics-to-words mapping
dictionary automatically.
Now, we could simply apply standard statistical
machine translation methods, treating verbalizations
as ?translations? of the semantics. These meth-
ods typically rely on one-parallel corpora consist-
ing of text pairs, one in each ?language? (but cf.
Simard (1999); see Section 5). However, learning the
kind of semantics-to-words mapping that we desire
from one-parallel data alone is difficult even for hu-
mans. First, given the same semantic input, differ-
ent authors may (and do) delete or insert informa-
tion (see Figure 1); hence, direct comparison between
a semantic text and a single verbalization may not
provide enough information regarding their underly-
ing correspondences. Second, a single verbalization
certainly fails to convey the variety of potential lin-
guistic realizations of the concept that an expressive
lexical chooser would ideally have access to.
The multiple-sequence idea Our approach is
motivated by an analogous situation that arises in
computational biology. In brief, an important bioin-
1Throughout, fonts denote a mapping dictionary?s two
information types: semantics and realizations.
                                            Association for Computational Linguistics.
                    Language Processing (EMNLP), Philadelphia, July 2002, pp. 164-171.
                         Proceedings of the Conference on Empirical Methods in Natural
Suppose
Given
Assume
that
product
their
=
and
that
  0 end
also
Prove
start
some "sausages"
0
as in the theorem
  statement
a
      =
zero
b
a * b
is
=
zero
  0
are equal to
Figure 2: Computed lattice for verbalizations from Figure 1. Note how the three indicated ?sausages?
roughly correspond to the three arguments of show-from(a=0,b=0,a?b=0). (The phrases ?as in the theorem
statement? and ?their product? correspond to chains of nodes, but are drawn as single nodes for clarity. Shading
indicates argument-value matches (Section 3.1). All lattice figures omit punctuation nodes for clarity.)
(1) Given a and b as in the theorem statement,
prove that a?b=0.
(2) Suppose that a and b are equal to zero.
Prove that their product is also zero.
(3) Assume that a=0 and b=0.
Figure 1: Three different human verbalizations of
show-from(a=0,b=0,a?b=0).
formatics problem ? Gusfield (1997) refers to it as
?The Holy Grail? ? is to determine commonalities
within a collection of biological sequences such as
proteins or genes. Because of mutations within indi-
vidual sequences, such as changes, insertions, or dele-
tions, pair-wise comparison of sequences can fail to
reveal which features are conserved across the entire
group. Hence, biologists compare multiple sequences
simultaneously to reveal hidden structure character-
istic to the group as a whole.
Our work applies multiple-sequence alignment
techniques to the mapping-dictionary acquisition
problem. The main idea is that using a multi-parallel
corpus ? one that supplies several alternative ver-
balizations for each semantic expression ? can en-
hance both the accuracy and the expressiveness of
the resulting dictionary. In particular, matching a
semantic expression against a composite of the com-
mon structural features of a set of verbalizations
ameliorates the effect of ?mutations? within indi-
vidual verbalizations. Furthermore, the existence of
multiple verbalizations helps the system learn several
ways to express concepts.
To illustrate, consider a sample semantic expres-
sion from the mathematical theorem-proving do-
main. The expression show-from(a=0,b=0,a?b=0)
means ?assuming the two premises a = 0 and b = 0,
show that the goal a ? b = 0 holds?. Figure 1 shows
three human verbalizations of this expression. Even
for so formal a domain as mathematics, the verbal-
izations vary considerably, and none directly matches
the entire semantic input. For instance, it is not ob-
vious without domain knowledge that ?Given a and
b as in the theorem statement? matches ?a=0? and
?b=0?, nor that ?their product? and ?a?b? are equiv-
alent. Moreover, sentence (3) omits the goal argu-
ment entirely. However, as Figure 2 shows, the com-
bination of these verbalizations, as computed by our
multiple-sequence alignment method, exhibits high
structural similarity to the semantic input: the indi-
cated ?sausage? structures correspond closely to the
three arguments of show-from.
2 Multiple-sequence alignment
This section describes general multiple-sequence
alignment; we discuss its application to learning
mapping dictionaries in the next section.
A multiple-sequence alignment algorithm takes as
input n strings and outputs an n-row correspondence
table, or multiple-sequence alignment (MSA). (We
explain how the correspondences are actually com-
puted below.) The MSA?s rows correspond to se-
quences, and each column indicates which elements
of which strings are considered to correspond at that
point; non-correspondences, or ?gaps?, are repre-
sented by underscores ( ). See Figure 3(i).
a b a d
a _  _  c  d
b a _  d
a d e d
a  _  _  c  _ endstart
_
_
_
(ii)(i)
d
a
b/d a/e
c
Figure 3: (i) An MSA of five sequences (the first is
?abad?); (ii) The corresponding lattice.
From an MSA, we can compute a lattice . Each
lattice node, except for ?start? and ?end?, corre-
sponds to an MSA column. The edges are induced
by traversing each of the MSA?s rows from left to
right. See Figure 3(ii).
Alignment computation The sum-of-pairs dy-
namic programming algorithm and pairwise iterative
alignment algorithm sketched here are described in
full in Gusfield (1997) and Durbin et al (1998).
Let ? be the set of elements making up the se-
quences to be aligned, and let sim(x, y), x and y ?
??{ }, be a domain-specific similarity function that
assigns a score to every possible pair of alignment el-
ements, including gaps. Intuitively, we prefer MSAs
in which many high-similarity elements are aligned.
In principle, we can use dynamic programming
over alignments of sequence prefixes to compute the
highest-scoring MSA, where the sum-of-pairs score
for an MSA is computed by summing sim(x, y) over
each pair of entries in each column. Unfortunately,
these computations are exponential in n, the number
of sequences. (In fact, finding the optimal MSA when
n is a variable is NP-complete (Wang and Jiang,
1994).) Therefore, we use iterative pairwise align-
ment, a commonly-used polynomial-time approxi-
mation procedure, instead. This algorithm greedily
merges pairs of MSAs of (increasingly larger) subsets
of the n sequences; which pair to merge is determined
by the average score of all pairwise alignments of se-
quences from the two MSAs.
Aligning lattices We can apply the above se-
quence alignment algorithm to lattices as well as
sequences, as is indeed required by pairwise itera-
tive alignment. We simply treat each lattice as a
sequence whose ith symbol corresponds to the set of
nodes at distance i from the start node. We mod-
ify the similarity function accordingly: any two new
symbols are equivalent to subsets S1 and S2 of ?,
so we define the similarity of these two symbols as
max(x,y)?S1?S2 sim(x, y).
3 Dictionary Induction
Our goal is to produce a semantics-to-words map-
ping dictionary by comparing semantic sequences
to MSAs of multiple verbalizations. We assume
only that the semantic representation uses predicate-
argument structure, so the elementary semantic
units are either terms (e.g., 0), or predicates taking
arguments (e.g., show-from(prem1, prem2, goal),
whose arguments are two premises and a goal). Note
that both types of units can be verbalized by multi-
word sequences.
Now, semantic units can occur several times in
the corpus. In the case of predicates, we would
like to combine information about a given pred-
icate from all its appearances, because doing so
would yield more data for us to learn how to ex-
press it. On the other hand, correlating verbaliza-
tions across instances instantiated with different ar-
gument values (e.g., show-from(a=0,b=0,a*b=0)
vs. show-from(c>0,d>0,c/d>0)) makes alignment
harder, since there are fewer obvious matches (e.g.,
?a?b=0? does not greatly resemble ?c/d>0?); this
seems to discourage aligning cross-instance verbal-
izations.
We resolve this apparent paradox by a novel three-
phase approach:
? In the per-instance alignment phase (Section
3.1), we handle each separate instance of a se-
mantic predicate individually. First, we com-
pute a separate MSA for each instance?s ver-
balizations. Then, we abstract away from the
particular argument values of each instance by
replacing lattice portions corresponding to ar-
gument values with argument slots, thereby cre-
ating a slotted lattice.
? In the cross-instance alignment phase (Section
3.2), for each predicate we align together all the
slotted lattices from all of its instances.
? In the template induction phase (Section 3.3),
we convert the aligned slotted lattices into tem-
plates ? sequences of words and argument po-
sitions ? by tracing slotted lattice paths.
Finally, we enter the templates into the mapping dic-
tionary.
3.1 Per-instance alignment
As mentioned above, the first job of the per-instance
alignment phase is to separately compute for each in-
stance of a semantic unit an MSA of all its verbaliza-
tions. To do so, we need to supply a scoring function
capturing the similarity in meaning between words.
Since such similarity can be domain-dependent, we
use the data to induce ? again via sequence align-
ment ? a paraphrase thesaurus T that lists linguis-
tic items with similar meanings. (This process is
described later in section 3.1.1.) We then set
sim(x, y) =
?
?
?
?
?
1 x = y, x ? ?;
0.5 x ? y;
?0.01 exactly one of x, y is ;
?0.5 otherwise (mismatch)
where ? is the vocabulary and x ? y denotes that T
lists x and y as paraphrases.2 Figure 2 shows the lat-
tice computed for the verbalizations of the instance
2These values were hand-tuned on a held-out develop-
ment corpus, described later. Because we use progressive
alignment, the case x = y = does not occur.
show-from(a=0,b=0,a?b=0) listed in Figure 1. The
structure of the lattice reveals why we informally re-
fer to lattices as ?sausage graphs?.
Next, we transform the lattices into slotted lat-
tices. We use a simple matching process that finds,
for each argument value in the semantic expression,
a sequence of lattice nodes such that each node con-
tains a word identical to or a paraphrase of (accord-
ing to the paraphrase thesaurus) a symbol in the
argument value (these nodes are shaded in Figure
2). The sequences so identified are replaced with a
?slot? marked with the argument variable (see Fig-
ure 4).3 Notice that by replacing the argument val-
ues with variable labels, we make the commonalities
between slotted lattices for different instances more
clear, which is useful for the cross-instance alignment
step.
and thatSuppose
Given
Assume
start
that
Prove
slots
end
goalprem1 prem2
Figure 4: Slotted lattice, computed from the lattice
in Figure 2, for show-from(prem1, prem2, goal).
3.1.1 Paraphrase thesaurus creation
Recall that the paraphrase thesaurus plays
a role both in aligning verbalizations and in
matching lattice nodes to semantic argument
values. The main idea behind our para-
phrase thesaurus induction method, motivated
by Barzilay and McKeown (2001), is that paths
through lattice ?sausages? often correspond to al-
ternate verbalizations of the same concept, since
the sausage endpoints are contexts common to all
the sausage-interior paths. Hence, to extract para-
phrases, we first compute all pairwise alignments of
parallel verbalizations, discarding those of score less
than four in order to eliminate spurious matches.4
Parallel sausage-interior paths that appear in sev-
eral alignments are recorded as paraphrases. Then,
we iterate, realigning each pair of sentences, but with
previously-recognized paraphrases treated as identi-
cal, until no new paraphrases are discovered. While
the majority of the derived paraphrases are single
3This may further change the topology by forcing
other nodes to be removed as well. For example, the
slotted lattice in Figure 4 doesn?t contain the node se-
quence ?their product?.
4Pairwise alignments yield fewer candidate alignments
from which to select paraphrases, allowing simple scoring
functions to produce decent results.
words, the algorithm also produces several multi-
word paraphrases, such as ?are equal to? for ?=?.
To simplify subsequent comparisons, these phrases
(e.g., ?are equal to?) are treated as single tokens.
Here are four paraphrase pairs we extracted from
the mathematical-proof domain:
(conclusion, result) (0, zero)
(applying, by) (expanding, unfolding)
(See Section 4.2 for a formal evaluation of the para-
phrases.) We treat thesaurus entries as degenerate
slotted lattices containing no slots; hence, terms and
predicates are represented in the same way.
3.2 Cross-instance alignment
Figure 4 is an example where the verbalizations for
a single instance yield good information as to how to
realize a predicate. (For example, ?Assume [prem1]
and [prem2], prove [goal]?, where the brackets en-
close arguments marked with their type.) Some-
times, though, the situation is more complicated.
Figure 5 shows two slotted lattices for different in-
stances of rewrite(lemma, goal) (meaning, rewrite
goal by applying lemma); the first slotted lattice is
problematic because it contains context-dependent
information (see caption). Hence, we engage in cross-
instance alignment to merge information about the
predicate. That is, we align the slotted lattices for
all instances of the predicate (see Figure 6); the re-
sultant unified slotted lattice reveals linguistic ex-
pressions common to verbalizations of different in-
stances. Notice that the argument-matching process
in the per-instance alignment phase helps make these
commonalities more evident by abstracting over dif-
ferent values of the same argument (e.g., lemma100
and lemma104 are both relabeled ?lemma?).
3.3 Template induction
Finally, it remains to create the mapping dictionary
from unified slotted lattices. While several strate-
gies are possible, we chose a simple consensus se-
quence method. Define the node weight of a given
slotted lattice node as the number of verbalization
paths passing through it (downweighted if it contains
punctuation or the words ?the?, ?a?, ?to?, ?and?, or
?of?). The path weight of a slotted lattice path is a
length-normalized sum of the weights of its nodes.5
We produce as a template the words from the consen-
sus sequence, defined as the maximum-weight path,
which is easily computed via dynamic programming.
For example, the template we derive from Figure 6?s
slotted lattice is We use lemma [lemma] to get
[goal ].
5Shorter paths are preferred, but we discard sequences
shorter than six words as potentially spurious.
Then we can use lemma lemma an =
?a
?n and get goal
start end
Now the fact about division to the goal
we can useapply lemma lemma to get goal
start end
then the left-hand side
Figure 5: Slotted lattices for the predicate rewrite(lemma,goal) derived from two instances:
(instance I) rewrite(lemma100,a-n*((-a)/(-n))=-(-a-(-n)*((-a)/(-n)))), and
(instance II) rewrite(lemma104,A-(-(A/(-N)))*N = A-(A/(-N))*(-N));
each instance had two verbalizations. In instance (I), both verbalizations contain the context-dependent in-
formation ? an = ?a?n? (the statement of lemma100); also, argument-matching failed on the context-dependent
phrase ?the fact about division?.
Now the fact about division an =
?a
?n and the goal
start we can useapply lemma lemma to get goal end
Then the left-hand side
Figure 6: Unified slotted lattice computed by cross-instance alignment of Figure 5?s slotted lattices. The
consensus sequence is shown in bold (recall that node weight roughly corresponds to in-degree).
While this method is quite efficient, it does not
fully exploit the expressive power of the lattice,
which may encapsulate several valid realizations. We
leave to future work experimenting with alternative
template-induction techniques; see Section 5.
4 Evaluation
We implemented our system on formal mathemati-
cal proofs created by the Nuprl system, which has
been used to create thousands of proofs in many
mathematical fields (Constable et al, 1986). Gen-
erating natural-language versions of proofs was first
addressed several decades ago (Chester, 1976). But
now, large formal-mathematics libraries are available
on-line.6 Unfortunately, they are usually encoded in
highly technical languages (see Figure 7(i)). Natural-
language versions of these proofs would make them
more widely accessible, both to users lacking famil-
iarity with a specific prover?s language, and to search
engines which at present cannot search the symbolic
language of formal proofs.
Besides these practical benefits, the formal math-
ematics domain has the further advantage of being
particularly suitable for applying statistical genera-
tion techniques. Training data is available because
6See http://www.cs.cornell.edu/Info/Projects/-
NuPrl/ or http://www.mizar.org, for example.
theorem-prover developers frequently provide verbal-
izations of system outputs for explanatory purposes.
In our case, a multi-parallel corpus of Nuprl proof
verbalizations already exists (Holland-Minkley et al,
1999) and forms the core of our training corpus.
Also, from a research point of view, the examples
from Figure 1 show that there is a surprising variety
in the data, making the problem quite challenging.
All evaluations reported here involved judgments
from graduate students and researchers in computer
science. We authors were not among the judges.
4.1 Corpus
Our training corpus consists of 30 Nuprl proofs and
83 verbalizations. On average, each proof consists of
5.08 proof steps, which are the basic semantic unit in
Nuprl; Figure 7(i) shows an example of three Nuprl
steps. An additional five proofs, disjoint from the
test data, were used as a development set for setting
the values of all parameters.7
Pre-processing First, we need to divide the ver-
balization texts into portions corresponding to in-
dividual proof steps, since per-instance alignment
handles verbalizations for only one semantic unit at
a time. Fortunately, Holland-Minkley et al (1999)
7See http://www.cs.cornell.edu/Info/Projects/
NuPrl/html/nlp for all our data.
(i) (ii) (iii)
UnivCD(? i:N.|i| = |-i|, i:N, |i| = |-i|)
BackThruLemma(|i| = |-i|, i= ? i,absval eq)
Unfold(i= ? i, (), pm equal)
Assume that i is an integer,
we need to show |i| = | ? i|.
From absval eq lemma,
|i| = | ? i| reduces to
i = ?i. By the definition of
pm equal, i = ?i is proved.
Assume i is an integer. By
the absval eq lemma, the
goal becomes |i| = | ? i|.
Now, the original expression
can be rewritten as i = ?i.
Figure 7: (i) Nuprl proof (test lemma ?h? in Figure 8). (ii) Verbalization produced by our system. (iii)
Verbalization produced by traditional generation system; note that the initial goal is never specified, which
means that in the phrase ?the goal becomes?, we don?t know what the goal is.
showed that for Nuprl, one proof step roughly corre-
sponds to one sentence in a natural language verbal-
ization. So, we align Nuprl steps with verbalization
sentences using dynamic programming based on the
number of symbols common to both the step and
the verbalization. This produced 382 pairs of Nuprl
steps and corresponding verbalizations. We also did
some manual cleaning on the training data to reduce
noise for subsequent stages.8
4.2 Per-component evaluation
We first evaluated three individual components
of our system: paraphrase thesaurus induction,
argument-value selection in slotted lattice induction,
and template induction. We also validated the utility
of multi-parallel, as opposed to one-parallel, data.
Paraphrase thesaurus We presented two judges
with all 71 paraphrase pairs produced by our system.
They identified 87% and 82%, respectively, as being
plausible substitutes within a mathematical context.
Argument-value selection We next measured
how well our system matches semantic argument val-
ues with lattice node sequences. We randomly se-
lected 20 Nuprl steps and their corresponding verbal-
izations. From this sample, a Nuprl expert identified
the argument values that appeared in at least one
corresponding verbalization; of the 46 such values,
our system correctly matched lattice node sequences
to 91%. To study the relative effectiveness of using
multi-parallel rather than one-parallel data, we also
implemented a baseline system that used only one
(randomly-selected) verbalization among the multi-
ple possibilities. This single-verbalization baseline
matched only 44% of the values correctly, indicating
the value of a multi-parallel-corpus approach.
Templates Thirdly, we randomly selected 20 in-
duced templates; of these, a Nuprl expert determined
8We employed pattern-matching tools to fix incorrect
sentence boundaries, converted non-ascii symbols to a
human-readable format, and discarded a few verbaliza-
tions which were unrelated to the underlying proof.
that 85% were plausible verbalizations of the corre-
sponding Nuprl. This was a very large improvement
over the single-verbalization baseline?s 30%, again
validating the multi-parallel-corpus approach.
4.3 Evaluation of the generated texts
Finally, we evaluated the quality of the text our
system generates by comparing its output against
the system of Holland-Minkley et al (1999), which
produces accurate and readable Nuprl proof verbal-
izations. Their system has a hand-crafted lexical
chooser derived via manual analysis of the same cor-
pus that our system was trained on. To run the ex-
periments, we replaced Holland-Minkley et. al?s lexi-
cal chooser with the mapping dictionary we induced.
(An alternative evaluation would have been to com-
pare our output with human-authored texts. But
this wouldn?t have allowed us to evaluate the perfor-
mance of the lexical chooser alone, as human proof
generation may differ in aspects other than lexical
choice.) The test set serving as input to the two sys-
tems consisted of 20 held-out proofs, unseen through-
out the entirety of our algorithm development work.
We evaluated the texts on two dimensions: readabil-
ity and fidelity to the mathematical semantics.
Readability We asked 11 judges to compare the
readability of the texts produced from the same
Nuprl proof input: Figure 7(ii) and (iii) show an
example text pair.9 (The judges were not given the
original Nuprl proofs.) Figure 8 shows the results.
Good entries are those that are not preferences for
the traditional system, since our goal, after all, is to
show that MSA-based techniques can produce out-
put as good or better than a hand-crafted system.
We see that for every lemma and for every judge,
our system performed quite well. Furthermore, for
more than half of the lemmas, more than half the
9To prevent judges from identifying the system pro-
ducing the text, the order of presentation of the two sys-
tems? output was randomly chosen anew for each proof.
Lemma % good
Judge a b c d e f g h i j k l m n o p q r s t
A ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? 100
B ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? 75
C ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? 70
D ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? 70
E ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? 70
F ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? 85
G ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? 85
H ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? 100
I ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? 60
J ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? 85
K ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? 65
% good 55 82 91 73 91 82 73 82 82 64 73 82 82 82 82 91 64 82 73 91
> 50% ?? X X X X X X X X X X X X X
Figure 8: Readability results. ?: preference for our system. ?: preference for hand-crafted system. ?: no
preference. X: > 50% of the judges preferred the statistical system?s output.
judges found our system?s output to be distinctly
better than the traditional system?s.
Fidelity We asked a Nuprl-familiar expert in formal
logic to determine, given the Nuprl proofs and output
texts, whether the texts preserved the main ideas of
the formal proofs without introducing ambiguities.
All 20 of our system?s proofs were judged correct,
while only 17 of the traditional system?s proofs were
judged to be correct.
5 Related Work
Nuprl creates proofs at a higher level of abstrac-
tion than other provers do, so we were able to learn
verbalizations directly from the Nuprl proofs them-
selves. In other natural-language proof generation
systems (Huang and Fiedler, 1997; Siekmann et al,
1999) and other generation applications, the seman-
tic expressions to be realized are the product of the
system?s content planning component, not the proof
or data. But our techniques can still be incorporated
into such systems, because we can map verbalizations
to the content planner?s output. Hence, we believe
our approach generalizes to other settings.
Previous research on statistical generation has ad-
dressed different problems. Some systems learn
from verbalizations annotated with semantic con-
cepts (Ratnaparkhi, 2000; Oh and Rudnicky, 2000);
in contrast, we use un-annotated corpora. Other
work focuses on surface realization ? choosing
among different lexical and syntactic options sup-
plied by the lexical chooser and sentence planner
? rather than on creating the mapping dictionary;
although such work also uses lattices as input to
the stochastic realizer, the lattices themselves are
constructed by traditional knowledge-based means
(Langkilde and Knight, 1998; Bangalore and Ram-
bow, 2000). An exciting direction for future research
is to apply these statistical surface realization meth-
ods to the lattices our method produces.
Word lattices are commonly used in speech recog-
nition to represent different transcription hypothe-
ses. Mangu et al (2000) compress these lattices into
confusion networks with structure reminiscent of our
?sausage graphs?, utilizing alignment criteria based
on word identity and external information such as
phonetic similarity.
Using alignment for grammar and lexicon in-
duction has been an active area of research, both
in monolingual settings (van Zaanen, 2000) and
in machine translation (MT) (Brown et al, 1993;
Melamed, 2000; Och and Ney, 2000) ? interestingly,
statistical MT techniques have been used to derive
lexico-semantic mappings in the ?reverse? direction
of language understanding rather than generation
(Papineni et al, 1997; Macherey et al, 2001). In
a preliminary study, applying IBM-style alignment
models in a black-box manner (i.e., without modifi-
cation) to our setting did not yield promising results
(Chong, 2002). On the other hand, MT systems can
often model crossing alignment situations; these are
rare in our data, but we hope to account for them in
future work.
While recent proposals for evaluation of MT sys-
tems have involved multi-parallel corpora (Thomp-
son and Brew, 1996; Papineni et al, 2002), statis-
tical MT algorithms typically only use one-parallel
data. Simard?s (1999) trilingual (rather than multi-
parallel) corpus method, which also computes MSAs,
is a notable exception, but he reports mixed exper-
imental results. In contrast, we have shown that
through application of a novel composition of align-
ment steps, we can leverage multi-parallel corpora to
create high-quality mapping dictionaries supporting
effective text generation.
Acknowledgments
We thank Stuart Allen, Eli Barzilay, Stephen Chong,
Michael Collins, Bob Constable, Jon Kleinberg, John
Lafferty, Kathy McKeown, Dan Melamed, Golan Yona,
the Columbia NLP group, and the anonymous reviewers
for many helpful comments. Thanks also to the Cor-
nell Nuprl and Columbia NLP groups, Hubie Chen, and
Juanita Heyerman for participating in our evaluation,
and the Nuprl group for generating verbalizations. We
are grateful to Amanda Holland-Minkley for help run-
ning the comparison experiments. Portions of this work
were done while the first author was visiting Cornell Uni-
versity. This paper is based upon work supported in
part by the National Science Foundation under ITR/IM
grant IIS-0081334 and a Louis Morin scholarship. Any
opinions, findings, and conclusions or recommendations
expressed above are those of the authors and do not nec-
essarily reflect the views of the National Science Founda-
tion.
References
Srinivas Bangalore and Owen Rambow. 2000. Exploiting
a probabilistic hierarchical model for generation. In
Proc. of COLING.
Regina Barzilay and Kathleen McKeown. 2001. Extract-
ing paraphrases from a parallel corpus. In Proc. of the
ACL/EACL, pages 50?57.
Peter Brown, Stephen Della Pietra, Vincent Della Pietra,
and Robert Mercer. 1993. The mathematics of sta-
tistical machine translation: Parameter estimation.
Computational Linguistics, 19(2):263?311.
Daniel Chester. 1976. The translation of formal proofs
into English. Artificial Intelligence, 7:261?278.
Stephen Chong. 2002. Word alignment of proof verbal-
izations using generative statistical models. Technical
Report TR2002-1864, Cornell Computer Science.
R. Constable, S. Allen, H. Bromley, W. Cleaveland,
J. Cremer, R. Harper, D. Howe, T. Knoblock,
N. Mendler, P. Panangaden, J. Sasaki, and S. Smith.
1986. Implementing Mathematics with the Nuprl De-
velopment System. Prentice-Hall.
Richard Durbin, Sean Eddy, Anders Krogh, and Graeme
Mitchison. 1998. Biological Sequence Analysis. Cam-
bridge University Press.
Dan Gusfield. 1997. Algorithms on Strings, Trees, and
Sequences: Computer Science and Computational Bi-
ology. Cambridge University Press.
Amanda M. Holland-Minkley, Regina Barzilay, and
Robert L. Constable. 1999. Verbalization of high-level
formal proofs. In Proc. of AAAI, pages 277?284.
Xiaorong Huang and Armin Fiedler. 1997. Proof verbal-
ization as an application of NLG. In Proc. of IJCAI.
Tim J. P. Hubbard, Arthur M. Lesk, and Anna Tramon-
tano. 1996. Gathering them in to the fold. Nature
Structural Biology, 3(4):313, April.
Irene Langkilde and Kevin Knight. 1998. Generation
that exploits corpus-based statistical knowledge. In
Proc. of ACL/COLING, pages 704?710.
Klaus Macherey, Franz Josef Och, and Hermann Ney.
2001. Natural language understanding using statistical
machine translation. In Proc. of EUROSPEECH.
Lidia Mangu, Eric Brill, and Andreas Stolcke. 2000.
Finding consensus in speech recognition: Word error
minimization and other applications of confusion net-
works. Computer, Speech and Language, 14(4):373?
400.
I. Dan Melamed. 2000. Models of translational equiv-
alence among words. Computational Linguistics,
26(2):221?249.
Franz Josef Och and Hermann Ney. 2000. Improved sta-
tistical alignment models. In Proc. of the ACL, pages
440?447.
Alice Oh and Alexander Rudnicky. 2000. Stochastic lan-
guage generatation for spoken dialogue systems. In
Proc. of the ANLP/NAACL 2000 Workshop on Con-
versational Systems, pages 27?32.
Kishore A. Papineni, Salim Roukos, and R. Todd Ward.
1997. Feature-based language understanding. In Proc.
of EUROSPEECH, volume 3, pages 1435 ? 1438.
Kishore A. Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. Bleu: A method for automatic
evaluation of machine translation. In Proc. of the
ACL.
Adwait Ratnaparkhi. 2000. Trainable methods for sur-
face natural language generation. In Proc. of the
NAACL, pages 194?201.
Ehud Reiter and Robert Dale. 2000. Building Natural
Language Generation System. Cambridge University
Press.
Jo?rg H. Siekmann, Stephan M. Hess, Christoph
Benzmu?ller, Lassaad Cheikhrouhou, Armin Fiedler,
Helmut Horacek, Michael Kohlhase, Karsten Konrad,
Andreas Meier, Erica Melis, Martin Pollet, and Volker
Sorge. 1999. L?UI: Lovely ?MEGA user interface.
Formal Aspects of Computing, 11(3).
Michel Simard. 1999. Text-translation alignment:
Three languages are better than two. In Proc. of
EMNLP/VLC, pages 2?11.
Henry S. Thompson and Chris Brew. 1996.
Automatic evaluation of computer generated
text: Final report on the TextEval project.
http://www.cogsci.ed.ac.uk/?chrisbr/papers/mt-
eval-final/mt-eval-final.html.
Menno van Zaanen. 2000. Bootstrapping syntax and
recursion using alignment-based learning. In Proc. of
ICML, pages 1063?1070.
Lusheng Wang and Tao Jiang. 1994. On the complexity
of multiple sequence alignment. Journal of Computa-
tional Biology, 1(4):337?348.
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 327?335,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Get out the vote: Determining support or opposition from Congressional
floor-debate transcripts
Matt Thomas, Bo Pang, and Lillian Lee
Department of Computer Science, Cornell University
Ithaca, NY 14853-7501
mattthomas84@gmail.com, pabo@cs.cornell.edu, llee@cs.cornell.edu
Abstract
We investigate whether one can determine
from the transcripts of U.S. Congressional
floor debates whether the speeches repre-
sent support of or opposition to proposed
legislation. To address this problem, we
exploit the fact that these speeches occur
as part of a discussion; this allows us to
use sources of information regarding re-
lationships between discourse segments,
such as whether a given utterance indicates
agreement with the opinion expressed by
another. We find that the incorporation
of such information yields substantial im-
provements over classifying speeches in
isolation.
1 Introduction
One ought to recognize that the present
political chaos is connected with the de-
cay of language, and that one can prob-
ably bring about some improvement by
starting at the verbal end. ? Orwell,
?Politics and the English language?
We have entered an era where very large
amounts of politically oriented text are now avail-
able online. This includes both official documents,
such as the full text of laws and the proceedings of
legislative bodies, and unofficial documents, such
as postings on weblogs (blogs) devoted to politics.
In some sense, the availability of such data is sim-
ply a manifestation of a general trend of ?every-
body putting their records on the Internet?.1 The
1It is worth pointing out that the United States? Library of
Congress was an extremely early adopter of Web technology:
the THOMAS database (http://thomas.loc.gov) of congres-
online accessibility of politically oriented texts in
particular, however, is a phenomenon that some
have gone so far as to say will have a potentially
society-changing effect.
In the United States, for example, governmen-
tal bodies are providing and soliciting political
documents via the Internet, with lofty goals in
mind: electronic rulemaking (eRulemaking) ini-
tiatives involving the ?electronic collection, dis-
tribution, synthesis, and analysis of public com-
mentary in the regulatory rulemaking process?,
may ?[alter] the citizen-government relationship?
(Shulman and Schlosberg, 2002). Additionally,
much media attention has been focused recently
on the potential impact that Internet sites may have
on politics2, or at least on political journalism3.
Regardless of whether one views such claims as
clear-sighted prophecy or mere hype, it is obvi-
ously important to help people understand and an-
alyze politically oriented text, given the impor-
tance of enabling informed participation in the po-
litical process.
Evaluative and persuasive documents, such as
a politician?s speech regarding a bill or a blog-
ger?s commentary on a legislative proposal, form a
particularly interesting type of politically oriented
text. People are much more likely to consult such
evaluative statements than the actual text of a bill
or law under discussion, given the dense nature of
legislative language and the fact that (U.S.) bills
often reach several hundred pages in length (Smith
et al, 2005). Moreover, political opinions are ex-
sional bills and related data was launched in January 1995,
when Mosaic was not quite two years old and Altavista did
not yet exist.
2E.g., ?Internet injects sweeping change into U.S. poli-
tics?, Adam Nagourney, The New York Times, April 2, 2006.
3E.g., ?The End of News??, Michael Massing, The New
York Review of Books, December 1, 2005.
327
plicitly solicited in the eRulemaking scenario.
In the analysis of evaluative language, it is fun-
damentally necessary to determine whether the au-
thor/speaker supports or disapproves of the topic
of discussion. In this paper, we investigate the
following specific instantiation of this problem:
we seek to determine from the transcripts of
U.S. Congressional floor debates whether each
?speech? (continuous single-speaker segment of
text) represents support for or opposition to a pro-
posed piece of legislation. Note that from an ex-
perimental point of view, this is a very convenient
problem to work with because we can automati-
cally determine ground truth (and thus avoid the
need for manual annotation) simply by consulting
publicly available voting records.
Task properties Determining whether or not a
speaker supports a proposal falls within the realm
of sentiment analysis, an extremely active re-
search area devoted to the computational treatment
of subjective or opinion-oriented language (early
work includes Wiebe and Rapaport (1988), Hearst
(1992), Sack (1994), and Wiebe (1994); see Esuli
(2006) for an active bibliography). In particu-
lar, since we treat each individual speech within
a debate as a single ?document?, we are consider-
ing a version of document-level sentiment-polarity
classification, namely, automatically distinguish-
ing between positive and negative documents (Das
and Chen, 2001; Pang et al, 2002; Turney, 2002;
Dave et al, 2003).
Most sentiment-polarity classifiers proposed in
the recent literature categorize each document in-
dependently. A few others incorporate various
measures of inter-document similarity between the
texts to be labeled (Agarwal and Bhattacharyya,
2005; Pang and Lee, 2005; Goldberg and Zhu,
2006). Many interesting opinion-oriented docu-
ments, however, can be linked through certain re-
lationships that occur in the context of evaluative
discussions. For example, we may find textual4
evidence of a high likelihood of agreement be-
4Because we are most interested in techniques applicable
across domains, we restrict consideration to NLP aspects of
the problem, ignoring external problem-specific information.
For example, although most votes in our corpus were almost
completely along party lines (and despite the fact that same-
party information is easily incorporated via the methods we
propose), we did not use party-affiliation data. Indeed, in
other settings (e.g., a movie-discussion listserv) one may not
be able to determine the participants? political leanings, and
such information may not lead to significantly improved re-
sults even if it were available.
tween two speakers, such as explicit assertions (?I
second that!?) or quotation of messages in emails
or postings (see Mullen and Malouf (2006) but cf.
Agrawal et al (2003)). Agreement evidence can
be a powerful aid in our classification task: for ex-
ample, we can easily categorize a complicated (or
overly terse) document if we find within it indica-
tions of agreement with a clearly positive text.
Obviously, incorporating agreement informa-
tion provides additional benefit only when the in-
put documents are relatively difficult to classify
individually. Intuition suggests that this is true
of the data with which we experiment, for several
reasons. First, U.S. congressional debates contain
very rich language and cover an extremely wide
variety of topics, ranging from flag burning to in-
ternational policy to the federal budget. Debates
are also subject to digressions, some fairly natural
and others less so (e.g., ?Why are we discussing
this bill when the plight of my constituents regard-
ing this other issue is being ignored??)
Second, an important characteristic of persua-
sive language is that speakers may spend more
time presenting evidence in support of their po-
sitions (or attacking the evidence presented by
others) than directly stating their attitudes. An
extreme example will illustrate the problems in-
volved. Consider a speech that describes the U.S.
flag as deeply inspirational, and thus contains only
positive language. If the bill under discussion is a
proposed flag-burning ban, then the speech is sup-
portive; but if the bill under discussion is aimed at
rescinding an existing flag-burning ban, the speech
may represent opposition to the legislation. Given
the current state of the art in sentiment analysis,
it is doubtful that one could determine the (proba-
bly topic-specific) relationship between presented
evidence and speaker opinion.
Qualitative summary of results The above dif-
ficulties underscore the importance of enhancing
standard classification techniques with new infor-
mation sources that promise to improve accuracy,
such as inter-document relationships between the
documents to be labeled. In this paper, we demon-
strate that the incorporation of agreement model-
ing can provide substantial improvements over the
application of support vector machines (SVMs) in
isolation, which represents the state of the art in
the individual classification of documents. The en-
hanced accuracies are obtained via a fairly primi-
tive automatically-acquired ?agreement detector?
328
total train test development
speech segments 3857 2740 860 257
debates 53 38 10 5
average number of speech segments per debate 72.8 72.1 86.0 51.4
average number of speakers per debate 32.1 30.9 41.1 22.6
Table 1: Corpus statistics.
and a conceptually simple method for integrat-
ing isolated-document and agreement-based in-
formation. We thus view our results as demon-
strating the potentially large benefits of exploiting
sentiment-related discourse-segment relationships
in sentiment-analysis tasks.
2 Corpus
This section outlines the main steps of the process
by which we created our corpus (download site:
www.cs.cornell.edu/home/llee/data/convote.html).
GovTrack (http://govtrack.us) is an independent
website run by Joshua Tauberer that collects pub-
licly available data on the legislative and fund-
raising activities of U.S. congresspeople. Due to
its extensive cross-referencing and collating of in-
formation, it was nominated for a 2006 ?Webby?
award. A crucial characteristic of GovTrack from
our point of view is that the information is pro-
vided in a very convenient format; for instance,
the floor-debate transcripts are broken into sepa-
rate HTML files according to the subject of the
debate, so we can trivially derive long sequences
of speeches guaranteed to cover the same topic.
We extracted from GovTrack all available tran-
scripts of U.S. floor debates in the House of Rep-
resentatives for the year 2005 (3268 pages of tran-
scripts in total), together with voting records for all
roll-call votes during that year. We concentrated
on debates regarding ?controversial? bills (ones in
which the losing side generated at least 20% of the
speeches) because these debates should presum-
ably exhibit more interesting discourse structure.
Each debate consists of a series of speech seg-
ments, where each segment is a sequence of un-
interrupted utterances by a single speaker. Since
speech segments represent natural discourse units,
we treat them as the basic unit to be classified.
Each speech segment was labeled by the vote
(?yea? or ?nay?) cast for the proposed bill by the
person who uttered the speech segment.
We automatically discarded those speech seg-
ments belonging to a class of formulaic, generally
one-sentence utterances focused on the yielding
of time on the house floor (for example, ?Madam
Speaker, I am pleased to yield 5 minutes to the
gentleman from Massachusetts?), as such speech
segments are clearly off-topic. We also removed
speech segments containing the term ?amend-
ment?, since we found during initial inspection
that these speeches generally reflect a speaker?s
opinion on an amendment, and this opinion may
differ from the speaker?s opinion on the underly-
ing bill under discussion.
We randomly split the data into training, test,
and development (parameter-tuning) sets repre-
senting roughly 70%, 20%, and 10% of our data,
respectively (see Table 1). The speech segments
remained grouped by debate, with 38 debates as-
signed to the training set, 10 to the test set, and 5
to the development set; we require that the speech
segments from an individual debate all appear in
the same set because our goal is to examine clas-
sification of speech segments in the context of the
surrounding discussion.
3 Method
The support/oppose classification problem can be
approached through the use of standard classifiers
such as support vector machines (SVMs), which
consider each text unit in isolation. As discussed
in Section 1, however, the conversational nature
of our data implies the existence of various rela-
tionships that can be exploited to improve cumu-
lative classification accuracy for speech segments
belonging to the same debate. Our classification
framework, directly inspired by Blum and Chawla
(2001), integrates both perspectives, optimizing
its labeling of speech segments based on both in-
dividual speech-segment classification scores and
preferences for groups of speech segments to re-
ceive the same label. In this section, we discuss
the specific classification framework that we adopt
and the set of mechanisms that we propose for
modeling specific types of relationships.
329
3.1 Classification framework
Let s1, s2, . . . , sn be the sequence of speech seg-
ments within a given debate, and let Y and
N stand for the ?yea? and ?nay? class, respec-
tively. Assume we have a non-negative func-
tion ind(s, C) indicating the degree of preference
that an individual-document classifier, such as an
SVM, has for placing speech-segment s in class
C. Also, assume that some pairs of speech seg-
ments have weighted links between them, where
the non-negative strength (weight) str(`) for a
link ` indicates the degree to which it is prefer-
able that the linked speech segments receive the
same label. Then, any class assignment c =
c(s1), c(s2), . . . , c(sn) can be assigned a cost
?
s
ind(s, c(s))+
?
s,s?: c(s) 6=c(s?)
?
` between s,s?
str(`),
where c(s) is the ?opposite? class from c(s). A
minimum-cost assignment thus represents an opti-
mum way to classify the speech segments so that
each one tends not to be put into the class that
the individual-document classifier disprefers, but
at the same time, highly associated speech seg-
ments tend not to be put in different classes.
As has been previously observed and exploited
in the NLP literature (Pang and Lee, 2004; Agar-
wal and Bhattacharyya, 2005; Barzilay and Lap-
ata, 2005), the above optimization function, unlike
many others that have been proposed for graph or
set partitioning, can be solved exactly in an prov-
ably efficient manner via methods for finding min-
imum cuts in graphs. In our view, the contribution
of our work is the examination of new types of
relationships, not the method by which such re-
lationships are incorporated into the classification
decision.
3.2 Classifying speech segments in isolation
In our experiments, we employed the well-known
classifier SVMlight to obtain individual-document
classification scores, treating Y as the positive
class and using plain unigrams as features.5 Fol-
lowing standard practice in sentiment analysis
(Pang et al, 2002), the input to SVMlight con-
sisted of normalized presence-of-feature (rather
than frequency-of-feature) vectors. The ind value
5SVMlight is available at svmlight.joachims.org. Default
parameters were used, although experimentation with differ-
ent parameter settings is an important direction for future
work (Daelemans and Hoste, 2002; Munson et al, 2005).
for each speech segment s was based on the signed
distance d(s) from the vector representing s to the
trained SVM decision plane:
ind(s,Y) def=
?
???
???
1 d(s) > 2?s;(
1 + d(s)2?s
)
/2 |d(s)| ? 2?s;
0 d(s) < ?2?s
where ?s is the standard deviation of d(s) over all
speech segments s in the debate in question, and
ind(s,N ) def= 1? ind(s,Y).
We now turn to the more interesting problem of
representing the preferences that speech segments
may have for being assigned to the same class.
3.3 Relationships between speech segments
A wide range of relationships between text seg-
ments can be modeled as positive-strength links.
Here we discuss two types of constraints that are
considered in this work.
Same-speaker constraints: In Congressional
debates and in general social-discourse contexts,
a single speaker may make a number of comments
regarding a topic. It is reasonable to expect that in
many settings, the participants in a discussion may
be convinced to change their opinions midway
through a debate. Hence, in the general case we
wish to be able to express ?soft? preferences for all
of an author?s statements to receive the same label,
where the strengths of such constraints could, for
instance, vary according to the time elapsed be-
tween the statements. Weighted links are an ap-
propriate means to express such variation.
However, if we assume that most speakers do
not change their positions in the course of a dis-
cussion, we can conclude that all comments made
by the same speaker must receive the same label.
This assumption holds by fiat for the ground-truth
labels in our dataset because these labels were
derived from the single vote cast by the speaker
on the bill being discussed.6 We can implement
this assumption via links whose weights are essen-
tially infinite. Although one can also implement
this assumption via concatenation of same-speaker
speech segments (see Section 4.3), we view the
fact that our graph-based framework incorporates
6We are attempting to determine whether a speech seg-
ment represents support or not. This differs from the problem
of determining what the speaker?s actual opinion is, a prob-
lem that, as an anonymous reviewer put it, is complicated by
?grandstanding, backroom deals, or, more innocently, plain
change of mind (?I voted for it before I voted against it?)?.
330
both hard and soft constraints in a principled fash-
ion as an advantage of our approach.
Different-speaker agreements In House dis-
course, it is common for one speaker to make ref-
erence to another in the context of an agreement
or disagreement over the topic of discussion. The
systematic identification of instances of agreement
can, as we have discussed, be a powerful tool for
the development of intelligently selected weights
for links between speech segments.
The problem of agreement identification can be
decomposed into two sub-problems: identifying
references and their targets, and deciding whether
each reference represents an instance of agree-
ment. In our case, the first task is straightfor-
ward because we focused solely on by-name ref-
erences.7 Hence, we will now concentrate on the
second, more interesting task.
We approach the problem of classifying refer-
ences by representing each reference with a word-
presence vector derived from a window of text
surrounding the reference.8 In the training set,
we classify each reference connecting two speak-
ers with a positive or negative label depending on
whether the two voted the same way on the bill un-
der discussion9. These labels are then used to train
an SVM classifier, the output of which is subse-
quently used to create weights on agreement links
in the test set as follows.
Let d(r) denote the distance from the vector
representing reference r to the agreement-detector
SVM?s decision plane, and let ?r be the standard
deviation of d(r) over all references in the debate
in question. We then define the strength agr of the
agreement link corresponding to the reference as:
agr(r) def=
?
??
??
0 d(r) < ?agr;
? ? d(r)/4?r ?agr ? d(r) ? 4?r;
? d(r) > 4?r.
The free parameter ? specifies the relative impor-
7One subtlety is that for the purposes of mining agree-
ment cues (but not for evaluating overall support/oppose
classification accuracy), we temporarily re-inserted into our
dataset previously filtered speech segments containing the
term ?yield?, since the yielding of time on the House floor
typically indicates agreement even though the yield state-
ments contain little relevant text on their own.
8We found good development-set performance using the
30 tokens before, 20 tokens after, and the name itself.
9Since we are concerned with references that potentially
represent relationships between speech segments, we ignore
references for which the target of the reference did not speak
in the debate in which the reference was made.
Agreement classifier
(?reference?agreement??)
Devel.
set
Test
set
majority baseline 81.51 80.26
Train: no amdmts; ?agr = 0 84.25 81.07
Train: with amdmts; ?agr = 0 86.99 80.10
Table 2: Agreement-classifier accuracy, in per-
cent. ?Amdmts?=?speech segments containing the
word ?amendment??. Recall that boldface indi-
cates results for development-set-optimal settings.
tance of the agr scores. The threshold ?agr con-
trols the precision of the agreement links, in that
values of ?agr greater than zero mean that greater
confidence is required before an agreement link
can be added.10
4 Evaluation
This section presents experiments testing the util-
ity of using speech-segment relationships, evalu-
ating against a number of baselines. All reported
results use values for the free parameter ? derived
via tuning on the development set. In the tables,
boldface indicates the development- and test-set
results for the development-set-optimal parameter
settings, as one would make algorithmic choices
based on development-set performance.
4.1 Preliminaries: Reference classification
Recall that to gather inter-speaker agreement in-
formation, the strategy employed in this paper is
to classify by-name references to other speakers
as to whether they indicate agreement or not.
To train our agreement classifier, we experi-
mented with undoing the deletion of amendment-
related speech segments in the training set. Note
that such speech segments were never included in
the development or test set, since, as discussed in
Section 2, their labels are probably noisy; how-
ever, including them in the training set alows the
classifier to examine more instances even though
some of them are labeled incorrectly. As Table
2 shows, using more, if noisy, data yields bet-
ter agreement-classification results on the devel-
opment set, and so we use that policy in all subse-
quent experiments.11
10Our implementation puts a link between just one arbi-
trary pair of speech segments among all those uttered by a
given pair of apparently agreeing speakers. The ?infinite-
weight? same-speaker links propagate the agreement infor-
mation to all other such pairs.
11Unfortunately, this policy leads to inferior test-set agree-
331
Agreement classifier Precision (in percent):
Devel. set Test set
?agr = 0 86.23 82.55
?agr = ? 89.41 88.47
Table 3: Agreement-classifier precision.
An important observation is that precision may
be more important than accuracy in deciding
which agreement links to add: false positives with
respect to agreement can cause speech segments
to be incorrectly assigned the same label, whereas
false negatives mean only that agreement-based
information about other speech segments is not
employed. As described above, we can raise
agreement precision by increasing the threshold
?agr, which specifies the required confidence for
the addition of an agreement link. Indeed, Table
3 shows that we can improve agreement precision
by setting ?agr to the (positive) mean agreement
score ? assigned by the SVM agreement-classifier
over all references in the given debate12. How-
ever, this comes at the cost of greatly reducing
agreement accuracy (development: 64.38%; test:
66.18%) due to lowered recall levels. Whether
or not better speech-segment classification is ulti-
mately achieved is discussed in the next sections.
4.2 Segment-based speech-segment
classification
Baselines The first two data rows of Table
4 depict baseline performance results. The
#(?support?) ? #(?oppos?) baseline is meant
to explore whether the speech-segment classifica-
tion task can be reduced to simple lexical checks.
Specifically, this method uses the signed differ-
ence between the number of words containing the
stem ?support? and the number of words contain-
ing the stem ?oppos? (returning the majority class
if the difference is 0). No better than 62.67% test-
set accuracy is obtained by either baseline.
Using relationship information Applying an
SVM to classify each speech segment in isolation
leads to clear improvements over the two base-
line methods, as demonstrated in Table 4. When
we impose the constraint that all speech segments
uttered by the same speaker receive the same la-
bel via ?same-speaker links?, both test-set and
ment classification. Section 4.5 contains further discussion.
12We elected not to explicitly tune the value of ?agr in or-
der to minimize the number of free parameters to deal with.
Support/oppose classifer
(?speech segment?yea??)
Devel.
set
Test
set
majority baseline 54.09 58.37
#(?support?)?#(?oppos?) 59.14 62.67
SVM [speech segment] 70.04 66.05
SVM + same-speaker links 79.77 67.21
SVM + same-speaker links . . .
+ agreement links, ?agr = 0 89.11 70.81
+ agreement links, ?agr = ? 87.94 71.16
Table 4: Segment-based speech-segment classifi-
cation accuracy, in percent.
Support/oppose classifer
(?speech segment?yea??)
Devel.
set
Test
set
SVM [speaker] 71.60 70.00
SVM + agreement links . . .
with ?agr = 0 88.72 71.28
with ?agr = ? 84.44 76.05
Table 5: Speaker-based speech-segment classifica-
tion accuracy, in percent. Here, the initial SVM is
run on the concatenation of all of a given speaker?s
speech segments, but the results are computed
over speech segments (not speakers), so that they
can be compared to those in Table 4.
development-set accuracy increase even more, in
the latter case quite substantially so.
The last two lines of Table 4 show that the
best results are obtained by incorporating agree-
ment information as well. The highest test-set re-
sult, 71.16%, is obtained by using a high-precision
threshold to determine which agreement links to
add. While the development-set results would in-
duce us to utilize the standard threshold value of 0,
which is sub-optimal on the test set, the ?agr = 0
agreement-link policy still achieves noticeable im-
provement over not using agreement links (test set:
70.81% vs. 67.21%).
4.3 Speaker-based speech-segment
classification
We use speech segments as the unit of classifica-
tion because they represent natural discourse units.
As a consequence, we are able to exploit relation-
ships at the speech-segment level. However, it is
interesting to consider whether we really need to
consider relationships specifically between speech
segments themselves, or whether it suffices to sim-
ply consider relationships between the speakers
332
of the speech segments. In particular, as an al-
ternative to using same-speaker links, we tried a
speaker-based approach wherein the way we de-
termine the initial individual-document classifica-
tion score for each speech segment uttered by a
person p in a given debate is to run an SVM on the
concatenation of all of p?s speech segments within
that debate. (We also ensure that agreement-link
information is propagated from speech-segment to
speaker pairs.)
How does the use of same-speaker links com-
pare to the concatenation of each speaker?s speech
segments? Tables 4 and 5 show that, not sur-
prisingly, the SVM individual-document classifier
works better on the concatenated speech segments
than on the speech segments in isolation. How-
ever, the effect on overall classification accuracy
is less clear: the development set favors same-
speaker links over concatenation, while the test set
does not.
But we stress that the most important obser-
vation we can make from Table 5 is that once
again, the addition of agreement information leads
to substantial improvements in accuracy.
4.4 ?Hard? agreement constraints
Recall that in in our experiments, we created
finite-weight agreement links, so that speech seg-
ments appearing in pairs flagged by our (imper-
fect) agreement detector can potentially receive
different labels. We also experimented with forc-
ing such speech segments to receive the same la-
bel, either through infinite-weight agreement links
or through a speech-segment concatenation strat-
egy similar to that described in the previous sub-
section. Both strategies resulted in clear degrada-
tion in performance on both the development and
test sets, a finding that validates our encoding of
agreement information as ?soft? preferences.
4.5 On the development/test set split
We have seen several cases in which the method
that performs best on the development set does
not yield the best test-set performance. However,
we felt that it would be illegitimate to change the
train/development/test sets in a post hoc fashion,
that is, after seeing the experimental results.
Moreover, and crucially, it is very clear that
using agreement information, encoded as prefer-
ences within our graph-based approach rather than
as hard constraints, yields substantial improve-
ments on both the development and test set; this,
we believe, is our most important finding.
5 Related work
Politically-oriented text Sentiment analysis has
specifically been proposed as a key enabling tech-
nology in eRulemaking, allowing the automatic
analysis of the opinions that people submit (Shul-
man et al, 2005; Cardie et al, 2006; Kwon et al,
2006). There has also been work focused upon de-
termining the political leaning (e.g., ?liberal? vs.
?conservative?) of a document or author, where
most previously-proposed methods make no di-
rect use of relationships between the documents to
be classified (the ?unlabeled? texts) (Laver et al,
2003; Efron, 2004; Mullen and Malouf, 2006). An
exception is Grefenstette et al (2004), who exper-
imented with determining the political orientation
of websites essentially by classifying the concate-
nation of all the documents found on that site.
Others have applied the NLP technologies of
near-duplicate detection and topic-based text cat-
egorization to politically oriented text (Yang and
Callan, 2005; Purpura and Hillard, 2006).
Detecting agreement We used a simple method
to learn to identify cross-speaker references indi-
cating agreement. More sophisticated approaches
have been proposed (Hillard et al, 2003), in-
cluding an extension that, in an interesting re-
versal of our problem, makes use of sentiment-
polarity indicators within speech segments (Gal-
ley et al, 2004). Also relevant is work on the gen-
eral problems of dialog-act tagging (Stolcke et al,
2000), citation analysis (Lehnert et al, 1990), and
computational rhetorical analysis (Marcu, 2000;
Teufel and Moens, 2002).
We currently do not have an efficient means
to encode disagreement information as hard con-
straints; we plan to investigate incorporating such
information in future work.
Relationships between the unlabeled items
Carvalho and Cohen (2005) consider sequential
relations between different types of emails (e.g.,
between requests and satisfactions thereof) to clas-
sify messages, and thus also explicitly exploit the
structure of conversations.
Previous sentiment-analysis work in different
domains has considered inter-document similar-
ity (Agarwal and Bhattacharyya, 2005; Pang and
Lee, 2005; Goldberg and Zhu, 2006) or explicit
333
inter-document references in the form of hyper-
links (Agrawal et al, 2003).
Notable early papers on graph-based semi-
supervised learning include Blum and Chawla
(2001), Bansal et al (2002), Kondor and Lafferty
(2002), and Joachims (2003). Zhu (2005) main-
tains a survey of this area.
Recently, several alternative, often quite sophis-
ticated approaches to collective classification have
been proposed (Neville and Jensen, 2000; Laf-
ferty et al, 2001; Getoor et al, 2002; Taskar et
al., 2002; Taskar et al, 2003; Taskar et al, 2004;
McCallum and Wellner, 2004). It would be inter-
esting to investigate the application of such meth-
ods to our problem. However, we also believe
that our approach has important advantages, in-
cluding conceptual simplicity and the fact that it is
based on an underlying optimization problem that
is provably and in practice easy to solve.
6 Conclusion and future work
In this study, we focused on very general types
of cross-document classification preferences, uti-
lizing constraints based only on speaker identity
and on direct textual references between state-
ments. We showed that the integration of even
very limited information regarding inter-document
relationships can significantly increase the accu-
racy of support/opposition classification.
The simple constraints modeled in our study,
however, represent just a small portion of the
rich network of relationships that connect state-
ments and speakers across the political universe
and in the wider realm of opinionated social dis-
course. One intriguing possibility is to take ad-
vantage of (readily identifiable) information re-
garding interpersonal relationships, making use of
speaker/author affiliations, positions within a so-
cial hierarchy, and so on. Or, we could even at-
tempt to model relationships between topics or
concepts, in a kind of extension of collaborative
filtering. For example, perhaps we could infer that
two speakers sharing a common opinion on evo-
lutionary biologist Richard Dawkins (a.k.a. ?Dar-
win?s rottweiler?) will be likely to agree in a de-
bate centered on Intelligent Design. While such
functionality is well beyond the scope of our cur-
rent study, we are optimistic that we can develop
methods to exploit additional types of relation-
ships in future work.
Acknowledgments We thank Claire Cardie, Jon
Kleinberg, Michael Macy, Andrew Myers, and the
six anonymous EMNLP referees for valuable dis-
cussions and comments. We also thank Reviewer
1 for generously providing additional post hoc
feedback, and the EMNLP chairs Eric Gaussier
and Dan Jurafsky for facilitating the process (as
well as for allowing authors an extra proceedings
page. . .). This paper is based upon work sup-
ported in part by the National Science Founda-
tion under grant no. IIS-0329064. Any opinions,
findings, and conclusions or recommendations ex-
pressed are those of the authors and do not neces-
sarily reflect the views or official policies, either
expressed or implied, of any sponsoring institu-
tions, the U.S. government, or any other entity.
References
A. Agarwal, P. Bhattacharyya. 2005. Sentiment anal-
ysis: A new approach for effective use of linguis-
tic knowledge and exploiting similarities in a set of
documents to be classified. In Proceedings of the
International Conference on Natural Language Pro-
cessing (ICON).
R. Agrawal, S. Rajagopalan, R. Srikant, Y. Xu. 2003.
Mining newsgroups using networks arising from so-
cial behavior. In Proceedings of WWW, 529?535.
N. Bansal, A. Blum, S. Chawla. 2002. Correla-
tion clustering. In Proceedings of the Symposium
on Foundations of Computer Science (FOCS), 238?
247. Journal version in Machine Learning Journal,
special issue on theoretical advances in data cluster-
ing, 56(1-3):89?113 (2004).
R. Barzilay, M. Lapata. 2005. Collective content selec-
tion for concept-to-text generation. In Proceedings
of HLT/EMNLP, 331?338.
A. Blum, S. Chawla. 2001. Learning from labeled and
unlabeled data using graph mincuts. In Proceedings
of ICML, 19?26.
C. Cardie, C. Farina, T. Bruce, E. Wagner. 2006. Us-
ing natural language processing to improve eRule-
making. In Proceedings of Digital Government Re-
search (dg.o).
V. Carvalho, W. W. Cohen. 2005. On the collective
classification of email ?speech acts?. In Proceedings
of SIGIR, 345?352.
W. Daelemans, V. Hoste. 2002. Evaluation of ma-
chine learning methods for natural language pro-
cessing tasks. In Proceedings of the Third Interna-
tional Conference on Language Resources and Eval-
uation (LREC), 755?760.
S. Das, M. Chen. 2001. Yahoo! for Amazon: Extract-
ing market sentiment from stock message boards. In
Proceedings of the Asia Pacific Finance Association
Annual Conference (APFA).
K. Dave, S. Lawrence, D. M. Pennock. 2003. Mining
the peanut gallery: Opinion extraction and semantic
classification of product reviews. In Proceedings of
WWW, 519?528.
334
M. Efron. 2004. Cultural orientation: Classifying sub-
jective documents by cociation [sic] analysis. In
Proceedings of the AAAI Fall Symposium on Style
and Meaning in Language, Art, Music, and Design,
41?48.
A. Esuli. 2006. Sentiment classification bibliography.
liinwww.ira.uka.de/bibliography/Misc/Sentiment.html.
M. Galley, K. McKeown, J. Hirschberg, E. Shriberg.
2004. Identifying agreement and disagreement in
conversational speech: Use of Bayesian networks to
model pragmatic dependencies. In Proceedings of
the 42nd ACL, 669?676.
L. Getoor, N. Friedman, D. Koller, B. Taskar. 2002.
Learning probabilistic models of relational structure.
Journal of Machine Learning Research, 3:679?707.
Special issue on the Eighteenth ICML.
A. B. Goldberg, J. Zhu. 2006. Seeing stars
when there aren?t many stars: Graph-based semi-
supervised learning for sentiment categorization.
In TextGraphs: HLT/NAACL Workshop on Graph-
based Algorithms for Natural Language Processing.
G. Grefenstette, Y. Qu, J. G. Shanahan, D. A. Evans.
2004. Coupling niche browsers and affect analysis
for an opinion mining application. In Proceedings
of RIAO.
M. Hearst. 1992. Direction-based text interpretation as
an information access refinement. In P. Jacobs, ed.,
Text-Based Intelligent Systems, 257?274. Lawrence
Erlbaum Associates.
D. Hillard, M. Ostendorf, E. Shriberg. 2003. Detection
of agreement vs. disagreement in meetings: Train-
ing with unlabeled data. In Proceedings of HLT-
NAACL.
T. Joachims. 2003. Transductive learning via spectral
graph partitioning. In Proceedings of ICML, 290?
297.
R. I. Kondor, J. D. Lafferty. 2002. Diffusion kernels
on graphs and other discrete input spaces. In Pro-
ceedings of ICML, 315?322.
N. Kwon, S. Shulman, E. Hovy. 2006. Multidimen-
sional text analysis for eRulemaking. In Proceed-
ings of Digital Government Research (dg.o).
J. Lafferty, A. McCallum, F. Pereira. 2001. Condi-
tional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proceedings
of ICML, 282?289.
M. Laver, K. Benoit, J. Garry. 2003. Extracting policy
positions from political texts using words as data.
American Political Science Review.
W. Lehnert, C. Cardie, E. Riloff. 1990. Analyzing re-
search papers using citation sentences. In Program
of the Twelfth Annual Conference of the Cognitive
Science Society, 511?18.
D. Marcu. 2000. The theory and practice of discourse
parsing and summarization. MIT Press.
A. McCallum, B. Wellner. 2004. Conditional mod-
els of identity uncertainty with application to noun
coreference. In Proceedings of NIPS.
T. Mullen, R. Malouf. 2006. A preliminary investiga-
tion into sentiment analysis of informal political dis-
course. In Proceedings of the AAAI Symposium on
Computational Approaches to Analyzing Weblogs,
159?162.
A. Munson, C. Cardie, R. Caruana. 2005. Optimizing
to arbitrary NLP metrics using ensemble selection.
In Proceedings of HLT-EMNLP, 539?546.
J. Neville, D. Jensen. 2000. Iterative classification in
relational data. In Proceedings of the AAAI Work-
shop on Learning Statistical Models from Relational
Data, 13?20.
B. Pang, L. Lee. 2004. A sentimental education:
Sentiment analysis using subjectivity summarization
based on minimum cuts. In Proceedings of the ACL,
271?278.
B. Pang, L. Lee. 2005. Seeing stars: Exploiting class
relationships for sentiment categorization with re-
spect to rating scales. In Proceedings of the ACL.
B. Pang, L. Lee, S. Vaithyanathan. 2002. Thumbs
up? Sentiment classification using machine learning
techniques. In Proceedings of EMNLP, 79?86.
S. Purpura, D. Hillard. 2006. Automated classifica-
tion of congressional legislation. In Proceedings of
Digital Government Research (dg.o).
W. Sack. 1994. On the computation of point of view.
In Proceedings of AAAI, pg. 1488. Student abstract.
S. Shulman, D. Schlosberg. 2002. Electronic rulemak-
ing: New frontiers in public participation. Prepared
for the Annual Meeting of the American Political
Science Association.
S. Shulman, J. Callan, E. Hovy, S. Zavestoski. 2005.
Language processing technologies for electronic
rulemaking: A project highlight. In Proceedings of
Digital Government Research (dg.o), 87?88.
S. S. Smith, J. M. Roberts, R. J. Vander Wielen. 2005.
The American Congress. Cambridge University
Press, fourth edition.
A. Stolcke, N. Coccaro, R. Bates, P. Taylor, C. Van Ess-
Dykema, K. Ries, E. Shriberg, D. Jurafsky, R. Mar-
tin, M. Meteer. 2000. Dialogue act modeling for
automatic tagging and recognition of conversational
speech. Computational Linguistics, 26(3):339?373.
B. Taskar, P. Abbeel, D. Koller. 2002. Discriminative
probabilistic models for relational data. In Proceed-
ings of UAI, Edmonton, Canada.
B. Taskar, C. Guestrin, D. Koller. 2003. Max-margin
Markov networks. In Proceedings of NIPS.
B. Taskar, V. Chatalbashev, D. Koller. 2004. Learn-
ing associative Markov networks. In Proceedings of
ICML.
S. Teufel, M. Moens. 2002. Summarizing scientific
articles: Experiments with relevance and rhetorical
status. Computational Linguistics, 28(4):409?445.
P. Turney. 2002. Thumbs up or thumbs down? Seman-
tic orientation applied to unsupervised classification
of reviews. In Proceedings of the ACL, 417?424.
J. M. Wiebe, W. J. Rapaport. 1988. A computational
theory of perspective and reference in narrative. In
Proceedings of the ACL, 131?138.
J. M. Wiebe. 1994. Tracking point of view in narrative.
Computational Linguistics, 20(2):233?287.
H. Yang, J. Callan. 2005. Near-duplicate detection
for eRulemaking. In Proceedings of Digital Gov-
ernment Research (dg.o).
J. Zhu. 2005. Semi-supervised learning literature
survey. Computer Sciences Technical Report TR
1530, University of Wisconsin-Madison. Available
at http://www.cs.wisc.edu/?jerryzhu/pub/ssl survey.pdf;
has been updated since the initial 2005 version.
335
Proceedings of the 43rd Annual Meeting of the ACL, pages 115?124,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Seeing stars: Exploiting class relationships for sentiment categorization with
respect to rating scales
Bo Pang    and Lillian Lee    
(1) Department of Computer Science, Cornell University
(2) Language Technologies Institute, Carnegie Mellon University
(3) Computer Science Department, Carnegie Mellon University
Abstract
We address the rating-inference problem,
wherein rather than simply decide whether
a review is ?thumbs up? or ?thumbs
down?, as in previous sentiment analy-
sis work, one must determine an author?s
evaluation with respect to a multi-point
scale (e.g., one to five ?stars?). This task
represents an interesting twist on stan-
dard multi-class text categorization be-
cause there are several different degrees
of similarity between class labels; for ex-
ample, ?three stars? is intuitively closer to
?four stars? than to ?one star?.
We first evaluate human performance at
the task. Then, we apply a meta-
algorithm, based on a metric labeling for-
mulation of the problem, that alters a
given  -ary classifier?s output in an ex-
plicit attempt to ensure that similar items
receive similar labels. We show that
the meta-algorithm can provide signifi-
cant improvements over both multi-class
and regression versions of SVMs when we
employ a novel similarity measure appro-
priate to the problem.
1 Introduction
There has recently been a dramatic surge of inter-
est in sentiment analysis, as more and more people
become aware of the scientific challenges posed and
the scope of new applications enabled by the pro-
cessing of subjective language. (The papers col-
lected by Qu, Shanahan, and Wiebe (2004) form a
representative sample of research in the area.) Most
prior work on the specific problem of categorizing
expressly opinionated text has focused on the bi-
nary distinction of positive vs. negative (Turney,
2002; Pang, Lee, and Vaithyanathan, 2002; Dave,
Lawrence, and Pennock, 2003; Yu and Hatzivas-
siloglou, 2003). But it is often helpful to have more
information than this binary distinction provides, es-
pecially if one is ranking items by recommendation
or comparing several reviewers? opinions: example
applications include collaborative filtering and de-
ciding which conference submissions to accept.
Therefore, in this paper we consider generalizing
to finer-grained scales: rather than just determine
whether a review is ?thumbs up? or not, we attempt
to infer the author?s implied numerical rating, such
as ?three stars? or ?four stars?. Note that this differs
from identifying opinion strength (Wilson, Wiebe,
and Hwa, 2004): rants and raves have the same
strength but represent opposite evaluations, and ref-
eree forms often allow one to indicate that one is
very confident (high strength) that a conference sub-
mission is mediocre (middling rating). Also, our
task differs from ranking not only because one can
be given a single item to classify (as opposed to a
set of items to be ordered relative to one another),
but because there are settings in which classification
is harder than ranking, and vice versa.
One can apply standard  -ary classifiers or regres-
sion to this rating-inference problem; independent
work by Koppel and Schler (2005) considers such
115
methods. But an alternative approach that explic-
itly incorporates information about item similarities
together with label similarity information (for in-
stance, ?one star? is closer to ?two stars? than to
?four stars?) is to think of the task as one of met-
ric labeling (Kleinberg and Tardos, 2002), where
label relations are encoded via a distance metric.
This observation yields a meta-algorithm, applicable
to both semi-supervised (via graph-theoretic tech-
niques) and supervised settings, that alters a given
 -ary classifier?s output so that similar items tend to
be assigned similar labels.
In what follows, we first demonstrate that hu-
mans can discern relatively small differences in (hid-
den) evaluation scores, indicating that rating infer-
ence is indeed a meaningful task. We then present
three types of algorithms ? one-vs-all, regression,
and metric labeling ? that can be distinguished by
how explicitly they attempt to leverage similarity
between items and between labels. Next, we con-
sider what item similarity measure to apply, propos-
ing one based on the positive-sentence percentage.
Incorporating this new measure within the metric-
labeling framework is shown to often provide sig-
nificant improvements over the other algorithms.
We hope that some of the insights derived here
might apply to other scales for text classifcation that
have been considered, such as clause-level opin-
ion strength (Wilson, Wiebe, and Hwa, 2004); af-
fect types like disgust (Subasic and Huettner, 2001;
Liu, Lieberman, and Selker, 2003); reading level
(Collins-Thompson and Callan, 2004); and urgency
or criticality (Horvitz, Jacobs, and Hovel, 1999).
2 Problem validation and formulation
We first ran a small pilot study on human subjects
in order to establish a rough idea of what a reason-
able classification granularity is: if even people can-
not accurately infer labels with respect to a five-star
scheme with half stars, say, then we cannot expect a
learning algorithm to do so. Indeed, some potential
obstacles to accurate rating inference include lack
of calibration (e.g., what an understated author in-
tends as high praise may seem lukewarm), author
inconsistency at assigning fine-grained ratings, and
Rating diff. Pooled Subject 1 Subject 2
 or more 100% 100% (35) 100% (15)
2 (e.g., 1 star) 83% 77% (30) 100% (11)
1 (e.g.,   star) 69% 65% (57) 90% (10)
0 55% 47% (15) 80% ( 5)
Table 1: Human accuracy at determining relative
positivity. Rating differences are given in ?notches?.
Parentheses enclose the number of pairs attempted.
ratings not entirely supported by the text1.
For data, we first collected Internet movie reviews
in English from four authors, removing explicit rat-
ing indicators from each document?s text automati-
cally. Now, while the obvious experiment would be
to ask subjects to guess the rating that a review rep-
resents, doing so would force us to specify a fixed
rating-scale granularity in advance. Instead, we ex-
amined people?s ability to discern relative differ-
ences, because by varying the rating differences rep-
resented by the test instances, we can evaluate mul-
tiple granularities in a single experiment. Specifi-
cally, at intervals over a number of weeks, we au-
thors (a non-native and a native speaker of English)
examined pairs of reviews, attemping to determine
whether the first review in each pair was (1) more
positive than, (2) less positive than, or (3) as posi-
tive as the second. The texts in any particular review
pair were taken from the same author to factor out
the effects of cross-author divergence.
As Table 1 shows, both subjects performed per-
fectly when the rating separation was at least 3
?notches? in the original scale (we define a notch
as a half star in a four- or five-star scheme and 10
points in a 100-point scheme). Interestingly, al-
though human performance drops as rating differ-
ence decreases, even at a one-notch separation, both
subjects handily outperformed the random-choice
baseline of 33%. However, there was large variation
in accuracy between subjects.2
1For example, the critic Dennis Schwartz writes that ?some-
times the review itself [indicates] the letter grade should have
been higher or lower, as the review might fail to take into con-
sideration my overall impression of the film ? which I hope to
capture in the grade? (http://www.sover.net/?ozus/cinema.htm).
2One contributing factor may be that the subjects viewed
disjoint document sets, since we wanted to maximize experi-
mental coverage of the types of document pairs within each dif-
ference class. We thus cannot report inter-annotator agreement,
116
Because of this variation, we defined two differ-
ent classification regimes. From the evidence above,
a three-class task (categories 0, 1, and 2 ? es-
sentially ?negative?, ?middling?, and ?positive?, re-
spectively) seems like one that most people would
do quite well at (but we should not assume 100%
human accuracy: according to our one-notch re-
sults, people may misclassify borderline cases like
2.5 stars). Our study also suggests that people could
do at least fairly well at distinguishing full stars in
a zero- to four-star scheme. However, when we
began to construct five-category datasets for each
of our four authors (see below), we found that in
each case, either the most negative or the most pos-
itive class (but not both) contained only about 5%
of the documents. To make the classes more bal-
anced, we folded these minority classes into the ad-
jacent class, thus arriving at a four-class problem
(categories 0-3, increasing in positivity). Note that
the four-class problem seems to offer more possi-
bilities for leveraging class relationship information
than the three-class setting, since it involves more
class pairs. Also, even the two-category version of
the rating-inference problem for movie reviews has
proven quite challenging for many automated clas-
sification techniques (Pang, Lee, and Vaithyanathan,
2002; Turney, 2002).
We applied the above two labeling schemes to
a scale dataset3 containing four corpora of movie
reviews. All reviews were automatically pre-
processed to remove both explicit rating indicators
and objective sentences; the motivation for the latter
step is that it has previously aided positive vs. neg-
ative classification (Pang and Lee, 2004). All of the
1770, 902, 1307, or 1027 documents in a given cor-
pus were written by the same author. This decision
facilitates interpretation of the results, since it fac-
tors out the effects of different choices of methods
for calibrating authors? scales.4 We point out that
but since our goal is to recover a reviewer?s ?true? recommen-
dation, reader-author agreement is more relevant.
While another factor might be degree of English fluency, in
an informal experiment (six subjects viewing the same three
pairs), native English speakers made the only two errors.
3Available at http://www.cs.cornell.edu/People/pabo/movie-
review-data as scale dataset v1.0.
4From the Rotten Tomatoes website?s FAQ: ?star systems
are not consistent between critics. For critics like Roger Ebert
and James Berardinelli, 2.5 stars or lower out of 4 stars is al-
ways negative. For other critics, 2.5 stars can either be positive
it is possible to gather author-specific information
in some practical applications: for instance, systems
that use selected authors (e.g., the Rotten Tomatoes
movie-review website ? where, we note, not all
authors provide explicit ratings) could require that
someone submit rating-labeled samples of newly-
admitted authors? work. Moreover, our results at
least partially generalize to mixed-author situations
(see Section 5.2).
3 Algorithms
Recall that the problem we are considering is multi-
category classification in which the labels can be
naturally mapped to a metric space (e.g., points on a
line); for simplicity, we assume the distance metric

	 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 365?368,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
For the sake of simplicity:
Unsupervised extraction of lexical simplifications from Wikipedia
Mark Yatskar, Bo Pang, Cristian Danescu-Niculescu-Mizil and Lillian Lee
my89@cornell.edu, bopang@yahoo-inc.com, cristian@cs.cornell.edu, llee@cs.cornell.edu
Abstract
We report on work in progress on extract-
ing lexical simplifications (e.g., ?collaborate?
? ?work together?), focusing on utilizing
edit histories in Simple English Wikipedia for
this task. We consider two main approaches:
(1) deriving simplification probabilities via an
edit model that accounts for a mixture of dif-
ferent operations, and (2) using metadata to
focus on edits that are more likely to be sim-
plification operations. We find our methods
to outperform a reasonable baseline and yield
many high-quality lexical simplifications not
included in an independently-created manu-
ally prepared list.
1 Introduction
Nothing is more simple than greatness; indeed, to be
simple is to be great. ?Emerson, Literary Ethics
Style is an important aspect of information pre-
sentation; indeed, different contexts call for differ-
ent styles. Here, we consider an important dimen-
sion of style, namely, simplicity. Systems that can
rewrite text into simpler versions promise to make
information available to a broader audience, such as
non-native speakers, children, laypeople, and so on.
One major effort to produce such text is the
Simple English Wikipedia (henceforth SimpleEW)1,
a sort of spin-off of the well-known English
Wikipedia (henceforth ComplexEW) where hu-
man editors enforce simplicity of language through
rewriting. The crux of our proposal is to learn lexical
simplifications from SimpleEW edit histories, thus
leveraging the efforts of the 18K pseudonymous in-
dividuals who work on SimpleEW. Importantly, not
all the changes on SimpleEW are simplifications; we
thus also make use of ComplexEW edits to filter out
non-simplifications.
Related work and related problems Previous
work usually involves general syntactic-level trans-
1http://simple.wikipedia.org
formation rules [1, 9, 10].2 In contrast, we explore
data-driven methods to learn lexical simplifications
(e.g., ?collaborate? ? ?work together?), which are
highly specific to the lexical items involved and thus
cannot be captured by a few general rules.
Simplification is strongly related to but distinct
from paraphrasing and machine translation (MT).
While it can be considered a directional form of
the former, it differs in spirit because simplification
must trade off meaning preservation (central to para-
phrasing) against complexity reduction (not a con-
sideration in paraphrasing). Simplification can also
be considered to be a form of MT in which the two
?languages? in question are highly related. How-
ever, note that ComplexEW and SimpleEW do not
together constitute a clean parallel corpus, but rather
an extremely noisy comparable corpus. For ex-
ample, Complex/Simple same-topic document pairs
are often written completely independently of each
other, and even when it is possible to get good
sentence alignments between them, the sentence
pairs may reflect operations other than simplifica-
tion, such as corrections, additions, or edit spam.
Our work joins others in using Wikipedia revi-
sions to learn interesting types of directional lexical
relations, e.g, ?eggcorns?3 [7] and entailments [8].
2 Method
As mentioned above, a key idea in our work is to
utilize SimpleEW edits. The primary difficulty in
working with these modifications is that they include
not only simplifications but also edits that serve
other functions, such as spam removal or correction
of grammar or factual content (?fixes?). We describe
two main approaches to this problem: a probabilis-
tic model that captures this mixture of different edit
operations (?2.1), and the use of metadata to filter
out undesirable revisions (?2.2).
2One exception [5] changes verb tense and replaces pro-
nouns. Other lexical-level work focuses on medical text [4, 2],
or uses frequency-filtered WordNet synonyms [3].
3A type of lexical corruption, e.g., ?acorn???eggcorn?.
365
2.1 Edit model
We say that the kth article in a Wikipedia corre-
sponds to (among other things) a title or topic (e.g.,
?Cat?) and a sequence ~dk of article versions caused
by successive edits. For a given lexical item or
phrase A, we write A ? ~dk if there is any version
in ~dk that contains A. From each ~dk we extract a
collection ek = (ek,1, ek,2, . . . , ek,nk) of lexical edit
instances, repeats allowed, where ek,i = A ? a
means that phrase A in one version was changed to
a in the next, A 6= a; e.g., ?stands for? ? ?is the
same as?. (We defer detailed description of how we
extract lexical edit instances from data to ?3.1.) We
denote the collection of ~dk in ComplexEW and Sim-
pleEW as C and S, respectively.
There are at least four possible edit operations: fix
(o1), simplify (o2), no-op (o3), or spam (o4). How-
ever, for this initial work we assume P (o4) = 0.4
Let P (oi | A) be the probability that oi is applied
to A, and P (a | A, oi) be the probability of A ? a
given that the operation is oi. The key quantities of
interest are P (o2 | A) in S, which is the probability
thatA should be simplified, and P (a | A, o2), which
yields proper simplifications of A. We start with an
equation that models the probability that a phrase A
is edited into a:
P (a | A) =
?
oi??
P (oi | A)P (a | A, oi), (1)
where ? is the set of edit operations. This involves
the desired parameters, which we solve for by esti-
mating the others from data, as described next.
Estimation Note that P (a | A, o3) = 0 if A 6= a.
Thus, if we have estimates for o1-related probabili-
ties, we can derive o2-related probabilities via Equa-
tion 1. To begin with, we make the working as-
sumption that occurrences of simplification in Com-
plexEW are negligible in comparison to fixes. Since
we are also currently ignoring edit spam, we thus
assume that only o1 edits occur in ComplexEW.5
Let fC(A) be the fraction of ~dk in C
containing A in which A is modified:
fC(A) =
|{~dk?C|?a,i such that ek,i=A?a}|
|{~dk?C|A?~dk}|
.
4Spam/vandalism detection is a direction for future work.
5This assumption also provides useful constraints to EM,
which we plan to apply in the future, by reducing the number of
parameter settings yielding the same likelihood.
We similarly define fS(A) on ~dk in S. Note that we
count topics (version sequences), not individual ver-
sions: if A appears at some point and is not edited
until 50 revisions later, we should not conclude
that A is unlikely to be rewritten; for example, the
intervening revisions could all be minor additions,
or part of an edit war.
If we assume that the probability of any particular
fix operation being applied in SimpleEW is propor-
tional to that in ComplexEW? e.g., the SimpleEW
fix rate might be dampened because already-edited
ComplexEW articles are copied over ? we have6
P? (o1 | A) = ?fC(A)
where 0 ? ? ? 1. Note that in SimpleEW,
P (o1 ? o2 | A) = P (o1 | A) + P (o2 | A),
where P (o1 ? o2 | A) is the probability that A is
changed to a different word in SimpleEW, which we
estimate as P? (o1 ? o2 | A) = fS(A). We then set
P?(o2 | A) = max (0, fS(A)? ?fC(A)).
Next, under our working assumption, we estimate
the probability of A being changed to a as a fix
by the proportion of ComplexEW edit instances that
rewrite A to a:
P? (a | A, o1) =
|{(k, i) pairs | ek,i = A? a ? ~dk ? C}|
?
a? |{(k, i) pairs | ek,i = A? a
? ? ~dk ? C}|
.
A natural estimate for the conditional probability
of A being rewritten to a under any operation type
is based on observations of A ? a in SimpleEW,
since that is the corpus wherein both operations are
assumed to occur:
P? (a | A) =
|{(k, i) pairs | ek,i = A? a ? ~dk ? S}|
?
a? |{(k, i) pairs | ek,i = A? a
? ? ~dk ? S}|
.
Thus, from (1) we get that for A 6= a:
P?(a | A,o2) =
P?(a | A)? P?(o1 | A)P?(a | A,o1)
P?(o2 | A)
.
2.2 Metadata-based methods
Wiki editors have the option of associating a com-
ment with each revision, and such comments some-
times indicate the intent of the revision. We there-
fore sought to use comments to identify ?trusted?
6Throughout, ?hats? denote estimates.
366
revisions wherein the extracted lexical edit instances
(see ?3.1) would be likely to be simplifications.
Let ~rk = (r1k, . . . , r
i
k, . . .) be the sequence of revi-
sions for the kth article in SimpleEW, where rik is the
set of lexical edit instances (A ? a) extracted from
the ith modification of the document. Let cik be the
comment that accompanies rik, and conversely, let
R(Set) = {rik|c
i
k ? Set}.
We start with a seed set of trusted comments,
Seed. To initialize it, we manually inspected a small
sample of the 700K+ SimpleEW revisions that bear
comments, and found that comments containing a
word matching the regular expression *simpl* (e.g,
?simplify?) seem promising. We thus set Seed :=
{ ? simpl?} (abusing notation).
The SIMPL method Given a set of trusted revi-
sions TRev (in our case TRev = R(Seed)), we
score each A ? a ? TRev by the point-wise mu-
tual information (PMI) between A and a.7 We write
RANK(TRev) to denote the PMI-based ranking of
A? a ? TRev, and use SIMPL to denote our most
basic ranking method, RANK(R(Seed)).
Two ideas for bootstrapping We also considered
bootstrapping as a way to be able to utilize revisions
whose comments are not in the initial Seed set.
Our first idea was to iteratively expand the set
of trusted comments to include those that most of-
ten accompany already highly ranked simplifica-
tions. Unfortunately, our initial implementations in-
volved many parameters (upper and lower comment-
frequency thresholds, number of highly ranked sim-
plifications to consider, number of comments to add
per iteration), making it relatively difficult to tune;
we thus omit its results.
Our second idea was to iteratively expand the
set of trusted revisions, adding those that contain
already highly ranked simplifications. While our
initial implementation had fewer parameters than
the method sketched above, it tended to terminate
quickly, so that not many new simplifications were
found; so, again, we do not report results here.
An important direction for future work is to differ-
entially weight the edit instances within a revision,
as opposed to placing equal trust in all of them; this
7PMI seemed to outperform raw frequency and conditional
probability.
could prevent our bootstrapping methods from giv-
ing common fixes (e.g., ?a?? ?the?) high scores.
3 Evaluation8
3.1 Data
We obtained the revision histories of both Sim-
pleEW (November 2009 snapshot) and ComplexEW
(January 2008 snapshot). In total, ?1.5M revisions
for 81733 SimpleEW articles were processed (only
30% involved textual changes). For ComplexEW,
we processed ?16M revisions for 19407 articles.
Extracting lexical edit instances. For each ar-
ticle, we aligned sentences in each pair of adja-
cent versions using tf-idf scores in a way simi-
lar to Nelken and Shieber [6] (this produced sat-
isfying results because revisions tended to repre-
sent small changes). From the aligned sentence
pairs, we obtained the aforementioned lexical edit
instances A ? a. Since the focus of our study
was not word alignment, we used a simple method
that identified the longest differing segments (based
on word boundaries) between each sentence, except
that to prevent the extraction of entire (highly non-
matching) sentences, we filtered out A ? a pairs if
either A or a contained more than five words.
3.2 Comparison points
Baselines RANDOM returns lexical edit instances
drawn uniformly at random from among those ex-
tracted from SimpleEW. FREQUENT returns the
most frequent lexical edit instances extracted from
SimpleEW.
Dictionary of simplifications The SimpleEW ed-
itor ?Spencerk? (Spencer Kelly) has assembled a list
of simple words and simplifications using a combi-
nation of dictionaries and manual effort9. He pro-
vides a list of 17,900 simple words ? words that do
not need further simplification ? and a list of 2000
transformation pairs. We did not use Spencerk?s set
as the gold standard because many transformations
we found to be reasonable were not on his list. In-
stead, we measured our agreement with the list of
transformations he assembled (SPLIST).
8Results at http://www.cs.cornell.edu/home/llee/data/simple
9http://www.spencerwaterbed.com/soft/simple/about.html
367
3.3 Preliminary results
The top 100 pairs from each system (edit model10
and SIMPL and the two baselines) plus 100 ran-
domly selected pairs from SPLIST were mixed and
all presented in random order to three native English
speakers and three non-native English speakers (all
non-authors). Each pair was presented in random
orientation (i.e., either as A ? a or as a ? A),
and the labels included ?simpler?, ?more complex?,
?equal?, ?unrelated?, and ??? (?hard to judge?). The
first two labels correspond to simplifications for the
orientations A ? a and a ? A, respectively. Col-
lapsing the 5 labels into ?simplification?, ?not a sim-
plification?, and ??? yields reasonable agreement
among the 3 native speakers (? = 0.69; 75.3% of the
time all three agreed on the same label). While we
postulated that non-native speakers11 might be more
sensitive to what was simpler, we note that they dis-
agreed more than the native speakers (? = 0.49) and
reported having to consult a dictionary. The native-
speaker majority label was used in our evaluations.
Here are the results; ?-x-y? means that x and y are
the number of instances discarded from the precision
calculation for having no majority label or majority
label ???, respectively:
Method Prec@100 # of pairs
SPLIST 86% (-0-0) 2000
Edit model 77% (-0-1) 1079
SIMPL 66% (-0-0) 2970
FREQUENT 17% (-1-7) -
RANDOM 17% (-1-4) -
Both baselines yielded very low precisions ?
clearly not all (frequent) edits in SimpleEW were
simplifications. Furthermore, the edit model yielded
higher precision than SIMPL for the top 100 pairs.
(Note that we only examined one simplification per
A for those A where P? (o2 | A) was well-defined;
thus ?# of pairs? does not directly reflect the full
potential recall that either method can achieve.)
Both, however, produced many high-quality pairs
(62% and 71% of the correct pairs) not included in
SPLIST. We also found the pairs produced by these
two systems to be complementary to each other. We
10We only considered those A such that freq(A ? ?) >
1 ? freq(A) > 100 on both SimpleEW and ComplexEW. The
final top 100 A ? a pairs were those with As with the highest
P (o2 | A). We set ? = 1.
11Native languages: Russian; Russian; Russian and Kazakh.
believe that these two approaches provide a good
starting point for further explorations.
Finally, some examples of simplifications found
by our methods: ?stands for? ? ?is the same
as?, ?indigenous? ? ?native?, ?permitted? ? ?al-
lowed?, ?concealed? ? ?hidden?, ?collapsed? ?
?fell down?, ?annually?? ?every year?.
3.4 Future work
Further evaluation could include comparison with
machine-translation and paraphrasing algorithms. It
would be interesting to use our proposed estimates
as initialization for EM-style iterative re-estimation.
Another idea would be to estimate simplification pri-
ors based on a model of inherent lexical complexity;
some possible starting points are number of sylla-
bles (which is used in various readability formulae)
or word length.
Acknowledgments We first wish to thank Ainur Yessenalina
for initial investigations and helpful comments. We are
also thankful to R. Barzilay, T. Bruce, C. Callison-Burch, J.
Cantwell, M. Dredze, C. Napoles, E. Gabrilovich, & the review-
ers for helpful comments; W. Arms and L. Walle for access to
the Cornell Hadoop cluster; J. Cantwell for access to computa-
tional resources; R. Hwa & A. Owens for annotation software;
M. Ulinski for preliminary explorations; J. Cantwell, M. Ott, J.
Silverstein, J. Yatskar, Y. Yatskar, & A. Yessenalina for annota-
tions. Supported by NSF grant IIS-0910664.
References
[1] R. Chandrasekar, B. Srinivas. Automatic induction of rules
for text simplification. Knowledge-Based Systems, 1997.
[2] L. Dele?ger, P. Zweigenbaum. Extracting lay paraphrases
of specialized expressions from monolingual comparable
medical corpora. Workshop on Building and Using Com-
parable Corpora, 2009.
[3] S. Devlin, J. Tait. The use of a psycholinguistic database in
the simplification of text for aphasic readers. In Linguistic
Databases, 1998.
[4] N. Elhadad, K. Sutaria. Mining a lexicon of technical terms
and lay equivalents. Workshop on BioNLP, 2007.
[5] B. Beigman Klebanov, K. Knight, D. Marcu. Text simplifi-
cation for information-seeking applications. OTM Confer-
ences, 2004.
[6] R. Nelken, S. M. Shieber. Towards robust context-sensitive
sentence alignment for monolingual corpora. EACL, 2006.
[7] R. Nelken, E. Yamangil. Mining Wikipedia?s article re-
vision history for training computational linguistics algo-
rithms. WikiAI, 2008.
[8] E. Shnarch, L. Barak, I. Dagan. Extracting lexical reference
rules from Wikipedia. ACL, 2009.
[9] A. Siddharthan, A. Nenkova, K. McKeown. Syntactic
simplification for improving content selection in multi-
document summarization. COLING, 2004.
[10] D. Vickrey, D. Koller. Sentence simplification for seman-
tic role labeling/ ACL, 2008.
368
Proceedings of the ACL 2010 Conference Short Papers, pages 247?252,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Don?t ?have a clue??
Unsupervised co-learning of downward-entailing operators
Cristian Danescu-Niculescu-Mizil and Lillian Lee
Department of Computer Science, Cornell University
cristian@cs.cornell.edu, llee@cs.cornell.edu
Abstract
Researchers in textual entailment have
begun to consider inferences involving
downward-entailing operators, an inter-
esting and important class of lexical items
that change the way inferences are made.
Recent work proposed a method for learn-
ing English downward-entailing operators
that requires access to a high-quality col-
lection of negative polarity items (NPIs).
However, English is one of the very few
languages for which such a list exists. We
propose the first approach that can be ap-
plied to the many languages for which
there is no pre-existing high-precision
database of NPIs. As a case study, we
apply our method to Romanian and show
that our method yields good results. Also,
we perform a cross-linguistic analysis that
suggests interesting connections to some
findings in linguistic typology.
1 Introduction
Cristi: ?Nicio? ... is that adjective you?ve mentioned.
Anca: A negative pronominal adjective.
Cristi: You mean there are people who analyze that
kind of thing?
Anca: The Romanian Academy.
Cristi: They?re crazy.
?From the movie Police, adjective
Downward-entailing operators are an interest-
ing and varied class of lexical items that change
the default way of dealing with certain types of
inferences. They thus play an important role in
understanding natural language [6, 18?20, etc.].
We explain what downward entailing means by
first demonstrating the ?default? behavior, which
is upward entailing. The word ?observed? is an
example upward-entailing operator: the statement
(i) ?Witnesses observed opium use.?
implies
(ii) ?Witnesses observed narcotic use.?
but not vice versa (we write i ? ( 6?) ii). That
is, the truth value is preserved if we replace the
argument of an upward-entailing operator by a su-
perset (a more general version); in our case, the set
?opium use? was replaced by the superset ?narcotic
use?.
Downward-entailing (DE) (also known as
downward monotonic or monotone decreasing)
operators violate this default inference rule: with
DE operators, reasoning instead goes from ?sets to
subsets?. An example is the word ?bans?:
?The law bans opium use?
6? (?)
?The law bans narcotic use?.
Although DE behavior represents an exception to
the default, DE operators are as a class rather com-
mon. They are also quite diverse in sense and
even part of speech. Some are simple negations,
such as ?not?, but some other English DE opera-
tors are ?without?, ?reluctant to?, ?to doubt?, and
?to allow?.1 This variety makes them hard to ex-
tract automatically.
Because DE operators violate the default ?sets
to supersets? inference, identifying them can po-
tentially improve performance in many NLP tasks.
Perhaps the most obvious such tasks are those in-
volving textual entailment, such as question an-
swering, information extraction, summarization,
and the evaluation of machine translation [4]. Re-
searchers are in fact beginning to build textual-
entailment systems that can handle inferences in-
volving downward-entailing operators other than
simple negations, although these systems almost
all rely on small handcrafted lists of DE operators
[1?3, 15, 16].2 Other application areas are natural-
language generation and human-computer interac-
tion, since downward-entailing inferences induce
1Some examples showing different constructions for ana-
lyzing these operators: ?The defendant does not own a blue
car? 6? (?) ?The defendant does not own a car?; ?They are
reluctant to tango? 6? (?) ?They are reluctant to dance?;
?Police doubt Smith threatened Jones? 6? (?) ?Police doubt
Smith threatened Jones or Brown?; ?You are allowed to use
Mastercard? 6? (?) ?You are allowed to use any credit card?.
2The exception [2] employs the list automatically derived
by Danescu-Niculescu-Mizil, Lee, and Ducott [5], described
later.
247
greater cognitive load than inferences in the oppo-
site direction [8].
Most NLP systems for the applications men-
tioned above have only been deployed for a small
subset of languages. A key factor is the lack
of relevant resources for other languages. While
one approach would be to separately develop a
method to acquire such resources for each lan-
guage individually, we instead aim to ameliorate
the resource-scarcity problem in the case of DE
operators wholesale: we propose a single unsuper-
vised method that can extract DE operators in any
language for which raw text corpora exist.
Overview of our work Our approach takes the
English-centric work of Danescu-Niculescu-Mizil
et al [5] ? DLD09 for short ? as a starting point,
as they present the first and, until now, only al-
gorithm for automatically extracting DE operators
from data. However, our work departs signifi-
cantly from DLD09 in the following key respect.
DLD09 critically depends on access to a high-
quality, carefully curated collection of negative
polarity items (NPIs) ? lexical items such as
?any?, ?ever?, or the idiom ?have a clue? that tend
to occur only in negative environments (see ?2
for more details). DLD09 use NPIs as signals of
the occurrence of downward-entailing operators.
However, almost every language other than En-
glish lacks a high-quality accessible NPI list.
To circumvent this problem, we introduce a
knowledge-lean co-learning approach. Our al-
gorithm is initialized with a very small seed set
of NPIs (which we describe how to generate), and
then iterates between (a) discovering a set of DE
operators using a collection of pseudo-NPIs ? a
concept we introduce ? and (b) using the newly-
acquired DE operators to detect new pseudo-NPIs.
Why this isn?t obvious Although the algorith-
mic idea sketched above seems quite simple, it is
important to note that prior experiments in that
direction have not proved fruitful. Preliminary
work on learning (German) NPIs using a small
list of simple known DE operators did not yield
strong results [14]. Hoeksema [10] discusses why
NPIs might be hard to learn from data.3 We cir-
cumvent this problem because we are not inter-
ested in learning NPIs per se; rather, for our pur-
3In fact, humans can have trouble agreeing on NPI-hood;
for instance, Lichte and Soehn [14] mention doubts about
over half of Ku?rschner [12]?s 344 manually collected German
NPIs.
poses, pseudo-NPIs suffice. Also, our prelim-
inary work determined that one of the most fa-
mous co-learning algorithms, hubs and authorities
or HITS [11], is poorly suited to our problem.4
Contributions To begin with, we apply our al-
gorithm to produce the first large list of DE opera-
tors for a language other than English. In our case
study on Romanian (?4), we achieve quite high
precisions at k (for example, iteration achieves a
precision at 30 of 87%).
Auxiliary experiments explore the effects of us-
ing a large but noisy NPI list, should one be avail-
able for the language in question. Intriguingly, we
find that co-learning new pseudo-NPIs provides
better results.
Finally (?5), we engage in some cross-linguistic
analysis based on the results of applying our al-
gorithm to English. We find that there are some
suggestive connections with findings in linguistic
typology.
Appendix available A more complete account
of our work and its implications can be found in a
version of this paper containing appendices, avail-
able at www.cs.cornell.edu/?cristian/acl2010/.
2 DLD09: successes and challenges
In this section, we briefly summarize those aspects
of the DLD09 method that are important to under-
standing how our new co-learning method works.
DE operators and NPIs Acquiring DE opera-
tors is challenging because of the complete lack of
annotated data. DLD09?s insight was to make use
of negative polarity items (NPIs), which are words
or phrases that tend to occur only in negative con-
texts. The reason they did so is that Ladusaw?s hy-
pothesis [7, 13] asserts that NPIs only occur within
the scope of DE operators. Figure 1 depicts exam-
ples involving the English NPIs ?any?5 and ?have
a clue? (in the idiomatic sense) that illustrate this
relationship. Some other English NPIs are ?ever?,
?yet? and ?give a damn?.
Thus, NPIs can be treated as clues that a DE
operator might be present (although DE operators
may also occur without NPIs).
4We explored three different edge-weighting schemes
based on co-occurrence frequencies and seed-set member-
ship, but the results were extremely poor; HITS invariably
retrieved very frequent words.
5The free-choice sense of ?any?, as in ?I can skim any pa-
per in five minutes?, is a known exception.
248
NPIs
DE operators any3 have a clue, idiomatic sense
not or n?t X We do n?t have any apples X We do n?t have a clue
doubt XI doubt they have any apples X I doubt they have a clue
no DE operator ? They have any apples ? They have a clue
Figure 1: Examples consistent with Ladusaw?s hypothesis that NPIs can only occur within the scope of
DE operators. A X denotes an acceptable sentence; a ? denotes an unacceptable sentence.
DLD09 algorithm Potential DE operators are
collected by extracting those words that appear in
an NPI?s context at least once.6 Then, the potential
DE operators x are ranked by
f(x) :=
fraction of NPI contexts that contain x
relative frequency of x in the corpus
,
which compares x?s probability of occurrence
conditioned on the appearance of an NPI with its
probability of occurrence overall.7
The method just outlined requires access to a
list of NPIs. DLD09?s system used a subset of
John Lawler?s carefully curated and ?moderately
complete? list of English NPIs.8 The resultant
rankings of candidate English DE operators were
judged to be of high quality.
The challenge in porting to other languages:
cluelessness Can the unsupervised approach of
DLD09 be successfully applied to languages other
than English? Unfortunately, for most other lan-
guages, it does not seem that large, high-quality
NPI lists are available.
One might wonder whether one can circumvent
the NPI-acquisition problem by simply translating
a known English NPI list into the target language.
However, NPI-hood need not be preserved under
translation [17]. Thus, for most languages, we
lack the critical clues that DLD09 depends on.
3 Getting a clue
In this section, we develop an iterative co-
learning algorithm that can extract DE operators
in the many languages where a high-quality NPI
6DLD09 policies: (a) ?NPI context? was defined as the
part of the sentence to the left of the NPI up to the first
comma, semi-colon or beginning of sentence; (b) to encour-
age the discovery of new DE operators, those sentences con-
taining one of a list of 10 well-known DE operators were dis-
carded. For Romanian, we treated only negations (?nu? and
?n-?) and questions as well-known environments.
7DLD09 used an additional distilled score, but we found
that the distilled score performed worse on Romanian.
8http://www-personal.umich.edu/?jlawler/aue/npi.html
database is not available, using Romanian as a
case study.
3.1 Data and evaluation paradigm
We used Rada Mihalcea?s corpus of?1.45 million
sentences of raw Romanian newswire articles.
Note that we cannot evaluate impact on textual
inference because, to our knowledge, no publicly
available textual-entailment system or evaluation
data for Romanian exists. We therefore examine
the system outputs directly to determine whether
the top-ranked items are actually DE operators or
not. Our evaluation metric is precision at k of a
given system?s ranked list of candidate DE oper-
ators; it is not possible to evaluate recall since no
list of Romanian DE operators exists (a problem
that is precisely the motivation for this paper).
To evaluate the results, two native Romanian
speakers labeled the system outputs as being
?DE?, ?not DE? or ?Hard (to decide)?. The la-
beling protocol, which was somewhat complex
to prevent bias, is described in the externally-
available appendices (?7.1). The complete system
output and annotations are publicly available at:
http://www.cs.cornell.edu/?cristian/acl2010/.
3.2 Generating a seed set
Even though, as discussed above, the translation
of an NPI need not be an NPI, a preliminary re-
view of the literature indicates that in many lan-
guages, there is some NPI that can be translated
as ?any? or related forms like ?anybody?. Thus,
with a small amount of effort, one can form a min-
imal NPI seed set for the DLD09 method by us-
ing an appropriate target-language translation of
?any?. For Romanian, we used ?vreo? and ?vreun?,
which are the feminine and masculine translations
of English ?any?.
3.3 DLD09 using the Romanian seed set
We first check whether DLD09 with the two-
item seed set described in ?3.2 performs well on
Romanian. In fact, the results are fairly poor:
249
0 5 9 10 150
5
10
15
20
25
30
35
40
k=10
k=20
k=30
k=40
k=50
k=80
Iteration
Num
ber 
of D
E?o
pera
tors
10 20 30 40 50 60 70 800
10
20
30
40
50
60
70
80
90
100
k
Pre
cisi
on a
t k (i
n %)
 
 DEHard
Figure 2: Left: Number of DE operators in the top k results returned by the co-learning method at each iteration.
Items labeled ?Hard? are not included. Iteration 0 corresponds to DLD09 applied to {?vreo?, ?vreun?}. Curves for
k = 60 and 70 omitted for clarity. Right: Precisions at k for the results of the 9th iteration. The bar divisions are:
DE (blue/darkest/largest) and Hard (red/lighter, sometimes non-existent).
for example, the precision at 30 is below 50%.
(See blue/dark bars in figure 3 in the externally-
available appendices for detailed results.)
This relatively unsatisfactory performance may
be a consequence of the very small size of the NPI
list employed, and may therefore indicate that it
would be fruitful to investigate automatically ex-
tending our list of clues.
3.4 Main idea: a co-learning approach
Our main insight is that not only can NPIs be used
as clues for finding DE operators, as shown by
DLD09, but conversely, DE operators (if known)
can potentially be used to discover new NPI-like
clues, which we refer to as pseudo-NPIs (or pNPIs
for short). By ?NPI-like? we mean, ?serve as pos-
sible indicators of the presence of DE operators,
regardless of whether they are actually restricted
to negative contexts, as true NPIs are?. For exam-
ple, in English newswire, the words ?allegation? or
?rumor? tend to occur mainly in DE contexts, like
? denied ? or ? dismissed ?, even though they are
clearly not true NPIs (the sentence ?I heard a ru-
mor? is fine). Given this insight, we approach the
problem using an iterative co-learning paradigm
that integrates the search for new DE operators
with a search for new pNPIs.
First, we describe an algorithm that is the ?re-
verse? of DLD09 (henceforth rDLD), in that it re-
trieves and ranks pNPIs assuming a given list of
DE operators. Potential pNPIs are collected by ex-
tracting those words that appear in a DE context
(defined here, to avoid the problems of parsing or
scope determination, as the part of the sentence to
the right of a DE operator, up to the first comma,
semi-colon or end of sentence); these candidates x
are then ranked by
fr(x) :=
fraction of DE contexts that contain x
relative frequency of x in the corpus
.
Then, our co-learning algorithm consists of the
iteration of the following two steps:
? (DE learning) Apply DLD09 using a set N
of pseudo-NPIs to retrieve a list of candidate
DE operators ranked by f (defined in Section
2). Let D be the top n candidates in this list.
? (pNPI learning) Apply rDLD using the set D
to retrieve a list of pNPIs ranked by fr; ex-
tend N with the top nr pNPIs in this list. In-
crement n.
Here, N is initialized with the NPI seed set. At
each iteration, we consider the output of the al-
gorithm to be the ranked list of DE operators re-
trieved in the DE-learning step. In our experi-
ments, we initialized n to 10 and set nr to 1.
4 Romanian results
Our results show that there is indeed favorable
synergy between DE-operator and pNPI retrieval.
Figure 2 plots the number of correctly retrieved
DE operators in the top k outputs at each iteration.
The point at iteration 0 corresponds to a datapoint
already discussed above, namely, DLD09 applied
to the two ?any?-translation NPIs. Clearly, we see
general substantial improvement over DLD09, al-
though the increases level off in later iterations.
250
(Determining how to choose the optimal number
of iterations is a subject for future research.)
Additional experiments, described in the
externally-available appendices (?7.2), suggest
that pNPIs can even be more effective clues than
a noisy list of NPIs. (Thus, a larger seed set
does not necessarily mean better performance.)
pNPIs also have the advantage of being derivable
automatically, and might be worth investigating
from a linguistic perspective in their own right.
5 Cross-linguistic analysis
Applying our algorithm to English: connec-
tions to linguistic typology So far, we have
made no assumptions about the language on which
our algorithm is applied. A valid question is, does
the quality of the results vary with choice of appli-
cation language? In particular, what happens if we
run our algorithm on English?
Note that in some sense, this is a perverse ques-
tion: the motivation behind our algorithm is the
non-existence of a high-quality list of NPIs for
the language in question, and English is essen-
tially the only case that does not fit this descrip-
tion. On the other hand, the fact that DLD09 ap-
plied their method for extraction of DE operators
to English necessitates some form of comparison,
for the sake of experimental completeness.
We thus ran our algorithm on the English
BLLIP newswire corpus with seed set {?any?} .
We observe that, surprisingly, the iterative addi-
tion of pNPIs has very little effect: the precisions
at k are good at the beginning and stay about the
same across iterations (for details see figure 5 in
in the externally-available appendices). Thus, on
English, co-learning does not hurt performance,
which is good news; but unlike in Romanian, it
does not lead to improvements.
Why is English ?any? seemingly so ?powerful?,
in contrast to Romanian, where iterating beyond
the initial ?any? translations leads to better re-
sults? Interestingly, findings from linguistic typol-
ogy may shed some light on this issue. Haspel-
math [9] compares the functions of indefinite pro-
nouns in 40 languages. He shows that English is
one of the minority of languages (11 out of 40)9 in
which there exists an indefinite pronoun series that
occurs in all (Haspelmath?s) classes of DE con-
texts, and thus can constitute a sufficient seed on
9English, Ancash Quechua, Basque, Catalan, French,
Hindi/Urdu, Irish, Portuguese, Swahili, Swedish, Turkish.
its own. In the other languages (including Roma-
nian),10 no indirect pronoun can serve as a suffi-
cient seed. So, we expect our method to be vi-
able for all languages; while the iterative discov-
ery of pNPIs is not necessary (although neither is
it harmful) for the subset of languages for which a
sufficient seed exists, such as English, it is essen-
tial for the languages for which, like Romanian,
?any?-equivalents do not suffice.
Using translation Another interesting question
is whether directly translating DE operators from
English is an alternative to our method. First, we
emphasize that there exists no complete list of En-
glish DE operators (the largest available collec-
tion is the one extracted by DLD09). Second, we
do not know whether DE operators in one lan-
guage translate into DE operators in another lan-
guage. Even if that were the case, and we some-
how had access to ideal translations of DLD09?s
list, there would still be considerable value in us-
ing our method: 14 (39%) of our top 36 highest-
ranked Romanian DE operators for iteration 9 do
not, according to the Romanian-speaking author,
have English equivalents appearing on DLD09?s
90-item list. Some examples are: ?abt?inut? (ab-
stained), ?criticat? (criticized) and ?react?ionat? (re-
acted). Therefore, a significant fraction of the
DE operators derived by our co-learning algorithm
would have been missed by the translation alterna-
tive even under ideal conditions.
6 Conclusions
We have introduced the first method for discov-
ering downward-entailing operators that is univer-
sally applicable. Previous work on automatically
detecting DE operators assumed the existence of
a high-quality collection of NPIs, which renders it
inapplicable in most languages, where such a re-
source does not exist. We overcome this limita-
tion by employing a novel co-learning approach,
and demonstrate its effectiveness on Romanian.
Also, we introduce the concept of pseudo-NPIs.
Auxiliary experiments described in the externally-
available appendices show that pNPIs are actually
more effective seeds than a noisy ?true? NPI list.
Finally, we noted some cross-linguistic differ-
ences in performance, and found an interesting
connection between these differences and Haspel-
math?s [9] characterization of cross-linguistic vari-
ation in the occurrence of indefinite pronouns.
10Examples: Chinese, German, Italian, Polish, Serbian.
251
Acknowledgments We thank Tudor Marian for
serving as an annotator, Rada Mihalcea for ac-
cess to the Romanian newswire corpus, and Claire
Cardie, Yejin Choi, Effi Georgala, Mark Liber-
man, Myle Ott, Joa?o Paula Muchado, Stephen Pur-
pura, Mark Yatskar, Ainur Yessenalina, and the
anonymous reviewers for their helpful comments.
Supported by NSF grant IIS-0910664.
References
[1] Roy Bar-Haim, Jonathan Berant, Ido Da-
gan, Iddo Greental, Shachar Mirkin, Eyal
Shnarch, and Idan Szpektor. Efficient seman-
tic deduction and approximate matching over
compact parse forests. In Proceedings of the
Text Analysis Conference (TAC), 2008.
[2] Eric Breck. A simple system for detecting
non-entailment. In Proceedings of the Text
Analysis Conference (TAC), 2009.
[3] Christos Christodoulopoulos. Creating a nat-
ural logic inference system with combinatory
categorial grammar. Master?s thesis, Univer-
sity of Edinburgh, 2008.
[4] Ido Dagan, Oren Glickman, and Bernardo
Magnini. The PASCAL Recognising Textual
Entailment challenge. In Machine Learn-
ing Challenges, Evaluating Predictive Un-
certainty, Visual Object Classification and
Recognizing Textual Entailment, First PAS-
CAL Machine Learning Challenges Work-
shop, pages 177?190. Springer, 2006.
[5] Cristian Danescu-Niculescu-Mizil, Lillian
Lee, and Richard Ducott. Without a ?doubt??
Unsupervised discovery of downward-
entailing operators. In Proceedings of
NAACL HLT, 2009.
[6] David Dowty. The role of negative polar-
ity and concord marking in natural language
reasoning. In Mandy Harvey and Lynn San-
telmann, editors, Proceedings of SALT IV,
pages 114?144, 1994.
[7] Gilles Fauconnier. Polarity and the scale
principle. In Proceedings of the Chicago Lin-
guistic Society (CLS), pages 188?199, 1975.
Reprinted in Javier Gutierrez-Rexach (ed.),
Semantics: Critical Concepts in Linguistics,
2003.
[8] Bart Geurts and Frans van der Slik. Mono-
tonicity and processing load. Journal of Se-
mantics, 22(1):97?117, 2005.
[9] Martin Haspelmath. Indefinite Pronouns.
Oxford University Press, 2001.
[10] Jack Hoeksema. Corpus study of negative
polarity items. IV-V Jornades de corpus lin-
guistics 1996-1997, 1997. http://odur.let.rug.
nl/?hoeksema/docs/barcelona.html.
[11] Jon Kleinberg. Authoritative sources in a hy-
perlinked environment. In Proceedings of
the 9th ACM-SIAM Symposium on Discrete
Algorithms (SODA), pages 668?677, 1998.
Extended version in Journal of the ACM,
46:604?632, 1999.
[12] Wilfried Ku?rschner. Studien zur Negation im
Deutschen. Narr, 1983.
[13] William A. Ladusaw. Polarity Sensitivity as
Inherent Scope Relations. Garland Press,
New York, 1980. Ph.D. thesis date 1979.
[14] Timm Lichte and Jan-Philipp Soehn. The re-
trieval and classification of Negative Polar-
ity Items using statistical profiles. In Sam
Featherston and Wolfgang Sternefeld, edi-
tors, Roots: Linguistics in Search of its Ev-
idential Base, pages 249?266. Mouton de
Gruyter, 2007.
[15] Bill MacCartney and Christopher D. Man-
ning. Modeling semantic containment and
exclusion in natural language inference. In
Proceedings of COLING, pages 521?528,
2008.
[16] Rowan Nairn, Cleo Condoravdi, and Lauri
Karttunen. Computing relative polarity for
textual inference. In Proceedings of In-
ference in Computational Semantics (ICoS),
2006.
[17] Frank Richter, Janina Rado?, and Manfred
Sailer. Negative polarity items: Corpus
linguistics, semantics, and psycholinguis-
tics: Day 2: Corpus linguistics. Tutorial
slides: http://www.sfs.uni-tuebingen.de/?fr/
esslli/08/byday/day2/day2-part1.pdf, 2008.
[18] V??ctor Sa?nchez Valencia. Studies on natural
logic and categorial grammar. PhD thesis,
University of Amsterdam, 1991.
[19] Johan van Benthem. Essays in Logical Se-
mantics. Reidel, Dordrecht, 1986.
[20] Ton van der Wouden. Negative contexts:
Collocation, polarity and multiple negation.
Routledge, 1997.
252
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 892?901,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
You Had Me at Hello: How Phrasing Affects Memorability
Cristian Danescu-Niculescu-Mizil Justin Cheng Jon Kleinberg Lillian Lee
Department of Computer Science
Cornell University
cristian@cs.cornell.edu, jc882@cornell.edu, kleinber@cs.cornell.edu, llee@cs.cornell.edu
Abstract
Understanding the ways in which information
achieves widespread public awareness is a re-
search question of significant interest. We
consider whether, and how, the way in which
the information is phrased ? the choice of
words and sentence structure ? can affect this
process. To this end, we develop an analy-
sis framework and build a corpus of movie
quotes, annotated with memorability informa-
tion, in which we are able to control for both
the speaker and the setting of the quotes. We
find that there are significant differences be-
tween memorable and non-memorable quotes
in several key dimensions, even after control-
ling for situational and contextual factors. One
is lexical distinctiveness: in aggregate, memo-
rable quotes use less common word choices,
but at the same time are built upon a scaf-
folding of common syntactic patterns. An-
other is that memorable quotes tend to be more
general in ways that make them easy to ap-
ply in new contexts ? that is, more portable.
We also show how the concept of ?memorable
language? can be extended across domains.
1 Hello. My name is Inigo Montoya.
Understanding what items will be retained in the
public consciousness, and why, is a question of fun-
damental interest in many domains, including mar-
keting, politics, entertainment, and social media; as
we all know, many items barely register, whereas
others catch on and take hold in many people?s
minds.
An active line of recent computational work has
employed a variety of perspectives on this question.
Building on a foundation in the sociology of diffu-
sion [27, 31], researchers have explored the ways in
which network structure affects the way information
spreads, with domains of interest including blogs
[1, 11], email [37], on-line commerce [22], and so-
cial media [2, 28, 33, 38]. There has also been recent
research addressing temporal aspects of how differ-
ent media sources convey information [23, 30, 39]
and ways in which people react differently to infor-
mation on different topics [28, 36].
Beyond all these factors, however, one?s everyday
experience with these domains suggests that the way
in which a piece of information is expressed ? the
choice of words, the way it is phrased ? might also
have a fundamental effect on the extent to which it
takes hold in people?s minds. Concepts that attain
wide reach are often carried in messages such as
political slogans, marketing phrases, or aphorisms
whose language seems intuitively to be memorable,
?catchy,? or otherwise compelling.
Our first challenge in exploring this hypothesis is
to develop a notion of ?successful? language that is
precise enough to allow for quantitative evaluation.
We also face the challenge of devising an evaluation
setting that separates the phrasing of a message from
the conditions in which it was delivered ? highly-
cited quotes tend to have been delivered under com-
pelling circumstances or fit an existing cultural, po-
litical, or social narrative, and potentially what ap-
peals to us about the quote is really just its invoca-
tion of these extra-linguistic contexts. Is the form
of the language adding an effect beyond or indepen-
dent of these (obviously very crucial) factors? To
investigate the question, one needs a way of control-
892
ling ? as much as possible ? for the role that the
surrounding context of the language plays.
The present work (i): Evaluating language-based
memorability Defining what makes an utterance
memorable is subtle, and scholars in several do-
mains have written about this question. There is
a rough consensus that an appropriate definition
involves elements of both recognition ? people
should be able to retain the quote and recognize it
when they hear it invoked ? and production ? peo-
ple should be motivated to refer to it in relevant sit-
uations [15]. One suggested reason for why some
memes succeed is their ability to provoke emotions
[16]. Alternatively, memorable quotes can be good
for expressing the feelings, mood, or situation of an
individual, a group, or a culture (the zeitgeist): ?Cer-
tain quotes exquisitely capture the mood or feeling
we wish to communicate to someone. We hear them
... and store them away for future use? [10].
None of these observations, however, serve as
definitions, and indeed, we believe it desirable to
not pre-commit to an abstract definition, but rather
to adopt an operational formulation based on exter-
nal human judgments. In designing our study, we
focus on a domain in which (i) there is rich use of
language, some of which has achieved deep cultural
penetration; (ii) there already exist a large number of
external human judgments ? perhaps implicit, but
in a form we can extract; and (iii) we can control for
the setting in which the text was used.
Specifically, we use the complete scripts of
roughly 1000 movies, representing diverse genres,
eras, and levels of popularity, and consider which
lines are the most ?memorable?. To acquire memo-
rability labels, for each sentence in each script, we
determine whether it has been listed as a ?memo-
rable quote? by users of the widely-known IMDb
(the Internet Movie Database), and also estimate the
number of times it appears on the Web. Both of these
serve as memorability metrics for our purposes.
When we evaluate properties of memorable
quotes, we compare them with quotes that are not as-
sessed as memorable, but were spoken by the same
character, at approximately the same point in the
same movie. This enables us to control in a fairly
fine-grained way for the confounding effects of con-
text discussed above: we can observe differences
that persist even after taking into account both the
speaker and the setting.
In a pilot validation study, we find that human
subjects are effective at recognizing the more IMDb-
memorable of two quotes, even for movies they have
not seen. This motivates a search for features in-
trinsic to the text of quotes that signal memorabil-
ity. In fact, comments provided by the human sub-
jects as part of the task suggested two basic forms
that such textual signals could take: subjects felt that
(i) memorable quotes often involve a distinctive turn
of phrase; and (ii) memorable quotes tend to invoke
general themes that aren?t tied to the specific setting
they came from, and hence can be more easily in-
voked for future (out of context) uses. We test both
of these principles in our analysis of the data.
The present work (ii): What distinguishes mem-
orable quotes Under the controlled-comparison
setting sketched above, we find that memorable
quotes exhibit significant differences from non-
memorable quotes in several fundamental respects,
and these differences in the data reinforce the two
main principles from the human pilot study. First,
we show a concrete sense in which memorable
quotes are indeed distinctive: with respect to lexi-
cal language models trained on the newswire por-
tions of the Brown corpus [21], memorable quotes
have significantly lower likelihood than their non-
memorable counterparts. Interestingly, this distinc-
tiveness takes place at the level of words, but not
at the level of other syntactic features: the part-of-
speech composition of memorable quotes is in fact
more likely with respect to newswire. Thus, we can
think of memorable quotes as consisting, in an ag-
gregate sense, of unusual word choices built on a
scaffolding of common part-of-speech patterns.
We also identify a number of ways in which mem-
orable quotes convey greater generality. In their pat-
terns of verb tenses, personal pronouns, and deter-
miners, memorable quotes are structured so as to be
more ?free-standing,? containing fewer markers that
indicate references to nearby text.
Memorable quotes differ in other interesting as-
pects as well, such as sound distributions.
Our analysis of memorable movie quotes suggests
a framework by which the memorability of text in
a range of different domains could be investigated.
893
We provide evidence that such cross-domain prop-
erties may hold, guided by one of our motivating
applications in marketing. In particular, we analyze
a corpus of advertising slogans, and we show that
these slogans have significantly greater likelihood
at both the word level and the part-of-speech level
with respect to a language model trained on mem-
orable movie quotes, compared to a corresponding
language model trained on non-memorable movie
quotes. This suggests that some of the principles un-
derlying memorable text have the potential to apply
across different areas.
Roadmap ?2 lays the empirical foundations of our
work: the design and creation of our movie-quotes
dataset, which we make publicly available (?2.1), a
pilot study with human subjects validating IMDb-
based memorability labels (?2.2), and further study
of incorporating search-engine counts (?2.3). ?3 de-
tails our analysis and prediction experiments, using
both movie-quotes data and, as an exploration of
cross-domain applicability, slogans data. ?4 surveys
related work across a variety of fields. ?5 briefly
summarizes and indicates some future directions.
2 I?m ready for my close-up.
2.1 Data
To study the properties of memorable movie quotes,
we need a source of movie lines and a designation
of memorability. Following [8], we constructed a
corpus consisting of all lines from roughly 1000
movies, varying in genre, era, and popularity; for
each movie, we then extracted the list of quotes from
IMDb?s Memorable Quotes page corresponding to
the movie.1
A memorable quote in IMDb can appear either as
an individual sentence spoken by one character, or
as a multi-sentence line, or as a block of dialogue in-
volving multiple characters. In the latter two cases,
it can be hard to determine which particular portion
is viewed as memorable (some involve a build-up to
a punch line; others involve the follow-through after
a well-phrased opening sentence), and so we focus
in our comparisons on those memorable quotes that
1This extraction involved some edit-distance-based align-
ment, since the exact form of the line in the script can exhibit
minor differences from the version typed into IMDb.
1 2 3 4 5 6 7 8 9 10Decile0
100
200
300
400
500
600
700
800
Num
ber o
f mem
orabl
e quo
tes
Figure 1: Location of memorable quotes in each decile
of movie scripts (the first 10th, the second 10th, etc.),
summed over all movies. The same qualitative results
hold if we discard each movie?s very first and last line,
which might have privileged status.
appear as a single sentence rather than a multi-line
block.2
We now formulate a task that we can use to eval-
uate the features of memorable quotes. Recall that
our goal is to identify effects based in the language
of the quotes themselves, beyond any factors arising
from the speaker or context. Thus, for each (single-
sentence) memorable quote M , we identify a non-
memorable quote that is as similar as possible to M
in all characteristics but the choice of words. This
means we want it to be spoken by the same charac-
ter in the same movie. It also means that we want
it to have the same length: controlling for length is
important because we expect that on average, shorter
quotes will be easier to remember than long quotes,
and that wouldn?t be an interesting textual effect to
report. Moreover, we also want to control for the
fact that a quote?s position in a movie can affect
memorability: certain scenes produce more mem-
orable dialogue, and as Figure 1 demonstrates, in
aggregate memorable quotes also occur dispropor-
tionately near the beginnings and especially the ends
of movies. In summary, then, for each M , we pick a
contrasting (single-sentence) quote N from the same
movie that is as close in the script as possible to M
(either before or after it), subject to the conditions
that (i) M and N are uttered by the same speaker,
(ii) M and N have the same number of words, and
(iii) N does not occur in the IMDb list of memorable
2We also ran experiments relaxing the single-sentence as-
sumption, which allows for stricter scene control and a larger
dataset but complicates comparisons involving syntax. The
non-syntax results were in line with those reported here.
894
Movie First Quote Second Quote
Jackie Brown Half a million dollars will always be missed. I know the type, trust me on this.
Star Trek: Nemesis I think it?s time to try some unsafe velocities. No cold feet, or any other parts of our
anatomy.
Ordinary People A little advice about feelings kiddo; don?t ex-
pect it always to tickle.
I mean there?s someone besides your
mother you?ve got to forgive.
Table 1: Three example pairs of movie quotes. Each pair satisfies our criteria: the two component quotes are spoken
close together in the movie by the same character, have the same length, and one is labeled memorable by the IMDb
while the other is not. (Contractions such as ?it?s? count as two words.)
quotes for the movie (either as a single line or as part
of a larger block).
Given such pairs, we formulate a pairwise com-
parison task: given M and N , determine which is
the memorable quote. Psychological research on
subjective evaluation [35], as well as initial experi-
ments using ourselves as subjects, indicated that this
pairwise set-up easier to work with than simply pre-
senting a single sentence and asking whether it is
memorable or not; the latter requires agreement on
an ?absolute? criterion for memorability that is very
hard to impose consistently, whereas the former sim-
ply requires a judgment that one quote is more mem-
orable than another.
Our main dataset, available at http://www.cs.
cornell.edu/?cristian/memorability.html,3 thus con-
sists of approximately 2200 such (M,N) pairs, sep-
arated by a median of 5 same-character lines in the
script. The reader can get a sense for the nature of
the data from the three examples in Table 1.
We now discuss two further aspects to the formu-
lation of the experiment: a preliminary pilot study
involving human subjects, and the incorporation of
search engine counts into the data.
2.2 Pilot study: Human performance
As a preliminary consideration, we did a small pilot
study to see if humans can distinguish memorable
from non-memorable quotes, assuming our IMDB-
induced labels as gold standard. Six subjects, all na-
tive speakers of English and none an author of this
paper, were presented with 11 or 12 pairs of mem-
orable vs. non-memorable quotes; again, we con-
trolled for extra-textual effects by ensuring that in
each pair the two quotes come from the same movie,
are by the same character, have the same length, and
3Also available there: other examples and factoids.
subject number of matches with
IMDb-induced annotation
A 11/11 = 100%
B 11/12 = 92%
C 9/11 = 82%
D 8/11 = 73%
E 7/11 = 64%
F 7/12 = 58%
macro avg ? 78%
Table 2: Human pilot study: number of matches to
IMDb-induced annotation, ordered by decreasing match
percentage. For the null hypothesis of random guessing,
these results are statistically significant, p < 2?6 ? .016.
appear as nearly as possible in the same scene.4 The
order of quotes within pairs was randomized. Im-
portantly, because we wanted to understand whether
the language of the quotes by itself contains signals
about memorability, we chose quotes from movies
that the subjects said they had not seen. (This means
that each subject saw a different set of quotes.)
Moreover, the subjects were requested not to consult
any external sources of information.5 The reader is
welcome to try a demo version of the task at http:
//www.cs.cornell.edu/?cristian/memorability.html.
Table 2 shows that all the subjects performed
(sometimes much) better than chance, and against
the null hypothesis that all subjects are guessing ran-
domly, the results are statistically significant, p <
2?6 ? .016. These preliminary findings provide ev-
idence for the validity of our task: despite the appar-
ent difficulty of the job, even humans who haven?t
seen the movie in question can recover our IMDb-
4In this pilot study, we allowed multi-sentence quotes.
5We did not use crowd-sourcing because we saw no way to
ensure that this condition would be obeyed by arbitrary subjects.
We do note, though, that after our research was completed and
as of Apr. 26, 2012, ? 11,300 people completed the online test:
average accuracy: 72%, mode number correct: 9/12.
895
induced labels with some reliability.6
2.3 Incorporating search engine counts
Thus far we have discussed a dataset in which mem-
orability is determined through an explicit label-
ing drawn from the IMDb. Given the ?produc-
tion? aspect of memorability discussed in ?1, we
should also expect that memorable quotes will tend
to appear more extensively on Web pages than non-
memorable quotes; note that incorporating this in-
sight makes it possible to use the (implicit) judg-
ments of a much larger number of people than are
represented by the IMDb database. It therefore
makes sense to try using search-engine result counts
as a second indication of memorability.
We experimented with several ways of construct-
ing memorability information from search-engine
counts, but this proved challenging. Searching for
a quote as a stand-alone phrase runs into the prob-
lem that a number of quotes are also sentences that
people use without the movie in mind, and so high
counts for such quotes do not testify to the phrase?s
status as a memorable quote from the movie. On
the other hand, searching for the quote in a Boolean
conjunction with the movie?s title discards most of
these uses, but also eliminates a large fraction of
the appearances on the Web that we want to find:
precisely because memorable quotes tend to have
widespread cultural usage, people generally don?t
feel the need to include the movie?s title when in-
voking them. Finally, since we are dealing with
roughly 1000 movies, the result counts vary over an
enormous range, from recent blockbusters to movies
with relatively small fan bases.
In the end, we found that it was more effective to
use the result counts in conjunction with the IMDb
labels, so that the counts played the role of an ad-
ditional filter rather than a free-standing numerical
value. Thus, for each pair (M,N) produced using
the IMDb methodology above, we searched for each
of M and N as quoted expressions in a Boolean con-
junction with the title of the movie. We then kept
only those pairs for which M (i) produced more than
five results in our (quoted, conjoined) search, and (ii)
produced at least twice as many results as the cor-
6The average accuracy being below 100% reinforces that
context is very important, too.
responding search for N . We created a version of
this filtered dataset using each of Google and Bing,
and all the main findings were consistent with the
results on the IMDb-only dataset. Thus, in what fol-
lows, we will focus on the main IMDb-only dataset,
discussing the relationship to the dataset filtered by
search engine counts where relevant (in which case
we will refer to the +Google dataset).
3 Never send a human to do a machine?s job.
We now discuss experiments that investigate the hy-
potheses discussed in ?1. In particular, we devise
methods that can assess the distinctiveness and gen-
erality hypotheses and test whether there exists a no-
tion of ?memorable language? that operates across
domains. In addition, we evaluate and compare the
predictive power of these hypotheses.
3.1 Distinctiveness
One of the hypotheses we examine is whether the
use of language in memorable quotes is to some ex-
tent unusual. In order to quantify the level of dis-
tinctiveness of a quote, we take a language-model
approach: we model ?common language? using
the newswire sections of the Brown corpus [21]7,
and evaluate how distinctive a quote is by evaluat-
ing its likelihood with respect to this model ? the
lower the likelihood, the more distinctive. In or-
der to assess different levels of lexical and syntactic
distinctiveness, we employ a total of six Laplace-
smoothed8 language models: 1-gram, 2-gram, and
3-gram word LMs and 1-gram, 2-gram and 3-gram
part-of-speech9 LMs.
We find strong evidence that from a lexical per-
spective, memorable quotes are more distinctive
than their non-memorable counterparts. As indi-
cated in Table 3, for each of our lexical ?common
language? models, in about 60% of the quote pairs,
the memorable quote is more distinctive.
Interestingly, the reverse is true when it comes to
7Results were qualitatively similar if we used the fiction por-
tions. The age of the Brown corpus makes it less likely to con-
tain modern movie quotes.
8We employ Laplace (additive) smoothing with a smoothing
parameter of 0.2. The language models? vocabulary was that of
the entire training corpus.
9Throughout we obtain part-of-speech tags by using the
NLTK maximum entropy tagger with default parameters.
896
?common language?
model
IMDb-only +Google
lexical
1-gram 61.13%??? 59.21%???
2-gram 59.22%??? 57.03%???
3-gram 59.81%??? 58.32%???
syntactic
1-gram 43.60%??? 44.77%???
2-gram 48.31% 47.84%
3-gram 50.91% 50.92%
Table 3: Distinctiveness: percentage of quote pairs
in which the the memorable quote is more distinctive
than the non-memorable one according to the respec-
tive ?common language? model. Significance accord-
ing to a two-tailed sign test is indicated using *-notation
(???=?p<.001?).
syntax: memorable quotes appear to follow the syn-
tactic patterns of ?common language? as closely as
or more closely than non-memorable quotes. To-
gether, these results suggest that memorable quotes
consist of unusual word sequences built on common
syntactic scaffolding.
3.2 Generality
Another of our hypotheses is that memorable quotes
are easier to use outside the specific context in which
they were uttered ? that is, more ?portable? ? and
therefore exhibit fewer terms that refer to those set-
tings. We use the following syntactic properties as
proxies for the generality of a quote:
? Fewer 3rd-person pronouns, since these com-
monly refer to a person or object that was intro-
duced earlier in the discourse. Utterances that
employ fewer such pronouns are easier to adapt
to new contexts, and so will be considered more
general.
? More indefinite articles like a and an, since
they are more likely to refer to general concepts
than definite articles. Quotes with more indefi-
nite articles will be considered more general.
? Fewer past tense verbs and more present
tense verbs, since the former are more likely
to refer to specific previous events. Therefore
utterances that employ fewer past tense verbs
(and more present tense verbs) will be consid-
ered more general.
Table 4 gives the results for each of these four
metrics ? in each case, we show the percentage of
Generality metric IMDb-only +Google
fewer 3rd pers. pronouns 64.37%??? 62.93%???
more indef. article 57.21%??? 58.23%???
less past tense 57.91%??? 59.74%???
more present tense 54.60%??? 55.86%???
Table 4: Generality: percentage of quote pairs in which
the memorable quote is more general than the non-
memorable ones according to the respective metric. Pairs
where the metric does not distinguish between the quotes
are not considered.
quote pairs for which the memorable quote scores
better on the generality metric.
Note that because the issue of generality is a com-
plex one for which there is no straightforward single
metric, our approach here is based on several prox-
ies for generality, considered independently; yet, as
the results show, all of these point in a consistent
direction. It is an interesting open question to de-
velop richer ways of assessing whether a quote has
greater generality, in the sense that people intuitively
attribute to memorable quotes.
3.3 ?Memorable? language beyond movies
One of the motivating questions in our analysis
is whether there are general principles underlying
?memorable language.? The results thus far suggest
potential families of such principles. A further ques-
tion in this direction is whether the notion of mem-
orability can be extended across different domains,
and for this we collected (and distribute on our web-
site) 431 phrases that were explicitly designed to
be memorable: advertising slogans (e.g., ?Quality
never goes out of style.?). The focus on slogans is
also in keeping with one of the initial motivations
in studying memorability, namely, marketing appli-
cations ? in other words, assessing whether a pro-
posed slogan has features that are consistent with
memorable text.
The fact that it?s not clear how to construct a col-
lection of ?non-memorable? counterparts to slogans
appears to pose a technical challenge. However, we
can still use a language-modeling approach to as-
sess whether the textual properties of the slogans are
closer to the memorable movie quotes (as one would
conjecture) or to the non-memorable movie quotes.
Specifically, we train one language model on memo-
rable quotes and another on non-memorable quotes
897
(Non)memorable
language models
Slogans Newswire
lexical
1-gram 56.15%?? 33.77%???
2-gram 51.51% 25.15%???
3-gram 52.44% 28.89%???
syntactic
1-gram 73.09%??? 68.27%???
2-gram 64.04%??? 50.21%
3-gram 62.88%??? 55.09%???
Table 5: Cross-domain concept of ?memorable? lan-
guage: percentage of slogans that have higher likelihood
under the memorable language model than under the non-
memorable one (for each of the six language models con-
sidered). Rightmost column: for reference, the percent-
age of newswire sentences that have higher likelihood un-
der the memorable language model than under the non-
memorable one.
Generality metric slogans mem. n-mem.
% 3rd pers. pronouns 2.14% 2.16% 3.41%
% indefinite articles 2.68% 2.63% 2.06%
% past tense 14.60% 21.13% 26.69%
Table 6: Slogans are most general when compared to
memorable and non-memorable quotes. (%s of 3rd pers.
pronouns and indefinite articles are relative to all tokens,
%s of past tense are relative to all past and present verbs.)
and compare how likely each slogan is to be pro-
duced according to these two models. As shown in
the middle column of Table 5, we find that slogans
are better predicted both lexically and syntactically
by the former model. This result thus offers evi-
dence for a concept of ?memorable language? that
can be applied beyond a single domain.
We also note that the higher likelihood of slogans
under a ?memorable language? model is not simply
occurring for the trivial reason that this model pre-
dicts all other large bodies of text better. In partic-
ular, the newswire section of the Brown corpus is
predicted better at the lexical level by the language
model trained on non-memorable quotes.
Finally, Table 6 shows that slogans employ gen-
eral language, in the sense that for each of our
generality metrics, we see a slogans/memorable-
quotes/non-memorable quotes spectrum.
3.4 Prediction task
We now show how the principles discussed above
can provide features for a basic prediction task, cor-
responding to the task in our human pilot study:
given a pair of quotes, identify the memorable one.
Our first formulation of the prediction task uses
a standard bag-of-words model10. If there were
no information in the textual content of a quote
to determine whether it were memorable, then an
SVM employing bag-of-words features should per-
form no better than chance. Instead, though, it ob-
tains 59.67% (10-fold cross-validation) accuracy, as
shown in Table 7. We then develop models using
features based on the measures formulated earlier
in this section: generality measures (the four listed
in Table 4); distinctiveness measures (likelihood ac-
cording to 1, 2, and 3-gram ?common language?
models at the lexical and part-of-speech level for
each quote in the pair, their differences, and pair-
wise comparisons between them); and similarity-
to-slogans measures (likelihood according to 1, 2,
and 3-gram slogan-language models at the lexical
and part-of-speech level for each quote in the pair,
their differences, and pairwise comparisons between
them).
Even a relatively small number of distinctive-
ness features, on their own, improve significantly
over the much larger bag-of-words model. When
we include additional features based on generality
and language-model features measuring similarity to
slogans, the performance improves further (last line
of Table 7).
Thus, the main conclusion from these prediction
tasks is that abstracting notions such as distinctive-
ness and generality can produce relatively stream-
lined models that outperform much heavier-weight
bag-of-words models, and can suggest steps toward
approaching the performance of human judges who
? very much unlike our system ? have the full cul-
tural context in which movies occur at their disposal.
3.5 Other characteristics
We also made some auxiliary observations that may
be of interest. Specifically, we find differences in let-
ter and sound distribution (e.g., memorable quotes
? after curse-word removal ? use significantly
more ?front sounds? (labials or front vowels such
as represented by the letter i) and significantly fewer
?back sounds? such as the one represented by u),11
10We discarded terms appearing fewer than 10 times.
11These findings may relate to marketing research on sound
symbolism [7, 19, 40].
898
Feature set # feats Accuracy
bag of words 962 59.67%
distinctiveness 24 62.05%?
generality 4 56.70%
slogan sim. 24 58.30%
all three types together 52 64.27%??
Table 7: Prediction: SVM 10-fold cross validation results
using the respective feature sets. Random baseline accu-
racy is 50%. Accuracies statistically significantly greater
than bag-of-words according to a two-tailed t-test are in-
dicated with *(p<.05) and **(p<.01).
word complexity (e.g., memorable quotes use words
with significantly more syllables) and phrase com-
plexity (e.g., memorable quotes use fewer coordi-
nating conjunctions). The latter two are in line with
our distinctiveness hypothesis.
4 A long time ago, in a galaxy far, far away
How an item?s linguistic form affects the reaction it
generates has been studied in several contexts, in-
cluding evaluations of product reviews [9], political
speeches [12], on-line posts [13], scientific papers
[14], and retweeting of Twitter posts [36]. We use
a different set of features, abstracting the notions of
distinctiveness and generality, in order to focus on
these higher-level aspects of phrasing rather than on
particular lower-level features.
Related to our interest in distinctiveness, work in
advertising research has studied the effect of syntac-
tic complexity on recognition and recall of slogans
[5, 6, 24]. There may also be connections to Von
Restorff?s isolation effect Hunt [17], which asserts
that when all but one item in a list are similar in some
way, memory for the different item is enhanced.
Related to our interest in generality, Knapp et al
[20] surveyed subjects regarding memorable mes-
sages or pieces of advice they had received, finding
that the ability to be applied to multiple concrete sit-
uations was an important factor.
Memorability, although distinct from ?memoriz-
ability?, relates to short- and long-term recall. Thorn
and Page [34] survey sub-lexical, lexical, and se-
mantic attributes affecting short-term memorability
of lexical items. Studies of verbatim recall have also
considered the task of distinguishing an exact quote
from close paraphrases [3]. Investigations of long-
term recall have included studies of culturally signif-
icant passages of text [29] and findings regarding the
effect of rhetorical devices of alliterative [4], ?rhyth-
mic, poetic, and thematic constraints? [18, 26].
Finally, there are complex connections between
humor and memory [32], which may lead to interac-
tions with computational humor recognition [25].
5 I think this is the beginning of a
beautiful friendship.
Motivated by the broad question of what kinds of in-
formation achieve widespread public awareness, we
studied the the effect of phrasing on a quote?s mem-
orability. A challenge is that quotes differ not only
in how they are worded, but also in who said them
and under what circumstances; to deal with this dif-
ficulty, we constructed a controlled corpus of movie
quotes in which lines deemed memorable are paired
with non-memorable lines spoken by the same char-
acter at approximately the same point in the same
movie. After controlling for context and situation,
memorable quotes were still found to exhibit, on av-
erage (there will always be individual exceptions),
significant differences from non-memorable quotes
in several important respects, including measures
capturing distinctiveness and generality. Our ex-
periments with slogans show how the principles we
identify can extend to a different domain.
Future work may lead to applications in market-
ing, advertising and education [4]. Moreover, the
subtle nature of memorability, and its connection to
research in psychology, suggests a range of further
research directions. We believe that the framework
developed here can serve as the basis for further
computational studies of the process by which infor-
mation takes hold in the public consciousness, and
the role that language effects play in this process.
My mother thanks you. My father thanks you.
My sister thanks you. And I thank you: Re-
becca Hwa, Evie Kleinberg, Diana Minculescu, Alex
Niculescu-Mizil, Jennifer Smith, Benjamin Zimmer, and
the anonymous reviewers for helpful discussions and
comments; our annotators Steven An, Lars Backstrom,
Eric Baumer, Jeff Chadwick, Evie Kleinberg, and Myle
Ott; and the makers of Cepacol, Robitussin, and Sudafed,
whose products got us through the submission deadline.
This paper is based upon work supported in part by NSF
grants IIS-0910664, IIS-1016099, Google, and Yahoo!
899
References
[1] Eytan Adar, Li Zhang, Lada A. Adamic, and
Rajan M. Lukose. Implicit structure and the
dynamics of blogspace. In Workshop on the
Weblogging Ecosystem, 2004.
[2] Lars Backstrom, Dan Huttenlocher, Jon Klein-
berg, and Xiangyang Lan. Group formation
in large social networks: Membership, growth,
and evolution. In Proceedings of KDD, 2006.
[3] Elizabeth Bates, Walter Kintsch, Charles R.
Fletcher, and Vittoria Giuliani. The role of
pronominalization and ellipsis in texts: Some
memory experiments. Journal of Experimental
Psychology: Human Learning and Memory, 6
(6):676?691, 1980.
[4] Frank Boers and Seth Lindstromberg. Find-
ing ways to make phrase-learning feasible: The
mnemonic effect of alliteration. System, 33(2):
225?238, 2005.
[5] Samuel D. Bradley and Robert Meeds.
Surface-structure transformations and advertis-
ing slogans: The case for moderate syntactic
complexity. Psychology and Marketing, 19:
595?619, 2002.
[6] Robert Chamblee, Robert Gilmore, Gloria
Thomas, and Gary Soldow. When copy com-
plexity can help ad readership. Journal of Ad-
vertising Research, 33(3):23?23, 1993.
[7] John Colapinto. Famous names. The New
Yorker, pages 38?43, 2011.
[8] Cristian Danescu-Niculescu-Mizil and Lillian
Lee. Chameleons in imagined conversations:
A new approach to understanding coordination
of linguistic style in dialogs. In Proceedings
of the Workshop on Cognitive Modeling and
Computational Linguistics, 2011.
[9] Cristian Danescu-Niculescu-Mizil, Gueorgi
Kossinets, Jon Kleinberg, and Lillian Lee.
How opinions are received by online commu-
nities: A case study on Amazon.com helpful-
ness votes. In Proceedings of WWW, pages
141?150, 2009.
[10] Stuart Fischoff, Esmeralda Cardenas, Angela
Hernandez, Korey Wyatt, Jared Young, and
Rachel Gordon. Popular movie quotes: Re-
flections of a people and a culture. In Annual
Convention of the American Psychological As-
sociation, 2000.
[11] Daniel Gruhl, R. Guha, David Liben-Nowell,
and Andrew Tomkins. Information diffusion
through blogspace. Proceedings of WWW,
pages 491?501, 2004.
[12] Marco Guerini, Carlo Strapparava, and
Oliviero Stock. Trusting politicians? words
(for persuasive NLP). In Proceedings of
CICLing, pages 263?274, 2008.
[13] Marco Guerini, Carlo Strapparava, and Go?zde
O?zbal. Exploring text virality in social net-
works. In Proceedings of ICWSM (poster),
2011.
[14] Marco Guerini, Alberto Pepe, and Bruno
Lepri. Do linguistic style and readability of
scientific abstracts affect their virality? In Pro-
ceedings of ICWSM, 2012.
[15] Richard Jackson Harris, Abigail J. Werth,
Kyle E. Bures, and Chelsea M. Bartel. Social
movie quoting: What, why, and how? Ciencias
Psicologicas, 2(1):35?45, 2008.
[16] Chip Heath, Chris Bell, and Emily Steinberg.
Emotional selection in memes: The case of
urban legends. Journal of Personality, 81(6):
1028?1041, 2001.
[17] R. Reed Hunt. The subtlety of distinctiveness:
What von Restorff really did. Psychonomic
Bulletin & Review, 2(1):105?112, 1995.
[18] Ira E. Hyman Jr. and David C. Rubin. Mem-
orabeatlia: A naturalistic study of long-term
memory. Memory & Cognition, 18(2):205?
214, 1990.
[19] Richard R. Klink. Creating brand names with
meaning: The use of sound symbolism. Mar-
keting Letters, 11(1):5?20, 2000.
[20] Mark L. Knapp, Cynthia Stohl, and Kath-
leen K. Reardon. ?Memorable? mes-
sages. Journal of Communication, 31(4):27?
41, 1981.
[21] Henry Kuc?era and W. Nelson Francis. Compu-
tational analysis of present-day American En-
glish. Dartmouth Publishing Group, 1967.
900
[22] Jure Leskovec, Lada Adamic, and Bernardo
Huberman. The dynamics of viral market-
ing. ACM Transactions on the Web, 1(1), May
2007.
[23] Jure Leskovec, Lars Backstrom, and Jon Klein-
berg. Meme-tracking and the dynamics of the
news cycle. In Proceedings of KDD, pages
497?506, 2009.
[24] Tina M. Lowrey. The relation between
script complexity and commercial memorabil-
ity. Journal of Advertising, 35(3):7?15, 2006.
[25] Rada Mihalcea and Carlo Strapparava. Learn-
ing to laugh (automatically): Computational
models for humor recognition. Computational
Intelligence, 22(2):126?142, 2006.
[26] Milman Parry and Adam Parry. The making of
Homeric verse: The collected papers of Mil-
man Parry. Clarendon Press, Oxford, 1971.
[27] Everett Rogers. Diffusion of Innovations. Free
Press, fourth edition, 1995.
[28] Daniel M. Romero, Brendan Meeder, and Jon
Kleinberg. Differences in the mechanics of
information diffusion across topics: Idioms,
political hashtags, and complex contagion on
Twitter. Proceedings of WWW, pages 695?704,
2011.
[29] David C. Rubin. Very long-term memory for
prose and verse. Journal of Verbal Learning
and Verbal Behavior, 16(5):611?621, 1977.
[30] Nathan Schneider, Rebecca Hwa, Philip Gi-
anfortoni, Dipanjan Das, Michael Heilman,
Alan W. Black, Frederick L. Crabbe, and
Noah A. Smith. Visualizing topical quotations
over time to understand news discourse. Tech-
nical Report CMU-LTI-01-103, CMU, 2010.
[31] David Strang and Sarah Soule. Diffusion in or-
ganizations and social movements: From hy-
brid corn to poison pills. Annual Review of So-
ciology, 24:265?290, 1998.
[32] Hannah Summerfelt, Louis Lippman, and
Ira E. Hyman Jr. The effect of humor on mem-
ory: Constrained by the pun. The Journal of
General Psychology, 137(4), 2010.
[33] Eric Sun, Itamar Rosenn, Cameron Marlow,
and Thomas M. Lento. Gesundheit! Model-
ing contagion through Facebook News Feed. In
Proceedings of ICWSM, 2009.
[34] Annabel Thorn and Mike Page. Interactions
Between Short-Term and Long-Term Memory
in the Verbal Domain. Psychology Press, 2009.
[35] Louis L. Thurstone. A law of comparative
judgment. Psychological Review, 34(4):273?
286, 1927.
[36] Oren Tsur and Ari Rappoport. What?s in
a Hashtag? Content based prediction of the
spread of ideas in microblogging communities.
In Proceedings of WSDM, 2012.
[37] Fang Wu, Bernardo A. Huberman, Lada A.
Adamic, and Joshua R. Tyler. Information flow
in social groups. Physica A: Statistical and
Theoretical Physics, 337(1-2):327?335, 2004.
[38] Shaomei Wu, Jake M. Hofman, Winter A. Ma-
son, and Duncan J. Watts. Who says what to
whom on Twitter. In Proceedings of WWW,
2011.
[39] Jaewon Yang and Jure Leskovec. Patterns of
temporal variation in online media. In Pro-
ceedings of WSDM, 2011.
[40] Eric Yorkston and Geeta Menon. A sound idea:
Phonetic effects of brand names on consumer
judgments. Journal of Consumer Research, 31
(1):43?51, 2004.
901
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 175?185,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
The effect of wording on message propagation:
Topic- and author-controlled natural experiments on Twitter
Chenhao Tan
Dept. of Computer Science
Cornell University
chenhao@cs.cornell.edu
Lillian Lee
Dept. of Computer Science
Cornell University
llee@cs.cornell.edu
Bo Pang
Google Inc.
bopang42@gmail.com
Abstract
Consider a person trying to spread an
important message on a social network.
He/she can spend hours trying to craft the
message. Does it actually matter? While
there has been extensive prior work look-
ing into predicting popularity of social-
media content, the effect of wording per
se has rarely been studied since it is of-
ten confounded with the popularity of the
author and the topic. To control for these
confounding factors, we take advantage
of the surprising fact that there are many
pairs of tweets containing the same url and
written by the same user but employing
different wording. Given such pairs, we
ask: which version attracts more retweets?
This turns out to be a more difficult task
than predicting popular topics. Still, hu-
mans can answer this question better than
chance (but far from perfectly), and the
computational methods we develop can do
better than both an average human and a
strong competing method trained on non-
controlled data.
1 Introduction
How does one make a message ?successful?? This
question is of interest to many entities, including
political parties trying to frame an issue (Chong
and Druckman, 2007), and individuals attempting
to make a point in a group meeting. In the first
case, an important type of success is achieved if
the national conversation adopts the rhetoric of the
party; in the latter case, if other group members
repeat the originating individual?s point.
The massive availability of online messages,
such as posts to social media, now affords re-
searchers new means to investigate at a very large
scale the factors affecting message propagation,
also known as adoption, sharing, spread, or vi-
rality. According to prior research, important fea-
tures include characteristics of the originating au-
thor (e.g., verified Twitter user or not, author?s
messages? past success rate), the author?s social
network (e.g., number of followers), message tim-
ing, and message content or topic (Artzi et al,
2012; Bakshy et al, 2011; Borghol et al, 2012;
Guerini et al, 2011; Guerini et al, 2012; Hansen
et al, 2011; Hong et al, 2011; Lakkaraju et al,
2013; Milkman and Berger, 2012; Ma et al, 2012;
Petrovi?c et al, 2011; Romero et al, 2013; Suh et
al., 2010; Sun et al, 2013; Tsur and Rappoport,
2012). Indeed, it?s not surprising that one of the
most retweeted tweets of all time was from user
BarackObama, with 40M followers, on November
6, 2012: ?Four more years. [link to photo]?.
Our interest in this paper is the effect of alterna-
tive message wording, meaning how the message
is said, rather than what the message is about. In
contrast to the identity/social/timing/topic features
mentioned above, wording is one of the few fac-
tors directly under an author?s control when he or
she seeks to convey a fixed piece of content. For
example, consider a speaker at the ACL business
meeting who has been tasked with proposing that
Paris be the next ACL location. This person can-
not on the spot become ACL president, change the
shape of his/her social network, wait until the next
morning to speak, or campaign for Rome instead;
but he/she can craft the message to be more hu-
morous, more informative, emphasize certain as-
pects instead of others, and so on. In other words,
we investigate whether a different choice of words
affects message propagation, controlling for user
and topic: would user BarackObama have gotten
significantly more (or fewer) retweets if he had
used some alternate wording to announce his re-
election?
Although we cannot create a parallel universe
175
Table 1: Topic- and author-controlled (TAC) pairs. Topic control = inclusion of the same URL.
author tweets #retweets
natlsecuritycnn t
1
: FIRST ON CNN: After Petraeus scandal, Paula Broadwell looks to recapture ?normal life.? http://t.co/qy7GGuYW n
1
= 5
t
2
: First on CNN: Broadwell photos shared with Security Clearance as she and her family fight media portrayal of her [same URL] n
2
= 29
ABC t
1
: Workers, families take stand against Thanksgiving hours: http://t.co/J9mQHiIEqv n
1
= 46
t
2
: Staples, Medieval Times Workers Say Opening Thanksgiving Day Crosses the Line [same URL] n
2
= 27
cactus music t
1
: I know at some point you?ve have been saved from hunger by our rolling food trucks friends. Let?s help support them!
http://t.co/zg9jwA5j
n
1
= 2
t
2
: Food trucks are the epitome of small independently owned LOCAL businesses! Help keep them going! Sign the petition [same
URL]
n
2
= 13
in which BarackObama tweeted something else
1
,
fortunately, a surprising characteristic of Twitter
allows us to run a fairly analogous natural exper-
iment: external forces serendipitously provide an
environment that resembles the desired controlled
setting (DiNardo, 2008). Specifically, it turns out
to be unexpectedly common for the same user to
post different tweets regarding the same URL ?
a good proxy for fine-grained topic
2
? within a
relatively short period of time.
3
Some example
pairs are shown in Table 1; we see that the paired
tweets may differ dramatically, going far beyond
word-for-word substitutions, so that quite interest-
ing changes can be studied.
Looking at these examples, can one in fact tell
from the wording which tweet in a topic- and
author-controlled pair will be more successful?
The answer may not be a priori clear. For example,
for the first pair in the table, one person we asked
found t
1
?s invocation of a ?scandal? to be more
attention-grabbing; but another person preferred
t
2
because it is more informative about the URL?s
content and includes ?fight media portrayal?. In
an Amazon Mechanical Turk (AMT) experiment
(?4), we found that humans achieved an average
accuracy of 61.3%: not that high, but better than
chance, indicating that it is somewhat possible for
humans to predict greater message spread from
different deliveries of the same information.
Buoyed by the evidence of our AMT study that
wording effects exist, we then performed a battery
of experiments to seek generally-applicable, non-
1
Cf. the Music Lab ?multiple universes? experiment to
test the randomness of popularity (Salganik et al, 2006).
2
Although hashtags have been used as coarse-grained
topic labels in prior work, for our purposes, we have no assur-
ance that two tweets both using, say, ?#Tahrir? would be at-
tempting to express the same message but in different words.
In contrast, see the same-URL examples in Table 1.
3
Moreover, Twitter presents tweets to a reader in strict
chronological order, so that there are no algorithmic-ranking
effects to compensate for in determining whether readers saw
a tweet. And, Twitter accumulates retweet counts for the en-
tire retweet cascade and displays them for the original tweet
at the root of the propagation tree, so we can directly use
Twitter?s retweet counts to compare the entire reach of the
different versions.
Twitter-specific features of more successful phras-
ings. ?5.1 applies hypothesis testing (with Bonfer-
roni correction to ameliorate issues with multiple
comparisons) to investigate the utility of features
like informativeness, resemblance to headlines,
and conformity to the community norm in lan-
guage use. ?5.2 further validates our findings via
prediction experiments, including on completely
fresh held-out data, used only once and after an
array of standard cross-validation experiments.
4
We achieved 66.5% cross-validation accuracy and
65.6% held-out accuracy with a combination of
our custom features and bag-of-words. Our clas-
sifier fared significantly better than a number of
baselines, including a strong classifier trained on
the most- and least-retweeted tweets that was even
granted access to author and timing metadata.
2 Related work
The idea of using carefully controlled experiments
to study effective communication strategies dates
back at least to Hovland et al (1953). Recent
studies range from examining what characteris-
tics of New York Times articles correlate with high
re-sharing rates (Milkman and Berger, 2012) to
looking at how differences in description affect
the spread of content-controlled videos or images
(Borghol et al, 2012; Lakkaraju et al, 2013).
Simmons et al (2011) examined the variation of
quotes from different sources to examine how tex-
tual memes mutate as people pass them along, but
did not control for author. Predicting the ?success?
of various texts such as novels and movie quotes
has been the aim of additional prior work not al-
ready mentioned in ?1 (Ashok et al, 2013; Louis
and Nenkova, 2013; Danescu-Niculescu-Mizil et
al., 2012; Pitler and Nenkova, 2008; McIntyre and
Lapata, 2009). To our knowledge, there have been
no large-scale studies exploring wording effects in
a both topic- and author-controlled setting. Em-
ploying such controls, we find that predicting the
more effective alternative wording is much harder
than the previously well-studied problem of pre-
4
And after crossing our fingers.
176
dicting popular content when author or topic can
freely vary.
Related work regarding the features we consid-
ered is deferred to ?5.1 (features description).
3 Data
Our main dataset was constructed by first gath-
ering 1.77M topic- and author-controlled (hence-
forth TAC) tweet pairs
5
differing in more than just
spacing.
6
We accomplished this by crawling time-
lines of 236K user ids that appear in prior work
(Kwak et al, 2010; Yang and Leskovec, 2011)
via the Twitter API. This crawling process also
yielded 632K TAC pairs whose only difference
was spacing, and an additional 558M ?unpaired?
tweets; as shown later in this paper, we used these
extra corpora for computing language models and
other auxiliary information. We applied non-
obvious but important filtering ? described later
in this section ? to control for other external fac-
tors and to reduce ambiguous cases. This brought
us to a set of 11,404 pairs, with the gold-standard
labels determined by which tweet in each pair was
the one that received more retweets according to
the Twitter API. We then did a second crawl to
get an additional 1,770 pairs to serve as a held-out
dataset. The corresponding tweet IDs are available
online at http://chenhaot.com/pages/
wording-for-propagation.html. (Twit-
ter?s terms of service prohibit sharing the actual
tweets.)
Throughout, we refer to the textual content of
the earlier tweet within a TAC pair as t
1
, and of the
later one as t
2
. We denote the number of retweets
received by each tweet by n
1
and n
2
, respectively.
We refer to the tweet with higher (lower) n
i
as the
?better (worse)? tweet.
Using ?identical? pairs to determine how to
compensate for follower-count and timing ef-
fects. In an ideal setting, differences between
n
1
and n
2
would be determined solely by dif-
ferences in wording. But even with a TAC pair,
retweets might exhibit a temporal bias because of
the chronological order of tweet presentation (t
1
might enjoy a first-mover advantage (Borghol et
al., 2012) because it is the ?original?; alternatively,
5
No data collection/processing was conducted at Google.
6
The total excludes: tweets containing multiple URLs;
tweets from users posting about the same URL more than five
times (since such users might be spammers); the third, fourth,
or fifth version for users posting between three and five tweets
for the same URL; retweets (as identified by Twitter?s API or
by beginning with ?RT @?); non-English tweets.
3 6 12 18 24 36 48time lag (hours)2468
10121416D >1K f?ers>2.5K f?ers>5K f?ers>10K f?ers
(a) For identical TAC pairs,
retweet-count deviation vs.
time lag between t
1
and
t
2
, for the author follower-
counts given in the legend.
0 2 4 6 8n102
468
10
E?(n 2|n 1)
>5K f?ers,<12hrsotherwise
(b) Avg. n
2
vs. n
1
for iden-
tical TAC pairs, highlighting
our chosen time-lag and fol-
lower thresholds. Bars: stan-
dard error. Diagonal line:
p
Epn
2
|n
1
q ? n
1
.
Figure 1: (a): The ideal case where n
2
? n
1
when t
1
? t
2
is best approximated when t
2
oc-
curs within 12 hours of t
1
and the author has at
least 10,000 or 5,000 followers. (b): in our chosen
setting (blue circles), n
2
indeed tends to track n
1
,
whereas otherwise (black squares), there?s a bias
towards retweeting t
1
.
t
2
might be preferred because retweeters consider
t
1
to be ?stale?). Also, the number of followers an
author has can have complicated indirect effects
on which tweets are read (space limits preclude
discussion).
We use the 632K TAC pairs wherein t
1
and
t
2
are identical
7
to check for such confounding
effects: we see how much n
2
deviates from n
1
in such settings, since if wording were the only
explanatory factor, the retweet rates for identical
tweets ought to be equal. Figure 1(a) plots how
the time lag between t
1
and t
2
and the author?s
follower-count affect the following deviation esti-
mate:
D ?
?
0?n
1
?10
| pEpn
2
|n
1
q ? n
1
|,
where
p
Epn
2
|n
1
q is the average value of n
2
over
pairs whose t
1
is retweeted n
1
times. (Note that
the number of pairs whose t
1
is retweeted n
1
times
decays exponentially with n
1
; hence, we condi-
tion on n
1
to keep the estimate from being domi-
nated by pairs with n
1
? 0, and do not consider
n
1
? 10 because there are too few such pairs to es-
timate
p
Epn
2
|n
1
q reliably.) Figure 1(a) shows that
the setting where we (i) minimize the confound-
ing effects of time lag and author?s follower-count
and (ii) maximize the amount of data to work with
7
Identical up to spacing: Twitter prevents exact copies by
the same author appearing within a short amount of time, but
some authors work around this by inserting spaces.
177
is: when t
2
occurs within 12 hours after t
1
and
the author has more than 5,000 followers. Figure
1(b) confirms that for identical TAC pairs, our cho-
sen setting indeed results in n
2
being on average
close to n
1
, which corresponds to the desired set-
ting where wording is the dominant differentiating
factor.
8
Focus on meaningful and general changes.
Even after follower-count and time-lapse filtering,
we still want to focus on TAC pairs that (i) ex-
hibit significant/interesting textual changes (as ex-
emplified in Table 1, and as opposed to typo cor-
rections and the like), and (ii) have n
2
and n
1
suf-
ficiently different so that we are confident in which
t
i
is better at attracting retweets. To take care of
(i), we discarded the 50% of pairs whose similar-
ity was above the median, where similarity was
tf-based cosine.
9
For (ii), we sorted the remain-
ing pairs by n
2
? n
1
and retained only the top and
bottom 5%.
10
Moreover, to ensure that we do not
overfit to the idiosyncrasies of particular authors,
we cap the number of pairs contributed by each
author to 50 before we deal with (ii).
4 Human accuracy on TAC pairs
We first ran a pilot study on Amazon Mechan-
ical Turk (AMT) to determine whether humans
can identify, based on wording differences alone,
which of two topic- and author- controlled tweets
is spread more widely. Each of our 5 AMT tasks
involved a disjoint set of 20 randomly-sampled
TAC pairs (with t
1
and t
2
randomly reordered);
subjects indicated ?which tweet would other peo-
ple be more likely to retweet??, provided a short
justification for their binary response, and clicked
a checkbox if they found that their choice was a
?close call?. We received 39 judgments per pair in
aggregate from 106 subjects total (9 people com-
pleted all 5 tasks). The subjects? justifications
were of very high quality, convincing us that they
all did the task in good faith
11
. Two examples for
8
We also computed the Pearson correlation between n
1
and n
2
, even though it can be dominated by pairs with smaller
n
1
. The correlation is 0.853 for ?? 5K f?ers, ?12hrs?,
clearly higher than the 0.305 correlation for ?otherwise?.
9
Idf weighting was not employed because changes to fre-
quent words are of potential interest. Urls, hashtags, @-
mentions and numbers were normalized to [url], [hashtag],
[at], and [num] before computing similarity.
10
For our data, this meant n
2
? n
1
? 10 or ? ?15. Cf.
our median number of retweets: 30.
11
We also note that the feedback we got was quite pos-
itive, including: ?...It?s fun to make choices between close
tweets and use our subjective opinion. Thanks and best of
the third TAC pair in Table 1 were: ?[t
1
makes] the
cause relate-able to some people, therefore show-
ing more of an appeal as to why should they click
the link and support? and, expressing the opposite
view, ?I like [t
2
] more because [t
1
] starts out with
a generalization that doesn?t affect me and try to
make me look like I had that experience before?.
If we view the set of 3900 binary judgments
for our 100-TAC-pair sample as constituting in-
dependent responses, then the accuracy for this
set is 62.4% (rising to 63.8% if we exclude the
587 judgments deemed ?close calls?). However, if
we evaluate the accuracy of the majority response
among the 39 judgments per pair, the number rises
to 73%. The accuracy of the majority response
generally increases with the dominance of the ma-
jority, going above 90% when at least 80% of the
judgments agree (although less than a third of the
pairs satisfied this criterion).
Alternatively, we can consider the average ac-
curacy of the 106 subjects: 61.3%, which is bet-
ter than chance but far from 100%. (Variance was
high: one subject achieved 85% accuracy out of
20 pairs, but eight scored below 50%.) This re-
sult is noticeably lower than the 73.8%-81.2% re-
ported by Petrovi?c et al (2011), who ran a sim-
ilar experiment involving two subjects and 202
tweet pairs, but where the pairs were not topic- or
author-controlled.
12
We conclude that even though propagation pre-
diction becomes more challenging when topic
and author controls are applied, humans can
still to some degree tell which wording attracts
more retweets. Interested readers can try this
out themselves at http://chenhaot.com/
retweetedmore/quiz.
5 Experiments
We now investigate computationally what word-
ing features correspond to messages achieving a
broader reach. We start (?5.1) by introducing a set
of generally-applicable and (mostly) non-Twitter-
specific features to capture our intuitions about
what might be better ways to phrase a message.
We then use hypothesis testing (?5.1) to evaluate
the importance of each feature for message prop-
luck with your research? and ?This was very interesting and
really made me think about how I word my own tweets. Great
job on this survey!?. We only had to exclude one person (not
counted among the 106 subjects), doing so because he or she
gave the same uninformative justification for all pairs.
12
The accuracy range stems from whether author?s social
features were supplied and which subject was considered.
178
Table 2: Notational conventions for tables in ?5.1.
One-sided paired t-test for feature efficacy
????: p?1e-20 ????: p?1-1e-20
??? : p?0.001 ??? : p?0.999
?? : p?0.01 ?? : p?0.99
? : p?0.05 ? : p?0.95
?: passes our Bonferroni correction
One-sided binomial test for feature increase
(Do authors prefer to ?raise? the feature in t
2
?)
YES : t
2
has a higher feature score than t
1
, ? ? .05
NO : t
2
has a lower feature score than t
1
, ? ? .05
(x%): %pf
2
? f
1
q, if sig. larger or smaller than 50%
agation and the extent to which authors employ
it, followed by experiments on a prediction task
(?5.2) to further examine the utility of these fea-
tures.
5.1 Features: efficacy and author preference
What kind of phrasing helps message propaga-
tion? Does it work to explicitly ask people to share
the message? Is it better to be short and concise or
long and informative? We define an array of fea-
tures to capture these and other messaging aspects.
We then examine (i) how effective each feature is
for attracting more retweets; and (ii) whether au-
thors prefer applying a given feature when issuing
a second version of a tweet.
First, for each feature, we use a one-sided paired
t-test to test whether, on our 11K TAC pairs, our
score function for that feature is larger in the bet-
ter tweet versions than in the worse tweet versions,
for significance levels ? ? .05, .01, .001, 1e-20.
Given that we did 39 tests in total, there is a risk
of obtaining false positives due to multiple test-
ing (Dunn, 1961; Benjamini and Hochberg, 1995).
To account for this, we also report significance re-
sults for the conservatively Bonferroni-corrected
(?BC?) significance level ? = 0.05/39=1.28e-3.
Second, we examine author preference for ap-
plying a feature. We do so because one (but by no
means the only) reason authors post t
2
after having
already advertised the same URL in t
1
is that these
authors were dissatisfied with the amount of atten-
tion t
1
got; in such cases, the changes may have
been specifically intended to attract more retweets.
We measure author preference for a feature by the
percentage of our TAC pairs
13
where t
2
has more
?occurrences? of the feature than t
1
, which we de-
note by ?%pf
2
? f
1
q?. We use the one-sided bi-
nomial test to see whether %pf
2
? f
1
q is signifi-
cantly larger (or smaller) than 50%.
13
For our preference experiments, we added in pairs where
n
2
? n
1
was not in the top or bottom 5% (cf. ?3, meaningful
changes), since to measure author preference it?s not neces-
sary that the retweet counts differ significantly.
Table 3: Explicit requests for sharing (where only
occurrences POS-tagged as verbs count, according
to the Gimpel et al (2011) tagger).
effective? author-preferred?
rt ???? * ??
retweet ???? * YES (59%)
spread ??? ? * YES (56%)
please ??? ? * ??
pls ? ??? ??
plz ?? ?? ??
Table 4: Informativeness.
effective? author-preferred?
length (chars) ???? * YES (54%)
verb ???? * YES (56%)
noun ???? * ??
adjective ??? ? * YES (51%)
adverb ??? ? * YES (55%)
proper noun ??? ? * NO? (45%)
number ???? * NO? (48%)
hashtag ? ??? ??
@-mention ??? ? * YES (53%)
Not surprisingly, it helps to ask people to share.
(See Table 3; the notation for all tables is ex-
plained in Table 2.) The basic sanity check we
performed here was to take as features the number
of occurrences of the verbs ?rt?, ?retweet?, ?please?,
?spread?, ?pls?, and ?plz? to capture explicit re-
quests (e.g. ?please retweet?).
Informativeness helps. (Table 4) Messages that
are more informative have increased social ex-
change value (Homans, 1958), and so may be
more worth propagating. One crude approxima-
tion of informativeness is length, and we see that
length helps.
14
In contrast, Simmons et al (2011)
found that shorter versions of memes are more
likely to be popular. The difference may result
from TAC-pair changes being more drastic than
the variations that memes undergo.
A more refined informativeness measure is
counts of the parts of speech that correspond
to content. Our POS results, gathered using a
Twitter-specific tagger (Gimpel et al, 2011), echo
those of Ashok et al (2013) who looked at predict-
14
Of course, simply inserting garbage isn?t going to lead
to more retweets, but adding more information generally in-
volves longer text.
179
Table 5: Conformity to the community and one?s
own past, measured via scores assigned by various
language models.
effective? author-preferred?
twitter unigram ??? ? * YES (54%)
twitter bigram ??? ? * YES (52%)
personal unigram ??? ? * YES (52%)
personal bigram ??? NO? (48%)
ing the success of books. The diminished effect of
hashtag inclusion with respect to what has been re-
ported previously (Suh et al, 2010; Petrovi?c et al,
2011) presumably stems from our topic and author
controls.
Be like the community, and be true to yourself
(in the words you pick, but not necessarily in
how you combine them). (Table 5) Although dis-
tinctive messages may attract attention, messages
that conform to expectations might be more eas-
ily accepted and therefore shared. Prior work has
explored this tension: Lakkaraju et al (2013), in a
content-controlled study, found that the more up-
voted Reddit image titles balance novelty and fa-
miliarity; Danescu-Niculescu-Mizil et al (2012)
(henceforth DCKL?12) showed that the memora-
bility of movie quotes corresponds to higher lexi-
cal distinctiveness but lower POS distinctiveness;
and Sun et al (2013) observed that deviating from
one?s own past language patterns correlates with
more retweets.
Keeping in mind that the authors in our data
have at least 5000 followers
15
, we consider two
types of language-conformity constraints an au-
thor might try to satisfy: to be similar to what
is normal in the Twitter community, and to be
similar to what his or her followers expect. We
measure a tweet?s similarity to expectations by its
score according to the relevant language model,
1
|T |
?
xPT logpppxqq, where T refers to either all
the unigrams (unigram model) or all and only bi-
grams (bigram model).
16
We trained a Twitter-
community language model from our 558M un-
paired tweets, and personal language models from
each author?s tweet history.
Imitate headlines. (Table 6) News headlines are
often intentionally written to be both informative
and attention-getting, so we introduce the idea of
15
This is not an artificial restriction on our set of authors; a
large follower count means (in principle) that our results draw
on a large sample of decisions whether to retweet or not.
16
The tokens [at], [hashtag], [url] were ignored in the
unigram-model case to prevent their undue influence, but re-
tained in the bigram model to capture longer-range usage
(?combination?) patterns.
Table 6: LM-based resemblance to headlines.
effective? author-preferred?
headline unigram ?? ?? YES (53%)
headline bigram ???? * YES (52%)
Table 7: Retweet score.
effective? author-preferred?
rt score ?? ?? * NO? (49%)
verb rt score ???? * ??
noun rt score ??? ? * ??
adjective rt score ? ??? YES (50%)
adverb rt score ? ??? YES (51%)
proper noun rt score ??? NO? (48%)
scoring by a language model built from New York
Times headlines.
17
Use words associated with (non-paired)
retweeted tweets. (Table 7) We expect that
provocative or sensationalistic tweets are likely
to make people react. We found it difficult to
model provocativeness directly. As a rough
approximation, we check whether the changes in
t
2
with respect to t
1
(which share the same topic
and author) involve words or parts-of-speech that
are associated with high retweet rate in a very
large separate sample of unpaired tweets (retweets
and replies discarded). Specifically, for each word
w that appears more than 10 times, we compute
the probability that tweets containing w are
retweeted more than once, denoted by rspwq. We
define the rt score of a tweet as max
wPT rspwq,
where T is all the words in the tweet, and the
rt score of a particular POS tag z in a tweet as
max
wPT&tagpwq?zrspwq.
Include positive and/or negative words. (Ta-
ble 8) Prior work has found that including posi-
tive or negative sentiment increases message prop-
agation (Milkman and Berger, 2012; Godes et al,
2005; Heath et al, 2001; Hansen et al, 2011). We
measured the occurrence of positive and negative
words as determined by the connotation lexicon
of Feng et al (2013) (better coverage than LIWC).
Measuring the occurrence of both simultaneously
was inspired by Riloff et al (2013).
Refer to other people (but not your audience).
(Table 9) First-person has been found useful for
success before, but in the different domains of sci-
entific abstracts (Guerini et al, 2012) and books
(Ashok et al, 2013).
17
To test whether the results stem from similarity to news
rather than headlines per se, we constructed a NYT-text LM,
which proved less effective. We also tried using Gawker
headlines (often said to be attention-getting) but pilot studies
revealed insufficient vocabulary overlap with our TAC pairs.
180
Table 8: Sentiment (contrast is measured by pres-
ence of both positive and negative sentiments).
effective? author-preferred?
positive ??? ? * ??
negative ??? ? * ??
contrast ??? ? * ??
Table 9: Pronouns.
effective? author-preferred?
1st person singular ??? YES (51%)
1st person plural ??? YES (52%)
2nd person ??? YES (57%)
3rd person singular ?? ?? YES (55%)
3rd person plural ? ??? YES (58%)
Generality helps. (Table 10) DCKL?12 posited
that movie quotes are more shared in the culture
when they are general enough to be used in multi-
ple contexts. We hence measured the presence of
indefinite articles vs. definite articles.
The easier to read, the better. (Table 11) We
measure readability by using Flesch reading ease
(Flesch, 1948) and Flesch-Kincaid grade level
(Kincaid et al, 1975), though they are not de-
signed for short texts. We use negative grade level
so that a larger value indicates easier texts to read.
Final question: Do authors prefer to do what
is effective? Recall that we use binomial tests to
determine author preference for applying a feature
more in t
2
. Our preference statistics show that au-
thor preferences in many cases are aligned with
feature efficacy. But there are several notable ex-
ceptions: for example, authors tend to increase the
use of @-mentions and 2nd person pronouns even
though they are ineffective. On the other hand,
they did not increase the use of effective ones
like proper nouns and numbers; nor did they tend
to increase their rate of sentiment-bearing words.
Bearing in mind that changes in t
2
may not always
be intended as an effort to improve t
1
, it is still in-
teresting to observe that there are some contrasts
between feature efficacy and author preferences.
5.2 Predicting the ?better? wording
Here, we further examine the collective efficacy
of the features introduced in ?5.1 via their perfor-
mance on a binary prediction task: given a TAC
pair (t
1
, t
2
), did t
2
receive more retweets?
Our approach. We group the features introduced
in ?5.1 into 16 lexicon-based features (Table 3,
8, 9, 10), 9 informativeness features (Table 4), 6
language model features (Table 5, 6), 6 rt score
features (Table 7), and 2 readability features (Ta-
ble 11). We refer to all 39 of them together as
Table 10: Generality.
effective? author-preferred?
indefinite articles (a,an) ??? ? * ??
definite articles (the) ??? YES (52%)
Table 11: Readability.
effective? author-preferred?
reading ease ?? ?? YES (52%)
negative grade level ? ??? YES (52%)
custom features. We also consider tagged bag-of-
words (?BOW?) features, which includes all the
unigram (word:POS pair) and bigram features that
appear more than 10 times in the cross-validation
data. This yields 3,568 unigram features and 4,095
bigram features, for a total of 7,663 so-called
1,2-gram features. Values for each feature are nor-
malized by linear transformation across all tweets
in the training data to lie in the range r0, 1s.18
For a given TAC pair, we construct its feature
vector as follows. For each feature being consid-
ered, we compute its normalized value for each
tweet in the pair and take the difference as the fea-
ture value for this pair. We use L2-regularized lo-
gistic regression as our classifier, with parameters
chosen by cross validation on the training data.
(We also experimented with SVMs. The perfor-
mance was very close, but mostly slightly lower.)
A strong non-TAC alternative, with social infor-
mation and timing thrown in. One baseline re-
sult we would like to establish is whether the topic
and author controls we have argued for, while
intuitively compelling for the purposes of trying
to determine the best way for a given author to
present some fixed content, are really necessary
in practice. To test this, we consider an alterna-
tive binary L2-regularized logistic-regression clas-
sifier that is trained on unpaired data, specifically,
on the collection of 10,000 most retweeted tweets
(gold-standard label: positive) plus the 10,000
least retweeted tweets (gold-standard label: neg-
ative) that are neither retweets nor replies. Note
that this alternative thus is granted, by design,
roughly twice the training instances that our clas-
sifiers have, as a result of having roughly the same
number of tweets, since our instances are pairs.
Moreover, we additionally include the tweet au-
thor?s follower count, and the day and hour of
posting, as features. We refer to this alternative
classifier as  TAC+ff+time. (Mnemonic: ?ff? is
used in bibliographic contexts as an abbreviation
18
We also tried normalization by whitening, but it did not
lead to further improvements.
181
(a) Cross-validation and heldout accuracy for various feature sets. Blue lines inside
bars: performance when custom features are restricted to those that pass our Bon-
ferroni correction (no line for readability because no readability features passed).
Dashed vertical line:  TAC+ff+time performance.
1000 3000 5000 7000 900058%60%
62%64%66%
68%70% custom+1,2-gramcustom1,2-gramhuman
(b) Cross-validation accuracy vs data size.
Human performance was estimated from a
disjoint set of 100 pairs (see ?4).
Figure 2: Accuracy results. Pertinent significance results are as follows. In cross-validation, custom+1,2-
gram is significantly better than  TAC+ff+time (p=0) and 1,2-gram (p=3.8e-7). In heldout validation,
custom+1,2-gram is significantly better than  TAC+ff+time (p=3.4e-12) and 1,2-gram (p=0.01) but not
unigram (p=0.08), perhaps due to the small size of the heldout set.
for ?and the following?.) We apply it to a tweet
pair by computing whether it gives a higher score
to t
2
or not.
Baselines. To sanity-check whether our classifier
provides any improvement over the simplest meth-
ods one could try, we also report the performance
of the majority baseline, our request-for-sharing
features, and our character-length feature.
Performance comparison. We compare the ac-
curacy (percentage of pairs whose labels were cor-
rectly predicted) of our approach against the com-
peting methods. We report 5-fold cross validation
results on our balanced set of 11,404 TAC pairs
and on our completely disjoint heldout data
19
of
1,770 TAC pairs; this set was never examined dur-
ing development, and there are no authors in com-
mon between the two testing sets.
Figure 2(a) summarizes the main results. While
 TAC+ff+time outperforms the majority base-
line, using all the features we proposed beats
 TAC+ff+time by more than 10% in both cross-
validation (66.5% vs 55.9%) and heldout valida-
tion (65.6% vs 55.3%). We outperform the aver-
age human accuracy of 61% reported in our Ama-
zon Mechanical Turk experiments (for a different
data sample);  TAC+ff+time fails to do so.
The importance of topic and author con-
trol can be seen by further investigation of
 TAC+ff+time?s performance. First, note that
19
To construct this data, we used the same criteria as in
?3: written by authors with more than 5000 followers, posted
within 12 hours, n
2
? n
1
? 10 or ? ?15, and cosine simi-
larity threshold value the same as in ?3, cap of 50 on number
of pairs from any individual author.
it yields an accuracy of around 55% on our
alternate-version-selection task,
20
even though its
cross-validation accuracy on the larger most- and
least-retweeted unpaired tweets averages out to a
high 98.8%. Furthermore, note the superior per-
formance of unigrams trained on TAC data vs
 TAC+ff+time ? which is similar to our uni-
grams but trained on a larger but non-TAC dataset
that included metadata. Thus, TAC pairs are a use-
ful data source even for non-custom features. (We
also include individual feature comparisons later.)
Informativeness is the best-performing custom
feature group when run in isolation, and outper-
forms all baselines, as well as  TAC+ff+time;
and we can see from Figure 2(a) that this is not
due just to length. The combination of all our 39
custom features yields approximately 63% accu-
racy in both testing settings, significantly outper-
forming informativeness alone (p?0.001 in both
cases). Again, this is higher than our estimate of
average human performance.
Not surprisingly, the TAC-trained BOW fea-
tures (unigram and 1,2-gram) show impressive
predictive power in this task: many of our custom
features can be captured by bag-of-word features,
in a way. Still, the best performance is achieved
20
One might suspect that the problem is that
 TAC+ff+time learns from its training data to over-
rely on follower-count, since that is presumably a good
feature for non-TAC tweets, and for this reason suffers when
run on TAC data where follower-counts are by construction
non-informative. But in fact, we found that removing the
follower-count feature from  TAC+ff+time and re-training
did not lead to improved performance. Hence, it seems that
it is the non-controlled nature of the alternate training data
that explains the drop in performance.
182
by combining our custom and 1,2-gram features
together, to a degree statistically significantly bet-
ter than using 1,2-gram features alone.
Finally, we remark on our Bonferroni correc-
tion. Recall that the intent of applying it is to
avoid false positives. However, in our case, Fig-
ure 2(a) shows that our potentially ?false? posi-
tives ? features whose effectiveness did not pass
the Bonferroni correction test ? actually do raise
performance in our prediction tests.
Size of training data. Another interesting obser-
vation is how performance varies with data size.
For n ? 1000, 2000, . . . , 10000, we randomly
sampled n pairs from our 11,404 pairs, and com-
puted the average cross-validation accuracy on the
sampled data. Figure 2(b) shows the averages over
50 runs of the aforementioned procedure. Our cus-
tom features can achieve good performance with
little data, in the sense that for sample size 1000,
they outperform BOW features; on the other hand,
BOW features quickly surpass them. Across the
board, the custom+1,2-gram features are consis-
tently better than the 1,2-gram features alone.
Top features. Finally, we examine some of
the top-weighted individual features from our ap-
proach and from the competing  TAC+ff+time
classifier. The top three rows of Table 12 show the
best custom and best and worst unigram features
for our method; the bottom two rows show the best
and worst unigrams for  TAC+ff+time. Among
custom features, we see that community and per-
sonal language models, informativeness, retweet
scores, sentiment, and generality are represented.
As for unigram features, not surprisingly, ?rt? and
?retweet? are top features for both our approach
and  TAC+ff+time. However, the other unigrams
for the two methods seem to be a bit different in
spirit. Some of the unigrams determined to be
most poor only by our method appear to be both
surprising and yet plausible in retrospect: ?icymi?
(abbreviation for ?in case you missed it?) tends to
indicate a direct repetition of older information,
so people might prefer to retweet the earlier ver-
sion; ?thanks? and ?sorry? could correspond to
personal thank-yous and apologies not meant to
be shared with a broader audience, and similarly
@-mentioning another user may indicate a tweet
intended only for that person. The appearance of
[hashtag] in the best  TAC+ff+time unigrams is
consistent with prior research in non-TAC settings
(Suh et al, 2010; Petrovi?c et al, 2011).
Table 12: Features with largest coefficients, de-
limited by commas. POS tags omitted for clarity.
Our approach
best 15 custom twitter bigram, length (chars), rt
(the word), retweet (the word), verb, verb retweet score,
personal unigram, proper noun, number, noun, positive
words, please (the word), proper noun retweet score,
indefinite articles (a,an), adjective
best 20 unigrams rt, retweet, [num], breaking,
is, win, never, ., people, need, official, officially, are,
please, november, world, girl, !!!, god, new
worst 20 unigrams :, [at], icymi, also, comments,
half, ?, earlier, thanks, sorry, highlights, bit, point, up-
date, last, helping, peek, what, haven?t, debate
 TAC+ff+time
best 20 unigrams [hashtag], teen, fans, retweet,
sale, usa, women, butt, caught, visit, background, up-
coming, rt, this, bieber, these, each, chat, houston, book
worst 20 unigrams :, ..., boss, foundation, ?, ?,
others, john, roll, ride, appreciate, page, drive, correct,
full, ?, looks, @ (not as [at]), sales, hurts
6 Conclusion
In this work, we conducted the first large-scale
topic- and author-controlled experiment to study
the effects of wording on information propagation.
The features we developed to choose the bet-
ter of two alternative wordings posted better per-
formance than that of all our comparison algo-
rithms, including one given access to author and
timing features but trained on non-TAC data, and
also bested our estimate of average human perfor-
mance. According to our hypothesis tests, help-
ful wording heuristics include adding more infor-
mation, making one?s language align with both
community norms and with one?s prior messages,
and mimicking news headlines. Readers may
try out their own alternate phrasings at http:
//chenhaot.com/retweetedmore/ to see
what a simplified version of our classifier predicts.
In future work, it will be interesting to examine
how these features generalize to longer and more
extensive arguments. Moreover, understanding
the underlying psychological and cultural mecha-
nisms that establish the effectiveness of these fea-
tures is a fundamental problem of interest.
Acknowledgments. We thank C. Callison-Burch,
C. Danescu-Niculescu-Mizil, J. Kleinberg, P.
Mahdabi, S. Mullainathan, F. Pereira, K. Raman,
A. Swaminathan, the Cornell NLP seminar par-
ticipants and the reviewers for their comments; J.
Leskovec for providing some initial data; and the
anonymous annotators for all their labeling help.
This work was supported in part by NSF grant IIS-
0910664 and a Google Research Grant.
183
References
Yoav Artzi, Patrick Pantel, and Michael Gamon. 2012.
Predicting responses to microblog posts. In Pro-
ceedings of NAACL (short paper).
Vikas Ganjigunte Ashok, Song Feng, and Yejin Choi.
2013. Success with style: Using writing style to
predict the success of novels. In Proceedings of
EMNLP.
Eitan Bakshy, Jake M. Hofman, Winter A. Mason, and
Duncan J. Watts. 2011. Everyone?s an influencer:
Quantifying influence on twitter. In Proceedings of
WSDM.
Yoav Benjamini and Yosef Hochberg. 1995. Control-
ling the false discovery rate: A practical and pow-
erful approach to multiple testing. Journal of the
Royal Statistical Society. Series B (Methodological),
pages 289?300.
Youmna Borghol, Sebastien Ardon, Niklas Carlsson,
Derek Eager, and Anirban Mahanti. 2012. The
untold story of the clones: Content-agnostic factors
that impact YouTube video popularity. In Proceed-
ings of KDD.
Dennis Chong and James N. Druckman. 2007. Fram-
ing theory. Annual Review of Political Science,
10:103?126.
Cristian Danescu-Niculescu-Mizil, Justin Cheng, Jon
Kleinberg, and Lillian Lee. 2012. You had me at
hello: How phrasing affects memorability. In Pro-
ceedings of ACL.
John DiNardo. 2008. Natural experiments and quasi-
natural experiments. In The New Palgrave Dictio-
nary of Economics. Palgrave Macmillan.
Olive Jean Dunn. 1961. Multiple comparisons among
means. Journal of the American Statistical Associa-
tion, 56(293):52?64.
Song Feng, Jun Seok Kang, Polina Kuznetsova, and
Yejin Choi. 2013. Connotation lexicon: A dash of
sentiment beneath the surface meaning. In Proceed-
ings of ACL.
Rudolph Flesch. 1948. A new readability yardstick.
Journal of applied psychology, 32(3):221.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein,
Michael Heilman, Dani Yogatama, Jeffrey Flanigan,
and Noah A. Smith. 2011. Part-of-speech Tagging
for Twitter: Annotation, Features, and Experiments.
In Proceedings of NAACL (short paper).
David Godes, Dina Mayzlin, Yubo Chen, Sanjiv
Das, Chrysanthos Dellarocas, Bruce Pfeiffer, Barak
Libai, Subrata Sen, Mengze Shi, and Peeter Verlegh.
2005. The firm?s management of social interactions.
Marketing Letters, 16(3-4):415?428.
Marco Guerini, Carlo Strapparava, and G?ozde
?
Ozbal.
2011. Exploring text virality in social networks. In
Proceedings of ICWSM (poster).
Marco Guerini, Alberto Pepe, and Bruno Lepri. 2012.
Do linguistic style and readability of scientific ab-
stracts affect their virality? In Proceedings of
ICWSM (poster).
Lars Kai Hansen, Adam Arvidsson, Finn
?
Arup Nielsen,
Elanor Colleoni, and Michael Etter. 2011. Good
friends, bad news-affect and virality in Twitter.
Communications in Computer and Information Sci-
ence, 185:34?43.
Chip Heath, Chris Bell, and Emily Sternberg. 2001.
Emotional selection in memes: The case of urban
legends. Journal of personality and social psychol-
ogy, 81(6):1028.
George C. Homans. 1958. Social Behavior as Ex-
change. American Journal of Sociology, 63(6):597?
606.
Liangjie Hong, Ovidiu Dan, and Brian D. Davison.
2011. Predicting popular messages in Twitter. In
Proceedings of WWW.
Carl I. Hovland, Irving L. Janis, and Harold H. Kelley.
1953. Communication and Persuasion: Psycholog-
ical Studies of Opinion Change, volume 19. Yale
University Press.
J. Peter Kincaid, Robert P. Fishburne Jr., Richard L.
Rogers, and Brad S. Chissom. 1975. Derivation
of new readability formulas (automated readability
index, fog count and flesch reading ease formula)
for navy enlisted personnel. Technical report, DTIC
Document.
Haewoon Kwak, Changhyun Lee, Hosung Park, and
Sue Moon. 2010. What is Twitter, a social network
or a news media? In Proceedings of WWW.
Himabindu Lakkaraju, Julian McAuley, and Jure
Leskovec. 2013. What?s in a name? Understanding
the interplay between titles, content, and communi-
ties in social media. In Proceedings of ICWSM.
Annie Louis and Ani Nenkova. 2013. What makes
writing great? First experiments on article quality
prediction in the science journalism domain. Trans-
actions of ACL.
Zongyang Ma, Aixin Sun, and Gao Cong. 2012. Will
this #hashtag be popular tomorrow? In Proceedings
of SIGIR.
Neil McIntyre and Mirella Lapata. 2009. Learning to
tell tales: A data-driven approach to story genera-
tion. In Proceedings of ACL-IJCNLP.
Katherine L Milkman and Jonah Berger. 2012. What
makes online content viral? Journal of Marketing
Research, 49(2):192?205.
184
Sa?sa Petrovi?c, Miles Osborne, and Victor Lavrenko.
2011. RT to win! Predicting message propagation
in Twitter. In Proceedings of ICWSM.
Emily Pitler and Ani Nenkova. 2008. Revisiting
readability: A unified framework for predicting text
quality. In Proceedings of EMNLP.
Ellen Riloff, Ashequl Qadir, Prafulla Surve, Lalin-
dra De Silva, Nathan Gilbert, and Ruihong Huang.
2013. Sarcasm as contrast between a positive sen-
timent and negative situation. In Proceedings of
EMNLP.
Daniel M. Romero, Chenhao Tan, and Johan Ugander.
2013. On the interplay between social and topical
structure. In Proceedings of ICWSM.
Matthew J. Salganik, Peter Sheridan Dodds, and Dun-
can J. Watts. 2006. Experimental study of inequal-
ity and unpredictability in an artificial cultural mar-
ket. Science, 311(5762):854?856.
Matthew P. Simmons, Lada A Adamic, and Eytan Adar.
2011. Memes online: Extracted, subtracted, in-
jected, and recollected. In Proceedings of ICWSM.
Bongwon Suh, Lichan Hong, Peter Pirolli, and Ed H.
Chi. 2010. Want to be retweeted? Large scale an-
alytics on factors impacting retweet in Twitter net-
work. In Proceedings of SocialCom.
Tao Sun, Ming Zhang, and Qiaozhu Mei. 2013. Unex-
pected relevance: An empirical study of serendipity
in retweets. In Proceedings of ICWSM.
Oren Tsur and Ari Rappoport. 2012. What?s in a hash-
tag?: Content based prediction of the spread of ideas
in microblogging communities. In Proceedings of
WSDM.
Jaewon Yang and Jure Leskovec. 2011. Patterns of
temporal variation in online media. In Proceedings
of WSDM.
185
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 403?408,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
A Corpus of Sentence-level Revisions in Academic Writing:
A Step towards Understanding Statement Strength in Communication
Chenhao Tan
Dept. of Computer Science
Cornell University
chenhao@cs.cornell.edu
Lillian Lee
Dept. of Computer Science
Cornell University
llee@cs.cornell.edu
Abstract
The strength with which a statement is
made can have a significant impact on the
audience. For example, international rela-
tions can be strained by how the media in
one country describes an event in another;
and papers can be rejected because they
overstate or understate their findings. It is
thus important to understand the effects of
statement strength. A first step is to be able
to distinguish between strong and weak
statements. However, even this problem
is understudied, partly due to a lack of
data. Since strength is inherently relative,
revisions of texts that make claims are a
natural source of data on strength differ-
ences. In this paper, we introduce a corpus
of sentence-level revisions from academic
writing. We also describe insights gained
from our annotation efforts for this task.
1 Introduction
It is important for authors and speakers to find the
appropriate ?pitch? to convey a desired message
to the public. Indeed, sometimes heated debates
can arise around the choice of statement strength.
For instance, on March 1, 2014, an attack at Kun-
ming?s railway station left 29 people dead and
more than 140 others injured.
1
In the aftermath,
Chinese media accused Western media of ?soft-
pedaling the attack and failing to state clearly that
it was an act of terrorism?.
2
In particular, regard-
ing the statement by the US embassy that referred
to this incident as the ?terrible and senseless act
of violence in Kunming?, a Weibo user posted ?If
you say that the Kunming attack is a ?terrible and
1
http://en.wikipedia.org/wiki/2014_
Kunming_attack
2
http://sinosphere.blogs.nytimes.
com/2014/03/03/u-n-security-council-
condemns-terrorist-attack-in-kunming/
senseless act of violence?, then the 9/11 attack can
be called a ?regrettable traffic incident??.
3
This example is striking but not an isolated case,
for settings in which one party is trying to con-
vince another are pervasive; scenarios range from
court trials to conference submissions. Since the
strength and scope of an argument can be a cru-
cial factor in its success, it is important to under-
stand the effects of statement strength in commu-
nication.
A first step towards addressing this question is
to be able to distinguish between strong and weak
statements. As strength is inherently relative, it is
natural to look at revisions that change statement
strength, which we refer to as ?strength changes?.
Though careful and repeated revisions are presum-
ably ubiquitous in politics, legal systems, and jour-
nalism, it is not clear how to collect them; on the
other hand, revisions to research papers may be
more accessible, and many researchers spend sig-
nificant time on editing to convey the right mes-
sage regarding the strength of a project?s contribu-
tions, novelty, and limitations. Indeed, statement
strength in science communication matters to writ-
ers: understating contributions can affect whether
people recognize the true importance of the work;
at the same time, overclaiming can cause papers to
be rejected.
With the increasing popularity of e-print ser-
vices such as the arXiv
4
, strength changes in scien-
tific papers are becoming more readily available.
Since the arXiv started in 1991, it has become
?the standard repository for new papers in mathe-
matics, physics, statistics, computer science, biol-
ogy, and other disciplines? (Krantz, 2007). An in-
triguing observation is that many researchers sub-
mit multiple versions of the same paper on arXiv.
For instance, among the 70K papers submitted in
3
http://www.huffingtonpost.co.uk/2014/
03/03/china-kunming-911_n_4888748.html
4
http://arxiv.org/
403
ID Pairs
1
S1: The algorithm is studied in this paper .
S2: The algorithm is proposed in this paper .
2
S1: ... circadian pattern and burstiness in human communication activity .
S2: ... circadian pattern and burstiness in mobile phone communication .
3
S1: ... using minhash techniques , at a significantly lower cost and with same privacy guarantees .
S2: ... using minhash techniques , with lower costs .
4
S1: the rows and columns of the covariate matrix then have certain physical meanings ...
S2: the rows and columns of the covariate matrix could have different meanings ...
5
S1: they maximize the expected revenue of the seller but induce efficiency loss .
S2: they maximize the expected revenue of the seller but are inefficient .
Table 1: Examples of potential strength differences.
2011, almost 40% (27.7K) have multiple versions.
Many differences between these versions consti-
tute a source of valid and motivated strength dif-
ferences, as can be seen from the sentential revi-
sions in Table 1. Pair 1 makes the contribution
seem more impressive by replacing ?studied? with
?proposed?. Pair 2 downgrades ?human commu-
nication activity? to ?mobile phone communica-
tion?. Pair 3 removes ?significantly? and the em-
phasis on ?same privacy guarantees?. Pair 4 shows
an insertion of hedging, a relatively well-known
type of strength reduction. Pair 5 is an interesting
case that shows the complexity of this problem: on
the one hand, S2 claims that something is ?ineffi-
cient?, which is an absolute statement, compared
to ?efficiency loss? in S1, where the possibility of
efficiency still exists; on the other hand, S1 em-
ploys an active tone that emphasizes a causal rela-
tionship.
The main contribution of this work is to provide
the first large-scale corpus of sentence-level revi-
sions for studying a broad range of variations in
statement strength. We collected labels for a sub-
set of these revisions. Given the possibility of all
kinds of disagreement, the fair level of agreement
(Fleiss? Kappa) among our annotators was decent.
But in some cases, the labels differed from our ex-
pectations, indicating that the general public can
interpret the strength of scientific statements dif-
ferently from researchers. The participants? com-
ments may further shed light on science commu-
nication and point to better ways to define and un-
derstand strength differences.
2 Related Work and Data
Hedging, which can lead to strength differences,
has received some attention in the study of science
communication (Salager-Meyer, 2011; Lewin,
1998; Hyland, 1998; Myers, 1990). The CoNLL
2010 Shared Task was devoted to hedge detection
(Farkas et al, 2010). Hedge detection was also
used to understand scientific framing in debates
over genetically-modified organisms in food (Choi
et al, 2012).
Revisions on Wikipedia have been shown use-
ful for various applications, including spelling
correction (Zesch, 2012), sentence compression
(Yamangil and Nelken, 2008), text simplification
(Yatskar et al, 2010), paraphrasing (Max and Wis-
niewski, 2010), and textual entailment (Zanzotto
and Pennacchiotti, 2010). But none of the cat-
egories of Wikipedia revisions previously exam-
ined (Daxenberger and Gurevych, 2013; Bronner
and Monz, 2012; Mola-Velasco, 2011; Potthast et
al., 2008; Daxenberger and Gurevych, 2012) re-
late to statement strength. After all, the objective
of editing on Wikipedia is to present neutral and
objective articles.
Public datasets of science communication are
available, such as the ACL Anthology,
5
collec-
tions of NIPS papers,
6
and so on. These datasets
are useful for understanding the progress of disci-
plines or the evolution of topics. But the lack of
edit histories or revisions makes them not imme-
diately suitable for studying strength differences.
Recently, there have been experiments with open
peer review.
7
Records from open reviewing can
provide additional insights into the revision pro-
cess once enough data is collected.
5
http://aclweb.org/anthology/
6
http://nips.djvuzone.org/txt.html
7
http://openreview.net
404
title abstract intro middle conclusion0.0
100000.0200000.0300000.0
400000.0500000.0600000.0
700000.0800000.0900000.0
numberofc
hanges
57% 71% 65%
58%
62%
deletiontyporewrite
(a) Number of changes vs sections.
?middle? refers to the sections be-
tween introduction and conclusion.
math cond-mat astro-ph cs quant-ph0.0
100000.0200000.0
300000.0400000.0
500000.0600000.0
numberofc
hanges 57%
61% 67% 56% 59%
deletiontyporewrite
(b) Top 5 categories in number of
changes.
stat q-bio q-fin cs quant-ph0.0
0.10.2
0.30.4
0.5
numberofc
hangesper
sentence 54% 58% 58% 56% 59%
deletiontyporewrite
(c) Top 5 categories in number of
changes over the number of sen-
tences.
Figure 1: In all figures, different colors indicate different types of changes.
3 Dataset Description
Our main dataset was constructed from all papers
submitted in 2011 on the arXiv. We first extracted
the textual content from papers that have multiple
versions of tex source files. All mathematical en-
vironments were ignored. Section titles were not
included in the final texts but are used in align-
ment.
In order to align the first version and the fi-
nal version of the same paper, we first did macro
alignment of paper sections based on section titles.
Then, for micro alignment of sentences, we em-
ployed a dynamic programming algorithm similar
to that of Barzilay and Elhadad (2003). Instead of
cosine similarity, we used an idf-weighted longest-
common-subsequence algorithm to define the sim-
ilarity between two sentences, because changes in
word ordering can also be interesting. Formally,
the similarity score between sentence i and sen-
tence j is defined as
Simpi, jq ?
Weighted-LCSpS
i
, S
j
q
maxp
?
wPS
i
idfpwq,
?
wPS
j
idfpwqq
,
where S
i
and S
j
refer to sentence i and sentence j.
Since it is likely that a new version adds or deletes
a large sequence of sentences, we did not impose a
skip penalty. We set the mismatch penalty to 0.1.
8
In the end, there are 23K papers where the first
version was different from the last version.
9
We
8
We did not allow cross matching (i.e., i? j?1, i?1?
j), since we thought matching this case as pi ? 1, iq ? j or
i ? pj, j ? 1q can provide context for annotation purposes.
But in the end, we focused on labeling very similar pairs.
This decision had little effect.
9
This differs from the number in Section 1 because arti-
cles may not have the tex source available, or the differences
between versions may be in non-textual content.
categorize sentential revisions into the following
three types:
? Deletion: we cannot find a match in the final
version.
? Typo: all sequences in a pair of matched sen-
tences are typos, where a sequence-level typo
is one where the edit distance between the
matched sequences is less than three.
? Rewrite: matched sentences that are not ty-
pos. This type is the focus of this study.
What kinds of changes are being made? One
might initially think that typo fixes represent a
large proportion of revisions, but this is not cor-
rect, as shown in Figure 1a. Deletions represent a
substantial fraction, especially in the middle sec-
tion of a paper. But it is clear that the majority of
changes are rewrites; thus revisions on the arXiv
indeed provide a great source for potential strength
differences.
Who makes changes? Figure 1b shows that the
Math subarchive makes the largest number of
changes. This is consistent with the mathematics
community?s custom of using the arXiv to get find-
ings out early. In terms of changes per sentence
(Figure 1c), statistics and quantitative studies are
the top subareas.
Further, Figure 2 shows the effect of the number
of authors. It is interesting that both in terms of
sheer number and percentage, single-authored pa-
pers have the most changes. This could be because
a single author enjoys greater freedom and has
stronger motivation to make changes, or because
multiple authors tend to submit a more polished
initial version. This echoes the finding in Posner
405
You should mark S2 as Stronger if
? (R1) S2 strengthens the degree of some aspect of S1, for example, S1 has the word ?better?,
whereas S2 uses ?best?, or S2 removes the word ?possibly?
? (R2) S2 adds more evidence or justification (we don?t count adding details)
? (R3) S2 sounds more impressive in some other way: the authors? work is more important/novel-
/elegant/applicable/etc.
If instead S1 is stronger than S2 according to the reasons above, select Weaker. If the changes
aren?t strengthenings or weakenings according to the reason above, select No Strength Change.
If there are both strengthenings and weakenings, or you find that it is really hard to tell whether the
change is stronger or weaker, then select I can?t tell.
Table 2: Definition of labels in our labeling tasks.
1 2 3 4 5 >5number of authors46.048.050.052.0
54.056.058.060.062.0
64.0
numberofchange
s
(a) Number of changes vs
number of authors.
1 2 3 4 5 >5number of authors26%27%
28%29%
30%
percentageofcha
nges
(b) Percentage of changed
sentences vs number of au-
thors.
Figure 2: Error bars represent standard error. (a):
up until 5 authors, a larger number of authors in-
dicates a smaller number of changes. (b): per-
centage is measured over the number of sentences
in the first version; there is an interior minimum
where 2 or 3 authors make the smallest percentage
of sentence changes on a paper.
and Baecker (1992) that the collaborative writing
process differs considerably from individual writ-
ing. Also, more than 25% of the first versions are
changed, which again shows that substantive edits
are being made in these resubmissions.
4 Annotating Strength Differences
In order to study statement strength, reliable
strength-difference labels are needed. In this sec-
tion, we describe how we tried to define strength
differences, compiled labeling instructions, and
gathered labels using Amazon Mechanical Turk.
Label definition and collection procedure. We
focused on matched sentences from abstracts
and introductions to maximize the proportion of
strength differences (as opposed to factual/no
strength changes). We required pairs to have sim-
ilarity score larger than 0.5 in our labeling task to
make pairs more comparable. We also replaced
all math environments with ?[MATH]?.
10
We ob-
tained 108K pairs that satisfy the above condi-
tions, available at http://chenhaot.com/
pages/statement-strength.html. To
create the pool of pairs for labeling, we randomly
sampled 1000 pairs and then removed pairs that
we thought were processing errors.
We used Amazon Mechanical Turk. It may
initially seem surprising to have annotations of
technical statements not done by domain experts;
we did this intentionally because it is common to
communicate unfamiliar topics to the public in po-
litical and science communication (we comment
on non-expert rationales later). We use the follow-
ing set of labels: Stronger, Weaker, No Strength
Change, I can?t tell. Table 2 gives our definitions.
The instructions included 8 pairs as examples and
10 pairs to label as a training exercise. Partici-
pants were then asked to choose labels and write
mandatory comments for 50 pairs. According to
the comments written by participants, we believe
that they did the labeling in good faith.
Quantitative overview. We collected 9 labels
each for 500 pairs. Among the 500 pairs, Fleiss?
Kappa was 0.242, which indicates fair agreement
(Landis and Koch, 1977). We took a conserva-
tive approach and only considered pairs with an
absolute majority label, i.e., at least 5 of 9 label-
ers chose the same label. There are 386 pairs that
satisfy this requirement (93 weaker, 194 stronger,
99 no change). On this subset of pairs, Fleiss?
Kappa is 0.322, and 74.4% of pairs were strength
changes. Considering all the possible disagree-
ment, this result was acceptable.
Qualitative observations. We were excited
about the labels from these participants: despite
10
These decisions were made based on the results and feed-
back that we got from graduate students in an initial labeling.
406
ID Matched sentences and comments
1
S1: ... using data from numerics and experiments .
S2: ... using data sets from numerics in the point particle limit and one experimental data set .
(stronger) S2 is more specific in its description which seems stronger.
(weaker) ?one experimental data set? weakens the sentence
2
S1: we also proved that if [MATH] is sufficiently homogeneous then ...
S2: we also proved that if [MATH] is not totally disconnected and sufficiently homogeneous then ...
(stronger) We have more detail/proof in S2
(stronger) the words ?not totally disconnected? made the sentence sound more impressive.
3
S1: we also show in general that vectors of products of jack vertex operators form a basis of symmetric functions .
S2: we also show in general that the images of products of jack vertex operators form a basis of symmetric functions .
(weaker) Vectors sounds more impressive than images
(weaker) sentence one is more specific
4
S1: in the current paper we discover several variants of qd algorithms for quasiseparable matrices .
S2: in the current paper we adapt several variants of qd algorithms to quasiseparable matrices .
(stronger) in S2 Adapt is stronger than just the word discover. adapt implies more of a proactive measure.
(stronger) s2 sounds as if they?re doing something with specifics already, rather than hunting for a way to do it
Table 3: Representative examples of surprising labels, together with selected labeler comments.
the apparent difficulty of the task, we found that
many labels for the 386 pairs were reasonable.
However, in some cases, the labels were counter-
intuitive. Table 3 shows some representative ex-
amples.
First, participants tend to take details as evi-
dence even when these details are not germane to
the statement. For pair 1, while one turker pointed
out the decline in number of experiments, most
turkers simply labeled it as stronger because it was
more specific. ?Specific? turned out to be a com-
mon reason used in the comments, even though we
said in the instructions that only additional justifi-
cation and evidence matter. This echoes the find-
ing in Bell and Loftus (1989) that even unrelated
details influenced judgments of guilt.
Second, participants interpret constraints/condi-
tions not in strictly logical ways, seeming to care
little about scope at times. For instance, the ma-
jority labeled pair 2 as ?stronger?. But in S2 for
that pair, the result holds for strictly fewer pos-
sible worlds. But it should be said that there
are cases that labelers interpreted logically, e.g.,
?compelling evidence? subsumes ?compelling ex-
perimental evidence?.
Both of the above cases share the property that
they seem to be correlated with a tendency to
judge lengthier statements as stronger. Another
interesting case that does not share this character-
istic is that participants can have a different un-
derstanding of domain-specific terms. For pair 3,
the majority thought that ?vectors? sounds more
impressive than ?images?; for pair 4, the major-
ity considered ?adapt? stronger than ?discover?.
This issue is common when communicating new
topics to the public not only in science commu-
nication but also in politics and other scenarios. It
may partly explain miscommunications and misin-
terpretations of scientific studies in journalism.
11
5 Looking ahead
Our observations regarding the annotation results
raise questions regarding what is a generalizable
way to define strength differences, how to use the
labels that we collected, and how to collect la-
bels in the future. We believe that this corpus of
sentence-level revisions, together with the labels
and comments from participants, can provide in-
sights into better ways to approach this problem
and help further understand strength of statements.
One interesting direction that this enables is a
potentially new kind of learning problem. The
comments indicate features that humans think
salient. Is it possible to automatically learn new
features from the comments?
The ultimate goal of our study is to understand
the effects of statement strength on the public,
which can lead to various applications in public
communication.
Acknowledgments
We thank J. Baldridge, J. Boyd-Graber, C.
Callison-Burch, and the reviewers for helpful
comments; P. Ginsparg for providing data; and S.
Chen, E. Kozyri, M. Lee, I. Lenz, M. Ott, J. Park,
K. Raman, M. Reitblatt, S. Roy, A. Sharma, R.
Sipos, A. Swaminathan, L. Wang, W. Xie, B. Yang
and the anonymous annotators for all their label-
ing help. This work was supported in part by NSF
grant IIS-0910664 and a Google Research Grant.
11
http://www.phdcomics.com/comics/
archive.php?comicid=1174
407
References
Regina Barzilay and Noemie Elhadad. 2003. Sentence
alignment for monolingual comparable corpora. In
Proceedings of the 2003 conference on Empirical
methods in natural language processing, pages 25?
32.
Brad E Bell and Elizabeth F Loftus. 1989. Trivial per-
suasion in the courtroom: The power of (a few) mi-
nor details. Journal of Personality and Social Psy-
chology, 56(5):669.
Amit Bronner and Christof Monz. 2012. User Edits
Classification Using Document Revision Histories.
In Proceedings of the 13th Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics, pages 356?366.
Eunsol Choi, Chenhao Tan, Lillian Lee, Cristian
Danescu-Niculescu-Mizil, and Jennifer Spindel.
2012. Hedge detection as a lens on framing in the
GMO debates: A position paper. In Proceedings
of the Workshop on Extra-Propositional Aspects of
Meaning in Computational Linguistics, pages 70?
79.
Johannes Daxenberger and Iryna Gurevych. 2012. A
Corpus-Based Study of Edit Categories in Featured
and Non-Featured Wikipedia Articles. In COLING,
pages 711?726.
Johannes Daxenberger and Iryna Gurevych. 2013.
Automatically Classifying Edit Categories in
Wikipedia Revisions. In Proceedings of the 2013
Conference on Empirical Methods in Natural
Language Processing.
Rich?ard Farkas, Veronika Vincze, Gy?orgy M?ora, J?anos
Csirik, and Gy?orgy Szarvas. 2010. The CoNLL-
2010 shared task: Learning to detect hedges and
their scope in natural language text. In CoNLL?
Shared Task, pages 1?12.
Ken Hyland. 1998. Hedging in scientific research
articles. John Benjamins Pub. Co., Amsterdam;
Philadelphia.
Steven G. Krantz. 2007. How to Write Your First Pa-
per. Notices of the AMS.
J. Richard Landis and Gary G. Koch. 1977. The
Measurement of Observer Agreement for Categor-
ical Data. Biometrics, 33(1):159?174.
Beverly A. Lewin. 1998. Hedging: Form and func-
tion in scientific research texts. In Genre Studies
in English for Academic Purposes, volume 9, pages
89?108. Universitat Jaume I.
Aurlien Max and Guillaume Wisniewski. 2010.
Mining Naturally-occurring Corrections and Para-
phrases from Wikipedia?s Revision History. In Pro-
ceedings of The seventh international conference on
Language Resources and Evaluation.
Santiago M Mola-Velasco. 2011. Wikipedia Vandal-
ism Detection. In Proceedings of the 20th Interna-
tional Conference Companion on World Wide Web,
pages 391?396.
Greg Myers. 1990. Writing biology: Texts in the social
construction of scientific knowledge. University of
Wisconsin Press, Madison, Wis.
Ilona R Posner and Ronald M Baecker. 1992. How
people write together [groupware]. In System
Sciences, 1992. Proceedings of the Twenty-Fifth
Hawaii International Conference on, pages 127?
138.
Martin Potthast, Benno Stein, and Robert Ger-
ling. 2008. Automatic Vandalism Detection in
Wikipedia. In Advances in Information Retrieval,
pages 663?668. Springer Berlin Heidelberg.
Franc?oise Salager-Meyer. 2011. Scientific discourse
and contrastive linguistics: hedging. European Sci-
ence Editing, 37(2):35?37.
Elif Yamangil and Rani Nelken. 2008. Mining
Wikipedia Revision Histories for Improving Sen-
tence Compression. In Proceedings of the 46th
Annual Meeting of the Association for Computa-
tional Linguistics on Human Language Technolo-
gies: Short Papers, pages 137?140.
Mark Yatskar, Bo Pang, Cristian Danescu-Niculescu-
Mizil, and Lillian Lee. 2010. For the sake of sim-
plicity: Unsupervised extraction of lexical simplifi-
cations from Wikipedia. In Human Language Tech-
nologies: The 2010 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 365?368.
Fabio Massimo Zanzotto and Marco Pennacchiotti.
2010. Expanding textual entailment corpora from
Wikipedia using co-training. In Proceedings of the
2nd Workshop on Collaboratively Constructed Se-
mantic Resources.
Torsten Zesch. 2012. Measuring Contextual Fitness
Using Error Contexts Extracted from the Wikipedia
Revision History. In Proceedings of the 13th Con-
ference of the European Chapter of the Association
for Computational Linguistics, pages 529?538.
408
Proceedings of the Fourteenth Conference on Computational Natural Language Learning, page 55,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Clueless: Explorations in unsupervised, knowledge-lean extraction of
lexical-semantic information
Invited Talk
Lillian Lee
Department of Computer Science, Cornell University
llee@cs.cornell.edu
I will discuss two current projects on automatically extracting certain types of lexical-semantic
information in settings wherein we can rely neither on annotations nor existing knowledge resources
to provide us with clues. The name of the game in such settings is to find and leverage auxiliary sources
of information.
Why is it that if you know I?ll give a silly talk, it follows that you know I?ll give a talk, whereas if you
doubt I?ll give a good talk, it doesn?t follow that you doubt I?ll give a talk? This pair of examples
shows that the word ?doubt? exhibits a special but prevalent kind of behavior known as downward
entailingness ? the licensing of reasoning from supersets to subsets, so to speak, but not vice versa. The
first project I?ll describe is to identify words that are downward entailing, a task that promises to enhance
the performance of systems that engage in textual inference, and one that is quite challenging since it is
difficult to characterize these items as a class and no corpus with downward-entailingness annotations
exists. We are able to surmount these challenges by utilizing some insights from the linguistics literature
regarding the relationship between downward entailing operators and what are known as negative polarity
items ? words such as ?ever? or the idiom ?have a clue? that tend to occur only in negative contexts.
A cross-linguistic analysis indicates some potentially interesting connections to findings in linguistic
typology.
That previous paragraph was quite a mouthful, wasn?t it? Wouldn?t it be nice if it were written in plain
English that was easier to understand? The second project I?ll talk about, which has the eventual aim to
make it possible to automatically simplify text, aims to learn lexical-level simplifications, such as ?work
together? for ?collaborate?. (This represents a complement to prior work, which focused on syntactic
transformations, such as passive to active voice.) We exploit edit histories in Simple English Wikipedia
for this task. This isn?t as simple (ahem) as it might at first seem because Simple English Wikipedia and
the usual Wikipedia are far from a perfect parallel corpus and because many edits in Simple Wikipedia
do not constitute simplifications. We consider both explicitly modeling different kinds of operations and
various types of bootstrapping, including as clues the comments Wikipedians sometimes leave when they
edit.
Joint work with Cristian Danescu-Niculescu-Mizil, Bo Pang, and Mark Yatskar.
55
Proceedings of the 2nd Workshop on Cognitive Modeling and Computational Linguistics, pages 76?87,
Portland, Oregon, June 2011. c?2011 Association for Computational Linguistics
Chameleons in imagined conversations: A new approach to understanding
coordination of linguistic style in dialogs
Cristian Danescu-Niculescu-Mizil and Lillian Lee
Department of Computer Science, Cornell University
cristian@cs.cornell.edu, llee@cs.cornell.edu
Abstract
Conversational participants tend to immedi-
ately and unconsciously adapt to each other?s
language styles: a speaker will even adjust the
number of articles and other function words in
their next utterance in response to the number
in their partner?s immediately preceding utter-
ance. This striking level of coordination is
thought to have arisen as a way to achieve so-
cial goals, such as gaining approval or empha-
sizing difference in status. But has the adap-
tation mechanism become so deeply embed-
ded in the language-generation process as to
become a reflex? We argue that fictional di-
alogs offer a way to study this question, since
authors create the conversations but don?t re-
ceive the social benefits (rather, the imagined
characters do). Indeed, we find significant co-
ordination across many families of function
words in our large movie-script corpus. We
also report suggestive preliminary findings on
the effects of gender and other features; e.g.,
surprisingly, for articles, on average, charac-
ters adapt more to females than to males.
1 Introduction
?...it is dangerous to base any sociolinguistic argu-
mentation on the evidence of language in fictional
texts only? (Bleichenbacher (2008), crediting Mare?
(2000))
The chameleon effect is the ?nonconscious
mimicry of the postures, mannerisms, facial expres-
sions, and other behaviors of one?s interaction part-
ners? (Chartrand and Bargh, 1999).1 For exam-
ple, if one conversational participant crosses their
1The term is a reference to the movie Zelig, wherein a ?hu-
arms, their partner often unconsciously crosses their
arms as well. The effect occurs for language, too,
ranging from matching of acoustic features such as
accent, speech rate, and pitch (Giles et al, 1991;
Chartrand and van Baaren, 2009) to lexico-syntactic
priming across adjacent or nearby utterances (Bock,
1986; Pickering and Garrod, 2004; Ward and Lit-
man, 2007; Reitter et al, 2011).
Our work focuses on adjacent-utterance coordina-
tion with respect to classes of function words. To ex-
emplify the phenomenon, we discuss two short con-
versations.
? First example: The following exchange from the
movie ?The Getaway? (1972) demonstrates quanti-
fier coordination.
Doc: At least you were outside.
Carol: It doesn?t make much difference where you are [...]
Note that ?Carol? used a quantifier, one that is differ-
ent than the one ?Doc? employed. Also, notice that
?Carol? could just as well have replied in a way that
doesn?t include a quantifier, for example, ?It doesn?t
really matter where you are...?.
? Second example: Levelt and Kelter (1982) report
an experiment involving preposition coordination.
Shopkeepers who were called and asked ? At what
time does your shop close?? were significantly more
likely to say ? At five o?clock? than ?five o?clock?.2
man chameleon? uncontrollably takes on the characteristics of
those around him. The term is meant to contrast with ?aping?,
a word connoting intentional imitation.
Related terms include adaptation, alignment, entrainment,
priming, and Du Bois? dialogic syntax.
2This is an example of lexical matching manifested as part
of syntactic coordination.
76
Coordination of function-word class has been pre-
viously documented in several settings (Niederhof-
fer and Pennebaker, 2002; Taylor and Thomas,
2008; Ireland et al, 2011; Gonzales et al, 2010),
the largest-scale study being on Twitter (Danescu-
Niculescu-Mizil et al, 2011).
Problem setting People don?t consciously track
function words (Levelt and Kelter, 1982; Segalowitz
and Lane, 2004; Petten and Kutas, 1991) ? it?s not
easy to answer the question, ?how many preposi-
tions were there in the sentence I just said??. There-
fore, it is quite striking that humans nonetheless in-
stantly adapt to each other?s function-word rates. In-
deed, there is active debate regarding what mecha-
nisms cause nonconscious coordination (Ireland et
al., 2011; Branigan et al, 2010).
One line of thought is that convergence represents
a social strategy3 whose aim is to gain the other?s so-
cial approval (Giles, 2008; Street and Giles, 1982)
or enhance the other?s comprehension (Clark, 1996;
Bortfeld and Brennan, 1997).4 This hypothesis is
supported by studies showing that coordination is af-
fected by a number of social factors, including rel-
ative social status (Natale, 1975; Gregory and Web-
ster, 1996; Thakerar et al, 1982) and gender role
(Bilous and Krauss, 1988; Namy et al, 2002; Ire-
land and Pennebaker, 2010).
But an important question is whether the adap-
tation mechanism has become so deeply embed-
ded in the language-generation process as to have
transformed into a reflex not requiring any social
triggering.5 Indeed, it has been argued that un-
conscious mimicry is partly innate (Chartrand and
Bargh, 1999), perhaps due to evolutionary pressure
to foster relationships (Lakin et al, 2003).
To answer this question, we take a radical ap-
proach: we consider a setting in which the per-
sons generating the coordinating dialog are different
from those engaged in the dialog (and standing to
reap the social benefits) ? imagined conversations,
specifically, scripted movie dialogs.
3In fact, social signaling may also be the evolutionary cause
of chameleons? color-changing ability (Stuart-Fox et al, 2008).
4For the purpose of our discussion, we are conflating social-
approval and audience-design hypotheses under the category of
social strategy.
5This hypothesis relates to characterizations of alignment as
an unmediated mechanism (Pickering and Garrod, 2004).
Life is beautiful, but cinema is paradise A pri-
ori, it is not clear that movie conversations would ex-
hibit convergence. Dialogs between movie charac-
ters are not truthful representations of real-life con-
versations. They often are ?too carefully polished,
too rhythmically balanced, too self-consciously art-
ful? (Kozloff, 2000), due to practical and artis-
tic constraints and scriptwriting practice (McKee,
1999). For example, mundane phenomena such as
stuttering and word repetitions are generally nonex-
istent on the big screen. Moreover, writers have
many goals to accomplish, including the need to ad-
vance the plot, reveal character, make jokes as funny
as possible, and so on, all incurring a cognitive load.
So, the question arises: do scripted movie di-
alogs, in spite of this quasi-artificiality and the
aforementioned generation/engagement gap, exhibit
the real-life phenomenon of stylistic convergence?
When imagining dialogs, do scriptwriters (noncon-
sciously6) adjust the respondent?s replies to echo the
initiator?s use of articles, prepositions, and other ap-
parently minor aspects of lexical choice? According
to our results, this is indeed the case, which has fas-
cinating implications.
First, this provides evidence that coordination, as-
sumed to be driven by social motivations, has be-
come so deeply embedded into our ideas of what
conversations ?sound like? that the phenomenon oc-
curs even when the person generating the dialog is
not the recipient of the social benefits.7
Second, movies can be seen as a controlled en-
vironment in which preconceptions about the rela-
tion between communication patterns and the social
features of the participants can be studied. This
gives us the opportunity to understand how people
(scriptwriters) nonconsciously expect convergence
to relate to factors such as gender, status and rela-
tion type. Are female characters thought to accom-
modate more to male characters than vice-versa?
Furthermore, movie scripts constitute a corpus
that is especially convenient because meta-features
6The phenomenon of real-life language convergence is not
widely known among screenplay authors (Beth F. Milles, pro-
fessor of acting and directing, personal communication).
7Although some writers may perhaps imagine themselves
"in the shoes" of the recipients, recall that authors generally
don?t include in their scripts the repetitions and ungrammati-
calities of "real-life" speech.
77
like gender can be more or less readily obtained.
Contributions We check for convergence in a
corpus of roughly 250,000 conversational exchanges
from movie scripts (available at http://www.
cs.cornell.edu/~cristian/movies).
Specifically, we examine the set of nine families of
stylistic features previously utilized by Ireland et
al. (2011), and find a statistically significant con-
vergence effect for all these families. We thereby
provide evidence that language coordination is so
implanted within our conception of conversational
behavior that, even if such coordination is socially
motivated, it is exhibited even when the person
generating the language in question is not receiving
any of the presumed social advantages.
We also study the effects of gender, narrative im-
portance, and hostility. Intriguingly, we find that
these factors indeed ?affect? movie characters? lin-
guistic behavior; since the characters aren?t real,
and control of stylistic lexical choice is largely non-
conscious, the effects of these factors can only be
springing from patterns existing in the scriptwriters?
minds.
Our findings, by enhancing our understanding of
linguistic adaptation effects in stylistic word choice
and its relation to various socially relevant factors,
may in the future aid in practical applications. Such
an understanding would give us insight into how
and what kinds of language coordination yield more
satisfying interactions ? convergence has been al-
ready shown to enhance communication in organiza-
tional contexts (Bourhis, 1991), psychotherapy (Fer-
rara, 1991), care of the mentally disabled (Hamilton,
1991), and police-community interactions (Giles et
al., 2007). Moreover, a deeper understanding can aid
human-computer interaction by informing the con-
struction of natural-language generation systems,
since people are often more satisfied with encoun-
ters exhibiting appropriate linguistic convergence
(Bradac et al, 1988; van Baaren et al, 2003), even
when the other conversational participant is known
to be a computer (Nass and Lee, 2000; Branigan et
al., 2010).
2 Related work not already mentioned
Linguistic style and human characteristics Us-
ing stylistic (i.e., non-topical) elements like arti-
cles and prepositions to characterize the utterer in
some way has a long history, including in author-
ship attribution (Mosteller and Wallace, 1984; Juola,
2008), personality-type classification (Argamon et
al., 2005; Oberlander and Gill, 2006; Mairesse et al,
2007), gender categorization (Koppel et al, 2002;
Mukherjee and Liu, 2010; Herring and Paolillo,
2006), identification of interactional style (Jurafsky
et al, 2009; Ranganath et al, 2009), and recognizing
deceptive language (Hancock et al, 2008; Mihalcea
and Strapparava, 2009).
Imagined conversations There has been work in
the NLP community applying computational tech-
niques to fiction, scripts, and other types of text
containing imagined conversations. For example,
one recent project identifies conversational networks
in novels, with the goal of evaluating various liter-
ary theories (Elson et al, 2010; Elson and McKe-
own, 2010). Movie scripts were used as word-sense-
disambiguation evaluation data as part of an effort
to generate computer animation from the scripts (Ye
and Baldwin, 2006). Sonderegger (2010) employed
a corpus of English poetry to study the relation-
ship between pronunciation and network structure.
Rayson et al (2001) computed part-of-speech fre-
quencies for imaginative writing in the British Na-
tional Corpus, finding a typology gradient progress-
ing from conversation to imaginative writing (e.g.,
novels) to task-oriented speech to informative writ-
ing. The data analyzed by Oberlander and Gill
(2006) consisted of emails that participants were in-
structed to write by imagining that they were going
to update a good friend on their current goings-on.
3 Movie dialogs corpus
To address the questions raised in the introduc-
tion, we created a large set of imagined conver-
sations, starting from movie scripts crawled from
various sites.8 Metadata for conversation analy-
sis and duplicate-script detection involved mostly-
automatic matching of movie scripts with the IMDB
movie database; clean-up resulted in 617 unique ti-
tles tagged with genre, release year, cast lists, and
8The source of these scripts and more detail about the corpus
are given in the README associated with the Cornell movie-
dialogs corpus, available at http://www.cs.cornell.
edu/~cristian/movies .
78
IMDB information. We then extracted 220,579
conversational exchanges between pairs of charac-
ters engaging in at least 5 exchanges, and auto-
matically matched these characters to IMDB to re-
trieve gender (as indicated by the designations ?ac-
tor? or ?actress?) and/or billing-position information
when possible (?9000 characters, ?3000 gender-
identified and ?3000 billing-positioned). The latter
feature serves as a proxy for narrative importance:
the higher up in the credits, the more important the
character tends to be in the film.
To the best of our knowledge, this is the largest
dataset of (metadata-rich) imaginary conversations
to date.
4 Measuring linguistic style
For consistency with prior work, we employed the
nine LIWC-derived categories (Pennebaker et al,
2007) deemed by Ireland et al (2011) to be pro-
cessed by humans in a generally non-conscious fash-
ion. The nine categories are: articles, auxiliary
verbs, conjunctions, high-frequency adverbs, im-
personal pronouns, negations, personal pronouns,
prepositions, and quantifiers (451 lexemes total).
It is important to note that language coordination
is multimodal: it does not necessarily occur simulta-
neously for all features (Ferrara, 1991), and speakers
may converge on some features but diverge on others
(Thakerar et al, 1982); for example, females have
been found to converge on pause frequency with
male conversational partners but diverge on laugh-
ter (Bilous and Krauss, 1988).
5 Measuring convergence
Niederhoffer and Pennebaker (2002) use the correla-
tion coefficient to measure accommodation with re-
spect to linguistic style features. While correlation
at first seems reasonable, it has some problematic as-
pects in our setting (we discuss these problems later)
that motivate us to employ an alternative measure.
We instead use a convergence measure introduced
in Danescu-Niculescu-Mizil et al (2011) that quan-
tifies how much a given feature family t serves as an
immediate trigger or stimulus, meaning that one per-
son?s utterance exhibiting such a feature triggers the
appearance of that feature in the respondent?s imme-
diate reply.
For example, we might be studying whether one
person A?s inclusion of articles in an utterance trig-
gers the usage of articles in respondent B?s reply.
Note that this differs from asking whether B uses ar-
ticles more often when talking to A than when talk-
ing to other people (it is not so surprising that peo-
ple speak differently to different audiences). This
also differs from asking whether B eventually starts
matching A?s behavior in later utterances within the
same conversation. We specifically want to know
whether each utterance by A triggers an immediate
change in B?s behavior, as such instantaneous adap-
tation is what we consider the most striking aspect
of convergence, although immediate and long-term
coordination are clearly related.
We now describe the statistic we employ to mea-
sure the extent to which person B accommodates to
A. Consider an arbitrary conversational exchange
started by A, and let a denote A?s initiating utterance
and b??a denote B?s reply to a.9 Note that we use
lowercase to emphasize when we are talking about
individual utterances rather than all the utterances of
the particular person, and that thus, the arrow in b??a
indicates that we mean the reply to the specific sin-
gle utterance a. Let at be the indicator variable for a
exhibiting t, and similarly for bt??a. Then, we define
the convergence ConvA,B(t) of B to A as:
P (bt??a = 1|a
t = 1)? P (bt??a = 1). (1)
Note that this quantity can be negative (indicating
divergence). The overall degree Conv(t) to which t
serves as a trigger is then defined as the expectation
of ConvA,B(t) over all initiator-respondent pairs:
Conv(t)
def
= Epairs(A,B)(ConvA,B(t)). (2)
Comparison with correlation: the importance
of asymmetry10 Why do we employ ConvA,B ,
Equation (1), instead of the well-known correlation
coefficient? One reason is that correlation fails to
9We use ?initiating? and ?reply? loosely: in our terminology,
the conversation ?A: ?Hi.? B: ?Eaten?? A: ?Nope.?? has two
exchanges, one initiated by A?s ?Hi?, the other by B?s ?Eaten??.
10Other asymmetric measures based on conditional prob-
ability of occurrence have been proposed for adaptation
within monologues (Church, 2000) and between conversations
(Stenchikova and Stent, 2007). Since our focus is different, we
control for different factors.
79
capture an important asymmetry. The case where
at = 1 but bt??a = 0 represents a true failure to ac-
commodate; but the case where at = 0 but bt??a = 1
should not, at least not to the same degree. For ex-
ample, a may be very short (e.g., ?What??) and thus
not contain an article, but we don?t assume that this
completely disallows B from using articles in their
reply. In other words, we are interested in whether
the presence of t acts as a trigger, not in whether
b??a exhibits t if and only if a does, the latter being
what correlation detects.11
It bears mentioning that since at and bt??a are
binary, a simple calculation shows that the covari-
ance12 cov(at, bt??a) = ConvA,B(t) ? P (a
t = 1).
But, the two terms on the right hand side are
not independent: raising P (at = 1) could cause
ConvA,B(t) to decrease by affecting the first term
in its definition, P (bt??a = 1|a
t = 1) (see eq. 1).
6 Experimental results
6.1 Convergence exists in fictional dialogs
For each ordered pair of characters (A,B) and for
each feature family t, we estimate equation (1) in a
straightforward manner: the fraction of B?s replies
to t-manifesting A utterances that themselves ex-
hibit t, minus the fraction of all replies of B to A
that exhibit t.13 Fig. 1 compares the average values
of these two fractions (as a way of putting conver-
gence values into context), showing positive differ-
ences for all of the considered families of features
(statistically significant, paired t-test p < 0.001); this
demonstrates that movie characters do indeed con-
verge to each other?s linguistic style on all consid-
ered trigger families.14
11One could also speculate that it is easier for B to (uncon-
sciously) pick up on the presence of t than on its absence.
12The covariance of two random variables is their correlation
times the product of their standard deviations.
13For each t, we discarded pairs of characters where some
relevant count is < 10, e.g., where B had fewer than 10 replies
manifesting the trigger.
14We obtained the same qualitative results when measuring
convergence via the correlation coefficient, doing so for the sake
of comparability with prior work (Niederhoffer and Pennebaker,
2002; Taylor and Thomas, 2008).
Ne
ga
tio
n
In
de
f. 
pr
on
.
Qu
an
tif
ie
r
Au
x.
 v
er
b
Ad
ve
rb
Pe
rs
. p
ro
n.
Co
nj
un
ct
io
n
Ar
tic
le
Pr
ep
os
iti
on
0.00
0.10
0.20
0.30
0.40
0.50
0.60
0.70
0.80
Figure 1: Implicit depiction of convergence for each trig-
ger family t, illustrated as the difference between the
means of P (bt??a = 1|a
t = 1) (right/light-blue bars) and
P (bt??a = 1) (left/dark-blue bars). (This implicit repre-
sentation allows one to see the magnitude of the two com-
ponents making up our definition of convergence.) The
trigger families are ordered by decreasing convergence.
All differences are statistically significant (paired t-test).
In all figures in this paper, error bars represent standard
error, estimated via bootstrap resampling (Koehn, 2004).
(Here, the error bars, in red, are very tight.)
Movies vs. Twitter One can ask how our results
on movie dialogs correspond to those for real-life
conversations. To study this, we utilize the results
of Danescu-Niculescu-Mizil et al (2011) on a large-
scale collection of Twitter exchanges as data on
real conversational exchanges. Figure 2 depicts the
comparison, revealing two interesting effects. First,
Twitter users coordinate more than movie characters
on all the trigger families we considered, which does
show that the convergence effect is stronger in actual
interchanges. On the other hand, from the perspec-
tive of potentially using imagined dialogs as prox-
ies for real ones, it is intriguing to see that there is
generally a correspondence between how much con-
vergence occurs in real dialogs for a given feature
family and how much convergence occurs for that
feature in imagined dialogs, although conjunctions
and articles show a bit less convergence in fictional
80
Ne
ga
tio
n
Ind
ef.
 pr
on
.
Qu
an
tifi
er
Co
nju
nc
tio
n
Ar
tic
le
Au
x. 
ve
rb
Ad
ve
rb
Pe
rs.
 pr
on
.
Pre
po
sit
ion
0.00
0.01
0.02
0.03
0.04
0.05
Co
nv
er
ge
nc
e
Twitter
Movies
Figure 2: Convergence in Twitter conversations (left bars)
vs. convergence in movie dialogs (right bars; corre-
sponds to the difference between the two respective bars
in Fig. 1) for each trigger family. The trigger families are
ordered by decreasing convergence in Twitter.
Ar
tic
le
Au
x. 
ve
rb
Ad
ve
rb
Co
nju
nc
tio
n
Ind
ef.
 pr
on
.
Ne
ga
tio
n
Pe
rs.
 pr
on
.
Pre
po
sit
ion
Qu
an
tifi
er0.01
0.00
0.01
0.02
0.03
0.04
0.05
Co
nv
er
ge
nc
e
Adjacent
Non-adjacent
Figure 3: Immediate vs. within-conversation effects
(for conversations with at least 5 utterances). Sup-
pose that we have a conversation a1 b2 a3 b4 a5 . . .. The
lefthand/dark-green bars show the usual convergence
measure, which involves the utterance pair a1 and b2. The
righthand/mustard-green bars show convergence based
on pairs like a1 and b4 ? utterances in the same con-
versation, but not adjacent. We see that there is a much
stronger triggering effect for immediately adjacent utter-
ances.
exchanges than this pattern would suggest.
6.2 Potential alternative explanations
Immediate vs. within-conversation effects An
additional natural question is, how much are these
accommodation effects due to an immediate trigger-
ing effect, as opposed to simply being a by-product
of utterances occurring within the same conversa-
tion? For instance, could the results be due just to
the topic of the conversation?
To answer this question requires measuring ?con-
vergence? between utterances that are not adjacent,
but are still in the same conversation. To this end,
we first restricted attention to those conversations
in which there were at least five utterances, so that
they would have the structure a1 b2 a3 b4 a5.... We
then measure convergence not between adjacent ut-
terances, like a1 and b2, but where we skip an utter-
ance, such as the pair a1, b4 or b2, a5. This helps
control for topic effects, since b4 and a1 are still
close and thus fairly likely to be on the same sub-
ject.15
Figure 3 shows that the level of convergence al-
ways falls off after the skipped utterance, sometimes
dramatically so, thus demonstrating that the level
of immediate adaptation effects we see cannot be
solely explained by the topic of conversation or other
conversation-level effects. These results accord with
the findings of Levelt and Kelter (1982), where in-
terposing ?interfering? questions lowered the chance
of a question?s preposition being echoed by the re-
spondent, and Reitter et al (2006), where the effects
of structural priming were shown to decay quickly
with the distance between the priming trigger and
the priming target.
Towards the same end, we also performed ran-
domization experiments in which we shuffled the or-
der of each participant?s utterances in each conversa-
tion, while maintaining alternation between speak-
ers. We again observed drop-offs in this randomized
condition in comparison to immediate convergence,
the main focus of this paper.
Self-coordination Could our results be explained
entirely by the script author converging to their own
self, given that self-alignment has been documented
15It is true that they might be on different topics, but in fact
even b2 might be on a different subject from a1.
81
(Pickering and Garrod, 2004; Reitter et al, 2006)?
If that were the case, then the characters that the au-
thor is writing about should converge to themselves
no more than they converge to different characters.
But we ran experiments showing that this is not the
case, thus invalidating this alternative hypothesis. In
fact, characters converge to themselves much more
than they converge to other characters.
6.3 Convergence and imagined relation
We now analyze how convergence patterns vary with
the type of relationship between the (imagined) par-
ticipants. Note that, given the multimodal charac-
ter of convergence, treating each trigger family sep-
arately is the most appropriate way to proceed, since
in past work, for the same experimental factor (e.g.,
gender), different features converge differently (re-
fer back to ?4). For clarity of exposition, we discuss
in detail only the results for the Articles feature fam-
ily; but the results for all trigger families are sum-
marized in Fig. 7, discussed later.
Imagined gender Fig. 4(a) shows how conver-
gence on article usage depends on the gender of the
initiator and respondent. Females are more influ-
ential than males: movie characters of either gen-
der accommodate more to female characters than to
male characters (compare the Female initiator bar
with the Male initiator bar, statistically significant,
independent t-test, p < 0.05). Also, female char-
acters seem to accommodate slightly more to other
characters than male characters do (though not sta-
tistically significantly so in our data).
We also compare the amount of convergence be-
tween all the possible types of gendered initiator-
respondent pairs involved (Fig. 4(b)). One can ob-
serve, for example, that male characters adapt less in
same-gender situations (Male-Male conversations)
than in mixed-gender situations (Female initiator-
Male respondent), while the opposite is true for fe-
male characters (Female-Female vs. Male-Female).
Interpreting these results lies beyond the scope
of this paper. We note that these results could be
a correlate of many factors, such as the roles that
male and female characters are typically assigned in
movie scripts.16
16A comparison to previously reported results on real-life
gender effects is not straightforward, since they pertain to differ-
Narrative importance Does the relative impor-
tance bestowed by the scriptwriter to the characters
affect the amount of linguistic coordination he or she
(nonconsciously) embeds in their dialogs? Fig. 5
shows that, on average, the lead character converges
to the second-billed character more than vice-versa
(compare left bar in 1st resp. group with left bar in
2nd resp. group).
One possible confounding factor is that there is
significant gender imbalance in the data (82% of all
lead characters are males, versus only 51% of the
secondary characters). Could the observed differ-
ence be a direct consequence of the relation between
gender and convergence discussed above? The an-
swer is no: the same qualitative observation holds if
we restrict our analysis to same-gender pairs (com-
pare the righthand bars in each group in Fig. 517).
It would be interesting to see whether these re-
sults could be brought to bear on previous results
regarding the relationship between social status and
convergence, but such interpretation lies beyond the
scope of this paper, since the connection between
billing order and social status is not straightforward.
Quarreling The level of contention in conversa-
tions has also been shown to be related to the amount
of convergence (Giles, 2008; Niederhoffer and Pen-
nebaker, 2002; Taylor and Thomas, 2008). To test
whether this tendency holds in the case of imagined
conversations, as a small pilot study, we manually
classified the conversations between 24 main pairs
of characters from romantic comedies18 as: quarrel-
ing, some quarreling and no quarreling. Although
the experiment was too small in scale to provide
statistical significance, the results (Fig. 6) suggest
that indeed the level of convergence is affected by
ent features; Ireland and Pennebaker (2010) show that females
match their linguistic style more than males, where style match-
ing is averaged over the same 9 trigger families we employ (they
do not report gender effect for each family separately).
17Figure 5 also shows that our convergence measure does
achieve negative values in practice, indicating divergence. Di-
vergence is a rather common phenomenon which deserves at-
tention in future work; see Danescu-Niculescu-Mizil et al
(2011) for an account.
18We chose the romantic comedy genre since it is often char-
acterized by some level of contention between the two people
in the main couple.
82
All
F i
nit
.
M 
ini
t.
F r
es
p.
M 
re
sp
.0.00
0.01
0.02
0.03
0.04
Co
nv
er
ge
nc
e
article
(a)
    
   A
ll
M-
M F-M M-
F F-F
0.00
0.01
0.02
0.03
0.04
Co
nv
er
ge
nc
e
article
(b)
Figure 4: Relation between Article convergence and imagined gender. (a) compares cases when the initiator and
respondent are Male or Female; (b) compares types of gendered initiator-respondent relations: Male-Male, Female-
Male, Male-Female, Female-Female. For comparison, the All bars represents the general Article convergence (illus-
trated in Fig. 1 as the difference between the two respective bars).
Ind
ef. 
pro
n.
Art
icle
Co
nju
nct
ion
Pre
pos
itio
n
Ad
ver
b
Per
s. p
ron
.
Au
x. v
erb
Ne
gat
ion
Qu
ant
ifie
r0.010
0.005
0.000
0.005
0.010
0.015
(a) F resp. minus M resp.
Art
icle
Pre
pos
itio
n
Ind
ef. 
pro
n.
Co
nju
nct
ion
Per
s. p
ron
.
Au
x. v
erb
Ne
gat
ion
Ad
ver
b
Qu
ant
ifie
r0.010
0.005
0.000
0.005
0.010
0.015
(b) F init. minus M init.
Art
icle
Ne
gat
ion
Qu
ant
ifie
r
Ad
ver
b
Per
s. p
ron
.
Ind
ef. 
pro
n.
Pre
pos
itio
n
Au
x. v
erb
Co
nju
nct
ion
0.010
0.005
0.000
0.005
0.010
0.015
(c) 1st resp. minus 2nd resp.
Per
s. p
ron
.
Art
icle
Ind
ef.
 pr
on.
Ne
gat
ion
Co
nju
nct
ion
Qu
ant
ifie
r
Au
x. v
erb
Pre
pos
itio
n
Ad
ver
b0.06
0.02
0.02
0.06
(d) Quarrel minus No quarrel
Figure 7: Summary of the relation between convergence and imagined gender (a and b), billing order (c), and quarrel-
ing (d). The bars represent the difference between the convergence observed in the respective cases; e.g., the Article
(red) bar in (a) represents the difference between the F resp. and the M resp. bars in Fig. 4(a). In each plot, the
trigger families are sorted according to the respective difference, but the color assigned to each family is consistent
across plots. The scale of (d) differs from the others.
the presence of controversy: quarreling exhibited
considerably more convergence for articles than the
other categories (the same holds for personal and in-
definite pronouns; see Fig. 7). Interestingly, the
reverse is true for adverbs; there, we observe di-
vergence for contentious conversations and conver-
gence for non-contentious conversations (detailed
plot omitted due to space constraints). This corre-
sponds to Niederhoffer and Pennebaker?s (2002) ob-
servations made on real conversations in their study
of the Watergate transcripts: when the relationship
between the two deteriorated, Richard Nixon con-
verged more to John Dean on articles, but diverged
on other features.19
Results for the other features Our results above
suggest some intriguing interplay between conver-
gence and gender, status, and level of hostility in
imagined dialogs, which may shed light on how
people (scriptwriters) nonconsciously expect con-
19Adverbs were not included in their study.
83
All
1s
t r
es
p.
2n
d r
es
p.0.02
0.01
0.00
0.01
0.02
0.03
Co
nv
er
ge
nc
e
article
Figure 5: Comparison of the convergence of first-billed
(lead) characters to second-billed characters (left bar in
1st resp. group) to that of second-billed characters to
leads (left bar in 2nd resp. group); righthand bars (dark
green) in each group show results for Male-Male pairs
only.
vergence to relate to such factors. (Interpreting these
sometimes apparently counterintuitive findings is
beyond the scope of this paper, but represents a fas-
cinating direction for future work.) Fig. 7 shows
how the nature of these relations depends on the trig-
ger family considered. The variation among families
is in line with the previous empirical results on the
multimodality of convergence in real conversations,
as discussed in ?4.
7 Summary and future work
We provide some insight into the causal mecha-
nism behind convergence, a topic that has gener-
ated substantial scrutiny and debate for over 40 years
(Ireland et al, 2011; Branigan et al, 2010). Our
work, along with Elson and McKeown (2010), ad-
vocates for the value of fictional sources in the study
of linguistic and social phenomena. To stimulate
such studies, we render our metadata-rich corpus of
movie dialog public.
In ?1, we described some practical applications
of a better understanding of the chameleon effect in
language; it boils down to improving communica-
tion both between humans and between humans and
computers. Also, our results on contention could
Ro
m.
 co
m.
No
 qu
arr
el
Sm
. q
ua
rre
l
Qu
arr
el0.01
0.00
0.01
0.02
0.03
0.04
0.05
0.06
Co
nv
erg
en
ce
article
Figure 6: Relation between contention and convergence.
The third bar combines quarreling and some quarreling
to ameliorate data sparsity. For comparison, Rom. com.
shows convergence calculated on all the conversations of
the 24 romantic-comedy pairs considered in this experi-
ment.
be used to further automatic controversy detection
(Mishne and Glance, 2006; G?mez et al, 2008).
Moreover, if we succeeded in linking our results
on narrative importance to relative social status, we
might further the development of systems that can
infer social relationships in online social networks
when conversational data is present but other, more
explicit cues are absent (Wyatt et al, 2008; Bram-
sen et al, 2011). Such systems could be valuable to
the rapidly expanding field of analyzing social net-
works.
Acknowledgments Many thanks for their help are
due to Claire Cardie, Catalin Draghici, Susan Du-
mais, Shimon Edelman, Michael Gamon, Jon Klein-
berg, Magdalena Naroz?niak, Alex Niculescu-Mizil,
Myle Ott, Bo Pang, Morgan Sonderegger, plus NLP
seminar participants Eric Baumer, Bin Lu, Chenhao
Tan, Lu Wang, Bishan Yang, Ainur Yessenalina, and
Marseille, and the anonymous reviewers (who went
far beyond the call of duty!). Supported by NSF IIS-
0910664, the Yahoo! FREP program, and a Yahoo!
Key Scientific Challenges award.
84
References
Shlomo Argamon, Sushant Dhawle, and Moshe Koppel.
2005. Lexical predictors of personality type. Proceed-
ings of the 2005 Joint Annual Meeting of the Interface
and the Classification Society of North America.
Frances Bilous and Robert Krauss. 1988. Dominance
and accommodation in the conversational behavior of
same- and mixed-gender dyads. Language and Com-
munication, 8:183?194.
Lukas Bleichenbacher. 2008. Multilingualism in the
movies: Hollywood characters and their language
choices. francke verlag, Jan.
J. Kathryn Bock. 1986. Syntactic persistence in lan-
guage production. Cognitive Psychology, 18(3):355
? 387.
Heather Bortfeld and Susan E. Brennan. 1997. Use and
acquisition of idiomatic expressions in referring by na-
tive and non-native speakers. Discourse Processes,
23(2):119?147.
Richard Y. Bourhis. 1991. Organizational communi-
cation and accommodation: Toward some conceptual
and empirical links. In Howard Giles, Justine Coup-
land, and Nikolas Coupland, editors, Contexts of Ac-
commodation. Cambridge University Press.
James J. Bradac, Anthony Mulac, and Ann House. 1988.
Lexical diversity and magnitude of convergent ver-
sus divergent style shifting: Perceptual and evaluative
consequences. Language and Communication, 8:213?
228, Nov.
Philip Bramsen, Martha Escobar-Molana, Ami Patel, and
Rafael Alonso. 2011. Extracting social power rela-
tionships from natural language. In Proceedings of
ACL HLT.
Holly P. Branigan, Martin J. Pickering, Jamie Pearson,
and Janet F. McLean. 2010. Linguistic alignment be-
tween people and computers. Journal of Pragmatics,
42(9):2355?2368.
Tanya L. Chartrand and John A. Bargh. 1999. The
chameleon effect: The perception-behavior link and
social interaction. J. Personality and Social Psychol-
ogy, 76(6):893?910.
Tanya L. Chartrand and Rick van Baaren. 2009. Chap-
ter 5: Human mimicry. In Mark P. Zanna, editor, Ad-
vances in Experimental Social Psychology, volume 41,
pp. 219?274. Academic Press.
Kenneth W. Church. 2000. Empirical estimates of adap-
tation: the chance of two noriegas is closer to p/2 than
p2. In Proceedings of COLING, pp. 180?186.
Herbert H. Clark. 1996. Using language. Cambridge
University Press, second edition.
Cristian Danescu-Niculescu-Mizil, Michael Gamon, and
Susan Dumais. 2011. Mark my words! Linguistic
style accommodation in social media. In Proceedings
of WWW.
David Elson and Kathleen McKeown. 2010. Automatic
attribution of quoted speech in literary narrative. In
Proceedings of AAAI.
David Elson, Nicholas Dames, and Kathleen McKeown.
2010. Extracting social networks from literary fiction.
In Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics, pp. 138?147.
Kathleen Ferrara. 1991. Accommodation in therapy. In
Accommodation theory: Communication, context, and
consequences. Cambridge University Press.
Howard Giles, Justine Coupland, and Nikolas Coup-
land. 1991. Accommodation theory: Communica-
tion, context, and consequences. In Accommodation
theory: Communication, context, and consequences.
Cambridge University Press.
Howard Giles, Michael Willemyns, Cynthia Gallois, and
Michelle Anderson. 2007. Accommodating a new
frontier: The context of law enforcement. In Klaus
Fiedler, editor, Social Communication, Frontiers of
Social Psychology, chapter 5, pp. 129?162.
Howard Giles. 2008. Communication accommodation
theory. In Engaging theories in interpersonal commu-
nication: Multiple perspectives. Sage Publications.
Vicen? G?mez, Andreas Kaltenbrunner, and Vicente
L?pez. 2008. Statistical analysis of the social network
and discussion threads in Slashdot. In Proceedings of
WWW, pp. 645?654.
Amy L. Gonzales, Jeffrey T. Hancock, and James W. Pen-
nebaker. 2010. Language style matching as a predic-
tor of social dynamics in small groups. Communica-
tion Research, 37(1):3?19, Feb.
Stanford W. Gregory and Stephen Webster. 1996. A
nonverbal signal in voices of interview partners effec-
tively predicts communication accommodation and so-
cial status perceptions. J. Personality and Social Psy-
chology, 70(6):1231?1240.
Heidi Hamilton. 1991. Accommodation and mental
disability. In Accommodation theory: Communica-
tion, context, and consequences. Cambridge Univer-
sity Press.
Jeffrey T. Hancock, Lauren E. Curry, Saurabh Goorha,
and Michael Woodworth. 2008. On lying and be-
ing lied to: A linguistic analysis of deception in
computer-mediated communication. Discourse Pro-
cesses, 45(1):1?23.
Susan C. Herring and John C. Paolillo. 2006. Gender
and genre variation in weblogs. Journal of Sociolin-
guistics, Jan.
Walter Hill. 1972. The Getaway. Directed by Sam Peck-
inpah.
85
Molly E. Ireland and James W. Pennebaker. 2010. Lan-
guage style matching in writing: Synchrony in essays,
correspondence, and poetry. J. Personality and Social
Psychology, 99(3):549?571.
Molly E. Ireland, Richard B. Slatcher, Paul W. Eastwick,
Lauren E. Scissors, Eli J. Finkel, and James W. Pen-
nebaker. 2011. Language style matching predicts re-
lationship initiation and stability. Psychological Sci-
ence, 22:39?44.
Patrick Juola. 2008. Authorship Attribution. Now Pub-
lishers.
Dan Jurafsky, Rajesh Ranganath, and Dan McFarland.
2009. Extracting social meaning: Identifying inter-
actional style in spoken conversation. In Proceedings
of the NAACL, pp. 638?646.
Philipp Koehn. 2004. Statistical significance tests
for machine translation evaluation. Proceedings of
EMNLP, pp. 388?395.
Moshe Koppel, Shlomo Argamon, and Anat Shimoni.
2002. Automatically categorizing written texts by au-
thor gender. Literary and Linguistic Computing.
Sarah Kozloff. 2000. Overhearing Film Dialogue. Uni-
versity of California Press.
Jessica L. Lakin, Valerie E. Jefferis, Clara Michelle
Cheng, and Tanya L. Chartrand. 2003. The chameleon
effect as social glue: Evidence for the evolutionary sig-
nificance of nonconscious mimicry. Journal of Non-
verbal Behavior, 27:145?162.
Willem J.M. Levelt and Stephanie Kelter. 1982. Surface
form and memory in question answering. Cognitive
Psychology, 14(1):78?106.
Fran?ois Mairesse, Marilyn A. Walker, Matthias R. Mehl,
and Roger K. Moore. 2007. Using linguistic cues for
the automatic recognition of personality in conversa-
tion and text. JAIR, pp. 457?500.
Petr Mare?. 2000. Fikce, konvence a realita: K v?ce-
jazyc?nosti v ume?leck?ch textech [fiction, convention,
and reality: On multilingualism in literary texts].
Slovo a slovesnost, 61(1):47?53.
Robert McKee. 1999. Story: Substance, Structure, Style,
and the Principles of Screenwriting. Methuen.
Rada Mihalcea and Carlo Strapparava. 2009. The lie
detector: Explorations in the automatic recognition
of deceptive language. In Proceedings of the ACL-
IJCNLP 2009 Conference Short Papers, pp. 309?312.
Gilad Mishne and Natalie Glance. 2006. Leave a reply:
An analysis of weblog comments. Third annual work-
shop on the Weblogging ecosystem.
Frederick Mosteller and David L. Wallace. 1984. Ap-
plied Bayesian and Classical Inference: The Case of
the Federalist Papers. Springer-Verlag.
Arjun Mukherjee and Bing Liu. 2010. Improving gender
classification of blog authors. EMNLP.
Randall Munroe. 2010. http://xkcd.com/813/.
Laura L. Namy, Lynne C. Nygaard, and Denise Sauerteig.
2002. Gender differences in vocal accommodation. J.
Language and Social Psychology, 21(4):422?432.
Clifford Nass and Kwan Min Lee. 2000. Does computer-
generated speech manifest personality? An experi-
mental test of similarity-attraction. In Proceedings of
CHI, pp. 329?336.
Michael Natale. 1975. Convergence of mean vocal in-
tensity in dyadic communication as a function of so-
cial desirability. J. Personality and Social Psychol-
ogy, 32(5):790?804.
Kate G. Niederhoffer and James W. Pennebaker. 2002.
Linguistic style matching in social interaction. J. Lan-
guage and Social Psychology.
Jon Oberlander and Alastair J. Gill. 2006. Language
with character: A stratified corpus comparison of in-
dividual differences in e-mail communication. Dis-
course Processes, 42(3):239?270.
James W. Pennebaker, Roger J. Booth, and Martha E.
Francis. 2007. Linguistic inquiry and word
count (LIWC): A computerized text analysis program.
http://www.liwc.net/.
Cyma Van Petten and Marta Kutas. 1991. Influences of
semantic and syntactic context on open- and closed-
class words. Memory and Cognition, 19(1):95?112.
Martin Pickering and Simon Garrod. 2004. Toward a
mechanistic psychology of dialogue. Behavioral and
Brain Sciences, 27(02):169?190.
Rajesh Ranganath, Dan Jurafsky, and Dan McFarland.
2009. It?s not you, it?s me: Detecting flirting and
its misperception in speed-dates. In Proceedings of
EMNLP, pp. 334?342.
Paul Rayson, Andrew Wilson, and Geoffrey Leech.
2001. Grammatical word class variation within the
British National Corpus Sampler. Language and Com-
puters, 36:295?306(12).
David Reitter, Johanna D. Moore, and Frank Keller.
2006. Priming of syntactic rules in task-oriented di-
alogue and spontaneous conversation. In Proceedings
of the Conference of the Cognitive Science Society.
David Reitter, Frank Keller, and Johanna D. Moore.
2011. A Computational Cognitive Model of Syntac-
tic Priming. Cognitive Science.
Sidney J. Segalowitz and Korri Lane. 2004. Perceptual
fluency and lexical access for function versus content
words. Behavioral and Brain Sciences, 27(02):307?
308.
Morgan Sonderegger. 2010. Applications of graph the-
ory to an English rhyming corpus. Computer Speech
& Language, In Press, Corrected Proof.
Svetlana Stenchikova and Amanda Stent. 2007. Mea-
suring adaptation between dialogs. In Proc. of the 8th
SIGdial Workshop on Discourse and Dialogue.
86
Richard L. Street and Howard Giles. 1982. Speech ac-
commodation theory. In Social cognition and commu-
nication. Sage Publications.
Devi Stuart-Fox and Adnan Moussalli. 2008. Selection
for social signalling drives the evolution of chameleon
colour change. PLoS Biol, 6(1):e25, 01.
Paul J. Taylor and Sally Thomas. 2008. Linguistic style
matching and negotiation outcome. Negotiation and
Conflict Management Research, 1(3):263?281.
Jitendra N. Thakerar, Howard Giles, and Jenny Cheshire.
1982. Psychological and linguistic parameters of
speech accommodation theory. In C. Fraser and K.R.
Scherer, editors, Advances in the Social Psychology of
Language. Cambridge.
Rick B. van Baaren, Rob W. Holland, Bregje Steenaert,
and Ad van Knippenberg. 2003. Mimicry for money:
Behavioral consequences of imitation. Journal of Ex-
perimental Social Psychology, 39(4):393?398.
Arthur Ward and Diane Litman. 2007. Dialog conver-
gence and learning. In Artificial Intelligence in Edu-
cation (AIED), pp. 262?269.
Danny Wyatt, Jeff Bilmes, Tanzeem Choudhury, and
James A. Kitts. 2008. Towards the automated so-
cial analysis of situated speech data. In Proceedings
of Ubicomp, pp. 168?171.
Patrick Ye and Timothy Baldwin. 2006. Verb sense
disambiguation using selectional preferences extracted
with a state-of-the-art semantic role labeler. In Pro-
ceedings of the Australasian Language Technology
Workshop 2006, pp. 139?148.
87
Proceedings of the ACL-2012 Workshop on Extra-Propositional Aspects of Meaning in Computational Linguistics (ExProM-2012),
pages 70?79, Jeju, Republic of Korea, 13 July 2012. c?2012 Association for Computational Linguistics
Hedge Detection as a Lens on Framing in the GMO Debates:
A Position Paper
Eunsol Choi?, Chenhao Tan?, Lillian Lee?, Cristian Danescu-Niculescu-Mizil? and Jennifer Spindel?
?Department of Computer Science, ?Department of Plant Breeding and Genetics
Cornell University
ec472@cornell.edu, chenhao|llee|cristian@cs.cornell.edu, jes462@cornell.edu
Abstract
Understanding the ways in which participants
in public discussions frame their arguments is
important in understanding how public opin-
ion is formed. In this paper, we adopt the po-
sition that it is time for more computationally-
oriented research on problems involving fram-
ing. In the interests of furthering that goal,
we propose the following specific, interesting
and, we believe, relatively accessible ques-
tion: In the controversy regarding the use
of genetically-modified organisms (GMOs) in
agriculture, do pro- and anti-GMO articles dif-
fer in whether they choose to adopt a more
?scientific? tone?
Prior work on the rhetoric and sociology of
science suggests that hedging may distin-
guish popular-science text from text written
by professional scientists for their colleagues.
We propose a detailed approach to studying
whether hedge detection can be used to un-
derstanding scientific framing in the GMO de-
bates, and provide corpora to facilitate this
study. Some of our preliminary analyses sug-
gest that hedges occur less frequently in scien-
tific discourse than in popular text, a finding
that contradicts prior assertions in the litera-
ture. We hope that our initial work and data
will encourage others to pursue this promising
line of inquiry.
1 Introduction
1.1 Framing, ?scientific discourse?, and GMOs
in the media
The issue of framing (Goffman, 1974; Scheufele,
1999; Benford and Snow, 2000) is of great im-
portance in understanding how public opinion is
formed. In their Annual Review of Political Science
survey, Chong and Druckman (2007) describe fram-
ing effects as occurring ?when (often small) changes
in the presentation of an issue or an event produce
(sometimes large) changes of opinion? (p. 104);
as an example, they cite a study wherein respon-
dents answered differently, when asked whether a
hate group should be allowed to hold a rally, depend-
ing on whether the question was phrased as one of
?free speech? or one of ?risk of violence?.
The genesis of our work is in a framing question
motivated by a relatively current political issue. In
media coverage of transgenic crops and the use of
genetically modified organisms (GMOs) in food, do
pro-GMO vs. anti-GMO articles differ not just with
respect to word choice, but in adopting a more ?sci-
entific? discourse, meaning the inclusion of more
uncertainty and fewer emotionally-laden words? We
view this as an interesting question from a text anal-
ysis perspective (with potential applications and im-
plications that lie outside the scope of this article).
1.2 Hedging as a sign of scientific discourse
To obtain a computationally manageable character-
ization of ?scientific discourse?, we turned to stud-
ies of the culture and language of science, a body
of work spanning fields ranging from sociology to
applied linguistics to rhetoric and communication
(Gilbert and Mulkay, 1984; Latour, 1987; Latour
and Woolgar, 1979; Halliday and Martin, 1993; Baz-
erman, 1988; Fahnestock, 2004; Gross, 1990).
One characteristic that has drawn quite a bit of
attention in such studies is hedging (Myers, 1989;
70
Hyland, 1998; Lewin, 1998; Salager-Meyer, 2011).1
Hyland (1998, pg. 1) defines hedging as the ex-
pression of ?tentativeness and possibility? in com-
munication, or, to put it another way, language cor-
responding to ?the writer withholding full commit-
ment to statements? (pg. 3). He supplies many
real-life examples from scientific research articles,
including the following:
1. ?It seems that this group plays a critical role in
orienting the carboxyl function? (emphasis Hy-
land?s)
2. ?...implies that phytochrome A is also not nec-
essary for normal photomorphogenesis, at least
under these irradiation conditions? (emphasis
Hyland?s)
3. ?We wish to suggest a structure for the salt of
deoxyribose nucleic acid (D.N.A.)? (emphasis
added)2
Several scholars have asserted the centrality of hedg-
ing in scientific and academic discourse, which cor-
responds nicely to the notion of ?more uncertainty?
mentioned above. Hyland (1998, p. 6) writes, ?De-
spite a widely held belief that professional scientific
writing is a series of impersonal statements of fact
which add up to the truth, hedges are abundant in
science and play a critical role in academic writing?.
Indeed, Myers (1989, p. 13) claims that in scien-
tific research articles, ?The hedging of claims is so
common that a sentence that looks like a claim but
has no hedging is probably not a statement of new
knowledge?.3
Not only is understanding hedges important to un-
derstanding the rhetoric and sociology of science,
but hedge detection and analysis ? in the sense of
identifying uncertain or uncertainly-sourced infor-
mation (Farkas et al, 2010) ? has important appli-
cations to information extraction, broadly construed,
and has thus become an active sub-area of natural-
language processing. For example, the CoNLL 2010
1In linguistics, hedging has been studied since the 1970s
(Lakoff, 1973).
2This example originates from Watson and Crick?s land-
mark 1953 paper. Although the sentence is overtly tentative,
did Watson and Crick truly intend to be polite and modest in
their claims? See Varttala (2001) for a review of arguments re-
garding this question.
3Note the inclusion of the hedge ?probably?.
Shared Task was devoted to this problem (Farkas
et al, 2010).
Putting these two lines of research together, we
see before us what appears to be an interesting in-
terdisciplinary and, at least in principle, straightfor-
ward research program: relying on the aforemen-
tioned rhetoric analyses to presume that hedging is
a key characteristic of scientific discourse, build a
hedge-detection system to computationally ascertain
which proponents in the GMO debate tend to use
more hedges and thus, by presumption, tend to adopt
a more ?scientific? frame.4
1.3 Contributions
Our overarching goal in this paper is to convince
more researchers in NLP and computational linguis-
tics to work on problems involving framing. We
try to do so by proposing a specific problem that
may be relatively accessible. Despite the apparent
difficulty in addressing such questions, we believe
that progress can be made by drawing on observa-
tions drawn from previous literature across many
fields, and integrating such work with movements
in the computational community toward considera-
tion of extra-propositional and pragmatic concerns.
We have thus intentionally tried to ?cover a lot of
ground?, as one referee put it, in the introductory
material just discussed.
Since framing problems are indeed difficult, we
elected to narrow our scope in the hope of making
some partial progress. Our technical goal here, at
this workshop, where hedge detection is one of the
most relevant topics to the broad questions we have
raised, is not to learn to classify texts as being pro-
vs. anti-GMO, or as being scientific or not, per se.5
Our focus is on whether hedging specifically, con-
sidered as a single feature, is correlated with these
different document classes, because of the previous
research attention that has been devoted to hedging
in particular and because of hedging being one of the
topics of this workshop. The point of this paper is
4However, this presumption that more hedges characterize a
more scientific discourse has been contested. See section 2 for
discussion and section 4.2 for our empirical investigation.
5Several other groups have addressed the problem of try-
ing to identify different sides or perspectives (Lin et al, 2006;
Hardisty et al, 2010; Beigman Klebanov et al, 2010; Ahmed
and Xing, 2010).
71
thus not to compare the efficacy of hedging features
with other types, such as bag-of-words features. Of
course, to do so is an important and interesting di-
rection for future work.
In the end, we were not able to achieve satisfac-
tory results even with respect to our narrowed goal.
However, we believe that other researchers may be
able to follow the plan of attack we outline below,
and perhaps use the data we are releasing, in order
to achieve our goal. We would welcome hearing the
results of other people?s efforts.
2 How should we test whether hedging
distinguishes scientific text?
One very important point that we have not yet ad-
dressed is: While the literature agrees on the impor-
tance of hedging in scientific text, the relative de-
gree of hedging in scientific vs. non-scientific text is
a matter of debate.
On the one side, we have assertions like those of
Fahnestock (1986), who shows in a clever, albeit
small-scale, study involving parallel texts that when
scientific observations pass into popular accounts,
changes include ?removing hedges ... thus con-
ferring greater certainty on the reported facts? (pg.
275). Similarly, Juanillo, Jr. (2001) refers to a shift
from a forensic style to a ?celebratory? style when
scientific research becomes publicized, and credits
Brown (1998) with noting that ?celebratory scien-
tific discourses tend to pay less attention to caveats,
contradictory evidence, and qualifications that are
highlighted in forensic or empiricist discourses. By
downplaying scientific uncertainty, it [sic] alludes to
greater certainty of scientific results for public con-
sumption? (Juanillo, Jr., 2001, p. 42).
However, others have contested claims that the
popularization process involves simplification, dis-
tortion, hype, and dumbing down, as Myers (2003)
colorfully puts it; he provides a critique of the rel-
evant literature. Varttala (1999) ran a corpus anal-
ysis in which hedging was found not just in pro-
fessional medical articles, but was also ?typical of
popular scientific articles dealing with similar top-
ics? (p. 195). Moreover, significant variation in use
of hedging has been found across disciplines and au-
thors? native language; see Salager-Meyer (2011) or
Varttala (2001) for a review.
To the best of our knowledge, there have been no
large-scale empirical studies validating the hypoth-
esis that hedges appear more or less frequently in
scientific discourse.
Proposed procedure Given the above, our first
step must be to determine whether hedges are more
or less prominent in ?professional scientific? (hence-
forth ?prof-science??) vs. ?public science? (hence-
forth ?pop-science?) discussions of GMOs. Of
course, for a large-scale study, finding hedges re-
quires developing and training an effective hedge de-
tection algorithm.
If the first step shows that hedges can indeed be
used to effectively distinguish prof-science vs. pop-
science discourse on GMOs, then the second step is
to examine whether the use of hedging in pro-GMO
articles follows our inferred ?scientific? occurrence
patterns to a greater extent than the hedging in anti-
GMO articles.
However, as our hedge classifier trained on the
CoNLL dataset did not perform reliably on the dif-
ferent domain of prof-science vs. pop-science dis-
cussions of GMOs, we focus the main content of this
paper on the first step. We describe data collection
for the second step in the appendix.
3 Data
To accomplish the first step of our proposed pro-
cedure outlined above, we first constructed a prof-
science/pop-science corpus by pulling text from
Web of Science for prof-science examples and from
LexisNexis for pop-science examples, as described
in Section 3.1. Our corpus will be posted online
at https://confluence.cornell.edu/display/llresearch/
HedgingFramingGMOs.
As noted above, computing the degree of hedg-
ing in the aforementioned corpus requires access to
a hedge-detection algorithm. We took a supervised
approach, taking advantage of the availability of the
CoNLL 2010 hedge-detection training and evalua-
tion corpora, described in Section 3.2
3.1 Prof-science/pop-science data: LEXIS and
WOS
As mentioned previously, a corpus of prof-science
and pop-science articles is required to ascertain
whether hedges are more prevalent in one or the
72
Dataset Doc type # docs # sentences Avg sentence length Flesch reading ease
Prof-science/pop-science corpus
WOS abstracts 648 5596 22.35 23.39
LEXIS (short) articles 928 36795 24.92 45.78
Hedge-detection corpora
Bio (train) abstracts, articles 1273, 9 14541 (18% uncertain) 29.97 20.77
Bio (eval) articles 15 5003 (16% uncertain) 31.30 30.49
Wiki (train) paragraphs 2186 11111 (22% uncertain) 23.07 35.23
Wiki (eval) paragraphs 2346 9634 (23% uncertain) 20.82 31.71
Table 1: Basic descriptive statistics for the main corpora we worked with. We created the first two. Higher Flesch
scores indicate text that is easier to read.
other of these two writing styles. Since our ultimate
goal is to look at discourse related to GMOs, we re-
strict our attention to documents on this topic.
Thomson Reuter?s Web of Science (WOS), a
database of scientific journal and conference arti-
cles, was used as a source of prof-science samples.
We chose to collect abstracts, rather than full scien-
tific articles, because intuition suggests that the lan-
guage in abstracts is more high-level than that in the
bodies of papers, and thus more similar to the lan-
guage one would see in a public debate on GMOs.
To select for on-topic abstracts, we used the phrase
?transgenic foods? as a search keyword and dis-
carded results containing any of a hand-selected list
of off-topic filtering terms (e.g., ?mice? or ?rats?).
We then made use of domain expertise to manually
remove off-topic texts. The process yielded 648 doc-
uments for a total of 5596 sentences.
Our source of pop-science articles was Lexis-
Nexis (LEXIS). On-topic documents were collected
from US newspapers using the search keywords ?ge-
netically modified foods? or ?transgenic crops? and
then imposing the additional requirement that at
least two terms on a hand-selected list7 be present
in each document. After the removal of duplicates
and texts containing more than 2000 words to delete
excessively long articles, our final pop-science sub-
corpus was composed of 928 documents.
7The term list: GMO, GM, GE, genetically modified, ge-
netic modification, modified, modification, genetic engineer-
ing, engineered, bioengineered, franken, transgenic, spliced,
G.M.O., tweaked, manipulated, engineering, pharming, aqua-
culture.
3.2 CoNLL hedge-detection training data 8
As described in Farkas et al (2010), the motivation
behind the CoNLL 2010 shared task is that ?distin-
guishing factual and uncertain information in texts is
of essential importance in information extraction?.
As ?uncertainty detection is extremely important for
biomedical information extraction?, one component
of the dataset is biological abstracts and full arti-
cles from the BioScope corpus (Bio). Meanwhile,
the chief editors of Wikipedia have drawn the at-
tention of the public to specific markers of uncer-
tainty known as weasel words9: they are words or
phrases ?aimed at creating an impression that some-
thing specific and meaningful has been said?, when,
in fact, ?only a vague or ambiguous claim, or even
a refutation, has been communicated?. An example
is ?It has been claimed that ...?: the claimant has not
been identified, so the source of the claim cannot be
verified. Thus, another part of the dataset is a set
of Wikipedia articles (Wiki) annotated with weasel-
word information. We view the combined Bio+Wiki
corpus (henceforth the CoNLL dataset) as valuable
for developing hedge detectors, and we attempt to
study whether classifiers trained on this data can be
generalized to other datasets.
3.3 Comparison
Table 1 gives the basic statistics on the main datasets
we worked with. Though WOS and LEXIS differ in
the total number of sentences, the average sentence
length is similar. The average sentence length in Bio
is longer than that in Wiki. The articles in WOS
are markedly more difficult to read than the articles
8http://www.inf.u-szeged.hu/rgai/conll2010st/
9http://en.wikipedia.org/wiki/Weasel word
73
in LEXIS according to Flesch reading ease (Kincaid
et al, 1975).
4 Hedging to distinguish scientific text:
Initial annotation
As noted in Section 1, it is not a priori clear whether
hedging distinguishes scientific text or that more
hedges correspond to a more ?scientific? discourse.
To get an initial feeling for how frequently hedges
occur in WOS and LEXIS, we hand-annotated a
sample of sentences from each. In Section 4.1, we
explain the annotation policy of the CoNLL 2010
Shared Task and our own annotation method for
WOS and LEXIS. After that, we move forward in
Section 4.2 to compare the percentage of uncertain
sentences in prof-science vs. pop-science text on
this small hand-labeled sample, and gain some ev-
idence that there is indeed a difference in hedge oc-
currence rates, although, perhaps surprisingly, there
seem to be more hedges in the pop-science texts.
As a side benefit, we subsequently use the
hand-labeled sample we produce to investigate the
accuracy of an automatic hedge detector in the
WOS+LEXIS domain; more on this in Section 5.
4.1 Uncertainty annotation
CoNLL 2010 Shared Task annotation policy As
described in Farkas et al (2010, pg. 4), the data an-
notation polices for the CoNLL 2010 Shared Task
were that ?sentences containing at least one cue
were considered as uncertain, while sentences with
no cues were considered as factual?, where a cue
is a linguistic marker that in context indicates un-
certainty. A straightforward example of a sentence
marked ?uncertain? in the Shared Task is ?Mild blad-
der wall thickening raises the question of cystitis.?
The annotated cues are not necessarily general, par-
ticularly in Wiki, where some of the marked cues
are as specific as ?some of schumann?s best choral
writing?, ?people of the jewish tradition?, or ?certain
leisure or cultural activities?.
Note that ?uncertainty? in the Shared Task def-
inition also encompassed phrasing that ?creates an
impression that something important has been said,
but what is really communicated is vague, mislead-
ing, evasive or ambiguous ... [offering] an opinion
without any backup or source?. An example of such
Dataset % of uncertain sentences
WOS (estimated from 75-sentence sample) 20
LEXIS (estimated from 78-sentence sample) 28
Bio 17
Wiki 23
Table 2: Percentages of uncertain sentences.
a sentence, drawn from Wikipedia and marked ?un-
certain? in the Shared Task, is ?Some people claim
that this results in a better taste than that of other diet
colas (most of which are sweetened with aspartame
alone).?; Farkas et al (2010) write, ?The ... sentence
does not specify the source of the information, it is
just the vague term ?some people? that refers to the
holder of this opinion?.
Our annotation policy We hand-annotated 200
randomly-sampled sentences, half from WOS and
half from LEXIS10, to gauge the frequency with
which hedges occur in each corpus. Two annota-
tors each followed the rules of the CoNLL 2010
Shared Task to label sentences as certain, uncertain,
or not a proper sentence.11 The annotators agreed on
153 proper sentences of the 200 sentences (75 from
WOS and 78 from LEXIS). Cohen?s Kappa (Fleiss,
1981) was 0.67 on the annotation, which means that
the consistency between the two annotators was fair
or good. However, there were some interesting cases
where the two annotators could not agree. For ex-
ample, in the sentence ?Cassava is the staple food of
tropical Africa and its production, averaged over 24
countries, has increased more than threefold from
1980 to 2005 ... ?, one of the annotators believed
that ?more than? made the sentence uncertain. These
borderline cases indicate that the definition of hedg-
ing should be carefully delineated in future studies.
4.2 Percentages of uncertain sentences
To validate the hypothesis that prof-science articles
contain more hedges, we computed the percentage
10We took steps to attempt to hide from the annotators any
explicit clues as to the source of individual sentences: the sub-
set of authors who did the annotation were not those that col-
lected the data, and the annotators were presented the sentences
in random order.
11The last label was added because of a few errors in scraping
the data.
74
of uncertain sentences in our labeled data. As shown
in Table 2, we observed a trend contradicting ear-
lier studies. Uncertain sentences were more frequent
in LEXIS than in WOS, though the difference was
not statistically significant12 (perhaps not surprising
given the small sample size). The same trend was
seen in the CoNLL dataset: there, too, the percent-
age of uncertain sentences was significantly smaller
in Bio (prof-science articles) than in Wiki. In order
to make a stronger argument about prof-science vs
pop-science, however, more annotation on the WOS
and LEXIS datasets is needed.
5 Experiments
As stated in Section 1, our proposal requires devel-
oping an effective hedge detection algorithm. Our
approach for the preliminary work described in this
paper is to re-implement Georgescul?s (2010) algo-
rithm; the experimental results on the Bio+Wiki do-
main, given in Section 5.1, are encouraging. Then
we use this method to attempt to validate (at a larger
scale than in our manual pilot annotation) whether
hedges can be used to distinguish between prof-
science and pop-science discourse on GMOs. Un-
fortunately, our results, given in Section 5.2, are
inconclusive, since our trained model could not
achieve satisfactory automatic hedge-detection ac-
curacy on the WOS+LEXIS domain.
5.1 Method
We adopted the method of Georgescul (2010): Sup-
port Vector Machine classification based on a Gaus-
sian Radial Basis kernel function (Vapnik, 1998; Fan
et al, 2005), employing n-grams from annotated cue
phrases as features, as described in more detail be-
low. This method achieved the top performance in
the CoNLL 2010 Wikipedia hedge-detection task
(Farkas et al, 2010), and SVMs have been proven
effective for many different applications. We used
the LIBSVM toolkit in our experiments13.
As described in Section 3.2, there are two separate
datasets in the CoNLL dataset. We experimented on
them separately (Bio, Wiki). Also, to make our clas-
sifier more generalizable to different datasets, we
12Throughout, ?statistical significance? refers to the student
t-test with p < .05.
13http://www.csie.ntu.edu.tw/?cjlin/libsvm/
also trained models based on the two datasets com-
bined (Bio+Wiki). As for features, we took advan-
tage of the observation in Georgescul (2010) that the
bag-of-words model does not work well for this task.
We used different sets of features based on hedge
cue words that have been annotated as part of the
CoNLL dataset distribution14. The basic feature set
was the frequency of each hedge cue word from the
training corpus after removing stop words and punc-
tuation and transforming words to lowercase. Then,
we extracted unigrams, bigrams and trigrams from
each hedge cue phrase. Table 3 shows the number
of features in different settings. Notice that there are
many more features in Wiki. As mentioned above,
in Wiki, some cues are as specific as ?some of schu-
mann?s best choral writing?, ?people of the jewish
tradition?, or ? certain leisure or cultural activities?.
Taking n-grams from such specific cues can cause
some sentences to be classified incorrectly.
Feature source #features
Bio 220
Bio (cues + bigram + trigram) 340
Wiki 3740
Wiki (cues + bigram + trigram) 10603
Table 3: Number of features.
Best cross-validation performance
Dataset (C, ?) P R F
Bio (40, 2?3) 84.0 92.0 87.8
Wiki (30, 2?6) 64.0 76.3 69.6
Bio+Wiki (10, 2?4) 66.7 78.3 72.0
Table 4: Best 5-fold cross-validation performance for Bio
and/or Wiki after parameter tuning. As a reminder, we
repeat that our intended final test set is the WOS+LEXIS
corpus, which is disjoint from Bio+Wiki.
We adopted several techniques from Georgescul
(2010) to optimize performance through cross vali-
dation. Specifically, we tried different combinations
of feature sets (the cue phrases themselves, cues +
14For the Bio model, we used cues extracted from Bio. Like-
wise, the Wiki model used cues from Wiki, and the Bio+Wiki
model used cues from Bio+Wiki.
75
Evaluation set Model P R F
WOS+LEXIS Bio 54 68 60
WOS+LEXIS Wiki 38 54 45
WOS+LEXIS Bio+Wiki 21 93 34
Sub-corpus performance of the model based on Bio
WOS Bio 58 73 65
LEXIS Bio 52 64 57
Table 5: The upper part shows the performance on WOS
and LEXIS based on models trained on the CoNLL
dataset. The lower part gives the sub-corpus results for
Bio, which provided the best performance on the full
WOS+LEXIS corpus.
unigram, cues + bigram, cues + trigram, cues + uni-
gram + bigram + trigram, cues + bigram + trigram).
We tuned the width of the RBF kernel (?) and the
regularization parameter (C) via grid search over the
following range of values: {2?9, 2?8, 2?7, . . . , 24}
for ?, {1, 10, 20, 30, . . . , 150} for C. We also tried
different weighting strategies for negative and pos-
itive classes (i.e., either proportional to the number
of positive instances, or uniform). We performed 5-
fold cross validation for each possible combination
of experimental settings on the three datasets (Bio,
Wiki, Bio+Wiki).
Table 4 shows the best performance on all three
datasets and the corresponding parameters. In the
three datasets, cue+bigram+trigram provided the
best performance, and the weighted model con-
sistently produced superior results to the uniform
model. The F1 measure for Bio was 87.8, which
was satisfactory, while the F1 results for Wiki were
69.6, which were the worst of all the datasets.
This resonates with our observation that the task on
Wikipedia is more subtly defined and thus requires
a more sophisticated approach than counting the oc-
currences of bigrams and trigrams.
5.2 Results on WOS+LEXIS
Next, we evaluated whether our best classifier
trained on the CoNLL dataset can be generalized to
other datasets, in particular, the WOS and LEXIS
corpus. Performance was measured on the 153 sen-
tences on which our annotators agreed, a dataset
that was introduced in Section 4.1. Table 5 shows
how the best models trained on Bio, Wiki, and
Evaluation set (C, ?) P R F
WOS + LEXIS (50, 2?9) 68 62 65
WOS (50, 2?9) 85 73 79
LEXIS (50, 2?9) 57 54 56
Table 6: Best performance after parameter tuning
based on the 153 labeled WOS+LEXIS sentences; this
gives some idea of the upper-bound potential of our
Georgescul-based method. The training set is Bio, which
gave the best performance in Table 5.
Bio+Wiki, respectively, performed on the 153 la-
beled sentences. First, we can see that the perfor-
mance degraded significantly compared to the per-
formance for in-domain cross validation. Second, of
the three different models, Bio showed the best per-
formance. Bio+Wiki gave the worst performance,
which hints that combining two datasets and cue
words may not be a promising strategy: although
Bio+Wiki shows very good recall, this can be at-
tributed to its larger feature set, which contains all
available cues and perhaps as a result has a very high
false-positive rate. We further investigated and com-
pared performance on LEXIS and WOS for the best
model (Bio). Not surprisingly, our classifier works
better in WOS than in LEXIS.
It is clear that there exist domain differences be-
tween the CoNLL dataset and WOS+LEXIS. To bet-
ter understand the poor cross-domain performance
of the classifier, we tuned another model based on
the performance on the 153 labeled sentences us-
ing Bio as training data. As we can see in Table
6, the performance on WOS improved significantly,
while the performance on LEXIS decreased. This
is probably caused by the fact that WOS is a col-
lection of scientific paper abstracts, which is more
similar to the training corpus than LEXIS, which is
a collection of news media articles15. Also, LEXIS
articles are hard to classify even with the tuned
model, which challenges the effectiveness of a cue-
words frequency approach beyond professional sci-
entific texts. Indeed, the simplicity of our reim-
plementation of Georgescul?s algorithm seems to
cause longer sentences to be classified as uncer-
tain, because cue phrases (or n-grams extracted from
15The Wiki model performed better on LEXIS than on WOS.
Though the performance was not good, this result further rein-
forces the possibility of a domain-dependence problem.
76
cue phrases) are more likely to appear in lengthier
sentences. Analysis of the best performing model
shows that the false-positive sentences are signifi-
cantly longer than the false-negative ones.16
Dataset Model % classified uncertain
WOS Bio 16
LEXIS Bio 19
WOS Tuned 15
LEXIS Tuned 14
Table 7: For completeness, we report here the percentage
of uncertain sentences in WOS and LEXIS according to
our trained classifiers, although we regard these results as
unreliable since those classifiers have low accuracy. Bio
refers to the best model trained on Bio only in Section 5.1,
while Tuned refers to the model in Table 6 that is tuned
based on the 153 labeled sentences in WOS+LEXIS.
While the cross-domain results were not reliable,
we produced preliminary results on whether there
exist fewer hedges in scientific text. We can see that
the relative difference in certain/uncertain ratios pre-
dicted by the two different models (Bio, Tuned) are
different in Table 7. In the tuned model, the differ-
ence between LEXIS and WOS in terms of the per-
centage of uncertain sentences was not statistically
significant, while in the Bio model, their difference
was statistically significant. Since the performance
of our hedge classifier on the 153 hand-annotated
WOS+LEXIS sentences was not reliable, though,
we must abstain from making conclusive statements
here.
6 Conclusion and future work
In this position paper, we advocated that researchers
apply hedge detection not only to the classic moti-
vation of information-extraction problems, but also
to questions of how public opinion forms. We pro-
posed a particular problem in how participants in de-
bates frame their arguments. Specifically, we asked
whether pro-GMO and anti-GMO articles differ in
adopting a more ?scientific? discourse. Inspired by
earlier studies in social sciences relating hedging to
texts aimed at professional scientists, we proposed
16Average length of true positive sentences : 28.6, false pos-
itive sentences 35.09, false negative sentences: 22.0.
addressing the question with automatic hedge de-
tection as a first step. To develop a hedge clas-
sifier, we took advantage of the CoNLL dataset
and a small annotated WOS and LEXIS dataset.
Our preliminary results show there may exist a gap
which indicates that hedging may, in fact, distin-
guish prof-science and pop-science documents. In
fact, this computational analysis suggests the possi-
bility that hedges occur less frequently in scientific
prose, which contradicts several prior assertions in
the literature.
To confirm the argument that pop-science tends
to use more hedging than prof-science, we need
a hedge classifier that performs more reliably in
the WOS and LEXIS dataset than ours does. An
interesting research direction would be to develop
transfer-learning techniques to generalize hedge
classifiers for different datasets, or to develop a gen-
eral hedge classifier relatively robust to domain dif-
ferences. In either case, more annotated data on
WOS and LEXIS is needed for better evaluation or
training.
Another strategy would be to bypass the first step,
in which we determine whether hedges are more
or less prominent in scientific discourse, and pro-
ceed directly to labeling and hedge-detection in pro-
GMO and anti-GMO texts. However, this will not
answer the question of whether advocates in debates
other than on GMO-related topics employ a more
scientific discourse. Nonetheless, to aid those who
wish to pursue this alternate strategy, we have col-
lected two sets of opinionated articles on GMO (pro-
and anti-); see appendix for more details.
Acknowledgments We thank Daniel Hopkins and
Bonnie Webber for reference suggestions, and the
anonymous reviewers for helpful and thoughtful
comments. This paper is based upon work sup-
ported in part by US NSF grants IIS-0910664 and
IIS-1016099, a US NSF graduate fellowship to JS,
Google, and Yahoo!
References
Amr Ahmed and Eric P Xing. Staying informed: su-
pervised and semi-supervised multi-view topical
analysis of ideological perspective. In EMNLP,
pages 1140?1150, 2010.
Charles Bazerman. Shaping Written Knowledge:
77
The Genre and Activity of the Experimental Ar-
ticle in Science. University of Wisconsin Press,
Madison, Wis., 1988.
Beata Beigman Klebanov, Eyal Beigman, and
Daniel Diermeier. Vocabulary choice as an indi-
cator of perspective. In ACL Short Papers, pages
253?257, Stroudsburg, PA, USA, 2010. Associa-
tion for Computational Linguistics.
Robert D. Benford and David A. Snow. Framing
processes and social movements: An overview
and assessment. Annual Review of Sociology, 26:
611?639, 2000.
Richard Harvey Brown. Toward a democratic sci-
ence: Scientific narration and civic communica-
tion. Yale University Press, New Haven, 1998.
Dennis Chong and James N. Druckman. Framing
theory. Annual Review of Political Science, 10:
103?126, 2007.
Jeanne Fahnestock. Accommodating Science. Writ-
ten Communication, 3(3):275?296, 1986.
Jeanne Fahnestock. Preserving the figure: Consis-
tency in the presentation of scientific arguments.
Written Communication, 21(1):6?31, 2004.
Rong-En Fan, Pai-Hsuen Chen, and Chih-Jen Lin.
Working set selection using second order in-
formation for training support vector machines.
JMLR, 6:1889?1918, December 2005. ISSN
1532-4435.
Richa?rd Farkas, Veronika Vincze, Gyo?rgy Mo?ra,
Ja?nos Csirik, and Gyo?rgy Szarvas. The CoNLL-
2010 shared task: Learning to detect hedges and
their scope in natural language text. In CoNLL?
Shared Task, pages 1?12, 2010.
Joseph L. Fleiss. Statistical Methods for Rates
and Proportions. Wiley series in probability and
mathematical statistics. John Wiley & Sons, New
York, second edition, 1981.
Maria Georgescul. A hedgehop over a max-margin
framework using hedge cues. In CONLL?
Shared-Task, pages 26?31, 2010.
G. Nigel Gilbert and Michael Joseph Mulkay. Open-
ing Pandora?s box: A sociological analysis of sci-
entists? discourse. CUP Archive, 1984.
Erving Goffman. Frame analysis: An essay on the
organization of experience. Harvard University
Press, 1974.
Alan G. Gross. The rhetoric of science. Harvard
University Press, Cambridge, Mass., 1990.
Michael Alexander Kirkwood Halliday and
James R. Martin. Writing science: Literacy and
discursive power. Psychology Press, London
[u.a.], 1993.
Eric A Hardisty, Jordan Boyd-Graber, and Philip
Resnik. Modeling perspective using adaptor
grammars. In EMNLP, pages 284?292, 2010.
Ken Hyland. Hedging in scientific research articles.
John Benjamins Pub. Co., Amsterdam; Philadel-
phia, 1998.
Napoleon K. Juanillo, Jr. Frames for Public Dis-
course on Biotechnology. In Genetically Modified
Food and the Consumer: Proceedings of the 13th
meeting of the National Agricultural Biotechnol-
ogy Council, pages 39?50, 2001.
J. Peter Kincaid, Robert P. Fishburne, Richard L.
Rogers, and Brad S. Chissom. Derivation of new
readability formulas for navy enlisted personnel.
Technical report, National Technical Information
Service, Springfield, Virginia, February 1975.
George Lakoff. Hedges: A study in meaning cri-
teria and the logic of fuzzy concepts. Journal of
Philosophical Logic, 2(4):458?508, 1973.
Bruno Latour. Science in action: How to follow sci-
entists and engineers through society. Harvard
University Press, Cambridge, Mass., 1987.
Bruno Latour and Steve Woolgar. Laboratory life:
The social construction of scientific facts. Sage
Publications, Beverly Hills, 1979.
Beverly A. Lewin. Hedging: Form and function
in scientific research texts. In Genre Studies in
English for Academic Purposes, volume 9, pages
89?108. Universitat Jaume I, 1998.
Wei-Hao Lin, Theresa Wilson, Janyce Wiebe, and
Alexander Hauptmann. Which side are you on?
identifying perspectives at the document and sen-
tence levels. In CoNLL, 2006.
Greg Myers. The pragmatics of politeness in sci-
entific articles. Applied Linguistics, 10(1):1?35,
1989.
78
Greg Myers. Discourse studies of scientific popular-
ization: Questioning the boundaries. Discourse
Studies, 5(2):265?279, 2003.
Franc?oise Salager-Meyer. Scientific discourse and
contrastive linguistics: hedging. European Sci-
ence Editing, 37(2):35?37, 2011.
Dietram A. Scheufele. Framing as a theory of media
effects. Journal of Communication, 49(1):103?
122, 1999.
Vladimir N. Vapnik. Statistical Learning Theory.
Wiley-Interscience, 1998.
Teppo Varttala. Remarks on the communicative
functions of hedging in popular scientific and spe-
cialist research articles on medicine. English for
Specific Purposes, 18(2):177?200, 1999.
Teppo Varttala. Hedging in scientifically oriented
discourse: Exploring variation according to dis-
cipline and intended audience. PhD thesis, Uni-
versity of Tampere, 2001.
7 Appendix: pro- vs. anti-GMO dataset
Here, we describe the pro- vs. anti-GMO dataset we
collected, in the hopes that this dataset may prove
helpful in future research regarding the GMO de-
bates, even though we did not use the corpus in the
project described in this paper.
The second step of our overall procedure out-
lined in the introduction ? that step being to ex-
amine whether the use of hedging in pro-GMO arti-
cles corresponds with our inferred ?scientific? oc-
currence patterns more than that in anti-GMO ar-
ticles ? requires a collection of opinionated arti-
cles on GMOs. Our first attempt to use news me-
dia articles (LEXIS) was unsatisfying, as we found
many articles attempt to maintain a neutral position.
This led us to collect documents from more strongly
opinionated organizational websites such as Green-
peace (anti-GMO), Non GMO Project (anti-GMO),
or Why Biotechnology (pro-GMO). Articles were
collected from 20 pro-GMO and 20 anti-GMO or-
ganizational web sites.
After the initial collection of data, near-duplicates
and irrelevant articles were filtered through cluster-
ing, keyword searches and distance between word
vectors at the document level. We have collected
762 ?anti? documents and 671 ?pro? documents.
We reduced this to a 404 ?pro? and 404 ?con?
set as follows. Each retained ?document? con-
sists of only the first 200 words after excluding the
first 50 words of documents containing over 280
words. This was done to avoid irrelevant sections
such as Educators have permission to reprint arti-
cles for classroom use; other users, please contact
editor@actionbioscience.org for reprint permission.
See reprint policy.
The data will be posted online at
https://confluence.cornell.edu/display/llresearch/
HedgingFramingGMOs.
79
Proceedings of the ACL 2014 Workshop on Language Technologies and Computational Social Science, page 1,
Baltimore, Maryland, USA, June 26, 2014.
c?2014 Association for Computational Linguistics
Is It All in the Phrasing? Computational Explorations in How We Say
What We Say, and Why It Matters
Lillian Lee
Cornell University
Abstract
Louis Armstrong (is said to have) said, ?I don?t need words ? it?s all in the phrasing?. As
someone who does natural-language processing for a living, I?m a big fan of words; but lately, my
collaborators and I have been studying aspects of phrasing (in the linguistic, rather than musical
sense) that go beyond just the selection of one particular word over another. I?ll describe some of
these projects in this talk. The issues we?ll consider include: Does the way in which something
is worded in and of itself have an effect on whether it is remembered or attracts attention, beyond
its content or context? Can we characterize how different sides in a debate frame their arguments,
in a way that goes beyond specific lexical choice (e.g., ?pro-choice? vs. ?pro-life?)? The settings
we?ll explore range from movie quotes that achieve cultural prominence; to posts on Facebook,
Wikipedia, Twitter, and the arXiv; to framing in public discourse on the inclusion of genetically-
modified organisms in food.
Joint work with Lars Backstrom, Justin Cheng, Eunsol Choi, Cristian Danescu-Niculescu-Mizil,
Jon Kleinberg, Bo Pang, Jennifer Spindel, and Chenhao Tan.
References
Lars Backstrom, Jon Kleinberg, Lillian Lee, and Cristian Danescu-Niculescu-Mizil. 2013. Characterizing and
curating conversation threads: Expansion, focus, volume, re-entry. In Proceedings of WSDM, pages 13?22.
Eunsol Choi, Chenhao Tan, Lillian Lee, Cristian Danescu-Niculescu-Mizil, and Jennifer Spindel. 2012. Hedge
detection as a lens on framing in the GMO debates: A position paper. In Proceedings of the Workshop on
Extra-Propositional Aspects of Meaning in Computational Linguistics, pages 70?79.
Cristian Danescu-Niculescu-Mizil, Justin Cheng, Jon Kleinberg, and Lillian Lee. 2012. You had me at hello: How
phrasing affects memorability. In Proceedings of ACL, pages 892?901.
Chenhao Tan and Lillian Lee. 2014. A corpus of sentence-level revisions in academic writing: A step towards
understanding statement strength in communication. In Proceedings of ACL (short paper).
Chenhao Tan, Lillian Lee, and Bo Pang. 2014. The effect of wording on message propagation: Topic- and author-
controlled natural experiments on twitter. In Proceedings of ACL.
1
Proceedings of the SIGDIAL 2014 Conference, page 141,
Philadelphia, U.S.A., 18-20 June 2014. c?2014 Association for Computational Linguistics
Keynote: Language Adaptation
Lillian Lee
Cornell University, U.S.A.
llee@cs.cornell.edu
As we all know, more and more of life is now manifested online,
and many of the digital traces that are left by human activity are in-
creasingly recorded in natural-language format. This availability offers
us the opportunity to glean user-modeling information from individual
users? linguistic behaviors. This talk will discuss the particular phe-
nomenon of individual language adaptation, both in the short term
and in the longer term. We?ll look at connections between how people
adapt their language to particular conversational partners or groups,
on the one hand, and on the other hand, those people?s relative power
relationships, quality of relationship with the conversational partner,
and propensity to remain a part of the group.
141
