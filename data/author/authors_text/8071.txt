Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions, pages 17?20,
Sydney, July 2006. c?2006 Association for Computational Linguistics
An intelligent search engine and GUI-based efficient MEDLINE search
tool based on deep syntactic parsing
Tomoko Ohta
Yoshimasa Tsuruoka??
Jumpei Takeuchi
Jin-Dong Kim
Yusuke Miyao
Akane Yakushiji?
Kazuhiro Yoshida
Yuka Tateisi?
Department of Computer Science, University of Tokyo
Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033 JAPAN
{okap, yusuke, ninomi, tsuruoka, akane, kmasuda, tj jug,
kyoshida, harasan, jdkim, yucca, tsujii}@is.s.u-tokyo.ac.jp
Takashi Ninomiya?
Katsuya Masuda
Tadayoshi Hara
Jun?ichi Tsujii
Abstract
We present a practical HPSG parser for
English, an intelligent search engine to re-
trieve MEDLINE abstracts that represent
biomedical events and an efficient MED-
LINE search tool helping users to find in-
formation about biomedical entities such
as genes, proteins, and the interactions be-
tween them.
1 Introduction
Recently, biomedical researchers have been fac-
ing the vast repository of research papers, e.g.
MEDLINE. These researchers are eager to search
biomedical correlations such as protein-protein or
gene-disease associations. The use of natural lan-
guage processing technology is expected to re-
duce their burden, and various attempts of infor-
mation extraction using NLP has been being made
(Blaschke and Valencia, 2002; Hao et al, 2005;
Chun et al, 2006). However, the framework of
traditional information retrieval (IR) has difficulty
with the accurate retrieval of such relational con-
cepts. This is because relational concepts are
essentially determined by semantic relations of
words, and keyword-based IR techniques are in-
sufficient to describe such relations precisely.
This paper proposes a practical HPSG parser
for English, Enju, an intelligent search engine for
the accurate retrieval of relational concepts from
?Current Affiliation:
?School of Informatics, University of Manchester
?Knowledge Research Center, Fujitsu Laboratories LTD.
?Faculty of Informatics, Kogakuin University
?Information Technology Center, University of Tokyo
F-Score
GENIA treebank Penn Treebank
HPSG-PTB 85.10% 87.16%
HPSG-GENIA 86.87% 86.81%
Table 1: Performance for Penn Treebank and the
GENIA corpus
MEDLINE, MEDIE, and a GUI-based efficient
MEDLINE search tool, Info-PubMed.
2 Enju: An English HPSG Parser
We developed an English HPSG parser, Enju 1
(Miyao and Tsujii, 2005; Hara et al, 2005; Ni-
nomiya et al, 2005). Table 1 shows the perfor-
mance. The F-score in the table was accuracy
of the predicate-argument relations output by the
parser. A predicate-argument relation is defined
as a tuple ??,wh, a, wa?, where ? is the predi-
cate type (e.g., adjective, intransitive verb), wh
is the head word of the predicate, a is the argu-
ment label (MOD, ARG1, ..., ARG4), and wa is
the head word of the argument. Precision/recall
is the ratio of tuples correctly identified by the
parser. The lexicon of the grammar was extracted
from Sections 02-21 of Penn Treebank (39,832
sentences). In the table, ?HPSG-PTB? means that
the statistical model was trained on Penn Tree-
bank. ?HPSG-GENIA? means that the statistical
model was trained on both Penn Treebank and GE-
NIA treebank as described in (Hara et al, 2005).
The GENIA treebank (Tateisi et al, 2005) consists
of 500 abstracts (4,446 sentences) extracted from
MEDLINE.
Figure 1 shows a part of the parse tree and fea-
1http://www-tsujii.is.s.u-tokyo.ac.jp/enju/
17
Figure 1: Snapshot of Enju
ture structure for the sentence ?NASA officials
vowed to land Discovery early Tuesday at one
of three locations after weather conditions forced
them to scrub Monday?s scheduled return.?
3 MEDIE: a search engine for
MEDLINE
Figure 2 shows the top page of the MEDIE. ME-
DIE is an intelligent search engine for the accu-
rate retrieval of relational concepts from MED-
LINE 2 (Miyao et al, 2006). Prior to retrieval, all
sentences are annotated with predicate argument
structures and ontological identifiers by applying
Enju and a term recognizer.
3.1 Automatically Annotated Corpus
First, we applied a POS analyzer and then Enju.
The POS analyzer and HPSG parser are trained
by using the GENIA corpus (Tsuruoka et al,
2005; Hara et al, 2005), which comprises around
2,000 MEDLINE abstracts annotated with POS
and Penn Treebank style syntactic parse trees
(Tateisi et al, 2005). The HPSG parser generates
parse trees in a stand-off format that can be con-
verted to XML by combining it with the original
text.
We also annotated technical terms of genes and
diseases in our developed corpus. Technical terms
are annotated simply by exact matching of dictio-
2http://www-tsujii.is.s.u-tokyo.ac.jp/medie/
nary entries and the terms separated by space, tab,
period, comma, hat, colon, semi-colon, brackets,
square brackets and slash in MEDLINE.
The entire dictionary was generated by apply-
ing the automatic generation method of name vari-
ations (Tsuruoka and Tsujii, 2004) to the GENA
dictionary for the gene names (Koike and Takagi,
2004) and the UMLS (Unified Medical Language
System) meta-thesaurus for the disease names
(Lindberg et al, 1993). It was generated by ap-
plying the name-variation generation method, and
we obtained 4,467,855 entries of a gene and dis-
ease dictionary.
3.2 Functions of MEDIE
MEDIE provides three types of search, seman-
tic search, keyword search, GCL search. GCL
search provides us the most fundamental and pow-
erful functions in which users can specify the
boolean relations, linear order relation and struc-
tural relations with variables. Trained users can
enjoy all functions in MEDIE by the GCL search,
but it is not easy for general users to write ap-
propriate queries for the parsed corpus. The se-
mantic search enables us to specify an event verb
with its subject and object easily. MEDIE auto-
matically generates the GCL query from the se-
mantic query, and runs the GCL search. Figure 3
shows the output of semantic search for the query
?What disease does dystrophin cause??. This ex-
ample will give us the most intuitive understand-
ings of the proximal and structural retrieval with a
richly annotated parsed corpus. MEDIE retrieves
sentences which include event verbs of ?cause?
and noun ?dystrophin? such that ?dystrophin? is the
subject of the event verbs. The event verb and its
subject and object are highlighted with designated
colors. As seen in the figure, small sentences in
relative clauses, passive forms or coordination are
retrieved. As the objects of the event verbs are
highlighted, we can easily see what disease dys-
trophin caused. As the target corpus is already
annotated with diseases entities, MEDIE can ef-
ficiently retrieve the disease expressions.
4 Info-PubMed: a GUI-based
MEDLINE search tool
Info-PubMed is a MEDLINE search tool with
GUI, helping users to find information about
biomedical entities such as genes, proteins, and
18
Figure 2: Snapshot of MEDIE: top page?
Figure 3: Snapshot of MEDIE: ?What disease does
dystrophin cause??
the interactions between them 3.
Info-PubMed provides information from MED-
LINE on protein-protein interactions. Given the
name of a gene or protein, it shows a list of the
names of other genes/proteins which co-occur in
sentences from MEDLINE, along with the fre-
quency of co-occurrence.
Co-occurrence of two proteins/genes in the
same sentence does not always imply that they in-
teract. For more accurate extraction of sentences
that indicate interactions, it is necessary to iden-
tify relations between the two substances. We
adopted PASs derived by Enju and constructed ex-
traction patterns on specific verbs and their argu-
ments based on the derived PASs (Yakusiji, 2006).
Figure 4: Snapshot of Info-PubMed (1)
Figure 5: Snapshot of Info-PubMed (2)
Figure 6: Snapshot of Info-PubMed (3)
4.1 Functions of Info-PubMed
In the ?Gene Searcher? window, enter the name
of a gene or protein that you are interested in.
For example, if you are interested in Raf1, type
?raf1? in the ?Gene Searcher? (Figure 4). You
will see a list of genes whose description in our
dictionary contains ?raf1? (Figure 5). Then, drag
3http://www-tsujii.is.s.u-tokyo.ac.jp/info-pubmed/
19
one of the GeneBoxes from the ?Gene Searcher?
to the ?Interaction Viewer.? You will see a list
of genes/proteins which co-occur in the same
sentences, along with co-occurrence frequency.
The GeneBox in the leftmost column is the one
you have moved to ?Interaction Viewer.? The
GeneBoxes in the second column correspond to
gene/proteins which co-occur in the same sen-
tences, followed by the boxes in the third column,
InteractionBoxes.
Drag an InteractionBox to ?ContentViewer? to
see the content of the box (Figure 6). An In-
teractionBox is a set of SentenceBoxes. A Sen-
tenceBox corresponds to a sentence in MEDLINE
in which the two gene/proteins co-occur. A Sen-
tenceBox indicates whether the co-occurrence in
the sentence is direct evidence of interaction or
not. If it is judged as direct evidence of interac-
tion, it is indicated as Interaction. Otherwise, it is
indicated as Co-occurrence.
5 Conclusion
We presented an English HPSG parser, Enju, a
search engine for relational concepts from MED-
LINE, MEDIE, and a GUI-based MEDLINE
search tool, Info-PubMed.
MEDIE and Info-PubMed demonstrate how the
results of deep parsing can be used for intelligent
text mining and semantic information retrieval in
the biomedical domain.
6 Acknowledgment
This work was partially supported by Grant-in-Aid
for Scientific Research on Priority Areas ?Sys-
tems Genomics? (MEXT, Japan) and Solution-
Oriented Research for Science and Technology
(JST, Japan).
References
C. Blaschke and A. Valencia. 2002. The frame-based
module of the SUISEKI information extraction sys-
tem. IEEE Intelligent Systems, 17(2):14?20.
Y. Hao, X. Zhu, M. Huang, and M. Li. 2005. Dis-
covering patterns to extract protein-protein interac-
tions from the literature: Part II. Bioinformatics,
21(15):3294?3300.
H.-W. Chun, Y. Tsuruoka, J.-D. Kim, R. Shiba, N. Na-
gata, T. Hishiki, and J. Tsujii. 2006. Extraction
of gene-disease relations from MedLine using do-
main dictionaries and machine learning. In Proc.
PSB 2006, pages 4?15.
Carl Pollard and Ivan A. Sag. 1994. Head-Driven
Phrase Structure Grammar. University of Chicago
Press.
Yusuke Miyao and Jun?ichi Tsujii. 2005. Probabilis-
tic disambiguation models for wide-coverage HPSG
parsing. In Proc. of ACL?05, pages 83?90.
Tadayoshi Hara, Yusuke Miyao, and Jun?ichi Tsu-
jii. 2005. Adapting a probabilistic disambiguation
model of an HPSG parser to a new domain. In Proc.
of IJCNLP 2005.
Takashi Ninomiya, Yoshimasa Tsuruoka, Yusuke
Miyao, and Jun?ichi Tsujii. 2005. Efficacy of beam
thresholding, unification filtering and hybrid parsing
in probabilistic HPSG parsing. In Proc. of IWPT
2005, pages 103?114.
Yuka Tateisi, Akane Yakushiji, Tomoko Ohta, and
Jun?ichi Tsujii. 2005. Syntax Annotation for the
GENIA corpus. In Proc. of the IJCNLP 2005, Com-
panion volume, pp. 222?227.
Yusuke Miyao, Tomoko Ohta, Katsuya Masuda, Yoshi-
masa Tsuruoka, Kazuhiro Yoshida, Takashi Ni-
nomiya and Jun?ichi Tsujii. 2006. Semantic Re-
trieval for the Accurate Identification of Relational
Concepts in Massive Textbases. In Proc. of ACL ?06,
to appear.
Yoshimasa Tsuruoka, Yuka Tateisi, Jin-Dong Kim,
Tomoko Ohta, John McNaught, Sophia Ananiadou,
and Jun?ichi Tsujii. 2005. Part-of-speech tagger for
biomedical text. In Proc. of the 10th Panhellenic
Conference on Informatics.
Y. Tsuruoka and J. Tsujii. 2004. Improving the per-
formance of dictionary-based approaches in protein
name recognition. Journal of Biomedical Informat-
ics, 37(6):461?470.
Asako Koike and Toshihisa Takagi. 2004.
Gene/protein/family name recognition in biomed-
ical literature. In Proc. of HLT-NAACL 2004
Workshop: Biolink 2004, pages 9?16.
D.A. Lindberg, B.L. Humphreys, and A.T. McCray.
1993. The unified medical language system. Meth-
ods in Inf. Med., 32(4):281?291.
Akane Yakushiji. 2006. Relation Information Extrac-
tion Using Deep Syntactic Analysis. Ph.D. Thesis,
University of Tokyo.
20
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 284?292,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Automatic Construction of Predicate-argument Structure Patterns
for Biomedical Information Extraction
Akane Yakushiji? ? Yusuke Miyao? Tomoko Ohta?
?Department of Computer Science, University of Tokyo
Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033 JAPAN
? School of Informatics, University of Manchester
POBox 88, Sackville St, MANCHESTER M60 1QD, UK
{akane, yusuke, okap, yucca, tsujii}@is.s.u-tokyo.ac.jp
Yuka Tateisi? ? Jun?ichi Tsujii? ?
Abstract
This paper presents a method of automat-
ically constructing information extraction
patterns on predicate-argument structures
(PASs) obtained by full parsing from a
smaller training corpus. Because PASs
represent generalized structures for syn-
tactical variants, patterns on PASs are ex-
pected to be more generalized than those
on surface words. In addition, patterns
are divided into components to improve
recall and we introduce a Support Vector
Machine to learn a prediction model using
pattern matching results. In this paper, we
present experimental results and analyze
them on how well protein-protein interac-
tions were extracted from MEDLINE ab-
stracts. The results demonstrated that our
method improved accuracy compared to a
machine learning approach using surface
word/part-of-speech patterns.
1 Introduction
One primitive approach to Information Extrac-
tion (IE) is to manually craft numerous extrac-
tion patterns for particular applications and this
is presently one of the main streams of biomedi-
cal IE (Blaschke and Valencia, 2002; Koike et al,
2003). Although such IE attempts have demon-
strated near-practical performance, the same sets
of patterns cannot be applied to different kinds of
information. A real-world task requires several
kinds of IE, thus manually engineering extraction
Current Affiliation:
? FUJITSU LABORATORIES LTD.
? Faculty of Informatics, Kogakuin University
patterns, which is tedious and time-consuming
process, is not really practical.
Techniques based on machine learning (Zhou et
al., 2005; Hao et al, 2005; Bunescu and Mooney,
2006) are expected to alleviate this problem in
manually crafted IE. However, in most cases, the
cost of manually crafting patterns is simply trans-
ferred to that for constructing a large amount of
training data, which requires tedious amount of
manual labor to annotate text.
To systematically reduce the necessary amount
of training data, we divided the task of construct-
ing extraction patterns into a subtask that general
natural language processing techniques can solve
and a subtask that has specific properties accord-
ing to the information to be extracted. The former
subtask is of full parsing (i.e. recognizing syntactic
structures of sentences), and the latter subtask is of
constructing specific extraction patterns (i.e. find-
ing clue words to extract information) based on the
obtained syntactic structures.
We adopted full parsing from various levels
of parsing, because we believe that it offers the
best utility to generalize sentences into normal-
ized syntactic relations. We also divided patterns
into components to improve recall and we intro-
duced machine learning with a Support Vector
Machine (SVM) to learn a prediction model us-
ing the matching results of extraction patterns. As
an actual IE task, we extracted pairs of interacting
protein names from biomedical text.
2 Full Parsing
2.1 Necessity for Full Parsing
A technique that many previous approaches have
used is shallow parsing (Koike et al, 2003; Yao
et al, 2004; Zhou et al, 2005). Their assertion is
284
Distance Count (%) Sum (%)
?1 54 5.0 5.0
0 8 0.7 5.7
1 170 15.7 21.4
2?5 337 31.1 52.5
6?10 267 24.6 77.1
11? 248 22.9 100.0
Distance ?1 means protein word has been annotated as in-
teracting with itself (e.g. ?actin polymerization?). Distance 0
means words of the interacting proteins are directly next to
one another. Multi-word protein names are concatenated as
long as they do not cross tags to annotate proteins.
Table 1: Distance between Interacting Proteins
that shallow parsers are more robust and would be
sufficient for IE. However, their claims that shal-
low parsers are sufficient, or that full parsers do
not contribute to application tasks, have not been
fully proved by experimental results.
Zhou et al (2005) argued that most informa-
tion useful for IE derived from full parsing was
shallow. However, they only used dependency
trees and paths on full parse trees in their experi-
ment. Such structures did not include information
of semantic subjects/objects, which full parsing
can recognize. Additionally, most relations they
extracted from the ACE corpus (Linguistic Data
Consortium, 2005) on broadcasts and newswires
were within very short word-distance (70% where
two entities are embedded in each other or sep-
arated by at most one word), and therefore shal-
low information was beneficial. However, Table 1
shows that the word distance is long between in-
teracting protein names annotated on the AImed
corpus (Bunescu and Mooney, 2004), and we have
to treat long-distance relations for information like
protein-protein interactions.
Full parsing is more effective for acquiring gen-
eralized data from long-length words than shallow
parsing. The sentences at left in Figure 1 exem-
plify the advantages of full parsing. The gerund
?activating? in the last sentence takes a non-local
semantic subject ?ENTITY1?, and shallow parsing
cannot recognize this relation because ?ENTITY1?
and ?activating? are in different phrases. Full pars-
ing, on the other hand, can identify both the sub-
ject of the whole sentence and the semantic subject
of ?activating? have been shared.
2.2 Predicate-argument Structures
We applied Enju (Tsujii Laboratory, 2005a) as
a full parser which outputs predicate-argument
structures (PASs). PASs are well normalized
forms that represent syntactic relations. Enju
is based on Head-driven Phrase Structure Gram-
mar (Sag and Wasow, 1999), and it has been
trained on the Penn Treebank (PTB) (Marcus et
al., 1994) and a biomedical corpus, the GENIA
Treebank (GTB) (Tsujii Laboratory, 2005b). We
used a part-of-speech (POS) tagger trained on the
GENIA corpus (Tsujii Laboratory, 2005b) as a
preprocessor for Enju. On predicate-argument re-
lations, Enju achieved 88.0% precision and 87.2%
recall on PTB, and 87.1% precision and 85.4% re-
call on GTB.
The illustration at right in Figure 1 is a PAS
example, which represents the relation between
?activate?, ?ENTITY1? and ?ENTITY2? of all sen-
tences to the left. The predicate and its argu-
ments are words converted to their base forms,
augmented by their POSs. The arrows denote
the connections from predicates to their arguments
and the types of arguments are indicated as arrow
labels, i.e., ARGn (n = 1, 2, . . .), MOD. For ex-
ample, the semantic subject of a transitive verb is
ARG1 and the semantic object is ARG2.
What is important here is, thanks to the strong
normalization of syntactic variations, that we can
expect that the construction algorithm for extract-
ing patterns that works on PASs will need a much
smaller training corpus than those working on
surface-word sequences. Furthermore, because of
the reduced diversity of surface-word sequences at
the PAS level, any IE system at this level should
demonstrate improved recall.
3 Related Work
Sudo et al (2003), Culotta and Sorensen (2004)
and Bunescu and Mooney (2005) acquired sub-
structures derived from dependency trees as ex-
traction patterns for IE in general domains. Their
approaches were similar to our approach using
PASs derived from full parsing. However, one
problem with their systems is that they could
not treat non-local dependencies such as seman-
tic subjects of gerund constructions (discussed in
Section 2), and thus rules acquired from the con-
structions were partial.
Bunescu and Mooney (2006) also learned ex-
traction patterns for protein-protein interactions
by SVM with a generalized subsequence kernel.
Their patterns are sequences of words, POSs, en-
tity types, etc., and they heuristically restricted
length and word positions of the patterns. Al-
285
ENTITY1 recognizes and activates ENTITY2.
ENTITY2 activated by ENTITY1 are not well characterized.
The herpesvirus encodes a functional ENTITY1 that activates human ENTITY2.
ENTITY1 can functionally cooperate to synergistically activate ENTITY2.
The ENTITY1 plays key roles by activating ENTITY2.
ENTITY1/NN
activate/VB ENTITY2/NN
ARG1 ARG2
Figure 1: Syntactical Variations of ?activate?
though they achieved about 60% precision and
about 40% recall, these heuristic restrictions could
not be guaranteed to be applied to other IE tasks.
Hao et al (2005) learned extraction patterns
for protein-protein interactions as sequences of
words, POSs, entity tags and gaps by dynamic
programming, and reduced/merged them using a
minimum description length-based algorithm. Al-
though they achieved 79.8% precision and 59.5%
recall, sentences in their test corpus have too
many positive instances and some of the pat-
terns they claimed to have been successfully con-
structed went against linguistic or biomedical in-
tuition. (e.g. ?ENTITY1 and interacts with EN-
TITY2? should be replaced by a more general pat-
tern because they aimed to reduce the number of
patterns.)
4 Method
We automatically construct patterns to extract
protein-protein interactions from an annotated
training corpus. The corpus needs to be tagged to
denote which protein words are interacting pairs.
We follow five steps in constructing extraction
patterns from the training corpus. (1) Sentences
in the training corpus are parsed into PASs and
we extract raw patterns from the PASs. (2) We
divide the raw patterns to generate both combi-
nation and fragmental patterns. Because obtained
patterns include inappropriate ones (wrongly gen-
erated or too general), (3) we apply both kinds of
patterns to PASs of sentences in the training cor-
pus, (4) calculate the scores for matching results
of combination patterns, and (5) make a prediction
model with SVM using these matching results and
scores.
We extract pairs of interacting proteins from a
target text in the actual IE phase, in three steps.
(1) Sentences in the target corpus are parsed into
PASs. (2) We apply both kinds of extraction pat-
terns to these PASs and (3) calculate scores for
combination pattern matching. (4) We use the pre-
diction model to predict interacting pairs.
ENTITY1
ENTITY2
CD4/NN protein/NN
interact/VB
with/IN polymorphic/JJ
region/NN
of/INMHCII/NN
MOD ARG1 ARG1 ARG2 ARG1 ARG2
ARG1
Parsing Result
Raw Pattern
CD4 protein interacts with polymorphic regions of MHCII .
ENTITY1
ENTITY2
Sentence in Training Corpus
protein/NN
interact/VB
with/IN
region/NN
of/IN
MOD ARG1 ARG1 ARG2 ARG1 ARG2
(1) (2) (3) (4) (5) (6)
p
0
p
1
p
2
p
3
p
4
p
5
p
6
ENTITY2/NN
ENTITY1/NN
Figure 2: Extraction of Raw Pattern
4.1 Full Parsing and Extraction of Raw
Patterns
As the first step in both the construction phase and
application phase of extraction patterns, we parse
sentences into PASs using Enju.1 We label all
PASs of the protein names as protein PASs.
After parsing, we extract the smallest set of
PASs, which connect words that denote interact-
ing proteins, and make it a raw pattern. We take
the same method to extract and refine raw patterns
as Yakushiji et al (2005). Connecting means we
can trace predicate-argument relations from one
protein word to the other in an interacting pair.
The procedure to obtain a raw pattern (p0, . . . , pn)
is as follows:
predicate(p): PASs that have p as their argument
argument(p): PASs that p has as its arguments
1. pi = p0 is the PAS of a word correspondent
to one of interacting proteins, and we obtain
candidates of the raw pattern as follows:
1-1. If pi is of the word of the other interact-
ing protein, (p0, . . . , pi) is a candidate
of the raw pattern.
1-2. If not, make pattern candidates
for each pi+1 ? predicate(pi) ?
argument(pi) ? {p0, . . . , pi} by
returning to 1-1.
2. Select the pattern candidate of the smallest
set as the raw pattern.
1Before parsing, we concatenate each multi-word protein
name into the one word as long as the concatenation does not
cross name boundaries.
286
3. Substitute variables (ENTITY1, ENTITY2) for
the predicates of PASs correspondent to the
interacting proteins.
The lower part of Figure 2 shows an example
of the extraction of a raw pattern. ?CD4? and
?MHCII? are words representing interacting pro-
teins. First, we set the PAS of ?CD4? as p0.
argument(p0) includes the PAS of ?protein?, and
we set it as p1 (in other words, tracing the arrow
(1)). Next, predicate(p1) includes the PAS of ?in-
teract? (tracing the arrow (2) back), so we set it
as p2. We continue similarly until we reach the
PAS of ?MHCII? (p6). The result of the extracted
raw pattern is the set of p0, . . . , p6 with substitut-
ing variables ENTITY1 and ENTITY2 for ?CD4?
and ?MHCII?.
There are some cases where an extracted raw
pattern is not appropriate and we need to re-
fine it. One case is when unnecessary coordi-
nations/parentheses are included in the pattern,
e.g. two interactions are described in a combined
representation (?ENTITY1 binds this protein and
ENTITY2?). Another is when two interacting pro-
teins are connected directly by a conjunction or
only one protein participates in an interaction. In
such cases, we refine patterns by unfolding of co-
ordinations/parentheses and extension of patterns,
respectively. We have omitted detailed explana-
tions because of space limitations. The details are
described in the work of Yakushiji et al (2005).
4.2 Division of Patterns
Division for generating combination patterns is
based on observation of Yakushiji et al (2005) that
there are many cases where combinations of verbs
and certain nouns form IE patterns. In the work
of Yakushiji et al (2005), we divided only patterns
that include only one verb. We have extended the
division process to also treat nominal patterns or
patterns that include more than one verb.
Combination patterns are not appropriate for
utilizing individual word information because they
are always used in rather strictly combined ways.
Therefore we have newly introduced fragmental
patterns which consist of independent PASs from
raw patterns, in order to use individual word infor-
mation for higher recall.
4.2.1 Division for Generating Combination
Patterns
Raw patterns are divided into some compo-
nents and the components are combined to con-
ENTITY1/NN protein/NN interact/VBwith/IN region/NN
of/IN
ENTITY2/NN
MOD ARG1 ARG1 ARG2 ARG1 ARG2
*/VBwith/IN
ARG1ARG2
*/NN
ENTITY/NN
protein/NN
MOD
region/NN of/INENTITY/NN
ARG1
ARG2
interact/VB
ARG1
=
*/NN
*/VB
ARG1
*/NN
=
$X
$X
Main
Prep
Entity
Entity
Entity
MainEntity Main
Main
Entity
Raw Pattern
Combination Pattern
Figure 3: Division of Raw Pattern into Combina-
tion Pattern Components (Entity-Main-Entity)
struct combination patterns according to types of
the division. There are three types of division of
raw patterns for generating combination patterns.
These are:
(a) Two-entity Division
(a-1) Entity-Main-Entity Division
(a-2) Main-Entity-Entity Division
(b) Single-entity Division, and
(c) No Division (Naive Patterns).
Most raw patterns, where entities are at both
ends of the patterns, are divided into Entity-Main-
Entity. Main-Entity-Entity are for the cases where
there are PASs other than entities at the ends of
the patterns (e.g. ?interaction between ENTITY1
and ENTITY2?). Single-entity is a special Main-
Entity-Entity for interactions with only one partic-
ipant (e.g. ?ENTITY1 dimerization?).
There is an example of Entity-Main-Entity divi-
sion in Figure 3. First, the main component from
the raw pattern is the syntactic head PAS of the
raw pattern. If the raw pattern corresponds to a
sentence, the syntactic head PAS is the PAS of the
main verb. We underspecify the arguments of the
main component, to enable them to unify with the
PASs of any words with the same POSs. Next, if
there are PASs of prepositions connecting to the
main component, they become prep components.
If there is no PAS of a preposition next to the main
component on the connecting link from the main
component to an entity, we make the pseudo PAS
of a null preposition the prep component. The left
prep component ($X) in Figure 3 is a pseudo PAS
of a null preposition. We also underspecify the ar-
guments of prep components. Finally, the remain-
ing two parts, which are typically noun phrases, of
the raw pattern become entity components. PASs
287
corresponding to the entities of the original pair
are labeled as only unifiable with the entities of
other pairs.
Main-Entity-Entity division is similar, except
we distinguish only one prep component as a
double-prep component and the PAS of the coor-
dinate conjunction between entities becomes the
coord component. Single-entity division is simi-
lar to Main-Entity-Entity division and the differ-
ence is that single-entity division produces no co-
ord and one entity component. Naive patterns are
patterns without division, where no division can be
applied (e.g. ?ENTITY1/NN in/IN complexes/NN
with/IN ENTITY2/NN?).
All PASs on boundaries of components are la-
beled to determine which PAS on a boundary of
another component can be unified. Labels are rep-
resented by subscriptions in Figure 3. These re-
strictions on component connection are used in the
step of constructing combination patterns.
Constructing combination patterns by combin-
ing components is equal to reconstructing orig-
inal raw patterns with the original combination
of components, or constructing new raw patterns
with new combinations of components. For exam-
ple, an Entity-Main-Entity pattern is constructed
by combination of any main, any two prep and any
two entity components. Actually, this construction
process by combination is executed in the pattern
matching step. That is, we do not off-line con-
struct all possible combination patterns from the
components and only construct the combination
patterns that are able to match the target.
4.2.2 Division for Generating Fragmental
Patterns
A raw pattern is splitted into individual PASs
and each PAS becomes a fragmental pattern. We
also prepare underspecified patterns where one or
more of the arguments of the original are under-
specified, i.e., are able to match any words of
the same POSs and the same label of protein/not-
protein. We underspecify the PASs of entities in
fragmental patterns to enable them to unify with
any PASs with the same POSs and a protein la-
bel, although in combination patterns we retain the
PASs of entities as only unifiable with entities of
pairs. This is because fragmental patterns are de-
signed to be less strict than combination patterns.
4.3 Pattern Matching
Matching of combination patterns is executed as
a process to match and combine combination pat-
tern components according to their division types
(Entity-Main-Entity, Main-Entity-Entity, Single-
entity and No Division). Fragmental matching is
matching all fragmental patterns to PASs derived
from sentences.
4.4 Scoring for Combination Matching
We next calculate the score of each combination
matching to estimate the adequacy of the combina-
tion of components. This is because new combina-
tion of components may form inadequate patterns.
(e.g. ?ENTITY1 be ENTITY2? can be formed of
components from ?ENTITY1 be ENTITY2 recep-
tor?.) Scores are derived from the results of com-
bination matching to the source training corpus.
We apply the combination patterns to the train-
ing corpus, and count pairs of True Positives (TP)
and False Positives (FP). The scores are calculated
basically by the following formula:
Score = TP/(TP + FP ) + ? ? TP
This formula is based on the precision of the pat-
tern on the training corpus, i.e., an estimated pre-
cision on a test corpus. ? works for smoothing,
that is, to accept only patterns of large TP when
FP = 0. ? is set as 0.01 empirically. The formula
is similar to the Apriori algorithm (Agrawal and
Srikant, 1995) that learns association rules from a
database. The first term corresponds to the confi-
dence of the algorithm, and the second term corre-
sponds to the support.
For patterns where TP = FP = 0, which
are not matched to PASs in the training corpus
(i.e., newly produced by combinations of com-
ponents), we estimates TP ? and FP ? by using
the confidence of the main and entity compo-
nents. This is because main and entity components
tend to contain pattern meanings, whereas prep,
double-prep and coord components are rather
functional. The formulas to calculate the scores
for all cases are:
Score =
8
>
>
>
<
>
>
>
:
TP/(TP + FP ) + ? ? TP
(TP + FP ?= 0)
TP ?/(TP ? + FP ?)
(TP = FP = 0, TP ? + FP ? ?= 0)
0 (TP = FP = TP ? = FP ? = 0)
288
Combination Pattern
(1) Combination of components in combination
matching
(2) Main component in combination matching
(3) Entity components in combination matching
(4) Score for combination matching (SCORE)
Fragmental Pattern
(5) Matched fragmental patterns
(6) Number of PASs of example that are not matched
in fragmental matching
Raw Pattern
(7) Length of raw pattern derived from example
Table 2: Features for SVM Learning of Prediction
Model
TP ? =
8
>
<
>
:
TP ?main + TP ?entity1(+TP ?entity2)
(for Two-entity, Single-entity)
0 (for Naive)
FP ? = (similar to TP ? but TP ?x is replaced by FP ?x)
TP ?main =
8
>
>
>
>
>
<
>
>
>
>
>
:
TPmain:two/(TPmain:two + FPmain:two)
 
TPmain:two + FPmain:two ?= 0,
for Two-entity
!
TPmain:single/(TPmain:single + FPmain:single)
 
TPmain:single + FPmain:single ?= 0,
for Single-entity
!
0 (other cases)
TP ?entityi =
8
>
<
>
:
TPentityi/(TPentityi + FPentityi)
?
TPentityi + FPentityi ?= 0
?
0 (other cases)
FP ?x =
?
similar to TP ?x but TP ?y in the
numerators is replaced by FP ?y
?
? TP : number of TPs by the combination of components
? TPmain:two: sum of TPs by two-entity combinations
that include the same main component
? TPmain:single: sum of TPs by single-entity combina-
tions that include the same main component
? TPentityi: sum of TPs by combinations that include
the same entity component which is not the straight en-
tity component
? FPx: similar to TPx but TP is replaced by FP
The entity component ?ENTITY/NN?, which
only consists of the PAS of an entity, adds no infor-
mation to combinations of components. We call
this component a straight entity component and
exclude its effect from the scores.
4.5 Construction of Prediction Model
We use an SVM to learn a prediction model to de-
termine whether a new protein pair is interacting.
We used SV M light (Joachims, 1999) with an rbf
kernel, which is known as the best kernel for most
tasks. The prediction model is based on the fea-
tures of Table 2.
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
Prec
ision
Recall
ALLSCOREERK
Figure 4: Results of IE Experiment
ENTITY1
FGF-2/NN
bind well to FGFR1 but
interact/VB with/IN
poorly/RB
ENTITY2
KGFR/NN
ARG1 ARG1 ARG2
Figure 5: Example Demonstrating Advantages of
Full Parsing
5 Results and Discussion
5.1 Experimental Results on the AImed
Corpus
To evaluate extraction patterns automatically con-
structed with our method, we used the AImed cor-
pus, which consists of 225 MEDLINE (U.S. Na-
tional Library of Medicine, 2006) abstracts (1969
sentences) annotated with protein names and
protein-protein interactions, for the training/test
corpora. We used tags for the protein names given.
We measured the accuracy of the IE task using
the same criterion as Bunescu andMooney (2006),
who used an SVM to construct extraction patterns
on word/POS/type sequences from the AImed cor-
pus. That is, an extracted interaction from an ab-
stract is correct if the proteins are tagged as inter-
acting with each other somewhere in that abstract
(document-level measure).
Figure 4 plots our 10-fold cross validation and
the results of Bunescu and Mooney (2006). The
line ALL represents results when we used all fea-
tures for SVM learning. The line SCORE repre-
sents results when we extracted pairs with higher
combination matching scores than various thresh-
old values. And the line ERK represents results
by Bunescu and Mooney (2006).
The line ALL obtained our best overall F-
measure 57.3%, with 71.8% precision and 48.4%
recall. Our method was significantly better than
Bunescu and Mooney (2006) for precision be-
289
tween 50% and 80%. It also needs to be noted
that SCORE, which did not use SVM learning
and only used the combination patterns, achieved
performance comparable to that by Bunescu and
Mooney (2006) for the precision range from 50%
to 80%. And for this range, introducing the frag-
mental patterns with SVM learning raised the re-
call. This range of precision is practical for the
IE task, because precision is more important than
recall for significant interactions that tend to be
described in many abstracts (as shown by the
next experiment), and too-low recall accompa-
nying too-high precision requires an excessively
large source text.
Figure 5 shows the advantage of introducing
full parsing. ?FGF-2? and ?KGFR? is an interact-
ing protein pair. The pattern ?ENTITY1 interact
with ENTITY2? based on PASs successfully ex-
tracts this pair. However, it is difficult to extract
this pair with patterns based on surface words, be-
cause there are 5 words between ?FGF-2? and ?in-
teract?.
5.2 Experimental Results on Abstracts of
MEDLINE
We also conducted an experiment to extract in-
teracting protein pairs from a large amount of
biomedical text, i.e. about 14 million titles and
8 million abstracts in MEDLINE. We constructed
combination patterns from all 225 abstracts of the
AImed corpus, and calculated a threshold value
of combination scores that produced about 70%
precision and 30% recall on the training corpus.
We extracted protein pairs with higher combi-
nation scores than the threshold value. We ex-
cluded single-protein interactions to reduce time
consumption and we used a protein name recog-
nizer in this experiment2.
We compared the extracted pairs with a man-
ually curated database, Reactome (Joshi-Tope et
al., 2005), which published 16,564 human pro-
tein interaction pairs as pairs of Entrez Gene
IDs (U.S. National Library of Medicine, 2006).
We converted our extracted protein pairs into pairs
of Entrez Gene IDs by the protein name recog-
nizer.3 Because there may be pairs missed by Re-
2Because protein names were recognized after the pars-
ing, multi-word protein names were not concatenated.
3Although the same protein names are used for humans
and other species, these are considered to be human proteins
without checking the context. This is a fair assumption be-
cause Reactome itself infers human interaction events from
experiments on model organisms such as mice.
Total 89
Parsing Error/Failure 35
(Related to coordinations) (14)
Lack of Combination Pattern Component 33
Requiring Anaphora Resolution 9
Error in Prediction Model 8
Requiring Attributive Adjectives 5
Others 10
More than one cause can occur in one error, thus the sum of
all causes is larger than the total number of False Negatives.
Table 3: Causes of Error for FNs
actome or pairs that our processed text did not in-
clude, we excluded extracted pairs of IDs that are
not included in Reactome and excluded Reactome
pairs of IDs that do not co-occur in the sentences
of our processed text.
After this postprocessing, we found that we had
extracted 7775 human protein pairs. Of them, 155
pairs were also included in Reactome ([a] pseudo
TPs) and 7620 pairs were not included in Reac-
tome ([b] pseudo FPs). 947 pairs of Reactome
were not extracted by our system ([c] pseudo False
Negatives (FNs)). However, these results included
pairs that Reactome missed or those that only co-
occurred and were not interacting pairs in the text.
There may also have been errors with ID assign-
ment.
To determine such cases, a biologist investi-
gated 100 pairs randomly selected from pairs of
pseudo TPs, FPs and FNs retaining their ratio of
numbers. She also checked correctness of the as-
signed IDs. 2 pairs were selected from pseudo
TPs, 88 pairs were from pseudo FPs and 10 pairs
were from pseudo FNs. The biologist found that
57 pairs were actual TPs (2 pairs of pseudo TPs
and 55 pairs of pseudo FPs) and 32 pairs were ac-
tual FPs of the pseudo FPs. Thus, the precision
was 64.0% in this sample set. Furthermore, even
if we assume that all pseudo FNs are actual FNs,
the recall can be estimated by actual TPs / (actual
TPs + pseudo FNs) ? 100 = 83.8%.
These results mean that the recall of an IE sys-
tem for interacting proteins is improved for a large
amount of text even if it is low for a small corpus.
Thus, this justifies our assertion that a high degree
of precision in the low-recall range is important.
5.3 Error Analysis
Tables 3 and 4 list causes of error for FNs/FPs on
a test set of the AImed corpus using the predic-
tion model with the best F-measure with all the
290
Total 35
Requiring Attributive Adjectives 13
Corpus Error 11
Error in Prediction Model 5
Requiring Negation Words 2
Parsing Error 1
Others 3
Table 4: Causes of Error for FPs
features. Different to Subsection 5.1, we individ-
ually checked each occurring pair of interacting
proteins. The biggest problems were parsing er-
ror/failure, lack of necessary patterns and learning
of inappropriate patterns.
5.3.1 Parsing Error
As listed in Table 3, 14 (40%) of the 35 pars-
ing errors/failures were related to coordinations.
Many of these were caused by differences in the
characteristics of the PTB/GTB, the training cor-
pora for Enju, and the AImed Corpus. For ex-
ample, Enju failed to obtain the correct structure
for ?the ENTITY1 / ENTITY1 complex? because
words in the PTB/GTB are not segmented with
?/? and Enju could not be trained on such a case.
One method to solve this problem is to avoid seg-
menting words with ?/? and introducing extraction
patterns based on surface characters, such as ?EN-
TITY1/ENTITY2 complex?.
Parsing errors are intrinsic problems to IE meth-
ods using parsing. However, from Table 3, we can
conclude that the key to gaining better accuracy
is refining of the method with which the PAS pat-
terns are constructed (there were 46 related FNs)
rather than improving parsing (there were 35 FNs).
5.3.2 Lack of Necessary Patterns and
Learning of Inappropriate Patterns
There are two different reasons causing the
problems with the lack of necessary patterns and
the learning of inappropriate patterns: (1) the
training corpus was not sufficiently large to sat-
urate IE accuracy and (2) our method of pattern
construction was too limited.
Effect of Training Corpus Size To investigate
whether the training corpus was large enough to
maximize IE accuracy, we conducted experiments
on training corpora of various sizes. Figure 6 plots
graphs of F-measures by SCORE and Figure 7
plots the number of combination patterns on train-
ing corpora of various sizes. From Figures 6 and 7,
the training corpus (207 abstracts at a maximum)
 0.35
 0.4
 0.45
 0.5
 0.55
 0  50  100  150  200
F-m
eas
ure
 by
 SC
OR
E
Training Corpus Size (Number of Abstracts)
Figure 6: Effect of Training Corpus Size (1)
 0
 100
 200
 300
 400
 500
 600
 0  50  100  150  200
Nu
mb
er
Training Corpus Size (Number of Abstracts)
Raw Patterns (before division)Main ComponentEntity ComponentOther ComponentNaive Pattern
Figure 7: Effect of Training Corpus Size (2)
is not large enough. Thus increasing corpus size
will further improve IE accuracy.
Limitation of the Present Pattern Construc-
tion The limitations with our pattern construc-
tion method are revealed by the fact that we
could not achieve a high precision like Bunescu
and Mooney (2006) within the high-recall range.
Compared to theirs, one of our problems is that our
method could not handle attributives. One exam-
ple is ?binding property of ENTITY1 to ENTITY2?.
We could not obtain ?binding? because the small-
est set of PASs connecting ?ENTITY1? and ?EN-
TITY2? includes only the PASs of ?property?, ?of?
and ?to?. To handle these attributives, we need dis-
tinguish necessary attributives from those that are
general4 by semantic analysis or bootstrapping.
Another approach to improve our method is to
include local information in sentences, such as
surface words between protein names. Zhao and
Grishman (2005) reported that adding local infor-
mation to deep syntactic information improved IE
results. This approach is also applicable to IE in
other domains, where related entities are in a short
4Consider the case where a source sentence for a pattern is
?ENTITY1 is an important homodimeric protein.? (?homod-
imeric? represents that two molecules of ?ENTITY1? interact
with each other.)
291
distance like the work of Zhou et al (2005).
6 Conclusion
We proposed the use of PASs to construct pat-
terns as extraction rules, utilizing their ability to
abstract syntactical variants with the same rela-
tion. In addition, we divided the patterns for gen-
eralization, and used matching results for SVM
learning. In experiments on extracting of protein-
protein interactions, we obtained 71.8% precision
and 48.4% recall on a small corpus and 64.0% pre-
cision and 83.8% recall estimated on a large text,
which demonstrated the obvious advantages of our
method.
Acknowledgement
This work was partially supported by Grant-in-Aid
for Scientific Research on Priority Areas ?Sys-
tems Genomics? (MEXT, Japan) and Solution-
Oriented Research for Science and Technology
(JST, Japan).
References
R. Agrawal and R Srikant. 1995. Mining Sequential
Patterns. In Proc. the 11th International Conference
on Data Engineering, pages 3?14.
Christian Blaschke and Alfonso Valencia. 2002. The
Frame-Based Module of the SUISEKI Informa-
tion Extraction System. IEEE Intelligent Systems,
17(2):14?20.
Razvan Bunescu and Raymond J. Mooney. 2004.
Collective information extraction with relational
markov networks. In Proc. ACL?04, pages 439?446.
Razvan C. Bunescu and Raymond J. Mooney. 2005. A
Shortest Path Dependency Kernel for Relation Ex-
traction. In Proc. HLT/EMNLP 2005, pages 724?
731.
Razvan Bunescu and Raymond Mooney. 2006. Subse-
quence kernels for relation extraction. In Advances
in Neural Information Processing Systems 18, pages
171?178. MIT Press.
Aron Culotta and Jeffrey Sorensen. 2004. Dependency
tree kernels for relation extraction. In Proc. ACL?04,
pages 423?429.
Yu Hao, Xiaoyan Zhu, Minlie Huang, and Ming
Li. 2005. Discovering patterns to extract protein-
protein interactions from the literature: Part II.
Bioinformatics, 21(15):3294?3300.
Thorsten Joachims. 1999. Making Large-Scale SVM
Learning Practical. In Advances in Kernel Methods
? Support Vector Learning. MIT-Press.
G Joshi-Tope, M Gillespie, I Vastrik, P D?Eustachio,
E Schmidt, B de Bono, B Jassal, GR Gopinath,
GR Wu, L Matthews, S Lewis, E Birney, and Stein
L. 2005. Reactome: a knowledgebase of biologi-
cal pathways. Nucleic Acids Research, 33(Database
Issue):D428?D432.
Asako Koike, Yoshiyuki Kobayashi, and Toshihisa
Takagi. 2003. Kinase Pathway Database: An
Integrated Protein-Kinase and NLP-Based Protein-
Interaction Resource. Genome Research, 13:1231?
1243.
Linguistic Data Consortium. 2005. ACE Program.
http://projects.ldc.upenn.edu/ace/.
Mitchell Marcus, Grace Kim, Mary Ann
Marcinkiewicz, Robert MacIntyre, Ann Bies,
Mark Ferguson, Karen Katz, and Britta Schas-
berger. 1994. The Penn Treebank: Annotating
predicate argument structure. In Proc. AAI ?94.
Ivan A. Sag and Thomas Wasow. 1999. Syntactic The-
ory. CSLI publications.
Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman.
2003. An improved extraction pattern representa-
tion model for automatic IE pattern acquisition. In
Proc. ACL 2003, pages 224?231.
Tsujii Laboratory. 2005a. Enju - A practical HPSG
parser. http://www-tsujii.is.s.u-tokyo.ac.jp/enju/.
Tsujii Laboratory. 2005b. GENIA Project.
http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/.
U.S. National Library of Medicine. 2006. PubMed.
http://www.pubmed.gov.
Akane Yakushiji, Yusuke Miyao, Yuka Tateisi, and
Jun?ichi Tsujii. 2005. Biomedical information ex-
traction with predicate-argument structure patterns.
In Proc. SMBM 2005, pages 60?69.
Daming Yao, Jingbo Wang, Yanmei Lu, Nathan No-
ble, Huandong Sun, Xiaoyan Zhu, Nan Lin, Don-
ald G. Payan, Ming Li, and Kunbin Qu. 2004. Path-
wayFinder: Paving The Way Towards Automatic
Pathway Extraction. In Bioinformatics 2004: Proc.
the 2nd APBC, volume 29 of CRPIT, pages 53?62.
Shubin Zhao and Ralph Grishman. 2005. Extracting
relations with integrated information using kernel
methods. In Proc. ACL?05, pages 419?426.
GuoDong Zhou, Jian Su, Jie Zhang, and Min Zhang.
2005. Exploring various knowledge in relation ex-
traction. In Proc. ACL?05, pages 427?434.
292
A Debug Tool for Practical Grammar Development
Akane Yakushiji? Yuka Tateisi?? Yusuke Miyao?
?Department of Computer Science, University of Tokyo
Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033 JAPAN
?CREST, JST (Japan Science and Technology Corporation)
Honcho 4-1-8, Kawaguchi-shi, Saitama 332-0012 JAPAN
{akane,yucca,yusuke,yoshinag,tsujii}@is.s.u-tokyo.ac.jp
Naoki Yoshinaga? Jun?ichi Tsujii??
Abstract
We have developed willex, a tool that
helps grammar developers to work effi-
ciently by using annotated corpora and
recording parsing errors. Willex has two
major new functions. First, it decreases
ambiguity of the parsing results by com-
paring them to an annotated corpus and
removing wrong partial results both au-
tomatically and manually. Second, willex
accumulates parsing errors as data for the
developers to clarify the defects of the
grammar statistically. We applied willex
to a large-scale HPSG-style grammar as
an example.
1 Introduction
There is an increasing need for syntactical parsers
for practical usages, such as information extrac-
tion. For example, Yakushiji et al (2001) extracted
argument structures from biomedical papers using
a parser based on XHPSG (Tateisi et al, 1998),
which is a large-scale HPSG. Although large-scale
and general-purpose grammars have been devel-
oped, they have a problem of limited coverage.
The limits are derived from deficiencies of gram-
mars themselves. For example, XHPSG cannot treat
coordinations of verbs (ex. ?Molybdate slowed but
did not prevent the conversion.?) nor reduced rel-
atives (ex. ?Rb mutants derived from patients with
retinoblastoma.?). Finding these grammar defects
and modifying them require tremendous human ef-
fort.
Hence, we have developed willex that helps to im-
prove the general-purpose grammars. Willex has two
major functions. First, it reduces a human workload
to improve the general-purpose grammar through
using language intuition encoded in syntactically
tagged corpora in XML format. Second, it records
data of grammar defects to allow developers to have
a whole picture of parsing errors found in the target
corpora to save debugging time and effort by priori-
tizing them.
2 What Is the Ideal Grammar Debugging?
There are already other grammar developing tools,
such as a grammar writer of XTAG (Paroubek et al,
1992), ALEP (Schmidt et al, 1996), ConTroll (Go?tz
and Meurers, 1997), a tool by Nara Institute of Sci-
ence and Technology (Miyata et al, 1999), and [incr
tsdb()] (Oepen et al, 2002). But these tools have
following problems; they largely depend on human
debuggers? language intuition, they do not help users
to handle large amount of parsing results effectively,
and they let human debuggers correct the bugs one
after another manually and locally.
To cope with these shortcomings, willex proposes
an alternative method for more efficient debugging
process.
The workflow of the conventional grammar devel-
oping tools and willex are different in the following
ways. With the conventional tools, human debug-
gers must check each sentence to find out grammar
defects and modify them one by one. On the other
hand, with willex human debuggers check sentences
that are tagged with syntactical structure, one by
one, find grammar defects, and record them, while
willex collects the whole grammar defect records.
Then human debuggers modify the found grammar
defects. This process allows human debuggers to
make priority over defects that appear more fre-
quently in the corpora, or defects that are more crit-
ical for purposes of syntactical parsing. Indeed, it
is possible for human debuggers using the conven-
tional tools to collect and modify the defects but
willex saves the trouble of human debuggers to col-
lect defects to modify them more efficiently.
3 Functions of willex
To create the new debugging tool, we have extended
will (Imai et al, 1998). Will is a browser of parsing
results of grammars based on feature structures. Will
and willex are implemented in JAVA.
3.1 Using XML Tagged Corpora
Willex uses sentence boundaries, word chunking,
and POSs/labels encoded in XML tagged corpora.
First, with the information of sentence boundaries
and word chunking, ambiguity of sentences is re-
duced, and ambiguity at parsing phase is also re-
duced. A parser connected to willex is assumed to
produce only results consistent with the information.
An example is shown in Figure 1 (<su> is a senten-
tial tag and <np> is a tag for noun phrases).
I  saw  a girl  with a telescope
I  saw  a girl  with a telescope


<su> I saw <np> a girl with a telescope </np></su>
Figure 1: An example of pa sing results along with
word chunking
Next, willex compares POSs/labels encoded in
XML tags and parsing results, and deletes improper
parsing trees. Therefore, it reduces numbers of par-
tial parsing trees, which appear in the way of parsing
and should be checked by human debuggers. In ad-
dition, human debuggers can delete partial parsing
trees manually later. Figure 2 shows a concrete ex-
ample. (NP and S are labels for noun and sentential
phrases respectively.)
POS/label from Tagged Corpus
POSs/labels from Partial Results
<NP> A cat </NP> knows everything
A      cat
D      N N       V
A      cat
NP S  
Figure 2: An example of deletion by using
POSs/labels
3.2 Output of Grammar Defects
Willex has a function to output information of gram-
mar defects into a file in order to collect the de-
fects data and treat them statistically. In addition,
we can save a log of debugging experiences which
show what grammar defects are found.
An example of an output file is shown in Table
1. It includes sentence numbers, word ranges in
which parsing failed, and comments input by a hu-
man debugger. For example, the first row of the ta-
ble means that the sentence #0 has coordinations of
verb phrases at position #3?#12, which cannot be
parsed. ?OK? in the second row means the sen-
tence is parsed correctly (i.e., no grammar defects
are found in the sentence). The third row means that
the word #4 of the sentence #2 has no proper lexical
entry.
The word ranges are specified by human debug-
gers using a GUI, which shows parsing results in
CKY tables and parse trees. The comments are input
by human debuggers in a natural language or chosen
from the list of previous comments. A postprocess-
ing module of willex sorts the error data by the com-
ments to help statistical analysis.
Table 1: An example of file output
Sentence # Word # comment
0 3?12 V-V coordination
1 ? OK
2 4 no lexical entry
4 Experiments and Discussion
We have applied willex to rental-XTAG, an HPSG-
style grammar converted from the XTAG English
grammar (The XTAG Research Group, 2001) by a
grammar conversion (Yoshinaga and Miyao, 2001).1
The corpus used is MEDLINE abstracts with tags
based on a slightly modified version of GDA-
DTD2 (Hasida, 2003). The corpus is ?partially
parsed?; the attachments of prepositional phrases are
annotated manually.
The tags do not always specify the correct struc-
tures based on rental-XTAG (i.e., the grammar as-
sumed by tags is different from rental-XTAG), so we
prepared a POS/label conversion table. We can use
tagged corpora based on various grammars different
from the grammar that the parser is assuming by us-
ing POS/label conversion tables.
We investigated 208 sentences (average 24.2
words) from 26 abstracts. 73 sentences were parsed
successfully and got correct results. Thus the cover-
age was 35.1%.
4.1 Qualitative Evaluation
Willex received three major positive feedbacks from
a user; first, the function of restricting partial results
was helpful, as it allows human debuggers to check
fewer results, second, the function to delete incorrect
partial results manually was useful, because there
are some cases that tags do not specify POSs/labels,
and third, human debuggers could use the record-
ing function to make notes to analyze them carefully
later.
However, willex also received some negative eval-
uations; the process of locating the cause of pars-
ing failure in a sentence was found to be a bit trou-
blesome. Also, willex loses its accuracy if the hu-
man debuggers themselves have trouble understand-
ing the correct syntactical structure of a sentence.3
1Since XTAG and rental-XTAG generate equivalent parse
results for the same input, debugging rental-XTAG means de-
bugging XTAG itself.
2GDA has no tags which specify prepositional phrases, so
we add <prep> and <prepp>.
3Thus, we divided the process of identifying grammar de-
fects to two steps. First, a non-expert roughly classifies pars-
ing errors and records temporary memorandums. Then, the
non-expert shows typical examples of sentences in each class
to experts and identifies grammar defects based on experts? in-
ference. Here, we can make use of the recording function of
We found from these evaluations that the func-
tions of willex can be used effectively, though more
automation is needed.
4.2 Quantitative Evaluation
Figure 3 shows the decrease in partial parsing trees
caused by using the tagged corpus. (Data of 10 sen-
tences among the 208 sentences are shown.) The
graph shows that human workload was reduced by
using the tagged corpus.
0
5000
10000
15000
20000
25000
30000
35000
10 15 20 25 30 35 40
n
u
m
b
e
r
 
o
f
 
p
a
r
t
i
a
l
 
r
e
s
u
l
t
s
length of a sentence (number of words)
without any info.with chunk info.with chunk and POS/label info.
Figure 3: Examples of numbers of partial results
4.3 Defects of rental-XTAG
Table 2 shows the defects of rental-XTAG which are
found by using willex.
Table 2: The defects of rental-XTAG
the defects of rental-XTAG #
no lexical entry 62
cannot handle reduced relative 35
cannot handle V-V coordination 22
Adjective does not post-modify NP 9
cannot parse ?, but not? 4
cannot handle objective to-infinitive 3
?, which ...? does not post-modify NP 3
cannot handle reduced as-relative clause 2
cannot parse ?greater than?(?>?) 2
misc. 17
From this table, it is inferred that (1) lack of lexi-
cal entries, (2) inability to parse reduced relative and
willex.
(3) inability to parse coordinations of verbs are seri-
ous problems of rental-XTAG.
4.4 Conflicts Between the Modified GDA and
rental-XTAG
Conflicts between rental-XTAG and the grammar on
which the modified GDA based cause parsing fail-
ures. Statistics of the conflicts is shown in Table 3.
Table 3: Conflicts between the modified GDA and
rental-XTAG
modified GDA rental-XTAG #
adjectival phrase verbal phrase 36
bracketing except ?,? 10
bracketing of ?,? 8
treatment of omitted words 2
misc. 5
These conflicts cannot be resolved by a simple
POS/label conversion table. One resolution is insert-
ing a preprocess module that deletes and moves tags
which cause conflicts.
We do not consider these conflicts as grammar de-
fects but the difference of grammars to be absorbed
in the conversion phase.
5 Conclusion and Future Work
We developed a debug tool, willex, which uses XML
tagged corpora and outputs information of grammar
defects. By using tagged corpora, willex succeeded
to reduce human workload. And by recording gram-
mar defects, it provides debugging environment with
a bigger perspective. But there remains a prob-
lem that a simple POS/label conversion table is not
enough to resolve conflicts of a debugged grammar
and a grammar assumed by tags. The tool should
support to handle the complicated conflicts.
In the future, we will try to modify willex to infer
causes of parsing errors (semi-)automatically. It is
difficult to find a point of parsing failure automati-
cally, because subsentences that have no correspon-
dent partial results are not always the failed point.
Hence, we will expand willex to find the longest
subsentences that are parsed successfully. Words,
POS/labels and features of the subsentences can be
clues to infer the causes of parsing errors.
References
Thilo Go?tz and Walt Detmar Meurers. 1997. The Con-
Troll system as large grammar development platform.
In Proc. of Workshop on Computational Environments
for Grammar Development and Linguistic Engineer-
ing, pages 38?45.
Koiti Hasida. 2003. Global docu-
ment annotation (GDA). available in
http://www.i-content.org/GDA/.
Hisao Imai, Yusuke Miyao, and Jun?ichi Tsujii. 1998.
GUI for an HPSG parser. In Information Processing
Society of Japan SIG Notes NL-127, pages 173?178,
September. In Japanese.
Takashi Miyata, Kazuma Takaoka, and Yuji Mat-
sumoto. 1999. Implementation of GUI debugger for
unification-based grammar. In Information Process-
ing Society of Japan SIG Notes NL-129, pages 87?94,
January. In Japanese.
Stephan Oepen, Emily M. Bender, Uli Callmeier, Dan
Flickinger, and Melanie Siegel. 2002. Parallel dis-
tributed grammar engineering for practical applica-
tions. In Proc. of the Workshop on Grammar Engi-
neering and Evaluation, pages 15?21.
Patrick Paroubek, Yves Schabes, and Aravind K. Joshi.
1992. XTAG ? a graphical workbench for developing
Tree-Adjoining grammars. In Proc. of the 3rd Confer-
ence on Applied Natural Language Processing, pages
216?223.
Paul Schmidt, Axel Theofilidis, Sibylle Rieder, and
Thierry Declerck. 1996. Lean formalisms, linguis-
tic theory, and applications. Grammar development in
ALEP. In Proc. of COLING ?96, volume 1, pages
286?291.
Yuka Tateisi, Kentaro Torisawa, Yusuke Miyao, and
Jun?ichi Tsujii. 1998. Translating the XTAG english
grammar to HPSG. In Proc. of TAG+4 workshop,
pages 172?175.
The XTAG Research Group. 2001. A Lex-
icalized Tree Adjoining Grammar for English.
Technical Report IRCS Research Report 01-03,
IRCS, University of Pennsylvania. available in
http://www.cis.upenn.edu/?xtag/.
Akane Yakushiji, Yuka Tateisi, Yusuke Miyao, and
Jun?ichi Tsujii. 2001. Event extraction from biomedi-
cal papers using a full parser. In Pacific Symposium on
Biocomputing 2001, pages 408?419, January.
Naoki Yoshinaga and Yusuke Miyao. 2001. Grammar
conversion from LTAG to HPSG. In Proc. of the sixth
ESSLLI Student Session, pages 309?324.
Finding Anchor Verbs for Biomedical IE
Using Predicate-Argument Structures
Akane YAKUSHIJI? Yuka TATEISI?? Yusuke MIYAO?
?Department of Computer Science, University of Tokyo
Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033 JAPAN
?CREST, JST (Japan Science and Technology Agency)
Honcho 4-1-8, Kawaguchi-shi, Saitama 332-0012 JAPAN
{akane,yucca,yusuke,tsujii}@is.s.u-tokyo.ac.jp
Jun?ichi TSUJII??
Abstract
For biomedical information extraction, most sys-
tems use syntactic patterns on verbs (anchor verbs)
and their arguments. Anchor verbs can be se-
lected by focusing on their arguments. We propose
to use predicate-argument structures (PASs), which
are outputs of a full parser, to obtain verbs and their
arguments. In this paper, we evaluated PAS method
by comparing it to a method using part of speech
(POSs) pattern matching. POS patterns produced
larger results with incorrect arguments, and the re-
sults will cause adverse effects on a phase selecting
appropriate verbs.
1 Introduction
Research in molecular-biology field is discovering
enormous amount of new facts, and thus there is
an increasing need for information extraction (IE)
technology to support database building and to find
novel knowledge in online journals.
To implement IE systems, we need to construct
extraction rules, i.e., rules to extract desired infor-
mation from processed resource. One subtask of the
construction is defining a set of anchor verbs, which
express realization of desired information in natural
language text.
In this paper, we propose a novel method of
finding anchor verbs: extracting anchor verbs from
predicate-argument structures (PASs) obtained by
full parsing. We here discuss only finding anchor
verbs, although our final purpose is construction
of extraction rules. Most anchor verbs take topi-
cal nouns, i.e., nouns describing target entities for
IE, as their arguments. Thus verbs which take top-
ical nouns can be candidates for anchor verbs. Our
method collects anchor verb candidates by choosing
PASs whose arguments are topical nouns. Then, se-
mantically inappropriate verbs are filtered out. We
leave this filtering phase as a future work, and dis-
cuss the acquisition of candidates. We have also in-
vestigated difference in verbs and their arguments
extracted by naive POS patterns and PAS method.
When anchor verbs are found based on whether
their arguments are topical nouns, like in (Hatzivas-
siloglou and Weng, 2002), it is important to obtain
correct arguments. Thus, in this paper, we set our
goal to obtain anchor verb candidates and their cor-
rect arguments.
2 Background
There are some works on acquiring extraction rules
automatically. Sudo et al (2003) acquired subtrees
derived from dependency trees as extraction rules
for IE in general domains. One problem of their sys-
tem is that dependency trees cannot treat non-local
dependencies, and thus rules acquired from the con-
structions are partial. Hatzivassiloglou and Weng
(2002) used frequency of collocation of verbs and
topical nouns and verb occurrence rates in several
domains to obtain anchor verbs for biological inter-
action. They used only POSs and word positions
to detect relations between verbs and topical nouns.
Their performance was 87.5% precision and 82.4%
recall. One of the reasons of errors they reported is
failures to detect verb-noun relations.
To avoid these problems, we decided to use PASs
obtained by full parsing to get precise relations be-
tween verbs and their arguments. The obtained pre-
cise relations will improve precision. In addition,
PASs obtained by full parsing can treat non-local
dependencies, thus recall will also be improved.
The sentence below is an example which sup-
ports advantage of full parsing. A gerund ?activat-
ing? takes a non-local semantic subject ?IL-4?. In
full parsing based on Head-Driven Phrase Structure
Grammar (HPSG) (Sag and Wasow, 1999), the sub-
ject of the whole sentence and the semantic subject
of ?activating? are shared, and thus we can extract
the subject of ?activating?.
IL-4 may mediate its biological effects by activat-
ing a tyrosine-phosphorylated DNA binding pro-
tein.
interacts
ARG1 it
1 with
MODIFY
ARG1      regions
2
1
of
MODIFY
ARG1 molecules
2
,
,
(a) (b) (c)
It interacts with non-polymorphic regions of major his-
tocompatibility complex class II molecules.
Figure 1: PAS examples
with
MODIFY
interacts
ARG1 it
ARG1 regions
Core verb
serves
ARG1      IL-5
1
ARG2
to
ARG1
ARG2
stimulate
ARG1
ARG2 binding
1
Core verb
1
Figure 2: Core verbs of PASs
3 Anchor Verb Finding by PASs
By using PASs, we extract candidates for anchor
verbs from a sentence in the following steps:
1. Obtain all PASs of a sentence by a full
parser. The PASs correspond not only to verbal
phrases but also other phrases such as preposi-
tional phrases.
2. Select PASs which take one or more topical
nouns as arguments.
3. From the selected PASs in Step 2, select PASs
which include one or more verbs.
4. Extract a core verb, which is the innermost ver-
bal predicate, from each of the chosen PASs.
In Step 1, we use a probabilistic HPSG parser
developed by Miyao et al (2003), (2004). PASs
obtained by the parser are illustrated in Figure 1.1
Bold words are predicates. Arguments of the predi-
cates are described in ARGn (n = 1, 2, . . .). MOD-
IFY denotes the modified PAS. Numbers in squares
denote shared structures. Examples of core verbs
are illustrated in Figure 2. We regard all arguments
in a PAS are arguments of the core verb.
Extraction of candidates for anchor verbs from
the sentence in Figure 1 is as follows. Here, ?re-
gions? and ?molecules? are topical nouns.
In Step 1, we obtain all the PASs, (a), (b) and (c),
in Figure 1.
1Here, named entities are regarded as chunked, and thus
internal structures of noun phrases are not illustrated.
Next, in Step 2, we check each argument of (a),
(b) and (c). (a) is discarded because it does not have
a topical noun argument.2 (b) is selected because
ARG1 ?regions? is a topical noun. Similarly, (c) is
selected because of ARG1 ?molecules?.
And then, in Step 3, we check each POS of a
predicate included in (b) and (c). (b) is selected be-
cause it has the verb ?interacts? in 1 which shares
the structure with (a). (c) is discarded because it
includes no verbs.
Finally, in Step 4, we extract a core verb from (b).
(b) includes 1 asMODIFY, and the predicate of 1
is the verb, ?interacts?. So we extract it.
4 Experiments
We investigated the verbs and their arguments ex-
tracted by PAS method and POS pattern matching,
which is less expressive in analyzing sentence struc-
tures but would be more robust.
For topical nouns and POSs, we used the GENIA
corpus (Kim et al, 2003), a corpus of annotated ab-
stracts taken from National Library of Medicine?s
MEDLINE database. We defined topical nouns as
the names tagged as protein, peptide, amino acid,
DNA, RNA, or nucleic acid. We chose PASs which
take one or more topical nouns as an argument or
arguments, and substrings matched by POS patterns
which include topical nouns. All names tagged in
the corpus were replaced by their head nouns in
order to reduce complexity of sentences and thus
reduce the task of the parser and the POS pattern
matcher.
4.1 Implementation of PAS method
We implemented PAS method on LiLFeS, a
unification-based programming system for typed
feature structures (Makino et al, 1998; Miyao et al,
2000).
The selection in Step 2 described in Section 3
is realized by matching PASs with nine PAS tem-
plates. Four of the templates are illustrated in Fig-
ure 3.
4.2 POS Pattern Method
We constructed a POS pattern matcher with a par-
tial verb chunking function according to (Hatzivas-
siloglou and Weng, 2002). Because the original
matcher has problems in recall (its verb group de-
tector has low coverage) and precision (it does not
consider other words to detect relations between
verb groups and topical nouns), we implemented
2(a) may be selected if the anaphora (?it?) is resolved. But
we regard anaphora resolving is too hard task as a subprocess
of finding anchor verbs.
*any*
ARG1 N1
N1 = topical noun
*any*
ARG1 N1
ARG2 N2
N1 = topical noun
or N2 = topical noun
? ?
*any*
MODIFY *any*
ARG1 N1
N1 = topical noun
*any*
MODIFY *any*
ARG1 N1
ARG2 N2
N1 = topical noun
or N2 = topical noun
Figure 3: PAS templates
N ? V G ? N
N ? V G
V G ? N
N : is a topical noun
V G: is a verb group which is accepted by a finite state
machine described in (Hatzivassiloglou andWeng, 2002)
or one of {VB, VBD, VBG, VBN, VBP, VBZ}
?: is 0?4 tokens which do not include {FW, NN, NNS,
NNP, NNPS, PRP, VBG, WP, *}
(Parts in Bold letters are added to the patterns of Hatzi-
vassiloglou and Weng (2002).)
Figure 4: POS patterns
our POS pattern matcher as a modified version of
one in (Hatzivassiloglou and Weng, 2002).
Figure 4 shows patterns in our experiment. The
last verb of V G is extracted if all of Ns are topical
nouns. Non-topical nouns are disregarded. Adding
candidates for verb groups raises recall of obtained
relations of verbs and their arguments. Restriction
on intervening tokens to non-nouns raises the preci-
sion, although it decreases the recall.
4.3 Experiment 1
We extracted last verbs of POS patterns and core
verbs of PASs with their arguments from 100 ab-
stracts (976 sentences) of the GENIA corpus. We
took up not the verbs only but tuples of the verbs
and their arguments (VAs), in order to estimate ef-
fect of the arguments on semantical filtering.
Results
The numbers of VAs extracted from the 100 ab-
stracts using POS patterns and PASs are shown in
Table 1. (Total ? VAs of verbs not extracted by the
other method) are not the same, because more than
one VA can be extracted on a verb in a sentence.
POS patterns method extracted more VAs, although
POS patterns PASs
Total 1127 766
VAs of verbs
not extracted 478 105
by the other
Table 1: Numbers of VAs extracted from the 100
abstracts
Appropriate Inappropriate Total
Correct 43 12 55
Incorrect 20 23 43
Total 63 35 98
Table 2: Numbers of VAs extracted by POS patterns
(in detail)
their correctness is not considered.
4.4 Experiment 2
For the first 10 abstracts (92 sentences), we man-
ually investigated whether extracted VAs are syn-
tactically or semantically correct. The investigation
was based on two criteria: ?appropriateness? based
on whether the extracted verb can be used for an an-
chor verb and ?correctness? based on whether the
syntactical analysis is correct, i.e., whether the ar-
guments were extracted correctly.
Based on human judgment, the verbs that rep-
resent interactions, events, and properties were se-
lected as semantically appropriate for anchor verbs,
and the others were treated as inappropriate. For ex-
ample, ?identified? in ?We identified ZEBRA pro-
tein.? is not appropriate and discarded.
We did not consider non-topical noun arguments
for POS pattern method, whereas we considered
them for PAS method. Thus decision on correctness
is stricter for PAS method.
Results
The manual investigation results on extracted
VAs from the 10 abstracts using POS patterns and
PASs are shown in Table 2 and 3 respectively.
POS patterns extracted more (98) VAs than PASs
(75), but many of the increment were from incor-
rect POS pattern matching. By POS patterns, 43
VAs (44%) were extracted based on incorrect anal-
ysis. On the other hand, by PASs, 20 VAs (27%)
were extracted incorrectly. Thus the ratio of VAs
extracted by syntactically correct analysis is larger
on PAS method.
POS pattern method extracted 38 VAs of verbs
not extracted by PAS method and 7 of them are cor-
rect. For PAS method, correspondent numbers are
Appropriate Inappropriate Total
Correct 44 11 55
Incorrect 14 6 20
Total 58 17 75
Table 3: Numbers of VAs extracted by PASs (in de-
tail)
11 and 4 respectively. Thus the increments tend to
be caused by incorrect analysis, and the tendency is
greater in POS pattern method.
Since not all of verbs that take topical nouns are
appropriate for anchor verbs, automatic filtering is
required. In the filtering phase that we leave as a
future work, we can use semantical classes and fre-
quencies of arguments of the verbs. The results with
syntactically incorrect arguments will cause adverse
effect on filtering because they express incorrect re-
lationship between verbs and arguments. Since the
numbers of extracted VAs after excluding the ones
with incorrect arguments are the same (55) between
PAS and POS pattern methods, it can be concluded
that the precision of PAS method is higher. Al-
though there are few (7) correct VAs which were
extracted by POS pattern method but not by PAS
method, we expect the number of such verbs can be
reduced using a larger corpus.
Examples of appropriate VAs extracted by only
one method are as follows: (A) is correct and (B)
incorrect, extracted by only POS pattern method,
and (C) is correct and (D) incorrect, extracted by
only PAS method. Bold words are extracted verbs
or predicates and italic words their extracted argu-
ments.
(A) This delay is associated with down-regulation
of many erythroid cell-specific genes, including
alpha- and beta-globin, band 3, band 4.1, and . . . .
(B) . . . show that several elements in the . . . region of
the IL-2R alpha gene contribute to IL-1 respon-
siveness, . . . .
(C) The CD4 coreceptor interacts with non-
polymorphic regions of . . . molecules on
non-polymorphic cells and contributes to T cell
activation.
(D) Whereas activation of the HIV-1 enhancer follow-
ing T-cell stimulation is mediated largely through
binding of the . . . factor NF-kappa B to two adja-
cent kappa B sites in . . . .
5 Conclusions
We have proposed a method of extracting anchor
verbs as elements of extraction rules for IE by us-
ing PASs obtained by full parsing. To compare
our method with more naive and robust methods,
we have extracted verbs and their arguments using
POS patterns and PASs. POS pattern method could
obtain more candidate verbs for anchor verbs, but
many of them were extracted with incorrect argu-
ments by incorrect matching. A later filtering pro-
cess benefits by precise relations between verbs and
their arguments which PASs obtained. The short-
coming of PAS method is expected to be reduced by
using a larger corpus, because verbs to extract will
appear many times in many forms. One of the future
works is to extend PAS method to handle events in
nominalized forms.
Acknowledgements
This work was partially supported by Grant-in-
Aid for Scientific Research on Priority Areas (C)
?Genome Information Science? from the Ministry
of Education, Culture, Sports, Science and Technol-
ogy of Japan.
References
Vasileios Hatzivassiloglou and Wubin Weng. 2002.
Learning anchor verbs for biological interaction
patterns from published text articles. Interna-
tional Journal of Medical Informatics, 67:19?32.
Jin-Dong Kim, Tomoko Ohta, Yuka Teteisi, and
Jun?ichi Tsujii. 2003. GENIA corpus ? a se-
mantically annotated corpus for bio-textmining.
Bioinformatics, 19(suppl. 1):i180?i182.
Takaki Makino, Minoru Yoshida, Kentaro Tori-
sawa, and Jun-ichi Tsujii. 1998. LiLFeS ? to-
wards a practical HPSG parser. In Proceedings
of COLING-ACL?98.
Yusuke Miyao, Takaki Makino, Kentaro Torisawa,
and Jun-ichi Tsujii. 2000. The LiLFeS abstract
machine and its evaluation with the LinGO gram-
mar. Natural Language Engineering, 6(1):47 ?
61.
Yusuke Miyao, Takashi Ninomiya, and Jun?ichi
Tsujii. 2003. Probabilistic modeling of argument
structures including non-local dependencies. In
Proceedings of RANLP 2003, pages 285?291.
Yusuke Miyao, Takashi Ninomiya, and Jun?ichi
Tsujii. 2004. Corpus-oriented grammar develop-
ment for acquiring a Head-driven Phrase Struc-
ture Grammar from the Penn Treebank. In Pro-
ceedings of IJCNLP-04.
Ivan A. Sag and Thomas Wasow. 1999. Syntactic
Theory. CSLI publications.
Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman.
2003. An improved extraction pattern represen-
tation model for automatic IE pattern acquisition.
In Proceedings of ACL 2003, pages 224?231.
Syntax annotation for the GENIA corpus  
Yuka Tateisi1 Akane Yakushiji2 Tomoko Ohta1 Jun?ichi Tsujii2,3,1
1 CREST, Japan Science and Technology Agency 
4-1-8, Honcho, Kawaguchi-shi, Saitama 332-0012 Japan 
2 Department of Computer Science, University of Tokyo 
7-3-1 Hongo, Bunkyo-ku, Tokyo 113-0033, Japan 
3 School of Informatics, University of Manchester 
POBox 88, Sackville St, MANCHESTER M60 1QD, UK 
{yucca,akane,okap,tsujii}@is.s.u-tokyo.ac.jp 
 
Abstract 
Linguistically annotated corpus based 
on texts in biomedical domain has been 
constructed to tune natural language 
processing (NLP) tools for bio-
textmining. As the focus of information 
extraction is shifting from "nominal" 
information such as named entity to 
"verbal" information such as function 
and interaction of substances, applica-
tion of parsers has become one of the 
key technologies and thus the corpus 
annotated for syntactic structure of sen-
tences is in demand. A subset of the 
GENIA corpus consisting of 500 
MEDLINE abstracts has been anno-
tated for syntactic structure in an XML-
based format based on Penn Treebank 
II (PTB) scheme. Inter-annotator 
agreement test indicated that the writ-
ing style rather than the contents of the 
research abstracts is the source of the 
difficulty in tree annotation, and that 
annotation can be stably done by lin-
guists without much knowledge of bi-
ology with appropriate guidelines 
regarding to linguistic phenomena par-
ticular to scientific texts. 
1 Introduction 
Research and development for information ex-
traction from biomedical literature (bio-
textmining) has been rapidly advancing due to 
demands caused by information overload in the 
genome-related field. Natural language process-
ing (NLP) techniques have been regarded as 
useful for this purpose. Now that focus of in-
formation extraction is shifting from extraction 
of ?nominal? information such as named entity 
to ?verbal? information such as relations of enti-
ties including events and functions, syntactic 
analysis is an important issue of NLP applica-
tion in biomedical domain. In extraction of rela-
tion, the roles of entities participating in the 
relation must be identified along with the verb 
that represents the relation itself. In text analysis, 
this corresponds to identifying the subjects, ob-
jects, and other arguments of the verb. 
Though rule-based relation information ex-
traction systems using surface pattern matching 
and/or shallow parsing can achieve high-
precision (e.g. Koike et al, 2004) in a particular 
target domain, they tend to suffer from low re-
call due to the wide variation of the surface ex-
pression that describe a relation between a verb 
and its arguments. In addition,  the portability of 
such systems is low because the system has to 
be re-equipped with different set of rules when 
different kind of relation is to be extracted. One 
solution to this problem is using deep parsers 
which can abstract the syntactic variation of a 
relation between a verb and its arguments repre-
sented in the text, and constructing extraction 
rule on the abstract predicate-argument structure. 
To do so, wide-coverage and high-precision 
parsers are required. 
While basic NLP techniques are relatively 
general and portable from domain to domain, 
customization and tuning are inevitable, espe-
cially in order to apply the techniques effec-
tively to highly specialized literatures such as 
research papers and abstracts. As recent ad-
vances in NLP technology depend on machine-
learning techniques, annotated corpora from 
which system can acquire rules (including 
grammar rules, lexicon, etc.) are indispensable 
220
resources for customizing general-purpose NLP 
tools. In bio-textmining, for example, training 
on part-of-speech (POS)-annotated GENIA cor-
pus was reported to improve the accuracy of 
JunK tagger (English POS tagger) (Kazama et 
al., 2001) from  83.5% to 98.1% on MEDLINE 
abstracts (Tateisi and Tsujii, 2004), and the 
FraMed corpus (Wermter and Hahn, 2004) was 
used to train TnT tagger on German (Brants, 
2000) to improve its accuracy from 95.7% to 
98% on clinical reports and other biomedical 
texts. Corpus annotated for syntactic structures 
is expected to play a similar role in tuning pars-
ers to biomedical domain, i.e., similar improve-
ment on the performance of parsers is expected 
by using domain-specific treebank as a resource 
for learning. For this purpose, we construct 
GENA Treebank (GTB), a treebank on research 
abstracts in biomedical domain. 
2 Outline of the Corpus 
The base text of GTB is that of the GENIA cor-
pus constructed at University of Tokyo (Kim et 
al., 2003), which is a collection of research ab-
stracts selected from the search results of 
MEDLINE database with keywords (MeSH 
terms) human, blood cells and transcription fac-
tors. In the GENIA corpus, the abstracts are en-
coded in an XML scheme where each abstract is 
numbered with MEDLINE UID and contains 
title and abstract. The text  of title and abstract is 
segmented into sentences in which biological 
terms are annotated with their semantic classes. 
The GENIA corpus is also annotated for part-of-
speech (POS) (Tateisi and Tsujii, 2004), and 
coreference is also annotated in a part of the 
GENIA corpus by MedCo project at Institute for 
Infocomm Research, Singapore (Yang et al 
2004).  
GTB is the addition of syntactic information 
to the GENIA corpus. By annotating various 
linguistic information on a same set of text, the 
GENIA corpus will be a resource not only for 
individual purpose such as named entity extrac-
tion or training parsers but also for integrated 
systems such as information extraction using 
deep linguistic analysis. Similar attempt of con-
structing integrated corpora is being done in 
University of Pennsylvania, where a corpus of 
MEDLINE abstracts in CYP450 and oncology 
domains where annotated for named entities, 
POS, and tree structure of sentences (Kulick et 
al, 2004).  
2.1 Annotation Scheme 
The annotation scheme basically follows the 
Penn Treebank II (PTB) scheme (Beis et al 
1995), encoded in XML. A non-null constituent 
is marked as an element, with its syntactic cate-
gory (which may be combined with its function 
tags indicating grammatical roles such as  -SBJ, 
-PRD, and -ADV) used as tags. A null constitu-
ent is marked as a childless element whose tag 
corresponds to its categories. Other function tags 
are encoded as attributes. Figure 1 shows an ex-
ample of annotated sentence in XML, and the 
corresponding PTB notation. The label ?S? 
means ?sentence?, ?NP? noun phrase, ?PP? 
prepositional phrase, and ?VP? verb phrase.  
The label ?NP-SBJ? means that the element is 
an NP that serves as the subject of the sentence. 
A null element, the trace of the object of ?stud-
ied? moved by passivization, is denoted by 
? <NP NULL="NONE" ref="i55"/>? in XML 
and ?*-55? in PTB notation. The number ?55? 
which refers to the identifier of the moved ele-
ment, is denoted by ?id? and ?ref? attributes in 
XML, and is denoted as a part of a label in PTB. 
In addition to changing the encoding, we 
made some modifications to the scheme. First, 
analysis within the noun phrase is simplified. 
Second, semantic division of adverbial phrases 
such as ??TMP? (time) and ??MNR? (manner) 
are not used: adverbial constituents other than 
?ADVP? (adverbial phrases) or ?PP? used ad-
verbially are marked with ?ADV tags but not 
with semantic tags. Third, a coordination struc-
ture is explicitly marked with the attribute 
SYN=?COOD? whereas in the original PTB 
scheme it is not marked as such.   
 In our GTB scheme, ?NX? (head of a com-
plex noun phrase) and ?NAC? (a certain kind of 
nominal modifier within a noun phrase) of the 
PTB scheme are not used. A noun phrase is gen-
erally left unstructured. This is mainly in order 
to simplify the process of annotation. In case of 
biomedical abstracts, long noun phrases often 
involve multi-word technical terms whose syn-
tactic structure is difficult to determine without 
deep domain knowledge. However, the structure 
of noun phrases are usually independent of the 
structure outside the phrase, so that it would be 
221
easier to analyze the phrases involving such 
terms independently (e.g. by biologists) and 
later merge the two analysis together. Thus we 
have decided that we leave noun phrases un-
structured in GTB annotation unless their analy-
sis is necessary for determining the structure 
outside the phrase. One of the exception is the 
cases that involves coordination where it is nec-
essary to explicitly mark up the coordinated 
constituents. 
In addition, we have added special attributes 
?TXTERR?, ?UNSURE?,  and ?COMMENT? 
for later inspection. The ?TXTERR? is used 
when the annotator suspects that there is a 
grammatical error in the original text; the 
?UNSURE? attribute is used when the annotator 
is not confident; and the ?COMMENT? is used 
for free comments (e.g. reason of using 
?UNSURE?) by the annotator.  
2.2   Annotation Process 
The sentences in the titles and abstracts of the 
base text of GENIA corpus are annotated manu-
ally using an XML editor used for the Global 
Document Annotation project (Hasida 2000). 
Although the sentence boundaries were adopted 
from the corpus, the tree structure annotation 
was done independently of POS- and term- an-
notation already done on the GENIA corpus. 
The annotator was a Japanese non-biologist who 
has previously involved in the POS annotation 
of the GENIA corpus and accustomed to the 
style of research abstracts in English. Manually 
annotated abstracts are automatically converted 
to the PTB format, merged with the POS annota-
tion of the GENIA corpus (version 3.02). 
3 Annotation Results 
So far, 500 abstracts are annotated and con-
verted to the merged PTB format. In the merg-
ing process, we found several annotation errors. 
The 500 abstracts with correction of these errors 
are made publicly available as ?The GENIA 
Treebank Beta Version? (GTB-beta).   
For further clean-up, we also tried to parse 
the corpus by the Enju parser (Miyao and Tsujii 
2004), and identify the error of the corpus by 
investigating into the parse errors. Enju is an 
HPSG parser that can be trained with PTB-type 
corpora which is reported to have 87% accuracy 
on Wall Street Journal portion of Penn Treebank 
corpus. Currently the accuracy of the parser 
drops down to 82% on GTB-beta, and although 
proper quantitative analysis is yet to be done, it 
was found that the mismatches between labels of 
the treebank and the GENIA POS corpus (e.g. 
an ?ing form labeled as noun in the POS corpus 
and as the head of a verb phrase in the tree cor-
pus) are a major source of parse error. The cor-
rection is complicated because several errors in 
the GENIA POS corpus were found in this 
cleaning-up process. When the cleaning-up 
process is done, we will make the corpus pub-
licly available as the proper release. 
<S><PP>In <NP>the present paper </NP></PP>, 
<NP-SBJ id="i55"><NP>the binding 
</NP><PP>of <NP>a [125I]-labeled aldosterone 
derivative </NP></PP><PP>to <NP><NP>plasma 
membrane rich fractions </NP><PP>of HML 
</PP></NP></PP></NP-SBJ><VP>was 
<VP>studied <NP NULL="NONE" 
ref="i55"/></VP> 
</VP>.</S> 
 
4 Inter-Annotator Agreement 
We have also checked inter-annotator agreement. 
Although the PTB scheme is popular among 
natural language processing society, applicabil-
ity of the scheme to highly specialized text such 
as research abstract is yet to be discussed. Espe-
cially, when the annotation is done by linguists, 
lack of domain knowledge might decrease the 
stability and accuracy of annotation. 
A small part of the base text set (10 ab-
stracts) was annotated by another annotator. The 
10 abstracts were chosen randomly, had 6 to 17 
sentences per abstract (total 108 sentences). The 
new annotator had a similar background as the 
first annotator that she is a Japanese non-
biologist who has experiences in translation of 
(S (PP In (NP the present paper)), (NP-SBJ-55 (NP 
the binding) (PP of (NP a [125I]-labeled aldosterone 
derivative)) (PP to (NP (NP plasma membrane rich 
fractions) (PP of HML)))) (VP was (VP studied *-
55)).) 
Figure 1. The sentence ?In the present paper, the binding of 
a [125I]-labeled aldosterone derivative to plasma mem-
brane rich fractions of HML was studied? annotated in 
XML and PTB formats.  
222
technical documents in English and in corpus 
annotation of  English texts. 
The two results were examined manually, 
and there were 131 disagreements. Almost every 
sentence had at least one disagreement. We have 
made the ?gold standard? from the two sets of 
abstracts by resolving the disagreements, and the 
accuracy of the annotators against this gold 
standard were 96.7% for the first annotator and 
97.4% for the second annotator. 
 Of the disagreement, the most prominent 
were the cases involving coordination, espe-
cially the ones with ellipsis. For example, one 
annotator annotated the phrase ?IL-1- and IL-18-
mediated function? as in Figure 2a, the other 
annotated as Figure 2b.  
 Such problem is addressed in the PTB 
guideline and both formats are allowed as alter-
natives. As coordination with ellipsis occurs 
rather frequently in research abstracts, this kind 
of phenomena has higher effect on decrease of 
the agreement rate than in Penn Treebank. Of 
the 131 disagreements, 25 were on this type of 
coordination. 
Another source of disagreement is the at-
tachment of modifiers such as prepositional 
phrases and pronominal adjectives. However, 
most are ?benign ambiguity? where the differ-
ence of the structure does not affect on interpre-
tation, such as ?high expression of STAT in 
monocytes? where the prepositional phrase ?in 
monocytes? can attach to ?expression? or 
?STAT? without much difference in meaning, 
and ?is augmented when the sensitizing tumor is 
a genetically modified variant? where the wh-
clause can attach to ?is augmented? or ?aug-
mented? without changing the meaning. The 
PTB guideline states that the modifier should be 
attached at the higher level in the former case 
and at the lower case in the latter. In the annota-
tion results, one annotator consistently attached 
the modifiers in both cases at the higher level, 
and the other consistently at the lower level, in-
dicating that the problem is in understanding the 
scheme rather than understanding the sentence. 
Only 15 cases were true ambiguities that needed 
knowledge of biology to solve, in which 5 in-
volved coordination (e.g., the scope of ?various? 
in ?various T cell lines and peripheral blood 
cells?) .  
 Although the number was small, there were 
disagreements on how to annotate a mathemati-
cal formula such as ?n=2? embedded in the sen-
tence, since mathematical formulae were outside 
the scope of the original PTB scheme. One an-
notator annotated this kind of phrase consis-
tently as a phrase with ?=? as an adjective, the 
other annotated as phrase with ?=? as a verb. 
There were 6 such cases. Another disagreement 
particular to abstracts is a treatment of labeled 
sentences. There were 8 sentences in two ab-
stracts where there is a label like ?Background:?.  
One annotator included the colon (?:?) in the la-
bel, while the other did not. Yet another is that 
one regarded the phrase ?Author et al as coor-
dination, and the other regarded ?et al as a 
modifier.   
<NP SYN="COOD"> 
<NP><ADJP>IL-1- <ADJP NULL="QSTN"/></ADJP> 
         <NP NULL="RNR" ref="i20"/></NP> 
and  
<NP>IL-18-mediated <NP NULL="RNR" ref="i20"/></NP> 
<NP id="i20">function </NP> 
 Other disagreements are more general type 
such as regarding ?-ed? form of a verb as an ad-
jective or a participle, miscellaneous errors such 
as omission of a subtype of label (such as ?-
PRD? or ?-SBJ) or the position of <PRN> tags 
<NP> 
<ADJP SYN="COOD"> 
  <ADJP>IL-1- <ADJP NULL="QSTN"/></ADJP> 
  and  
  <ADJP>IL-18-mediated </ADJP></ADJP> 
function  
</NP> 
    NP    
       
  ADJP   Function  
       
ADJP     and ADJP    
         
IL-1  *   IL-18 mediated   
Figure 2a. Annotation of a coordinated phrase by the first 
annotator. A* denotes a null constituent. 
</NP> 
        NP     
       
 NP And NP   
         
    ADJP  *20 IL-18 meidiated NP  
          
IL-1 *      function20
Figure 2b. Annotation of the same phrase as in Figure 2a 
by the second annotator.  A * denotes a null constituent 
and ?20? denotes coindexing. 
223
with regards to ?,? for the inserted phrase, or the 
errors which look like just ?careless?. Such dis-
agreements and mistakes are at least partially 
eliminated when reliable taggers and parsers are 
available for preprocessing 
5 Discussion 
The result of the inter-annotator agreement 
test indicates that the writing style rather than 
the contents of the research abstracts is the 
source of the difficulty in tree annotation. Con-
trary to the expectation that the lack of domain 
knowledge causes a problem in annotation on 
attachments of modifiers, the number of cases 
where annotation of modifier attachment needs 
domain knowledge is small. This indicates that 
linguists can annotate most of syntactic structure 
without an expert level of domain knowledge.  
A major source of difficulty is coordination, 
especially the ones involving ellipsis. Coordina-
tion is reported to be difficult phenomena in an-
notation of different levels in the GENIA corpus 
(Tateisi and Tsujii, 2004), (Kim et al, 2003). In 
addition to the fact that this is the major source 
of inter-annotator agreement, the annotator often 
commented the coordinated structure as ?unsure?. 
The problem of coordination can be divided into 
two with different nature: one is that the annota-
tion policy is still not well-established for the 
coordination involving ellipsis, and the other is 
an ambiguity when the coordinated phrase has 
modifiers.  
Syntax annotation of coordination with ellip-
sis is difficult in general but the more so in an-
notation of abstracts than in the case of general 
texts, because in abstracts authors tend to pack 
information in limited number of words. The 
PTB guideline dedicates a long section for this 
phenomena and allows alternatives in annotation, 
but there are still cases which are not well-
covered by the scheme. For example, in addition 
to the disagreement, the phrase illustrated in 
Figure 2a and Figure 2b shows another problem 
of the annotation scheme. Both annotators fail to 
indicate that it is ?mediated? that was to be after 
?IL-1? because there is no mechanism of 
coindexing a null element with a part of a token.  
This problem of ellipsis can frequently occur 
in research abstracts, and it can be argued that 
the tokenization criteria must be changed for 
texts in biomedical domain (Yamamoto and Sa-
tou, 2004) so that such fragment as ?IL-18? and 
?mediated? in ?IL-18-ediated? should be regarede 
as separate tokens. The Pennsylvania biology 
corpus (Kulick et al, 2004) partially solves this 
problem by separating a token where two or 
more subtokens are connected with hyphens, but 
in the cases where a shared part of the word is 
not separated by a hyphen (e.g. ?metric? of ?ste-
reo- and isometric alleles?) the word including 
the part is left uncut. The current GTB follows 
the GENIA corpus that it retains the tokeniza-
tion criteria of the original Penn Treebank, but 
this must be reconsidered in future. 
 For analysis of coordination with ellipsis, if 
the information on full forms is available, one 
strategy would be to leave the inside structure of 
coordination unannotated in the treebank corpus 
(and in the phase of text analysis the structure is 
not established in the phase of parsing but with a 
different mechanism) and later merge it with the 
coordination structure annotation. The GENIA 
term corpus annotates the full form of a techni-
cal term whose part is omitted in the surface as 
an attribute of the ?<cons>? element indicating a 
technical term (Kim et al, 2003). In the above-
mentioned Pennsylvania corpus, a similar 
mechanism (?chaining?) is used for recovering 
the full form of named entities. However, in 
both corpora, no such information is available 
outside the terms/entities.  
The cases where scope of modification in 
coordinated phrases is problematic are few but 
they are more difficult in abstracts than in gen-
eral texts because the resolution of ambiguity 
needs domain knowledge. If term/entity annota-
tion is already done, that information can help 
resolve this type of ambiguity, but again the 
problem is that outside the terms/entities such 
information is not available. It would be practi-
cal to have the structure flat but specially 
marked when the tree annotators are unsure and 
have a domain expert resolve the ambiguity, as 
the sentences that needs such intervention seems 
few. Some cases of ambiguity in modifier at-
tachment (which do not involve coordination) 
can be solved with similar process. 
We believe that other type of disagreements 
can be solved with supplementing criteria for 
linguistic phenomena not well-covered by the 
scheme, and annotator training. Automatic pre-
processing by POS taggers and parsers can also 
help increase the consistent annotation. 
224
6 Conclusion 
A subset of the GENIA corpus is annotated 
for syntactic (tree) structure. Inter-annotator 
agreement test indicated that the annotation can 
be done stably by linguists without much 
knowledge in biology, provided that proper 
guideline is established for linguistic phenomena 
particular to scientific research abstracts. We 
have made the 500-abstract corpus in both XML 
and PTB formats and made it publicly available 
as ?the GENIA Treebank beta version? (GTB-
beta). We are in further cleaning up process of 
the 500-abstract set, and at the same time, initial 
annotation of the remaining abstracts is being 
done, so that the full GENIA set of 2000 ab-
stracts will be annotated with tree structure.  
For parsers to be useful for information ex-
traction, they have to establish a map between 
syntactic structure and more semantic predicate-
argument structure, and between the linguistic 
predicate-argument structures to the factual rela-
tion to be extracted. Annotation of various in-
formation on a same set of text can help 
establish these maps. For the factual relations, 
we are annotating relations between proteins and 
genes in cooperation with a group of biologists. 
For predicate-argument annotation, we are in-
vestigating the use of the parse results of the 
Enju parser. 
Acknowledgments 
The authors are grateful to annotators and col-
leagues that helped the construction of the cor-
pus. This work is partially supported by Grant-
in-Aid for Scientific Research on Priority Area 
C ?Genome Information Science? from the Min-
istry of Education, Culture, Sports, Science and 
Technology of Japan. 
References 
 Brants,T.(2000). TnT: a statistical part-of-speech 
tagger, Proceedings of the sixth conference on Ap-
plied natural language processing, pp.224-231, 
Morgan Kaufmann Publishers Inc.  
Beis.A., Ferguson,M., Katz,K., and Mac-
Intire,R.(1995). Bracketing Guidelines for Tree-
bank II Style: Penn Treebank Project, University 
of Pennsylvania 
Hasida, K. (2000). GDA: Annotated Document as 
Intelligent Content.  Proceedings of 
COLING?2000 Workshop on Semantic Annotation 
and Intelligent Content. 
Kazama,J., Miyao,Y., and Tsujii,J.(2001) A Maxi-
mum Entropy Tagger with Unsupervised Hidden 
Markov Models, Proceedings of the Sixth Natural 
Language Processing Pacific Rim Symposium, pp. 
333-340.  
Kim,J-D, Ohta,T., Tateisi,Y. and Tsujii,J. (2003). 
GENIA corpus - a semantically annotated corpus 
for bio-textmining. Bioinformatics. 19(suppl. 1). 
pp. i180-i182. Oxford University Press.  
Koike,A., Niwa,Y., and Takagi,T. (2004) Automatic 
extraction of gene/protein biological functions 
from biomedical text. Bioinformatics, Advanced 
Access published on October 27, 2004; 
doi:10.1093/bioinformatics/bti084.Oxford Univer-
sity Press. 
Kulick,S., Bies,A., Liberman,M., Mandel,M., 
McDonald,R., Palmer,M., Schein,A., Ungar,L., 
Winters,S. and White,P. (2004)  Integrated Anno-
tation for Biomedical Information Extraction. 
BioLINK 2004: Linking Biological Literature, On-
tologies, and Databases, pp. 61-68.Association for 
Computational Linguistics. 
Miyao,Y. and Tsujii,J. (2004a). Deep Linguistic 
Analysis for the Accurate Identification of Predi-
cate-Argument Relations. Proceedings of 
COLING 2004. pp. 1392-1397. 
Tateisi,Y. and Tsujii,J. (2004). Part-of-Speech Anno-
tation of Biology Research Abstracts. Proceedings 
of the 4th International Conference on Language 
Resource and Evaluation (LREC2004). IV. pp. 
1267-1270, European Language Resources Asso-
ciation. 
Wermter, J. and Hahn, U. (2004). An annotated Ger-
man-language medical text corpus. GMDS 2004 
meeting, 
http://www.egms.de/en/meetings/gmds2004/04gm
ds168.shtml. 
Yamamoto,K., and Satou,K (2004). Low-level Text 
Processing for Life Science, Proceedings of the 
SIG meeting on Natural Language Processing, In-
formation Processing Society of Japan, IPSJ-
SIGNL-159 (In Japanese). 
 Yang,XF., Zhou,GD., Su,J., and Tan.,CL (2004). 
Improving Noun Phrase Coreference Resolution 
by Matching Strings. Proceedings of 1st Interna-
tional Joint Conference on Natural Language 
Processing (IJCNLP'2004), pp226-233.
 
225
Encoding Biomedical Resources in TEI: the Case of the GENIA Corpus
Tomaz? Erjavec
Dept. of Intelligent Systems
Joz?ef Stefan Institute, Ljubljana
Yuka Tateisi
CREST
Japan Science and
Technology Corporation
Jin-Dong Kim
Dept. of Information Science
University of Tokyo
Tomoko Ohta
CREST
Japan Science and
Technology Corporation
Jun-ichi Tsujii
CREST JST &
Dept. of Information Science
University of Tokyo
Abstract
It is well known that standardising the
annotation of language resources signifi-
cantly raises their potential, as it enables
re-use and spurs the development of com-
mon technologies. Despite the fact that
increasingly complex linguistic informa-
tion is being added to biomedical texts,
no standard solutions have so far been
proposed for their encoding. This pa-
per describes a standardised XML tagset
(DTD) for annotated biomedical corpora
and other resources, which is based on
the Text Encoding Initiative Guidelines
P4, a general and parameterisable stan-
dard for encoding language resources. We
ground the discussion in the encoding of
the GENIA corpus, which currently con-
tains 2,000 abstracts taken from the MED-
LINE database, and has almost 100,000
hand-annotated terms marked for seman-
tic class from the accompanying ontol-
ogy. The paper introduces GENIA and
TEI and implements a TEI parametrisa-
tion and conversion for the GENIA cor-
pus. A number of aspects of biomedi-
cal language are discussed, such as com-
plex tokenisation, prevalence of contrac-
tions and complex terms, and the linkage
and encoding of ontologies.
1 Introduction
With the growing research on processing texts from
the biomedical domain, the number of resources,
esp. corpora, is increasing rapidly. Such corpora can
be heavily annotated, e.g., with meta-data, words
and part-of-speech tags, named entities, phrases,
terms, concepts, translation equivalents, etc. Cor-
pora are invaluable to the further development of
technologies for utilising the information in biomed-
ical texts, as they provide them with training and
testing data. Given the value of such resources, it
is important to ensure their reusability and increase
their interchange potential ? a step in this direc-
tion is developing common encodings for biomedi-
cal corpora.
Standardisation of resource encoding practices
has now, for some time, been in the forefront of at-
tention. Most of these advances are Web-driven, and
include XML and related recommendations, such as
XSLT, XML Schemas, XPointer, SAX, etc. The
higher level standards, of meta-data (RDF) and on-
tologies (OWL) have been especially influential in
encoding biomedical resources. However, there re-
mains the question how to best encode the structure
of the text themselves, how to mark-up added lin-
guistic analyses, and how to implement linkages be-
tween the text and and further resources, such as lex-
ica, thesauri and ontologies. As discussed in (Ide
and Brew, 2000), in order to qualify as a ?good?
annotated corpus, its encoding should provide for
reusabilty and extensibily.
In this paper we build on previous work (Erjavec
et al, 2003) and show how to develop a standard-
ised encoding for biomedical corpora. We base
our discussion on the case of the GENIA corpus
(Ohta et al, 2002), which is originaly encoded in
GPML, the GENIA Project Markup Language, an
XML DTD. We re-encode the corpus into a stan-
dardised annotation scheme, based on the Text En-
coding Initiative Guidelines P4 (Sperberg-McQueen
and Burnard, 2002), and specify a constructive map-
ping from the original DTD to the developed encod-
ing via a XSLT transformation.
One of the motivations for such an re-encoding
is that TEI is well-designed and widely accepted ar-
chitecture, which has been often used for annotating
language corpora, and by porting to it, GENIA, and
other projects, can gain new insights into possible
encoding practices and maybe make the corpus bet-
ter suited for interchange. As the transformation to
TEI is fully automatic, there is also no need to aban-
don the original markup format (in this case GPML),
which, as it has been crafted specially for the corpus,
provides a tighter encoding than can be possible with
the more general TEI.
The paper thus proposes the creation of a prac-
tical annotation scheme for linguistically annotated
(biomedical) corpora, the conversion to which is
automatic and supports consistency checking and
validation. The paper also serves as a guide to
parametrising TEI and draws attention to certain as-
pects of biomedical corpora which are likely to face
all that wish to process such texts.
The paper is structured as follows: Section 2 in-
troduces the GENIA corpus; Section 3 introduces
the TEI, gives some pros and cons of using it,
and the method of parametrising TEI for particular
projects; Section 4 discusses such a parametrisation
for biomedical corpora and explains the conversion
of the GENIA corpus to TEI; Section 5 discusses
some challenging properties of biomedical text an-
notations; finally, Section 6 offers some conclusions
and directions for further work.
2 The GENIA Corpus
The GENIA corpus (Ohta et al, 2002) is be-
ing developed in the scope of the GENIA project,
which seeks to develop information extraction tech-
niques for scientific texts using NLP technol-
ogy. The corpus consists of semantically anno-
tated published abstracts from the biomedical do-
main. The corpus is a collection of articles ex-
tracted from the on-line MEDLINE abstracts (U.S.
National Center for Biotechnology Information,
http://www.ncbi.nlm.nih.gov/, PubMed database).
Since the focus of the corpus is on biological re-
actions concerning transcription factors in human
blood cells, articles were selected that contain the
MeSH terms human, blood cell and transcription
factor.
As usual for the field, the articles are composed
largely of structurally very complex technical terms,
and are almost incomprehensible to a layperson. A
typical heading e.g., reads IL-2 gene expression and
NF-kappa B activation through CD28 requires reac-
tive oxygen production by 5-lipoxygenase.
The main value of the GENIA corpus comes from
its annotation: all the abstracts and their titles have
been marked-up by two domain experts for bio-
logically meaningful terms, and these terms have
been semantically annotated with descriptors from
the GENIA ontology.
The GENIA ontology is a taxonomy of, currently,
47 biologically relevant nominal categories, such as
body part, virus, or RNA domain or region; the tax-
onomy has 35 terminal categories.
The terms of the corpus are semantically de-
fined as those sentence constituents that can be cate-
gorised using the terminal categories from the ontol-
ogy. Syntactically such constituents are quite varied:
they include qualifiers and can be recursive.
The GENIA corpus is encoded in the Genia
Project Markup Language. The GPML is an XML
DTD (Kim et al, 2001) where each article con-
tains its MEDLINE ID, title and abstract. The texts
of the abstracts are segmented into sentences, and
these contain the constituents with their semantic
classification. The GENIA ontology is provided to-
gether with the GENIA corpus and is encoded in
DAML+OIL (http://www.daml.org/ ), the standard
XML-based ontology description language. This
structure and its annotation will be further discussed
below.
A suite of supporting tools has been developed or
tuned for the GENIA corpus and GPML: the term
annotation is performed with the XMLMind editor;
an XPath-based concordancer has been developed
for searching the corpus; and CSS stylesheets are
available for browsing it.
At the time of writing, the latest version of the
GENIA corpus is 3.01, which has been released
in April 2003. It consists of 2,000 abstracts with
over 400,000 words and more than 90,000 marked-
up terms. This version has not yet been marked-
up with tokens or PoS information, although an
earlier version (Genia-V3.0p) has been. The GE-
NIA corpus is available free of charge from the GE-
NIA project homepage, at http://www-tsujii.is.s.u-
tokyo.ac.jp/GENIA/.
3 The Text Encoding Initiative
The Text Encoding Initiative was established in
1987 as a systematised attempt to develop a fully
general text encoding model and set of encoding
conventions based upon it, suitable for processing
and analysis of any type of text, in any language,
and intended to serve the increasing range of ex-
isting (and potential) applications and uses. The
TEI Guidelines for Electronic Text Encoding and
Interchange were first published in April 1994 in
two substantial green volumes, known as TEI P3.
In May 1999, a revised edition of TEI P3 was
produced, correcting several typographic and other
errors. In December 2000 the TEI Consortium
(http://www.tei-c.org/ ) was set up to maintain and
develop the TEI standard. In 2002, the Consortium
announced the availability of a major revision of TEI
P3, the TEI P4 (Sperberg-McQueen and Burnard,
2002) the object of which is to provide equal sup-
port for XML and SGML applications using the TEI
scheme. The revisions needed to make TEI P4 have
been deliberately restricted to error correction only,
with a view to ensuring that documents conforming
to TEI P3 will not become illegal when processed
with TEI P4. For GENIA, we are using the XML-
compatible version of TEI P4.
In producing P4, many possibilities for other,
more fundamental changes have been identified.
With the establishment of the TEI Council, it be-
came possible to agree on a programme of work to
enhance and modify the Guidelines more fundamen-
tally over the coming years. TEI P5 will be the next
full revision of the Guidelines. The work on P5 has
started, and the date of its appearance will likely be
in 2004 and there are currently several TEI Working
Groups addressing various parts of the Guidelines
that need attention.
More than 80 projects spanning over 30 languages
have so far made use of the TEI guidelines, pro-
ducing diverse resources, e.g., text-critical editions
of classical works. TEI has also been influential
in corpus encoding, where the best known exam-
ple is probably the British National Corpus. How-
ever, while the TEI has been extensively used for
annotating PoS tagged corpora, it been less popu-
lar for encoding texts used by the the Information
Retrieval/Extraction community; here, a number of
other initiatives have taken the lead in encoding, say,
ontologies or inter-document linking.
3.1 Pros and cons of using TEI
Why, if a corpus is already encoded in XML using
a home-grown DTD, to re-encoded it in TEI at all?
One reasons is certainly the validation aspect of the
exercise: re-coding a corpus, or any other resource,
reveals hidden (and in practice incorrect) assump-
tions about its structure. Re-coding to a standard
recommendation also forces the corpus designers to
face issues which might have been overlooked in the
original design.
There are also other advantages of using TEI as
the interchange format: (1) it is a wide-coverage,
well-designed (modular and extensible), widely ac-
cepted and well-maintained architecture; (2) it pro-
vides extensive documentation, which comprises not
only the Guidelines but also papers and documen-
tation (best practices) of various projects; (3) it of-
fers community support via the tei-l public discus-
sion list; (4) various TEI-dedicated software already
exists, and more is likely to become available; and
(5) using it contributes to the adoption of open stan-
dards and recommendations.
However, using a very general recommendation
which tries to cater for any possible situation brings
with it also several disadvantages:
Tag abuse TEI might not have elements / attributes
with the exact meaning we require. This re-
sults in a tendency to misuse tags for purposes
they were not meant for; however, it is a case
of individual judgement to decide whether to
(slightly) abuse a tag, or to implement a lo-
cal extension to add the attribute or element re-
quired.
Tag bloat Being a general purpose recommenda-
tion, TEI can ? almost by definition ? never
be optimal for a specific application. Thus a
custom developed DTD will be leaner, have
less (redundant) tags and simpler content mod-
els.
TEI for humanities While the Guidelines cover a
vast range of text types and annotations, they
are maybe the least developed for ?high level?
NLP applications or have failed to keep abreast
of ?cutting-edge? initiatives. As will be seen,
critical areas are the encoding of ontologies, of
lexical databases and of feature structures.
3.2 Building the TEI DTD
The TEI Guidelines (Sperberg-McQueen and
Burnard, 2002) consist of the formal part, which
is a set of SGML/XML DTD fragments, and the
documentation, which explains the rationale behind
the elements available in these fragments, as well as
giving overall information about the structure of the
TEI.
The formal SGML/XML part of TEI comes as a
set of DTD fragments or tagsets. A TEI DTD for a
particular application is then constructed by select-
ing an appropriate combination of such tagsets. TEI
distinguishes the following types of tagsets:
Core tagset : standard components of the TEI main
DTD in all its forms; these are always included
without any special action by the encoder.
Base tagsets : basic building blocks for specific text
types; exactly one base must be selected by the
encoder, unless one of the combined bases is
used.
Additional tagsets : extra tags useful for particular
purposes. All additional tagsets are compatible
with all bases and with each other; an encoder
may therefore add them to the selected base in
any combination desired.
User defined tagsets : these extra tags give the pos-
sibility of extending and overriding the defi-
nitions provided in the TEI tagset. Further-
more, they give the option of explicitly includ-
<!DOCTYPE teiCorpus.2 SYSTEM
"http://www.tei-c.org/P4X/DTD/tei2.dtd"
[<!ENTITY % TEI.XML "INCLUDE">
<!ENTITY % TEI.prose "INCLUDE">
<!ENTITY % TEI.linking "INCLUDE">
<!ENTITY % TEI.analysis "INCLUDE">
<!ENTITY % TEI.corpus "INCLUDE">
<!ENTITY % TEI.extensions.ent SYSTEM
?geniaex.ent?>
<!ENTITY % TEI.extensions.dtd SYSTEM
?geniaex.dtd?>
]>
Figure 1: The XML TEI prolog for GENIA
ing or ignoring (disallowing) each particular el-
ement licensed by the chosen base and addi-
tional tagsets.
While a project-particular XML DTD can be con-
structed by including and ignoring the TEI DTD
fragments directly (as exemplified in Figure 1), it is
also possible to build ? for easier processing ? a
one-file DTD with the help of the on-line TEI Pizza
Chef service, available from the TEI web site.
4 Parametrising TEI for biomedical
corpora
In previous work (Erjavec et al, 2003) we have al-
ready proposed a TEI parametrisation of GENIA
which was quite broad in its scope. Because a num-
ber of tagsets could prove useful in the long term
this parametrisation collected not only those that we
considered necessary for the current version of GE-
NIA, but also some that might prove of service in the
future. Furthermore, we supported the encoding of
both version 2.1 and 3.0 of the corpus. The resulting
DTD was thus very generous in what kinds of data it
caters for. To focus the discussion we, in the current
paper, only address tagset that are immediately rele-
vant to annotating biomedical texts. In Figure 1 we
define the XML DTD that can be used for encoding
biomedical resources, and that we used for GENIA
V3.01. The XML prolog given in this Figure defines
that ?teiCorpus.2? is the root element of the corpus,
that the external DTD resides at the given URL be-
longing to the TEI Consortium, and that a number
of TEI modules, detailed below, are being used to
parametrise the TEI to arrive at our particular DTD.
4.1 TEI.XML
TEI P4 allows both standard SGML and XML en-
codings. Including the TEI.XML option indicates
that the target DTD is to be expressed in XML.
4.2 TEI.prose
The base tagset does not declare many elements but
rather inherits all of the TEI core, which includes the
TEI header, and text elements. A TEI document will
typically have as its root element ?TEI.2? which is
composed of the ?teiHeader?, followed by the ?text?;
c.f. right hand side of Figure 2, but note that the root
element from the TEI.corpus module is used for the
complete corpus.
The TEI header describes an encoded work so that
the text (corpus) itself, its source, its encoding, and
its revisions are all thoroughly documented.
TEI.prose also contains elements and attributes
for describing text structure, e.g. ?div? for text divi-
sion, ?p? for paragraph, ?head? for text header, etc.
The tagset is therefore useful for encoding the gross
structure of the corpus texts; for an illustration again
see Figure 2.
4.3 TEI.linking
This additional tagset provides mechanisms for link-
ing, segmentation, and alignment. The elements
provided here enable links to be made e.g., between
the articles and their source URLs, or between con-
cepts and their hypernyms.
It should be noted that while the TEI treatment
of external pointers had been very influential, it was
overtaken and made obsolete by newer recommen-
dations. However, the TEI does have a Working
Group on Stand-Off Markup, XLink and XPointer,
which should produce new TEI encoding recom-
mendations for this area in 2003.
4.4 TEI.analysis
This additional tagset is used for associating sim-
ple linguistic analyses and interpretations with text
elements. It can be used to annotate words, ?w?,
clauses, ?cl?, and sentences, ?s? with dedicated tags,
as well as arbitrary and possibly nested segments
with the ?seg?. Such elements can be, via at-
tributes, associated with their analyses. This tagset
has proved very popular for PoS-annotated corpora;
for an illustration see Figure 3.
4.5 TEI.corpus
This additional tagset introduces a new root element,
?teiCorpus.2?, which comprises a (corpus) header
and a series of ?TEI.2? elements. The TEI.corpus
tagset alo extends the certain header elements to
provide more detailed descriptions of the corpus ma-
terial.
4.6 TEI.extensions.ent
The file gives, for each element sanctioned by the
chosen modules, whether we include or ignore it in
our parametrisation. While this is not strictly neces-
sary (without any such specification, all the elements
would be included) we thought it wise to constrain
the content models somewhat, to reduce the bewil-
dering variety of choices that the TEI otherwise of-
fers. Also, such an entity extension file gives the
complete list of all the TEI elements that are allowed
(and disallowed) in GENIA, which might prove use-
ful for documentation purposes.
4.7 TEI.extensions.dtd
This file specifies the changes we have made to TEI
elements. We have e.g., added the url attribute to
?xptr? and ?xref ? and tagging attributes to word and
punctuation elements.
4.8 Conversion of GPML to TEI
Because the source format of GENIA will remain
the simpler GPML, it is imperative to have an au-
tomatic procedure for converting to the TEI inter-
change format. The translation process takes advan-
tage of the fact that both the input and output are
encoded in XML, which makes it possible to use the
XSL Transformation Language, XSLT that defines a
standard declarative specification of transformations
between XML documents. There also exist a num-
ber of free XSLT processors; we used Daniel Veil-
lard?s xsltproc.
The transformation is written as a XSLT
stylesheet, which makes reference to two docu-
ments: the GENIA ontology in TEI and the template
for the corpus header. The stylesheet then resolves
the GPML encoded corpus into TEI. The translation
of the corpus is thus fully automatic, except for the
taxonomy, which was translated by hand.
Figure 2 illustrates the top level structure of the
corpus, and how it differs between the GPML and
TEI encodings. The most noticeable difference is,
apart from the renaming of elements, the addition
of headers to the corpus and texts. In the GENIA
?teiHeader? we give e.g., the name, address, avail-
ability, sampling description, and, for each abstract?s
?sourceDesc?, two ?xptr?s: the first gives the URL of
the HTML article in the MEDLINE database, while
the second is the URL of the article in the origi-
nal XML. It should be noted that we use a locally
defined url attribute for specifying the value of the
pointer.
5 Characteristics of biomedical texts
In this section we review some challenges that
biomedical texts present to the processing and en-
coding of linguistic information, and the manner of
their encoding in our DTD.
5.1 Tokens
Tokenisation, i.e., the identification of words and
punctuation marks, is the lowest level of linguistic
analysis, yet is, in spite (or because) of this of con-
siderable importance. As all other levels of linguis-
tic markup make direct or direct reference to the to-
ken stream of the text, so if this is incorrect, errors
will propagate to all other annotations.
It is also interesting to note that current annota-
tion practice is more and more leaning toward stand-
off markup, i.e., annotations that are separated from
the primary data (text) and make reference to it only
via pointers. However, it is beneficial to have some
markup in the primary data to which it is possible to
refer, and this markup is, almost exclusivelly, that of
tokens; see e.g., (Freese et al, 2003).
Version V1.1 of GENIA has been also annotated
with LTG tools (Grover et al, 2002). In short, the
corpus is tokenised, and then part-of-speech tagged
with two taggers, each one using a different tagset,
and the nouns and verbs lemmatised. Additionally,
the deverbal nominalisations are assigned their ver-
bal stems.
The conversion to TEI is also able to handle this
additional markup, by using the TEI.analysis mod-
ule. The word and punctuation tokens are encoded
as ?w? and ?c? elements respectively, which are fur-
ther marked with type and lemma and the locally de-
fined c1, c2 and vstem. An example of such markup
<s>
<w c1="DT" c2="DB">All</w>
<c type="HYPH" c1=":" c2="-">-</c>
<w c1="VBZ" c2="JJ">trans</w>
<w c1="JJ" c2="JJ">retinoic</w>
<w lemma="acid" c1="NN" c2="NN1">acid</w>
<c type="BR" c1="(" c2="(">(</c>
<w lemma="Ra" c1="NN" c2="NP1">RA</w>
<c type="BR" c1=")" c2=")">)</c>
<w lemma="be" c1="VBZ" c2="VBZ">is</w>
<w c1="DT" c2="AT1">an</w>
<w c1="JJ" c2="JJ">important</w>
...
Figure 3: TEI encoding of annotated tokens
is given in Figure 3.
Given the high density of technical terms,
biomedical texts are rife with various types of con-
tractions, such as abbreviations, acronyms, prefixes,
etc. As seen already in Figure 3, one of the
more problematic apects of tokenisaton are paren-
theses. Almost all tokenisers (e.g., the LT one, or
the UPENN tokeniser) take these as separate tokens,
but many are in biomedical texts parts of terms. So,
out of almost 35,000 distinct terms that have been
marked up in the GENIA corpus, over 1,700 con-
tain parentheses. Some examples: (+)-pentazocine,
(3H)-E2 binding, (gamma(c))-like molecule.
Correct tokenisation of the biomedical texts is
thus a challenging tasks, and it is fair to say that,
from a linguistic processing perspective, complex
tokenisation is one of the defining characteristics of
such corpora.
5.2 Terms
Annotation of terms is a prerequisite for meaningful
processing of biomedical texts, yet it is often diffi-
cult to decide what constitutes a term in a text, and
how to abstract away from local variations. Biomed-
ical texts are largerly (one could almost say excu-
sivelly) composed of terms, and, as mentioned, this
brings with it complex abbreviatory mechanisms.
Even though TEI offers a ?term? element, we
chose, in line with the original GPML encoding, to
rather use the TEI.analysis clause (?cl?) element to
encode terms. In GENIA, the terms have been hand-
annotated, and marked up with concepts from the
GENIA ontology; this was also the defining factor
of term-hood, namely that the term could be linked
<!DOCTYPE set SYSTEM "gpml.dtd"> <!DOCTYPE teiCorpus.2 SYSTEM "genia-tei.dtd">
<set> <TEIcorpus.2>
<article> <teiHeader type="corpus">
<articleinfo><bibliomisc> *Corpus_header*</teiHeader>
*MEDLINE_ID* <TEI.2 id="*MEDLINE_ID*">
</bibliomisc></articleinfo> <teiHeader type="text">
<title> *Article_header*</teiHeader>
*Title_of_article* <text><body>
</title> <div type="abstract">
<abstract> <head>*Title_of_article*</head>
*Abstract_of_article* <p>*Abstract_of_article*</p>
</abstract> </div>
</article> </body></text></TEI.2>
*More_articles* *More_articles*
</set> </TEIcorpus.2>
Figure 2: The GPML and TEI structure of the corpus
to a terminal concept of the GENIA ontology.
In spite of the simple semantic definition, the syn-
tactic structure of the terms in the corpus varies
dramatically. Biomedical terms are in some ways
similar to named entities (names of people, orga-
nizations, etc.) but from the linguistic perspective,
they are different in that named entities are mostly
proper nouns, while terms mostly contain common
nouns, and the two differ in their syntactic proper-
ties. Terms in the corpus can also be nested, where
complex terms are composed out of simpler ones,
e.g., ?cl??cl?IL-2 gene?/cl? transcription?/cl?.
This nesting, and the reference to ontology con-
cepts is often far from simple, as (partial) terms can
appear in coordinated clauses involving ellipsis. For
example, ?CD2 and CD 25 receptors? refers to two
terms, CD2 receptors and CD25 receptors, but only
the latter actually appears in the text.
In such cases by parsing the coordination
all the terms can be identified and annotated;
the TEI encoding achieves this by specifyng
the propositional formula involving the par-
ticipating concepts in the function attribute;
for example, ?cl function=?(AND G.tissue
G.tissue)? ana=?G.tissue???cl?normal?/cl? and
?cl?hypopigmented?/cl? ?cl?skin samples?/cl??/cl?.
The ana attribute encodes the IDREF of the con-
cept; currently, only same valued concepts are either
conjoined or disjoined.
The number of ?cl? elements in the GENIA cor-
pus is 96,582, among which 89,682 are simple terms
and 1,583 are nested terms that are contain 3,431
terms. 5,137 terms do not yet have the ana attribute
for concept identification, so the total number of
ontology-linked terms is 93,293.
5.3 Ontologies
One of the more interesting questions in recoding
GENIA in TEI was how to encode the ontology. The
ontology is in GENIA GPML encoded in a separate
document, conforming to the OIL+DAML specifi-
cation. This, inter alia, means that that XML file
heavily relies on XML Namespaces and the RDF
recommendation. An illustrative fragment is given
on the left side of Figure 4.
Currently the GENIA ontology has a simple tree-
like structure, i.e., it corresponds to a taxonomy,
so we translated it to the TEI ?taxonomy? element,
which is contained in the ?classDecl? of the header
?encodingDesc?. The TEI defines this element
as ?[the classification declaration] contains one or
more taxonomies defining any classificatory codes
used elsewhere in the text?, i.e., is exactly suited for
our purposes.
There are quite substantial differences between
the two encodings: the DAML+OIL models class
inclusion with links, while the TEI does it as XML
element inclusion. This is certainly the simpler and
more robust solution, but requires that the ontol-
ogy is a taxonomy, i.e., tree structured. The sec-
ond difference is in the status of the identifiers: in
DAML+OIL they are general #CDATA links, which
need a separate (XLink/XPointer) mechanisms for
their resolution. In TEI they are XML ID attributes,
<daml:Class rdf:ID="source"></daml:Class> <taxonomy id="G.taxonomy">
<daml:Class rdf:ID="natural"> <category id="G.source">
<rdfs:subClassOf rdf:resource="#source"/> <catDesc>biological source</catDesc>
</daml:Class> <category id="G.natural">
<daml:Class rdf:ID="organism"> <catDesc>natural</catDesc>
<rdfs:subClassOf rdf:resource="#natural"/> <category id="G.organism">
</daml:Class> <catDesc>organism</catDesc>
<daml:Class rdf:ID="multi_cell"> <category id="G.multi_cell">
<rdfs:subClassOf rdf:resource="#organism"/> <catDesc>multi-cellular</catDesc>
</daml:Class> </category>
... ...
Figure 4: The GENIA DAML+OIL and TEI ontology
and can rely on the XML parser to resolve them.
While this is a simpler solution, it does support
document-internal reference only.
6 Conclusions
The paper proposed an XML paramterisation of TEI
P4 developed for linguistically annotated biomedi-
cal corpora, and applied it to the GENIA corpus.
The conversion from the Genia Project Markup Lan-
guage to this encoding has been implemented in
XSLT and both the TEI-conformant parametrisation
(TEI extension file and one-file DTD) and the XSLT
stylesheets are, together with a report documenting
them, available at http://nl.ijs.si/et/genia/, while the
GENIA corpus is freely available from http://www-
tsujii.is.s.u-tokyo.ac.jp/GENIA/.
The paper gave a survey of the TEI modules that
can be useful for encoding a wide variety of linguis-
tically annotated corpora. This contribution, it is
hoped, can thus serve as a blueprint for parametris-
ing TEI for diverse corpus resources.
Further work involves the inclusion of other
knowledge sources into the corpus, say of Medi-
cal Subject Headings (MeSH), Unified Medical Lan-
guage System (UMLS), International Classification
of Disease (ICD), etc. The place of these annota-
tions in the corpus will have to be considered, and
their linking to the existing information determined.
References
Tomaz? Erjavec, Jin-Dong Kim, Tomoko Ohta, Yuka
Tateisi, and Jun ichi Tsujii. 2003. Stretching the TEI:
Converting the GENIA corpus. In Proceedings of the
EACL-03 Workshop on Linguistically Interpreted Cor-
pora (LINC-03), pages 117?124, Budapest. ACL.
Marion Freese, Ulrich Heid, and Martin Emele. 2003.
Enhancing XCES to XCOMFORT: An Extensible
Modular Architecture for Manipulation of Text Re-
sources. In Proceedings of the EACL-03 Workshop
on Language Technology and the Semantic Web: 3rd
Workshop on NLP and XML (NLPXML-2003), pages
33?40, Budapest. ACL.
Claire Grover, Ewan Klein, Alex Lascarides, and Maria
Lapata. 2002. XML-based NLP Tools for Analysing
and Annotating Medical Language. In 2nd Workshop
on NLP and XML (CoLing Workshop NLPXML-2002).
http://www.ltg.ed.ac.uk/software/ttt/.
Nancy Ide and Chris Brew. 2000. Requrements, Tools
and Architectures for Annotated Corpora. In Proceed-
ings of Data Architectures and Software Support for
Large Corpora, pages 1?5, Budapest. ELRA.
Jin-Dong Kim, Tomoko Ohta, and Jun-ichi Tsujii. 2001.
XML-based Linguistic Annotation of Corpus. In Pro-
ceedings of the first NLP and XML Workshop, pages
44?53.
Tomoko Ohta, Yuka Tateisi, and Jin-Dong Kim. 2002.
The GENIA Corpus: an Annotated Research Abstract
Corpus in Molecular Biology Domain. In Proceedings
of the Human Language Technology Conference, page
To appear.
C. M. Sperberg-McQueen and Lou Burnard, editors.
2002. Guidelines for Electronic Text Encoding and
Interchange, The XML Version of the TEI Guidelines.
The TEI Consortium. http://www.tei-c.org/.
Event Detection and Summarization in Weblogs with Temporal Collocations 
Chun-Yuan Teng and Hsin-Hsi Chen 
Department of Computer Science and Information Engineering 
National Taiwan University 
Taipei, Taiwan 
{r93019, hhchen}@csie.ntu.edu.tw 
Abstract 
 
This paper deals with the relationship between weblog content and time. With the proposed temporal mutual information, we analyze 
the collocations in time dimension, and the interesting collocations related to special events. The temporal mutual information is 
employed to observe the strength of term-to-term associations over time. An event detection algorithm identifies the collocations that 
may cause an event in a specific timestamp. An event summarization algorithm retrieves a set of collocations which describe an event. 
We compare our approach with the approach without considering the time interval. The experimental results demonstrate that the 
temporal collocations capture the real world semantics and real world events over time. 
 
1. 
2. 
Introduction 
Compared with traditional media such as online news 
and enterprise websites, weblogs have several unique 
characteristics, e.g., containing abundant life experiences 
and public opinions toward different topics, highly 
sensitive to the events occurring in the real world, and 
associated with the personal information of bloggers. 
Some works have been proposed to leverage these 
characteristics, e.g., the study of the relationship between 
the content and bloggers? profiles (Adamic & Glance, 
2005; Burger & Henderson, 2006; Teng & Chen, 2006), 
and content and real events (Glance, Hurst & Tornkiyo, 
2004; Kim, 2005; Thelwall, 2006; Thompson, 2003). 
In this paper, we will use temporal collocation to 
model the term-to-term association over time.  In the past, 
some useful collocation models (Manning & Sch?tze, 
1999) have been proposed such as mean and variance, 
hypothesis test, mutual information, etc. Some works 
analyze the weblogs from the aspect of time like the 
dynamics of weblogs in time and location (Mei, et al, 
2006), the weblog posting behavior (Doran, Griffith & 
Henderson, 2006; Hurst, 2006), the topic extraction (Oka, 
Abe & Kato, 2006), etc. The impacts of events on social 
media are also discussed, e.g., the change of weblogs after 
London attack (Thelwall, 2006), the relationship between 
the warblog and weblogs (Kim, 2005; Thompson, 2003), 
etc. 
This paper is organized as follows. Section 2 defines 
temporal collocation to model the strength of term-to-term 
associations over time.  Section 3 introduces an event 
detection algorithm to detect the events in weblogs, and 
an event summarization algorithm to extract the 
description of an event in a specific time with temporal 
collocations. Section 4 shows and discusses the 
experimental results.  Section 5 concludes the remarks. 
Temporal Collocations 
We derive the temporal collocations from Shannon?s 
mutual information (Manning & Sch?tze, 1999) which is 
defined as follows (Definition 1). 
Definition 1 (Mutual Information) The mutual 
information of two terms x and y is defined as: 
)()(
),(log),(),(
yPxP
yxPyxPyxI =  
where P(x,y) is the co-occurrence probability of x and y, 
and P(x) and P(y) denote the occurrence probability of x 
and y, respectively. 
Following the definition of mutual information, we 
derive the temporal mutual information modeling the 
term-to-term association over time, and the definition is 
given as follows.  
 Definition 2 (Temporal Mutual Information) Given 
a timestamp t and a pair of terms x and y, the temporal 
mutual information of x and y in t is defined as: 
)|()|(
)|,(log)|,()|,(
tyPtxP
tyxPtyxPtyxI =
where P(x,y|t) is the probability of co-occurrence of terms 
x and y in timestamp t, P(x|t) and P(y|t) denote the 
probability of occurrences of x and y in timestamp t, 
respectively. 
To measure the change of mutual information in time 
dimension, we define the change of temporal mutual 
information as follows. 
Definition 3 (Change of Temporal Mutual 
Information) Given time interval [t1, t2], the change of 
temporal mutual information is defined as: 
12
12
21
)|,()|,(),,,(
tt
tyxItyxIttyxC ?
?=  
where C(x,y,t1,t2) is the change of temporal mutual 
information of terms x and y in time interval [t1, t2], I(x,y| 
t1) and I(x,y| t2) are the temporal mutual information in 
time t1 and t2, respectively. 
3. Event Detection 
Event detection aims to identify the collocations 
resulting in events and then retrieve the description of 
events. Figure 1 sketches an example of event detection. 
The weblog is parsed into a set of collocations. All 
collocations are processed and monitored to identify the 
plausible events.  Here, a regular event ?Mother?s day? 
and an irregular event ?Typhoon Chanchu? are detected.  
The event ?Typhoon Chanchu? is described by the words  
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1: An Example of Event Detection
?Typhoon?, ?Chanchu?, ?2k?, ?Eye?, ?Path? and 
?chinaphillippine?.  
The architecture of an event detection system includes 
a preprocessing phase for parsing the weblogs and 
retrieving the collocations; an event detection phase 
detecting the unusual peak of the change of temporal 
mutual information and identifying the set of collocations 
which may result in an event in a specific time duration; 
and an event summarization phase extracting the 
collocations related to the seed collocations found in a 
specific time duration. 
The most important part in the preprocessing phase is 
collocation extraction. We retrieve the collocations from 
the sentences in blog posts. The candidates are two terms 
within a window size. Due to the size of candidates, we 
have to identify the set of tracking terms for further 
analysis. In this paper, those candidates containing 
stopwords or with low change of temporal mutual 
information are removed. 
In the event detection phase, we detect events by 
using the peak of temporal mutual information in time 
dimension.  However, the regular pattern of temporal 
mutual information may cause problems to our detection. 
Therefore, we remove the regular pattern by seasonal 
index, and then detect the plausible events by measuring 
the unusual peak of temporal mutual information. 
If a topic is suddenly discussed, the relationship 
between the related terms will become higher. Two 
alternatives including change of temporal mutual 
information and relative change of temporal mutual 
information are employed to detect unusual events. Given 
timestamps t1 and t2 with temporal mutual information 
MI1 and MI2, the change of temporal mutual information 
is calculated by (MI2-MI1). The relative change of 
temporal mutual information is calculated by (MI2-
MI1)/MI1. 
For each plausible event, there is a seed collocation, 
e.g., ?Typhoon Chanchu?. In the event description 
retrieval phase, we try to select the collocations with the 
highest mutual information with the word w in a seed 
collocation. They will form a collocation network for the 
event.  Initially, the seed collocation is placed into the 
network.  When a new collocation is added, we compute 
the mutual information of the multiword collocations by 
the following formula, where n is the number of 
collocations in the network up to now. 
?= n iMInInformatioMutualMultiwo  
If the multiword mutual information is lower than a 
threshold, the algorithm stops and returns the words in the 
collocation network as a description of the event.  Figure 
2 sketches an example.  The collocations ?Chanchu?s 
path?, ?Typhoon eye?, and ?Chanchu affects? are added 
into the network in sequence based on their MI. 
We have two alternatives to add the collocations to 
the event description. The first method adds the 
collocations which have the highest mutual information 
as discussed above. In contrast, the second method adds 
the collocations which have the highest product of mutual 
information and change of temporal mutual information. 
 
 
 
 
 
 
Figure 2: An Example of Collocation network 
4. 
4.1. 
Experiments and Discussions 
Temporal Mutual Information versus 
Mutual Information 
In the experiments, we adopt the ICWSM weblog data 
set (Teng & Chen, 2007; ICWSM, 2007). This data set 
collected from May 1, 2006 through May 20, 2006 is 
about 20 GB. Without loss of generality, we use the 
English weblog of 2,734,518 articles for analysis. 
To evaluate the effectiveness of time information, we 
made the experiments based on mutual information 
(Definition 1) and temporal mutual information 
(Definition 2). The former called the incremental 
approach measures the mutual information at each time 
point based on all available temporal information at that 
time. The latter called the interval-based approach 
considers the temporal mutual information in different 
time stamps.  Figures 3 and 4 show the comparisons 
between interval-based approach and incremental 
approach, respectively, in the event of Da Vinci Code.   
We find that ?Tom Hanks? has higher change of 
temporal mutual information compared to ?Da Vinci 
Code?. Compared to the incremental approach in Figure 4, 
the interval-based approach can reflect the exact release 
date of ?Da Vinci Code.? 
 rd
=i 1 4.2. Evaluation of Event Detection 
We consider the events of May 2006 listed in 
wikipedia1 as gold standard. On the one hand, the events 
posted in wikipedia are not always complete, so that we 
adopt recall rate as our evaluation metric.  On the other 
hand, the events specified in wikipedia are not always 
discussed in weblogs.  Thus, we search the contents of 
blog post to verify if the events were touched on in our 
blog corpus. Before evaluation, we remove the events 
listed in wikipedia, but not referenced in the weblogs. 
 
 
 
 
 
 
 
 
 
 
 
Figure 3: Interval-based Approach in Da Vinci Code  
 
 
 
 
 
 
 
 
Figure 4: Incremental Approach in Da Vinci Code 
gure 5 sketches the idea of evaluation.  The left side 
of t s figure shows the collocations detected by our event 
dete tion system, and the right side shows the events 
liste  in wikipedia.  After matching these two lists, we 
can find that the first three listed events were correctly 
identified by our system.  Only the event ?Nepal Civil 
War? was listed, but not found. Thus, the recall rate is 
75% in this case. 
 
 
 
 
 
 
 
Figure 5: Evaluation of Event Detection Phase 
As discussed in Section 3, we adopt change of 
temporal mutual information, and relative change of 
temporal mutual information to detect the peak. In Figure 
6, we compare the two methods to detect the events in 
weblogs. The relative change of temporal mutual 
information achieves better performance than the change 
of temporal mutual information. 
                                                     
1 http://en.wikipedia.org/wiki/May_2006 
Table 1 and Table 2 list the top 20 collocations based 
on these two approaches, respectively. The results of the 
first approach show that some collocations are related to 
the feelings such as ?fell left? and time such as ?Saturday 
night?. In contrast, the results of the second approach 
show more interesting collocations related to the news 
events at that time, such as terrorists ?zacarias 
moussaoui? and ?paramod mahajan.? These two persons 
were killed in May 3. Besides, ?Geena Davis? got the 
golden award in May 3. That explains why the 
collocations detected by relative change of temporal 
mutual information are better than those detected by 
change of temporal mutual information. 
-20
-15
-10
-5
0
5
10
1 3 5 7 9 11 13 15 17 19
Time (day)
M
ut
ua
l i
nf
or
m
at
io
n
Da-Vinci Tom Hanks
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 6: Performance of Event Detection Phase 
-15
-10
-5
0
5
10
1 3 5 7 9 11 13 15 17 19
Time (day)
M
ut
ua
l i
nf
or
m
at
io
n
Da-Vinci Tom Hanks
Collocations CMI Collocations CMI 
May 03 9276.08 Current music 1842.67
Illegal immigrants 5833.17 Hate studying 1722.32
Feel left 5411.57 Stephen Colbert 1709.59
Saturday night 4155.29 Thursday night 1678.78
Past weekend 2405.32 Can?t believe 1533.33
White house 2208.89 Feel asleep 1428.18
Red sox 2208.43 Ice cream 1373.23
Album tool 2120.30 Oh god 1369.52
Sunday morning 2006.78 Illegalimmigration 1368.12
16.56
f 
CMI
32.50
31.63
29.09
28.45
28.34
28.13Sunday night 1992.37 Pretty cool 13
Table 1: Top 20 collocations with highest change o
temporal mutual information 
Collocations CMI Collocations 
casinos online 618.36 Diet sodas 
zacarias moussaoui 154.68 Ving rhames 
Tsunami warning 107.93 Stock picks 
Conspirator zacarias 71.62 Happy hump 
Artist formerly 57.04 Wong kan 
Federal  
Jury 
41.78 Sixapartcom 
movabletype Wed 3 39.20 Aaron echolls 27.48
Pramod mahajan 35.41 Phnom penh 25.78
BBC  
Version 
35.21 Livejournal 
sixapartcom 
23.83  Fi
hi
c
dGeena davis 33.64 George yeo 20.34
Table 2: Top 20 collocations with highest relative change 
of mutual information 
4.3. Evaluation of Event Summarization 
As discussed in Section 3, we have two methods to 
include collocations to the event description. Method 1 
employs the highest mutual information, and Method 2 
utilizes the highest product of mutual information and 
change of temporal mutual information. Figure 7 shows 
the performance of Method 1 and Method 2. We can see 
that the performance of Method 2 is better than that of 
Method 1 in most cases. 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 7: Overall Performance of Event Summarization 
The results of event summarization by Method 2 are 
shown in Figure 8. Typhoon Chanchu appeared in the 
Pacific Ocean on May 10, 2006, passed through 
Philippine and China and resulted in disasters in these 
areas on May 13 and 18, 2006.  The appearance of the 
typhoon Chanchu cannot be found from the events listed 
in wikipedia on May 10.  However, we can identify the 
appearance of typhoon Chanchu from the description of 
the typhoon appearance such as ?typhoon named? and 
?Typhoon eye.  In addition, the typhoon Chanchu?s path 
can also be inferred from the retrieved collocations such 
as ?Philippine China? and ?near China?. The response of 
bloggers such as ?unexpected typhoon? and ?8 typhoons? 
is also extracted.   
 
 
 
 
 
 
 
 
 
 
Figure 8: Event Summarization for Typhoon Chanchu 
5. Concluding Remarks 
This paper introduces temporal mutual information to 
capture term-term association over time in weblogs. The 
extracted collocation with unusual peak which is in terms 
of relative change of temporal mutual information is 
selected to represent an event.  We collect those 
collocations with the highest product of mutual 
information and change of temporal mutual information 
to summarize the specific event.  The experiments on 
ICWSM weblog data set and evaluation with wikipedia 
event lists at the same period as weblogs demonstrate the 
feasibility of the proposed temporal collocation model 
and event detection algorithms. 
Currently, we do not consider user groups and 
locations. This methodology will be extended to model 
the collocations over time and location, and the 
relationship between the user-preferred usage of 
collocations and the profile of users. 
Acknowledgments 
Research of this paper was partially supported by 
National Science Council, Taiwan (NSC96-2628-E-002-
240-MY3) and Excellent Research Projects of National 
Taiwan University (96R0062-AE00-02). 
References 
Adamic, L.A., Glance, N. (2005). The Political 
Blogosphere and the 2004 U.S. Election: Divided 
They Blog. In: Proceedings of the 3rd International 
Workshop on Link Discovery, pp. 36--43. 
Burger, J.D., Henderson J.C. (2006). An Exploration of 
Observable Features Related to Blogger Age. In: 
Proceedings of AAAI 2006 Spring Symposium on 
Computational Approaches to Analysing Weblogs, pp. 
15--20. 
Doran, C., Griffith, J., Henderson, J. (2006). Highlights 
from 12 Months of Blogs. In: Proceedings of AAAI 
2006 Spring Symposium on Computational 
Approaches to Analysing Weblogs, pp. 30--33. 
Glance, N., Hurst, M., Tornkiyo, T. (2004). Blogpulse: 
Automated Trend Discovery for Weblogs. In: 
Proceedings of WWW 2004 Workshop on the 
Weblogging Ecosystem: Aggregation, Analysis, and 
Dynamics. 
Hurst, M. (2006). 24 Hours in the Blogosphere. In: 
Proceedings of AAAI 2006 Spring Symposium on 
Computational Approaches to Analysing Weblogs, pp. 
73--77. 
ICWSM (2007). http://www.icwsm.org/data.html 
Kim, J.H. (2005). Blog as an Oppositional Medium? A 
Semantic Network Analysis on the Iraq War Blogs. In: 
Internet Research 6.0: Internet Generations. 
 
Manning, C.D., Sch?tze, H. (1999). Foundations of 
Statistical Natural Language Processing, The MIT 
Press, London England. 
Mei, Q., Liu, C., Su, H., Zhai, C. (2006). A Probabilistic 
Approach to Spatiotemporal Theme Pattern Mining on 
Weblogs. In: Proceedings of the 15th International 
Conference on World Wide Web, Edinburgh, Scotland, 
pp. 533--542. 
Oka, M., Abe, H., Kato, K. (2006). Extracting Topics 
from Weblogs Through Frequency Segments. In: 
Proceedings of WWW 2006 Annual Workshop on the 
Weblogging Ecosystem: Aggregation, Analysis, and 
Dynamics. 
Teng, C.Y., Chen, H.H. (2006). Detection of Bloggers? 
Interest: Using Textual, Temporal, and Interactive 
Features. In: Proceeding of IEEE/WIC/ACM 
International Conference on Web Intelligence, pp. 
366--369. 
Teng, C.Y., Chen, H.H. (2007). Analyzing Temporal 
Collocations in Weblogs. In: Proceeding of 
International Conference on Weblogs and Social 
Media, 303--304. 
Thelwall, M. (2006). Blogs During the London Attacks: 
Top Information Sources and Topics. In: Proceedings 
of 3rd Annual Workshop on the Weblogging 
Ecosystem: Aggregation, Analysis and Dynamics. 
Thompson, G. (2003). Weblogs, Warblogs, the Public 
Sphere, and Bubbles. Transformations, 7(2). 
Proceedings of the BioNLP Workshop on Linking Natural Language Processing and Biology at HLT-NAACL 06, pages 136?137,
New York City, June 2006. c?2006 Association for Computational Linguistics
Subdomain adaptation of a POS tagger with a small corpus 
 
1 Introduction    
For the domain of biomedical research abstracts, 
two large corpora, namely GENIA (Kim et al
2003) and Penn BioIE (Kulik et al2004) are avail-
able. Both are basically in human domain and the 
performance of systems trained on these corpora 
when they are applied to abstracts dealing with 
other species is unknown. In machine-learning-
based systems, re-training the model with addition 
of corpora in the target domain has achieved prom-
ising results (e.g. Tsuruoka et al2005, Lease et al
2005). In this paper, we compare two methods for 
adaptation of POS taggers trained for GENIA and 
Penn BioIE corpora to Drosophila melanogaster 
(fruit fly) domain. 
2 Method 
Maximum Entropy Markov Models (MEMMs) 
(Ratnaparkhi 1996) and their extensions (Tutanova 
et al2003, Tsuruoka et al2005) have been success-
fully applied to English POS tagging. Here we use 
second-order standard MEMMs for learning POS. 
where the model parameters are determined with 
maximum entropy criterion in combination a regu-
larization method called inequality constraints 
(Kazama and Tsujii 2003). This regularization 
method has one non-negative meta-parameter 
called width-factor that controls the ?fitness? of the 
model parameters to the training data.
We used two methods of adapting a POS tagging 
model. One is to add the domain corpus to the 
training set. The other is to use the reference distri-
bution modeling, in which the training is per-
                                                          
    This work is partially supported by SORST program, Japan 
Science and Technology Agency. 
formed only on the domain corpus and the infor-
mation about the original training set is incorpo-
rated in the form of the reference distribution in 
the maximum entropy formulation (Johnson et al
2000, Hara et al2005). 
A set of 200 MEDLINE abstracts on D. 
melanogaster, was manually annotated with POS 
according to the scheme of the GENIA POS corpus 
(Tateisi et al2004) by one annotator. The new cor-
pus consists of 40,200 tokens in 1676 sentences. 
From this corpus which we call ?Fly? hereafter, 
1024 sentences are randomly taken and used for 
training. Half of the remaining is used for devel-
opment and the rest is used for testing.  
We measured the accuracy of the POS tagger 
trained in three settings:  
Original: The tagger is trained with the union of 
Wall Street Journal (WSJ) section of Penn 
Treebank (Marcus et al1993), GENIA, and 
Penn BioIE. In WSJ, Sections 0-18 for train-
ing, 19-21 for development, and 22-24 for 
test. In GENIA and Penn BioIE, 90% of the 
corpus is used for training and the rest is 
used for test. 
Combined: The tagger is trained with the union 
of the Original set plus N sentences from Fly.  
Refdist: Tagger is trained with N sentences 
from Fly, plus the Original set as reference. 
In Combined and Refdist settings, N is set to 8, 16, 
32, 64, 128, 256, 512, 1024 sentences to measure 
the learning curve. 
3 Results 
The accuracies of the tagger trained in the Origi-
nal setting were 96.4% on Fly, 96.7% on WSJ, 
Yuka Tateisi Yoshimasa Tsuruoka Jun-ichi Tsujii
Faculty of Informatics 
 Kogakuin University 
 Nishishinjuku 1-24-2 
Shinjuku-ku, Tokyo, 163-
8677, Japan 
School of Informatics 
 University of Manchester 
 Manchester M60 1QD, U.K. 
Dept. of Computer Science 
University of Tokyo 
Hongo 7-3-1, Bunkyo-ku, 
Tokyo 113-0033, Japan 
School of Informatics 
 University of Manchester 
 Manchester M60 1QD, U.K.
136
98.1% on GENIA and 97.7% on Penn BioIE cor-
pora respectively. In the Combined setting, the ac-
curacies were 97.9% on Fly, 96.7% on WSJ, 
98.1% on GENIA and 97.7% on Penn BioIE. With 
Refdist setting, the accuracy on the Fly corpus was 
raised but those for WSJ and Penn BioIE corpora 
dropped from Original. When the width factor w 
was 10, the accuracy was 98.1% on Fly, but 95.4% 
on WSJ, 98.3% on GENIA and 96.6% on Penn 
BioIE. When the tagger was trained only on WSJ 
the accuracies were 88.7% on Fly, 96.9% on WSJ, 
85.0% on GENIA and 86.0% on Penn BioIE. 
When the tagger was trained only on Fly, the accu-
racy on Fly was even lower (93.1%). The learning 
curve indicated that the accuracies on the Fly cor-
pus were still rising in both Combined and Refdist 
settings, but both accuracies are almost as high as 
those of the original tagger on the original corpora 
(WSJ, GENIA and Penn BioIE), so in practical 
sense, 1024 sentences is a reasonable size for the 
additional corpus. When the width factor was 
smaller (2.5 and 5) the accuracies on the Fly cor-
pus were saturated with N=1024 with lower values 
(97.8% with w=2.5 and 98.0% with w=5).  
The amount of resources required for the Com-
bined and the Refdist settings were drastically dif-
ferent. In the Combined setting, the learning time 
was 30632 seconds and the required memory size 
was 6.4GB. On the other hand, learning in the Ref-
dist setting took only 21 seconds and the required 
memory size was 157 MB. 
The most frequent confusions involved the con-
fusion between FW (foreign words) with another 
class. Further investigation revealed that most of 
the error involved Linnaean names of species. Lin-
naean names are tagged differently in the GENIA 
and Penn BioIE corpora. In the GENIA corpus, 
tokens that constitute a Linnaean name are tagged 
as FW (foreign word) but in the Penn BioIE corpus 
they are tagged as NNP (proper noun). This seems 
to be one of the causes of the drop of accuracy on 
the Penn BioIE corpus when more sentences from 
the Fly corpus, whose tagging scheme follows that 
of GENIA, are added for training.
4 Conclusions 
We compared two methods of adapting a POS tag-
ger trained on corpora in human domain to fly do-
main. Training in Refdist setting required much 
smaller resources to fit to the target domain, but 
the resulting tagger is less portable to other do-
mains. On the other hand, training in Combined 
setting is slower and requires huge memory, but 
the resulting tagger is more robust, and fits rea-
sonably to various domains. 
References 
Tadayoshi Hara, Yusuke Miyao and Jun'ichi Tsujii. 
2005. Adapting a probabilistic disambiguation model 
of an HPSG parser to a new domain. In Proceedings 
of  IJCNLP 2005, LNAI 3651, pp. 199-210. 
Mark Johnson and Stefan Riezler. 2000.  Exploiting 
auxiliary distributions in stochastic unification-based 
grammars. In Proceedings of 1st NAACL.  
Jun?ichi Kazama and Jun?ichi Tsujii. 2003. Evaluation 
and extension of maximum entropy models with ine-
quality constraints. In Proceedings of EMNLP 2003. 
Jin-Dong Kim, Tomoko Ohta, Yuka Tateisi, and 
Jun?ichi Tsujii. 2003. GENIA corpus ? a semanti-
cally annotated corpus for bio-textmining. Bioinfor-
matics, 19(Suppl. 1):i180?i182. 
Seth Kulick, Ann Bies, Mark Liberman, Mark Mandel, 
Ryan McDonald, Martha Palmer, Andrew Schein, 
Lyle Ungar, Scott Winters, and Pete White. 2004. In-
tegrated annotation for biomedical information ex-
traction. In Proceedings of BioLINK 2004, pp. 61?68. 
Matthew Lease and Eugene Charniak. 2005. Parsing 
Biomedical Literature, In Proceedings of  IJCNLP 
2005, LNAI 3651, pp. 58-69. 
Mitchell P. Marcus, Beatrice Sanorini and Mary Ann 
Marcinkiewicz. 1993. Building a large annotated 
corpus of English: the Penn Treebank.  Computa-
tional Linguistics, Vol.19, pp. 313-330.  
Adwait Ratnaparkhi. 1996. A Maximum Entropy Model 
for Part-Of-Speech Tagging. In Proceedings of 
EMNLP 1996. 
Yuka Tateisi and Jun'ichi Tsujii. (2004). Part-of-Speech 
Annotation of Biology Research Abstracts. In the 
Proceedings of LREC2004, vol. IV, pp. 1267-1270. 
Kristina Toutanova,  Dan Klein, Christopher Manning 
and Yoram Singer. 2003. Feature-Rich Part-of-
Speech Tagging with a Cyclic Dependency Network. 
In Proceedings of HLT-NAACL 2003, pp. 173-180. 
Yoshimasa Tsuruoka, Yuka Tateishi, Jin-Dong Kim, 
Tomoko Ohta, John McNaught, Sophia Ananiadou, 
and Jun'ichi Tsujii. 2005. Developing a Robust Part-
of-Speech Tagger for Biomedical Text. In Proceed-
ings of 10th Panhellenic Conference on Informatics, 
LNCS 3746, pp. 382-392.  
137
Coling 2008: Proceedings of the workshop on Cross-Framework and Cross-Domain Parser Evaluation, pages 17?23
Manchester, August 2008
Toward an Underspecifiable Corpus Annotation Scheme 
Yuka Tateisi 
Department of Informatics, Kogakuin University  
1-24-2 Nishi-shinjuku, Shinjuku-ku, Tokyo, 163-8677, Japan 
yucca@cc.kogakuin.ac.jp 
 
Abstract 
The Wall Street Journal corpora provided 
for the Workshop on Cross-Framework 
and Cross-Domain Parser Evaluation 
Shared Task are investigated in order to 
see how the structures that are difficult 
for an annotator of dependency structure 
are encoded in the different schemes. 
Non-trivial differences among the 
schemes are found. The paper also inves-
tigates the possibility of merging the in-
formation encoded in the different cor-
pora.  
1 Background 
This paper takes a look at several annotation 
schemes related to dependency parsing, from the 
viewpoint of a corpus annotator. The dependency 
structure is becoming a common criterion for 
evaluating parsers in biomedical text mining 
(Clegg and Shepherd, 2007; Pyssalo et al, 
2007a), since their purpose in using parsers are to 
extract predicate-argument relations, which are 
easier to access from dependency than constitu-
ency structure. One obstacle in applying depend-
ency-based evaluation schemes to parsers for 
biomedical texts is the lack of a manually anno-
tated corpus that serves as a gold-standard. 
Aforementioned evaluation works used corpora 
automatically converted to the Stanford depend-
ency scheme (de Marneffe et al, 2006) from 
gold-standard phrase structure trees in the Penn 
Treebank (PTB) (Marcus et al, 1993) format. 
However, the existence of errors in the automatic 
conversion procedure, which are not well-
                                                 
  ? 2008. Licensed under the Creative Commons Attribu-
tion-Noncommercial-Share Alike 3.0 Unported license 
(http://creativecommons.org/licenses/by-nc-sa/3.0/). Some 
rights reserved. 
documented, makes the suitability of the result-
ing corpus for parser evaluation questionable, 
especially in comparing PTB-based parsers and 
parsers based on other formalisms such as CCG 
and HPSG (Miyao et al, 2007). To overcome the 
obstacle, we have manually created a depend-
ency-annotated corpus in the biomedical field 
using the Rasp Grammatical Relations (Briscoe 
2006) scheme (Tateisi et al, 2008). In the anno-
tation process, we encountered linguistic phe-
nomena for which it was difficult to decide the 
appropriate relations to annotate, and that moti-
vated the investigation of the sample corpora 
provided for the Workshop on Cross-Framework 
and Cross-Domain Parser Evaluation Shared 
Task1, in which the same set of sentences taken 
from the Wall Street Journal section from Penn 
Treebank is annotated with different schemes. 
The process of corpus annotation is assigning 
a label from a predefined set to a substring of the 
text. One of the major problems in the process is 
the annotator's lack of confidence in deciding 
which label should be annotated to the particular 
substring of the text, thus resulting in the incon-
sistency of annotation. The lack of confidence 
originates from several reasons, but typical situa-
tions can be classified into two types:  
1) The annotator can think of two or more 
ways to annotate the text, and cannot decide 
which is the best way. In this case, the annotation 
scheme has more information than the annotator 
has. For example, the annotation guideline of 
Penn Treebank (Bies et al 1995) lists alterna-
tives for annotating structures involving null 
constituents that exist in the Treebank. 
 2) The annotator wants to annotate a certain 
information that cannot be expressed properly 
with the current scheme. This is to say, the anno-
tator has more information than the scheme can 
express. 
                                                 
1 http://www-tsujii.is.s.u-tokyo.ac.jp/pe08-st/ 
17
For example, Tateisi et al(2000) report that, in 
the early version of the GENIA corpus,  some 
cases of inter-annotator discrepancy occur be-
cause the class of names to be assigned (e.g. 
PROTEIN) is too coarse-grained for annotators, 
and the result led to a finer-graded classification 
(e.g. PROTEIN-FAMILY, PROTEIN-
COMPLEX) of names in the published version 
of GENIA (Kim et al, 2003). 
In practice, the corpus designers deal with 
these problems by deciding how to annotate the 
questionable cases, and describing them in the 
guidelines, often on an example-by-example ba-
sis. Still, these cases are sources of errors when 
the decision described in the guideline is against 
the intuition of the annotator. 
If the scheme allows the annotator to annotate 
the exact amount of information that (s)he has, 
(s)he would not be uncertain about how to anno-
tate the information. However, because the in-
formation that an annotator has varies from anno-
tator to annotator it is not practical to define a 
scheme for each annotator. Moreover, the result-
ing corpus would not be very useful, for a corpus 
should describe a "common standard" that is 
agreed by (almost) everyone. 
One solution would be to design a scheme that 
is as information-rich as possible, in the way that 
it can be "underspecified" to the amount of the 
information that an annotator has. When the cor-
pus is published, the annotation can be reduced 
to the "most-underspecified" level to ensure the 
uniformity and consistency of annotations, that is, 
to the level that all the annotators involved can 
agree (or the corpus can be published as-is with 
underspecification left to the user). For example, 
annotators may differ in decision about whether 
the POS of "human" in the phrase "human anno-
tator" is an NN (common noun) or a JJ (adjec-
tive), but everyone would agree that it is not, for 
example, a VBN (past participle of a verb). In 
that case, the word can be annotated with an un-
derspecified label like "NN or JJ". The Penn 
Treebank POS corpus (Santrini, 1990) allows 
such underspecification (NN|JJ). In the depend-
ency structure annotation, Grammatical Relations 
(Briscoe 2006), for example, allows underspeci-
fication of dependency types by defining the 
class hierarchy of dependency types. The under-
specified annotation is obviously better than dis-
carding the annotation because of inconsistency, 
for the underspecified annotation have much 
more information than nothing at all, and can 
assure consistency over the entire corpus. 
Defining an underspecification has another use. 
There are corpora in similar but different 
schemes, for a certain linguistic aspect (e.g. syn-
tactic structure) based on formalisms suited for 
the application that the developers have in mind. 
That makes the corpus difficult for the use out-
side the group involved in the development of 
the corpus. In addition to the difficulty of using 
the resources across the research groups, the ex-
istence of different formalisms is an obstacle for 
users of NLP systems to compare and evaluate 
the systems. One scheme may receive a de facto 
status, as is the case with the Penn Treebank, but 
it is still unsuitable for applications that require 
the information not encoded in the formalisms or 
to compare systems based on widely different 
formalisms (e.g., CCG or HPSG in the case of 
syntactic parsing).  
If some common aspects are extracted from 
the schemes based on different formalisms, the 
corpus annotated with the (common) scheme will 
be used as a standard for (coarse-grained) evalua-
tion and comparison between systems based on 
different formalisms. If an information-rich 
scheme can be underspecified into a "common" 
level, the rich information in the corpus will be 
used locally for the system development and the 
"common" information can be used by people 
outside the developers' group. The key issue for 
establishing the "common" level would be to 
provide the systematic way to underspecify the 
individual scheme. 
In this paper, the schemes of dependency cor-
pora provided for the Shared Task are compared 
on the problematic linguistic phenomena encoun-
tered in annotating biomedical abstracts, in order 
to investigate the possibility of making the "com-
mon, underspecified" level of annotation. The 
compared schemes are mainly CONLL shared 
task structures (CONLL) 1 , Rasp Grammatical 
Relations (GR) , PARC 700 dependency struc-
tures (PARC)2 and Stanford dependency struc-
tures (Stanford; de Marneffe et al 2006),  with 
partial reference to UTokyo HPSG Treebank 
predicate-argument structures (HPSG; Miyao 
2006) and CCGBank predicate-argument struc-
tures (CCG; Hockenmaier and Steedman 2005). 
2 Underspecification 
In dependency annotation, two types of informa-
tion are annotated to sentences. 
                                                 
1 http://www.yr-bcn.es/conll2008/ 
2 http://www2.parc.com/isl/groups/nltt/fsbank/ 
triplesdoc.html 
18
? Dependency structure: what is dependent 
on what  
? Dependency type: how the dependent 
depends on the head 
For the latter information, schemes like GR and 
Stanford incorporates the hierarchy of 
dependency types and allows systematic 
underspecification but that does not totally solve 
the problem. A case of GR is addressed later. If 
type hierarchy over different schemes can be 
established, it helps cross-scheme comparison. 
For the former information, in cases where some 
information in a corpus is omitted in another (e.g. 
head percolation), the corpus with less 
information is considered as the 
underspecification of the other, but when a 
different structure is assigned, there is no 
mechanism to form the underspecified structure 
so far proposed. In the following section, the 
sample corpora are investigated trying to find the 
difference in annotation, especially of the 
structural difference. 
3 How are problematic structures en-
coded in the sample corpora? 
The Wall Street Journal corpora provided for the 
shared task is investigated in order to look for the 
structures that the annotator of our dependency 
corpus commented as difficult, and to see how 
they are encoded in the different schemes. The 
subsections describe the non-trivial differences 
among the annotation schemes that are found.  
The subsections also discuss the underspecifiable 
annotation where possible. 
3.1 Multi-word Terms 
The structure inside multi-word terms, or more 
broadly, noun-noun sequence in general, have 
been left unannotated in Penn Treebank, and the 
later schemes follow the decision. Here, under-
specification is realized in practice. In depend-
ency schemes where dependency is encoded by a 
set of binary relations, the last element of the 
term is regarded as a head, and the rest of the 
element of the term is regarded as dependent on 
the last. In the PARC annotation, proper names 
like "Los Angeles" and "Alex de Castro" are 
treated as one token.  
However, there are noun sequences in which 
the head is clearly not the last token. For exam-
ple, there are a lot of names in the biomedical 
field where a subtype is specified (e.g. Human 
Immunodeficiency Virus Type I). If the sequence 
is considered as a name (of a type of virus in this 
example), it may be reasonable to assign a flat 
structure to it, wherever the head is. On the 
other hand, a flat structure is not adequate for 
analyzing a structure like "Human Immunodefi-
ciency Virus Type I and Type II".  Thus it is 
conventional to assign to a noun phrase "a flat 
structure unless coordination is involved" in the 
biomedical corpora, e.g., GENIA and Bioinfer 
(Pyssalo et al, 2007b). However, adopting this 
convention can expose the corpus to a risk that 
the instances of a same name can be analyzed 
differently depending on context. 
 
Human Immunodeficiency Virus Type 
I is a ... 
id(name0, Human Immunodeficiency 
Virus Type I) 
id(name1, Human Immunodeficiency 
Virus) 
id(name2, Type I) 
concat(name0, name1, name2) 
subject(is, name0) 
 
Human Immunodeficiency Virus Type 
I and Type II 
id(name3, Type II) 
conj(coord0, name2) 
conj(coord0, name3) 
conj_form(coord0, and) 
 
A possible solution is to annotate a certain 
noun sequence as a term with a non-significant 
internal structure, and where needed, the internal 
structure may be annotated independently of the 
outside structure. The PARC annotation can be 
regarded as doing this kind of annotation by 
treating a multi-word term as token and totally 
ignore the internal structure. Going a step further, 
using IDs to the term and sub-terms, the internal 
structure of a term  can be annotated, and the 
whole term or a subcomponent can be used out-
side, retaining the information where the se-
quence refers to parts of the same name. For ex-
ample, Figure 1 is a PARC-like annotation using 
name-IDs, where id(ID, name) is for assigning 
an ID to a name or a part of a name, and name0, 
name1, name2, and name3 are IDs for "Hu-
man Immunodeficiency Virus Type I", "Human 
Immunodeficiency Virus", "Type I", "Type II", 
and "Human Immunodeficiency Virus Type II" 
respectively, and concat(a, b, c) means that 
strings b and c is concatenated to make string a.  
adjunct(name1, coord0) 
Figure 1. PARC-like annotation with explicit 
annotation of names 
19
3.2 Coordination 
The example above suggests that the coordina-
tion is a problematic structure. In our experience, 
coordination structures, especially ones with el-
lipsis, were a major source of annotation incon-
sistency. In fact, there are significant differences 
in the annotation of coordination in the sample 
corpora, as shown in the following subsections. 
What is the head? 
Among the schemes used in the sample corpora, 
CCG does not explicitly annotate the coordina-
tion but encodes them as if the coordinated con-
stituents exist independently 3 . The remaining 
schemes may be divided into determination of 
the head of coordination. 
? GR, PARC, and HPSG makes the coor-
dinator (and, etc) the head 
? CONLL and Stanford makes the preced-
ing component the head 
For example, in the case with "makes and dis-
tributes", the former group encodes the relation 
into two binary relations where "and" is the head 
(of both), and "makes" and "distributes" are the 
dependent on "and". In the latter group, CONLL 
encodes the coordination into two binary rela-
tions: one is the relation where "makes" is the 
head and "and" is the dependant and another 
where "and" is the head and "distributes" is the 
dependent. In Stanford scheme, the coordinator 
is encoded into the type of relation (conj_and) 
where "makes" is the head and "distributes" is 
the dependent.  As for the CCG scheme, the in-
formation that the verbs are coordinated by "and" 
is totally omitted. The difference of policy on 
head involves structural discrepancy where un-
derspecification does not seem easy. 
Distribution of the dependents 
Another difference is in the treatment of depend-
ents on the coordinated head. For example, the 
first sentence of the corpus can be simplified to 
"Bell makes and distributes products". The sub-
ject and object of the two verbs are shared: 
"Bell" is the subject of "makes" and "distributes", 
and "products" is their direct object. The subject 
                                                                                                 
3 Three kinds of files for annotating sentence structures are 
provided in the original CCGbank corpus: the human-
readable corpus files, the machine-readable derivation files, 
and the predicate-argument structure files. 
The coordinators are marked in the human-readable corpus 
files, but not in the predicate-argument structure files from 
which the sample corpus for the shared task was derived. 
is treated as dependent on the coordinator in GR, 
dependent on the coordinator as well as both 
verbs in PARC 4 , dependent on both verbs in 
HPSG and Stanford (and CCG), and dependent 
on "makes" in CONLL. As for the object, "prod-
ucts" is treated as dependent on the coordinator 
in GR and PARC, dependent on both verbs in 
HPSG (and CCG), and dependent on "makes" in 
CONLL and Stanford. The Stanford scheme uni-
formly treats subject and object differently: The 
subject is distributed among the coordinated 
verbs, and the object is treated as dependent on 
the first verb only. 
A different phenomenon was observed for 
noun modifiers. For example, semantically, 
"electronic, computer and building products" in 
the first sentence should be read as "electronic 
products and computer products and building 
products" not as "products that have electronic 
and computer and building nature". That is, the 
coordination should be read distributively. The 
distinction between distributive and non-
distributive reading is necessary for applications 
such as information extraction. For example, in 
the biomedical text, it must be determined 
whether "CD4+ and CD8+ T cells" denotes "T 
cells expressing CD4 and T cells expressing 
CD8" or "T cells expressing both CD4 and CD8".  
Coordinated noun modifier is treated differ-
ently among the corpora. The coordinated adjec-
tives are dependent on the noun (like in non-
distributive reading) in GR, CONLL, and PARC, 
while the adjectives are treated as separately de-
pendent on the noun in Stanford and HPSG (and 
CCG). In the PARC scheme, there is a relation 
named coord_level denoting the syntactic 
type of the coordinated constituents. For example, 
in the annotation of the first sentence of the sam-
ple corpus ("...electronic, computer and building 
products"), coord_level(coord~19, AP) 
denotes that the coordinated constituents are AP, 
as syntactically speaking adjectives are coordi-
nated. It seems that distributed and non-
distributed readings (semantics) are not distin-
guished.  
It can be said that GR and others are annotat-
ing syntactic structure of the dependency while 
HPSG and others annotate more semantic struc-
 
4 According to one of the reviewers this is an error in the 
distributed version of the PARC corpus that is the result of 
the automatic conversion. The correct structure is the one in 
which the subject is only dependent on both verbs but not 
on the coordinator (an example is parc_23.102 in 
http://www2.parc.com/isl/groups/nltt/fsbank/parc700-2006-
05-30.fdsc); the same would hold of the object.
20
ture. Ideally, the mechanism for encoding the 
syntactic and semantic structure separately on the 
coordination should be provided, with an option 
to decide whether one of them is left unanno-
tated. 
For example, the second example shown in 
Figure 1 ("Human Immunodeficiency Virus 
Type I and Type II") can be viewed as a coordi-
nation of two modifiers ("Type I" and "Type II") 
syntactically, and as a coordination of two names 
("Human Immunodeficiency Virus Type I" and 
"Human Immunodeficiency Virus Type II") se-
mantically. Taking this into consideration, the 
structure shown in Figure 1 can be enhanced into 
the one shown in Figure 2 where conj_sem is 
for representing the semantic value of coordina-
tion, and coord0_S denotes that the dependen-
cies are related semantically to coord0. Provid-
ing two relations that work as cood_level in 
the PARC scheme, one for the syntactic level and 
the other for the semantic level, may be another 
solution: if a parallel of coord_level, say, 
coord_level_sem, can be used in addition to 
encode the semantically coordinated constituents, 
distributive reading of "electronic, computer and 
building products" mentioned above may be ex-
pressed by coord_level_sem(coord~19, 
NP)indicating that it is a noun phrases with 
shared head that are coordinated. 
 
Human Immunodeficiency Virus Type 
I and Type II 
id(name0, Human Immunodeficiency 
Virus Type I) 
id(name1, Human Immunodeficiency 
Virus) 
id(name2, Type I) 
concat(name0, name1, name2) 
id(name3, Type II) 
id(name4, Human Immunodeficiency 
Virus Type II) 
concat(name4, name1, name3) 
conj(coord0, name2) 
conj(coord0, name3) 
conj_form(coord0, and) 
adjunct(name1, coord0) 
conj_sem(coord0_S, name0) 
conj_sem(coord0_S, name4)
Figure 2. Annotation of coordinated names on 
syntactic and semantic levels 
 
Coordinator 
Two ways of expressing the coordination be-
tween three items are found in the corpora: re-
taining the surface form or not. 
 
cotton , soybeans and rice 
eggs and butter and milk 
 
For example, the structures for the two phrases 
above are different in the CONLL corpus while 
others ignore the fact that the former uses a 
comma while "and" is used in the latter. That is, 
the CONLL scheme encodes the surface struc-
ture, while others encode the deeper structure, for 
semantically the comma in the former example 
means "and". The difference can be captured by 
retrieving the surface form of the sentences in the 
corpora that ignore the surface structure. How-
ever, encoding surface form and deeper structure 
would help to capture maximal information and 
to compare the structures across different annota-
tions more smoothly. 
3.3 Prepositional phrases 
Another major source of inconsistency involved 
prepositional phrases. The PP-attachment prob-
lem (where the PP should be attached) is a prob-
lem traditionally addressed in parsing, but in the 
case of dependency, the type of attachment also 
becomes a problem. 
Where is the head? 
The focus of the PP-attachment problem is the 
head where the PP should attach. In some cases,a 
the correct place to attach can be determined 
from the broader context in which the problem-
atic sentence appears, and in some other cases 
the attachment ambiguity is "benign" in the sense 
that there is little or no difference in meaning 
caused by the difference in the attachment site. 
However, in highly specialized domain like bio-
medical papers, annotators of grammatical struc-
tures do not always have full access to the mean-
ing, and occasionally, it is not easy to decide 
where to attach the PP, whether the ambiguity is 
benign, etc. Yet, it is not always that the annota-
tor of a problematic sentence has no information 
at all: the annotator cannot usually choose from 
the few candidates selected by the (partial) un-
derstanding of the sentence, and not from all pos-
sible sites the PP can syntactically attach. 
No schemes provided for the task allow the list-
ing of possible candidates of the phrases where a 
PP can attach (as allowed in the case of Penn 
Treebank POS corpus). As with the POS, a 
scheme for annotating ambiguous attachment 
should be incorporated. This can be more easily 
realized for dependency annotation, where the 
structure of a sentence is decomposed into list of 
21
local dependencies, than treebank annotation, 
where the structure is annotated as a whole. Sim-
ply listing the possible dependencies, with a flag 
for ambiguity, should work for the purpose. Pref-
erably, the flag encodes the information about 
whether the annotator thinks the ambiguity is 
benign, i.e. the annotator believes that the ambi-
guity does not affect the semantics significantly. 
Complement or Modifier 
In dependency annotation, the annotator must 
decide whether the PP dependent of a verb or a 
verbal noun is an obligatory complement or an 
optional modifier. External resources (e.g. dic-
tionary) can be used for common verbs, but for 
technical verbs such resources are not yet widely 
available, and collecting and investigating a large 
set of actual use of the verbal is not an easy task.  
Dependency types for encoding PP-attachment 
are varied among the schemes. Schemes such as 
CONLL and Stanford do not distinguish between 
complements and modifiers, and they just anno-
tate the relation that the phrase "attaches as a PP". 
HPSG in theory can distinguish complements 
and modifiers, but in the actual corpus, all PPs 
appear as modifiers5. GR does not mark the type 
of the non-clausal modifying phrase but distin-
guish PP-complements (iobj), nominal com-
plements (dobj) and modifiers. PARC has more 
distinction of attachment type (e.g. obj, obl, 
adjunct). 
If the inconsistency problem involving the 
type of PP attachment lies in the distinction be-
tween complements and modifiers, treatment of 
CONLL and Stanford looks better than that of 
GR and PARC. However, an application may 
require the distinction (a candidate of such appli-
cation is relation information extraction using 
predicate-argument structure) so that analysis 
with the schemes that cannot annotate such dis-
tinction at all is not suitable for such kind of ap-
plications. On the other hand, GR does have 
type-underspecification (Briscoe 2006) but the 
argument (complement) - modifier distinction is 
at the top level of the hierarchy and underspecifi-
cation cannot be done without discarding the in-
formation that the dependent is a PP. 
A dependent of a verbal has two aspects of 
distinction: complement/modifier and grammati-
cal category (whether it is an NP, a PP, an AP, 
etc). The mechanism for encoding these aspects 
separately should be provided, with an option to 
                                                 
5 The modifier becomes a head in HPSG and in CCG unlike 
other formalisms.  
decide if one is left unannotated. A possible an-
notation scheme using IDs is illustrated in Figure 
3, where type of dependency and type of the de-
pendent are encoded separately. A slash indicates 
the alternatives from which to choose one (or 
more, in ambiguous cases).  
 
Dependency(ID, verb, dependent) 
Dependent_type(ID, MOD/ARG) 
Dependent_form(ID, PP/NP/AP/...) 
Figure 3: An illustration of attachment to a ver-
bal head 
 
4 Toward a Unified Scheme 
The observation suggests that, for difficult lin-
gustic phenomena, different aspects of the phe-
nomena are annotated by different schemes. It 
also suggests that there are at least two problems 
in defining the type of dependencies: one is the 
confusion of the level of analysis, and another is 
that several aspects of dependency are encoded 
into one label. 
The confusion of the level of analysis means 
that, as seen in the case of coordination, the syn-
tactic-level analysis and semantic-level analysis 
receive the same or similar label across the 
schemes. In each scheme only one level of analy-
sis is provided, but it is not always explicit which 
level is provided in a particular scheme. Thus, it 
is inconvenient and annoying for an annotator 
who wants to annotate the other level or both 
levels at once. 
As seen in the case of PP-dependents of 
verbals, because different aspects, or features, are 
encoded in one label, type-underspecification 
becomes a less convenient mechanism. If labels 
are properly decomposed into a set of feature 
values, and a hierarchy of values is provided for 
each feature, the annotation labels can be more 
flexible and it is easier for an annotator to choose 
a label that can encode the desired information. 
The distinction of syntax/semantics (or there may 
be more levels) can be incorporated into one of 
the features. Other possible features include the 
grammatical categories of head and dependent, 
argument/modifier distinction, and  role of argu-
ments or modifiers like the one annotated in 
Propbank (Palmer et al, 2005). 
Decomposing labels into features have another 
use. It would make the mapping between one 
scheme and another more transparent.  
As the dependency structure of a sentence is 
encoded into a list of local information in de-
22
pendency schemes, it can be suggested that tak-
ing the union of the annotation of different 
schemes can achieve the encoding of the union 
of information that the individual schemes can 
encode, except for conflicting representations 
such as the head of coordinated structures, and 
the head of modifiers in HPSG. If the current 
labels are decomposed into features, it would 
enable one to take non-redundant union of in-
formation, and mapping from the union to a par-
ticular scheme would be more systematic. In 
many cases listed in the previous section, indi-
vidual schemes could be obtained by systemati-
cally omitting some relations in the union, and 
common information among the schemes (the 
structures that all of the schemes concerned can 
agree) could be retrieved by taking the intersec-
tion of annotations. An annotator can annotate 
the maximal information (s)he knows within the 
framework of the union, and mapped into the 
predefined scheme when needed.  
Also, providing a mechanism for annotating 
ambiguity should be provided. As for depend-
ency types the type hierarchy of features de-
scribed above can help. As for the ambiguity of 
attachment site and others that involve the prob-
lem of what is dependent on what, listing of pos-
sible candidates with a flag of ambiguity can 
help.  
Acknowledgments 
I am grateful for the anonymous reviewers for 
suggestions and comments. 
References 
Bies, Ann, Mark Ferguson, Karen Katz, Robert Mac-
Intyre, Victoria Tredinnick, Grace Kim, Mary Ann 
Marcinkiewicz, and Britta Schasberger , 1995. 
Bracketing Guidelines for Treebank II Style Penn 
Treebank Project. Technical report, University of 
Pennsylvania. 
Briscoe, Ted. 2006. An introduction to tag sequence 
grammars and the RASP system parser. Technical 
Report (UCAM-CL-TR-662), Cambridge Univer-
sity Computer Laboratory. 
Clegg, Andrew B. and Adrian J Shepherd. 2007. 
Benchmarking natural-language parsers for bio-
logical applications using dependency graphs. 
BMC Bioinformatics 8:24.  
Hockenmaier, Julia and Mark Steedman. 2005. 
CCGbank: User?s Manual, Technical Report (MS-
CIS-05-09), University of Pennsylvania. 
Kim, J-D., Ohta, T.,  Teteisi Y., Tsujii, J. (2003). 
GENIA corpus - a semantically annotated corpus 
for bio-textmining. Bioinformatics. 19(suppl. 1), pp. 
i180-i182. 
de Marneffe, Marie-Catherine, Bill MacCartney, and 
Christopher D. Manning. 2006. Generating typed 
dependency parses from phrase structure parses. 
Proceedings of LREC 2006, Genoa, Italy. 
Miyao, Yusuke. From Linguistic Theory to Syntactic 
Analysis: Corpus-Oriented Grammar Development 
and Feature Forest Model. 2006. PhD Thesis, Uni-
versity of Tokyo. 
Miyao, Yusuke, Kenji Sagae, Jun'ichi Tsujii. 2007. 
Towards Framework-Independent Evaluation of 
Deep Linguistic Parsers. In Proceedings of Gram-
mar Engineering across Frameworks, Stanford, 
California, USA, pp. 238-258. 
Palmer, Martha, Paul Kingsbury, Daniel Gildea. 2005. 
"The Proposition Bank: An Annotated Corpus of 
Semantic Roles". Computational Linguistics 31 
(1): 71?106.
Pyysalo, Sampo, Filip Ginter, Veronika Laippala, 
Katri Haverinen, Juho Heimonen, and Tapio Sala-
koski. 2007a. On the unification of syntactic anno-
tations under the Stanford dependency scheme: A 
case study on BioInfer and GENIA. Proceedings of 
BioNLP Workshop at ACL 2007, Prague, Czech 
Republic . 
Pyysalo, Sampo, Filip Ginter, Juho Heimonen, Jari 
Bj?rne, Jorma Boberg, Jouni J?rvinen and Tapio 
Salakoski. 2007b. BioInfer: a corpus for informa-
tion extraction in the biomedical domain. BMC 
Bioinformatics 8:50. 
Santorini, Beatrice. 1990. Part-of-Speech Tagging 
Guidelines for the Penn Treebank Project. Techni-
cal report, University of Pennsylvania. 
Tateisi, Yuka, Ohta, Tomoko, Nigel Collier, Chikashi 
Nobata and Jun'ichi Tsujii. 2000. Building an An-
notated Corpus from Biology Research Papers. In 
the Proceedings of COLING 2000 Workshop on 
Semantic Annotation and Intelligent Content. Lux-
embourg. pp. 28-34. 
Tateisi,Yuka, Yusuke Miyao, Kenji Sagae, Jun'ichi 
Tsujii. 2008. GENIA-GR: a Grammatical Relation 
Corpus for Parser Evaluation in the Biomedical 
Domain. In the Proceedings of the Sixth Interna-
tional Language Resources and Evaluation 
(LREC'08). Marrakech, Morocco. 
23
Proceedings of the 2011 Workshop on Biomedical Natural Language Processing, ACL-HLT 2011, pages 164?173,
Portland, Oregon, USA, June 23-24, 2011. c?2011 Association for Computational Linguistics
Parsing Natural Language Queries for Life Science Knowledge
Tadayoshi Hara
National Institute of Informatics
2-1-2 Hitotsubashi, Chiyoda-ku,
Tokyo 101-8430, JAPAN
harasan@nii.ac.jp
Yuka Tateisi
Faculty of Informatics, Kogakuin University
1-24-2 Nishi-shinjuku, Shinjuku-ku,
Tokyo 163-8677, JAPAN
yucca@cc.kogakuin.ac.jp
Jin-Dong Kim
Database Center for Life Science
2-11-16 Yayoi, Bunkyo-ku,
Tokyo 113-0032, JAPAN
jdkim@dbcls.rois.ac.jp
Yusuke Miyao
National Institute of Informatics
2-1-2 Hitotsubashi, Chiyoda-ku,
Tokyo 101-8430, JAPAN
yusuke@nii.ac.jp
Abstract
This paper presents our preliminary work on
adaptation of parsing technology toward natu-
ral language query processing for biomedical
domain. We built a small treebank of natu-
ral language queries, and tested a state-of-the-
art parser, the results of which revealed that
a parser trained on Wall-Street-Journal arti-
cles and Medline abstracts did not work well
on query sentences. We then experimented
an adaptive learning technique, to seek the
chance to improve the parsing performance on
query sentences. Despite the small scale of the
experiments, the results are encouraging, en-
lightening the direction for effective improve-
ment.
1 Introduction
Recent rapid progress of life science resulted in a
greatly increased amount of life science knowledge,
e.g. genomics, proteomics, pathology, therapeutics,
diagnostics, etc. The knowledge is however scat-
tered in pieces in diverse forms over a large number
of databases (DBs), e.g. PubMed, Drugs.com, Ther-
apy database, etc. As more and more knowledge is
discovered and accumulated in DBs, the need for
their integration is growing, and corresponding ef-
forts are emerging (BioMoby1, BioRDF2, etc.).
Meanwhile, the need for a query language with
high expressive power is also growing, to cope with
1http://www.biomoby.org/
2http://esw.w3.org/HCLSIG BioRDF Subgroup
the complexity of accumulated knowledge. For ex-
ample, SPARQL3 is becoming an important query
language, as RDF4 is recognized as a standard in-
teroperable encoding of information in databases.
SPARQL queries are however not easy for human
users to compose, due to its complex vocabulary,
syntax and semantics. We propose natural language
(NL) query as a potential solution to the problem.
Natural language, e.g. English, is the most straight-
forward language for human beings. Extra training
is not required for it, yet the expressive power is
very high. If NL queries can be automatically trans-
lated into SPARQL queries, human users can access
their desired knowledge without learning the com-
plex query language of SPARQL.
This paper presents our preliminary work for
NL query processing, with focus on syntactic pars-
ing. We first build a small treebank of natural
language queries, which are from Genomics track
(Hersh et al, 2004; Hersh et al, 2005; Hersh et al,
2006; Hersh et al, 2007) topics (Section 2 and 3).
The small treebank is then used to test the perfor-
mance of a state-of-the-art parser, Enju (Ninomiya
et al, 2007; Hara et al, 2007) (Section 4). The
results show that a parser trained on Wall-Street-
Journal (WSJ) articles and Medline abstracts will
not work well on query sentences. Next, we ex-
periment an adaptive learning technique, to seek the
chance to improve the parsing performance on query
sentences. Despite the small scale of the experi-
ments, the results enlighten directions for effective
3http://www.w3.org/TR/rdf-sparql-query/
4http://www.w3.org/RDF/
164
GTREC
04 05 06 07
Declarative 1 0 0 0
Imperative 22 60 0 0
Infinitive 1 0 0 0
Interrogative
- WP/WRB/WDT 3 / 1 / 11 0 / 0 / 0 6 / 22 / 0 0 / 0 / 50
- Non-wh 5 0 0 0
NP 14 0 0 0
Total 58 60 28 50
Table 1: Distribution of sentence constructions
improvement (Section 5).
2 Syntactic Features of Query Sentences
While it is reported that the state-of-art NLP tech-
nology shows reasonable performance for IR or
IE applications (Ohta et al, 2006), NLP technol-
ogy has long been developed mostly for declara-
tive sentences. On the other hand, NL queries in-
clude wide variety of sentence constructions such
as interrogative sentences, imperative sentences, and
noun phrases. Table 1 shows the distribution of the
constructions of the 196 query sentences from the
topics of the ad hoc task of Genomics track 2004
(GTREC04) and 2005 (GTREC05) in their narra-
tive forms, and the queries for the passage retrieval
task of Genomics track 2006 (GTREC06) and 2007
(GTREC07).
GTREC04 set has a variety of sentence construc-
tions, including noun phrases and infinitives, which
are not usually considered as full sentences. In the
2004 track, the queries were derived from interviews
eliciting information needs of real biologists, with-
out any control on the sentence constructions.
GTREC05 consists only of imperative sentences.
In the 2005 track, a set of templates were derived
from an analysis of the 2004 track and other known
biologist information needs. The derived templates
were used as the commands to find articles describ-
ing biological interests such as methods or roles of
genes. Although the templates were in the form
?Find articles describing ...?, actual obtained imper-
atives begin with ?Describe the procedure or method
for? (12 sentences), ?Provide information about?
(36 sentences) or ?Provide information on? (12 sen-
tences).
GTREC06 consists only of wh-questions where a
wh-word constitutes a noun phrase by itself (i.e. its
 
S  
 
  
 
VP  
 
  
 
NP  
 
  
 
PP  
 
  
 
NP  
 
  
 
PP  
 
  
NP NP NP NP 
        VB NNS IN NN IN NN 
[ ] Find articles abut function of FancD2 
 
Figure 1: The tree structure for an imperative sentence
part-of-speech is the WP in Penn Treebank (Marcus
et al, 1994) POS tag set) or is an adverb (WRB). In
the 2006 track, the templates for the 2005 track were
reformulated into the constructions of questions and
were then utilized for deriving the questions. For ex-
ample, the templates to find articles describing the
role of a gene involved in a given disease is refor-
mulated into the question ?What is the role of gene
in disease??
GTREC07 consists only of wh-questions where a
wh-word serves as a pre-nominal modifier (WDT).
In the 2007 track, unlike in those of last two years,
questions were not categorized by the templates, but
were based on biologists? information needs where
the answers were lists of named entities of a given
type. The obtained questions begin with ?what +
entity type? (45 sentences), ?which + entity type? (4
sentences), or ?In what + entity type? (1 sentence).
In contrast, the GENIA Treebank Corpus (Tateisi
et al, 2005)5 is estimated to have no imperative sen-
tences and only seven interrogative sentences (see
Section 5.2.2). Thus, the sentence constructions in
GTREC04?07 are very different from those in the
GENIA treebank.
3 Treebanking GTREC query sentences
We built a treebank (with POS) on 196 query sen-
tences following the guidelines of the GENIA Tree-
bank (Tateisi and Tsujii, 2006). The queries were
first parsed using the Stanford Parser (Klein and
Manning, 2003), and manual correction was made
5http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/home/
wiki.cgi?page=GENIA+Treebank
165
 SBARQ  
 
  
 
SQ  
 
   
 VP    
WHNP[i168]   NP[i169?i168]  NP[?i169] PP               
WDT NNS VBP   VBN   IN NN 
What toxicities are [ ] associated [ ] with cytarabine 
 
Figure 2: The tree structure for an interrogative sentence
by the second author. We tried to follow the guide-
line of the GENIA Treebank as closely as possible,
but for the constructions that are rare in GENIA, we
used the ATIS corpus in Penn Treebank (Bies et al,
1995), which is also a collection of query sentences,
for reference.
Figure 1 shows the tree for an imperative sen-
tence. A leaf node with [ ] corresponds to a null
constituent. Figure 2 shows the tree for an inter-
rogative sentence. Coindexing is represented by
assigning an ID to a node and a reference to the
ID to the node which is coindexed. In Figure 2,
WHNP[i168] means that the WHNP node is indexed
as i168, NP[i169?i168] means that the NP node is
indexed as i169 and coindexed to the i168 node, and
NP[?i169] means that the node is coindexed to the
i169 node. In this sentence, which is a passive wh-
question, it is assumed that the logical object (what
toxicities) of the verb (associate) is moved to the
subject position (the place of i169) and then moved
to the sentence-initial position (the place of i168).
As most of the query sentences are either impera-
tive or interrogative, there are more null constituents
compared to the GENIA Corpus. In the GTREC
query treebank, 184 / 196 (93.9%) sentences con-
tained one or more null constituents, whereas in GE-
NIA, 12,222 / 18,541 (65.9%) sentences did. We ex-
pected there are more sentences with multiple null
constituents in GTREC compared to GENIA, due to
the frequency of passive interrogative sentences, but
on the contrary the number of sentences containing
more than one null constituents are 65 (33.1%) in
GTREC, and 6,367 (34.5%) in GENIA. This may be
due to the frequency of relative clauses in GENIA.
4 Parsing system and extraction of
imperative and question sentences
We introduce the parser and the POS tagger whose
performances are examined, and the extraction of
imperative or question sentences from GTREC tree-
bank on which the performances are measured.
4.1 HPSG parser
The Enju parser (Ninomiya et al, 2007)6 is a deep
parser based on the HPSG formalism. It produces
an analysis of a sentence that includes the syntac-
tic structure (i.e., parse tree) and the semantic struc-
ture represented as a set of predicate-argument de-
pendencies. The grammar is based on the standard
HPSG analysis of English (Pollard and Sag, 1994).
The parser finds a best parse tree scored by a max-
ent disambiguation model using a Cocke-Kasami-
Younger (CKY) style algorithm.
We used a toolkit distributed with the Enju parser
for training the parser with a Penn Treebank style
(PTB-style) treebank. The toolkit initially converts
the PTB-style treebank into an HPSG treebank and
then trains the parser on it. We used a toolkit dis-
tributed with the Enju parser for extracting a HPSG
lexicon from a PTB-style treebank. The toolkit ini-
tially converts the PTB-style treebank into an HPSG
treebank and then extracts the lexicon from it.
The HPSG treebank converted from the test sec-
tion was used as the gold-standard in the evaluation.
As the evaluation metrics of the Enju parser, we used
labeled and unlabeled precision/recall/F-score of the
predicate-argument dependencies produced by the
parser. A predicate-argument dependency is repre-
sented as a tuple of ?wp, wa, r?, where wp is the
predicate word, wa is the argument word, and r is
the label of the predicate-argument relation, such
as verb-ARG1 (semantic subject of a verb) and
prep-ARG1 (modifiee of a prepositional phrase).
4.2 POS tagger
The Enju parser assumes that the input is already
POS-tagged. We use a tagger in (Tsuruoka et al,
2005). It has been shown to give a state-of-the-art
accuracy on the standard Penn WSJ data set and also
on a different text genre (biomedical literature) when
trained on the combined data set of the WSJ data and
6http://www-tsujii.is.s.u-tokyo.ac.jp/enju
166
the target genre (Tsuruoka et al, 2005). Since our
target is biomedical domain, we utilize the tagger
adapted to the domain as a baseline, which we call
?the GENIA tagger?.
4.3 Extracting imperative and question
sentences from GTREC treebank
In GTREC sentences, two major constructions of
sentences can be observed: imperative and question
sentences. These two types of sentences have differ-
ent sentence constructions and we will observe the
impact of each or both of these constructions on the
performances of parsing or POS-tagging. In order
to do so, we collected imperative and question sen-
tences from our GTREC treebank as follows:
? GTREC imperatives - Most of the impera-
tive sentences in GTREC treebank begin with
empty subjects ?(NP-SBJ */-NONE-)?. We ex-
tracted such 82 imperative sentences.
? GTREC questions - Interrogative sentences
are annotated with the phrase label ?SBARQ?
or ?SQ?, where ?SBARQ? and ?SQ? respec-
tively denote a wh-question and an yes/no ques-
tion. We extracted 98 interrogative sentences
whose top phrase labels were either of them.
5 Experiments
We examine the POS-tagger and the parser for the
sentences in the GTREC corpus. They are adapted
to each of GTREC overall, imperatives, and ques-
tions. We then observe how the parsing or POS-
tagging accuracies are improved and analyze what
is critical for parsing query sentences.
5.1 Experimental settings
5.1.1 Dividing corpora
We prepared experimental datasets for the follow-
ing four domains:
? GENIA Corpus (GENIA) (18,541 sentences)
Divided into three parts for training (14,849
sentences), development test (1,850 sentences),
and final test (1,842 sentences).
? GTREC overall (196 sentences)
Divided into two parts: one for ten-folds cross
validation test (17-18 ? 10 sentences) and the
other for error analysis (17 sentences)
Target GENIA tagger Adapted tagger
GENIA 99.04% -
GTREC (overall) 89.98% 96.54%
GTREC (imperatives) 90.32% 97.30%
GRREC (questions) 89.25% 94.77%
Table 2: Accuracy of the POS tagger for each domain
? GTREC imperatives (82 sentences)
Divided into two parts: one for ten-folds cross
validation test (7-8 ? 10 sentences) and the
other for error analysis (7 sentences)
? GTREC questions (98 sentences)
Divided into two parts: one for ten-folds cross
validation test (9 ? 10 sentences) and the other
for error analysis (8 sentences)
5.1.2 Adaptation of POS tagger and parser
In order to adapt the POS tagger and the parser to
a target domain, we took the following methods.
? POS tagger - For the GTREC overall / impera-
tives / questions, we replicated the training data
for 100,000 times and utilized the concatenated
replicas and GENIA training data in (Tsuruoka
et al, 2005) for training. For POS tagger, the
number of replicas of training data was deter-
mined among 10n(n = 0, . . . , 5) by testing
these numbers on development test sets in three
of ten datasets of cross validation.
? Enju parser - We used a toolkit in the Enju
parser (Hara et al, 2007). As a baseline model,
we utilized the model adapted to the GENIA
Corpus. We then attempted to further adapt the
model to each domain. In this paper, the base-
line model is called ?the GENIA parser?.
5.2 POS tagger and parser performances
Table 2 and 3 respectively show the POS tagging and
the parsing accuracies for the target domains, and
Figure 3 and 4 respectively show the POS tagging
and the parsing accuracies for the target domains
given by changing the size of the target training data.
The POS tagger could output for each word either
of one-best POS or POS candidates with probabili-
ties, and the Enju parser could take either of the two
output types. The bracketed numbers in Table 3 and
167
Parser GENIA Adapted
POS Gold GENIA tagger Adapted tagger Gold GENIA tagger Adapted tagger
For GENIA 88.54 88.07 (88.00) - - - -
For GTREC overall 84.37 76.81 (72.43) 83.46 (81.96) 89.00 76.98 (74.44) 86.98 (85.42)
For GTREC imperatives 85.19 78.54 (77.75) 85.71 (85.48) 89.42 74.40 (74.84) 88.97 (88.67)
For GTREC questions 85.45 76.25 (67.27) 83.55 (80.46) 87.33 81.41 (71.90) 84.87 (82.70)
[ using POS candidates with probabilities (using only one best POS) ]
Table 3: Accuracy of the Enju parser for GTREC
70
75
80
85
90
0 20 40 60 80 100 120 140
F
-
s
c
o
r
e
Corpus size (sentences)
70
75
80
85
90
0 20 40 60
F
-
s
c
o
r
e
Corpus size (sentences)
65
70
75
80
85
90
95
0 20 40 60 80
F
-
s
c
o
r
e
Corpus size (sentences)
Adapted parser, gold POS
Adapted parser, adapted tagger (prob.)
GENIA parser, adapted tagger (prob.)
Adapted parser, GENIA tagger (prob.)
Adapted parser, adapted tagger (1best)
GENIA parser, adapted tagger (1best)
Adapted parser, GENIA tagger (1best)
For GTREC imperatives For GTREC questionsFor GTREC overall
Figure 4: Parsing accuracy vs. corpus size
88
90
92
94
96
98
0 50 100 150
A
c
c
u
r
a
c
y
 (
%
)
Corpus size (sentences)
GTREC overall
GTREC imperatives
GTREC questions
Figure 3: POS tagging accuracy vs. corpus size
the dashed lines in Figure 4 show the parsing accu-
racies when we utilized one-best POS given by the
POS tagger, and the other numbers and lines show
the accuracies given by POS candidates with proba-
bilities. In the rest of this section, when we just say
?POS tagger?, the tagger?s output is POS candidates
with probabilities.
Table 4 and 5 respectively compare the types of
POS tagging and parsing errors for each domain
between before and after adapting the POS tagger,
and Table 6 compares the types of parsing errors for
Correct ? Error GENIA tagger Adapted tagger
For GTREC overall (17 sentences)
NN ? NNP 4 0.6
VB ? NN 4 0
WDT ? WP 4 0
NN ? JJ 1 1.9
For GTREC imperative (seven sentences)
FW ? NNP / NN / JJ 7 4
VB ? NN 4 0
NN ? NNP 2 0
For GTREC question (eight sentences)
WDT ? WP 3 0
VB ? VBP 2 1
NNS ? VBZ 2 0
(The table shows only error types observed more than
once for either of the taggers)
Table 4: Tagging errors for each of the GTREC corpora
each domain between before and after adapting the
parser. The numbers of errors for the rightmost col-
umn in each of the tables were given by the average
of the ten-folds cross validation results.
In the following sections, we examine the im-
pact of the performances of the POS taggers or the
parsers on parsing the GTREC documents.
168
GENIA parserError types GENIA tagger Adapted tagger
For GTREC overall (17 sentences)
Failure in detecting verb 12 0.2
Root selection 6 0
Range of NP 5 5
PP-attachment 4 3
Determiner / pronoun 4 1
Range of verb subject 4 4
Range of verb object 3 3
Adjective / modifier noun 2 3
For GTREC imperatives (seven sentences)
Failure in detecting verb 8 0
Root selection 4 0
Range of NP 3 4
PP-attachment 3 1.8
Range of PP 2 2
For GTREC questions (eight sentences)
Range of coordination 5 3
Determiner / pronoun 3 0
PP-attachment 3 1
Range of PP 2 2
Subject for verb 2 1
(The table shows only the types of parsing errors observed more
than once for either of the parsers)
Table 5: Impact of adapting POS tagger on parsing errors
5.2.1 Impact of POS tagger on parsing
In Table 2, for each of the GTREC corpora,
the GENIA tagger dropped its tagging accuracy by
around nine points, and then recovered five to seven
points by the adaptation. According to this behav-
ior of the tagger, Table 3 shows that the GENIA and
the adapted parsers with the GENIA tagger dropped
their parsing accuracies by 6?15 points in F-score
from the accuracies with the gold POS, and then re-
covered the accuracies within two points below the
accuracies with the gold POS. The performance of
the POS tagger would thus critically affect the pars-
ing accuracies.
In Figure 3, we can observe that the POS tagging
accuracy for each corpus rapidly increased only for
first 20?30 sentences, and after that the improvement
speed drastically declined. Accordingly, in Figure 4,
the line for the adapted parser with the adapted tag-
ger (the line with triangle plots) rose rapidly for the
first 20?30 sentences, and after that slowed down.
We explored the tagging and parsing errors, and
analyze the cause of the initial accuracy jump and
the successive improvement depression.
Gold POSError types GENIA parser Adapted parser
For GTREC overall (17 sentences)
Range of NP 5 1.3
Range of verb subject 3 2.6
PP-attachment 3 2.7
Whether verb takes
object & complement 3 2.9
Range of verb object 2 1
For GTREC imperatives (seven sentences)
Range of NP 4 1.1
PP-attachment 2 1.6
Range of PP 2 0.3
Preposition / modifier 2 2
For GTREC questions (eight sentences)
Coordination / conjunction 2 2.2
Auxiliary / normal verb 2 2.6
Failure in detecting verb 2 2.6
(The table shows only the types of parsing errors observed more
than once for either of the parsers)
Table 6: Impact of adapting parser on parsing errors
Cause of initial accuracy jump
In Table 4, ?VB ? NN? tagging errors were
observed only in imperative sentences and drasti-
cally decreased by the adaptation. In a impera-
tive sentence, a verb (VB) usually appears as the
first word. On the other hand, the GENIA tagger
was trained mainly on the declarative sentences and
therefore would often take the first word in a sen-
tence as the subject of the sentence, that is, noun
(NN). When the parser received a wrong NN-tag for
a verb, the parser would attempt to believe the infor-
mation (?failure in detecting verb? in Table 6) and
could then hardly choose the NN-tagged word as a
main verb (?root selection? in Table 6). By adapting
the tagger, the correct tag was given to the verb and
the parser could choose the verb as a main verb.
?WDT ? WP? tagging errors were observed only
in the question sentences and also drastically de-
creased. For example, in the sentence ?What toxici-
ties are associated with cytarabine??, ?What? works
as a determiner (WDT) which takes ?toxicities?,
while the GENIA tagger often took this ?What? as a
pronoun (WP) making a phrase by itself. This would
be because the training data for the GENIA tagger
would contain 682 WP ?what? and only 27 WDT
?what?. WP ?what? could not make a noun phrase
by taking a next noun, and then the parsing of the
parsing would corrupt (?determiner / pronoun? in
Table 5). By adapting the tagger, ?WDT? tag was
169
given to ?What?, and the parser correctly made a
phrase ?What toxicities?.
Since the variation of main verbs in GTREC im-
peratives is very small (see Section 2) and that of
interrogatives is also very small, in order to cor-
rect the above two types of errors, we would require
only small training data. In addition, these types of
errors widely occurred among imperatives or ques-
tions, the accuracy improvement by correcting the
errors was very large. The initial rapid improvement
would thus occur.
Cause of improvement depression
?NN ? NNP? tagging errors would come from
the description style of words. In the GTREC
queries, technical terms, such as the names of dis-
eases or proteins, sometimes begin with capital char-
acters. The GENIA tagger would take the capi-
talized words not as a normal noun (NN) but as a
proper noun (NNP). By adaptation, the tagger would
have learned the capital usage for terms and the er-
rors then decreased.
However, in order to achieve such improvement,
we would have to wait until a target capitalized term
is added to the training corpus. ?FW ? NNP / NN
/ JJ?, ?NN ? JJ?, and several other errors would be
similar to this type of errors in the point that, they
would be caused by the difference in annotation pol-
icy or description style between the training data for
the GENIA tagger and the GTREC queries.
?VB ? VBP? errors were found in questions. For
example, ?affect? in the question ?How do muta-
tions in Sonic Hedgehog genes affect developmen-
tal disorders?? was base form (VB), while the GE-
NIA tagger took it as a present tense (VBP) since
the GENIA tagger would be unfamiliar with such
verb behavior in questions. By adaptation, the tag-
ger would learn that verbs in the domain tend to take
base forms and the errors then decreased.
However, the tagger model based on local context
features could not substantially solve the problem.
VBP of course could appear in question sentences.
We observed that a verb to be VBP was tagged with
VB by the adapted tagger. In order to distinguish
VB from VBP, we should capture longer distance
dependencies between auxiliary and main verbs.
In tagging, the fact that the above two types of
errors occupied most of the errors other than the er-
rors involved in the initial jump, would be related
to why the accuracy improvement got so slowly,
which would lead to the improvement depression of
the parsing performances. With the POS candidates
with probabilities, the possibilities of correct POSs
would increase, and therefore the parser would give
higher parsing performances than using only one-
best POSs (see Table 3 and Figure 4).
Anyway, the problems were not substantially
solved. For these tagging problems, just adding the
training data would not work. We might need re-
construct the tagging system or re-consider the fea-
ture designs of the model.
5.2.2 Impact of parser itself on parsing
For the GTREC corpora, the GENIA parser with
gold POSs lowered the parsing accuracy by more
than three points than for the GENIA Corpus, while
the adaptation of the parser recovered a few points
for each domain (second and fifth column in Table
3). Figure 4 would also show that we could improve
the parser?s performance with more training data for
each domain. For GTREC questions, the parsing ac-
curacy dropped given the maximum size of the train-
ing data. Our training data is small and therefore
small irregular might easily make accuracies drop or
rise. 7 We might have to prepare more corpora for
confirming our observation.
Table 6 would imply that the major errors for all
of these three corpora seem not straightforwardly as-
sociated with the properties specific to imperative or
question sentences. Actually, when we explored the
parse results, errors on the sentence constructions
specific to the two types of sentences would hardly
be observed. (?Failure in detecting verb? errors in
GTREC questions came from other causes.) This
would mean that the GENIA parser itself has poten-
tial to parse the imperative or question sentences.
The training data of the GENIA parser consists
of the WSJ Penn Treebank and the GENIA Corpus.
As long as we searched with our extraction method
in Section 4.3, the WSJ and GENIA Corpus seem
respectively contain 115 and 0 imperative, and 432
7This time we could not analyze which training data affected
the decrease, because through the cross validation experiments
each sentence was forced to be once final test data. However,
we would like to find the reason for this accuracy decrease in
some way.
170
and seven question sentences. Unlike the POS tag-
ger, the parser could convey more global sentence
constructions from these sentences.
Although the GENIA parser might understand the
basic constructions of imperative or question sen-
tences, by adaptation of the parser to the GTREC
corpora, we could further learn more local construc-
tion features specific to GTREC, such as word se-
quence constructing a noun phrase, attachment pref-
erence of prepositions or other modifiers. The error
reduction in Table 6 would thus be observed.
However, we also observed that several types of
errors were still mostly unsolved after the adapta-
tion. Choosing whether to add complements for
verbs or not, and distinguishing coordinations from
conjunctions seems to be difficult for the parser. If
two question sentences were concatenated by con-
junctions into one sentence, the parser would tend to
fail to analyze the sentence construction for the lat-
ter sentence. The remaining errors in Table 6 would
imply that we should also re-consider the model de-
signs or the framework itself for the parser in addi-
tion to just increasing the training data.
6 Related work
Since domain adaptation has been an extensive re-
search area in parsing research (Nivre et al, 2007),
a lot of ideas have been proposed, including un-
/semi-supervised approaches (Roark and Bacchiani,
2003; Blitzer et al, 2006; Steedman et al, 2003;
McClosky et al, 2006; Clegg and Shepherd, 2005;
McClosky et al, 2010) and supervised approaches
(Titov and Henderson, 2006; Hara et al, 2007).
Their main focus was on adapting parsing models
trained with a specific genre of text (in most cases
PTB-WSJ) to other genres of text, such as biomed-
ical research papers. A major problem tackled in
such a task setting is the handling of unknown words
and domain-specific ways of expressions. However,
as we explored, parsing NL queries involves a sig-
nificantly different problem; even when all words in
a sentence are known, the sentence has a very differ-
ent construction from declarative sentences.
Although sentence constructions have gained lit-
tle attention, a notable exception is (Judge et al,
2006). They pointed out low accuracy of state-of-
the-art parsers on questions, and proposed super-
vised parser adaptation by manually creating a tree-
bank of questions. The question sentences are anno-
tated with phrase structure trees in the PTB scheme,
although function tags and empty categories are
omitted. An LFG parser trained on the treebank then
achieved a significant improvement in parsing ac-
curacy. (Rimell and Clark, 2008) also worked on
question parsing. They collected question sentences
from TREC 9-12, and annotated the sentences with
POSs and CCG (Steedman, 2000) lexical categories.
They reported a significant improvement in CCG
parsing without phrase structure annotations.
On the other hand, (Judge et al, 2006) also im-
plied that just increasing the training data would not
be enough. We went further from their work, built
a small but complete treebank for NL queries, and
explored what really occurred in HPSG parsing.
7 Conclusion
In this paper, we explored the problem in parsing
queries. We first attempted to build a treebank on
queries for biological knowledge and successfully
obtained 196 annotated GTREC queries. We next
examined the performances of the POS tagger and
the HPSG parser on the treebank. In the experi-
ments, we focused on the two dominant sentence
constructions in our corpus: imperatives and ques-
tions, extracted them from our corpus, and then also
examined the parser and tagger for them.
The experimental results showed that the POS
tagger?s mis-tagging to main verbs in imperatives
and wh-interrogatives in questions critically de-
creased the parsing performances, and that our
small corpus could drastically decrease such mis-
tagging and consequently improve the parsing per-
formances. The experimental results also showed
that the parser itself could improve its own perfor-
mance by increasing the training data. On the other
hand, the experimental results suggested that the
POS tagger or the parser performance would stag-
nate just by increasing the training data.
In our future research, on the basis of our findings,
we would like both to build more training data for
queries and to reconstruct the model or reconsider
the feature design for the POS tagger and the parser.
We would then incorporate the optimized parser and
tagger into NL query processing applications.
171
References
Ann Bies, Mark Ferguson, Karen Katz, and Robert Mac-
Intyre. 1995. Bracketing guidelines for Treebank II
style ? Penn Treebank project. Technical report, De-
partment of Linguistics, University of Pennsylvania.
John Blitzer, Ryan Mcdonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In Proceedings of the 2006 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 120?128, Sydney, Australia.
A. B. Clegg and A. Shepherd. 2005. Evaluating and in-
tegrating treebank parsers on a biomedical corpus. In
Proceedings of the ACL 2005 Workshop on Software,
Ann Arbor, Michigan.
Tadayoshi Hara, Yusuke Miyao, and Jun?ichi Tsujii.
2007. Evaluating impact of re-training a lexical dis-
ambiguation model on domain adaptation of an hpsg
parser. In Proceedings of 10th International Confer-
ence on Parsing Technologies (IWPT 2007), pages 11?
22.
William R. Hersh, Ravi Teja Bhupatiraju, L. Ross,
Aaron M. Cohen, Dale Kraemer, and Phoebe Johnson.
2004. TREC 2004 Genomics Track Overview. In Pro-
ceedings of the Thirteenth Text REtrieval Conference,
TREC 2004.
William R. Hersh, Aaron M. Cohen, Jianji Yang,
Ravi Teja Bhupatiraju, Phoebe M. Roberts, and
Marti A. Hearst. 2005. TREC 2005 Genomics Track
Overview. In Proceedings of the Fourteenth Text RE-
trieval Conference, TREC 2005.
William R. Hersh, Aaron M. Cohen, Phoebe M. Roberts,
and Hari Krishna Rekapalli. 2006. TREC 2006 Ge-
nomics Track Overview. In Proceedings of the Fif-
teenth Text REtrieval Conference, TREC 2006.
William R. Hersh, Aaron M. Cohen, Lynn Ruslen, and
Phoebe M. Roberts. 2007. TREC 2007 Genomics
Track Overview. In Proceedings of The Sixteenth Text
REtrieval Conference, TREC 2007.
John Judge, Aoife Cahill, and Josef van Genabith.
2006. Questionbank: Creating a Corpus of Parsing-
Annotated Questions. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and 44th Annual Meeting of the ACL, pages 497?504.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate Unlexicalized Parsing. In Proceedings of the 41st
Annual Meeting of the Association for Computational
Linguistics, pages 423?430.
Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz,
Robert Macintyre, Ann Bies, Mark Ferguson, Karen
Katz, and Britta Schasberger. 1994. The Penn Tree-
bank: Annotating predicate argument structure. In
Proceedings of ARPA Human Language Technology
Workshop.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Reranking and self-training for parser adapta-
tion. In Proceedings of the 21st International Con-
ference on Computational Linguistics and the 44th
annual meeting of the Association for Computational
Linguistics, pages 337?344, Sydney, Australia.
David McClosky, Eugene Charniak, and Mark Johnson.
2010. Automatic Domain Adaptation for Parsing. In
Proceedings of the 2010 Annual Conference of the
North American Chapter of the ACL, pages 28?36, Los
Angeles, California.
Takashi Ninomiya, Takuya Matsuzaki, Yusuke Miyao,
and Jun?ichi Tsujii. 2007. A log-linear model with an
n-gram reference distribution for accurate hpsg pars-
ing. In Proceedings of 10th International Conference
on Parsing Technologies (IWPT 2007), pages 60?68.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The CoNLL 2007 shared task on depen-
dency parsing. In Proceedings of the CoNLL Shared
Task Session of EMNLP-CoNLL 2007, pages 915?932,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Carl Pollard and Ivan A. Sag. 1994. Head-Driven Phrase
Structure Grammar. University of Chicago Press.
Laura Rimell and Stephen Clark. 2008. Adapting a
Lexicalized-Grammar Parser to Contrasting Domains.
In Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing, pages 475?
584.
Brian Roark and Michiel Bacchiani. 2003. Supervised
and unsupervised PCFG adaptation to novel domains.
In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology,
pages 126?133, Edmonton, Canada.
Mark Steedman, Miles Osborne, Anoop Sarkar, Stephen
Clark, Rebecca Hwa, Julia Hockenmaier, Paul Ruhlen,
Steven Baker, and Jeremiah Crim. 2003. Bootstrap-
ping statistical parsers from small datasets. In Pro-
ceedings of the tenth conference on European chap-
ter of the Association for Computational Linguistics,
pages 331?338, Budapest, Hungary.
Mark Steedman. 2000. The Syntactic Process. THE
MIT Press.
Yuka Tateisi and Jun?ichi Tsujii. 2006. GENIA Anno-
tation Guidelines for Treebanking. Technical Report
TR-NLP-UT-2006-5, Tsujii Laboratory, University of
Tokyo.
Yuka Tateisi, Akane Yakushiji, Tomoko Ohta, and
Jun?ichi Tsujii. 2005. Syntax Annotation for the GE-
NIA corpus. In Proceedings of the Second Interna-
tional Joint Conference on Natural Language Process-
172
ing (IJCNLP 2005), Companion volume, pages 222?
227.
Ivan Titov and James Henderson. 2006. Porting statis-
tical parsers with data-defined kernels. In Proceed-
ings of the Tenth Conference on Computational Natu-
ral Language Learning, pages 6?13, New York City.
Yoshimasa Tsuruoka, Yuka Tateishi, Jin-Dong Kim,
Tomoko Ohta, John McNaught, Sophia Ananiadou,
and Jun?ichi Tsujii. 2005. Developing a robust part-
of-speech tagger for biomedical text. In Advances in
Informatics - 10th Panhellenic Conference on Infor-
matics, volume LNCS 3746, pages 382?392, Volos,
Greece, November. ISSN 0302-9743.
173
Proceedings of the 7th Linguistic Annotation Workshop & Interoperability with Discourse, pages 140?148,
Sofia, Bulgaria, August 8-9, 2013. c?2013 Association for Computational Linguistics
Relation Annotation for Understanding Research Papers
Yuka Tateisi? Yo Shidahara? Yusuke Miyao? Akiko Aizawa?
?National Institute of Informatics, Tokyo, Japan
{yucca,yusuke,aizawa}@nii.ac.jp
?Freelance Annotator
yo.shidahara@gmail.com
Abstract
We describe a new annotation scheme for
formalizing relation structures in research
papers. The scheme has been developed
through the investigation of computer sci-
ence papers. Using the scheme, we are
building a Japanese corpus to help develop
information extraction systems for digital
libraries. We report on the outline of the
annotation scheme and on annotation ex-
periments conducted on research abstracts
from the IPSJ Journal.
1 Introduction
Present day researchers need services for search-
ing research papers. Search engines and pub-
lishing companies provide specialized search ser-
vices, such as Google Scholar, Microsoft Aca-
demic Search, and Science Direct. Academic so-
cieties provide archives of journal articles and/or
conference proceedings such as the ACL Anthol-
ogy. These services focus on simple keyword-
based searches as well as extralinguistic relations
among research papers, authors, and research top-
ics. However, because contemporary research is
becoming increasingly complicated and interre-
lated, intelligent content-based search systems are
desired (Banchs, 2012). A typical query in compu-
tational linguistics could be what tasks have CRFs
been used for?, which includes the elements of
a typical schema for searching research papers;
researchers want to find relationships between a
technique and its applications (Gupta and Man-
ning, 2011). Answers to this query can be found
in various forms in published papers, for example,
(1) CRF-based POS tagging has achieved state-of-
the-art accuracy.
(2) CRFs have been successfully applied to se-
quence labeling problems including POS tagging
and named entity recognition.
(3) We apply feature reduction to CRFs and show
its effectiveness in POS tagging.
(4) This study proposes a new method for the ef-
ficient training of CRFs. The proposed method is
evaluated for POS tagging tasks.
Note that the same semantic relation, i.e., the
use of CRFs for POS tagging, is expressed by var-
ious syntactic constructs: internal structures of the
phrase in (1), clause-level structures in (2), inter-
clause structures in (3), and discourse-level struc-
tures in (4). This implies that an integrated frame-
work is required to represent semantic relations for
phrase-level, clause-level, inter-clause level, and
discourse-level structures. Another interesting fact
is that we can recognize various fragments of in-
formation from single texts. For example, from
sentence (1), we can identify CRF is applied to
POS tagging, state-of-the-art accuracy is achieved
for POS tagging, and CRFs achieve high POS tag-
ging accuracy, all of which is valuable content for
different search requests. This indicates that we
need a framework that can cover (almost) all con-
tent in a text.
In this paper we describe a new annotation
scheme for formalizing typical schemas for repre-
senting relations among concepts in research pa-
pers, such as techniques, resources, and effects.
Our study aims to establish a framework for rep-
resenting the semantics of research papers to help
construct intelligent search systems. In particular,
we focus on the formalization of typical schemas
that we believe exemplify common query charac-
teristics.
From the above observations, we have de-
veloped the following criteria for our proposed
framework: use the same scheme for annotating
contents in all levels of linguistic structures, an-
notate (almost) all contents presented in texts, and
capture relations necessary for surveying research
papers. We investigated 71 computer science ab-
stracts (498 sentences) and defined an annotation
140
scheme comprising 16 types of semantic relations.
Computer science is particularly suitable for our
purpose because it is primarily concerned with ab-
stract concepts rather than concrete entities, which
are typically the primary focus of empirical sci-
ences such as physics and biology. In addition,
computer and computational methods can be ap-
plied to an extraordinarily wide range of top-
ics; computer science papers might discuss a bus
timetable (for automatic optimization), a person?s
palm (as a device for projecting images), or look-
ing over another person!Gs shoulder (to obtain pass-
words). Therefore, to annotate all computer sci-
ence papers, we cannot develop predefined entity
ontologies, which is the typical approach taken in
biomedical text mining (Kim et al, 2011).
However, most computer science papers have
characteristic schemata: the papers describe a
problem, postulate a method, apply the method to
the problem using particular data or devices, and
perform experiments to evaluate the method. The
typical schemata clearly represent the structure of
interests in this research field. Therefore, we can
focus on typical schemata, such as application of
a method to a problem and evaluation of a method
for a task. As we will demonstrate in this paper,
the proposed annotation scheme can cover almost
all content, from phrase levels to discourse levels,
in computer science papers.
Note that this does not necessarily mean that our
framework can only be applied to computer sci-
ence literature. The characteristics of the schemata
described above are universal in contemporary sci-
ence and engineering, and many other activities in
human society. Thus, the framework presented in
this study can be viewed as a starting point for re-
search focusing on representative schemata of hu-
man activities.
2 Related Work
Traditionally, research on searching research pa-
pers has focused more on the social aspects of
papers and their authors, such as citation links
and co-authorship analysis implemented in the
aforementioned services. Recently, research on
content-based analysis of research papers has been
emerging.
For example, methods of document zoning have
been proposed for research papers in biomedicine
(Mizuta et al, 2006; Agarwal and Yu, 2009; Li-
akata et al, 2010; Guo et al, 2011; Varga et
al., 2012), and chemistry and computational lin-
guistics (Teufel et al, 2009). Zoning provides
a sentence-based information structure of papers
to help identify the components such as the pro-
posed method and the results obtained in the study.
As such, zoning can narrow down the sections of
a paper in which the answer to a query can be
found. However, zoning alone cannot always cap-
ture the relation between the concepts described in
the sections as it focuses on relation at a sentence
level. For example, the examples (1), (2), (3) in the
previous section require intra-sentence analysis to
capture the relation between CRF and POS tag-
ging. Our annotation scheme, which can be seen
as conplementary to zoning, attempts to provide
a structure for capturing the relationship between
concepts at a finer-grained level than a sentence.
Establishing semantic relations among scien-
tific papers has also been studied. For example,
the ACL Anthology Searchbench (Scha?fer et al,
2011) provides querying by predicate-argument
relations. The system accepts specifications of
subject, predicate, and object, and searches for
texts that semantically match the query using the
results from an HPSG parser. It can also search
by topics automatically extracted from the papers.
Gupta and Manning (2011) proposed a method for
extracting Focus, Domain, and Technique from pa-
pers in the ACL anthology: Focus is a research
article?s main contribution, Domain is an applica-
tion domain, and Technique is a method or a tool
used to achieve the Focus. The change in these as-
pects over time is traced to measure the influence
of research communities on each other. Fukuda et
al. (2012) developed a method of technical trend
analysis that can be applied to both patent appli-
cations and academic papers, using the distribu-
tion of named entities. However, as processes and
functions are key concepts in computer science,
elements are often described in a unit with its own
internal structures which include data, systems,
and other entities as substructures. Thus, tech-
nical concepts such as technique cannot be cap-
tured fully by extracting named entities. Gupta
and Manning (2011) analyzed the internal struc-
tures of concepts syntactically using a dependency
parser, but did not further investigate the structure
semantically.
In addition to the methodological aspects of re-
search, i.e., what techniques are applied to what
domain, a research paper can include other infor-
141
mation that we also want to capture, such as how
the author evaluates current systems and methods
or the previous efforts of others. An attempt to
identify the evaluation and other meta-aspects of
scientific papers was made by Thompson et al
(2011), which, on top of the biomedical events
annotated in the GENIA event corpus (Kim et
al., 2008), annotated meta-knowledge such as the
certainty level of the author, polarity (positive?
negative), and manner (strong?weak) of events, as
well as source (whether the event is attributed to
the current study or previous studies), along with
the clue mentioned in the text. For in-domain
relations within and between the events, they re-
lied on the underlying GENIA annotation, which
maps events and their participants to a subset of
Gene Ontology (The Gene Ontology Consortium,
2000), a standard ontology in genome science.
We cannot assume the existence of standard do-
main ontology in the variety of domains to which
computer systems are applied, as was mentioned
in Section 1. On the other hand, using domain-
general linguistic frameworks, such as FrameNet
(Ruppenhofer et al, 2006) or the Lexical Concep-
tual Structure (Jackendoff, 1990) is also not sat-
isfactory for our purpose. These frameworks at-
tempt to identify the relations lexicalized by verbs
and their case arguments; however, they do not
consider discourse or other levels of linguistic rep-
resentation. In addition, relying on a linguistic the-
ory requires that annotators understand linguistics.
Most computer scientists, the best candidates for
performing the annotation task, would not have the
necessary knowledge of linguistics and would re-
quire training, which would increase costs for cor-
pus annotation.
3 Annotation Scheme
The principle is to employ a uniform structure to
represent semantic relations in scientific papers
in phrase-level, clause-level, inter-clause level,
and discourse-level structures. For this purpose,
a bottom-up strategy that identifies relations be-
tween the entities mentioned is used. This strat-
egy is similar to dependency parsing/annotation,
which identifies the relations between constituents
to find the overall structure of sentences.
We did not want the relations to be uncondi-
tionally concrete and domain-specific, because, as
mentioned in the previous section, new concepts
and relations that may not be expressed by pre-
In this paper, we propose a novel strategy for
parallel preconditioning of large scale linear
systems by means of a two-level approximate
inverse technique with AISM method. Accord-
ing to the numerical results on an origin 2400 by
using MPI, the proposed parallel technique of
computing the approximate inverse makes the
speedup of about 136.72 times with 16 proces-
sors.
Figure 1: Sample Abstract
defined (concrete, domain-specific) concepts and
relations may be created. For the same reason,
we did not set specific entity types on the basis of
domain ontology. We simply classified entities as
?general object,? ?specific object,? and ?measure-
ment.?
To illustrate our scheme, consider the two-
sentence abstract1 shown in Figure 12.
In the first sentence, we can read that a method
called two-level approximate inverse is used for
parallel preconditioning (1), the preconditioning
is applied to large-scale linear systems, the AISM
method is a subcomponent or a substage of the
two-level technique, and the author claims that the
use of two-level approximate inverse is a novel
strategy.
In the second sentence, we can read that the
author has conducted a numerical experiment,
the experiment was conducted on an origin 2400
(a computer system), message Passing Interface
(MPI, a standardized method for message passing)
was used in the experiment, the proposed parallel
technique was 136.72 times quicker than existing
methods, and the speedup was achieved using 16
processors.
In addition, by comparing the two sentences, we
can determine that the proposed parallel technique
in the second sentence refers to the parallel pre-
conditioning using two-level approximate inverse
mentioned in the first sentence. Consequently, we
can infer the author?s claim that the parallel pre-
conditioning using two-level approximate inverse
achieved 136.72 times speedup.
We define binary relations including
APPLY TO(A, B) (A method A is applied
to achieve the purpose B or used for do-
ing B), EVALUATE(A, B) (A is evaluated as
1Linjie Zhang, Kentaro Moriya and Takashi Nodera.
2008. Two-level Parallel Computation for Approximate In-
verse with AISM Method. IPSJ Journal, 48 (6): 2164-2168.
2Although the annotation was done for abstracts in
Japanese, we present examples in English except where we
discuss issues that we believe are specific to Japanese.
142
APPLY TO(two-level approximate inverse, parallel preconditioning)
APPLY TO(parallel preconditioning, large scale linear systems)
SUBCONCEPT(AISM method, two-level approximate inverse)
EVALUATE(two-level approximate inverse, novel)
RESULT(numerical results, 136.72 times speedup)
CONDITION(origin 2400, 136.72 times speedup)
APPLY TO(MPI, numerical results)
EVALUATE(the proposed parallel technique, 136.72 times speedup)
CONDITION(16 processors, 136.72 times speedup)
EQUIVALENCE(the proposed parallel technique, two-level approximate inverse)
Figure 2: Relations Found in the Sentences in Figure 1
B), SUBCONCEPT(A, B) (A is a part of B),
RESULT(A, B) (The result of experiment A is B),
CONDITION(A, B) (The condition A holds in
situation B), and EQUIVALENCE(A, B) (A and
B refer to the same entity), with which we can
express the relations mentioned in the example, as
shown in Figure 2.
Note that it is the use of two-level approximate
inverse for parallel preconditioning(A) that the au-
thor claims to be novel. However, the relation in A
is already represented by the first APPLY TO rela-
tion. Consequently, it is sufficient to annotate the
EVALUATE relation between two-level approxi-
mate inverse and novel. This is approximately
equivalent to paraphrasing the use of two-level ap-
proximate inverse for parallel preconditioning is
novel as two-level approximate inverse used for
parallel preconditioning is novel. The same holds
for the equivalence relation involving the proposed
method.
Expressing the content as the set of relations fa-
cilitates discovery of a concept that plays a par-
ticular role in the work. For example, if a reader
wants to know the method for achieving paral-
lel preconditioning, X, which satisfies the relation
APPLY TO(X, parallel preconditioning) must be
searched for. By using the APPLY TO relations
mentioned in Figure 2 and inference on an is-a re-
lation expressed by the SUBCONCEPT, we can ob-
tain the result that AISM method is used for paral-
lel preconditioning.
After a series of trial annotations on 71 abstracts
from the IPSJ Journal (a monthly peer-reviewed
journal published by the Information Processing
Society of Japan), the following tag set was fixed.
The annotation was conducted by the two of the
authors of this paper.
3.1 Entity and Relation Types
The current tag set has 16 relation types and three
entity types. An entity is whatever can be an argu-
Type Definition Example
OBJECT the name of concrete entities such as
a system, a person, and a company
Origin
2400, SGI
MEASURE value, measurement, necessity, obli-
gation, expectation, and possibility
novel,
136.72
TERM any other
Table 1: Entity Tags
ment or a participant in a relation. Entity types
are OBJECT, MEASURE, or TERM, as shown in
Table 1. Note that, unlike most schemes where
the term entity refers to a nominal (named entity),
in our scheme, almost all syntactic types of con-
tent words can be an entity, including numbers,
verbs, adjectives, adverbs, and even some auxil-
iaries. The 16 types of relations are shown in Ta-
ble 2. They are binary relations are directed from
A to B.
All relations except EVALUATE COMPARE, and
ATTRIBUTE can hold between any types of en-
tity. EVALUATE and COMPARE relations hold
between an entity (of any type) and an entity
of the MEASURE type. The entities involved
in an ATTRIBUTE relation must not be of the
MEASURE type.
The INPUT and OUTPUT relations were intro-
duced to deal with the distinction between the data
and method used in computer systems. We ex-
tend the use of the scheme to annotate the in-
ner structure of sentences and predicates, by es-
tablishing the relations between verbs and their
case elements. For example, in automatically
generated test data, obviously test data is an
output of the action of generate, and automati-
cally is the manner of generation. We annotate
the test data as an OUTPUT and automatically
as an ATTRIBUTE of generate. In another ex-
ample, a protocol that combines biometrics and
zero-knowledge proof, the protocol is the product
of an action of combining biometrics and zero-
143
Type Definition Example
APPLY TO(A, B) A method A is applied to achieve the purpose B or used for
conducting B
CRFA-based taggerB
RESULT(A, B) A results in B in the sense that B is either an experimental
result, a logical conclusion, or a side effect of A
experimentA shows the increaseB in F-
score compared to the baseline
PERFORM(A, B) A is the agent of an intentional action B a frustrated playerA of a gameB
INPUT(A, B) A is the input of a system or a process B, A is something
obtained for B
corpusA for trainingB
OUTPUT(A, B) A is the output of a system or a processB, A is something
generated from B
an imagea displayedB on a palm
TARGET(A, B) Ais the target of an action B, which does not suffer alteration to driveB a busA
ORIGIN(A, B) A is the starting point of action B to driveB from ShinjukuA
DESTINATION(A, B) A is the ending point of action B an image displayedB on a palmA
CONDITION(A, B) The condition A holds in situation B, e.g, time, location, ex-
perimental condition
a surveyB conducted in Indiaa
ATTRIBUTE(A, B) A is an attribute or a characteristic of B accuracyA of the taggerB
STATE(A, B) A is the sentiment of a person B other than the author, e.g. a
user of a computer system or a player of a game
a frustratedA playerB of a game
EVALUATE(A, B) A is evaluated as B in comparison to C experiment shows an increaseB
COMPARE(C, B) in F?scoreA compared to the baselineC
SUBCONCEPT(A, B) A is-a, or is a part-of B a corpusA such as PTBa
EQUIVALENCE(A, B) terms A and B refer to the same entity: definition, abbrevia-
tion, or coreference
DoSB (denial ? of ? serviceA) attack
SPLIT(A, B) a term is split by parenthesical expressions into A and B DoSB (denial-of-service) attackA
Table 2: Relation Tags
knowledge proof. Therefore, both biometrics and
zero-knowledge proof are annotated as INPUTs of
combines, and protocol is annotated as OUTPUT
of combines. This scheme is not only used for
computer-related verbs, but is further extended
to any verb phrases or phrases with nominalized
verbs. In change in a situation, situation is an-
notated as both INPUT and OUTPUT of change.
It is as if we regard change as a machine that
changes something, and when we input a situa-
tion, the change-machine processes it and output
a different situation. Similarly, in evolution of mo-
bile phones, mobile phones is annotated as both
INPUT and OUTPUT of evolution. Here we re-
gard evolution as a machine, and when we input
(old-style) mobile phones, the evolution-machine
processes them and outputs (new-style) mobile
phones. We have found that a wide variety of pred-
icates can be interpreted using these relations.
3.2 Other Features
Although we aim to annotate all possible relations
mentioned, some conventions are introduced to re-
duce the workload.
First, we do not annotate the structure within
entities. No nested entities are allowed, and com-
pound words are treated as a single word. In ad-
dition, polarity (negation) is not expressed as a re-
lation but as a part of an entity. We assume that
the internal structure of entities can be analyzed
by mechanisms such as technical term recognition.
On the other hand, nested and crossed relations are
allowed.
Second, we do not annotate words that indicate
the existence of relations. This is because the re-
lations are usually indicated by case markers and
punctuation 3 and marking them up was found to
be a considerable mental workload. In addition,
words and phrases that directly represent the re-
lations themselves are not annotated as entities.
For example, in CG iteration was applied to the
problem, we directly CG relation and the problem
directly with APPLY TO and skip the phrase was
applied to.
Third, relations other than EQUIVALENCE and
SUBCONCEPT are annotated within a sentence.
We assume that the discourse-level relation can be
inferred by the composition of relations.
In addition, the annotation of frequent verbs and
their case elements was examined in the trial pro-
cess. Verbs were classified, according to the pat-
tern of the annotated relation with the case ele-
ments. For example, verbs semantically similar to
assemble and compile form a class. The semantic
role of the direct object of these verbs varies by
context. For example, the materials in phrases like
compile source codes or the product in phrases like
3This is in the case with Japanese. In languages such as
English, there may be no trigger words, as the semantic rela-
tions are often expressed by the structure of sentences.
144
compile the driver from the source codes. In our
scheme, the former is the INPUT of the verb, and
the latter is the OUTPUT of the verb. Another ex-
ample is the class of verbs that includes learn and
obtain. The direct object (what is learned) is the
INPUT to the system but is also the result or an
output of the learning process. In such cases, we
decided that both INPUT and OUTPUT should be
annotated between the verb and its object.
Other details of annotation fixed in the process
of trial annotation include:
1) The span of entities, which is determined to be
the longest possible sequences delimited by case
suffix (-ga,-wo, etc.) in the case of nominals and to
separate the -suru suffix of verbs and the -da suffix
of adjectives but retain other conjugation suffixes;
2) How to annotate evaluation sentences involv-
ing nouns derived from adjectives that imply eval-
uation and measurement, such as necessity, diffi-
culty, and length. The initial agreement was that
we would consider that they lose MEASURE-ness
when nominalized; however, with the similarity of
Japanese expressions hitsuyou/mondai de aru (is
necessary/problematic) and hitsuyou/mondai ga
aru(there is a necessity/problem), there was con-
fusion about which word should be the MEASURE
argument necessary for the EVALUATE relation.
It was determined that, for example, in hit-
suyou/mondai de aru, de aru, a copula, is ig-
nored and hitsuyou/mondai is the MEASURE. In
hitsuyou/mondai ga aru, aru is the MEASURE;
3) How to annotate phrases like the tagger was
better in precision, where it can be understood that
the system is evaluated as being better in precision.
While what is actually measured in the evaluation
process described in the paper is the precision (an
attribute) of the tagger and the sentence has almost
the same meaning as the tagger?s precision was
better, the surface (syntactic) subject of is better
is the tagger. This can lead to two possibilities
for the target of the EVALUATE relation. We de-
cided that the EVALUATE relation holds between
precision and better, and the ATTRIBUTE relation
holds between precision and tagger, as illustrated
in Figure 3.
A set of annotation guidelines was compiled as
the result of the trial annotation, including the clas-
sifications and the pattern of annotation on fre-
quent verbs and their arguments.
Figure 3: Annotation of the tagger was better in
precision
Entity Relation
Conunt % Conunt %
Total 1895 100.0 Total 2269 100.0
OK 1658 87.5 OK 1110 48.9
Type 56 3.0 Type 250 11.0
Span 67 3.5 Direction 6 0.3
Direction+Type 106 4.7
None 114 6.0 None 797 35.1
Table 3: Tag Counts
4 Annotation Experiment
We conducted an experiment on another 30 ab-
stracts (197 sentences) from the IPSJ Journal. The
two annotators who participated in the develop-
ment of the guidelines annotated the abstracts in-
dependently, and inter-annotator discrepancy was
checked. The annotation was performed man-
ually using the brat annotation tool(Stenetorp et
al., 2012). No automatic preprocessing was per-
formed. Figure 4 shows the annotation results for
the abstract shown in Figure 1. The 30 pairs of an-
notation results were aligned automatically; The
results are shown in Tables 3, 4, and 5.
Table 3 shows the matches between the two
annotators. ?Total? denotes the count of enti-
ties/relations that at least one annotator found,
?OK? denotes complete matches, ?Type? denotes
cases where two annotations on the same span
have different entity/relation types, ?Span? de-
notes entities where two annotations partially
overlap, ?Direction? denotes the count of relations
where (only) the direction is different, and ?Direc-
tion+Type?denotes relations where the same pair
of entities were in different types of relation and
in opposite directions, and ?None? denotes cases
where no counterpart was found in the other re-
sult.
Tables 4 and 5 are the confusion matrices for
entity type and relation type, respectively. The
differences in the span and direction are ignored.
Agreement in F-score calculated in the same man-
ner as in Brants (2000) for each relation is shown
in column F, with the overall (micro-average) F-
score shown in the bottom row of column F.
If we assume the number of cases that none of
145
Figure 4: Annotation Results with brat
TERM OBJECT MEASURE NONE Total F(%)
TERM 1458 2 38 14 1512 94.9
OBJECT 0 17 0 0 17 94.4
MEASURE 28 0 238 18 284 83.8
None 74 0 8 X 82
Total 1560 19 284 32 93.0
Table 4: Confusion Matrix for Entity
the annotators recognized (the value of the cell
X in the tables) to be zero, the observed agree-
ment and Cohen?s ? coefficient are 90.3% and
70.0% for entities, and 49.3% and 43.5% for re-
lations, respectively. If we ignore the count for the
cases where one annotator did not recognize the
entity/relation (?None? rows and columns in the
tables), the observed agreement and ? are 96.1%
and 89.3% for entities, and 76.1% and 74.3% for
relations, respectively. The latter statistics indi-
cate the agreement on types for entities/relations
that both annotators recognized.
These results show that entity annotation was
consistent between the annotators but the agree-
ment for relation annotation varied, depending on
the relation type. Table 5 shows that agreement
for DESTINATION, ORIGIN, EVALUATE, and
SPLIT was reasonably high, but was low for
CONDITION and TARGET. The rise in agreement
(simple and ?) by excluding cases where only one
annotator recognized the relation indicate that the
problem is recognition, rather than classification,
of relations4.
From the investigation of the annotated text, the
following was found:
(1) ATTRIBUTE/CONDITION decision was in-
consistent in phrases involving EVALUATE rela-
tion, such as the disk space is smaller for the im-
age (Figure 5). The EVALUATE relation between
the disk space and smaller was agreed; however,
the two annotators recognized different relations
between the image and other words. One annota-
4The same observation was true for entities
tor recognized the ATTRIBUTE relation between
the disk space and the image (?the disk space as a
feature of the image is smaller?). The other recog-
nized the CONDITION relation between the image
and smaller (?the disk space is smaller in the case
of the image?).
(2) We were not in complete agreement about
skipping phrases that directly represent a relation.
The expressions to be skipped in the 71 trial ab-
stracts were listed in the guidelines; however, it is
difficult to exhaust all such expressions.
(3) In the case of some verbs, an argument can
be INPUT and OUTPUT simultaneously (Section
3.1). We agreed that an object that undergoes alter-
ation in a process should be tagged as both INPUT
and OUTPUT but one that does not undergo al-
teration or which is just moved is the TARGET.
Conflicts occurred for verbs that denote preven-
tion of some situations such as prevent, avoid, and
suppress, as illustrated in Figure 6. One annota-
tor claimed that the possibility of DoS attacks is
reduced to zero; hence the argument of the verb
should be annotated with INPUT and OUTPUT.
The other claims that since the DoS attack itself
does not change, it is a TARGET.
(4) In a coordination expression, logical inference
may be implicitly stated. For example, in it re-
quires the linguistic knowledge and is costly, the
reason for costly is likely to be the need for lin-
guistic knowledge, i.e., employment of an expert
linguist. However, the relation is not readily ap-
parent. We wanted to capture the relation in such
cases, but the disagreement shows that it is diffi-
cult to judge such a relation consistently.
(5) The decision on whether to split expressions
like XX dekiru and XX kanou (can/able to XX) was
also problematic. The guideline was to split them.
This contradicts the decision for the compound
words in general that we do not split them; how-
ever, we determined that dekiru/kanou cases had
146
APP ATT COMP COND DEST EQU EVAL IN ORIG OUT PER RES SPL STA SUB TAR None Total F(%)
APPLY TO 136 9 0 2 1 1 2 10 1 0 0 3 0 0 1 0 65 231 53.0
ATTRIBUTE 14 154 0 19 6 0 9 5 1 0 7 1 0 0 3 0 28 247 59.7
COMPARE 0 0 6 1 0 0 0 0 0 0 0 0 0 0 0 0 4 11 54.5
CONDITION 4 11 1 77 0 0 1 4 0 0 0 5 0 0 0 0 49 152 48.7
DESTINATION 6 0 0 0 39 0 0 0 0 1 0 0 0 0 0 0 4 50 77.2
EQUIVALENCE 4 1 0 1 0 54 0 0 0 0 0 0 0 0 4 0 23 87 60.0
EVALUATE 0 11 0 0 0 0 215 3 0 9 0 0 0 0 0 1 41 280 76.1
INPUT 12 2 0 0 0 1 4 96 0 11 0 0 0 0 0 9 15 150 58.7
ORIGIN 0 0 0 0 0 0 0 0 16 0 0 0 0 0 0 0 2 18 78.0
OUTPUT 2 1 0 3 0 0 4 23 0 141 0 0 0 0 0 18 37 229 56.5
PERFORM 1 0 0 0 0 0 0 0 0 0 19 0 0 0 0 0 2 22 74.5
RESULT 8 1 0 0 0 0 1 1 0 0 0 38 0 0 0 0 22 71 54.3
SPLIT 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 2 80.0
STATE 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
SUBCONCEPT 14 10 0 3 0 4 5 0 0 2 0 0 0 0 81 0 34 153 58.1
TARGET 6 2 1 3 2 0 7 12 0 14 1 0 0 0 0 42 6 96 47.7
None 75 67 3 55 3 33 37 23 5 92 2 22 1 0 37 10 X 465
Total 282 269 11 164 51 93 285 177 23 270 29 69 3 0 126 80 332 59.8
Table 5: Confusion Matrix for Relation
Figure 5: ATTRIBUTE/CONDITION Disagreement
Figure 6: INPUT/OUTPUT/TARGET Disagreement
to be exceptions because the possibility of XX is
expressed by dekiru/kanou and it seemed natural
to relate XX and dekiru/kanou with EVALUATE.
Unfortunately, confusion about splitting them re-
mains.
5 Conclusions
We set up a scheme to annotate the content of re-
search papers comprehensively. Sixteen semantic
relations were defined, and guidelines for anno-
tating semantic relations between concepts using
the relations were established. The experimen-
tal results on 30 abstracts show that fairly good
agreement was achieved, and that while entity-
and relation-type determination can be performed
consistently, determining whether a relation exists
between particular pairs of entities remains prob-
lematic. We also found several discrepancy pat-
terns that should be resolved and included in a fu-
ture revision of the guidelines.
Traditionally, in semantic annotation of texts
in the science/engineering domains, corpus cre-
ators focus on specific types of entities or events
in which they are interested. On the other hand,
we did not assume such specific types of entities
or events, and we attempted to design a scheme
that annotates more general relations in computer
science/engineering domain.
Although the annotation is conducted for com-
puter science abstracts in Japanese, we believe the
scheme can be used for other languages, or for
the broader science/engineering domains. The an-
notated corpus can provide data for constructing
comprehensive semantic relation extraction sys-
tems. This would be challenging but worthwhile
since such systems are in great demand. Such
relation extraction systems will be the basis for
content-based retrieval and other applications, in-
cluding paraphrasing and translation.
The abstracts annotated in the course of the ex-
periment have been cleaned up and are available
on request. We are planning to increase the vol-
ume and make the corpus widely available.
In the future, we will assess machine-learning
performance and incorporate the relation extrac-
tion mechanisms into search systems. Comparison
of the annotated structure and the structures that
can be given by existing semantic theories could
be an interesting theoretical subject for future re-
search.
Acknowledgments
This study was partially supported by the Japan
Ministry of Education, Culture, Sports, Science
and Technology Grant-in-Aid for Scientific Re-
search (B) No. 22300031.
147
References
Shashank Agarwal and Hong Yu. 2009. Automatically
classifying sentences in full-text biomedical articles
into introduction, methods, results and discussion.
Bioinformatics, 25(23):3174?3180.
Rafael E. Banchs, editor. 2012. Proceedings of the
ACL-2012 Special Workshop on Rediscovering 50
Years of Discoveries. Association for Computational
Linguistics.
Thorsten Brants. 2000. Inter-annotator agreement for
a German newspaper corpus. In Proceedings of the
Second International Conference on Language Re-
sources and Evaluation.
Satoshi Fukuda, Hidetsugu Nanba, and Toshiyuki
Takezawa. 2012. Extraction and visualization of
technical trend information from research papers
and patents. In Proceedings of the 1st International
Workshop on Mining Scientific Publications.
Yufan Guo, Anna Korhonen, and Thierry Poibeau.
2011. A weakly-supervised approach to argumen-
tative zoning of scientific documents. In Proceed-
ings of the 2011 Conference on Empirical Methods
in Natural Language Processing, pages 273?283.
Sonal Gupta and Christopher D Manning. 2011. An-
alyzing the dynamics of research by extracting key
aspects of scientific papers. In Proceedings of 5th
IJCNLP.
Ray Jackendoff. 1990. Semantic Structures. The MIT
Press.
Jin-Dong Kim, Tomoko Ohta, and Jun ichi Tsujii.
2008. Corpus annotation for mining biomedical
events from literature. BMC Bioinformatics, 9.
Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, Robert
Bossy, Ngan Nguyen, and Jun?ichi Tsujii. 2011.
Overview of bionlp shared task 2011. In Proceed-
ings of BioNLP Shared Task 2011 Workshop, pages
1?6.
Maria Liakata, Simone Teufel, Advaith Siddharthan,
and Colin Batchelor. 2010. Corpora for concep-
tualisation and zoning of scientific papers. In Pro-
ceedings of LREC 2010.
Yoko Mizuta, Anna Korhonen, Tony Mullen, and Nigel
Collier. 2006. Zone analysis in biology articles as a
basis for information extraction. International Jour-
nal of Medical Informatics, 75(6):468?487.
Josef Ruppenhofer, Michael Ellsworth, Miriam R.L.
Petruck, Christopher R. Johnson, and Jan Schef-
fczyk. 2006. FrameNet II: Extended Theory and
Practice. International Computer Science Institute.
Ulrich Scha?fer, Bernd Kiefer, Christian Spurk, Jo?rg
Steffen, and Rui Wang. 2011. The ACL anthology
searchbench. In Proceedings of the ACL-HLT 2011
System Demonstrations, pages 7?13.
Pontus Stenetorp, Sampo Pyysalo, Goran Topic?,
Tomoko Ohta, Sophia Ananiadou, and Jun?ichi Tsu-
jii. 2012. brat: a web-based tool for NLP-assisted
text annotation. In Proceedings of the Demonstra-
tions Session at EACL.
Simone Teufel, Advaith Siddharthan, and Colin Batch-
elor. 2009. Towards discipline-independent ar-
gumentative zoning: evidence from chemistry and
computational linguistics. In Proceedings of the
2009 Conference on Empirical Methods in Natural
Language Processing, pages 1493?1502.
The Gene Ontology Consortium. 2000. Gene ontol-
ogy: tool for the unification of biology. Nature Ge-
netics, 25(1):25?29.
Paul Thompson, Raheel Nawaz, John McNaught, and
Sophia Ananiadou. 2011. Enriching a biomedi-
cal event corpus with meta-knowledge annotation.
BMC Bioinformatics, 12.
Andrea Varga, Daniel Preotiuc-Pietro, and Fabio
Ciravegna. 2012. Unsupervised document zone
identification using probabilistic graphical models.
In Proceedings of LREC 2012, pages 1610?1617.
148
