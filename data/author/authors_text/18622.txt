Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically Rich Languages, pages 97?107,
Seattle, Washington, USA, 18 October 2013. c?2013 Association for Computational Linguistics
An Empirical Study on the Effect of Morphological and Lexical Features in
Persian Dependency Parsing
Mojtaba Khallash, Ali Hadian and Behrouz Minaei-Bidgoli
Department of Computer Engineering
Iran University of Science and Technology
{khallash,hadian}@comp.iust.ac.ir, b minaei@iust.ac.ir
Abstract
This paper investigates the impact of dif-
ferent morphological and lexical information
on data-driven dependency parsing of Per-
sian, a morphologically rich language. We
explore two state-of-the-art parsers, namely
MSTParser andMaltParser, on the recently re-
leased Persian dependency treebank and es-
tablish some baselines for dependency pars-
ing performance. Three sets of issues are
addressed in our experiments: effects of us-
ing gold and automatically derived features,
finding the best features for the parser, and
a suitable way to alleviate the data sparsity
problem. The final accuracy is 87.91% and
88.37% labeled attachment scores for Malt-
Parser and MSTParser, respectively.
1 Introduction
Researchers have paid a lot of attention to data-
driven dependency parsing in recent years (Bohnet
and Kuhn, 2012; Bohnet and Nivre, 2012; Balles-
teros and Nivre, 2013). This approach is language-
independent and is solely dependent on the availabil-
ity of annotated corpora. Using data-driven parsers
for some languages requires careful selection of fea-
tures and tuning of the parameters to reach maxi-
mum performance. Difficulty of dependency pars-
ing in each language depends on having either free
word order or morphological information. Lan-
guages with free word order have a high degree
of freedom in arranging the words of a sentence.
Consequently, they usually have a high percentage
of non-projective structures. Morphology is deter-
mined by large inventory of word forms (Tsarfaty et
al., 2010).
According to the results from CoNLL shared
task 2007, languages are classified to three classes,
namely low, medium and high accuracy languages.
Among them, low-accuracy languages have high de-
gree of free word order along with inflection (Nivre
et al, 2007a). Languages which are more challeng-
ing in parsing are called morphologically rich lan-
guages (MRLs). In MRLs, multiple levels of infor-
mation, concerning syntactic units and relations, are
expressed at the word-level (Tsarfaty et al, 2010).
Free word order can be handled by non-projective
parsing algorithms via either post-processing the
output of a strictly projective parser (Nivre and
Nilsson, 2005), combining adjacent (Nivre, 2009)
or non-adjacent sub-structures (McDonald et al,
2005). Nevertheless, there is no general solution
for resolving rich morphology issue and hence many
researcher focus on features of a specific language.
Most data-driven dependency parsers do not use any
information that is specific to the language being
parsed, but it is shown that using language specific
features has a crucial role in improving the overall
parsing accuracy (Ambati et al, 2010a).
Persian is an Indo-European language that is writ-
ten in Perso-Arabic script (written from right to
left). The canonical word order of Persian is SOV
97
(subject-object-verb), but there are a lot of frequent
exceptions in word order that turn this language into
a free word order language (Shamsfard, 2011). This
language has a high degree of free word order and
complex inflections. As an example of rich mor-
phology, there are more than 100 conjugates and
2800 declensions for some lemmas in Persian (Ra-
sooli et al, 2011).
Dependency treebank for Persian (Rasooli et al,
2013) language has newly become available. Due to
the lack of deep research on dependency parsing in
Persian, we establish some baselines for dependency
parsing performance. We also conduct a set of ex-
periments in order to estimate the effect of errors in
morphological disambiguation on the parsers. We
show that with two simple changes to the input data,
performance of the two parsers can be improved for
both gold (manually annotated) and predicted data.
The remainder of the paper is organized as fol-
lows. Section 2 presents a brief overview of recent
studies on parsing morphologically rich languages.
In section 3, we introduce available morphological
features annotated in our experiments. Section 4 de-
scribes the experimental setup, including corpus and
parsers we use, and presents our experiments. Ex-
perimental evaluation and analysis of parsing errors
are demonstrated in Section 5. Finally, we draw con-
clusions and suggest future work in Section 6.
2 Related work
Many studies have been done on using morpholog-
ical features for parsing morphologically rich lan-
guages, (e.g. Bengoetxea and Gojenola (2010),
Seeker and Kuhn (2013), etc.). Koo et al (2008) in-
troduce cluster-based features that incorporate word
clusters derived from a large corpus of plain text, to
improve statistical dependency parsing for English
and Czech. Agirre et al (2011) use lexical semantic
information derived from WordNet.
Marton et al (2011) augment the baseline model
for Arabic with nine morphological features. They
show that using predicted features causes a substan-
tial drop in accuracy while it greatly improves per-
formance in the gold settings. They show that us-
ing noisy morphological information is worse than
using nothing at all. Same phenomenon is re-
ported for Hebrew (Goldberg and Elhadad, 2010),
except that using morphological-agreement feature
improves the accuracy of both gold and predicted
morphological information.
Another interesting research direction is to find
the most beneficial features for dependency parsing
for each language. Ambati et al (2010b) explored
the pool of features for Hindi through a series of ex-
periments. In their setting, features are incremen-
tally selected to create the best parser feature set. In
Korean, Choi and Palmer (2011b) focus on feature
extraction and suggest a rule-based way of selecting
important morphemes to use only these as features
to build dependency parsing models.
For the Persian language, Seraji et al (2012b) in-
vestigated state-of-the-art dependency parsing algo-
rithms on UPDT1 (Seraji et al, 2012a). They test
three feature settings, namely gold POS tags for both
the training and the test sets (GG), gold POS tags for
the training set and auto-generated POS tags for the
test set (GA), and auto-generated POS tags for both
the training and the test sets (AA). The best result
is obtained in GG setting with 68.68% and 63.60%
LAS, for MaltParser (Nivre et al, 2007b) and MST-
Parser (McDonald et al, 2005) respectively. Using
AA and GA settings show worse results than GG,
namely 2.29% and 3.66% drop in accuracy for Malt-
Parser, and 1.8% and 3.23% drop for MSTParser.
They only explore the effect of gold and non-gold
POS tags with a small treebank with about 1,300
sentences. We apply GG and AA settings in our ex-
periments on a larger treebank that contains richer
morphological information. We define pool of 10
morphological and lexical semantic features in or-
der to create the best feature set for the parser.
3 Features of Persian
In this section, among possible morphological and
semantic features that exist in Persian, we briefly re-
view a subset of them that is either annotated in Per-
sian dependency treebank (Rasooli et al, 2013) or is
available from other studies.
3.1 Features from Treebank
Table 1 represents the features available in the Per-
sian dependency treebank, along with possible val-
ues for each feature.
1Uppsala Persian Dependency Treebank
98
Feature Values
Attachment {NXT, PRV, ISO}
Animacy {animate, inanimate}
Number {singular, plural}
Person {1, 2, 3}
Comparison {positive, comparative, superlative}
TMA see Table 2
Table 1: Description of features in Treebank
In some special cases, we have to break a word
into smaller parts in order to capture the syntac-
tic relations between the elements of the sentence.
For example, the two-word sentence XQ? ??'
 @Y? ?se-
dAyam kard? (called me), consist of three mor-
phemes: @Y? (calling), ??'
 (me), and XQ? (to do)
that have NXT (attached to the next word), PRV
(attached to the previous word), and ISO (isolated
word) attachment, respectively.
Person and number play a role in constraining
syntactic structure. Verbs usually agree with sub-
ject in person and number (Shamsfard, 2011). This
agreement is useful feature to detect subject of sen-
tence. for example in ?Y	JJ 	?P A? ?m